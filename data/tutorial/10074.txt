different approaches for identifying important concepts 

in probabilistic biomedical text summarization 

 

milad moradi, nasser ghadiri1 

 

department of electrical and computer engineering, isfahan university of technology, 

isfahan 84156-83111, iran 

e-mail: milad.moradi@ec.iut.ac.ir, nghadiri@cc.iut.ac.ir 

 

 

abstract   automatic text summarization tools help users in biomedical domain to acquire their intended 

information  from  various  textual  resources  more  efficiently.  some  of  the  biomedical  text  summarization 

systems put the basis of their sentence selection approach on the frequency of concepts extracted from the 

input  text.  however,  it  seems  that  exploring  other  measures  rather  than  the  frequency  for  identifying  the 

valuable content of the input document, and considering the correlations existing between concepts may be 

more useful for this type of summarization. in this paper, we describe a bayesian summarizer for biomedical 

text  documents.  the  bayesian  summarizer  initially  maps  the  input  text  to  the  unified  medical  language 

system (umls) concepts, then it selects the important ones to be used as classification features. we introduce 

different feature selection approaches to identify the most important concepts of the text and to select the most 

informative content according to the distribution of these concepts. we show that with the use of an appropriate 

feature  selection  approach,  the  bayesian  biomedical  summarizer  can  improve  the  performance  of 

summarization. we perform extensive evaluations on a corpus of scientific papers in biomedical domain. the 

results show that the bayesian summarizer outperforms the biomedical summarizers that rely on the frequency 

of  concepts,  the  domain-independent  and  baseline  methods  based  on  the  recall-oriented  understudy  for 

gisting evaluation (id8) metrics. moreover, the results suggest that using the meaningfulness measure 

and considering the correlations of concepts in the feature selection step lead to a significant increase in the 

performance of summarization. 

keywords   medical id111; data mining; bayesian classification; feature selection; umls concept; 

sentence classification; 

                                                            
1 corresponding author. address: department of electrical and computer engineering, isfahan university of technology, 
isfahan 84156-83111, iran. phone : +98-31-3391-9058, fax: +98-31-3391-2450, alternate email: nghadiri@gmail.com 

manuscript                                                                  1                                                                    29 march 2017 

 

1.  introduction 

biomedical information available for researchers and clinicians is accessible from a variety of sources such 

as scientific literature databases, electronic health record (ehr) systems, web documents, e-mailed reports 

and  multimedia  documents  [1,  2].  the  scientific  literature  provides  a  valuable  source  of  information  to 

researchers.  it  is  widely  used  as  a  rich  source  for  assessing  the  newcomers  in  a  particular  field,  gathering 

information for constructing research hypotheses, and collecting information for interpretation of experimental 

results [3]. it is interesting to know that the us national library of medicine has indexed over 24 million 

citations from more than 5,500 biomedical journals in its medline bibliographic database2. however, a large 

amount of data cannot be effectively used to attain the desirable information in a limited time. the required 

information should be accessed easily at the right time, and in the most appropriate form [2]. for clinicians 

and researchers, efficiently seeking useful information from the ever-increasing body of knowledge and other 

resources is excessively time-consuming. managing this information overload is shown to be a difficult task 

without the help of automatic tools. 

automatic  text  summarization  is  a  promising  approach  to  overcome  the  information  overload  problem, 

reducing the amount of text that must be read [4]. it can be used to obtain the gist efficiently on a topic of 

interest [1]. it helps the clinicians and researchers to save their time and effort required to seek information. 

five reasons have been identified for producing summaries from full-text documents even when they provide 

abstracts [4]. the reasons include 1) there are variants of an ideal summary in addition to the abstract, 2) some 

content  of  the  full-text  may  be  missed  in  the  abstract,  3)  customized  summaries  are  useful  in  question 

answering systems, 4) automatic summaries allow abstract services to scale the number of documents they can 

evaluate, and 5) assessing the quality of sentence selection methods can be helpful in development of multi-

document summarization systems [4]. 

in recent years, various summarization methods have been proposed based on biomedical concepts [4-10]. 

they have improved the performance of biomedical text summarization, focusing on concepts extracted from 

the source text rather than terms. this concept-level analysis of text is performed with the help of biomedical 

knowledge  sources,  such  as  the  unified  medical  language  system  (umls).  it  has  been  shown  that 

constructing a frequency distribution model from the concepts of the original text and following this model to 

create  the  summary  yields  better  performance  compared  to  traditional  term-based  methods  [10].  another 

successful biomedical summarization approach relies on concept chaining, identifying strong chains based on 

the  frequency  of  their  contained  concepts,  and  sentence  scoring  according  to  the  presence  of  the  relevant 

concepts from the strong chains [9]. regarding such summarization methods, some questions should be taken 

into account for summarizing a document. do similar approaches, like id203 distribution modeling of 

concepts,  yield  a  desirable  summarization  performance  for  concept-based  biomedical  text  summarization? 

                                                            
2 http://www.nlm.nih.gov/databases/databases_medline.html 

manuscript                                                                  2                                                                    29 march 2017 

 

should the summarizer consider all the concepts extracted from the source text? are there any concepts which 

can be regarded as redundant and removed to increase the accuracy of the frequency or id203 distribution 

model? are there any criteria rather than the frequency to identify the important concepts? can the model be 

more accurate by considering the correlations existing between concepts in the source text? in this paper, we 

address these questions, describing a bayesian summarization method based on the id203 distribution of 

concepts within the input document. we also introduce and evaluate different feature selection approaches to 

select the relevant concepts of the text and to use them as the classification features. 

the bayesian summarizer initially maps the input text to biomedical concepts contained in the umls [11], 

a well-known knowledge source in biomedical sciences maintained by the us national library of medicine. 

then, it identifies important concepts and selects them as classification features. to this end, we discuss five 

different feature selection strategies based on various criteria and methods. the first strategy is the simplest 

one  that  considers  all  the  extracted  concepts.  the  second  method  discards  the  concepts  which  seem  to  be 

potentially redundant and unnecessary. the third method filters the concepts using the ranking method and 

according to the frequency of the concepts. the fourth method utilizes a meaningfulness measure defined by 

the  helmholtz  principle  [12]  to  identify  the  important  concepts.  the  fifth  method  discovers  the  correlated 

concepts that  represent the  subtopics  of the  text  by  using  frequent  itemset  mining  [13],  a  well-known  data 

mining  technique.  after  the  feature  selection  step,  the  summarizer  represents  each  sentence  as  a  vector  of 

boolean features and specifies the value of features according to the occurrence of important concepts in the 

sentence. afterwards, in the classification stage, it classifies the sentences into summary and non-summary 

classes using the na  ve bayes classification method [14]. the classifier estimates the posterior id203 of 

classifying  a  sentence  using  the  prior  and  likelihood  probabilities  of  concepts.  the  summarizer  selects  the 

sentences that obtain the highest posterior odds ratio (por) values and puts them together to form the final 

summary. 

to  evaluate  the  performance  of  the  proposed  method,  we  conduct  a  set  of  experiments  on  a  corpus  of 

scientific papers from the biomedical domain and compare the results with other concept-based biomedical 

summarizers. we also evaluate the usefulness of the five different feature selection approaches to determine 

the competency of each one for this type of summarization. the results demonstrate that when the bayesian 

summarizer uses the fourth and the fifth feature selection methods, it performs significantly better than the 

other  frequency-based  biomedical  summarizers  regarding  the  most  commonly  used  recall-oriented 

understudy for gisting evaluation (id8) metrics [15]. 

the main contributions of this paper are: 

     using  the  na  ve  bayes  classifier  in  concept-based  biomedical  text  summarization  for  classifying  the 

sentences of a document based on the id203 distribution of important concepts within the source 

text, 

manuscript                                                                  3                                                                    29 march 2017 

 

     evaluating different feature selection approaches to identify the important concepts of a document and 

using them as classification features, 

     using different measure, i.e. the meaningfulness, rather than the frequency to determine the important 

concepts of a document and using them as classification features, and 

     discovering the correlated concepts of a document using itemset mining and using them as classification 

features. 

the remainder of the paper is organized as follows. section 2 gives an overview of text summarization, as 

well as a review of the related work in biomedical summarization. in section 3, we introduce our biomedical 

summarization method based on a bayesian classifier. we also define different feature selection approaches 

that can be utilized in our summarization process. then, we describe the evaluation methodology in section 4. 

the results of the preliminary experiments and the evaluations are presented in section 5 and discussed in 

section 6. finally, section 7 draws the conclusion and describes future lines of work. 

 

2.  background and related work 

 

2.1.biomedical text summarization 

text  summarization  methods  can  be  divided  into  abstractive  and  extractive  approaches  [1,  16].  an 

abstractive summarizer uses natural language processing (nlp) methods to process and analyze the input 

text, then it infers and produces a new version. on the other hand, an extractive summarizer selects the most 

representative units (paragraphs, sentences, or phrases) from the original wording and puts them together into 

shorter form. another classification of text summarization differentiates  single-document and multi-document 

inputs [1, 2]. a single-document summarizer produces a summary which is the result of condensing only one 

document. in contrast, a multi-document summarizer gets a cluster of inputs and provides a single summary. 

another  classification  of  summarization  methods  is  based  on  the  requirements  of  users:  generic  vs.  user-

oriented  (also  known  as  query-focused  summarizers)  [1,  2,  17].  a  general  summary  presents  an  overall 

implication of input document(s) without any specified preferences regarding content. while a user-oriented 

summary  is  biased  towards  a  given  query  or  some  keywords  to  address  a  user   s  specific  information 

requirement. summarization systems can be supervised or unsupervised regarding whether they need training 

data [18]. a supervised system learns from labeled data to select the essential content of new documents, while 

an  unsupervised  system  generates  summaries  for  new  documents  without  relying  on  any  training  data.  in 

addition to the  above categorizations, there are  other types  of  summaries  including  indicative,  informative, 

multi-lingual, mono-lingual, cross-lingual, web-based, e-mail based, personalized, sentiment-based, survey, 

and update summaries [18]. the bayesian summarizer described in this paper is extractive, single-document, 

generic, and unsupervised. 

manuscript                                                                  4                                                                    29 march 2017 

 

in the biomedical field, various summarization methods have been proposed. these methods have been 

reviewed in a survey of early work [2] and in a systematic review of recently published research [1]. there 

have  been  some  research works  towards  abstractive biomedical summarization.  they  could be regarded as 

tools for providing decision support data from medline citations [19], summarizing research related to the 

treatment of diseases [20], helping in evidence-based medical care [21], summarizing drug information [22], 

and id57 of medline citations [23, 24]. these methods mostly produce graphical 

summaries.  on  the  other  hand,  the  majority  of  extractive  biomedical  summarization  systems  focus  on 

producing textual summaries. extractive summarization methods have been widely studied in the biomedical 

domain for different tasks, such as summarizing clinical notes [25], developing clinical decision support tools 

for patient-specific recommendation and treatment [26, 27], and the summarization of ehrs [28]. 

many biomedical summarizers utilize the umls knowledge source to map the input text to a wide range 

of  biomedical  and  generic  concepts.  this  mapping  helps  the  systems  to  be  domain-specific  and  act  more 

accurately compared to traditional term-based methods. there are several knowledge sources such as mesh, 

snomed,  go,  omim,  uwda  and  ncbi  taxonomy  widely  used  in  knowledge-intensive  data  and  text 

processing  tasks  in  the  biomedical  domain.  these  knowledge  sources  along  with  over  100  controlled 

vocabularies,  classification  systems,  and  additional  information  sources  have  been  unified into the  umls. 

plaza [29] performed an investigation on the impact of different knowledge sources on the performance of a 

summarization  system.  the  evaluations  showed  that  the  quality  of  generated  summaries  was  improved 

significantly  with  the  use  of  an  appropriate  knowledge  source.  we  make  use  of  the  umls  concepts  to 

incorporate the domain knowledge into the text modeling and the summarization process of our summarizer. 

some  of  the  biomedical  summarization  methods  employed  graph  representation  along  with  the  umls 

concepts  for  semantic  modeling  of  the  input  text.  plaza  et  al.  [5]  proposed  a  graph-based  approach  to 

biomedical  summarization.  they  used  the  umls  concepts  and  the  semantic  relations  between  them  to 

construct  a  semantic  graph  that is  representative  of the  input  document. their system  determined  different 

topics within the text by applying a degree-based id91 algorithm on the semantic graph. another work 

[30]  performed  the  task  of  summarization  based  on  a  genetic  graph-based  id91  algorithm.  using  the 

continuity of concept relations rather than the centroid method, it separated clusters and identified main topics. 

menendez et al. [8] applied a combination of both genetic id91 and graph connectivity information to 

improve the performance of the previous semantic graph-based summarization systems. compared to these 

approaches, our bayesian summarization method utilizes a simpler modelling, representing the sentences of 

the input document as vectors of features. the features are important concepts within the input text. 

merging the domain knowledge and traditional methods, some domain-specific tools have been proposed 

for biomedical summarization. one of the studies [31] identified a set of medical cue terms and phrases and 

combined  them  with  commonly  used  traditional  features  such  as  word  frequency,  sentence  position,  the 

similarity  with  the  title  of  the  article,  and  sentence  length.  the  summarizer  used  the  domain-specific  and 

manuscript                                                                  5                                                                    29 march 2017 

 

generic  features  for  sentence  scoring  and  summary  generation.  sarkar  et  al.  [32]  proposed  a  supervised 

summarization  method  based  on  id112  and  c4.5  decision tree  as the  base learner. they  utilized  the  key 

terms in mesh as a source of domain knowledge, as well as, other features including centroid overlap, first 

sentence overlap, sentence position, sentence length, and acronyms. in a hybrid summarization system [33], a 

classifier  was  utilized  to  learn  and  group  sentences  into  six  types  of  population,  intervention,  background, 

outcome,  study,  and  other.  the  system  also  used  a  learning  corpus  to  identify  important  umls  concepts 

commonly appearing in summaries. relative sentence position and sentence length were other features used 

by the summarizer. another study [7] evaluated different positional approaches for sentence extraction in a 

semantic  graph-based  method  for  biomedical  literature  summarization.  the  study  showed  that  sentences 

appearing in various sections of a biomedical article should be assigned different weights. we do not use any 

positional information in our summarization method. this allows the method to be applicable to input texts in 

which positional information may not be indicative of the importance and the informativeness of sentences. 

some of the biomedical summarizers use the frequency of umls concepts extracted from the source text 

as the  basis  of  their summarization  approach.  biochain  method  [9]  pursued the lexical  chaining  idea  [34], 

creating  chains  and  putting  each  group  of  semantically  related terms  into  a chain.  biochain used  concepts 

rather than terms and put concepts belonging to the same semantic type into a chain. it computed the score of 

each chain using the frequency information of concepts, identified strong chains and concepts, and selected 

summary sentences according to the presence of strong concepts. freqdist method [10] performed the task of 

sentence selection based on the frequency distribution of concepts within the source text. it initially created a 

frequency distribution model from the source text, also a summary frequency distribution model. using an 

iterative  sentence  selection  procedure,  it  selected  a  sentence  that  led  to  the  closest  alignment  between  the 

summary and original text frequency distributions in each iteration. 

in  contrast to  biochain  and  freqdist, our  method  does  not  merely  make  use  of  the  concept  frequency. 

employing  the  na  ve  bayes  classifier,  it  selects  the  sentences  according  to  the  id203  distribution  of 

concepts within the input text. it still benefits from the frequency information in the form of two coefficients 

that provide the classifier with additional knowledge. compared to freqdist, our method does not consider the 

distribution of all the extracted concepts. we evaluate different feature selection strategies to discard redundant 

and unnecessary concepts. compared to biochain, our method does not merely rely on the concept frequency 

to identify important concepts. we use another metric, namely the meaningfulness, in the form of a feature 

selection  method  that  yields  better  summarization  performance.  moreover,  as  one  of  the  feature  selection 

approaches, we extract correlated concepts and use each set of them as a classification feature. this correlation 

information provides the classifier with some additional knowledge to decide more accurately, leading to an 

increase in the performance of the summarizer. 

in  domain-independent  text  summarization,  some  methods  have  been  proposed  based  on  bayesian 

approach. one of the basic methods [35] employed a set of features such as sentence length cut-off, fixed-

manuscript                                                                  6                                                                    29 march 2017 

 

phrase, paragraph, thematic word, and uppercase word to represent the sentences of a text document. it trained 

a bayesian classifier on a training corpus and used the classifier to summarize new documents. bayesum 

[36], a supervised and query-focused multi-document summarizer built on bayesian id136 and language 

modeling  techniques,  represented  documents  and  queries  as  id203  distributions  of  words  from  a 

vocabulary. estimating a sentence model for each sentence, it ranked sentences based on the language model 

and the similarity of sentences to the query model. wang et al. [37] proposed a bayesian sentence-based topic 

model  for  multi-document  summarization.  they  employed  a  variational  bayesian  algorithm  to  model  the 

id203 distribution of selecting sentences given topics and to estimate the model   s parameters. compared 

to these methods, our biomedical summarizer does not use any complicated id96 approaches and 

does not need any training data. it uses the prior id203 of concepts to estimate the id203 of selecting 

sentences for inclusion in the summary. we give the na  ve bayes classifier some additional knowledge in the 

form of two coefficients and different feature selection approaches. 

to the authors    knowledge, no biomedical summarization method has been proposed so far based on the 

na  ve bayes classifier and estimating the id203 of summary sentences by following the distribution of 

concepts within the source text. the rationale of our approach will be presented in section 6.5 where it will be 

showed that for a corpus of 400 biomedical papers, the concepts within both the full-text papers and the ideal 

summaries (abstracts) follow the same distribution. 

2.2.bayesian classification 

our proposed summarization scheme consists of two main phases, preparation and classification. in the 

preparation  phase,  we  perform  concept  extraction,  feature  selection,  and  sentence  representation.  in  the 

classification phase, we utilize a bayesian classifier to select sentences for the final summary. in the following, 

we give an overview of bayesian classification [14]. 

in general, a bayesian classifier is based on the id47, defined by eq. 1 below: 

(cid:1)(cid:2)(cid:3)|(cid:5)(cid:6)=(cid:1)(cid:2)(cid:5)|(cid:3)(cid:6)(cid:1)(cid:2)(cid:3)(cid:6)
(cid:1)(cid:2)(cid:5)(cid:6)

 

(1) 

where c and x are random variables. in classification tasks, they refer to observing class c and instance x, 

class c given instance x. in classification, it could be interpreted as the id203 of instance x being in class 

respectively. x is a vector containing the values of features. (cid:1)(cid:2)(cid:3)|(cid:5)(cid:6) is the posterior id203 of observing 
c, and is what the classifier tries to determine. (cid:1)(cid:2)(cid:5)|(cid:3)(cid:6) is the likelihood, which is the id203 of observing 
instance x given class c. it is computed from the training data. (cid:1)(cid:2)(cid:3)(cid:6) and (cid:1)(cid:2)(cid:5)(cid:6) are the prior probabilities of 
c given instance x, and the most probable class, the class that maximizes (cid:1)(cid:2)(cid:3)|(cid:5)(cid:6), should be selected as the 

within the training data. using eq. 1, the classifier can compute the id203 of each class of target variable 

observing class c and instance x, respectively. they measure how frequent the class c and instance x are 

manuscript                                                                  7                                                                    29 march 2017 

 

result  of  classification.  this  decision  rule  is  known  as  maximum  a  posteriori  (map).  it  is  represented  as 

follows: 

(cid:3)=argmax(cid:13)(cid:14) (cid:1)(cid:2)(cid:5)|(cid:3) =(cid:3)(cid:15))(cid:1)((cid:3) =(cid:3)(cid:15))

(cid:1)((cid:5))

  

(2) 

where (cid:3)(cid:15)  is  the  jth class (or  value)  of  target  variable  c. in  eq. 2, the  denominator is removed  because  it is 
constant and does not depend on (cid:3)(cid:15). 

making  a  conditional  independence  assumption,  the  na  ve  bayes  classifier  reduces  the  number  of 

id203  values  that  must  be  estimated.  it  assumes  that  the  id203  of  each  value  of  feature  xi  is 

independent of the value of any other features, given the class variable cj. therefore, the na  ve bayes classifier 

finds the most probable class for the target variable by simplifying the joint id203 calculation as follows: 

(cid:3) =argmax(cid:13)(cid:14) (cid:1)((cid:3) =(cid:3)(cid:15))(cid:17)(cid:1)((cid:5)(cid:18)|(cid:3) =(cid:3)(cid:15))

(cid:18)

  

(3) 

the  posterior  odds  ratio  (por)  is  a  well-known  measure  to  assess  the  confidence  of  bayesian 

classification. the por shows a measure of the strength of evidence for a particular classification compared 

to other class values [38]. it is calculated as follows: 

(cid:1)(cid:19)(cid:20)(cid:18) = (cid:1)((cid:3) =(cid:3)(cid:15)|(cid:5))
(cid:1)((cid:3) =(cid:3)(cid:21)|(cid:5)) 

(4) 

where (cid:1)(cid:19)(cid:20)(cid:18)  is  the  posterior  odds  ratio  that  measures  the  strength  of  evidence  in  favor  of  classifying  the 
instance (cid:5) as class variable (cid:3) =(cid:3)(cid:15) against classifying the instance (cid:5) as class variable (cid:3) =(cid:3)(cid:21). 

 

3.  the bayesian summarizer 

our  bayesian  summarization  method  consists  of  five  steps  including  (1)  mapping  text  to  biomedical 

concepts, (2) feature selection, (3) preparing sentences for classification, (4) sentence classification using na  ve 

bayes,  and  (5)  summary  generation.  fig.  1  illustrates  the  architecture  of  the  bayesian  summarizer.  in  the 

following subsections, we give a detailed description of each step. 

manuscript                                                                  8                                                                    29 march 2017 

 

fig. 1. the architecture of our proposed bayesian biomedical text summarization method. 

 

3.1.mapping text to biomedical concepts 

 

firstly, the summarizer maps the input text to the concepts of the umls metathesaurus. the metathesaurus 

is  a  large,  multi-lingual,  and  multi-purpose  lexicon  that  contains  millions  of  biomedical  and  health  related 

concepts, their relationships and their synonymous names [39]. in addition to the metathesaurus, the umls 

includes two main components, namely specialist lexicon and semantic network. the specialist lexicon is 

a lexicographic information database intended to use in nlp systems. it contains commonly occurring english 

words  and  biomedical  vocabulary  and  records  their  syntactic,  morphological  and  orthographic  information 

[40].  the  semantic  network  consists  of  a  set  of  broad  subject  categories  known  as  semantic  types.  these 

semantic types provide a categorization of all the concepts included in the metathesaurus. it also contains a set 

of semantic relations between the semantic types [41]. 

for mapping biomedical text documents to the umls metathesaurus concepts, the us national library of 

medicine  has  developed  metamap  program  [42].  using  a  knowledge-intensive  approach  based  on  nlp, 

computational  linguistic  and  symbolic  techniques,  metamap  identifies  noun  phrases  in  a  text  and  extracts 

corresponding concepts. metamap may return multiple concepts when a noun phrase is mapped to more than 

one concept. in this situation, the summarizer selects all the mappings returned by metamap. it has been shown 

that the all mappings strategy can be useful in concept-based biomedical text summarization [43]. metamap 

returns a semantic type along with each concept that determines the semantic category of the concept. as noted 

manuscript                                                                  9                                                                    29 march 2017 

 

above, the semantic types are included in the umls semantic network. for the mapping step, we use the 

2016 version of metamap program and the 2015ab umls release as the knowledge base. fig. 2 shows a 

sample sentence and the concepts identified in the first step. 

fig. 2. a sample sentence and its identified concepts from the umls metathesaurus.  

 

3.2.feature selection 

in  this  step,  the  summarizer  identifies  important  concepts  within  the  input  document.  to  this  aim,  we 

introduce  five  feature  selection  approaches.  in  the  classification  step,  the  summarizer  uses  the  important 

concepts as classification features. we evaluate and discuss the impact of the feature selection approaches on 

the performance of the bayesian summarizer in section 5 and 6. 

in this subsection, we use a sample document3 to present some examples of the feature selection strategies. 

the sample document is a scientific article about genetic overlap between autism, schizophrenia and bipolar 

disorders. it contains 85 sentences. 

3.2.1.  first approach: using all extracted concepts as features 

the  simplest  approach  to  feature  selection  for  the  bayesian  summarizer  is  to  consider  all  the  distinct 

extracted  concepts.  it  means  that  the  summarizer  decides  about  the  summary  and  non-summary  sentences 

considering the distribution of all the contained concepts regardless of whether they are important or not. for 

example,  after  concept  extraction,  the  sample  document  contains  1042  concepts,  440  of  which  are  distinct 

concepts that are used as the classification features. from these 440 distinct concepts, 267 concepts appear 

only  one time  in the document   s  sentences,  and  the  three  most frequent  concepts  appear in 30,  20,  and 19 

sentences. 

we use the first feature selection approach as a baseline to assess the amount of improvement obtained by 

the  other  strategies.  it  also  shows  the  impact  of  redundant  and  noisy  features  on  the  performance  of  the 

summarization method. 

                                                           
3 available at: http://genomemedicine.biomedcentral.com/articles/10.1186/gm102 

manuscript                                                                  10                                                                    29 march 2017 

 

3.2.2.  second approach: filtering out generic features 

there are some semantic types that their concepts can be discarded in the analysis of the input document. 

they are generic and broad concepts and almost frequently appear in majority of documents either in general 

english and biomedical texts. these semantic types have been identified empirically [5] and include functional 

concept,  qualitative  concept,    quantitative  concept,  temporal  concept,  spatial  concept,  mental  process, 

language,  idea  or  concept,  and  intellectual  product.  in  the  second  feature  selection  method,  we  remove 

concepts that belong to these semantic types and use remaining concepts as classification features. in this way, 

we remove a set of potentially redundant and misleading features, and we expect an improvement in the quality 

of produced summaries. using this feature selection method, the summarizer discards the following concepts 

from  the  sentence  represented  in  fig.  2:  widening,  analysis  aspect,  further,  relationships  and  etiology 

aspects. 

as an example of the second approach, 234 distinct concepts remain after removing generic concepts from 

the sample document. 

3.2.3.  third approach: filtering features by ranking 

filtering is a category of feature selection methods [44]. in filtering methods, features are scored based on 

some ranking criteria such as relevance, correlation, mutual information, and the frequency of occurrence [44, 

45]. then, the high-ranked features are selected according to a threshold or a predefined number of features. 

for the third feature selection strategy, we filter features by the frequency of their corresponding concepts in 

the text. after mapping the input text to the umls concepts and removing the potentially redundant features 

(similar to the second approach), the summarizer adds remaining features to a list named feature_list. then, it 

ranks the features based on the frequency of corresponding concepts such that a higher rank is assigned to the 

feature that its corresponding concept appears more frequently. finally, it filters the features of the feature_list 

using a threshold. it removes a feature if its frequency is less than the threshold value. in the following, we 

introduce three possible thresholds: 

(cid:22)(cid:23) =(cid:24) 
(cid:22)(cid:25) =(cid:24)+(cid:27) 
(cid:22)(cid:28) =(cid:24)+2(cid:27) 

(5) 

(6) 

(7) 

where (cid:24) is the arithmetic mean of all concept frequencies in the feature_list, and (cid:27) is the standard deviation of 

all concept frequencies in the feature_list. 

in section 5, we evaluate these three thresholds and use the best one as an optimum threshold for the third 

feature selection method. 

manuscript                                                                  11                                                                    29 march 2017 

 

as  an  example,  when  we use  eq.  5 as the threshold value,  from  the  initial  440 distinct  concepts  of the 

sample document, 49 concepts are selected as features. when we use eq. 6 and eq. 7 as the threshold value, 

25 and eight features are selected, respectively. fig. 3 shows the identified important concepts for the sample 

document along with their semantic types and frequencies. in this example, we use eq. 7 as the threshold value. 

the (cid:24), (cid:27), and (cid:22)(cid:28) are equal to 2.435, 3.444, and 9.323 respectively. there are 234 concepts in the feature_list, 

eight of which are selected as features (fig. 3). 

 

fig. 3. the identified important concepts by filtering (ranking and selecting based on a given threshold) for the sample document. the 
semantic types are represented in brackets. the features are sorted based on their frequencies. 

3.2.4.  fourth approach: selecting meaningful features by the helmholtz principle 

the helmholtz principle from the gestalt theory of human perception defines a measure of meaningfulness 

for rapid change detection and keyword extraction in unstructured and textual data [12, 46]. in data mining 

context,  the  helmholtz  principle  states  that  essential  features  and  interesting  events  are  observed  in  large 

deviations from randomness [47]. in id111 research, the helmholtz principle has been used for document 

processing and keyword extraction [12], automatic text summarization [48], and supervised and unsupervised 

feature  selection  [49].  the  primary  study  [12]  dealt  with  words  to  extract  the  meaningful  units  of  a  text 

document, but we deal with concepts instead. 

in the fourth method, the feature selection process is modeled as follows: let d be the input document and 

p be a part of d. we consider p as a paragraph, but it can be any structural unit of the input document, such as 

a  sentence  or  a  page.  after  mapping  the  input  document  d  to  the  umls  concepts,  we  remove  generic 

(potentially redundant) concepts, similar to the second approach, and add remaining concepts to the set c.  for 

each concept c in the set c, we start to compute the meaningfulness measure by calculating the number of 

false alarms (nfa) in each p. if the concept c appears m times in the p and k times in the whole document 

d, then the nfa is calculated as follows: 

(cid:30)(cid:31) (!,(cid:1),#)=$%&    1(cid:30))*(cid:23) 

where n is equal to +,/./ that l is the total number of concepts in the document d, and b is the total number 
of concepts in the paragraph p. in eq. 8, 0(cid:21))1 is a binomial coefficient computed as follows: 

(8) 

manuscript                                                                  12                                                                    29 march 2017 

 

$%&   =

%!

&!(%3&)! 

to measure the meaningfulness value of concept c from d inside p, the following formula is used: 

4567879(!,(cid:1),#)=3 1&	log (cid:30)(cid:31) (cid:2)!, (cid:1), #(cid:6) 

(9) 

(10) 

eventually, we construct a set meaningfulset(  ) of meaningful concepts. we add each concept c in c that 

its  4567879(cid:2)!, #(cid:6)  value  is  greater  than      to  the  meaningfulset(  )  where  4567879(cid:2)!, #(cid:6)  is  the  maximum  of 

4567879(cid:2)!, (cid:1), #(cid:6) over all paragraphs p, and    is a parameter that determines the level of meaningfulness. the 

summarizer selects the concepts included in the meaningfulset(  ) as classification features. in section 5 and 

6, we evaluate and discuss the optimum value of the parameter   . 

 

fig. 4. the identified meaningful concepts as the features by the helmholtz principle (  =0.5) for the sample document. the semantic 
types have been represented in brackets. the features have been sorted based on the descending order of their meaning measures. 

for example, using this feature selection approach for the sample document, when    is equal to 1, 0.5, 0, 

and    0.5, the number of selected features is 9, 19, 175, and 186 respectively. fig. 4 shows the meaningful 

concepts of the sample document identified by the helmholtz principle with a meaningfulness level of   =0.5. 

as can be seen, there is no obvious relation between the meaning and frequency values. the meaning values 

depend on the appearing pattern of the concepts in the paragraphs and the whole document. 

3.2.5.  fifth approach: extracting correlated features by itemset mining 

in the four previous approaches, features are representative of single concepts. however, some correlations 

may be exist among multiple concepts. it means that some dependent concepts appear together in sentences 

manuscript                                                                  13                                                                    29 march 2017 

 

and point to one of the subtopics of the text. in the fifth feature selection method, we utilize frequent itemset 

mining to extract correlated concepts and use each set of dependent concepts as a classification feature. 

frequent itemset mining is a data mining technique for finding items that appear together in a dataset [13]. 

it can be effectively used in our context to discover correlated concepts that frequently appear in the input text. 

for the fifth method, we utilize a well-known itemset mining algorithm, namely the apriori [50]. although 

the apriori algorithm is usually used for association rule mining, we make use of its ability to extract frequent 

itemsets. this algorithm works with datasets structured in a transactional format. in a transactional dataset, 

there  are  several  transactions  each  one  contains  some  items.  we  can  consider  the  input  document  as  a 

transactional dataset such that the apriori algorithm deals with each sentence and its contained concepts as a 

transaction  and  its  items.  therefore,  we  perform  itemset  mining  to  discover  frequent  itemsets  containing 

concepts that frequently appear in the input text. 

each itemset has a property, named itemset support, calculated by dividing the number of sentences that 

contain the itemset by the total number of sentences in the document. an itemset is said to be frequent if its 

support value is greater than or equal to a given minimum support threshold   . a k-itemset is an itemset which 

contains k items. if all the subsets of a k-itemset are frequent, the itemset is said to be a frequent k-itemset. 

given an input document, its extracted concepts, and a minimum support threshold   , we perform the apriori 

algorithm to discover correlated concepts. each set of k correlated concepts discovered as a frequent k-itemset 

demonstrates a subtopic of the document. in section 5 and 6, we evaluate and discuss the optimum value of 

the parameter   . 

fig. 5. the frequent itemsets extracted as the classification features from the sample document (minimum support threshold   =0.07). 

 

manuscript                                                                  14                                                                    29 march 2017 

 

in the fifth feature selection strategy, after mapping the input document to the umls concepts, we remove 

generic concepts (similar to the second approach) and apply the apriori algorithm. the summarizer uses the 

extracted frequent itemsets as classification features. for instance, when the minimum support threshold    is 

equal to 0.03, 0.07, and 0.1, the number of extracted frequent itemsets is 132, 32, and 15 respectively. fig. 5 

shows  the  extracted  frequent  itemsets  extracted  from  the  sample  document.  in  this  example,  the  value  of 

minimum support threshold    is 0.07. the apriori algorithm returns a total number of 32 frequent itemsets. 

as can be seen, seven itemsets contain more than one concept. these itemsets convey the correlations that 

exist among some dependent concepts. 

3.3.preparing sentences for classification 

after concept extraction and feature selection, the summarizer must represent the sentences of the input 

document in an appropriate format to be prepared for the classification step. it considers all the selected features 

as boolean such that for a given sentence, it sets the value of a feature to true if the corresponding concept 

appears in the sentence, otherwise it sets the value to false. some features contain more than one concept when 

the summarizer uses the fifth feature selection method. for such features, the summarizer sets the value to 

true if all the corresponding concepts appear in the sentence. 

after assigning feature values, the summarizer discards the sentences whose all feature values are false. 

we consider these sentences as unimportant and do not use them in the subsequent steps. for example, let the 

summarizer uses the third feature selection strategy and the threshold (cid:22)(cid:28) defined in eq. 7 to filter the features 

for the sample document. fig. 3 shows the selected features. the sample document consists of 85 sentences, 

of which 16 sentences do not contain any important concepts. the summarizer considers these 16 sentences as 

unimportant and discardeds them for the subsequent operations. therefore, 69 sentences remain for preparation 

and classification. note that we use this example hereafter to simplify the explanation of the remaining steps. 

in  case  of  using  the  other  feature  selection  approaches,  the  summarizer  performs  the  preparation  and 

classification stages likewise. 

in this step, the summarizer creates a vector of features for each remaining sentence. it assigns a number to 

each vector according to the corresponding sentence number. for example, there are 69 vectors for the sample 

document,  each  one  has  eight  features.  each  feature  corresponds  to  an  important  concept  identified  in  the 

previous step. the summarizer assigns the values of features in the ith vector according to the presence and 

absence of important concepts in the ith sentence. for each vector, there is a target class or a target variable, 

named summary, which is initially unknown. in the classification step, our bayesian summarizer determines 

the value of the target class as yes or no for all the vectors. 

as an example, in the sample document, the 46th sentence is    therefore, just as for nrxn1 deletions, it is 

apparent that these large cnvs confer risk of a range of neurodevelopmental phenotypes, including autism, 

mental retardation, and schizophrenia   . in this sentence, four important concepts can be seen, identified by 

manuscript                                                                  15                                                                    29 march 2017 

 

the threshold (cid:22)(cid:28) and represented in fig. 3, including autistic disorder, schizophrenia, deletion mutation and 

nrxn1 gene. hence, in the 46th vector, the values of these four features are true, and the values of the other 

features are false. fig. 6 shows the 46th vector in the sample document. the summarizer assigns the feature 

values for all the vectors just in the same way as the above example. 

after this step, we have a collection of vectors and their feature values. their class variables are unknown, 

and the summarizer must classify them as summary sentences (yes) or non-summary sentences (no). every 

document has its particular set of concepts and the features of each text are different from others. 

fig. 6. the 46th sentence-vector, corresponding to the 46th sentence in the sample document. the summary class variable is initially 
unknown. 

 

3.4.sentence classification 

the  compression  rate  is  a  parameter  in  summarization  systems  which  determines  the  percentage  of  the 

input  text  that  must  be  extracted  as  the  final  summary.  the  summarizer  does  not  initially  know  about  the 

(cid:1)((cid:3) =(cid:3)(cid:15))  in  eq.  3,  the  prior  id203  of  class  variable  values.  however,  it  knows  what  percentage  of 
sentences  that  must  be  selected  as  summary,  and  it  can  estimates  the  (cid:1)(<=&&6>?=@5a)  and 
(cid:1)(<=&&6>?=(cid:30)b). for instance, the total number of sentences in the sample document is 85. suppose the 

compression  rate  is  0.3.  it  means  that  30%  of  the  text  (about  26  sentences)  must  be  selected  for  the  final 

summary. in the preparation step, the summarizer discarded the sentences that did not include any important 

concepts, and 69 vectors remained for the classification step. the summarizer does not know which of these 

69 vectors has yes value for the summary class variable, but it knows 26 vectors must have the yes value. 

hence, for  this  example,  the (cid:1)(<=&&6>?=@5a)  is equal to  26  /  69 =  0.377  and (cid:1)(<=&&6>?=(cid:30)b)  is 
equal to 43 / 69 = 0.623. generally, the (cid:1)(<=&&6>?=@5a) is equal to the number of sentences that must be 

selected for the final summary (according to the compression rate) divided by the total number of remaining 

sentences for the classification step. 

the  bayesian  summarizer  follows  an  assumption  about  the (cid:1)(c(cid:18)|(cid:3)=(cid:3)(cid:15)(cid:6)  in  eq.  3  that  simplifies  the 

estimation  of  the  likelihood  probabilities.  the  summarizer  assumes  that  the  summary  can  convey  an 

manuscript                                                                  16                                                                    29 march 2017 

 

informative content if it follows the distribution of important concepts within the input document. with regard 

to this assumption, the summarizer can estimate the likelihood probabilities, i.e. the id203 of observing 

an important concept given class yes or no. for example, the concept schizophrenia appears in 30 sentences 

within the sample document, and its distribution within all the vectors is equal to 30 / 69 = 0.435. therefore, 

the  id203  of  observing  concept  schizophrenia  given  class  yes,  i.e.  the  (cid:1)(<!   8ebf   5>786=
g>=5|<=&&6>?=@5a),  would  be  equal  to  0.435.  likewise,  the  id203  of  not  observing  concept 
schizophrenia  given  class  yes,  i.e. (cid:1)(<!   8ebf   5>786=(cid:31)6ha5|<=&&6>?=@5a),  would  be  equal  to  1      

0.435 = 0.565. likewise, the summarizer estimates the likelihood probabilities of observing and not observing 

a concept given class value no. 

in  this  step,  the  summarizer  can estimate  the  posterior  id203  of  class  values  given  a  vector.  if  the 

summarizer selects the value that maximizes the posterior id203 of summary variable given ith vector 

(similar to eq. 3) the number of sentences classified as yes may be less than the number of sentences that must 

be selected for the summary. this comes true, because in a vector the number of features having the true value 

is often less than the number of features having the false value. therefore, the summarizer should decide about 

the summary class values in a different way compared with eq. 3. the summarizer estimates the posterior 

id203 of class values for each vector and assesses the strength of evidence for class value yes using the 

por measure. 

we incorporate two coefficients into the estimation of posterior probabilities to discriminate between the 

presence and absence of more important and less important concepts.  in our context, the presence of important 

concepts (the true value for the features) and their prior probabilities are contributing factors in the selection 

a  sentence  for  the  summary.  on  the  other  hand,  when  the  suumarizer  uses  the  bayes  rule,  it  does  not 

discriminate between the presence and the absence of more important (high-frequent) and less relevant (low-

frequent) concepts. in this case, the summarizer decides based on the highest probable values of features. as 

we show in section 5.1.2, for the majority of documents, even the high-frequent concepts appear in less than 

50% of the sentences of a document. this shows that for the majority of documents, the most probable value 

for all features is false. therefore, for estimating the posterior probabilities of the class value yes and no, the 

summarizer  should  take  into  account  this  issue.  we  address  this  problem  employing  two  coefficients  in 

estimation of the posterior id203 of class values. the coefficients increase and decrease the impact of 

important concepts on the posterior id203 of class values based on the frequency of concepts and whether 

they occur in a sentence or not. we evaluate and discuss the impact of using these coefficients on the accuracy 

of the bayesian summarizer in section 5 and 6. 

3.4.1.  the id203 of inclusion in the summary 

to estimate the posterior probabilities, firstly, the summarizer estimates the posterior id203 of class 

value yes given ith vector by rewriting eq. 3, as follows: 

manuscript                                                                  17                                                                    29 march 2017 

 

(cid:1)(<=&&6>? =@5a|i(cid:18)(cid:6)=(cid:1)(cid:2)<=&&6>?=@5a(cid:6)(cid:17)(cid:1)(cid:2)j(cid:18)(cid:21)|<=&&6>?=@5a(cid:6)(cid:2)(cid:3)(cid:23)(cid:6)

where i(cid:18) is the ith vector, (cid:1)(cid:2)<=&&6>?=@5a|i(cid:18)(cid:6) is the posterior id203 of classifying the ith vector as 
yes given i(cid:18), j(cid:18)(cid:21) is the kth feature in the ith vector, and (cid:1)(cid:2)j(cid:18)(cid:21)|<=&&6>?=@5a(cid:6) is the likelihood id203, 
i.e. the id203 of observing  j(cid:18)(cid:21)=g>=5 or j(cid:18)(cid:21)=(cid:31)6ha5 given class variable <=&&6>?=@5a. the value 
of the coefficient (cid:3)(cid:23) depends on whether the j(cid:18)(cid:21) is true or false and is specified as follows: 

 

(11) 

(cid:21)

(cid:3)(cid:23)=kj>5l(cid:18)(cid:21)            8j j(cid:18)(cid:21) =g>=5
1j>5l(cid:18)(cid:21)            8j j(cid:18)(cid:21) =(cid:31)6ha5 

(12) 

where j>5l(cid:18)(cid:21) is the frequency of the concept corresponding to the j(cid:18)(cid:21). depending on whether the value of j(cid:18)(cid:21) 
is true or false, the coefficient (cid:3)(cid:23) affects the (cid:1)(j(cid:18)(cid:21)|<=&&6>?=@5a(cid:6) in two ways: 
1.  when  the j(cid:18)(cid:21)  is  true,  the  frequency  of  corresponding  concept  is  multiplied  by  the (cid:1)(cid:2)j(cid:18)(cid:21)|<=&&6>?=
@5a(cid:6).  thus,  the  values  of  the  (cid:1)(cid:2)j(cid:18)(cid:21)|<=&&6>?=@5a(cid:6)  and  (cid:1)(cid:2)<=&&6>?=@5a|<i(cid:18)(cid:6)  increase. 
2.  when  the j(cid:18)(cid:21)  is  false,  the  inverted  frequency  of  corresponding  concept  is  multiplied  by  the  
the  (cid:1)(cid:2)j(cid:18)(cid:21)|<=&&6>?=@5a(cid:6)  and 
(cid:1)(cid:2)j(cid:18)(cid:21)|<=&&6>?=@5a(cid:6),  and  as  a 
(cid:1)(cid:2)<=&&6>?=@5a|<i(cid:18)(cid:6) decrease. in this case, a higher frequency decreases the (cid:1)(cid:2)j(cid:18)(cid:21)|<=&&6>?=@5a(cid:6) 

consequently, the presence of more frequent concepts increases the chance of selecting a sentence for the 

with  a  higher  rate.  hence,  the  absence  of  more  frequent  concepts  decreases  the  chance  of  selecting  a 

the  values  of 

summary. 

result, 

sentence for the summary. 

 
3.4.2.  the id203 of exclusion from the summary 

after  estimating  the  id203  of  inclusion  in  the  summary,  the  summarizer  estimates  the  posterior 

id203 of class value no given ith vector as follows: 

(cid:1)(cid:2)<=&&6>?=(cid:30)b|i(cid:18)(cid:6)=(cid:1)(cid:2)<=&&6>?=(cid:30)b(cid:6)(cid:17)(cid:1)(cid:2)j(cid:18)(cid:21)|<=&&6>?=(cid:30)b(cid:6)(cid:2)(cid:3)(cid:25)(cid:6)

where i(cid:18) is the ith vector, (cid:1)(cid:2)<=&&6>?=(cid:30)b|i(cid:18)(cid:6) is the posterior id203 of classifying the ith vector as no 
given i(cid:18), j(cid:18)(cid:21) is the kth feature in the ith vector, and (cid:1)(cid:2)j(cid:18)(cid:21)|<=&&6>?=(cid:30)b(cid:6) is the likelihood id203, i.e. 
the id203 of observing j(cid:18)(cid:21)=g>=5 or j(cid:18)(cid:21)=(cid:31)6ha5, given class variable <=&&6>?=(cid:30)b. the value of 
the coefficient (cid:3)(cid:25) depends on whether the j(cid:18)(cid:21) is true or false and is specified as follows: 

 

(13) 

(cid:21)

manuscript                                                                  18                                                                    29 march 2017 

 

(cid:3)(cid:25) =

o 1j>5l(cid:18)(cid:21)            8j j(cid:18)(cid:21) =g>=5
mn
j>5l(cid:18)(cid:21)            8j j(cid:18)(cid:21) =(cid:31)6ha5 

(14) 

(15) 

where j>5l(cid:18)(cid:21) is the frequency of the concept corresponding to the j(cid:18)(cid:21). similar to the (cid:3)(cid:23), depending on whether 
the value of j(cid:18)(cid:21) is true or false, the (cid:3)(cid:25) affects the (cid:1)(j(cid:18)(cid:21)|<=&&6>?=(cid:30)b) in two ways: 
1.  when  the  j(cid:18)(cid:21)  is  true,  the  inverted  frequency  of  corresponding  concept  is  multiplied  by  the 
the  (cid:1)(j(cid:18)(cid:21)|<=&&6>?=(cid:30)b)  and 
(cid:1)(j(cid:18)(cid:21)|<=&&6>?=(cid:30)b),  and  as  a 
(cid:1)(<=&&6>?=(cid:30)b|<i(cid:18)) decrease. in this case, a higher frequency decreases the (cid:1)(j(cid:18)(cid:21)|<=&&6>?=(cid:30)b) 

the  values  of 

result, 

with a higher rate. hence, the presence of more frequent concepts decreases the id203 of not selecting 

a sentence for the summary. 

2.  when  the j(cid:18)(cid:21)  is  false, the  frequency  of  corresponding  concept is  multiplied by  the (cid:1)(j(cid:18)(cid:21)|<=&&6>?=
(cid:30)b).  thus, 

the  (cid:1)(j(cid:18)(cid:21)|<=&&6>?=(cid:30)b)  and  (cid:1)(<=&&6>?=(cid:30)b|<i(cid:18)) 

the  values  of 

increase. 

consequently, the absence of more frequent concepts increases the id203 of not selecting a sentence 

for the summary. 

after estimating the id203 of classifying each vector as yes and no, the summarizer needs to decide 

which sentences should be selected for inclusion in the final summary. as mentioned earlier, if the classifier 

selects for each vector the class value which maximizes the posterior id203 of summary class variable, 

the number of sentences classified as yes may be less than the number of sentences required for the summary. 

therefore,  we  employ  the  por  measure,  early  explained  in  section  2.2, to  classify  the  vectors.  for  the  ith 

vector, the summarizer computes the value of the por by rewriting eq. 4, as follows: 

(cid:1)(cid:19)(cid:20)(cid:18) =(cid:1)(<=&&6>? =@5a|<i(cid:18))
(cid:1)(<=&&6>? =(cid:30)b|<i(cid:18))  

where (cid:1)(cid:19)(cid:20)(cid:18)  is  the  posterior  odds  ratio  of  the  ith  vector.  the  values  of  the (cid:1)(<=&&6>?=@5a|<i(cid:18))  and 
(cid:1)(<=&&6>?=(cid:30)b|<i(cid:18)) are the posterior probabilities of classifying the ith vector as yes and no given <i(cid:18), 

which the summarizer estimated earlier. 

the por demonstrates a measure of the strength of evidence for a particular class value. therefore, the 

greater pori for a vector, the higher strength of evidence in favor of classifying the vector as yes. 

after calculating the por value for all the vectors, the summarizer can decide which sentences should be 

selected for the summary. it sorts the vectors in descending order of their por values and assigns the top-

ranked n vectors to the class yes where n is the number of sentences which must be selected to make the 

summary specified by the compression rate. the summarizer assigns the remaining vectors to the class no. 

manuscript                                                                  19                                                                    29 march 2017 

 

in  the  following,  we  explain  a  redundancy  reduction  method  that  the  summarizer  can  use  to  decrease 

redundant information in the summary. 

3.4.3.  the redundancy reduction method 

the problem of redundancy in text summarization concerns the same repeated information conveyed by 

multiple  sentences  in  a  summary.  compared  to  multi-document  summarization,  it  is  less  probable  to  find 

redundant  information  in  a  summary  produced  for  a  single  document  [51].  however,  redundancy  removal 

approaches can also be useful in single-document summarization [10]. maximal marginal relevance (mmr) 

[52]  is  a  well-known  method  for  removing  redundancy,  especially  in  query-focused  summarization.  it 

computes  cosine  similarities  between  sentences  and  a  query,  also  between  sentences  and  already  selected 

sentences. then, it assigns a marginal relevance to each sentence and adds the sentence with the maximum 

marginal relevance to the summary. the mmr computes a linear combination of two functions, i.e. relevance 

and novelty. the relevance function needs a query to assess the relatedness of sentences. since the bayesian 

summarizer does not use any query for summarization, the mmr approach is not applicable to our method. 

hence,  we  propose  a  redundancy  reduction  method  based  on  our  context  by  gradually  updating  the 

probabilities  to  decrease  the  chance  of  high-probable  concepts  and  increase  the  chance  of  less-probable 

concepts for inclusion in the summary.  

we employ an iterative method aimed at reducing the redundancy that can emerge in the summary due to 

the large prior id203 of high-frequency concepts. when the possible redundancy does not matter to the 

summarizer, as explained earlier, the summarizer estimates the prior, likelihood and posterior probabilities. 

then, it computes the por values for all the sentences and ranks them based on their por values. finally, it 

assigns  the  top  n  sentences  to  the  class  value  yes.  on  the  other  hand,  when  the  summarizer  employs  the 

redundancy  reduction  method,  it  performs  the  sentence  selection  process  differently.  in  this  case,  when  it 

computes the por values, it assigns only the sentence having the highest por value to the class value yes. 

next, it estimates the prior, likelihood, and posterior probabilities again, but it does not consider the previously 

selected sentences and their concepts in the subsequent estimations. accordingly, it reduces the id203 of 

observing the high-frequency concepts included in the sentences already selected. moreover, the summarizer 

increases the chance of observing the low-frequency concepts in the summary. in the subsequent iterations, it 

computes  the  por  values  based  on  new  probabilities.  it  repeatedly  estimates  the  probabilities  without 

considering the sentences already selected for the summary and selects the sentence with the maximum por 

value until the number of summary sentences (assigned to the class value yes) reaches n. finally, it assigns 

the remaining sentences to the class value no. 

in section 5 and 6, we evaluate and discuss the efficiency of the redundancy reduction method. 

 

manuscript                                                                  20                                                                    29 march 2017 

 

3.5.summary generation 

in the previous step, the summarizer assigned the sentences to the class values yes and no. in the summary 

generation step, it adds the sentences of the class value yes to the summary. it arranges the summary sentences 

in the same order as they appear in the primary document. finally, it adds the figures and tables in the main 

document referred to in the summary. fig. 7 shows the summary of the sample document produced by the 

bayesian summarizer. in this example, for brevity reasons, the compression rate is 10%. it means that the size 

of the summary must be 10% of the input document. 

fig. 7. the summary of the sample document generated by the bayesian summarizer (compression rate=10%). 

 

4.  evaluation method 

 

4.1.evaluation corpus 

the most common method of evaluating summaries generated by an automatic summarizer is to compare 

them  against  manually  generated  summaries,  known  as  model  or  reference  summaries.  in  such  evaluation 

method, we measure the similarity between the content of system and model summaries. the more content 

shared  between  system  and  model  summaries,  the  better  the  performance  of  the  summarization  system. 

obtaining manually generated summaries is a challenging and time-consuming task, because they have to be 

provided  by  human  experts.  moreover,  human-generated  model  summaries  are  highly  subjective.  to  the 

authors     knowledge,  there  is  no  corpus  of  model  summaries  for  single-document  biomedical  text 

manuscript                                                                  21                                                                    29 march 2017 

 

summarization.  however,  most  scientific  papers  have  an  abstract  which  can  be  considered  as  the  model 

summary for evaluation [5]. 

to compare our bayesian biomedical summarization method against other summarizers, we use a collection 

of  400  biomedical  scientific  papers  randomly  selected  from  the  biomed  central   s  corpus  for  text  mining 

research4. the size of evaluation corpus is large enough to allow the results of the assessment to be significant 

[53]. we use the abstracts of the papers as the model summaries to evaluate the performance of the system-

generated  summaries.  we  perform  our  preliminary  experiments  using  a  separate  development  corpus 

containing 100 papers randomly selected from the biomed central   s corpus. 

4.2.id74 

a common feature which we assess in the performance evaluation of text summarization systems is the 

informativeness. it is a feature for representing how much information from the original text is provided by 

the summary [54]. in this paper, we use the id8 package [15] to evaluate the performance of the bayesian 

summarizer  in  terms  of  the  informative  content  of  summaries.  the  id8  package  compares  a  system-

generated  summary  with  one  or  more  model  summaries,  estimates  the  shared  content  between  them  by 

calculating the proportion of shared id165s, and produces different scores in terms of different metrics. the 

id8 metrics produce a score between 0 and 1. the higher scores for a system summary, the greater content 

overlap between the system and model summaries. in our evaluations, we use the following id8 metrics: 

     id8-1 (r-1). it computes the number of shared unigrams (1-grams) between the system and model 

summaries. 

     id8-2 (r-2). it computes the number of shared bigrams (2-grams) between the system and model 

summaries. 

     id8-w-1.2 (r-w-1.2).  it computes the union of the longest common subsequences between the 

system and model summaries. it takes into account the presence of consecutive matches. 

     id8-su4  (r-su4).  it  computes  the  overlap  of  skip-bigrams  (pairs  of  words  having  intervening 

word gaps) between the system and model summaries. it allows a skip distance of four between bigrams. 

in spite of their simplicity, the id8 metrics have shown a high degree of correlation with human judges 

[15]. 

4.3.preliminary experiments and parameterization 

we introduce five feature selection approaches in section 3.2. the first approach selects the classification 

features by simply considering all the extracted concepts. the second approach tries to reduce the number of 

features by filtering out the generic features that seem to be potentially redundant. in the third approach, we 

                                                            
4 http://old.biomedcentral.com/about/datamining 

manuscript                                                                  22                                                                    29 march 2017 

 

rank the features based on the frequency of corresponding concepts and use a threshold as a filtering criterion. 

we introduce three possible threshold values in section 3.2.3 based on the average and the standard deviation 

of the frequency of concepts. in feature selection experiments, we evaluate these three values to specify the 

optimum  threshold  for  this  type  of  filtering.  in  the  fourth  feature  selection  approach,  we  measure  a 

meaningfulness value for each concept using the helmholtz principle. we use a parameter    that determines 

the level of meaningfulness for the concepts selected as classification features. we perform a set of experiments 

to tune the parameter   . in the fifth approach, we utilize an itemset mining method to extract correlated concepts 

in the form of frequent itemsets and use them as classification features. the itemset mining extracts frequent 

itemsets according to a minimum support threshold   . we tune the parameter    performing a set of preliminary 

experiments. 

in the other set of preliminary experiments, we assess the impact of the coefficients (cid:3)(cid:23) and (cid:3)(cid:25), introduced 

in section 3.4, on the performance of our summarization method. we evaluate the quality of the produced 

summaries in two situations, the presence and the absence of the coefficients. 

in section 3.4.3, we introduce a redundancy reduction method to decrease the potential redundancy in the 

summary. we conduct another set of experiments to investigate the impact of the redundancy reduction method 

on the performance of the bayesian summarizer. it seems that the redundancy reduction strategy may achieve 

more percent of improvement under smaller compression rates. we also assess the percentage of improvement 

under smaller and larger compression rates than 30%. 

4.4.comparison with other summarization methods 

we compare the bayesian summarizer with six summarization methods and two baselines. three methods 

of comparison systems are biomedical summarizers, i.e. freqdist [10], biochain [9], and chainfreq [4]. we 

implement these three summarizers as explained in their original papers. two comparison methods are domain-

independent  and  term-based,  i.e.  summa  [55]  and  swesum  [56].  one  of  the  methods,  microsoft 

autosummarize,  is  a  commercial  application.  the  two  baseline  methods  are  lead  baseline  and  random 

baseline. the size of the summaries generated by all the summarizers is 30% of the original documents. the 

choice of 30% as the compression rate is based on a well-accepted de facto standard that says the size of a 

summary should be between 15% and 35% of the original text [57]. in the following, we give a brief description 

of the competitor methods. 

freqdist [10] is a biomedical summarization method which uses concept frequency distribution to identify 

important sentences. it initially maps the input text to the umls concepts. then, it creates an empty summary 

frequency distribution and a source text frequency distribution model counting the concepts. afterwards, using 

an  iterative  sentence  selection  process,  freqdist  creates  a  candidate  summary  and  compares  the frequency 

distribution  of  the  candidate  summary  with  the  distribution  of  the  source  text.  the  method  evaluates  the 

sentences  based  on  how  much  they  align  the  frequency  distribution  of  the  summary  to  the  original  text.  it 

manuscript                                                                  23                                                                    29 march 2017 

 

selects a sentence in each iteration and adds it to the summary such that the two frequency distributions to be 

aligned as closely as possible. the original study has compared five similarity functions to find the best one 

for evaluating the similarity of frequency distributions. we implement and compare freqdist method with the 

dice   s coefficient as it has reported the highest id8 scores in the original study. 

biochain [9] is a biomedical summarizer based on the lexical chaining idea. it extracts the umls concepts 

from the input document, considers the semantic types as the head of chains, and puts the concepts of the same 

semntic  type  in  the  same  chain.  biochain  selects  the  strong  chains  based  on  the  core  concepts  and  their 

frequency,  identifies  the  strong  concepts  of  each  strong  chain,  and  uses  the  strong  concepts  to  score  the 

sentences. finally, it extracts the high-scoring sentences and generates the final summary. 

chainfreq [4] is a hybrid summarizer which makes use of both freqdist and biochain methods. it uses 

biochain to identify important sentences containing strong concepts. then, it sends the candidate sentences to 

freqdist to reduce the redundancy and to select the subset of sentences which aligns the summary frequency 

distribution  to  the  source  text.  in  the  original  study,  two  variants  of  biochain  have  been  evaluated  for 

chainfreq. the first variant uses all the concepts of strong chains to score the sentences, while the second one 

uses the most frequent concept of each strong chain. in the evaluations, the first method has obtained higher 

id8 scores. accordingly, we implement the first method as a part of chainfreq for our evaluations. we 

also implement freqdist using the dice   s coefficient. 

in addition to the above biomedical summarizers, we use three domain-independent comparison methods 

in the evaluations to assess the performance of our method against traditional term-based approaches. we give 

a description of these methods in the following. 

summa [55] is a summarizer which uses generic and statistical features to score the sentences of an input 

document. the features that we use for the evaluations include the frequency of terms within the sentence, the 

position of the sentence within the document, the similarity between the sentence and the first sentence of the 

document, and the similarity between the sentence and the title. 

swesum [56] is an online and multi-lingual summarizer based on generic features. for the evaluations, we 

use the following set of features: presence of the sentence in the first line of the text, presence of numerical 

values  in  the  sentence,  and  presence  of  keywords  in  the  sentence.  we  also  set  the  type  of  text  feature  to 

   academic   . 

microsoft autosummarize is a feature of the microsoft word software5. this summarizer performs based 

on a word frequency algorithm. it assigns a score to each sentence of a document according to the frequency 

of words contained in the sentence. 

                                                            
5 microsoft word 2007, microsoft corporation 

manuscript                                                                  24                                                                    29 march 2017 

 

our  two  baselines  for  the  evaluations  are  lead  baseline  that  returns  the  first  n  sentences  of  the  input 

document as the summary, and random baseline that randomly selects n sentences from the document and 

generates a summary. 

 

5.  results 

 

5.1.preliminary experiments 

in this subsection, we first present the results of parameterization and the preliminary experiments which 

specifies the best settings for the feature selection approaches. then, we present the results of experiments 

conducted to assess the impact of the coefficients and the redundancy reduction method on the performance 

of the bayesian summarizer. for brevity reasons, we only report the r-2 and r-su4 scores for the preliminary 

experiment results. 

5.1.1.  feature selection 

as explained in section 4.3, we conduct a set of preliminary experiments to tune the parameters and find 

the best settings of the feature selection methods. we introduce three possible threshold values for the third 

method which uses a ranking and filtering strategy. table 1 shows the id8 scores obtained by the bayesian 

summarizer using the three thresholds. the summarizer obtains the highest scores when it uses the threshold 

(cid:22)(cid:28). we use this threshold as the optimal value in the subsequent experiments. 

table 1. id8 scores obtained by the bayesian summarizer using the third feature selection approach and three threshold values. 
the best result for each id8 score is shown in bold type. 

 

(cid:22)(cid:23) = p9(j>5l) 
(cid:22)(cid:25) = p9(j>5l)+<qr_r5p(j>5l) 
(cid:22)(cid:28) = p9(j>5l)+(2  <qr_r5p(j>5l)) 

 

id8-2 

id8-su4 

0.3188 

0.3242 

0.3346 

0.3769 

0.3828 

0.3932 

in the fourth feature selection approach, we use the helmholtz principle to identify meaningful concepts. 

in this method, there is a parameter    which specifies the meaningfulness threshold. fig. 8 shows the id8 

scores obtained by the bayesian summarizer using the fourth feature selection method and different values of 

the theshold    between [   1.5, 0.6]. the threshold values of    1.3 and    1.2 report the best scores (r-2: 0.3411 

and r-su4: 0.3980). we set the optimal value of this parameter to    1.2 in the subsequent experiments.  fig. 9 

shows the average number of meaningful concepts selected as classification features for the different values 

of the threshold    in the given range. 

manuscript                                                                  25                                                                    29 march 2017 

 

id8-2

id8-su4

e
r
o
c
s
 

e
g
u
o
r

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

-1.5

-1.4

-1.3

-1.2

-1.1

-1

-0.9

-0.8

-0.7

-0.6

-0.5

-0.4

-0.3

-0.2

-0.1

0

0.1

0.2

0.3

0.4

0.5

0.6

meaningfulness level   

 

fig.  8.  id8  scores  for  the  bayesian  summarizer  using  the  fourth  feature  selection  method  and  the  different  values  of  the 
meaningfulness threshold. 

 

s
e
r
u
t
a
e
f
 
f
o
 
r
e
b
m
u
n
e
g
a
r
e
v
a
e
h
t

 

 

400

350

300

250

200

150

100

50

0

-1.5

-1.4

-1.3

-1.2

-1.1

-1

-0.9

-0.8

-0.7

-0.6

-0.5

-0.4

-0.3

-0.2

-0.1

0

0.1

0.2

0.3

0.4

0.5

0.6

meaningfulness level   

 

fig. 9. the average number of features for the different values of the meaningfulness level in the fourth feature selection method. 

manuscript                                                                  26                                                                    29 march 2017 

 

in the fifth feature selection approach, we use frequent itemset mining to extract correlated concepts, i.e. 

frequent itemsets, and to select them as classification features. the itemset mining method employs a parameter 

   as the minimum support threshold to discover frequent itemsets. fig. 10 shows the id8 scores obtained 

by the bayesian summarizer using the fifth feature selection method and different values of the threshold    

between [0.02, 0.23]. the threshold value of 0.09 reports the best scores (r-2: 0.3543 and r-su4: 0.4094). 

we choose this value as the optimal parameter for the subsequent experiments. table 2 presents the average 

number of frequent itemsets selected as features for different values of the threshold    in the given range. 

id8-2

id8-su4

e
r
o
c
s
 

e
g
u
o
r

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09

0.1

0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19

0.2

0.21 0.22 0.23

minimum support threshold   

 

fig. 10. id8 scores for the bayesian summarizer using the fifth feature selection method and the different values of the minimum 
support threshold.  

5.1.2.  the impact of the coefficients 

we mention in section 3.4 that for the majority of documents, even the high-frequent concepts appear in 

less than 50% of the sentences of a document. we examine the most frequent concept of every document in 

the development corpus. the results show that only in nine documents there are concepts that appear in more 

than 50% of the sentences of the document. this means there are only nine documents containing at least one 

feature with a most probable value of true. for the other 91 documents, the most probable value of all the 

features is false. in average, the most frequent concept of a document in the development corpus appears in 

36% of sentences in the corresponding document. we also investigate these statistics for the evaluation corpus. 

the results show that only in 33 documents there are concepts that appear in more than 50% of the sentences 

of the document. for the other 367 documents, the most frequent concept of each document appears in less 

manuscript                                                                  27                                                                    29 march 2017 

 

than  50%  of  the  sentences.  in  average,  the  most  frequent  concept  of  a  document  in  the  evaluation  corpus 

appears in 35% of sentences in the corresponding document. regarding these observations, as noted in section 

3.4, the coefficients help the summarizer to discriminate between the true and false values of features, leading 

to a more accurate sentence classification. 

table 2. the average number of features for different values of the minimum support threshold in the fifth feature selection method 
(extraction of correlated features by itemset mining). 

   

0.02 

0.03 

0.04 

0.05 

0.06 

0.07 

0.08 

0.09 

0.1 

0.11 

0.12 

the average number of features 

2549 

975 

557 

322 

166 

133 

74 

60 

52 

36 

24 

   

0.13 

0.14 

0.15 

0.16 

0.17 

0.18 

0.19 

0.2 

0.21 

0.22 

0.23 

the average number of features 

19 

17 

15 

13 

12 

10 

7 

6 

5 

3 

3 

 

we perform a set of experiments to assess the impact of the coefficients (cid:3)(cid:23) and (cid:3)(cid:25) on the quality of the 

produced  summaries.  we  evaluate  the  bayesian  summarizer  using  the  best  settings  of  all  the  five  feature 

selection  methods,  with  and  without  using  the  coefficients.  table  3  shows  the  id8  scores  for  these 

experiments. as can be seen, the summarizer reports higher scores when it utilizes the coefficients. 

table 3. id8 scores obtained by the bayesian summarizer using the five different feature selection approaches, with and without 
using the coefficients. 

 

first approach 

second approach 

third approach ((cid:22)(cid:28)) 

id8-2 

with the 

coefficients 

without the 
coefficients 

0.3097 

0.2762 

0.3246 

0.2966 

0.3346 

0.3155 

fourth approach (  =   1.2) 

0.3411 

0.2724 

fifth approach (  =0.09) 

0.3543 

0.3391 

 

 

 

 

 

 

 

 

id8-su4 

with the 

coefficients 

without the 
coefficients 

0.3740 

0.3435 

0.3857 

0.3592 

0.3932 

0.3743 

0.3980 

0.3429 

0.4094 

0.3919 

manuscript                                                                  28                                                                    29 march 2017 

 

5.1.3.  the impact of the redundancy reduction method 

performing  another  set  of  preliminary  experiments,  we  assess  the  impact  of  the  redundancy  reduction 

method  on  the  quality  of  the  produced  summaries.  table  4  gives  the  id8  scores  for  the  bayesian 

summarizer  when  the  five  different  feature  selection  approaches  are  used,  with  and  without  using  the 

redundancy reduction method. 

we compare the r-2 scores assigned to the bayesian summarizer in two cases of using and not using the 

redundancy  reduction  method  under  different  compression  rates.  table  5  presents  the  percentage  of 

improvement obtained using the redundancy reduction method for different compression rates. as can be seen, 

the percentage of improvement for the smaller compression rates is higher than for the greater rates. 

table 4. id8 scores obtained by the bayesian summarizer using the five different feature selection approaches, with and without 
using the redundancy reduction method. 

 

first approach 

second approach 

third approach ((cid:22)(cid:28)) 

id8-2 

redundancy 
reduction: yes 

redundancy 
reduction: no 

0.3097 

0.2861 

0.3246 

0.3094 

0.3346 

0.3202 

fourth approach (  =   1.2) 

0.3411 

0.3289 

fifth approach (  =0.09) 

0.3543 

0.3415 

 

 

 

 

 

 

 

 

id8-su4 

redundancy 
reduction: yes 

redundancy 
reduction: no 

0.3740 

0.3589 

0.3857 

0.3691 

0.3932 

0.3770 

0.3980 

0.3857 

0.4094 

0.3903 

table  5.  the  percentage  of  improvement  achieved  by  the  bayesian  summarizer  using  the  redundancy  reduction  method.  the 
percentages have been computed for id8-2 scores using the five different feature selection methods under different compression 
rates of 15%, 20%, 25%, 30%, and 35%. 

 

first approach 

second approach 

third approach ((cid:22)(cid:28)) 

fourth approach (  =   1.2) 

fifth approach (  =0.09) 

 

compression rate 

15% 

20% 

25% 

30% 

35% 

9.1 

5.9 

5.5 

4.8 

4.7 

8.9 

5.6 

5.1 

4.5 

4.5 

8.5 

5.2 

4.9 

4.1 

4.1 

8.2 

4.9 

4.5 

3.7 

3.7 

7.9 

4.7 

4.4 

3.5 

3.4 

 

 

 

manuscript                                                                  29                                                                    29 march 2017 

table 6. the average number of concepts covered in the summaries produced by the bayesian summarizer, using the five different 
feature selection approaches, with and without using the redundancy reduction method. 

 

first approach 

second approach 

third approach ((cid:22)(cid:28)) 

fourth approach (  =   1.2) 

fifth approach (  =0.09) 

 

with redundancy reduction 

without redundancy reduction 

330 

336 

302 

332 

303 

297 

312 

272 

308 

284 

we  also  assess  the  average  number  of  concepts  covered  in  the  produced  summaries  by  the  bayesian 

summarizer, with and without using the redundancy reduction method. table 6 shows these results when the 

summarizer uses the five different feature selection methods. as the results show, the summaries cover more 

concepts in average when the summarizer benefits from the redundancy reduction method.  

5.2.evaluation results 

comparing the bayesian summarizer with the other methods, we evaluate the performance of our method 

for biomedical text summarization. table 7 shows the id8 scores obtained by the bayesian summarizer 

and the comparison methods. we evaluate the bayesian summarizer all the five feature selection approaches. 

in  order  to  test  the  statistical  significance  of  the  results,  we  use  a  wilcoxon  signed-rank  test  with  a  95% 

confidence interval. 

according to the results, when the bayesian summarizer uses the fourth and fifth feature selection methods, 

it significantly outperforms all the comparison methods in terms of all the reported id8 scores (p < 0.05). 

using  the  third  feature  selection  method,  the  bayesian  summarizer  significantly  performs  better  than 

biochain, the domain-independent, and the baseline methods in terms of all the id8 scores (p < 0.05). in 

comparison with chainfreq and freqdist, the results report a significant improvement for all the scores except 

for the r-w-1.2 (p > 0.05). when we run the bayesian summarizer with the second feature selection method, 

it significantly performs better than the domain-independent and baseline summarizers (p < 0.05). among the 

biomedical  summarization  methods,  its  improvement  is  significant  for all  the  id8  scores  compared to 

biochain, and its improvement is significant only for r-2 score with respect to freqdist (p < 0.05). although 

it  obtains  better  scores  than  chainfreq,  its  improvement  is  not  significant  for  all  the  scores  (p  >  0.05). 

eventually, the bayesian summarizer significantly performs better than the domain independent competitors 

and baselines in terms of all the scores when it uses the first feature selection method (p < 0.05). compared to 

biochain, it improves all the scores, but the improvement is only significant for the r-1. 

manuscript                                                                  30                                                                    29 march 2017 

 

table 8 gives the average, minimum, and maximum number of features selected by the five feature selection 

methods for documents in the evaluation corpus.  

table 7. id8 scores obtained by the bayesian summarizer and the comparison methods. the best result for each id8 score is 
shown in bold type. the summarizers are sorted based on the decreasing order of their id8-2 scores. 

 

id8-1 

id8-2 

id8-w-1.2 

id8-su4 

bayesian summarizer- fifth approach (  =0.09) 

0.7886 

0.3529 

0.1113 

0.4104 

bayesian summarizer- fourth approach (  =   1.2) 

bayesian summarizer- third approach (uv) 

0.7760 

0.3442 

0.0975 

0.4019 

0.7634 

0.3351 

0.0862 

0.3961 

bayesian summarizer- second approach 

0.7549 

0.3263 

0.0808 

0.3872 

chainfreq 

freqdist 

0.7507 

0.3182 

0.0798 

0.3791 

0.7498 

0.3121 

0.0789 

0.3782 

bayesian summarizer- first approach 

0.7496 

0.3114 

0.0785 

0.3759 

biochain 

summa 

swesum 

lead baseline 

autosummarize 

random baseline 

 

0.7378 

0.3080 

0.0746 

0.3691 

0.7006 

0.2865 

0.0714 

0.3418 

0.6997 

0.2817 

0.0703 

0.3386 

0.6351 

0.2459 

0.0681 

0.3035 

0.6158 

0.2407 

0.0656 

0.2948 

0.5602 

0.2243 

0.0615 

0.2711 

table 8. the average, minimum, and maximum number of features for the documents of the evaluation corpus using the five different 
feature selection approaches. 

 

average 

minimum 

maximum 

bayesian summarizer- first approach 

bayesian summarizer- second approach 

bayesian summarizer- third approach (uv) 

bayesian summarizer- fourth approach (  =   1.2) 

bayesian summarizer- fifth approach (  =0.09) 

980 

519 

22 

344 

61 

399 

175 

6 

117 

7 

1625 

925 

35 

620 

399 

 

 

 

manuscript                                                                  31                                                                    29 march 2017 

 

6.  discussion 

 

6.1.feature selection and parameterization 

as reported in table 1, when the summarizer uses the third feature selection method with the threshold (cid:22)(cid:28), 
the scores  are  higher than for the  other  two  threshold  values.  for  a  given  document,  the  value  of the (cid:22)(cid:23)  is 
always less than for the other two thresholds, and the value of (cid:22)(cid:28) is always greater than for the other ones. for 
the thresholds (cid:22)(cid:23), (cid:22)(cid:25), and (cid:22)(cid:28) is equal to 108, 35, and 19, respectively. this shows that when we use the (cid:22)(cid:23) as 

the preliminary experiments evaluation corpus, the average number of selected features for a document using 

the threshold value, the number of selected features is relatively high. in this case, only some of the features 

indicate to essential concepts, and the summarizer can be misled by numerous unimportant features. on the 

other hand, when we use the (cid:22)(cid:25) and (cid:22)(cid:28), the number of selected features decreases to almost less than one third. 

this  reduction  in  the  number  of  features  helps  the  summarizer  to  decide  more  accurately,  considering  the 

features which point to important concepts indeed. 

as fig. 8 shows, when the summarizer uses the fourth feature selection method, the best scores are reported 

for the both meaningfulness level of    1.3 and    1.2. in this case, the average number of features is 328. as 

showed in fig. 9, for the threshold values greater than zero, the average number of features falls to nearly 50. 

this rapid decrease happens because for the majority of concepts the meaningfulness value is zero. as the 

number  of  features  decreases  rapidly,  the  quality  of  summarization  also  decreases,  because  many  features 

which could help the summarizer to perform more accurately are no longer available. the results show that 

the meaningfulness threshold of    1.3 discards many features which can be considered as redundant ones. when 

we assign threshold values smaller than -1.3, redundant features decrease the performance of the summarizer. 

there are other measures, such as inverse document frequency (idf), inverse sentence frequency (isf), 

and inverse term frequency (itf) that are widely adopted in id111 research [58]. such measures may 

seem to be functionally similar to the meaningfulness measure defined by the helmholtz principle. however, 

by studying the theoretical and practical functions of the measures, we found the meaningfulness more useful 

than others for selecting important concepts in the bayesian summarizer for several reasons. first, the idf, 

isf, and itf measures are generally defined for a corpus of documents, but the bayesian summarizer analyzes 

one document at a time, and the definition of such measures may not make sense for single documents. second, 

if we define the isf or itf weights in a document, concepts appearing in fewer sentences are assigned more 

discriminative  power.  such  weighting  scheme  could  not  be  useful  in  our  context.  on  the  other  hand,  the 

meaningfulness measure has been adopted successfully in single-document text analysis. furthermore, as the 

example in fig. 4 shows, the weights assigned by the helmholtz principle do not have any obvious relation to 

the frequency of concepts. according to the formulas in section 3.2.4, the meaningfulness value depends on 

manuscript                                                                  32                                                                    29 march 2017 

 

multiple factors, such as the frequency of concepts within each paragraph and within the whole document, the 

length of each paragraph, and the length of the document. 

as fig. 10 shows, when the summarizer uses the fifth feature selection method, the best scores are reported 

for the minimum support threshold of 0.09. for this value, the average number of features (itemsets) is 60. for 

the  smaller thresholds  which  produce  more  number of  features,  the  id8  scores  decrease  slightly.  this 

shows  that  more  number  of  features  could  not  significantly  reduce  the  accuracy  of  the  summarizer  in  this 

feature  selection  method.  although  the  numerous  features  mislead  the  summarizer  to  some  extent,  it  still 

benefits from the knowledge about the correlated concepts provided by the itemsets. when the threshold    

tends to be greater than 0.09, the average number of features decreases, and the performance of the summarizer 

also decreases. particularly, when the threshold is greater than 0.14, the average number of features drops to 

less than 17 and the id8 scores are reduced considerably. this shows that when the number of features 

decreases in fifth feature selection method, the knowledge of the summarizer about important and correlated 

concepts is inadequate. it decides according to a limited number of high-supporting itemsets whereas there are 

a lot of useful itemsets discarded by an extreme threshold. 

6.2.the coefficients c1 and c2 

as can be seen in table 3, for all the five feature selection methods, the bayesian summarizer obtains better 

id8  scores  when  it  uses  the  coefficients.  since  for  the  majority  of  documents  even  the  most  frequent 

features do not appear in more than 50% of the sentences of a document, the most probable value for almost 

every feature is false. when the method does not use the coefficients, it decides based on the most probable 

values of the features. in this case, the summarizer needs additional knowledge about the features and their 

importance to decide more accurately. using the coefficients, when the value of a feature is true in a sentence, 

the  id203  of  selecting  the  sentence  for  the  summary  increases  in  proportion  to  the  occurrence  of  the 

feature. moreover, the id203 of not selecting the sentence decreases. on the other hand, when the value 

of  a  feature  is  false  in  a  sentence,  the  id203  of  selecting  the  sentence  for  the  summary  decreases  in 

proportion to the occurrence of the feature. moreover, the id203 of not selecting the sentence increases. 

adopting  this  strategy,  the  summarizer  can  discriminate  between  the  presence  and  absence  of  important 

concepts. as a result, the summarizer performs better and reports higher scores. 

6.3.the redundancy reduction method 

as table 4 shows, when the redundancy reduction method takes part in the summarization method, the 

summarizer reports better scores. with the help of the redundancy reduction method, the summarizer gives 

sentences containing low-frequency concepts a higher chance to be included in the summary. therefore, the 

summary can cover more number of concepts while it still conveys important concepts and subtopics. when 

summaries  cover  more  relevant  information,  their  informativeness  increases.  as  a  results,  the  summarizer 

obtains higher scores. 

manuscript                                                                  33                                                                    29 march 2017 

 

table 5 suggests that the percentage of improvement for the smaller compression rates is higher than for 

the greater rates. this happens because for smaller compression rates, fewer sentences must be selected for the 

summary. hence, when we use the redundancy reduction method and the summarizer selects new sentences 

containing new information, the scores report a more impressive improvement. on the other hand, for greater 

compression rates, the summarizer selects more sentences. in this way, the summary automatically includes 

new information. the summary presents the new information along with some potentially redundant sentences 

which were selected earlier or would be selected later. the redundancy reduction method may discard these 

redundant sentences and select new relevant information. therefore, the performance of the summarizer may 

be improved. however, this improvement for greater compression rates is less than for smaller rates, because 

new relevant information is automatically included in the summary by the greater compression rates. 

as  table  6  shows,  using  the  redundancy  method,  the  summaries  cover  more  concepts  in  average.  in 

addition, as can be seen in table 4, the usage of the redundancy reduction method leads to an increase in the 

scores for all the feature selection methods. however, when we compare each pair of feature selection methods, 

the greater average number of concepts covered in the produced summaries does not necessarily lead to better 

summarization  performance.  for  example,  in  both  cases  of  using  and  not  using  the  redundancy  reduction 

method, the average number of concepts covered in the summaries for the fifth feature selection method is less 

than  the  average  for  the  first,  second  and  fourth  methods.  nevertheless,  the  summarizer  obtains  the  best 

id8  scores  using  the  fifth  method.  this  suggests  that  with  the  use  of  an  appropriate  feature  selection 

method,  the  summaries  convey  more  informative  content  even  if  they  cover  fewer  concepts.  these  results 

demonstrate  that  both  the  redundancy  reduction  method  and  an  appropriate  feature  selection  method  are 

essential to enhance the performance of the summarizer. the lack of each one has a negative impact on the 

quality of produced summaries. 

6.4.comparison with other summarizers 

as table 7 shows, when the bayesian summarizer utilizes the first feature selection method, it performs 

better than the domain-independent competitors. this suggests that using concepts as classification features in 

our method can be a better approach compared to the summarizers which employ word frequency methods, 

positional  features,  and  term  similarity  features.  our  summarizer  performs  slightly  worse  than  the  two 

biomedical summarizers, i.e. freqdist and chainfreq, when it considers all extracted concepts as features. in 

this case, it seem that potentially redundant and unrelated concepts negatively affect the quality of produced 

summaries. 

comparing the results of the first and second feature selection methods, we observe that when generic and 

potentially redundant concepts are discarded, the summarizer can decide more accurately and obtains higher 

scores. looking at the number of features selected by the first and second methods in table 8, it seems that 

almost a half of concepts extracted from a document can be considered as unnecessary. removing redundant 

manuscript                                                                  34                                                                    29 march 2017 

 

concepts from classification features, we observe a slight increase in the performance of the summarizer. the 

results of the second feature selection method show that, with respect to biochain, considering the distribution 

of non-generic concepts along with the redundancy reduction method improves the performance of biomedical 

summarization. with respect to freqdist and chainfreq, we still need to make more refinement in our feature 

selection strategy to increase the quality of produced summaries. 

the third feature selection method employs a ranking and filtering strategy. as the results show, the use of 

all extracted concepts to constructing a frequency distribution model, i.e. freqdist, can be outperformed by 

the  bayesian  modeling  in  combination  with  an  optimized  feature  selection  based  on  the  filtering  method. 

although chainfreq does not use all extracted concepts and utilizes biochain as a filtering method for its 

hybrid method, the bayesian summarizer obtains relatively better scores using an appropriate threshold for 

filtering the features. when we employ the third method for feature selection, the maximum, minimum and 

average  number  of  features  for  a  document  in  the  evaluation  corpus  is  35,  6,  and  22  respectively.  as  the 

numbers in table 8 show, the third method considerably reduces the average number of features compared to 

the second method, from 519 to 22. this reduction helps the summarizer to decide more accurately. regarding 

the  results  of  the  second  method,  this  filtering  strategy  leads  to  a  slight  increase  in  the  performance  of 

summarization. 

the results of the fourth feature selection method show that the meaningfulness is a better measure than the 

frequency for feature selection in the bayesian summarizer. as table 8 shows, the fourth method produces a 

large number  of features  compared  to the  third  method,  344  versus 22 for  the average  number  of features. 

however, the fourth method yields better summarization performance. this suggests that the meaningfulness 

can be considered as a more efficient measure to remove unimportant concepts in the bayesian summarizer. 

this  measure  provides  the  summarizer  with  a  set  of  indeed  important  concepts  to  decide  more  optimally. 

however,  some  of  concepts  selected  as  features  may  not  be  considered  as  a  frequent  concept.  when  the 

summarizer utilizes this measure, it still makes use of the frequency of concepts in the form of the coefficients. 

in  fact,  the  summarizer  combines  information  about  the  meaningfulness  measure  and  the  frequency  of 

concepts.  using  this  approach,  the  bayesian  summarizer  obtains  higher  scores  than  all  the  biomedical 

competitors. 

as  table  7  shows, the  bayesian  summarizer  reports the  highest scores  when it utilizes the fifth  feature 

selection method and uses frequent itemsets as features. using this method, the summarizer implicitly takes 

into account correlations and appearing patterns existing among concepts. the results show that this feature 

selection  strategy  and  the  bayesian  modeling  yield  better  summarization  quality  than  the  biomedical 

summarizers relying on the frequency of single concepts. according to the results, the fifth feature selection 

method performs slightly better than the fourth method. this suggests that the bayesian summarizer can utilize 

either information about correlated concepts or the meaningfulness measure as two useful feature selection 

approaches to improve the performance of summarization. comparing the results of different feature selection 

manuscript                                                                  35                                                                    29 march 2017 

 

methods, we observe that frequent itemsets can be more useful than the frequency of single concepts in the 

bayesian summarizer. 

according  to  the  results,  our  bayesian  summarizer  significantly  outperforms  the  domain-independent 

comparison methods, i.e. summa, swesum, and autosummarize, in biomedical text summarization. these 

methods utilize statistical, similarity-based, and word frequency features for sentence selection. the results 

show that these term-based methods cannot be considered as useful summarizers for biomedical text. on the 

other hand, using domain knowledge and efficient feature selection methods, the bayesian summarizer can 

perform more efficiently than the comparison methods. 

fig. 11. the frequency distribution of concepts within the full-text papers in the evaluation corpus. 

 

 

fig. 12. the frequency distribution of concepts within the abstracts in the evaluation corpus.  

manuscript                                                                  36                                                                    29 march 2017 

 

6.5.justification of the basic assumption 

as we explained in section 3.4, the bayesian summarizer estimates the posterior id203 of selecting 

and  not  selecting  sentences  for  the  summary  based  on  the  prior  id203  of  concepts  within  the  input 

document.  in  other  words,  the  summarizer  assumes  that  the  distribution  of  important  concepts  within  the 

summary should be similar to the original text. this assumption has been justified by reeve et al. [10]. they 

selected a corpus of 24 biomedical full-text papers and used the abstracts of the papers as the ideal summaries. 

they constructed two frequency distribution models from the concepts of the full-text papers and the abstracts 

and  showed  that  these  two  frequency  distribution  models  follow  zipfian  distribution.  regarding  this 

observation, they suggested that a full-text paper and a version of its ideal summary (abstract) have the same 

frequency distribution form. we perform a similar experiment using a larger corpus. we extract concepts from 

the full-texts and the abstracts of our evaluation corpus containing 400 biomedical papers. fig. 11 shows the 

frequency  distribution  of  16,353  concepts  within  the  full-text  papers,  and  fig.  12  shows  the  frequency 

distribution of 6,645 concepts within the abstracts. as can be observed from fig. 11 and fig. 12, both the full-

texts  and  the  abstracts  (considered  as  the  ideal  summaries)  follow  zipfian  distribution.  this  observation 

justifies the basic assumption of our proposed biomedical text summarization method that uses the distribution 

of  concepts  within  the  document  to  estimate  the  posterior  id203  of  the  sentences  for  inclusion  in  the 

summary. 

 

7.  conclusion 

in this paper, we propose a biomedical text summarization method using a bayesian classification approach. 

the method classifies the sentences of the input document as summary and non-summary according to the 

distribution  of  important  concepts  within  the  text.  we  introduce  different  feature  selection  approaches  to 

identify the important concepts of the document and using them as classification features. the summarizer 

uses two coefficients in id203 estimation to discriminate between the presence and absence of important 

concepts. it also employs a simple redundancy reduction method to reduce the potential redundancy in the 

summary. conducting a set of preliminary experiments on a development corpus containing 100 biomedical 

papers, we tune the parameters of the system and assess the efficiency of the coefficients and the redundancy 

reduction method. the results show that the coefficients and the redundancy reduction method have a positive 

impact  on  the  quality  of  produced  summaries,  leading  to  an  improvement  in  the  performance  of  the 

summarizer. 

we  evaluate  the  performance  of  the  bayesian  summarization  method  in  comparison  with  the  other 

biomedical summarizers relying on the frequency of concepts, domain-independent summarizers, and baseline 

methods  using  an  evaluation  corpus  of  400  biomedical  papers.  the  results  show  that  when  the  bayesian 

summarizer  utilizes  the  meaningfulness  measure  rather  than  the  frequency  of  single  concepts  for  selecting 

manuscript                                                                  37                                                                    29 march 2017 

 

features, it outperforms the other summarizers. moreover, when the summarizer employs itemset mining and 

uses  correlated  concepts  as  classification  features,  it  significantly  performs  better  than  the  comparison 

methods. summing up the results, we can draw the following conclusions that answer to the questions raised 

in section 1: 

     a bayesian classification method can be utilized for the id203 distribution modeling of concept-based 

biomedical text summarization. an efficient feature selection method is required to enhance the accuracy 

of the classification method. 

     the  summarizer  should  not  consider  all  the  extracted  concepts  from  the  input  document.  there  many 

redundant concepts which may have a negative impact on the accuracy of the model and can be discarded 

by the summarizer. 

     the meaningfulness measure defined by the helmholtz principle can be a useful criterion, rather than the 

frequency, to identify important concepts and use them as classification features. 

     using itemset mining to discover correlated concepts and incorporating these correlations into the feature 

selection phase provide the summarizer with a more accurate model, leading to an increase in the obtained 

scores. 

in our future research we intend to concentrate on extending our bayesian biomedical summarizer to deal 

with multi-document and query-focused summarization. to do so, a more complicated redundancy reduction 

method should be studied. it seems that there is much more room for studying the helmholtz principle from 

the gestalt theory in the context of concept-based summarization. balinsky et al. [59] have modeled document 

sentences as a small world network using the helmholtz principle and investigated some applications such as 

text summarization. future work can involve exploring this type of modeling for concept-based biomedical 

text summarization. the study of using other discriminative classifiers in this type of summarization can be 

considered as another potential topic for future research. 

 

conflict of interest 

the authors declare that they have no conflict of interest. 

 

references 

[1]  mishra  r,  bian  j,  fiszman  m,  weir  cr,  jonnalagadda  s,  mostafa  j,  et  al. text  summarization  in  the  biomedical 
domain: a systematic review of recent research. journal of biomedical informatics. 2014;52:457-67. 
[2]  afantenos  s,  karkaletsis  v,  stamatopoulos  p.  summarization  from  medical  documents:  a  survey.  artificial 
intelligence in medicine. 2005;33:157-77. 
[3] fleuren ww, alkema w. application of id111 in the biomedical domain. methods. 2015;74:97-106. 
[4] reeve lh, han h, brooks ad. the use of domain-specific concepts in biomedical text summarization. information 
processing & management. 2007;43:1765-76. 

manuscript                                                                  38                                                                    29 march 2017 

 

[5] plaza l, d  az a, gerv  s p. a semantic graph-based approach to biomedical summarisation. artificial intelligence in 
medicine. 2011;53:1-14. 
[6] chen p, verma r. a query-based medical information summarization system using ontology knowledge.  19th ieee 
symposium on computer-based medical systems (cbms'06): ieee; 2006. p. 37-42. 
[7]  plaza  l,  carrillo-de-albornoz  j.  evaluating  the  use  of  different  positional  strategies  for  sentence  selection  in 
biomedical literature summarization. bmc bioinformatics. 2013;14:1. 
[8] men  ndez hd, plaza l, camacho d. combining graph connectivity and genetic id91 to improve biomedical 
summarization.  2014 ieee congress on evolutionary computation (cec): ieee; 2014. p. 2740-7. 
[9] reeve l, han h, brooks ad. biochain: lexical chaining methods for biomedical text summarization.  proceedings 
of the 2006 acm symposium on applied computing: acm; 2006. p. 180-4. 
[10] reeve lh, han h, nagori sv, yang jc, schwimmer ta, brooks ad. concept frequency distribution in biomedical 
text summarization.  proceedings of the 15th acm international conference on information and knowledge management: 
acm; 2006. p. 604-11. 
[11] nelson sj, powell t, humphreys b. the unified medical language system (umls) project. encyclopedia of library 
and information science. 2002:369-78. 
[12] balinsky aa, balinsky hy, simske sj. on helmholtz's principle for documents processing.  proceedings of the 10th 
acm symposium on document engineering: acm; 2010. p. 283-6. 
[13]  agrawal  r,  imieli  ski  t,  swami  a.  mining  association  rules  between  sets  of  items  in  large  databases.  acm 
sigmod record. 1993;22:207-16. 
[14] mitchell t. generative and discriminative classifiers: naive bayes and id28. manuscript available at 
http://www cs cm edu/~ tom/newchapters html. 2005. 
[15] lin c-y. id8: a package for automatic evaluation of summaries.  text summarization branches out: proceedings 
of the acl-04 workshop2004. 
[16] gupta v, lehal gs.  a survey of text summarization  extractive techniques. journal of emerging technologies in 
web intelligence. 2010;2:258-68. 
[17]  alguliev  rm,  aliguliyev  rm,  hajirahimova  ms,  mehdiyev  ca.  mcmr:  maximum  coverage  and  minimum 
redundant text summarization model. id109 with applications. 2011;38:14514-22. 
[18] gambhir m, gupta v. recent automatic text summarization techniques: a survey.  artificial intelligence review. 
2016:1-66. 
[19] workman te, fiszman m, hurdle jf. text summarization as a decision support aid. bmc medical informatics and 
decision making. 2012;12:41. 
[20] zhang h, fiszman m, shin d, miller cm, rosemblat g, rindflesch tc. degree centrality for semantic abstraction 
summarization of therapeutic studies. journal of biomedical informatics. 2011;44:830-8. 
[21] fiszman m, demner-fushman d, kilicoglu h, rindflesch tc. id54 of medline citations for 
evidence-based medical treatment: a topic-oriented evaluation. journal of biomedical informatics. 2009;42:801-13. 
[22] kilicoglu h. summarizing drug information in medline citations. 2006. 
[23] fiszman m, rindflesch tc, kilicoglu h. abstraction summarization for managing the biomedical research literature.  
proceedings  of  the  hlt-naacl  workshop  on  computational  lexical  semantics:  association  for  computational 
linguistics; 2004. p. 76-83. 
[24] zhang h, fiszman m, shin d, wilkowski b, rindflesch tc. id91 cliques for graph-based summarization of 
the biomedical research literature. bmc bioinformatics. 2013;14:1. 
[25]  moen  h,  peltonen  l-m,  heimonen  j,  airola  a,  pahikkala  t,  salakoski  t,  et  al.  comparison  of  automatic 
summarisation methods for clinical free text notes. artificial intelligence in medicine. 2016. 
[26] del fiol g, mostafa j, pu d, medlin r, slager s, jonnalagadda sr, et al. formative evaluation of a patient-specific 
clinical knowledge summarization tool. international journal of medical informatics. 2016;86:126-34. 
[27] morid ma, fiszman m, raja k, jonnalagadda sr, del fiol g. classification of clinically useful sentences in clinical 
evidence resources. journal of biomedical informatics. 2016;60:14-22. 
[28]  pivovarov  r,  elhadad  n.  automated  methods  for  the  summarization  of  electronic  health  records.  journal  of  the 
american medical informatics association. 2015;22:938-47. 
[29] plaza l. comparing different knowledge sources for the id54 of biomedical literature. journal 
of biomedical informatics. 2014;52:319-28. 
[30]  men  ndez  hd,  plaza  l,  camacho  d.  a  genetic  graph-based  id91  approach  to  biomedical  summarization.  
proceedings of the 3rd international conference on web intelligence, mining and semantics: acm; 2013. p. 10. 
[31]  sarkar  k.  using  domain  knowledge  for  text  summarization  in  medical  domain.  international  journal  of  recent 
trends in engineering. 2009;1:200-5. 
[32] sarkar k, nasipuri m, ghose s. using machine learning for medical document summarization. international journal 
of database theory and application. 2011;4:31-48. 
[33] sarker a, moll   d, paris c. extractive summarisation of medical documents using domain knowledge and corpus 
statistics. the australasian medical journal. 2012;5:478. 

manuscript                                                                  39                                                                    29 march 2017 

 

[34] barzilay r, elhadad m. using lexical chains for text  summarization.  advances in  automatic text summarization. 
1999:111-21. 
[35] kupiec j, pedersen j, chen f. a trainable document summarizer.  proceedings of the 18th annual international acm 
sigir conference on research and development in information retrieval: acm; 1995. p. 68-73. 
[36] daum   iii h, marcu d. bayesian query-focused summarization.  proceedings of the 21st international conference 
on computational linguistics and the 44th annual meeting of the association for computational linguistics: association 
for computational linguistics; 2006. p. 305-12. 
[37] wang d, zhu s, li t, gong y. id57 using sentence-based topic models.  proceedings of 
the acl-ijcnlp 2009 conference short papers: association for computational linguistics; 2009. p. 297-300. 
[38] larose dt, larose cd. data mining and predictive analytics: john wiley & sons; 2015. 
[39] national library of medicine. umls metathesaurus fact sheet. 
[40] national library of medicine. umls specialist lexicon fact sheet. 
[41] national library of medicine. umls semantic network fact sheet. 
[42]  aronson  ar.  effective  mapping  of  biomedical  text  to  the  umls  metathesaurus:  the  metamap  program.  
proceedings of the amia symposium: american medical informatics association; 2001. p. 17. 
[43]  plaza  l,  stevenson  m,  d  az  a.  resolving  ambiguity  in  biomedical  text  to  improve  summarization.  information 
processing & management. 2012;48:755-66. 
[44] chandrashekar g, sahin f. a survey on feature selection methods. computers & electrical engineering. 2014;40:16-
28. 
[45]  forman  g.  an  extensive  empirical  study  of  feature  selection  metrics  for  text  classification.  journal  of  machine 
learning research. 2003;3:1289-305. 
[46]  balinsky  a,  balinsky  h,  simske  s.  on  the  helmholtz  principle  for  data  mining.  hewlett-packard  development 
company, lp. 2011. 
[47] balinsky a, balinsky h, simske s. rapid change detection and id111.  proceedings of the 2nd conference on 
mathematics in defence (ima), defence academy, uk2011. 
[48] balinsky h, balinsky a, simske sj. automatic text summarization and small-world networks.  proceedings of the 
11th acm symposium on document engineering: acm; 2011. p. 175-84. 
[49] tutkan m, ganiz mc, akyoku   s. helmholtz principle based supervised and unsupervised feature selection methods 
for id111. information processing & management. 2016. 
[50] agrawal  r, mannila h, srikant r, toivonen h, verkamo ai. fast discovery of  association  rules.  advances in 
knowledge discovery and data mining. 1996;12:307-28. 
[51]  ferreira  r,  de  souza  cabral  l,  freitas  f,  lins  rd,  de  fran  a  silva  g,  simske  sj,  et  al.  a  multi-document 
summarization system based on statistics and linguistic treatment. id109 with applications. 2014;41:5780-7. 
[52]  carbonell  j,  goldstein  j.  the  use  of  mmr,  diversity-based  reranking  for  reordering  documents  and  producing 
summaries.    proceedings  of  the  21st  annual  international  acm  sigir  conference  on  research  and  development  in 
information retrieval: acm; 1998. p. 335-6. 
[53] lin c-y. looking  for a few good  metrics:  automatic  summarization evaluation-how  many samples are enough?  
ntcir2004. 
[54] mani i. summarization evaluation: an overview. 2001. 
[55] saggion h. summa: a robust and adaptable summarization tool. traitement automatique des langues. 2008;49. 
[56] swesum: automatic text summarizer. 
[57] mitkov r. the oxford handbook of computational linguistics: oxford university press; 2005. 
[58]  blake  c.  a  comparison  of  document,  sentence,  and  term  event  spaces.    proceedings  of  the  21st  international 
conference on computational linguistics and the 44th annual meeting of the association for computational linguistics: 
association for computational linguistics; 2006. p. 601-8. 
[59] balinsky h, balinsky a, simske s. document sentences as a small world.  systems, man, and cybernetics (smc), 
2011 ieee international conference on: ieee; 2011. p. 2583-8. 

 

 

manuscript                                                                  40                                                                    29 march 2017 

