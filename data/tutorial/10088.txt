6
1
0
2

 

v
o
n
4
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
4
4
2
4
0

.

1
1
6
1
:
v
i
x
r
a

classify or select: neural architectures for
extractive document summarization

ramesh nallapati, bowen zhou
ibm watson
yorktown heights, ny 10598 usa
{nallapati,zhou}@us.ibm.com

mingbo ma
oregon state university
kelley engineering center, corvallis, or, 97331
mam@oregonstate.edu

abstract

we present two novel and contrasting recurrent neural network (id56) based
architectures for extractive summarization of documents. the classi   er based
architecture sequentially accepts or rejects each sentence in the original document
order for its membership in the    nal summary. the selector architecture, on the
other hand, is free to pick one sentence at a time in any arbitrary order to piece
together the summary.
our models under both architectures jointly capture the notions of salience and
redundancy of sentences. in addition, these models have the advantage of being
very interpretable, since they allow visualization of their predictions broken up by
abstract features such as information content, salience and redundancy.
we show that our models reach or outperform state-of-the-art supervised models
on two different corpora. we also recommend the conditions under which one
architecture is superior to the other based on experimental evidence.

1

introduction

document summarization is an important problem that has many applications in information re-
trieval and natural language understanding. summarization techniques are mainly classi   ed into
two categories: extractive and abstractive. extractive methods aim to select salient snippets, sen-
tences or passages from documents, while abstractive summarization techniques aim to concisely
paraphrase the information content in the documents.
a vast majority of the literature on document summarization is devoted to extractive summarization.
traditional methods for extractive summarization can be broadly classi   ed into greedy approaches
(e.g., carbonell & goldstein (1998)), graph based approaches (e.g., radev & erkan (2004)) and
constraint optimization based approaches (e.g., mcdonald (2007)).
recently, neural network based approaches have become popular for extractive summarization. for
example, kageback et al. (2014) employed the recursive autoencoder (socher et al. (2011)) to sum-
marize documents, producing best performance on the opinosis dataset (ganesan et al. (2010)).
yin & pei (2015) applied convolutional neural networks (id98) to project sentences to continuous
vector space and then select sentences by minimizing the cost based on their    prestige    and    di-
verseness   , on the task of multi-document extractive summarization. another related work is that of
cao et al. (2016), who address the problem of query-focused id57 using
query-attention-weighted id98s.
recently, with the emergence of strong generative neural models for text bahdanau et al. (2014),
abstractive techniques are also becoming increasingly popular (rush et al. (2015), nallapati et al.
(2016b) and nallapati et al. (2016a)). despite the emergence of abstractive techniques, extractive
techniques are still attractive as they are less complex, less expensive, and generate grammatically
and semantically correct summaries most of the time.
in a very recent work, cheng & lapata
(2016) proposed an attentional encoder-decoder for extractive single-document summarization and
trained it on daily mail corpus, a large news data set, achieving state-of-the-art performance. like
cheng & lapata (2016), our work also focuses only on sentential extractive summarization of single
documents using neural networks.

1

2 two architectures

our architectures are motivated by two intuitive strategies that humans tend to adopt when they are
tasked with extracting salient sentences in a document. the    rst strategy, which we call classify,
involves reading the whole document once to understand its contents, and then traversing through
the sentences in the original document order and deciding whether or not each sentence belongs
to the summary. the other strategy that we call select involves memorizing the whole document
once as before, and then picking sentences that should belong to the summary one at a time, in any
order of one   s choosing. qualitatively, the latter strategy appears to be a better one since it allows
us to make globally optimal decisions at each step. while it may be harder for humans to follow
this strategy since we are forgetful by nature, one may expect that the select strategy could deliver
an advantage for the machines, since    forgetfulness    is not a real    concern    for them. in this work,
we will explore both the strategies empirically and make a recommendation on which strategy is
optimal under what conditions.
broadly, our classify architecture involves an id56 based sequence classi   cation model that se-
quentially classi   es each sentence into 0/1 binary labels, while the select architecture involves a
generative model that sequentially generates the indices of the sentences that should belong to the
summary. we will    rst discuss the components shared by both the architectures and then we will
present each architecture separately.
shared building blocks: both architectures begin with word-level bidirectional gated recurrent
unit (gru) based id56s (chung et al. (2014)) run independently over each sentence in the docu-
ment, where each time-step of the id56 corresponds to a word index in the sentence. the average
pooling of the concatenated hidden states of this bidirectional id56 is then used as an input to an-
other bidirectional id56 whose time steps correspond to sentence indices in the document. the
concatenated hidden states    h    from the forward and backward layers of this second layer of bidi-
rectional id56 at each time step are used as corresponding sentence representations. we also use
the average pooling of the sentence representations as the id194    d   . both ar-
chitectures also maintain a dynamic summary representation    s    whose estimation is architecture
dependent. models under each architecture compute a score for each sentence towards its summary
membership. motivated by the need to build humanly interpretable models, we compute this score
by explicitly modeling abstract features such as salience, novelty and information content as shown
below:

score(hj, sj, d, pj) = wc  (wt

c hj)
+ws  (cos(hj, d))
p pj)
   wr  (cos(hj, sj))
+b),

+wp  (wt

#(content richness)
#(salience w.r.t. document)

#(positional importance)
#(redundancy w.r.t. summary)
#(bias term)

(1)

where j is the index of the sentence in the document, pj is the positional embedding of the sentence
computed by concatenation of embeddings corresponding to forward and backward position indices
of the sentence in the document; cos(a, b) is the standard cosine similarity between the two vectors
a and b; wc and wp are parameter vectors to model content richness and positional importance
of sentences respectively; and wc, ws, wp and wr are scalar weights to model relative importance of
various abstract features, and are learned automatically. in the equation above, the abstract feature
that each term represents is printed against the term in comments. in other words, assuming the
importance weights are positive, in order for a sentence to score high for summary membership, it
needs to be highly salient, content rich and occupy important positions in the document, while being
least redundant with respect to the summary generated till that point. note that our formulation of
the scoring function simultaneously captures both salience of the sentence hj with respect to the
document d as well as its redundancy with respect to the current summary representation sj. in
the next subsection, we will describe the estimation of dynamic summary representation sj and the
formulation of the cost function for training in each architecture. we will also present shallow and
deep models under each architecture.

2

2.1 classifier architecture

(cid:96)(w, w, b) =     n(cid:88)

nd(cid:88)

in this architecture, we sequentially visit each sentence in the original document order and binary-
classify the sentence in terms of whether it belongs to the summary. the id203 of the sentence
belonging to the summary, p (yj = 1) is given as follows:

p (yj = 1|hj, sj, d, pj) =   (score(hj, sj, d, pj)

(2)
the objective function to minimize at training is the negative log-likelihood of the training data
labels:

(yd

j log p (yd

j = 1|hd

j , dd) + (1     yd

j ) log(1     p (yd

j = 1|hd

j , sd

j , sd

j , dd))

d=1

j=1

where n is the size of the training corpus and nd is the number of sentences in the document d.
now the only detail that remains is how the dynamic summary representation sj is estimated. this
is where the shallow and deep models under this architecture differ, and we describe them below.

figure 1: the shallow and deep versions of the classi   er architecture for extractive summarization.

shallow model: in the shallow model, we estimate the dynamic summary representation as the
running sum of the representations of the sentences visited so far weighted by their id203 of
being in the summary.

j   1(cid:88)
j   1(cid:88)

i=1

sj =

sj =

hiyi

#(training time)

hip (yi = 1|hi, si, d) #(test time)

i=1

(3)
in other words, at training time, since the summary membership of sentences is known, the proba-
bilities are binary, whereas at test time we use a weighted pooling based on the estimated id203
that each sentence belongs to the summary. there is no need to normalize the summary represen-
tations since the cosine similarity metric we use in the scoring function of eq. (1) automatically
normalizes them.

3

1001abstractfeaturesbinarylabelssummaryrepresentationssent. 1sent. 2sent. 3sent. 4doc.rep.(a) shallow classifier modelsentence representationsabstractfeatures1001doc.rep.hidden statesabstractfeaturesbinarylabelssummaryrepresentationssent. 1sent. 2sent. 3sent. 4doc.rep.h  1h  2h  3h  4(b) deep classifier modelsentence representationsgruhidden statesdeep model: in the deep model, we introduce an additional layer of unidirectional sentence-level
gru-id56 that takes as input the sentence representations hj at each time-step. the hidden state of
the new gru   hj = gru (hj) is used as a replacement for sentence representation hj in computing
summary membership scores using eq. (1) as well as in computing the dynamic summary represen-
tation using eq. (3). the main idea behind using this additional layer of gru is to allow a greater
degree of non-linearity in computing the summary representation.
the graphical representations of the shallow and deep models under the classi   er architecture are
displayed in figure 1 with their full set of dependencies.

2.2 selector architecture

in this architecture, the models do not make decisions in the sequence of sentence ordering; instead,
they pick one sentence at a time in an order that they deem    t. the act of picking a sentence
is cast as a sequential generative model in which one sentence-index is emitted at each time step
that maximizes the score in eq. 1. accordingly, the id203 of picking a sentence with index
i(j) = k     {1, . . . , nd} at time-step j is given by the softmax over the scoring function:

exp(score(hk, sj, d, pk))

l   {1,...,nd} exp(score(hl, sj, d, pl))

(4)

p (i(j) = k|sj, hk, d) =

(cid:80)
(cid:96)(w, w, b) =     n(cid:88)

md(cid:88)

the id168 in this case is the negative log-likelihood of the selected sentences in the ground
truth data as shown below.

log p (i(j)(d)|hi(j)(d), sd

j , dd)

(5)

d=1

j=1

where md is the number of sentences selected in the ground truth of document d,
{i(1)(d), . . . , i(md)(d)} is the ordered list of selected sentence indices in the ground truth of docu-
ment d. the dependence of the id168 on the order of the selected sentences can be gauged by
the fact that the id203 of selecting a sentence at time step j depends on the dynamic summary
representation sj, which is estimated based on the all sentences selected up to time step j     1.
at test time, at each time-step, the model emits the index of the sentence that has the best score
given the current summary representation as shown below.

i(j) = arg max

k   {1,...,nd} score(hk, sj, d, pk)

(6)

the estimation of dynamic summary representation is done differently for the shallow and deep
selector models as described below.
shallow model: in this model, we sum the representations of the selected sentences until the time
step j as the dynamic summary representation. this is true for both training time and test time.

sj =

hi(i).

(7)

i=1

deep model: in the deep model, we introduce an additional gru-id56 whose time steps corre-
spond to the sentence index emission events. at each time-step, it takes as input the representation
of the previously selected sentence hi(j   1), and computes a new hidden state   hj = gru (hi(j   1)).
unlike the shallow model that maintains a separate vector for summary representation sj, we use
  hj as the summary representation sj at time step j. this makes sense for the case of the selector
architecture since both at training and test time we make hard decisions of sentence selection, with
the effect that the hidden state of the new gru can capture a non-linear aggregation of the sentences
selected until time step j     1.
fig. 2 shows the graphical representation of the selector architecture with all the dependencies
between the nodes. the architecture is the same for both shallow and deep models with the only
difference being that the simple summary representation in the former is replaced with a gated
recurrent unit in the latter.

4

j   1(cid:88)

figure 2: selector architecture for extractive summarization. the shallow and deep versions are identical
except for the fact that the simple vector representation for summary representation in the shallow version is
replaced with a gated recurrent unit in the deep version.

3 related work

previous researcher such as shen et al. (2007) have proposed modeling extractive document sum-
marization as a sequence classi   cation problem using id49. our approach is
different from theirs in the sense that we use id56s in our model that do not require any handcrafted
features for representing sentences and documents.
the selector architecture broadly involves ranking of sentences by some criterion, therefore does
correspond to traditional methods for extractive summarization such as textrank (mihalcea & ta-
rau (2004)) that also involve ranking of sentences by salience and novelty. however, to the best of
our knowledge, our selector framework is a novel deep learning framework for extractive summa-
rization. broader efforts are being made in the deep learning community to build more sophisticated
sequence to sequence models towards the objective of automatically learning complex tasks such
as sorting sequences (oriol vinyals (2015); graves et al. (2014)), but their utility for extractive
summarization remains to be explored.
in the deep learning framework, the extractive summarization work of cheng & lapata (2016) is the
closest to our work. their model is based on an encoder-decoder approach where the encoder learns
the representation of sentences and documents while the decoder classi   es each sentence using an
attention mechanism. broadly, their model is also in the classi   er framework, but architecturally,
our approaches are different. while their approach can be termed as a multi-pass approach where
both the encoder and decoder consume the same sentence representations, our approach is a deep
one where the representations learned by the bidirectional gru encoder are in turn consumed by the
classi   er or selector models. another key difference between our work and theirs is that unlike our
unsupervised greedy approach to convert abstractive summaries to extractive labels, cheng & lapata
(2016) chose to train a separate supervised classi   er using manually created labels on a subset of the
data. this may yield more accurate gold extractive labels which may help boost the performance of
their models, but incurs additional annotation costs.

4 experiments and results

pseudo ground-truth generation: in order to train our extractive classi   er and selector models, for
each document we need ground truth in the form of sentence-level binary labels and ordered list of
selected sentences respectively. however, most summarization corpora only contain human written
abstractive summaries as ground truth. to solve this problem, we use an unsupervised approach to

5

sent. 1sent. 2doc.rep.sent. 1 feat.softmaxsent. id 1nullsent. 3rep.sentence & documentrepresentationssent. 1 feat.sent. 1 feat.sent. 2 feat.sent. 3 feat.softmaxsent. id 1sent. 1 feat.sent. 2 feat.sent. 3 feat.softmaxsent. id 2sent. 1 feat.sent. 2 feat.sent. 3 feat.softmaxsent. id 3abstract featuresgeneratedsentence indicessent. 1 feat.sent. 2 feat.sent. 3 feat.sent. 1 feat.sent. 2 feat.sent. 3 feat.sent. 1 feat.sent. 2 feat.sent. 3 feat.summaryrepresentationsabstract featuresconvert the abstractive summaries to extractive labels. our approach is based on the idea that the
selected sentences from the document should be the ones that maximize the id8 score with respect
to gold abstractive summaries. since it is computationally expensive to    nd a globally optimal
subset of sentences that maximizes the id8 score, we employ a greedy approach, where we add
one sentence at a time incrementally to the summary, such that the id8 score of the current set of
selected sentences is maximized with respect to the entire gold summary. we stop adding sentences
when either none of the remaining candidate sentences improves the id8 score upon addition to
the current summary set or when the maximum summary length is reached. we return this ordered
list of sentences as the ground-truth for the selector architecture. the ordered list is converted into
binary summary-membership labels that are consumed by the classi   er architecture for training.
we note that similar approaches have been employed by other researchers such as svore et al. (2007)
to handle the problem of converting abstractive summaries to extractive ground truth. we would also
like to point readers to a recent work by cao et al. (2015) that proposes an ilp based approach to
solve this problem optimally. since this is not the focus of this work, we chose a simple greedy
algorithm.
corpora: for our experiments, we used the daily mail corpus originally constructed by hermann
et al. (2015) for the task of passage-based id53, and re-purposed for the task of
document summarization as proposed in cheng & lapata (2016) for extractive summarization and
nallapati et al. (2016a) for abstractive summarization. overall, we have 196,557 training documents,
12,147 validation documents and 10,396 test documents from the daily mail corpus. on average,
there are about 28 sentences per document in the training set, and an average of 3-4 sentences in the
reference summaries. the average word count per document in the training set is 802.
we also used the duc 2002 single-document summarization dataset1 consisting of 567 documents
as an additional out-of-domain test set to evaluate our models.
evaluation: in our experiments below, we evaluate the performance of our models using different
variants of the id8 metric2 computed with respect to the gold abstractive summaries. following
cheng & lapata (2016), we use limited length id8 recall at 75 bytes of summary as well as 275
bytes on the daily mail corpus. on duc 2002 corpus, following the of   cial guidelines, we use
limited length id8 recall at 75 words. we report the scores from id8-1, id8-2 and id8-
l, which are computed using matches of unigrams, bigrams and longest common subsequences
respectively, with the ground truth summaries.
baselines: on all datasets, we use lead-3 model, which simply produces the leading three sen-
tences of the document as the summary, as a baseline. on the daily mail and duc 2002 corpora,
we also report performance of lreg, a feature-rich logistic classi   er used as a baseline by cheng &
lapata (2016). on duc 2002 corpus, we report several baselines such as integer linear program-
ming based approach (woodsend & lapata (2010)), and graph based approaches such as tgraph
(parveen et al. (2015)) and urank (wan (2010)) which achieve very high performance on this
corpus. in addition, we also compare with the state-of-the art deep learning supervised extractive
model from cheng & lapata (2016).
experimental settings: we used 100-dimensional id97 (mikolov et al. (2013)) embeddings
trained on the daily mail corpus as our embedding initialization. we limited the vocabulary size
to 150k and the maximum sentence length to 50 words, to speed up computation. we    xed the
model hidden state size at 200. we used a batch size of 32 at training time, and employed adadelta
(zeiler (2012)) to train our model. we employed gradient clipping and l-2 id173 to prevent
over   tting and an early stopping criterion based on validation cost.
at test time, for the classi   er models we pick sentences sorted by the predicted probabilities until we
exceed the length limit as determined by the id8 metric. likewise, we allow the selector models
to emit sentence indices until the desired summary length is reached. for the selector model, we
also make sure the emitted sentence ids are not repeated across time steps by traversing down the
sorted predicted probabilities of the softmax layer at each time step until we reach a sentence-id
that was not emitted before.

1http://www-nlpir.nist.gov/projects/duc/guidelines/2002.html
2http://www.beid8.com/pages/default.aspx

6

we note that it is possible to optimize the classi   er performance at test time using the viterbi
algorithm to compute the best sequence of labels, subject to the markovian assumptions of the
architecture and model. similarly, it is also possible to further boost the selector   s performance by
using id125 at test time. however, in this work we used greedy classi   cation/selection for
id136 since our primary interest is in comparing the two architectures, and our choice allows us
to make a fair apples-to-apples comparison.
results on daily mail corpus: table 1 shows the performance comparison of our models with
state-of-the-art model of cheng & lapata (2016) and other baselines on the dailymail corpus using
id8 recall at two different summary lengths.

model

lead-3
lreg(500)
cheng    16
shal.-select
deep-select
shal.-cls.
deep-cls.

recall at 75 bytes

id8-1
21.9
18.5
22.7
25.6
26.1
26.0
26.2*   0.4

id8-2
7.2
6.9
8.5
10.3
10.7
10.5
10.7*   0.4

id8-l
11.6
10.2
12.5
14.0
14.4
14.23
14.4*   0.4

recall at 275 bytes

id8-1
40.5
n/a
42.2
41.3
41.3
42.1
42.2   0.2

id8-2
14.9
n/a
17.3*
16.8
15.3
16.8
16.8   0.2

id8-l
32.6
n/a
34.8
34.9
33.5
34.8
35.0   0.2

table 1: performance of various models on the entire daily mail test set using the limited length recall
variants of id8 with respect to the abstractive ground truth at 75 bytes and 275 bytes. entries with asterisk
are statistically signi   cant using 95% con   dence interval with respect to the nearest state-of-the-art model, as
estimated by the id8 script.

the results show that contrary to our initial expectation, the classi   er architecture is superior to the
selector architecture. within each architecture, the deeper models are better performing than the
shallower ones. our deep classi   er model outperforms cheng & lapata (2016) with a statistically
signi   cant margin at 75 bytes, while matching their model at 275 bytes. one potential reason our
models do not consistently outperform the extractive model of cheng & lapata (2016) is the ad-
ditional supervised training they used to create sentence-level extractive labels to train their model.
our models instead use an unsupervised greedy approximation to create extractive labels from ab-
stractive summaries, and as a result, may generate noisier ground truth than theirs.
results on the out-of-domain duc 2002 corpus: we also evaluated the models trained on the
dailymail corpus on the out-of-domain duc 2002 set as shown in table 2. the performance trend
is similar to that on daily mail. our best model, deep classi   er is again statistically on par with the
model of cheng & lapata (2016). however, both models perform worse than graph-based tgraph
(parveen et al. (2015)) and urank (wan (2010)) algorithms, which are the state-of-the-art models
on this corpus. deep learning based supervised models such as ours and that of cheng & lapata
(2016) perform very well on the domain they are trained on, but may suffer from id20
issues when tested on a different corpus such as duc 2002.

5 discussion

impact of document structure: in all our experiments thus far, the classi   er architecture has
proven superior to the selector architecture. we conjecture that decision making in the same se-
quence as the original sentence ordering is perhaps advantageous in document summarization since
there is a smooth sequential discourse structure in news stories starting with the main highlights
of the story in the beginning, more elaborate description in the middle and ending with conclusive
remarks. if this is true, then in scenarios where sentence ordering is less structured, the selector
architecture should be superior since it has freedom to select salient sentences in any arbitrary or-
der. such scenarios actually do occur in practice, e.g., summarization of a cluster of tweets on a
topic where there is no speci   c discourse structure between individual tweets, or in multi-document
summarization where a pair of sentences across document boundaries have no speci   c ordering. in
order to test this hypothesis, we simulated such data in the daily mail corpus by randomly shuf   ing
the sentences in each document in the training set and retraining models under both the architec-
tures, and evaluating them on the original test sets. the results, summarized in table 3, show that

7

lead-3
lreg
ilp
tgraph
urank
cheng et al    16
shallow-selector
deep-selector
shallow-classi   er
deep-classi   er

id8-1
43.6
43.8
45.4
48.1
48.5*
47.4
44.6
45.9
45.9
46.8   0.9

id8-2
21.0
20.7
21.3
24.3*
21.5
23.0
20.0
21.5
21.5
22.6   0.9

id8-l
40.2
40.3
42.8
-
-
43.5
41.1
42.4
42.3
43.1   0.9

table 2: performance of various models on the duc 2002 set using the limited length recall variants of
id8 at 75 words. our deep classi   er is statistically within the margin of error at 95% c.i. with respect
to the model of cheng & lapata (2016), but both are lower than state-of-the-art results due to out-of-domain
training.

the classi   er architecture suffers bigger losses than the selector architecture when the document
structure is destroyed. in fact, the selector architecture performs slightly better than the classi   er
architecture when trained on the shuf   ed data, indicating that our hypothesis may indeed be true.

shallow-selector
shallow-classi   er
deep-selector
deep-classi   er

trained on original data

trained on shuf   ed sentences
id8-1 id8-2 id8-l id8-1 id8-2 id8-l
41.3
42.1
41.3
42.2

33.0
32.9
32.5
32.9

40.6
40.1
40.5
40.1

15.6
15.3
15.3
15.1

16.8
16.8
15.3
16.8

34.9
35.0
33.5
35.0

table 3: simulated experiment to demonstrate the impact of document discourse structure on model perfor-
mance. evaluation is done using id8 limited length recall at 275 bytes. the selector architecture exhibits
superior performance when the discourse structure of the document is destroyed.

qualitative analysis: one of the advantages of our model design is teasing out various abstract
features for the sake of interpretability of system predictions. in the appendix, we present a visual-
ization (see fig. 3 in the appendix) of the system predictions based on the scores for various abstract
features listed in eq. (1). we also present the learned importance weights of these features in table
4. a few representative documents are also presented in the appendix highlighting the sentences
chosen by our models for summarization.

6 conclusion and future work

in this work, we propose two neural architectures for extractive summarization. our proposed mod-
els under these architectures are not only very interpretable, but also achieve state-of-the-art perfor-
mance on two different data sets. we also empirically compare our two frameworks and suggest
conditions under which each of them can deliver optimal performance.
as part of our future work, we plan to further investigate the applicability of the novel selector ar-
chitecture to relatively less structured summarization problems such as summarization of multiple
documents or topical clusters of tweets. in addition, we also intend to perform additional experi-
ments on the daily mail dataset such as incorporating id125 in both model id136 as well
in pseudo ground truth generation that may result in further performance improvements.

8

references
dzmitry bahdanau, kyunghyun cho, and yoshua bengio. id4 by jointly

learning to align and translate. arxiv preprint arxiv:1409.0473, 2014.

ziqiang cao, chengyao chen, wenjie li, sujian li, furu wei, and ming zhou. tgsum: build
tweet guided id57 dataset. corr, abs/1511.08417, 2015. url http:
//arxiv.org/abs/1511.08417.

ziqiang cao, wenjie li, sujian li, and furu wei. attsum: joint learning of focusing and summa-

rization with neural attention. arxiv preprint arxiv:1604.00125, 2016.

jaime carbonell and jade goldstein. the use of mmr, diversity-based reranking for reordering
in proceedings of the 21st annual international acm
documents and producing summaries.
sigir conference on research and development in information retrieval, pp. 335   336. acm,
1998.

jianpeng cheng and mirella lapata. neural summarization by extracting sentences and words. 54th
annual meeting of the association for computational linguistics, 2016. url http://arxiv.
org/abs/1603.07252.

junyoung chung, c   aglar g  ulc  ehre, kyunghyun cho, and yoshua bengio. empirical evaluation
of gated recurrent neural networks on sequence modeling. corr, abs/1412.3555, 2014. url
http://arxiv.org/abs/1412.3555.

kavita ganesan, chengxiang zhai, and jiawei han. opinosis: a graph-based approach to abstractive
summarization of highly redundant opinions. in proceedings of the 23rd international conference
on computational linguistics, pp. 340   348. association for computational linguistics, 2010.

alex graves, greg wayne, and ivo danihelka. id63s. corr, abs/1410.5401,

2014. url http://arxiv.org/abs/1410.5401.

karl moritz hermann, tom  as kocisk  y, edward grefenstette, lasse espeholt, will kay, mustafa su-
leyman, and phil blunsom. teaching machines to read and comprehend. corr, abs/1506.03340,
2015. url http://arxiv.org/abs/1506.03340.

mikael kageback, olof mogren, nina tahmasebi, and devdatt dubhashi. extractive summarization

using continuous vector space models. pp. 31   39. 2014.

ryan mcdonald. a study of global id136 algorithms in id57. pp.

557   564. 2007.

rada mihalcea and paul tarau. textrank: bringing order into texts. empirical methods in natural

language processing, 2004.

tomas mikolov, ilya sutskever, kai chen, greg s corrado, and jeff dean. distributed represen-
tations of words and phrases and their compositionality. in advances in neural information pro-
cessing systems, pp. 3111   3119, 2013.

ramesh nallapati, bowen zhou, cicero dos santos, caglar gulcehre, and bing xiang. abstractive
text summarization using sequence-to-sequence id56s and beyond. the signll conference on
computational natural language learning, 2016a.

ramesh nallapati, bowen zhou, and bing xiang. sequence-to-sequence id56s for text summariza-

tion. international conference on learning representations, workshop track, 2016b.

manjunath kudlur oriol vinyals, samy bengio. order matters: sequence to sequence for sets.

international conference on learning representations, 2015.

daraksha parveen, hans-martin ramsl, and michael strube. topical coherence for graph-based
extractive summarization. in proceedings of the conference on empirical methods in natural
language processing, pp. 19491954, 2015.

dragomir radev and g  unes erkan. lexrank: graph-based lexical centrality as salience in text

summarization. journal of arti   cial intelligence research, pp. 457   479, 2004.

9

alexander m rush, sumit chopra, and jason weston. a neural attention model for abstractive

sentence summarization. arxiv preprint arxiv:1509.00685, 2015.

dou shen, jian-tao sun, hua li, qiang yang, and zheng chen. document summarization using

conditional random    elds. in proceedings of ijcai, 2007.

richard socher, eric h. huang, jeffrey pennin, christopher d. manning, and andrew y. ng. dy-
namic pooling and unfolding recursive autoencoders for paraphrase detection. pp. 801   809. 2011.

krysta m. svore, lucy vanderwende, and christopher j.c. burges. enhancing single-document
summarization by combining ranknet and third-party sources. in proceedings of the joint con-
ference on empirical methods in natural language processing and computational natural lan-
guage learning, pp. 448   457, 2007.

xiaojun wan. towards a uni   ed approach to simultaneous single-document and multidocument

summarizations. in in proceedings of the 23rd coling, pp. 11371145, 2010.

kristian woodsend and mirella lapata. automatic generation of story highlights. in in proceedings

of the 48th acl, pp. 565574, 2010.

wenpeng yin and yulong pei. optimizing sentence modeling and selection for document sum-
marization. in proceedings of the 24th international conference on arti   cial intelligence, pp.
1383   1389. aaai press, 2015.

matthew d. zeiler. adadelta: an adaptive learning rate method. corr, abs/1212.5701, 2012.

url http://arxiv.org/abs/1212.5701.

7 appendix

in this section, we will present some additional qualitative and quantitative analysis of our models
that we hope will shed some light on their behavior.

7.1 visualization of model output

in addition to being state-of-the-art performers, our models have the additional advantage of being
very interpretable. the clearly separated terms in the scoring function (see eqn. 1) allow us to tease
out various factors responsible for the classi   cation/selection of each sentence. this is illustrated in
figure 3, where we display a representative document from our validation set along with normalized
scores from each abstract feature from the deep classi   er model. such visualization is especially
useful in explaining to the end-user the decisions made by the system.

7.2

learned importance weights

we display in table 4 the learned importance weights corresponding to various abstract features
for deep sentence selector. con   rming our intuition, the model learns that salience and redundancy
are the most important predictive features for summary membership of a sentence, followed by
positional feature and content based feature. further, when the same model is trained on documents
with randomly shuf   ed sentences, it learns very small weight for the positional features, which is
exactly what one expects.

training condition
original data
shuf   ed data

salience content
14.83
2.85

42.75
9.69

position redundancy
40.99
16.08

-31.09
0.20

table 4: learned weights of various abstract features from the deep sentence selector model. salience and
redundancy are the most important features as learned by the model, followed by position and content. the
negative sign for position weights has no particular signi   cance. the positional feature gets very low weight
when the document structure is destroyed by randomly shuf   ing sentences in each document the training data.

10

figure 3: visualization of deep classi   er output on a representative document. each row is a sentence in
the document, while the shading-color intensity in the    rst column is proportional to its id203 of being in
the summary, as estimated by the scoring function. in the columns are the normalized scores from each of the
abstract features in eqn. (1) as well as the    nal prediction id203 (last column). sentence 2 is estimated to
be the most salient, while the longest one, sentence 4, is considered the most content-rich, and not surprisingly,
the    rst sentence the most novel. the third sentence gets the best position based score.

11

goldsummary: redpathhas ended his eight-year association with salesharks. redpathspent five years as a player and three as a coach at sale.he has thanked the owners, coaches and players for their support.bryan redpathhas left his coaching role at salesharks with immediate effect. 0.10.10.90.10.3the 43 -year -old scot ends an eight-year association with the avivapremiership side, having spent five years with them as a player and three as a coach. 0.90.60.90.90.7redpathreturned to sale in june 2012 as director of rugby after starting a coaching career at gloucester and progressing to the top job at kingsholm. 0.80.50.50.90.6saliencecontentnoveltypositionprob.progressing to the top job at kingsholm. redpathspent five years with salesharks as a player and a further three as a coach but with salesharks struggling four months into redpath'stenure, he was removed from the director of rugby role at the salford-based side and has since been operating as head coach . 0.80.90.70.80.9   i would like to thank the owners, coaches, players and staff for all their help and support since i returned to the club in 2012. 0.40.10.10.70.2also to the supporters who have been great with me both as a player and as a coach,' redpathsaid. 0.60.00.20.30.27.3 ablation experiments

we evaluated the performance of the deep selector and deep classi   er models on the validation
set by deleting one abstract feature at a time from the model, with replacement. the performance
numbers, displayed in table 5, show that removing any of the features results in a small loss in
performance. note that the priority of features in the ablation experiments need not correspond to
their priority in terms of learned weights in table 4, since feature correlations may affect the two
metrics differently. for the deep classi   er, content and redundancy seem to matter the most while
for the deep selector, dropping positional features hurts the most. based on this analysis, we plan
to investigate more thoroughly the reasons behind the poor ablation performance of salience and
redundancy in the classi   er and selector models respectively.

deep classi   er

deep selector

features
all
-salience
-position
-content
-redundancy

id8-1 id8-2 id8-l id8-1 id8-2 id8-l
42.43
42.40
41.78
41.12
41.67

41.55
40.82
39.06
40.68
41.46

16.52
15.99
14.32
15.83
16.50

32.41
31.45
29.85
31.13
32.31

17.32
17.27
16.76
15.78
16.86

34.07
34.09
33.58
33.23
32.93

table 5: ablation experiments on the validation set to gauge the relative importance of each abstract feature.
the top row is where all four abstract features are present. the following rows corresponding to removal of
one feature at a time with replacement. evaluation is done using id8 limited length recall at 275 bytes. bold
faced entries correspond to largest reduction in performance when the corresponding features are dropped.

7.4 representative documents and extractive summaries

we display a couple of representative documents, one each from the daily mail and duc corpora,
highlighting the sentences chosen by deep classi   er and comparing them with the gold summaries
in table 6. the examples demonstrate qualitatively that the model performs a reasonably good job
in identifying the key messages of a document.

12

document: @entity0 have an interest in @entity3 defender
@entity2 but are unlikely to make a move until january . the
00 - year - old @entity6 captain has yet to open talks over a
new contract at @entity3 and his current deal runs out in 0000
. @entity3 defender @entity2 could be targeted by @entity0 in the
january transfer window @entity0 like @entity2 but do n   t expect
@entity3 to sell yet they know he will be free to talk to foreign clubs
from january . @entity12 will make a 0million offer for @entity3
goalkeeper @entity14 this summer .
the 00 - year - old is poised
to leave @entity16 and wants to play for a @entity18 contender .
@entity12 are set to make a 0million bid for @entity2    s @en-
tity3 team - mate @entity14 in the summer
gold summary: @entity2    s contract at @entity3 expires at the end
of next season . 00 - year - old has yet to open talks over a new deal
at @entity16 . @entity14 is poised to leave @entity3 at the end of
the season
document: today , the foreign ministry said that control opera-
tions carried out by the corvette spiro against a korean-   agged
as received ship    shing illegally in argentine waters were car-
ried out     in accordance with international law and in coordi-
nation with the foreign ministry     . the foreign ministry thus ap-
proved the intervention by the argentine corvette when it discovered
the korean ship chin yuan hsing violating argentine jurisdictional
waters on 00 may . ... the korean ship , which had been    shing
illegally in argentine waters , was sunk by its own crew after
failing to answer to the argentine ship    s warnings . the crew was
transferred to the chin chuan hsing , which was sailing nearby and
approached to rescue the crew of the sinking ship .....
gold summary: the korean-   agged    shing vessel chin yuan hs-
ing was scuttled in waters off argentina on 00 may 0000 . adverse
weather conditions prevailed when the argentine corvette spiro spot-
ted the korean ship    shing illegally in restricted argentine waters .
the korean vessel did not respond to the corvette    s warning . instead
, the korean crew sank their ship , and transferred to another korean
ship sailing nearby . in accordance with a uk-argentine agreement ,
the argentine navy turned the surveillance of the second korean ves-
sel over to the british when it approached within 00 nautical miles
of the malvinas ( falkland ) islands .

table 6: example documents and gold summaries from daily mail (top) and duc 2002 (bottom) corpora.
the sentences chosen by deep classi   er for extractive summarization are highlighted in bold.

13

