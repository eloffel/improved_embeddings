a convolutional attention network

for extreme summarization of source code

6
1
0
2

 

y
a
m
5
2

 

 
 
]

g
l
.
s
c
[
 
 

2
v
1
0
0
3
0

.

2
0
6
1
:
v
i
x
r
a

miltiadis allamanis
school of informatics, university of edinburgh, edinburgh, eh8 9ab, united kingdom

m.allamanis@ed.ac.uk

hao peng   
school of electronics engineering and computer science, peking university, beijing 100871, china

penghao.pku@gmail.com

charles sutton
school of informatics, university of edinburgh, edinburgh, eh8 9ab, united kingdom

csutton@inf.ed.ac.uk

abstract

attention mechanisms in neural networks have
proved useful for problems in which the input
and output do not have    xed dimension. often
there exist features that are locally translation in-
variant and would be valuable for directing the
model   s attention, but previous attentional archi-
tectures are not constructed to learn such features
speci   cally. we introduce an attentional neural
network that employs convolution on the input to-
kens to detect local time-invariant and long-range
topical attention features in a context-dependent
way. we apply this architecture to the problem
of extreme summarization of source code snip-
pets into short, descriptive function name-like
summaries. using those features, the model se-
quentially generates a summary by marginalizing
over two attention mechanisms: one that predicts
the next summary token based on the attention
weights of the input tokens and another that is
able to copy a code token as-is directly into the
summary. we demonstrate our convolutional at-
tention neural network   s performance on 10 pop-
ular java projects showing that it achieves bet-
ter performance compared to previous attentional
mechanisms.
1. introduction

deep learning for id170 problems, in which
a sequence (or more complex structure) of predictions need
to be made given an input sequence, presents special dif-
   culties, because not only are the input and output high-
dimensional, but the dimensionality is not    xed in ad-
vance. recent research has tackled these problems us-
ing neural models of attention (mnih et al., 2014), which

   work partially done while author was as an intern at the uni-

versity of edinburgh.

have had great recent successes in machine translation
(bahdanau et al., 2015) and image captioning (xu et al.,
2015). attentional models have been successful because
they separate two different concerns: predicting which in-
put locations are most relevant to each location of the out-
put; and actually predicting an output location given the
most relevant inputs.

in this paper, we suggest that many domains contain
translation-invariant features that can help to determine the
most useful locations for attention. for example, in a re-
search paper, the sequence of words    in this paper, we sug-
gest    often indicates that the next few words will be im-
portant to the topic of the paper. as another example, sup-
pose a neural network is trying to predict the name of a
method in the java programming language from its body.
if we know that this method name begins with get and
the method body contains a statement return ____ ; ,
then whatever token    lls in the blank is likely to be useful
for predicting the rest of the method name. previous ar-
chitectures for neural attention are not constructed to learn
translation-invariant features speci   cally.

we introduce a neural convolutional attentional model, that
includes a convolutional network within the attention mech-
anism itself. convolutional models are a natural choice
for learning translation-invariant features while using only
a small number of parameters and for this reason have
been highly successful in non-attentional models for im-
ages (lecun et al., 1998; krizhevsky et al., 2012) and text
classi   cation (blunsom et al., 2014). but to our knowledge
they have not been applied within an attentional mecha-
nism. our model uses a set of convolutional layers    
without any pooling     to detect patterns in the input and
identify    interesting    locations where attention should be
focused.

we apply this network to an    extreme    summarization prob-

a convolutional attention network for extreme summarization of source code

lem: we ask the network to predict a short and descriptive
name of a source code snippet (e.g. a method body) given
solely its tokens. source code has two distinct roles: it not
only is a means of instructing a cpu to perform a compu-
tation but also acts as an essential means of communica-
tion among developers who need to understand, maintain
and evolve software systems. for these reasons, software
engineering research has found that good names are impor-
tant to developers (liblit et al., 2006; takang et al., 1996;
binkley et al., 2013). additionally, learning to summarize
source code has important applications in software engi-
neering, such as in code understanding and in code search.
the highly structured form of source code makes convo-
lution naturally suited for the purpose of extreme summa-
rization. our choice of problem is inspired by previous
work (allamanis et al., 2015a) that tries to name existing
methods (functions) using a large set of hard-coded fea-
tures, such as features from the containing class and the
method signature. but these hard-coded features may not
be available for arbitrary code snippets and in dynamically
typed languages. in contrast, in this paper we consider a
more general problem: given an arbitrary snippet of code
    without any hard-coded features     provide a summary,
in the form of a descriptive method name.

this problem resembles a summarization task, where the
method name is viewed as the summary of the code. how-
ever, extreme source code summarization is drastically dif-
ferent from natural language summarization, because un-
like natural language, source code is unambiguous and
highly structured. furthermore, a good summary needs to
explain how the code instructions compose into a higher-
level meaning and not na  vely explain what the code does.
this necessitates learning higher-level patterns in source
code that uses both the structure of the code and the iden-
ti   ers to detect and explain complex code constructs. our
extreme summarization problem may also be viewed as a
translation task, in the same way that any summarization
problem can be viewed as translation. but a signi   cant dif-
ference from translation is that the input source code se-
quence tends to be very large (72 on average in our data)
and the output summary very small (3 on average in our
data). the length of the input sequence necessitates the ex-
traction of both temporally invariant attention features and
topical sentence-wide features and     as we show in this pa-
per     existing id4 techniques yield
sub-optimal results.

furthermore, source code presents the challenge of out-of-
vocabulary words. each new software project and each new
source    le introduces new vocabulary about aspects of the
software   s domain, data structures, and so on. this vocab-
ulary often does not appear in the training set. to address
this problem, we introduce a copy mechanism, which uses
the convolutional attentional mechanism to identify impor-

  

  

attention weight vectors
attention weight vectors

k2

lf eat attention features

k  

k  

k1

softmax

ht   1

w2

k2

relu

d

kl1

w1

kl2

emt

figure1.the architecture of the convolutional attentional net-
work. attention_features learns location-speci   c attention fea-
tures given an input sequence {mi} and a context vector ht   1.
given these features attention_weights    using a convolutional
layer and a softmax    computes the    nal attention weight vec-
tors such as    and    in this    gure.

tant tokens in the input even if they are out-of-vocabulary
tokens that do not appear in the training set. the decoder,
using a meta-attention mechanism, may choose to copy to-
kens directly from the input to the output sequence, resem-
bling the functionality of vinyals et al. (2015).

the key contributions of our paper are: (a) a novel convo-
lutional attentional network that successfully performs ex-
treme summarization of source code; (b) a comprehensive
approach to the extreme code summarization problem, with
interest both in the machine learning and software engineer-
ing community; and (c) a comprehensive evaluation of four
competing algorithms on real-world data that demonstrates
the advantage of our method compared to standard atten-
tional mechanisms.

2. convolutional attention model

our convolutional attentional model receives as input a
sequence of code subtokens1 c = [c<s>, c1, . . . cn , c</s>]
and outputs an extreme summary in the form of a con-
cise method name. the summary is a sequence of subto-
kens m = [m<s>, m1 . . . mm , m</s>], where <s> and </s>
are the special start and end symbols of every subtoken

1subtokens refer to the parts of a source code token e.g.

getinputstream has the get, input and stream subtokens.

a convolutional attention network for extreme summarization of source code

sequence. for example, in the shouldrender method
(top left of table 3) the input code subtokens are c =
[<s>, try, {, return, render, requested, . . .] while the tar-
get output is m = [<s>, should, render, </s>]. the neural
network predicts each summary subtoken sequentially and
models p (mt|m<s>, . . . , mt   1, c). information about the
previously produced subtokens m<s>, . . . , mt   1 is passed
into a recurrent neural network that represents the input
state with a vector ht   1. our convolutional attentional neu-
ral network (figure 1) uses the input state ht   1 and a se-
ries of convolutions over the embeddings of the tokens c
to compute a matrix of attention features lf eat, (figure 1)
that contains one vector of attention features for each se-
quence position. the resulting features are used to com-
pute one or more normalized attention vectors (e.g.    in
figure 1) which are distributions over input token locations
containing a weight (in r(0,1)) for each subtoken in c. fi-
nally, given the weights, a context representation is com-
puted and is used to predict the id203 distribution over
the targets mi. this model is a generative bimodal model
of summary text given a code snippet.

2.1. learning attention features

we describe our model from the bottom-up (figure 1).
first we discuss how to compute the attention features
lf eat from the input c and the previous hidden state
ht   1. the basic building block of our model is a convo-
lutional network (lecun et al., 1990; collobert & weston,
2008) for extracting position and context-dependent fea-
tures. the input to attention_features is a sequence of
code subtokens c of length len(c) and each location is
mapped to a matrix of attention features lf eat, with size
(len(c) + const)    k2 where the const is a    xed amount
of padding. the intuition behind attention_features is that
given the input c, it uses convolution to compute k2 fea-
tures for each location. by then using ht   1 as a multiplica-
tive gating-like mechanism, only the currently relevant fea-
tures are kept in l2. in the    nal stage, we normalize l2.
attention_features is described with the following pseu-
docode:

attention_features (code tokens c, context ht   1)

c     lookupandpad(c, e)
l1     relu(conv1d(c, kl1))
l2     conv1d(l1, kl2)     ht   1
lf eat     l2/ kl2k2
return lf eat

here e     r|v |  d contains the d dimensional embed-
ding of each subtoken in names and code (i.e. all possi-
ble cis and mis). the two convolution kernels are kl1    
rd  w1  k1 and kl2     rk1  w2  k2, where w1, w2 are
the window sizes of the convolutions and relu refers
to a recti   ed linear unit (nair & hinton, 2010). the vec-

tor ht   1     rk2 represents information from the previ-
ous subtokens m0 . . . mt   1. conv1d performs a one-
dimensional (throughout the length of sentence c) narrow
convolution. note that the input sequence c is padded
by lookupandpad. the size of the padding is such
that with the narrow convolutions, the attention vector (re-
turned by attention_weights) has exactly len(c) compo-
nents. the     operator is the elementwise multiplication of
a vector and a matrix, i.e. b = a     v for v     rm and
a a m    n matrix, bij = aij vi. we found the normal-
ization of l2 into lf eat to be useful during training. we
believe it helps because of the widely varying lengths of in-
puts c. note that no pooling happens in this model; the in-
put sequence c is of the same length as the output sequence
(modulo the padding).

to compute the    nal attention weight vector     a vector
with non-negative elements and unit norm     we de   ne at-
tention_weights as a function that accepts lf eat from at-
tention_features and a convolution kernel k of size k2   
w3   1. attention_weights returns the normalized attention
weights vector with length len(c) and is described by the
following pseudocode:

attention_weights (attention features lf eat, kernel k)

return softmax(conv1d(lf eat, k))

computing the state ht. predicting the full summary m
is a sequential prediction problem, where each subtoken mt
is sequentially predicted given the previous state contain-
ing information about the previous subtokens m0 . . . mt   1.
the state is passed through ht     rk2 computed by a gated
recurrent unit (cho et al., 2014) i.e.

gru(current input xt, previous state ht   1)

rt       (xtwxr + ht   1whr + br)
ut       (xtwxu + ht   1whu + bu)
ct     tanh(xtwxc + rt     (ht   1whc) + bc)
ht     (1     ut)     ht   1 + ut     ct
return ht

during testing the next state is computed by ht =
gru(emt , ht   1) i.e. using the embedding of the current
output subtoken mt. for id173 during training, we
use a trick similar to bengio et al. (2015) and with proba-
bility equal to the dropout rate we compute the next state as
ht = gru(  n, ht   1), where   n is the predicted embedding.

2.2. simple convolutional attentional model

we now use the components described above as build-
ing blocks for our extreme summarization model. we
   rst build conv_attention, a convolutional attentional
model that uses an attention vector    computed from
attention_weights to weight the embeddings of the to-
kens in c and compute the predicted target embedding
  n     rd.
it returns a distribution over all subtokens in

a convolutional attention network for extreme summarization of source code

v .

conv_attention (code c, previous state ht   1)

lf eat     attention_features(c, ht   1)
      attention_weights (lf eat, katt)
  n     pi   ieci
n     softmax(e   n    + b)
return tomap(n, v )

where b     r|v | is a bias vector and tomap returns a
map of each subtoken in vi     v associated with its prob-
ability ni. we train this model using maximum likelihood.
generating from the model works as follows: starting with
the special m0 = <s> subtoken and h0, at each timestep
t the next subtoken mt is generated using the probabil-
ity distribution n returned by conv_attention (c, ht   1).
given the new subtoken mt, we compute the next state
ht = gru(emt , ht   1). the process stops when the spe-
cial </s> subtoken is generated.

2.3. copy convolutional attentional model

we extend conv_attention by using an additional attention
vector    as a copying mechanism that can suggest out-of-
vocabulary subtokens. in our data a signi   cant proportion
of the output subtokens (about 35%) appear in c. motivated
by this, we extend conv_attention and allow a direct copy
from the input sequence c into the summary. now the net-
work when predicting mt, with id203    copies a token
from c into mt and with id203 1        predicts the tar-
get subtoken as in conv_attention. essentially,    acts as
a meta-attention. when copying, a token ci is copied into
mt with id203 equal to the attention weight   i. the
process is the following:

copy_attention (code c, previous state ht   1)

lf eat     attention_features (c, ht   1)
      attention_weights (lf eat, katt)
      attention_weights (lf eat, kcopy)
       max(  (conv1d(lf eat, k  )))
  n     pi   ieci
n     softmax(e   n    + b)
return   pos2voc(  , c) + (1       )tomap(n, v )

where    is the sigmoid function, katt, kcopy and k  
are different convolutional kernels, n     r|v |,   ,       
rlen(c), pos2voc returns a map of each subtoken in c
(which may include out-of-vocabulary tokens) to the prob-
abilities   i assigned by the copy mechanism. finally, the
predictions of the two attention mechanisms are merged, re-
turning a map that contains all potential target subtokens
in v     c and interpolating over the two attention mech-
anisms, using the meta-attention weight   . note that   
and    are analogous attention weights but are computed
from different kernels, and that n is computed exactly as in
conv_attention.

objective. to obtain signal for the copying mechanism
and   , we input to copy_attention a binary vector ic=mt
of size len(c) where each component is one if the code
subtoken is identical to the current target subtoken mt. we
can then compute the id203 of a correct copy over the
marginalization of the two mechanisms, i.e.

p (mt|ht   1, c) =    x

  iici=mt + (1       )  rmt

i

where the    rst term is the id203 of a correct copy
(weighted by   ) and the second term is the id203 of
the target token mt (weighted by 1       ). we use        (0, 1]
to penalize the model when the simple attention predicts
an unk but the subtoken can be predicted exactly by the
copy mechanism, otherwise    = 1. we arbitrarily used
   = e   10, although variations did not affect performance.

2.4. predicting names

to predict a full method name, we use a hybrid breath-
   rst search and id125. we start from the special
m0 = <s> subtoken and maintain a (max-)heap of the high-
est id203 partial predictions so far. at each step, we
pick the highest id203 prediction and predict its next
subtokens, pushing them back to the heap. when the </s>
subtoken is generated the suggestion is moved onto the list
of suggestions. since we are interested in the top k sugges-
tions, at each point, we prune partial suggestions that have
a id203 less than the current best kth full suggestion.
to make the process tractable, we limit the partial sugges-
tion heap size and stop iterating after 100 steps.

3. evaluation

dataset collection. we are interested in the extreme sum-
marization problem where we summarize a source code
snippet into a short and concise method-like name. al-
though such a dataset does not exist for arbitrary snippets of
source code, it is natural to consider existing method (func-
tion) bodies as our snippets and the method names picked
by the developers as our target extreme summaries.

to collect a good dataset of good quality, we cloned 11
open source java projects from github. we obtained the
most popular projects by taking the sum of the z-scores
of the number of watchers and forks of each project, us-
ing ghtorrent (gousios & spinellis, 2012). we selected
the top 11 projects that contained more than 10mb of
source code    les each and use libgdx as a development
set. these projects have thousands of forks and stars, being
widely known among software developers. the projects
along with short descriptions are shown in table 1. we
used this procedure to select a mature, large, and diverse
corpus of real source code. for each    le, we extract the
java methods, removing methods that are overridden, are

a convolutional attention network for extreme summarization of source code

abstract or are the constructors of a class. we    nd the
overridden methods by an approximate static analysis that
checks for inheritance relationships and the @override
annotation. overridden methods are removed, since they
are highly repetitive and their names are easy to predict.
any full tokens that are identical to the method name
(e.g. in recursion) are replaced with a special self to-
ken. we split and lowercase each method name and code
token into subtokens {mi} and {ci} on camelcase and
snake_case. the dataset and code can be found at
groups.inf.ed.ac.uk/cup/codeattention.

experimental setup. to measure the quality of our sug-
gestions we compute two scores. exact match is the per-
centage of the method names predicted exactly, while the
f1 score is computed in a per-subtoken basis. when sug-
gesting summaries, each model returns a ranked list. we
compute exact match and f1 at rank 1 and 5, as the best
score achieved by any one of the top suggestions (i.e. if the
   fth suggestion achieves the best f1 score, we use this one
for computing f1 at rank 5). using id7 (papineni et al.,
2002) would have been possible, but it would not be differ-
ent from f1 given the short lengths of our output sequences
(3 on average). we use each project separately, training one
network for each project and testing on the respective test
set. this is because each project   s domain varies widely
and little information can be transferred among them, due
to the principle of code reusability of software engineering.
we note that we attempted to train a single model using
all project training sets but this yielded signi   cantly worse
results for all algorithms. for each project, we split the
   les (top-level java classes) uniformly at random into train-
ing (65%), validation (5%) and test (30%) sets. we op-
timize hyperparameters using bayesian optimization with
spearmint (snoek et al., 2012) maximizing f1 at rank 5.

for comparison, we use two algorithms: a tf-idf algorithm
that computes a tf-idf vector from the code snippet subto-
kens and suggests the names of the nearest neighbors using
cosine similarity. we also use the standard attention model
of bahdanau et al. (2015) that uses a biid56 and fully con-
nected components, that has been successfully used in ma-
chine translation. we perform hyperparameter optimiza-
tions following the same protocol on libgdx.

training.
to train conv_attention and copy_attention
we optimize the objective using stochastic gradient
descent with rmsprop
and nesterov momentum
(sutskever et al., 2013; hinton et al., 2012). we use
dropout (srivastava et al., 2014) on all parameters, para-
metric leaky relus (maas et al., 2013; he et al., 2015)
and gradient clipping. each of the parameters of the model
is initialized with normal random noise around zero, except
for b that is initialized to the log of the empirical frequency
of each target token in the training set. for conv_attention

the optimized hyperparameters are k1 = k2 = 8, w1 = 24,
w2 = 29, w3 = 10, dropout rate 50% and d = 128.
for copy_attention the optimized hyperparameters are
k1 = 32, k2 = 16, w1 = 18, w2 = 19, w3 = 2, dropout
rate 40% and d = 128.

3.1. quantitative evaluation

table 1 shows the f1 scores achieved by the different meth-
ods for each project while table 2 shows a quantitative eval-
uation, averaged across all projects.    standard attention   
refers to the machine translation model of bahdanau et al.
(2015). the tf-idf algorithm seems to be performing very
well, showing that the bag-of-words representation of the
input code is a strong indicator of its name. interestingly,
the standard attention model performs worse than tf-idf
in this domain, while conv_attention and copy_attention
perform the best. the copy mechanism gives a good f1
improvement at rank 1 and a larger improvement at rank
5. although our convolutional attentional models have an
exact match similar to tf-idf, they achieve a much higher
precision compared to all other algorithms.

these differences in the data characteristics could be the
cause of the low performance achieved by the model of
bahdanau et al. (2015). although source code snippets re-
semble natural language sentences, they are more struc-
tured, much longer and vary greatly in length. in our train-
ing sets, each method has on average 72 tokens (median
25 tokens, standard deviation 156) and the output method
names are made up from 3 subtokens on average (   = 1.7).

oov accuracy.
we measure the out-of-vocabulary
(oov) word accuracy as the percentage of the out-
of-vocabulary subtokens that are correctly predicted by
copy_attention. on average, across our dataset, 4.4% of
the test method name subtokens are oov. naturally, the
standard attention model and tf-idf have an oov accuracy
of zero, since they are unable to predict those tokens. on
average we get a 10.5% oov accuracy at rank 1 and 19.4%
at rank 5. this shows that the copying mechanism is useful
in this domain and especially in smaller projects that tend
to have more oov tokens. we also note that oov accuracy
varies across projects, presumably due to different coding
styles.

topical vs. time-invariant feature detection. the dif-
ference of the performance between the copy_attention
and the standard attention model of bahdanau et al. (2015)
raises an interesting question. what does copy_attention
learn that cannot be learned by the standard attention
model? one hypothesis is that the biid56 of the standard
attention model fails to capture long-range features, espe-
cially in very long inputs. to test our hypothesis, we shuf   e
the subtokens in libgdx, essentially removing all features
that depend on the sequential information. without any lo-

a convolutional attention network for extreme summarization of source code

project name

git sha

description

cassandra
elasticsearch
gradle
hadoop-common
hibernate-orm
intellij-community
liferay-portal
presto
spring-framework
wild   y

distributed database
rest search engine
build system

53e370f
485915b
8263603
42a61a4 map-reduce framework
e65a883
d36c0c1
39037ca
4311896
826a00a
c324eaa

object/relational mapping
ide
portal framework
distributed sql query engine
application framework
application server

tf-idf

rank 1

rank 5

standard attention
rank 5
rank 1

f1

conv_attention
rank 1

rank 5

copy_attention
rank 1

rank 5

40.9
27.8
30.7
34.7
53.9
28.5
59.6
41.8
35.7
45.2

52.0
39.5
45.4
48.4
63.6
42.1
70.8
53.2
47.6
57.7

35.1
20.3
23.1
27.0
49.3
23.8
55.4
33.4
29.7
32.6

45.0
29.0
37.0
45.7
55.8
41.1
70.6
41.4
41.3
44.4

46.5
30.8
35.3
38.0
57.5
33.1
63.4
46.3
35.9
45.5

60.0
45.0
52.5
54.0
67.3
49.6
75.5
59.0
49.7
61.0

48.1
31.7
36.3
38.4
58.7
33.8
65.9
46.7
36.8
44.7

63.1
47.2
54.0
55.8
69.3
51.5
78.0
60.2
51.9
61.7

table1.open source java projects used and f1 scores achieved. standard attention refers to the model of bahdanau et al. (2015).

tf-idf
standard attention
conv_attention
copy_attention

f1 (%)

rank 1 rank 5
52.1
45.2
57.7
59.6

40.0
33.6
43.6
44.7

exact match (%)
rank 1 rank 5
29.3
24.9
29.8
33.7

24.3
17.4
20.6
23.5

precision (%)

rank 1 rank 5
55.2
47.1
73.7
74.9

41.6
35.2
57.4
58.9

recall (%)

rank 1 rank 5
51.9
42.1
51.9
54.2

41.8
35.1
39.4
40.1

table2. id74 averaged across projects. standard attention refers to the work of bahdanau et al. (2015).

cal features all models should reduce to achieving perfor-
mance similar to tf-idf. indeed, copy_attention now has
an f1 at rank 1 that is +1% compared to tf-idf (presumably
thanks to the language model-like structure of the output),
while the standard attention model worsens its performance
getting an f1 score (rank 1) of 26.2%, compared to the orig-
inal 41.8%. this suggests that the biid56 fails to capture
long-range topical attention features.

a simpler ht   1.
since the target summaries are quite
short, we tested a simpler alternative to the gru, assigning
ht   1 = w    [gmt   1 , gmt   2 ], where g     rd  |v | is a
new embedding matrix (different from the embeddings in
e) and w is a k2    d    2 tensor. this model is simpler and
slightly faster to train and achieves similar performance to
copy_attention, reducing f1 by less than 1%.

3.2. qualitative evaluation

figure 2 shows a visualization of a small method that il-
lustrates how copy_attention typically works. at the    rst
step, it focuses its attention at the whole method and de-
cides upon the    rst subtoken. in a large number of cases
this includes subtokens such as get, set, is, create
etc. in the next steps the meta-attention mechanism is
highly con   dent about the copying mechanism (   = 0.97
in figure 2) and sequentially copies the correct subtokens
from the code snippet into the name. we note that across
many examples the copying mechanism tends to have a sig-
ni   cantly more focused attention vector   , compared to the
attention vector   . presumably, this happens because of the
different training signals of the attention mechanisms.

a second example of copy_attention is seen in figure 3.
although due to space limitations this is a relatively short
method, it illustrates how the model has learned both time-
invariant features and topical features. it correctly detects
the == operator and predicts that the method has a high
id203 of starting with is. furthermore, in the next
step (prediction of the m2 bullets subtoken) it success-
fully learns to ignore the e pre   x (prepended on all enu-
meration variables in that project) and the flag subtoken
that does not provide useful information for the summary.

table 3 presents a set of hand-picked examples from
libgdx that show interesting challenges of the domain
and how our copy_attention handles them. understand-
ably, the model does not distinguish between should and
is     both implying a boolean return value     and in-
stead of shouldrender, isrender is suggested. the
getaspectratio, surfacearea and minrunlength
examples show the challenges of describing a previ-
ously unseen abstraction.
interestingly, the model cor-
rectly recognizes that a novel (unk) token should be pre-
dicted after get in getaspectratio. most surprisingly,
reverserange is predicted correctly, because of the struc-
ture of the code, even though no code tokens contain the
summary subtokens.

4. related work

convolutional neural networks have been used for image
classi   cation with great success (krizhevsky et al., 2012;
szegedy et al., 2015; lecun et al., 1990; 1998). more
related to this work is the use of convolutional neural

a convolutional attention network for extreme summarization of source code

boolean shouldrender()

void reverserange(object[] a, int lo, int hi)

try {

return renderrequested||iscontinuous;

} finally {

renderrequested = false;

}

hi--;
while (lo < hi) {

object t = a[lo];
a[lo++] = a[hi];
a[hi--] = t;

}

suggestions:

   is,continuous (10.6%)
   render,continuous (6.9%)    get,render (5.7%)

(27.3%)
   is,render
   is,requested (8.2%)

suggestions:    reverse,range (22.2%)    reverse (13.0%)
(3.2%)

   reverse,hi

(4.1%)

   reverse,lo
   merge,range (2.0%)

int createprogram()

verticalgroup right()

gl20 gl = gdx.gl20;
int program = gl.glcreateprogram();
return program != 0 ? program : -1;

align |= align.right;
align &= ~align.left;
return this;

suggestions:

   init (7.9%)
   render (5.0%)    initiate (5.0%)    load (3.4%)

   create (18.36%)

suggestions:    left (21.8%)    top (21.1%)    right (19.5%)

   bottom (18.5%)    align (3.7%)

boolean isbullet()

float getaspectratio()

return (m_flags & e_bulletflag)

== e_bulletflag;

return (height == 0) ?

float.nan : width / height;

suggestions:

   is,bullet (5.5%)
   is,enable (5.1%)    enable (2.8%)    mouse (2.7%)

   is (13.5%)

suggestions:

   get,unk (9.0%)

   get,height (8.7%)

   get,width (6.5%)    get (5.7%)    get,size (4.2%)

int minrunlength(int n)

jsonwriter pop()

if (debug) assert n >= 0;
int r = 0;
while (n >= min_merge) {

r |= (n & 1);
n >>= 1;

}
return n + r;

if (named) throw

new illegalstateexception(unkstring);

stack.pop().close();
current = stack.size == 0 ?

null : stack.peek();

return this;

suggestions:

   min (43.7%)

   merge (13.0%)

suggestions:    close (21.4%)    pop (10.2%)    first (6.5%)

   pref (1.9%)    space (1.0%)    min,all (0.8%)

   state (3.8%)    remove (2.2%)

rectangle setposition(float x, float y)

float surfacearea()

this.x = x;
this.y = y;
return this;

suggestions:

   set (54.0%)

   set,x
   set,bounds (1.68%)

(9.0%)

   set,y (12.8%)
(8.6%)

   set,position

return 4 * mathutils.pi *

this.radius * this.radius;

suggestions:

   dot (13.1%)
   crs,radius (9.0%)    dot,circle (6.5%)    crs (4.1%)

   dot,radius (26.5%)

table3.a sample of handpicked snippets (the sample is necessarily limited to short methods because of space limitations) and the
respective suggestions that illustrate some interesting challenges of the domain and how the copy_attention model handles them or
fails. note that the algorithms do not have access to the signature of the method but only to the body. examples taken from the libgdx
android/java graphics library test set.

a convolutional attention network for extreme summarization of source code

target

m1

set

m2

use

attention vectors

   = <s> { this . use browser cache = use browser cache ; } </s>
   = <s> { this . use browser cache = use browser cache ; } </s>

   = <s> { this . use browser cache = use browser cache ; } </s>
   = <s> { this . use browser cache = use browser cache ; } </s>

m3

browser

   = <s> { this . use browser cache = use browser cache ; } </s>
   = <s> { this . use browser cache = use browser cache ; } </s>

m4

cache

m5

end

   = <s> { this . use browser cache = use browser cache ; } </s>
   = <s> { this . use browser cache = use browser cache ; } </s>

   = <s> { this . use browser cache = use browser cache ; } </s>
   = <s> { this . use browser cache = use browser cache ; } </s>

  

0.012

0.974

0.969

0.583

0.066

figure2.visualization of copy_attention used to compute p (mt|m0 . . . mt   1, c) for setusebrowsercache in libgdx. the darker the
color of a subtoken, they higher its attention weight. this relationship is linear. yellow indicates the convolutional attention weight of the
conv_attention component, while purple the attention of the copy mechanism. since the values of    are usually spread across the tokens
the colors show a normalized   , i.e.   / k  k   . in contrast, the copy attention weights    are usually very peaky and we plot them as-is.
underlined subtokens are out-of-vocabulary.    shows the meta-attention id203 of using the copy attention    vs. the convolutional
attention   . more visualizations of libgdx methods can be found at http://groups.inf.ed.ac.uk/cup/codeattention/.

target

m1

is

attention vectors

   = <s> { return ( m flags & e bullet flag ) == e bullet flag ; } </s>
   = <s> { return ( m flags & e bullet flag ) == e bullet flag ; } </s>

m2

bullet

   = <s> { return ( m flags & e bullet flag ) == e bullet flag ; } </s>
   = <s> { return ( m flags & e bullet flag ) == e bullet flag ; } </s>

m3

end

   = <s> { return ( m flags & e bullet flag ) == e bullet flag ; } </s>
   = <s> { return ( m flags & e bullet flag ) == e bullet flag ; } </s>

  

0.012

0.436

0.174

figure3.visualization of copy_attention modeling p (mt|m0 . . . mt   1, c) for isbullet in libgdx. the copy_attention captures
location-invariant features and the topicality of the input code sequence. for information about the visualization see figure 2.

networks for text classi   cation (blunsom et al., 2014).
closely related is the work of denil et al. (2014) that
learns representations of documents using convolution but
uses the network activations to summarize a document
rather than an attentional model. rush et al. (2015) use
an attention-based encoder to summarize sentences, but
do not use convolution for their attention mechanism.
our work is also related to other work in attention
mechanisms for text (hermann et al., 2015) and images
(xu et al., 2015; mnih et al., 2014) that does not use
convolution to provide the attention values.
pointer
networks (vinyals et al., 2015) are similar to our copy
mechanism but use an id56 for providing attention.
finally, distantly related to this work is research on neural
architectures that learn code-like behaviors (graves et al.,
joulin & mikolov,
2014; zaremba & sutskever, 2014;
2015; grefenstette et al.,
2015;
2015; dyer et al.,
reed & de freitas, 2015; neelakantan et al., 2015).

in recent years, thanks to the insight of hindle et al. (2012)

the use of probabilistic models for software engineer-
ing applications has grown. research has mostly fo-
cused on token-level (nguyen et al., 2013; tu et al., 2014)
and syntax-level (maddison & tarlow, 2014) language
models of code and translation between programming
languages (karaivanov et al., 2014; nguyen et al., 2014).
movshovitz-attias et al. (2013) learns to predict code com-
ments using a source code topic model. allamanis et al.
(2015b) create a generative model of source code given a
natural language query and oda et al. (2015) use machine
translation to convert source code into pseudocode. closer
to our work, raychev et al. (2015) aim to predict names
and types of variables, whereas allamanis et al. (2014) and
allamanis et al. (2015a) suggest names for variables, meth-
ods and classes. similar to allamanis et al. (2015a), we
predict method names but using only the tokens within
a method and no other features (e.g. method signature).
mou et al. (2016) use syntax-level convolutional neural net-
works to learn vector representations for code and classify

a convolutional attention network for extreme summarization of source code

student submissions into tasks without considering nam-
ing. piech et al. (2015) also learn program embeddings
from student submissions using the program state, to assist
mooc students debug their submissions but do not con-
sider naming. additionally, compared to piech et al. (2015)
and mou et al. (2016) our work looks into highly diverse,
non-student submission code that performs a wide range of
real-world tasks.

5. discussion & conclusions

modeling and understanding source code artifacts through
machine learning can have a direct impact in software engi-
neering research. the problem of extreme code summariza-
tion is a    rst step towards the more general goal of devel-
oping machine learning representations of source code that
will allow machine learning methods to reason probabilis-
tically about code resulting in useful software engineering
tools that will help code construction and maintenance.

additionally, source code     and its derivative artifacts    
represent a new modality for machine learning with very
different characteristics compared to images and natural
language. therefore, models of source code necessitate re-
search into new methods that could have interesting par-
allels to images and natural language. this work is a
step towards this direction: our neural convolutional atten-
tional model attempts to    understand    the highly-structured
source code text by learning both long-range features and
localized patterns, achieving the best performance among
other competing methods on real-world source code.

acknowledgements

this work was supported by microsoft research through
its phd scholarship programme and the engineering
and physical sciences research council [grant number
ep/k024043/1]. we would like to thank daniel tarlow and
krzysztof geras for their insightful comments and sugges-
tions.

references
allamanis, miltiadis, barr, earl t, bird, christian, and sut-
ton, charles. learning natural coding conventions. in
symposium on the foundations of software engineering
(fse), 2014.

allamanis, miltiadis, barr, earl t, bird, christian, and
sutton, charles. suggesting accurate method and class
names. in proceedings of the 2015 10th joint meeting
on foundations of software engineering. acm, 2015a.

allamanis, miltiadis, tarlow, daniel, gordon, andrew, and

wei, yi. bimodal modelling of source code and natural
language. in icml, 2015b.

bahdanau, dzmitry, cho, kyunghyun, and bengio, yoshua.
id4 by jointly learning to align
and translate. in iclr, 2015.

bengio, samy, vinyals, oriol, jaitly, navdeep, and
shazeer, noam. scheduled sampling for sequence pre-
diction with recurrent neural networks. arxiv preprint
arxiv:1506.03099, 2015.

binkley, dave, davis, marcia, lawrie, dawn, maletic,
jonathan i, morrell, christopher, and sharif, bonita. the
impact of identi   er style on effort and comprehension.
empirical software engineering, 2013.

blunsom, phil, grefenstette, edward, kalchbrenner, nal,
et al. a convolutional neural network for modelling sen-
tences. in proceedings of the 52nd annual meeting of
the association for computational linguistics, 2014.

cho, kyunghyun, van merri  nboer, bart, bahdanau,
dzmitry, and bengio, yoshua. on the properties of neu-
ral machine translation: encoder   decoder approaches.
syntax, semantics and structure in statistical transla-
tion, pp. 103, 2014.

collobert, ronan and weston, jason. a uni   ed architecture
for natural language processing: deep neural networks
with multitask learning. in icml, 2008.

denil, misha, demiraj, alban, kalchbrenner, nal, blun-
som, phil, and de freitas, nando. modelling, visualising
and summarising documents with a single convolutional
neural network. arxiv preprint arxiv:1406.3830, 2014.

dyer, chris, ballesteros, miguel, ling, wang, matthews,
austin, and smith, noah a. transition-based depen-
dency parsing with stack long short-term memory.
in
acl, 2015.

gousios, georgios and spinellis, diomidis. ghtorrent:
github   s data from a    rehose. in mining software repos-
itories (msr), 2012 9th ieee working conference on,
2012.

graves, alex, wayne, greg, and danihelka, ivo. neural
turing machines. arxiv preprint arxiv:1410.5401, 2014.

grefenstette, edward, hermann, karl moritz, suleyman,
mustafa, and blunsom, phil. learning to transduce with
unbounded memory. in nips, 2015.

he, kaiming, zhang, xiangyu, ren, shaoqing, and sun,
jian. delving deep into recti   ers: surpassing human-
level performance on id163 classi   cation.
arxiv
preprint arxiv:1502.01852, 2015.

a convolutional attention network for extreme summarization of source code

hermann, karl moritz, kocisky, tomas, grefenstette, ed-
ward, espeholt, lasse, kay, will, suleyman, mustafa,
and blunsom, phil. teaching machines to read and com-
prehend. in nips, 2015.

hindle, abram, barr, earl t, su, zhendong, gabel, mark,
and devanbu, premkumar. on the naturalness of soft-
ware. in international conference on software engineer-
ing (icse), 2012.

hinton, geoffrey, srivastava, nitsh, and swersky, kevin.
neural networks for machine learning. online course at
coursera. org, lecture, 6, 2012.

joulin, armand and mikolov, tomas.

inferring algorith-
mic patterns with stack-augmented recurrent nets. arxiv
preprint arxiv:1503.01007, 2015.

karaivanov, svetoslav, raychev, veselin, and vechev, mar-
tin. phrase-based statistical translation of programming
languages.
in proceedings of the 2014 acm interna-
tional symposium on new ideas, new paradigms, and
re   ections on programming & software. acm, 2014.

krizhevsky, alex, sutskever, ilya, and hinton, geoffrey e.
id163 classi   cation with deep convolutional neural
networks. in nips, 2012.

lecun, y., boser, b., denker, j. s., howard, r. e., hab-
bard, w., jackel, l. d., and henderson, d. nips. chapter
handwritten digit recognition with a back-propagation
network. 1990.

lecun, yann, bottou, leon, bengio, yoshua, and haffner,
patrick. gradient-based learning applied to document
recognition. proceedings of the ieee, 1998.

liblit, ben, begel, andrew, and sweetser, eve. cognitive
perspectives on the role of naming in computer programs.
in proceedings of the 18th annual psychology of pro-
gramming workshop, 2006.

maas, andrew l., hannun, awni y., and ng, andrew y.
recti   ed linear units improve restricted boltzmann ma-
chines. in icml workshop on deep learning for audio,
speech, and language processing (wdlasl), 2013.

movshovitz-attias, dana, cohen, ww, and w. cohen,
william. natural language models for predicting pro-
gramming comments. in acl, 2013.

nair, vinod and hinton, geoffrey e. recti   ed linear units
improve restricted id82s. in icml, 2010.

neelakantan, arvind, le, quoc v, and sutskever, ilya. neu-
ral programmer: inducing latent programs with gradient
descent. arxiv preprint arxiv:1511.04834, 2015.

nguyen, anh tuan, nguyen, tung thanh, and nguyen,
tien n. migrating code with statistical machine trans-
lation.
in companion proceedings of the 36th inter-
national conference on software engineering. acm,
2014.

nguyen, tung thanh, nguyen, anh tuan, nguyen,
hoan anh, and nguyen, tien n. a statistical seman-
tic language model for source code. in joint meeting of
the european software engineering conference and the
acm sigsoft symposium on the foundations of soft-
ware engineering (esec/fse), 2013.

oda, yusuke, fudaba, hiroyuki, neubig, graham, hata,
hideaki, sakti, sakriani, toda, tomoki, and nakamura,
satoshi. learning to generate pseudo-code from source
code using id151. in automated
software engineering (ase), 2015 30th ieee/acm in-
ternational conference on. ieee, 2015.

papineni, kishore, roukos, salim, ward, todd, and zhu,
wei-jing. id7: a method for automatic evaluation of
machine translation. in proceedings of the 40th annual
meeting of the association for computational linguis-
tics, 2002.

piech, chris, huang, jonathan, nguyen, andy, phulsuk-
sombati, mike, sahami, mehran, and guibas, leonidas j.
learning program embeddings to propagate feedback on
student code. in icml, 2015.

raychev, veselin, vechev, martin, and krause, andreas.
predicting program properties from    big code   . in pro-
ceedings of the 42nd annual acm sigplan-sigact
symposium on principles of programming languages.
acm, 2015.

maddison, chris and tarlow, daniel. structured generative

models of natural source code. in icml, 2014.

reed, scott and de freitas, nando. neural programmer-

interpreters. arxiv preprint arxiv:1511.06279, 2015.

mnih, volodymyr, heess, nicolas, graves, alex, et al. re-

current models of visual attention. in nips, 2014.

mou, lili, li, ge, zhang, lu, wang, tao, and jin, zhi. con-
volutional neural networks over tree structures for pro-
gramming language processing. in proceedings of the
30th aaai conference on arti   cial intelligence, 2016.

rush, alexander m, chopra, sumit, and weston, jason. a
neural attention model for abstractive sentence summa-
rization. in emnlp, 2015.

snoek, jasper, larochelle, hugo, and adams, ryan p. prac-
tical bayesian optimization of machine learning algo-
rithms. in nips, 2012.

a convolutional attention network for extreme summarization of source code

srivastava, nitish, hinton, geoffrey e., krizhevsky, alex,
sutskever, ilya, and salakhutdinov, ruslan. dropout: a
simple way to prevent neural networks from over   tting.
journal of machine learning research, 2014.

sutskever, ilya, martens, james, dahl, george, and hin-
ton, geoffrey. on the importance of initialization and
momentum in deep learning. in icml, 2013.

szegedy, christian, liu, wei, jia, yangqing, sermanet,
pierre, reed, scott, anguelov, dragomir, erhan, du-
mitru, vanhoucke, vincent, and rabinovich, andrew.
going deeper with convolutions.
in proceedings of
the ieee conference on id161 and pattern
recognition, 2015.

takang, armstrong a, grubb, penny a, and macredie,
robert d. the effects of comments and identi   er names
on program comprehensibility: an experimental investi-
gation. j. prog. lang., 1996.

tu, zhaopeng, su, zhendong, and devanbu, premkumar.
on the localness of software.
in proceedings of the
22nd acm sigsoft international symposium on foun-
dations of software engineering. acm, 2014.

vinyals, oriol, fortunato, meire, and jaitly, navdeep.

id193. in nips, 2015.

xu, kelvin, ba, jimmy, kiros, ryan, cho, kyunghyun,
courville, aaron c, salakhutdinov, ruslan, zemel,
richard s, and bengio, yoshua. show, attend and tell:
neural image id134 with visual attention. in
icml, 2015.

zaremba, wojciech and sutskever, ilya. learning to exe-

cute. arxiv preprint arxiv:1410.4615, 2014.

