distraction-based neural networks for document summarization

qian chen   , xiaodan zhu      , zhenhua ling   , si wei   , hui jiang   

   university of science and technology of china, hefei, china

  national research council canada, ottawa, canada

   iflytek research, hefei, china
   york university, toronto, canada

6
1
0
2

 
t
c
o
6
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
2
6
4
8
0

.

0
1
6
1
:
v
i
x
r
a

cq1231@mail.ustc.edu.cn, zhu2048@gmail.com,

zhling@ustc.edu.cn, siwei@iflytek.com, hj@cse.yorku.ca

abstract

distributed representation learned with neural net-
works has recently shown to be effective in mod-
eling natural languages at    ne granularities such
as words, phrases, and even sentences. whether
and how such an approach can be extended to help
model larger spans of text, e.g., documents, is in-
triguing, and further investigation would still be de-
sirable. this paper aims to enhance neural network
models for such a purpose. a typical problem of
document-level modeling is automatic summariza-
tion, which aims to model documents in order to
generate summaries. in this paper, we propose neu-
ral models to train computers not just to pay atten-
tion to speci   c regions and content of input docu-
ments with id12, but also distract them
to traverse between different content of a document
so as to better grasp the overall meaning for sum-
marization. without engineering any features, we
train the models on two large datasets. the models
achieve the state-of-the-art performance, and they
signi   cantly bene   t from the distraction modeling,
particularly when input documents are long.

1 introduction
modeling the meaning of text lies in center of natural lan-
guage understanding. distributed representation learned with
neural networks has recently shown to be effective in mod-
eling    ne granularities of text, including words [collobert
et al., 2011; mikolov et al., 2013; chen et al., 2015],
phrases [yin and sch  utze, 2014; zhu et al., 2014], and ar-
guably sentences [socher et al., 2012; irsoy and cardie, ;
kalchbrenner et al., 2014; tai et al., 2015; zhu et al., 2015b;
zhu et al., 2015a; chen et al., 2016; zhu et al., 2016].

whether and how such an approach can be extended to help
model larger spans of text, e.g., documents, is intriguing, and
further investigation would still be desirable, although there
has been interesting research conducted recently along this
line [li et al., 2015; hu et al., 2015; wang and cho, 2015;

published in ijcai-2016: the 25th international joint confer-

ence on arti   cial intelligence.

hermann et al., 2015]. a typical problem of document-
level modeling is id54 [mani, 2001;
das and martins, 2007; nenkova and mckeown, 2013], in
which computers generate summaries for documents, based
on their shallow or deep understanding of the documents.

if one regards the process of representing input documents,
generating summaries, and the interaction between them to
be a (complicated) function,    tting such a function could
expect to have a large-scale annotated dataset to estimate
a large set of parameters, while on the other hand, hard-
coding summarization knowledge (in different forms) or lim-
iting the number of model parameters (e.g., as in many ex-
tractive summarization models) are often adopted when there
are no enough training data. this work explores the former
direction and utilizes relatively large datasets [hu et al., 2015;
hermann et al., 2015] to train neural summarization models.
in general, neural networks, as universal approximators, can
   t very complicated functions and have shown to be very ef-
fective on many problems recently.

understanding the input documents and generating sum-
maries are both challenging. on the understanding side,
much recent work seems to have suggested that distributed
representations (often vectors) by themselves may not be
adequate for representing sentences, let along with longer
documents. additional modeling such as soft or hard at-
tention has been applied to retrospect subsequences or even
words in input text to remedy the limits, which has shown
to improve performances of different tasks such as those
discussed in [bahdanau et al., 2014; luong et al., 2015;
rush et al., 2015] among others. we regard this to be a
mechanism that provides a connection between input docu-
ment modeling (encoding) and summary generating (decod-
ing), which could model a level of cognitive controls   human
summarizers themselves often move between the input docu-
ments and summaries when they summarize a document.

we consider this control layer to be important, and in this
paper we focus on better designing this control layer for sum-
marization. we propose neural models to train computers not
just to pay attention to speci   c regions and content of input
documents with id12, but also distract them to
traverse between different content of a document so as to bet-
ter grasp the overall meaning for summarization.

without engineering any features, we train the models with
two large datasets. the models achieve the state-of-the-art

performance and they signi   cantly bene   t from the distrac-
tion modeling, particularly when input documents are long.
we also explore several technologies that have been applied
to sentence-level tasks and extend them to document sum-
marization, and we present in this paper the technologies
that showed to help improve the summarization performance.
even when it is applied onto the models that have leveraged
these technologies, the distraction models can further im-
prove the performance signi   cantly. in general, our models
here aim to perform abstractive summarization.

2 related work
distributed representation distributed representation has
shown to be effective in modeling    ne granularities of text
as discussed above. much recent work has also attempted to
model longer spans of text with neural networks [li et al.,
2015; hu et al., 2015; lin et al., 2015; wang and cho, 2015;
hermann et al., 2015]. this includes research that in-
corporates document-level information for language model-
ing [wang and cho, 2015; lin et al., 2015] and that answers
questions [hermann et al., 2015] by comprehending input
documents with attention-based models. more relevant to
[li et al., 2015] learned distributed rep-
ours, the work of
resentation for short documents with the averaged length of
about a hundred word tokens, although the objective is not
summarization. summarization typically faces documents
longer than those, and summarization may be more neces-
sary when documents are long.
in this paper, we propose
neural models for summarizing typical news articles with up
to thousands of word tokens. we    nd it is necessary to enable
computers not just to pay attention to speci   c content of in-
put documents with id12, but also distract them
to traverse between different content so as to better grasp the
overall meaning for summarization, particularly when docu-
ments are long.
neural summarization models id54
[mani, 2001;
has been intensively studied for both text
das and martins, 2007; nenkova and mckeown, 2013] and
speech [zhu and penn, 2006; zhu et al., 2010]. most state-
of-the-art summarization models have focused on extractive
summarization, although some efforts have also been exerted
on abstractive summarization. recent neural summariza-
tion models include the recent efforts of [rush et al., 2015;
lopyrev, 2015; hu et al., 2015]. the research performed in
[rush et al., 2015] focuses on neural models for sentence
compression and rewriting, but not full document summariza-
tion. the work of [lopyrev, 2015] leverages neural networks
to generate news headline, where input documents are lim-
ited to 50 word tokens, and the work of [hu et al., 2015] also
deals with short texts (up to dozens of word tokens), in which
summarization problems such as content redundancy is less
prominent and attention-based models seem to be suf   cient.
however, summarization typically faces documents longer
than that and summarization is often more needed when doc-
uments are long.
in this work we attempt to explore neu-
ral summarization technologies for news articles with up to
thousands of word tokens, in which we    nd distraction-based
summarization models help improve performance. note that

our improvement is achieved over the model that has already
outperformed the attention-based model reported in [hu et
al., 2015] on short documents.

3 our approach
3.1 overview
we base our model on the general encoder-decoder frame-
work [sutskever et al., 2011; sutskever et al., 2014; cho
et al., 2014] that has shown to be effective recently on dif-
ferent tasks. this is a general sequence-to-sequence model-
ing framework in which the encoding part can be devoted to
model the input documents and the decoder to generate out-
put.

we believe the control layer that helps navigate the input
documents to optimize the generation objectives would be of
importance, and we will focus on the control layer in this
paper and enrich its expressiveness. speci   cally for summa-
rization, unlike much recent work that focuses more on atten-
tion in order to grasp local context or correspondence (e.g, in
machine translation and sentence compression), we force our
models to traverse between different content of a document
to avoid focusing on a region or same content, to better grasp
the overall meaning for the summarization objective.

we also explore several popular technologies that have
been applied to sentence-level tasks and extend them to docu-
ment summarization, and we present those that help improve
the summarization performance.

3.2 gru-based encoding and decoding
encoding the general document modeling and summariz-
ing framework takes in an input document x = x1,       , xtx
and write the summary of the document as the output y =
y1,       , yty. the summarization process is modeled as    nd-
ing the output text y    that maximizes the conditional proba-
bility arg maxy p(y|x), given gold summary sequences. as
discussed above, such a model has been found to be very ef-
fective in modeling sentences or sub-sentential text spans. we
will address the challenges faced at the document level.

on encoding we do not restrict the encoders    architectures
as if it is a recurrent neural network (id56). the recent litera-
ture shows long-short term memory (lstm) [hochreiter and
schmidhuber, 1997; sutskever et al., 2014] and gated recur-
rent units (gru) [bahdanau et al., 2014] are both good ar-
chitectures. in developing our systems we empirically found
gru achieved similar performance as lstm but it is fast to
train; we will therefore describe the gru implement of our
neural summarization models.

in the simplest uni-directional setting, when reading input
symbols from left to right, a gru learns the hidden annota-
tions hi at time i with

hi = gru(hi   1, e(xi))

(1)
where the hi     rn encodes all content seen so far at time i
which is computed from hi   1 and e(xi), where e(xi)     rm
is the m-dimensional embedding of the current word xi. the

forward propagation of gru is computed as follows.

hi = (1     ui) (cid:12) hi   1 + ui (cid:12)   hi
  hi = tanh(w e(xi) + u (ri (cid:12) hi   1))
ri = sigmoid(wre(xi) + urhi   1)
ui = sigmoid(wue(xi) + uuhi   1)

(2)
(3)
(4)
(5)
where wu, wr, w     rn  m and uu, ur, u     rn  n are
weight matrices, n is the number of hidden units, and (cid:12) is
element-wise multiplication.

in our work, we actually applied bi-directional grus (bi-
grus), which we found achieving better results than single
directional grus consistently. as its name suggests, in a
bi-gru unit, the annotation vector ht encodes the sequence
from two directions, modeling both the left and right context.
the bottom part of figure 1 shows the encoder intuitively,
while for more details, readers can refer to [bahdanau et al.,
2014] for further discussion.

figure 1: a high-level view of the summarization model.

generation when generating summaries, the decoder pre-
dicts the next word yt given all annotations obtained in en-
coding h = h1,       , htx, as well as all previously predicted
words y1,       , yt   1. the objective is a id203 over the
summary y with decomposition into the ordered condition-
als:

y    = argmax

y

= argmax

p(yt|y1,       , yt   1, h)

g(yt   1, st, ct)|yt

(6)

(7)

y

t=1

where equation (6) depicts a high level abstraction of gen-
erating a summary word yt over previous output as well as
input annotations h = h1,       , htx, and yt is a legal output
word at time t, while y    is the optimal summary found by the
model.

the id155 is further rewritten as equa-
tion (7) to factorize the model according to the structures of
neural networks. the function g(yt   1, st, ct) is a nonlinear

ty(cid:89)
ty(cid:89)

t=1

function that computes the id203 vector for all legal out-
put words at output time t, and g(yt   1, st, ct)|yt takes the el-
ement of the resulting vector corresponding to word yt, i.e.,
the predicated id203 for word yt.

the vector st and ct are the control layers that connect out-
put y and input h, which we will discuss in details in sec-
tion 3.3. for completeness, function g(.) is computed with:

(8)
g(yt   1, st, ct) =   (wo tanh(voe(yt   1) + uost + coct))
where    is a softmax function; wo     rk  n, uo     rn  n,
vo     rn  m and co     rn  2n are weight matrices; k is the
vocabulary size; e(yt   1)     rm is the m-dimensional embed-
ding of the previously predicted word yt   1.
3.3 the control layers
the document modeling and summary generation are de-
scribed above as two components:
input document encod-
ing and summary generation. a core problem is how these
two components are associated. in sentence-level modeling
such as machine translation and id103, attention
model is often applied to grasp local context and correspon-
dence between input and output texts. for example, in trans-
lation attention is shown to be very useful for aligning the
words of the target language (the language being translated
to) to the corresponding source words and their context.

attention can be regarded as a type of cognitive controls.
in modeling documents, we take a general viewpoint on this
control layer and propose distraction modeling to enable the
model to traverse over different content of a long document,
and we will show it improves the summarization performance
signi   cantly. in general, the control layer allows a compli-
cated examination over the input. in this section we describe
the controls that consider both attention and distraction to
navigate input documents and to generate summaries.
two-layer hidden output we    rst extended the recent two-
level hidden output model [luong et al., 2015] to our sum-
marization models. as presented later in the experiments,
the two-level hidden output model consistently improves
the summarization performance on different datasets. more
speci   cally, the updating of st follows a two-layer gru ar-
chitecture shown in the top part of figure 1.

t, ct)

st = gru1(s(cid:48)
s(cid:48)
t = gru2(st   1, e(yt   1))

(9)
(10)
the forward propagation of gru1 and gru2 are computed
similar to equation (1) above. gru1 and gru2 use untied
parameter matrices. the two-layer model allows for captur-
ing a direct interaction between s(cid:48)
t and ct, with the former
encoding the current and previous output information and the
latter encoding the current input content that is primed with
distraction and attention. we will discuss how these vectors
are computed below.
distraction in training we propose to enforce distraction
from two perspectives: adding the distraction constraints in
training as well as in decoding. we    rst discuss the distrac-
tion in training.
distraction over input content vectors in training we force
the model not to pay attention to the same content or same

  t,ihi

(12)

t=1

(cid:80)t   1
part of the input documents too much. we accumulate the
previously viewed content vector as a history content vector
j=1 cj and incorporate it into the currently computed c(cid:48)
t.

we refer to this model as m1.

ct = tanh(wcc(cid:48)

t     uc

cj)

(11)

t   1(cid:88)

j=1

where wc     r2n  2n and ua     r2n  2n are diagonal matri-
ces. and c(cid:48)
t is input content vectors that have not been di-
rectly penalized with history yet; c(cid:48)
t is directly computed with
conventional equation as follows:

tx(cid:88)

i=1

c(cid:48)
t =

where hi are annotation vectors that encode the current input
word and its context with the input gru described above in
equation (1). and   t,i is the attention weight put on hi at the
current output time t. the distraction-based ct computed in
equation (11) can then be incorporated in equation (8).
distraction over attention weight vectors we also propose to
add distraction directly on the attention weight vectors. sim-
ilarly as above, we accumulate the past attention weights as
j=1   j,i and use it to prime the
current attention weights. the model in [tu et al., 2016]
also uses history attention weights, but we use history here to
force distraction in order to avoid redundancy, which is not
a concern in the machine translation task. we refer to the
model as m2.

a history attention weight(cid:80)t   1

  (cid:48)
t,i = vt

a tanh(was(cid:48)

t + uahi     ba

  j,i)

(13)

t   1(cid:88)

j=1

where wa     rl  n, ua     rl  2n, ba     rl, and va     rl
are the weight matrices, and l is the number of hidden units.
note that was(cid:48)
t + uahi in the equation computes the con-
ventional attention without penalizing attention history.   (cid:48) is
often normalized with a softmax to generate attention weights
  t,i below, which is in turn used in equation (12).

(cid:80)tx

exp(  (cid:48)
t,i)
j=1 exp(  (cid:48)

t,j)

  t,i =

(14)

distraction in decoding in the decoding process, we also
enforced different types of distraction, one by computing
the difference between the distribution of the current atten-
tion weight   t and that of all previous attention weights
  1,       ,   t   1. since    can be seen as a proper probabilistic
distribution, normalized in equation (14), we used kullback-
leibler (kl) divergence to measure their difference with
equation (15), which was found to be consistently better than
several other distance metrics we tried on the held-out data.

we also enforced distraction in a similar way on the
attention-primed input content vector ct, as well as on the
hidden output vector st. both ct and st are not normalized
but are regular content vectors, where the cosine similarity

was found achieving a better performance than several alter-
natives (e.g., l1 and l2 distances) on the held-out data.

d  ,t = min

i   {1,      t   1}

kl(  t,   i)

dc,t = max

i   {1,      t   1}

ds,t = max

i   {1,      t   1}

cosine(ct, ci)

cosine(st, si)

(15)

(16)

(17)

the distraction score d   ,t was then added into the output
id203 and the id125 in order to encourage the
model to avoid redundant content.

scoret =

{log(pyt) +   1d  ,t +   2dc,t +   3ds,t} (18)

ty(cid:88)

where scoret was used as follows in the id125 with
distraction algorithm, and parameter   1,   2, and   3 were
determined on the development set. we refer to this model
as m3.

algorithm 1 id125 with distraction
require: vocabulary size k, beam size b, max output

length n
(cid:46) computed probabilities of all the words in vocabulary
(cid:46) choose the b most likely words and initialize the b hy-
potheses
for i = 1 : n do
(cid:46) for each hypothesis, compute the next conditional
probabilities, then have b    k candidates with the corre-
sponding probabilities

(cid:46) use the distraction-primed value score to choose b

most likely candidates
end for

unknown word replacement for summarization we bor-
rowed the unknown word replacement [jean et al., 2015]
from machine translation to our summarization models and
found it improved the performance when summarizing long
documents. speci   cally, due to the time complexity in han-
dling a larger vocabulary in the softmax layer in summary
generation, infrequent words were removed from the vocabu-
lary and were replaced with the symbol (cid:104)unk(cid:105). the thresh-
old of vocabulary size is data-dependent and will be detailed
later in the experiment set-up section.
after the    rst-round summary generated for a document, a
token labeled as (cid:104)unk(cid:105) will be replaced with a word in the
input documents. more speci   cally, we obtained the replace-
ment using equation (14); i.e., we used the largest element in
  t to    nd the source location for the current (cid:104)unk(cid:105).
4 experiment set-up
4.1 data
we experiment with our summarization models on two pub-
licly available corpora with different document lengths and
in different languages: a id98 news collection [hermann
et al., 2015] and a chinese corpus made available more re-
cently in [hu et al., 2015]. both are large datasets appropriate

for training neural models, which, as discussed above, em-
ploy a large number of parameters to    t the potentially com-
plicated summarization process involving representing input
documents, generating summaries, and interacting between
them.
id98 data the id98 data
[hermann et al., 2015] have
a human-generated real-life summary for each news arti-
cle. the dataset collected in was made available at github.1
the data was preprocessed with the stanford corenlp
tools [manning et al., 2014] for id121 and sentence-
boundary detection; all capital information is kept. to speed
up training, we removed the documents that are too long (over
2,500 word tokens) from the training and validation set, but
kept all documents in the test set, which does not change the
dif   culty of the task.
lcsts data the second corpus is lcsts, which is a chi-
nese corpus made available more recently in [hu et al., 2015].
the data is constructed from the chinese microblogging web-
site, sina weibo. we used the original training/testing split
mentioned in [hu et al., 2015], but additionally randomly
sampled a small part of the training data as our validation set.
table 1 gives more details about the two datasets. we can
see from the table that averaged document length of the id98
corpus is about seven time as long as the lcsts corpus, and
the summary is about 2-3 times longer.

id98

lcsts

train valid test
775.2 761.3 765.4
48.4 36.5 36.6

doc.l.
sum.l.
# doc. 81,824 1,184 1,093 2,400,000

train valid test
103.7 100.3 108.1
17.9 18.2 18.7
725

591

table 1: the id98 and lcsts dataset. the    rst two rows
of the table are the averaged document length (doc.l.) and
summary length (sum.l.) in terms of numbers of word to-
kens. the bottom row lists the number of documents in the
datasets.

4.2 training details
we used mini-batch stochastic id119 (sgd) to op-
timize log-likelihood, and adadelta [zeiler, 2012] to auto-
matically adapt the learning rate of parameters (  = 10   6
and    = 0.95).

for the id98 dataset, training was performed with shuf   ed
mini-batches of size 64 after sorting by length. we limit our
vocabulary to include the top 25,000 most frequent words.
other words were replaced with the token (cid:104)unk(cid:105), as dis-
cussed earlier in the paper. based on the validation data, we
set embedding dimension to be 120, the vector length in hid-
den layers to be 500 for uni-gru and 600 for bi-gru. an
end-of-sentence token was inserted between every sentence,
and an end-of-document token was added at the end. the
beam size of decoder was set to be 5.

for the lcsts data, a larger mini-batch size 256 was
found to be better based the observation on the validation set.

1https://github.com/deepmind/rc-data

same as in [hu et al., 2015], we used characters rather than
words as our tokens. the vocabulary size is 4000, embed-
ding dimension is 500, and the vector size of the hidden-layer
nodes is 500. id125 size is 5, same as in the id98
dataset.

we make our code publicly available2. our implementa-
tion uses python and is based on the theano library [bergstra
et al., 2010].
5 experimental results
5.1 results on the id98 dataset
overall performance our results on the id98 dataset are
presented in table 2. we used id8 scores [lin, 2004] to
measure performance. since the summary lengths are not
preset to be the same, we report f1 id8. the upper part
of the table includes the baseline results of a number of typ-
ical summarization algorithms, which we listed in the ta-
ble as luhn [luhn, 1958], edmundson [edmundson, 1969],
lsa [steinberger and jezek, 2004], lex-rank [erkan and
radev, 2004], text-rank [mihalcea and tarau, 2004], sum-
basic [vanderwende et al., 2007], and kl-sum [haghighi and
vanderwende, 2009]. these baseline results are implemented
in the open-source tool sumy3.

the results at the lower half of the table show that the
bi-gru encoder achieves a better performance than the uni-
gru encoder. this is consistent with the results on the lc-
sts dataset reported later in table 4. we show that two-level
output model we discussed in the method section is bene-
   cial, which is also consistent with the results on the lc-
sts dataset. in addition, the unknown replacement technique
yields an additional improvement.

over the strong model that has used these technologies (the
row marked as    +unk replace   ), the model in the last row
that incorporates all distraction modeling (m1, m2 and m3)
   nally achieves a id8-1 score of 27.1, a id8-2 score of
8.2, and a id8-l score of 18.7, signi   cantly improving the
three id8 scores by 5.8, 1.9, and 2.3, respectively. these
are also the largest improvement presented in the table, com-
pared with the other techniques listed. the table also lists the
details of how the model m1, m2, and m3 improve the per-
formance additively. again, the neural models do not engi-
neer any features and use only content but not any additional
formality features such as locations of input sentences, which
may bring additional improvement.
performance on different lengths of documents to observe
the effectiveness of the distraction model over different doc-
ument lengths, we further selected all short documents from
the id98 training dataset into a subset (subset-1) with aver-
age length at 335 word tokens, and a subset of data (subset-2)
that have the same number of documents as the subset-1, with
averaged document length at 680 word tokens. as shown in
table 3, on the data subset-2, the distraction model improves
the results more signi   cantly. the relative improvement is
29.0%, 25.6%, and 10.8%, compared with 25.9%, 20.5%,
and 8.1% on subset-1, respectively. in general the best per-
formance on both dataset is lower than that using all training

2our code is available at https://github.com/lukecq1231/nats
3 https://pypi.python.org/pypi/sumy

system
luhn
edmundson
lsa
lex-rank
text-rank
sum-basic
kl-sum
uni-gru
bi-gru
+two-level out
+unk replace
+distraction m1
+distraction m2
+distraction m3

id8-1 id8-2 id8-l

23.2
24.5
21.2
26.1
23.3
22.9
20.7
18.4
19.5
20.2
21.3
22.2
24.4
27.1

7.2
8.2
6.2
9.6
7.7
5.5
5.9
4.8
5.2
5.9
6.3
6.5
7.7
8.2

15.5
16.7
14.0
17.7
15.8
14.8
13.7
14.3
15.0
15.7
16.4
16.7
17.8
18.7

table 2: results on the id98 dataset.

data, suggesting using more training data can improve sum-
marization performance.

id8-1 id8-2 id8-l

w.o. distraction
+distraction
relative impr.

w.o. distraction
+distraction
relative impr.

subset-1

3.9
4.7

18.1
22.8
25.9% 20.5%
subset-2

18.6
24.0
29.0% 25.6%

3.9
4.9

13.6
14.7
8.1%

13.8
15.3
10.8%

table 3: results on two subsets of the id98 datasets with
different document lengths.

5.2 results on the lcsts dataset
we experiment with the proposed model on public lcsts
corpus. the baseline is the best result reported in [hu et al.,
2015]4. our modi   ed uni-gru achieves a slight improve-
ment over the reported results. the bi-gru attention-based
model achieves a better performance, con   rming the useful-
ness of bi-directional models for summarization as well as
that our implementation is the state-of-the-art and serves as
a very strong baseline in the id98 dataset discussed above.
note that since the input text length of lcsts is far shorter
than the id98 documents, each containing about 100 words
and roughly 6-8 sentences, we show that distraction does not
improve the performance, but in contrast, when documents
are longer, its bene   ts are signi   cant, achieving the biggest
improvement as discussed earlier. this suggests the effective-
ness of distraction modeling in helping summarize the more

4we thank the authors of [hu et al., 2015] for generously sharing
us the latest output of their models, which achieves a better perfor-
mance than the results reported in [hu et al., 2015]. we reported
here the updated scores higher performance as our baseline.

challenging longer documents, where summarization is often
more necessary than for short texts.

system
[hu et al., 2015]
uni-gru
bi-gru
+two-level att.
+unk replace
+distraction

id8-1 id8-2 id8-l

29.9
32.1
33.2
35.2
35.2
35.2

17.4
19.9
20.8
22.6
22.6
22.6

27.2
29.4
30.5
32.5
32.5
32.5

table 4: results on the lcsts dataset.

we also compare our models with the simple baseline that
selects the    rst n numbers of word tokens from the input
documents, which reaches its maximal id8 scores when
the    rst 30 tokens were taken, and achieves id8-1, id8-
2, and id8-l at 25.5, 14.1 and 21.4. and our models are
signi   cantly better than that. for the id98 data set, choos-
ing the    rst three sentences achieves the best results, which
reach id8-1, id8-2, and id8-l at 26.1, 9.6 and 17.8,
respectively. since the id98 data is news data, the baseline of
selecting    rst several sentences has known to be a very strong
baseline. again, the models we explore here are towards per-
forming abstractive summarization.

6 conclusions and future work

we propose to train neural document summarization models
not just to pay attention to speci   c regions of input documents
with id12, but also distract the models to different
content in order to better grasp the overall meaning of input
documents. without engineering any features, we train the
models on two large datasets. the models achieve the state-
of-the-art performance and they signi   cantly bene   t from the
distraction modeling, particularly when the input documents
are long. we also explore several recent technologies for
summarization and show that they help improve summariza-
tion performance as well. even if applied onto the models
that have already leveraged these technologies, the distraction
models can further improve the performance signi   cantly.

from a more general viewpoint, enriching the expressive-
ness of the control layers that link the input encoding layer
and the output decoding layer could be of importance to rem-
edy the shortcomings of the current models. we plan to per-
form more work along this direction.

acknowledgments

the    rst and the third author of this paper were supported
in part by the science and technology development of an-
hui province, china (grants no. 2014z02006), the funda-
mental research funds for the central universities (grant
no. wk2350000001) and the strategic priority research
program of the chinese academy of sciences (grant no.
xdb02070006).

references
[bahdanau et al., 2014] dzmitry bahdanau, kyunghyun cho, and
yoshua bengio. id4 by jointly learning
to align and translate. corr, abs/1409.0473, 2014.

[bergstra et al., 2010] j bergstra, o breuleux, f bastien, p lam-
blin, r pascanu, g desjardins, j turian, d warde-farley, and
y bengio. theano: a cpu and gpu math expression compiler. in
scipy, volume 4, page 3. austin, tx, 2010.

[chen et al., 2015] zhigang chen, wei lin, qian chen, si wei, hui
jiang, and xiaodan zhu. revisiting id27 for contrast-
ing meaning. in proceedings of acl, 2015.

[chen et al., 2016] qian chen, xiaodan zhu, zhenhua ling,
si wei, and hui jiang. enhancing and combining sequential and
tree lstm for natural language id136. in arxiv:1609.06038v1,
2016.

[cho et al., 2014] k. cho, b. van merri  enboer, c. gulcehre,
d. bahdanau, f. bougares, h. schwenk, and y. bengio. learning
phrase representations using id56 encoder-decoder for statistical
machine translation. in emnlp, 2014.

[collobert et al., 2011] r. collobert,

j. weston, l. bottou,
m. karlen, k. kavukcuoglu, and p. kuksa. natural language
processing (almost) from scratch. jmlr, 12:2493   2537, 2011.

[das and martins, 2007] dipanjan das and andre martins. a sur-

vey on automatic text summarization. 2007.

[edmundson, 1969] harold p edmundson. new methods in auto-

matic extracting. jacm, 16(2):264   285, 1969.

[erkan and radev, 2004] g  unes erkan and dragomir r radev.
lexrank: graph-based lexical centrality as salience in text sum-
marization. jair, pages 457   479, 2004.

[haghighi and vanderwende, 2009] aria haghighi and lucy van-
derwende. exploring content models for multi-document sum-
marization. in naacl, 2009.

[hermann et al., 2015] k. hermann, t. kocisky, e. grefenstette,
l. espeholt, w. kay, m. suleyman, and p. blunsom. teaching
machines to read and comprehend. in nips, 2015.

[hochreiter and schmidhuber, 1997] sepp hochreiter and j  urgen
schmidhuber. long short-term memory. neural computation,
9(8):1735   1780, 1997.

[hu et al., 2015] baotian hu, qingcai chen, and fangze zhu. lc-
sts: a large scale chinese short text summarization dataset. in
emnlp, 2015.

[irsoy and cardie, ] ozan irsoy and claire cardie. deep recursive
neural networks for compositionality in language. in nips, pages
2096   2104.

[jean et al., 2015] s  ebastien jean, kyunghyun cho, roland memi-
sevic, and yoshua bengio. on using very large target vocabulary
for id4. in acl, 2015.

[kalchbrenner et al., 2014] nal kalchbrenner, edward grefen-
stette, and phil blunsom. a convolutional neural network for
modelling sentences. acl, june 2014.

[li et al., 2015] jiwei li, minh-thang luong, and dan jurafsky. a
hierarchical neural autoencoder for paragraphs and documents.
in acl, 2015.

[lin et al., 2015] r. lin, s. liu, m. yang, m. li, m. zhou, and
s. li. hierarchical recurrent neural network for document mod-
eling. in emnlp, 2015.

[lin, 2004] chin-yew lin. id8: a package for automatic evalu-

ation of summaries. 2004.

[lopyrev, 2015] konstantin lopyrev. generating news headlines

with recurrent neural networks. corr, abs/1512.01712, 2015.

[luhn, 1958] hans peter luhn. the automatic creation of literature
abstracts. ibm journal of research and development, 2(2):159   
165, 1958.

[luong et al., 2015] thang luong, hieu pham, and christopher d.
manning. effective approaches to attention-based neural ma-
chine translation. in emnlp, 2015.

[mani, 2001] inderjeet mani. id54. j. ben-

jamins pub. co., amsterdam, 2001.

[manning et al., 2014] c manning, m surdeanu, j bauer, j finkel,
s bethard, and d mcclosky. the stanford corenlp natural lan-
guage processing toolkit. in acl, 2014.

[mihalcea and tarau, 2004] rada mihalcea and paul tarau. tex-

trank: bringing order into text. in emnlp, 2004.

[mikolov et al., 2013] t. mikolov, i. sutskever, k. chen, g. cor-
rado, and j. dean. distributed representations of words and
phrases and their compositionality. in nips, 2013.

[nenkova and mckeown, 2013] ani nenkova and kathleen mck-
eown. a survey of text summarization techniques. springer, 2013.
[rush et al., 2015] alexander m. rush, sumit chopra, and jason
weston. a neural attention model for abstractive sentence sum-
marization. in emnlp, 2015.

[socher et al., 2012] r. socher, b. huval, c. manning, and
a. ng. semantic compositionality through recursive matrix-
vector spaces. in emnlp, 2012.

[steinberger and jezek, 2004] j. steinberger and k. jezek. using
latent semantic analysis in text summarization and summary eval-
uation. in isim, pages 93   100, 2004.

[sutskever et al., 2011] ilya sutskever, james martens, and geof-
frey hinton. generating text with recurrent neural networks. in
icml, pages 1017   1024, 2011.

[sutskever et al., 2014] ilya sutskever, oriol vinyals,

and
sequence to sequence learning with neural

quoc vv le.
networks. in nips, pages 3104   3112, 2014.

[tai et al., 2015] kai sheng tai, richard socher, and christo-
pher manning. improved semantic representations from tree-
structured id137. in acl, 2015.
[tu et al., 2016] zhaopeng tu, zhengdong lu, yang liu, xiaohua
liu, and hang li. coverage-based id4.
corr, abs/1601.04811, 2016.

[vanderwende et al., 2007] l. vanderwende, h. suzuki, c. brock-
ett, and a. nenkova. beyond sumbasic: task-focused sum-
marization with sentence simpli   cation and lexical expansion.
ip&m, 43(6):1606   1618, 2007.

[wang and cho, 2015] tian wang and kyunghyun cho. larger-

context language modelling. corr, abs/1511.03729, 2015.

[yin and sch  utze, 2014] w. yin and h. sch  utze. an exploration
in acl 2014 student

of embeddings for generalized phrases.
research workshop, pages 41   47, june 2014.

[zeiler, 2012] matthew d. zeiler. adadelta: an adaptive learning

rate method. corr, abs/1212.5701, 2012.

[zhu and penn, 2006] xiaodan zhu and gerald penn. comparing
the roles of textual, acoustic and spoken-language features on
spontaneous conversation summarization. in naacl, 2006.

[zhu et al., 2010] xiaodan zhu, gerald penn, and frank rudz-
icz. summarizing multiple spoken documents: finding evidence
from untranscribed audio. in acl, 2010.

[zhu et al., 2014] xiaodan zhu, hongyu guo, saif mohammad,
and svetlana kiritchenko. an empirical study on the effect of
negation words on sentiment. in proceedings of the 52th annual
meeting of the association for computational linguistics, 2014.
[zhu et al., 2015a] xiaodan zhu, hongyu guo, and parinaz sob-
hani. neural networks for integrating compositional and non-
compositional sentiment in sentiment composition. in proceed-
ings of joint conference on lexical and computational seman-
tics, june 2015.

[zhu et al., 2015b] xiaodan zhu, parinaz sobhani, and hongyu
guo. long short-term memory over recursive structures. in
proceedings of international conference on machine learning,
2015.

[zhu et al., 2016] xiaodan zhu, parinaz sobhani, and hongyu
guo. dag-structured long short-term memory for semantic com-
positionality. in proceedings of the meeting of the north amer-
ican chapter of the association for computational linguistics
(naacl), 2016.

