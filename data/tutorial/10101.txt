neural sentence ordering

xinchi chen, xipeng qiu, xuanjing huang

shanghai key laboratory of intelligent information processing, fudan university

school of computer science, fudan university
{xinchichen13, xpqiu, xjhuang}@fudan.edu.cn

825 zhangheng road, shanghai, china

6
1
0
2

 
l
u
j
 

3
2

 
 
]
l
c
.
s
c
[
 
 

1
v
2
5
9
6
0

.

7
0
6
1
:
v
i
x
r
a

abstract

sentence ordering is a general and critical task
for id86 applications.
previous works have focused on improving
its performance in an external, downstream
task, such as id57.
given its importance, we propose to study it
as an isolated task. we collect a large corpus
of academic texts, and derive a data driven ap-
proach to learn pairwise ordering of sentences,
and validate the ef   cacy with extensive exper-
iments. source codes1 and dataset2 of this pa-
per will be made publicly available.

(1) he liked music when he was a boy.
(2) people are shocked by his potential.
(3) chopin is a great musician in poland.
(4) when he was 15, he    nished his    rst
waltz.
gold: (3) (1) (4) (2)

table 1: illustration of sentence ordering task. mul-
tiple discourse coherence relations might appear in a
single text. first sentence (3) declares a topic. sen-
tences (1) (4) are in chronological sequence. how-
ever, sentence (2) is a result, so it should be the last
sentence so as to abide to a cause-effect relation.

introduction

1
the goal of sentence ordering is to arrange a set of
sentences into a coherent text in a clear and con-
sistent manner (grosz et al., 1995; van berkum et
al., 1999; barzilay and lapata, 2008). the task
is general and yet challenging, and is especially
important for id86 (reiter
and dale, 1997).
its applications include multi-
document summarization, id53, and
concept-to-text generation.
improper ordering of
sentences can generate confusing texts, degrading
readability.

a text should be organized according to it
discourse coherence of the following properties:
rhetorical (mann and thompson, 1988) coherence
(hobbs, 1990), topical relevancy, chronological se-
quence, and cause-effect (hume, 1750; okazaki et
al., 2004). these properties intertwine with each

1https://github.com/fudannlp
2http://nlp.fudan.edu.cn/data/

other, and can be quite subtle, as shown in the ex-
ample in table 1.

most of previous researches of sentence order-
ing were integrated into an external and downstream
task, such as id57 (barzi-
lay and elhadad, 2002; lapata, 2003; bollegala
et al., 2010). the input sentences are extracted
from multiple sources, therefore their intrinsic co-
herence is relatively weak. consequently, it is some-
what dif   cult to judge of the order of given sen-
tences. moreover, these methods addressed the or-
dering problem of newspaper articles. ordering cri-
teria include majority ordering, chronological or-
dering, topical-closeness, precedence, and succes-
sion. among them, chronological ordering (i.e. or-
ders sentence by the publication date) can produce
satisfactory orderings (barzilay and elhadad, 2002;
okazaki et al., 2004). obviously, this is a natural
result for ordering sentences extracted from news-
paper articles, since the task is to arrange a large

number of time-series events concerning several top-
ics. besides, all these criteria can be considered as
reasonable hand-engineered features. nevertheless,
they cannot be adapted to other tasks or domains, as
our example shows.

in this paper, we stage this problem as a stan-
dalone task, and adopt a data driven approach. we
   rst derive neural model to encode each sentence
into distributed representation (dense vector), then
predict the pairwise ordering of sentences. next,
to avoid brute-force rearrangement, we use a beam
search to determine the most probable permutation.
for this purpose, we collect about a million
abstracts of research papers from arxiv website3.
these abstracts are well designed coherent texts,
and each of them involves several different criteria,
including chronological ordering, topical-closeness,
etc. for instance, abstracts might    rst declare the
shortcomings of previous methods, leading to the
reason why they propose the new one. that is cause-
effect relation. chronological sequences (marked by
keywords       rst   ,    then   , etc.) might appear when
they describe their models.

the contributions of this paper can be summa-

rized as follows:

1. we frame sentence ordering as an isolated task,
and collected a large corpus whose correct or-
dering goes beyond conventional criteria.

2. instead of relying on hand-designed features,
we explore a fully data-driven approach to
learn the order of a set of sentences.

3. we perform extensive empirical studies and

demonstrate the ef   cacy of our approach.

2 sentence ordering
2.1 task description
sentence ordering task takes a text s that is possibly
out-of-order sentences,

s = s1, s2, . . . , sns.

(1)

and    nds the gold order. a good model must has the
ability to capture the logic of a text. that is, the goal
is to discover an order o, which is equal to the gold
order o    of these sentences:

(cid:31) so   

2

(cid:31)        (cid:31) so   

ns

,

(2)

so   

1
3https://arxiv.org/

figure 1: neural network approach for pairwise or-
der prediction.

here, o is one of permutations of numbers in
{1, 2, . . . , ns}. for instance, in table 1, whereas the
current order o is [1, 2, 3, 4], o = o    = [3, 1, 4, 2] is
the gold order.

2.2 ranking model
sentence ordering task can be viewed as a rank-
ing problem.
in this paper, we adopt the preva-
lent pairwise ranking model (schapire and singer,
1998; f  urnkranz and h  ullermeier, 2003; zheng et
al., 2007). that is, the goal is to predict the order
of any two sentences pair (si, sj) as shown in fig-
ure 1. formally, given representations of sentences
e1, . . . , ens, embeddings from a sentence encoder
(section 3), we model the id203 pij that sen-
tence si precedes sj as:

(cid:124)
(cid:124)

(ei     ej) + bh),
hij + bp),

hij =   (wh
pij =   (wp

(3)
(4)
where wh     r2ds  h, bh     rh, wp     rh, bp    
r are trainable parameters.   (  ) is sigmoid function
and   (  ) is tanh function.

the score of sentences in order o can be cal-
culated as a log-likelihood maximization problem
(chen et al., 2013):

score(s, o, i, j) = log poioj ,

ns(cid:88)

ns(cid:88)

score(s, o) =

score(s, o, i, j),

(5)

(6)

i=1

j=i+1

where score(s, o, i, j) indicates the score for sen-
tence pairs (oi, oj) and score(s, o) indicates the
score of sentences s in order o. clearly, this can be
seen as a two-layer neural network.

look up tablepairwise order predictionsentence encodingewee1j...givee2jenjjquestion...eoure1ifirste2ieniiminimalpijhijijalgorithm 1 id125 for order prediction.
1: beam = [ ]
2: for i = 1; i <= ns; i + + do
3:

item = ([i], 0.0) # tuple (partial order, award)

order = item[0] + [j] # append new
if any duplicate(order)==true then

continue

beam.append(item)

for j = 1; j <= ns; j + + do

new beam = [ ]
for all item     beam do

4:
5: end for
6: # n + 1 elements have been generated
7: for n = 0; n < ns     1; n + + do
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23: end for
24:   o, score(s,   o) = best(beam)
25: return   o

end if
award = item[1]
for i = 0; i <= n; i + + do

end for
beam = n-best(new beam)

end for

end for
new beam.append((order, award))

award += score(s, order, i, n + 1)

2.3 order prediction
the order prediction phase aims to    gure out
the predicted sentence order   o which maximizes
score(s, o):

  o = arg max

o

score(s, o),

(7)

search all valid permutations by brute force to
discover the optimal   o is computationally expensive
and fundamentally non-scalable. therefore, we use
the beam-search strategy to    nd a sub-optimal order.
the details are show in algorithm 1.

3 sentence encoding
to    gure out the impacts of various sentence repre-
sentations, we employ three different sentence en-
coders to model sentences: continuous bag of words
(cbow), convolutional neural networks (id98) and
long short-term (lstm) neural networks. all these

models map words into a embedded space by look-
ing up a embedding table.

3.1 continues bag of words
continues bag of words (cbow) model (mikolov et
al., 2013) simply averages the embeddings of words
of a sentence. formally, given the embeddings of
nw words of a sentence e1, . . . , enw, we can get sen-
tence embedding e by an average operation:

nw(cid:88)

k=1

e =

1
nw

ek,

(8)

where e     rds and ek     rd, and ds = d are dimen-
sionalities of sentence embedding and word embed-
dings respectively.

3.2 convolutional neural networks
convolutional neural networks (id98s) (simard et
al., 2003) extract local features and gain the global
prominent features by a max-pooling operation over
sentence. formally, we represent sentence as:

covk =   (w
e = max

k

cov(   lf   1
(cid:124)
covk,

u=0 ek+u) + bcov),

(9)
(10)

where wcov     r(d  lf )  df and bcov     rdf
are trainable parameters, and   (  ) is tanh function.
here, k = 1, . . . , nw     lf + 1, and lf and df
are hyper-parameters indicating the    lter length and
number of feature maps respectively. notably, max
operation in eq (10) is a element-wise operation.

3.3 long short-term neural networks
long short-term (lstm) neural networks (hochre-
iter and schmidhuber, 1997) aim to maintain the
crucial information through time. lstm is an ad-
vanced recurrent neural network (id56), which alle-
viates the problem of gradient vanishment and ex-
plosion. formally, lstm has memory cells c    
rdr controlled by three kinds of gates: input gate
i     rdr, forget gate f     rdr and output gate
o     rdr:

             it

ot
ft
  ct

             =

               

  
  
  

            (cid:18)

(cid:21)

(cid:124)(cid:20) et

ht   1

(cid:19)

wg

+ bg

, (11)

attributes

# of abstracts

# of sentences per abstracts

# of words per abstracts

dev

train

test
884,912 110,614 110,615
5.37
134.58 134.80 134.58

5.38

5.39

table 2: details of arxiv datasets.

ct = ct   1 (cid:12) ft +   ct (cid:12) it,
ht = ot (cid:12)   (ct),

(12)
(13)
where wg     r(d+dr)  4dr and bg     r4dr are train-
able parameters. dr is a hyper-parameter indicating
the cell unit size as well as gate unit size.   (  ) is
sigmoid function and   (  ) is tanh function. here,
t = 1, . . . , nw. thus, we would represent sentence
as:

e = hnw .

(14)

f ir, si

4 training
in this paper, we use pairwise ranking model. thus,
we extract m gold sentence pairs {xi = (si
sec),
yi = 1}m
i=1 as positive samples from the whole cor-
pus. meanwhile, we construct m negative samples
by reversing the gold sentence pairs {xi+m = (si
sec,
f ir), yi+m = 0}m
i=1.
si
the objective is to minimize the id168
2m(cid:88)

yi log pxi + (1    yi) log(1    pxi),
(15)
where pxi is the id203 that sentence pair xi is
in correct order as eq (4). here, parameter set   
indicates all trainable parameters of our model.

j(  ) =     1
2m

j(  ):

i=1

we use shuf   ed mini-batch stochastic gradient
descent (sgd) algorithm together with adadelta
(zeiler, 2012) to train our model.

5 experiments
5.1 dataset
since abstracts of paper are always well written and
have strong logic clues, we evaluate our models on
all abstracts on arxiv website up to date4. ab-
stracts from arxiv can be mainly classi   ed into 7
categories: statistics, quantitative biology, physics,
computer science, nonlinear sciences, quantitative

4we collect all abstracts of paper before 2016-5-25.

physics (phys)

statistics (stat)

dev
2,465
1,943

categories (abbreviation)

computer science (cs)
nonlinear sciences (nlin)
quantitative finance (q   n)

train
19,223
quantitative biology (qbio) 15,495

test
2,497
1,866
821,795 102,584 102,892
84,689 10,624 10,453
1,695
13,273
5,201
670
216,153 26,819 26,854
table 3: details of category information of arxiv
datasets. notably, since categories could be over-
lapped, the size of total data set is smaller than the
sum of numbers of all 7 categories.

mathematics (math)

1,619
708

initial learning rate    = 0.2
hidden layer size
h = 100
filter length of id98 lf = 3
128
128

batch size
beam size

table 4: hyper-parameter con   gurations.

   nance and mathematics. the development set and
test set are the    rst and last 10% abstracts from shuf-
   ed data, and the training set consists of the remains.
the detailed information of arxiv dataset is shown
in table 2 and table 3. we use nltk toolkit (bird,
2006) to break paragraph into sentences.

5.2 hyper-parameters
table 4 gives the details of hyper-parameter con   g-
urations. id173 term with coef   cient    =
10   4 is omitted in eq (15) for simplicity. besides,
we set number of feature maps df of id98 and cell
unit size dr of lstm as same as id27
dimensionality d.

5.3 id74
to evaluation the results (predicted orders), we use
three types of metrics: id8-s, id8-n (lin,
2004) and p-all. unlike summarization task, the pre-
cision and recall rates are always the same in sen-
tence ordering task. thus, id8-s, id8-n could
be introduced in a simpler way. moreover, we also
introduce p-all metric to calculate the ratio of exact
matching orders.

5.3.1 id8-s

id8-s is skip-bigram co-occurrence statistics.
skip-bigram contains any pair of sentences in text,

id8-2

id8-s

metrics
models cbow id98 lstm cbow id98 lstm cbow id98 lstm cbow id98 lstm
0.7993 0.8004 0.8217 0.4421 0.4416 0.4742 0.2420 0.2420 0.2729 0.2881 0.2888 0.3178
25w
0.8002 0.8113 0.8278 0.4438 0.4579 0.4827 0.2437 0.2574 0.2818 0.2892 0.3022 0.3257
50w
100w 0.7982 0.8164 0.8296 0.4426 0.4669 0.4899 0.2423 0.2664 0.2892 0.2870 0.3114 0.3314
200w 0.7992 0.8192 0.8297 0.4422 0.4729 0.4916 0.2420 0.2716 0.2911 0.2866 0.3156 0.3343
random

id8-3

0.4999

0.2309

0.0582

0.0807

p-all

table 5: performances of different models on test set of arxiv dataset.

allowing for arbitrary gaps. suppose we have a cor-
pus including m texts s1, s2, . . . , sm . then, id8-
s could be formalized as:

|s(sm,   om)(cid:84) s(sm, om   )|

|s(sm, om   )|

,

m(cid:88)

m=1

id8-s =

1
m

(16)
where s(  ) is the set of all skip bigram sentence pairs
of a text. here, sm is the m-th text.   om and om    are
predicted and gold orders of m-th text respectively.

5.3.2 id8-n

id8-n is id165 co-occurrence statistics which

could be formalized as:

|n(sm,   om)(cid:84) n(sm, om   )|

|n(sm, om   )|

,

m(cid:88)

m=1

id8-n =

1
m

(17)
where n(  ) is the set of all n consecutive sentences
in a given order.

5.3.3 p-all

p-all aims to calculate the radio of exact matching

orders which could be formalized as:

p-all =

1
m

1{  om = om   },

(18)

where 1{  } is indicator function.
5.4 results
we use id8-s, id8-2, id8-3 and p-all met-
rics to evaluate our model with different sentence en-
coders. we also vary dimensionality of word embed-
dings, as shown in table 5. line    random    means
we randomly generate the orders for texts.

according to the results, we    nd the perfor-
mances of id98 and lstm increase with larger
id27 size, whereas the performance of

m(cid:88)

m=1

cbow peaks at 50. among 3 sentence encoders,
lstm outperforms others in any case, which is
much more effective than random baseline. espe-
cially, lstm achieves 0.3343 on p-all metric, which
means more than one third texts could be ranked
correctly (exactly matched), whereas random base-
line only achieves 0.0807 on p-all metric. id8-
s is much higher than other metrics, since any cor-
rect pair of sentences with arbitrary gaps contributes
to id8-s score.
in general, p-all is harder than
id8-3, then id8-2 and id8-s. however, we
   nd p-all scores are always higher than id8-3
scores here. the reason is that the texts with 2 sen-
tences contribute to p-all score, and their id8-3
scores are always 0 as shown in figure 2c.

detailed results figure 2 summarizes our perfor-
mance on different text sizes, with the embedding
dimension as 200. the x-axis of each sub    gure in-
dicates number of sentences. results show that per-
formances drop rapidly when texts scale up (number
of sentences increases). generally speaking, texts
with more sentences are more dif   cult to rank cor-
rectly. speci   cally, on p-all metric, cbow, id98
and lstm could achieve 0.8898, 0.9174 and 0.9272
with 2 sentences respectively, whereas random base-
line only makes it at 0.4977. however, the perfor-
mance drops rapidly. lstm only achieves 0.0015
on p-all with 10 sentences to rank. notably, id8-
3 score of texts with 2 sentences is 0 (figure 2c),
since there is no 3-grams in this case.

in addition, we investigate the performance on
different categories as shown in figure 3.
inter-
estingly, according to the category analysis results,
we    nd that mathematics and nonlinear sciences are
easier than other categories. speci   cally, lstm
could achieves 0.4585 on p-all metric, which means
nearly one half math texts could be predicted ex-
actly.

1

0.9

0.8

0.7

cbow
id98
lstm

1

0.8

0.6

0.4

0.2

cbow
id98
lstm

0.6

0.4

0.2

0

cbow
id98
lstm

1

0.5

0

cbow
id98
lstm

2

4

6

8

10

2

4

6

8

10

2

4

6

8

10

2

4

6

8

10

# of sentences
(a) id8-s

# of sentences
(b) id8-2

# of sentences
(c) id8-3

# of sentences
(d) p-all

figure 2: performances of different sentence encoders with 200 dimensional id27s on different
numbers of sentences on test set of arxiv dataset.

cbow

id98

lstm

cbow

id98

lstm

0.86
0.84
0.82
0.8
0.78

0.3

0.25

0.2

0.6

0.5

0.4

stat

qbio

phys

cs

nlin

q   n

math

stat

qbio

phys

cs

nlin

q   n

math

(a) id8-s

(b) id8-2

cbow

id98

lstm

cbow

id98

lstm

0.4
0.3
0.2

stat

qbio

phys

cs

nlin

q   n

math

stat

qbio

phys

cs

nlin

q   n

math

(c) id8-3

(d) p-all

figure 3: performances of different sentence encoders with 200 dimensional id27s on different
categories on test set of arxiv dataset.

(a) id98 for sentence 1

(b) id98 for sentence 2

(c) lstm for sentence 1

(d) lstm for sentence 2

figure 4: visualization of sentence 1 and sentence 2 using id98 and lstm.

cbow

2 our second question regarding
the function which computes
minimal indices is whether one
can compute a short list of can-
didate indices which includes a
minimal index for a given pro-
gram

1 our    rst question regarding
the set of minimal indices is
whether there exists an algo-
rithm which can correctly label
1 out of k indices as either min-
imal or non minimal

3 we give some negative results
and leave the possibility of pos-
itive results as open questions

id98

1 our    rst question regarding
the set of minimal indices is
whether there exists an algo-
rithm which can correctly label
1 out of k indices as either min-
imal or non minimal

lstm

1 our    rst question regarding
the set of minimal indices is
whether there exists an algo-
rithm which can correctly label
1 out of k indices as either min-
imal or non minimal

3 we give some negative results
and leave the possibility of pos-
itive results as open questions

2 our second question regarding
the function which computes
minimal indices is whether one
can compute a short list of can-
didate indices which includes a
minimal index for a given pro-
gram

2 our second question regarding
the function which computes
minimal indices is whether one
can compute a short list of can-
didate indices which includes a
minimal index for a given pro-
gram

3 we give some negative results
and leave the possibility of pos-
itive results as open questions

table 6: case study. color indicates importance of words in order prediction. the more important the
words are, the darker the color is.

models
pbegin
cbow 0.7837
id98
0.8294
lstm 0.8485
random 0.2306

pend
0.5762
0.6079
0.6237
0.2316

pmean
0.5263
0.5585
0.5760
0.2307

table 7: the performance of discerning the begin-
ning and the ending sentences of proposed models
on test set of arxiv dataset.

moreover, we observed that

the beginning
and the ending sentences are easier to discern
(mostafazadeh et al., 2016) as shown in table 7.
pbegin and pend indicate the ratio of correct begin-
ning and ending cases respectively. pmean indicates
the ratio of correct positions. notably, results on ta-
ble 7 are based on models with 200 dimensional em-
beddings.

5.5 case study
to gain further insight, we pick the abstract of the
paper    on approximate decidability of minimal pro-
grams    (teutsch and zimand, 2015) for case study.
first, we visualize which the key words in abstract
are important in order prediction. then, we visualize

the importance of words in scoring a given sentence
pair. all visualizations are based on model using 25
dimensional id27s trained on computer
science data, and the selected abstract is from test
set of computer science category.

5.5.1 text level visualization

we choose the last three sentences of the abstract
for visualization, as shown in table 6. the texts
in displayed orders are predicted by cbow, id98
and lstm respectively, and the sequence numbers
in front of sentences indicate gold orders. color in-
dicates importance of words in order prediction. the
more important the words are, the darker they are
coloured.

how to calculate the importance of words (color)?
inspired by the back-propagation strategy (erhan et
al., 2009; simonyan et al., 2013; li et al., 2015),
which measures how much each input unit con-
tributes to the    nal decision, we can approximate the
importance of words by their    rst derivatives. given
k in
a text s1, . . . , sn, the embedding of k-th word wi
i-th sentence si is ei
k. then, we de   ne aij(wi
k) as
k in predicting the order of
the importance of word wi

n(cid:88)

sentence pair (si, sj):

aij(wi

k) =

   pij
   ei
k

,

(19)

where pij     r is described in eq (4).

thus, we could de   ne the importance of a word

a(wi

k) in whole text as:

a(wi

k) =

pij|aij(wi

k)|,

(20)

j=i+1

where |    | is the norm of vector, and we use second
order norm here.

discussion according to the result, words such as
      rst    and    second    are indicative, as they imply
logic clues. also, since we only take the last three
sentences of the abstract, it is quite reasonable that
the word    results    appears in the last one or two sen-
tences. we also    nd cbow makes mistake in pre-
dicting the order of sentence pair (1, 2). speci   cally,
if score pcbow
indicates the reward of placing sen-
tence 2 in front of sentence 1, we could list the de-
tailed score information of sentence pair (1, 2) and
its reverse:

2,1

models
p1,2
cbow 0.4911
id98
0.7083
lstm 0.8744

p2,1
0.6097
0.3449
0.1110

table 8: detailed score information of sentence pair
(1, 2) and its reverse.

as shown in table 8, cbow believes the sentence
order (2, 1) gets higher score than the reverse. id98
and lstm correctly predict the order, and lstm
does so predict with high con   dence, with scores of
orders (1, 2) and (2, 1) as 0.8744 and 0.1110, respec-
tively.

5.5.2 sentence level visualization

to visualize the importance of words in predicting
order of sentence pair explicitly, we print the word
information aij(wi
k) of sentence 1 and sentence 2 in
figure 4. since cbow only takes a simple average
operation, word information aij(wi
k) in a sentence
is the same. thus, we only plot the results of id98
and lstm.

discussion as shown in figure 4, both id98 and
lstm notice the key words       rst    and    seconde   .
however, id98 also concentrates on other words
like    algorithm   ,    one    which may not be useful in
deciding the order. as the result in table 8, lstm
is more con   dent than id98 to rank the sentence 1
in front of sentence 2. in another word, lstm may
clearly capture more important clues or logical in-
formation than id98.

6 related work

a fundamental problem in text generation is infor-
mation ordering, including word and sentence order-
ing. comparing with word ordering (tillmann and
ney, 2000; zhang et al., 2012; zhang and clark,
2015; schmaltz et al., 2016), sentence ordering is
still less studied. existing works of sentence order-
ing focus to improve the external and downstream
applications, such as multi-document summariza-
tion and discourse coherence (van dijk, 1985; grosz
et al., 1995; van berkum et al., 1999; elsner et al.,
2007; barzilay and lapata, 2008). there is also a
lack of intrinsic evaluation for sentence ordering.

barzilay and elhadad (2002) proposed two naive
sentence ordering techniques, such as majority or-
dering and chronological ordering, in the context
of id57. lapata (2003)
proposed a probabilistic model that assumes the
id203 of any given sentence is determined by
its adjacent sentence and learns constraints on sen-
tence order from a corpus of domain speci   c texts.
okazaki et al. (2004) improved chronological order-
ing by resolving antecedent sentences of arranged
sentences and combining topical segmentation. bol-
legala et al. (2010) presented a bottom-up approach
to arrange sentences extracted for multi-document
summarization. to capture the association and or-
der of two textual segments (e.g. sentences), they
de   ned four criteria: chronology, topical-closeness,
precedence, and succession.

unlike these existing works, we propose a data-
driven method to learn the order of sentences. we
use neural models to encode sentences and learn the
pairwise orders. the text order can be further found
by a id125 process.

7 conclusions

although sentence ordering is an important factor
in id86, it still lacks of in-
trinsic evaluation for sentence ordering task. to ad-
dress this, this paper introduces a new large corpus
for evaluation of sentence ordering task. the corpus
is a collection of abstracts of academic papers. we
use this corpus to evaluate a range of neural models.
these neural models perform well for judging the
order of sentence pair, but perform relatively poor
on the whole abstract. therefore, sentence ordering
is still a challenging problem. we hope that our cor-
pus provides valuable training data and a testbed for
sentence ordering task.

in the future, we would like to integrate other
ranking models like list-wise model for sentence or-
dering task.

references
[barzilay and elhadad2002] regina barzilay and noemie
elhadad. 2002. inferring strategies for sentence order-
ing in multidocument news summarization. journal of
arti   cial intelligence research, pages 35   55.

[barzilay and lapata2008] regina barzilay and mirella
lapata. 2008. modeling local coherence: an entity-
based approach. computational linguistics, 34(1):1   
34.

[bird2006] steven bird. 2006. nltk: the natural language
toolkit. in proceedings of the coling/acl on inter-
active presentation sessions, pages 69   72. association
for computational linguistics.

[bollegala et al.2010] danushka

naoaki
okazaki, and mitsuru ishizuka. 2010. a bottom-up
approach to sentence ordering for multi-document
information processing & manage-
summarization.
ment, 46(1):89   109.

bollegala,

[chen et al.2013] xi chen, paul n bennett, kevyn
collins-thompson, and eric horvitz. 2013. pairwise
ranking aggregation in a crowdsourced setting. in pro-
ceedings of the sixth acm international conference on
web search and data mining, pages 193   202. acm.

[elsner et al.2007] micha elsner, joseph l austerweil,
and eugene charniak. 2007. a uni   ed local and
in hlt-
global model for discourse coherence.
naacl, pages 436   443.

[erhan et al.2009] dumitru erhan, yoshua bengio, aaron
courville, and pascal vincent.
2009. visualizing
higher-layer features of a deep network. university
of montreal, 1341.

[f  urnkranz and h  ullermeier2003] johannes

f  urnkranz
and eyke h  ullermeier. 2003. pairwise preference
in machine learning: ecml
learning and ranking.
2003, pages 145   156. springer.

[grosz et al.1995] barbara j grosz, scott weinstein, and
aravind k joshi. 1995. centering: a framework for
modeling the local coherence of discourse. computa-
tional linguistics, 21(2):203   225.

[hobbs1990] jerry r hobbs. 1990. literature and cog-
nition. number 21. center for the study of language
(csli).

[hochreiter and schmidhuber1997] sepp hochreiter and
j  urgen schmidhuber. 1997. long short-term memory.
neural computation, 9(8):1735   1780.

[hume1750] david hume. 1750. philosophical essays
concerning human understanding. georg olms ver-
lag.

[lapata2003] mirella lapata. 2003. probabilistic text
structuring: experiments with sentence ordering.
in
proceedings of the 41st annual meeting on associ-
ation for computational linguistics-volume 1, pages
545   552.

[li et al.2015] jiwei li, xinlei chen, eduard hovy,
visualizing and un-
arxiv preprint

and dan jurafsky.
derstanding neural models in nlp.
arxiv:1506.01066.

2015.

[lin2004] chin-yew lin.

2004. id8: a package
for automatic evaluation of summaries. in text sum-
marization branches out: proceedings of the acl-04
workshop, volume 8.

[mann and thompson1988] william c mann and san-
dra a thompson. 1988. rhetorical structure theory:
toward a functional theory of text organization. text-
interdisciplinary journal for the study of discourse,
8(3):243   281.

[mikolov et al.2013] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013. ef   cient estimation
of word representations in vector space. arxiv preprint
arxiv:1301.3781.

[mostafazadeh et al.2016] nasrin

mostafazadeh,
nathanael chambers, xiaodong he, devi parikh,
dhruv batra, lucy vanderwende, pushmeet kohli,
and james allen. 2016. a corpus and cloze evaluation
for deeper understanding of commonsense stories.
proceedings of naacl hlt, san diego, california,
june. association for computational linguistics.

[okazaki et al.2004] naoaki okazaki, yutaka matsuo,
and mitsuru ishizuka. 2004. improving chronological
sentence ordering by precedence relation. in proceed-
ings of the 20th international conference on computa-
tional linguistics, page 750.

[reiter and dale1997] ehud reiter and robert dale.
1997. building applied id86

international acm sigir conference on research and
development in information retrieval, pages 287   294.
acm.

systems. natural language engineering, 3(01):57   
87.

[schapire and singer1998] william w cohen robert e
schapire and yoram singer. 1998. learning to order
things. advances in neural information processing
systems, 10:451.

[schmaltz et al.2016] allen schmaltz, alexander m
rush, and stuart m shieber. 2016. word ordering
without syntax. arxiv preprint arxiv:1604.08633.

[simard et al.2003] patrice y simard, dave steinkraus,
and john c platt. 2003. best practices for convo-
lutional neural networks applied to visual document
analysis. in null, page 958. ieee.

[simonyan et al.2013] karen simonyan, andrea vedaldi,
and andrew zisserman.
2013. deep inside con-
volutional networks: visualising image classi   ca-
arxiv preprint
tion models and saliency maps.
arxiv:1312.6034.

[teutsch and zimand2015] jason teutsch and marius zi-
mand. 2015. on approximate decidability of minimal
programs. acm transactions on computation theory
(toct), 7(4):17.

[tillmann and ney2000] christoph tillmann and her-
mann ney. 2000. word re-ordering and dp-based
in pro-
search in id151.
ceedings of the 18th conference on computational
linguistics-volume 2, pages 850   856. association for
computational linguistics.

[van berkum et al.1999] jos ja van berkum, peter ha-
goort, and colin brown. 1999. semantic integration
in sentences and discourse: evidence from the n400.
cognitive neuroscience, journal of, 11(6):657   671.

[van dijk1985] teun a van dijk. 1985. semantic dis-
course analysis. handbook of discourse analysis,
2:103   136.

[zeiler2012] matthew d zeiler.

an adaptive learning rate method.
arxiv:1212.5701.

2012.

adadelta:
arxiv preprint

[zhang and clark2015] yue zhang and stephen clark.
2015.
discriminative syntax-based word order-
ing for text generation. computational linguistics,
41(3):503   538.

[zhang et al.2012] yue zhang, graeme blackwood, and
stephen clark. 2012. syntax-based word ordering in-
corporating a large-scale language model. in proceed-
ings of the 13th conference of the european chap-
ter of the association for computational linguistics,
pages 736   746. association for computational lin-
guistics.

[zheng et al.2007] zhaohui zheng, keke chen, gordon
sun, and hongyuan zha. 2007. a regression frame-
work for learning ranking functions using relative rel-
evance judgments. in proceedings of the 30th annual

