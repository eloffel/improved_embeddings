less is more: learning prominent and diverse topics for

data summarization

jian tang1, cheng li2, ming zhang1, qiaozhu mei2

1school of eecs, peking university, {tangjian, mzhang}@net.pku.edu.cn

2school of information, university of michigan, {lichengz, qmei}@umich.edu

6
1
0
2

 
c
e
d
1

 

 
 
]

g
l
.
s
c
[
 
 

2
v
1
2
9
9
0

.

1
1
6
1
:
v
i
x
r
a

abstract
statistical topic models e   ciently facilitate the exploration
of large-scale data sets. many models have been developed
and broadly used to summarize the semantic structure in
news, science, social media, and digital humanities. how-
ever, a common and practical objective in data exploration
tasks is not to enumerate all existing topics, but to quickly
extract representative ones that broadly cover the content
of the corpus, i.e., a few topics that serve as a good sum-
mary of the data. most existing topic models    t exactly the
same number of topics as a user speci   es, which have im-
posed an unnecessary burden to the users who have limited
prior knowledge. we instead propose new models that are
able to learn fewer but more representative topics for the
purpose of data summarization. we propose a reinforced
random walk that allows prominent topics to absorb tokens
from similar and smaller topics, thus enhances the diversity
among the top topics extracted. with this reinforced ran-
dom walk as a general process embedded in classical topic
models, we obtain diverse topic models that are able to ex-
tract the most prominent and diverse topics from data. the
id136 procedures of these diverse topic models remain as
simple and e   cient as the classical models. experimental
results demonstrate that the diverse topic models not only
discover topics that better summarize the data, but also re-
quire minimal prior knowledge of the users.

categories and subject descriptors
h.3.3 [information search and retrieval]: id111

general terms
algorithms, experimentation

keywords
data summarization, id96, diversity, random walk

1.

introduction

permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for pro   t or commercial advantage and that copies
bear this notice and the full citation on the    rst page. to copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speci   c
permission and/or a fee.
xxx xxxxx
copyright 20xx acm x-xxxxx-xx-x/xx/xx ...$10.00.

a huge amount of unstructured data has been contin-
uously generated from various online information sources
at an unprecedented speed, usually referred to as the    big
data.    a major challenge for data scientists has emerged
along with this trend, which is concerned with how to facil-
itate the understanding and exploration of the big data.

statistical topic models [4], e.g., the probabilistic latent
semantic analysis (plsa) [9] and the latent dirichlet allo-
cation (lda) [5], have been widely recognized as e   ective
tools to assist the real users in understanding and exploring
the data. this family of probabilistic models are designed
to automatically infer the hidden themes (a.k.a. topics) that
are salient in the data collection. the discovered topics can
be further utilized in other data mining tasks such as clas-
si   cation [11, 25], id91 [10, 22], id31 [18,
13], and user modeling [3].

these models commonly rely on a strong assumption that
the user knows the actual number of topics in the data. this
assumption may hold for small and restricted data sets, but
becomes impractical when the data is big and when the do-
main is open, thus has limited the usefulness of topic mod-
els in practical data exploration tasks. sophisticated treat-
ments have to be applied to topic models in order to relax
this assumption, which lead to various nonparametric [24]
and hierarchical versions of the models [12] that are much
more complicated. while these treatments successfully ad-
vanced the theoretical understanding of topic models, they
have not completely solved the problem.
in practice, the
quality of topics extracted by nonparametric models is usu-
ally compromised, while how to    nd the hierarchical struc-
ture of topics remains mysterious.

clearly, all the aforementioned challenges are due to the
practice that we wanted to extract all the topics in the data.
is this, however, a necessary practice at all? we    nd that in
many real world scenarios, the user actually wants to explore
a few most prominent topics out there, instead of enumerat-
ing all possible topics in the data. indeed, when searching
for news reports about an event, although an investigator
may still want to enumerate every aspect or detail of the
event, an ordinary web user only needs to read a few ar-
ticles that well summarize the major aspects of the event.
when exploring the literature of a new    eld, a researcher
may start with digesting several major trends of research
instead of investigating every research topic in that    eld.
that says, di   erent from investigation tasks where the con-
cern is to enumerate all the topics in a data set, in most
data exploration tasks the concern is to    nd the top k

topics that reasonably summarize, or cover the entire data
collection. this is analogical to text summarization tasks
where a summary of a limited length is created to cover
the most important points of the original document(s), or
to information retrieval tasks where a limited number of re-
sults are presented to represent the big picture of content
relevant to an information need. comparing to the conven-
tional applications of topic models, the number k in this
practice does not rely on the impractical assumption about
the user   s prior knowledge, but rather depends on the bud-
get or the personal need of the user (e.g., i have time to
digest three research topics, or i want to know the two most
representative perspectives in a debate).
in practice, this
number, k, can be much smaller than the actual (unknown)
number of topics in the data.

as a good summary of the data collection, the k topics
extracted should not only be meaningful and interpretable,
but also cover as much content of the original collection as
possible. this naturally requires the topics extracted to be
the most prominent ones in the data collection, while at the
meantime they should cover diverse aspects of the collection.
a straightforward way to generate such a summary is to
simply    t exactly k topics through a classical topic model.
however,    tting only a few topics to a big data collection is
likely to under-   t the data, making the extracted topics less
meaningful or interpretable.

an alternative approach may    rst    t a large number of
topics to the data and then pick the most important ones,
e.g., the ones with the largest sizes. these topics are likely
to be meaningful to the user. however, learning too many
topics runs the risk of over-   tting the data, making the ex-
tracted topics either too small or too similar to each other.
as a result, even the top ones may only cover partial aspects
of the collection. this challenge motivates us to investigate
new ways to extract the most representative, meaningful,
and yet non-redundant topics from the data.

in this paper, we propose a reinforced random walk among
a    social network    of the topics that allows prominent topics
to absorb tokens from similar and smaller topics. specif-
ically, during the id136 procedure, a topic network is
built to model the correlations, or interconnections among
the topics. for each word token in the collection, after it is
assigned to one of the topics, it is allowed to take a random
walk in the topic network, and possibly transit to other top-
ics. the id203 of transition from one topic to another
is initialized based on the similarity between the topics and
then reinforced by the size of the target topic, i.e., the num-
ber of tokens already assigned or transited to it. during this
process, the tokens belonging to the smaller topics are likely
to transit to the larger and similar topics. in other words,
the larger topics will absorb resource from its local neigh-
borhood (of similar but smaller topics). while the process
continues, a few prominent topics stand out from their lo-
cal neighborhood and become the most representative ones
in the topic network. at the meantime, these topics tend
to represent di   erent neighborhoods in the network, thus
enhances the diversity among the top topics (see figure 1).
we embedded this reinforced random walk process into
the classical topic models plsa and lda, and obtained
two diverse topic models divplsa (abbreviation for diverse
plsa) and divlda(abbreviation for diverse lda). the
learning procedures of the two models remain as simple and
e   cient as the classical ones but are able to learn the promi-

(a) a network of topics. nodes
are topics extracted by a clas-
sical topic model; topics with
a high similarity are connected
through an edge.

(b) network of topics after the
reinforced random walk. a few
prominent and non-redundant
topics stand out and present a
good coverage of the network.

figure 1: reinforced random walk on a    social   
network of topics.

nent and diverse topics pervading the data. top topics ex-
tracted by the diverse topic models are not only meaningful
but also serving as good summaries of the collection. exper-
imental results using four real-world datasets demonstrate
the e   ectiveness of the two diverse topic models for data
summarization, compared with classical topic models and
alternative approaches.
organization. the rest of this paper is organized as fol-
lows. section 2 discusses the related work. section 3 for-
mally de   nes the problem of id96 for data sum-
marization. section 4 introduces the reinforced random walk
and the proposed diverse topic models. the e   ectiveness of
the diverse topic models are empirically evaluated in section
5, and the study is concluded in section 6.

2. related work

our work presents a good analogy to extractive text sum-
marization [7], the goal of which is to construct a short sum-
mary of an individual document or multiple documents by
extracting the most representative sentences in the docu-
ment(s). instead of summarizing a single document or a few
documents with sentences, id96 is used to sum-
marize a large document collection with latent topics per-
vading the collection, in which the topics serve as the role
of    sentences    in document summarization. analogical to
text summarization, the coverage of the extracted topics in
the original data should be maximized and the redundancy
among the extracted topics should be minimized. in other
words, the extracted topics should be both prominent and
diverse. the diversity among the results has been recognized
to be important in not only text summarization [6], but also
many other applications including web search engines [1],
query suggestion [14], ranking in information networks [17],
and recommender systems [27].

many algorithms for enhancing the diversity of the re-
sults of a retrieval or a mining system have been proposed.
in [6], carbonell et al. proposed the maximal marginal rel-
evance (mmr) criterion for the problem of document re-
trieval, which aims to reduce the redundancy among the
retrieved documents while preserving their relevance to the
query. the mmr algorithm greedily selects the document
that has the largest marginal relevance to the query, which
is the relevance of the document to the query penalized by

                                                         035161816710121315171924891114                                             035161816710121315171924891114the largest similarity between the document and the selected
documents. in [26], a    soft    version of mmr algorithm called
grasshopper based on absorbing random walk is proposed.
in [17], mei et al. proposed a uni   ed process divrank to
generate the entire ranked list of vertices in information net-
works by balancing the prestige and diversity. the divrank
algorithm is built on top of the vertex-reinforced random
walk, in which the transition probabilities between the ver-
tices are reinforced by the number of visits to the target ver-
tex. neighboring vertices are competing resources against
each other, and    nally the id203 mass is distributed
among some diverse vertices. our proposed reinforced ran-
dom walk shares similar idea with the divrank algorithm.
instead of reinforced by the number of visits to each vertex
(topic in our case), the transition probabilities of the ran-
dom walk by the word tokens are reinforced by the size of
the topics. by doing this the larger topics would gradually
absorb tokens from the smaller similar ones and end up with
the most prominent and diverse topics in the data.

[21, 28].

another related direction is to learn diversi   ed mixture
components in area of mixture models
in most
of the existing mixture models, the mixture components
are identically drawn from a prior distribution. when the
data is over-   tted, multiple mixture components are used
to model one underlying group in the data, which results
in many redundant mixture components. in [21], petralia
and rao proposed a repulsive mixture prior which penal-
izes redundant components. zou and adams replaced the
underlying i.i.d. prior for mixture components with a deter-
minantal point process (dpp), which de   nes a id203
measure over the entire set of id203 distributions [28].
however, though these models are able to obtain more di-
verse mixture components than classical mixture models,
multiple similar mixture components are still used to model
one group in the data. in our proposed diverse topic models,
the larger mixture components are able to fully absorb the
smaller duplicated ones, and this indicates our models can
reduce the number of unnecessary mixture components.
3. problem definition

statistical topical models represent and summarize the
data through the discovery of hidden topics pervading the
data collection. each topic is formally de   ned as follows:

definition 1. a topic    is de   ned as a multinomial dis-
tribution over words in a vocabulary v , i.e. {p(w|  )}w   v .
without loss of generality, we assume there are k topics in
total in a given data collection, where k is unknown.

in the practice of data exploration, instead of enumerat-
ing all the topics, a more realistic need is to extract a given
number of representative or prominent topics in the data.
based on these representative topics, users can have a quick
understanding of the semantic structure of the data collec-
tion. therefore, it is desirable to be able to extract a list
of topics ranked according to their importance in the data.
formally, we de   ne the problem as follows:

definition 2. given a data collection d, the problem
of learning top-k topics aims to infer a list of topics
{  j}j=1,      k ranked according to their prominence in d,
where k is a number speci   ed by the user.

note that di   erent from conventional applications of topic
modeling, the number k here is speci   ed completely based

on the user   s need instead of the prior knowledge of the data,
and can be substantially smaller than the true number of
topics k (k << k), which is unknown. to extract top-k
representative topics in the data, a straightforward way is
to apply a classical topic model (e.g., lda) directly to    t
exactly k topics into the data (i.e., set k = k).
in this
case, one would expect the inferred topics to be the most
representative topics in the data. however,    tting only a few
topics to the data usually results in topics that are actually a
mixture of multiple topics, the coherence or interpretability
of which is compromised.
as it is di   cult for a user to    nd an appropriate number
of topics k, an alternative way is to    t the data with a large
number of topics, which usually yields semantically coherent
topics. these topics can then be ranked by the proportions
of the data they cover (e.g., number of word tokens assigned
to each topic). although these topics are likely to be co-
herence and meaningful, a large k may over-   t the data,
returning many duplicated topics or smallish topics. as a
result, even the top-ranked topics may have a small cover-
age of the original data, due to the fact that they are either
too small or redundant. to provide a good summary of the
data, it is desirable that the selected topics are both coher-
ent and interpretable, and have a high coverage of the data.
formally, we have

definition 3. given a data collection d and a given
integer k, the problem of topic summarization aims to
   nd a set of k topics {  j}j=1,      k such that every   j is
a coherent topic and the set of topics {  j} cover as much
information in d as possible.

apparently, neither of the simple approaches above is suit-
able for topic summarization. in next section, we introduce
a novel approach, diverse id96, to infer prominent
and diverse topics which better summarize the data.
4. diverse id96

in most existing topic models or mixture models, the top-
ics or mixture components are generally i.i.d. according to a
prior distribution. no restriction is placed upon the entire
set of topics. this becomes problematic when the data is
over-   tted, in which case each underlying theme of original
data may be represented by multiple similar topics or be
split into topics with a smaller granularity.

to tackle this problem, the diversity, or redundancy among
topics must be considered. recent work replaces the inde-
pendent assumption with priors favoring diversi   ed topics
(e.g., [21, 28]), usually through a id173 term over
the entire set of topics. a set that contains similar topics
will be penalized. although these treatments can infer more
diverse topics than the classical models, they still make the
same assumption on the number of topics, and multiple top-
ics are still being used to represent each underlying theme.
a more reasonable way is to merge similar topics into big-
ger ones, so that the top-ranked topics become more promi-
nent and also more diverse. in this paper we propose to let
the prominent topics merge with or absorb the smaller and
similar topics. naturally, we need a process under which
topics can communicate with each other and compete for
word tokens. if a topic absorbs all the word tokens belong-
ing to another topic, the latter topic is completely absorbed
by the former. in the following, we introduce a reinforced
random walk as such a process.

4.1 reinforced random walk

to model how the topics are competing word tokens from
each other, the relationships among the topics must be con-
sidered. speci   cally, we introduce a    social    network of
topics, or a topic network, which is an undirected graph
g = (v, e), where v is the set of topics and there is an edge
e(i, j)     e between each pair of topics (i, j). the weight of
e(i, j), w(i, j) is de   ned as the cosine similarity between the
word distributions   i and   j, i.e.,

w(i, j) =

.

(1)

  i      j

||  i||2    ||  j||2

we formulate the absorbing process among the topics as
a random walk process by the word tokens on the topic net-
work. speci   cally, if one token belonging to one topic tran-
sits to another topic, we can say a token of this topic is
absorbed by the other topic. if all the tokens belonging to
one topic are absorbed by other topics, this topic dies out.

as we want the larger topics to gradually absorb the smaller
and similar topics, the tokens belonging to the smaller topics
should be more likely to transit to the larger ones. this can
be achieved through reinforcing the transition id203
among the topics by the size of the topics, i.e. the number
of tokens belonging to the topic. speci   cally, the transition
id203 p(i, j) from topic i to j is de   ned as

p(i, j) =

p0(i, j)n   
j

di

,

(2)

where p0(i, j) is the    organic    transition id203 among
the topics, nj is the size of the topics j,    is the parameter
used to control the reinforcement intensity by the topic size,
and the normalizing factor di is calculate as

di =

p0(i, j)n   
j .

(3)

(cid:88)

j

the    organic    transition probabilities p0(i, j) among the
topics can be calculated based on the similarity between the
topics. meanwhile, it is also reasonable to assume that each
token can also have some id203 to stay on the current
topic instead of transiting to its neighbors.
in this case,
tokens belonging to the large topics can still stay on the
topic itself. speci   cally, we de   ne the p0(i, j) as follows:

p0(i, j) =

(cid:80)
   w(i,j)
j(cid:48) w(i,j(cid:48)) ,
1       ,

if j (cid:54)= i
if j = i

,

(4)

(cid:40)

where    is the id203 of transiting to neighboring topics.
the above random walk process is related to the stochastic
process vertex-reinforced random walk [20] and divrank [17],
in both of which the transition probabilities among the ver-
tices in the network are reinforced by the number of visits
to the vertices. in these processes, the neighboring vertices
are actually competing resources from each other. the re-
inforcement leads to the rich-get-richer phenomena and    -
nally the resources are distributed across the prominent and
diverse vertices in the network.

we embed the above random walk process into the infer-
ence processes of plsa and lda, and proposed two diverse
topic models divplsa and divlda. during the id136
processes, the topic assignment for each token is    rst cal-
culated based on the em algorithm (for plsa) or gibbs
sampling (for lda), and then a new topic assignment is
obtained based on the reinforced random walk.

4.2 diverse topic models
4.2.1 divplsa
the plsa model assumes each document is a mixture of
topics with di   erent proportions. given a collection d, the
log-likelihood of d under plsa assumption is calculated as:

(cid:88)

(cid:88)

k(cid:88)

l(d) =

n(d, w) log

  dk  kw,

(5)

d

w

k=1

where n(d, w) is the frequency of word w in document d,   dk
is the id203 of topic k in d,   kw is the id203 of
word w being generated by topic k and k is the number of
topics speci   ed. the parameters of the model, i.e. {  dk},
and {  kw}, are estimated by maximizing the log-likelihood.
an em algorithm is generally applied to solve the problem.
in the e-step, it calculates the posterior distribution of the
topic assignment of each token, i.e. p(z|d, w) based on the
current model parameters.
in the m-step, it updates the
model parameters based on the posterior id203 of topic
assignments calculated in the e-step.

we embedded the reinforced random walk process into
the e-step. for each token w, we    rst obtain the poste-
rior distribution of topic assignments p(z|d, w) (eqn. (6)).
then one-step reinforced random walk on the topic network
is conducted to determine whether this topic is absorbed
by other topics or staying on the current topic, which ob-
tains the new topic assignments p(  z|d, w) (eqn. (7)). the
transition probabilities among the topics p = {p(i, j)} can
be periodically updated in the m-step. we summarize the
detailed updating equations as below:

e-step:

m-step:

p(z = k|d, w) =

(cid:80)k

  dk  kw

k(cid:48)=1   dk(cid:48)   k(cid:48)w

k(cid:88)

p(  z =   k|d, w) =

p(z = k|d, w)p(k,   k)

  d  k    (cid:88)
    kw    (cid:88)
(cid:88)

w

d

n  k =

k=1

n(d, w)p(  z =   k|d, w)

n(d, w)p(  z =   k|d, w)

n(d, w)p(  z =   k|d, w)

(6)

(7)

(8)

(9)

(10)

d,w

in alg. 1, we present the detailed learning procedure of
the divplsa model. users still need to specify the starting
number of topics k, which can be strategically set very large
and let the data be over-   tted. during the learning process,
the small topics will be eventually absorbed by the larger
ones, i.e. the sizes of those topics become 0. we can monitor
the number of active topics (topics whose sizes are greater
than 0) k    during the process. when k    converges, the
whole algorithm stops.
4.2.2 divlda
the lda model is a bayesian treatment of the plsa
by placing the dirichlet priors upon both the document-
topic and topic-word distributions, which results the model
is computationally intractable. collapsed id150 [8]
algorithm is widely used for the id136 due to its simplic-
ity and e   ectiveness. in id150, each dimension of

algorithm 1: the divplsa model
input: training data d, the starting number of topics
k, the parameter to control the reinforcement intensity
by the topic size   ;
output: number of diverse topics k   , the word
distributions of topics {  k}k=1,...,k    ;
initialization: randomly initialize the document-topic
and topic-word distributions;
calculate the sizes of the topics and the transition
probabilities among the topics according to (2);
while k    no convergence do

e-step: for each word w of each document d in
{1, . . . ,|d|}

    calculate the posterior id203 of topic

assignments p(z|d, w) according to (6);

    conduct one-step reinforced random walk on

the topic network according to (7);

m-step:

    update document-topic, topic-word distributions

and topic sizes according to (8), (9) and (10);

    update the transition id203
among the topics according to (2);

    calculate the number of active topics k   ;

algorithm 2: the divlda model
input: training data d, the starting number of topics
k, the parameter to control the reinforcement intensity
by the topic size   , the total number of id150
iterations m ;
output: number of diverse topics k   , the word
distributions of topics {  k}k=1,...,k    ;
initialization: randomly sample the topic assignments
for all the word tokens.;
calculate the sizes of the topics and the transition
probabilities among the topics according to (2);
while iter     m do

for each token w of each document d in {1, . . . ,|d|}

    for the current assignment   k of token w, decrement

counts and sums:         nd  k,         n  kw,         n  k;

    sample the topic assignments zi = k for the token

according to (11);

    conduct one-step reinforced random walk on the topic

network according to (12);

    for the latest assignment   k, increment counts and

sums: + + nd  k, + + n  kw,+ + n  k;

update the transition id203 among the topics
according to (2);
optimize the parameter (cid:126)   according to [5];

end

end

the joint distribution is sampled alternatively based on the
distribution conditioned on all the other variables. specif-
ically, in lda the conditional distribution of the topic as-
signment zi associated with word wi is calculated via:

p(zi = k|wi = w, w   i, z   i)     (nd,k +   k)

nk,w +   
nk + v   

.

(11)

z   i represents the topic assignments of all the words exclud-
ing the current word. nd,k is the number of words assigned
to the kth topic in document d; nk,w is the number of times
that the word w is assigned to the kth topic; and nk is the
number of tokens assigned to the kth topic, i.e., the size of
topic k. for all these counts, the current token is excluded.
in divlda, instead of calculating the expectation of the
topic assignments as done in the divplsa, sampling is used
to obtain the topic assignments. for each token, we    rst
sample the topic assignment zi based on equation (11), and
then conduct one-step reinforced random walk starting from
zi to obtain a new topic assignment   zi according to

p(  zi =   k|zi = k) = p(k,   k)

(12)

the    nal document-topic and topic-word distributions can

be estimated via:

(cid:80)

ndk +   k
k(cid:48) (ndk(cid:48) +   k(cid:48) )

  dk =

  kw =

nkw +   
nk + v   

(13)

(14)

the detailed id136 process is summarized in alg. 2.

4.3 discussion

we discuss some practical issues of the two models.

convergence. in the above two models, after plugging in
the reinforced random walk process within the em or gibbs
sampling process, there are no explicit objective functions
any more. we empirically prove that both the number of
active topics and the data likelihood will converge (see fig-
ure 2(d) and 2(h)). we leave the theoretical justi   cation of
the convergence as the future work.
how to set the parameter   ? the parameter    controls
the transition id203 to the neighbor topics or staying
on itself. it takes similar e   ect as the step size in the gradient
descent method, and hence in practice a small    (e.g., 0.1)
can be used.
how to set the parameters    and k? we empirically
show that the performance of the two models are not sen-
sitive to the parameters    and k (see figure 5 and 6). in
practice, k can be set to be very large to let the data over-
   tted.    can be usually set within [1, 2] for divplsa and
[0.6, 1.5] for divlda.
scalability. both divplsa and divlda can be easily
scaled by making use of existing large-scale id96
techniques. the e-step of divplsa can be parallelized by
assigning the documents to di   erent processors or nodes. a
scaled version of divlda can be built on top of the existing
large scale lda model, e.g., the yahoo-lda model in [2].

5. experiment

in this section, we move forward to evaluate the e   ective-
ness of our proposed diverse topic models. we evaluate the
performances on four real-world data sets.

5.1 datasets
4conf. we start with a small data set, the 4conf data
set as in [16]. the data set is constructed from papers pub-
lished in four conferences including kdd, sigir, nips, and
www. every document corresponds to an author by ag-
gregating the titles of the author   s papers. stop words and
words appearing in less than 10 documents are removed.
this small data set allows us to interpret the topics intu-
itively and visually.
20ng. this is the widely used 20 newsgroup data set in
id111. stop words and words appearing in less than
20 documents are removed. we sample 1,000 documents
from the set as a holdout data set.
wikipedia. this includes 10,000 articles randomly sam-
pled from 4,636,797 wikipedia articles in english. stop
words and words appearing in less than 100 documents are
removed. we also hold out a sample of 1,000 articles.
dblp. the larger data set consists of all papers with ab-
stracts in the computer science bibliography as in [23]. stop
words and words appearing in less than 50 documents are
removed.

table 1 summarizes the statistics of all the data sets.

table 1: statistics of the data sets

dataset
4conf
20ng

8,486
11,267
wikipedia 10,000
529,434

dblp

# train # holdout

vocabulary # tokens

   

1,000
1,000

   

1,672
7,642

196,665
25,404

80,642

1,056,012
3,006,817
36,899,908

5.2 id74

to provide a good summary of the data, the topics need
to be highly coherent and also provide a high coverage of the
information in the original data. we introduce metrics to
evaluate the semantic coherence and the coverage of the top-
ics respectively. the quality of a topic is measured through
the semantic coherence of the word distribution while the
information coverage of a set of topics is measured through
their predictive performance on the holdout data set, using
the well-adopted perplexity metric.

k(cid:88)

k=1

topic semantic coherence. we measure the semantic
quality of the topics through the semantic coherence of the
topics.
in [19], newman et al. measures the semantic co-
herence of each topic as the average point-wise mutual in-
formation (pmi) of every word pair among the top-ranked
words in the topic. speci   cally, the overall semantic topic
coherence of a set of topics    = {  k}k

k=1 is calculated as:

pmi(  ) =

1
k

2

n (n     1)

log

p(wki, wkj )
p(wki)p(wkj )

,

(15)

1   i<j   n

where wk = (wk1, . . . , wkn ) are the words ranked at the
top n positions in topic   k. p(wki, wki) is the id203
that the pair of words co-occur in the same document while
p(wki) is id203 of a word wki appearing in a single
document. the top ranked 20 (n = 20) words are used in
our experiments.

to calculate the pmi, generally a large dataset has to be
used. the entire 20ng data set (12,267 documents) and the

(cid:88)

entire english wikipedia (4.6 million documents) are used in
calculating the pmi for the experiments on the 20ng data
set and on the wikipedia data set, respectively.

perplexity. we measure the information coverage of a set
of topics by perplexity, which measures the predictive perfor-
mance of these topics on the holdout data set. speci   cally,
each document wi in the held-out data set is split into two
parts wi = (wi1, wi2). the likelihood of the second part
of the documents wi2 (20% of the document) is calculated
based on the training data d and the    rst 80% words of the
document wi1. speci   cally, we have

p erplexity = exp(cid:8)    

(cid:80)
i log p(wi2|wi1, dtrain)

(cid:9)

(cid:80)

i |wi2|
5.3 algorithms for comparison

(16)

we compare the following algorithms for selecting top-k

topics for data summarization.

    plsa/lda. the classical plsa or lda model is

directly utilized to learn exactly k topics.

    plsa/lda-topk. we    rst train plsa/lda with
a large number of k topics, and then pick top-k topics
with the largest sizes.

    plsa/lda-mmr-topk. plsa/lda is used to
train a large number of k topics. then the mmr
algorithm [6] is used to select top-k topics from the
k topics. although the mmr algorithm is proposed
in a query-dependent setting, we adapt it to our sce-
nario. the relevance between the query and each topic
is measured as the coverage of this topic in the whole
data set, and the similarity among each pair of topics
is calculated as the cosine similarity of the word distri-
butions. the best results are reported by empirically
tuning the parameter    in the mmr algorithm.

    plsa/lda-divrank-topk. plsa/lda is used
to train a large number of k topics, and then the di-
vrank algorithm [17] is used to select top-k topics. in
the divrank algorithm, we treat the proportions of the
topics as the preference vector, and the weight between
each pair of topics is calculated as the cosine similarity
of the corresponding word distributions. the best re-
sults are reported by empirically tuning the parameter
   and    in the divrank algorithm.

    divplsa/divlda-topk. divplsa/divlda is ap-
plied on the data set and then the top-k topics with
the largest sizes are selected.

5.4 learning behavior of diverse topic mod-

els

we start the experimental results by investigating the
learning behavior of our proposed diverse topic models. we
take the divplsa model as an example. similar behavior of
the divlda is observed and hence we do not include here.
in figure 2, we show how the topic assignments of the docu-
ments and words change along the iterations in the 4conf
dataset. we    rst build a document-word bipartite network
with the edge weight as the word frequency in the document.
a drl [15] layout algorithm is applied on this bipartite net-
work to calculate the two-dimensional coordinates for both

(a) iteration 1

(b) iteration 20

(c) iteration 50

(d) #topics v.s. #iterations

(e) iteration 70

(f) iteration 90

(g) iteration 200

(h) likelihood v.s. #itera-
tions

figure 2: the learning behavior of divplsa in 4conf dataset.

the documents and words. in figure 2(a), each data point
represents a document, and the most popular 100 words in
the dataset are shown. di   erent colors represent di   erent
topics, and each document or word is assigned to its most
probable topic. the size of each word is determined based
on the id203 of the word in the assigned topic. we ex-
pect that documents and words belonging to the same topics
(colors) are likely to lie within a dense area.

we start with 20 topics in the divplsa model with    =
0.1,    = 1.9. in the    rst 50 em iterations, the same em iter-
ation for plsa is conducted and the reinforced random walk
process kicks in after the 50th iteration. in the    rst em iter-
ation (figure 2(a)), the topic assignments of all the tokens in
the data set are randomly initialized and we can see that the
colors in the entire plot are totally mixed. there is no dense
area taking the same color. as time goes on, dense area
with the same color gradually emerges, e.g., the 20th itera-
tion (figure 2(b)). the results become better when reaching
the 50th iteration. we can see the emergence of topics such
as    information retrieval   ,    web semantic   . however, we can
see that too many topics are    tted into the data, and many
of them are small and similar to each other. no clear topic
semantic structure is discovered. after the 50th iteration,
the reinforced random walk process kicks in and the smaller
topics are gradually absorbed by the larger and similar ones.
we can see better topical structure in 70th iterations, in
which fewer than 20 topics are active. in the 200th itera-
tion, the whole procedure stops and    ve active topics remain
(the other 15 topics are fully absorbed) including    informa-
tion retrieval   ,    web   ,    data mining   ,    neural network    and
   learning, bayesian   . recall that the dataset is collected
from the four conferences    sigir   ,    www   ,    kdd    and
   nips   , the    ve topics are a good summary of the original
data. it is also interesting to notice that two diverse topics

   neural network    and    learning, bayesian    are discovered in
the    nips    conference, which indicates there are two dis-
tinctive research communities in the machine learning area.
the    nal list of topics is represented in table 2.

table 2: topics extracted by divplsa

top-ranked words of each topic

learning classi   cation models id91 algorithm support bayesian

retrieval information text query system document evaluation

neural networks network model recognition visual learning

web search semantic xml extraction analysis content

data mining knowledge discovery large databases rules

in figure 2(d), we show how the number of active topics
changes along the iterations. in the    rst 50 iterations, the
number of active topics remains 20 as the random walk pro-
cess has not kicked in yet. the number of active topics be-
gins to decrease in around the 70th iterations as the smaller
topics are absorbed by the larger and similar ones, and dra-
matically decreases to 5 in around the 90th iterations. the
number of active topics converges after 90 iterations. during
the process, we can see that the absorbing process converges
quite quickly, taking around 20 em iterations.

figure 2(h) shows how the likelihood of the training data
changes over iterations. the likelihood keeps increasing at
   rst and starts to decrease in around the 70th iteration,
as the number of active topics decreases.
in around the
90 iterations, the likelihood stops decreasing as the number
of active topics converges. after this, the likelihood keeps
increasing and converges in around the 150th iterations.

overall, we can see that when the data is over-   tted, the
inferred topics by the classical topic models tend to be small
and duplicated, which is not good for data summarization
purpose. the reinforced random walk is able to merge the

playplayfatemah_a._alqallaflearningwebretrievaldatainformationneuralminingnetworksmodelsearchmodelsclassi   cationanalysissystemid91textnetworkapproachquerybaseddocumentdetectionsemanticsystemsalgorithmlargeef   cientevaluationknowledgeautomaticmodelingrecognitionalgorithmsprobabilisticadaptivediscoverysupportframeworkimagelanguagebayesianapplicationselectionvisualuservectorfeaturefastdynamicstructuretopic distributionhighcharts.com1: 5.2 %2: 5.3 %3: 4.9 %4: 5.1 %5: 5.1 %6: 5.2 %7: 4.7 %8: 5.1 %9: 4.9 %10: 4.9 %11: 5.0 %12: 5.0 %13: 4.8 %14: 4.8 %15: 5.1 %16: 4.8 %17: 5.1 %18: 5.1 %19: 5.1 %20: 4.9 %divplsahttp://localhost:8080/docnetwork/divplsa.html1 of 18/18/13 9:42 amplayplaylearningwebretrievaldatainformationneuralminingnetworksmodelsearchmodelsclassi   cationanalysissystemid91textnetworkapproachquerybaseddocumentdetectionsemanticsystemsalgorithmlargeef   cientevaluationknowledgeautomaticmodelingrecognitionalgorithmsprobabilisticadaptivediscoverysupportframeworkimagelanguagebayesianapplicationselectionvisualuservectorfeaturefastdynamicstructuretopic distributionhighcharts.com1: 6.5 %2: 4.5 %3: 4.9 %4: 3.9 %5: 4.6 %6: 3.9 %7: 5.4 %8: 4.6 %9: 4.9 %10: 4.8 %11: 4.6 %12: 5.4 %13: 4.7 %14: 4.1 %15: 6.0 %16: 5.9 %17: 5.9 %18: 7.0 %19: 3.4 %20: 5.0 %divplsahttp://localhost:8080/docnetwork/divplsa.html1 of 18/18/13 12:52 pmplayplaylearningwebretrievaldatainformationneuralminingnetworksmodelsearchmodelsclassi   cationanalysissystemid91textnetworkapproachquerybaseddocumentdetectionsemanticsystemsalgorithmlargeef   cientevaluationknowledgeautomaticmodelingrecognitionalgorithmsprobabilisticadaptivediscoverysupportframeworkimagelanguagebayesianapplicationselectionvisualuservectorfeaturefastdynamicstructuretopic distributionhighcharts.com1: 6.8 %2: 4.2 %3: 4.6 %4: 4.0 %5: 4.8 %6: 3.7 %7: 5.2 %8: 4.8 %9: 4.6 %10: 5.2 %11: 4.3 %12: 5.7 %13: 4.6 %14: 3.9 %15: 6.2 %16: 5.8 %17: 6.1 %18: 7.7 %19: 3.4 %20: 4.4 %divplsahttp://localhost:8080/docnetwork/divplsa.html1 of 18/18/13 12:52 pm0501001502005101520#iterations#topicsplayplaylearningwebretrievaldatainformationneuralminingnetworksmodelsearchmodelsclassi   cationanalysissystemid91textnetworkapproachquerybaseddocumentdetectionsemanticsystemsalgorithmlargeef   cientevaluationknowledgeautomaticmodelingrecognitionalgorithmsprobabilisticadaptivediscoverysupportframeworkimagelanguagebayesianapplicationselectionvisualuservectorfeaturefastdynamicstructuretopic distributionhighcharts.com1: 7.2 %2: 1.0 %3: 3.2 %4: 0.7 %5: 2.1 %6: 0.3 %7: 5.7 %8: 2.3 %9: 1.4 %10: 4.2 %11: 1.1 %12: 9.4 %13: 0.2 %14: 0.3 %15: 3.4 %16: 16.7 %17: 21.2 %18: 17.8 %20: 1.8 %divplsahttp://localhost:8080/docnetwork/divplsa.html1 of 18/18/13 12:53 pmplayplaylearningwebretrievaldatainformationneuralminingnetworksmodelsearchmodelsclassi   cationanalysissystemid91textnetworkapproachquerybaseddocumentdetectionsemanticsystemsalgorithmlargeef   cientevaluationknowledgeautomaticmodelingrecognitionalgorithmsprobabilisticadaptivediscoverysupportframeworkimagelanguagebayesianapplicationselectionvisualuservectorfeaturefastdynamicstructuretopic distributionhighcharts.com1: 5.2 %7: 0.6 %12: 14.7 %16: 18.9 %17: 37.7 %18: 22.9 %divplsahttp://localhost:8080/docnetwork/divplsa.html1 of 18/18/13 9:44 amplayplaylearningwebretrievaldatainformationneuralminingnetworksmodelsearchmodelsclassi   cationanalysissystemid91textnetworkapproachquerybaseddocumentdetectionsemanticsystemsalgorithmlargeef   cientevaluationknowledgeautomaticmodelingrecognitionalgorithmsprobabilisticadaptivediscoverysupportframeworkimagelanguagebayesianapplicationselectionvisualuservectorfeaturefastdynamicstructuretopic distributionhighcharts.com1: 14.2 %12: 16.2 %16: 20.5 %17: 26.0 %18: 23.0 %divplsahttp://localhost:8080/docnetwork/divplsa.html1 of 18/18/13 9:45 am050100150200   560000   540000   520000   500000   480000   460000#iterationslikelihoodplsa

algorithm

top-2 topics

god jesus church christ bible lord man faith

people writes article don god time good apr

writes system article    le mail don windows key

58.63%
41.37%
2.98%
2.95%
12.48%
12.19%
(c) comparison of the topics learned by di   erent methods.

server mit sun motif source version library tar
god people jesus christian bible don life time

   le program window    les ftp image version server

plsa-
topk
divplsa-
topk

pmi proportion
0.499
0.449
1.845
1.905
1.177
1.325

(a) perplexity

(b) semantic coherence

figure 3: performances on the 20ng dataset.

similar topics and end up with some large diverse topics.
both the number of topics and the likelihood of the data
will    nally converge.

5.5 evaluation for data summarization

next, we move forward to evaluate the performances of
di   erent algorithms for the task of topic summarization,
which aims to learn and select top-k topics for summarizing
the data. the performances are evaluated in terms of the
semantic coherence and information coverage of the topics,
which are measured by pmi and perplexity respectively.

the results on the 20ng dataset are presented in fig-
ure 3. figure 3(a) presents the information coverage of the
topics learned by various algorithms in terms of perplexity.
the lower the perplexity, the larger the information cover-
age. overall, the more topics are selected, the higher the
information coverage. all the methods except plsa/lda
are trained with 50 topics (the starting number of topics
is also 50 for divplsa/divlda). the plsa-topk algo-
rithm, which selects the largest top-k topics in the 50 topics
trained by the plsa model, achieves the worst performance.
in this case, the data is over-   tted and the inferred topics
are small and similar to each other, which would have a low
information coverage of the data. both plsa-mmr-topk
and plsa-divrank-topk outperform plsa-topk by se-
lecting the non-redundant largest top-k topics. lda-topk
achieves better performance than plsa-topk, which may
due to the fact that the largest topics learned by lda is
larger than the ones learned by plsa through an examina-
tion of the sizes of the largest topics learned by the two mod-
els. similarly, lda-mmr-topk and lda-divrank-topk
further improve the information coverage by taking into the
diversity among the selected topics into consideration.

as there are 20 categories in the 20ng dataset, it is rea-
sonable to think there are 20 topics in the dataset. there-
fore, we trained plsa/lda with 20 topics and the topics
with the largest size are selected (marked as plsa/lda-
20-topk). we can see that the topics selected from 20 top-
ics (plsa/lda-20-topk) has a high information coverage
than from 50 topics (plsa/lda-topk). this indicates that
if we can have a good estimation of the appropriate number
of topics in the data, it is likely to infer better topics with
a high information coverage. our divplsa and divlda
models (also starting from 50 topics) outperform all these
models by learning the most prominent and diverse topics
without worrying about estimating the appropriate number
of topics. though starting from a large number of topics,

the divplsa/divlda model is able to merge those simi-
lar topics and end up with the diverse ones. we are also
surprised to see that deploying plsa/lda to train just
k topics (denoted as plsa/lda) obtains the best perfor-
mance in terms of information coverage. however, though
the discovered topics have a high information coverage in
terms of perplexity, the semantic coherence of the topics is
pretty low and the topics are not interpretable to users.

in figure 3(b), we compare the semantic coherence of the
topics inferred by di   erent algorithms. though the k top-
ics directly learned by plsa/lda have a high information
coverage, the semantic coherence of the topics is the worst.
this is because the data is under-   tted and the inferred
topics are a mixture of multiple topics. therefore, the top-
ics are more likely to be the background topic of the whole
data collection (see the    rst row in figure 3(c)) and do not
have any semantic information. the semantic quality of the
top-k topics selected from 50 topics learned by plsa us-
ing various ways (plsa-topk, plsa-mmr-topk, plsa-
divrank-topk) are the best among all the models. this
may due to the granularity of these topics is quite small1,
which can be examined from the second row in figure 3(c).
the topics learned by the divplsa and divlda have a
reasonable good semantic quality meanwhile have a high in-
formation coverage, both of which are desirable for a good
summarization.

figure 4 compares the performance on the wikipedia
data set. to select an appropriate number of topics within
the data, we vary di   erent numbers of topics and choose
the best one 100 based on the predictive performance on
the holdout data set. all the models (except plsa and
lda directly trained with a few topics) are trained with 100
topics. similar behavior of di   erent models can be observed
in the data set as in the 20ng data set.

to summarize, our divplsa and divlda models give a
good summarization of the data by presenting top-k topics
with a high information coverage and a reasonable good se-
mantic quality. the k topics directly trained with plsa/lda
are not interpretable and cannot convey useful semantic in-
formation to users at all. the top-k topics selected from
a large number of topics trained by plsa/lda have good
semantic quality but low information coverage.
5.6 parameter sensitivity

in this part, we investigate the performance sensitivity of

1topics with a small granularity tend to yield a larger pmi

24681002000060000topkperplexityllllllllllllplsaldaplsa   topklda   topkplsa   20   topklda   20   topkplsa   mmr   topklda   mmr   topkplsa   divrank   topklda   divrank   topkdivplsa   topkdivlda   topk2468100.60.81.01.21.41.6topkpmillllllllllalgorithm

plsa

top-2 topics

user overlap university world school calculated time war
template article page wikipedia made delete image user

wikipedia http php user page article talk edit

album music song band released rock records single

plsa-
topk
divplsa-
topk
(c) comparison of the topics learned by di   erent methods.

   lm series show episode time man television movie
music album song band released single rock songs

pmi proportion
0.295
0.862
1.590
1.280
1.051
1.563

52.70%
47.30%
2.57%
2.40%
8.51%
5.23%

(a) perplexity

(b) semantic coherence

figure 4: performances on the wikipedia dataset.

(a) perplexity (divplsa)

(b) semantic coherence (di-
vplsa)

(c) perplexity (divlda)

(d) semantic coherence (di-
vlda)

figure 5: parameter sensitivity w.r.t.    on the 20ng dataset.

(a) perplexithy (divplsa) (b) semantic coherence (di-

vplsa)

(c) perplexity (divlda)

(d) semantic coherence (di-
vlda)

figure 6: parameter sensitivity w.r.t. k on the 20ng dataset.

divplsa and divlda for data summarization w.r.t the pa-
rameters    and k. the parameter    controls the intensity
of reinforcement by topic size on the transition probabili-
ties among the topics. the larger    is, the more likely the
larger topics will absorb the smaller topics. therefore, the
model will return fewer but larger topics with a larger   ,
which tend to have a higher information coverage and lower
semantic coherence. this can be observed from the results
shown in figure 5, in which the performances w.r.t    for
both divplsa and divlda are presented. we can see that
overall the performances are not sensitive to   . note that
divlda tends to use a smaller    than divplsa, this may
due to the id150 algorithm and the use of hyper-
parameter (cid:126)  , which is periodically optimized.
the performance sensitivities w.r.t k for both divplsa

and divlda in terms of perplexity and semantic coherence
are presented in figure 6. we can see that the performances
are also not sensitive to k.
5.7 summarization for dblp

finally, we give a summarization for the entire dblp
dataset with our diverse topic models. the divplsa model
is applied on the dataset with k = 100 and    = 1.1, and
end up with 36 topics. table 3 shows the most prominent
10 topics in the dataset.

6. conclusion and future work

in this paper, we proposed two diverse topic models di-
vplsa and divlda to learn the prominent and diverse

2468100e+002e+054e+056e+058e+05topkperplexityllllllllllllplsaldaplsa   topklda   topkplsa   20   topklda   20   topkplsa   mmr   topklda   mmr   topkplsa   divrank   topklda   divrank   topkdivplsa   topkdivlda   topk2468100.60.81.01.21.4topkpmilllllllllllllllll1.01.11.21.31.41.51.62000400060008000gperplexityltop   2top   4top   6top   8top   10lllllll1.01.11.21.31.41.51.60.60.81.01.21.4gpmiltop   2top   4top   6top   8top   10lllll0.60.70.80.91.02000400060008000gperplexityltop   2top   4top   6top   8top   10lllll0.60.70.80.91.00.60.81.01.21.4gpmiltop   2top   4top   6top   8top   10lllllll404550556065702000400060008000kperplexityltop   2top   4top   6top   8top   10lllllll404550556065700.60.81.01.21.4kpmiltop   2top   4top   6top   8top   10lllllll404550556065702000400060008000kperplexityltop   2top   4top   6top   8top   10lllllll404550556065700.60.81.01.21.4kpmiltop   2top   4top   6top   8top   10human label
   methodology   

   empirical studies   

   algorithm complexity   

   id99   
   computer-assisted learning   

   machine learning   
   parallel computing   

   virtual systems   

table 3: topic summarization for the dblp dataset.

top-ranked words

proportion

method proposed algorithm results approach based paper methods
study analysis results evaluation studies performance quality case
number algorithm time complexity bound show bounds polynomial

model models framework knowledge modeling approach domain representation

computer learning research students project technology science social

learning classi   cation model id91 neural training models statistical

performance memory parallel hardware implementation applications processor processors

system user users virtual interface interaction environment information

7.69%
4.55%
4.17%
3.98%
3.81%
3.65%
3.63%
3.46%

topics for data summarization. the two models are built
on top of a reinforced random walk on the topic network,
which allows the prominent topics to absorb tokens from
smaller and similar topics and improves the diversity among
the extracted topics. the id136 procedures for the two
models remain as simple and e   cient as the classical ones
and are appropriate for big data analysis. experiments on
four real-world datasets prove the e   ectiveness of the two
models for data summarization.

the future work are two-fold. first, we plan to inves-
tigate the theoretical convergence of the two diverse topic
models. currently the convergence of the two models is em-
pirically proved through the likelihood of the training data
and the number of active topics. we believe there is an
underlying objective function which tradeo   s between the
data likelihood and the diversity among the topics. second,
we plan to apply the reinforced random walk into more sce-
narios such as unsupervised id91, which will result in
prominent and diverse clusters in the data.

7. references
[1] r. agrawal, s. gollapudi, a. halverson, and s. ieong.

diversifying search results. in proceedings of the second
acm international conference on web search and data
mining, pages 5   14. acm, 2009.

[2] a. ahmed, m. aly, j. gonzalez, s. narayanamurthy, and

a. j. smola. scalable id136 in latent variable models. in
proceedings of the    fth acm international conference on
web search and data mining, pages 123   132. acm, 2012.

[3] a. ahmed, y. low, m. aly, v. josifovski, and a. j. smola.
scalable distributed id136 of dynamic user interests for
behavioral targeting. in kdd, pages 114   122, 2011.

[4] d. m. blei. probabilistic topic models. communications of

the acm, 55(4):77   84, 2012.

[5] d. m. blei, a. y. ng, and m. i. jordan. latent dirichlet

allocation. the journal of machine learning research,
3:993   1022, 2003.

[6] j. carbonell and j. goldstein. the use of mmr,

diversity-based reranking for reordering documents and
producing summaries. in proceedings of the 21st annual
international acm sigir conference on research and
development in information retrieval, pages 335   336. acm,
1998.

[7] g. erkan and d. r. radev. lexrank: graph-based lexical

centrality as salience in text summarization. j. artif. intell.
res.(jair), 22(1):457   479, 2004.

[8] t. l. gri   ths and m. steyvers. finding scienti   c topics.

proceedings of the national academy of sciences of the
united states of america, 101(suppl 1):5228   5235, 2004.

[9] t. hofmann. probabilistic latent semantic analysis. in

proceedings of the fifteenth conference on uncertainty in
arti   cial intelligence, pages 289   296. morgan kaufmann
publishers inc., 1999.

[10] a. karandikar. id91 short status messages: a topic

model based approach. phd thesis, university of maryland,

2010.

[11] s. lacoste-julien, f. sha, and m. i. jordan. disclda:

discriminative learning for id84 and
classi   cation. in advances in neural information processing
systems, pages 897   904, 2008.

[12] w. li and a. mccallum. pachinko allocation:

dag-structured mixture models of topic correlations. in
proceedings of the 23rd international conference on
machine learning, pages 577   584. acm, 2006.

[13] c. lin and y. he. joint sentiment/topic model for

id31. in proceedings of the 18th acm
conference on information and knowledge management,
pages 375   384. acm, 2009.

[14] h. ma, m. r. lyu, and i. king. diversifying query

suggestion results. in proc. of aaai, volume 10, 2010.
[15] s. martin, w. brown, r. klavans, and k. boyack. drl:

distributed recursive (graph) layout. sand2008-2936j:
sandia national laboratories, 2008.

[16] q. mei, d. cai, d. zhang, and c. zhai. id96
with network id173. in proceedings of the 17th
international conference on world wide web, pages
101   110. acm, 2008.

[17] q. mei, j. guo, and d. radev. divrank: the interplay of

prestige and diversity in information networks. in
proceedings of the 16th acm sigkdd international
conference on knowledge discovery and data mining, pages
1009   1018. acm, 2010.

[18] q. mei, x. ling, m. wondra, h. su, and c. zhai. topic

sentiment mixture: modeling facets and opinions in
weblogs. in proceedings of the 16th international conference
on world wide web, pages 171   180. acm, 2007.

[19] d. newman, j. h. lau, k. grieser, and t. baldwin.
automatic evaluation of topic coherence. in human
language technologies: the 2010 annual conference of
the north american chapter of the association for
computational linguistics, pages 100   108. association for
computational linguistics, 2010.

[20] r. pemantle. vertex-reinforced random walk. id203

theory and related fields, 92(1):117   136, 1992.

[21] f. petralia, v. rao, and d. b. dunson. repulsive mixtures.

in nips, pages 1898   1906, 2012.

[22] d. ramage, p. heymann, c. d. manning, and

h. garcia-molina. id91 the tagged web. in
proceedings of the second acm international conference
on web search and data mining, pages 54   63. acm, 2009.

[23] j. tang, j. zhang, l. yao, j. li, l. zhang, and z. su.
arnetminer: extraction and mining of academic social
networks. in kdd   08, pages 990   998, 2008.

[24] y. w. teh, m. i. jordan, m. j. beal, and d. m. blei.

hierarchical dirichlet processes. journal of the american
statistical association, 101(476), 2006.

[25] j. zhu, a. ahmed, and e. p. xing. medlda: maximum

margin supervised topic models for regression and
classi   cation. in proceedings of the 26th annual
international conference on machine learning, pages
1257   1264. acm, 2009.

[26] x. zhu, a. b. goldberg, j. van gael, and d. andrzejewski.

improving diversity in ranking using absorbing random
walks. in hlt-naacl, pages 97   104, 2007.

[27] c.-n. ziegler, s. m. mcnee, j. a. konstan, and g. lausen.

improving recommendation lists through topic
diversi   cation. in proceedings of the 14th international
conference on world wide web, pages 22   32. acm, 2005.

[28] j. zou and r. adams. priors for diversity in generative

latent variable models. in advances in neural information
processing systems 25, pages 3005   3013, 2012.

