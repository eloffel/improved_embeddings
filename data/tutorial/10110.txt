get to the point: summarization with pointer-generator networks

abigail see

stanford university

peter j. liu
google brain

christopher d. manning

stanford university

abisee@stanford.edu

peterjliu@google.com

manning@stanford.edu

7
1
0
2

 
r
p
a
5
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
8
6
3
4
0

.

4
0
7
1
:
v
i
x
r
a

abstract

neural sequence-to-sequence models have
provided a viable new approach for ab-
stractive text summarization (meaning
they are not restricted to simply selecting
and rearranging passages from the origi-
nal text). however, these models have two
shortcomings: they are liable to reproduce
factual details inaccurately, and they tend
to repeat themselves. in this work we pro-
pose a novel architecture that augments the
standard sequence-to-sequence attentional
model in two orthogonal ways.
first,
we use a hybrid pointer-generator network
that can copy words from the source text
via pointing, which aids accurate repro-
duction of information, while retaining the
ability to produce novel words through the
generator. second, we use coverage to
keep track of what has been summarized,
which discourages repetition. we apply
our model to the id98 / daily mail sum-
marization task, outperforming the current
abstractive state-of-the-art by at least 2
id8 points.

original text (truncated): lagos, nigeria (id98) a day after winning nige-
ria   s presidency, muhammadu buhari told id98   s christiane amanpour that
he plans to aggressively    ght corruption that has long plagued nigeria
and go after the root of the nation   s unrest. buhari said he   ll    rapidly give
attention    to curbing violence in the northeast part of nigeria, where the ter-
rorist group boko haram operates. by cooperating with neighboring nations
chad, cameroon and niger, he said his administration is con   dent it will
be able to thwart criminals and others contributing to nigeria   s instability.
for the    rst time in nigeria   s history, the opposition defeated the ruling party
in democratic elections. buhari defeated incumbent goodluck jonathan by
about 2 million votes, according to nigeria   s independent national electoral
commission. the win comes after a long history of military rule, coups
and botched attempts at democracy in africa   s most populous nation.

baseline id195 + attention: unk unk says his administration is con   -
dent it will be able to destabilize nigeria   s economy. unk says his admin-
istration is con   dent it will be able to thwart criminals and other nigerians.
he says the country has long nigeria and nigeria   s economy.

pointer-gen: muhammadu buhari says he plans to aggressively    ght cor-
ruption in the northeast part of nigeria. he says he   ll    rapidly give at-
tention    to curbing violence in the northeast part of nigeria. he says his
administration is con   dent it will be able to thwart criminals.

pointer-gen + coverage: muhammadu buhari says he plans to aggressively
   ght corruption that has long plagued nigeria. he says his administration is
con   dent it will be able to thwart criminals. the win comes after a long his-
tory of military rule, coups and botched attempts at democracy in africa   s
most populous nation.

figure 1: comparison of output of 3 abstrac-
tive summarization models on a news article. the
baseline model makes factual errors, a nonsen-
sical sentence and struggles with oov words
muhammadu buhari. the pointer-generator model
is accurate but repeats itself. coverage eliminates
repetition. the    nal summary is composed from
several fragments.

1

introduction

summarization is the task of condensing a piece of
text to a shorter version that contains the main in-
formation from the original. there are two broad
approaches to summarization: extractive and ab-
stractive. extractive methods assemble summaries
exclusively from passages (usually whole sen-
tences) taken directly from the source text, while
abstractive methods may generate novel words
and phrases not featured in the source text     as
a human-written abstract usually does. the ex-
tractive approach is easier, because copying large

chunks of text from the source document ensures
baseline levels of grammaticality and accuracy.
on the other hand, sophisticated abilities that are
crucial to high-quality summarization, such as
id141, generalization, or the incorporation
of real-world knowledge, are possible only in an
abstractive framework (see figure 5).

due to the dif   culty of abstractive summariza-
tion, the great majority of past work has been ex-
tractive (kupiec et al., 1995; paice, 1990; sag-
gion and poibeau, 2013). however, the recent suc-
cess of sequence-to-sequence models (sutskever

figure 2: baseline sequence-to-sequence model with attention. the model may attend to relevant words
in the source text to generate novel words, e.g., to produce the novel word beat in the abstractive summary
germany beat argentina 2-0 the model may attend to the words victorious and win in the source text.

et al., 2014), in which recurrent neural networks
(id56s) both read and freely generate text, has
made abstractive summarization viable (chopra
et al., 2016; nallapati et al., 2016; rush et al.,
2015; zeng et al., 2016). though these systems
are promising, they exhibit undesirable behavior
such as inaccurately reproducing factual details,
an inability to deal with out-of-vocabulary (oov)
words, and repeating themselves (see figure 1).

in this paper we present an architecture that
addresses these three issues in the context of
multi-sentence summaries. while most recent ab-
stractive work has focused on headline genera-
tion tasks (reducing one or two sentences to a
single headline), we believe that longer-text sum-
marization is both more challenging (requiring
higher levels of abstraction while avoiding repe-
tition) and ultimately more useful. therefore we
apply our model to the recently-introduced id98/
daily mail dataset (hermann et al., 2015; nallap-
ati et al., 2016), which contains news articles (39
sentences on average) paired with multi-sentence
summaries, and show that we outperform the state-
of-the-art abstractive system by at least 2 id8
points.

our hybrid pointer-generator network facili-
tates copying words from the source text via point-
ing (vinyals et al., 2015), which improves accu-
racy and handling of oov words, while retaining
the ability to generate new words. the network,
which can be viewed as a balance between extrac-
tive and abstractive approaches, is similar to gu
et al.   s (2016) copynet and miao and blunsom   s
(2016) forced-attention sentence compression,

that were applied to short-text summarization. we
propose a novel variant of the coverage vector (tu
et al., 2016) from id4,
which we use to track and control coverage of the
source document. we show that coverage is re-
markably effective for eliminating repetition.

2 our models

in this section we describe (1) our baseline
sequence-to-sequence model,
(2) our pointer-
generator model, and (3) our coverage mechanism
that can be added to either of the    rst two models.
the code for our models is available online.1

2.1 sequence-to-sequence attentional model
our baseline model is similar to that of nallapati
et al. (2016), and is depicted in figure 2. the to-
kens of the article wi are fed one-by-one into the
encoder (a single-layer bidirectional lstm), pro-
ducing a sequence of encoder hidden states hi. on
each step t, the decoder (a single-layer unidirec-
tional lstm) receives the id27 of the
previous word (while training, this is the previous
word of the reference summary; at test time it is
the previous word emitted by the decoder), and
has decoder state st. the attention distribution at
is calculated as in bahdanau et al. (2015):

et
i = vt tanh(whhi +wsst + battn)
at = softmax(et)

(1)
(2)

where v, wh, ws and battn are learnable parame-
ters. the attention distribution can be viewed as

1www.github.com/abisee/pointer-generator

...attention distribution<start>vocabulary distributioncontext vectorgermanyazoopartial summary"beat"germany  emerge  victorious     in           2-0          win       against  argentina    on       saturday    ...encoder hiddenstates decoderhidden statessource textfigure 3: pointer-generator model. for each decoder timestep a generation id203 pgen     [0,1] is
calculated, which weights the id203 of generating words from the vocabulary, versus copying words
from the source text. the vocabulary distribution and the attention distribution are weighted and summed
to obtain the    nal distribution, from which we make our prediction. note that out-of-vocabulary article
words such as 2-0 are included in the    nal distribution. best viewed in color.

a id203 distribution over the source words,
that tells the decoder where to look to produce the
next word. next, the attention distribution is used
to produce a weighted sum of the encoder hidden
states, known as the context vector h   
t :

h   
t =    i at
ihi

(3)
the context vector, which can be seen as a    xed-
size representation of what has been read from the
source for this step, is concatenated with the de-
coder state st and fed through two linear layers to
produce the vocabulary distribution pvocab:
t ] + b) + b(cid:48))

(4)
where v , v (cid:48), b and b(cid:48) are learnable parameters.
pvocab is a id203 distribution over all words
in the vocabulary, and provides us with our    nal
distribution from which to predict words w:

pvocab = softmax(v (cid:48)(v [st,h   

p(w) = pvocab(w)

(5)
during training, the loss for timestep t is the neg-
ative log likelihood of the target word w   
t for that
timestep:

losst =    logp(w   
t )

and the overall loss for the whole sequence is:

loss =

1
t    t

t=0 losst

(6)

(7)

2.2 pointer-generator network
our pointer-generator network is a hybrid between
our baseline and a pointer network (vinyals et al.,
2015), as it allows both copying words via point-
ing, and generating words from a    xed vocabulary.
in the pointer-generator model (depicted in figure
3) the attention distribution at and context vector
h   
t are calculated as in section 2.1. in addition, the
generation id203 pgen     [0,1] for timestep t is
calculated from the context vector h   
t , the decoder
state st and the decoder input xt:

h   h   

t + wt

s st + wt

pgen =    (wt

x xt + bptr)

(8)
where vectors wh   , ws, wx and scalar bptr are learn-
able parameters and    is the sigmoid function.
next, pgen is used as a soft switch to choose be-
tween generating a word from the vocabulary by
sampling from pvocab, or copying a word from the
input sequence by sampling from the attention dis-
tribution at. for each document let the extended
vocabulary denote the union of the vocabulary,
and all words appearing in the source document.
we obtain the following id203 distribution
over the extended vocabulary:
p(w) = pgenpvocab(w) + (1    pgen)   i:wi=w at
i (9)
note that if w is an out-of-vocabulary (oov)
word, then pvocab(w) is zero; similarly if w does

source textgermany  emerge  victorious     in           2-0          win       against  argentina    on       saturday    ......<start>vocabulary distributioncontext vectorgermanyazoobeatazoopartial summaryfinal distribution"argentina""2-0"attention distributionencoder hiddenstatesdecoder hidden statesnot appear in the source document, then    i:wi=w at
i
is zero. the ability to produce oov words is
one of the primary advantages of pointer-generator
models; by contrast models such as our baseline
are restricted to their pre-set vocabulary.

the id168 is as described in equations
(6) and (7), but with respect to our modi   ed prob-
ability distribution p(w) given in equation (9).

2.3 coverage mechanism
repetition is a common problem for sequence-
to-sequence models (tu et al., 2016; mi et al.,
2016; sankaran et al., 2016; suzuki and nagata,
2016), and is especially pronounced when gener-
ating multi-sentence text (see figure 1). we adapt
the coverage model of tu et al. (2016) to solve the
problem. in our coverage model, we maintain a
coverage vector ct, which is the sum of attention
distributions over all previous decoder timesteps:

ct =    t   1

t(cid:48)=0 at(cid:48)

(10)

intuitively, ct is a (unnormalized) distribution over
the source document words that represents the de-
gree of coverage that those words have received
from the attention mechanism so far. note that c0
is a zero vector, because on the    rst timestep, none
of the source document has been covered.

the coverage vector is used as extra input to the

attention mechanism, changing equation (1) to:

et
i = vt tanh(whhi +wsst + wcct

i + battn)

(11)

where wc is a learnable parameter vector of same
length as v. this ensures that the attention mecha-
nism   s current decision (choosing where to attend
next) is informed by a reminder of its previous
decisions (summarized in ct). this should make
it easier for the attention mechanism to avoid re-
peatedly attending to the same locations, and thus
avoid generating repetitive text.

we    nd it necessary (see section 5) to addition-
ally de   ne a coverage loss to penalize repeatedly
attending to the same locations:
covlosst =    i min(at

i,ct
i)

(12)

note that the coverage loss is bounded; in particu-
lar covlosst        i at
i = 1. equation (12) differs from
the coverage loss used in machine translation. in
mt, we assume that there should be a roughly one-
to-one translation ratio; accordingly the    nal cov-
erage vector is penalized if it is more or less than 1.

our id168 is more    exible: because sum-
marization should not require uniform coverage,
we only penalize the overlap between each atten-
tion distribution and the coverage so far     prevent-
ing repeated attention. finally, the coverage loss,
reweighted by some hyperparameter    , is added to
the primary id168 to yield a new composite
id168:

losst =    logp(w   

t ) +       i min(at

i,ct
i)

(13)

3 related work
neural abstractive summarization. rush et al.
(2015) were the    rst to apply modern neural net-
works to abstractive text summarization, achiev-
ing state-of-the-art performance on duc-2004
and gigaword, two sentence-level summarization
datasets. their approach, which is centered on the
attention mechanism, has been augmented with re-
current decoders (chopra et al., 2016), abstract
meaning representations (takase et al., 2016), hi-
erarchical networks (nallapati et al., 2016), vari-
ational autoencoders (miao and blunsom, 2016),
and direct optimization of the performance metric
(ranzato et al., 2016), further improving perfor-
mance on those datasets.

however, large-scale datasets for summariza-
tion of longer text are rare. nallapati et al. (2016)
adapted the deepmind question-answering dataset
(hermann et al., 2015) for summarization, result-
ing in the id98/daily mail dataset, and provided
the    rst abstractive baselines. the same authors
then published a neural extractive approach (nal-
lapati et al., 2017), which uses hierarchical id56s
to select sentences, and found that it signi   cantly
outperformed their abstractive result with respect
to the id8 metric. to our knowledge, these
are the only two published results on the full data-
set.

prior to modern neural methods, abstractive
summarization received less attention than extrac-
tive summarization, but jing (2000) explored cut-
ting unimportant parts of sentences to create sum-
maries, and cheung and penn (2014) explore sen-
tence fusion using dependency trees.

pointer-generator networks. the pointer net-
work (vinyals et al., 2015) is a sequence-to-
sequence model that uses the soft attention dis-
tribution of bahdanau et al.
(2015) to produce
an output sequence consisting of elements from

the input sequence. the pointer network has been
used to create hybrid approaches for id4 (gul-
cehre et al., 2016), id38 (merity
et al., 2016), and summarization (gu et al., 2016;
gulcehre et al., 2016; miao and blunsom, 2016;
nallapati et al., 2016; zeng et al., 2016).

our approach is close to the forced-attention
sentence compression model of miao and blun-
som (2016) and the copynet model of gu et al.
(2016), with some small differences: (i) we cal-
culate an explicit switch id203 pgen, whereas
gu et al. induce competition through a shared soft-
max function. (ii) we recycle the attention distri-
bution to serve as the copy distribution, but gu et
al. use two separate distributions.
(iii) when a
word appears multiple times in the source text, we
sum id203 mass from all corresponding parts
of the attention distribution, whereas miao and
blunsom do not. our reasoning is that (i) calcu-
lating an explicit pgen usefully enables us to raise
or lower the id203 of all generated words or
all copy words at once, rather than individually,
(ii) the two distributions serve such similar pur-
poses that we    nd our simpler approach suf   ces,
and (iii) we observe that the pointer mechanism
often copies a word while attending to multiple oc-
currences of it in the source text.

our approach is considerably different from
that of gulcehre et al. (2016) and nallapati et al.
(2016). those works train their pointer compo-
nents to activate only for out-of-vocabulary words
or named entities (whereas we allow our model to
freely learn when to use the pointer), and they do
not mix the probabilities from the copy distribu-
tion and the vocabulary distribution. we believe
the mixture approach described here is better for
abstractive summarization     in section 6 we show
that the copy mechanism is vital for accurately
reproducing rare but in-vocabulary words, and in
section 7.2 we observe that the mixture model en-
ables the language model and copy mechanism to
work together to perform abstractive copying.

coverage. originating from statistical ma-
chine translation (koehn, 2009), coverage was
adapted for id4 by tu et al. (2016) and mi et al.
(2016), who both use a gru to update the cov-
erage vector each step. we    nd that a simpler
approach     summing the attention distributions to
obtain the coverage vector     suf   ces. in this re-
spect our approach is similar to xu et al. (2015),
who apply a coverage-like method to image cap-

tioning, and chen et al. (2016), who also incorpo-
rate a coverage mechanism (which they call    dis-
traction   ) as described in equation (11) into neural
summarization of longer text.

temporal attention is a related technique that
has been applied to id4 (sankaran et al., 2016)
and summarization (nallapati et al., 2016).
in
this approach, each attention distribution is di-
vided by the sum of the previous, which effec-
tively dampens repeated attention. we tried this
method but found it too destructive, distorting the
signal from the attention mechanism and reducing
performance. we hypothesize that an early inter-
vention method such as coverage is preferable to
a post hoc method such as temporal attention     it
is better to inform the attention mechanism to help
it make better decisions, than to override its de-
cisions altogether. this theory is supported by the
large boost that coverage gives our id8 scores
(see table 1), compared to the smaller boost given
by temporal attention for the same task (nallapati
et al., 2016).

4 dataset
we use the id98/daily mail dataset (hermann
et al., 2015; nallapati et al., 2016), which con-
tains online news articles (781 tokens on average)
paired with multi-sentence summaries (3.75 sen-
tences or 56 tokens on average). we used scripts
supplied by nallapati et al. (2016) to obtain the
same version of the the data, which has 287,226
training pairs, 13,368 validation pairs and 11,490
test pairs. both the dataset   s published results
(nallapati et al., 2016, 2017) use the anonymized
version of the data, which has been pre-processed
to replace each named entity, e.g., the united na-
tions, with its own unique identi   er for the exam-
ple pair, e.g., @entity5. by contrast, we operate
directly on the original text (or non-anonymized
version of the data),2 which we believe is the fa-
vorable problem to solve because it requires no
pre-processing.

all

experiments,

5 experiments
our model has 256-
for
dimensional hidden states and 128-dimensional
id27s. for the pointer-generator mod-
els, we use a vocabulary of 50k words for both
source and target     note that due to the pointer net-
work   s ability to handle oov words, we can use

2at www.github.com/abisee/pointer-generator

id8

meteor

abstractive model (nallapati et al., 2016)*
seq-to-seq + attn baseline (150k vocab)
seq-to-seq + attn baseline (50k vocab)
pointer-generator
pointer-generator + coverage
lead-3 baseline (ours)
lead-3 baseline (nallapati et al., 2017)*
extractive model (nallapati et al., 2017)*

1

35.46
30.49
31.33
36.44
39.53
40.34
39.2
39.6

2

13.30
11.17
11.81
15.66
17.28
17.70
15.7
16.2

l

32.65
28.08
28.83
33.42
36.38
36.57
35.5
35.3

exact match + stem/syn/para

-

11.65
12.03
15.35
17.32
20.48

-
-

-

12.86
13.20
16.65
18.72
22.21

-
-

table 1: id8 f1 and meteor scores on the test set. models and baselines in the top half are
abstractive, while those in the bottom half are extractive. those marked with * were trained and evaluated
on the anonymized dataset, and so are not strictly comparable to our results on the original text. all our
id8 scores have a 95% con   dence interval of at most   0.25 as reported by the of   cial id8
script. the meteor improvement from the 50k baseline to the pointer-generator model, and from the
pointer-generator to the pointer-generator+coverage model, were both found to be statistically signi   cant
using an approximate randomization test with p < 0.01.

a smaller vocabulary size than nallapati et al.   s
(2016) 150k source and 60k target vocabularies.
for the baseline model, we also try a larger vocab-
ulary size of 150k.

note that the pointer and the coverage mecha-
nism introduce very few additional parameters to
the network: for the models with vocabulary size
50k, the baseline model has 21,499,600 parame-
ters, the pointer-generator adds 1153 extra param-
eters (wh   , ws, wx and bptr in equation 8), and cov-
erage adds 512 extra parameters (wc in equation
11).

unlike nallapati et al. (2016), we do not pre-
train the id27s     they are learned
from scratch during training. we train using ada-
grad (duchi et al., 2011) with learning rate 0.15
and an initial accumulator value of 0.1.
(this
was found to work best of stochastic gradient
descent, adadelta, momentum, adam and rm-
sprop). we use gradient clipping with a maximum
gradient norm of 2, but do not use any form of reg-
ularization. we use loss on the validation set to
implement early stopping.

during training and at test time we truncate the
article to 400 tokens and limit the length of the
summary to 100 tokens for training and 120 to-
kens at test time.3 this is done to expedite train-
ing and testing, but we also found that truncating
the article can raise the performance of the model

3the upper limit of 120 is mostly invisible:

the beam
search algorithm is self-stopping and almost never reaches
the 120th step.

(see section 7.1 for more details). for training,
we found it ef   cient to start with highly-truncated
sequences, then raise the maximum length once
converged. we train on a single tesla k40m gpu
with a batch size of 16. at test time our summaries
are produced using id125 with beam size 4.
we trained both our baseline models for about
600,000 iterations (33 epochs)     this is similar
to the 35 epochs required by nallapati et al.   s
(2016) best model. training took 4 days and 14
hours for the 50k vocabulary model, and 8 days 21
hours for the 150k vocabulary model. we found
the pointer-generator model quicker to train, re-
quiring less than 230,000 training iterations (12.8
epochs); a total of 3 days and 4 hours.
in par-
ticular, the pointer-generator model makes much
quicker progress in the early phases of training.
to obtain our    nal coverage model, we added the
coverage mechanism with coverage loss weighted
to    = 1 (as described in equation 13), and trained
for a further 3000 iterations (about 2 hours).
in
this time the coverage loss converged to about 0.2,
down from an initial value of about 0.5. we also
tried a more aggressive value of    = 2; this re-
duced coverage loss but increased the primary loss
function, thus we did not use it.

we tried training the coverage model without
the id168, hoping that the attention mech-
anism may learn by itself not to attend repeatedly
to the same locations, but we found this to be inef-
fective, with no discernible reduction in repetition.
we also tried training with coverage from the    rst

iteration rather than as a separate training phase,
but found that in the early phase of training, the
coverage objective interfered with the main objec-
tive, reducing overall performance.

6 results
6.1 preliminaries
our results are given in table 1. we evalu-
ate our models with the standard id8 metric
(lin, 2004b), reporting the f1 scores for id8-
1, id8-2 and id8-l (which respectively
measure the word-overlap, bigram-overlap, and
longest common sequence between the reference
summary and the summary to be evaluated). we
obtain our id8 scores using the pyid8
package.4 we also evaluate with the meteor
metric (denkowski and lavie, 2014), both in ex-
act match mode (rewarding only exact matches
between words) and full mode (which addition-
ally rewards matching stems, synonyms and para-
phrases).5

in addition to our own models, we also report
the lead-3 baseline (which uses the    rst three sen-
tences of the article as a summary), and compare
to the only existing abstractive (nallapati et al.,
2016) and extractive (nallapati et al., 2017) mod-
els on the full dataset. the output of our models is
available online.6

given that we generate plain-text summaries but
nallapati et al. (2016; 2017) generate anonymized
summaries (see section 4), our id8 scores
are not strictly comparable. there is evidence
to suggest that the original-text dataset may re-
sult in higher id8 scores in general than the
anonymized dataset     the lead-3 baseline is higher
on the former than the latter. one possible expla-
nation is that multi-word named entities lead to
a higher rate of id165 overlap. unfortunately,
id8 is the only available means of compar-
ison with nallapati et al.   s work. nevertheless,
given that the disparity in the lead-3 scores is
(+1.1 id8-1, +2.0 id8-2, +1.1 id8-
l) points respectively, and our best model scores
exceed nallapati et al. (2016) by (+4.07 id8-
1, +3.98 id8-2, +3.73 id8-l) points, we
may estimate that we outperform the only previous
abstractive system by at least 2 id8 points all-
round.

4pypi.python.org/pypi/pyid8/0.1.3
5www.cs.cmu.edu/~alavie/meteor
6www.github.com/abisee/pointer-generator

s
e
t
a
c
i
l
p
u
d

e
r
a

t
a
h
t

%

30

20

10

0

1-gra m s

2-gra m s

3-gra m s

4-gra m s

se nte n c es

pointer-generator, no coverage
pointer-generator + coverage
reference summaries

figure 4: coverage eliminates undesirable repe-
tition. summaries from our non-coverage model
contain many duplicated id165s while our cover-
age model produces a similar number as the ref-
erence summaries.

6.2 observations
we    nd that both our baseline models perform
poorly with respect to id8 and meteor, and
in fact the larger vocabulary size (150k) does not
seem to help. even the better-performing baseline
(with 50k vocabulary) produces summaries with
several common problems. factual details are fre-
quently reproduced incorrectly, often replacing an
uncommon (but in-vocabulary) word with a more-
common alternative. for example in figure 1,
the baseline model appears to struggle with the
rare word thwart, producing destabilize instead,
which leads to the fabricated phrase destabilize
nigeria   s economy. even more catastrophically,
the summaries sometimes devolve into repetitive
nonsense, such as the third sentence produced by
the baseline model in figure 1.
in addition, the
baseline model can   t reproduce out-of-vocabulary
words (such as muhammadu buhari in figure 1).
further examples of all these problems are pro-
vided in the supplementary material.

our pointer-generator model achieves much
better id8 and meteor scores than the
baseline, despite many fewer training epochs. the
difference in the summaries is also marked: out-
of-vocabulary words are handled easily, factual
details are almost always copied correctly, and
there are no fabrications (see figure 1). however,
repetition is still very common.

our pointer-generator model with coverage im-
proves the id8 and meteor scores further,
convincingly surpassing the best abstractive model

article: smugglers lure arab and african migrants by offer-
ing discounts to get onto overcrowded ships if people bring
more potential passengers, a id98 investigation has revealed.
(...)
summary: id98 investigation uncovers the business inside
a human smuggling ring.

article: eyewitness video showing white north charleston
police of   cer michael slager shooting to death an unarmed
black man has exposed discrepancies in the reports of the
   rst of   cers on the scene. (...)
summary: more questions than answers emerge in con-
troversial s.c. police shooting.

figure 5: examples of highly abstractive reference
summaries (bold denotes novel words).

of nallapati et al.
(2016) by several id8
points. despite the brevity of the coverage train-
ing phase (about 1% of the total training time),
the repetition problem is almost completely elimi-
nated, which can be seen both qualitatively (figure
1) and quantitatively (figure 4). however, our best
model does not quite surpass the id8 scores
of the lead-3 baseline, nor the current best extrac-
tive model (nallapati et al., 2017). we discuss this
issue in section 7.1.

7 discussion
7.1 comparison with extractive systems
it is clear from table 1 that extractive systems tend
to achieve higher id8 scores than abstractive,
and that the extractive lead-3 baseline is extremely
strong (even the best extractive system beats it by
only a small margin). we offer two possible ex-
planations for these observations.

firstly, news articles tend to be structured with
the most important information at the start; this
partially explains the strength of the lead-3 base-
line. indeed, we found that using only the    rst 400
tokens (about 20 sentences) of the article yielded
signi   cantly higher id8 scores than using the
   rst 800 tokens.

secondly, the nature of the task and the id8
metric make extractive approaches and the lead-
3 baseline dif   cult to beat. the choice of con-
tent for the reference summaries is quite subjective
    sometimes the sentences form a self-contained
summary; other times they simply showcase a few
interesting details from the article. given that the
articles contain 39 sentences on average, there are
many equally valid ways to choose 3 or 4 high-
lights in this style. abstraction introduces even
more options (choice of phrasing), further decreas-

ing the likelihood of matching the reference sum-
mary. for example, smugglers pro   t from des-
perate migrants is a valid alternative abstractive
summary for the    rst example in figure 5, but
it scores 0 id8 with respect to the reference
summary. this in   exibility of id8 is exac-
erbated by only having one reference summary,
which has been shown to lower id8   s relia-
bility compared to multiple reference summaries
(lin, 2004a).

due to the subjectivity of the task and thus
the diversity of valid summaries, it seems that
id8 rewards safe strategies such as select-
ing the    rst-appearing content, or preserving orig-
inal phrasing. while the reference summaries do
sometimes deviate from these techniques, those
deviations are unpredictable enough that the safer
strategy obtains higher id8 scores on average.
this may explain why extractive systems tend to
obtain higher id8 scores than abstractive, and
even extractive systems do not signi   cantly ex-
ceed the lead-3 baseline.

to explore this issue further, we evaluated our
systems with the meteor metric, which rewards
not only exact word matches, but also matching
stems, synonyms and paraphrases (from a pre-
de   ned list). we observe that all our models re-
ceive over 1 meteor point boost by the inclu-
sion of stem, synonym and paraphrase matching,
indicating that they may be performing some ab-
straction. however, we again observe that the
lead-3 baseline is not surpassed by our models.
it may be that news article style makes the lead-
3 baseline very strong with respect to any metric.
we believe that investigating this issue further is
an important direction for future work.

7.2 how abstractive is our model?

we have shown that our pointer mechanism makes
our abstractive system more reliable, copying fac-
tual details correctly more often. but does the ease
of copying make our system any less abstractive?
figure 6 shows that our    nal model   s sum-
maries contain a much lower rate of novel id165s
(i.e., those that don   t appear in the article) than the
reference summaries, indicating a lower degree of
abstraction. note that the baseline model produces
novel id165s more frequently     however, this
statistic includes all the incorrectly copied words,
unk tokens and fabrications alongside the good
instances of abstraction.

l
e
v
o
n

e
r
a

t
a
h
t

%

100
80
60
40
20
0

1-gra m s

2-gra m s

3-gra m s

4-gra m s

se nte n c es

pointer-generator + coverage
sequence-to-sequence + attention baseline
reference summaries

figure 6: although our best model is abstractive,
it does not produce novel id165s (i.e., id165s
that don   t appear in the source text) as often as
the reference summaries. the baseline model
produces more novel id165s, but many of these
are erroneous (see section 7.2).

article: andy murray (...) is into the semi-   nals of the mi-
ami open , but not before getting a scare from 21 year-old
austrian dominic thiem, who pushed him to 4-4 in the sec-
ond set before going down 3-6 6-4, 6-1 in an hour and three
quarters. (...)
summary: andy murray defeated dominic thiem 3-6 6-4,
6-1 in an hour and three quarters.

article: (...) wayne rooney smashes home during manch-
ester united    s 3-1 win over aston villa on saturday. (...)
summary: manchester united beat aston villa 3-1 at old
trafford on saturday.

figure 7: examples of abstractive summaries pro-
duced by our model (bold denotes novel words).

in particular, figure 6 shows that our    nal
model copies whole article sentences 35% of the
time; by comparison the reference summaries do
so only 1.3% of the time. this is a main area for
improvement, as we would like our model to move
beyond simple sentence extraction. however, we
observe that the other 65% encompasses a range of
abstractive techniques. article sentences are trun-
cated to form grammatically-correct shorter ver-
sions, and new sentences are composed by stitch-
ing together fragments. unnecessary interjections,
clauses and parenthesized phrases are sometimes
omitted from copied passages. some of these abil-
ities are demonstrated in figure 1, and the supple-
mentary material contains more examples.

figure 7 shows two examples of more impres-
sive abstraction     both with similar structure. the
dataset contains many sports stories whose sum-
maries follow the x beat y (cid:104)score(cid:105) on (cid:104)day(cid:105) tem-

plate, which may explain why our model is most
con   dently abstractive on these examples. in gen-
eral however, our model does not routinely pro-
duce summaries like those in figure 7, and is not
close to producing summaries like in figure 5.

the value of the generation id203 pgen
also gives a measure of the abstractiveness of our
model. during training, pgen starts with a value
of about 0.30 then increases, converging to about
0.53 by the end of training. this indicates that
the model    rst learns to mostly copy, then learns
to generate about half the time. however at test
time, pgen is heavily skewed towards copying, with
a mean value of 0.17. the disparity is likely
due to the fact that during training, the model re-
ceives word-by-word supervision in the form of
the reference summary, but at test time it does
not. nonetheless, the generator module is use-
ful even when the model is copying. we    nd
that pgen is highest at times of uncertainty such
as the beginning of sentences, the join between
stitched-together fragments, and when producing
periods that truncate a copied sentence. our mix-
ture model allows the network to copy while si-
multaneously consulting the language model     en-
abling operations like stitching and truncation to
be performed with grammaticality.
in any case,
encouraging the pointer-generator model to write
more abstractively, while retaining the accuracy
advantages of the pointer module, is an exciting
direction for future work.

8 conclusion
in this work we presented a hybrid pointer-
generator architecture with coverage, and showed
that it reduces inaccuracies and repetition. we ap-
plied our model to a new and challenging long-
text dataset, and signi   cantly outperformed the
abstractive state-of-the-art result. our model ex-
hibits many abstractive abilities, but attaining
higher levels of abstraction remains an open re-
search question.

9 acknowledgment
we thank the acl reviewers for their helpful com-
ments. this work was begun while the    rst author
was an intern at google brain and continued at
stanford. stanford university gratefully acknowl-
edges the support of the darpa deft program
afrl contract no. fa8750-13-2-0040. any opin-
ions in this material are those of the authors alone.

references
dzmitry bahdanau, kyunghyun cho, and yoshua ben-
gio. 2015. id4 by jointly
learning to align and translate. in international con-
ference on learning representations.

qian chen, xiaodan zhu, zhenhua ling, si wei, and
hui jiang. 2016. distraction-based neural networks
in international joint
for modeling documents.
conference on arti   cial intelligence.

jackie chi kit cheung and gerald penn. 2014. unsu-
pervised sentence enhancement for automatic sum-
marization. in empirical methods in natural lan-
guage processing.

sumit chopra, michael auli, and alexander m rush.
2016. abstractive sentence summarization with at-
in north amer-
tentive recurrent neural networks.
ican chapter of the association for computational
linguistics.

michael denkowski and alon lavie. 2014. meteor
universal: language speci   c translation evaluation
in eacl 2014 workshop
for any target language.
on id151.

john duchi, elad hazan, and yoram singer. 2011.
adaptive subgradient methods for online learning
journal of machine
and stochastic optimization.
learning research 12:2121   2159.

jiatao gu, zhengdong lu, hang li, and victor ok
incorporating copying mechanism in
in association for

li. 2016.
sequence-to-sequence learning.
computational linguistics.

chin-yew lin. 2004b. id8: a package for auto-
matic evaluation of summaries. in text summariza-
tion branches out: acl workshop.

stephen merity, caiming xiong, james bradbury, and
pointer sentinel mixture
richard socher. 2016.
in nips 2016 workshop on multi-class
models.
and multi-label learning in extremely large label
spaces.

haitao mi, baskaran sankaran, zhiguo wang, and abe
ittycheriah. 2016. coverage embedding models for
id4. in empirical methods in
natural language processing.

yishu miao and phil blunsom. 2016. language as a
latent variable: discrete generative models for sen-
tence compression. in empirical methods in natu-
ral language processing.

ramesh nallapati, feifei zhai, and bowen zhou. 2017.
summarunner: a recurrent neural network based
sequence model for extractive summarization of
documents. in association for the advancement of
arti   cial intelligence.

ramesh nallapati, bowen zhou, cicero dos santos,
c   aglar gulc  ehre, and bing xiang. 2016. abstrac-
tive text summarization using sequence-to-sequence
id56s and beyond. in computational natural lan-
guage learning.

chris d paice. 1990. constructing literature abstracts
by computer: techniques and prospects. information
processing & management 26(1):171   186.

caglar gulcehre, sungjin ahn, ramesh nallapati,
bowen zhou, and yoshua bengio. 2016. pointing
in association for computa-
the unknown words.
tional linguistics.

marc   aurelio ranzato, sumit chopra, michael auli,
and wojciech zaremba. 2016. sequence level train-
ing with recurrent neural networks. in international
conference on learning representations.

karl moritz hermann, tomas kocisky, edward
grefenstette, lasse espeholt, will kay, mustafa su-
leyman, and phil blunsom. 2015. teaching ma-
chines to read and comprehend. in neural informa-
tion processing systems.

hongyan jing. 2000. sentence reduction for automatic
in applied natural language

text summarization.
processing.

philipp koehn. 2009. id151.

cambridge university press.

julian kupiec, jan pedersen, and francine chen. 1995.
a trainable document summarizer. in international
acm sigir conference on research and develop-
ment in information retrieval.

chin-yew lin. 2004a.

looking for a few good
metrics: id54 evaluation-how
in nacsis/nii test
many samples are enough?
collection for information retrieval (ntcir) work-
shop.

alexander m rush, sumit chopra, and jason weston.
2015. a neural attention model for abstractive sen-
tence summarization. in empirical methods in nat-
ural language processing.

horacio saggion and thierry poibeau. 2013. auto-
matic text summarization: past, present and future.
in multi-source, multilingual information extrac-
tion and summarization, springer, pages 3   21.

baskaran sankaran, haitao mi, yaser al-onaizan, and
abe ittycheriah. 2016. temporal attention model
arxiv preprint
for id4.
arxiv:1608.02927 .

ilya sutskever, oriol vinyals, and quoc v le. 2014.
sequence to sequence learning with neural net-
works. in neural information processing systems.

jun suzuki and masaaki nagata. 2016. id56-based
encoder-decoder approach with word frequency es-
timation. arxiv preprint arxiv:1701.00138 .

sho takase, jun suzuki, naoaki okazaki, tsutomu hi-
rao, and masaaki nagata. 2016. neural headline
generation on id15.
in
empirical methods in natural language process-
ing.

zhaopeng tu, zhengdong lu, yang liu, xiaohua liu,
and hang li. 2016. modeling coverage for neural
in association for computa-
machine translation.
tional linguistics.

oriol vinyals, meire fortunato, and navdeep jaitly.
2015. id193. in neural information pro-
cessing systems.

kelvin xu, jimmy ba, ryan kiros, kyunghyun cho,
aaron c courville, ruslan salakhutdinov, richard s
zemel, and yoshua bengio. 2015. show, attend and
tell: neural image id134 with visual
attention. in international conference on machine
learning.

wenyuan zeng, wenjie luo, sanja fidler, and raquel
ef   cient summarization with
arxiv preprint

urtasun. 2016.
read-again and copy mechanism.
arxiv:1611.03382 .

supplementary material
this appendix provides examples from the test set, with side-by-side comparisons of the reference sum-
maries and the summaries produced by our models. in each example:

    italics denote out-of-vocabulary words
    red denotes factual errors in the summaries
    green shading intensity represents the value of the generation id203 pgen
    yellow shading intensity represents    nal value of the coverage vector at the end of    nal model   s

summarization process.

article (truncated): andy murray came close to giving himself some extra preparation time for
his wedding next week before ensuring that he still has un   nished tennis business to attend to .
the world no 4 is into the semi-   nals of the miami open , but not before getting a scare from
21 year-old austrian dominic thiem , who pushed him to 4-4 in the second set before going
down 3-6 6-4 , 6-1 in an hour and three quarters . murray was awaiting the winner from the
last eight match between tomas berdych and argentina    s juan monaco . prior to this tournament
thiem lost in the second round of a challenger event to soon-to-be new brit aljaz bedene . andy
murray pumps his    rst after defeating dominic thiem to reach the miami open semi    nals .
muray throws his sweatband into the crowd after completing a 3-6 , 6-4 , 6-1 victory in    orida
. murray shakes hands with thiem who he described as a     strong guy     after the game . and
murray has a fairly simple message for any of his fellow british tennis players who might be
agitated about his imminent arrival into the home ranks : do n   t complain . instead the british no
1 believes his colleagues should use the assimilation of the world number 83 , originally from
slovenia , as motivation to better themselves .

reference summary:
british no 1 defeated dominic thiem in miami open quarter    nals .
andy murray celebrated his 500th career win in the previous round .
third seed will play the winner of tomas berdych and juan monaco in the semi    nals of the atp masters
1000 event in key biscayne .

baseline:
andy murray beat unk bedene 6-3 , 6-4 , 6-1 in an hour and three quarters .
british no 1 believes his colleagues should use the maze of the world number 83 , originally from
slovenia , as motivation to better themselves .

pointer-generator, no coverage:
andy murray came close to giving himself some extra preparation for his wedding next week before
getting a scare from 21 year-old austrian dominic thiem .
the world no 4 is into the semi-   nals of the miami open , but not before getting a scare from 21 year-old
austrian dominic thiem .

pointer-generator, with coverage:
andy murray defeated dominic thiem 3-6 6-4 , 6-1 in an hour and three quarters .
murray was awaiting the winner from the last eight match between tomas berdych and argentina
   s juan monaco .
prior to this tournament thiem lost in the second round of a challenger event to soon-to-be new
brit aljaz bedene .

figure 8: the baseline model reports the wrong score 6-3, substitutes bedene for thiem and strug-
gles with the uncommon word assimilation. the pointer-network models accurately reproduce the out-
of-vocabulary words thiem and aljaz. note that the    nal model produces the novel word defeated to
incorporate several fragments into a single sentence.

article (truncated): munster have signed new zealand international francis saili on a two-year
deal . utility back saili , who made his all blacks debut against argentina in 2013 , will move
to the province later this year after the completion of his 2015 contractual commitments . the
24-year-old currently plays for auckland-based super rugby side the blues and was part of the
new zealand under-20 side that won the junior world championship in italy in 2011 . saili    s
signature is something of a coup for munster and head coach anthony foley believes he will
be a great addition to their backline . francis saili has signed a two-year deal to join munster
and will link up with them later this year .     we are really pleased that francis has committed
his future to the province ,     foley told munster    s of   cial website .     he is a talented centre
with an impressive skill-set and he possesses the physical attributes to excel
in the northern
hemisphere .     i believe he will be a great addition to our backline and we look forward to
welcoming him to munster .     saili has been capped twice by new zealand and was part of the
under 20 side that won the junior championship in 2011 .

reference summary:
utility back francis saili will join up with munster later this year .
the new zealand international has signed a two-year contract .
saili made his debut for the all blacks against argentina in 2013 .

baseline:
dutch international francis unk has signed a two-year deal to join irish unk super rugby side the
blues .
unk    s signature is something of a coup for munster and his head coach anthony foley believes he will
be a great addition to their respective prospects .
unk has been capped twice by new zealand .

pointer-generator, no coverage:
new zealand international francis saili will move to the province later this year .
utility back saili made his all blacks debut against argentina in 2013 .
utility back saili will move to the province later this year .

pointer-generator, with coverage:
francis saili has signed a two-year deal to join munster later this year .
the 24-year-old was part of
championship in italy in 2011 .
saili    s signature is something of a coup for munster and head coach anthony foley .

the new zealand under-20 side that won the junior world

figure 9: the baseline model incorrectly substitutes dutch for new zealand (perhaps re   ecting the euro-
pean bias of the dataset), fabricates irish, and struggles with out-of-vocabulary words saili and auckland-
based. though it is not clear why, the phrase addition to our backline is changed to the nonsensical
addition to their respective prospects. the pointer-generator model    xes these accuracy problems, and
the addition of coverage    xes the repetition problem. note that the    nal model skips over large passages
of text to produce shorter sentences.

article (truncated): right from the moment he breezed through the doors at old trafford , louis
van gaal was a man with a plan . the    rst season , he stated , would see him deliver manchester
united back into their rightful place in the champions league . he would restore them to the
premier league top four but loftier aims of silverware would have to wait . his three-year vision
would allow for such thoughts but ,    rst things    rst , united needed to be dining from european
football    s top table again . louis van gaal is close to delivering his    rst-season aim of returning
man united into champions league . wayne rooney smashes home during manchester united    s
3-1 win over aston villa on saturday . united    s win over aston villa took them third , eight
points ahead of    fth-placed liverpool in the table . april 12 manchester city ( h ) . april 18
chelsea ( a ) . april 26 everton ( a ) . may 2 west bromwich albion ( h ) . may 9 crystal
palace ( a ) . may 17 arsenal ( h ) . may 24 hull city ( a ) . one season out of the champions
league was far from ideal , but two seasons would be an absolute disaster and something , he
understood , that would not be tolerated . in november , even that was looking optimistic . a
1-0 defeat to manchester city meant that united had taken just 13 points from their opening 10
matches - it was their worst start to a league campaign since 1986 , when ron atkinson only
lasted until november .

reference summary:
man united have an eight-point cushion from    fth-place liverpool .
van gaal looks likely to deliver on his promise of top four    nish .
but the dutchman has a three-year vision mapped out .
next season will have to see united mount sustained challenge for title .
they must also reach the later stages of the champions league .

baseline:
manchester united beat aston villa 3-1 at old trafford on saturday .
louis van gaal is close to delivering his unk aim of returning man united into the premier league top
four .
louis van gaal is close to delivering his unk aim of returning man united into champions league .

pointer-generator, no coverage:
louis van gaal is close to delivering his    rst-season aim of returning man united into champions league.
united    s win over aston villa took them third , eight points ahead of    fth-placed liverpool in the table .
louis van gaal is close to delivering his    rst-season aim of returning man united into champions league.

pointer-generator, with coverage:
manchester united beat aston villa 3-1 at old trafford on saturday .
louis van gaal is close to delivering his    rst-season aim of returning man united into champions
league .
united needed to be dining from european football    s top table again .

figure 10: in this example, both our baseline model and    nal model produce a completely abstractive
   rst sentence, using a novel word beat.

article (truncated): having been on the receiving end of a 6-1 thumping , a defeat like that
could be justi   ably met with a backlash by angry supporters . watching a 3-1    rst leg aggregate
advantage turn into a 7-4 de   cit come the end of the reverse encounter too could send many
fans apoplectic at
the capitulation of their side . however that does n   t appear the case for
those devoted to porto . porto supporters gave their team a hero    s welcome following their
6-1 defeat at bayern munich on tuesday . porto star striker jackson martinez was one of many
players to look perplexed by their warm reception . porto boss julen lopetegui ( left ) was
hugged by fans congratulating him on their champions league run . police escorts were needed
to keep the delirious supporters at bay as the porto team bus drove past . the team bus was met
with a cacophony of noise from porto supporters proudly chanting about their club . on their
return from a humiliating champions league quarter-   nal loss at the hands of bayern munich on
tuesday night , the squad were given a heroes reception as they arrived back in portugal . in the
early hours of wednesday morning , fans mobbed the squad congratulating them on their run
in the tournament . star striker jackson martinez and ricardo quaresma were one of many porto
players who looked perplexed as they were hugged by fans before they making their way on to
the team bus - set upon a cacophony of    ercely-proud chanting . it was the    rst time that porto
, who had been unbeaten in this season    s tournament up until tuesday night , had reached the
quarter-   nals of the champions league since the 2008-09 season .

reference summary:
bayern munich beat porto 6-1 in their champions league tie on tuesday .
result saw bayern win quarter-   nal encounter 7-4 on aggregate .
it was the    rst-time porto had reached that stage since the 2008-09 season .

baseline:
porto beat bayern munich 2-0 in the champions league on tuesday night .
porto star james unk was one of many players involved in the match .
the squad were given a trophy as they arrived back in portugal .

pointer-generator, no coverage:
porto star striker jackson martinez was one of many players to look perplexed by their warm reception.
porto star striker jackson martinez was one of many players to look perplexed by their warm reception.
porto star striker jackson martinez was one of many players to look perplexed by their warm reception.

pointer-generator, with coverage:
porto star striker jackson martinez was one of many players to look perplexed by their warm
reception .
porto boss julen lopetegui was hugged by fans congratulating him on their champions league
run .
however that does n   t appear the case for those devoted to porto .
police escorts were needed to keep the delirious supporters at bay .

figure 11: the baseline model makes several factual inaccuracies: it claims porto beat bayern munich
not vice versa, the score is changed from 7-4 to 2-0, jackson is changed to james and a heroes reception
is replaced with a trophy. our    nal model produces sentences that are individually accurate, but they
do not make sense as a whole. note that the    nal model omits the parenthesized phrase ( left ) from its
second sentence.

article:
( id98 )     it    s showtime !     michael keaton paid homage     ever so slightly     to
his roles in     beetlejuice     and     batman     in his third turn hosting     saturday night live    
this weekend . keaton acknowledged in his opening monologue that a lot has changed since
he    rst hosted the comedy sketch show in 1982 .     i had a baby     he    s 31 . i also have
a new girlfriend     she    s 28 ,     he said . fans who were hoping for a full-blown revival of
keaton    s most memorable characters might have been a little disappointed . snl cast members
taran killam and bobby moynihan begged the actor with a song to     play     batman and
beetlejuice with them . all they got in response were a couple of one-liners . overall , keaton
   s performance drew high marks from viewers and critics for its     deadpan     manner and    
unpredictable weirdness ,     in the words of a.v . club    s dennis perkins . fans also delighted
in a cameo from     walking dead     star norman reedus during weekend update . keaton scored
some laughs from the audience as an ad executive who    s not very good at his job , a confused
grandfather and a high school teacher who gets asked to the prom in a riff on the romantic
comedy     she    s all that .     other crowd-pleasing spots included a scientology parody music
video and a news conference spoo   ng the ncaa student-athlete debate . the show also poked fun
at id98 with cast member cecily strong playing anchor brooke baldwin . baldwin said on twitter
that she    s taking it as a     crazy compliment     and shared a clip from the skit .

reference summary:
michael keaton hosted     saturday night live     for the    rst time in 1982 .
in 2015 , his nods to starring roles in     beetlejuice     and     batman     are brief .

baseline:
    snl     wins     saturday night live     .
keaton acknowledged in his opening     saturday night live     .
keaton acknowledged in his opening night    s     unk     .
the show also pokes fun at id98 with cast member mariah carey .

pointer-generator, no coverage:
    beetlejuice     michael keaton paid homage to his roles in     beetlejuice     and     batman     in his third
turn hosting     saturday night live     this weekend .
keaton acknowledged in his opening monologue that a lot has changed since he    rst hosted the comedy
sketch show in 1982 .

pointer-generator, with coverage:
michael keaton paid homage to his roles in     beetlejuice     and     batman     in his third turn
hosting     saturday night live     .
keaton acknowledged in his opening monologue that a lot has changed since he    rst hosted the
comedy sketch show in 1982 .

figure 12: baseline model replaces cecily strong with mariah carey, and produces generally nonsensical
output. the baseline model may be struggling with the out-of-vocabulary word beetlejuice, or perhaps
the unusual non-news format of the article. note that the    nal model omits     ever so slightly     from its
   rst sentence.

article (truncated): they are supposed to be the dream team who can solve the conundrum
of how to win the election . but david cameron and boris johnson were left scratching their
heads today as they struggled with a children    s jigsaw teaching toddlers about
the seasons
. as the london mayor tried to hammer ill-   tting pieces together with his hands , the prime
minister tried out what could be a new election slogan , telling him :     if in doubt , wedge it
in .     after being put right by a four-year-old who spotted their errors , the pair had more fun
   nger painting with tory blue paint . david cameron and boris johnson were left stumped by
the puzzle at advantage children    s day nursery in surbiton , as three-year-old stephanie looked
on . when they tried to put the puzzle back together , they hit trouble after it proved to be
more dif   cult than expected . the conservative duo made their    rst appearance together on the
campaign trail with a visit to advantage day nursery in surbiton , south west london . they were
supposed to be highlighting tory plans to double free childcare for 600,000 children aged three
and four . they sat at a table with joshua and stephanie , both aged three , and four-year-olds
hamish and leo where they broke up a puzzle which depicted the four seasons . when they tried
to put it back together , they hit trouble after it proved to be more dif   cult than expected .

reference summary:
david cameron and boris johnson visit nursery to promote childcare .
duo left    ummoxed by jigsaw aimed at teaching children about seasons .
london mayor resorted to brute force to hammer wrong pieces together .
prime minister gave his approval , telling boris :     if in doubt , wedge it in     .

baseline:
david cameron and boris johnson were left mysti   ed by the puzzle .
they were at the nursery in unk to promote a tory pledge to double free childcare for 25 children aged
three and four .
the pair were at the nursery in unk to promote a tory pledge to double free childcare for children aged
three and four .

pointer-generator, no coverage:
london mayor tried to hammer ill-   tting pieces together with his hands .
they were supposed to be highlighting tory plans to double free childcare .
they were supposed to be highlighting tory plans to double free childcare .
they were supposed to be highlighting tory plans to double free childcare .

pointer-generator, with coverage:
london mayor tried to hammer ill-   tting pieces together with his hands .
prime minister tried out what could be a new election slogan .
but they hit trouble after it proved to be more dif   cult than expected .

figure 13: the baseline model appropriately replaces stumped with novel word mysti   ed. however, the
reference summary chooses    ummoxed (also novel) so the choice of mysti   ed is not rewarded by the
id8 metric. the baseline model also incorrectly substitutes 600,000 for 25. in the    nal model   s
output we observe that the generation id203 is largest at the beginning of sentences (especially the
   rst verb) and on periods.

article (truncated):
lagos , nigeria ( id98 ) a day after winning nigeria    s presidency ,
muhammadu buhari
told id98    s christiane amanpour
that he plans to aggressively    ght
corruption that has long plagued nigeria and go after the root of the nation    s unrest . buhari
said he    ll     rapidly give attention     to curbing violence in the northeast part of nigeria
, where the terrorist group boko haram operates . by cooperating with neighboring nations
chad , cameroon and niger , he said his administration is con   dent it will be able to thwart
criminals and others contributing to nigeria    s instability . for the    rst time in nigeria    s history
, the opposition defeated the ruling party in democratic elections . buhari defeated incumbent
goodluck jonathan by about 2 million votes , according to nigeria    s independent national
electoral commission . the win comes after a long history of military rule , coups and botched
attempts at democracy in africa    s most populous nation .

reference summary:
muhammadu buhari tells id98    s christiane amanpour that he will    ght corruption in nigeria .
nigeria is the most populous country in africa and is grappling with violent boko haram extremists .
nigeria is also africa    s biggest economy , but up to 70 % of nigerians live on less than a dollar a day .

baseline:
unk unk says his administration is con   dent it will be able to destabilize nigeria    s economy .
unk says his administration is con   dent it will be able to thwart criminals and other nigerians .
he says the country has long nigeria and nigeria    s economy .

pointer-generator, no coverage:
muhammadu buhari says he plans to aggressively    ght corruption in the northeast part of nigeria .
he says he    ll     rapidly give attention     to curbing violence in the northeast part of nigeria .
he says his administration is con   dent it will be able to thwart criminals .

pointer-generator, with coverage:
muhammadu buhari says he plans to aggressively    ght corruption that has long plagued nigeria
.
he says his administration is con   dent it will be able to thwart criminals .
the win comes after a long history of military rule , coups and botched attempts at democracy
in africa    s most populous nation .

figure 14: the baseline model incorrectly changes thwart criminals and others contributing to nigeria   s
instability to destabilize nigeria   s economy     which has a mostly opposite meaning. it also produces a
nonsensical sentence. note that our    nal model produces the novel word says to paraphrase told id98    s
christiane amanpour.

article: cairo ( id98 ) at least 12 people were killed sunday , and more injured , in separate
attacks on a police station , a checkpoint and along a highway in egypt    s northern sinai
,
authorities said . six people , including one civilian , were killed when a car bomb exploded
near the police station in al-arish , capital of north sinai , health ministry spokesman hossam
abdel-ghafar told ahram online . he said 40 people were injured . ansar beit al-maqdis , an isis
af   liate , claimed responsibility for the attack , which came hours after another operation that
the group also claimed . in that earlier attack , a    rst lieutenant , a sergeant and four conscripts
were killed when their armored vehicle was attacked on the highway from al-arish to sheikh
zuweid in northern sinai , the military said . two other soldiers were injured and taken to a
military hospital . ansar beit al-maqdis has claimed many attacks against the army and police
in sinai . a third attack sunday on a checkpoint in rafah left three security personnel injured
, after unknown assailants opened    re at them , according to state media . the attacks come
as the military announced a reshuf   e of several senior military positions , state media reported
. among those being replaced are the generals in charge of military intelligence and egypt    s
second    eld army , which is spearheading the battle against the insurgents in the northern sinai
. egypt    s army has been    ghting a decade-long militant islamist insurgency , which has spiked
since the ouster of muslim brotherhood president mohamed morsy in the summer of 2013 .
hundreds of police and soldiers , as well as civilians , have been killed in militant attacks in
the past months . ian lee reported from cairo . anas hamdan reported from atlanta .

reference summary:
six people , including one civilian , are killed when a car bomb explodes near a police station .
six others are killed when their armored vehicle is attacked on a highway in northern sinai .
ansar beit al-maqdis , an isis af   liate , claims responsibility .

baseline:
he says the attacks come after another operation that killed 12 .
the attacks come as military announced a u.n. peacekeeping force in northern sinai .

pointer-generator, no coverage:
ansar beit al-maqdis , an isis af   liate , claimed responsibility for the attack .
ansar beit al-maqdis , an isis af   liate , claimed responsibility for the attack .
the attacks come as the military announced a reshuf   e of several senior military positions .

including one civilian , were killed when a car bomb explodes near the police

pointer-generator, with coverage:
six people ,
station .
ansar beit al-maqdis , an isis af   liate , claimed responsibility for the attack .
egypt    s army has been    ghting a decade-long militant islamist insurgency .

figure 15: the baseline model fabricates a completely false detail about a u.n. peacekeeping force that
is not mentioned in the article. this is most likely inspired by a connection between u.n. peacekeeping
forces and northern sinai in the training data. the pointer-generator model is more accurate, correctly
reporting the reshuf   e of several senior military positions.

