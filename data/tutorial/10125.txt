8
0
0
2

 
t
c
o
9
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
6
1
4
3

.

0
1
8
0
:
v
i
x
r
a

text as statistical mechanics object

k. koroutchev1 and e.korutcheva2   

1escuela polit  ecnica superior, universidad aut  onoma de madrid,

28049 canto blanco, madrid, spain

2g.nadjakov inst. solid state physics,

bulgarian academy of sciences, so   a, bulgaria

and

the abdus salam international center for theoretical physics,

trieste, italy

miramare, trieste

july, 2007

keywords: human written text, text statistics, grammar, id178, energy, statistical
mechanics

abstract

in this article we present a model of human written text based on statistical
mechanics approach by deriving the potential energy for di   erent parts of the
text using large text corpus.

we have checked the results numerically and found that the    speci   c heat   
parameter e   ectively separates the closed class words from the speci   c terms
used in the text.

1

introduction

let us imagine that we are looking for some article on the web. probably the    rst
thing we will do is to enter in google and type some keywords. if we type a query
like    i am looking for an article about statistical mechanics of images   , although it is
exactly what we want, we will probably get nothing related to the subject or we will
get only a content partially related to it. so, it would be better to re   ne the query to
something like    image    statistical mechanics       in order to get some reasonable results.
to extract useful words we do not use the structure of the language     actually we
ignore it. also, we hardly use the common words of the sentence in the query. what
we use are some statistical estimations of the parts of the query and words that stick

   also at depto. f    sica fundamental, uned c/senda del rey no 9, 28080 madrid, spain

1

well with the meaning of the query. this leads us to the idea to regard the text as a
statistical mixture of its parts that sticks well with its meaning. of course, the text
must stick well with the language in which it is written as well. therefore, we can
consider the text as conditioned to the language in which it is written. we can even
consider that the text must stick well to the area in which it belongs, as for example
   nonlinear physics    or    novels of 17th century   .

google is a product of some 15 years evolution of web page search. what this
evolution shows is that if we are looking for the meaning of a text, we must look for
speci   c, statistically salient keywords, that are supposed to be present in it, largely
ignoring the syntactic and the semantics structure of the language. this gives us the
inspiration to build a statistical mechanics model of a human written text, considering
it as composed by its    particles        the words.

the best way to do the analysis of a text, written is some language1, is to have
some exact descriptions of the language, for example, weighted id18.
however, it is not clear if such a description exists, because usually people do not speak
grammatically correctly. some trivial grammars always exist, for example grammars
that allow all possible strings of the alphabet. due to the fact that these strings do
not constrain the expressions, the information they carry regarding the statistical
properties of the language, is very poor. moreover, having in mind the zipf   s law
of the frequency distribution of the words [1], even if reasonable grammar exists,
in a single text of arbitrary length we will have some 40% halomorphemes.2 as a
consequence, the length of the grammar will be of the order of the length of the text
for any text we choose. therefore, it is easier to consider the language as a set of
all the texts spoken/written in that language. using statistical arguments, we do not
need all texts, but only a signi   cantly large random set of texts in order to treat the
problem.

the model we investigate consists of a text t and a vocabulary v , written in one
and the same language. the vocabulary is formed using all the words of some huge
collection of texts, written in that language.

a text that treats some well-de   ned subject is highly restricted by this subject.
the language, as a whole, has no such restriction. therefore, the relative excess (or
higher frequency) of a word in the vocabulary is a normal situation.

on contrary, the relative excess of a word in the text has a speci   c meaning. if
the word is with much higher frequency of occurrence in the text than in the common
language, that can be interpreted as an indication that this text treats exactly a
subject expressed by this word, e.g. that the word is a speci   c term or keyword in the
text. this is the    rst class of words in the text that we will consider in this paper.

on the other hand, the text will always contain words that are common in the lan-
guage, which have more or less the same frequency in any text and in the vocabulary.
a large fraction of the words of that type will be formed by the so-called function
words. these words by themselves carry no meaning, but are essential for expressing

1here we will not consider texts like the genome   s sequence, computer logs, multi-language texts

and similar.

2halomorphemes are the words that occur only once in a text.

2

the language structure. a typical example of a function word in english is the word
   the   . a similar and more strictly de   ned category is the class of closed class words
that, by de   nition, are the words which do not change their form in any text.

finally, the third class of words that will follow more or less the same frequency
distribution in the text and in the vocabulary are the common words. they serve to
transmit the meaning of the text, but are common for every text that must explain
some concept, for example, like the word    explain    in this sentence.

the paper is organized in the following way: in section 2 we de   ne the model and
the approximations used. in section 3 we calculate numerically the thermodynamic
quantities for a set of arbitrary selected texts. we show that the speci   c terms (key-
words) and the rest of the text have di   erent thermodynamic behavior. finally in
section 4 we present our conclusions.

2 the model

the relationship between some text and its language can be considered as a condi-
tional distribution. in this article however we consider the following approach:

we consider the vocabulary as a solid state basement, composed by    molecules   ,
which are actually the parts of the text (the words of the language). the text itself
is considered as a liquid solution of    molecules   , derived in the same manner as the
vocabulary. the text and the vocabulary    react    and there exists some energy gain
when the reaction takes place, so some    molecules    are settled down on the solid base.
the excess of the    molecules    (words) of a given type in the vocabulary, e.g. in
the    solid    compound, has no signi   cant meaning. therefore, we can concentrate only
on the    liquid    phase, considering this phase as a signi   cant one. equivalently, we
can consider only on the deposited part of the    molecules    that have been entered in
reaction with the vocabulary. the molecules as a    rst approximation can be assumed
to react only if they represent one and the same word.

more rigorously, our model consists of a vocabulary with length lv, a text of length
lt, the    molecules    (words) of the text, w, that are matched with the    molecules    of
the vocabulary and the corresponding number of occurrences of these    molecules   :
nt(w) for the text and nv(w) for the vocabulary. in order to ful   ll the requirement of
equal molar mass we can introduce some standard text length l0 and normalize the
number of occurrence of w according to this length:

nt(w) = l0

nt(w)

lt

, nv(w) = l0

nv(w)

lv

.

for convenience we choose l0 = lt in the numerical experiments. we denote by m(w)
the number of deposited molecules, normalized to a length l0. this parameter will
be used below as an order parameter for the system.

the problem of regarding the text as a thermodynamic system consists of de   ning
the    molecules    w and the energy of interaction e(w) = e(m(w), nt(w), nv(w), l0)
between them.

3

in this article we will regard the    molecules    as usual english words, consisting of
continuous strings of letters, separated by non-letter symbols. in the rest of the article,
we will not distinguish between    molecules    and words. we will assume that the words
are independent, e.g. that there is no interaction between the di   erent words. due
to this independence, the extensive thermodynamics quantities, as for example the
free energy, will be given by the sum of the individual quantities corresponding to
the di   erent words. therefore we can build a theory, based on a single word, and
extrapolate it on the whole text.

further, we will consider that the language (the solid compound) imposes some
potential energy    eld depending on the parameters nv, lv, but not on the text, e.g.
not on nt. we also assume that the system is in thermal equilibrium.

according to the general thermodynamics principles, the state of the system can
be described only by its energy e. the id203 p (m) of the state with m deposited
molecules (words) is:

p (m)     g(m) exp(     e(m, nt, nv, l0)),

(1)

where e(m, nt, nv, l0) is the energy of settling m molecules, g(m) is the number of
degeneration of these states and    is the inverse temperature        1/t .

the number of degeneration is just the number of ways one can select m molecules

out of a set of nt molecules, e.g. (cid:16)nt

due to the fact that we have only nt molecules.

m(cid:17). note that this number is strictly zero if m > nt

regarding that system, one can impose the requirement that its thermodynamic
properties scale with the length of the texts, e.g. if we scale simultaneously the size
of the vocabulary and the size of the text by s, then the thermodynamic potentials
scale as:

e(sm, snt, snv, sl0) = se(m, nt, nv, l0)

and

log(g(sm)) = s log(g(m)).

considering the frequency of occurrence of a single word w in a text with length l,
in order to speak about something measurable, we must regard the case where the
word occurs in the text x     1 times.

let us assume that we have a language derived from some context free weighted
grammar with terminal symbols     the words of the language. being interested only
on the frequency of the words, we can transform any language de   nition a :   w   into
a : w    . further, all words di   erent from w can be regarded as one and the same
word, say v, thus obtaining a grammar with only two terminal symbols. we can join
further in one symbol all non-terminal symbols that can not produce w.

the simplest grammar of this type is:

s : wr
r : vr [p]
r :

  

[1     p],

4

(2)

where s is the axiom, r is an auxiliary non-terminal symbol and    is en empty string.
within the square brackets we represented the probabilities for the corresponding
de   nitions. this grammar generates sentences containing w with lengths that have
exponentially falling frequencies distributions.

fixing the length of the text, we can obtain the id203 distribution of w as a
sum of    nite number of exponentially distributed variables. by de   nition [2], this is
the gamma distribution:

p (x; w) = e   xbxa   1ba/  (a).

(3)

here the parameter a is proportional to the length of the text l, while the parameter
b does not depend on it, but rather on the word and the class of text we are regarding.
although the above consideration is not strict, it can give an idea about the type of
the word frequency distribution for a text with a    xed length. using a general context
free grammar, one can    nd the expressions for the id203 and the length of the
sentence, following the techniques developed in [3].

we have checked the hypothesis for the gamma distribution on a set of about
19000 english texts given by the gutenberg collection and have found an excellent
agreement with the experimental data for all the words with frequency of occurrence
p(w) > 5/10000.

later we have studies the asymptotic behavior of the distribution. for this aim, we

have replicated the text s times and have considered the limit lims      [log p (sx; w; sa, b)]/s.
one can easily    nd that this limit is a     bx     a log a + a log x + a log b. using that the
mean of x is   x = ab, we    nally obtained the following expression for the asymptotic
behavior of p (x):

ep(x; w) = log p (x) =   xb(cid:20)1    

x
  x

+ log(cid:18) x

  x(cid:19)(cid:21) .

(4)

ep can be regarded as a potential energy of the word w in the language. the linear
member accounts for the excess of words of a given type in the text, while the loga-
rithmic one corresponds to the entropic part of the energy [4]. a typical energy curve
is given in fig. 1.

xm=1

using the above considerations, the corresponding partition function for a given

word w is:

nt

z(w,   ) =

g(m, nt)) exp(     ep(m, nt)),

(5)

of the equation. thus, the expression for the partition function is:

m(cid:17) and we have omitted the argument w in the right hand side

where g(m, nt) = (cid:16)nt

z(w,   ) =

nt

xm=1

exp(     etot(m, nt)),

where

etot(m, nt) =    

nvb(cid:20)1    

m
nv

1
  

m! +
log nt
+ log(cid:18) m
nv(cid:19)(cid:21)

5

(6)

(7)

)

 
e
v
a

x
/
x
(

p

e

6

5

4

3

2

1

0
0

1

2

x/xave

3

4

5

figure 1: potential energy of a word according to the number of words. it consists of two
parts     the logarithmic falling part, varying for values of the argument from zero to the
mean frequency of the word, and a linear increasing part, predominant at the range where
the frequency of the word is larger than its mean frequency.

is the total energy corresponding to a given word w. it is composed by a potential
part ep and a combinatorial part 1

   log g(m, nt).

finally, the full free energy of the text is given by the sum over the di   erent words

of the text:

f (  ) =    

1

   xw

log z(w,   ).

(8)

the equation for the order parameter m can be obtained by using the saddle-point
method. its application, combined with the stirling approximation, log n!     n log n    
n, n     1, gives the following equation for the order parameter m:

detot
dm

=

1
  

log

m

nt     m

+ b

nv     m

m

= 0.

(9)

this equation can be solved in closed form and the solution is:

m = nt

b  nv/nt

b  nv/nt + w (b  nv/nt eb     b  nv /nt)

,

(10)

where w (.) is the lambert w function [2].

the id178 s for a single word is:

s        

   f
   t

= nt log nt     m log m    

(nt     m) log(nt     m).

(11)

substituting eq. (10) in eq. (11), we obtained the behavior of the id178 as a

function of the inverse temperature and the ratio nv/nt shown in fig. 2.

finally, the second derivative of the free energy, which is related to the    speci   c

heat   ,

cv =    t      2f
   t 2!v

,

6

2
2

b   
b

4
4

6
6

8
8

0.6

0.4

s

0.2

0

0
0

2
2

4
4

6
6
nv nt
nv nt

8
8

10

figure 2: the id178 for a single word.

0.3

cv

0.2

0.1

0
0
0

20

15

10

t

5

2
2

4
4

nv nt
nv nt

6
6

8
8

0

10

cv

0.3

0.2

0.1

1

2

3

  

figure 3: the    speci   c heat    cv .

is represented in fig. 3. we have used the notation adopted in the thermodynamics for
isochoric process, cv , although what is    xed in the present approach is the number of
occurrences for a given word. a section of that    gure for b = 1, nv = 5 is represented
in the right panel of the same    gure.

3 numerical experiments

to check the above results experimentally on real texts, we used a collection of about
19000 english texts from the gutenberg project (gc) with size of 5.107 words (gc).
we also used a collection of 500 articles from the area of the non-linear physics (nl)
given by the repository xxx.archiv.gov. in order to avoid problems with the di   erent
multiple versions of the articles, we used only the    rst version of each one. finally,
we used a list of 257 closed-class words of english instead of function words.

for estimating the parameters a and b of the distribution of a single word, we used
gc. we found that b is within the range [0.01-20] with an average value 0.25. the
value of the parameter a, for a text with a length l = 10000, is within the range [0-2.6].
note that the parameters a and b are well de   ned and with a su   cient con   dence

7

only if p(w)l     1. for practical purposes we can say that within a corpus of 108
words, these two parameters are well de   ned only for the 2400 most frequent words.
for the rest of the words, we used some simplifying assumption, due to the fact that
one can not prove or disprove reliably the hypothesis by using two degrees of freedom
(a and b), having less then four measures for their estimation. the hypothesis, we
have adopted, was that the less frequent words have the same value for the parameter
b of the distribution. thus we can join all the words, that are not frequent enough,
and estimate that parameter. the results are very close to the mean vale of b. the
parameter a, being proportional to the length of the text, is not so critical for the
estimation (actually we need only nv and b).

cv

15

10

5

cv

0.8

0.6

0.4

0.2

0.1

0.2

0.3

0.4

t

0.5

0.1

0.2

0.3

0.4

t

0.5

figure 4: the speci   c heat cv for di   erent words belonging to one and the same text. the
upper two curves in the left panel represent two di   erent terms (   topology    and    topolog-
ical   ). the lower curves of the left panel represent two function words (   the    and    are   ).
on the right panel, the curve of the word    are    is zoomed in order to represent it together
with the typical common word    important   .

figs. 4 show the typical behavior of the speci   c heat cv for di   erent kind of
words: for terms (the two upper curves on the left panel), for function words (the
two lower curves of the same panel) and for common words (the lower curve on the
right panel). one can observe that cv (nt, nv, b) represents di   erent behavior for the
di   erent word classes with corresponding maxima belonging to di   erent temperature
ranges.

because the function words have a higher frequency of occurrence, one can expect
that they play a predominant role for the behavior of the speci   c heat. as we can
observe, this is not true: the speci   c heat for the terms is much higher than the one
corresponding to the function words. these results can be interpreted as an indication
than the most vulnerable parts of the speech are carried by the common words, while
the most resistant ones are carried by the domain-speci   c terms.

4 conclusion

in the present article we proposes a statistical mechanics approach for the analysis of
the human written text. by introducing an energy, that describes the system, taking
into consideration a realistic distribution of the words inside a large text corpus, we
were able to derive the thermodynamic parameters of the system in a closed analytical
form.

8

by studying the behavior of the speci   c heat of the system, we have shown that
this quantity is di   erent for di   erent kinds of words (terms, function words and com-
mon words).

we have applied the above method to di   erent corpora of texts and we have found
one and the same universal behavior, which does not depend on the particular text.
our numerical results show that the    speci   c heat    e   ectively separates the closed

class words from the speci   c terms and the common words used in the text.

acknowledgments:

the authors thank the    nancial help from the international center for theoretical
physics, trieste, italy, where the main part of this work has been performed. espe-
cially we would like to thank the statistical physics group at ictp for stimulating
discussions. the work is partly supported by grants tin 2004-07676-g01-01 (k.k.)
and dgi.m.cyt fis2005-1729 (e.k.) from the spanish ministry of science and ed-
ucation.

references

[1] g.k. zipf, the psychobiology of language, houghton-mi   in, new york, ny,

(1935).

[2] m. abramowitz and i. a. stegun, (eds.) handbook of mathematical functions

with formulas, graphs, and mathematical tables, wiley, (1984).

[3] a. stolcke and j. segal, precise id165 probabilities from stochastic context-free
grammars, annual meeting of the acl; proc. 32nd annual meeting on acl,
las cruces, new mexico, (1994) pp. 74 - 79.

[4] y. peng, and m. goldberger, statistical physics approach to categorize biological
signals: from heart rate dynamics to dna sequences, chaos, 17 (2007) v015115.

9

