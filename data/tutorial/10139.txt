the inconsistency of the h-index 

 

ludo waltman and nees jan van eck 

 

centre for science and technology studies, leiden university, the netherlands 

{waltmanlr, ecknjpvan}@cwts.leidenuniv.nl 

 
 
 
 
 

the h-index is a popular bibliometric indicator for assessing individual scientists. we criticize the h-
index from a theoretical point of view. we argue that for the purpose of measuring the overall scientific 
impact of a scientist (or some other unit of analysis) the h-index behaves in a counterintuitive way. in 
certain cases, the mechanism used by the h-index to aggregate publication and citation statistics into a 
single number leads to inconsistencies in the way in which scientists are ranked. our conclusion is that 
the  h-index  cannot  be  considered  an  appropriate  indicator  of  a  scientist   s  overall  scientific  impact. 
based on recent theoretical insights, we discuss what kind of indicators can be used as an alternative to 
the h-index. we pay special attention to the highly cited publications indicator. this indicator has a lot 
in common with the h-index, but unlike the h-index it does not produce inconsistent rankings. 

1. introduction 

the introduction of the h-index (or hirsch index) in 2005 has had an enormous 
influence on bibliometric and scientometric research. as can be seen in figure 1, in 
2010 and 2011 almost one out of four publications in scientometrics and journal of 
informetrics  cited  the  paper  in  which  physicist  jorge  e.  hirsch  proposed  his  index 
(hirsch, 2005). a large  part of the literature building on hirsch    work is concerned 
with introducing variants, extensions, and generalizations of the h-index. in a recent 
study (bornmann, mutz, hug, & daniel, 2011), no less than 37 variants of the h-index 
were listed. 

research  into  the  development  of  new  h-index  variants  and,  more  generally,  of 
new  bibliometric  indicators  often  proceeds  in  a  somewhat  ad  hoc  fashion  (see  also 
marchant, 2009b). researchers take an indicator, identify a property of the indicator 
which  they  argue  is  undesirable,  and  then  propose  a  new  indicator  which  does  not 
have  this  undesirable  property.  the  weakness  of  this  approach  to  indicator 
development is that in most cases it is unsystematic. the choice of a new indicator is 
often made in a somewhat arbitrary way, and there usually is no clear overall picture 
of the properties of the new indicator and of the way in which the indicator compares 
with existing indicators. 

the literature on bibliometric indicators, and in particular on the h-index and its 
variants, is also strongly empirically oriented. for instance, new indicators are often 
justified mainly based on empirical grounds, by arguing that the results produced by 
an  indicator  are  in  agreement  with  what  appears  to  be  intuitively  reasonable. 
similarly,  comparisons  between  indicators  are  often  performed  empirically,  for 
instance  by  analyzing  the  strength  of  the  correlation  between  indicators  (e.g., 
bornmann et al., 2011). instead of studying indicators empirically, indicators can also 
be  studied  from  a  theoretical  point  of  view.  in  theoretically  oriented  research, 
indicators are compared with each other based on their mathematical properties. if an 
indicator  has  properties  that  are  considered  desirable,  this  provides  support  for  the 
indicator, and the other way around, if an indicator has properties that are considered 

 

1 

undesirable,  this  may  be  a  reason  for  rejecting  the  indicator.  in  the  literature  on 
bibliometric indicators, theoretical approaches are less commonly used than empirical 
ones. nevertheless, in recent years, a considerable number of theoretical studies have 
appeared,  both  on  the  h-index  (e.g.,  marchant,  2009a;  quesada,  2010;  woeginger, 
2008)1 and on other types of indicators (e.g., albarr  n, ortu  o, & ruiz-castillo, 2011; 
bouyssou  &  marchant,  2011a,  2011b;  marchant,  2009b;  palacios-huerta  &  volij, 
2004; ravallion & wagstaff, 2011; waltman & van eck, 2009a, 2010; waltman, van 
eck,  van  leeuwen,  visser,  &  van  raan,  2011).  in  the  present  paper,  we  build  on 
some of this earlier research. 
 

figure 1. percentage publications in scientometrics and journal of informetrics citing 
hirsch (2005). (data retrieved from the web of science database on august 6, 2011. 
data for 2011 are incomplete.) 
 

 

the main objective of our paper is to criticize the h-index and its variants from a 
theoretical  point  of  view.2  because  of  the  somewhat  unsystematic  and  strongly 
empirically oriented nature of most h-index research, we believe that a fundamental 
problem of the h-index has remained largely unnoticed. we argue that for the purpose 
of  measuring  the  overall  scientific  impact  of  a  scientist  (or  some  other  unit  of 
analysis) the h-index exhibits counterintuitive behavior. more specifically, we assert 
that the mechanism used by the h-index to aggregate publication and citation statistics 
into a single number leads to an inconsistent way of ranking scientists. the conclusion 
that  we  draw  from  this  is  that  the  h-index  cannot  be  considered  an  appropriate 
indicator of the overall scientific impact of a scientist. 

it is not our aim to argue in favor of a single alternative to the h-index. instead, 
based on recent theoretical insights (marchant, 2009a, 2009b; waltman & van eck, 
2009a), we discuss a large family of bibliometric indicators that do not suffer from the 
same  fundamental  problem  as  the  h-index.  this  family  of  indicators  offers  a  broad 
                                                 
1  another  stream  of  theoretical  research,  which  is  less  relevant  for  our  present  work,  studies  the 
properties of the h-index in a model-based framework. examples of this literature include the work of 
burrell (2007), egghe and rousseau (2006), and gl  nzel (2006). 
2  an  earlier  version  of  the  argument  that  we  are  going  to  present  has  been  published  in  a  short 
contribution to the issi newsletter (waltman & van eck, 2009b). 

 

2 

range of theoretically well-founded alternatives to the h-index. there is one indicator 
to which we pay special attention. this is the highly cited publications indicator. this 
indicator  has  a  lot  in  common  with  the  h-index,  but  unlike  the  h-index  it  does  not 
produce inconsistent rankings. 

the organization of this paper is as follows. we first discuss the h-index and some 
of its properties. we then argue that the h-index has an inconsistency problem, and we 
discuss possible alternatives to the h-index. we close the paper with some concluding 
remarks. because we want the paper to be accessible to a broad audience, we present 
our  theoretical  argument  completely  in  intuitive  terms.  we  use  almost  no  formal 
mathematical notation. 

2. definition of the h-index 

the  h-index  is  defined  as  follows:  a  scientist  has  an  h-index  of  h  if  h  of  his 
publications each have at least  h citations and his remaining publications each have 
fewer than  h + 1 citations (hirsch, 2005). the definition of the  h-index can also be 
applied  to  other  units  of  analysis  than  scientists,  for  instance  to  research  groups 
(hirsch, 2005; van raan, 2006) and journals (braun, gl  nzel, & schubert, 2006). in 
this paper, we mostly use scientists as our unit of analysis, but the argumentation that 
we present applies equally well to other units of analysis. 

figure  2  provides  a  graphical  illustration  of  the  calculation  of  the  h-index.  the 
figure  shows  the  distribution  of  citations  over  the  publications  of  a  scientist.  the 
publications of the scientist are sorted in decreasing order of their number of citations. 
the figure also shows a 45 degree line through the origin. the h-index is obtained by 
identifying the intersection point of the 45 degree line and the citation curve and by 
taking  the  corresponding  number  of  publications.  in  the  example  in  figure  2,  this 
yields an h-index of six. 
 

figure  2.  graphical  illustration  of  the  calculation  of  the  h-index.  the  h-index  is 
obtained  by  identifying  the  intersection  point  of  the  45  degree  line  and  the  citation 
curve and by taking the corresponding number of publications. in this example, the h-
index equals six. 
 

 

 

3 

the  h-index  provides  an  alternative  to  simply  counting  the  total  number  of 
citations of a scientist. according to hirsch (2005), a disadvantage of the total number 
of citations indicator is that it    may be inflated by a small number of    big hits       (p. 
16569). in other words, the total number of citations indicator is too sensitive to one 
or  a  few  highly  cited  publications.  the  h-index  does  not  have  this  disadvantage. 
hirsch    argument in favor of the h-index over the total number of citations indicator 
seems to be accepted by most authors,3 and it is sometimes added that the h-index not 
only  has  the  advantage  of  being  relatively  insensitive  to  a  few  highly  cited 
publications but also of being insensitive to large numbers of lowly cited and uncited 
publications (e.g., bornmann & daniel, 2007; braun et al., 2006). 

the h-index can also be seen as an alternative to counting the number of highly 
cited publications of a scientist, where a publication is regarded as highly cited if its 
number  of  citations  exceeds  a  certain  threshold.  according  to  hirsch  (2005),  a 
disadvantage  of  the  highly  cited  publications  indicator  is  that  the  threshold  for 
determining  what  counts  as  highly  cited     is  arbitrary  and  will  randomly  favor  or 
disfavor individuals    (p. 16569). in hirsch    view, the h-index has the advantage that it 
does  not  depend  on  a  parameter  with  an  arbitrary  value.  as  discussed  in  an  earlier 
paper (van eck & waltman, 2008), we do not agree with this reasoning. although the 
h-index  does  not  have  any  explicit  parameters,  its  definition  does  involve 
arbitrariness.  for  instance,  the  h-index  could  equally  well  have  been  defined  as 
follows: a scientist has an h-index of h if h of his publications each have at least 2h 
citations and his remaining publications each have fewer than 2(h + 1) citations. or 
the following definition could have been proposed: a scientist has an h-index of h if h 
of  his  publications each  have at  least  h  /  2  citations  and  his  remaining  publications 
each have fewer than (h + 1) / 2 citations. a priori, we see no good reason why the 
original definition of the h-index would be better than these two alternative definitions 
and  other similar  ones. because  of  this,  we  conclude  that, just  like  the highly  cited 
publications indicator, the h-index is subject to arbitrariness. the arbitrariness of the 
definition of the  h-index  is  illustrated  graphically in  figure  3.  in  this  figure,  like  in 
figure 2, the h-index is obtained by identifying the intersection point of the 45 degree 
line and the citation curve. our point is that there is no clear reason for the use of a 45 
degree line. as illustrated in figure 3, one could equally well use, for instance, a 30 
degree  line  or  a  60  degree  line.  clearly,  different  lines  may  cause  scientists  to  be 
ranked  differently.  we  note  that  the  arbitrariness  of  the  definition  of  the  h-index  is 
also recognized by lehmann, jackson, and lautrup (2006, 2008) and ellison (2010). 
lehmann et al. (2008) correctly point out that the h-index is based on a comparison of 
two quantities with different units (i.e., publications and citations). such a comparison 
always involves arbitrariness.4 we will come back to the arbitrariness of the definition 
of the h-index later on in this paper. 

the above discussion has focused on a few specific aspects of the  h-index. the 
literature on the  h-index is extensive, and we cannot fully cover it here. for review 
papers  of  the  h-index  literature,  we  refer  to  bornmann  and  daniel  (2007),  alonso, 
cabrerizo,  herrera-viedma,  and  herrera  (2009),  egghe  (2010),  and  norris  and 

                                                 
3  however,  egghe  (2006)  considers  the  insensitivity  of  the  h-index  to  highly  cited  publications  a 
disadvantage. this has motivated the development of the g-index, which seems to be one of the most 
popular variants of the h-index. 
4 given the arbitrariness of the definition of the h-index, there is room for generalizing the index and 
for  proposing  simple  variants  of  it.  generalizations  of  the  h-index  are  discussed  by  van  eck  and 
waltman (2008), deineko and woeginger (2009), ellison (2010), and egghe (2011). simple variants of 
the h-index are discussed by kosmulski (2006) and wu (2010). 

 

4 

oppenheim  (2010).  a  bibliometric  study  of  the  h-index  literature  is  reported  by 
zhang, thijs, and gl  nzel (in press). 
 

figure  3.  graphical  illustration  of  the  arbitrariness  of  the  definition  of  the  h-index. 
the  solid  line  is  a  45  degree  line.  the  dashed  lines  are  a  30  degree  line  and  a  60 
degree line. in the calculation of the h-index, the 45 degree line is used, but one could 
equally well use, for instance, the 30 degree line or the 60 degree line. 
 

 

3. inconsistency of the h-index 

we  now  turn  to  what  we  consider  to  be  the  counterintuitive  behavior  of  the  h-
index. our aim is to show that the way in which the h-index aggregates publication 
and citation statistics into a single number leads to inconsistent results. 

before going  into  more  detail,  it is important  to  discuss  the distinction  between 
size-dependent  and  size-independent  indicators  of  scientific  impact  (for  a  similar 
distinction,  see  waltman  &  van  eck,  2009a).  size-dependent  indicators  never 
decrease  when  a  scientist  obtains  an  additional  publication.  examples  of  size-
dependent indicators are the number of publications, the total number of citations, and 
the number of highly cited publications of a scientist. the h-index is another example. 
size-independent  indicators,  on  the  other  hand,  are  normalized  for  the  size  of 
someone   s oeuvre. for instance, consider a scientist who has three publications, one 
with a citations, one with b citations, and one with c citations, and consider another 
scientist who has six publications, two with a citations, two with b citations, and two 
with c citations. because the oeuvres of the two scientists differ from each other only 
in  size and  not in citation  characteristics, a  size-independent indicator  will  have the 
same  value  for  both  scientists.  examples  of  size-independent  indicators  are  the 
average  number  of  citations  per  publication,  the  median  number  of  citations  per 
publication, and the percentage highly cited publications. 

size-dependent and size-independent indicators serve different purposes. given a 
set of publications, a size-dependent indicator is usually interpreted as a measure of 
the overall scientific impact of the publications, while a size-independent indicator is 
interpreted  as  a  measure  of  the  average  scientific  impact  per  publication.  in  some 
cases the use of a size-dependent indicator is more appropriate than the use of a size-

 

5 

independent  one,  and  in  other  cases  it  is the  other  way  around.  for  instance,  in the 
case  of  the  oeuvre  of  a  scientist,  measuring  the  overall  impact  of  the  oeuvre  is 
probably  more  useful  than  measuring  the  average  impact  per  publication.  this  is 
because measures of the average impact per publication fail to take the productivity of 
a  scientist  into  account (hirsch,  2005).  in  the  case  of  journals,  however,  things  are 
different.  when  comparing  the  impact  of  journals,  one  usually  does  not  want  size 
differences  to  affect  the  comparison.  this  means  that  one  needs  to  use  a  size-
independent indicator (e.g., the impact factor). 

below, we discuss three examples of situations in which the h-index is used as a 
measure of the overall scientific impact of a set of publications. in each example, the 
h-index  behaves  in  a  way  that  we  consider  counterintuitive.  the  examples  are 
intended to make clear that, at least for the purpose of measuring the overall impact of 
a set of publications, the h-index provides inconsistent results. 

3.1. example 1 

in this example, we show that the h-index violates the following property: 

 

if two scientists achieve the same relative performance improvement, their ranking 

relative to each other should remain unchanged. 

 
according to this property, if scientist x is ranked higher than scientist y and both 
scientists  achieve  the  same  relative  performance  improvement,  then  after  the 
performance improvement scientist x should still be ranked higher than scientist y. 

suppose that scientists x and y both work at the same university. this university 
uses  the  h-index  as  a  measure  of  the  overall  scientific  impact  of  its  scientists. 
scientists x and y have both been active as a researcher for five years. in this five-
year period, scientist x has produced twelve publications, nine with twelve citations 
each  and  three  with  four  citations  each.  scientist  y  has  produced  ten  publications, 
seven  with  fifteen  citations  each  and  three  with  five  citations  each.  it  follows  that 
scientists x and y have h-indices of, respectively, nine and seven (see the left panel 
of figure 4). therefore, based on the h-index, the university concludes that scientist x 
has  a  larger  impact  than  scientist  y.  suppose  next  that  scientists  x  and  y  keep 
producing  publications  at  the  same  rate  as  before.  in  terms  of  citations,  their  new 
publications perform equally well as their old ones. this means that after another five-
year period scientist x has a total of twenty-four publications, eighteen with twelve 
citations  each  and  six  with  four  citations  each.  scientist  y  has  a  total  of  twenty 
publications, fourteen with fifteen citations each and six with five citations each. (for 
simplicity, we assume that publications from the first five-year period do not receive 
additional  citations.)  hence,  the  h-index  of  scientist  x  has  increased  from  nine  to 
twelve, and the h-index of scientist y has increased from seven to fourteen (see the 
right panel of figure 4). as a consequence, the university concludes that scientist y 
has a larger impact than scientist x. in other words, compared with five years earlier, 
the ranking of the two scientists has reversed. 

in  our  view,  the  above  result  is  counterintuitive.  even  though  in  relative  terms 
scientists  x  and  y  have  both  achieved  exactly  the  same  performance  improvement 
(i.e., they have both doubled their publications and citations), their ranking relative to 
each other has not stayed the same. we consider this very difficult to justify, and we 
therefore conclude that the h-index has ranked scientists x and y in an inconsistent 
way. 
 

 

6 

figure 4. graphical illustration of the calculation of the h-index for scientist x (solid 
citation  curve)  and  scientist  y  (dashed  citation  curve)  in  example  1.  the  left  panel 
shows the situation after the first five-year period. the right panel shows the situation 
after the second five-year period. 

 

3.2. example 2 

we  now  turn  to  our  second  example.  in  this  example,  the  h-index  is  shown  to 

violate the following property: 
 

if two scientists achieve the same absolute performance improvement, their ranking 

relative to each other should remain unchanged. 

 
this property is quite similar to the property considered in the previous example. the 
only difference is that the property in the previous example is concerned with relative 
performance 
is  concerned  with  absolute 
performance improvements. 

improvements  while 

this  property 

suppose that scientists x and y each have seven publications. scientist x has five 
publications  with  five  citations  each  and  two  publications  with  two  citations  each. 
scientist y has four publications with six citations each and three publications with 
three citations each. the h-indices of scientists x and y then equal, respectively, five 
and four (see the left panel of figure 5). interpreting the h-index as a measure of the 
overall scientific impact of a scientist, it follows that scientist x is ranked higher than 
scientist  y  in  terms  of  overall  impact.  suppose  next  that  the  two  scientists  jointly 
produce  two new  publications. these publications  each  receive  eight  citations.  this 
does  not  change  the  h-index  of  scientist  x.  however,  the  h-index  of  scientist  y 
increases from four to six (see the right panel of figure 5). hence, after the production 
of the two joint publications, scientist y has a higher h-index than scientist x. this 
means that the ranking of the two scientists in terms of overall impact has reversed. 

we believe the above result to be counterintuitive. scientists x and y have both 
achieved exactly the same performance improvement (i.e., two publications with six 
citations each), but despite of this their ranking has reversed. from the point of view 
of measuring the overall impact of a scientist, we do not see how this can be justified. 
like  in  the  previous  example,  our  conclusion  therefore  is  that  the  h-index  has 
provided inconsistent rankings of scientists x and y. 
 

 

7 

figure 5. graphical illustration of the calculation of the h-index for scientist x (solid 
citation  curve)  and  scientist  y  (dashed  citation  curve)  in  example  2.  the  left  panel 
shows the initial situation. the right panel shows the situation after the production of 
two joint publications. 

 

3.3. example 3 

the  focus  of  our  third  example  is  on  consistency  between  rankings  at  different 

levels of aggregation. more specifically, we consider the following property: 
 
if scientist x1 is ranked higher than scientist y1 and scientist x2 is ranked higher than 
scientist y2, then a research group consisting of scientists x1 and x2 should be ranked 

higher than a research group consisting of scientists y1 and y2. 

 
we show that the h-index does not satisfy this property. 

in this example, we use the h-index as a measure of the overall scientific impact of 
both individual scientists and research groups. suppose we have two research groups, 
research group x and research group  y, each consisting of two scientists. research 
group x consists of scientists x1 and x2, and research group y consists of scientists 
y1  and  y2.  scientists  x1  and  x2  both  have  seven  publications.  each  of  their 
publications  has  been  cited  nine  times.  scientists  y1  and  y2  both  have  six 
publications,  and  each  of  their  publications  has  ten  citations.  the  h-indices  of 
scientists  x1  and  x2  then  equal  seven,  while  the  h-indices  of  scientists  y1  and  y2 
equal six (see the left panel of figure 6). hence, according to the h-index, scientists 
x1  and  x2  have  a  larger  impact  than  scientists  y1  and  y2.  it  now  seems  natural  to 
expect  that  research  group  x  also  has  a  larger  impact  than  research  group  y. 
however, the curious thing is that according to the h-index this is not the case. the h-
indices  of  research  groups  x  and  y  equal,  respectively,  nine  and  ten  (see  the  right 
panel of figure 6), which indicates that research group x has a smaller impact than 
research group y. this is exactly opposite to what we would expect based on way in 
which the individual scientists are ranked. we therefore conclude that in our example 
the  h-index fails  to  provide consistent  rankings of  individual  scientists  and  research 
groups. 
 

 

8 

 
figure 6. graphical illustration of the calculation of the h-index in example 3. the left 
panel  shows  the  calculation  of  the  h-index  for  scientists  x1  and  x2  (solid  citation 
curve)  and  scientists  y1  and  y2  (dashed  citation  curve).  the  right  panel  shows  the 
calculation  of  the  h-index  for  research  group  x  (solid  citation  curve)  and  research 
group y (dashed citation curve). 

3.4. discussion of the examples 

in  our  view,  the  above  three  examples  show  that,  from  the  perspective  of 
measuring  the  overall  impact  of  a  set  of  publications,  the  h-index  behaves  in  a 
counterintuitive  way.  the  mechanism  used  by  the  h-index  to  aggregate  publication 
and  citation  statistics  into  a  single  number  leads  to  inconsistent  results.  because  of 
this, our conclusion is that the h-index cannot be considered an appropriate indicator 
of the overall scientific impact of a set of publications. 

given the extensive literature on the h-index, it is remarkable that the inconsistent 
nature  of  the  index  has  remained  largely  unnoticed.  to  the  best  of  our  knowledge, 
apart from our own contributions (waltman & van eck, 2009a, 2009b), the only work 
in  which  the  inconsistency  problem  of  the  h-index  is  discussed  are  papers  by 
marchant (2009a) and bouyssou and marchant (2011b).5 marchant (2009a) points out 
that the h-index violates the property that we have discussed in our second example 
above. he concludes that    the ranking based on the h-index is in many circumstances 
probably not reasonable    (p. 335). building on our own earlier work (waltman & van 
eck,  2009a),  bouyssou  and  marchant  provide  a  mathematical  analysis  of  the  close 
relationship between the properties discussed in our second and third example above. 
it is important to note that the inconsistency problem discussed above affects not 
only the h-index but also all kinds of variants, extensions, and generalizations of this 
index.  for  instance,  variants  of  the  h-index  such  as  the  g-index  (egghe,  2006),  the 
h(2)-index  (kosmulski,  2006),  and  the  w-index  (wu,  2010)  suffer  from  similar 
counterintuitive behavior as the h-index itself. the same holds for generalizations of 
the  h-index  like  those  proposed  by  van  eck and waltman  (2008)  and  deineko  and 
woeginger  (2009).  some  indicators  that are  not  related to  the  h-index also  have  an 
inconsistency problem. examples include the total number of citations of a scientist   s 
n most highly cited publications and the total number of citations of a scientist   s x% 

                                                 
5  rousseau  (2008)  provides  a  short  discussion  of  the  paper  by  marchant  (2009a).  the  inconsistency 
problem of the h-index is also mentioned very briefly in a book review by van raan (2010). 

 

9 

most  highly  cited  publications.  for  all  these  indicators,  it  is  possible  to  construct 
examples similar to the above ones that show counterintuitive behavior. 

in  the  literature,  various  shortcomings  of  the  h-index  have  been  discussed.  for 
instance, some authors argue that the h-index does not give sufficient credit to highly 
cited  publications  (e.g.,  egghe,  2006),  other  authors  claim  that  the  index  tends  to 
undervalue  scientists  with  a  selective  publication  strategy  (e.g.,  costas  &  bordons, 
2007), and still other authors point out limitations of the index in the way in which co-
authorship is dealt with (e.g., egghe, 2008; hirsch, 2010; schreiber, 2008). one may 
wonder  whether 
just  another 
shortcoming  of  the  h-index  or  whether  in  some  sense  this  problem  is  of  a  more 
fundamental nature. in our view, the inconsistency problem is more fundamental. the 
other problems can all be addressed, at least to a certain degree, by making a suitable 
modification  to  the  h-index,  but  without  leaving  the  h-index  framework  altogether. 
the  inconsistency  problem  is  more  fundamental  because  it  affects  not  only  the  h-
index itself but also all its variants. as will be discussed later on in this paper, solving 
the inconsistency problem requires a different class of indicators. 

inconsistency  problem 

the  above-discussed 

is 

3.5. size-independent indicators 

what needs to be emphasized is that the above discussion only pertains to size-
dependent  indicators,  that  is,  indicators  aimed  at  measuring  the  overall  scientific 
impact of a set of publications. size-independent indicators, which aim to measure the 
average scientific impact of the publications in a given set, serve a different purpose 
and therefore have different requirements. 

consider  for  instance  the  average  number  of  citations  per  publication  indicator. 
suppose  we  have  two  journals,  journal  x  and  journal  y.  journal  x  has  five 
publications, each with six citations. journal y has twenty publications, each with five 
citations.  hence,  according  to  the  average  number  of  citations  per  publication 
indicator, journal x is ranked higher than journal y. suppose next that both journals 
obtain  five  new  publications.  these  new  publications  do  not  have  any  citations. 
journal  x   s  average  number  of  citations  per  publication  then  decreases  from  six  to 
three, while journal y   s average number of citations per publication decreases from 
five  to  four.  so  the  ranking  of  the  two  journals  reverses.  does  this  mean  that  the 
average  number  of  citations  per  publication  indicator  provides  inconsistent  results? 
the  answer  to  this  question  is  no.  because  the  average  number  of  citations  per 
publication  indicator  aims  to  measure  the  average  impact  of  the  publications  of  a 
journal (rather than the overall impact), the reversal of the ranking of journals x and 
y is completely legitimate. journal y initially had four times as many publications as 
journal  x.  therefore,  journal  y   s  average  impact  per  publication  should  be  less 
sensitive to adding new publications than journal x   s average impact per publication. 
in this case, the newly added publications were uncited, which caused both journals    
average impact per publication to decrease. however, because of the size difference 
between the journals, the decrease was more severe in the case of journal x than in 
the case of journal y. this caused the ranking of the journals to reverse. 

the  above  example  illustrates  the  difference  in  the  requirements  for  size-
dependent  and  size-independent  indicators.  the  important  thing  to  keep  in  mind  is 
that the validity of a size-independent indicator should not be judged based on criteria 
developed for size-dependent indicators (and the other way around). in the discussion 
in the next section, we only consider size-dependent indicators. we refer to bouyssou 
and  marchant  (2011a)  and  waltman  et  al.  (2011)  for  theoretical  work  on  size-
independent indicators. 

 

10 

4. alternatives to the h-index 

given  the  inconsistency  problem  of  the  h-index  and  many  other  indicators,  one 
may wonder what kind of alternative indicators can be used that do not have a similar 
problem.  in  particular,  the  question  arises  whether  there  are  indicators  that  have 
similar properties as the h-index but that do not suffer from inconsistency problems. 

to  address  these  questions,  we  need  to  discuss  an  important  recent  theoretical 
paper by marchant (2009b; see also waltman & van eck, 2009a). marchant considers 
a special class of indicators which he refers to as scoring rules.6 to calculate a scoring 
rule  for  a  set  of  publications,  one  first  calculates  a  score  for  each  individual 
publication  in  the  set.  the  score  of  a  publication  is  determined  by  the  number  of 
citations of the publication. this is done in such a way that an increase in the number 
of  citations  of  a  publication  will  never  lead  to  a  decrease  in  the  score  of  the 
publication. after calculating a score for each individual publication, the scoring rule 
is  obtained  by  calculating  the  sum  of  the  individual  publication  scores  (or,  more 
generally,  by  calculating  an  increasing  function  of  this  sum).  somewhat  more 
formally,  given a set of n publications with  c1, c2,    , cn citations, a scoring rule is 
equal to (or, more generally, is an increasing function of) 
 

f(c1) + f(c2) +     + f(cn), 

 
where f is an increasing function that determines the score of a publication based on 
the number of times the publication has been cited. 

a number of well-known indicators are scoring rules. in particular, the number of 
publications indicator is a scoring rule in which all publications have the same score, 
irrespective of their number of citations, and the total number of citations indicator is 
a  scoring  rule  in  which  the  score  of  a  publication  is  proportional  to  the  number  of 
citations of the publication. the highly cited publications indicator is a scoring rule as 
well.  in  this  scoring  rule,  publications  whose  number  of  citations  exceeds  a  certain 
threshold all have the same positive score and all other publications have a score of 
zero. 

an attractive feature of scoring rules is that they do not suffer from inconsistency 
problems. moreover, as shown by marchant (2009b), almost all indicators that are not 
a  scoring  rule  do  suffer  from  inconsistency  problems.7  when  looking  for  an 
alternative  to  the  h-index,  this  essentially  means  that  we  can  restrict  ourselves  to 
scoring  rules.  in  general,  indicators  that  are  not  a  scoring  rule  will  have  similar 
shortcomings as the h-index and therefore do not solve the fundamental problem we 
have with this index. 

there are many different scoring rules, and it is not our aim to designate a single 
scoring rule as the best one. however, we do want to discuss some scoring rules that 
may  serve  as  suitable  alternatives  to  the  h-index.  as  we  have  discussed,  the  main 
advantage of the h-index is often claimed to be its relative insensitivity to publications 
with  a  very  large  number  of  citations,  and  sometimes  also  its  insensitivity  to  large 
numbers  of  publications  with  no  or  almost  no  citations.  a  scoring  rule  that  in  this 
respect behaves similarly to the h-index is the highly cited publications indicator, that 
                                                 
6 in fact, marchant (2009b) uses the term    scoring rule    not to refer to the indicators themselves but to 
the rankings implied by the indicators. for simplicity, we do not make this distinction. 
7 it is possible to construct indicators that do not suffer from inconsistency problems even though they 
are  not  scoring  rules.  this  leads  to  fairly  complicated  indicators  that  violate  the  archimedeanness 
axiom defined by marchant (2009b). we do not think that these indicators are of much use for practical 
purposes. 

 

11 

is, the indicator that counts the number of publications with at least a certain number 
of citations.8 this indicator only cares about whether a publication counts as highly 
cited or not. the indicator is insensitive to the exact number of citations of a highly 
cited  publication.  also,  like  the  h-index,  the  highly  cited  publications  indicator  is 
insensitive to large numbers of lowly cited and uncited publications. another point of 
similarity between the highly cited publications indicator and the h-index is that both 
indicators have a simple and easy-to-explain calculation. so in many respects the two 
indicators  are  similar.  however,  compared  with  the  h-index,  the  highly  cited 
publications  indicator  has  the  advantage  that  it  does  not  suffer  from  inconsistency 
problems.  because  of  this  important  advantage,  we  regard  the  highly  cited 
publications indicator as a more appropriate indicator of scientific impact than the h-
index. 

the  highly  cited  publications  indicator  is  sometimes  argued  to  have  the 
disadvantage  that  it  depends  on  an  essentially  arbitrary  parameter  for  determining 
which  publications  count  as  highly  cited and  which  do not.  this  is  for  instance the 
objection of hirsch (2005) against the highly cited publications indicator. however, 
as we have pointed out earlier in this paper, the h-index is subject to the same kind of 
arbitrariness as the highly cited publications indicator. the only difference is that the 
h-index  does  not  have  any  explicit  parameters,  which  makes  it  somewhat  more 
difficult to recognize the arbitrary elements in its definition. because the highly cited 
publications  indicator  and  the  h-index  are  both  subject  to  arbitrariness,  we  do  not 
consider  arbitrariness  a  good  argument  for  rejecting  one  indicator  in  favor  of  the 
other. 

apart  from  the  highly  cited  publications  indicator,  there  are  all  kinds  of  other 
scoring rules that may be used as an alternative to the h-index. for instance, one may 
use a scoring rule in which the score of a publication is given by a (strictly) concave 
function (such as the square root or the natural logarithm) of the number of citations 
of the publication (e.g., lundberg, 2007). like the h-index, such a scoring rule will 
typically  be  relatively  insensitive  to  publications  with  a  very  large  number  of 
citations.  compared  with  the  highly  cited  publications  indicator,  a  scoring  rule  that 
uses a concave function to determine the score of a publication has the advantage that 
the score of a publication increases in a gradual way as the number of citations of the 
publication increases. in the case of the highly cited publications indicator, there is an 
abrupt increase when the number of citations passes the highly cited threshold. this 
may  be  considered  somewhat  unsatisfactory.  clearly,  determining 
the  most 
appropriate  score  function  for  a  scoring  rule  is  a  difficult  problem.  we refer  to  the 
recent work of ravallion and wagstaff (2011) for a theoretical framework in which 
this problem can be explored further. 

5. discussion and conclusion 

in  this  paper,  we  have  criticized  the  h-index  and  its  variants  from  a  theoretical 
point of view. we have argued that for the purpose of measuring the overall scientific 
impact  of  a  scientist  (or  some  other  unit  of  analysis)  the  h-index  behaves  in  a 
counterintuitive  way.  in  certain  cases,  the  mechanism  used  by  the  h-index  to 
aggregate  publication  and  citation  statistics 
to 
inconsistencies in the way in which scientists are ranked. our conclusion is that the h-

into  a  single  number 

leads 

                                                 
8  for  studies  on  highly  cited  publications  indicators,  we  refer  to  plomp  (1990,  1994)  and  tijssen, 
visser, and van leeuwen (2002). 

 

12 

index cannot be considered an appropriate indicator of the overall scientific impact of 
a scientist. 

based  on  recent  theoretical  insights  (marchant,  2009a,  2009b;  waltman  &  van 
eck, 2009a), we have discussed a large family of bibliometric indicators that, unlike 
the h-index, do not suffer from inconsistency problems. the indicators in this family 
are referred to as scoring rules. it has not been our aim to argue in favor of a single 
alternative to  the  h-index.  however,  as  we  have  pointed  out,  there  is  one  particular 
scoring  rule  that  has  a  lot  in  common  with  the  h-index.  this  is  the  highly  cited 
publications  indicator.  like  the  h-index,  the  highly  cited  publications  indicator  is 
robust both to publications with a very large number of citations and to publications 
with  no  or  almost  no  citations.  yet,  compared  with  the  h-index,  the  highly  cited 
publications  indicator  has  the  important  advantage  that  it  does  not  suffer  from 
inconsistency problems. because of this, we believe that the highly cited publications 
indicator  provides  an  attractive  alternative  to  the  h-index,  at  least  in  situations  in 
which the above-mentioned robustness is regarded as a desirable feature. 

it may of course be that the h-index has certain good properties which for instance 
the  highly  cited  publications  indicator  does  not  have.  we  now  discuss  some 
suggestions  in  this  direction.  hirsch  (2007)  argues  that  compared  with  other 
bibliometric indicators the mechanism of the h-index is well suited to deal with high 
impact  publications  co-authored  by  scientists  with  different  levels  of  seniority  (or 
different  levels  of  ability).  according  to  hirsch,  this  is  because,  in  a  certain  sense, 
senior  authors  receive  more  credit  from  such  high  impact  publications  than  junior 
authors (for the full argument, see hirsch, 2007, p. 19197). although we consider this 
an interesting argument, it depends crucially on the assumption that in the case of a 
high impact publication co-authored by junior and senior scientists most credit should 
go to the senior authors. moreover, even if one accepts this assumption, we think it is 
doubtful whether the somewhat better way of dealing with co-authored publications is 
worth  sacrificing  the  consistency  of  one   s  measurements.  a  somewhat  related 
argument in favor of the h-index is that    the focus of the index shifts in a natural way 
when comparing researchers at different levels. when comparing young researchers it 
emphasizes whether they have written a few papers that have had some impact, and 
when  comparing  distinguished  senior  researchers  it  ignores  minor  papers  and 
considers only papers that have a substantial number of citations    (ellison, 2010, p. 
2). we agree that this can be seen as an advantage of the h-index over for instance the 
highly cited publications indicator. on the other hand, however, we believe that in the 
case  of  young scientists publication and  citation statistics  are  only  of  limited  value, 
and we therefore do not consider this a very significant advantage of the h-index. we 
also  note  that  the  above  arguments  in  favor  of  the  h-index  pertain  specifically  to 
situations in which the h-index is used at the level of individual scientists. this means 
that the arguments cannot serve as a justification for the use of the h-index at other 
levels of aggregation, such as at the level of research groups or journals. 

as already mentioned in the beginning of this paper, most of the literature on the 
h-index  is  empirically  oriented.  perhaps  the  most  convincing  empirical  work  in 
support  of  the  h-index  is  hirsch     follow-up  study  on  his  original  h-index  paper 
(hirsch,  2007).  in  his  follow-up  study,  hirsch  performs  a  comparison  of  four 
bibliometric  indicators,  namely  the  number  of  publications,  the  total  number  of 
citations,  the  average  number  of  citations  per  publication,  and  the  h-index.  hirsch 
uses  two  (relatively  small)  samples  of  physicists  and  looks  at  publications  and 
citations in  two  time  periods.  the  focus  of  hirsch     study  is  on  the  degree  to  which 
indicators  calculated  based  on  the  first  time  period  yield  accurate  predictions  of 

 

13 

indicators calculated based on the second time period. interestingly, the h-index not 
only turns out to be the indicator that is best able to predict its own future value, but it 
also turns out to be the indicator that is best able to predict the future value of the total 
number of citations indicator. in our view, hirsch    study provides a reasonable degree 
of  empirical  support to  the  h-index,  at least for  applications  in  the  field of  physics. 
unfortunately, apart from the h-index, hirsch    study does not include any other robust 
indicators,  such  as  the  highly  cited  publications indicator.  in  future empirical  work, 
we  consider  it  essential  that  other  robust  indicators  are  taken  into  account  as  well, 
especially  indicators  belonging  to  the  family  of  scoring  rules.  this  will  make  it 
possible to identify indicators that behave in a satisfactory way both from a theoretical 
and from an empirical point of view. 

there is one final remark that we want to make. the idea of the h-index is to have 
a  single  number  that  provides  a  rough  approximation  of  the  scientific  impact  of  a 
scientist.  because  of  our  focus  on  the  h-index,  we  have  also  adopted  this  single-
indicator viewpoint in this paper. we emphasize, however, that for practical purposes 
it  is  usually  desirable  to  have  a  set  of  bibliometric  indicators,  each  emphasizing  a 
different  aspect  of  the  scientific  impact  of  a  scientist.  when  scientists  are  being 
evaluated  and  compared,  it  may  sometimes  be  even  better  to  look  directly  at  their 
citation distributions (e.g., using plots similar to the ones shown in this paper) rather 
than  to  focus  on  indicators  derived  from  these  distributions.  this  always  yields  the 
most  comprehensive  picture  of  the  impact  of  someone   s  work  as  measured  by 
publication and citation data. 

acknowledgements 

we would like to thank rodrigo costas, thierry marchant, ronald rousseau, and 

ton van raan for their feedback on an earlier draft of this paper. 

references 
albarr  n,  p.,  ortu  o,  i.,  &  ruiz-castillo,  j.  (2011).  the  measurement  of  low-  and 
high-impact  in  citation  distributions:  technical  results.  journal  of  informetrics, 
5(1), 48   63. 

alonso,  s.,  cabrerizo,  f.j.,  herrera-viedma,  e.,  &  herrera,  f.  (2009).  h-index:  a 
review  focused  in  its  variants,  computation  and  standardization  for  different 
scientific fields. journal of informetrics, 3(4), 273   289. 

bornmann, l., & daniel, h.-d. (2007). what do we know about the h index? journal 
of  the  american  society  for  information  science  and  technology,  58(9),  1381   
1385. 

bornmann,  l.,  mutz,  r.,  hug,  s.e.,  &  daniel,  h.-d.  (2011).  a  multilevel  meta-
analysis of studies reporting correlations between the h index and 37 different h 
index variants. journal of informetrics, 5(3), 346   359. 

bouyssou,  d.,  &  marchant,  t.  (2011a).  bibliometric  rankings  of  journals  based  on 

impact factors: an axiomatic approach. journal of informetrics, 5(1), 75   86. 

bouyssou,  d.,  &  marchant,  t.  (2011b).  ranking  scientists  and  departments  in  a 
consistent manner. journal of the american society for information science and 
technology, 62(9), 1761   1769. 

braun,  t.,  gl  nzel,  w.,  &  schubert,  a.  (2006).  a  hirsch-type  index  for  journals. 

scientometrics, 69(1), 169   173. 

burrell, q.l. (2007). hirsch   s h-index: a stochastic model. journal  of  informetrics, 

1(1), 16   25. 

 

14 

costas,  r.,  &  bordons,  m.  (2007).  the  h-index:  advantages,  limitations  and  its 
relation  with  other  bibliometric  indicators  at  the  micro  level.  journal  of 
informetrics, 1(3), 193   203. 

deineko,  v.g.,  &  woeginger,  g.j.  (2009).  a  new  family  of  scientific  impact 

measures: the generalized kosmulski-indices. scientometrics, 80(3), 819   826. 

egghe, l. (2006). theory and practise of the g-index. scientometrics, 69(1), 131   152. 
egghe,  l.  (2008).  mathematical  theory  of  the  h-  and  g-index  in  case  of  fractional 
counting of authorship. journal of the american society for information science 
and technology, 59(10), 1608   1616. 

egghe, l. (2010). the hirsch index and related impact measures. annual review of 

information science and technology, 44, 65   114. 

egghe, l. (2011). characterizations of the generalized wu- and kosmulski-indices in 

lotkaian systems. journal of informetrics, 5(3), 439   445. 

egghe,  l.,  &  rousseau,  r.  (2006).  an  informetric  model  for  the  hirsch-index. 

scientometrics, 69(1), 121   129. 

ellison,  g.  (2010).  how  does  the  market  use  citation  data?  the  hirsch  index  in 
economics  (nber  working  paper  16419).  national  bureau  of  economic 
research. 

hirsch,  j.e.  (2005).  an  index  to  quantify  an  individual   s  scientific  research  output. 

proceedings of the national academy of sciences, 102(46), 16569   16572. 

hirsch,  j.e.  (2007).  does  the  h  index  have  predictive  power?  proceedings  of  the 

national academy of sciences, 104(49), 19193   19198. 

hirsch,  j.e.  (2010).  an  index  to  quantify  an  individual   s  scientific  research  output 
that takes into account the effect of multiple coauthorship. scientometrics, 85(3), 
741   754. 

gl  nzel, w. (2006). on the h-index     a mathematical approach to a new measure of 

publication activity and citation impact. scientometrics, 67(2), 315   321. 

kosmulski, m. (2006). a new hirsch-type index saves time and works equally well as 

the original h-index. issi newsletter, 2(3), 4   6. 

lehmann, s., jackson, a.d., & lautrup, b.e. (2006). measures for measures. nature, 

444, 1003   1004. 

lehmann,  s.,  jackson,  a.d.,  &  lautrup,  b.e.  (2008).  a  quantitative  analysis  of 

indicators of scientific performance. scientometrics, 76(2), 369   390. 

lundberg,  j.  (2007).  lifting  the  crown   citation  z-score.  journal  of  informetrics, 

1(2), 145   154. 

marchant, t. (2009a). an axiomatic characterization of the ranking based on the  h-
index  and  some  other  bibliometric  rankings  of  authors.  scientometrics,  80(2), 
327   344. 

marchant,  t.  (2009b).  score-based  bibliometric  rankings  of  authors.  journal  of  the 

american society for information science and technology, 60(6), 1132   1137. 

norris,  m.,  &  oppenheim,  c.  (2010).  the  h-index:  a  broad  review  of  a  new 

bibliometric indicator. journal of documentation, 66(5), 681   705. 

palacios-huerta,  i.,  &  volij,  o.  (2004).  the  measurement  of  intellectual  influence. 

econometrica, 72(3), 963   977. 

plomp,  r.  (1990).  the  significance  of  the  number  of  highly  cited  papers  as  an 

indicator of scientific prolificacy. scientometrics, 19(3   4), 185   197. 

plomp, r. (1994). the highly cited papers of professors as an indicator of a research 

group   s scientific performance. scientometrics, 29(3), 377   393. 

quesada,  a.  (2010).  more  axiomatics  for  the  hirsch  index.  scientometrics,  82(2), 

413   418. 

 

15 

ravallion, m., & wagstaff, a. (2011). on measuring scholarly influence by citations. 

scientometrics, 88(1), 321   337. 

rousseau,  r.  (2008).  luckily,  science  focuses  on  achievements.  some  thoughts 

related to the h-index. issi newsletter, 4(3), 49   50. 

schreiber, m. (2008). a modification of the h-index: the hm-index accounts for multi-

authored manuscripts. journal of informetrics, 2(3), 211   216. 

tijssen,  r.j.w.,  visser,  m.s.,  &  van  leeuwen,  t.n.  (2002).  benchmarking 
international scientific excellence: are highly cited research papers an appropriate 
frame of reference? scientometrics, 54(3), 381   397. 

van eck, n.j., & waltman, l. (2008). generalizing the h- and g-indices. journal of 

informetrics, 2(4), 263   271. 

van raan, a.f.j. (2006). comparison of the hirsch-index with standard bibliometric 
judgment  for  147  chemistry  research  groups. 

indicators  and  with  peer 
scientometrics, 67(3), 491   502. 

van raan, t. (2010). bibliometrics: measure for measure [review of the book  the 

publish or perish book]. nature, 468, 763. 

waltman,  l.,  &  van  eck,  n.j.  (2009a).  a  taxonomy  of  bibliometric  performance 
indicators  based  on  the  property  of  consistency.  in  proceedings  of  the  12th 
international  conference  on  scientometrics  and  informetrics  (pp.  1002   1003). 
available at http://hdl.handle.net/1765/15182. 

waltman,  l.,  &  van  eck,  n.j.  (2009b).  a  simple  alternative  to  the  h-index.  issi 

newsletter, 5(3), 46   48. 

waltman,  l.,  &  van  eck,  n.j.  (2010).  the  relation  between  eigenfactor,  audience 
factor,  and  influence  weight.  journal  of  the  american  society  for  information 
science and technology, 61(7), 1476   1486. 

waltman, l., van eck, n.j., van leeuwen, t.n., visser, m.s., & van raan, a.f.j. 
(2011). towards a new crown indicator: some theoretical considerations. journal 
of informetrics, 5(1), 37   47. 

woeginger,  g.j.  (2008).  an  axiomatic  characterization  of  the  hirsch-index. 

mathematical social sciences, 56(2), 224   232. 

wu,  q.  (2010).  the  w-index:  a  measure  to assess  scientific  impact  by  focusing  on 
widely cited papers. journal of the american society for information science and 
technology, 61(3), 609   614. 

zhang, l., thijs, b., & gl  nzel, w. (in press). the diffusion of h-related literature. 

journal of informetrics. 

 

16 

