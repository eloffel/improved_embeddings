four degrees of separation

lars backstrom    paolo boldi    marco rosa   

johan ugander   

sebastiano vigna   

january 6, 2012

2
1
0
2

 

n
a
j
 

5

 
 
]
i
s
.
s
c
[
 
 

3
v
0
7
5
4

.

1
1
1
1
:
v
i
x
r
a

abstract

frigyes karinthy,
in his 1929 short story    l  ncszemek   
(   chains   ) suggested that any two persons are distanced by
at most six friendship links.1 stanley milgram in his famous
experiment [20, 23] challenged people to route postcards to a
   xed recipient by passing them only through direct acquain-
tances. the average number of intermediaries on the path
of the postcards lay between 4.4 and 5.7, depending on the
sample of people chosen.

we report the results of the    rst world-scale social-network
graph-distance computations, using the entire facebook net-
work of active users (    721 million users,     69 billion friend-
ship links). the average distance we observe is 4.74, cor-
responding to 3.74 intermediaries or    degrees of separation   ,
showing that the world is even smaller than we expected, and
prompting the title of this paper. more generally, we study
the distance distribution of facebook and of some interest-
ing geographic subgraphs, looking also at their evolution over
time.

the networks we are able to explore are almost two orders
of magnitude larger than those analysed in the previous liter-
ature. we report detailed statistical metadata showing that
our measurements (which rely on probabilistic algorithms)
are very accurate.

1

introduction

at the 20th world   wide web conference, in hyderabad, in-
dia, one of the authors (sebastiano) presented a new tool for

   facebook.
   dsi, universit   degli studi di milano, italy. paolo boldi, marco
rosa and sebastiano vigna have been partially supported by a ya-
hoo! faculty grant and by miur prin    query log e web crawling   .

1the exact wording of the story is slightly ambiguous:    he bet us
that, using no more than    ve individuals, one of whom is a personal ac-
quaintance, he could contact the selected individual [. . . ]   . it is not com-
pletely clear whether the selected individual is part of the    ve, so this
could actually allude to distance    ve or six in the language of graph the-
ory, but the    six degrees of separation    phrase stuck after john guare   s
1990 eponymous play. following milgram   s de   nition and guare   s inter-
pretation (see further on), we will assume that    degrees of separation   
is the same as    distance minus one   , where    distance    is the usual path
length (the number of arcs in the path).

1

studying the distance distribution of very large graphs: hy-
peranf [3]. building on previous graph compression [4] work
and on the idea of di   usive computation pioneered in [21],
the new tool made it possible to accurately study the dis-
tance distribution of graphs orders of magnitude larger than
it was previously possible.

one of the goals in studying the distance distribution is the
identi   cation of interesting statistical parameters that can
be used to tell proper social networks from other complex
networks, such as web graphs. more generally, the distance
distribution is one interesting global feature that makes it
possible to reject probabilistic models even when they match
local features such as the in-degree distribution.

in particular, earlier work had shown that the spid 2,
which measures the dispersion of the distance distribution,
appeared to be smaller than 1 (underdispersion) for so-
cial networks, but larger than one (overdispersion) for web
graphs [3]. hence, during the talk, one of the main open
questions was    what is the spid of facebook?   .

lars backstrom happened to listen to the talk, and sug-
gested a collaboration studying the facebook graph. this
was of course an extremely intriguing possibility: beside test-
ing the    spid hypothesis   , computing the distance distribution
of the facebook graph would have been the largest milgram-
like [20] experiment ever performed, orders of magnitudes
larger than previous attempts (during our experiments face-
book has     721 million active users and     69 billion friend-
ship links).

this paper reports our    ndings in studying the distance
distribution of the largest electronic social network ever cre-
ated. that world is smaller than we thought: the average
distance of the current facebook graph is 4.74. moreover, the
spid of the graph is just 0.09, corroborating the conjecture [3]
that proper social networks have a spid well below one. we
also observe, contrary to previous literature analysing graphs
orders of magnitude smaller, both a stabilisation of the aver-
age distance over time, and that the density of the facebook
graph over time does not neatly    t previous models.

towards a deeper understanding of the structure of the
facebook graph, we also apply recent compression techniques

2the spid (shortest-paths index of dispersion) is the variance-to-

mean ratio of the distance distribution.

that exploit the underlying cluster structure of the graph to
increase locality. the results obtained suggests the existence
of overlapping clusters similar to those observed in other so-
cial networks.

replicability of scienti   c results is important. while for
obvious nondisclosure reasons we cannot release to the pub-
lic the actual 30 graphs that have been studied in this paper,
we distribute freely the derived data upon which the tables
and    gures of this papers have been built, that is, the web-
graph properties, which contain structural information about
the graphs, and the probabilistic estimations of their neigh-
bourhood functions (see below) that have been used to study
their distance distributions. the software used in this paper
is distributed under the (l)gpl general public license.3

2 related work
the most obvious precursor of our work is milgram   s cele-
brated    small world    experiment, described    rst in [20] and
later with more details in [23]: milgram   s works were actually
following a stream of research started in sociology and psy-
chology in the late 50s [12]. in his experiment, milgram aimed
at answering the following question (in his words):    given two
individuals selected randomly from the population, what is
the id203 that the minimum number of intermediaries
required to link them is 0, 1, 2, . . . , k?   .

the technique milgram used (inspired by [22]) was the fol-
lowing: he selected 296 volunteers (the starting population)
and asked them to dispatch a message to a speci   c individ-
ual (the target person), a stockholder living in sharon, ma,
a suburb of boston, and working in boston. the message
could not be sent directly to the target person (unless the
sender knew him personally), but could only be mailed to
a personal acquaintance who is more likely than the sender
to know the target person. the starting population was se-
lected as follows: 100 of them were people living in boston,
100 were nebraska stockholders (i.e., people living far from
the target but sharing with him their profession) and 96 were
nebraska inhabitants chosen at random.

in a nutshell, the results obtained from milgram   s exper-
iments were the following: only 64 chains (22%) were com-
pleted (i.e., they reached the target); the average number of
intermediaries in these chains was 5.2, with a marked dif-
ference between the boston group (4.4) and the rest of the
starting population, whereas the di   erence between the two
other subpopulations was not statistically signi   cant; at the
other end of the spectrum, the random (and essentially clue-
less) group from nebraska needed 5.7 intermediaries on av-
erage (i.e., rounding up,    six degrees of separation   ). the
main conclusions outlined in milgram   s paper were that the
average path length is small, much smaller than expected,

3see http://{webgraph,law}.dsi.unimi.it/.

2

and that geographic location seems to have an impact on the
average length whereas other information (e.g., profession)
does not.

there is of course a fundamental di   erence between our ex-
periment and what milgram did: milgram was measuring the
average length of a routing path on a social network, which is
of course an upper bound on the average distance (as the peo-
ple involved in the experiment were not necessarily sending
the postcard to an acquaintance on a shortest path to the
destination).4 in a sense, the results he obtained are even
more striking, because not only do they prove that the world
is small, but that the actors living in the small world are able
to exploit its smallness. it should be remarked, however, that
in [20, 23] the purpose of the authors is to estimate the num-
ber of intermediaries: the postcards are just a tool, and the
details of the paths they follow are studied only as an artifact
of the measurement process. the interest in e   cient routing
lies more in the eye of the beholder (e.g., the computer scien-
tist) than in milgram   s: with at his disposal an actual large
database of friendship links and algorithms like the ones we
use, he would have dispensed with the postcards altogether.
incidentally, there have been some attempts to repro-
duce milgram-like routing experiments on various large net-
works [18, 14, 11], but the results in this direction are still
very preliminary because notions such as identity, knowledge
or routing are still poorly understood in social networks.

we limited ourselves to the part of milgram   s experiment
that is more clearly de   ned, that is, the measurement of
shortest paths. the largest experiment similar to the ones
presented here that we are aware of is [15], where the authors
considered a communication graph with 180 million nodes
and 1.3 billion edges extracted from a snapshot of the mi-
crosoft messenger network; they    nd an average distance of
6.6 (i.e., 5.6 intermediaries; again, rounding up, six degrees of
separation). note, however, that the communication graph
in [15] has an edge between two persons only if they com-
municated during a speci   c one-month observation period,
and thus does not take into account friendship links through
which no communication was detected.

the authors of [24], instead, study the distance distribu-
tion of some small-sized social networks. in both cases the
networks were undirected and small enough (by at least two
orders of magnitude) to be accessed e   ciently in a random
fashion, so the authors used sampling techniques. we re-
mark, however, that sampling is not easily applicable to di-

4incidentally, this observation is at the basis of one of the most in-
tense monologues in guare   s play: ouisa, unable to locate paul, the
con man who convinced them he is the son of sidney poitier, says    i
read somewhere that everybody on this planet is separated by only six
other people. six degrees of separation. between us and everybody else
on this planet.
[. . . ] but to    nd the right six people.    note that this
fragment of the monologue clearly shows that guare   s interpretation of
the    six degree of separation    idea is equivalent to distance seven in the
graph-theoretical sense.

rected networks (such as twitter) that are not strongly con-
nected, whereas our techniques would still work (for some
details about the applicability of sampling, see [8]).

analysing the evolution of social networks in time is also
a lively trend of research. leskovec, kleinberg and faloutsos
observe in [16] that the average degree of complex networks
increase over time while the e   ective diameter shrinks. their
experiments are conducted on a much smaller scale (their
largest graph has 4 millions of nodes and 16 millions of arcs),
but it is interesting that the phenomena observed seems quite
consistent. probably the most controversial point is the hy-
pothesis that the number of edges m(t) at time t is related
to the number of nodes n(t) by the following relation:

m(t)     n(t)a,

where a is a    xed exponent usually lying in the interval
(1 . . 2). we will discuss this hypothesis in light of our    nd-
ings.

3 de   nitions and tools
the neighbourhood function ng(t) of a graph g returns for
each t     n the number of pairs of nodes (cid:104)x, y(cid:105) such that
y is reachable from x in at most t steps.
it provides data
about how fast the    average ball    around each node expands.
from the neighbourhood function it is possible to derive the
distance distribution (between reachable pairs), which gives
for each t the fraction of reachable pairs at distance exactly
t.

in this paper we use hyperanf, a di   usion-based algo-
rithm (building on anf [21]) that is able to approximate
quickly the neighbourhood function of very large graphs; our
implementation uses, in turn, webgraph [4] to represent in
a compressed but quickly accessible form the graphs to be
analysed.

hyperanf is based on the observation (made in [21]) that

b(x, r), the ball of radius r around node x, satis   es

b(x, r) =

b(y, r     1)     { x}.

(cid:91)

x   y

since b(x, 0) = { x}, we can compute each b(x, r) incremen-
tally using sequential scans of the graph (i.e., scans in which
we go in turn through the successor list of each node). the
obvious problem is that during the scan we need to access
randomly the sets b(x, r     1) (the sets b(x, r) can be just
saved on disk on a update    le and reloaded later).

the space needed for such sets would be too large to be
kept in main memory. however, hyperanf represents these
sets in an approximate way, using hyperloglog counters [10],
which should be thought as dictionaries that can answer reli-
ably just questions about size. each such counter is made of

a number of small (in our case, 5-bit) registers. in a nutshell,
a register keeps track of the maximum number m of trail-
ing zeroes of the values of a good hash function applied to
the elements of a sequence of nodes: the number of distinct
elements in the sequence is then proportional to 2m. a tech-
nique called stochastic averaging is used to divide the stream
into a number of substreams, each analysed by a di   erent reg-
ister. the result is then computed by aggregating suitably
the estimation from each register (see [10] for details).

the main performance challenge to solve is how to quickly
compute the hyperloglog counter associated to a union of
balls, each represented, in turn, by a hyperloglog counter:
hyperanf uses an algorithm based on word-level parallelism
that makes the computation very fast, and a carefully engi-
neered implementation exploits multicore architectures with
a linear speedup in the number of cores.

another important feature of hyperanf is that it uses
a systolic approach to avoid recomputing balls that do not
change during an iteration. this approach is fundamental to
be able to compute the entire distance distribution, avoiding
the arbitrary termination conditions used by previous ap-
proaches, which have no provable accuracy (see [3] for an
example).

3.1 theoretical error bounds
the result of a run of hyperanf at the t-th iteration is an
estimation of the neighbourhood function in t. we can see it
as a random variable

(cid:88)

0   i<n

  ng(t) =

xi,t

where each xi,t is the hyperloglog counter that counts
nodes reached by node i in t steps (n is the number of nodes of
the graph). when m registers per counter are used, each xi,t
   
has a guaranteed relative standard deviation   m     1.06/
m.
it is shown in [3] that the output   ng(t) of hyperanf
at the t-th iteration is an asymptotically almost unbiased
estimator of ng(t), that is

e[   ng(t)]

ng(t)

= 1 +   1(n) + o(1) for n        ,

where   1 is the same as in [10][theorem 1] (and |  1(x)| <
5    10   5 as soon as m     16). moreover,   ng(t) has a relative
standard deviation not greater than that of the xi   s, that is

(cid:113)

var[   ng(t)]
ng(t)

      m.

in particular, our runs used m = 64 (  m = 0.1325) for all
graphs except for the two largest facebook graphs, where we

3

used m = 32 (  m = 0.187). runs were repeated so to obtain
a uniform relative standard deviation for all graphs.

unfortunately, the relative error for the neighbourhood
function becomes an absolute error for the distance distri-
bution. thus, the theoretical bounds one obtains for the
moments of the distance distribution are quite ugly. actu-
ally, the simple act of dividing the neighbourhood function
values by the last value to obtain the cumulative distribution
function is nonlinear, and introduces bias in the estimation.
to reduce bias and provide estimates of the standard er-
ror of our measurements, we use the jackknife [9], a classical
nonparametric method for evaluating arbitrary statistics on
a data sample, which turns out to be very e   ective in prac-
tice [3].

4 experiments

the graphs analysed in this paper are graphs of facebook
users who were active in may of 2011; an active user is one
who has logged in within the last 28 days. the decision to
restrict our study to active users allows us to eliminate ac-
counts that have been abandoned in early stages of creation,
and focus on accounts that plausibly represent actual indi-
viduals. in accordance with facebook   s data retention poli-
cies, historical user activity records are not retained, and his-
torical graphs for each year were constructed by considering
currently active users that were registered on january 1st of
that year, along with those friendship edges that were formed
prior that that date. the    current    graph is simply the graph
of active users at the time when the experiments were per-
formed (may 2011). the graph predates the existence of
facebook    subscriptions   , a directed relationship feature in-
troduced in august 2011, and also does not include    pages   
(such as celebrities) that people may    like   . for standard
user accounts on facebook there is a limit of 5 000 possible
friends.

we decided to extend our experiments in two directions:
regional and temporal. we thus analyse the entire facebook
graph (fb), the usa subgraph (us), the italian subgraph (it)
and the swedish (se) subgraph. we also analysed a com-
bination of the italian and swedish graph (itse) to check
whether combining two regional but distant networks could
signi   cantly change the average distance, in the same spirit
as in the original milgram   s experiment.5 for each graph we
compute the distance distribution from 2007 up to today by
performing several hyperanf runs, obtaining an estimate
of values of neighbourhood function with relative standard
deviation at most 5.8%:
in several cases, however, we per-

5to establish geographic location, we use the users    current geo-ip
location; this means, for example, that the users in the it-2007 graph
are users who are today in italy and were on facebook on january 1,
2007 (most probably, american college students then living in italy).

4

formed more runs, obtaining a higher precision. we report
the jackknife [9] estimate of derived values (such as average
distances) and the associated estimation of the standard er-
ror.

4.1 setup
the computations were performed on a 24-core machine with
72 gib of memory and 1 tib of disk space.6 the    rst task
was to import the facebook graph(s) into a compressed form
for webgraph [4], so that the multiple scans required by
hyperanf   s di   usive process could be carried out relatively
quickly. this part required some massaging of facebook   s
internal ids into a contiguous numbering: the resulting cur-
rent fb graph (the largest we analysed) was compressed to
345 gb at 20 bits per arc, which is 86% of the information-

theoretical lower bound (log(cid:0)n2
arcs we need at least(cid:4)log(cid:0)n2

(cid:1) bits, there n is the number
(cid:1)(cid:5) bits per graph: the purpose of

of nodes and m the number of arcs).7 whichever coding we
choose, for half of the possible graphs with n nodes and m

compression is precisely to choose the coding so to represent
interesting graphs in a smaller space than that required by
the bound.

to understand what is happening, we recall that web-
graph uses the bv compression scheme [4], which applies
three intertwined techniques to the successor list of a node:
    successors are (partially) copied from previous nodes
if successors lists are similar

m

m

within a small window,
enough;

    successors are intervalised, that is, represented by a left
extreme and a length, if signi   cant contiguous successor
sequences appear;

    successors are gap-compressed if they pass the previous
phases:
instead of storing the actual successor list, we
store the di   erences of consecutive successors (in increas-
ing order) using instantaneous codes.

thus, a graph compresses well when it exhibits similarity
(nodes with near indices have similar successor lists) and lo-
cality (successor lists have small gaps).
the better-than-random result above (usually, randomly
permuted graphs compressed with webgraph occupy 10    
20% more space than the lower bound) has most likely been
induced by the renumbering process, as in the original stream
of arcs all arcs going out from a node appeared consecutively;

6we remark that the commercial value of such hardware is of the

order of a few thousand dollars.

7note that we measure compression with respect to the lower bound
on arcs, as webgraph stores directed graphs; however, with the addi-
tional knowledge that the graph is undirected, the lower bound should
be applied to edges, thus doubling, in practice, the number of bits used.

is a local maximum at two, showing that there is some lo-
cality, but the bulk of the id203 mass is around 20   21,
which is slightly less than the information-theoretical lower
bound (    23).

in the graph permuted with llp, however, the distribu-
tion radically changes: it is now (mostly) beautifully mono-
tonically decreasing, with a very small bump at 23, which
testi   es the existence of a small core of    randomness    in the
graph that llp was not able to tame.

regarding similarity, we see an analogous phenomenon:
the number of successors represented by copy has doubled,
going from 9% to 18%. the last datum is in line with other
social networks (web graphs, on the contrary, are extremely
redundant and more than 80% of the successors are usually
copied). moreover, disabling copying altogether results in
modest increase in size (    5%), again in line with other so-
cial networks, which suggests that for most applications it
is better to disable copying at all to obtain faster random
access.

the compression ratio is around 53%, which is similar to
other similar social networks, such as livejournal (55%) or
dblp (40%) [2]8. for other graphs (see table 1), however,
it is slightly worse. this might be due to several phenomena:
first, our llp runs were executed with only half the number
or clusters, and for each cluster we restricted the number of
iterations to just four, to make the whole execution of llp
feasible. thus, our runs are capable of    nding considerably
less structure than the runs we had previously performed for
other networks. second, the number of nodes is much larger:
there is some cost in writing down gaps (e.g., using   ,    or
   codes) that is dependent on their absolute magnitude, and
the lower bound does not take into account that cost.

4.2 running
since most of the graphs, because of their size, had to be ac-
cessed by memory mapping, we decided to store all counters
(both those for b(x, r     1) and those for b(x, r)) in main
memory, to avoid eccessive i/o. the runs of hyperanf on
the current whole facebook graph used 32 registers, so the
space for counters was about 27 gib (e.g., we could have
analysed a graph with four times the number of nodes on
the same hardware). as a rough measure of speed, a run on
the llp-compressed current whole facebook graph requires
about 13.5 hours. note that this timings would scale linearly
with an increase in the number of cores.

4.3 general comments
in september 2006, facebook was opened to non-college stu-
dents: there was an instant surge in subscriptions, as our

8the interested reader will    nd similar data for several type of net-

works at the law web site (http://law.dsi.unimi.it/).

5

figure 1: the change in distribution of the logarithm of
the gaps between successors when the current fb graph is
permuted by layered label propagation. see also table 1.

as a consequence, the renumbering process assigned consec-
utive labels to all yet-unseen successors (e.g., in the initial
stages successors were labelled contiguously), inducing some
locality.

it is also possible that the    natural    order for facebook
(essentially, join order) gives rise to some improvement over
the information-theoretical lower bound because users often
join the network at around the same time as several of their
friends, which causes a certain amount of locality and simi-
larity, as circle of friends have several friends in common.

we were interested in the    rst place to establish whether
more locality could be induced by suitably permuting the
graph using layered labelled propagation [2] (llp). this ap-
proach (which computes several id91s with di   erent lev-
els of granularity and combines them to sort the nodes of a
graph so to increase its locality and similarity) has recently
led to the best compression ratios for social networks when
combined with the bv compression scheme. an increase in
compression means that we were able to partly understand
the cluster structure of the graph.

we remark that each of the id91s required by llp is
in itself a tour de force, as the graphs we analyse are almost
two orders of magnitude larger than any network used for
experiments in the literature on graph id91.
indeed,
applying llp to the current facebook graph required ten
days of computation on our hardware.

we applied layered labelled propagation and re-compressed
our graphs (the current version), obtaining a signi   cant im-
provement. in table 1 we show the results: we were able to
reduce the graph size by 30%, which suggests that llp has
been able to discover several signi   cant clusters.

the change in structure can be easily seen from figure 1,
where we show the distribution of the binary logarithm of
gaps between successors for the current fb graph. the
smaller the gaps, the higher the locality. in the graph with
renumbered facebook ids, the distribution is bimodal: there

 0 5e+09 1e+10 1.5e+10 2e+10 2.5e+10 0 5 10 15 20 25 30frequencylogarithm of successor gapsbefore llpafter llporiginal

llp

it

14.8 (83%)
10.3 (58%)

se

14.0 (86%)
10.2 (63%)

itse

15.0 (82%)
10.3 (56%)

us

17.2 (82%)
11.6 (56%)

fb

20.1 (86%)
12.3 (53%)

table 1: the number of bits per link and the compression ratio (with respect to the information-theoretical lower bound)
for the current graphs in the original order and for the same graphs permuted by layered label propagation [2].

figure 2: the id203 mass functions of the distance
distributions of the current graphs (truncated at distance 10).

figure 3: the average distance graph. see also table 6.

data shows.
in particular, the it and se subgraphs from
january 1, 2007 were highly disconnected, as shown by the
incredibly low percentage of reachable pairs we estimate in
table 9. even facebook itself was rather disconnected, but
all the data we compute stabilizes (with small oscillations)
after 2009, with essentially all pairs reachable. thus, we con-
sider the data for 2007 and 2008 useful to observe the evolu-
tion of facebook, but we do not consider them representative
of the underlying human social-link structure.

2007
2008
2009
2010
2011
current

it
1.31
5.88
50.82
122.92
198.20
226.03

se
3.90
46.09
69.60
100.85
140.55
154.54

itse
1.50
36.00
55.91
118.54
187.48
213.30

us

119.61
106.05
111.78
128.95
188.30
213.76

fb
99.50
76.15
88.68
113.00
169.03
190.44

table 4: average degree of the datasets.

it
0.04
25.54

se
10.23
93.90

itse
0.19
80.21

us

100.00
99.26

fb
68.02
89.04

2007
2008

table 9: percentage of reachable pairs 2007   2008.

4.4 the distribution

figure 2 displays the id203 mass functions of the cur-
rent graphs. we will discuss later the variation of the average
distance and spid, but qualitatively we can immediately dis-
tinguish the regional graphs, concentrated around distance
four, and the whole facebook graph, concentrated around
distance    ve. the distributions of it and se, moreover, have
signi   cantly less id203 mass concentrated on distance
   ve than itse and us. the variance data (table 7 and fig-
ure 4) show that the distribution became quickly extremely
concentrated.

6

02468100.00.10.20.30.40.50.6distance% pairslitseitseusfblllllllllll0246810yearaverage distance20072008200920102011currlitseitseusfbllllll2007
2008
2009
2010
2011
current

it

159.8 k (105.0 k)
335.8 k (987.9 k)
4.6 m (116.0 m)
11.8 m (726.9 m)
17.1 m (1.7 g)
19.8 m (2.2 g)

se

11.2 k (21.8 k)
1.0 m (23.2 m)
1.6 m (55.5 m)
3.0 m (149.9 m)
4.0 m (278.2 m)
4.3 m (335.7 m)

itse

172.1 k (128.8 k)
1.4 m (24.3 m)
6.2 m (172.1 m)
14.8 m (878.4 m)
21.1 m (2.0 g)
24.1 m (2.6 g)

us

8.8 m (529.3 m)
20.1 m (1.1 g)
41.5 m (2.3 g)
92.4 m (6.0 g)
131.4 m (12.4 g)
149.1 m (15.9 g)

fb

13.0 m (644.6 m)
56.0 m (2.1 g)
139.1 m (6.2 g)
332.3 m (18.8 g)
562.4 m (47.5 g)
721.1 m (68.7 g)

table 2: number of nodes and friendship links of the datasets. note that each friendship link, being undirected, is
represented by a pair of symmetric arcs.

2007
2008
2009
2010
2011
current

it

itse

387.0 k

se
51.0 k 461.9 k

fb
us
2.3 g
1.8 g
9.2 g
3.9 m 96.7 m 107.8 m 4.0 g
28.7 g
477.9 m 227.5 m 840.3 m 9.1 g
4.5 g 26.0 g
93.3 g
9.6 g 53.6 g 238.1 g
9.7 g 68.5 g 344.9 g

3.6 g 623.0 m
1.1 g
8.0 g
8.3 g
1.2 g

table 3: size in bytes of the datasets.

lower bounds from hyperanf runs
fb

itse
41
24
17
19
17
19

2007
14
2008
16
2009
15
2010
15
2011
35
current
58
exact diameter of the giant component
current
41

25

23

27

30

it
41
28
21
18
17
19

se
17
17
16
19
20
19

us
13
17
16
19
18
20

the density of the network, on the contrary, decreases.10
in figure 5 we plot the density (number of edges divided
by number of nodes) of the graphs against the number of
nodes (see also table 5). there is some initial alternating
behaviour, but on the more complete networks (fb and us)
the trend in sparsi   cation is very evident.

geographical concentration, however, increases density: in
figure 5 we can see the lines corresponding to our regional
graphs clearly ordered by geographical concentration, with
the fb graph in the lowest position.

table 10: lower bounds for the diameter of all graphs, and
exact values for the giant component (> 99.7%) of current
graphs computed using the ifub algorithm.

4.6 average distance
the results concerning average distance11 are displayed in
figure 3 and table 6. the average distance12 on the face-

4.5 average degree and density

table 4 shows the relatively quick growth in time of the av-
erage degree of all graphs we consider. the more users join
the network, the more existing friendship links are uncovered.
in figure 6 we show a loglog-scaled plot of the same data:
with the small set of points at our disposal, it is di   cult to
draw reliable conclusions, but we are not always observing
the power-law behaviour suggested in [16]: see, for instance,
the change of the slope for the us graph.9

9we remind the reader that on a log-log plot almost anything    looks
like    a straight line. the quite illuminating examples shown in [17], in
particular, show that goodness-of-   t tests are essential.

7

10we remark that the authors of [16] call densi   cation the increase
of the average degree, in contrast with established literature in graph
theory, where density is the fraction of edges with respect to all possi-
ble edges (e.g., 2m/(n(n     1))). we use    density   ,    densi   cation    and
   sparsi   cation    in the standard sense.

11the data we report is about the average distance between reach-
able pairs, for which the name average connected distance has been
proposed [5]. this is the same measure as that used by travers and
milgram in [23]. we refrain from using the word    connected    as it
somehow implies a bidirectional (or, if you prefer, undirected) connec-
tion. the notion of average distance between all pairs is useless in a
graph in which not all pairs are reachable, as it is necessarily in   nite,
so no confusion can arise.

12in some previous literature (e.g., [16]), the 90% percentile (possibly
with some interpolation) of the distance distribution, called e   ective
diameter, has been used in place of the average distance. having at
our disposal tools that can compute easily the average distance, which
is a parameterless, standard feature of the distance distribution that

2007
2008
2009
2010
2011
current

it

8.224e-06
1.752e-05
1.113e-05
1.039e-05
1.157e-05
1.143e-05

se

3.496e-04
4.586e-05
4.362e-05
3.392e-05
3.551e-05
3.557e-05

itse

8.692e-06
2.666e-05
9.079e-06
7.998e-06
8.882e-06
8.834e-06

us

1.352e-05
5.268e-06
2.691e-06
1.395e-06
1.433e-06
1.434e-06

fb

7.679e-06
1.359e-06
6.377e-07
3.400e-07
3.006e-07
2.641e-07

table 5: density of the datasets.

2007
2008
2009
2010
2011
current

it

10.25 (  0.17)
6.45 (  0.03)
4.60 (  0.02)
4.10 (  0.02)
3.88 (  0.01)
3.89 (  0.02)

se

5.95 (  0.07)
4.37 (  0.03)
4.11 (  0.01)
4.08 (  0.02)
3.91 (  0.01)
3.90 (  0.04)

itse

8.66 (  0.14)
4.85 (  0.05)
4.94 (  0.02)
4.43 (  0.03)
4.17 (  0.02)
4.16 (  0.01)

us

4.32 (  0.02)
4.75 (  0.02)
4.73 (  0.02)
4.64 (  0.02)
4.37 (  0.01)
4.32 (  0.01)

fb

4.46 (  0.04)
5.28 (  0.03)
5.26 (  0.03)
5.06 (  0.01)
4.81 (  0.04)
4.74 (  0.02)

table 6: the average distance (   standard error). see also figure 3 and 7.

book current graph is 4.74.13 moreover, a closer look at the
distribution shows that 92% of the reachable pairs of individ-
uals are at distance    ve or less.

we note that both on the it and se graphs we    nd a sig-
ni   cantly lower, but similar value. we interpret this result as
telling us that the average distance is actually dependent on
the geographical closeness of users, more than on the actual
size of the network. this is con   rmed by the higher average
distance of the itse graph.

during the fastest growing years of facebook our graphs
show a quick decrease in the average distance, which how-
ever appears now to be stabilizing. this is not surprising, as
   shrinking diameter    phenomena are always observed when
a large network is    uncovered   , in the sense that we look at
larger and larger induced subgraphs of the underlying global
human network. at the same time, as we already remarked,
density was going down steadily. we thus see the small-world
phenomenon fully at work: a smaller fraction of arcs connect-
ing the users, but nonetheless a lower average distance.

to make more concrete the    degree of separation    idea, in
table 11 we show the percentage of reachable pairs within
the ceiling of the average distance (note, again, that it is the
percentage relatively to the reachable pairs):
for instance,
in the current facebook graph 92% of the pairs of reachable
users are within distance    ve   four degrees of separation.

has been used in social sciences for decades, we prefer to stick to it.
experimentally, on web and social graphs the average distance is about
two thirds of the e   ective diameter plus one [3].

13note that both karinthy and guare had in mind the maximum, not
the average number of degrees, so they were actually upper bounding
the diameter.

4.7 spid

the spid is the index of dispersion   2/   (a.k.a. variance-to-
mean ratio) of the distance distribution. some of the authors
proposed the spid [3] as a measure of the    webbiness    of a so-
cial network. in particular, networks with a spid larger than
one should be considered    web-like   , whereas networks with a
spid smaller than one should be considered    properly social   .
we recall that a distribution is called under- or over-dispersed
depending on whether its index of dispersion is smaller or
larger than 1 (e.g., variance smaller or larger than the aver-
age distance), so a network is considered properly social or
not depending on whether its distance distribution is under-
or over-dispersed.

the intuition behind the spid is that    properly social    net-
works strongly favour short connections, whereas in the web
long connection are not uncommon. as we recalled in the in-
troduction, the starting point of the paper was the question
   what is the spid of facebook   ? the answer, con   rming the
data we gathered on di   erent social networks in [3], is shown
in table 8. with the exception of the highly disconnected
regional networks in 2007   2008 (see table 9), the spid is well
below one.

interestingly, across our collection of graphs we can con   rm
that there is in general little correlation between the average
distance and the spid: kendall   s    is    0.0105; graphical ev-
idence of this fact can be seen in the scatter plot shown in
figure 7.

if we consider points associated with a single network,
though, there appears to be some correlation between av-
erage distance and spid, in particular in the more connected

8

2007
2008
2009
2010
2011
current

it

32.46 (  1.49)
3.78 (  0.18)
0.64 (  0.04)
0.40 (  0.01)
0.38 (  0.03)
0.42 (  0.03)

se

3.90 (  0.12)
0.69 (  0.04)
0.56 (  0.02)
0.50 (  0.02)
0.50 (  0.02)
0.52 (  0.04)

itse

16.62 (  0.87)
1.74 (  0.15)
0.84 (  0.02)
0.64 (  0.03)
0.61 (  0.02)
0.57 (  0.01)

us

0.52 (  0.01)
0.82 (  0.02)
0.62 (  0.02)
0.53 (  0.02)
0.39 (  0.01)
0.40 (  0.01)

fb

0.65 (  0.02)
0.86 (  0.03)
0.69 (  0.05)
0.52 (  0.01)
0.42 (  0.03)
0.41 (  0.01)

table 7: the variance of the distance distribution (   standard error). see also figure 4.

2007
2008
2009
2010
2011
current

it

3.17 (  0.106)
0.59 (  0.026)
0.14 (  0.007)
0.10 (  0.003)
0.10 (  0.006)
0.11 (  0.007)

se

0.66 (  0.016)
0.16 (  0.008)
0.14 (  0.004)
0.12 (  0.005)
0.13 (  0.006)
0.13 (  0.010)

itse

1.92 (  0.078)
0.36 (  0.028)
0.17 (  0.004)
0.14 (  0.006)
0.15 (  0.004)
0.14 (  0.003)

us

0.12 (  0.003)
0.17 (  0.003)
0.13 (  0.003)
0.11 (  0.004)
0.09 (  0.003)
0.09 (  0.003)

fb

0.15 (  0.004)
0.16 (  0.005)
0.13 (  0.009)
0.10 (  0.002)
0.09 (  0.005)
0.09 (  0.003)

table 8: the index of dispersion of distances, a.k.a. spid (   standard error). see also figure 7.

networks (the values for kendall   s    are all above 0.6, except
for se). however, this is just an artifact, as the correlation
between spid and average distance is inverse (larger average
distance, smaller spid). what is happening is that in this
case the variance (see table 7) is changing in the same direc-
tion: smaller average distances (which would imply a larger
spid) are associated with smaller variances. figure 8 displays
the mild correlation between average distance and variance in
the graphs we analyse: as a network gets tighter, its distance
distribution also gets more concentrated.

4.8 diameter
hyperanf cannot provide exact results about the diameter:
however, the number of steps of a run is necessarily a lower
bound for the diameter of the graph (the set of registers can
stabilize before a number of iterations equal to the diameter
because of hash collisions, but never after). while there are
no statistical guarantees on this datum, in table 10 we re-
port these maximal observations as lower bounds that di   er
signi   cantly between regional graphs and the overall face-
book graph   there are people that are signi   cantly more    far
apart    in the world than in a single nation.14

to corroborate this information, we decided to also ap-
proach the problem of computing the exact diameter directly,
although it is in general a daunting task: for very large graphs
matrix-based algorithms are simply not feasible in space, and
the basic algorithm running n breadth-   rst visits is not fea-
sible in time. we thus implemented a highly parallel version
14incidentally, as we already remarked, this is the measure that

karinthy and guare actually had in mind.

9

of the ifub (iterative fringe upper bound) algorithm intro-
duced in [6] (extending the ideas of [7, 19]) for undirected
graphs.

the basic idea is as follows: consider some node x, and
   nd (by a breadth-   rst visit) a node y farthest from x. find
now a node z farthest from y: d(y, z) is a (usually very good)
lower bound on the diameter, and actually it is the diameter
if the graph is a tree (this is the    double sweep    algorithm).
we now consider a node c halfway between y and z: such
a node is    in the middle of the graph    (actually, it would be
a center if the graph was a tree), so if h is the eccentricy of
c (the distance of the farthest node from c) we expect 2h to
be a good upper bound for the diameter.

if our upper and lower bound match, we are    nished. oth-
erwise, we consider the fringe: the nodes at distance exactly
h from c. clearly, if m is the maximum of the eccentrici-
ties of the nodes in the fringe, max{ 2(h     1), m } is a new
(and hopefully improved) upper bound, and m is a new (and
hopefully improved) lower bound. we then iterate the pro-
cess by examining fringes closer to the root until the bounds
match.

our implementation uses a multicore breadth-   rst visit:
the queue of nodes at distance d is segmented into small
blocks handled by each core. at the end of a round, we
have computed the queue of nodes at distance d + 1. our
implementation was able to discover the diameter of the cur-
rent us graph (which    ts into main memory, thanks to llp
compression) in about twenty minutes. the diameter of face-
book required ten hours of computation of a machine with
1tib of ram (actually, 256gib would have been su   cient,
always because of llp compression).

2007
2008
2009
2010
2011
current

it

65% (11)
77% (7)
90% (5)
98% (5)
90% (4)
88% (4)

se

64% (6)
93% (5)
96% (5)
97% (5)
86% (4)
86% (4)

itse

67% (9)
77% (5)
75% (5)
91% (5)
95% (5)
97% (5)

us

95% (5)
83% (5)
86% (5)
91% (5)
97% (5)
97% (5)

fb

91% (5)
91% (6)
94% (6)
97% (6)
89% (5)
91% (5)

table 11: percentage of reachable pairs within the ceiling of the average distance (shown between parentheses).

figure 4: the graph of variances of the distance distributions.
see also table 7.

the values reported in table 10 con   rm what we discov-
ered using the approximate data provided by the length of
hyperanf runs, and suggest that while the distribution has
a low average distance and it is quite concentrated, there
are nonetheless (rare) pairs of nodes that are much farther
apart. we remark that in the case of the current fb graph,
the diameter of the giant component is actually smaller than
the bound provided by the hyperanf runs, which means
that long paths appear in small (and likely very irregular)
components.

4.9 precision
as already discussed in [3], it is very di   cult to obtain strong
theoretical bounds on data derived from the distance distri-
bution. the problem is that when passing from the neigh-
bourhood function to the distance distribution, the relative
error bound becomes an absolute error bound: since the dis-

10

figure 5: a plot correlating number of nodes to graph den-
sity (for the graph from 2009 on).

figure 6: a plot correlating number of nodes to the average
degree (for the graphs from 2009 on).

051015202530yeardistance variance20072008200920102011currlitseitseusfbllllll 1e-07 1e-06 1e-05 0.0001 1e+06 1e+07 1e+08 1e+09graph densitynodesfbusitseitse 1e+08 1e+09 1e+10 1e+11 1e+12 1e+06 1e+07 1e+08 1e+09arcsnodesfbusitseitsefigure 7: a scatter plot showing the (lack of) correlation
between the average distance and the spid.

figure 8: a scatter plot showing the mild correlation between
the average distance and the variance.

figure 9: the evolution of the relative error in a hyper-
anf computation with relative standard deviation 9.25% on
a small social network (dblp-2010).

tance distribution attains very small values (in particular in
its tail), there is a concrete risk of incurring signi   cant errors
when computing the average distance or other statistics. on
the other hand, the distribution of derived data is extremely
concentrated [3].

there is, however, a clear empirical explanation of the un-
expected accuracy of our results that is evident from an anal-
ysis of the evolution of the empirical relative error of a run
on a social network. we show an example in figure 9.

    in the very    rst steps, all counters contain essentially
disjoint sets; thus, they behave as independent random
variables, and under this assumption their relative error
should be signi   cantly smaller than expected:
indeed,
this is clearly visible from figure 9.

    in the following few steps, the distribution reaches its
highest value. the error oscillates, as counters are now
signi   cantly dependent from one another, but in this
part the actual value of the distribution is rather large,
so the absolute theoretical error turns out to be rather
good.

    finally, in the tail each counter contains a very large
subset of the reachable nodes: as a result, all counters
behave in a similar manner (as the hash collisions are
essentially the same for every counter), and the rela-
tive error stabilises to an almost    xed value. because
of this stabilisation, the relative error on the neighbour-
hood function transfers, in practice, to a relative error
on the distance distribution. to see why this happen,
observe the behaviour of the variation of the relative er-
ror, which is quite erratic initially, but then converges
quickly to zero. the variation is the only part of the
relative error that becomes an absolute error when pass-
ing to the distance distribution, so the computation on
the tail is much more accurate than what the theoretical
bound would imply.

11

llll4.04.24.44.64.85.05.20.100.120.140.16average distancespidlitseitseusfbllll4.04.24.44.64.85.05.20.40.50.60.70.8average distancevariance-0.05 0 0.05 0.1 0.15 0.2 0.25 0.3 0 5 10 15 20tid203 mass functionrelative errorvariation of relative errorwe remark that our considerations remain valid for any
di   usion-based algorithm using approximate, statistically de-
pendent counters (e.g., anf [21]).

5 conclusions

in this paper we have studied the largest electronic social net-
work ever created (    721 million active facebook users and
their     69 billion friendship links) from several viewpoints.
first of all, we have con   rmed that layered labelled prop-
agation [2] is a powerful paradigm for increasing locality of
a social network by permuting its nodes. we have been able
to compress the us graph at 11.6 bits per link   56% of the
information-theoretical lower bound, similarly to other, much
smaller social networks.

we then analysed using hyperanf the complete facebook
graph and 29 other graphs obtained by restricting geographi-
cally or temporally the links involved. we have in fact carried
out the largest milgram-like experiment ever performed. the
average distance of facebook is 4.74, that is, 3.74    degrees of
separation   , prompting the title of this paper. the spid of
facebook is 0.09, well below one, as expected for a social
network. geographically restricted networks have a smaller
average distance, as it happened in milgram   s original exper-
iment. overall, these results help paint the picture of what
the facebook social graph looks like. as expected, it is a
small-world graph, with short paths between many pairs of
nodes. however, the high degree of compressibility and the
study of geographically limited subgraphs show that geog-
raphy plays a huge role in forming the overall structure of
network. indeed, we see in this study, as well as other stud-
ies of facebook [1] that, while the world is connected enough
for short paths to exist between most nodes, there is a high
degree of locality induced by various externalities, geography
chief amongst them, all reminiscent of the model proposed in
[13].

when milgram    rst published his results, he in fact o   ered
two opposing interpretations of what    six degrees of separa-
tion    actually meant. on the one hand, he observed that
such a distance is considerably smaller than what one would
naturally intuit. but at the same time, milgram noted that
this result could also be interpreted to mean that people are
on average six    worlds apart   :    when we speak of    ve15 in-
termediaries, we are talking about an enormous psychological
distance between the starting and target points, a distance
which seems small only because we customarily regard       ve   
as a small manageable quantity. we should think of the two
points as being not    ve persons apart, but       ve circles of ac-

quaintances    apart      ve    structures    apart.    [20]. from this
gloomier perspective, it is reassuring to see that our    ndings
show that people are in fact only four world apart, and not
six: when considering another person in the world, a friend
of your friend knows a friend of their friend, on average.

references
[1] lars backstrom, eric sun, and cameron marlow. find
me if you can:
improving geographical prediction with
social and spatial proximity. in proceedings of the 19th
international conference on world wide web, pages 61   
70. acm, 2010.

[2] paolo boldi, marco rosa, massimo santini, and sebas-
tiano vigna. layered label propagation: a multiresolu-
tion coordinate-free ordering for compressing social net-
works. in sadagopan srinivasan, krithi ramamritham,
arun kumar, m. p. ravindra, elisa bertino, and ravi
kumar, editors, proceedings of the 20th international
conference on world wide web, pages 587   596. acm,
2011.

[3] paolo boldi, marco rosa, and sebastiano vigna. hy-
peranf: approximating the neighbourhood function of
very large graphs on a budget. in sadagopan srinivasan,
krithi ramamritham, arun kumar, m. p. ravindra,
elisa bertino, and ravi kumar, editors, proceedings of
the 20th international conference on world wide web,
pages 625   634. acm, 2011.

[4] paolo boldi and sebastiano vigna. the webgraph
framework i: compression techniques.
in proc. of the
thirteenth international world wide web conference
(www 2004), pages 595   601, manhattan, usa, 2004.
acm press.

[5] andrei broder, ravi kumar, farzin maghoul, prab-
hakar raghavan, sridhar rajagopalan, raymie stata,
andrew tomkins, and janet wiener. graph structure
in the web: experiments and models. computer net-
works, 33(1   6):309   320, 2000.

[6] p. crescenzi, r. grossi, m. habib, l. lanzi, and
a. marino. on computing the diameter of real-world
undirected graphs. presented at workshop on graph
algorithms and applications (zurich   july 3, 2011) and
selected for submission to the special issue of theoreti-
cal computer science in honor of giorgio ausiello in the
occasion of his 70th birthday, 2011.

15five is the median of the number of intermediaries reported in the
   rst paper by milgram [20], from which our quotation is taken. more
experiments were performed with travers [23] with a slightly greater
average, as reported in section 2.

[7] pierluigi crescenzi, roberto grossi, claudio imbrenda,
leonardo lanzi, and andrea marino. finding the di-
ameter in real-world graphs: experimentally turning a

12

[18] david liben-nowell, jasmine novak, ravi kumar,
prabhakar raghavan, and andrew tomkins. geographic
routing in social networks. proceedings of the national
academy of sciences of the united states of america,
102(33):11623   11628, august 2005.

[19] cl  mence magnien, matthieu latapy, and michel habib.
fast computation of empirically tight bounds for the
diameter of massive graphs.
j. exp. algorithmics,
13:10:1.10   10:1.9, 2009.

[20] stanley milgram. the small world problem. psychology

today, 2(1):60   67, 1967.

[21] christopher r. palmer, phillip b. gibbons, and christos
faloutsos. anf: a fast and scalable tool for data mining
in massive graphs. in kdd    02: proceedings of the eighth
acm sigkdd international conference on knowledge
discovery and data mining, pages 81   90, new york, ny,
usa, 2002. acm.

[22] anatol rapoport and william j. horvath. a study of a
large sociogram. behavorial science, 6:279   291, october
1961.

[23] je   rey travers and stanley milgram. an experimen-
sociometry,

tal study of the small world problem.
32(4):425   443, 1969.

[24] qi ye, bin wu, and bai wang. distance distribution and
average shortest path length estimation in real-world
networks. in proceedings of the 6th international con-
ference on advanced data mining and applications: part
i, volume 6440 of lecture notes in computer science,
pages 322   333. springer, 2010.

lower bound into an upper bound.
in mark de berg
and ulrich meyer, editors, algorithms - esa 2010, 18th
annual european symposium, liverpool, uk, september
6-8, 2010. proceedings, part i, volume 6346 of lecture
notes in computer science, pages 302   313. springer,
2010.

[8] pierluigi crescenzi, roberto grossi, leonardo lanzi,
and andrea marino. a comparison of three algorithms
for approximating the distance distribution in real-world
graphs. in alberto marchetti-spaccamela and michael
segal, editors, theory and practice of algorithms in
(computer) systems, volume 6595 of lecture notes in
computer science, pages 92   103. springer berlin, 2011.

[9] bradley efron and gail gong. a leisurely look at
the bootstrap, the jackknife, and cross-validation. the
american statistician, 37(1):36   48, 1983.

[10] philippe flajolet,   ric fusy, olivier gandouet, and
fr  d  ric meunier. hyperloglog: the analysis of a near-
optimal cardinality estimation algorithm. in proceedings
of the 13th conference on analysis of algorithm (aofa
07), pages 127   146, 2007.

[11] sharad goel, roby muhamad, and duncan watts. so-
cial search in "small-world" experiments. in proceedings
of the 18th international conference on world wide web,
pages 701   710. acm, 2009.

[12] michael gurevitch. the social structure of acquaintance-
ship networks. phd thesis, massachusetts institute of
technology, dept. of economics, 1961.

[13] jon m. kleinberg. navigation in a small world. nature,

406(6798):845   845, 2000.

[14] silvio lattanzi, alessandro panconesi, and d. sivaku-
mar. milgram-routing in social networks. in proceedings
of the 20th international conference on world wide web,
pages 725   734. acm, 2011.

[15] jure leskovec and eric horvitz. planetary-scale views
on a large instant-messaging network. in proceeding of
the 17th international conference on world wide web,
pages 915   924. acm, 2008.

[16] jure leskovec, jon kleinberg, and christos faloutsos.
graph evolution: densi   cation and shrinking diameters.
acm transactions on knowledge discovery from data
(tkdd), 1(1):2   es, 2007.

[17] lun li, david l. alderson, john doyle, and walter will-
inger. towards a theory of scale-free graphs: de   nition,
properties, and implications. internet math., 2(4), 2005.

13

