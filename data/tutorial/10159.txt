leveraging user diversity to harvest knowledge

on the social web

jeon-hyung kang

information sciences institute

university of southern california
marina del rey, california 90292

email:jeonhyuk@usc.edu

kristina lerman

information sciences institute

university of southern california
marina del rey, california 90292

email: lerman@isi.edu

1
1
0
2

 
t
c
o
1
2

 

 
 
]

r

i
.
s
c
[
 
 

1
v
1
5
8
4

.

0
1
1
1
:
v
i
x
r
a

abstract   social web users are a very diverse group with vary-
ing interests, levels of expertise, enthusiasm, and expressiveness.
as a result, the quality of content and annotations they create to
organize content is also highly variable. while several approaches
have been proposed to mine social annotations, for example,
to learn folksonomies that re   ect how people relate narrower
concepts to broader ones, these methods treat all users and
the annotations they create uniformly. we propose a framework
to automatically identify experts, i.e., knowledgeable users who
create high quality annotations, and use their knowledge to guide
folksonomy learning. we evaluate the approach on a large body
of social annotations extracted from the photosharing site flickr.
we show that using expert knowledge leads to more detailed
and accurate folksonomies. moreover, we show that including
annotations from non-expert, or novice, users leads to more
comprehensive folksonomies than experts    knowledge alone.

i. introduction

knowledge production is no longer solely in the hands
of professionals: on many social web sites ordinary people
create and annotate a wide variety of content. on the social
photosharing site flickr (http://   ickr.com), for example, users
can publish photographs, tag them with descriptive keywords,
such as insect or macro, and organize them within per-
sonal directories. while an individual   s annotations express her
particular world view, collectively social annotations provide
valuable evidence for harvesting social knowledge, including
f olksonomies (f olk + taxonomies) that show how people
relate broader concepts to narrower ones. social knowledge
is idiosyncratic and may at times con   ict with knowledge
expressed in professionally curated taxonomies. for example,
many people consider spiders to be insects, at odds with
the linnean taxonomy of living organisms. however, such
knowledge is necessary to make sense of and leverage user-
generated content on the social web. thus, to    nd all images
of spiders, you will sometimes have to look for insects.
recently, plangprasopchok et al. [1] proposed a method
to learn folksonomies by integrating structured annotations
from many users, speci   cally, personal directories created by
individual flickr users to organize their photos. the method
extends af   nity propagation [2] to use structural information
to concurrently combine many shallow personal directories
into a larger common taxonomy. the method assumes that
the quality of annotation from all users is the same. how-

ever, social web users are highly diverse and vary in their
degree of expertise and expressiveness. knowledgeable users
create high quality, detailed annotations, often using technical
terms. they specify intermediate concepts within multi-level
directories, e.g., linking jumping spider to spiders to
arachnids to invertebrates. we call such users ex-
perts. novice users, on the other hand, are far less expressive,
creating shallow directories that jump granularity levels, e.g.,
linking spiders to bugs. using experts    knowledge enables
us to learn more accurate and detailed folksonomies.

diversity is important for groups and organizations [3]. it
can lead to better group decision making and organizational
robustness [4], as long as individual knowledge and opinions
are aggregated correctly [5]. hence, identifying experts from
the content they create, or from recommendations of other
people, has been an active research area. previous works used
natural
language analysis [6], [7] and id96 [8]
techniques to identify experts from the text of documents
they created, often combining it with analysis of the structure
of links within an organization [9], [10]. annotations on the
social web can help identify diverse classes of users. however,
while previous researchers classi   ed users based on their
annotation practices [11], they did not attempt to automatically
distinguish expert from novice users.

in this paper we propose methods to automatically identify
expert users who provide high quality annotations and leverage
their knowledge in folksonomy learning. first, in section ii,
we describe and evaluate a method that examines structured
annotations to automatically identify expert users. speci   cally,
our method analyzes the structure and content of personal
directories created by flickr users. in section iii we extend
the id136 method of plangprasopchok et al. [1] to use
experts    knowledge to guide the folksonomy learning process.
in section iv we show that the id136 method that exploits
user diversity by putting greater weight on annotations created
by experts can learn more accurate and detailed folksonomies
than one that
ignores diversity. surprisingly, however, we
show that while experts    knowledge is required to learn more
accurate folksonomies, novice knowledge is needed to learn
more complete folksonomies. we also carry out a detailed
investigation of the robustness of our method.

ii. identifying expert users

experts are knowledgeable individuals who can answer
questions within organizations and generate high quality data.
identifying such people is an important research topic in data
mining, management science, and social network analysis.
researchers have proposed a variety of algorithms for au-
tomatic expert identi   cation, including language [7], proba-
bilistic topic-based [8] and statistical [6] models and network
analysis tools [12], [10], that identify experts based on the
documents or email messages they exchange within organiza-
tions. hybrid approaches that combine topics and relationships
between users [9] have also been explored.

expert identi   cation is even more important for mining
user-generated content, since social web users form an ex-
tremely diverse group, with widely varying levels of expertise
and enthusiasm for different topics. as a result, the quality
of data they create also varies tremendously. one way to
differentiate data quality is by identifying expert users. we
extend the features used to measure diversity in groups [3]
and use them within a supervised expert classi   cation method.
the features measure users    expertise based on the structure of
annotations they create. unlike previous works that examined
(textual) data people create, our method looks directly at
knowledge structures they express through annotations.

a. structured annotations

social web sites allow users to annotate content they create
or share with others. in addition to tagging content, some
sites also allow users to organize it hierarchically. del.icio.us
users can group related tags into bundles, and flickr users
can group related photos into sets and then group related
sets in collections, thereby creating personal directories to
organize photos. the sites themselves do not
impose any
rules on the vocabulary or semantics of directories; in practice
users employ them to represent relations between broader and
narrower categories or concepts. personal directories offer
rich evidence for harvesting social knowledge and have been
used to learn communal taxonomies of concepts, otherwise
known as folksonomies [13], [1].

following plangprasopchok et al. [13] we call a directory
a user creates to organize photos on flickr a sapling.1 the
root node of the sapling corresponds to a user   s collection,
and inherits its name, while the leaves correspond to the
collection   s constituent sets (or other collections) and inherit
their names. the photos the user assigns to a set are tagged,
and we propagate these tags to sets and to their parent col-
lections. while most users create shallow saplings consisting
of a top-level collection and constituent sets (see fig. 1(a)),
others create detailed, multi-level hierarchies about a topic of
interest (fig. 1(b)). we call the latter users experts and the
former novice. by manually inspecting saplings created by
flickr users, we found that structure and semantic consistency
are two important factors distinguishing expert from novice

1saplings are not always tree-like. in these cases we convert them to trees.

(a)

(b)

fig. 1. saplings created by (a) novice and (b) expert users.

users. speci   cally, we have identi   ed the following hallmarks
of an expert:

    generally creates many saplings with distinct concepts
    creates deep (> 2 levels as in fig. 1(b)) or broad saplings
    provides top-level concepts that are meaningful to others.
overly-broad concepts, such as    life   ,    things   ,    misc   ,    all
sets   , etc., imply novice users

    does not jump many levels, (e.g., attach    los angeles    to
   world   ) nor mix concepts of different granularity level
(e.g.,    table mountain    and    equatorial guinea    are never
siblings, as in fig. 1(a))

    does not create con   icts (e.g., attach    los angeles    to
   journey    in one sapling while attaching    journey    to    los
angeles    in another)

    does not create multiple child concepts with same name

(e.g.,    ve    los angeles    sets under    journey   ).

b. features

to automate expert identi   cation, we convert the observa-
tions above into quantitative features. we divide the features
into two classes: user-level and sapling-level features.

1) user features: experts express a variety of concepts.
user-variety measures the number of saplings (n) and

numtwigs the number of relations a user creates.

size. we measure this by id178 bu = (   (cid:80)

user-balance measures how uniform the saplings are in
i pi ln pi)/ ln n,
where pi is the number of nodes in sapling i divided by the
total number of nodes the user creates.

user-disparity measures differences between concepts ex-
pressed in user   s saplings [3]. we compute disparity using
jensen-shannon divergence between the tag distributions of
the two saplings:

js(  i||  j) =

(0.5d(  i||  k) + 0.5d(  j||  k)),

(1)

(cid:88)

(cid:88)

i,j

i,j

   other stuff   , sorted by frequency of occurrence. a peaked
distribution (fig. 2(a)) indicates agreement among users about
sub-concepts and implies that the root concept is meaningful
to others. a    at distribution (fig. 2(b)) implies there is little
agreement about the root concept, with practically each user
expressing a different sub-concept. this indicates that
the
root concept is vague. we quantify the peakedness of the
distribution by measuring how many unique nodes are needed
to cover 30%, 50% and 70% of child nodes. for example, to
cover 70% of the distinct children of the root    europe   , we need
to look at 21.3% of the most frequent children, while to cover
the same fraction of children of    other stuff   , we need to look
at 64.6% of the most frequent children. other root concepts in
our data set that are meaningful to many users include    nature   ,
   animal   ,       ower   ,    bird   ,    usa   ,    sport   , while the vaguer, less
meaningful concepts include    location   ,    subject   ,    everything
else   ,    landscape   ,    random   ,    stuff   , and    miscellaneous   .

other features characterizing root diversity include the num-
ber of people who have created a root node with that name,
and the number of unique children the root has over all users.
c. automatically identifying experts

we collected saplings created by 7,121 flickr users who
were members of wildlife and nature photography public
groups. we trained a model
to use the features above to
automatically identify experts among these users. we trained
the model on a small set of manually labeled data and used it
to label a larger test set. we then examined and labeled new
predictions made by the model, added them to the training
set and retrained the model. we iterated this self-training
procedure on the unlabeled test data to discover new experts,
and re-trained the model with the enriched data.

to create the initial training set, we asked three annotators to
review saplings created by 200 flickr users randomly selected
from the set of 1000 who speci   ed most relations. annotators
used the criteria above to identify experts. each user   s saplings
were laid out hierarchically using yed graph visualization
tool. annotators identi   ed 20   45 experts among 200 users.
we treated 19 experts all annotators agreed upon as positive,
and the rest as negative, examples in the training set.

training set cross validation

training positive
libid166 examples examples

iterations

1
2
3
4
5
6
7
8

f

f

pr re

id79
pr re

j48
pr re
f
0.44 0.58 0.50 0.67 0.42 0.52 0.80 0.63 0.70
0.56 0.50 0.53 0.61 0.50 0.55 0.76 0.42 0.54
0.56 0.42 0.49 0.57 0.48 0.52 0.86 0.57 0.69
0.51 0.55 0.53 0.50 0.53 0.45 0.88 0.71 0.78
0.44 0.44 0.44 0.57 0.47 0.51 0.80 0.58 0.67
0.50 0.41 0.45 0.54 0.34 0.42 0.84 0.50 0.63
0.42 0.39 0.40 0.57 0.39 0.46 0.88 0.66 0.75
0.61 0.49 0.55 0.79 0.35 0.48 1.00 0.88 0.93

200
274
292
293
297
292
311
315

19
38
42
42
43
43
43
43

j48, id79, and libid166 model cross validation

results at each iteration. the size of the training set

table i

increases at each iteration as positive predictions made by the

model are added to the training set.

we trained three different models (j48 [14], random-
forest [15], and libid166 [16]) on the training set of 200

(a)

(b)

fig. 2. frequency distribution of distinct children of root nodes (a)    nature   
and (b)    other stuff   .

where   i represents tag distribution of sapling i,   k = 1/2(  i +
  j) and d(.) is id181. disparitynor-
malized simply divides the above measure by the number of
nodes in the saplings.

2) sapling features: experts express detailed knowledge

vs =(cid:80)

in particulars topics, not necessarily all topics.

sapling-variety combines depth and breadth of the sapling:
i=1:l i    ni, where l is the depth of the sapling and
ni is the number of nodes at level i. this gives more credit
to deeper representations if both saplings are equally large.

(cid:80)

(cid:0)(   (cid:80)

j)/ ln(ni)(cid:1), where ni

sapling-balance measures how balanced the sapling is
at each level. we quantify balance by normalized id178
based on expected number of nodes at current level given
the number of nodes at the previous level: bs = 1/l   
is number of
nodes at level i, pi
j is proportion of children of j   th node
at level i. for example, if there are 4 nodes in level 1 with
3, 3, 1, 2 children respectively, then n1 is 4, p1
j is (3/9, 3/9,
1/9, 2/9). to balance level 2, we need between two and three
children per parent.

j ln pi

j pi

i=1:l

several features measure concept consistency and node
uniqueness. inconsistency can be computed by the number of
con   icts (i.e. attaching node a to node b in one sapling and
b to node a in another sapling); agreement is quanti   ed by
how many users create the same parent-child relation; node
(or twig) uniqueness is computed by the ratio of unique node
names to the total number of nodes in the sapling. other
features include sapling depth, breadth, number of nodes and
terminal leaves it has, and the ratio of number of leaves to
the total number of nodes in the sapling.

root-diversity is an important hallmark of experts. ex-
perts create generalizable knowledge using categories that
are meaningful to others. a vague concept, such as    misc   ,
   other   ,    things   , will mean different things to different people.
consequently, there will be little agreement about the child
concepts of such root nodes, with every user specifying a
different child. there is far more agreement about the children
of more speci   c concepts, such as    europe   . we quantify the
generalizability of a concept by the the distribution of distinct
child nodes across all users.

given a concept (sapling root), we extract all sub-concepts
users have speci   ed as children of this root. figure 2 shows
the distributions of unique children of the roots    nature    and

feature rank

id166 relieff infogain chisquared avg.

feature name

sapling-depth

sapling-number of leaves

sapling-balance
user-balance
sapling-variety

sapling-number of children

root-diversity-50%

user-variety

user-disparitynormalized

number of twigs

sapling-number of unique twig ratio
sapling-number of unique term ratio

sapling-number of con   icts

1
5
12
7
4
9
10
14
8
11
3
6
2

1
3
2
4
14
15
8
12
13
20
16
10
22

1
3
2
7
4
6
14
10
13
8
16
18
19

1
3
2
7
4
5
14
10
13
9
16
18
19

1
2
3
4
5
6
8
7
9
10
13
14
19

feature selection results, with features sorted by their

table ii

average rank.

labeled users, and applied the models to classify unlabeled
test data. we aggregated positive predictions made by all three
models, manually labeled them, and iterated the procedure.
table i reports results of cross validation at each iteration.
we reached 100% precision, 88% recall and 93% f-score with
libid166 after eight iterations and stopped at this point. after
eight iterations, our training set had 315 users, of which 43
were experts. note that only a small fraction of all users can
be classi   ed as experts. self-training enabled us to enrich the
training set with positive examples without having to label
thousands of users. results of 10-fold cross validation of
libid166 on labeled data was 84% precision, 65% recall, and
74% f-score. applying the    nal model to the entire data set
identi   ed 66 experts in total.

to see which features are important, we used four feature
selection algorithms: id166 attribute evaluation [17], relief
for attribute estimation
[18], information gain attribute
evaluation [?], and chi squared attribute evaluation [19].
id166 attribute evaluation method based its decision function
on the support vectors of the borderline cases, while others
based their decisions on the average cases. this difference
leads to different rankings of features. relief evaluates the
importance of a feature by repeatedly sampling an instance
and estimating how well feature values distinguish among in-
stances near each other. table ii reports how different features
are ranked by these algorithms. all methods identify sapling
depth as the most important feature for identifying experts.
all methods besides id166 choose the number of leaves in the
sapling, and how balanced they are within the sapling, as the
next most important features. generally, sapling-level features
are judged to be more important than user-level features by
all methods, similar to intuitions of human annotators.

iii. using expert knowledge in folksonomy

learning

plangprasopchok et al. [1] proposed a method to learn
folksonomies by id91 many saplings created by dif-
ferent users. their relational af   nity propagation (rap) is
a probabilistic method for id91 structured data into a
common deeper and bushier tree. rap merges root nodes

(a)

(b)

(c)

fig. 3. relational af   nity propagation (rap): (a) two saplings being merged.
dashed lines surround a group of nodes assigned to the same exemplar (in
orange). (b) binary variable matrix corresponding to the con   guration in (a).
(c) factor graph formulation of binary rap.

of different saplings to extend the breadth of the learned
folksonomy, and it merges a child node of one sapling to the
root of another to extend its depth. rap is based on af   nity
propagation (ap) [2], and it identi   es a set of exemplars that
best represent all the data. exemplars emerge as messages
are passed between data items, with each item seeking an
assignment to the most similar exemplar. ap identi   es a set
of exemplars, or clusters, which maximize the net similarity
between exemplars and data items assigned to them.
following binary ap framework of [20], let c be an n   n
matrix, were n is a number of data items. a binary variable
cij = 1 if node (data item) i is assigned to node j (i.e., j is
an exemplar of i); otherwise, cij = 0. ap uses constraints to
guide the id136 process to ensure cluster consistency. the
   rst constraint, ii, which is imposed on the row i, indicates
j cij = 1).
the second constraint, ej, which is imposed on the column
j, indicates that if an item other than j chooses j as its
exemplar, then j must be its own exemplar (cjj = 1). ap
avoids assigning exemplars which violate these constraints.

that a data item can belong to only one exemplar ((cid:80)

a similarity function s(.) measures the similarity of a node
to its exemplar. if cij = 1, then we add s(cij) to the objective
function; otherwise, s(cij) = 0. the self-similarity, s(cjj),
also called preference, is usually set to less than the maximum
similarity value in order to avoid creating a con   guration with
n exemplars. in general, the higher the value of preference for
a particular item, the more likely it is to become an exemplar.
setting all preferences to the same value indicates that all
items are equally likely to become exemplars. the global
objective function measures the quality of a con   guration (i.e.,
exemplars and items assigned to them):

s(c11,       , cn n ) =

sij(cij) +

ii(ci1,       , cin )

(cid:88)

i

ej(c1j,       , c1n ).

(2)

j

a message passing algorithm [2] is used to    nd a con   guration
that maximizes the net similarity without violating i and e
constraints.

(cid:88)
(cid:88)

i,j

+

a. relational af   nity propagation

message update formulas for   ,   ,   ,   ,    and   :

in order to cluster structured data into a tree, plangprasop-
chok et al. [1] introduced a new    single parent    constraint. the
f -constraint allows a node to select another as an exemplar
only if their parents belong to the same exemplar, thus ensuring
that the learned structure forms a tree. consider id91
structured data in fig. 3(a), where exemplars are in orange, and
dashed lines surround nodes assigned to the same exemplar.
when child nodes i and k decide whether to merge with node
j, the f -constraint checks whether their parents h and m
belong to the same exemplar. figure 3(b) shows the binary
variable matrix corresponding the con   guration in (a). this
con   guration is undesirable since it does not correspond to
a tree: nodes i and k are assigned to exemplar j, but their
parents belong to different exemplars.

in its original formulation, the f -constraint was imposed on
child nodes only and could result in undesirable con   gurations.
the f -constraint checks whether i and k can be assigned
to j, and since they cannot,
it forces them into separate
clusters. while the con   guration is valid, it leads to a shallow
folksonomy. we modify the f -constraint
to prevent such
situations. the modi   ed f -constraint is imposed on both child
and parent nodes, if the parent node is also an exemplar:

                     child i : cij = 1 and

ex(pa(i)) (cid:54)= ex(pa(ne(j)))
otherwise

0

fj(c1j, . . . , cn j) =

where ne(.) returns a set of nodes that share the exemplar of its
argument, pa(.) returns index of the parent of its argument,
and ex(.) returns the index of the argument   s exemplar. in
the illustration in fig. 3, suppose that k is found to be
similar enough to j so that they can be merged. to decide
whether i too can choose j as an exemplar, the modi   ed f -
constraint checks whether the parent exemplar of node i is
the same as the parent exemplar of any of j   s neighbors. if
(cid:80)
no, i won   t be able to pick j as an exemplar. the objective
function in eq. 2 is modi   ed by the addition of the new term
j fj(c1j,       , c1n ); we use max-sum method to optimize it.

b. integrating expert knowledge

rap provides a framework to integrate experts    knowledge
in folksonomy learning. we do this simply by giving the nodes
from saplings created by experts higher preference, or self-
similarity, values. this means that these nodes will be more
likely to become exemplars, and expert knowledge will guide
the folksonomy learning process.

c. implementing rap

binary rap may be written as a factor graph shown
in fig. 3(c). following ref. [21] and ref. [1], we derived

(3)
(4)

(5)

(6)

(7)

i(cid:54)=j

  ij = s(i,j)+  ij +  ij ,
  ij =     maxk(cid:54)=j   ik,

         
         

(cid:80)
min [0,  jj +(cid:80)
(cid:80)
min [0,  jj +(cid:80)

  ij =

k(cid:54)=j max [  kj ,0]

i=j

k /   {i,j} max [  kj ,0]]

i(cid:54)=j

  ij = s(i,j)+  ij +  ij ,

  ij =

k(cid:54)=j;k   s{ne(j)} max [  kj ,0]

i=j

k /   {i,j};k   s{ne(j)} max [  kj ,0]]

(8)
  ij = s(i,j)+  ij +  ij .
in eqs. (7) and (8) s{ne(j)} represents set of nodes sharing
same parent exemplar as neighbors of j. note that we do not
need to check all neighbors of j, but just one child node among
all neighbors, since all nodes in ne(j) must already share
parent exemplar. these message update equation will make
our model favor the valid con   guration, which maximizes the
objective function s(c11,       , cn n ). since message passing
algorithms can be written in max-sum form, they can be easily
parallelized on multi-core computers [22]. we implemented
the message update formulas using map-reduce parallel pro-
gramming framework [23], which ran on 30+ node cluster.

iv. experimental results

we measured the impact of expert knowledge on the folk-
sonomies learned from flickr data. our data set consists of
20,759 saplings created by 7,121 users. a node can be a col-
lection or a set. the tags of all photos within a set are assigned
to the set node and propagated to the collection node. we
stemmed all terms (tags, set and collection names) using the
porter id30 algorithm and measured similarity between
a pair of nodes i and j by the number of common tags tij
they have among their top 40 tags: s(i, j) = min(1., tij/4).
we infer exemplars and clusters by initializing all messages
to zero, and update exemplar assignments at each iteration
until convergence. we check convergence by monitoring the
number of exemplars and the stability of net similarity.

we selected 31 seed terms consistent with ref. [1] and
generated folksonomies for these seed terms using rap with
and without expert knowledge. to learn a folksonomy, we    rst
need to select relevant saplings from the data set. we created a
snowball sample of relevant saplings as follows. for the seed
term that will be the root of the learned folksonomy,    rst we
retrieve all saplings whose root has the same name as the seed
term. we then retrieve saplings whose root has the same name
as one of the children in the    rst set of saplings, and so on.
we include expert knowledge in one of two ways: (1) using
snowball sample of relevant saplings, including those created
by the 66 experts the model identi   ed; (2) in addition to these,
use all saplings created by the experts in the snowball sample.
besides varying the amount of expert knowledge used by
the learning algorithm, we can also vary its weight. we used
the following strategies to vary the emphasis placed on expert
knowledge: (1) treat all users uniformly by setting preference
values of all nodes to the mean of similarity scores (ordinary

(a)

(b)

fig. 4.
knowledge (expert nodes in orange).

folksonomies for    africa    learned (a) without and (b) with expert

rap); (2) set preference values of expert nodes to twice the
mean, while all other preference values are set to the mean.
as an illustration, consider portion of the    africa    folkson-
omy, shown in fig. 4(a) learned using saplings such as those in
fig. 1, but without differentiating between expert and novice
users. the root has a child    christmas   , because some people
spent their christmas holidays in africa. since    christmas   
is linked to many other concepts such as    family   ,    card   ,
etc, it introduces irrelevant concepts into    africa    folksonomy.
figure 4(b) shows portion of the    africa    folksonomy learned
with expert knowledge. now the 40 nodes (   xmas   ,    family   ,
   card   , etc.) originally placed under    africa           christmas   
were moved to    holiday           christmas   . moreover,    table
mountain    and other nodes under    africa           cape town   
were moved under    africa           south africa           cape town   .
as we can see from this illustration, adding expert knowledge
helps produce a more relevant and detailed folksonomy.

a. automatic evaluation

table iii reports results of running rap in three different
settings for 31 seed terms: (m1) relevant saplings collected by
the snowball sample with no differentiation between novice
and expert users (all preference values set to the mean); (m2)
using relevant saplings plus all other saplings from experts,
with no differentiation between users (mean +exp); (m3)
same saplings as before, but with higher preference values
for experts (2*mean + exp). while the learning algorithm
generally produces several trees, we evaluate only the most
   popular    tree, one that aggregates the greatest number of
saplings. the popular tree learned by m1 contained between
14 and 7925 nodes (2001.26 on average), and that learned by
m2 between 16 and 8114 nodes (1947.87 on average), while
folksonomies learned by m3 were smaller, between 14 and
5667 nodes (1292.81 on average).

we automatically measure the quality of the learned folk-
sonomies by comparing them to the reference taxonomy
from the open directory project (odp) [24]. we applied
two metrics: lexical precision (lp) and taxonomic overlap
(to) [25]. lp measures term overlap between the learned and

reptil

invertebr

central america

south africa

seed

cat

africa
craft
   sh
dog
build

north america
south america

unit kingdom

australia
insect
   ora
vertebr
urban

unit state

bird
plant
canada

asia
sport
europ
fauna
countri
anim
   ower
world
citi

average

to

m1: mean

dp
lp
3
0.857 0.8412
3
0.181 0.5792
3
0.130 0.5866
3
0.032 0.7052
3
0.068 0.5022
3
0.379 0.6109
3
0.263 0.486
3
0.072 0.3261
4
0.013 0.4796
3
0.003 0.3508
3
0.228 0.4116
3
0.234 0.5394
3
0.258 0.5612
5
0.027 0.2901
3
0.127 0.4504
4
0.034 0.3892
4
0.061 0.3942
4
0.038 0.5203
3
0.052 0.3996
3
0.124 0.475
4
0.038 0.301
3
0.231 0.6005
4
0.055 0.4379
4
0.226 0.4328
4
0.249 0.5333
4
0.261 0.4448
4
0.086 0.4777
4
0.059 0.4328
5
0.054 0.3773
5
0.025 0.3137
5
0.005 0.4936
3.61 0.131 0.472 4.74 0.144 0.476

m2: mean + exp
dp
3
4
3
3
3
4
5
5
4
5
5
4
4
4
3
4
4
4
5
6
6
5
6
6
5
4
7
6
7
8
5

lp
to
0.857 0.841
0.197 0.599
0.134 0.586
0.024 0.587
0.019 0.389
0.396 0.610
0.155 0.441
0.079 0.335
0.014 0.496
0.037 0.366
0.217 0.466
0.095 0.416
0.171 0.541
0.027 0.349
0.127 0.450
0.034 0.390
0.061 0.394
0.038 0.525
0.051 0.397
0.115 0.461
0.039 0.305
0.219 0.583
0.052 0.449
0.208 0.444
0.252 0.535
0.240 0.438
0.075 0.530
0.054 0.446
0.053 0.391
0.027 0.358
0.005 0.500

m3: 2*mean + exp

19.51

30
1.1
0
0

dp
to %exp
lp
2
1.000 0.9199 14.28
4
0.183 0.6272 15.07
3
9.8
0.130 0.5821
3
0.472 0.8065
0
3
0.060 0.478
4
0.457 0.671
5
0.155 0.4157
3
0.174 0.4719
4
0.020 0.6661
5
2.77
0.004 0.3714
6
0.265 0.444
14.13
4
0.292 0.5991 18.21
4
6.18
0.179 0.5789
4
4.96
0.032 0.3721
3
0.131 0.4523
3.52
3
17.5
0.273 0.5986
4
2.64
0.061 0.3946
4
7.93
0.038 0.5236
5
3.97
0.058 0.4497
3
6.25
0.351 0.584
4
0.075 0.4595
6.11
6
5.49
0.216 0.5753
5
0.056 0.4676
11.8
6
0.263 0.4575 16.46
5
0.276 0.5382 10.91
5
0.264 0.4549 11.26
7
0.118 0.515
15.11
7
9.5
0.112 0.4838
7
0.053 0.3887
5.41
9
0.025 0.3549 17.04
5.67
5
0.007 0.502
4.58 0.187 0.523
9.44

evaluation of folksonomies learned for 31 (stemmed) seed

table iii

terms.

reference taxonomies, independent of their structure, while
to measures the overlap of ancestors and descendants of
a pair of terms from the learned and reference taxonomies
without considering their order. we also measure the depth
of the taxonomy. we observe that while rap leads to few
or no structural inconsistencies, integrating expert knowledge
into the learning process improves the quality of the learned
taxonomies (higher lp and to scores) and how detailed
they are (greater depth), while also removing irrelevant nodes
(smaller trees).

is expert knowledge alone suf   cient to produce high qual-
ity folksonomies? the last column in table iii shows the
percentage of nodes in the learned folksonomy that can be
attributed to experts. on average, this fraction is less than 10%.
we conclude that integrating knowledge from both expert and
novice users leads to more comprehensive folksonomies than
using expert knowledge alone.

b. manual evaluation

automatic method was not comprehensive, since it can only
evaluate portions of the learned folksonomies that used the
vocabulary of the reference taxonomy. therefore, we also
carried out a manual evaluation using the coding analysis
toolkit (cat) [26], which provides a web interface for users
to answer customized questions. each question presented to
the user a portion of the learned folksonomy, laid out as a
tree, and asked if it was correct. since the trees were generally
very large, we reduced their size as follows. for a pair of

folksonomies learned by methods m 1 and m 3 for some seed
term, we identi   ed leaf nodes with the same name and the
same ancestors in the two trees and removed them from both
trees. applying this strategy iteratively eliminated on average
50% to 70% of the nodes. if the reduced tree was still large, we
segmented it into disjoint subtrees with at most 10 child nodes
at any level. we asked    ve annotators to determine whether
each reduced tree (or subtree) was correct (837 questions
total). overall annotators judged 45.30% of the trees learned
by method m 1 and 68.24% learned by m 3 to be correct.
thus, using expert knowledge leads to better folksonomies.

we calculated statistical signi   cance of results of automatic
and manual annotation. we    nd that the difference in to
scores between rap without and with expert knowledge is
signi   cant at 95% level with t(31)=2.265, p     0.05. moreover,
rap with expert knowledge improves correctness by 23% on
the manual annotation task. we believe that combining auto-
matic and manual evaluation leads to a convincing evaluation
of folksonomy learning.

c. robustness

(a)

(b)

fig. 5.
robustness of proposed method, as measured by the taxonomic
overlap (to), with respect to (a) preference values and (b) percentage of
experts misidenti   ed.

finally, we address robustness of the method with respect
to changes in the preference values assigned to expert nodes.
we ran our algorithm for six preference values of the form
x   mean, where x     {0, 0.5, 1.0, 1.5, 2.0, 3.0}. we report
t o scores for three seeds (   invertebrate   ,    africa   , and    bird   )
in fig. 5(left). the quality of the learned folksonomies, as
measured by to, rises with preference values, and saturates
around x = 2.0.

another question is how the accuracy of automatic expert
identi   cation affects the quality of the learned folksonomies.

for this experiment, we randomly selected n% of expert nodes
and swapped their preference values with the same number
of randomly selected novice nodes. we varied percentage of
swapped nodes from 0% to 100% and report t o scores for the
three learned folksonomies in fig. 5(right). as we increased
the number of swapped nodes, t o scores dropped by 9%-
12%. note that when all expert nodes were swapped for
novice nodes, i.e., random novice nodes had their preference
values set to 2   mean, the t o scores were similar to those
that did not differentiate between expert and novice nodes.
the difference between 100% and 0% swapped is similar to
rap with and without expert knowledge, as expected. we
conclude that even moderately high errors (up to 50%) in
expert identi   cation do not signi   cantly degrade the quality
of the learned folksonomies.

v. related work

expert identi   cation has been addressed by researchers in
several different    elds. existing works analyze the (textual)
content of documents people create,
the link structure of
the interactions between people, or a combination of both
methods. zhang et al. [9] proposed a probabilistic algorithm
to    nd experts on a given topic by using local information
about a person (e.g., publications) and relationships between
people. a similar approach was used by maybury [6] to    nd
experts within organizations from the documents (publications,
publicly shared folders) they create and relations between them
(project information, citations). balog et al. [7] used genera-
tive language models to identify experts among authors of
documents, while deng et al. [8] explored topic-based model
for    nding experts in academic    elds. davitz et al. [10] used
network analysis tools to identify experts based on the docu-
ments or email messages they create within their organizations.
content quality analysis in social media has been investigated
from many research. agichtein et al. [27] investigated methods
to measuring quality of contents by content, user relationship
features. hu et al. [28] proposed quality accessing model using
the interaction data between articles and their contributors. our
approach is similar in spirit, in that we look at the contents of
data people create to identify experts, although we have not yet
included relations between people into analysis. unlike these
earlier methods, we use the structure of annotations to measure
their expertise on a topic. while korner et al. [11] proposed
a method to differentiate users in social tagging systems, they
classify users as categorizers and describers based on their
tag usage, and show that there is more semantic agreement
between describers. they do not attempt to learn taxonomies
nor differentiate the quality of annotations.

with the advent of id104 services, labeling large
datasets has become easier. however, due to variations in
annotators    abilities, signi   cant post-processing is required.
to address this problem, welinder et al. [29] proposed a
labeling strategy based on the estimation of most likely value
of current labels and annotator   s abilities. sheng et al. [30]
studied repeated   labeling strategies to improve label quality.
our work is different in the sense that on the social web users

freely choose content to label, as well as labels themselves
(tags, directories), that re   ect their own interest in content.
our work is also related to broader efforts to    crowdsource   
knowledge production, embodied, for example, by    citizen
science    projects and    wisdom of crowds    approaches [31].
researchers have studied methods that aggregate data of
varying quality [32], [33]. however, the amount and variation
of data in these studies was limited. our approach can auto-
matically identify the quality of data and aggregate it from
thousands of users.

vi. conclusion

in this paper, we propose a framework to automatically
identify experts based on the linguistic and structural features
of the annotations they create, and use experts    annotations to
guide the folksonomy learning process. we show that using
experts    knowledge can produce more accurate and detailed
folksonomies. we also show that proposed method is robust
to errors in expert identi   cation. our work generalizes beyond
flickr to other structured data sources (ebay categories, deli-
cious bundles, bibsonomy relations,    le systems).

in future work, we would like to extend automatic expert
identi   cation procedure using bayesian approach [31]. experts
are able to be modeled in continuous variable rather than 1
or 0 binary variable. by identifying experts in more detail,
we could control the degree to which experts knowledge is
used. we would also like to extend rap to apply to other
structure learning problems, such as alignment of biological
data. finally, we would like incorporate more ef   cient infer-
ence algorithm and compare the aproach to other statistical
relational learning approaches.

acknowledgment

we would like to thank jong-hyop kim for implementing
cat evaluation and the annotators for evaluating the learned
folksonomies. this work is based on research supported by
the national science foundation under awards iis-0812677
and cmmi-0753124.

references

[1] a. plangprasopchok, k. lerman, and l. getoor,    a probabilistic ap-
proach for learning folksonomies from structured data,    in proc. 4th
acm web search and data mining conf. (wsdm), february 2011.

[2] b. frey and d. dueck,    id91 by passing messages between data

points,    science, vol. 315, no. 5814, p. 972, 2007.

[3] a. stirling,    a general framework for analysing diversity in science,
technology and society,    j. royal society interface, vol. 4, no. 15, p.
707, 2007.

[4] j. surowiecki, the wisdom of crowds, 1st ed. new york, ny: anchor

books, 2005.

[5] s. e. page, the difference: how the power of diversity creates better
groups, firms, schools, and societies. princeton, nj, usa: princeton
university press, 2007.

[6] m. t. maybury,    knowledge on demand: knowledge and expert

discovery,    j. universal computing, vol. 8, no. 5, pp. 491+, 2002.

[7] k. balog, l. azzopardi, and m. de rijke,    formal models for expert
   nding in enterprise corpora,    in proc. 29th annual int. acm sigir
conference on research and development
in information retrieval.
acm, 2006, pp. 43   50.

[8] h. deng, i. king, and m. lyu,    formal models for expert    nding on
dblp bibliography data,    in data mining, 2008. icdm   08. eighth ieee
int. conf. on.

ieee, 2009, pp. 163   172.

[9] j. zhang, j. tang, and j. li,    expert    nding in a social network,   
advances in databases: concepts, systems and applications, pp. 1066   
1069, 2010.

[10] j. davitz, j. yu, s. basu, d. gutelius, and a. harris,    ilink: search and
routing in social networks,    in proc. knowledge discovery and data
mining conf. (kdd-2007), 2007.

[11] c. krner, d. benz, m. strohmaier, a. hotho, and g. stumme,    stop
thinking, start tagging - tag semantics emerge from collaborative ver-
bosity,    in proceedings of
the 19th international world wide web
conference (www 2010). raleigh, nc, usa: acm, apr. 2010.

[12] c. s. campbell, p. p. maglio, a. cozzi, and b. dom,    expertise
identi   cation using email communications,    in in cikm 03: proceedings
of the twelfth international conference on information and knowledge
management. acm press, 2003, pp. 528   531.

[13] a. plangprasopchok, k. lerman, and l. getoor,    growing a tree in the
forest: constructing folksonomies by integrating structured metadata,   
in proc. int. conf. on knowledge discovery and data mining (kdd),
2010.

[14] j. r. quinlan, c4.5: programs for machine learning.
ca, usa: morgan kaufmann publishers inc., 1993.

san francisco,

[15] l. breiman and e. schapire,    id79s,    in machine learning,

[16] c. chung chang and c.-j. lin,    libid166: a library for support vector

2001, pp. 5   32.

machines,    2001.

[17] i. guyon, j. weston, s. barnhill, and v. vapnik,    gene selection for
cancer classi   cation using support vector machines,    mach. learn.,
vol. 46, pp. 389   422, march 2002.

[18] k. kira and l. a. rendell,    a practical approach to feature selection,    in
proceedings of the ninth international workshop on machine learning,
ser. ml    92. morgan kaufmann publishers inc., 1992, pp. 249   256.
[19] p. e. greenwood and m. s. nikulin, a guide to chi-squared testing,

1st ed. new york, ny: wiley.

[20] i. givoni and b. frey,    a binary variable model for af   nity propagation,   

neural computation, vol. 21, no. 6, pp. 1589   1600, 2009.

[21] c. bishop, pattern recognition and machine learning.

springer new

york, 2006, vol. 4.

[22] c. chu, s. kim, y. lin, y. yu, g. bradski, a. ng, and k. olukotun,
   map-reduce for machine learning on multicore,    in advances in neural
information processing systems 19. the mit press, 2007, p. 281.

[23] j. dean and s. ghemawat,    mapreduce: simpli   ed data processing on

large clusters,    comm. acm, vol. 51, no. 1, pp. 107   113, 2008.

[24] v. p`amies,    open directory project,    2008.
[25] k. dellschaft and s. staab,    on how to perform a gold standard based
evaluation of ontology learning,    the semantic web-iswc 2006, pp.
228   241, 2006.

[26] c.-j. lu and s. w. shulman,    rigor and    exibility in computer-based
qualitative research: introducing the coding analysis toolkit,    int. j.
multiple research approaches, vol. 2, pp. 105   117, 2008.

[27] e. agichtein, c. castillo, d. donato, a. gionis, g. mishne, e. agichtein,
c. castillo, d. donato, a. gionis, and g. mishne,    finding high-quality
content in social media with an application to community-based question
answering,    in in proceedings of wsdm, 2008.

[28] m. hu, e. peng lim, a. sun, h. w. lauw, and b. quy vuong,

   measuring article quality in wikipedia: models and evaluation.   

[29] p. p. p. welinder,    online id104: rating annotators and obtaining

cost-effective labels,    in cvpr, 2010.

[30] v. s. sheng, f. provost, and p. g. ipeirotis,    get another label?
improving data quality and data mining using multiple, noisy labelers.   
[31] m. steyvers, m. d. lee, b. miller, and p. hemmer, the wisdom of
crowds in the recollection of order information. mit press, 2009,
pp. 1785   1793.

[32] p. hemmer, m. steyvers, and b. miller,    the wisdom of crowds with
informative priors,    in proc. 32nd annual conf. of the cognitive science
society, 2010.

[33] j. yu, w.-k. wong, and r. hutchinson,    modeling experts and novices
in citizen science data for species distribution modeling,    in proc.
ieee int. conf. on data mining, 2010.

