comparing intermittency and network measurements of

words and their dependency on authorship

diego raphael amancio
institute of physics of s  ao carlos

university of s  ao paulo, p. o. box 369, postal code 13560-970

s  ao carlos, s  ao paulo, brazil

diego.amancio@usp.br

eduardo g. altmann

max planck institute for the physics of complex systems

dresden, germany

edugalt@pks.mpg.de

osvaldo novais oliveira jr.
institute of physics of s  ao carlos

university of s  ao paulo, p. o. box 369, postal code 13560-970

s  ao carlos, s  ao paulo, brazil

chu@ifsc.usp.br

luciano da fontoura costa
institute of physics of s  ao carlos

university of s  ao paulo, p. o. box 369, postal code 13560-970

s  ao carlos, s  ao paulo, brazil

ldfcosta@gmail.com

published as: new journal of physics, 123024 (2011)

http://dx.doi.org/10.1088/1367-2630/13/12/123024

supplementary information at:

http://iopscience.iop.org/1367-2630/13/12/123024/media

1
1
0
2

 
c
e
d
 
8
2

 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 

1
v
5
4
0
6

.

2
1
1
1
:
v
i
x
r
a

1

contents

1 introduction

2 statistical quanti   cation of the role of words in texts

2.1 database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 network measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
intermittency measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3

3 evaluating the author dependency

3.1 from properties of words to properties of books . . . . . . . . . . . . . . . . . . . . .
3.2 machine learning methods and evaluation . . . . . . . . . . . . . . . . . . . . . . . .
3.3 e   ciency of the classi   cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
3.4 relative importance of di   erent features

4 discussion and conclusions

interpretation of the results

4.1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 comparison with other prediction methods . . . . . . . . . . . . . . . . . . . . . . . .
4.3 summary of conclusions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4

5
5
5
10

11
11
12
12
14

16
16
17
18

2

abstract

many features from texts and languages can now be inferred from statistical analyses us-

ing concepts from complex networks and dynamical systems. in this paper we quantify how

topological properties of word co-occurrence networks and intermittency (or burstiness) in word

distribution depend on the style of authors. our database contains 40 books from 8 authors

who lived in the 19th and 20th centuries, for which the following network measurements were

obtained: id91 coe   cient, average shortest path lengths, and betweenness. we found that

the two factors with stronger dependency on the authors were the skewness in the distribution

of word intermittency and the average shortest paths. other factors such as the betweeness

and the zipf   s law exponent show only weak dependency on authorship. also assessed was the

contribution from each measurement to authorship recognition using three machine learning

methods. the best performance was a ca. 65 % accuracy upon combining complex network

and intermittency features with the nearest neighbor algorithm. from a detailed analysis of

the interdependence of the various metrics it is concluded that the methods used here are

complementary for providing short- and long-scale perspectives of texts, which are useful for

applications such as identi   cation of topical words and information retrieval.

3

1

introduction

the application of ideas from statistical physics to text analysis has a long tradition since shannon   s
usage of id178 as the central concept in id205 [1]. in recent years, physicists have
proposed new approaches based on concepts from complex networks [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
13, 14, 15] and dynamical systems [16, 17, 18, 19, 20]. in the former, text is represented as complex
networks with words (nodes) being connected (links) using procedures depending on their syntactic or
semantic relationships [2]. several of these networks share topological properties such as the scaling
in the degree [3, 4] and the small world feature [5, 6]. the co-occurrence networks, where adjacent
words are linked to each other, are probably the most popular for applications owing to their ability
to capture important syntactic and semantic aspects of texts with a straightforward construction
procedure. these networks were employed to evaluate writing quality [7] and machine translations [8,
9], to generate and evaluate summaries [10], to construct spell checkers [11], to recognize patterns
in poetry [12] and prose [13, 14] and to study general properties of written language [15]. while co-
occurrence networks focus mainly on short scales, an increasingly popular approach addresses longer
text scales [16, 17, 18, 21, 22, 23, 24]. the usefulness of this latter approach stems from the    nding
that topical words are unevenly distributed along the text when compared to a random process
or to function words. this observation can be quantitatively investigated using di   erent analogies
and measures familiar to the communities of statistical physics and dynamical systems, including
level statistics [22, 24], burstiness [17, 18, 19], id178 [21], and intermittency measures [20]. the
author dependency on the features mentioned above has been noticed [14, 17], but little work has
been devoted to quantify the extent of this dependency and to test its usefulness to the automatic
detection of authors.

in the    eld of authorship recognition (or stylometry), one tries to identify the author of documents
whose identity is lacking [25]. some simple quantitative proposals, such as the use of word length to
distinguish between authors, go back to the mid 19th century (see ref. [26] for a historical account).
one important recent contribution was given by mosteller and wallace [27] who showed that the
frequency of function words (such as    any   ,    from   ,    an   ,    there    and    upon   ) can be used to
characterize the style of authors. this feature is so strong that even letter pair frequencies can provide
a good distinguishability between authors [26]. frequent words are also responsible for the success
of the approach     proposed and investigated by physicists     that consist in quantifying the similarity
between two books based on the distance between their word-frequency rankings [28, 29, 30]. more
recently, new features have been proposed: word length, sentence length; frequency of punctuation
marks and contractions; frequency of graphemes, collocations and words. a summary of these recent
results is given in ref. [31].

in this paper we investigate how the metrics of complex networks and intermittency     familiar to
physicists     depend on the style of authors. we start quantifying the metrics for each word (sec. 2).
these are used in the de   nition of global features for each book that are tested according to their
e   ciency in algorithms of authorship classi   cation (sec. 3). finally, we discuss the importance of
each feature (sec. 4). the primarily goal of this paper is not to improve state-of-the-art methods of

4

automatic authorship recognition. instead, we wish to estimate the dependency on authorsihp of the
selected metrics. we evaluate our results using authorship classi   cation tests because they provide a
statistical rigorous method to quantify the importance of di   erent features. nevertheless, our study
reveals interesting insights which are potentially useful in real applications and therefore we include
a comparison to more traditional statistical natural language methods (that increase the e   ciency
from 62.5% to 90.0% correct attributions, see sec. 4.2).

2 statistical quanti   cation of the role of words in texts

2.1 database

our database contains 5 novels from each of 8 authors who lived between 1809 and 1975, which are
available in an online repository (http://www.gutenberg.org/). the list of books is summarized in
sec. 1 of the supplementary information (si). to avoid e   ects from the length of the texts, each
book was limited to their    rst 18,200 tokens, which corresponds to the length of the shortest book.
in the remainder of this section we illustrate our results using the book    the adventures of sally   ,
by p. g. wodehouse. the results for all books appear in (si)-sec. 3 and are discussed in sec. 3
below.

pre-processing of the text.

before extracting complex networks and intermittency measurements from the texts, some prepro-
cessing steps were applied. initially, a pre-compiled list of stopwords including articles, prepositions
and adverbs were removed from the text (see si-sec. 2). previous work for recognizing authorship
used the frequency of function words, but we decided not to use them in our study because we are in-
terested in the interrelation between words with a pronounced semantic content. this procedure has
been employed in many works (see e.g. refs. [7, 8, 9, 12, 15]) and it is crucial to determine how these
techniques depend on the writing style of each author. next, a lemmatization step was applied to
the remaining words using the mxpost part-of-speech tagger based on the ratnaparki   s model [32].
table 1 exempli   es the application of these pre-processing steps. with this standardization, we
grouped together all words referring to a same concept, despite the di   erences in    exion.

2.2 network measurements

complex networks have been used to characterize di   erent properties of languages [5, 6, 7, 8, 9, 10,
14, 33]. here we are interested in author-speci   c characteristics and therefore we adopt a network
description based on word co-occurrence [5, 7, 8, 9, 14, 33], where nodes are words and links are
established between subsequent words. this procedure is illustrated in fig. 1. the network is
de   ned by a set v = {v1, v2, . . . vn} of vertices, and a set e of edges and is represented as a
nonsymmetric weighted matrix w . by construction, w is a square matrix of size n, where n is the
number of distinct words after the pre-processing step. the elements wij of w indicate the strength

5

table 1:
example of the pre-processing steps applied to the texts. an extract (   rst column)
obtained from the book    the adventures of sally   , by pelham grenville wodehouse is shown after
the removal of the stopwords (second column) and after lemmatization (third column).

without stopwords
original
asked sally
what   s that ? asked sally.
pay my bill for last week,
pay bill last week
due this morning. sally got up morning sally got
quickly, and    itting down the
table, put her arm round her
friend   s shoulder and whispered friend shoulder whispered friend shoulder whisper
in her ear.

after lemmatization
ask sally
pay bill last week
morning sally get
quickly    it
table put arm

quickly    itting
table put arm

ear

ear

that vi is connected to vj (vi     vj), i.e. the number of times word vj appears immediately after word
vi. additionally, we used the non-weighted and undirected network corresponding to w , denoted by
the matrix a whose elements aij = 1 if the words represented by the vertices vi and vj appeared as
neighbors at least once in the text. otherwise, aij = 0.

we shall use the statistical properties of complex network measurements in the networks w and
a. in this section we discuss the word-speci   c local measurements and in sec. 3.1 we show how to
connect them to obtain a global characterization of the network. the number of occurrences ni of
each word i represented by node vi is

ni = sin

i = sout

i =

wij,

(1)

j

the degree distribution of w is proportional to the normalized frequency (fi = ni/nt , nt =(cid:80)

where sin and sout are respectively the weight (or strength [34, 35]) of the ingoing and outgoing edges
1. therefore, the degree of each node vi is the frequency of appearance of word vi and
of node vi
i ni)
distribution of words, given by the zipf   s law [36, 37]. below we discuss three typical measurements:
id91 coe   cient, average shortest path length, and betweenness centrality.

id91 coe   cient c.

the id91 coe   cient (c) measures the id203 that the neighbors of a given vertex vi are
connected. this measurement has been widely employed in complex networks, e.g. to verify the
presence of communities [38, 39, 40] and to distinguish random networks from other small world
networks [5, 41]. traditionally, the id91 coe   cient is de   ned without considering weights or
directions as:

(cid:80)

(cid:80)

ci = 3

k>j>i aijaikajk

k>j>i aijaik + ajiajk + akiakj

,

(2)

which is equivalent to the fraction of the number of triangles among all possible triads of connected
nodes, and therefore ranges from 0 to 1. with regard to the interpretation of this measure, ferrer

for the    rst word is ni = kout(i) =(cid:80)

j wij and for the last word it is ni = kin(i) =(cid:80)

1the triple equality in eq. (1) is not valid for the    rst and for the last word in the text. the correct expression

i wij.

6

(cid:88)

figure 1: example of networks: (a) the subgraph obtained for the sentences shown in table 1; and
(b) the global network obtained from the    rst 1, 000 associations of the same book.

i cancho and sol  e [5] found that the id91 coe   cient of networks representing text was much
larger than the one expected just by chance (i.e., the value expected for the corresponding random
networks).

we singled out the words (and their neighbors) with the highest and lowest values of id91
coe   cient in the book    the adventures of sally   , by p. g. wodehouse, which are shown in table
2 for the frequency n = 52. from the de   nition, one expects that words with highest c to have
neighbors also connected to each other. this is the case of the words    sand    and    excitement   . on
the other hand, words whose neighbors are not related to each other at all display low values of c (e.g.
there is no link between the neighbors of    full    or between the neighbors of    high   ). qualitatively,
the id91 coe   cient quanti   es how words are connected to speci   c contexts. indeed, the words
   sand    and    excitement    tend to be more restricted to a speci   c context, while    full    tends to appear
in a myriad of contexts. therefore, it seems that the id91 coe   cient can be useful to detect
authorship by quantifying the tendency of using semantic-speci   c or generic words.

average shortest path length l.

a shortest path (or geodesic path) between two nodes is de   ned as the path whose sum of edge
weights is minimum. we start de   ning dij as the length of the shortest path between vi and vj
(in this case a is employed). then the average shortest (or geodesic) path length for vi (li) is the
average shortest path to all other (n-1) nodes of the network:

n(cid:88)

j=1

li =

1

n     1

dij,

(3)

2the words with ni < 5 were considered to lack statistics and were disregarded in all the analysis involving the

id91 c.

7

sallyaskpaybilllastweekmorninggetquicklyflittableputarmfriendshoulderwhisperear(a)(b)sallyaskpaybilllastweekmorninggetquicklyflittableputarmfriendshoulderwhisperear(a)(b)sallyaskpaybilllastweekmorninggetquicklyflittableputarmfriendshoulderwhisperear(a)(b)sallyaskpaybilllastweekmorninggetquicklyflittableputarmfriendshoulderwhisperear(a)(b)table 2: words of the book    the adventures of sally    with the highest and lowest id91
coe   cients (the average id91 (cid:104)c(cid:105) = 0.085), for words with ni = 5. the    ve words with c = 0
were randomly selected among the 18 words with n = 5 and c = 0.

word
shortly

excitement

sand

nose

country

startle

high

gold

gift

full

neighbors
twelve, see, say, sally, news,
never, heaven,    nd, enter, carmylle
thing, suppressed, sally, mince, last,
can, come, bristle, brief, apart
watch, want, sit, shu   ing, seat,
here, golden,    rst, dark, roville
voice, tip, sort, smut, smooth
sally, oh, glance, tell, come
time, still, somewhere, say, place
may, happen, great, glorious
shy, seem, mill, little, gratify,
gather,    rst, everyday, displeased, considerably
recess, mouth, motive, lapse,    gure,
even, disposal, critical, collar, check
voice, spin, pencil, loan, knob,
information, high, heavy, frame, buy
tongue, take, sort, potential, mean,
few, easily, compensating, blessing, acquire
tuesday, peal, later, home, happy,
gratitude, gleaming, glance, color, battle

n c
5

0.27

5

5

5

5

5

5

5

5

5

0.25

0.18

0.18

0.18

0.00

0.00

0.00

0.00

0.00

which takes low values if vi is close to the other nodes.

the words with the lowest l include the characters    sally    (l = 2.35, n = 347) and    fillmore   
(l = 2.51, n = 138), in addition to high-frequency words, such as    say    (l = 2.45, n = 349),    good   
(l = 2.46, n = 107) and    man    (l = 2.50, n = 193). as for the words with the highest l, we
found:    white-clad    (l = 6.33, n = 1),    a   ability    (l = 6.31, n = 1),    whirl    (l = 5.89, n = 1),
   jazz    (l = 5.87, n = 1),    war-aims    (l = 5.84, n = 1). interestingly, all these 5 words appeared
only once in the text, indicating that one of the reasons for a high l could be the low frequency n .
however, l is not only a consequence of the frequency n of the words, as low frequency words can
also take low values of l. this is illustrated in table 3, which compares words with the same n but
di   erent l. the frequency has a limited in   uence on l, with a pearson correlation corr(l,n ) =
-0.36 calculated over all words. actually, the determining factor is the neighborhood of the word. to
understand why this happens, consider the words    a   ability    and    repose   . while the former has
as neighbors the words    jaunty    (n = 1) and    white-clad    (n = 1), the latter has as neighbors the
words    sally    (n = 347) and night (n = 20). therefore, one may infer that l actually quanti   es
the importance of a word according to its distance to the most frequent words. since we removed
stopwords, the shortest path may be thought of as quantifying the distance from a word to the
core-content words of the book.

8

table 3: comparing the average shortest path length l for words with the same frequency n of the
book    the adventures of sally   . for a given n , l may vary widely, which shows the dependency of
l on the neighborhood connectivity.
word
red
shudder
maxwell
dark
a   ability

ni li word ni li
5
4
3
2
1

earth
lucky
funny
kiss
repose

3.71
3.97
5.55
5.15
6.34

2.99
3.00
3.10
3.08
3.11

5
4
3
2
1

table 4: comparing the betweenness b for words of the book    the adventures of sally    with the
same frequency n . for a given n , the betweenness may vary widely, since low frequency words may
have high betweenness as they may appear in di   erent contexts.

word
say
know
tell
allow
heaven
rugger
   sh
paper-knife
worship
thaw

betweenness.

word

ni bi
349
143
65
20
10
5
4
3
2
1

745, 634 sally
243, 357 fillmore
53, 904 gerald
roville
15, 816
second
1, 147
855
worthy
spectator
174
group
233
sell
44
price
11

ni bi
347
138
62
21
10
5
4
3
2
1

1, 192, 881
393, 955
108, 528
32, 449
22, 004
10, 503
14, 746
8, 320
8, 346
8, 295

betweenness b is a measurement of centrality, with higher values being assigned to the nodes con-
sidered as the most relevant in terms of linking di   erent words. in other words, with b one attempts
to quantify the frequency of access of each node, assuming that a given target node in the network is
reached from a speci   c source node via shortest paths. betweenness is de   ned as follows. let   i
st be
the number of distinct shortest paths between the source node vs and the target node vt that pass
through the node vi. if gst is the total number of shortest paths between vs and vt, then bi is given
by:

bi =

.

(4)

(cid:88)

(cid:88)

s

t

  i
st
gst

in the context of text analysis, high frequency words tend to have high b. however, some
words may play the role of articulation points by linking concepts related to distinct communities.
to illustrate this, we show in table 4 that words with similar n may take very di   erent b. a
comparison between the left and right columns suggests that words with high b connect concepts
because of their probable appearance in various contexts. therefore, similarly to the id91
coe   cient c, the betweenness centrality b seems to quantify the variety of contexts in which a word
can appear. note, however, that b is based on a global connectivity pattern, in contrast to c.

9

table 5:
in the book    the adventures of sally   , by p. g. wodehouse, there are nt = 15, 173
words (tokens), 3, 657 di   erent word types, and 716 words with ni     5. the 5 words with highest
  t /t are shown in the left part of the table. for comparison, in the right we show for each of these
words another word with the closest frequency.

word
jules
hobson
ginger
carmyle
bunbury

ni
26
31
115
54
20

ii       t /t word ni
26
4.31
4.09
31
117
3.86
53
3.60
3.59
20

turn
here
get
feel
people

i       t /t
1.55
1.35
1.24
0.87
1.39

2.3 intermittency measurements

the uneven distribution of words across di   erent documents is an essential feature exploited in
statistical natural language processing. for instance, by investigating words appearing over con-
centrated in speci   c documents (when compared to their overall frequency) one can detect keywords,
topics, and authorship [27, 42]. this is the basic idea of the term frequency - inverse document
frequency (tf-idf) and related measures that are also at the core of search engines [42]. how-
ever, there are numerous situations where the comparison to a general database is not available or
is not interesting. for instance, when authorship has to be attributed without previous knowledge
of texts written by the potential authors. here we approach these problems by taking advantage
of the    nding that words are unevenly distributed not only across documents but also within
them [16, 17, 18, 21, 22, 23, 24].

the quanti   cation of the uneven distribution of words has been proposed based on measures
commonly used by physicists [16, 24]. following refs. [21, 22, 24], we use the statistics of recurrence
times, a standard quanti   cation of intermittency or burstiness in time series [18, 19]. in texts, time
is counted by the number of words and for each word i the recurrence time tj is de   ned as the
number of words between two successive occurrences of i (the j and j + 1 occurrence) plus one. for
instance, the recurrence times for the word    the    in the previous sentence are t1 = 9 and t2 = 7.
a word that appears ni times in a text of size nt leads to a sequence of nt     1 inter-occurrence
times {t1, t2, ..., tnt    1}. in order to incorporate also the time until the    rst tf and after the last tl
in this case t = nt /ni, where the overline
occurrence of the word, we consider tn = tf + tl.
denotes average over the di   erent tj   s. note that the mean recurrence time ti gives no additional
information than the frequency ni. the intermittency of the word appears in the variance of tj   s
around t and can be quanti   ed by i       t /t where   t =
. randomly distributed words
in the text have i = 1 (in the limit of large n and small ni/nt ), intermittent words have i > 1, and
words appearing in regular intervals have i < 1. we calculate the intermittency measure ii =   t /t
for all words with ni     5 in each of the books (   ltered texts) described in sec. 2.1. the words with
ni < 5 were considered to lack statistics and were disregarded.

t 2     t

(cid:113)

2

in table 5 we compare words with highest i =   /t to words with similar frequency. it is clear
that the most intermittent words (largest   t /t ) are topical words (e.g., name of characters and

10

locations), regardless of their frequency.
indeed, 15 out of the 16 most intermittent words. are
directly connected to speci   c characters. a similar behavior is observed in all books of our database.
intermittency is therefore a good characterization of topical words that in turn plays an important
role in the author-speci   c characteristic of the texts. the relationship between   t /t and the function
of the words has been investigated in detail in refs. [16, 17, 18, 21, 22, 24]. in the next section we
explore the fact that these properties are also author speci   c [17].

3 evaluating the author dependency

3.1 from properties of words to properties of books

in the previous section we introduced    ve quantities characterizing properties of words in the text:
frequency (n ), average shortest path length (l), betweenness (b), id91 coe   cient (c), and
intermittency (i = {  t /t}). the values of these quantities for all words in the books in our
database can be found in si-sec. 3. we now analyze the global distribution of these measurements
for all the words in a given book by plotting the empirical id203 density function   (x) for the
measurements x = {n, l, b, c, i}. fig. 2 shows the results for one book, and similar distributions
were obtained for the other books. the shortest path l, id91 c, and intermittency i have a well
de   ned peak and width (akin to a gaussian distribution), but the frequency n and betweenness b
have broad tail distributions (as in power law distributions   (x)     x     ). the tail in n corresponds
to the well-known zipf   s law, which also appears in b as expected from the large correlation between
b and n (corr(b,n ) = 0.95 in the book of fig. 2). with the two di   erent behaviors we propose
two sets of measurements, one for x = {l, c, i} and another for x = {n, b}.

(cid:80)m

our goal is to obtain quantities characterizing important features of these distributions to be used
as global measurements of the books. the most natural choice is the average value (cid:104)x(cid:105), where (cid:104). . .(cid:105)    
1
i=1 . . . corresponds to an average over the m di   erent words. for the network measures l, c, i
m
this corresponds to the average values over nodes, a quantity considered as characteristic of the
network [7, 12, 14, 43]. for x = {n, b}, the highly frequent words contribute strongly to (cid:104)x(cid:105)
due to the long tails. to compensate for this e   ect, we consider also a modi   ed average de   ned
as (cid:104)x(cid:105)2     (cid:104)log x(cid:105) for x = {n, b}. for x = {l, c, i} the opposite is true, i.e., (cid:104)x(cid:105) is dominated
by the large number of low frequency words. accordingly, we introduce a modi   ed average as
i xi log ni, i.e., a weighted average with weights proportional to the logarithm of the
frequency. the quantities (cid:104)x(cid:105) and (cid:104)x(cid:105)2 are expected to give a good account of    typical    values of x.
however, in sec. 2 we mentioned that important information is conveyed by words with large x,
i.e., in the tails of the distributions shown in fig. 2. in order to characterize the fat-tail distributions
of x = {n, b}, we used the coe   cient   x of a power-law    t to the tails of   (x)3. an additional
motivation for using   n comes from the suggestion in ref. [45] that it serves as a quanti   cation of
the style of texts. the large values of x = {l, c, i} were characterized by calculating the skewness

(cid:104)x(cid:105)2     (cid:80)

3the    tting was performed to the cumulative distribution with logarithmic binning size, as suggested in ref. [44].

a cut-o    x > 3 103 was used for x = b (see fig. 2e), no cut-o    was used for x = n .

11

of   (x), a measure of the asymmetry of the distribution. in summary, the three features we use for
each of the    ve quantities x = {n, b, l, c, i} are:

average value:

(cid:104)x(cid:105)

for x = {n, b, l, c, i}.

(cid:104)x(cid:105)2 =

(cid:40) (cid:104)log(x)(cid:105)
             in x     
skewness(x)     (cid:104)(cid:16) x   (cid:104)x(cid:105)

(cid:104)x log n(cid:105)/(cid:104)log n(cid:105)

(cid:17)3(cid:105)

  x

for x = {n, b},
for x = {l, c, i}.
for x = {n, b},
for x = {l, c, i}.

modi   ed average:

right tail:   (x) =

(5)

(6)

(7)

these features are given in fig. 2 for one book (see si-sec. 3 for all 40 books). obviously, the choice
of the quantities above is inevitably arbitrary. our choice was intended to capture features of the
distribution, rather than giving a parametric description of the full distribution. in particular, the
power-law    t in eq. (7) does not intend to fully describe the distributions, as apparent in fig. 2(d,e).

3.2 machine learning methods and evaluation

in order to quantify the ability of the features described above to distinguish between authors, we
employ machine learning algorithms which induce classi   ers from a training database. the robustness
of our results is tested with three widely used algorithms based on di   erent principles. the    rst is
known as c4.5 [46], and generates id90 based on the information gained by each feature;
the second algorithm is the naive bayes [47], which is based on the id47; and the third and
simplest algorithm is the nearest neighbor [48], which classi   es an unknown instance according to
the nearest neighbor of that instance in a normalized space involving all features. for more details,
see si-sec. 4.

3.3 e   ciency of the classi   cation

we consider the problem of distinguishing between 8 authors, using    ve books to represent each
author   s style. more speci   cally, each book described in sec. 2.1 was characterized by the set of
15 features discussed in sec. 3.1 ((cid:104)x(cid:105), (cid:104)x(cid:105)2 and   (x) for x = {n, b, l, c, i}). the authorship
assignment was performed using the algorithms in sec. 3.2 applied to a training dataset independent
of the test book using the cross validation methodology (see si-sec. 4). this technique ensures that
the training and evaluation sets are di   erent and it is equivalent to assigning the authorship of one
book in an experiment where 4 books of 8 authors were used as a training dataset. the    nal output
of the algorithms is the assignment of a speci   c author to each book tested, and the e   ciency is
quanti   ed simply as the fraction of successful assignments.

the results are summarized in table 6 and indicate accuracy rates between 42.5% and 50.0%
when all 15 features were used. these results were statistically signi   cant by a large amount, con-
   rming that these features successfully capture author speci   c characteristics. to further explore the

12

cumulative distribution   (x     x)     (cid:82)    

figure 2: id203 density function   (x) obtained from the di   erent words of the book    the
adventures of sally   , by p. g. wodehouse. (a) x = l shortest path, (b) x = c id91 coe   cient,
(c) x = i intermittency, (d) x = n frequency, and (e) x = b betweenness. in (d) and (e) the
x   (x)dx is shown, with the density   (x) depicted in the
inset. the legends indicate the features de   ned in eqs. (5)-(7) obtained for these distributions, and
cor(x, n ) indicates the pearson correlation coe   cient between x and n calculated over all words.

13

100101102n  - frequency10-310-210-1100  (n   n)<n>=4.4<n>2=2.0  (n)=2.3cor(n,n)=+1.0102103104105106b - betweeness centrality10-310-210-1100  (b   b)<b>=5301<b>2=191   (b)=2.1cor(b,n)=+0.952345l -  shortest path00.51  (l)<l>=3.65<l>2=3.20  (l)=0.8cor(l,n)=-0.3600.10.20.3c - id91 coefficient0246810  (c)<c>=0.095<c>2=0.093  (c)=1.7cor(c,n)=-0.590123i - intermittency00.51  (i)<i>=1.13<i>2=1.19  (i)=2.1cor(i,n)=+0.18010n00.6  (n)080b00.02  (b)(a)(b)(c)(d)(e)table 6: accuracy rate achieved for the three machine learning algorithms using all 15 features
and the best combination of these features. the accuracy is estimated based on 40 authorship
assignments. the p-values correspond to the id203 of getting by chance a higher or equal
accuracy in one (all features) and in 215 = 32, 768 (best case) trials. the features included in the
best cases can be found in si-tables s1-s4.

algorithms

all 15 features
best case

decision tree c4.5 nearest neighbor knn
47.5 % (p = 6 10   8)
50.0 % (p = 1 10   8)
65.0 % (p = 4 10   10)
62.5 % (p = 5 10   9)

naive bayes

42.5 % (p = 2 10   6)
62.5 % (p = 5 10   9)

accuracy of di   erent methods, we considered cases in which only some of the features were included
in the algorithms. we tested all 215 = 32, 768 combinations of the 15 features and obtained a best
result of 65.0% of correct assignments.

3.4 relative importance of di   erent features

in evaluating the importance of the di   erent features on the    nal results it is essential to identify
their mutual dependency. we start from the list of all 215 = 32, 768 combinations of features ordered
by decreasing accuracy (as shown in si-tables s1-s4). we wish to quantify when feature y appears in
the top of this list. to this end, we count the fraction of the 214 feature combinations that include y
with accuracy higher or equal to a threshold. the    nal estimate is then given by the area-under-the
curve of the roc plot obtained by varying the threshold [49]. this procedure is equivalent to the
mann-whitney u test [50]. the motivation for using this method is that it evaluates the importance
of a speci   c feature by taking into account how it combines with the other features to improve the
accuracy of the prediction. the method depends both on the prediction algorithm and on the other
features.

the features were ranked based on the method described above. the results for the 3 prediction
algorithms are given in the three    rst columns of table 7. the three features appearing as the
most prominent are (cid:104)n(cid:105) (average frequency),   (i) (skewness of intermittency), and (cid:104)n(cid:105)2 (average
logarithmic frequency). in order to state the importance of features beyond speci   c algorithm it is
important to quantify in which extent the results obtained for the three algorithms (   rst 3 columns)
are consistent with each other. to this end we compute the spearman   s rank correlation and obtain
the values 0.29 (p-value = 0.145), 0.49 (p-value= 0.032) and 0.67 (p-value= 0.003) for the pairs
c4.5/knn, c4.5/bayes and knn/bayes, respectively. the p-values are computed under the null
hypothesis that the rankings are independent. altogether, the three p-values indicate that the three
rankings are consistent with each other. this is a strong indication that our analysis goes beyond
algorithm-speci   c results and indeed captures the in   uence from the features.

it is interesting to compare the results to evaluations taking into account each feature separately.
this can be done either by considering the accuracy of the prediction using only the speci   c feature
or by comparing the information gained by including the feature [51]. this last method has the
advantage of being independent of the prediction algorithm. these results are shown in the 4 last

14

table 7: ranking of features based on the accuracy rate of the classi   ers, where 1 in the table
means best, 2 second best and so on. the results for each classi   er algorithm (c4.5, knn and bayes)
are reported using di   erent ranking procedures combined with multiple features (mann-whitney u
test, columns 1-3, information gain (column 4) and accuracy using only one feature (columns 5-7)).
the last column reports the pearson correlation between each feature and the vocabulary size m
(number of di   erent words) calculated over the 40 books in our database. the features in the table
are ordered according to the decreasing geometric mean of the ranks obtained in the 3 multiple
features analysis (this ordering is the same achieved by considering for each feature the likelihood of
reaching by chance a ranking as good as the one in each of the three ranking schemes). the areas
under the curve in the multiple features analysis ranged between 56% and 69%.

multiple features
c4.5 knn bayes

single feature

correlation

info c4.5 knn bayes

with m

6
2
1
7
5
10
8
12
4
3
9
11
13
15
14

1
2
6
4
8
3
7
11
13
14
9
10
5
12
15

1
2
3
6
5
10
8
4
11
14
9
7
12
13
15

3
10
2
9
1
15
8
6
13
11
7
5
12
4
14

2
12
1
5
3
15
5
5
10
8
9
4
13
10
14

5
9
2
3
1
12
7
5
9
9
14
3
15
8
12

1
10
3
8
2
12
5
5
13
9
5
4
10
13
15

-0.90
-0.08
-0.96
0.85
0.98
-0.34
0.85
-0.87
-0.13
-0.07
0.88
-0.87
-0.29
0.81
0.07

(cid:104)n(cid:105)2
  (i)
(cid:104)n(cid:105)
(cid:104)l(cid:105)
(cid:104)b(cid:105)
(cid:104)i(cid:105)2
(cid:104)l(cid:105)2
(cid:104)c(cid:105)
  (l)
  (b)
(cid:104)b(cid:105)2
(cid:104)c(cid:105)2
(cid:104)i(cid:105)
  (n )
  (c)

15

table 8: list of the 2 most relevant combinations of features, as revealed by a factorial analysis
for the c4.5, knn and bayes classi   er algorithms. as expected from table 7,   (i) provides good
results when used in conjunction with other features, such as (cid:104)n(cid:105), (cid:104)n(cid:105)2   (b) and (cid:104)i(cid:105)2
(cid:104)n(cid:105), (cid:104)n(cid:105)2 and (cid:104)b(cid:105)

  (l), (cid:104)c(cid:105) and (cid:104)c(cid:105)2

bayes

knn

c4.5

.

1st combination
2nd combination

  (i) and (cid:104)n(cid:105)

  (i) and   (b)
  (i) and (cid:104)n(cid:105)2

(cid:104)i(cid:105)2 and   (i)

columns of table 7. note that some features appearing as very important in the multiple features
analysis are not informative when taken alone (e.g., the skewness of the intermittency   (i)). on the
other hand, features that are well ranked in the single feature analysis do not always appear among
the most important features when multiple features are considered (e.g., the weighted average of the
id91 (cid:104)c(cid:105)2). these observations show the nontrivial mutual dependency of the features. to
further explore this we performed a factorial analysis (see si-sec. 5) using the 12 most important
features in table 7, with the most important combinations being summarized in table 8. as expected
from table 7, in fact   (i) appears among the 2 best combinations of features in all three algorithms,
which con   rms that its e   ectiveness is correlated with its interdependence with other features.

4 discussion and conclusions

4.1

interpretation of the results

we are now in a position to use the word-speci   c analysis (sec. 2) and the distribution (sec. 3.1) of
the quantities x = {n, b, l, c, i} to assess the importance of the di   erent features (cid:104)x(cid:105),(cid:104)x(cid:105)2, and
  (x) in table 7:

n frequency. this was the most e   cient quantity for recognizing authorship with (cid:104)n(cid:105) and (cid:104)n(cid:105)2
among the 3 most important features. noting that (cid:104)n(cid:105) is proportional to the inverse number
of distinct words m :

(cid:104)n(cid:105) =

length of book

(8)
one infers that the distinguishing feature between the authors captured by (cid:104)n(cid:105) is the di   erent
vocabulary sizes. the modi   ed average (cid:104)n(cid:105)2 = (cid:104)log n(cid:105) also captures this aspect, including
the proportion of frequent and infrequent words. on the other hand, the poor performance of
  (n ) (=    in   (n )     n     ) is a clear signature of the universal, author-independent, character
of zipf   s law (at    xed book size [37]).

m

,

b betweenness. the average betweenness (cid:104)b(cid:105) was useful because of its strong correlation with the
vocabulary size of the book m (last column in table 7). in network terms, this corresponds to a
linear relationship between (cid:104)b(cid:105) and network size (m , number of nodes) and can be understood
by noting that the number of terms in the sum of de   nition of bi in eq. (4) is proportional
to m 2, so that (cid:104)b(cid:105) is expected to scale linearly with m for a    xed book size. the fact that (cid:104)b(cid:105)2

16

and   (b) had a poor performance indicates that the number of words with high betweenness
is not a relevant feature to distinguish between authors.

l shortest path. this was the network quantity with best performance.

(cid:104)l(cid:105) quanti   es the
typical distance of words to the central hubs of the network (frequent words). the good
performance points to a dependence on the style of the authors. the poorer performance
of (cid:104)l(cid:105)2 and   (l) indicates that the style dependency in l is more prominent in the typical
values than, respectively, in the frequent and large l words.

c id91. the poor performance of all values related to this quantity suggests that authors
have very little freedom in choosing the id91 of words co-occurrence networks. the last
position in the ranking of   (c) in table 7 suggests that the fraction of words used in speci   c
contexts (high c) is author independent. the two averages (cid:104)c(cid:105)2 and (cid:104)c(cid:105) take similar values
(as seen in fig. 2 and in si-sec. 3, recall the restriction ni     5 used in sec. 2.2). they perform
well only when used alone, possibly because of their correlation to vocabulary size m .

i intermittency. apart from the frequency, intermittency was the most important quantity in
table 7 with the skewness of the distribution   (i) playing a prominent role. in view of the
results in sec. 2.3,   (i) may be interpreted as the fraction of all words that are topical or
   keyword like   . the poor performance of (cid:104)i(cid:105) is not surprising since i is normalized by frequency
(i       t /t ) and therefore (cid:104)i(cid:105)     1 is expected. indeed, from all 5 quantities the i   features
have shown altogether the smallest absolute value of correlation with vocabulary size (tab. 7),
explaining why even   (i) has a poor relative performance when used alone. finally, (cid:104)i(cid:105)2
performs better than (cid:104)i(cid:105) suggesting that frequent words are the more relevant ones.

4.2 comparison with other prediction methods

even if the main goal of this paper is to evaluate the importance of di   erent factors, it is also useful
to compare the accuracy of our results with other methods of authorship attribution. uzuner and
katz [52] used a database of books similar to ours, produced by 8 authors. they used    ve sets of
features, including simple statistics and more sophisticated syntactic analysis (table 3 of ref. [52]).
our best results (accuracy of 65%) is comparable to their second best case obtained using    syntactic
elements of expression    (62%), being signi   cantly worse only than their best result, achieved using
function words (87%). in an extensive review, grieve [31] reported accuracies obtained with a set
of 34 features varying between 33% and 87% for the case of 5 authors, and between 18% and 80%
for 10 authors (table 9 in ref. [31]). our best results are above the median of their results achieved
by using di   erent features. their best results again are based on the relative frequency of function
words. these results are in accordance with the long tradition started by mosteller and wallace to
use the frequency of function words to distinguish between authors [27].

in order to con   rm this in our database, we implemented a series of prediction schemes using the
frequency ni of frequent (mostly function) words. di   erently from the approach described in this
paper that used average and scaling properties as features, now the frequencies of speci   c words are

17

used directly as input features of the prediction algorithms. we used only the knn algorithm because
the other algorithms did not provide good results when too many features were included. when the
list of 70 stopwords from table 2.5 of ref. [27] was used, we obtained an accuracy of 62.5%, i.e.,
comparable to our best results. following ref. [31], we considered two other lists of words: all 1, 978
words that appear in at least one book of each of the authors, leading to accuracy of 90%; and all 209
words that appear at least once in every book in our database, leading to an accuracy of 82.5%. we
recall that in order to concentrate on analysis that focus on words with pronounced semantic content
instead of function words, we have deliberately excluded a list of stopwords that comprised 80%
of the cases listed in ref. [27]. therefore, our best combination of features compares well to other
methods which demand more sophisticated syntactic analysis of the text. measurements of complex
network and intermittency are indeed able to capture many of the author-dependent characteristics.
in order to illustrate how measures analyzed in this paper can be complementary to traditional
methods we have performed a very simple experiment using as features the frequency and intermit-
tency of the set of words composed by the    ve most frequent words in each book. the accuracy
in classifying the books only by the frequency was 72.5 % and only by intermittency was 37.5 %.
although this last accuracy rate is not impressive, it is statistically signi   cant (p = 2.2 10   4) and
shows that the intermittency values of speci   c words across distinct authors is di   erent. the accu-
racy is increased to 80 % when both features were included. it remains to be shown in future works
how our results can improve state of the art methods of authorship attribution.

4.3 summary of conclusions

we have shown that the style of di   erent authors leave    ngerprints in very general statistical measures
of texts based on the network of co-occurrence of words and on intermittency or burstiness of words.
the statistically signi   cant scores obtained in authorship attribution unequivocally show that the
style dependence of these features can be used in practice. regarding the prominence of the di   erent
features, we note that both the results and ranking of features may depend on the database, selected
features and attribution algorithms. accordingly, as emphasized in ref. [31], di   erent algorithms and
features have to be tested in a given corpus before any real application of authorship attribution.
however, the robustness of our results using three radically di   erent attribution algorithms strongly
suggests that the di   erent features have importance that go beyond speci   c algorithms. two features
should be highlighted: (i) the skewness of the distribution of intermittent words   (i), which is based
on the long-scale distribution of words and detects the extent into which topical words (keywords,
large i       t /t ) were used in the book; and (ii) the mean shortest path of the word co-occurrence
network (cid:104)l(cid:105), which is based on the short-range connectivity of words and detects the typical distance
of words to all other words. the di   erent natures of these two quantities suggest a complementary
role for capturing both short- and long-scale properties of the text, as well as typical and exceptional
words.

our focus in this paper was on the evaluation of the di   erent features, rather than on maximizing
the e   ciency of the authorship attribution algorithms. this is apparent when comparing the best

18

accuracy rates we achieved using our approach (62.5%) and using previous proposals (90.0%), as
discussed in sec. 4.2. a further limitation of approaches based on intermittency and networks
is that they require large pieces of text. while the root of the success of previous methods rely
on the observation that function words are a powerful tool to detect the style of authors [27, 31,
52], in the complex network and intermittency approaches used in this paper the focus is on the
content words.
in this sense the results we achieve can be thought as being complementary to
the analysis using function words. more speci   cally, our results suggest that using   (i) and (cid:104)l(cid:105) can
improve authorship recognition techniques when used in combination with the many di   erent features
currently employed [31]. finally, the successful application of these measurements to characterize the
style of authors suggests that the quantities discussed here can be further explored in other linguistic
tasks, an approach that has been limited to a few works (see e.g. [14, 53]).

19

references

[1] shannon c e 1948 bell system technical journal 27 379

[2] sol  e r v 2010 complexity 15 20

[3] ferrer i cancho r and sol  e r v 2003 procs. natl. acad. sci. usa 100 788

[4] ferrer i cancho r 2005 physica a 345 275

[5] ferrer i cancho r and sol  e r v 2001 proceedings of the royal society of london b 268 2261

[6] ferrer i cancho r, sol  e r v and k  ohler r 2004 physical review e 69 051915

[7] antiqueira l, nunes m g v, oliveira jr o n and costa l f 2007 physica a 373 811

[8] amancio d r, nunes m g v, oliveira jr. o n, pardo t a s, antiqueira l, costa l f 2011

physica a 390 131

[9] amancio d r, antiqueira l, pardo t a s, costa l f, oliveira jr. o n, nunes m g v 2008

international journal of modern physics c 19 583

[10] antiqueira l, oliveira jr. o n, costa l f and nunes m g v 2009 information sciences 179

584

[11] choudhury m, thomas m, mukherjee a, basu a and ganguly n 2007 proceedings of the second
workshop on textgraphs: graph-based algorithms for natural language processing p. 81688

[12] roxas r m and tapang g 2010 international journal of modern physics c 21 503

[13] stevanak j t, larue d m and lincoln d c 2010 arxiv: 1007.3254

[14] antiqueira l, pardo t a s, nunes m g v, oliveira jr. o n and costa l f 2006 proceeedings

of the workshop in information and human language technology

[15] masucci a p and rodgers g j 2006 physical review e 74 026102

[16] montemurro m and zanette d 2002 advances in complex systems 5 7

[17] berryman m j, allison a, abbott d 2003 fluctuation and noise letters 3 l1

[18] altmann e g, pierrehumbert j b, motter a e 2009 plos one 4 e7678

[19] goh k i and barabasi a l 2008 europhysics letters 81 48002

[20] allegrini p, grigolini p and palatella l 2004 chaos soliton fract 20 95

[21] herrera j p and pury p a 2008 eur. phys. j. b 63 135

20

[22] carpena p, bernaola-galv  an p, hackenberg m, coronado a v, oliver j l 2009 physical review

e 79 3

[23] katz s m 1966 natural language engineering 2 15

[24] ortu  no m, carpena p, bernaola-galv  an p, mu  noz e and somoza a m 2002 europhys. lett. 57

759

[25] oakes m 2004 proceedings of the 5th international conference on recent advances in soft

computing

[26] tankard jr. w j 2001 applications of computer content analysis (chapter 4)

[27] mosteller f and wallace d l 1963 journal of the american statistical association 58 302

[28] havlin s 1995 physica a 216 148

[29] vilensky b 1996 physica a 231 705

[30] yang a c c, peng c k, yien w k and goldberger a l 2003 physica a 329 473

[31] grieve j 2007 literary and linguistic computing 22 3

[32] ratnaparki a 1996 proceedings of the empirical methods in natural language processing con-

ference

[33] dorogovtsev s n and mendes j f j 2001 proceedings of the royal society of london b 268

2603

[34] barth  elemy m, barrat a, pastor-satorras r and vespignani a 2005 physica a 346 34

[35] costa l f, sporns o, antiqueira l, nunes m g v and oliveira jr. o n 2007 applied physics

letters 91 054107.

[36] zipf g k 1949 addison-wesley human behavior and the principle of least e   ort

[37] bernhardsson s, rocha l and minnhagen p 2009 new journal of physics 11 123015

[38] girvan m and newman m e j 2002 proc. natl. acad. sci. usa 99 7821

[39] newman m e j and girvan m 2004 physical review e 69 026113

[40] newman m e j 2010 oxford university press networks: an introduction

[41] m barthelemy, amaral l a n 1999 physical review letters 82 3180

[42] manning c d and sch  utze h 1999 foundations of statistical natural language processing, the

mit press, cambridge

21

[43] costa l f, rodrigues f a, travieso g, villas boas p r 2007 advances in physics 56 167

[44] bauke h 2007 european physical journal b 58 167

[45] stevanak j t, larue d m, carr l d 2010 arxiv: 1007.3254

[46] quinlan r 1993 morgan kaufmann publishers

[47] john g h and langley p 1995 11 conference on uncertainty in arti   cial intelligence, p. 338

[48] aha d w, kibler d and albert m k 1991 machine learning 6 37

[49] k a spackman 1989 proceedings of the sixth international workshop on machine learning p.

160

[50] mann h b, whitney d r 1947 annals of mathematical statistics 18 50660

[51] witten i h, frank e and hall m a 2011 data mining: practical machine learning tools and

techniques, third edition

[52] uzuner o and katz b 2005 sigir workshop on stylistic analysis of text for information

[53] paranyuhkin d 2010 document available online at http://issuu.com/deemeetree/docs/text-

network-analysis (accessed june 2011)

22

