the pulse of news in social media: forecasting popularity

roja bandari

department of electrical engineering

ucla

los angeles, ca 90024

roja@ucla.edu

sitaram asur

social computing lab

hp labs

palo alto, ca 94304
sitaram.asur@hp.com

bernardo a. huberman

social computing lab

hp labs

palo alto, ca 94304

bernardo.huberman@hp.com

2
1
0
2

 

b
e
f
2

 

 
 
]

y
c
.
s
c
[
 
 

1
v
2
3
3
0

.

2
0
2
1
:
v
i
x
r
a

abstract

news articles are extremely time sensitive by nature.
there is also intense competition among news items to
propagate as widely as possible. hence, the task of pre-
dicting the popularity of news items on the social web
is both interesting and challenging. prior research has
dealt with predicting eventual online popularity based
on early popularity.
it is most desirable, however, to
predict the popularity of items prior to their release,
fostering the possibility of appropriate decision mak-
ing to modify an article and the manner of its publi-
cation. in this paper, we construct a multi-dimensional
feature space derived from properties of an article and
evaluate the ef   cacy of these features to serve as predic-
tors of online popularity. we examine both regression
and classi   cation algorithms and demonstrate that de-
spite randomness in human behavior, it is possible to
predict ranges of popularity on twitter with an overall
84% accuracy. our study also serves to illustrate the
differences between traditionally prominent sources and
those immensely popular on the social web.

1

introduction

news articles are very dynamic due to their relation to con-
tinuously developing events that typically have short lifes-
pans. for a news article to be popular, it is essential for it to
propagate to a large number of readers within a short time.
hence there exists a competition among different sources to
generate content which is relevant to a large subset of the
population and becomes virally popular.

traditionally, news reporting and broadcasting has been
costly, which meant that large news agencies dominated the
competition. but the ease and low cost of online content cre-
ation and sharing has recently changed the traditional rules
of competition for public attention. news sources now con-
centrate a large portion of their attention on online mediums
where they can disseminate their news effectively and to a
large population. it is therefore common for almost all ma-
jor news sources to have active accounts in social media ser-
vices like twitter to take advantage of the enormous reach
these services provide.

due to the time-sensitive aspect and the intense competi-
tion for attention, accurately estimating the extent to which
copyright c(cid:13) 2012, association for the advancement of arti   cial
intelligence (www.aaai.org). all rights reserved.

a news article will spread on the web is extremely valuable
to journalists, content providers, advertisers, and news rec-
ommendation systems. this is also important for activists
and politicians who are using the web increasingly more to
in   uence public opinion.

however, predicting online popularity of news articles is
a challenging task. first, context outside the web is often not
readily accessible and elements such as local and geographi-
cal conditions and various circumstances that affect the pop-
ulation make this prediction dif   cult. furthermore, network
properties such as the structure of social networks that are
propagating the news, in   uence variations among members,
and interplay between different sections of the web add other
layers of complexity to this problem. most signi   cantly, in-
tuition suggests that the content of an article must play a
crucial role in its popularity. content that resonates with a
majority of the readers such as a major world-wide event can
be expected to garner wide attention while speci   c content
relevant only to a few may not be as successful.

given the complexity of the problem due to the above
mentioned factors, a growing number of recent studies
(szab  o and huberman 2010), (lee, moon, and salamatian
2010), (tatar et al. 2011), (kim, kim, and cho 2011), (ler-
man and hogg 2010) make use of early measurements of an
item   s popularity to predict its future success. in the present
work we investigate a more dif   cult problem, which is pre-
diction of social popularity without using early popularity
measurements, by instead solely considering features of a
news article prior to its publication. we focus this work on
observable features in the content of an article as well as its
source of publication. our goal is to discover if any predic-
tors relevant only to the content exist and if it is possible to
make a reasonable forecast of the spread of an article based
on content features.

the news data for our study was collected from
feedzilla 1    a news feed aggregator    and measurements of
the spread are performed on twitter 2, an immensely popu-
lar microblogging social network. social popularity for the
news articles are measured as the number of times a news
url is posted and shared on twitter.

to generate features for the articles, we consider four dif-

1www.feedzilla.com
2www.twitter.com

ferent characteristics of a given article. namely:
    the news source that generates and posts the article
    the category of news this article falls under
    the subjectivity of the language in the article
    named entities mentioned in the article
we quantify each of these characteristics by a score making
use of different scoring functions. we then use these scores
to generate predictions of the spread of the news articles us-
ing regression and classi   cation methods. our experiments
show that it is possible to estimate ranges of popularity with
an overall accuracy of 84% considering only content fea-
tures. additionally, by comparing with an independent rat-
ing of news sources, we demonstrate that there exists a sharp
contrast between traditionally popular news sources and the
top news propagators on the social web.

in the next section we provide a survey of recent liter-
ature related to this work. section 3 describes the dataset
characteristics and the process of feature score assignment.
in section 4 we will present the results of prediction meth-
ods. finally, in section 5 we will conclude the paper and
discuss future possibilities for this research.

2 related work

stochastic models of information diffusion as well as deter-
ministic epidemic models have been studied extensively in
an array of papers, reaf   rming theories developed in sociol-
ogy such as diffusion of innovations (rogers 1995). among
these are models of viral marketing (leskovec, adamic,
and huberman 2007), models of attention on the web (wu
and huberman 2007), cascading behavior in propagation of
information (gruhl et al. 2004) (leskovec et al. 2007)
and models that describe heavy tails in human dynamics
(v  azquez et al. 2006). while some studies incorporate fac-
tors for content    tness into their model (simkin and roy-
chowdhury 2008), they only capture this in general terms
and do not include detailed consideration of content features.
salganik, dodds, and watts performed a controlled exper-
iment on music, comparing quality of songs versus the ef-
fects of social in   uence(salganik, dodds, and watts 2006).
they found that song quality did not play a role in popularity
of highly rated songs and it was social in   uence that shaped
the outcome. the effect of user in   uence on information
diffusion motivates another set of investigations (kempe,
kleinberg, and tardos 2003), (cosley et al. 2010),(agarwal
et al. 2008), (lerman and hogg 2010).

on the subject of news dissemination, (leskovec, back-
strom, and kleinberg 2009) and (yang and leskovec 2011)
study temporal aspects of spread of news memes online,
with (lerman and ghosh 2010) empirically studying spread
of news on the social networks of digg and twitter and (sun
et al. 2009) studying facebook news feeds.

a growing number of recent studies predict spread of in-
formation based on early measurements (using early votes
on digg, likes on facebook, click-throughs, and comments
on forums and sites). (szab  o and huberman 2010) found
that eventual popularity of items posted on youtube and digg
has a strong correlation with their early popularity; (lee,

moon, and salamatian 2010) and (tatar et al. 2011) predict
the popularity of a discussion thread using features based
on early measurements of user comments. (kim, kim, and
cho 2011) propose the notion of a virtual temperature of we-
blogs using early measurements. (lerman and hogg 2010)
predict digg counts using stochastic models that combine de-
sign elements of the site -that in turn lead to collective user
behavior- with information from early votes.

finally, recent work on variation in the spread of content
has been carried out by (romero, meeder, and kleinberg
2011) with a focus on categories of twitter hashtags (similar
to keywords). this work is aligned with ours in its atten-
tion to importance of content in variations among popular-
ity, however they consider categories only, with news being
one of the hashtag categories. (yu, chen, and kwok 2011)
conduct similar work on social marketing messages.

3 data and features

this section describes the data, the feature space, and feature
score assignment in detail.

3.1 dataset description
data was collected in two steps:    rst, a set of articles were
collected via a news feed aggregator, then the number of
times each article was linked to on twitter was found.
in
addition, for some of the feature scores, we used a 50-day
history of posts on twitter. the latter will be explained in
section 3.2 on feature scoring.

figure 1: log-log distribution of tweets.

online news feed aggregators are services that collect and
deliver news articles as they are published online. using the
api for a news feed aggregator named feedzilla, we col-
lected news feeds belonging to all news articles published
online during one week (august 8th to 16th, 2011). the
feed for an article includes a title, a short summary of the
article, its url, and a time-stamp. in addition, each article is
pre-tagged with a category either provided by the publisher
or in some manner determined by feedzilla. a fair amount
of cleaning was performed to remove redundancies, resolve
naming variations, and eliminate spam through the use of
automated methods as well as manual inspection. as a re-

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1510501005001000110100100010000figure 2: normalized t-density scores for categories

sult over 2000 out of a total of 44,000 items in the data were
discarded.

the next phase of data collection was performed using
topsy 3 , a twitter search engine that searches all mes-
sages posted on twitter. we queried for the number of times
each news link was posted or reshared on twitter (tweeted
or retweeted). earlier research (leskovec, backstrom, and
kleinberg 2009) on news meme buildup and decay suggest
that popular news threads take about 4 days until their pop-
ularity starts to plateau. therefore, we allowed 4 days for
each link to fully propagate before querying for the number
of times it has been shared.

the    rst half of the data was used in category score as-
signment (explained in the next section). the rest we parti-
tioned equally into 10,000 samples each for training and test
data for the classi   cation and regression algorithms. fig-
ure 1 shows the log distribution of total tweets over all data,
demonstrating a long tail shape which is in agreement with
other    ndings on distribution of twitter information cas-
cades (zhou et al. 2010). the graph also shows that articles
with zero tweets lie outside of the general linear trend of the
graph because they did not propagate on the twitter social
network.

our objective is to design features based on content to
predict the number of tweets for a given article. in the next
section we will describe these features and the methods used
to assign values or scores to features.

3.2 feature description and scoring
choice of features is motivated by the following questions:
does the category of news affect its popularity within a so-
cial network? do readers prefer factual statements or do
they favor personal tone and emotionally charged language?
does it make a difference whether famous names are men-
tioned in the article? does it make a difference who pub-
lishes a news article?

these questions motivate the choice of the following char-
acteristics of an article as the feature space:
the category
that the news belongs to (e.g. politics, sports, etc.), whether
the language of the text is objective or subjective, whether
(and what) named entities are mentioned, and what is the

source that published the news. these four features are cho-
sen based on their availability and relevance, and although
it is possible to add any other available features in a similar
manner, we believe the four features chosen in this paper to
be the most relevant.

we would like to point out that we use the terms article
and link interchangeably since each article is represented by
its url link.
category score news feeds provided by feedzilla are
pre-tagged with category labels describing the content. we
adopted these category labels and designed a score for them
which essentially represents a prior disribution on the popu-
larity of categories. figure 2 shows a plot of categories and
the number of article links in each category. we observe that
news related to technology has a more prominent presence
in our dataset and most probably on twitter as a whole. fur-
thermore, we can see categories (such as health) with low
number of published links but higher rates of tweet per link.
these categories perhaps have a niche following and loyal
readers who are intent on posting and retweeting its links.

observing the variations in average tweets per link from
figure 2 we use this quantity to represent the prior popu-
larity for a category. in order to assign a value (i.e. score)
to each category, we use the the    rst 22,000 points in the
dataset to compute the average tweet per article link in that
category. we call this average tweet per link the t-density
score and we will use this measure in score assignments for
some other features as well.
subjectivity another feature of an article that can affect
the amount of online sharing is its language. we want to
examine if an article written in a more emotional, more per-
sonal, and more subjective voice can resonate stronger with
the readers. accordingly, we design a binary feature for
subjectivity where we assign a zero or one value based on
whether the news article or commentary is written in a more
subjective voice, rather than using factual and objective lan-
guage. we make use of a subjectivity classi   er from ling-
pipe (alias-i. 2008) a natural language toolkit. since this
requires training data, we use transcripts from well-known
tv and radio shows belonging to rush limbaugh 4 and keith

3http://topsy.com

4http://www.rushlimbaugh.com

0  0.2  0.4  0.6  0.8  1  1.2  blogs  world  news  top  news  technology  business  sports  usa  industry  politics  entertainment  life  style  universities  jobs  science  music  celebrities  society  art  health  hobbies  video  games  video  events  fun  stuff  products  travel  shopping  religion  and  spirituality  programming  columnists  etc  number  of  links  t   density  olberman 5 as the corpus for subjective language. on the
other hand, transcripts from cspan 6 as well as the parsed
text of a number of articles from the website firstmonday 7
are used as the training corpus for objective language. the
above two training sets provide a very high training accuracy
of 99% and manual inspection of    nal results con   rmed that
the classi   cation was satisfactory. figure 3 illustrates the
distribution of average subjectivity per source, showing that
some sources consistently publish news in a more objective
language and a somwhat lower number in a more subjective
language.

different scores,    rst the aggregate number of times articles
from a source were shared, and second the t-density of each
source (average number of times each article belonging to a
source was shared). the latter proved to be a better score
assignment compared to the aggregate.

figure 4: distribution of log of source t-density scores

figure 3: distribution of average subjectivity of sources.

named entities
in this paper, a named entity refers to a
known place, person, or organization. intuition suggests that
mentioning well-known entities can affect the spread of an
article, increasing its chances of success. for instance, one
might expect articles on obama to achieve a larger spread
than those on a minor celebrity. and it has been well doc-
umented that fans are likely to share almost any content
on celebrities like justin bieber, oprah winfrey or ashton
kutcher. we made use of the stanford-ner 8 entity extrac-
tion tool to extract all the named entities present in the title
and summary of each article. we then assign scores to over
40,000 named entities by studying historical prominence of
each entity on twitter over the timeframe of a month. the
assigned score is the average t-density (tweet per link) of
each named entity. to assign a score for a given article we
use three different values: the number of named entities in
an article, the highest score among all the named entities in
an article, and the average score among the entities.
source score the data includes articles from 1350 unique
sources on the web. we assign scores to each source based
on the historical success of each source on twitter. for this
purpose, we collected the number of times articles from each
source were shared on twitter in the past. we used two

5http://www.msnbc.msn.com/id/32390086
6http://www.c-span.org
7http://   rstmonday.org
8http://nlp.stanford.edu/software/crf-ner.shtml

to investigate whether it is better to use a smaller por-
tion of more recent history, or a larger portion going back
farther in time and possibly collecting outdated information,
we start with the two most recent weeks prior to our data col-
lection and increase the number of days, going back in time.
figure 5 shows the trend of correlation between the t-density
of sources in historical data and their true t-density of our
dataset. we observe that the correlation increases with more
datapoints from the history until it begins to plateau near 50
days. using this result, we take 54 days of history prior to
the    rst date in our dataset. we    nd that the correlation of the
assigned score found in the above manner has a correlation
of 0.7 with the t-density of the dataset. meanwhile, the cor-
relation between the source score and number of tweets of
any given article is 0.35, suggesting that information about
the source of publication alone is not suf   cient in predicting
popularity. figure 4 shows the distribution of log of source
scores (t-density). taking the log of source scores produces
a more normal shape, leading to improvements in regression
algorithms.

we plot the timeline of t-densities for a few sources and
   nd that t-density of a source can vary greatly over time.
figure 6 shows the t-density values belonging to the technol-
ogy blog mashable and blog maverick, a weblog of promi-
nent entrepreneur, mark cuban. the t-density scores cor-
responding to each of these sources are 74 and 178 respec-
tively. however, one can see that mashable has a more con-
sistent t-density compared to blog maverick.

in order to improve the score to re   ect consistency we
devise two methods; the    rst method is to smooth the mea-
surements for each source by passing them through a low-
pass    lter. second is to weight the score by the percent-
age of times a source   s t-density is above the mean t-density
over all sources, penalizing sources that drop low too often.
the mean value of t-densities over all sources is 6.4. fig-

histogram of avgsubjectivityavgsubjectivityfrequency0.00.20.40.60.81.0050100150200histogram of log(t_density)log(t_density)frequency012345020406080100120140figure 5: correlation trend of source scores with t-density
in data. correlation increases with more days of historical
data until it plateaus after 50 days.

(a) tweets and links

(b) t-density

figure 7: temporal variations of tweets, links, and t-density
over all sources

figure 6: timeline of t-density (tweet per link) of two
sources.

correlation

total links total tweets
0.57

0.35

t-density
-0.05

ure 7 shows the temporal variations of tweets and links over
all sources. notice that while both tweets and links have a
weekly cycle, the t-density (tweets over links) does not have
this periodic nature.
are top traditional news sources the most propagated?
as we assign scores to sources in our dataset, we are inter-
ested to know whether sources successful in this dataset are
those that are conventionally considered prominent.

google news 9 is one of the major aggregators and
providers of news on the web. while inclusion in google
news results is free, google uses its own criteria to rank
the content and place some articles on its homepage, giv-
ing them more exposure. freshness, diversity, and rich tex-
tual content are listed as the factors used by google news to
automatically rank each article as it is published. because
google does not provide overall rankings for news sources,
to get a rating of sources we use newsknife 10. newsknife
is a service that rates top news sites and journalists based
on analysis of article   s positions on the google news home-
page and sub-pages internationally. we would like to know
whether the sources that are featured more often on google
news (and thus deemed more prominent by google and rated
more highy by newsknife) are also those that become most
popular on our dataset.

accordingly we measure the correlation values for the 90

9http://news.google.com/
10http://www.newsknife.com

table 1: correlation values between newsknife source
scores and their performance on twitter dataset.

top newsknife sources that are also present in our dataset.
the values are shown in table 1. it can be observed that the
ratings correlate positively with the number of links pub-
lished by a source (and thus the sum of their tweets), but
have no correlation (-0.05) with t-density which re   ects the
number of tweets that each of their links receives. for our
source scoring scheme this correlation was about 0.7.

table 2 shows a list of

top sources according to
newsknife, as well as those most popular sources in our
dataset. while newsknife rates more traditionally promi-
nent news agencies such as reuters and the wall street jour-
nal higher, in our dataset the top ten sources (with high-
est t-densities) include sites such as mashable, allfacebook
(the unof   cial facebook blog), the google blog, marketing
blogs, as well as weblogs of well-known people such as
seth godin   s weblog and mark cuban   s blog (blogmaver-
ick). it is also worth noting that there is a bias toward news
and opinion on web marketing, indicating that these sites ac-
tively use their own techniques to increase their visibility on
twitter.

while traditional sources publish many articles, those
more successful on the social web garner more tweets. a
comparison shows that a newsknife top source such as the
christian science monitor received an average of 16 tweets
in our dataset with several of its articles not getting any

0.62  0.64  0.66  0.68  0.7  0.72  10  20  30  40  50  60  correla   on  time  (days)  0  100  200  300  400  500  600  1  11  21  31  41  51  61  71  81  t   density  3me  (days)  blog  maverick  mashable  k  400k  800k  1200k  1600k  2000k  1  8  15  22  29  36  43  50  ,me  (days)  tweets  links  4  5  6  7  8  1  8  15  22  29  36  43  50  t   density  4me  (days)  tweets. on the other hand, mashable gained an average of
nearly 1000 tweets with its least popular article still receiv-
ing 360 tweets. highly ranked news blogs such as the huff-
ington post perform relatively well in twitter, possibly due
to their active twitter accounts which share any article pub-
lished on the site.

all data
tech category
within twitter

id75
0.34
0.43
0.33

id166 regression
0.32
0.36
0.25

table 4: regression results

newsknife

twitter dataset

reuters, los angeles times, new york
times, wall street journal, usa to-
day, washington post, abc news,
bloomberg, christian science monitor,
bbc news
blog maverick, search engine land,
duct-tape marketing,
seth   s blog,
google blog, allfacebook, mashable,
search engine watch

table 2: highly rated sources on newsknife versus those
popular on the twitter dataset

4 prediction

in this work, we evaluate the performance of both regression
and classi   cation methods to this problem. first, we apply
regression to produce exact values of tweet counts, evalu-
ating the results by the r-squared measure. next we de   ne
popularity classes and predict which class a given article will
belong to. the following two sections describe these meth-
ods and their results.

variable
s
c
subj
entct
entmax
entavg

description
source t-density score
category t-density score
subjectivity (0 or 1)
number of named entities
highest score among named entities
average score of named entities

table 3: feature set (prediction inputs)

4.1 regression
once score assignment is complete, each point in the data
(i.e. a given news article) will correspond to a point in the
feature space de   ned by its category, subjectivity, named en-
tity, and source scores. as described in the previous section,
category, source, and named entity scores take real values
while the subjectivity score takes a binary value of 0 or 1.
table 3 lists the features used as inputs of regression algo-
rithms. we apply three different regression algorithms - lin-
ear regression, k-nearest neighbors (knn) regression and
support vector machine (id166) regression.

since the number of tweets per article has a long-tail
distribution (as discussed previously in figure 1), we per-
formed a logarithmic transformation on the number of
tweets prior to carrying out the regression. we also used the
log of source and category scores to normalize these scores

further. based on this transformation, we reached the fol-
lowing relationship between the    nal number of tweets and
feature scores.

ln(t ) = 1.24ln(s) + 0.45ln(c) + 0.1entmax     3

where s is the source t-density score, c is the category t-
density score, and entmax is the maximum t-density of all
entities found in the article. equivalently,

t = s1.24c 0.45e   (0.1entmax+3)

with coef   cient of determination r2 = 0.258. note that the
r2 is the coef   cient of determination and relates to the mean
squared error and variance:

r2 = 1     m se
v ar

alternatively, the following model provided improved re-

sults:
t 0.45 = (0.2s     0.1entct     0.1entavg + 0.2entmax)2
with an improved r2 = 0.34. using support vector machine
(id166) regression (chang and lin 2011), we reached similar
values for r2 as listed in table 4.

in k-nearest neighbor regression, we predict the tweets
of a given article using values from it   s nearest neighbors.
we measure the euclidean distance between two articles
based on their position in the feature space (hastie, tibshi-
rani, and friedman 2008). parameter k speci   es the num-
ber of nearest neighbors to be considered for a given article.
results with k = 7 and k = 3 for a 10k test set are r-
sq= 0.05, with mean squared error of 5101.695. we observe
that knn performs increasingly more poorly as the dataset
becomes larger.

category-speci   c prediction one of the weakest predic-
tors in regression was the category score. one of the reasons
for this is that there seems to be a lot of overlap across cat-
egories. for example, one would expect world news and
top news to have some overlap, or the category usa would
feature articles that overlap with others as well. so the cate-
gories provided by feedzilla are not necessarily disjoint and
this is the reason we observe a low prediction accuracy.

to evaluate this hypothesis, we repeated the prediction
algorithm for particular categories of content. using only
the articles in the technology category, we reached an r2
value of 0.43, indicating that when employing regression we
can predict the popularity of articles within one category (i.e.
technology) with better results.

4.2 classi   cation
feature scores derived from historical data on twitter are
based on articles that have been tweeted and not those arti-
cles which do not make it to twitter. as discussed in sec-
tion 3.1 this is evident in how the zero-tweet articles do not
follow the linear trend of the rest of datapoints in figure 1.
consequently, we do not include a zero-tweet class in our
classi   cation scheme and perform the classi   cation by only
considering those articles that were posted on twitter.

table 5 shows three popularity classes a (1 to 20 tweets),
b (20 to 100 tweets), c (more than 100) and the number of
articles in each class in the set of 10,000 articles. table 6
lists the results of support vector machine (id166) classi   ca-
tion, decision tree, and id112 (hall et al. 2009) for classi-
fying the articles. all methods were performed with 10-fold
cross-validation. we can see that classi   cation can perform
with an overall accuracy of 84% in determining whether an
article will belong to a low-tweet, medium-tweet, or high-
tweet class.

in order to determine which features play a more signi   -
cant role in prediction, we repeat id166 classi   cation leaving
one of the features out at each step. we found that publica-
tion source plays a more important role compared to other
predictors, while subjectivity, categories, and named enti-
ties do not provide much improvement in prediction of news
popularity on twitter.
predicting zero-tweet articles we perform binary clas-
si   cation to predict which articles will be at all mentioned
on twitter (zero tweet versus nonzero tweet articles). us-
ing id166 classi   cation we can predict    with 66% accuracy   
whether an article will be linked to on twitter or whether it
will receive zero tweets. we repeat this operation by leav-
ing out one feature at a time to see a change in accuracy.
we    nd that the most signi   cant feature is the source, fol-
lowed by its category. named entities and subjectivity did
not provide more information for this prediction. so despite
one might expect, we    nd that readers overall favor neither
subjectivity nor objectivity of language in a news article.

it is interesting to note that while category score does not
contribute in prediction of popularity within twitter, it does
help us determine whether an article will be at all mentioned
on this social network or not. this could be due to a large
bias toward sharing technology-related articles on twitter.

class name range of tweets number of articles
a
b
c

1   20
20   100
100   2400

7,600
1,800
600

table 5: article classes

5 discussion and conclusion

in this work we predicted the popularity of news items on
twitter using features extracted from the content of news ar-
ticles. we have taken into account four features that cover
the spectrum of the information that can be gleaned from the

method
id112
j48 id90
id166
naive bayes

accuracy
83.96%
83.75%
81.54%
77.79%

table 6: classi   cation results

content - the source of the article, the category, subjectivity
in the language and the named entities mentioned. our re-
sults show that while these features may not be suf   cient to
predict the exact number of tweets that an article will garner,
they can be effective in providing a range of popularity for
the article on twitter. we achieved an overall accuracy of
84% using classi   ers. it is important to bear in mind that
while it is intriguing to pay attention to the most popular ar-
ticles    those that become viral on the web    a great number
of articles spread in medium numbers. these medium levels
can target highly interested and informed readers and thus
the mid-ranges of popularity should not be dismissed.

interestingly we have found that in terms of number of
retweets, the top news sources on twitter are not necessarily
the conventionally popular news agencies and various tech-
nology blogs such as mashable and the google blog are very
widely shared in social media. overall, we discovered that
one of the most important predictors of popularity was the
source of the article. this is in agreement with the intuition
that readers are likely to be in   uenced by the news source
that disseminates the article. on the other hand, the cate-
gory feature did not perform well. one reason for this is that
we are relying on categories provided by feedzilla, many of
which overlap in content. thus a future task is to extract
categories independently and ensure little overlap. combin-
ing other layers of complexity described in the introduction
opens up the possibility of better prediction. it would be in-
teresting to incorporate network factors such as the in   uence
of individual propagators to this work.

references

[agarwal et al. 2008] agarwal, n.; liu, h.; tang, l.; and
yu, p. s. 2008.
identifying the in   uential bloggers in a
community. in proceedings of the international conference
on web search and web data mining, wsdm    08, 207   218.
new york, ny, usa: acm.
[alias-i. 2008] alias-i. 2008. lingpipe 4.1.0. http://alias-
i.com/lingpipe.
[chang and lin 2011] chang, c.-c.,
2011.
chines.
and technology 2:27:1   27:27.
http://www.csie.ntu.edu.tw/   cjlin/libid166.
[cosley et al. 2010] cosley, d.; huttenlocher, d.; kleinberg,
j.; lan, x.; and suri, s. 2010. sequential in   uence mod-
els in social networks. in 4th international conference on
weblogs and social media.
[gruhl et al. 2004] gruhl, d.; liben-nowell, d.; guha,

and lin, c.-j.
libid166: a library for support vector ma-
acm transactions on intelligent systems
software available at

[sun et al. 2009] sun, e.; rosenn, i.; marlow, c.; and lento,
t. m. 2009. gesundheit! modeling contagion through face-
book news feed. in icwsm. the aaai press.
[szab  o and huberman 2010] szab  o, g.,
and huberman,
b. a. 2010. predicting the popularity of online content.
commun. acm 53(8):80   88.
[tatar et al. 2011] tatar, a.; leguay, j.; antoniadis, p.; lim-
bourg, a.; de amorim, m. d.; and fdida, s. 2011. pre-
dicting the popularity of online articles based on user com-
ments. in proceedings of the international conference on
web intelligence, mining and semantics, wims    11, 67:1   
67:8. new york, ny, usa: acm.
[v  azquez et al. 2006] v  azquez, a.; oliveira, j. a. g.; dezs  o,
z.; goh, k.-i.; kondor, i.; and barab  asi, a.-l. 2006. mod-
eling bursts and heavy tails in human dynamics. phys. rev.
e 73:036127.
[wu and huberman 2007] wu, f., and huberman, b. a.
2007. novelty and collective attention. proceedings of the
national academy of sciences 104(45):17599   17601.
[yang and leskovec 2011] yang, j., and leskovec, j. 2011.
patterns of temporal variation in online media. in wsdm,
177   186. acm.
[yu, chen, and kwok 2011] yu, b.; chen, m.; and kwok, l.
2011. toward predicting popularity of social marketing mes-
sages. in sbp, volume 6589 of lecture notes in computer
science, 317   324. springer.
[zhou et al. 2010] zhou, z.; bandari, r.; kong, j.; qian, h.;
and roychowdhury, v. 2010.
information resonance on
twitter: watching iran. in proceedings of the first workshop
on social media analytics, soma    10, 123   131. new york,
ny, usa: acm.

r. v.; and tomkins, a. 2004. information diffusion through
blogspace. sigkdd explorations 6(2):43   52.
[hall et al. 2009] hall, m.;
frank, e.; holmes, g.;
pfahringer, b.; reutemann, p.; and witten, i. h. 2009. the
weka data mining software: an update. sigkdd explor.
newsl. 11:10   18.
[hastie, tibshirani, and friedman 2008] hastie, t.; tibshi-
rani, r.; and friedman, j. 2008. the elements of statistical
learning: data mining, id136, and prediction. springer
series in statistics. springer.
[kempe, kleinberg, and tardos 2003] kempe, d.; klein-
berg, j. m.; and tardos,   e. 2003. maximizing the spread
in kdd, 137   146.
of in   uence through a social network.
acm.
[kim, kim, and cho 2011] kim, s.-d.; kim, s.-h.; and
cho, h.-g. 2011. predicting the virtual temperature of web-
blog articles as a measurement tool for online popularity. in
ieee 11th international conference on computer and in-
formation technology (cit), 449    454.
[lee, moon, and salamatian 2010] lee, j. g.; moon, s.; and
salamatian, k. 2010. an approach to model and predict the
popularity of online contents with explanatory factors.
in
web intelligence, 623   630. ieee.
[lerman and ghosh 2010] lerman, k., and ghosh, r. 2010.
information contagion: an empirical study of the spread of
news on digg and twitter social networks. in icwsm. the
aaai press.
[lerman and hogg 2010] lerman, k., and hogg, t. 2010.
using a model of social dynamics to predict popularity of
news. in www, 621   630. acm.
[leskovec, adamic, and huberman 2007] leskovec,
j.;
adamic, l. a.; and huberman, b. a. 2007. the dynamics
of viral marketing. tweb 1(1).
[leskovec, backstrom, and kleinberg 2009] leskovec,
j.;
backstrom, l.; and kleinberg, j. m. 2009. meme-tracking
in kdd, 497   506.
and the dynamics of the news cycle.
acm.
[leskovec et al. 2007] leskovec, j.; mcglohon, m.; falout-
sos, c.; glance, n.; and hurst, m. 2007. cascading behavior
in large blog graphs. in in sdm.
[rogers 1995] rogers, e. 1995. diffusion of innovations.
free pr.
[romero, meeder, and kleinberg 2011] romero, d. m.;
meeder, b.; and kleinberg, j. 2011. differences in the
mechanics of information diffusion across topics:
idioms,
political hashtags, and complex contagion on twitter.
in
proceedings of the 20th international conference on world
wide web, www    11, 695   704. new york, ny, usa:
acm.
[salganik, dodds, and watts 2006] salganik, m. j.; dodds,
p. s.; and watts, d. j. 2006. experimental study of in-
equality and unpredictability in an arti   cial cultural market.
science 311(5762):854   856.
[simkin and roychowdhury 2008] simkin, m. v., and roy-
chowdhury, v. p. 2008. a theory of web traf   c. epl (euro-
physics letters) 82(2):28006.

