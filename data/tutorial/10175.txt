multidimensional counting grids:

inferring word order from disordered bags of words

nebojsa jojic

microsoft research

redmond, wa

abstract

models of bags of words typically assume
topic mixing so that the words in a single bag
come from a limited number of topics. we
show here that many sets of bag of words ex-
hibit a very di   erent pattern of variation than
the patterns that are e   ciently captured by
topic mixing. in many cases, from one bag
of words to the next, the words disappear
and new ones appear as if the theme slowly
and smoothly shifted across documents (pro-
viding that the documents are somehow or-
dered). examples of latent structure that de-
scribe such ordering are easily imagined. for
example, the advancement of the date of the
news stories is re   ected in a smooth change
over the theme of the day as certain evolving
news stories fall out of favor and new events
create new stories. overlaps among the sto-
ries of consecutive days can be modeled by
using windows over linearly arranged tight
distributions over words. we show here that
such strategy can be extended to multiple
dimensions and cases where the ordering of
data is not readily obvious. we demonstrate
that this way of modeling covariation in word
occurrences outperforms standard topic mod-
els in classi   cation and prediction tasks in ap-
plications in biology, text modeling and com-
puter vision.

1 introduction

in machine learning research, data samples are often
represented as bags of words without particular order.
this choice is often motivated by the di   culty or com-
putational e   ciency of modeling the known structure
of the data, e.g., language. a striking example is the
current id161 research, where spatial struc-

alessandro perina
microsoft research

redmond, wa

figure 1: usage counts for    ve terms in id98 news
blog over the    rst 18 days of march

ture of visual features is largely discarded by most ob-
ject recognition algorithms. there are also examples
of data where the structure is truly unknown. a gene
expression array can be modeled as a bag of genes
with expression levels simply corresponding to counts,
because most of the time little is known about the cel-
lular pathways that employ these genes. without such
knowledge there is no clear gene ordering. but biology
is also abundant with situations where the raw data
of interest truly has no structure. for example, the
mammalian immune systems sees the virus inside the
cell not as a whole but as a set of disordered peptides
sampled inside from the viral proteins and presented
on cellular surface for immune surveillance.

id91 and id84 techniques
such as id45 (lsi, (deerwester
et al., 1990)) and id44 (lda,
(blei et al., 2003)) are among the most popular ap-
proaches to modeling disordered bags of words. in case
of subspace models such as lda, each bag of words is
modeled as a mixture of topics, and each topic is rep-
resented by a tight distribution over words.
in this

020406080123456789101112131415161718marchipadsheenlibyaearthquakenuclearpaper, we point out that much of the variability in
many interesting datasets is better modeled in terms
of multidimensional thematic shifts, rather than out-
right mixing. in our model, certain words/features are
dropped and others added as a consequence of move-
ment in some hypothetical space. while our goal is
to infer the properties of this space and the embed-
ding of the data into it, it is useful to    rst consider an
example of the data where such spatial embedding is
directly available. figure 1 shows the count of    ve dif-
ferent words in news stories in id98   s news blog1 for
the    rst 18 days of march 2011. a text document is
often modeled as a bag of its words, and if the id98   s
daily news blogs were to be modeled that way, then
the highlighted words would participate in the bags
with frequencies that indicate thematic shifts induced
by the timeline of key events. the beginning of the
month saw a spike in news on the launch of apple   s
ipad 2, but words sheen, and libya, are also abun-
dant. a week later, it seems that the interest of the
american public quickly shifted to the events in the
professional and personal life of actor charlie sheen,
overshadowing a steady trickle of news on the upris-
ing in the middle east. then the catastrophic natural
events in japan caused a large spike of the usage of the
word earthquake, followed a few days later by a signi   -
cant increase in the usage of the word nuclear, re   ect-
ing the problems in the nuclear plants that started to
unfold as consequence of the earthquake and the ensu-
ing tsunami. with the expectation of the un resolu-
tion on libya, this word regains in its dominance to-
wards the end of this period. from a signal processing
perspective, the distribution of the word counts across
the timeline seems to be caused by a few point sources,
with this excitation then going through an averaging
   lter. news people are used to the thematic shifts of
this nature, where each new story enjoys some limited
life time and is then suppressed by other more inter-
esting topics. from the machine learning perspective,
this situation can be well modeled by creating a series
of relatively tight word distributions (corresponding
to point sources on the timeline), and then combining
several consecutive distributions to form the expected
histogram of words for any given day. the thematic
shifts over the days are then simply modeled by mov-
ing the averaging window across the timeline of these
point sources. thus even though each blog is consid-
ered a disordered bag of words, a good model would in
fact order them along a line to induce constraints on
the word mixing that gives rise to the observed bags
of words.

however, even in case of data where the additional
metadata that would provide such ordering (date in

1http://news.blogs.id98.com/

figure 2: an example of a counting grid geometry. in
general, the data is embedded into a hypercube which
is wrapped around along each dimension to avoid local
minima that would be caused by abrupt cuts along any
dimension.

this case) does not exist, we can see thematic shifts.

in this paper we provide a simple model for data
in which much of the variation is induced by the-
matic shifts expressed as movement through an in-
ferred space. as in the news example, we assume that
there is some space into which a set of tight distri-
butions is embedded, and that these distributions are
then combined using a windowing operation to create
a resultant distribution from which the observed bags
of words or features are generated. however, we do not
assume that the mapping is given a priori. for sim-
plicity, we assume that the space is a discrete grid of
counts, but of arbitrary dimension (we experimented
with 2- and 3-dimensional grids) and we consider itera-
tive estimation of counts on this grid and the mapping
of the data to the overlapping windows on it. our
experiments indicate that the thematic shifts are in-
deed present in a variety of datasets, and as a result,
our model outperforms standard topic mixing (lda)
there. we analyzed a wide variety of data types, in-
cluding text, images, gene expression and viral pep-
tides, and used the learned counting grids to perform
regression or classi   cation.

2 the counting grid model

formally, the basic counting grid   i,z is a set of nor-
malized counts of words/features indexed by z on the
d-dimensional discrete grid indexed by i = (i1, . . . , id)
where each id     [1 . . . ed] and e = (e1, . . . , ed) de-
scribes the extent of the counting grid. since    is a
z   i,z = 1 everywhere on the
grid. a given bag of words/features, represented by

grid of distributions, (cid:80)

 e1 e2 e3 w k1 k2 k3 w2 w3 w1= [e1,e2,e3] = [w1,w2,w3]     =  e1e2 e3w1 w2 w3    ,z ....  =[0,0,0]  =[1,0,0]  =[1,1,1]  =[0,1,1]++i3 i1 i2counts {cz} is assumed to follow a count distribution
found somewhere in the counting grid. in particular,
using windows of dimensions w = [w1, . . . , wd], each
bag can be generated by    rst averaging all counts in
the hypercube window wk = [k . . . k + w] starting at
d-dimensional grid location k and extending in each
direction d by wd grid positions to form the histogram
  i,z, and then generating a set of

hk,z = 1(cid:81)

(cid:80)

i   wk

features in the bag. in other words, the position of the
window k in the grid is a latent variable given which
the id203 of the bag of features {cz} is
p({cz}|k) =

(cid:0) (cid:88)

(cid:1)cz ,

(hk,z)cz =

(cid:89)

(cid:89)

  i,z

1(cid:81)

wd

d

z

d wd

z

i   wk

(1)
relaxing the terminology, we will refer to e and w
respectively as the counting grid and the window size.
we will also often refer to the ratio of the window
volumes,   , as a capacity of the model in terms of
an equivalent number of topics, as this is how many
nonoverlapping windows can be    t onto the grid.fine
variation achievable by moving the windows in be-
tween any two close by but nonoverlapping windows
is useful if we expect such smooth thematic shifts to
occur in the data, and we illustrate in our experiments
that indeed it does. finally, with wk we indicate the
particular window placed at location k (see fig. 2).

2.1

id136 and learning

to compute the log likelihood of the data, log p , we
need to sum over the latent variables k before comput-
ing the logarithm, which, as in mixture models, or as
in epitomes (jojic et al., 2003), which are much more
similar to the counting grids, makes it di   cult to per-
form assignment of the latent variables (in our case
positions in the counting grid) while also estimating
the model parameters. this makes an iterative exact
or a variational em algorithm necessary. bounding
(variationally), the non-constant part of log p , we get

log p     b =    (cid:88)
(cid:88)

t

(cid:88)
(cid:88)

kt

+

qkt log qkt +

(2)

(cid:88)

(cid:88)

i   wkt

qkt

ct
z log

  i,z,

t

kt

z

where qkt, or in shorthand, qt
k is the variational distri-
bution over the latent mapping onto the counting grid
of the t-th bag. each of these variational distributions
can be varied to maximize the bound. in fact, for a
given counting grid   , the bound is maximized when
each distribution qt is equal to the exact posterior dis-
tribution. this is a standard variational derivation of
the exact e step, which leads to

i     exp
qt

z    log hi,z,
ct

(3)

(cid:88)

z

which simply establishes that the choice of k should
minimize the kl divergence between the counts in the

bag and the counts hk,z = (cid:80)   i,z in the appropri-

ate window wk in the counting grid. for each t, the
above expression is normalized over all possible win-
dow choices k.

log

(cid:88)

    (cid:88)

to optimize the bound b with respect to parameters
we note    rst that it is the second term in eq. 1 that
involves these parameters, and that it requires another
summation before applying the logarithm. the sum-
mation is over the grid positions i within the window
wk, which we can again bound using a variational dis-
tribution and the jensen   s inequality:

(cid:88)
positive and(cid:80)

where rt

i   wk

i   wkt

i   wkt

  i,z = log

rt
i,k,z log

i,k,z      i,z
rt
rt
i   wkt
i,k,z

  i,z
rt
i,k,z
(4)
i,k,z is a distribution over locations i, i.e. r is
rt
i,k,z = 1 and is indexed by k as
the id172 is done di   erently in each window,
and is indexed by z as it can be di   erent for di   erent
features, and indexed by t as the term is inside the
summation over t, so a di   erent distribution r could
z}. this distribution
be needed for di   erent bags {ct
could be thought of as information about what pro-
portion of these cz features of type z was contributed
by each of the di   erent sources   i,z in the window wk.
however, by performing constrained optimization (so
that r adds up to one), we    nd that assuming a    xed
set of parameters   , the distribution rt
i,k,z that maxi-
mizes the bound is independent of t, i.e., the same for
each bag:

  i,z(cid:80)

i   wk

=

  i,z

(cid:81)
d wd    hk,z

  i,z

rt
i,k,z =

.

(5)

if we do consider distributions r as a feature mapping
to the counting grid, then this result is again intuitive.
if all we know is that a bag containing cz features of
type z is mapped to the grid section wk, and have
no additional information about what proportions of
these cz features were contributed from di   erent incre-
mental counts   i,z, then the best guess is that these
proportions follow the proportions among   i,z inside
the window. if we assume now that r and q distribu-
tions are    xed, then combining eqs. 3,4 and minimiz-
ing the resulting bound wrt parameters   i,z under the
id172 constraint over features z, we obtain the
update rule,

    i,z    (cid:88)
(cid:88)
    i,z       i,z   (cid:88)

t

k|i   wk

ct
z

i,k,z

k    ct
z    rt
qt
(cid:88)

qt
k
hk,z

t

k|i   wk

which by eq. 5 reduces to

(6)

(7)

the steps in eqs. 3 and 7 constitute the e and m
step which can be iterated till convergence (within a
desired precision). the    rst step aligns all bags of
features to grid windows that (re)match the bags    his-
tograms, and the second re-estimates the counting grid
so that these same histogram matches are even bet-
ter. thus, starting with non-informative (but symme-
try breaking) initialization, this iterative process will
jointly estimate the counting grid and align all bags to
it. to avoid severe local minima, it is important, how-
ever, to consider the counting grid as a torus, and
consider all windowing operations accordingly, as was
previously proposed for learning epitomes (jojic et al.,
2003, 2010), a model that quilts spatially-organized
images and videos. this prevents the problems with
grid boundaries which otherwise could not be crossed
when more space is needed to grow the layout of the
features.

2.2 computational e   ciency

i   wk

require computing (cid:80)

careful examination of the steps reveals that by the ef-
   cient use of cumulative sums,both the e and m steps
are linear in the size of the counting grid. both steps
fi, which can be done by
   rst computing, in linear time the cumulative sums
of f and then computing appropriate linear combina-
tions.
(cid:80)
(i,j)   (m,n) fi,j, and then set (cid:80)
for example, in the 2d case we have i = (i, j), k =
(k, (cid:96)) and one can compute the cumulative sum fm,n =
fi,j =
fk+w1+1,(cid:96)+w2+1     fk,(cid:96)+w2+1     fk+w1+1,(cid:96) + fk,(cid:96).
this can be generalized by associating to each vertex
v of the hypercube wk (2d vertices in total) a binary
vector   v. di   erent vertices of wk share various co-
ordinates, as along a dimension, say d, a vertex v can
only assume two values ( kd or kd + wd). we de   ne
elements of vector   v as follows

(i,j)   wk,(cid:96)

(cid:26) 1 if vd = kd + wd

  v
d =

0 else

the value of    for some vertex is shown in fig.2.
given this we can write

(cid:88)

i   wk

2d(cid:88)

v=1

fi =

(   1)

|1     v|    fi+  v   w

(8)

simply computing the m-step (eq. 7) using the target
label in place of counts (ct

k    [yt = l]
qt

(cid:80)

k|i   wk

t

z):

(cid:80)
(cid:80)
(cid:80)
(cid:80)
(cid:80)
(cid:80)
(cid:80)

t

t

t
k|i   wk

qt
k|i   wk
k
k    yt
qt
qt
k|i   wk
k

.

(9)

(10)

  (i, l) =

or   (i) =

where (9) is used for discrete, and (10) for the contin-
uous labels.

the label embedding    can then be used for classi   -
cation or regression, in what is essentially a nearest-
neighbor strategy: when a new data point is embed-
ded based on its bag of words, the label is simply read
out from   , which is dominated by the training points
which were mapped in the same region. other options
to using the cgs for classi   cation/regression are of
course available, but we opted for this simplest one in
our experiments, for the reasons we discuss next.

3 experiments

as we are mostly interested in the quality of unsu-
pervised learning of the distribution over the bags of
words, in the majority of the experiments we com-
pared lda with cgs in the following setting. each
data sample consists of a bag of words and a label.
the bags were used without labels to train a cg and
lda models that capture covariation in word occur-
rences, with cgs mostly modeling thematic shifts, and
lda modeling topic mixing. then, the label predic-
tion task is performed in a leave-one-out setting, where
each data point is taken out of the set used to estimate
the label embedding    of the rest of the data, and then
the left out sample   s mapping is used to read out the   
in the appropriate location (or if the posterior is uncer-
tain, to average out the embedded labels accordingly).
lda is used in a similar fashion: the most similarly
mapped points are used to predict the label for the left
out sample. for lda we have also investigated linear
regression based on the latent mapping. finally, a va-
riety of other methods are occasionally compared to,
and slightly di   erent evaluation methods described in
individual subsections, when appropriate.

where the     is the pointwise multiplication.

3.1 text: cmu 20 newsgroup

2.3 label embedding

once a cg is learned, one may embed other discrete
(e.g., class labels yt = l,
l = 1...l ) or continuous
(e.g., hiv yt) values on the grid. this is achieved us-
ing the posterior probabilities qt
k already inferred and

in the cmu newsgroup dataset2, each news post is
treated as a document (a bag of words) labeled by
one of 20 labels representing the news group of its
origin (as opposed to our introductory toy example,

2http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-

20/www/data/news20.html

figure 3: news-20 classi   cation results. bow stands for bag-of-words, movbf is the mixture of von-mises fisher
(mardia and jupp, 2000). the asterisk *, indicates a method that uses the original feature set. we describe a
particular cg by specifying e and w in the form e/w. if only one value is reported for e or w, then the cg
has the same size in all the dimensions (hypercube). b) embedding of the three class labels in 1d 2d and 3d
cgs . ( the accuracy for 1d cgs does not exceed 60%). c) embedding of the three class labels of the easiest
subset of the 20 newsgroup dataset: news-20-di   erent, with posts from rec.sport.baseball, sci.space and
alt.atheism (see (banerjee and basu, 2007) for details).

the date of the post was not available). follow-
ing previous work (banerjee and basu, 2007) we re-
duced the dataset into subsets with varying similar-
ities among the news groups. we consider and re-
port here the results only for the more challenging,
news-20-same, with 1700 posts from the highly re-
lated groups comp.os.ms-windows, comp.windows.x
and comp.graphics. we    rst compared the embed-
ding of documents from all three classes provided by
cgs (a torus with overlapping windows) and lda
(a simplex of topic proportions) based respectively
on label embedding function   , and on the perfor-
mance of the k-nearest neighbor classi   er (k=3) us-
ing the topic proportions    of lda employing the
kl divergence to evaluate distances. as in (reisinger
et al., 2010), comparisons with lda were performed
in 10    10-fold crossvalidation. for both methods in
each training/split test, we reduced the vocabulary
to 2000 words using the feature reduction method of
(peng et al., 2005) on the training data to save on
computation time. we have also evaluated the use of
simple bag of words comparisons in knn setting for
classi   cation. figure 3a summarizes the cg results:
we learned several 2d and 3d grids, varying e, w
and   . we repeated the tests trying to take the same
  for the 2d and 3d cases to evaluate how document

classes embed in spaces of di   erent dimensions3.
in
all the tests, 3d counting grids have been found to
perform slightly better suggesting that more than 2
dimensions are needed to embed such complex classes.
for sake of comparison, we also added recent results
from (reisinger et al., 2010), including their test of
vonmf model and their best result for their own new
model, spherical admixture model (sam), which mod-
els topics using vonmf distribution and tf-idf features
as input. cg outperforms other models (and with an
even larger margin if sam uses the simple count (tf)
features, as cgs did in our experiments rather than
the more complex derived features as in the citation;
for example, cgs reduce sam   s classi   cation error in
this setting by over 30%).

to illustrate the label embedding    for the training
documents in the learned counting grids, we show
where the image labels for the three classes in each
dataset ended up in one of the tests: the cg is col-
ored red green and blue based on the fractions of the
documents from each of the three classes mapped to
the cg locations (fig. 3b ). the 3d grids are rendered
semi-transparent. the    gure demonstrates a complex
structure with lots of transitions among classes in 2d.

3for example, the 2d grid e = [40,40], w = [4,4], has
roughly the same    of the 3d grid of size e=[10,10,10],
w=[3,3,2]

a)55657585cg 2d(40/6)cg 2d(40/4)cg 2d(40/2)cg 3d(10/3,3,2)cg 3d(13/4,3,2)cg 3d(16/4,3,2)cg 3d(40,40,2/4,4,2)cg 3d(40,20,4/4,4,2)4010016040100160100100ldabow*bowmovmf*sam*accuracy (%)cg ( w / e )  counting grid resultslda + other methods resultsb)2d cg3d cg1d cgc)3d cg1d cg2d cgfor comparison, in (fig. 3c), we show the embedding
for an easier subset of the newsgroup dataset where
the blobs of data points of the same class are larger
and better separated. the best capacity was found to
be        100 for both 2 and 3 dimensions. interestingly,
keeping the same ratio, but varying the dimensions of
the window we    nd that small windows (i.e., size 2  2)
provides gradation of window overlap that is too coarse
(either half the window, or full overlap, or no overlap),
while increasing the size of the window beyond 4 along
each dimension to allow for more re   ned overlaps, did
not increase the performance. it is interesting to see
that the model does not easily over train. although
for example the 40    40 counting grid consists of 1600
independent word distributions   i,z, these positive pa-
rameters are summed up in large groups to represent
the data and these groups (windows) have a large over-
lap. and so, while 100 independent nonoverlapping
windows for    = 100 can be designated on the grid,
cramming in more components expressible as sums of
the subsets of the same set of positive numbers is hard
unless some real structure in data is discovered. only
for small window sizes do we start to see overtrain-
ing. additional results for the 2d case are reported
in table 1 and show that cg are quite robust to the
choice of the grid size, when given enough room (  ) to
accommodate for the variation in the documents4.

table 1: classi   cation rate (%) vs cgs size for a    xed
ratio    of grid size to window size. cgs are squared so
we report only one dimensions, i.e., e = 20 = [20, 20]

e/ w 20/2
acc.
39.30

40/4
81.28

60/6
80.34

80/8
78.22

100/10

78.18

3.2

images: scene classi   cation

in case of visual data, modeled as bags of features,
cg model can account for misalignment of scenes as
a change of location in the counting grid. the smooth
thematic motion here corresponds to the motion of
the camera. the visual scene dataset,
introduced
in (oliva and torralba, 2001), is composed by two
datasets composed by four natural and four arti   cial
(man-made) categories and it is widely used by the
vision community. each class contains roughly 250
images.
following the standard bag-of-visual -words (li and
perona, 2005) approach we extracted sift features
from 16x16 pixel windows computed over a grid
spaced of 8 pixels and clustered the descriptors in
z = 200 visual words. we describe an image as a
bag of their features. the feature maps for images

4we reached similar conclusions for the 3d case.

are originally of the size 30    30 feature maps. for
the 2d case, we used this for the window size, as the
original features really did have a spatial arrangement
in a window this size. we kept the same volume for
the 3d case, setting w = [10, 10, 9]. we varied the
grid size, keeping cubic grids, choosing it form the set
[2, 3, 4]. as discussed above, we ran the experiments to
compare cg and lda representations in terms of the
power of the knn classi   er using their unsupervised
data embedding, i.e., we used all training and test
images of all classes to estimate a cg or an lda
thus embedding the data into the space of hidden
variables for these models without using the scene
labels, and then used these hidden variables (position
in the grid for cg or topic proportions for lda) to
perform 10    10 cross-validation on label prediction
in appropriate training/test splits. results, including
sam (reisinger et al., 2010) which was performed on
the same data are shown in    gure 4a as bars (stacked
red+blue bars, labeled with    90%   ). as in (reisinger
et al., 2010) we repeated the evaluation but inverting
training and test sets so that training used only
1/10-th of the data (blue bars, labeled with    10%   ).
in both cases cgs outperform other methods and is
robust to overtraining.

we also ran experiments we dubbed supervised clas-
si   cation in which we trained a model (cg or lda)
per class, varying the cardinality of the training set.
subsequently we classi   ed the test samples using like-
lihood tests. we repeated this process 5 times, aver-
aging the results. we compared 2d, 3d cgs and lda
in    gure 4b, where the two dimensional case seems to
outperform the higher dimensions (we also tried 4 di-
mensions) and in general, cgs outperform lda with
a large margin across the training data cardinalities.
in    gure 4c we compared the 2d cgs with the simi-
lar approach of (perina and jojic, 2011), where in the
m-step the original structure of the image is kept and
placed onto the grid and where the window size is equal
to the window size. results show that even discarding
all the spatial information, the accuracy does not drop
much.
finally we compared with the state of the art combin-
ing cgs with the discriminative layer put by (perina
et al., 2009). we reached the 91.12% and 91.77% in
the 2d case, 91.01% and 89.49% in the 3d case, re-
spectively on natural and on arti   cial images, match-
ing with statistical signi   cance the performance of the
well known method presented in (bosch et al., 2006)
(accuracies of 90.2% and 92.5%).

figure 4: scene classi   cation. a) unsupervised embedding evaluation in terms of the classi   cation based on the
latent space. (reisinger et al., 2010) (sam) does not report result for    arti   cial    dataset. we kept    xed w and
we varied e; the number in brackets is the coverage   . b-c) supervised image classi   cation results using di   erent
counting grids and lda models for di   erent classes (each class contains 250 images). see text for details.

3.3 biology: cellular presentation of viral

peptides and viral load

the immune system is among the most interesting
and most complex adaptive systems in higher organ-
isms.
it consists of a number of interacting subsys-
tems employing various infection clearing paths, and
cellular presentation plays a central role in many of

them. most of the cells present a sample of peptides
derived from cellular proteins as a means of advertis-
ing their states to the immune system. this facilitates
globally coordinated action against viral infection. as
the immune pressure depends on cellular presentation,
the variation in cellular presentation across patients is
expected to re   ect on the variation in viral load, at

4%20%60%natural images5560657075804%20%60%ar   cial imagesa)accuracy (%)55606570758085cg 2d (8)cg 2d (16)cg 3d (6)cg 3d (8)cg 3d (16)ldasamnatural imagescg 2d (8)cg 2d (16)cg 3d (6)cg 3d (8)cg 3d (16)ldaar   cial imagesb)accuracy (%)% of the data used for training(  )10%90%accuracy (%)4%20%60%606570758085556065707580854%20%60%cg 2d  (2)cg 2d (4)cg 3d  (2)cg 3d (4)lda% of the data used for training6065707580natural imagesar   cial imagesc)cg 2d  (2)cg 2d (4)cg fft (2)cg fft (4)matically across patients for a variety of reasons, e.g.
gender, previous exposures to related viruses, etc., de-
tection of statistically signi   cant links between cellular
presentation and viral load is expected to have impor-
tant consequences to vaccine research (kiepiela et al.,
2004) .

3.4 biology: promoter classi   cation

we considered a dataset composed by e. coli promoter
gene sequences (dna) with associated imperfect do-
main theory (towell et al., 1990). the task is to rec-
ognize promoters in strings that represent nucleotides
(a, g, t, or c). a promoter is a genetic region which
initiates the    rst step in the expression of an adja-
cent gene (transcription). the input features are 57
sequential dna nucleotides (   xed length). a special
notation is used to simplify specifying locations in the
dna sequence. the biological literature counts loca-
tions relative to the site where transcription begins.
fifty nucleotides before and six following this location
constitute an example. we transformed the sequences
in bag of features representations as we explained in
the previous section. results, obtained using leave-
one-out (loo) validation, show that cgs ( 83.01%)
outperform id48 ( 67.3% ) and methods based on
them (jaakkola and haussler, 1998), as the    sher ker-
nel ( 79.2% ) which are specialized for sequences.

3.5 biology: microarray expression

classi   cation

previous work (perina et al., 2010) has interpreted
microarray expression values as counts in    bags-
of-genes   , with good classi   cation rates have been
reached. following the same intuition we perform here
microarray classi   cation.
we used the dataset of the study of prostate can-
cer in (dhanasekaran, 2001) consisting of 54 sam-
ples with 9984 features. the samples are subdi-
vided in di   erent classes: 14 samples are labelled
as benign prostatic hyperplasia (labelled bph), 3 as
normal adjacent prostate (nap), 1 as normal adja-
cent tumor (nat), 14 as localized prostate cancer
(pca), 1 prostatitis (pro) and 20 as metastatic tu-
mors (met). the 6 classes can be divided in three
macro-classes: non-cancer (bph,nap,pro), cancer
(nat,pca), metastatic tumor (met). as in (rogers
et al., 2005), we    ltered the genes by variance and keep
only the most variable    ve hundred.
we compare the results of 2d and 3d with lda as we
did in the previous experiments. classi   cation errors
have been computed using 10-fold cross validation. we
learned several squared counting grids of several ca-
pacities by picking di   erent values for the width of the
square 2d window from the set {5, 8, 11}, and simi-

figure 5: hiv viral load regression. shown is the vari-
ation of the correlation factor    for a range of capacities
   and number of lda topics k.

least to some extent (moore et al., 2002; hertz et al.,
2010). we analyzed predicted cellular presentation of
181 hiv patients from the western australia cohort
(moore et al., 2002). to avoid confounding e   ects of
hiv clade, we analyzed only the clade b infected pa-
tients. we represented each patient   s cellular presen-
tation by a set of 492 counts over that many 9-long
peptides from the gag protein, previously found to
be targeted by the immune system. the counts were
calculated based on the patients hla class i types
and the hla-peptide binding estimation procedure
discussed in (hertz et al., 2010). we trained count-
ing grids and lda models of varying complexity on
all the data and then predicted the patients viral load
based on the representation of the data in the latent
space in leave-one-out cross validation framework, as
discussed above (fig. 5). we found that 3d counting
grids outperformed slightly the 2d grids in this case,
and that both outperformed lda. in fig. 6 we also
show the evolution of the embedding    of the log viral
loads for the    rst 180 patients (used to predict the log
viral load of the 181st patient) over the iterations of
counting grid learning. as discussed above, the bags
of words (peptides) are mapped to the counting grid
iteratively as the grid is estimated as to best model
the bags, but the regression target, the viral load, was
not used during the learning of cgs or lda models.
however, the inferred mapping after each iteration can
be used to visualize how the embedded viral load   
evolves. the emergence of areas of high (red) and
low (blue) viral load indicates that as the structure in
the cellular presentation is discovered, it does indeed
re   ect the variation in viral load. the presented cor-
relation factors between true and predicted viral loads
are computed after the convergence. the correlation
factors of above 0.3 which we uniformly    nd across a
range of values    indicate that cellular presentation
of the gag protein explains more than 9% of the log
viral load. in comparison, targeting e   ciency analy-
sis of gag (hertz et al., 2010) could only explain less
than 4% of viral load. although viral load varies dra-

0.20.30.4257101320304060100   / kcg 3dldapairwise correlation factorfigure 6: hiv viral load embedding.

table 2: microarray classi   cation results (%).

dataset

brain (pomeroy, 2002)
colon (alon et al., 1999)

cg 2d cg 3d lda
82.11
86.30
87.40
76.62

87.84
89.20

larly the width of the square 2d counting grid from
{23, 32, 42, 57}, and by picking the width of the cubic
3d window from {3, 4, 5} , and the cubic 3d counting
grid width from {8, 10, 12, 14}. we tried to keep ca-
pacity factor    comparable between the 2d and the 3d
cases. we compared our results with the lda (rogers
et al., 2005), varying the number of topics k, in the
set of   . results shown in fig.7 demonstrate that
cgs outperform with lda across a range of choices
of k. it is worth noting that to the best of our knowl-
edge, the state-of-the art on this dataset is 91.2% (pe-
rina et al., 2010) and 2d grids outperform this value
across a range of complexities. we also run classi   -
cation tasks on the brain and colon cancer datasets
(pomeroy, 2002; alon et al., 1999) which consist of 5
and 2 classes, respectively. in this case we    xed w and
e to best values we found for the previous microarray
set, and we embed the data for this single complex-
ity and then performed classi   cation as above. the
results are summarized in tab. 2 and compared with
the lda results published in (perina et al., 2010). in
this cases, 3d cgs performs better and the result we
got on colon cancer dataset is currently the state of
the art.

4 conclusions

we have introduced the multidimensional counting
grid model which outperforms the subspace models in
modeling a variety of datasets where we found that
thematic shifts seem to be a better    t to capturing
correlations in word occurrence. we found that 2d
grids outperform 3d grids in the visual scene clas-
si   cation for most scenes, which indicates the possi-
bility that the model primarily captures image mis-

figure 7: microarray classi   cation accuracy (%) for
the prostate cancer dataset for various values of the
capacity   , and the number of lda topics. the
bottom of the table shows the actual widths of the
square(cube) windows (w) and counting grids (e).
all cgs are of uniform dimensions.

alignment. on the other hand, the topology needed to
capture variation in cellular presentation of hiv gag
is better embedded in 3d counting grid. in fact, 3d
counting grids are slightly better in most, but not all
applications, and more research is needed into the ef-
fect that dimensionality as well as aspect ratio of the
counting grids have on model quality in various ap-
plications. another interesting observation that war-
rants more research is the fact that while the optimal
window size for text seemed to be relatively small (up
to 4 in one dimension), the hiv presentation model-
ing required larger patches than text modeling (using
8  8  8 windows into a larger grid perform slightly bet-
ter than 4    4    4 windowing of smaller grids with the
same ratio   . larger patch sizes allow for    ner-grained
overlapping of the data on the counting grid. finally,
the lda and cg models seem to model slightly dif-
ferent aspects of the data, and while lda may su   er
from having to model thematic shifts in data that has

1 iteration9 iterations11 iterations80 iterations[ ... ]1009590858075cg 2dcg 3dlda   /    4814162627415071130accuracy (%)23/118/523/88/442/1112/532/810/442/812/457/1114/532/510/357/814/442/512/357/514/32d cg: e/w3d cg: e/wthem, the basic cg model does not allow for other
types of mixing (in images for example, cgs capture
camera motion across scenes, but lda might capture
mixing of di   erent objects, and we can imagine this
analogy carried over to other types of data). thus,
a straightforward ways of combining aspects of both
models should be investigated further. the most sim-
ilar previously published model to counting grids is
the epitome model (jojic et al., 2003, 2010; ni et al.,
2008), which was also based on overlapping patches in
the latent space, and which also reaped bene   ts from
shift-invariance. however, epitomes relied on the data
samples already being ordered into an array (raw im-
age patches, for example), while cg model opens this
modeling strategy to a much wider set of data types.

references

u. alon, n. barkai, d. a. notterman, k. gishdagger,
s. ybarradagger, d. mackdagger, and a. j. levine.
broad patterns of gene expression revealed by clus-
tering analysis of tumor and normal colon tissues
probed by oligonucleotide arrays. proceedings of the
national academy of sciences of the united states
of america, 96(12):6745   6750, june 1999.

a. banerjee and s. basu. topic models over text
streams: a study of batch and online unsuper-
vised learning. in proceedings of sdm, 2007.

d. blei, a. ng, and m. jordan. latent dirichlet al-
location. journal of machine learning research, 3:
993   1022, january 2003.

a. bosch, a. zissermann, and x. munoz. scene clas-

si   cation via plsa. in proceedings of eccv, 2006.

s. deerwester, s. dumais, g. furnas, t. landauer,
and r. harshman. indexing by latent semantic anal-
ysis. journal of the american society for informa-
tion science, 41(6):391   407, 1990.

s. dhanasekaran. delineation of prognostic biomark-
ers in prostate cancer. nature, 412(6849):822   826,
2001.

t. hertz et al. mapping the landscape of host-
pathogen coevolution: hla class i binding and
its relationship with evolutionary conservation in
human and viral proteins. journal of virology, 85
(85):1310   1321, 2010.

t. jaakkola and d. haussler. exploiting generative
models in discriminative classi   ers. in in advances
in neural information processing systems 11, 1998.

n. jojic, b. frey, and a. kannan. epitomic analysis
of appearance and shape. in proceedings of iccv,
2003.

n. jojic, a. perina, and v. murino. structural epit-
ome: a way to summarize one   s visual experience. in

advances in neural information processing systems
23, 2010.

p. kiepiela et al. dominant in   uence of hla-b in me-
diating the potential co-evolution of hiv and hla.
nature, 432(85):769   775, 2004.

f li and p. perona. a bayesian hierarchical model for
learning natural scene categories. in proceedings
of cvpr, 2005.

k. mardia and p. jupp. directional statistics. john

wiley and sons ltd., 2000.

c. moore et al. evidence of hiv-1 adaptation to
hla-restricted immune responses at a population
level. science, 296(5572):436   442, 2002.

k. ni, a. kannan, a. criminisi, and j. winn. epito-
mic location recognition. in proceedings of cvpr,
2008.

a. oliva and a. torralba. modeling the shape of the
scene: a holistic representation of the spatial en-
velope. international journal of id161,
42(3):145   175, 2001.

h. peng, f. long, and c. ding. feature selection based
on mutual information: criteria of max-dependency,
max-relevance and min-redundancy. ieee transac-
tions on pattern analysis and machine intelligence,
27(8):1226   1238, 2005.

a. perina, m. cristani, u. castellani, v. murino, and
n. jojic. free energy score space. in advances in
neural information processing systems 22, 2009.

a. perina and n. jojic. image analysis by counting on

a grid. in proceedings of cvpr, 2011.

a. perina, p. lovato, v. murino, and m. bicego.
biologically-aware id44 (balda)
for the classi   cation of expression microarray.
in
proceedings of
the 5th iapr international con-
ference on pattern recognition in bioinformatics,
prib   10, pages 230   241, 2010.

s. pomeroy. prediction of central nervous system em-
bryonal tumour outcome based on gene expression.
nature, 415(6870):436   442, 2002.

j. reisinger, a. waters, b. silverthorn,

and
r. mooney. spherical topic models. in proceedings
of icml, 2010.

s. rogers, m. girolami, c. campbell, and r. bre-
itling. the latent process decomposition of cdna
microarray data sets. ieee/acm transactions on
computational biology and bioinformatics, 2:143   
156, 2005.

g.g. towell, j. shavlik, and m. noordewier. re   ne-
ment of approximate domain theories by knowledge-
based neural networks.
in in proceedings of the
eighth national conference on arti   cial intelli-
gence, 1990.

