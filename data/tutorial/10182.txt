this is an unpublished manuscript,    rst made available on the web in 2006. it is a much-revised version of
an earlier manuscript entitled    distributional measures as proxies for semantic relatedness   , which was    rst
made available on the web in 2005.

2
1
0
2

 
r
a

m
8

 

 
 
]
l
c
.
s
c
[
 
 

1
v
8
5
8
1

.

3
0
2
1
:
v
i
x
r
a

distributional measures of semantic
distance: a survey

saif mohammad
university of toronto

graeme hirst
university of toronto

the ability to mimic human notions of semantic distance has widespread applications. some
measures rely only on raw text (distributional measures) and some rely on knowledge sources
such as id138. although extensive studies have been performed to compare id138-based
measures with human judgment, the use of distributional measures as proxies to estimate
semantic distance has received little attention. even though they have traditionally performed
poorly when compared to id138-based measures, they lay claim to certain uniquely attractive
features, such as their applicability in resource-poor languages and their ability to mimic both
semantic similarity and semantic relatedness. therefore, this paper presents a detailed study of
distributional measures. particular attention is paid to    esh out the strengths and limitations of
both id138-based and distributional measures, and how distributional measures of distance
can be brought more in line with human notions of semantic distance. we conclude with a brief
discussion of recent work on hybrid measures.

1. introduction

semantic distance is a measure of how close or distant the meanings of two units of
language are. the units of language may be words, phrases, sentences, paragraphs, or
documents. the nouns dance and choreography, for example, are closer in meaning than
the nouns clown and bridge, and so are said to be semantically closer. the semantic dis-
tances between words (or more precisely, between concepts) can be used as fundamental
building blocks for measuring semantic distance between larger units of language. the
ability to mimic human judgments of semantic distance is useful in numerous natural
language tasks including machine translation, id51, thesaurus
creation, information retrieval, text summarization, and identifying discourse structure.
this paper describes the state-of-the-art in corpus-based measures of semantic distance
between these fundamental units of language. it identi   es the signi   cant challenges that
existing approaches to semantic distance face and in the process    eshes out questions
that lead to a better understanding of why two concepts are considered semantically
close. the paper concludes with a discussion of new hybrid approaches, that show the
potential to address these challenges.

units of language, especially words, may have more than one possible meaning.
however, their context may be used to determine the intended senses. for example, star
can mean both celestial body and celebrity; however, star in   stars are powered by
nuclear fusion   refers only to celestial body and is much closer to sun than to famous.
thus, semantic distance between words in context is in fact the distance between their
underlying senses or lexical concepts. therefore, in this paper, we take word senses
to be a particular kind of concept. when we refer directly to a concept (written in
small capitals), it is with the understanding that it is the sense of one or more words,

   2006 saif mohammad and graeme hirst.

as re   ected in the name that we give the concept. we do not, in this paper, consider
concepts that are unlexicalized.

humans consider two concepts to be semantically close if there is a sharing of some
meaning. speci   cally, two lexical concepts are semantically close if there is a lexical
semantic relation between the concepts. putting it differently, the reason why two con-
cepts are considered semantically close can be attributed to a lexical semantic relation
that binds them. according to cruse (1986), a lexical semantic relation is a relation
between lexical units   a surface form along with a sense. as he points out, the number
of semantic relations that bind concepts is innumerable; but certain relations, such as
hyponymy, meronymy, antonymy, and troponymy, are more systematic and have en-
joyed more attention in the linguistics community. however, as morris and hirst (2004)
point out, these relations are far out-numbered by others, which they call non-classical
relations. here are a few of the kinds of non-classical relations they observed: positive
qualities (brilliant, kind), concepts pertaining to a concept (kind, chivalrous,
formal pertaining to gentlemanly), and commonly co-occurring words (locations
such as homeless, shelter; problem   solution pairs such as homeless, shelter).

1.1 semantic relatedness and semantic similarity

semantic distance is of two kinds: semantic similarity and semantic relatedness. the
former is a subset of the latter, but the two terms may be used interchangeably in
certain contexts, making it even more important to be aware of their distinction. two
concepts are considered to be semantically similar if there is a synonymy (or near-
synonymy), hyponymy (hypernymy), antonymy, or troponymy relation between them
(examples include apples   bananas, doctor   surgeon, dark   bright). two word
senses are considered to be semantically related if there is any lexical semantic relation
at all between them   classical or non-classical (examples include apples   bananas,
surgeon   scalpel, tree   shade).

1.2 human judgments of semantic distance

humans are adept at estimating semantic distance; but consider the following ques-
tions: how strongly will two people agree/disagree on distance estimates? will the
agreement vary over different sets of concepts? are we equally good at estimating
semantic similarity and semantic relatedness? in our minds, is there a clear distinction
between related and unrelated concepts or are concept-pairs spread across the whole
range from synonymous to unrelated? some of the earliest work that begins to address
these questions is by rubenstein and goodenough (1965). they conducted quantita-
tive experiments with human subjects (51 in all) who were asked to rate 65 english
word pairs on a scale from 0.0 to 4.0 as per their semantic distance. the word pairs
chosen ranged from almost synonymous to unrelated. however, they were all noun
pairs and those that were semantically close were also semantically similar; the dataset
did not contain word pairs that are semantically related but not semantically similar.
the subjects repeated the annotation after two weeks and the new distance values
had a pearson   s correlation r of 0.85 with the old ones. miller and charles (1991) also
conducted a similar study on 30 word pairs taken from the rubenstein-goodenough
pairs. these annotations had a high correlation (r = 0.97) with the mean annotations
of rubenstein and goodenough (1965). resnik (1999) repeated these experiments and
found the inter-annotator correlation (r) to be 0.90.

2

mohammad and hirst

distributional measures of semantic distance

table 1
different datasets that are manually annotated with distance values. pearson   s correlation
coef   cient (r) was used to determine inter-annotator correlation (last column).

dataset
rubenstein and goodenough
miller and charles
resnik and diab
gurevych
zesch and gurevych

year language
1965
1991
2000
2005
2006

english
english
english
german
german

# pairs

65
30
27
65
350

pos
n
n
v
n

n, v, a

# subjects

51
38
10
24
8

r
-
.90

.76 and .79

.81
.69

note: rubenstein and goodenough (1965) do not report inter-subject correlation, but
determine intra-subject correlation to be 0.85 for 36 (out of the 65) word pairs for which
similarity judgments were repeated by 15 (of the 51) subjects.

correlation (r)

resnik and diab (2000) conducted annotations of 48 verb pairs and found
inter-annotator
to be 0.76 (when the verbs were presented
without context) and 0.79 (when presented in context). gurevych (2005) and
zesch, gurevych, and m  hlh  user (2007) asked native german speakers to mark
two different sets of german word pairs with distance values. set 1 was a german
translation of the rubenstein and goodenough (1965) dataset. it had 65 noun   noun
word pairs. set 2 was a larger dataset containing 350 word pairs made up of nouns,
verbs, and adjectives. the semantically close word pairs in the 65-word set were mostly
synonyms or hypernyms (hyponyms) of each other, whereas those in the 350-word set
had both classical and non-classical relations with each other. details of these semantic
distance benchmarks are summarized in table 1. inter-subject correlations (last column
in table 1) are indicative of the degree of ease in annotating the datasets.

it should be noted here that even though the annotators were presented with word-
pairs and not concept-pairs, it is reasonable to assume that they were annotated as per
their closest senses. for example, given the noun pair bank and interest, most if not
all will identify it as semantically related even though both words have more than
one sense and many of the sense   sense combinations are unrelated (for example, the
river bank sense of bank and the special attention sense of interest). the high
agreement and correlation values suggest that humans are quite good and consistent at
estimating semantic distance of noun-pairs; however, annotating verbs and adjectives
and a combination of parts of speech is harder. this also means that estimating semantic
relatedness is harder than estimating semantic similarity.

apart from showing that humans can indeed estimate semantic distance, these
datasets act as    gold standards" to evaluate automatic distance measures. however,
lack of large amounts of data from human subject experimentation limits the reliability
of this mode of evaluation. therefore automatic distance measures are also evaluated
by their usefulness in natural language tasks such as those mentioned earlier.

1.3 automatic measures of semantic distance

automatic measures of semantic distance quantify the semantic distance between word
pairs. they give values within a certain range (for example, 0 to 1), such that one end
of this range represent maximal closeness or synonymy, while the other end represents

3

maximal distance. depending on which end is which, measures of semantic distance can
be classi   ed as measures of distance (larger values indicate greater distance and less
closeness) and measures of closeness (larger values indicate shorter distance and more
closeness).1 a measure of closeness can be easily converted to a measure of distance by
applying a suitable inverse function, or vice versa.

two classes of automatic methods have been traditionally used to determine
semantic distance. knowledge-rich measures of concept-distance, such as those of
jiang and conrath (1997), leacock and chodorow (1998), and resnik (1995), rely on the
structure of a knowledge source, such as id138, to determine the distance between
two concepts de   ned in it.2 distributional measures of word-distance (knowledge-
lean measures), such as cosine and   -skew divergence (lee 2001), rely on the distri-
butional hypothesis, which states that two words tend to be semantically close if they
occur in similar contexts (firth 1957). distributional measures rely simply on text (and
possibly some shallow syntactic processing) and can give the distance between any two
words that occur at least a few times.

the

have

been widely

various id138-based measures

studied
(budanitsky and hirst 2006; patwardhan, banerjee, and pedersen 2003). the study
of distributional measures on the whole has received much less attention.3 even
though, as weeds (2003) and mohammad and hirst (2006b) show,
they perform
poorly when compared to id138-based measures, the distributional measures of
word-distance have many attractive features, including their ability to measure both
semantic similarity and semantic relatedness. further, they are not dependent on costly
knowledge sources that do not exist for most languages. this paper therefore focuses
on distributional measures and analyzes their strengths and limitations. particular
attention is paid to the different kinds of distributional measures and their components.
the motivation is that a better understanding of distributional measures will lead
to bringing them more in line with human notions of semantic distance, while still
maintaining their applicability to resource-poor languages and their ability to mimic
both semantic similarity and semantic distance.

2. knowledge-rich approaches to semantic distance

before we begin our examination of distributional measures, we look brie   y at the
resource-based measures. in some ways they are complementary to distributional
measures and so the discussion will set the context for the analysis of distributional
measures.

creation of electronically available ontologies and semantic networks such as
id138 has allowed their use to help solve numerous natural language problems
including the measurement of
semantic distance. budanitsky and hirst (2006),
hirst and budanitsky (2005), and patwardhan, banerjee, and pedersen (2003) have
done an extensive survey of the various id138-based measures, their comparisons

1 a note about terminology: in many contexts, the term distance measures refers to the complete set of

measures (irrespective of what the different ends of the range signify). in certain other contexts (as in this
paragraph), distance measures refers only to those measures that give larger values to signify greater
distance. the context, usually by its reference to this numeric property or lack thereof will make clear the
intended meaning of the term.

2 the nodes in id138 (synsets) represent word senses and edges between nodes represent semantic

relations such as hyponymy and meronymy.

3 see curran (2004) and weeds, weir, and mccarthy (2004) for other work that compares various

distributional measures.

4

mohammad and hirst

distributional measures of semantic distance

with human judgment on selected word pairs, and their usefulness in applications such
as real-word id147 and id51. hence, this section
provides only a brief summary of the major knowledge-rich measures of semantic
distance.

2.1 measures that exploit id138   s semantic network

a number of id138-based measures consider two concepts to be close if they are
close to each other in id138. one of the earliest and simplest measures is rada et al.   s
(1989) edge-counting method. the shortest path in the network between the two target
concepts (target path) is determined. the more edges there are between two words, the
more distant they are. elegant as it may be, the measure hinges on the largely incorrect
assumption that all the network edges correspond to identical semantic distance.

nodes in a network may be connected by different kinds of lexical relations such
as hyponymy, meronymy, and so on. edge counts apart, hirst and st-onge   s (1998)
measure takes into account the fact that if the target path consists of edges that belong
to many different relations, then the target concepts are likely more distant. the idea is
that if we start from a particular node c1 and take a path via a particular relation (say,
hyponymy), to a certain extent the concepts reached will be semantically related to c1.
however, if during the way we take edges belonging to different relations (other than
hyponymy), very soon we may reach words that are unrelated. hirst and st-onge   s
measure of semantic relatedness is:

hs(c1, c2) = c     path length     k    d

(1)

where c1 and c2 are the target concepts, d is the number of times an edge pertaining
to a relation different from that of the preceding edge is taken, and c and k are
empirically determined constants. more recently, yang and powers (2005) proposed a
weighted edge-counting method to determine semantic relatedness using the hyper-
nymy/hyponymy, holonymy/meronymy, and antonymy links in id138.

leacock and chodorow (1998) used just one relation (hyponymy) and modi   ed the
path length formula to re   ect the fact that edges lower down in the hyponymy hierarchy
correspond to smaller semantic distance than the ones higher up. for example, synsets
pertaining to sports car and car (low in the hierarchy) are much more similar than those
pertaining to transport and instrumentation (higher up in the hierarchy) even though both
pairs of nodes are separated by exactly one edge in the hierarchy. their formula is:

lc(c1, c2) =     log

len(c1, c2)

2d

(2)

where d is the maximum depth of the taxonomy.

resnik (1995) suggested a measure that combines corpus statistics with id138.
he proposed that since the lowest common subsumer or lowest superordinate (lso)
of the target nodes represents what is similar between them, the semantic similarity
between the two concepts is directly proportional to how speci   c the lso is. the more
general the lso is, the larger the semantic distance between the target nodes. this speci-
   city is measured by the formula for information content (ic)   the negative logarithm
of the id203 of the lso:

res(c1, c2) = ic(lso(c1, c2)) =     log p(lso(c1, c2))

(3)

5

(4)

(5)

observe that using information content has the effect of inherently scaling the semantic
similarity measure by the depth of the taxonomy. usually, the lower the lowest super-
ordinate, the lower the id203 of occurrence of the lso and the concepts subsumed
by it, and hence, the higher its information content.

as per resnik   s formula, given a particular lowest superordinate, the exact po-
sitions of the target nodes below it in the hierarchy do not have any effect on the
semantic similarity. intuitively, we would expect that word pairs closer to the lso are
more semantically similar than those that are distant. jiang and conrath (1997) and
lin (1997) incorporate this notion into their measures which are arithmetic variations of
the same terms. the jiang and conrath (1997) measure (jc) determines how dissimilar
each target concept is from the lso (ic(c1)     ic(lso(c1, c2)) and ic(c2)     ic(lso(c1, c2))).
the    nal semantic distance between the two concepts is then taken to be the sum of
these differences. lin (1997) (like resnik) points out that the lso is what is common
between the two target concepts and that its information content is the common infor-
mation between the two concepts. his formula (lin) can be thought of as taking the dice
coef   cient of the information in the two target concepts.

jc(c1, c2) = 2 log p(lso(c1, c2))     (log(p(c1)) + (log(p(c2)))

lin(c1, c2) =

2    log p(lso(c1, c2))

log(p(c1)) + (log(p(c2))
that

showed

budanitsky and hirst (2006)

jiang-conrath measure
has the highest correlation (0.850) with the miller and charles noun pairs
and performs better
these measures in a id147 task.
patwardhan, banerjee, and pedersen (2003) achieved similar results using the measure
for id51.

than all

the

all of

the approaches described above rely heavily (if not

solely) on
the hypernymy/hyponymy network in id138;
they are designed for, and
evaluated on, noun   noun pairs. however, more recently, resnik and diab (2000)
and yang and powers (2006) developed measures aimed at verb   verb pairs.
resnik and diab (2000) ported several measures which are traditionally applied
on the noun hypernymy/hyponymy network (edge counting, and the measures of
resnik (1995), and lin (1997)) to the relatively shallow verb troponymy network.
the two information content   based measures ranked a carefully chosen set of 48
verbs best in order of their semantic distance.4 yang and powers (2006) ported their
earlier work on nouns (yang and powers 2005) to verbs. in order to compensate for
the relatively shallow verb troponymy hierarchy and the lack of a corresponding
holonymy/meronymy hierarchy, they proposed several back-off models   the most
useful one being the distance between a noun pair that has the same lexical form as
the verb pair. however, the approach has too many tuned parameters (9 in all) and
performed poorly on a set of 36 toefl word-choice questions involving verb targets
and alternatives.

2.2 measures that rely on dictionaries and thesauri

lesk (1986) introduced a method to perform id51 using word
glosses (de   nitions). the glosses of the senses of a target word are compared with
those of its context and the number of word overlaps is determined. the sense with the

4 only those verbs were selected which require a theme, and the sub-categorization frames had to match.

6

mohammad and hirst

distributional measures of semantic distance

greatest number of overlaps is chosen as the intended sense of the target. inspired by
this approach, banerjee and pedersen (2003) proposed a semantic relatedness measure
that deems two concepts to be more semantically related if there is more overlap in
their glosses. notably, they overcome the problem of short glosses by considering the
glosses of concepts related to the target concepts through the id138 lexical semantic
relations such as hyponymy/hypernymy. they also give more weight to larger overlap
sequences. patwardhan and pedersen (2006) proposed another gloss-based semantic re-
latedness measure which performed slightly worse than the extended gloss overlap
measure in a id51 task, but markedly better at ranking the
miller and charles (1991) word pairs. they create aggregate co-occurrence vectors for a
id138 sense by adding the co-occurrence vectors of the words in its id138 gloss.
the distance between two senses is then determined by the cosine of the angle between
their aggregate vectors. such aggregate co-occurrence vectors are expected to be noisy
because they are created from data that is not sense-annotated.

jarmasz and szpakowicz (2003) use the taxonomic structure of roget   s thesaurus to
determine semantic similarity. two words are considered maximally similar if they
occur in the same semicolon group in the thesaurus. then on, decreasing in similarity
are word pairs in the same paragraph, words pairs in different paragraphs belonging to
the same part of speech and within the same category, word pairs in the category, and so
on until word pairs which have nothing in common except that they are in the thesaurus
(maximally distant). they show that this simple approach performs remarkably well at
ranking word pairs and determining the correct answer in sets of toefl, esl, and
reader   s digest word choice problems.

2.3 challenges

in this section, we review some of the shortcomings of resource-based measures in order
to motivate and to compare them with distributional measures that we will introduce
in section 3.

2.3.1 lack of high-quality id138-like knowledge sources. ontologies, id138s,
and semantic networks are available for a few languages such as english, german, and
hindi. creating them requires human experts and it is time intensive. thus, for most
languages, we cannot use id138-based measures simply due to the lack of a word-
net in that language. further, even if created, updating an ontology is again expensive
and there is usually a lag between the current state of language usage/comprehension
and the semantic network representing it. further, the complexity of human languages
makes creation of even a near-perfect semantic network of its concepts impossible. thus
in many ways the ontology-based measures are only as good as the networks on which
they are based.

on the other hand, distributional measures require only text. large corpora, billions
of words in size, may now be collected by a simple web crawler. large corpora of more-
formal writing are also available (for example, the wall street journal or the american
printing house for the blind (aphb) corpus). this makes distributional measures very
attractive.

2.3.2 poor estimation of semantic relatedness. as morris and hirst (2004) pointed out,
a large number of concept pairs, such as strawberry   cream and doctor   scalpel,
have a non-classical relation between them (strawberries are usually eaten with
cream and a doctor uses a scalpel to make an incision). these words are not

7

semantically similar, but rather semantically related. an ontology- or id138-based
measure will correctly identify the amount of semantic relatedness only if such relations
are explicitly coded into the knowledge source. further, the most accurate id138-
based measures rely only on its extensive is-a hierarchy. this is because networks of
other lexical-relations such as meronymy are much less developed. further, the net-
works for different parts of speech are not well connected. all this means that, while
id138-based measures accurately estimate semantic similarity between nouns, their
estimation of semantic relatedness, especially in pairs other than noun   noun, is at best
poor and at worse non-existent. on the other hand, distributional measures can be used
to determine both semantic relatedness and semantic similarity.

2.3.3 inability to cater to speci   c domains. given a concept pair, measures that rely
only on id138 and no text, such as that of rada et al. (1989), give just one distance
value. however, two concepts may be very close in a certain domain but not so much in
another. for example, space and time are close in the domain of quantum mechanics
but not so much in most others. ontologies have been made for speci   c domains, which
may be used to determine semantic similarity speci   c to these domains. however, the
number of such ontologies is very limited. some of the more successful id138-based
measures, such as that of jiang and conrath (1997), that rely also on text, do indeed
capture domain-speci   city to some extent, but the distance values are still largely
shaped by the underlying network, which is not domain-speci   c. on the other hand,
distributional measures rely primarily (if not completely) on text, and large amounts of
corpora speci   c to particular domains can easily be collected.

2.3.4 computational complexity and storage requirements. as applications for lin-
guistic distance become more sophisticated and demanding, it becomes attractive to
pre-compute and store the distance values between all possible pairs of words or
senses. however both id138-based and distributional measures have large space
requirements to do this, requiring matrices of size n    n, where n is very large. in
case of id138-based measures, n is the number of senses (81,000 just for nouns). in
case of distributional measures, n is the size of the vocabulary (at least 100,000 for most
languages). given that the above matrices tend to be sparse5 and that computational
capabilities are continuing to improve, the above limitation may not seem hugely
problematic, but as we see more and more natural language applications in embedded
systems and hand-held devices, such as cell phones, ipods, and medical equipment,
memory and computational power become serious constraints.

2.3.5 reluctance to cross the language barrier. both id138-based and distributional
measures have largely been used in a monolingual framework. even though semantic
distance seems to hold promise in tasks such as machine translation and multi-lingual
text summarization that inherently involve two or more languages, automatic measures
of semantic distance have rarely been applied. with the development of the euroword-
net, involving interconnected networks of seven different languages, it is possible that
we shall see more cross-lingual work using id138-based measures in the future.
however, such an interconnected network will be very hard to create for more-different
language pairs such as english and chinese or english and arabic.

5 even though id138-based and distributional measures give non-zero closeness values to a large

number of term pairs, values below a suitable threshold can be reset to 0.

8

mohammad and hirst

distributional measures of semantic distance

3. knowledge-lean, distributional approaches to semantic distance

3.1 the distributional hypotheses: the original and the revised

distributional measures are inspired by the maxim    you shall know a word by the
company it keeps    (firth 1957). these measures rely simply on raw text and possi-
bly some shallow syntactic processing. they are much less resource-hungry than the
semantic measures, but they measure the distance between words rather than word-
senses or concepts. two words are considered close if they occur in similar contexts.
the context of a target word is usually taken to be the set of words within a certain
window around it, for example,   5 words or the complete sentence. the set of contexts
of a target word is usually represented by the set of words in these contexts, their
strength of association (soa) with the target word, and possibly their syntactic relation
with the target, for example verb   object, subject   verb, and so on. the strength of co-
occurrence association between the target and another word quanti   es how much more
(or less) than chance the two words occur together in text. commonly used measures of
association are id155 (cp) and pointwise mutual information (pmi).
the distance between the sets of contexts of two target words can be used as a proxy
for their semantic distance, as words found in similar contexts tend to be semantically
similar   the distributional hypothesis (firth 1957; harris 1968).

the hypothesis makes intuitive sense, as budanitsky and hirst (2006) point out: if
two words have many co-occurring words in common, then similar things are being
said about both of them and so they are likely to be semantically similar. conversely, if
two words are semantically similar, then they are likely to be used in a similar fashion in
text and thus end up with many common co-occurrences. for example, the semantically
similar bug and insect are expected to have a number of common co-occurring words
such as crawl, squash, small, woods, and so on, in a large-enough text corpus.

the distributional hypothesis only mentions semantic similarity and not semantic
relatedness. this, coupled with the fact that the difference between semantic related-
ness and semantic similarity is somewhat nuanced and can be missed, meant that
almost all work employing the distributional hypothesis was labeled as estimating
semantic similarity. however, it should be noted that distributional measures can
be used to estimate both semantic similarity and semantic relatedness. even though
sch  tze and pedersen (1997) and landauer, foltz, and laham (1998), for example, use
the term similarity and not relatedness, their lsa-based distance measures in fact es-
timate semantic relatedness and not semantic similarity. we propose more-speci   c
distributional hypotheses that make clear how distributional measures can be used to
estimate semantic similarity and how they can be used to measure semantic relatedness:

hypothesis of the distributionally close and semantically related:
two target words are distributionally close and semantically related if they have many
common strongly co-occurring words.
(for example, doctor   surgeon and doctor   scalpel. see example co-occurring words in
table 2.)

hypothesis of the distributionally close and semantically similar:
two target words are distributionally close and semantically similar if they have many
common strongly co-occurring words that each have the same syntactic relation with
the two targets.
(for example, doctor   surgeon, but not doctor   scalpel. see syntactic relations with example
co-occurring words in table 2.)

9

table 2
example: common syntactic relations of target words with co-occurring words.

semantically similar
target pair
doctor (n)
surgeon (n)

semantically related
target pair
doctor (n)
scalpel (n)

cut (v)

co-occurring words
hardworking (adj)

patient (n)

subject   verb
subject   verb

noun   quali   er
noun   quali   er

subject   object
subject   object

subject   verb

noun   quali   er

subject   object

prepositional object   verb

   

prepositional object   object

the idea is that both semantically similar and semantically related word pairs will
have many common co-occurring words. however, words that are semantically similar
belong to the same broad part of speech (noun, verb, etc.), but the same need not be
true for words that are semantically related. therefore, words that are semantically
similar will tend to have the same syntactic relation, such as verb   object or subject   
verb, with most common co-occurring words. thus, the two words are considered
semantically related simply if they have many common co-occurring words. but to be
semantically similar as well, the words must have the same syntactic relation with co-
occurring words. consider the word pair doctor   operate. in a large enough body of text,
the two words are likely to have the following common co-occurring words: patient,
scalpel, surgery, recuperate, and so on. all these words will contribute to a high score of
relatedness. however, they do not have the same syntactic relation with the two targets.
(the word doctor is almost always used as a noun while operate is a verb.) thus, as per
the two revised distributional hypotheses, doctor and operate will correctly be identi   ed
as semantically related but not semantically similar. the word pair doctor   nurse, on the
other hand, will be identi   ed as both semantically related and semantically similar.

in order to clearly differentiate from the distance as calculated by a id138-based
semantic measure (described earlier in section 2.1), the distance calculated by a corpus-
based distributional measure will be referred to as distributional distance.

3.2 corpus-based measures of distributional distance

we now describe speci   c distributional measures that rely on the distributional hy-
potheses; depending on which speci   c hypothesis they use, they mimic either semantic
similarity or semantic relatedness.

3.2.1 spatial metrics: cos, l1, l2. consider a multidimensional space in which the num-
ber of dimensions is equal to the size of the vocabulary. a word w can be represented
by a point in this space such that the component of ~w in a dimension (corresponding to
word x, say) is equal to the strength of association (soa) of w with x (soa(w, x)). thus,
the vectors corresponding to two words are close together, and thereby get a low dis-
tributional distance score, if they share many co-occurring words and the co-occurring

10

mohammad and hirst

distributional measures of semantic distance

words have more or less the same strength of association with the two target words. the
distance between two vectors can be calculated in different ways as described below.

cosine. the cosine method (denoted by cos) is one of the earliest and most widely used
distributional measures. given two words w1 and w2, the cosine measure calculates the
cosine of the angle between ~w1 and ~w2. if a large number of words co-occur with both
w1 and w2, then ~w1 and ~w2 will have a small angle between them and the cosine will be
large; signifying a large relatedness/similarity between them. the cosine measure gives
scores in the range from 0 (unrelated) to 1 (synonymous). so the higher the value, the
less distant the target word-pair is.

cos(w1, w2) =

   w   c(w1)   c(w2) (p(w|w1)    p(w|w2))

q   w   c(w1) p(w|w1)2   q   w   c(w2) p(w|w2)2

(6)

where c(w) is the set of words that co-occur (within a certain window) with the word
w in a corpus. in this instantiation of the cosine measure, id155 of the
co-occurring words given the target words is used as the strength of association.

the cosine was used, among others, by sch  tze and pedersen (1997) and
yoshida, yukawa, and kuwabara (2003), who suggest methods of automatically gen-
erating distributional thesauri from text corpora. sch  tze and pedersen (1997) use the
tipster category b corpus (harman 1993) (450,000 unique terms) and the wall street
journal to create a large but sparse co-occurrence matrix of 3,000 medium-frequency
words (frequency rank between 2,000 and 5,000). id45 (singular
value decomposition) (sch  tze and pedersen 1997) is used to reduce the dimensionality
of the matrix and get for each term a word vector of its 20 strongest co-occurrences. the
cosine of a target word   s vector with each of the other word vectors is calculated and
the words that give the highest scores comprise the thesaurus entry for the target word.
yoshida, yukawa, and kuwabara (2003) believe that words that are closely related
for one person may be distant for another. they use around 40,000 html docu-
ments to generate personalized thesauri for six different people. documents used to
create the thesaurus for a person are retrieved from the subject   s home page and a
web crawler which accesses linked documents. the authors also suggest a root-mean-
squared method to determine the similarity of two different thesaurus entries for the
same word.

manhattan and euclidean distances. distance between two points (words) in vector space
can also be calculated using the formulae for manhattan distance a.k.a. the l1 norm
(denoted by l1) or euclidean distance a.k.a. the l2 norm (denoted by l2). in the
manhattan distance (7) (dagan, lee, and pereira (1997), dagan, lee, and pereira (1999),
and lee (1999)), the difference in strength of association of w1 and w2 with each word
that they co-occur with is summed. the greater the difference, the greater is the distri-
butional distance between the two words. euclidean distance (8) (lee 1999) employs
the root mean square of the difference in association to get the    nal distributional
distance. both the l1 and l2 norms give scores in the range between 0 (zero distance
or synonymous) and in   nity (maximally distant or unrelated).

l1(w1, w2) =

   

| p(w|w1)     p(w|w2) |

w   c(w1)   c(w2)

(7)

11

l2(w1, w2) = s

   

(p (w|w1)     p (w|w2))2

(8)

w   c(w1)   c(w2)

the above formulae use id155 of the co-occurring words given a target
word as the strength of association.

lee (1999) compared the ability of all three spatial metrics to determine the proba-
bility of an unseen (not found in training data) word pair. the measures in order of their
performance (from better to worse) were: l1 norm, cosine, and l2 norm. weeds (2003)
determined the correlation of word pair ranking as per a handful of distributional
measures with human rankings (miller and charles (1991) word pairs). she used verb-
object pairs from the british national corpus (bnc) and found the correlation of l1 norm
with human rankings to be 0.39.

3.2.2 mutual information   based measures: hindle, lin. hindle (1990) was one of the
   rst to factor the strength of association of co-occurring words into a distributional
similarity measure.6 consider the nouns nj and nk that exist as objects of verb vi in
different instances within a text corpus. hindle used the following formula to determine
the distributional similarity of nj and nk solely from their occurrences as object of vi:

hinobj(vi, nj, nk) =

min(i(vi, nj), i(vi, nk)),

if i(vi, nj) > 0 and i(vi, nk) > 0

| max(i(vi, nj), i(vi, nk)) |,

if i(vi, nj) < 0 and i(vi, nk) < 0
otherwise

0,

(9)

                  
               

i(n, v) stands for the pointwise mutual information (pmi) between the noun n and
verb v (note that in case of negative pmi values, the maximum function captures the
pmi which is lower in absolute value). the measure follows from the distributional
hypothesis   the more similar the associations of co-occurring words with the two target
words, the more semantically similar they are. hindle used pmi7 as the strength of
association. using the minimum of the two pmis captures the similarity in the strength
of association of vi with each of the two nouns.

hindle used an analogous formula to calculate distributional similarity (hinsubj)
using the subject   verb relation. the overall distributional similarity between any two
nouns is calculated by the formula:

hin(n1, n2) =

n
   

i=0(cid:0)hinobj(vi, n1, n2) + hinsubj(vi, n1, n2)(cid:1)

(10)

the measure gives similarity scores from 0 (maximally dissimilar) to in   nity (maximally
similar or synonymous). note that in hindle   s measure, the set of co-occurring words
used is restricted to include only those words that have the same syntactic relation with
both target words (either verb   object or verb   subject). this is therefore a measure that

6 see grefenstette (1992) for an approach that does not incorporate strength of association of co-occurring

words. he, like hindle (1990), uses syntactic dependencies to to characterize the set of contexts of a target
word. the jaccard coef   cient is used to determine how similar the two sets of contexts are.

7 hindle (1990) and lin (1998b) both refer to pointwise mutual information as mutual information.

12

mohammad and hirst

distributional measures of semantic distance

mimics semantic similarity and not semantic relatedness. a form of hindle   s measure
where all co-occurring words are used, making it a measure that mimics semantic
relatedness, is shown below:

hinrel(w1, w2) =    

w   c(w)

min(i(w, w1), i(w, w2)),

if i(w, w1) > 0 and i(w, w2) > 0

| max(i(w, w1), i(w, w2)) |,

if i(w, w1) < 0 and i(w, w2) < 0
otherwise

0,

(11)

                  
               

where c(w) is the set of words that co-occur with word w.

lin (1998b) suggests a different measure derived from his information-theoretic
de   nition of similarity (lin 1998a). further, he uses a broad set of syntactic relations
apart from just subject   verb and verb   object relations and shows that using multiple
relations is bene   cial even for hindle   s measure. he    rst extracts triples of the form
(x, r, y) from the partially parsed text, where the word x is related to y by the syntactic
relation r. lin de   nes the distributional similarity between two words, w1 and w2, as
follows:

lin(w1, w2) =

   (r,w)     t(w1)     t(w2) (i(w1, r, w) + i(w2, r, w))

   (r,w   )     t(w1) i(w1, r, w   ) +    (r,w      )     t(w2) i(w2, r, w      )

(12)

where t(x) is the set of all word pairs (r, y) such that the pointwise mutual information
i(x, r, y), is positive. note that this is different from hindle (1990) where even the
cases of negative pmi were considered. church and hanks (1990) showed that it is
hard to accurately predict negative word association ratios with con   dence, and so,
co-occurrence pairs with negative pmi are ignored. the measure gives similarity scores
from 0 (maximally dissimilar) to 1 (maximally similar).

like hindle   s measure, lin   s is a measure of distributional similarity. however,
it distinguishes itself from that of hindle in two respects. first, lin normalizes the
similarity score between two words (numerator of (12)) by their cumulative strengths
of association with the rest of the co-occurring words (denominator of (12)). this is a
signi   cant improvement as now high pmi of the target words with shared co-occurring
words alone does not guarantee a high distributional similarity score. as an additional
requirement, the target words must have low pmi with words they do not both co-
occur with. second, hindle uses the minimum of the pmi between each of the target
words and the shared co-occurring word, while lin uses the sum. taking the sum has
the drawback of not penalizing for a mismatch in strength of co-occurrence, as long as
w1 and w2 both co-occur with a word.

hindle (1990) used a portion of the associated press news stories (6 million words) to
classify the nouns into semantically related classes. lin (1998b) used his measure to gen-
erate a distributional thesaurus from a 64-million-word corpus of the wall street journal,
san jose mercury, and ap newswire. he also provides a framework for evaluating such
automatically generated thesauri by comparing them with id138-based and roget-
based thesauri. he shows that the distributional thesaurus created with his measure
is closer to the id138 and roget-based thesauri than that created using hindle   s
measure.

3.2.3 relative id178   based measures: kld, asd, jsd.

13

id181. given two id203 mass functions p(x) and q(x), their
relative id178 d(pkq) is:

d(pkq) =    
x   x

p(x) log

p(x)
q(x)

for q(x) 6= 0

(13)

intuitively, if p(x) is the accurate id203 mass function corresponding to a random
variable x, then d(pkq) is the information lost when approximating p(x) by q(x). in
other words, d(pkq) is indicative of how different the two distributions are. relative
id178 is also called the id181 or the kullback-leibler dis-
tance (denoted by kld).

pereira, tishby, and lee (1993) and dagan, lee, and pereira (1994) point out that
words have probabilistic distributions with respect to neighboring syntactically related
words. for example, there exists a certain probabilistic distribution (d1(p(v|n1)), say) of
a particular noun n1 being the object of any verb. this distribution can be estimated
by corpus counts of parsed or chunked text. let d2 (p(v|n2)) be the corresponding
distribution for noun n2. these distributions (d1 and d2) de   ne the contexts of the two
nouns (n1 and n2, respectively). as per the distributional hypothesis, the more these
contexts are similar, the more n1 and n2 are semantically similar. thus the kullback-
leibler distance between the two distributions is indicative of the semantic distance
between the nouns n1 and n2.

kld(n1, n2) = d(d1kd2)

=    v   vb p(v|n1) log p(v|n1)
p(v|n2)
=    v   vb   (n1)   vb   (n2) p(v|n1) log p(v|n1)

for p(v|n2) 6= 0
p(v|n2) for p(v|n2) 6= 0

(14)

where vb is the set of all verbs and vb   (x) is the set of verbs that have x as the object.
note again that the set of co-occurring words used is restricted to include only verbs
that each have the same syntactic relation (verb   object) with both target nouns. this too
is therefore a measure that mimics semantic similarity and not semantic relatedness.

it should be noted that the verb   object relationship is not inherent to the measure
and that one or more of any other syntactic relations may be used. one may also
estimate semantic relatedness by using all words co-occurring with the target words.
thus a more generic expression of the id181 is as follows:

kld(w1, w2) = d(d1kd2)

=    w   v p(w|w1) log p(w|w1)
p(w|w2)
=    w   c(w1)   c(w2) p(w|w1) log p(w|w1)

for p(w|w2) 6= 0
p(w|w2) for p(w|w2) 6= 0

(15)

where v is the vocabulary (all the words found in a corpus). c(w), as mentioned earlier,
is the set of words occurring (within a certain window) with word w.

it should be noted that the kullback-leibler distance is not symmetric; that is, the
distance from w1 to w2 is not necessarily, and even not likely, the same as the distance
from w2 to w1. this asymmetry is counterintuitive to the general notion of semantic
similarity of words, although weeds (2003) has argued in favor of asymmetric measures.
further, it is very likely that there are instances such that p(w1|v) is greater than 0 for a
particular verb v, while due to data sparseness or grammatical and semantic constraints,

14

mohammad and hirst

distributional measures of semantic distance

the training data has no sentence where v has the object w2. this makes p(w2|v) equal
to 0 and the ratio of the two probabilities in   nite. id181 is not
de   ned in such cases, but approximations may be made by considering smoothed
values for the denominator.

pereira, tishby, and lee (1993) used kld to create clusters of nouns from verb-
object pairs corresponding to the thousand most frequent nouns in the grolier   s ency-
clopedia, june 1991 version (10 million words). dagan, lee, and pereira (1994) used kld
to estimate the probabilities of bigrams that were not seen in a text corpus. they point
out that a signi   cant number of possible bigrams are not seen in any given text corpus.
the probabilities of such bigrams may be determined by taking a weighted average
of the probabilities of bigrams composed of distributionally similar words. use of
kullback-leibler distance as the semantic distance metric yielded a 20% improvement
in perplexity on the wall street journal and dictation corpora provided by arpa   s hlt
program (paul 1991).

it should be noted here that the use of distributionally similar words to estimate
unseen bigram probabilities will likely lead to erroneous results in case of less-preferred
and strongly-preferred collocations (word pairs). inkpen and hirst (2002) point out
that even though words like task and job are semantically very similar, the collocations
they form with other words may have varying degrees of usage. while daunting task is
a strongly-preferred collocation, daunting job is rarely used. thus using the id203
of one bigram to estimate that of another will not be bene   cial in such cases.

  -skew divergence. the   -skew divergence (asd) is a slight modi   cation of the kullback-
leibler divergence that obviates the need for smoothed probabilities. it has the follow-
ing formula:

asd(w1, w2) =

   

p(w|w1) log

w   c(w1)   c(w2)

p(w|w1)

  p(w|w2) + (1       )p(w|w1)

(16)

where    is a parameter that may be varied but is usually set to 0.99. note that the
denominator within the logarithm is never zero with a non-zero numerator. also, the
measure retains the asymmetric nature of the id181. lee (2001)
shows that   -skew divergence performs better than id181 in
estimating word co-occurrence probabilities. weeds (2003) achieves a correlation of
0.48 and 0.26 with human judgment on the miller and charles word pairs using
asd(w1, w2) and asd(w2, w1), respectively.

jensen-shannon divergence. a relative id178   based measure that overcomes the prob-
lem of asymmetry in id181 is the jensen-shannon divergence
a.k.a. total divergence to the average a.k.a. information radius. it is denoted by jsd
and has the following formula:

jsd(w1, w2) = d(cid:18)d1k

1
2

(d1 + d2)(cid:19) + d(cid:18)d2k
w   c(w1)   c(w2) p(w|w1) log

   

=

1
2

(d1 + d2)(cid:19)

p(w|w1)

1
2 (p(w|w1) + p(w|w2))

+

(17)

15

p(w|w2) log

p(w|w2)

2 (p(w|w1) + p(w|w2))!

1

(18)

the jensen-shannon divergence is the sum of the id181 between
each of the individual co-occurrence distributions d1 and d2 of the target words with
the average distribution ( d1+d2
). further, it can be shown that the jensen-shannon
divergence avoids the problem of zero denominator. the jensen-shannon divergence
is therefore always well de   ned and, like   -skew divergence, obviates the need for
smoothed estimates.

2

the id181,   -skew divergence, and jensen-shannon diver-
gence all give distributional distance scores from 0 (synonymous) to in   nity (unrelated).

3.2.4 latent semantic analysis. landauer, foltz, and laham (1998) proposed latent
semantic analysis (lsa), which can be used to determine distributional distance be-
tween words or between sets of words.8 unlike the various approaches described earlier
where a word   word co-occurrence matrix is created, the    rst step of lsa involves the
creation of a word   paragraph, word   document, or similar such word-passage matrix,
where a passage is some grouping of words. a cell for word w and passage p is populated
with the number of times w occurs in p or, for even better results, a function of this
frequency that captures how much information the occurrence of the word in a text
passage carries.

next,

the dimensionality of

this matrix is reduced by applying singular
value decomposition (svd), a standard matrix decomposition technique. this
smaller set of dimensions represents abstract (unknown) concepts. then the origi-
nal word   passage matrix is recreated, but this time from the reduced dimensions.
landauer, foltz, and laham (1998) point out that this results in new matrix cell val-
ues that are different from what they were before. more speci   cally, words that are
expected to occur more often in a passage than what the original cell values re   ect are
incremented. then a standard vector distance measure, such as cosine, that captures the
distance between distributions of the two target words is applied.

lsa was used by sch  tze and pedersen (1997), turney (2001) and rapp (2003) to
measure distributional distance, with encouraging results. however, there is no non-
heuristic way to determine when the dimension reduction should stop. further, the
generic concepts represented by the reduced dimensions are not interpretable; that is,
one cannot determine which concepts they represent in a given sense inventory. this
means that lsa cannot directly be used for tasks such as unsupervised sense disam-
biguation or estimating semantic similarity of known concepts. lsa is computation-
ally expensive as singular value decomposition, a key component for dimensionality
reduction, requires computationally intensive matrix operations. this makes lsa less
scalable to large amounts of text (gorman and curran 2006). finally, it too, like other
distributional word-distance measures con   ates the many senses of a word (see section
4.6.1 ahead for more discussion on sense con   ation).

3.2.5 co-occurrence retrieval models. the distributional measures suggested by
weeds (2003) are based on a notion of substitutability: the more appropriate it is to

8 landauer, foltz, and laham (1998) describe it as a measure of similarity, but in fact it is a distributional

measure that mimics semantic relatedness.

16

mohammad and hirst

distributional measures of semantic distance

substitute word w1 in place of word w2 in a suitable natural language task, the more
semantically similar they are. she uses co-occurrence retrieval (the retrieval of words
that co-occur with a target word from text) to determine the degree to which one word
is substitutable by another. the degree of substitutability of w2 with w1 is dependent
on how the proportion of co-occurrences of w1 that are also co-occurrences of w2 and
the proportion of co-occurrences of w2 that are also co-occurrences of w1. thus weeds   s
distributional measures have a precision component and a recall component (which
may or may not incorporate the strength of co-occurrence association). the    nal score is
a weighted sum of the precision, recall, and standard f measure (see equation (19)9).
the weights determine the importance of precision and recall and are determined
empirically. if precision and recall are equally important, then it results in a symmetric
measure which gives identical scores for the distributional similarity of w1 with w2 and
w2 with w1. otherwise, we get an asymmetric measure which assigns different scores to
the two cases.

crm(w1, w2) =   " 2    p    r

p + r # + (1       )"  [p] + (1       )[r]#

(19)

   and    are tuned parameters that lie between 0 and 1.

both precision and recall can be considered as the product of a core formula (de-
noted by core) and a penalty function (denoted by penalty). weeds03 provides six (three
times two) distinct formulae for precision and recall, depending on the strength of co-
occurrence (three alternatives) and whether or not a penalty is applied for differences
in strength of association of common co-occurring words (two alternatives).

depending on the strength of association, the crms are classi   ed as type-based,
token-based, and mutual information   based. the crms that use simple counts of the
common co-occurrences and not the strength of associations as core precision and recall
values are called type-based crms (denoted by the superscript type). the crms that
use conditional probabilities of the shared co-occurring words with the target words
are called token-based crms (denoted by the superscript token). the crms that use
pointwise mutual information of the shared co-occurring words with target words
are called mutual information   based crms (denoted by the superscript mi). the core
precision and recall formulae for type, token, and mutual information   based crms are
listed below:

core

type
p

(w1, w2) =

core

type
r

(w1, w2) =

| c(w1)     c(w2) |

| c(w1) |

| c(w1)     c(w2) |

| c(w2) |

coretoken

p

(w1, w2) =

   

p(w|w1)

w   c(w1)   c(w2)

coretoken

r

(w1, w2) =

   

p(w|w2)

w   c(w1)   c(w2)

9 p is short for p(w1, w2), while r is short for r(w1, w2). the abbreviations are made due to space

constraints and to improve readability.

(20)

(21)

(22)

(23)

17

coremi
p

(w1, w2) =

coremi
r

(w1, w2) =

   w   c(w1)   c(w2) i(w, w1)

   w   c(w1) i(w, w1)

   w   c(w1)   c(w2) i(w, w2)

   w   c(w2) i(w, w2)

(24)

(25)

where c(x) is the set of words that co-occur with x.

depending on the penalty function, the crms are classi   ed as additive and
difference-weighted. the crms that do not penalize difference in strength of co-
occurrence are called additive crms (denoted by the subscript add); those that do pe-
nalize are called difference-weighted crms (subscript dw). the penalty is a conditional
id203   based function (26, 27) for the token- and type-based crms, and a mutual
information   based function (28, 29) for the mutual information   based crm.

penaltytype

p = penaltytoken

p =

penaltytype

r = penaltytoken

r =

min(p(w|w1), p(w|w2))

p(w|w1)

min(p(w|w1), p(w|w2))

p(w|w2)

penaltymi

p =

penaltymi

r =

min(i(w, w1), i(w, w2))

i(w, w1)

min(i(w, w1), i(w, w2))

i(w, w2)

the six pairs of precision and recall difference-weighted crms are thus as follows:

ptype
add (w1, w2) =

rtype

add (w1, w2) =

| c(w1)     c(w2) |

| c(w1) |

| c(w1)     c(w2) |

| c(w2) |

ptype
dw (w1, w2) =

rtype
dw (w1, w2) =

   |c(w1)   c(w2)|

min(p(w|w1),p(w|w2))

p(w|w1)

| c(w1) |

   |c(w1)   c(w2)|

min(p(w|w1),p(w|w2))

p(w|w2)

| c(w2) |

ptoken
add (w1, w2) =

   

p(w|w1)

w   c(w1)   c(w2)

rtoken

add (w1, w2) =

   

p(w|w2)

w   c(w1)   c(w2)

ptoken
dw (w1, w2) =

   

min(p(w|w1), p(w|w2))

w   c(w1)   c(w2)

18

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

mohammad and hirst

distributional measures of semantic distance

rtoken

dw (w1, w2) =

   

min(p(w|w2), p(w|w1))

(37)

w   c(w1)   c(w2)

pmi
add(w1, w2) =

rmi

add(w1, w2) =

   w   c(w1)   c(w2) i(w, w1)

   w   c(w1) i(w, w1)

   w   c(w1)   c(w2) i(w, w2)

   w   c(w2) i(w, w2)

pmi
dw(w1, w2) =

rmi

dw(w1, w2) =

   w   c(w1)   c(w2) min(i(w, w1), i(w, w2))

   w   c(w1) i(w, w1)

   w   c(w1)   c(w2) min(i(w, w1), i(w, w2))

   w   c(w2) i(w, w2)

(38)

(39)

(40)

(41)

note that in case of the difference-weighted token and mutual information   based pre-
cision and recall formulae, there is a cancellation of a pair of terms obtained from the
core formulae and the penalty.

asymmetry in substitutability is intuitive, as in many cases it may be acceptable to
substitute a word, say dog, with another, say animal, but the reverse is not acceptable as
often. since weeds uses substitutability as a measure of semantic similarity, she believes
that distributional similarity between two words should re   ect this property as well.
hence, like the id181, all her distributional similarity models are
asymmetric.

weeds (2003) extracted verb   object pairs of 2,000 nouns from the british national
corpus (bnc). the verbs related to the target words by the verb   object relation were
used. thus each of the co-occurring verbs is related to the target nouns by the same
syntactic relation and therefore the measures mimic semantic similarity, not relat-
edness. correlation with human judgment (miller and charles word pairs) showed
that difference-weighted (r = 0.61) and additive mutual information   based measures
(r = 0.62) performed far better than the other crms.

4. the anatomy of a distributional measure

even though there are numerous distributional measures, many of which may seem
dramatically different from each other, all distributional measures perform two func-
tions: (1) create distributional pro   les (dps), and (2) calculate the distance between
two dps.

the distributional pro   le of a word is the strength of association between it and each
of the lexical, syntactic, and/or semantic units that co-occur with it. commonly used
measures of strength of association are id155 (0 to 1) and pointwise
mutual information (       to    ). commonly used units of co-occurrence with the target
are other words, and so we speak of the lexical distributional pro   le of a word (lexical
dpw). the co-occurring words may be all those in a predetermined window around
the target, or may be restricted to those that have a certain syntactic (e.g., verb   object)
or semantic (e.g., agent   theme) relation with the target word. we will refer to the former
kind of dps as relation-free. usually in the latter case, separate association values are
calculated for each of the different relations between the target and the co-occurring

19

table 3
measures of dp distance, measures of strength of association, and standard combinations.
measures of strength of association that are traditionally used are marked in bold. the use of
other measures of association remains to be explored.

measures of dp distance
  -skew divergence (asd)
cosine (cos)
dice coef   cient (dice)
euclidean distance (l2 norm)
hindle   s measure (hin)
id181 (kld)
manhattan distance (l1 norm)
jensen   shannon divergence (jsd)
lin   s measure (lin)

measures of strength of association
   coef   cient (phi)
id155 (cp)
cosine (cos)
dice coef   cient (dice)
odds ratio (odds)
pointwise mutual information (pmi)
yule   s coef   cient (yule)

standard combinations
  -skew divergence with    coef   cient (asd   cp)
cosine with id155 (cos   cp)
dice coef   cient with id155 (dice   cp)
euclidean distance with id155 (l2 norm   cp)
hindle   s measure with pointwise mutual information (hin   pmi)
id181 with id155 (kld   cp)
manhattan distance with id155 (l1 norm   cp)
jensen   shannon divergence with id155 (jsd   cp)
lin   s measure with pointwise mutual information (lin   pmi)

units. we will refer to such dps as relation-constrained. typical relation-free dps
are those of sch  tze and pedersen (1997) and yoshida, yukawa, and kuwabara (2003).
typical relation-constrained dps are those of lin (1998a) and lee (2001). below are con-
trived, but plausible, examples of each for the word pulse; the numbers are conditional
probabilities:

relation-free dp
pulse: beat .28, racing .2, grow .13, beans .09, heart .04, . . .

relation-constrained dp
pulse: hbeat, subject   verbi .34, hracing, noun   qualifying adjectivei .22, hgrow,
subject   verbi .14, . . .

since the dps represent the contexts of the two target words, the distance between
the dps is the distributional distance and, as per the distributional hypothesis, a proxy
for semantic distance. a measure of dp distance, such as cosine, calculates the distance
between two distributional pro   les. while any of the measures of dp distance may be
used with any of the measures of strength of association, in practice only certain combi-
nations are used (see table 3) and certain other combinations might not be meaningful
(for example, id181 with    coef   cient). observe from table 3 that

20

mohammad and hirst

distributional measures of semantic distance

all standard-combination distributional measures (or at least those that are described in
this paper) use either id155 or pmi as the measure of association.

4.1 simple co-occurrences versus syntactically related words

harris (1968), one of the early proponents of the distributional hypothesis, used syn-
tactically related words to represent the context of a word. however, the strength
of association of any word appearing in the context of the target words may
be used to determine their distributional similarity. dagan, lee, and pereira (1997),
lee (1999), and weeds (2003) represent the context of a noun with verbs whose ob-
ject it is (single syntactic relation), hindle (1990) represents the context of a noun
with verbs with which it shares the verb-object or subject-verb relation, while
lin (1998b) uses words related to a noun by any of the many pre-decided syntac-
tic relations to determine distributional similarity. sch  tze and pedersen (1997) and
yoshida, yukawa, and kuwabara (2003) use all co-occurring words in a pre-decided
window size. although lin (1998b) shows that the use of multiple syntactic relations
is more bene   cial as compared to just one, mccarthy et al. (2007) show that results
obtained using just word co-occurrences produced almost as good results as those
obtained using syntactically related words. further, use of syntactically related words
entails the requirement of chunking or parsing the data.

4.2 compositionality

the various measures of distributional similarity may be divided into two kinds as per
their composition. in certain measures, each co-occurring word contributes to some
   nite calculable distributional distance between the target words. the    nal score of
distributional distance is the sum of these contributions. we will call such measures
compositional measures. the relative id178   based measures, l1 norm and l2 norm,
fall in this category. on the other hand, the cosine measure, along with hindle   s and
lin   s mutual information   based measures, belong to the category of what we call
non-compositional measures. each co-occurring word shared by both target words
contributes a score to the numerator and the denominator of the measures    formula.
words that co-occur with just one of the two target words contribute scores only to the
denominator. the ratio is calculated once all co-occurring words are considered. thus
the distributional distance contributed by individual co-occurrences is not calculable
and the    nal semantic distance cannot be broken down into compositional distances
contributed by each of the co-occurrences. it is not clear as to which of the two kinds
of measures (compositional or non-compositional) resembles human judgment more
closely and how much they differ in their ranking of word pairs.

4.2.1 primary compositional measures. the compositional measures of distributional
similarity (or relatedness) capture the contribution to distance between the target words
(w1 and w2) due to a co-occurring word by three primary mathematical manipulations
of the co-occurrence distributions (d1 and d2): the difference, denoted by dif (as in
l1 norm), division, denoted by div (as in the relative id178   based measures), and
product, denoted by pdt (as in the id155 or mutual information   
based cosine method). we will call the three types of compositional measures primary
compositional measures (pcm). their form is depicted below:

21

dif =

   

|p(w|w1)     p(w|w2)|

w   c(w1)   c(w2)

div =

pdt =

   

w   c(w1)   c(w2)(cid:12)(cid:12)(cid:12)(cid:12)

   

w   c(w1)   c(w2)

log

p(w|w1)

p(w|w2)(cid:12)(cid:12)(cid:12)(cid:12)

p(w|w1)    p(w|w2)

scaling factor

(42)

(43)

(44)

observe that by taking absolute values in expressions (42) and (43), the variation in the
distributions for different co-occurring words has an additive affect rather than one of
cancellation. this corresponds to our distributional hypothesis     the more the disparity
in distributions, the more is the semantic distance between the target words. the prod-
uct form (44) also achieves this and is based on this theorem: the product of any two
numbers will always be less than or equal to the square of their average. in other words,
the more two numbers are close to each other in value, the higher is the ratio of their
product to a suitable scaling factor (for example, the square of their average). note that
the difference and division measures give higher values when there is large disparity
between the strength of association of co-occurring words with the target words. they
are therefore measures of distributional distance and not distributional similarity. the
product method gives higher values when the strengths of association are closer, and is
a measure of distributional relatedness.

although all three methods seem intuitive, each produces different distributional
similarity values and more importantly, given a set of word pairs, each is likely to rank
them differently. for example, consider the division and difference expressions applied
to word pairs (w1, w2) and (w3, w4). for simplicity, let there be just one word w    in the
context of all the words. given:

p(w   |w1) = 0.91
p(w   |w2) = 0.80
p(w   |w3) = 0.60
p(w   |w4) = 0.50

the distributional distance between word pairs as per the difference pcm:

dif (w1, w2) = |0.91     0.8| = 0.11

dif (w3, w4) = |0.6     0.5| = 0.1

the distributional distance between word pairs as per the division pcm:

div(w1, w2) = (cid:12)(cid:12)(cid:12)(cid:12)

log

0.91

0.8 (cid:12)(cid:12)(cid:12)(cid:12)

= 0.056

22

mohammad and hirst

distributional measures of semantic distance

div(w3, w4) = (cid:12)(cid:12)(cid:12)(cid:12)

0.6

0.5(cid:12)(cid:12)(cid:12)(cid:12)

log

= 0.079

observe that for the same set of co-occurrence probabilities, the difference-based mea-
sure ranks the (w3, w4) pair more distributionally similar (lower distributional distance),
while the division-based measure gives lower distributional similarity values for word
pairs having large co-occurrence probabilities. this behavior is not intuitive and it
remains to be seen, by experimentation, as to which of the three, difference, division or
product, yields distributional similarity measures closest to human notions of semantic
similarity.

the l1 norm is a basic implementation of the difference method. a simple product-

based measure of distributional similarity is as proposed below:

pdtavg(w1, w2) =

   

w   c(w1)   c(w2)

p(w|w1)    p(w|w2)

( 1
2 (p(w|w1) + p(w|w2)))2

(45)

the scaling factor used is the square of the average id203. it can be proved
that if the sum of two variables is equal to a constant (k, say), their values must
be equal to k/2 in order to get the largest product. now, let k be equal to the sum
of p(w|w1)/(p(w|w1) + p(w|w2)) and p(w|w2)/(p(w|w1) + p(w|w2)). this sum will
always be equal to 1 and hence the product (z) will be largest only when the two
numbers are equal i.e. p(w|w1) is equal to p(w|w2). in other words, the farther p(w|w1)
and p(w|w2) are from their average, the smaller is the product z. therefore, the measure
gives high scores for low disparity in strengths of co-occurrence and low scores other-
wise. the incorporation of 1/2 in the scaling factor results in a measure that ranges
between 0 and 1.

the relative id178   based methods use a weighted division method. observe that
both id181 (formula repeated below for convenience     equation
(46)) and jensen-shannon divergence do not take absolute values of the division of co-
occurrence probabilities. this will mean that if p(w|w1) > p(w|w2), the logarithm of
their ratio will be positive and if p(w|w1) < p(w|w2), the logarithm will be a negative
number. therefore, there will be a cancellation of contributions to distributional distance
by words that have higher co-occurrence id203 with respect to w1 and words that
have a higher co-occurrence id203 with respect to w2. observe however that the
weight p(w|w1) multiplying the logarithm means that in general the positive logarithm
values receive higher weight than the negative ones, resulting in a net positive score.
therefore, with no absolute value of the logarithm, as in the kld, the weight plays
a crucial role. a modi   ed id181 (dabs) which incorporates the
absolute value is suggested in equation (47):10

kld(w1, w2) = d(d1kd2) =

   

p(w|w1) log

w   c(w1)   c(w2)

p(w|w1)
p(w|w2)

(46)

10 it should be noted that any changes to the formula for id181 means that the

resulting measure is no longer id181; these measures are denoted by kld (and a
suitable subscript and/or superscript simply to indicate that they are derived from the kullback-leibler
divergence.

23

kldabs(w1, w2) = dabs(d1kd2) =

   

w   c(w1)   c(w2)

log

p(w|w1)(cid:12)(cid:12)(cid:12)(cid:12)

p(w|w1)

p(w|w2)(cid:12)(cid:12)(cid:12)(cid:12)

(47)

the updated jensen-shannon divergence measure will remain the same as in equation
(17), except that it is a manipulation of dabs and not the original kullback-leibler
divergence (relative id178).

jsdabs(w1, w2) = dabs(d1k

1
2

(d1 + d2)) + dabs(d2k

1
2

(d1 + d2))

(48)

note that once the absolute value of the logarithm is taken, it no longer makes much
sense to use an asymmetric weight (p(w|w1)) as in the kld or as necessary to use a
weight at all. equation (49) shows a simple division-based measure. it is an unweighted
form of kldabs(w1, w2) and so we will call it kldabs
unw.

kldabs

unw(w1, w2) = div(w1, w2) =

(49)

log

   

w   c(w1)   c(w2)(cid:12)(cid:12)(cid:12)(cid:12)

p(w|w1)

p(w|w2)(cid:12)(cid:12)(cid:12)(cid:12)

experimental evaluation of these suggested modi   cations of kullback-leibler diver-
gence will be interesting.

4.2.2 weighting the pcms. the performance of the primary compositional measures
may be improved by adding suitable weights to the distributional distance contributed
by each co-occurrence. the idea is that some co-occurrences may be better indicators
of semantic distance than others. usually, a formulation of the strength of association
of the co-occurring word with the target words is used as weight, the hypothesis being
that a strong co-occurrence is likely to be a strong indicator of semantic closeness.

weighting the primary compositional measures results in some of the existing mea-
sures. for example, as pointed out earlier, the id181 is a weighted
form of the division measure (not considering the absolute value). here, the conditional
id203 of a co-occurring word with respect to the    rst word (p(w|w1)) is used as the
weight. since the absolute value of the logarithm is not taken and because the weight
(p(w|w1)) is dependent on the    rst word and not the other, id181
is asymmetric. below is a symmetric weight function:

weightavgwt(w1, w2) =

1
2

(p(w|w1) + p(w|w2))

(50)

l2 norm is a weighted version of the l1 norm, the weight being p(w|w1)     p(w|w2). a
simple product measure with weights is shown below:

pdtavg

avgwt =

   

w   c(w1)   c(w2)

1
2

(p(w|w1) + p(w|w2))

p(w|w1)    p(w|w2)

( 1
2 (p(w|w1) + p(w|w2)))2

=

   

w   c(w1)   c(w2)

p(w|w1)    p(w|w2)
1
2 (p(w|w1) + p(w|w2))

(51)

24

mohammad and hirst

distributional measures of semantic distance

a possibly better weight function (which is also symmetric) hinges on the following
hypothesis: the stronger the association of a co-occurring word with a target word, the
better the indicator of semantic properties of the target word it is. equation (52) shows
the corresponding weight function:

weightmaxwt(w1, w2) =

max (p(w|w1), p(w|w2))

   w      c(w1)   c(w2) max (p(w   |w1), p(w   |w2))

(52)

likely to have different strengths of associations
the co-occurring word is
with the two target words. taking the maximum of
the two as the weight
(dagan, marcus, and markovitch (1995)) will mean that more weight is given to a co-
occurring word if it has high strength of association with any of the two target words.
as dagan, marcus, and markovitch (1995) point out, there is strong evidence for dis-
similarity if the strength of association with the other target word is much lower than
the maximum, and strong evidence of similarity if the strength of association with both
target words is more or less the same.

4.3 predictors of semantic relatedness

given a pair of target words, the vocabulary may be divided into three sets: (1) the set
of words that co-occur with both target words (common); (2) words that co-occur with
exactly one of the two target words (exclusive); (3) words that do not co-occur with
either of the two target words. hindle (1990) uses evidence only from words that co-
occur with both target words to determine the distributional similarity. all the other
measures discussed in this paper so far also use words that co-occur with just one target
word.

one can argue that the more there are common co-occurrences between two words,
the more they are related. for example, drink and sip may be considered related as they
have a number of common co-occurrences such as water, tea and so on. similarly, drink
and chess can be deemed unrelated as words that co-occur with one do not with the
other. for example, water and tea do not usually co-occur with chess, while castle and
move are not found close to drink. measures that use all co-occurrences (common and ex-
clusive) tap into this intuitive notion. however, certain strong exclusive co-occurrences
can adversely effect the measure. consider the classic strong coffee vs powerful coffee
example (halliday (1966)). the words strong and powerful are semantically very related.
however, the word coffee is likely to co-occur with strong but not with powerful. further,
strong and coffee can be expected to have a large value of association as given by a
suitable measure, say pmi. this large pmi value, if used in the distributional relatedness
formula, can greatly reduce the    nal value. thus it is not clear if the bene   t of using all
co-occurrences is outweighed by the drawback pointed out.

a further advantage of using only common co-occurrences is that the kullback-

leibler divergence can now be used without the need of smoothed probabilities.

kldcom(w1, w2) =

   

p(w|w1) log

w   c(w1)   c(w2)

p(w|w1)
p(w|w2)

(53)

observe that we are taking the intersection of the set of co-occurring words instead of
union as in the original formula (15).

25

4.4 capitalizing on asymmetry

given a hypernym-hyponym pair (automobile-car, say) asymmetric distributional mea-
sures such as the id181,    skew divergence, and the crms
generate different values as the distributional distance of w1 with w2 as compared
to that of w2 with w1. usually, if w1 is a more generic concept than w2, the mea-
sures    nd w1 to be more distributionally similar to w2 than the other way round (see
(mirkin, dagan, and geffet 2007) for work on lexical entailment using the kullback-
leibler divergence). weeds (2003) argues that this behavior is intuitive as it is more often
acceptable to substitute a generic concept in place of a speci   c one than vice versa, and
substitutability is a indicator of semantic similarity.

on the other hand, in most cases the notion of asymmetric semantic similarity is
counterintuitive, and possibly detrimental. in many natural language tasks, one needs
the distance between two words and there is no order information. further, in case two
words share a hypernym-hyponym relation, they are likely to be highly semantically
similar. thus given two words, it may make sense to always choose the higher of the
two distributional similarity values suggested by an asymmetric measure as the    nal
distributional similarity between the two. this way an asymmetric measure (simasym)
can easily be converted into a symmetric one (simmax), while still capitalizing on the
asymmetry to generate more suitable distributional distance values for hypernym-
hyponym word pairs. equation (54) states the formula for the proposed conversion.
a speci   c implementation of the kl divergence formula is given in equation (55):

simmax(w1, w2) = max(simasym(w1, w2), simasym(w2, w1))

kldmax(w1, w2) = max(kld(w1, w2), kld(w2, w1))

(54)

(55)

another way to convert an asymmetric measure of distributional distance into a sym-
metric one is by taking the average (formula 56) of the two possible similarity values.
a speci   c implementation on the kl divergence formula is given in equations (57)
through (60):

simavg(w1, w2) =

kldavg(w1, w2) =

1
2
1
2

(simasym(w1, w2) + simasym(w2, w1))

(kld(w1, w2) + kld(w2, w1))

=

=

=

1
2

1
2

1
2

   

w   c(w1)   c(w2)(cid:18)p(w|w1) log
w   c(w1)   c(w2)(cid:18)p(w|w1) log

   

p(w|w1)
p(w|w2)

p(w|w1)
p(w|w2)

+ p(w|w2) log

    p(w|w2) log

p(w|w2)

p(w|w1)(cid:19)
p(w|w2)(cid:19)

p(w|w1)

   

(p(w|w1)     p(w|w2)) log

w   c(w1)   c(w2)

p(w|w1)
p(w|w2)

(56)

(57)

(58)

(59)

(60)

4.5 summarizing the distributional measures

table 4 summarizes the properties of various distributional measures discussed in this
paper.

26

table 4: distributional measures and their properties.

distributional measure
measure

type

compo-
sitional

asd

cos

distance

closeness

crms

closeness

dicecp

closeness

dif or l1

distance

div

distance

(cid:8)

x

x

x

(cid:8)

(cid:8)

pcm

division

n.a.

n.a.

n.a.

formula

   w   c(w1)   c(w2) p(w|w1) log

p(w|w1)

  p(w|w2)+(1     )p(w|w1)

   w   c(w1)   c(w2)(p(w|w1)  p(w|w2))

q   w   c(w1) p(w|w1)2  q   w   c(w2) p(w|w2)2
  (cid:20) 2  p  r
p+r (cid:21) + (1       )(cid:20)  [p] + (1       )[r](cid:21)

2     w   c(w1)   c(w2) min(p(w|w1),p(w|w2))

   w   c(w1) p(w|w1)+   w   c(w2) p(w|w2)

difference

   w   c(w1)   c(w2) | p(w|w1)     p(w|w2) |

division

hindle

closeness

x

n.a.

   w   c(w)

jaccardcp

closeness

x

n.a.

2
7

note: for measures that are not compositional, the type of pcm is not applicable.

   w   c(w1)   c(w2)(cid:12)(cid:12)(cid:12)

log p(w|w1)

p(w|w2)(cid:12)(cid:12)(cid:12)

min(i(w, w1), i(w, w2)),

if i(w, w1) > 0 and i(w, w2) > 0

| max(i(w, w1), i(w, w2)) |,

if i(w, w1) < 0 and i(w, w2) < 0

0,

otherwise

   w   c(w1)   c(w2) min(p(w|w1),p(w|w2))
   w   c(w1)   c(w2) max(p(w|w1),p(w|w2))

                        
                     

symm-

etric

strength of
association

x

(cid:8)

x

(cid:8)

(cid:8)

(cid:8)

(cid:8)

(cid:8)

cp

cp

both

cp

cp

cp

pmi

cp

m
o
h
a
m
m
a
d
a
n
d
h

i
r
s
t

d
i
s
t
r
i
b
u
t
i
o
n
a
l

m
e
a
s
u
r
e
s
o
f
s
e
m
a
n
t
i
c
d
i
s
t
a
n
c
e

2
8

distributional measures and their properties (continued).

distributional measure
measure

type

compo-
sitional

pcm

jsd

distance

(cid:8)

division

kld

distance

kldcom

distance

kldabs

distance

kldavg

distance

kldmax

distance

l2

lin

distance

closeness

pdtavg

closeness

pdtavg

avgwt

closeness

(cid:8)

(cid:8)

(cid:8)

(cid:8)

(cid:8)

(cid:8)

x

(cid:8)

(cid:8)

div.

div.

div.

div.

div.

difference

n.a.

pdt.

pdt.

formula

symm-

etric

strength of
association

   w   c(w1)   c(w2)(cid:16)p(w|w1) log

p(w|w1)

1
2 (p(w|w1)+p(w|w2))

+

p(w|w2)

p(w|w2) log

2 (p(w|w1)+p(w|w2))(cid:17)
   w   c(w1)   c(w2) p(w|w1) log p(w|w1)
p(w|w2)

1

   w   c(w1)   c(w2) p(w|w1) log p(w|w1)
p(w|w2)

   w   c(w1)   c(w2) p(w|w1)(cid:12)(cid:12)(cid:12)

log p(w|w1)

p(w|w2)(cid:12)(cid:12)(cid:12)

1

2    w   c(w1)   c(w2) (p(w|w1)     p(w|w2)) log p(w|w1)
p(w|w2)

max(kld(w1, w2), kld(w2, w1))

q   w   c(w1)   c(w2) (p (w|w1)     p (w|w2))2

   (r,w)     t(w1)     t(w2)(i(w1,r,w)+i(w2,r,w))

   (r,w   )     t(w1) i(w1,r,w   )+   (r,w      )     t(w2) i(w2,r,w      )

   w   c(w1)   c(w2)

p(w|w1)  p(w|w2)

( 1
2 (p(w|w1)+p(w|w2)))2

   w   c(w1)   c(w2)

p(w|w1)  p(w|w2)
1
2 (p(w|w1)+p(w|w2))

(cid:8)

x

x

x

(cid:8)

(cid:8)

(cid:8)

(cid:8)

(cid:8)

(cid:8)

cp

cp

cp

cp

cp

cp

cp

pmi

cp

cp

note: for measures that are not compositional, the type of pcm is not applicable.

mohammad and hirst

distributional measures of semantic distance

4.6 challenges
4.6.1 con   ation of word senses. the distributional hypothesis (firth 1957) states that
words that occur in similar contexts tend to be semantically close. but when words have
more than one sense, it is not at all clear what semantic distance between them actually
means. further, a word in each of its senses is likely to co-occur with different sets of
words. for example, bank in the financial institution sense is likely to co-occur with
interest, money, accounts, and so on, whereas the river bank sense might have words
such as river, erosion, and silt around it. since words that occur together in text tend
to refer to senses that are closest in meaning to one another, in most natural language
applications, what is needed is the distance between the closest senses of the two target
words. however, because distributional measures calculate distance from occurrences
of the target word in all its occurrences and hence all its senses, they fail to get the
desired result. also note that the id84 inherent to latent semantic
analysis, a special kind of distributional measure, has the effect of making the pre-
dominant senses of the words more dominant while de-emphasizing the other senses.
therefore, an lsa-based approach will also con   ate information from the different
senses, and even more emphasis will be placed on the predominant senses. given the
semantically close target nouns play and actor, for example, a distributional measure will
give a score that is some sort of a dominance-based average of the distances between
their senses. the noun play has the predominant sense of children   s recreation
(and not drama), so a distributional measure will tend to give the target pair a large
(and thus erroneous) distance score. id138-based measures do not suffer from this
problem as they give distance between concepts, not words.

4.6.2 lack of explicitly-encoded world knowledge and data sparseness. it is becoming
increasingly clear that more-accurate results can be achieved in a large number of
natural language tasks, including the estimation of semantic distance, by combining
corpus statistics with a knowledge source, such as a dictionary, published thesaurus, or
id138. this is because such knowledge sources capture semantic information about
concepts and, to some extent, world knowledge. for example, id138, as discussed
earlier, has an extensive is-a hierarchy. if it lists one concept, say german shepherd
as a hyponym of another, say dog, then we can be sure that the two are semantically
close. on the other hand, distributional measures do not have access to such explicitly
encoded information. further, unless the corpus used by a distributional measure has
suf   cient instances of german shepherd and dog, it will be unable to deem them
semantically close. since zipf   s law seems to hold even for the largest of corpora,
there will always be words that occur too few times to accurately determine their
distributional distance from others.

4.6.3 limitations shared with id138-based measures. in addition to the limitations
described above, which are unique to the knowledge-lean distributional measures, like
the knowledge-rich measures they also suffer from problems of requiring the calculation
of large distance matrices (as described in section 2.3.4 earlier) and the reluctance to
cross the language barrier (section 2.3.5).

5. a hybrid approach: distributional measures of concept-distance

so far we have looked at approaches that exploit the structure of a resource such as
id138, and corpus-based distributional approaches that make use of co-occurrence
statistics. a hybrid approach to semantic distance is one that reconciling the two,

29

combining the information about concepts, explicitly encoded in a linguistic re-
source, with the information about words, implicitly encoded in text by co-occurrence.
mohammad and hirst (2006b) and mohammad et al. (2007) have proposed one such
approach that combines corpus statistics with a published thesaurus to give the seman-
tic distance between concepts (rather than words).

5.1 the distributional hypothesis for concepts

as pointed out in section 4.6.1, words when used in different senses tend to keep
different    company" (co-occurring words). for example, consider the contrived but
plausible distributional pro   le of star:

star : space 0.21, movie 0.16, famous 0.15, light 0.12, constellation 0.11, heat 0.08, rich 0.07,
hydrogen 0.07, . . .

observe that it has words that co-occur both with star   s celestial body sense and star   s
celebrity sense. thus, it is clear that different senses of a word may have very different
distributional pro   les. using a single dp for the word will mean the union of those pro-
   les. while this might be useful for certain applications, mohammad and hirst (2006b)
argue that in a number of tasks (including estimating semantic distance), acquiring
different dps for the different senses is not only more intuitive, but also, as they show
through experiments, more useful. they show that the closer the distributional pro   les
of two concepts, the smaller is their semantic distance. below are example distributional
pro   les of two senses of star:

celestial body: space 0.36, light 0.27, constellation 0.11, hydrogen 0.07, . . .
celebrity: famous 0.24, movie 0.14, rich 0.14, fan 0.10, . . .

the values are the strength of association (usually pointwise mutual information or
id155) of the target concept with co-occurring words.

we have seen that creating distributional pro   les of words involves simple word   
word co-occurrence counts. the creation of dpcs, on the other hand, requires: (1) a con-
cept inventory that list all the concepts and words that refer to them, and (2) counts of
how often a concept co-occurs with a word in text. these two aspects will be discussed
in the next two sub-sections; however once created, any of the many distributional
measures can be used to estimate the distance between the dps of two target concepts
(just as in the case of traditional word-distance measures, where distributional measures
are used to estimate the distance between the dps of two target words). for example,
here is how mohammad and hirst adapt the formula for cosine (described earlier in
section 3.2.1) to estimate distributional distance between two concepts:

coscp(c1, c2) =

   w   c(c1)   c(c2) (p(w|c1)    p(w|c2))

q   w   c(c1) p(w|c1)2   q   w   c(c2) p(w|c2)2

(61)

c(x) is now the set of words that co-occur with concept x within a pre-determined
window. the conditional probabilities in the formula are taken from the distributional
pro   les of concepts.

30

mohammad and hirst

distributional measures of semantic distance

a

and

source

concept

suitable

inventory.

knowledge

5.1.1
mohammad and hirst (2006b) use the categories in the macquarie thesaurus, 812
in all, as very coarse-grained word senses or concepts, in contrast to approaches that
use id138 or other similarly    ne-grained sense inventories.11 their approach to
determining word   concept co-occurrence counts (described in the next sub-section)
requires a set of possibly ambiguous words that together unambiguously represent
each concept   for which a thesaurus is a natural choice. the use of categories in a
thesaurus as concepts means that this approach requires a concept   concept distance
matrix of size only 812    812   much smaller than (less than 0.01% of) the matrix
required by the id138-based and distributional measures.

5.1.2 estimating distributional pro   les of concepts. a word   category co-occurrence
matrix (wccm) is created having word types w as one dimension and thesaurus
categories c as another.

c1

c2 . . . cj

. . .
w1 m11 m12 . . . m1j . . .
w2 m21 m22 . . . m2j . . .
...
...
wi mi1 mi2 . . . mij . . .
...
. . .

. . .

. . .

...

...

...

...

...

...

the matrix is populated with co-occurrence counts from a large corpus. a particular cell
mij, corresponding to word wi and category or concept cj, is populated with the number
of times wi co-occurs (they use a window of   5 words) with any word that has cj as one
of its senses (i.e., wi co-occurs with any word listed under concept cj in the thesaurus).
this matrix, created after a    rst pass of the corpus, is called the base word   category
co-occurrence matrix (base wccm). a contingency table for any particular word w
and category c can be easily generated from the wccm by collapsing cells for all other
words and categories into one and summing up their frequencies.

c   c
w nwc nw  
  w n  c n    

a suitable statistic, such as pointwise mutual information or id155,
will then yield the strength of association between the word and the category.

as the base wccm is created from unannotated text, it will be noisy but nonethe-
less capture strong word   category co-occurrence associations reasonably accurately.
this is because the errors in determining the true category that a word co-occurs
with will be distributed thinly across a number of other categories. (for more discus-
sion of the general principle see resnik (1998).) a second pass of the corpus is made
and the base wccm is used to disambiguate the words in it. a new bootstrapped
wccm is created such that each cell mij, corresponding to word wi and concept cj,
is populated with the number of times wi co-occurs with any word used in sense cj.

11 it has been suggested for some time now that id138 is much too    ne-grained for certain natural

language applications (agirre and lopez de lacalle lekuona (2003) and citations therein).

31

mohammad and hirst (2006a) showed that this wccm, created after simple sense dis-
ambiguation, better captures word   concept co-occurrence values, and hence strengths
of association values, than the base wccm.

5.1.3 mimicking semantic relatedness and semantic similarity. the distributional
pro   les created by the above methodology are relation-free. this is because (1) all
co-occurring words (not just those that are related to the target by certain syntactic
relations) are used, and (2) the wccm, as described above, does not maintain separate
counts for the different syntactic relations between the target and co-occurring words.
thus, distributional measures that use these wccms will estimate semantic related-
ness between concepts. distributional measures that mimic semantic similarity, which
require relation-constrained dpcs, can easily be created from wccms that have rows
for each word   syntactic relation pair (rather than just for words).

5.1.4 performance. mohammad and hirst (2006b) evaluate this approach on two tasks:
ranking word pairs in order of their semantic distance and correcting real-word spelling
errors. on both tasks, distributional concept-distance measures markedly outperformed
distributional word-distance measures. the id138-based measures performed better
in the word-pair ranking task, but in the id147 task three of the four
distributional measures outperformed all id138-based measures except the jiang   
conrath measure. it should be noted, however, that these experiments evaluated only
semantic similarity of noun   noun pairs   for all other part-of-speech combinations and
semantic relatedness estimates, the id138-based measures are markedly less accu-
rate.

5.2 multilinguality

some of the best algorithms for semantic distance cannot be applied to most languages
because of a lack of high-quality linguistic resources. mohammad et al. (2007) showed
how text in one language l1 can be combined with a knowledge source in another l2
using a bilingual lexicon l1   l2 and a id64 and concept-disambiguation algo-
rithm to create cross-lingual distributional pro   les of concepts. these cross-lingual
dpcs model co-occurrence distributions of concepts, as per a knowledge source in
one language, with words from another language, and obtain semantic distance in a
resource-poor language using a knowledge source from a resource-rich one. cross-
lingual semantic distance and cross-lingual dpcs are also useful in tasks that inherently
involve two or more languages, such as machine translation, multilingual multidocu-
ment tasks of id91, coreference resolution, and information retrieval. we summa-
rize their approach here using german as l1 and english as l2; however, the algorithm
is language-pair independent.

5.2.1 cross-lingual senses, cross-lingual distributional pro   les, and cross-lingual dis-
tributional distance. given a german word wde in context, mohammad et al. (2007)
use a german   english bilingual lexicon to determine its different possible english
translations. each english translation wen may have one or more possible coarse senses,
as listed in an english thesaurus. these english thesaurus concepts (cen) will be referred

32

mohammad and hirst

distributional measures of semantic distance

celestial

celebrity

body

river
bank

financial
institution

furniture

judiciary

star

stern

bank

bench

bank

figure 1
the cross-lingual candidate senses of german words stern and bank.

}

cen

}

wen

}

wde

to as the cross-lingual candidate senses of the german word wde. figure 1 depicts
examples.12

as in the monolingual estimation of distributional concept-distance, the distance
between two concepts is calculated by    rst determining their dps. however, a concept
is now glossed by near-synonymous words in an english thesaurus, whereas its pro   le
is made up of the strengths of association with co-occurring german words. here are
constructed example cross-lingual distributional pro   les of the two cross-lingual candi-
date senses of the german word stern:13

celestial body (celestial body, sun, . . . ): raum 0.36, licht 0.27, konstellation 0.11, . . .
celebrity (celebrity, hero, . . . ): ber  hmt 0.24, film 0.14, reich 0.14, . . .

the cross-lingual dpcs are created from a cross-lingual word   category co-occurrence
matrix without the use of any word-aligned parallel corpora or sense-annotated data
(as described in the next subsection). just as in the case of monolingual distributional
concept-distance measures, distributional measures can be used to estimate the distance
between the cross-lingual dps of two target concepts. for example, the cosine formula
can be adapted to estimate cross-lingual distributional distance between two concepts
as shown below:

cos(cen

1 , cen

2 ) =

   wde   c(cen

1 )   c(cen

q   wde   c(cen

1 ) p(wde|cen

1 )    p(wde|cen

2 )(cid:0)p(wde|cen
1 )2   q   wde   c(cen

2 )(cid:1)
2 ) p(wde|cen

2 )2

(62)

c(x) is now the set of german words that co-occur with english concept x within a pre-
determined window. the conditional probabilities in the formula are taken from the
cross-lingual dpcs.

5.2.2 creating cross-lingual word   category co-occurrence matrix. a german   english
cross-lingual word   category co-occurrence matrix has german word types wde as one

12 they are called    candidate senses" because some of the senses of wen might not really be senses of wde.

for example, celestial body and celebrity are both senses of the english word star, but the german
word stern can only mean celestial body and not celebrity. similarly, the german bank can mean
financial institution or furniture, but not river bank or judiciary. an automated system has no
straightforward method of teasing out the actual cross-lingual senses of wde from those that are an artifact
of the translation step.

13 vocabulary of german words needed to understand this discussion: bank: 1.    nancial institution, 2.
bench (furniture); ber  hmt: famous; film: movie (motion picture); himmelsk  rper: heavenly body;
konstellation: constellation; licht: light; morgensonne: morning sun; raum: space; reich: rich; sonne:
sun; star: star (celebrity); stern: star (celestial body)

33

celestial body

}
cen

celestial body

sun

star

... }

wen

himmelsk  rper

sonne morgensonne

star stern

... }

wde

figure 2
words having celestial body as one of their cross-lingual candidate senses.

dimension and english thesaurus concepts cen as another.

cen
2

cen
1

. . . cen
j

. . .
wde
1 m11 m12 . . . m1j . . .
wde
2 m21 m22 . . . m2j . . .
...
...
wde
i mi1 mi2 . . . mij . . .
...
. . .

. . .

. . .

...

...

...

...

...

...

i and concept cen
j

the matrix is populated with co-occurrence counts from a large german corpus. a
particular cell mij, corresponding to word wde
, is populated with the
number of times the german word wde
i co-occurs (say a window of   5 words) with
any german word having cen
j as one of its cross-lingual candidate senses. for example,
the raum   celestial body cell will have the sum of the number of times raum co-
occurs with himmelsk  rper, sonne, morgensonne, star, stern, and so on (see figure 2).
this matrix, created after a    rst pass of the corpus, is called the cross-lingual base
wccm. a contingency table for any particular german word wde and english category
cen can be easily generated from the wccm by collapsing cells for all other words
and categories into one and summing up their frequencies. a suitable statistic, such
as pmi or id155, will yield the strength of association between the
german word and the english category. then a new bootstrapped cross-lingual wccm
is created, just as in the monolingual case.

5.2.3 performance. mohammad et al. (2007) evaluated the cross-lingual measures of
semantic distance on two tasks: (1) estimating semantic distance between words and
ranking the word pairs according to semantic distance, and (2) solving reader   s digest
   word power    problems. they compared these results with those obtained by conven-
tional state-of-the-art monolingual approaches with and without a knowledge source
in the target language l1 (germanet). the cross-lingual approach obtained much better
results than monolingual approaches that do not use a knowledge source. further, in
both tasks, the cross-lingual measures performed as well if not slightly better than the
germanet-based monolingual approaches, as well. this shows that the cross-lingual
approach is able to keep losses due to the translation step at a minimum, while allowing
the use of a superior knowledge source in another language to get better results.

5.3 challenges

distributional measures of concept-distance have many desirable features of both
knowledge-rich approaches and strictly corpus-based approaches   they have the high

34

mohammad and hirst

distributional measures of semantic distance

accuracies of knowledge-rich approaches, they can measure both semantic relatedness
and semantic similarity, and they have a strong corpus-reliance making them domain
adaptable. further, with the cross-lingual approach, a lack of high-quality knowledge
source in the target language is no longer a problem. however, certain issues remain.

5.3.1 integrating domain-speci   c terminology. the reliance on a knowledge source
means that the approach cannot measure the distance between words not listed in the
thesaurus. this is especially a problem for domain-speci   c jargon, which might not    nd
place in a general purpose knowledge source. automatic ways of integrating domain-
speci   c terminology into a general purpose knowledge source will be valuable to this
end.

5.3.2 choosing the right concept-granularity. mohammad and hirst (2006b) and
mohammad et al. (2007) have reported results using the categories of the thesaurus as
very coarse word senses. this level of granularity has worked well for the tasks they
experimented with; however, a relatively    ner sense inventory may be more suitable for
other tasks. words within a thesaurus category are grouped into paragraphs; and using
them (instead of categories) and determining when this    ner-grained sense-inventory
is more suitable for use remains to be explored.

5.3.3 identifying lexical semantic relations. word pairs can be semantically close
because of any of the classical lexical semantic relations, such as hypernymy, near-
synonymy, antonymy, troponymy, and meronymy, or the innumerable non-classical
relations. the various distributional approaches discussed in this paper determine
semantic distance without explicitly identifying the nature of the relationship. already,
there is some work on determining lexical entailment (mirkin, dagan, and geffet 2007)
and determining near-synonymy (lin et al. 2003). identifying antonymy (or more gen-
erally, contrasting word-pairs) is especially useful in many natural language tasks, even
if it is simply to discard them from a list of distributionally close terms. also, it will
be interesting for measures of semantic distance to characterize the nature of any non-
classical relationship shared by two words   not only determining if two terms are close
but also specifying (in some intuitive way) the aspect of meaning they share.

6. conclusion

a large number of important natural language problems, including machine transla-
tion, information retrieval, and id51, can be viewed in part as
semantic distance problems. numerous measures of semantic distance exist   those that
use a knowledge source and those that rely on corpora. yet, their use in real-world
applications has been limited. in this paper, we investigated how automatic measures
can be brought more in line with human notions of semantic distance, how they can be
made applicable to a large number of natural language tasks, and how they can be used
even for languages de   cient in high-quality linguistic resources.

even though corpus-based distributional measures of distance have traditionally
performed poorly when compared to id138-based measures, we have shown that
(1) there are a number of reasons that make distributional measures uniquely attractive,
and (2) that their potential is yet to be fully realized. distributional measures can be
easily applied to most languages (they can make do even with just raw text) and
they can be used to mimic both semantic similarity and semantic relatedness. with
this in mind, the paper presented a detailed study of many important distributional

35

measures, analyzed their limitations, and explained why their performance has been
relatively poor so far. understanding these limitations is crucial in the development
of new and better approaches, whether they have a distributional base or otherwise.
we concluded the paper with the discussion of a hybrid, yet distinctly distributional
approach, that presents one way to more accurately measure distributional distance
without compromising too much on essential properties such as the applicability to
resource-poor languages.

acknowledgments
we thank suzanne stevenson, gerald penn, and the computational linguistics group at the
university of toronto for helpful discussions. this research is    nancially supported by the
natural sciences and engineering research council of canada and the university of toronto.

references
[agirre and lopez de lacalle lekuona2003]

agirre, eneko and oier lopez de
lacalle lekuona. 2003. id91
id138 word senses. in proceedings of the
1st international conference on recent
advances in natural language processing
(ranlp-2003), borovets, bulgaria.
[banerjee and pedersen2003]banerjee,

satanjeev and ted pedersen. 2003.
extended gloss overlaps as a measure of
semantic relatedness. in proceedings of the
eighteenth international joint conference on
arti   cial intelligence (ijcai-03), pages
805   810, acapulco, mexico.

[budanitsky and hirst2006]budanitsky,

alexander and graeme hirst. 2006.
evaluating id138-based measures of
semantic distance. computational
linguistics, 32(1):13   47.

[church and hanks1990]church, kenneth w.
and patrick hanks. 1990. word association
norms, mutual information and
id69. computational linguistics,
16(1):22   29.

[cruse1986]cruse, d. allen. 1986. lexical
semantics. cambridge university press,
cambridge, uk.

[curran2004]curran, james r. 2004. from

distributional to semantic similarity. ph.d.
thesis, school of informatics, university of
edinburgh, edinburgh, uk.

[dagan, lee, and pereira1994]dagan, ido,
lillian lee, and fernando pereira. 1994.
similarity-based estimation of word
cooccurrence probabilities. in proceedings of
the 32nd annual meeting of the association of
computational linguistics (acl-1994),
pages 272   278, las cruces, new mexico.

[dagan, lee, and pereira1997]dagan, ido,
lillian lee, and fernando pereira. 1997.
similarity-based methods for word sense
disambiguation. in proceedings of the 35th

36

annual meeting of the association of
computational linguistics (acl-1997),
pages 56   63, madrid, spain.

[dagan, lee, and pereira1999]dagan, ido,
lillian lee, and fernando pereira. 1999.
similarity-based models of cooccurrence
probabilities. machine learning,
34(1   3):43   69.

[dagan, marcus, and markovitch1995]
dagan, ido, shaul marcus, and shaul
markovitch. 1995. contextual word
similarity and estimation from sparse data.
computer speech and language, 9:123   152.
[firth1957]firth, john r. 1957. a synopsis of

linguistic theory 1930   55. in studies in
linguistic analysis (special volume of the
philological society), pages 1   32, oxford,
england. the philological society.

[gorman and curran2006]gorman, james

and james r. curran. 2006. scaling
distributional similarity to large corpora.
in proceedings of the 21st international
conference on computational linguistics and
the 44th annual meeting of the association for
computational linguistics, pages 361   368,
sydney, australia.

[grefenstette1992]grefenstette, gregory.

1992. sextant: exploring unexplored
contexts for semantic extraction from
syntactic analysis. in proceedings of the 30th
annual meeting of the association for
computational linguistics (acl-1992),
pages 324   326, newark, delaware.

[gurevych2005]gurevych, iryna. 2005. using

the structure of a conceptual network in
computing semantic relatedness. in
proceedings of the 2nd international joint
conference on natural language processing
(ijcnlp-2005), pages 767   778, jeju island,
republic of korea.

[halliday1966]halliday, michael a. k. 1966.
lexis as a linguistic level. in j.c. catford
c.e. bazell, m.a.k halliday, and r.h.
robins, editors, in memory of j.r. firth,

mohammad and hirst

distributional measures of semantic distance

pages 148   162, london, uk. longmans
linguistics library.

[harman1993]harman, donna. 1993.
overview of the    rst text retrieval
conference. in proceedings of the 16th
annual international association for
computing machinery special interest group
on information retrieval (acm-sigir)
conference on research and development in
information retrieval, pages 36   47,
pittsburgh, pennsylvania.

[harris1968]harris, zellig. 1968. mathematical

structures of language. interscience
publishers, new york, ny.

[hindle1990]hindle, donald. 1990. noun
classi   cation from predicate-argument
structures. in proceedings of the 28th annual
meeting of the association of computational
linguistics (acl-1990), pages 268   275,
pittsburgh, pennsylvania.

[hirst and budanitsky2005]hirst, graeme

and alexander budanitsky. 2005.
correcting real-word spelling errors by
restoring lexical cohesion. natural
language engineering, 11(1):87   111.

[hirst and st-onge1998]hirst, graeme and

david st-onge. 1998. lexical chains as
representations of context for the detection
and correction of malapropisms. in
christiane fellbaum, editor, id138: an
electronic lexical database. the mit press,
cambridge, ma, chapter 13, pages
305   332.

[inkpen and hirst2002]inkpen, diana and

graeme hirst. 2002. acquiring collocations
for lexical choice between near-synonyms.
in siglex workshop on unsupervised
lexical acquisition, 40th meeting of the
association for computational linguistics,
philadelphia, pa.

[leacock and chodorow1998]leacock,
claudia and martin chodorow. 1998.
combining local context and id138
similarity for word sense identi   cation. in
christiane fellbaum, editor, id138: an
electronic lexical database. the mit press,
cambridge, ma, chapter 11, pages
265   283.

[lee1999]lee, lillian. 1999. measures of

distributional similarity. in proceedings of
the 37th conference on association for
computational linguistics (acl-1999),
pages 25   32, college park, maryland.

[lee2001]lee, lillian. 2001. on the

effectiveness of the skew divergence for
statistical language analysis. in proceedings
of the eigth international workshop on
arti   cial intelligence and statistics
(aistats-2001), pages 65   72, key west,
florida.

[lesk1986]lesk, michael. 1986. automatic

sense disambiguation using machine
readable dictionaries: how to tell a pine
cone from an ice cream cone. in proceedings
of the 5th annual international conference on
systems documentation, pages 24   26,
toronto, canada.

[lin1997]lin, dekang. 1997. using syntactic

dependency as local context to resolve
word sense ambiguity. in proceedings of the
8th conference of the european chapter of the
association for computational linguistics
(acl,eacl-1997), pages 64   71, madrid,
spain.

[lin1998a]lin, dekang. 1998a. automatic

retreival and id91 of similar words.
in proceedings of the 17th international
conference on computational linguistics
(coling-1998), pages 768   773, montreal,
canada.

[jarmasz and szpakowicz2003]jarmasz,

[lin1998b]lin, dekang. 1998b. automatic

mario and stan szpakowicz. 2003. roget   s
thesaurus and semantic similarity. in
proceedings of the international conference on
recent advances in natural language
processing (ranlp-2003), pages 212   219,
borovets, bulgaria.

[jiang and conrath1997]jiang, jay j. and

david w. conrath. 1997. semantic
similarity based on corpus statistics and
lexical taxonomy. in proceedings of
international conference on research on
computational linguistics (rocling x),
taipei, taiwan.

retrieval and id91 of similar words.
in proceedings of the 17th international
conference on computational linguistics
(coling-1998), pages 768   773, montreal,
canada.

[lin et al.2003]lin, dekang, shaojun zhao,

lijuan qin, and ming zhou. 2003.
identifying synonyms among
distributionally similar words. in
proceedings of the 18th international joint
conference on arti   cial intelligence
(ijcai-95), pages 1492   1493, acapulco,
mexico.

[landauer, foltz, and laham1998]landauer,

[mccarthy et al.2007]mccarthy, diana, rob

thomas k., peter w. foltz, and darrell
laham. 1998. introduction to latent
semantic analysis. discourse processes,
25(2   3):259   284.

koeling, julie weeds, and john carroll.
2007. unsupervised acquisition of
predominant word senses. computational
linguistics, 33(4).

37

[miller and charles1991]miller, george a.
and walter g. charles. 1991. contextual
correlates of semantic similarity. language
and cognitive processes, 6(1):1   28.

[mirkin, dagan, and geffet2007]mirkin,

shachar, ido dagan, and maayan geffet.
2007. integrating pattern-based and
distributional similarity methods for
lexical entailment acquisition. in
proceedings of the 21st international
conference on computational linguistics and
the 44th annual meeting of the association for
computational linguistics main conference
poster sessions, pages 579   586, sydney,
australia.

[mohammad et al.2007]mohammad, saif,

iryna gurevych, graeme hirst, and
torsten zesch. 2007. cross-lingual
distributional pro   les of concepts for
measuring semantic distance. in
proceedings of the joint conference on
empirical methods in natural language
processing and computational natural
language learning (emnlp/conll-2007),
pages 571   580, prague, czech republic.
[mohammad and hirst2006a]mohammad,

saif and graeme hirst. 2006a. determining
word sense dominance using a thesaurus.
in proceedings of the 11th conference of the
european chapter of the association for
computational linguistics (eacl), pages
121   128, trento, italy.

[mohammad and hirst2006b]mohammad,

saif and graeme hirst. 2006b.
distributional measures of
concept-distance: a task-oriented
evaluation. in proceedings of the conference
on empirical methods in natural language
processing (emnlp-2006), pages 35   43,
sydney, australia.

[morris and hirst2004]morris, jane and

graeme hirst. 2004. non-classical lexical
semantic relations. in proceedings of the
workshop on computational lexical
semantics, human language technology
conference of the north american chapter of
the association for computational linguistics,
pages 46   51, boston, massachusetts.

[patwardhan, banerjee, and pedersen2003]

patwardhan, siddharth, satanjeev
banerjee, and ted pedersen. 2003. using
measures of semantic relatedness for word
sense disambiguation. in proceedings of the
fourth international conference on intelligent
text processing and computational
linguistics (cicling-03), pages 17   21,
mexico city, mexico.

[patwardhan and pedersen2006]patwardhan,

siddharth and ted pedersen. 2006. using

38

id138 based context vectors to estimate
the semantic relatedness of concepts. in
proceedings of the european chapter of the
association for computational linguistics
workshop making sense of sense   bringing
computational linguistics and
psycholinguistics together, pages 1   8,
trento, italy.

[paul1991]paul, douglas b. 1991. experience
with a stack decoder-based id48 csr and
back-off id165 language models. in
proceedings of the speech and natural
language workshop, pages 284   288, palo
alto, california.

[pereira, tishby, and lee1993]pereira,

fernando, naftali tishby, and lillian lee.
1993. distributional id91 of english
words. in proceedings of the 31st annual
meeting of the association of computational
linguistics (acl-1993), pages 183   190,
columbus, ohio.

[rada et al.1989]rada, roy, hafedh mili,

ellen bicknell, and maria blettner. 1989.
development and application of a metric
on semantic nets. ieee transactions on
systems, man, and cybernetics, 19(1):17   30.

[rapp2003]rapp, reinhard. 2003. word

sense discovery based on sense descriptor
dissimilarity. in proceedings of the machine
translation summit ix, pages 315   322, new
orleans, louisiana.

[resnik1995]resnik, philip. 1995. using

information content to evaluate semantic
similarity. in proceedings of the 14th
international joint conference on arti   cial
intelligence (ijcai-95), pages 448   453,
montreal, canada.

[resnik1998]resnik, philip. 1998. id138

and class-based probabilities. in christiane
fellbaum, editor, id138: an electronic
lexical database. the mit press,
cambridge, massachusetts, pages 239   263.

[resnik1999]resnik, philip. 1999. semantic

similarity in a taxonomy: an
information-based measure and its
application to problems of ambiguity in
natural language. communications of the
association of computing machinery,
11:95   130.

[resnik and diab2000]resnik, philip and

mona diab. 2000. measuring verb
similarity. in proceedings of the 22nd annual
meeting of the cognitive science society
(cogsci 2000), pages 399   404, philadelphia,
pennsylvania.

[rubenstein and goodenough1965]
rubenstein, herbert and john b.
goodenough. 1965. contextual correlates
of synonymy. communications of the

mohammad and hirst

distributional measures of semantic distance

association of computing machinery,
8(10):627   633.

[sch  tze and pedersen1997]sch  tze, hinrich

and jan o. pedersen. 1997. a
cooccurrence-based thesaurus and two
applications to information retreival.
information processing and management,
33(3):307   318.

[turney2001]turney, peter. 2001. mining the
web for synonyms: pmi-ir versus lsa on
toefl. in proceedings of the twelfth
european conference on machine learning
(ecml-2001), pages 491   502, freiburg,
germany.

[weeds, weir, and mccarthy2004]weeds,
julie, david weir, and diana mccarthy.
2004. characterising measures of lexical
distributional similarity. in proceedings of
the 20th international conference on
computational linguistics (coling-04),
pages 1015   1021, geneva, switzerland.
[weeds2003]weeds, julie e. 2003. measures

and applications of lexical distributional
similarity. ph.d. thesis, department of
informatics, university of sussex,
brighton, uk.

[yang and powers2005]yang, dongqiang and

david powers. 2005. measuring semantic
similarity in the taxonomy of id138. in

proceedings of the twenty-eighth australasian
computer science conference (acsc-2005),
pages 315   322, newcastle, australia.

[yang and powers2006]yang, dongqiang and
david powers. 2006. verb similarity on the
taxonomy of id138. in proceedings of the
third international id138 conference
(gwc-2006), pages 121   128, jeju island,
republic of korea.

[yoshida, yukawa, and kuwabara2003]

yoshida, sen, takashi yukawa, and
kazuhiro kuwabara. 2003. constructing
and examining personalized
cooccurrence-based thesauri on web
pages. in proceedings of the 12th
international world wide web conference,
pages 20   24, budapest, hungary.

[zesch, gurevych, and m  hlh  user2007]

zesch, torsten, iryna gurevych, and max
m  hlh  user. 2007. comparing wikipedia
and german id138 by evaluating
semantic relatedness on multiple datasets.
in proceedings of human language
technologies: the annual conference of the
north american chapter of the association for
computational linguistics
(naacl,hlt-2007), pages 205   208,
rochester, new york.

39

40

