9
0
0
2

 

p
e
s
9
2

 

 
 
]
l
m

.
t
a
t
s
[
 
 

1
v
2
2
4
5

.

9
0
9
0
:
v
i
x
r
a

laplacian support vector machines

trained in the primal

stefano melacci
department of information engineering
university of siena
53100, siena, italy
mikhail belkin
department of computer science and engineering
ohio state university
columbus, oh 43210, usa

mela@dii.unisi.it

mbelkin@cse.ohio-state.edu

technical report

abstract

in the last few years, due to the growing ubiquity of unlabeled data, much e   ort has been
spent by the machine learning community to develop better understanding and improve
the quality of classi   ers exploiting unlabeled data. following the manifold id173
approach, laplacian support vector machines (lapid166s) have shown the state of the
art performance in semi   supervised classi   cation. in this paper we present two strategies
to solve the primal lapid166 problem, in order to overcome some issues of the original
dual formulation. whereas training a lapid166 in the dual requires two steps, using the
primal form allows us to collapse training to a single step. moreover, the computational
complexity of the training algorithm is reduced from o(n3) to o(n2) using preconditioned
conjugate gradient, where n is the combined number of labeled and unlabeled examples.
we speed up training by using an early stopping strategy based on the prediction on
unlabeled data or, if available, on labeled validation examples. this allows the algorithm
to quickly compute approximate solutions with roughly the same classi   cation accuracy as
the optimal ones, considerably reducing the training time. due to its simplicity, training
lapid166 in the primal can be the starting point for additional enhancements of the original
lapid166 formulation, such as those for dealing with large datasets. we present an extensive
experimental evaluation on real world data showing the bene   ts of the proposed approach.
keywords: laplacian support vector machines, manifold id173, semi   supervised
learning, classi   cation, optimization.

1. introduction

in semi-supervised learning one estimates a target classi   cation/regression function from
a few labeled examples together with a large collection of unlabeled data. in the last few
years there has been a growing interest in the semi   supervised learning in the scienti   c
community. many algorithms for exploiting unlabeled data in order to enhance the quality
of classi   ers have been recently proposed, see, e.g., (chapelle et al., 2006) and (zhu and
goldberg, 2009). the general principle underlying semi-supervised learning is that the

1

marginal distribution, which can be estimated from data alone, may suggest a suitable way
to adjust the target function. the two commons assumption on such distribution that,
explicitly or implicitly, are made by many of semi   supervised learning algorithms are the
cluster assumption (chapelle et al., 2003) and the manifold assumption (belkin et al., 2006).
the cluster assumption states that two points are likely to have the same class label if they
can be connected by a curve through a high density region. consequently, the separation
boundary between classes should lie in the lower density region of the space. for example,
this intuition underlies the transductive support vector machines (vapnik, 2000) and in
its di   erent implementations, such as tid166 in (joachims, 1999) or s3vm (demiriz and
bennett, 2000; chapelle et al., 2008). the manifold assumption states that the marginal
id203 distribution underlying the data is supported on or near a low   dimensional
manifold, and that the target function should change smoothly along the tangent direction.
many graph based methods have been proposed in this direction, but the most of them
only perform transductive id136 (joachims, 2003; belkin and niyogi, 2003; zhu et al.,
2003), that is classify the unlabeled data given in training. laplacian vector machines
(lapid166) (belkin et al., 2006) provide a natural out   of   sample extension, so that they
can classify data that becomes available after the training process, without having to retrain
the classi   er or resort to various heuristics.

in this paper, we focus on the lapid166 algorithm, that has shown to achieve the state
of the art performances in semi   supervised classi   cation. the original approach used to
train lapid166 in belkin et al. (2006) is based on the dual formulation of the problem, in
a traditional id166   like fashion. this dual problem is de   ned only on a number of dual
variables equal to l, the number of labeled points, and the the relationship between the l
variables and the    nal n coe   cients is given by a linear system of n equations and variables,
where n is the total number of training points, both labeled and unlabeled. the overall
cost of this    two step    process is o(n3).

motivated by the recent interest in solving the id166 problem in the primal (chapelle,
2007; joachims, 2006; shalev-shwartz et al., 2007), we present a way to solve the primal
lapid166 problem that can signi   cantly reduce training times and overcome some issues of
the original training algorithm. speci   cally, the contributions of this paper are the following:

1. we propose two methods for solving the lapid166 problem in the primal form (not
limited to the linear case), following the ideas presented in (chapelle, 2007) for
id166s. our matlab library can be downloaded from http://www.dii.unisi.it/~
melacci/lapid166p/. the solution can now be compactly computed in a    single step   
on the whole variable set. we show how to solve the problem by newton   s method,
comparing it with the supervised case. from this comparison it turns out that the
real advantages of the newton   s method for the id166 problem are lost in lapid166
due to the intrisic norm regularizer, and the complexity of this solution is still o(n3),
same as in the original dual formulation. on the other hand, preconditioned con-
jugate gradient can be directly applied. preconditioning by the kernel matrix come
at no additional cost, and convergence can be achieved with only a small number of
o(n2) iterations. complexity can be further reduced if the kernel matrix is sparse,
increasing the scalability of the algorithm.

2

2. an approximate solution of the dual form and the resulting approximation of the
target optimal function are not directly related due to the change of variables while
switching to the dual problem. training lapid166s in the primal overcomes this issue,
and it allows us to directly compute approximate solutions by controlling the number
conjugate gradient iterations.

3. an approximation of the target function with roughly the same classi   cation accuracy
as the optimal one can be achieved with a small number of iterations due to the
e   ects of the intrinsic norm regularizer of lapid166s on the training process. we
investigate those e   ects, showing that they make common stopping conditions for
iterative gradient based algorithms hard to tune, often leading to either a premature
stopping of the algorithm or to the execution of a large amount of iterations without
improvements to the classi   cation accuracy. we suggest to use a criterion built upon
the output of the classi   er on the available training data for terminating the iteration of
the algorithm. speci   cally, the stability of the prediction on the unlabeled data, or the
classi   cation accuracy on validation data (if available) can be exploited. a number of
experiments on several datasets support these types of criteria, showing that accuracy
similar to that of the optimal solution can be obtained in with signi   catly reduced
training time.

4. the primal solution of the lapid166 problem is based on an l2 hinge loss, that es-
tablishes a direct connection to the laplacian regularized least square classi   er
(laprlsc) (belkin et al., 2006). we discuss the similarities between primal lapid166
and laprlsc and we show that the proposed fast solution can be trivially applied
also to laprlsc.

the rest of the paper is organized as follows. in section 2 the basic principles behind
manifold id173 are resumed. section 2.1 describes the lapid166 algorithm in its
original formulation whereas section 3 discusses the proposed solutions of the primal form
and their details. the quality of an approximate solution and the data based early stopping
criterion are the key contents of section 4. in section 5 a parallel with the primal solution
of lapid166 and the one of laprlsc is drawn, describing some possible future work. an
extensive experimental analysis is presented in section 6, and,    nally, section 7 concludes
the paper.

2. manifold id173

first, we introduce some notation that will be used in this section and in the rest of
the paper. we take n = l + u to be the number of m dimensional training examples
xi     x     irm, collected in s = {xi, i = 1, . . . , n}. examples are ordered so that the    rst
l ones are labeled, with label yi     {   1, 1}, and the remaining u points are unlabeled. we
put s = l     u, where l = {(xi, yi), i = 1, . . . , l} is the labeled data set and u = {xi, i =
l + 1, . . . , n} is the unlabeled data set. labeled examples are generated accordingly to the
distribution p on x    ir, whereas unlabeled examples are drawn according to the marginal
distribution px of p . labels are obtained from the id155 distribution
p (y|x). l is the graph laplacian associated to s, given by l = d     w , where w is

3

j=1 wij). laplacian can be expressed in the normalized form, l = d    1

the adjacency matrix of the data graph (the entry in position i, j is indicated with wij)
and d is the diagonal matrix with the degree of each node (i.e. the element dii from d
is dii =(cid:80)n
2 ld    1
2 ,
and iterated to a degree p greater that one. by k     irn,n we denote the gram matrix
associated to the n points of s and the i, j   th entry of such matrix is the evaluation of the
id81 k(xi, xj), k : x    x     ir. the unknown target function that the learning
algorithm must estimate is indicated with f : x     ir, where f is the vector of the n values
of f on training data, f = [f(xi), xi     s]t . in a classi   cation problem, the decision function
that discriminates between classes is indicated with y(x) = g(f(x)), where we overloaded
the use of y to denote such function.

manifold id173 approach (belkin et al., 2006) exploits the geometry of the
marginal distribution px. the support of the id203 distribution of data is assumed to
have the geometric structure of a riemannian manifold m. the labels of two points that
are close in the intrinsic geometry of px (i.e. with respect to geodesic distances on m)
should be the same or similar in sense that the id155 distribution p (y|x)
should change little between two such points. this constraint is enforced in the learning
i that is empirically estimated from the point cloud
process by an intrinsic regularizer (cid:107)f(cid:107)2
of labeled and unlabeled data using the graph laplacian associated to them, since m is
truly unknown. in particular, choosing exponential weights for the adjacency matrix leads
to convergence of the graph laplacian to the laplace   beltrami operator on the manifold
(belkin and niyogi, 2008). as a result, we have

i =

(cid:107)f(cid:107)2

n(cid:88)i=1

n(cid:88)j=i

wij(f(xi)     f(xj))2 = f t lf .

(1)

consider that, in general, several natural choices of (cid:107)(cid:107)i exist (belkin et al., 2006).
in the established id173 framework for function learning, given a id81
k(  ,  ), its associated reproducing kernel hilber space (rkhs) hk of functions x     ir
with corresponding norm (cid:107)(cid:107)a, we estimate the target function by minimizing

f    = arg min
f   hk

l(cid:88)i=1

v (xi, yi, f) +   a(cid:107)f(cid:107)2

a +   i(cid:107)f(cid:107)2

i

(2)

where v is some id168 and   a is the weight of the norm of the function in the rkhs
(or ambient norm), that enforces a smoothness condition on the possible solutions, and
  i is the weight of the norm of the function in the low dimensional manifold (or intrinsic
norm), that enforces smoothness along the sampled m. for simplicity, we removed every
id172 factor of the weights of each term in the summation. the ambient regularizer
makes the problem well   posed, and its presence can be really helpful from a practical point
of view when the manifold assumption holds at a lesser degree.
it has been shown in belkin et al. (2006) that f    admits an expansion in terms of the n

points of s,

     
i k(xi, x).

f   (x) =

n(cid:88)i=1

4

(3)

the decision function that discriminates between class +1 and    1 is y(x) = sign(f   (x)).

figure 1 shows the e   ect of the intrinsic regularizer on the    clock    toy dataset. the su-
pervised approach de   nes the classi   cation hyperplane just by considering the two labeled
examples, and it does not bene   t from unlabeled data (figure 1(b)). with manifold reg-
ularization, the classi   cation appears more natural with respect to the geometry of the
marginal distribution (figure 1(c)).

(a)

(b)

(c)

figure 1: (a) the two class    clock    dataset. one class is the circular border of the clock,
the other one is the hour/minute hands. a large set of unlabeled examples (black
squares) and only one labeled example per class (red diamond, blue circle) are
selected.
- (b) the result of a maximum margin supervised classi   cation - (c)
the result of a semi   supervised classi   cation with intrinsic norm from manifold
id173.

the intrinsic norm of eq. 1 actually performs a transduction along the manifold that
enforces the values of f in nearby points with respect to geodesic distances on m to be the
   same   . from a merely practical point of view, the intrinsic regularizer can be excessively
strict in some situations. since the decision function y(x) relies only on the sign of the
target function f(x), if f has the same sign on nearby points along m then the graph
transduction is actually complete. requiring that f assumes exactly the same value on a
pair of nearby points could be considered as over constraining the problem.

this intuition is closely related to the ideas explored in sindhwani (2007); sindhwani and
rosenberg (2008); abernethy et al. (2008). in particular, in some restricted function spaces
the intrinsic regularizer could degenerate to the ambient one as it is not able to model
some underlying geometries of the given data. the manifold co-id173 (mcr)
framework (sindhwani and rosenberg, 2008) has been proposed to overcome such issue
using multi   view learning. it has been shown that mcr corresponds to adding some extra
slack variables in the objective function of eq. 2 to better    t the intrinsic regularizer. the
slack variables of mcr could be seen as a way to relax the regularizer. similarly, abernethy
et al. (2008) uses a slack based formulation to improve the    exibility of the graph regularizer
of their spam detector. this problem has been addressed also by tsang and kwok (2007),
where the intrinsic regularizer is an     insensitive loss. we will use these considerations in
section 4 to early stop the training algorithm.

5

   1   0.500.51   1   0.500.51   1   0.500.51   1   0.500.51   1   0.500.51   1   0.500.512.1 laplacian support vector machines

lapid166s follow the principles behind manifold id173 (eq. 2), where the loss func-
tion v (x, y, f) is the linear hinge loss (vapnik, 2000), or l1 loss. the interesting property
of such function is that well classi   ed labeled examples are not penalized by v (x, y, f),
independently by the value of f.

in order to train a lapid166 classi   er, the following problem must be solved

min
f   hk

l(cid:88)i=1

max(1     yif(xi), 0) +   a(cid:107)f(cid:107)2

a +   i(cid:107)f(cid:107)2
i .

(4)

the function f(x) admits the expansion of eq. 3, where an unregularized bias term b can
be added as in many id166 formulations.

the solution of lapid166 problem proposed by belkin et al. (2006) is based on the
dual form. by introducing the slack variables   i, the unconstrained primal problem can be
written as a constrained one:

subject to:

i = 1, . . . , l

min     irn

i=1   i +   a  t k   +   i   t klk  

,     irl(cid:80)l
yi((cid:80)n
j=1   ik(xi, xj) + b)     1       i,
  i     0,

i = 1, . . . , l

after the introduction of two sets of n multipliers   ,   , the lagrangian lg associated

to the problem is:

lg(  ,   , b,   ,   ) =

  i +

1
2

l(cid:88)i=1
l(cid:88)i=1

   

  i(yi(

  t (2  ak + 2  i klk)      
n(cid:88)j=1

  ik(xi, xj) + b)     1 +   i)    

  i  i

l(cid:88)i=1

in order to recover the dual representation we need to set:

   lg
   b

   lg
     i

= 0 =   

  iyi = 0

l(cid:88)i=1

= 0 =    1       i       i = 0 =    0       i     1

where the bounds on   i consider that   i,   i     0, since they are lagrange multipliers. using
the above identities, we can rewrite the lagrangian as a function of    and    only. assuming
(as stated in section 2) that the points in s are ordered such that the    rst l are labeled and
the remaining u are unlabeled, we de   ne with jl     irl,n the matrix [i 0] where i     irl,l is
the identity matrix and 0     irl,u is a rectangular matrix with all zeros. moreover, y     irl,l

6

is a diagonal matrix composed by the labels yi, i = 1, . . . , l. the lagrangian becomes

lg(  ,   ) =

=

1
2

1
2

  t (2  ak + 2  i klk)      

n(cid:88)j=1
  t (2  ak + 2  i klk)         t kj tl y    +

l(cid:88)i=1

  i(yi(

  ik(xi, xj) + b)     1) =
l(cid:88)i=1

  i.

setting to zero the derivative with respect to    establishes a direct relationships between

the    coe   cients and the    ones:

   lg
     

= 0 =    (2  ak + 2  i klk)       kj tl y    = 0

=       = (2  ai + 2  i kl)   1j tl y   

(5)

after substituting back in the lagrangian expression, we get the dual problem whose

solution leads to the optimal      :

2   t q  

max     irl(cid:80)l
subject to: (cid:80)l

i=1   i     1
i=1   iyi = 0

0       i     1,

i = 1, . . . , l

where

q = y j tl k(2  ai + 2  i kl)   1j tl y.

(6)

training the lapid166 classi   er requires to optimize this l variable problem, for example
using a standard quadratic id166 solver, and then to solve the linear system of n equations
and n variables of eq. 5 in order to get the coe   cients       that de   ne the target function
f   .
the overall complexity of this    two step    solution is o(n3), due to the matrix inversion
of eq. 5 (and 6). even if the l coe   cients       are sparse, since they come from a id166   like
dual problem, the expansion of f    will generally involves all n coe   cients      .

3. training in the primal

in this section we analyze the optimization of the primal form of the non linear lapid166
problem, following the growing interest in training id166s in the primal of the last few years
(joachims, 2006; chapelle, 2007; shalev-shwartz et al., 2007). primal optimization of a
id166 has strong similarities with the dual strategy (chapelle, 2007), and its implementation
does not require any particularly complex optimization libraries. the focus of researchers
has been mainly on the solution of the linear id166 primal problem, showing how it can
be solved fast and e   ciently (joachims, 2006; shalev-shwartz et al., 2007). most of the
existing results can be directly extended to the non linear case by reparametrizing the linear
i=1   ixi and introducing the gram matrix

output function f(x) = (cid:104)w, x(cid:105) + b with w =(cid:80)l

7

k. however this may result in a loss of e   ciency. in chapelle (2007); keerthi et al. (2006)
the authors investigated e   cient solutions for the non linear id166 case.

primal and dual optimization are two ways di   erent of solving the same problem, nei-
ther of which can in general be considered a    better    approach. therefore why should a
solution of the primal problem be useful in the case of lapid166? there are three primary
reasons why such a solution may be preferable. first, it allows us to e   ciently solve a single
problem without the need of a two step solution. second, it allows us to very quickly com-
pute good approximate solutions, while the exact relation between approximate solutions
of the dual and original problems may be involved. third, since it allows us to directly
   manipulate    the    coe   cients of f without passing through the    ones, greedy techniques
for incremental building of the lapid166 classi   er are easier to manage (sindhwani, 2007).
we believe that studying the primal lapid166 problem is the basis for future investigations
and improvements of this classi   er.

we rewrite the primal lapid166 problem of eq. 4 by considering the representation of
f of eq. 3, the intrinsic regularized of eq. 1, and by indicating with ki the i-th column of
the matrix k

min
     irn

,b   ir

l(cid:88)i=1

v (xi, yi, kt

i    + b) +   a  t k   +   i   t klk  .

note that, for completeness, we included the bias b in the expansion of f. such bias does
not a   ect the intrinsic norm that is actually a sum of squared di   erences of f evaluated
on pair of points1. we use a squared hinge loss, or l2 loss, for the labeled examples,
following chapelle (2007) (see figure 2). l2 loss makes the lapid166 problem continuous
and di   erentiable in f and so in   . the optimization problem after adding the scaling
constant 1

2 becomes

min
     irn

,b   ir

1
2

(

l(cid:88)i=1

max(1     yi(kt

i    + b), 0)2 +   a  t k   +   i   t klk  ).

(7)

we solved such convex problem by newton   s method and by preconditioned conjugate
gradient, comparing their complexities and the complexity of the original lapid166 solution,
and showing a parallel with the id166 case. the two solution strategies are analyzed in the
following subsections, while a large set of experimental results are collected in section 6.

3.1 newton   s method

the problem of eq. 7 is piecewise quadratic and the newton   s method appears a natural
choice for an e   cient minimization, since it builds a quadratic approximation of the function.
after indicating with z the vector z = [b,   t ]t , each newton   s step consists of the following
update

zt = zt   1     sh   1   

(8)

1. if the laplacian is normalized then the expression of the intrinsic norm changes. this must be taken

into account when computing the bias.

8

figure 2: l1 hinge loss, piecewise linear, continuous and non di   erentiable in yif(xi) = 1.

l2 hinge loss, continuous and di   erentiable.

where t is the iteration number, s is the step size, and     and h are the gradient and the
hessian of eq. 7 with respect to z. we will use the symbols       and    b to indicate the
gradient with respect to    and to b.
before continuing, we introduce the further concept of error vectors (chapelle, 2007).
the set of error vectors e is the subset of l with the points that generate a l2 hinge loss
value greater than zero. the classi   er does not penalize all the remaining labeled points,
since the f function on that points produces outputs with the same sign of the corresponding
label and with absolute value greater then or equal to it. in the classic id166 framework,
error vectors correspond to support vectors at the optimal solution. in the case of lapid166,
all points are support vectors at optimum in the sense that they all generally contribute to
the expansion of f.

we have

    =(cid:20)    b

      (cid:21) =(cid:32)
=(cid:18)

i=1 kiyi(yi(ki   + b)     1) +   ak   +   i klk   (cid:33) =
(cid:80)l
kie(k   + 1b)     kie y +   ak   +   i klk   (cid:19)

(cid:80)l
i=1 yi(yi(ki   + b)     1)
1t ie(k   + 1b)     1ie y

(9)

where 1 is the vector on n elements equal to 1 and y     {   1, 0, 1}n is the vector that collects
the l labels yi of the labeled training points and a set of u zeros. the matrix ie     irn,n
is a diagonal matrix where the only elements di   erent from 0 (and equal to 1) along the
main diagonal are in positions corresponding to points of s that belong to e at the current
iteration.

the hessian h is

h =(cid:18)    2
     (   b)

b

   (cid:19) = (cid:18) 1t ie1
   b(     )
   2
= (cid:18)      a 1t

0

kie1 kie k +   ak +   i klk (cid:19) =

1t ie k

k (cid:19)(cid:18) 0

ie1 ie k +   ai +   i lk (cid:19)

1t

9

   0.500.511.5200.511.52yif(xi)loss  l1l2note that the criterion function of eq. 7 is not twice di   erentiable everywhere, so that h
is the generalized hessian where the subdi   erential in the breakpoint of the hinge function
is set to 0. this leaves intact the least square nature of the problem, as in the modi   ed
newton   s method proposed by keerthi and decoste (2006) for linear id166s. in other words,
the contribute to the hessian of the l2 hinge loss is the same as the one of a squared loss
(yi     f(xi))2 applied to error vectors only.

combining the last two expressions we can write     as
k (cid:19) ie y.

from eq. 10, the newton   s update of eq. 8 becomes

    = hz    (cid:18) 1t
k (cid:19) ie y =

zt = zt   1     szt   1 + sh   1(cid:18) 1t
= (1     s)zt   1 + s(cid:18) 0
= (1     s)zt   1 + s(cid:18) 0

1t

ie1 ie k +   ai +   i lk (cid:19)   1(cid:18)      a 1t
ie1 ie k +   ai +   i lk (cid:19)   1(cid:18) 0
ie y (cid:19) .

1t

0

(10)

k (cid:19)   1(cid:18) 1t

k (cid:19) ie y =

(11)
looking at the update rule of eq. 11 the analogies and di   erences with the solution of
the linear system of eq. 5 can be clearly appreciated. in particular, eq. 5 relates the dual
variables    with the    ones using the information on the ambient and intrinsic regularizers.
the contribute of the labeled data has already been collected in the    variables, by solving
the dual problem. di   erently, in the update rule of of eq. 11 the information of the l2 loss
is represented by the ie k term.
the step size s must be computed by solving the one   dimensional minimization of
eq. 7 restricted on the ray from zt   1 to zt, with exact line search or backtracking (boyd
and vandenberghe, 2004). convergence is declared when the set of error vectors does not
change between two consecutive iterations of the algorithm. we can see that when s = 1,
eq. 11 shows that the vector zt   1 of the previous iteration is not explicitly included in
the update rule of zt. the only variable element that de   nes the new zt is ie, i.e. the
set of error vectors e. exactly like in the case of primal id166s (chapelle, 2007), in our
experiments setting s = 1 did not result in any convergence problems.

3.1.1 complexity analysis

updating the    coe   cients with the newton   s method costs o(n3), due to the matrix
inversion in the update rule. convergence is usually achieved in a tiny number of iterations,
no more than 5 in our experiments (see section 6).
in order to reduce the cost of each
iteration, a cholesky factorization of the hessian can be computed before performing the
   rst matrix inversion, and it can be updated using a rank   1 scheme during the following
iterations, with cost o(n2) for each update (seeger, 2008). on the other hand, this does
not allow us to simplify k in eq. 11, otherwise the resulting matrix to be inverted will not
be symmetric. since a lot of time is wasted in the product by k (that is usually dense),

10

using the update of cholesky factorization may not necessarily lead to a reduction of the
overall training time.

solving the primal problem using the newton   s method has the same complexity of the
original lapid166 solution based on the dual problem discussed in section 7. the only
bene   t of solving the primal problem with netwon   s method relies on the compact and
simple formulation that does not requires the    two step    approach and a quadratic id166
solver as in the original dual formulation.

it is interesting to compare the training of id166s in the primal with the one of lapid166s
for a better insight in the newton   s method based solution. id166s can bene   t from the
inversion of only a portion of the whole hessian matrix, that reduces the complexity of
each iteration to o(|e|). exploiting this useful aspect, the training algorithm can be run
incrementally, reducing the complexity of the whole training process. in detail, an initial
run on small portion of the available data is used to compute an approximate solution.
then the remaining training points, or some of them, are added. due to the hinge loss
and the currently estimated separating hyperplane, many of them will probably not belong
to e so that its maximum cardinality during the whole training process will reasonably
be smaller than n. moreover, if we    x the step size s = 1 the components of    that are
not associated to an error vector will become zeros after the update, so that the newton   s
method encourages sparser solutions.

in the case of lapid166 those bene   ts are lost due to the presence of the intrinsic norm
f t lf. as a matter of fact and independently by the set e, the constraints wij(f(xi)    
f(xj))2 make the hessian a full matrix, avoiding the described useful block inversion of
id166s. if the classi   er is build incrementally, the addiction of a new non   error vector point
makes the current solution no more optimal. following the considerations of section 2 on
i norm, this suggests that a di   erent regularizer may help the lapid166 solution
the (cid:107)f(cid:107)2
with newton   s method to gain the bene   ts of the id166 one. some steps in this direction
has been moved by tsang and kwok (2007), and we will investigate a similar approach, but
based on the primal problem, in future work.

finally, we are assuming that k and the matrix to invert on eq. 11 are non singular,
otherwise the    nal expansion of f will not be unique, even if the optimal value of the
criterion function of eq. 7 will be.

3.2 preconditioned conjugate gradient
instead of performing a costly newton   s step, the solution of the system     = 0 can be
computed by conjugate id119. in particular if we look at eq. 9, we can write the
system     = 0 as as hz = c,
hz = c =   (cid:18) 1t ie1

kie1 kie k +   ak +   i klk (cid:19) z =(cid:18) 1t ie y
kie y (cid:19) .

1t ie k

(12)

the convergence rate of conjugate gradient is related to the condition number of h (shewchuk,
1994). in the most general case, the presence of the terms kie k and klk leads to a not
so well conditioned system and to a slow convergence rate. fortunately the general    x
investigated by chapelle (2007) can be applied also in the case of lapid166s, due to the

11

quadratic form of the intrinsic regularizer. eq. 12 can be factorized as

(cid:18) 1 0t
0 k (cid:19)(cid:18) 1t ie1

ie1

1t ie k

ie k +   ai +   i lk (cid:19) z =(cid:18) 1 0t

0 k (cid:19)(cid:18) 1t ie y
ie y (cid:19) .

for instance, we can precondition the system of eq. 12 with the symmetic matrix

p =(cid:18) 1 0t
0 k (cid:19)

so that the condition number of the original system is sensibly decreased. in the precon-

ditioned gradient       = p    1    the two previously described terms are reduced to ie k and

lk. moreover, preconditioning is generally useful when such product can be e   ciently
computed and in our problem it comes at no additional computational cost. as in the
newton   s method, we are assuming that k is non singular, otherwise a small ridge can be
added to    x it.

classic rules for the update of the conjugate direction at each step are resumed by
shewchuk (1994). after some iterations the conjugacy of the descent directions tends to get
lost due to roundo       oating point error, so a restart of the preconditioned conjugate gra-
dient algorithm is required. the fletcher   reeves (fr) update is commonly used in linear
optimization. due to the piecewise nature of the problem, de   ned by the ie matrix, we ex-
ploited the pollak   ribier (pr) formula, where restart can be automatically performed when
the update term becomes negative (shewchuk, 1994)2. we experimentally evaluated that
for the lapid166 problem such formula is generally the best choice, both for convergence
speed and numerical stability. the iterative solution of lapid166 problem using precondi-
tioned conjugate gradient (pcg) is reported in algorithm 1. the    rst iteration is actually
a steepest descent one, and so it is after each restart of pcg, i.e. when    becomes zero in
algorithm 1.

convergence is usually declared when the norm of the preconditioned gradient falls below
a given threshold (chapelle, 2007), or when the current preconditioned gradient is roughly
orthogonal with the real gradient (shewchuk, 1994). we will investigate these conditions in
section 4.

3.2.1 line search
the optimal step length s    on the current direction of the pcg algorithm must be computed
by backtracking or exact line search. at a generic iteration t we have to solve

s    = arg min

s   0

obj(zt   1 + sdt   1)

(13)

where obj is the objective function of eq. 7.

the accuracy of the line search is crucial for the performance of pcg. when minimiz-
ing a quadratic form that leads to a linear expression of the gradient, line search can be
computed in closed form. in our case, we have to deal with the variations of the set e (and
of ie) for di   erent values of s, so that a closed form solution cannot be derived, and we
have to compute the optimal s in an iterative way.

2. note that in the linear case fr and pr are equivalent.

12

algorithm 1 preconditioned conjugate gradient (pcg) for primal lapid166s.
let t = 0, zt = 0, e = l,      t =    [1t y,   yt ]t , dt =      t
repeat
t = t + 1
find s    by line search on the line zt   1 + sdt   1
zt = zt   1 + s   dt   1
e = {xi     l s.t. (ki  t + bt)yi < 1}
     t =(cid:18)
1t ie(k  t + 1bt     y)
   = max(      tt
dt =          t +   dt   1
until goal condition

ie(k  t + 1bt     y) +   a  t +   i lk  t (cid:19)

     t   1t p      t   1

p (      t         t   1)

, 0)

due to the quadratic nature of eq. 13, the 1   dimensional newton   s method can be
directly used, but the average number of line search iterations per pcg step can be very
large, even if the cost of each of them is negligible with respect to the o(n2) of a pcg
iteration. we can e   ciently solve the line search problem analytically, as suggested by
keerthi and decoste (2006) for id166s.

in order to simplify the notation, we discard the iteration index t     1 in the following
description. given the pcg direction d, we compute for each point xi     l, being it an
error vector or not, the step length si for which its state switches. the state of a given
error vector switches when it leaves the e set, whether the state of a point initially not in e
switches when it becomes an error vector. we refer to the set of the former points with q1
while the latter is q2, with l = q1   q2. the derivative of eq. 13,   (s) =    obj(z + sd)/   s,
is piecewise linear, and si are the break points of such function.
let us consider, for simplicity, that si are in a non decreasing order, discarding the
negative ones. starting from s = 0, they de   ne a set of intervals where   (s) is linear and
the e set does not change. we indicate with   j(s) the linear portion of   (s) in the j   th
interval. starting with j = 1, if the value s     0 for which   j(s) crosses zero is within such
interval, then it is the optimal step size s   , otherwise the following interval must be checked.
the convergence of the process is guaranteed by the convexity of the function obj.

the zero crossing of   j(s) is given by s =

  j (0)     j (1), where the two points (0,   j(0))
and (1,   j(1)) determine the line   j(s). we indicate with fd(x) the function f(x) whose
coe   cients are in d = [db, dt

i d   + db, and we have

  ]t , i.e. fd(xi) = kt

  j (0)

  j(0) =(cid:80)xi   ej (f(xi)     yi)fd(xi) +   a  t kd   +   i   t klkd  
  j(1) =(cid:80)xi   ej (f(xi) + fd(xi)     yi)fd(xi) +   a(   + d  )t kd   +   i(   + d  )t klkd  
where ej is the set of error vectors for the j   th interval.
given   1(0) and   1(1), their successive values for increasing j can be easily computed
considering that only one point (that we indicate with xj) switches status moving from an

13

interval to the following one. from this consideration we derived the following update rules

  j+1(0) =   j(0) +   j(f(xj)     yj)fd(xj)
  j+1(1) =   j(1) +   j(f(xj) + fd(xj)     yi)fd(xj)

where   j is    1 if xj     q1 and it is +1 if r     q2.
3.2.2 complexity analysis

each pcg iteration requires to compute the k   product, leading to a complexity of o(n2)
to update the    coe   cients. the term lk   can then be computed e   ciently from k  ,
since the l matrix is generally sparse. note that, di   erently from the newton   s method and
from the original dual solution of the lapid166 problem, we never explicitly compute the
lk product, whereas we always compute matrix by vector products. even if l is sparse,
when the number of training point increases or l is iterated many times, a large amount
of time may be wasted in such matrix by matrix product, as we will show in section 6.
moreover, if the kernel matrix is sparse, the complexity drops to o(nnz), where nnz is the
maximum number of non null elements between k and l.

convergence of the conjugate gradient algorithm is theoretically declared in o(n) steps,
but a solution very close to the optimal one can be computed with far less iterations. the
convergence speed is related to the condition number of the hessian (shewchuk, 1994), that
it is composed by a sum of three contributes (eq. 12). as a consequence, their condition
numbers and weighting coe   cients (  a,   i) have a direct in   uence in the convergence speed,
and in particular the condition number of the k matrix. for example, using a bandwidth
of a gaussian kernel that lead to a k matrix close to the identity allows the algorithm to
converge very quickly, but the accuracy of the classi   er may not be su   cient.

finally, pcg can be e   ciently seeded with an initial rough estimate of the solution. this
can be crucial for an e   cient incremental building of the classi   er with reduced complexity,
following the one proposed for id166s by keerthi et al. (2006).

4. approximating the optimal solution

in order to reduce the training times, we want the pcg to converge as fast as possible to a
good approximation of the optimal solution. by appropriately selecting the goal condition
of algorithm 1, we can discard iterations that may not lead to signi   cant improvement in
the classi   er quality.

the common goal conditions for the pcg algorithm and, more generally, for gradient
based iterative algorithms, rely on the norm of the gradient (cid:107)   (cid:107) (boyd and vandenberghe,
2004), of the preconditioned gradient (cid:107)      (cid:107) (chapelle, 2007), on the mixed product(cid:112)      t   
(shewchuk, 1994). these values are usually normalized by the    rst estimate of each of
them. the value of the objective function obj or its relative decrement between two consec-
utive iterations can also be checked, requiring some additional computations since the pcg
algorithm never explicitly computes it. when one of such    stopping    values falls below
the chosen threshold    associated to it, the algorithm terminates3. moreover, a maximum

3. thresholds associated to di   erent conditions are obviously di   erent, but, for simplicity in the description,

we will refer to a generic threshold    .

14

number tmax of iterations is generally speci   ed. tuning these parameters is crucial both for
the time spent running the algorithm and the quality of the resulting solution.

it is really hard to    nd a trade   o    between good approximation and low number of
iterations, since    and tmax are strictly problem dependent. as an example, consider that
the surface of obj, the objective function of eq. 7, varies among di   erent choices of its
parameters. increasing or decreasing the values of   a and   i can lead to a less    at or a
more    at region around the optimal point. fixing in advance the values of    and tmax may
cause an early stop too far from the optimal solution, or it may result in the execution of a
large number of iterations without a signi   cant improvement on the classi   cation accuracy.

the latter situation can be particularly frequent for lapid166s. as described in section 2
the choice of the intrinsic norm f t lf introduces the soft constraint f(xi) = f(xj) for
nearby points xi, xj along the underlying manifold. this allows the algorithm to perform
a graph transduction and di   use the labels from points in l to the unlabeled data u.

when the di   usion is somewhat complete and the classi   cation hyperplane has assumed
a quite stable shape around the available training data, similar to the optimal one, the intrin-
sic norm will keep contributing to the gradient until a balance with respect to the ambient
norm (and to the l2 loss on error vectors) is found. due to the strictness of this constraint,
it will still require some iterations (sometimes many) to achieve the optimal solution with
(cid:107)   (cid:107) = 0, even if the decision function y(x) = sign(f(x)) will remain substantially the same.
the described common goal conditions do not    directly    take into account the decision of
the classi   er, so that they do not appear appropriate to early stop the pcg algorithm for
lapid166s.

we investigate our intuition on the    two moons    dataset of figure 3(a), where we com-
pare the decision boundary after each pcg iteration (figure 3(b)-(e)) with the optimal
solution (computed by newton   s method, figure 3(f)). starting with    = 0, the    rst itera-
tion exploits only the gradient of the l2 loss on labeled points, since both the regularizing
norms are zero. in the following iterations we can observe the label di   usion process along
the manifold. after only 4 iterations we get a perfect classi   cation of the dataset and a
separating boundary not far from the optimal one. all the remaining iterations until com-
plete convergence are used to slightly asses the coherence along the manifold required by the
intrinsic norm and the balancing with the smoothness of the function, as can be observed
by looking at the function values after 25 iterations. the most of changes in   uences regions
far from the support of px, and it is clear that an early stop after 4 pcg steps would be
enough to roughly approximate the accuracy of optimal solution.

in figure 4 we can observe the values of the previously described general stopping
criterion for pcg. after 4 iterations they are still sensibly decreasing, without re   ecting
real improvements in the classi   er quality. the value of the objective function obj starts to
become more stable only after, say, 16 iterations, but it is still slightly decreasing even if
it appears quite horizontal on the graph, due to its scale. it is clear that    xing in advance
the parameters    and tmax is random guessing and it will probably result in a bad trade   o   
between training time and accuracy.

15

(a) the    two moons    dataset

(b) 1 pcg iteration

(c) 4 pcg iterations (0% error rate)

(d) 8 pcg iterations

(e) 25 pcg iterations

(f) optimal solution

figure 3: (a) the    two moons    dataset (200 points, 2 classes, 2 labeled points indicated with
a red diamond and a blue circle, whereas the remaining points are unlabeled) - (b-
e) a lapid166 classi   er trained with pcg, showing the result after a    xed number
of iterations. the dark continuous line is the decision boundary (f(x) = 0) and
the con   dence of the classi   er ranges from red (f(x)     1) to blue (f(x)        1) -
(f) the optimal solution of the lapid166 problem computed by means of newton   s
method

16

   1012   1   0.500.511.5   1012   1   0.500.511.5   1012   1   0.500.511.5   1012   1   0.500.511.5   1012   1   0.500.511.5   1012   1   0.500.511.5figure 4: pcg example on the    two moons    dataset. the norm of the gradient (cid:107)   (cid:107), of the
preconditioned gradient (cid:107)      (cid:107), the value of the objective function obj and of the
mixed product(cid:112)      t    are displayed in function of the number of pcg iterations.
the vertical line represents the number of iterations after which the error rate is
0% and the decision boundary is quite stable.

4.1 early stopping conditions

following these considerations, we propose to early stop the pcg algorithm exploiting the
predictions of the classi   er on the available data. due to the high amount of unlabeled
training points in the semi   supervised learning framework, the stability of the decision
y(x), x     u, can be used as a reference to early stop the id119 (stability check).
moreover, if labeled validation data (set v) is available for classi   er parameters tuning, we
can formulate a good stopping condition based on the classi   cation accuracy on it (validation
check), that can be eventually merged to the previous one (mixed check).

in detail, when y(x) becomes quite stable between consecutive iterations or when err(v),
the error rate on v, is not decreasing anymore, then the pcg algorithm should be stopped.
due to their heuristic nature, it is generally better to compare the predictions every   
iterations and within a certain tolerance   . as a matter of fact, y(x) may slightly change
also when we are very close to the optimal solution, and err(v) is not necessarily an always
decreasing function. moreover, labeled validation data in the semi   supervised setting is
usually small with respect to the whole training data, labeled and unlabeled, and it may
not be enough to represent the structure of the dataset.

we propose very simple implementations of such conditions, that we used to achieve the
results of section 6. starting from these, many di   erent and more e   cient variants can be
formulated, but it goes beyond the scope of this paper. they are sketched in algorithms
2 and 3. we computed the classi   er decision every    n/2 iterations and we required the
classi   er to improve err(v) by one correctly classi   er example at every check, due to the
usually small size of v. sometimes this can also help to avoid a slight over   tting of the
classi   er.
generating the decision y(x) on unlabeled data does not require heavy additional ma-
chinery, since the k   product must be necessarily computed to perform every pcg it-

17

05101500.050.10.150.20.25tnormalized value  objk   kp        k     kalgorithm 2 the stability check for pcg stopping.

dold     0     iru
       1.5%
          n/2
every    iterations do the followings:
d = [y(xj), xj     u, j = 1, . . . , u]t
   = (100    (cid:107)d     dold(cid:107)1/u)%
if    <    then
stop pcg

else

dold = d

end if

algorithm 3 the validation check for pcg stopping.
require: v

errv old     100%
   1%
       100    |v|
          n/2
every    iterations do the followings:
if err(v) > (errv old       ) then
else

stop pcg

errv old = err(v)

end if

eration. its overall cost is o(u). di   erently, computing the accuracy on validation data
requires the evaluation of the id81 on validation points against the n training
ones, and o(|v|   n) products, that is negligible with respect to the cost of a pcg iteration.
finally, please note that even if these are generally early stopping conditions, sometimes
they can help in the opposite situation. for instance they can also detect that the classi   er
needs to move some more steps toward the optimal solution than the ones limited by the
selected tmax.

5. laplacian regularized least squares

laplacian regularized least square classi   er (laprlsc) has many analogies with the
proposed l2 hinge loss based lapid166s. laprlsc uses a squared id168 to penalize
wrongly classi   ed examples, leading to the following objective function

min
f   hk

l(cid:88)i=1

(yi     f(xi))2 +   a(cid:107)f(cid:107)2

a +   i(cid:107)f(cid:107)2
i .

(14)

18

the optimal    coe   cients and the optimal bias b, collected in the vector z, can be

obtained by solving the linear system

kil1 kilk +   ak +   i klk (cid:19) z =(cid:18) 1t y
(cid:18) |l|
ky (cid:19)

1t ilk

(15)

where il is the diagonal matrix     irn,n with the    rst l elements equal to 1 and the remaining
u elements equal to zero.
following the notation used for lapid166s, in laprlscs we have a set of error vectors
e that is actually    xed and equal to l. as a matter of fact a laprlsc requires the
estimated function to interpolate the given targets in order to not incur in a penalty. in a
hypothetic situation where all the labeled examples always belong to e during the training
of a lapid166 classi   er in the primal, then the solution will be the same of laprlsc.
solving the least squares problem of laprlsc can be performed by matrix inversion,
after factoring and simplifying the previously de   ned matrix p in eq. 15. otherwise the
proposed pcg approach and the early stopping conditions can be directly used. in this
case the classic instruments for linear optimization apply, and the required line search of
eq. 13 can be computed in closed form without the need of an iterative process,

s    =        t d

dt hd

where     and h are no more functions of e.
as shown by belkin et al. (2006); sindhwani and rosenberg (2008) and in the experi-
mental section of this paper, laprlsc, lapid166 and primal lapid166 allow us to achieve
similar classi   cation performances. the interesting property of the lapid166 problem is
that the e   ect of the id173 terms at a given iteration can be decoupled by the one
of the id168 on labeled points, since the gradient of the id168 for correctly
classi   ed points is zero and do not disturb classi   er design. this characteristic can be use-
ful as a starting point for the study of some alternative formulations of the intrinsic norm
regularizer.

6. experimental results

we ran a wide set of experiments to analyze the proposed solution strategies of the primal
lapid166 problem.
in this section we describe the selected datasets, our experimental
protocol and the details on the parameter selection strategy. then we show the main
result of the proposed approach, very fast training of the lapid166 classi   er with reduced
complexity by means of early stopped pcg. we compare the quality of the l2 hinge loss
lapid166s trained in the primal by newton   s method with respect to the l1 hinge loss dual
formulation and laprlscs. finally, we describe the convergence speed and the impact on
performances of our early stopping conditions.

as a baseline reference for the performances in the supervised setting, we selected two
popular regularized classi   ers, support vector machines (id166s) and regularized least
square classi   ers (rlscs). we implemented and tested all the algorithms using matlab
7.6 on a 2.33ghz machine with 6gb of memory. the dual problem of lapid166 has been
solved using the latest version of libid166 (fan et al., 2005). multiclass classi   cation has
been performed using the one   against   all approach.

19

dataset
g50c
coil20(b)
pcmac
uspst(b)
coil20
uspst
mnist3vs8
facemit

classes

2
2
2
2
20
10
2
2

50

size attributes
550
1440
1946
2007
1440
2007
13966
31022

1024
7511
256
1024
256
784
361

table 1: details of the datasets that have been used in the experiments.

6.1 datasets

we selected eight popular datasets for our experiments. most of them datasets has been
already used in previous works to evaluate several semi   supervised classi   cation algorithms
(sindhwani et al., 2005; belkin et al., 2006; sindhwani and rosenberg, 2008), and all of
them are available on the web. g50c4 is an arti   cial dataset generated from two unit
covariance normal distributions with equal probabilities. the class means are adjusted so
that the bayes error is 5%. the coil20 dataset is a collection of pictures of 20 di   erent
objects from the columbia university. each object has been placed on a turntable and at
every 5 degrees of rotation a 32x32 gray scale image was acquired. the uspst dataset is
a collection of handwritten digits form the usps postal system. images are acquired at
the resolution of 16x16 pixels. uspst refers to the test split of the original dataset. we
analyzed the coil20 and uspst dataset in their original 20 and 10   class versions and also
in their 2   class versions, to discard the e   ects on performances of the selected multiclass
strategy. coil20(b) discriminates between the    rst 10 and the last 10 objects, whereas
uspst(b) from the    rst 5 digits and the remaining ones. pcmac is a two   class dataset
generated from the famous 20   newsgroups collection, that collects posts on windows and
macintosh systems. mnist3vs8 is the binary version of the mnist dataset, a collection
of 28x28 gray scale handwritten digit images from nist. the goal is to separate digit 3
from digit 8. finally, the facemit dataset of the center for biological and computational
learning at mit contains 19x19 gray scale, pgm format, images of faces and non   faces.
the details of the described datasets are resumed in table 1.

6.2 experimental protocol

all presented results has been obtained by averaging them on di   erent splits of the available
data.
in particular, a 4   fold cross   validation has been performed, randomizing the fold
generation process for 3 times, for a total of 12 splits. each fold contains the same number
of per class examples as in the complete dataset. for each split, we have 3 folds that are
used for training the classi   er and the remaining one that constitutes the test set (t ).
training data has been divided in labeled (l), unlabeled (u) and validation sets (v), where
the last one is only used to tune the classi   er parameters. the labeled and validation sets
have been randomly selected from the training data such that at least one example per class
is assured to be present on each of them, without any additional balancing constraints. a

4. it can be downloaded from http://people.cs.uchicago.edu/~ vikass/manifoldid173.html.

20

dataset
g50c
coil20(b)
pcmac
uspst(b)
coil20
uspst
mnist3vs8
facemit

|l|
50
40
50
50
40
50
80
2

|u|
314
1000
1358
1409
1000
1409
11822
23973

|v|
50
40
50
50
40
50
80
50

|t |
136
360
488
498
360
498
1984
6997

table 2: the number of data points in each split of the selected datasets, where l and u
are the sets of labeled and unlabeled training points, respectively, v is the labeled
set for cross   validating parameters whereas t is the out   of   sample test set.

small number of labeled points has been generally selected, in order to simulate a semi   
supervised scenario where labeling data has a large cost. the mnist3vs8 and facemit
dataset are already divided in training and test data, so that the 4   fold generation process
was not necessary, and just the random subdivision of training data has been performed.
in particular, on the facemit dataset we exchanged the original training and test sets,
since, as a matter of fact, the latter is sensibly larger that the former.
in this case our
goal is just to show how we were able to handle a high amount of training data using the
proposed primal solution with pcg, whereas it was not possible to do it with the original
dual formulation of lapid166. due to the high unbalancing of such dataset, we report the
macro error rates for it (1    t p/2+ t n/2, where t p and t n are the rates of true positives
and true negatives). details are collected in table 2.

6.3 parameters

||xi   xj||

2  2

we selected a gaussian id81 in the form k(xi, xj) = exp   
for each
experiment, with the exception of the mnist3vs8 where a polynomial kernel of degree
9 was used, as suggest by decoste and sch  olkopf (2002). the other parameters were se-
in order to speedup this step, the values
lected by cross   validating them on the v set.
of the gaussian kernel width and of the parameters required to build the graph lapla-
cian (the number of neighbors, nn, and the degree, p) for the    rst six datasets were    xed
as speci   ed by sindhwani and rosenberg (2008). for details on the selection of such pa-
rameters please refer to sindhwani and rosenberg (2008); sindhwani et al. (2005). the
graph laplacian was computed by using its normalized expression. the optimal weights
of the ambient and intrinsic norms,   a,   i, were determined by varying them on the grid
{10   6, 10   4, 10   2, 10   1, 1, 10, 100} and chosen with respect to validation error. for the
facemit dataset also the value 10   8 was considered, due to the high amount of training
points. the selected parameter values are reported in table 8 of appendix a for repro-
ducibility of the experiments.

21

dataset

g50c
coil20(b)
pcmac
uspst(b)
coil20
uspst
mnist3vs8
facemit

laplacian id166s

dual (original) primal (newton) primal (pcg)
0.155 (0.004)
0.311 (0.012)
14.82 (0.104)
1.196 (0.015)
6.321 (0.441)
12.25 (0.2)
2064.18 (3.1)
-

0.134 (0.006)
0.367 (0.097)
15.756 (0.285)
1.4727 (0.2033)
7.26 (1.921)
17.74 (2.44)
2824.174 (105.07)
-

0.043 (0.006)
0.097 (0.026)
1.967 (0.269)
0.300 (0.030)
3.487 (1.734)
2.032 (0.434)
114.441 (0.235)
35.728 (0.868)

table 3: our main result. training times (in seconds) of laplacian id166s using di   erent
algorithms (standard deviation in brackets). the time required to solve the original
dual formulation and the primal solution with newton   s method are comparable,
whereas solving the laplacian id166s problem in the primal with early stopped
preconditioned conjugate gradient (pcg) o   ers a noticeable speedup.

6.4 results

before going further into details, the training times of lapid166s using the original dual
formulation and the primal one are reported in table 3, to empathize our main result5.
the last column refers to lapid166s trained using the best (in terms of accuracy) of the
proposed stopping heuristics for each speci   c dataset. as expected, training in the primal
by the newton   s method requires training times similar to the ones of the dual formulation.
on the other hand, training by pcg with the proposed early stopping conditions shows an
appreciable reduction of them on all datasets. as the size of labeled and unlabeled points
increases, the improvement becomes very evident. in the mnist3vs8 dataset we drop from
roughly half an hour to two minutes. both in the dual formulation of lapid166s and in the
primal one solved by means of newton   s method, a lot of time is spent in computing the
lk matrix product. even if l is sparse, as its size increases or when it is iterated the cost
of this product becomes quite high. it is also the case of the pcmac dataset, where the
training time drops from 15 seconds to only 2 seconds when solving with pcg. finally, also
the memory requirements are reduced, since there is no need to explicitly compute, store
and invert the hessian when pcg is used. as an example, we trained the classi   er on the
facemit dataset only using pcg. the high memory requirements of dual lapid166 and
primal lapid166 solved with newton   s method, coupled with the high computational cost
and slow training times, made the problem intractable for such techniques on our machine.
we investigate now the details of the solution of the primal lapid166 problem. in order
to compare the e   ects of the di   erent id168s of laprlscs, lapid166s trained in the
dual, and lapid166s trained in the primal, in table 4 the classi   cation errors of the described
techniques are reported. for this comparison, the optimal solution of primal lapid166s is
computed by means of the newton   s method. the manifold id173 based techniques
lead to comparable results, and, as expected, all semi   supervised approaches show a sensible
improvement over classic supervised classi   cation algorithms. the error rates of primal
lapid166s and laprlscs are really close, due to the described relationship of the l2 hinge

5. for a fair comparison of the training algorithms, the gram matrix and the laplacian were precomputed.

22

loss and the squared loss. we collected the average number of newton   s steps required
to compute the optimal solution in table 5.
in all our experiments we always declared
convergence in less than 6 steps.

in figure 5-12 we compared the error rates of lapid166s trained in the primal by new-
ton   s method with ones of pcg training, in function of the number of gradient steps t. for
this comparison,   a and   i were selected by cross   validating with the former (see appendix
a). the horizontal line on each graph represents the error rate of the optimal solution com-
puted with the newton   s method. the number of iterations required to converge to a
solution with the same accuracy of the optimal one is sensibly smaller than n. convergence
is achieved really fast, and only in the coil20 dataset we experienced a relatively slower
rate with respect to the other datasets. the error surface of each binary classi   er is quite    at
around optimum with the selected   a and   i, leading to some round   o    errors in gradient
descent based techniques, stressed by the large number of classes and the one   against   all
approach. moreover labeled training examples are highly unbalanced. as a matter of fact,
in the coil20(b) dataset we did not experience this behavior. finally, in the facemit
dataset the algorithm perfectly converges in a few iterations, showing that in this dataset
the most of information is contained in the labeled data (even if it is very small), and the
intrinsic constraint is easily ful   lled.

figure 5: g50c dataset: error rate on l, u, v, t of the laplacian id166 classi   er trained
in the primal by preconditioned conjugate gradient (pcg), with respect to the
number of gradient steps t. the error rate of the primal solution computed by
means of newton   s method is reported as a horizontal line.

in figure 13-14 we collected the values of the gradient norm (cid:107)   (cid:107), of the preconditioned
gradient norm (cid:107)      (cid:107), of the mixed product (cid:112)      t   , and of the objective function obj for
each dataset, normalized by their respective values at t = 0. the vertical line is an indica-
tive index of the number of iterations after which the error rate on all partitions (l, u,
v, t ) becomes equal to the one at the optimal solution. the curves generally keep sen-
sibly decreasing even after such line, without re   ecting real improvements in the classi   er
accuracy, and they di   er by orders of magnitude among the considered dataset, showing

23

012345678901020304050terror rate (%)  pcg(l)newton(l)012345678901020304050terror rate (%)  pcg(u)newton(u)012345678901020304050terror rate (%)  pcg(v)newton(v)012345678901020304050terror rate (%)  pcg(t)newton(t)dataset

g50c

coil20(b)

pcmac

uspst(b)

coil20

uspst

mnist3vs8

classi   er
id166
rlsc
laprlsc
lapid166 dual (original)
lapid166 primal (newton)

id166
rlsc
laprlsc
lapid166 dual (original)
lapid166 primal (newton)

id166
rlsc
laprlsc
lapid166 dual (original)
lapid166 primal (newton)

id166
rlsc
laprlsc
lapid166 dual (original)
lapid166 primal (newton)

id166
rlsc
laprlsc
lapid166 dual (original)
lapid166 primal (newton)

id166
rlsc
laprlsc
lapid166 dual (original)
lapid166 primal (newton)

id166
rlsc
laprlsc
lapid166 dual (original)
lapid166 primal (newton)

u
9.33 (2)
10.43 (5.26)
6.03 (1.32)
5.52 (1.15)
6.16 (1.48)

16.23 (2.63)
16.22 (2.64)
8.067 (2.05)
8.31 (2.19)
8.16 (2.04)

19.65 (6.91)
19.63 (6.91)
9.67 (0.74)
10.78 (1.83)
9.68 (0.77)

17 (2.74)
17.21 (3.02)
8.87 (1.88)
8.84 (2.2)
8.72 (2.15)

29.49 (2.24)
29.51 (2.23)
10.35 (2.3)
10.51 (2.06)
10.54 (2.03)

23.84 (3.26)
23.95 (3.53)
15.12 (2.9)
14.36 (2.55)
14.98 (2.88)

8.82 (1.11)
8.82 (1.11)
1.95 (0.05)
2.29 (0.17)
2.2 (0.14)

v
9.83 (3.46)
10.17 (4.86)
6.17 (3.66)
5.67 (2.67)
6.17 (3.46)

18.54 (6.2)
18.54 (6.17)
7.92 (3.96)
8.13 (4.01)
7.92 (3.96)

20.83 (6.85)
20.67 (6.95)
7.67 (4.08)
9.17 (4.55)
7.83 (4.04)

18.17 (5.94)
17.5 (5.13)
10.17 (4.55)
8.67 (4.38)
9.33 (3.85)

31.46 (7.79)
31.46 (7.79)
9.79 (4.94)
9.79 (4.94)
9.79 (4.94)

24.67 (4.54)
25.33 (4.03)
14.67 (3.94)
15.17 (4.04)
15 (3.57)

7.92 (4.73)
7.92 (4.73)
1.67 (1.44)
1.67 (1.44)
1.67 (1.44)

facemit

id166
rlsc
lapid166 primal (pcg)

39.8 (2.34)
39.8 (2.34)
29.97 (2.51)

38 (1.15)
38 (1.15)
36 (3.46)

t
10.06 (2.8)
11.21 (4.98)
6.54 (2.11)
5.51 (1.65)
7.27 (2.87)

15.93 (3)
15.97 (3.02)
8.59 (1.9)
8.68 (2.04)
8.56 (1.9)

20.09 (6.91)
20.04 (6.93)
9.34 (1.5)
11.05 (2.94)
9.37 (1.51)

17.1 (3.21)
17.27 (2.72)
9.42 (2.51)
9.68 (2.48)
9.42 (2.34)

28.98 (2.74)
28.96 (2.72)
11.3 (2.17)
11.44 (2.39)
11.32 (2.19)

23.6 (2.32)
24.01 (3.43)
16.44 (3.53)
14.91 (2.83)
15.38 (3.55)

8.22 (1.36)
8.22 (1.36)
1.8 (0.3)
1.98 (0.15)
2.02 (0.22)

34.61 (3.96)
34.61 (3.96)
27.97 (5.38)

table 4: comparison of the accuracy of lapid166s trained by solving the primal (newton   s
method) or the dual problem. the average classi   cation error (standard deviation
is reported brackets) is reported. fully supervised classi   ers (id166s, rlscs) rep-
resent the baseline performances. u is the set of unlabeled examples used to train
the semi   supervised classi   ers. v is the labeled set for cross   validating parameters
whereas t is the out   of   sample test set. results on the labeled training set l are
omitted since all algorithms correctly classify such a few labeled training points.

24

dataset
g50c
coil20(b)
pcmac
uspst(b)
coil20
uspst
mnist3vs8

newton   s steps

1 (0)
2.67 (0.78)
2.33 (0.49)
4.17 (0.58)
2.67 (0.75)
4.26 (0.76)
5 (0)

table 5: newton   s steps required to compute the optimal solution of the primal laplacian

id166 problem.

figure 6: coil20(b) dataset: error rate on l, u, v, t of the laplacian id166 classi   er
trained in the primal by preconditioned conjugate gradient (pcg), with respect
to the number of gradient steps t. the error rate of the primal solution computed
by means of newton   s method is reported as a horizontal line.

their strong problem dependency (di   erently from our proposed conditions). as described
in section 4, we can see how it is clearly impossible to de   ne a generic threshold on them to
appropriately stop the pcg descent (i.e. to    nd a good trade   o    between number of itera-
tions and accuracy). moreover, altering the values of the classi   er parameters can sensibly
change the shape of the error function, requiring a di   erent threshold every time. in those
datasets where points keep entering and leaving the e set as t increases (mainly during the
   rst steps) the norm of the gradient can show an instable behavior between consecutive
iterations, due to the piecewise nature of the problem, making the threshold selection task
ulteriorly complex. this is the case of the pcmac and uspst(b) dataset. in the mnist
data, the elements of kernel matrix non belonging to the main diagonal are very small due
to the high degree of the polynomial kernel, so that the gradient and the preconditioned
gradient are close.

using the proposed pcg goal conditions (section 4), we cross   validated the primal
lapid166 classi   er trained by pcg, and the selected parameters are reported in table 9 of

25

01020304050607001020304050terror rate (%)  pcg(l)newton(l)01020304050607001020304050terror rate (%)  pcg(u)newton(u)01020304050607001020304050terror rate (%)  pcg(v)newton(v)01020304050607001020304050terror rate (%)  pcg(t)newton(t)figure 7: pcmac dataset: error rate on l, u, v, t of the laplacian id166 classi   er trained
in the primal by preconditioned conjugate gradient (pcg), with respect to the
number of gradient steps t. the error rate of the primal solution computed by
means of newton   s method is reported as a horizontal line.

figure 8: uspst(b) dataset: error rate on l, u, v, t of the laplacian id166 classi   er
trained in the primal by preconditioned conjugate gradient (pcg), with respect
to the number of gradient steps t. the error rate of the primal solution computed
by means of newton   s method is reported as a horizontal line.

appendix a. in the uspst(b), coil20(b), and mnist3vs8 datasets, larger values for   a
or   i are selected by the validation process, since the convergence speed of pcg is enhanced.
in the other datasets, parameter values remain substantially the same of the ones selected
by solving with the newton   s method, suggesting that a reliable and fast cross   validation
can be performed with pcg and the proposed early stopping heuristics.

26

02468101214161801020304050terror rate (%)  pcg(l)newton(l)02468101214161801020304050terror rate (%)  pcg(u)newton(u)02468101214161801020304050terror rate (%)  pcg(v)newton(v)02468101214161801020304050terror rate (%)  pcg(t)newton(t)05010015020025030001020304050terror rate (%)  pcg(l)newton(l)05010015020025030001020304050terror rate (%)  pcg(u)newton(u)05010015020025030001020304050terror rate (%)  pcg(v)newton(v)05010015020025030001020304050terror rate (%)  pcg(t)newton(t)figure 9: coil20 dataset: error rate on l, u, v, t of the laplacian id166 classi   er trained
in the primal by preconditioned conjugate gradient (pcg), with respect to the
number of gradient steps t. the error rate of the primal solution computed by
means of newton   s method is reported as a horizontal line.

figure 10: uspst dataset: error rate on l, u, v, t of the laplacian id166 classi   er trained
in the primal by preconditioned conjugate gradient (pcg), with respect to the
number of gradient steps t. the error rate of the primal solution computed by
means of newton   s method is reported as a horizontal line.

in table 6 the training times, the number of pcg and line search iterations are collected,
whereas in table 7 the corresponding classi   cation error rates are reported, for a comparison
with the optimal solution computed using newton   s method. as already stressed, the
training times appreciably drop down when training a lapid166 in the primal using pcg
and our goal conditions, independently by the dataset. early stopping allows us to obtain
results comparable to the newton   s method or to the original two step dual formulation,

27

05010015020025030035040045001020304050terror rate (%)  pcg(l)newton(l)05010015020025030035040045001020304050terror rate (%)  pcg(u)newton(u)05010015020025030035040045001020304050terror rate (%)  pcg(v)newton(v)05010015020025030035040045001020304050terror rate (%)  pcg(t)newton(t)01020304050607001020304050terror rate (%)  pcg(l)newton(l)01020304050607001020304050terror rate (%)  pcg(u)newton(u)01020304050607001020304050terror rate (%)  pcg(v)newton(v)01020304050607001020304050terror rate (%)  pcg(t)newton(t)figure 11: mnist3vs8 dataset: error rate on l, u, v, t of the laplacian id166 classi   er
trained in the primal by preconditioned conjugate gradient (pcg), with respect
to the number of gradient steps t. the error rate of the primal solution computed
by means of newton   s method is reported as a horizontal line.

figure 12: facemit dataset: error rate on l, u, v, t of the laplacian id166 classi   er
trained in the primal by preconditioned conjugate gradient (pcg), with respect
to the number of gradient steps t. the error rate of the primal solution computed
by means of a very large set of pcg iterations is reported as a horizontal line.

showing a direct correlation between the proposed goal conditions and the quality of the
classi   er. moreover, our conditions are the same for each problem or dataset, overcoming
all the issues of the previously described ones.
in the coil20 dataset we can observe
performances less close to the one of the solution computed with newton   s method. this is
due to the already addressed motivations, and it also suggests that the stopping condition
should probably be checked while training in parallel the 20 binary classi   ers, instead of

28

05010015020001020304050terror rate (%)  pcg(l)newton(l)05010015020001020304050terror rate (%)  pcg(u)newton(u)05010015020001020304050terror rate (%)  pcg(v)newton(v)05010015020001020304050terror rate (%)  pcg(t)newton(t)0123456701020304050terror rate (%)  pcg(l)pcg(manyiters)(l)0123456720253035404550terror rate (%)  pcg(u)pcg(manyiters)(u)0123456720253035404550terror rate (%)  pcg(v)pcg(manyiters)(v)0123456720253035404550terror rate (%)  pcg(t)pcg(manyiters)(t)g50c

coil20(b)

pcmac

uspst(b)

figure 13: details of each pcg iteration. the value of the objective function obj, of the
gradient norm (cid:107)   (cid:107), of the preconditioned gradient norm (cid:107)      (cid:107), and of the mixed
product(cid:112)      t    are displayed in function of the number of pcg iterations (t).
the vertical line represents the number of iterations after which the error rate
is roughly the same of the one at the optimal solution.

separately checking it on each of them. a better tuning of the goal conditions or a di   erent
formulation of them can move the accuracy closer to the one of primal lapid166 trained
with newton   s method, but it goes beyond to the scope of this paper.
the number of pcg iterations is noticeably smaller than n. obviously it is function of
the gap between each checking of a stopping criterion, that we set to    n/2. the number
of iterations from the stability check is sometimes larger that the one from the validation
check (coil20(b), uspst, coil20). as a matter of fact, labeled validation data is more
informative than a stable, but unknown, decision on the unlabeled one. on the other hand
validation data could not represent test data enough accurately. using a mixed strategy
makes sense in those cases, as can be observed in the coil20 dataset. in our experiments
the mixed criterion has generally the same behavior of the most strict of the two heuristics
for each speci   c set of data. in the facemit dataset complete convergence is achieved in

29

01020304000.10.20.30.40.50.60.70.80.91tnormalized value  objk   kp        k     k01020304050607000.010.020.030.040.050.060.070.080.090.1tnormalized value  objk   kp        k     k01020304050607000.10.20.30.40.50.60.70.80.91tnormalized value  objk   kp        k     k05010015020025000.0050.010.0150.020.0250.03tnormalized value  objk   kp        k     kcoil20

uspst

mnist3vs8

facemit

figure 14: details of each pcg iteration. the value of the objective function obj, of the
gradient norm (cid:107)   (cid:107), of the preconditioned gradient norm (cid:107)      (cid:107), and of the mixed
product(cid:112)      t    are displayed in function of the number of pcg iterations (t).
the vertical line represents the number of iterations after which the error rate
is roughly the same of the one at the optimal solution.

just a few iterations, independently by the heuristics. the number of line search iterations
is usually very small and negligible with respect to the computational cost of the training
algorithm.

7. conclusions and future work

in this paper we described investigated in detail two strategies for solving the optimization
problem of laplacian support vector machines (lapid166s) in the primal. a very fast
solution can be achieved using preconditioned conjugate gradient coupled with an early
stopping criterion based on the stability of the classi   er decision. detailed experimental
results on real world data show the validity of such strategy. the computational cost for
solving the problem reduces from o(n3) to o(n2), where n is the total number of training

30

02040608010012014016000.0050.010.0150.020.0250.030.0350.040.0450.05tnormalized value  objk   kp        k     k01020304050607000.050.10.150.20.25tnormalized value  objk   kp        k     k05010015020000.0010.0020.0030.0040.0050.0060.0070.0080.0090.01tnormalized value  objk   kp        k     k012345600.10.20.30.40.50.60.70.80.91x 10   5tnormalized value  objk   kp        k     kdataset

g50c

coil20(b)

pcmac

uspst(b)

coil20

uspst

mnist3vs8

laplacian id166
dual
newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

dual
newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

dual
newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

dual
newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

dual
newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

dual
newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

dual
newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

training time
0.155 (0.004)
0.134 (0.006)
0.044 (0.006)
0.043 (0.006)
0.044 (0.006)

0.311 (0.012)
0.367 (0.097)
0.198 (0.074)
0.097 (0.026)
0.206 (0.089)

14.8203 (0.104)
15.756 (0.285)
1.897 (0.040)
1.967 (0.269)
4.610 (1.602)

1.196 (0.015)
1.4727 (0.2033)
0.300 (0.030)
0.281 (0.086)
0.324 (0.059)

6.321 (0.441)
7.26 (1.921)
3.297 (1.471)
1.769 (0.299)
3.487 (1.734)

12.25 (0.2)
17.74 (2.44)
1.953 (0.403)
2.032 (0.434)
2.158 (0.535)

2064.18 (3.1)
2824.174 (105.07)
114.441 (0.235)
124.69 (0.335)
124.974 (0.414)

facemit

pcg [stability check]
pcg [validation check]
pcg [mixed check]

35.728 (0.868)
35.728 (0.868)
35.728 (0.868)

pcg iters
-
-
20 (0)
20.83 (2.89)
20.83 (2.89)

-
-
74.67 (28.4)
37.33 (10.42)
78.67 (34.42)

-
-
38.00 (0)
39.58 (5.48)
91.83 (32.24)

-
-
58.58 (5.48)
55.42 (17.11)
63.33 (12.38)

-
-
65.47 (30.35)
34.07 (6.12)
69.53 (35.86)

-
-
41.17 (8.65)
42.91 (9.38)
45.60 (11.66)

-
-
110 (0)
110 (0)
110 (0)

3 (0)
3 (0)
3 (0)

ls iters
-
-
1 (0)
1 (0)
1 (0)

-
-
2.41 (1.83)
1 (0)
2.38 (1.79)

-
-
1.16 (0.45)
1.15 (0.44)
3.70 (3.09)

-
-
1.74 (0.90)
1.68 (0.90)
1.70 (0.89)

-
-
2.53 (1.90)
3.37 (2.22)
2.48 (1.87)

-
-
3.11 (1.73)
3.13 (1.73)
3.12 (1.72)

-
-
5.58 (2.79)
5.58 (2.79)
5.58 (2.79)

1 (0)
1 (0)
1 (0)

table 6: training time comparison among the laplacian id166s trained in the dual (dual),
lapid166 trained in the primal by means of newton   s method (newton) and by
means of preconditioned conjugate gradient (pcg) with the proposed early stop-
ping conditions (in square brackets). average training times (in seconds) and
their standard deviations, the number of pcg iterations, and of line search (ls)
iterations (per each pcg one) are reported.

31

dataset

g50c

coil20(b)

pcmac

uspst(b)

coil20

uspst

mnist3vs8

laplacian id166
newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

u
6.16 (1.48)
6.13 (1.46)
6.16 (1.48)
6.16 (1.48)

8.16 (2.04)
8.81 (2.23)
8.32 (2.28)
8.84 (2.28)

9.68 (0.77)
9.65 (0.78)
9.67 (0.76)
9.79 (0.72)

8.72 (2.15)
9.11 (2.14)
9.10 (2.17)
9.09 (2.17)

10.54 (2.03)
12.42 (2.68)
13.07 (2.73)
12.43 (2.69)

14.98 (2.88)
15.60 (3.45)
15.40 (3.38)
15.45 (3.53)

2.2 (0.14)
2.11 (0.06)
2.11 (0.06)
2.11 (0.06)

v
6.17 (3.46)
6.17 (3.46)
6.17 (3.46)
6.17 (3.46)

7.92 (3.96)
8.13 (3.71)
8.96 (4.05)
8.13 (3.71)

7.83 (4.04)
7.83 (4.04)
7.83 (4.04)
7.67 (3.80)

9.33 (3.85)
10.50 (4.36)
10.50 (4.36)
10.50 (4.36)

9.79 (4.94)
10.63 (4.66)
12.08 (4.75)
10.42 (4.63)

15 (3.57)
15.67 (3.60)
15.67 (3.98)
15.50 (3.92)

1.67 (1.44)
1.67 (1.44)
1.67 (1.44)
1.67 (1.44)

facemit

pcg [stability check]
pcg [validation check]
pcg [mixed check]

29.97 (2.51)
29.97 (2.51)
29.97 (2.51)

36 (3.46)
36 (3.46)
36 (3.46)

t
7.27 (2.87)
7.27 (2.87)
7.27 (2.87)
7.27 (2.87)

8.56 (1.9)
8.84 (1.93)
8.45 (1.58)
8.84 (1.96)

9.37 (1.51)
9.42 (1.50)
9.40 (1.50)
9.42 (1.28)

9.42 (2.34)
9.70 (2.55)
9.75 (2.59)
9.70 (2.55)

11.32 (2.19)
12.92 (2.14)
13.52 (2.12)
12.87 (2.20)

15.38 (3.55)
16.11 (3.95)
15.94 (4.04)
15.94 (4.08)

2.02 (0.22)
1.93 (0.2)
1.93 (0.2)
1.93 (0.2)

27.97 (5.38)
27.97 (5.38)
27.97 (5.38)

table 7: average classi   cation error (standard deviation is reported brackets) of lapla-
cian id166s trained in the primal by means of newton   s method (newton) and of
preconditioned conjugate gradient (pcg) with the proposed early stopping con-
ditions (in square brackets). u is the set of unlabeled examples used to train the
classi   ers. v is the labeled set for cross   validating parameters whereas t is the
out   of   sample test set. results on the labeled training set l are omitted since all
algorithms correctly classify such a few labeled training points.

32

points, both labeled and unlabeled, without the need of storing in memory the hessian
matrix and its inverse. training times are signi   cantly reduced on all selected benchmarks,
in particular, as the amount of training data increases. this solution can be a useful starting
point for applying greedy techniques for incremental classi   er building or for studying the
e   ects of a sparser kernel expansion of the classi   cation function, that we will address in
future work.

references

j. abernethy, o. chapelle, and c. castillo. witch: a new approach to web spam detection.

technical report 2008-001, yahoo! research, 2008.

m. belkin and p. niyogi. using manifold stucture for partially labeled classi   cation. ad-

vances in neural information processing systems, pages 953   960, 2003.

m. belkin and p. niyogi. towards a theoretical foundation for laplacian-based manifold

methods. journal of computer and system sciences, 74(8):1289   1308, 2008.

m. belkin, p. niyogi, and v. sindhwani. manifold id173: a geometric framework
for learning from labeled and unlabeled examples. the journal of machine learning
research, 7:2399   2434, 2006.

s.p. boyd and l. vandenberghe. id76. cambridge university press, 2004.

o. chapelle. training a support vector machine in the primal. neural computation, 19(5):

1155   1178, 2007.

o. chapelle, j. weston, and b. sch  olkopf. cluster kernels for semi-supervised learning.
in advances in neural information processing systems, pages 585   592. cambridge, ma,
usa: mit press, 2003.

o. chapelle, b. sch  olkopf, and a. zien. semi-supervised learning. mit press, 2006.

o. chapelle, v. sindhwani, and s.s. keerthi. optimization techniques for semi-supervised
support vector machines. the journal of machine learning research, 9:203   233, 2008.

d. decoste and b. sch  olkopf. training invariant support vector machines. machine learn-

ing, 46(1):161   190, 2002.

a. demiriz and k. bennett. optimization approaches to semi-supervised learning. com-

plementarity: applications, algorithms and extensions, 50:1   19, 2000.

r.e. fan, p.h. chen, and c.j. lin. working set selection using second order information
for training support vector machines. the journal of machine learning research, 6:
1889   1918, 2005.

t. joachims. transductive id136 for text classi   cation using support vector machines. in
proceedings of the international conference on machine learning, pages 200   209, 1999.

33

t. joachims. transductive learning via spectral graph partitioning. in proceedings of the

international conference on machine learning, volume 20, pages 290   297, 2003.

t. joachims. training linear id166s in linear time. in proceedings of the 12th acm sigkdd
international conference on knowledge discovery and data mining, pages 217   226. acm
new york, ny, usa, 2006.

s.s. keerthi and d. decoste. a modi   ed    nite id77 for fast solution of large

scale linear id166s. the journal of machine learning research, 6(1):341, 2006.

s.s. keerthi, o. chapelle, and d. decoste. building support vector machines with reduced

classi   er complexity. the journal of machine learning research, 7:1493   1515, 2006.

m. seeger. low rank updates for the cholesky decomposition. department of eecs,

university of california at berkeley, technical report, 2008.

s. shalev-shwartz, y. singer, and n. srebro. pegasos: primal estimated sub-gradient
solver for id166. in proceedings of the international conference on machine learning,
pages 807   814, 2007.

j.r. shewchuk. an introduction to the conjugate gradient method without the agonizing

pain. school of computer science, carnegie mellon university, techical report, 1994.

v. sindhwani. on semi-supervised kernel methods. phd thesis, university of chicago,

2007.

v. sindhwani and d.s. rosenberg. an rkhs for multi-view learning and manifold co-
in proceedings of the international conference on machine learning,

id173.
pages 976   983, 2008.

v. sindhwani, p. niyogi, and m. belkin. beyond the point cloud:

from transductive to
in proceedings of the international conference on machine

semi-supervised learning.
learning, volume 22, pages 825   832, 2005.

i.w. tsang and j.t. kwok. large-scale sparsi   ed manifold id173. advances in

neural information processing systems, 19:1401, 2007.

v.n. vapnik. the nature of statistical learning theory. springer, 2000.

x. zhu and a.b. goldberg. introduction to semi-supervised learning. morgan and clay-

pool, 2009.

x. zhu, z. ghahramani, and j. la   erty. semi-supervised learning using gaussian    elds and
id94. in proceedings of the international conference on machine learning,
volume 20, 2003.

34

appendix a.

this appendix collects all the parameters selected using our experimental protocol, for
reproducibility of the experiments (table 8 and table 9). details of the cross   validation
procedure are described in section 6.

in the most of the datasets, parameter values selected using the pcg solution remain
substantially the same of the ones selected by solving the primal problem with the newton   s
method, suggesting that a reliable and fast cross   validation can be performed with pcg and
the proposed early stopping heuristics. in the uspst(b), coil20(b), and mnist3vs8
datasets, larger values for   a or   i are selected when using pcg, since the convergence
speed of id119 is enhanced.

to emphasize this behavior, the training times and the resulting error rates of the pcg
solution computed using   a and   i tuned by means of the newton   s method (instead of
the ones computed by pcg with each speci   c goal condition) are reported in table 10
and in table 11. comparing these results with the ones presented in section 6, it can
be appreciated that both the convergence speed (table 6) and the accuracy of the pcg
solution (table 7) bene   t from an appropriate parameter selection.

35

dataset

g50c

classi   er
id166
rlsc
laprlsc
lapid166 dual (original)
lapid166 primal (newton)

  

17.5
17.5
17.5
17.5
17.5

coil20(b)

pcmac

uspst(b)

coil20

uspst

mnist3vs8

facemit

id166
rlsc
laprlsc
lapid166 dual (original)
lapid166 primal (newton)

id166
rlsc
laprlsc
lapid166 dual (original)
lapid166 primal (newton)

id166
rlsc
laprlsc
lapid166 dual (original)
lapid166 primal (newton)

id166
rlsc
laprlsc
lapid166 dual (original)
lapid166 primal (newton)

id166
rlsc
laprlsc
lapid166 dual (original)
lapid166 primal (newton)

id166
rlsc
laprlsc
lapid166 dual (original)
lapid166 primal (newton)

id166
rlsc
lapid166 primal (pcg)

0.6
0.6
0.6
0.6
0.6

2.7
2.7
2.7
2.7
2.7

9.4
9.4
9.4
9.4
9.4

0.6
0.6
0.6
0.6
0.6

9.4
9.4
9.4
9.4
9.4

9
9
9
9
9

4.3
4.3
4.3

nn
-
-
50
50
50

-
-
2
2
2

-
-
50
50
50

-
-
10
10
10

-
-
2
2
2

-
-
10
10
10

-
-
20
20
20

-
-
6

p
-
-
5
5
5

-
-
1
1
1

-
-
5
5
5

-
-
2
2
2

-
-
1
1
1

-
-
2
2
2

-
-
3
3
3

-
-
1

  a
10   1
1
10   6
1
10   1
10   6
10   6
10   6
10   2
10   6
10   6
10   6
10   6
10   6
10   6
10   6
10   1
10   4
10   6
10   6
10   6
10   6
10   6
10   6
10   6
10   1
10   6
10   6
10   6
10   4
10   6
10   6
10   6
10   6
10   6
10   6
10   6
10   6

  i
-
-

10   2
10
10

-
-
1

100

1

-
-

10   2
10   4
1

-
-

10   1
10   2
10   2

-
-
1
10
1

-
-

10   1
10   2
1

-
-

10   2
10   2
10   2

-
-

10   8

table 8: parameters selected by cross   validation for supervised algorithms (id166, rlsc)
and semi   supervised ones based on manifold id173, using di   erent loss
functions (laprlsc, lapid166 trained in the dual formulation and in the primal
one by means of newton   s method). the parameter    is the bandwidth of the
gaussian kernel or, in the mnist3vs8, the degree of the polynomial one.

36

dataset

g50c

coil20(b)

pcmac

uspst(b)

coil20

uspst

mnist3vs8

facemit

laplacian id166
newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

pcg [stability check]
pcg [validation check]
pcg [mixed check]

  a
10   1
10   1
10   1
10   1
10   6
10   6
1
10   6
10   6
10   4
10   4
10   6
10   6
10   6
10   6
10   6
10   6
10   6
10   6
10   6
10   4
10   4
10   4
10   4
10   6
10   6
10   6
10   6
10   6
10   6
10   6

  i
10
10
10
10

1
1

100

1

1
1
1
10   1
10   2
1
1
1

1
1
1
1

1
1
1
1
10   2
10   1
10   1
10   1
10   8
10   8
10   8

table 9: a comparison of the parameters selected by cross   validation for laplacian id166s
trained in the primal by means of newton   s method (newton) and preconditioned
conjugate gradient (pcg) with the proposed early stopping conditions (in square
brackets).

37

dataset

g50c

coil20(b)

pcmac

uspst(b)

coil20

uspst

mnist3vs8

laplacian id166
dual
newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

dual
newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

dual
newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

dual
newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

dual
newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

dual
newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

dual
newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

training time
0.155 (0.004)
0.134 (0.006)
0.044 (0.006)
0.043 (0.006)
0.044 (0.006)

0.311 (0.012)
0.367 (0.097)
0.198 (0.074)
0.095 (0.018)
0.206 (0.089)

14.8203 (0.104)
15.756 (0.285)
1.901 (0.022)
1.970 (0.265)
1.969 (0.268)

1.196 (0.015)
1.4727 (0.2033)
0.496 (0.172)
0.279 (0.096)
0.567 (0.226)

6.321 (0.441)
7.26 (1.921)
3.297 (1.471)
1.769 (0.299)
3.487 (1.734)

12.25 (0.2)
17.74 (2.44)
1.953 (0.403)
2.032 (0.434)
2.158 (0.535)

pcg iters
-
-
20 (0)
20.83 (2.89)
20.83 (2.89)

-
-
74.67 (28.4)
36 (7.24)
78.67 (34.42)

-
-
38.00 (0)
39.58 (5.48)
39.58 (5.48)

-
-
95.00 (33.40)
52.25 (18.34)
107.67 (43.88)

-
-
65.47 (30.35)
34.07 (6.12)
69.53 (35.86)

-
-
41.17 (8.65)
42.91 (9.38)
45.60 (11.66)

2064.18 (3.1)
2824.174 (105.07)
188.775 (0.237)
207.986 (35.330)
207.915 (35.438)

-
-
165 (0)
183.33 (31.75)
183.33 (31.75)

facemit

pcg [stability check]
pcg [validation check]
pcg [mixed check]

35.728 (0.868)
35.728 (0.868)
35.728 (0.868)

3 (0)
3 (0)
3 (0)

ls iters
-
-
1 (0)
1 (0)
1 (0)

-
-
2.41 (1.83)
3.26 (2.21)
2.38 (1.79)

-
-
1.18 (0.45)
1.18 (0.44)
1.18 (0.44)

-
-
6.56 (3.18)
6.83 (3.44)
6.49 (3.15)

-
-
2.53 (1.90)
3.37 (2.22)
2.48 (1.87)

-
-
3.11 (1.73)
3.13 (1.73)
3.12 (1.72)

-
-
6.78 (3.65)
6.65 (3.57)
6.65 (3.57)

1 (0)
1 (0)
1 (0)

table 10: training time comparison among the laplacian id166s trained in the dual (dual),
lapid166 trained in the primal by means of newton   s method (newton) and by
means of preconditioned conjugate gradient (pcg) with the proposed early stop-
ping conditions (in square brackets). parameters of the classi   ers were tuned us-
ing the newton   s method. average training times (in seconds) and their standard
deviations, the number of pcg iterations, and of line search (ls) iterations (per
each pcg one) are reported.

38

dataset

g50c

coil20(b)

pcmac

uspst(b)

coil20

uspst

mnist3vs8

laplacian id166
newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

newton
pcg [stability check]
pcg [validation check]
pcg [mixed check]

u
6.16 (1.48)
6.13 (1.46)
6.16 (1.48)
6.16 (1.48)

8.16 (2.04)
8.81 (2.23)
8.97 (2.32)
8.84 (2.28)

9.68 (0.77)
9.65 (0.76)
9.65 (0.76)
9.65 (0.76)

8.72 (2.15)
11.07 (2.27)
12.02 (2.22)
10.81 (2.39)

10.54 (2.03)
12.42 (2.68)
13.07 (2.73)
12.43 (2.69)

14.98 (2.88)
15.60 (3.45)
15.40 (3.38)
15.45 (3.53)

2.2 (0.14)
3.16 (0.15)
2.89 (0.62)
2.89 (0.62)

v
6.17 (3.46)
6.17 (3.46)
6.17 (3.46)
6.17 (3.46)

7.92 (3.96)
8.13 (3.71)
9.17 (3.74)
8.13 (3.71)

7.83 (4.04)
7.83 (4.04)
7.83 (4.04)
7.83 (4.04)

9.33 (3.85)
13.33 (4.21)
14.67 (2.99)
12.83 (4.78)

9.79 (4.94)
10.63 (4.66)
12.08 (4.75)
10.42 (4.63)

15 (3.57)
15.67 (3.60)
15.67 (3.98)
15.50 (3.92)

1.67 (1.44)
2.5 (1.25)
2.50 (1.25)
2.5 (1.25)

facemit

pcg [stability check]
pcg [validation check]
pcg [mixed check]

29.97 (2.51)
29.97 (2.51)
29.97 (2.51)

36 (3.46)
36 (3.46)
36 (3.46)

t
7.27 (2.87)
7.27 (2.87)
7.27 (2.87)
7.27 (2.87)

8.56 (1.9)
8.84 (1.93)
8.96 (1.64)
8.84 (1.96)

9.37 (1.51)
9.42 (1.43)
9.40 (1.43)
9.40 (1.43)

9.42 (2.34)
11.49 (2.55)
12.01 (2.14)
11.31 (2.71)

11.32 (2.19)
12.92 (2.14)
13.52 (2.12)
12.87 (2.20)

15.38 (3.55)
16.11 (3.95)
15.94 (4.04)
15.94 (4.08)

2.02 (0.22)
2.4 (0.38)
2.37 (0.44)
2.37 (0.44)

27.97 (5.38)
27.97 (5.38)
27.97 (5.38)

table 11: average classi   cation error (standard deviation is reported brackets) of laplacian
id166s trained in the primal by means of newton   s method and of preconditioned
conjugate gradient (pcg) with the proposed early stopping conditions (in square
brackets). parameters of the classi   ers were tuned using the newton   s method. u
is the set of unlabeled examples used to train the classi   ers. v is the labeled set
for cross   validating parameters whereas t is the out   of   sample test set. results
on the labeled training set l are omitted since all classi   ers perfectly    t such few
labeled training points.

39

