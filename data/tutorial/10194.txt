2
1
0
2

 

g
u
a
8
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
5
6
7
2

.

4
0
2
1
:
v
i
x
r
a

1

a practical approach to language complexity: a wikipedia case
study
taha yasseri1,   , andr  as kornai2, j  anos kert  esz1,3
1 department of theoretical physics, budapest university of technology and economics,
budapest, hungary
2 computer and automation research institute, hungarian academy of sciences,
budapest, hungary
3 center for network science, central european university, budapest, hungary
    e-mail: yasseri@phy.bme.hu

abstract

in this paper we present statistical analysis of english texts from wikipedia. we try to address the issue
of language complexity empirically by comparing the simple english wikipedia (simple) to comparable
samples of the main english wikipedia (main). simple is supposed to use a more simpli   ed language
with a limited vocabulary, and editors are explicitly requested to follow this guideline, yet in practice
the vocabulary richness of both samples are at the same level. detailed analysis of longer units (id165s
of words and part of speech tags) shows that the language of simple is less complex than that of main
primarily due to the use of shorter sentences, as opposed to drastically simpli   ed syntax or vocabulary.
comparing the two language varieties by the gunning readability index supports this conclusion. we
also report on the topical dependence of language complexity, e.g. that the language is more advanced
in conceptual articles compared to person-based (biographical) and object-based articles. finally, we
investigate the relation between con   ict and language complexity by analyzing the content of the talk
pages associated to controversial and peacefully developing articles, concluding that controversy has the
e   ect of reducing language complexity.

introduction

readability is one of the central issues of language complexity and applied linguistics in general [1].
despite the long history of investigations on readability measurement, and signi   cant e   ort to introduce
computational criteria to model and evaluate the complexity of text in the sense of readability, a conclusive
and fully representative scheme is still missing [2   4]. in recent years the large amount of machine readable
user generated text available on the web has o   ered new possibilities to address many classic questions
of psycholinguistics. recent studies, based on text-mining of blogs [5], web pages [6], online forums [7, 8],
etc, have advanced our understanding of natural languages considerably.

among all the potential online corpora, wikipedia, a multilingual online encyclopedia [9], which is
written collaboratively by volunteers around the world, has a special position. since wikipedia content
is produced collaboratively, it is a uniquely unbiased sample. as wikipedias exist in many languages, we
can carry out a wide range of cross-linguistic studies. moreover, the broad studies on social aspects of
wikipedia and its communities of users [10   18] makes it possible to develop sociolinguistic descriptions
for the linguistic observations.

one of the particularly interesting editions of wikipedia is the simple english wikipedia [19] (simple).
simple aims at providing an encyclopedia for people with only basic knowledge of english, in particular
children, adults with learning di   culties, and people learning english as a second language. see table 1
comparing the articles for    april    in simple and main. in this work, we reconsider the issue of language
complexity based on the statistical analysis of a corpus extracted from simple. we compare basic measures
of readability across simple and the standard english wikipedia (main) [20] to understand how simple is
simple in comparison. since there are no supervising editors involved in the process of writing wikipedia

2

articles, both simple and main are uncorrected (natural) output of the human language generation
ability. the text of wikipedias is emerging from contributions of a large number of independent editors,
therefore all di   erent types of personalization and bias are eliminated, making it possible to address the
fundamental concepts regardless of marginal phenomena.

readability studies on di   erent corpora have a long history; see [21] for a summary.

in a recent
study [22], readability of articles published in the annals of internal medicine before and after the
reviewing process is investigated, and a slight improvement in readability upon the review process is
reported. wikipedia is widely used to extract concepts, relations, facts and descriptions by applying
natural language processing techniques [23]. in [24   27] di   erent authors have tried to extract semantic
knowledge from wikipedia aiming at measuring semantic relatedness, lexical analysis and text classi-
   cation. wikipedia is used to establish topical indexing methods in [28]. tan and fuchun performed
query segmentation by combining generative language models and wikipedia information [29]. in a novel
approach, tyers and pienaarused used wikipedia to extract bilingual word pairs from interlingual hyper-
links connecting articles from di   erent language editions [30]. and more practically, sharo    and hartley
have been seeking for    suitable texts for language learners   , developing a new complexity measure, based
on both lexical and grammatical features [31]. comparisons between simple and main for the selected
set of articles show that in most cases simple has less complexity, but there exist exceptional articles,
which are more readable in main than in simple. in a complementary study [32], simple is examined
by measuring the flesch reading score [33]. they found that simple is not simple enough compared to
other english texts, but there is a positive trend for the whole wikipedia to become more readable as
time goes by, and that the tagging of those articles that need more simpli   cations by editors is crucial
for this achievement. in a new class of applications [34   36], simple is used to establish automated text
simpli   cation algorithms.

table 1. the articles on april in main english and simple english wikipedias.

main
april is the fourth month of the year in
the julian and gregorian calendars, and
one of four months with a length of 30
days. the traditional etymology is from
the latin aperire,    to open,    in allusion to
its being the season when trees and    owers
begin to    open   .

simple
april is the fourth month of the year. it
has 30 days. the name april comes from
that latin word aperire which means    to
open   . this probably refers to growing
plants in spring.

methods

we built our own corpora from the dumps [37] of simple and main wikipedias released at the end of
2010 using the wikiextractor developed at the university of pisa multimedia lab (see text s2 in the
supporting information for the availability of this and other software packages and corpora used in this
work). the simple corpus covers the whole text of simple wikipedia articles (no talk pages, categories
and templates). for the main english wikipedia,    rst we made a big single text including all articles,
and then created a corpus comparable to simple by randomly selecting texts having the same sizes as
the simple articles. in both samples html entities were converted to characters, mediawiki tags and
commands were discarded, but the anchor texts were kept.

simple uses signi   cantly shorter words (4.68 characters/word) than main (5.01 characters/word). we
can de   ne    same size    by equal number of characters (see condition cb in table 2), or by equal number of
words (condition wb). since sentence lengths are also quite di   erent (simple has 17.0 words/sentence on

table 2. vocabulary richness in main and simple

3

sr

cond
1.0002
cb
0.9997
cn
1.0000
wb
1.0000
wn
cbp
1.0002
cnp 0.9997
wbp 1.0000
wnp 1.0000

cm

0.8226
0.7782
0.8218
0.7774
0.8061
0.7568
0.8052
0.7563

cs

0.8167
0.7739
0.8167
0.7739
0.8013
0.7542
0.8013
0.7543

cm /cs
1.0072
1.0055
1.0061
1.0045
1.0059
1.0034
1.0049
1.0028

for the de   nition of conditions (character- or word-balanced, with or without puctuation, with or
without porter id30) see the methods section. sr is size ratio (number of characters in c
conditions, number of words in w conditions) for comparable main and simple corpora. cm and cs
are herdan   s c for main and simple. as the last column shows, the vocabulary richness of comparable
siid113 and main corpora di   ers at most by 0.72% depending on condition.

average, main has 25.2), the standard practice of computational linguistics of counting punctuation marks
as full word tokens may also be seen as problematic. therefore, we created two further conditions, cn
(character-balanced but no punctuation) and wn (word-balanced no punctuation). in both conditions,
we used the standard (koehn, see text s2) tokenizer to    nd the words, but in the n conditions we removed
the punctuation chars ,.?();"!:. another potential issue concerns id30, whether we consider the
tokens amazing, amazed, amazes as belonging to the same or di   erent types. to see whether this makes
any di   erence, we also created conditions cbp, wbp, cnp, and wnp by id30 both simple and
main using the standard porter stemmer [38]. table 2 compares for simple and main a classic measure
of vocabulary richness, herdan   s c, de   ned as log(#types)/log(#tokens), under these conditions.

for word and part of speech (pos) id165 statistics not all these conditions make sense, since auto-
matic pos taggers crucially rely on information in the a   xes that would be destroyed by id30, and
for the automatic detection of sentence boundaries punctuation is required [39]. we therefore used word-
balanced samples with punctuation kept in place (condition wb) but distinguished di   erent conditions
of id52 for the following reason. wikipedia, and encyclopedias in general, use an extraordinary
amount of proper names (three times as much as ordinary english as measured e.g. on the brown cor-
pus), many of which are multiword named entities. an ordinary pos tagger may not recognize that long
island is a single named entity and could tag it as jj nn (adjective noun) rather than as nnp nnp
(proper name phrase). therefore, we supplemented the original id52 (condition o) by a named
entity recognition (ner) system and rerun the id52 in light of the ner output (condition n). if
adjacent nnp-tagged elements are counted as a single ne phrase, we obtain the so (shortened original)
and sn (shortened ner-based) versions. since neither word-based nor pos-based id165s are very
meaningful if they span sentence boundaries, we also created    postprocessed    versions, where for odd n
those id165s where the boundary was in the middle were omitted, and the words/tags falling on the
shorter side were uniformly replaced by the boundary marker both for odd and even n.

to measure text readability, we limited ourselves to the    gunning fog index    f , [40, 41] which is
one of the simplest and most reliable among all di   erent recent and classic measures (see [42   44]). f is
calculated as

f = 0.4(

#words

#sentences

+ 100

#complex words

#words

)

where words are considered complex if they have three or more syllables. a simple interpretation of f is

4

the number of years of formal education needed to understand the text.

results and discussion

we present our results in three parts. first we report on overall comparison of main and simple at
di   erent levels of word and id165 statistics in addition to readability analysis. next we narrow down
the analysis further to compare selected articles and categories of articles, and examine the dependence of
language complexity on the text topic. finally, we explore the relation between controversy and language
complexity by considering the case of editorial wars and related discussion pages in wikipedia.

overall comparison

readability

in table 3, the gunning fog index calculated for 6 di   erent english corpora is reported. remarkably,
the fog index of simple is higher than that of dickens, whose writing style is sophisticated but doesn   t
rely on the use of longer latinate words which are hard to avoid in an encyclopedia. the british national
corpus, which is a reasonable approximation to what we would want to think of as    english in general    is
a third of the way between simple and main, demonstrating the accomplishments of simple editors, who
pushed simple half as much below average complexity as the encyclopedia genre pushes main above it.

table 3. readability of di   erent english corpora

corpus
dickens

sjm
wsj

f

corpus
8.6    0.1
simple
10.3    0.1
bnc
10.8    0.2 main

f

10.8    0.2
12.1    0.5
15.8    0.4

gunning fog index for 6 di   erent corpora of wsj: wall street journal   , charles dickens    books, sjm:
san jose mercury news   , bnc: british national corpus   , simple, and main.

   http://www.wsj.com
   http://www.mercurynews.com
   http://www.natcorp.ox.ac.uk

word statistics

vocabulary richness is compared for simple and main in table 2 using herdan   s c, a measure that
is remarkably stable across sample sizes: for example using only 95% of the word-balanced (condition
wb) samples we would obtain c values that di   er from the ones reported here by less than 0.066% and
0.044%. for technical reasons we could not balance the samples perfectly (there is no sense in cutting in
the middle of a line, let alone the middle of a word), but the size ratios (column sr in table 2) were kept
within 0.03%, two orders of magnitude less discrepancy than the 5% we used above, making the error
introduced by less than perfect balancing negligible.

the precise choice of condition has a signi   cant impact on c, ranging from a low of 0.754 (character-
balanced, no punctuation, porter id30) to a high of 0.8226 (character-balanced, punctuation in-
cluded, no id30), but practically no e   ect on the cm /cs ratio, which is between 0.28% and 0.72%
for all conditions reported here. in other words, we observe the same vocabulary richness in balanced
samples of simple and main quite independent of the speci   c processing and balancing steps taken. we

5

also experimented with several other tokenizers and stemmers, as well as inclusion or exclusion of nu-
merals or words with foreign (not iso-8859-1) characters, but the precise choice of condition made little
di   erence in that the discrepancy between cm and cs always stayed less than 1% (   0.27% to +0.72%).
the only condition where a more signi   cant di   erence of 3.4% could be observed was when simple was
directly paired with main by selecting, wherever possible, the corresponding main version of every simple
article.

as discussed in [45], one cannot reasonably expect the same result to hold for other traditional
measures of vocabulary richness such as type-token ratio, since these are not independent of sample size
asymptotically [46]. however, herdan   s law (also known as heaps    law, [47, 48]), which states that the
number of di   erent types v scales with the number of tokens n as v     n c, is known to be asymptotically
true for any distribution following zipf   s law [49], see [50   52]. in fig. 1 (left and middle panels) our study
of both laws in condition wb, are illustrated.

figure 1. word-level statistical analysis of main and simple. condition wb, as explained the
methods section. left: zipf   s law for the main (black) and simple (red) samples. middle: heaps    law
(same colors). the exponents are 0.72    0.01 (main) and 0.69    0.01 (simple). right: comparing token
frequencies in the two samples for 300 randomly selected words (   s    and    m    stand for simple and
main respectively), the correlation coe   cient is c=0.985. all three diagrams show that the two samples
have statistically almost the same vocabulary richness.

since all these results demonstrate the similarity of the simple and main samples in the sense of
unigram vocabulary richness, a conclusion that is quite contrary to the simple wikipedia stylistic guide-
lines [53], we performed some additional tests. first, we selected 300 words randomly and compared
the number of their appearance in both samples (right panel of fig. 1). next, we considered the word
id178 of simple and main, obtaining 10.2 and 10.6 bits respectively. again, the exact numbers depend
on the details of preprocessing, but the di   erence is in the 2.9% to 3.9% range in favor of main in every
condition, while the dependence on condition is in the 1.8% to 2.8% range. though 0.4 bits are above the
noise level, the numbers should be compared to the word id178 of mixed text, 9.8 bits, as measured on
the brown corpus, and of spoken conversation, 7.8 bits, as measured on the switchboard corpus. when
a switch in genre can bring over 30% decrease in word id178, a 3% di   erence pales in comparison.
altogether, both simple and main are close in word id178 to high quality newspaper prose such as the
wall street journal, 10.3 bits, and the san jose mercury news, 11.1 bits.

word id165 statistics

one e   ect not measured by the standard unigram techniques is the contribution of lexemes composed of
more than one word, including idiomatic expressions like    take somebody to task    and collocations like
   heavy drinker   . the simple wikipedia guidelines [53] explicitly warn against the use of idioms:    do not
use idioms (one or more words that together mean something other than what they say)   . one could
assume that simple editors rely more on such multiword patterns, and the id165 analysis presented
here supports this. in fig. 2 made under condition wb, the token frequencies of id165s are shown in a

zipf-style plot as a function of their rank. both the unigram statistics discussed in the previous section
and the 2-gram statistics presented here are nearly identical for simple and main, but 3-grams and higher
id165s begin to show some discrepancy between them. in reality, a sample of this small size (below
107 words) is too small to represent higher id165s well, as is clear from manual inspection of the top
5-grams of simple.

6

figure 2. id165 statistical analysis of main and simple. condition wb, as explained the
methods section. number of appearances of id165s in main (black) and simple (red) for n = 2   5 from
left to right. by increasing n, the di   erence between two samples becomes more signi   cant. in simple
there are more of the frequently appearing id165s than in main.

ignoring 5-grams composed of chinese characters (which are mapped into the same string by the
tokenizer), the top four entries, with over 4200 occurrences, all come from the string .
it is found
in the region. in fact, by grepping on high frequency id165s such as is a commune of we    nd over
six thousand entries in simple such as the following: alairac is a commune of 1,034 people (1999). it is
located in the region languedoc-roussillon in the aude department in the south of france. since most of
these entries came from only a handful of editors, we can be reasonably certain that they were generated
from geographic databases (gazetteers) using a simple    american chinese menu    substitution tool [54],
perhaps implemented as wikipedia robots.

since an estimated 12.3% of the articles in simple    t these patterns, it is no surprise that they
contribute somewhat to the apparent id165 simplicity of simple. indeed, the id178 di   erential between
main and simple, which is 0.39 bits absolute (1.7% relative) for 5-grams, decreases to 0.28 bits (1.2%
relative) if these articles are removed from simple and the main sample is decreased to match. (by
word count the robot-generated material is less than 2% of simple, so the adjustment has little impact.)
since higher id165s are seriously undersampled (generally, 109 words    gigaword corpora    are considered
necessary for word trigrams, while our entire samples are below 107 words) we cannot pursue the matter of
multiword patterns further, but note that the boundary between the machine-generated and the manually
written is increasingly blurred.

consider joyeuse is a commune in the french department of ard`eche in the region of rh  one-alpes.
it is the seat of the canton of joyeuse, an article that clearly started its history by semi-automatic or
fully automatic generation. by now (august 2012) the article is twice as long (either by manual writing
or semi-automatic import from the main english wikipedia), and its content is clearly beyond what any
gazetteer would list. with high quality robotic generation, editors will simply not know, or care, whether
they are working on a page that originally comes from a robot. therefore, in what follows we consider
simple in its entirety, especially as the part of speech (pos) statistics that we now turn to are not
particularly impacted by robotic generation.

part of speech statistics

figure 3 shows the distribution of the part of speech (pos) tags in main and simple for condition o
(word balanced, punctuation and possessive    s counted as separate words, as standard with the the penn
treebank pos set [55].) it is evident from comparing the    rst and second columns that the encyclopedia

genre is particularly heavy on named entities (proper nouns or phrases designating speci   c places,
people, and organizations [56]). since multiword entities like long island, benjamin franklin, national
academy of sciences are quite common, we also preprocessed the data using the hunner named entity
recognizer [57], and performed the id52 afterwards (condition n). when adjacent nnp
words are counted as one, we obtained the so and sn conditions. this obviously a   ects not just the
nnp counts, but also the higher id165s that contain nnp.

7

figure 3. part of speech statistics of main english and simple english wikipedias.
condition o, as explained the methods section. the legends are de   ned as nn: noun, singular or mass;
in: preposition or subordinating conjunction; nnp: proper noun, singular; dt: determiner; jj:
adjective; nns: noun, plural; vbd: verb, past tense; cc: coordinating conjunction; cd: cardinal
number; rb: adverb; vbn: verb, past participle; vbz: verb, 3rd person singular present; to: to; vb:
verb, base form; vbg: verb, gerund or present participle; prp: personal pronoun; vbp: verb, non-3rd
person singular present; prp$: possessive pronoun; pos: possessive ending; wdt: wh-determiner;
md: modal; nnps: proper noun, plural; wrb: wh-adverb; jjr: adjective, comparative; jjs:
adjective, superlative; wp: wh-pronoun; rp: particle; rbr: adverb, comparative; ex: existential
there; sym: symbol; rbs: adverb, superlative; fw: foreign word; pdt: predeterminer; wp$:
possessive wh-pronoun; ls: list item marker; uh: interjection;

again, the similarity of simple and main is quite striking: the cosine similarity measure of these
distributions is between 0.989 (condition o) and 0.991 (condition so), corresponding to an angle of
7.7 to 8.6 degrees. to put these numbers in perspective, note that the similarity between main and the

8

brown corpus is 0.901 (25.8 degrees), and between main and switchboard 0.671 (47.8 degrees). for
pos id165s, it makes sense to omit id165s with a sentence boundary at the center. for the pos
unigram models this means that we do not count the notably di   erent sentence lengths twice, a step
that would bring cosine similarity between simple and main to 0.992 (condition so) or 0.993 (condition
n), corresponding to an angle of 6.8 to 7.1 degrees. either way, the angle between simple and main is
remarkably acute.

while figure 3 shows some slight stylistic variation, e.g. that simple uses twice as many personal
pronouns (he, she, it, ...) as main, it is hard to reach any overarching generalizations about these,
both because most of the di   erences are statistically insigni   cant, and because they point in di   erent
directions. one may be tempted to consider the use of pronouns to be an indicator of simpler, more direct,
and more personal language, but by the same token one would have to consider the use of wh-adverbs
(how however whence whenever where whereby wherever wherein whereof why ...) to be a hallmark of
more sophisticated, more logical, and more impersonal style, yet it is simple that has 50% more of these.
figure 4 shows that the pos id165 zipf plots for n = 1, . . . , 5 are practically indistinguishable
across simple and main under condition n. (we are publishing this    gure as it is the worst     under
the other conditions, the match is even better.) in terms of cosine similarity, the same tendencies that
we established for unigram data remain true for bigram or higher pos id165s: the switchboard data
is quite far from both simple and main, the brown corpus is closer, and the wsj is closest. however,
simple and main are noticeably closer to one another than either of them is to wsj, as is evident from
the table 4, which gives the angle, in decimal degrees, between simple and main (column sm), main and
wsj (column mw), and simple and wsj (column sw) based on pos id165s for n = 2, . . . , 5, under
condition sn, with postprocessing of id165s spanning sentence boundaries. we chose this condition
because we believe it to be the least noisy, but we emphasize that the same relations are observed for all
other conditions, with or without sentence boundary postprocessing, with or without removal of machine-
generated entries from simple, with or without readjusting the main corpus to re   ect this change (all
32 combinations were investigated). the data leave no doubt that the wsj is closer to main than
to simple, but the angles are large enough, especially when compared to the simple/main column, to
discourage any attempt at explaining the syntax of main, or simple, based on the syntax of well-edited
journalistic prose. we conclude that the simplicity of simple, evident both from reading the material
and from the gunning fog index discussed above, is due primarily to main having considerably longer
sentences. a secondary e   ect may be the use of shorter subsentences (comma-separated stretches) as
well, but this remains unclear in that the number of subsentence separators (commas, colons, semicolons,
parens, quotation marks) per sentence is considerably higher in main (1.62) than in simple (1.01), so
a main subsentence is on the average not much longer than a simple subsentence (8.62 vs 7.96 content
words/subsentence).

table 4. statistical similarity between di   erent samples at di   erent length of id165s.

n
2
3
4
5

sm mw sw
13.1
33.8
40.4
16.5
49.8
20.1
28.7
58.2

28.3
33.4
40.8
47.9

angle, in decimal degrees, between simple and main (column sm), main and wsj (column mw), and
simple and wsj (column sw) based on pos id165s for n = 2, . . . , 5, under condition sn, with
postprocessing of id165s spanning sentence boundaries.

9

figure 4. pos-id165 statistical analysis of main and simple number of appearances of pos
id165s in main and simple for n = 1   5 under condition n.

topical comparison

clearly, readability of text is a very context dependent feature. the more conceptually complex a topic,
the more complex linguistic structures and the less readability are expected. to examine this intuitive
hypothesis, we considered di   erent articles in di   erent topical categories. instead of systematically cov-
ering all possible categories of articles, here we illustrate the phenomenon on a limited number of cases,
where signi   cant di   erences are observed. the readability index of 10 selected articles from di   erent
topical categories is measured and reported in in table 5.

table 5. comparison of readability in main and simple english wikipedias

article

philosophy

physics
politics

you   re my heart, you   re my soul (song)

real madrid c.f.
immanuel kant
albert einstein
barack obama

madonna (entertainer)

lionel messi

fmain fsimple
16.6
15.9
14.1
9.6
11.6
15.7
13.5
12.7
11.2
12.8

11.3
11.1
8.9
5.8
7.6
10.3
8.9
9.7
8.9
7.9

gunning fog index for the same example articles in main and simple.

10

while these results are clearly indicative of the main tendencies, for more reliable statistics we need
larger samples. to this end we sampled over     50 articles from 10 di   erent categories and averaged the
readability index for the articles within the category. results are shown in table 6. the numbers make
it clear that more sophisticated topics, e.g. philosophy and physics require more elaborate language
compared to the more common topics of politics and sport. in addition, there is considerable di   erence
between subjective and objective articles, in that the level of complexity is slightly higher in the former:
more objective articles (e.g. biographies) are more readable.

table 6. readability in di   erent topical categories

category
philosophy

physics
politics
songs

sport clubs
philosophers

physicists
politicians

singers
athletes

fmain
17.2  0.6
16.5  0.4
14.0  0.5
13.3  0.6
12.2  0.7
15.9  0.6
15.0   0.5
13.1  0.4
13.2  0.4
13.1  0.3

fsimple
12.7  0.8
11.3  0.7
11.2  0.8
11.0  0.7
10.1  0.6
11.5  0.8
10.0  0.7
10.2  0.6
10.1  0.5
10.1  0.6

gunning fog index for samples of articles in 10 di   erent categories in main and simple.

con   ict and controversy

wikipedia pages usually evolve in a smooth, constructive manner, but sometimes severe con   icts, so
called edit wars, emerge. a measure m of controversially was coined by appropriately weighting the
number of mutual reverts with the number of edits of the participants of the con   ict in our previous
works [18,58,59]. (for the exact de   nition and more details, see text s1 in the supporting information.)
by measuring m for articles, one could rank them according to controversiality (the intensity of editorial
wars on the article).

in order to enhance the collaboration, resolve the issues, and discuss the quality of the articles, editors
communicate to each other through the    talk pages    [60] both in controversial and in peacefully evolving
articles. depending on the controversially of the topic, the language that is used by editors for these
communications can become rather o   ensive and destructive.

in classical cognitive sociology [61], there is a distinction between    constructive    and    destructive   
con   icts.    destructive processes form a coherent system aimed at in   icting psychological, material or
physical damage on the opponent, while constructive processes form a coherent system aimed at achieving
one   s goals while maintaining or enhancing relations with the opponent    [62]. there are many characteris-
tics that distinguish these two types of interactions, such as the use of swearwords and taboo expressions,
but for our purposes the most important is the lowering of language complexity in the case of destructive
con   ict [62].

since we can locate destructive con   icts in wikipedia based on measuring m , a computation that
does not take linguistic factors into account, we can check independently whether linguistic complexity is
indeed decreased as the destructivity of the con   ict increases. to this end, we created two similarly sized
samples, one composed of 20 highly controversial articles like anarchism and jesus, the other composed
of 20 peacefully developing articles like deer and york. the gunning fog index was calculated both
for the articles and the corresponding talk pages for both samples. results are shown in table 7. we

see that the fog index of the con   ict pages is signi   cantly higher than those of the peaceful ones (with
99.9% con   dence calculated with welch   s t-test). this is in accord with the previous conclusion about
the topical origin of di   erences in the index (see table 6): clearly, con   ict pages are usually about rather
complex issues.

table 7. controversy and readability

11

farticle
ftalk

controversial peaceful
11.6  0.4
8.6  0.8

16.5  0.9
11.7  0.6

   f = farticle     ftalk
gunning fog index for two sample articles of highly controversial and peaceful articles and the
corresponding talk pages.

4.8

3.0

in both samples there is a notable decrease in the fog index when going from the main page to the
talk page, but this decrease is considerably larger for the con   ict pages (4.8 vs. 3.0, separated within a
con   dence interval of 85%). this is just as expected from earlier observations of linguistic behavior during
destructive con   ict [62]. the language complexities for controversial articles and the corresponding talk
pages are higher to begin with, but the amount of reduction in language complexity    f is much more
noticeable in the presence of destructive con   icts and severe editorial wars.

conclusions and future work

in this work we exploited the unique near-parallelism that obtains between the main and the simple
english wikipedias to study empirically the linguistic di   erences triggered by a single stylistic factor, the
e   ort of the editors to make simple simple. we have found, quite contrary to naive expectations, and to
simple wikipedia guidelines, that classic measures of vocabulary richness and syntactic complexity are
barely a   ected by the simpli   cation e   ort. the real impact of this e   ort is seen in the less frequent use
of more complex words, and in the use of shorter sentences, both directly contributing to a decreased fog
index.

simpli   cation of the lexicon, as measured by c or word id178, is hardly detectable, unless we
directly compare the corresponding simple and main articles, and even there the e   ect is small, 3.4%.
the amount of syntactic variety, as measured by pos id165 id178, is decreased from main to simple
by a more detectable, but still rather small amount, 2-3%, with an estimated 20-30% of this decrease
due to robotic generation of pages. altogether, the complexity of simple remains quite close to that of
newspaper text, and very far from the easily detectable simpli   cation seen in spoken language.

we believe our work can help future editors of the simple wikipedia, e.g. by adding robotic complexity
checkers. further investigation of the linguistic properties of wikipedias in general and the simple english
edition in particular could provide results of great practical utility not only in natural language processing
and applied linguistics, but also in foreign language education and improvement of teaching methods.
the methods used here may also    nd an application in the study of other purportedly simpler language
varieties such as creoles and child-direceted speech.

acknowledgments

ty thanks katarzyna samson for useful discussions. we thank attila zs  eder and g  abor recski for
helping us with the pos analysis. suggestions by the anonymous plos one referees led to signi   cant
improvements in the paper, and are gratefully acknowledged here.

(cid:88)

supporting information

text s1: controversy measure

12

to quantify the controversiality of an article based on its editorial history, we focus on    reverts   , i.e. when
an editor undoes another editor   s edit completely. to detect reverts, we    rst assign a md5 hash code [63]
to each revision of the article and then by comparing the hash codes, detect when two versions in the
history line are exactly the same. in this case, the latest edit (leading to the second identical revision)
is marked as a revert, and a pair of editors, namely a reverting and a reverted one, are recognized. a
   mutual revert    is recognized if a pair of editors (x, y) is observed once with x and once with y as the
reverter. the weight of an editor x is de   ned as the number of edits n performed by her, and the
weight of a mutually reverting pair is de   ned as the minimum of the weights of the two editors. the
controversiality m of an article is de   ned by summing the weights of all mutually reverting editor pairs,
excluding the topmost pair, and multiplying this number by the total number of editors e involved in
the article. in formula,

m = e

min(n d, n r),

(1)

all mutual reverts

where n r/d is the number of edits on the article committed by reverting/reverted editor. the sum is
taken over mutual reverts rather than single reverts because reverting is very much part of the normal
work   ow, especially for defending articles from vandalism. the minimum of the two weights is used
because con   icts between two senior editors contributing more to controversiality than con   icts between
a junior and a senior editor, or between two junior editors. for more details on how the above formula
de   ning m was selected and validated see [18] and especially text s1 in its supporting information.

text s2: corpora and analysis tools

to download wikipedia dumps use the static snapshots from http://dumps.wikimedia.org. to down-
load the dynamic content, especially the most updated version of individual articles, use the    me-
diawiki api    online platform accessible at http://www.mediawiki.org/wiki/api:main_page. the
brown, switchboard, and wsj corpora are distributed by the linguistic data consortium as part of the
id32, http://www.ldc.upenn.edu/catalog/catalogentry.jsp?catalogid=ldc99t42 the
id52 of these texts, while not necessarily 100% correct, is manually corrected and generally
considered a gold standard against which pos taggers are evaluated. many gigaword corpora (including
arabic, chinese, english, french, and spanish) are available from the ldc, see http://www.ldc.upenn.
edu/catalog/catalogsearch.jsp

to clean the text from wikimedia tags and external references, we used the wikiextractor devel-
oped at the university of pisa multimedia lab, available at http://medialab.di.unipi.it/wiki/
wikipedia_extractor. another system with similar capabilities is    wiki2text    http://wiki2text.
sourceforge.net. we used faster (   ex-based) versions of the original koehn tokenizer and mikheev
sentence splitter, available at https://github.com/zseder/webcorpus.

for english id30, the standard is the    porter id30 algorithm    http://tartarus.org/
~martin/porterstemmer. for other languages a good starting point is http://aclweb.org/aclwiki/
index.php?title=list_of_resources_by_language.

we calculated the gunning fog index using the code and algorithm of greg fast http://cpansearch.
perl.org/src/gregfast/lingua-en-syllable-0.251/syllable.pm. for part-of-speech tagging we
used the    hunpos tagger    http://code.google.com/p/hunpos/ and the    hunner ne recognizer   ,
which are speci   c applications of the    huntag tool   , available at https://github.com/recski/huntag/.
to perform the id165 analysis we used the    id165 extraction tools    http://homepages.inf.

ed.ac.uk/lzhang10/ngram.html of le zhang.

13

all the abovementioned code and packages are open source and available publicly under gpl, lgpl,

or similar licenses, but some corpora may have copyright restrictions.

references

1. paasche-orlow mk, taylor ha, brancati fl (2003) readability standards for informed-consent

forms as compared with actual readability. new england journal of medicine 348: 721-726.

2. klare gr (1974) assessing readability. reading research quarterly 10: pp. 62-102.

3. kanungo t, orr d (2009) predicting the readability of short web summaries. in: proceedings of
the second acm international conference on web search and data mining. new york, ny, usa:
acm, wsdm    09, pp. 202   211.

4. karmakar s, zhu y (2010) visualizing multiple text readability indexes. in: education and man-

agement technology (icemt), 2010 international conference on. pp. 133 -137.

5. lambiotte r, ausloos m, thelwall m (2007) word statistics in blogs and rss feeds: towards

empirical universal evidence. journal of informetrics 1: 277 - 286.

6. serrano m, flammini a, menczer f (2009) modeling statistical properties of written text. plos

one 4: e5372.

7. altmann eg, pierrehumbert jb, motter ae (2009) beyond word frequency: bursts, lulls, and

scaling in the temporal distributions of words. plos one 4: e7678.

8. altmann eg, pierrehumbert jb, motter ae (2011) niche as a determinant of word fate in online

groups. plos one 6: e19009.

9. wikipedia. http://www.wikipedia.org. [online; accessed 8-july-2012].

10. voss j (2005) measuring wikipedia.

international conference of the international society for

scientometrics and informetrics : 10th, stockholm (sweden), 24-28 july 2005.

11. ortega f, gonzalez barahona jm (2007) quantitative analysis of the wikipedia community of
users. in: proceedings of the 2007 international symposium on wikis. new york, ny, usa: acm,
wikisym    07, pp. 75   86.

12. halavais a, lacka    d (2008) an analysis of topical coverage of wikipedia. journal of computer-

mediated communication 13: 429   440.

13. javanmardi s, lopes c, baldi p (2010) modeling user reputation in wikis. statistical analysis and

data mining 3: 126   139.

14. laniado d, tasso r (2011) co-authorship 2.0: patterns of collaboration in wikipedia. in: pro-
ceedings of the 22nd acm conference on hypertext and hypermedia. new york, ny, usa: acm,
ht    11, pp. 201   210.

15. massa p (2011) social networks of wikipedia. in: proceedings of the 22nd acm conference on

hypertext and hypermedia. new york, ny, usa: acm, ht    11, pp. 221   230.

16. kimmons r (2011) understanding collaboration in wikipedia. first monday 16.

17. yasseri t, sumi r, kert  esz j (2012) circadian patterns of wikipedia editorial activity: a demo-

graphic analysis. plos one 7: e30091.

14

18. yasseri t, sumi r, rung a, kornai a, kert  esz j (2012) dynamics of con   icts in wikipedia. plos

one 7: e38869.

19. wikipedia. simple english wikipedia. http://simple.wikipedia.org. [online; accessed 8-july-

2012].

20. wikipedia. english wikipedia. http://www.en.wikipedia.org. [online; accessed 8-july-2012].

21. baumann j (2005) vocabulary-comprehension relationships. in: b. maloch, j.v. ho   man, d.l.
schallert, c.m. fairbankds and j. worthy (eds.), fifty-fourth yearbook of the national reading
conference. oak creek, wi: national reading conference, p. 117131.

22. roberts jc, fletcher rh, fletcher sw (1994) e   ects of peer review and editing on the readability
of articles published in annals of internal medicine. jama: the journal of the american medical
association 272: 119-121.

23. medelyan o, milne d, legg c, witten ih (2009) mining meaning from wikipedia. international

journal of human-computer studies 67: 716 - 754.

24. gabrilovich e, markovitch s (2007) computing semantic relatedness using wikipedia-based ex-
in: proceedings of the 20th international joint conference on arti   cal
plicit semantic analysis.
intelligence. san francisco, ca, usa: morgan kaufmann publishers inc., ijcai   07, pp. 1606   
1611.

25. zesch t, m  uller c, gurevych i (2008) extracting lexical semantic knowledge from wikipedia and

wiktionary. in: proc. of the 6th conference on language resources and evaluation (lrec).

26. wang p, domeniconi c (2008) building semantic kernels for text classi   cation using wikipedia.
in: proceedings of the 14th acm sigkdd international conference on knowledge discovery and
data mining. new york, ny, usa: acm, kdd    08, pp. 713   721.

27. gabrilovich e, markovitch s (2009) wikipedia-based semantic interpretation for natural language

processing. j artif int res 34: 443   498.

28. medelyan o, witten ih, milne d (2008) topic indexing with wikipedia. in: proceedings of the

aaai 2008 workshop on wikipedia and arti   cial intelligence. wikiai 2008, pp. 19   24.

29. tan b, peng f (2008) unsupervised query segmentation using generative language models and
wikipedia. in: proceedings of the 17th international conference on world wide web. new york,
ny, usa: acm, www    08, pp. 347   356.

30. tyers f, pienaar j (2008) extracting bilingual word pairs from wikipedia. in: proceedings of the

saltmil workshop at language resources and evaluation conference. lrec08.

31. sharo    sks, hartley a (2008) seeking needles in the web haystack: finding texts suitable for

language learners. in: 8th teaching and language corpora conference. talc-8.

32. besten md, dalle j (2008) keep it simple: a companion for simple wikipedia?

industry &

innovation 15: 169-178.

33. flesch r (1979) how to write plain english. new york: harper and row.

34. napoles c, dredze m (2010) learning simple wikipedia: a cogitation in ascertaining abecedarian
in: proceedings of the naacl hlt 2010 workshop on computational linguistics
language.
and writing. stroudsburg, pa, usa: association for computational linguistics, cl&w    10, pp.
42   50.

15

35. yatskar m, pang b, danescu-niculescu-mizil c, lee l (2010) for the sake of simplicity: unsuper-
vised extraction of lexical simpli   cations from wikipedia. in: human language technologies 2010
annual conference of the north american chapter of the association for computational linguis-
tics. stroudsburg, pa, usa: association for computational linguistics, hlt    10, pp. 365   368.

36. coster w, kauchak d (2011) simple english wikipedia: a new text simpli   cation task. in: pro-
ceedings of the 49th annual meeting of the association for computational linguistics. stroudsburg,
pa, usa: association for computational linguistics, hlt    11, pp. 665   669.

37. wikimedia. wikimedia downloads. http://dumps.wikimedia.org.

[online; accessed 8-july-

2012].

38. porter m.

the porter id30 algorithm.

http://tartarus.org/$\sim$martin/

porterstemmer/. [online; accessed 8-july-2012].

39. mikheev a (2002) periods, capitalized words, etc. computational linguistics 28: 289-318.

40. gunning r (1952) the technique of clear writing. new york: ny: mcgraw-hill international book

co.

41. gunning r (1969) the fog index after twenty years. journal of business communication 6: 3-13.

42. kincaid jp, fishburn rp, rogers rl, chissom bs (1975) derivation of new redability formulas
for navy enlisted personnel. technical report research branch report 8-75., naval air station,
milington, tenn.

43. collins-thompson k, callan j (2004) a id38 approach to predicting reading di   -

culty. in: proceedings of hlt/naacl.

44. dubay wh (2007) smart language: readers, readability, and the grading of text. costa mesa,

california: booksurge publishing.

45. tweedie f, baayen rh (1998) how variable may a constant be? measures of lexical richness in

perspective. computers and the humanities 32: 323-352.

46. kornai a (2002) how many words are there? glottometrics 4: 61-86.

47. herdan g (1964) quantitative linguistics. washington: butterworths.

48. heaps hs (1978) information retrieval: computational and theoretical aspects. orlando, fl,

usa: academic press, inc.

49. zipf gk (1935) the psycho-biology of language: an introduction to dynamic philology. cambridge,

ma: the mit press.

50. kornai a (1999) zipf   s law outside the middle range. in: rogers j, editor, proceedings of the sixth

meeting on mathematics of language. university of central florida, pp. 347   356.

51. baeza yates r, navarro g (2000) block addressing indices for approximate text retrieval. journal

of the american society for information science 51: 69   82.

52. van leijenhorst d, van der weide tp (2005) a formal derivation of heaps    law.

information

sciences 170: 263   272.

53. wikipedia. how to write simple english pages. http://simple.wikipedia.org/wiki/wikipedia:

how_to_write_simple_english_pages. [online; accessed 8-july-2012].

54. sproat r (2010) language, technology, and society. oxford: oxford university press.

55. the university of pennsylvania (penn) treebank tag-set. http://www.comp.leeds.ac.uk/ccalas/

tagsets/upenn.html. [online; accessed 8-july-2012].

16

56. chinchor na (1998) proceedings

the
ence (muc-7) named entity task de   nition.
sage understanding conference
http://www.itl.nist.gov/iaui/894.02/related projects/muc/.

of

seventh message understanding confer-
the seventh mes-
version 3.5,

proceedings of

in:

(muc-7). fairfax, va, p. 21 pages.

57. varga d, simon e (2007) hungarian id39 with a maximum id178 approach.

acta cybern 18: 293   301.

58. sumi r, yasseri t, rung a, kornai a, kert  esz j (2011) characterization and prediction of

wikipedia edit wars. in: proceedings of the acm websci   11 : 1   3.

59. sumi r, yasseri t, rung a, kornai a, kert  esz j (2011) edit wars in wikipedia.

in: social
computing / ieee international conference on privacy, security, risk and trust, 2011 ieee
international conference on. los alamitos, ca, usa: ieee computer society, socialcom    11, pp.
724-727.

60. wikipedia. help:using talk pages. http://en.wikipedia.org/wiki/help:using_talk_pages.

[online; accessed 8-july-2012].

61. deutsch m (1973) the resolution of con   ict: constructive and destructive processes. new haven:

yale university press.

62. samson k, nowak a (2010) linguistic signs of destructive and constructive processes in con   ict.

iacm 23rd annual conference paper .

63. rivest rl (1992) the md5 message-digest algorithm. internet request for comments : rfc 1321.

