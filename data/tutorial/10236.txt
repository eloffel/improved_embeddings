5
1
0
2

 

v
o
n
0
3

 

 
 
]

g
l
.
s
c
[
 
 

3
v
0
1
9
5
0

.

7
0
5
1
:
v
i
x
r
a

under review as a conference paper at iclr 2016

id91 is efficient for approximate
maximum inner product search

alex auvolat   
  ecole normale sup  erieure, france.

hugo larochelle   
twitter cortex, usa.,
universit  e de sherbrooke, canada

sarath chandar   , pascal vincent         
universit  e de montr  eal, canada.

yoshua bengio   
universit  e de montr  eal, canada.

abstract

ef   cient maximum inner product search (mips) is an important task that has
a wide applicability in id126s and classi   cation with a large
number of classes. solutions based on locality-sensitive hashing (lsh) as well
as tree-based solutions have been investigated in the recent literature, to perform
approximate mips in sublinear time. in this paper, we compare these to another
extremely simple approach for solving approximate mips, based on variants of
the id116 id91 algorithm. speci   cally, we propose to train a spherical k-
means, after having reduced the mips problem to a maximum cosine similarity
search (mcss). experiments on two standard id126 bench-
marks as well as on large vocabulary id27s, show that this simple
approach yields much higher speedups, for the same retrieval precision, than cur-
rent state-of-the-art hashing-based and tree-based methods. this simple method
also yields more robust retrievals when the query is corrupted by noise.

1

introduction

the maximum inner product search (mips) problem has recently received increased attention, as
it arises naturally in many large scale tasks. in id126s (koenigstein et al., 2012;
bachrach et al., 2014), users and items to be recommended are represented as vectors that are learnt
at training time based on the user-item rating matrix. at test time, when the model is deployed for
suggesting recommendations, given a user vector, the model will perform a dot product of the user
vector with all the item vectors and pick top k items with maximum dot product to recommend.
with millions of candidate items to recommend, it is usually not possible to do a full linear search
within the available time frame of only few milliseconds. this problem amounts to solving a k-
mips problem.
another common instance where the k-mips problem arises is in extreme classi   cation tasks (vi-
jayanarasimhan et al., 2014), with a huge number of classes. at id136 time, predicting the top-k
most likely class labels for a given data point can be cast as a k-mips problem. such extreme (prob-
abilistic) classi   cation problems occur often in natural language processing (nlp) tasks where the
classes are words in a predetermined vocabulary. for example in neural probabilistic language mod-
els (bengio et al., 2003) the probabilities of a next word given the context of the few previous words
is computed, in the last layer of the network, as a multiplication of the last hidden layer representa-
tion with a very large matrix (an embedding dictionary) that has as many columns as there are words
in the vocabulary. each such column can be seen as corresponding to the embedding of a vocabu-
lary word in the hidden layer space. thus an inner product is taken between each of these and the
hidden representation, to yield an inner product    score    for each vocabulary word. passed through
a softmax nonlinearity, these yield the predicted probabilities for all possible words. the ranking of
these id203 values is unaffected by the softmax layer, so    nding the k most probable words is

   equal contribution
   and cifar

1

under review as a conference paper at iclr 2016

i   x q(cid:62)xi

exactly equivalent to    nding the ones with the largest inner product scores, i.e. solving a k-mips
problem.
in many cases the retrieved result need not be exact: it may be suf   cient to obtain a subset of k vec-
tors whose inner product with the query is very high, and thus highly likely (though not guaranteed)
to contain some of the exact k-mips vectors. these examples motivate research on approximate
k-mips algorithms. if we can obtain large speedups over a full linear search without sacri   cing
too much on precision, it will have a direct impact on such large-scale applications.
formally the k-mips problem is stated as follows: given a set x = {x1, . . . , xn} of points and a
query vector q,    nd

argmax(k)

(1)
where the argmax(k) notation corresponds to the set of the indices providing the k maximum
values. such a problem can be solved exactly in linear time by calculating all the q(cid:62)xi and selecting
the k maximum items, but such a method is too costly to be used on large applications where we
typically have hundreds of thousands of entries in the set.
all the methods discussed in this article are based on the notion of a candidate set, i.e. a subset
of the dataset that they return, and on which we will do an exact k-mips, making its computation
much faster. there is no guarantee that the candidate set contains the target elements, therefore these
methods solve approximate k-mips. better algorithms will provide us with candidate sets that are
both smaller and have larger intersections with the actual k maximum inner product vectors.
mips is related to nearest neighbor search (nns), and to maximum similarity search. but it is
considered a harder problem because the inner product neither satis   es the triangular inequality as
distances usually do, nor does it satisfy a basic property of similarity functions, namely that the
similarity of an entry with itself is at least as large as its similarity with anything else: for a vector
x, there is no guarantee that xt x     xt y for all y. thus we cannot directly apply ef   cient nearest
neighbor search or maximum similarity search algorithms to the mips problem.
given a set x = {x1, . . . , xn} of points and a query vector q, the id92s problem with euclidean
distance is de   ned as:

argmin(k)

i   x ||q     xi||2

2 = argmax(k)

i   x qt xi     ||xi||2

2

2

and the maximum cosine similarity problem (k-mcss) is de   ned as:

argmax(k)
i   x

qt xi

||q|| ||xi|| = argmax(k)
i   x

qt xi
||xi||

(2)

(3)

id92s and k-mcss are different problems than k-mips, but it is easy to see that all three become
equivalent provided all data vectors xi have the same euclidean norm. several approaches to mips
make use of this observation and    rst transform a mips problem into a nns or mcss problem.
in this paper, we propose and empirically investigate a very simple approach for the approximate
k-mips problem. it consists in    rst reducing the problem to an approximate k-mcss problem
(as has been previously done in (shrivastava and li, 2015) ) on top of which we perform a spherical
id116 id91. the few clusters whose centers best match the query yield the candidate set.
the rest of the paper is organized as follows: in section 2, we review previously proposed ap-
proaches for mips. section 3 describes our proposed simple solution id116 mips in more details
and section 4 discusses ways to further improve the performance by using a hierarchical id116
version. in section 5, we empirically compare our methods to the state-of-the-art in tree-based and
hashing-based approaches, on two standard collaborative    ltering benchmarks and on a larger word
embedding datasets. section 6 concludes the paper with discussion on future work.

2 related work

there are two common types of solution for mips in the literature: tree-based methods and hashing-
based methods. tree-based methods are data dependent (i.e.    rst trained to adapt to the speci   c data
set) while hash-based methods are mostly data independent.

2

under review as a conference paper at iclr 2016

tree-based approaches: the maximum inner product search problem was    rst formalized in (ram
and gray, 2012). ram and gray (2012) provided a tree-based solution for the problem. speci   cally,
they constructed a ball tree with vectors in the database and bounded the maximum inner product
with a ball. their novel analytical upper bound for maximum inner product of a given point with
points in a ball made it possible to design a branch and bound algorithm to solve mips using the
constructed ball tree. ram and gray (2012) also proposes a dual-tree based search using cone trees
when you have a batch of queries. one issue with this ball-tree based approach (ip-tree) is that it
partitions the set of data points based on the euclidean distance, while the problem hasn   t effectively
been converted to nns. in contrast, pca-tree (bachrach et al., 2014), the current state-of-the-art
tree-based approach to mips,    rst converts mips to nns by appending an additional component
to the vector that ensures that all vectors are of constant norm. this is followed by pca and by a
balanced kd-tree style tree construction.
hashing based approaches: shrivastava and li (2014) is the    rst work to propose an explicit
asymmetric locality sensitive hashing (alsh) construction to perform mips. they converted
mips to nns and used the l2-lsh algorithm (datar et al., 2004). subsequently, shrivastava and
li (2015) proposed another construction to convert mips to mcss and used the signed random
projection (srp) hashing method. both works were based on the assumption that a symmetric-
lsh family does not exist for mips problem. later, neyshabur and srebro (2015) showed an
explicit construction of a symmetric-lsh algorithm for mips which had better performance than
the previous alsh algorithms. finally, vijayanarasimhan et al. (2014) propose to use winner-take-
all hashing to pick top-k classes to consider during training and id136 in large classi   cation
problems.
hierarchical softmax: a notable approach to address the problem of scaling classi   ers to a huge
number of classes is the hierarchical softmax (morin and bengio, 2005). it is based on prior clus-
tering of the words into a binary, or more generally n-ary tree that serves as a    xed structure for
the learning process of the model. the complexity of training is reduced from o(n) to o(log n).
due to its id91 and tree structure, it resembles the mips techniques we explore in this paper.
however, the approaches differ at a fundamental level. hierarchical softmax de   nes the id203
of a leaf node as the product of all the probabilities computed by all the intermediate softmaxes on
the way to that leaf node. by contrast, an approximate mips search imposes no such constraining
structure on the probabilistic model, and is better though as ef   ciently searching for top winners of
what amounts to a large ordinary    at softmax.

3

id116 id91 for approximate mips

in this section, we propose a simple id116 id91 based solution for approximate mips.

3.1 mips to mcss

we follow the previous work by shrivastava and li (2015) for reducing the mips problem to the
mcss problem by ingeniously rescaling the vectors and adding new components, making the norms
of all the vectors approximately the same. let x = {x1, . . . , xn} be our dataset. let u < 1 and
m     n    be parameters of the algorithm. the    rst step is to scale all the vectors in our dataset by
the same factor such that maxi ||xi||2 = u. we then apply two mappings p and q, one on the data
points and another on the query vector. these two mappings simply concatenate m new components
to the vectors making the norms of the data points all roughly the same. the mappings are de   ned
as follows:

2, 1/2     ||x||4

p (x) = [x, 1/2     ||x||2
q(x) = [x, 0, 0, . . . , 0]

(4)
(5)
as shown in shrivastava and li (2015), mapping p brings all the vectors to roughly the same
, with the last term vanishing as m     +   , since
norm: we have ||p (xi)||2
||xi||2     u < 1. we thus have the following approximation of mips by mcss for any query vector
q,

2 = m/4 + ||xi||2m+1

2, . . . , 1/2     ||x||2m
2 ]

2

argmax(k)

i

q(cid:62)xi (cid:39) argmax(k)

i

3

q(q)(cid:62)p (xi)

||q(q)||2    ||p (xi)||2

(6)

under review as a conference paper at iclr 2016

3.2 mcss using spherical id116
assuming all data points x1, . . . , xn have been transformed as xj     p (xj) so as to be scaled to
a norm of approximately 1, then the spherical id1161 algorithm (zhong, 2005) can ef   ciently
be used to do approximate mcss. algorithm 1 is a formal speci   cation of the spherical id116
algorithm, where we denote by ci the centroid of cluster i (i     {1, . . . , k}) and aj the index of the
cluster assigned to each point xj.

algorithm 1 spherical id116

aj     rand(k)
(cid:80)
while ci or aj changed at previous step do
||(cid:80)

ci    
aj     argmaxi   {1,...,k}x(cid:62)
j ci

j|aj =i xj
j|aj =i xj||

end while

the difference between standard id116 id91 and spherical id116 is that in the spherical
variant, the data points are clustered not according to their position in the euclidian space, but
according to their direction.
to    nd the one vector that has maximum cosine similarity to query point q in a dataset clustered
by this method, we    rst    nd the cluster whose centroid has the best cosine similarity with the query
vector     i.e. the i such that q(cid:62)ci is maximal     and consider all the points belonging to that cluster as
the candidate set. we then simply take argmaxj|aj =i q(cid:62)xj as an approximation for our maximum
cosine similarity vector. this method can be extended for    nding the k maximum cosine similarity
vectors: we compute the cosine similarity between the query and all the vectors of the candidate set
and take the k best matches.
one issue with constructing a candidate set from a single cluster is that the quality of the set will be
poor for points close to the boundaries between clusters. to alleviate this problem, we can increase
the size of candidate sets by constructing them instead from the top-p best matching clusters to
construct our candidate set.
we note that other approximate search methods exploit similar ideas. for example, bachrach et al.
(2014) proposes a so-called neighborhood boosting method for pca-tree, by considering the path
to each leaf as a binary vector (based on decision to go left or right) and given a target leaf, consider
all other leaves which are one hamming distance away.

4 hierarchical id116 for faster and more precise search

   

   

n points, we reduce the complexity of the search from o(n) to roughly o (

while using a single-level id91 of the data points might yield a suf   ciently fast search proce-
dure for moderately large databases, it can be insuf   cient for much larger collections.
n clusters so that each cluster contains
indeed, if we have n points, by id91 our dataset into
approximately
n).
   
if we use the single closest cluster as a candidate set, then the candidate set size is of the order of
n. but as mentioned earlier, we will typically want to consider the two or three closest clusters as a
candidate set, in order to limit problems arising from the query points close to the boundary between
clusters or when doing approximate k-mips with k fairly big (for example 100). a consequence
of increasing candidate sets this way is that they can quickly grow wastefully big, containing many
unwanted items. to restrict the candidate sets to a smaller count of better targeted items, we would
need to have smaller clusters, but then the search for the best matching clusters becomes the most
expensive part. to address this situation, we propose an approach where we cluster our dataset into
many small clusters, and then cluster the small clusters into bigger clusters, and so on any number
of times. our approach is thus a bottom-up id91 approach.

   

1note that we use k to refer to the number of top-k items to retrieve in search and k for the number of

clusters in id116. these two quantities are otherwise not the same.

4

under review as a conference paper at iclr 2016

for example, we can cluster our datasets in n2/3    rst-level, small clusters, and then cluster the
centroids of the    rst-level clusters into n1/3 second-level clusters, making our data structure a two-
layer hierarchical id91. this approach can be generalized to as many levels of id91 as
necessary.

figure 1: walk down a hierarchical id91 tree: at each level we have a
candidate set for the next level. in the    rst level, the dashed red boxed represent
the p best matches, which gives us a candidate set for the second level, etc.

to search for the small clusters that best match the query point and will constitute a good candidate
set, we go down the hierarchy keeping at each level only the p best matching clusters. this process
is illustrated in figure 1. since at all levels the clusters are of much smaller size, we can take much
larger values for p, for example p = 8 or p = 16.
formally, if we have l levels of id91, let il be a set of indices for the clusters at level l    
{0, . . . , l}. let c(l)
i } conveniently
i     il   1, i     il be the assignment of the
de   ned as being the data points themselves, and let a(l)
to the clusters of layer l     1. the candidate set is found using the method described in
centroids c(l)
i
algorithm 2. our candidate set is the set cl obtained at the end of the algorithm. in our approach,

, i     il be the centroids of the clusters at level l, with {c(l)

i

algorithm 2 search in hierarchical spherical id116

c0 = i0
for l = 0, . . . , l     1 do

al = argmax(p)
i   cl
i|a(l+1)
cl+1 =

i

(cid:111)

q(cid:62)c(l)
    al

i

(cid:110)

end for
return cl

we do a bottom-up id91, i.e. we    rst cluster the dataset into small clusters, then we cluster the
small cluster into bigger clusters, and so on until we get to the top level which is only one cluster.
other approaches have been suggested such as in (mnih and hinton, 2009), where the method
employed is a top-down id91 strategy where at each level the points assigned to the current
cluster are divided in smaller clusters. the approach of (mnih and hinton, 2009) also addresses the
problem that using a single lowest-level cluster as a candidate set is an inaccurate solution by having
the data points be in multiple clusters. we use an alternative solution that consists in exploring
several branches of the id91 hierarchy in parallel.

5 experiments

in this section, we will evaluate the proposed algorithm for approximate mips. speci   cally, we
analyze the following characteristics: speedup, compared to the exact full linear search, of retrieving
top-k items with largest inner product, and robustness of retrieved results to noise in the query.

5

.....................under review as a conference paper at iclr 2016

5.1 datasets

we have used 2 collaborative    ltering datasets and 1 id27 dataset, which are descibed
below:
movielens-10m: a collaborative    ltering dataset with 10,677 movies (items) and 69,888 users.
given the user-item matrix z, we follow the puresvd procedure described in (cremonesi et al.,
2010) to generate user and movie vectors. speci   cally, we subtracted the average rating of each user
from his individual ratings and considered unobserved entries as zeros. then we compute an svd
approximation of z with its top 150 singular components, z (cid:39) w   rt . each row in w    is used as
the vector representation of the user and each row in r is the vector representation of the movie. we
construct a database of all 10,677 movies and consider 60,000 randomly selected users as queries.
net   ix: another standard collaborative    ltering dataset with 17,770 movies (items) and 480,189
users. we follow the same procedure as described for movielens but construct 300 dimensional
vector representations, as is standard in the literature (neyshabur and srebro, 2015). we consider
60,000 randomly selected users as queries.
id97 embeddings: we use the 300-dimensional id97 embeddings released by mikolov
et al. (2013). we construct a database composed of the    rst 100,000 id27 vectors. we
consider two types of queries: 2,000 randomly selected word vectors from that database, and 2,000
randomly selected word vectors from the database corrupted with gaussian noise. this acts as a test
bench to evaluate the performance of different algorithms based on the characteristics of the queries.

5.2 baselines

we consider the following baselines to compare with.
pca-tree: pca-tree (bachrach et al., 2014) is the state-of-the-art tree-based method which was
shown to be superior to ip-tree (koenigstein et al., 2012). this method    rst converts mips to
nns by appending an additional component to the vectors to make them of constant norm. then
the principal directions are learnt and the data is projected using these principal directions. finally,
a balanced tree is constructed using as splitting criteria at each level the median of component
values along the corresponding principal direction. each level uses a different principal direction, in
decreasing order of variance.
srp-hash: this is the signed random projection hashing method for mips proposed in shrivastava
and li (2015). srp-hash converts mips to mcss by vector augmentation. we consider n hash
functions and each hash function considers p random projections of the vector to compute the hash.
wta-hash: winner takes all hashing (vijayanarasimhan et al., 2014) is another hashing-based
baseline which also converts mips to mcss by vector augmentation. we consider n hash func-
tions and each hash function does p different random permutations of the vector. then the pre   x
constituted by the    rst k elements of each permuted vector is used to construct the hash for the
vector.

5.3 speedup results

in these    rst experiments, we consider the two collaborative    ltering tasks and evaluate the speedup
provided by the different approximate k-mips algorithms (for k     {1, 10, 100}) compared to the
exact full search. note that this section does not include the hierarchical version of id116 in the
experiments, as the databases were small enough (less than 20,000) for    at id116 to perform well.
speci   cally, speedup is de   ned as

speedupa0 (a) =

time taken by algorithm a0
time taken by algorithm a

(7)

where a0 is the exact linear search algorithm that consists in computing the inner product with all
training items. because we want to compare the preformance of algorithms, rather than of specif-
ically optimized implementations, we approximate the time with the number of dot product opera-
tions computed by the algorithm2. in other words, our unit of time is the time taken by a dot product.

2for example, id116 algorithm was run using gpu while pca-tree was run using cpu.

6

under review as a conference paper at iclr 2016

all algorithms return a set of candidates for which we do exact linear seacrh. this induces a number
of dot products at least as large as the size of the identi   ed candidate set. in addition to the candidate
set size, the following operations count towards the count of dot products:
id116: dot products done with all cluster centroids involved in    nding the top-p clusters of the
(hierarchical) search.
pca-tree: dot product done to project the query to the pca space. note that if the tree is of depth
d, then we need to do d dot products to project the query.
srp-hash: total number of random projections of the data (each random projection is considered a
single dot product). if we have n hashes with p random projections each, then the cost is p     n.
wta-hash: a full random permutation of the vector involves the same number of query element
access operations as a single dot product. however, we consider only k pre   xes in the permutations,
which means we only need to do a fraction of dot product. while a dot product involves accessing
all d components of the vector, each permutation in wta-hash only needs to access k elements of
the vector. so we consider its cost to be a fraction k/d of the cost of a dot product. speci   cally, if
we have n hash functions each with p random permutations and consider pre   xes of length k, then
the total cost would be n     p     k/d where d is the dimension of the vector.
let us call true top-k the actual k elements from the database that have the largest inner products
with the query. let us call retrieved top-k the k elements, among the candidate set retrieved by a
speci   c approximate mips, that have the largest inner products with the query. we de   ne precision
for k-mips as the number of elements in the intersection of true top-k and retrived top-k vectors,
divided by k.

|retrieved top k     true top k|

precision at k =

(8)
we varied hyper-parameters of each algorithm (k in id116, depth in pca-tree, number of hash
functions in srp-hash and wta-hash), and computed the precision and speedup in each case.
resulting precision v.s. speedup curves obtained for the movielens-10m and net   ix datasets are
reported in figure 2. we make the following observations from these results:

k

crease rapidly after 10x speedup.

    hashing-based methods perform better with lower speedups. but their performance de-
    pca-tree performs better than srp-hash.
    wta-hash performs better than pca-tree with lower speedups. however, their perfor-
mance degrades faster as the speedup increases and pca-tree outperforms wta-hash
with higer speedups.
    id116 is a clear winner as the speed up increases. also, performance of id116 de-
grades very slowly with increase in speedup as compared to rapid decrease in performance
of other algorithms.

5.4 neighborhood preserving and robustness results

in this experiment, we consider a id27 retrieval task. as a    rst experiment, we con-
sider using a query set of 2,000 embeddings, corresponding to a subset of a large database of pre-
trained embeddings. note that while a query is thus present in the database, it is not guaranteed
to correspond to the top-1 mips result. also, we   ll be interested in the top-10 and top-100 mips
performance. algorithms which perform better in top-10 and top-100 mips for queries which al-
ready belong to the database preserve the neighborhood of data points better. figure 3 shows the
precision vs. speedup curve for top-1, top-10 and top-100 mips. from the results, we can see that
data dependent algorithms (id116 and pca-tree) better preserve the neighborhood, compared to
data independent algorithms (srp-hash, wta-hash), which is not surprising. however, id116
and hierarchical id116 performs signi   cantly better than pca-tree in top-10 and top-100 mips
suggesting that it is better than pca-tree in capturing the neighborhood. one reason might be that
id116 has the global view of the vector at every step while pca-tree considers one dimension at
a time.
as the next experiment, we would like to study how different algorithms behave with respect to the
noise in the query. for a fair comparison, we chose hyper-parameters for each model such that the

7

under review as a conference paper at iclr 2016

(a)

(d)

(b)

(e)

(c)

(f)

figure 2: speedup results in collaborative    ltering. (a-c) correspond to precision in top 1,10,100
mips on movielens-10m dataset, while (d-f) correspond to precision in top 1,10,100 mips on
net   ix dataset respectively. id116(3) means id116 algorithm that considers top 3 clusters as
candidate set.

(a)

(b)

(c)

figure 3: speedup results in id27 retrieval. (a-c) correspond to precision in top 1,10,100
mips respectively. id116(3) means id116 algorithm that considers top 3 clusters as candidate
set. and hier-id116(8)s means a 2 level hierarchical id116 algorithm that considers top 8 clus-
ters as candidate set.

speedup is the same (we set it to 30x) for all algorithms. we take 2,000 random id27s
from the database and corrupt them random gaussian noise. we vary the scale of the noise from
0 to 0.4 and plot the performance. figure 4 shows the performance of various algorithms on the
top-1, top-10, top-100 mips problems, as the noise increases. we can see that id116 always
performs better than other algorithms, even with increase in noise. also, the performance of k-
means remains reasonable, compared to other algorithms. these results suggest that our approach
might be particularly appropriate in a scenario where id27s are simultaneously being
trained, and are thus not    xed. in such a scenario, having a robust mips method would allow us to
update the mips model less frequently.

8

020406080100120speedup020406080100precision@1020406080100120speedup020406080100precision@10020406080100120speedup020406080100precision@100id116(3)wta-hashpca-tree+nbsrp-hash020406080100120140speedup020406080100precision@1020406080100120140speedup020406080100precision@10020406080100120140speedup020406080100precision@100id116(3)wta-hashpca-tree+nbsrp-hash020406080100120speedup6065707580859095100precision@1id116(3)wta-hashpca-tree+nbsrp-hashhier-id116(8)020406080100120speedup102030405060708090100precision@10020406080100120speedup020406080100precision@10under review as a conference paper at iclr 2016

(a)

(b)

(c)

figure 4: precision in top-k retrieval as the noise in the query increases. we increase the standard
deviation of the gaussian noise and we see that id116 performs better than other algorithms.

6 conclusion and future work

in this paper, we have proposed a new and ef   cient way of solving approximate k-mips based on
a simple id91 strategy, and showed it can be a good alternative to the more popular lsh or
tree-based techniques. we regard the simplicity of this approach as one of its strengths. empirical
results on three real-world datasets show that this simple approach clearly outperforms the other
families of techniques. it achieves a larger speedup while maintaining precision, and is more robust
to input corruption, an important property for generalization, as query test points are expected to not
be exactly equal to training data points. id91 mips generalizes better to related, but unseen
data than the hashing approaches we evaluated.
in future work, we plan to research ways to adapt on-the-   y the id91 for our approximate k-
mips as its input representation evolves during the learning of a model, leverage ef   cient k-mips
to speed up extreme classi   er training and improve precision and speedup by combining multiple
id91s.
finally, we mention that, while putting the    nal touches to this paper, another very recent and dif-
ferent mips approach, based on vector quantization, came to our knowledge (guo et al., 2015). we
highlight that the    rst arxiv post of our work predates their work. nevertheless, while we did not
have time to empirically compare to this approach here, we hope to do so in future work.

acknowledgements

the authors would like to thank the developers of theano (bergstra et al., 2010) for developing such
a powerful tool. we acknowledge the support of the following organizations for research funding
and computing support: samsung, nserc, calcul quebec, compute canada, the canada research
chairs and cifar.

references
yoram bachrach, yehuda finkelstein, ran gilad-bachrach, liran katzir, noam koenigstein, nir
nice, and ulrich paquet. speeding up the xbox recommender system using a euclidean transfor-
mation for inner-product spaces. in proceedings of the 8th acm conference on recommender
systems, recsys    14, pages 257   264, 2014.

yoshua bengio, r  ejean ducharme, pascal vincent, and christian janvin. a neural probabilistic
issn 1532-4435. url

language model. j. mach. learn. res., 3:1137   1155, march 2003.
http://dl.acm.org/citation.cfm?id=944919.944966.

james bergstra, olivier breuleux, fr  ed  eric bastien, pascal lamblin, razvan pascanu, guillaume
desjardins, joseph turian, david warde-farley, and yoshua bengio. theano: a cpu and gpu
in proceedings of the python for scienti   c computing conference
math expression compiler.
(scipy), june 2010.

9

0.000.050.100.150.200.250.300.350.400.45s.d of noise 020406080100precision@10.000.050.100.150.200.250.300.350.400.45s.d of noise 020406080100precision@100.000.050.100.150.200.250.300.350.400.45s.d of noise 0102030405060708090precision@100id116pca-tree+nbsrp-hashwta-hashhier-id116under review as a conference paper at iclr 2016

paolo cremonesi, yehuda koren, and roberto turrin. performance of recommender algorithms on
top-n recommendation tasks. in proceedings of the fourth acm conference on recommender
systems, recsys    10, pages 39   46, 2010.

mayur datar, nicole immorlica, piotr indyk, and vahab s. mirrokni. locality-sensitive hashing
scheme based on p-stable distributions. in proceedings of the twentieth annual symposium on
computational geometry, scg    04, pages 253   262, 2004.

ruiqi guo, sanjiv kumar, krzysztof choromanski, and david simcha. quantization based fast
inner product search. corr, abs/1509.01469, 2015. url http://arxiv.org/abs/1509.
01469.

noam koenigstein, parikshit ram, and yuval shavitt. ef   cient retrieval of recommendations in
in proceedings of the 21st acm international conference
a id105 framework.
on information and knowledge management, cikm    12, pages 535   544, new york, ny, usa,
2012. acm.

tomas mikolov, ilya sutskever, kai chen, greg s corrado, and jeff dean. distributed representa-
tions of words and phrases and their compositionality. in c.j.c. burges, l. bottou, m. welling,
z. ghahramani, and k.q. weinberger, editors, advances in neural information processing sys-
tems 26, pages 3111   3119. curran associates, inc., 2013.

andriy mnih and geoffrey hinton. a scalable hierarchical distributed language model. in advances

in neural information processing systems, volume 21, pages 1081   1088, 2009.

frederic morin and yoshua bengio. hierarchical probabilistic neural network language model.
in robert g. cowell and zoubin ghahramani, editors, proceedings of the tenth international
workshop on arti   cial intelligence and statistics, pages 246   252, 2005.

behnam neyshabur and nathan srebro. on symmetric and asymmetric lshs for inner product search.

in proceedings of the 31st international conference on machine learning, 2015.

parikshit ram and alexander g. gray. maximum inner-product search using cone trees. in pro-
ceedings of the 18th acm sigkdd international conference on knowledge discovery and data
mining, kdd    12, pages 931   939, 2012.

anshumali shrivastava and ping li. asymmetric lsh (alsh) for sublinear time maximum in-
ner product search (mips). in advances in neural information processing systems 27: annual
conference on neural information processing systems 2014, december 8-13 2014, montreal,
quebec, canada, pages 2321   2329, 2014.

anshumali shrivastava and ping li.

improved asymmetric locality sensitive hashing (alsh) for
maximum inner product search (mips). in proceedings of conference on uncertainty in arti   cial
intelligence (uai), 2015.

sudheendra vijayanarasimhan, jon shlens, rajat monga, and jay yagnik. deep networks with large

output spaces. arxiv preprint arxiv:1412.7479, 2014.

shi zhong. ef   cient online spherical id116 id91. in neural networks, 2005. ijid98   05.
proceedings. 2005 ieee international joint conference on, volume 5, pages 3180   3185. ieee,
2005.

10

