3
1
0
2

 
t
c
o
1
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
6
5
5
0

.

5
0
3
1
:
v
i
x
r
a

a quantum teleportation inspired algorithm

produces sentence meaning from word

meaning and grammatical structure

stephen clark

university of cambridge
computer laboratory

bob coecke, edward grefenstette, stephen pulman

university of oxford

department of computer science

mehrnoosh sadrzadeh
queen mary, london

school of electronic engineering and computer science

abstract

we discuss an algorithm which produces the meaning of a sentence given
meanings of its words, and its resemblance to quantum teleportation. in fact,
this protocol was the main source of inspiration for this algorithm which has
many applications in the area of natural language processing.

quantum teleportation [2] is one of the most conceptually challenging and
practically useful concepts that has emerged from the quantum information revo-
lution. for example, via logic-gate teleportation [16] it gave rise to the measurement-
based computational model, it also plays a key role in current investigations into
the nature of quantum correlations, e.g. [23], and it even has been proposed as
a model for time travel [3]. it also formed the cornerstone for a new axiomatic
approach and diagrammatic calculus for quantum theory [1, 7, 8].

1

arguably, when such a radically new concept emerges in a novel foundational
area of scienti   c investigation, one may expect that the resulting conceptual and
structural insights could also lead to progress in other areas, something which has
happened on many occasions in the history of physics. in the context of quantum
information, for example, it is well-known that quantum complexity theory has
helped to solve many problems in classical complexity theory.

here we explain how a high-level description of quantum teleportation with
emphasis on information    ows has successfully helped to solve a longstanding
open problem in the area of natural language processing (nlp), and the problem
of modeling meaning for natural language more generally [9, 12]. this work
featured as a cover heading in the new scientist (11 dec. 2011) [22], and has
been experimentally tested for its capability to perform key nlp tasks such as
id51 in context [15].1

the nlp problem. dictionaries explain the meanings of words; however, in
natural language words are organized as sentences, but we don   t have dictionaries
that explain the meanings of sentences. still, a sentence carries more information
than the words it is made up from; e.g. meaning(alice sends a message to
bob) (cid:54)= meaning(bob sends a message to alice). evidently, this is where
grammatical structure comes into play. consequently, we as humans must use
some algorithm that converts the meanings of words, via the grammatical struc-
ture, into the meaning of a sentence. all of this may seem to be only of academic
interest; however, search engines such as google face exactly the same challenge.
they typically read a string of words as a    bag of words   , ignoring the grammat-
ical structure. this is simply because (until recently) there was no mathematical
model for assigning meanings to sentences.2 on the other hand, there is a widely
used model for word meaning, the vector space model [24].

this vector space model of word meaning works as follows. one chooses a
set of context words which will form the basis vectors of a vector space.3 given
a word to which one wishes to assign meaning, e.g.    alice   , one relies on a large
corpus, e.g. (part of) the web, to establish the relative frequency that    alice    occurs

1emnlp is the leading conference on corpus-based experimental nlp.
2more precisely, there was no mathematical model for assigning meanings to sentences that
went beyond truthfulness. montague semantics [21] is a compositional model of meaning, but at
most assigns truth values to sentences, and evidently there is more to sentence meaning than the
mere binary assignment of either true or false.

3these context words may include nouns, verbs etc.; the vector space model built from the

british national corpus typically contains 10s of thousands of these words as basis vectors.

2

   close    to each of these basis words. the list of all these relative frequencies yields
a vector that represents this word, its meaning vector. now, if one wants to verify
synonymy of two words, it suf   ces to compute the inner-product of the meaning
vectors of these words, and verify how close it is to 1. indeed, since synonyms are
interchangeable, one would expect them to typically occur in the context of the
same words, and hence their meaning vectors should be the same in the statistical
limit. for example, in a corpus mainly consisting of computer science literature,
one would expect alice and bob to always occur in the same context and hence
their meaning vectors would almost be the same. of course, if the corpus were
english literature (cf. [4]), then this similarity would break down.

until recently, the state of affairs in computational linguistics was one of two
separate communities [13]. one community focused on non-compositional purely
distributional methods such as the vector space model described above. the other
community studied the compositional mathematical structure of sentences, build-
ing on work by chomsky [5], lambek [18] and montague [21]. this work is
mainly about the grammatical structure of sentences; grammatical type calculi
are algebraic gadgets that allow one to verify whether a sentence has a correct
grammatical structure.

caps, cups, and teleportation.
in [1], a novel axiomatic framework was pro-
posed to reason about quantum informatic processes, which admits a sound and
faithful purely diagrammatic calculus [7]; for some more recent developments
we refer to [8]. ideal post-selected teleportation provides the cornerstone for the
diagrammatic reasoning techniques, e.g. here is the derivation of the general tele-
portation protocol where the f-label represents both the measurement outcome
and the corresponding correction performed by bob [7]:

3

=ff=fffalicebob=alicebobfthe main conceptual idea behind these diagrams is that, besides their operational
physical meaning, they also admit a    logical reading    in terms of information    ow:

here, the red line represents the logical    ow which indicates that the state incom-
ing at alice   s side    rst gets acted upon by an operation f, and then by its adjoint
f   , which in the case that f is unitary results in the outgoing state at bob   s side
being identical to the incoming one at alice   s side.4

when interpreted in hilbert space, the key ingredients of this formalism are

   cups    and    caps   :

:= |00(cid:105) + |11(cid:105)

:= (cid:104)00| + (cid:104)11|

and the equation that governs them is:

(((cid:104)00| + (cid:104)11|)     id)(id     (|00(cid:105) + |11(cid:105))) = id

which diagrammatically depicts as:

4this    logical reading    of projectors on entangled states in terms of information    ow was    rst

proposed by one of the authors in [6].

4

falicebobf=in this language the choi-jamiolkowski isomorphism:

|  (cid:105) =

=

= (id     f )(|00(cid:105) + |11(cid:105))

interprets a bipartite state (the grey triangle) as a    cup    which changes the direction
of the information    ow together with an operation f that alters the information.

non-separatedness means topological connectedness:

|  (cid:105) =

(cid:54)=

= |  (cid:105)     |  (cid:105)

which is interpreted as the fact that information can    ow between the two sys-
tems involved. hence, when focussing on pure states, the cups effectively witness
entanglement in terms of the information    ows that it enables.

it is exactly this interpretation of the vectors representing the states of com-
pound quantum systems in terms of enabling information    ows that will provide
the cornerstone for our compositional and distributional model of meaning.

solution to the nlp problem: the intuition. before we explain the precise
algorithm that produces sentence meaning from word meaning, we provide the
analogy with the above.

a transitive verb requires both an object and a subject to yield a grammatically
correct sentence. consider the sentence    alice hates bob   . assume that the words
in it are represented by vectors, which as above we denote by triangles:

            
alice                 

hates              

bob =

note here that treating verbs as    compound    was already the case in grammatical
type calculi, as we discuss below. so how do these words interact to produce the
meaning of a sentence? for the verb to produce the meaning of the sentence, that
is, the statement of the fact that alice hates bob, it of course needs to know what
its subject and object are, that is, it requires knowing their meanings. therefore,
inspired by the above discussion on teleportation, we    feed    the meaning vectors
            
            
hates which then    spits out    the meaning of the
alice and

         
bob into the verb

5

falicebobhatessentence:

again, that the meaning of the sentence is produced by the transitive verb after
interacting with its nouns is also something that was the case in grammatical type
calculi.

in the same vein, for an intransitive verb, we obtain an even more direct ana-

logue to quantum teleportation:

(cid:32)(cid:88)

(cid:33)

(cid:104)ii|     id

            
alice                       

dreams) =

(

i

note here that non-separatedness of verbs is obvious: if in the sentence    alice
hates bob    hates would be disconnected, then the meaning of the sentence would
not depend on the meanings of    alice    and    bob   , so,    anyone hates everyone   !

a grammatical type calculus: lambek   s pregroups.
in order to give a precise
description of our algorithm we now give a brief account of lambek   s pregroup
grammar [19, 20].5

pregroups capture structural similarities across a wide range of language fam-
ilies [20]. they combine a remnant of group structure with partial ordering; the
usual (left and right) group laws for the inverse are replaced by four inequalities
involving distinct left and right pseudo-inverses x   1 and    1x:

x   1    x     1     x    x   1

x       1x     1        1x    x .

as a grammatical type calculus, its elements are basic grammatical types, e.g. the
noun type n and sentence type s. other types arise from the pseudo-inverses and

5an interesting aside: lambek published his paper on the widely used lambek grammars in
1958 [18]. his recent book on pregroups appeared 50 years later [20]! all this time lambek
worked in montreal, the location of the upcoming qip, and it was also in montreal (in 2004) that
during a talk by one of us he    rst pointed to the structural coincidence between pregroup grammars
and quantum axiomatics in terms of cups and caps.

6

alicebobhatesalicedreamsthe multiplication, e.g. the transitive verb type tv :=    1n    s    n   1. then:

(cid:122)(cid:125)(cid:124)(cid:123)n   

e.g. alice

(cid:125)(cid:124)

(cid:122)
   1n    s    n   1   e.g. bob

(cid:122)(cid:125)(cid:124)(cid:123)n = (n       1n)    s    (n   1    n)

e.g. hates

(cid:123)

    1    s    1
= s

such an inequality n  tv  n     s then stands for the fact that    noun     transitive verb
    noun    is a valid grammatical structure for a sentence. note here the correspon-
dence with our interpretation of such a sentence in terms of information    ow: the
verb requires two nouns to be    fed into it    to yield a sentence type; n   1 and    1n
capture    the verb requests type n on the left/right   . in fact, the inequalities using
n       1n     1 and n   1    n     1 can also be represented with    directed    caps:

s

n       1n    s    n   1    n

(1)

which represent the inequalities:
    1

   

n       1n

    1

   

n   1    n

moreover, a pregroup can be de   ned in terms of cups and caps. in category theo-
retic language, both the diagrammatic language for quantum axiomatics and pre-
groups are so-called compact closed categories [17, 25]; while the quantum lan-
guage is symmetric, pregroups have to be non-symmetric given the importance of
word-order in sentences.

solution to the nlp problem: the algorithm. assume a grammatically well-
typed sentence and a meaning vector       v j for each of its words, which we assume to
be represented in a vector space of which the tensor structure matches the structure
of its grammatical type,6 e.g.:

n ; v

tv =    1n    s    n   1 ; v     w     v

where w is the vector space in which we intend to represent the meanings of
sentences. then one proceeds as follows:

6how this can be achieved within the context of the vector space model of meaning is outlined

in [14] and used in [15].

7

1. compute the tensor product

               
w ords =       v 1     . . .         v k of the word meaning
verb                 noun2.
2. construct a linear map f that represents the type reduction as follows: given
the diagram that represents a type reduction (cf. (1) above), we interpret

vectors in order of appearance in the sentence; e.g.             noun1              
i(cid:104)ii|     id    (cid:80)
caps as(cid:80)

i(cid:104)ii| and straight wire as identities; e.g.(cid:80)

i(cid:104)ii|.

                     
sentence := f (

               
w ords)     w.

3. compute
hence the crux is:

the grammatical correctness veri   cation procedure be-
comes an actual linear map that transforms the meanings of words into the mean-
ing of the sentence by making these words interact via caps. does it work? the
proof is in the pudding. proof-of-concept examples are in [12], and concrete ex-
perimentally veri   ed applications are in [15].

we invite the reader to also look at [12] for the example sentence:

   alice does not like bob    ,

where    does    and    not    are assigned not empirical but    logical    meanings:

resulting in a more interesting information    ow structure. in ongoing work we
investigate how the structures that are used to represent classical data    ow in the
quantum teleportation protocol enable one to model more of these    logical words   .
for example, in [10] this was done for relative pronouns such as    who   ,    which   ,
   that    and    whose   .

finally, while the well-established structural similarities across language fam-
ilies in terms of grammatical type calculi may seem mysterious, the teleportation-
like information    ow interpretation presented here clearly explains them.

references

[1] s. abramsky and b. coecke (2004) a categorical semantics of quantum pro-
tocols. in: proceedings of 19th ieee conference on logic in computer sci-
ence, pages 415   425. ieee press. arxiv:quant-ph/0402130.

8

alicelikebobmeaning vectors of wordsgrammarnot[2] c. h. bennett, g. brassard, c. cr  epeau, r. jozsa, a. peres and w. k. woot-
ers (1993) teleporting an unknown quantum state via dual classical and
einstein-podolsky-rosen channels. physical review letters 70, 1895   1899.

[3] c. h. bennett and b. schumacher (2002) lecture at tata institute for fun-

damental research, mumbai, india.

[4] l. carroll (1865) alice   s adventures in wonderland. macmillan and co.

[5] n. chomsky (1957) syntactic structures. mouton.

[6] b. coecke (2004) the logic of entanglement. arxiv: quant-ph/0402014

[7] b. coecke (2010) quantum picturalism. contemporary physics 51, 59   83.

arxiv:0908.1787

[8] b. coecke and r. duncan (2011): interacting quantum observables: cat-
egorical algebra and diagrammatics. new journal of physics 13, 043016.
arxiv:0906.4725

[9] s. clark, b. coecke and m. sadrzadeh (2008) a compositional distributional
model of meaning. proceedings of aaai spring symposium on quantum
interaction, pages 133   140, aaai press.

[10] s. clark, b. coecke and m. sadrzadeh (2013) the frobenius anatomy of

relative pronouns. draft paper.

[11] b. coecke and d. pavlovic (2007) quantum measurements without sums.
in: mathematics of quantum computing and technology, g. chen,
l. kauffman and s. lamonaco (eds), pages 567   604. taylor and francis.
arxiv:quant-ph/0608035

[12] b. coecke, m. sadrzadeh and s. clark (2011) mathematical foundations
for a compositional distributional model of meaning. linguistic analysis    
lambek festschrift. arxiv:1003.4394 [cs.cl]

[13] g. gazdar (1996) paradigm merger in natural language processing. in:
computing tomorrow: future research directions in computer science,
pages 88   109, r. milner and i. wand (eds.), cambridge university press.

9

[14] e. grefenstette, m. sadrzadeh, s. clark, b. coecke and s. pulman (2011)
concrete compositional sentence spaces for a compositional distributional
model of meaning. in: proceedings of the 2011 international conference on
computational semantics (iwcs). arxiv:1101.0309 [cs.cl]

[15] e. grefenstette and m. sadrzadeh (2011) experimental support for a cate-
gorical compositional distributional model of meaning. in: proceedings of
the 2011 conference on empirical methods in natural language processing
(emnlp). arxiv:1106.4058 [cs.cl]

[16] d. gottesman and i. l. chuang (1999) quantum teleportation is a universal

computational primitive. nature 402, 390   393. arxiv:quant-ph/9908010

[17] g. m. kelly and m. l. laplaza (1980) coherence for compact closed cate-

gories. journal of pure and applied algebra 19, 193   213.

[18] j. lambek (1958) the mathematics of sentence structure. american mathe-

matical monthly 65, 154   169.

[19] j. lambek (1999) type grammar revisited. in: logical aspects of computa-

tional linguistics, lecture notes in arti   cial intelligence 1582, springer.

[20] j. lambek (2008) from word to sentence. polimetrica.

[21] r. montague (1974) formal philosophy: selected papers. yale university

press.

[22] new scientist (december 11, 2010) quantum links let computers understand

language.

[23] p. skrzypczyk, n. brunner and s. popescu (2009) emergence of quan-
tum correlations from nonlocality swapping. physical review letters 102,
110402. arxiv:0811.2937

[24] h. schuetze (1998) automatic word sense discrimination. computational

linguistics 24, 97   123.

[25] p. selinger (2011) a survey of graphical languages for monoidal categories.
in: new structures for physics, b. coecke (ed), pages 289   356. lecture
notes in physics 813, springer-verlag. arxiv:0908.3347

10

