traversing id13s in vector space

kelvin guu

stanford university
kguu@stanford.edu

john miller

stanford university

percy liang

stanford university

millerjp@stanford.edu

pliang@cs.stanford.edu

5
1
0
2

 

g
u
a
9
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
4
9
0
1
0

.

6
0
5
1
:
v
i
x
r
a

abstract

path queries on a id13 can
be used to answer compositional ques-
tions such as    what languages are spoken
by people living in lisbon?   . however,
id13s often have missing facts
(edges) which disrupts path queries. re-
cent models for knowledge base comple-
tion impute missing facts by embedding
id13s in vector spaces. we
show that these models can be recursively
applied to answer path queries, but that
they suffer from cascading errors. this
motivates a new    compositional    training
objective, which dramatically improves all
models    ability to answer path queries, in
some cases more than doubling accuracy.
on a standard knowledge base comple-
tion task, we also demonstrate that com-
positional training acts as a novel form of
structural id173, reliably improv-
ing performance across all base models
(reducing errors by up to 43%) and achiev-
ing new state-of-the-art results.
introduction

1
broad-coverage knowledge bases such as free-
base (bollacker et al., 2008) support a rich array
of reasoning and id53 applications,
but they are known to suffer from incomplete cov-
erage (min et al., 2013). for example, as of may
2015, freebase has an entity tad lincoln (abra-
ham lincoln   s son), but does not have his ethnic-
ity. an elegant solution to incompleteness is using
vector space representations: controlling the di-
mensionality of the vector space forces generaliza-
tion to new facts (nickel et al., 2011; nickel et al.,
2012; socher et al., 2013; riedel et al., 2013; nee-
lakantan et al., 2015). in the example, we would
hope to infer tad   s ethnicity from the ethnicity of
his parents.

figure 1: we propose performing path queries
such as tad lincoln/parents/location (   where
are tad lincoln   s parents located?   ) in a parallel
low-dimensional vector space. here, entity sets
(boxed) are represented as real vectors, and edge
traversal is driven by vector-to-vector transforma-
tions (e.g., id127).

however, what is missing from these vector
space models is the original strength of knowledge
bases: the ability to support compositional queries
(ullman, 1985).
for example, we might ask
what the ethnicity of abraham lincoln   s daugh-
ter would be. this can be formulated as a path
query on the id13, and we would like
a method that can answer this ef   ciently, while
generalizing over missing facts and even missing
or hypothetical entities (abraham lincoln did not
in fact have a daughter).

in this paper, we present a scheme to answer
path queries on knowledge bases by    composi-
tionalizing    a broad class of vector space mod-
els that have been used for knowledge base com-
pletion (see figure 1). at a high level, we inter-
pret the base vector space model as implementing
a soft edge traversal operator. this operator can
then be recursively applied to predict paths. our
interpretation suggests a new compositional train-
ing objective that encourages better modeling of

paths. our technique is applicable to a broad class
of composable models that includes the bilinear
model (nickel et al., 2011) and transe (bordes et
al., 2013).

we have two key empirical    ndings: first, we
show that compositional training enables us to
answer path queries up to at least length 5 by
substantially reducing cascading errors present in
the base vector space model. second, we    nd
that somewhat surprisingly, compositional train-
ing also improves upon state-of-the-art perfor-
mance for knowledge base completion, which is a
special case of answering unit length path queries.
therefore, compositional training can also be seen
as a new form of structural id173 for ex-
isting models.
2 task
we now give a formal de   nition of the task of an-
swering path queries on a knowledge base. let
e be a set of entities and r be a set of binary
relations. a id13 g is de   ned as a
set of triples of the form (s, r, t) where s, t     e
and r     r. an example of a triple in freebase is
(tad lincoln, parents, abraham lincoln).

a path query q consists of an initial anchor en-
tity, s, followed by a sequence of relations to be
traversed, p = (r1, . . . , rk). the answer or deno-

tation of the query,(cid:74)q(cid:75), is the set of all entities that

can be reached from s by traversing p. formally,
this can be de   ned recursively:

(cid:74)s(cid:75) def= {s},
(cid:74)q/r(cid:75) def= {t :    s    (cid:74)q(cid:75), (s, r, t)     g} .

(1)

(2)

for example, tad lincoln/parents/location is a
query q that asks:    where did tad lincoln   s par-
ents live?   .
for evaluation (see section 5 for details), we de-
   ne the set of candidate answers to a query c(q)
as the set of all entities that    type match   , namely
those that participate in the    nal relation of q at
least once; and let n (q) be the incorrect answers:
c (s/r1/       /rk) def= {t |    e, (e, rk, t)     g} (3)
(4)
knowledge base completion. knowledge base
completion (kbc)
is the task of predicting
whether a given edge (s, r, t) belongs in the graph
or not. this can be formulated as a path query
q = s/r with candidate answer t.

n (q) def= c (q)\(cid:74)q(cid:75).

3 compositionalization

in this section, we show how to compositional-
ize existing kbc models to answer path queries.
we start with a motivating example in section 3.1,
then present the general technique in section 3.2.
this suggests a new compositional training objec-
tive, described in section 3.3. finally, we illus-
trate the technique for several more models in sec-
tion 3.4, which we use in our experiments.

3.1 example
a common vector space model for knowledge
base completion is the bilinear model (nickel et
al., 2011). in this model, we learn a vector xe    
rd for each entity e     e and a matrix wr     rd  d
for each relation r     r. given a query s/r (ask-
ing for the set of entities connected to s via relation

r), the bilinear model scores how likely t    (cid:74)s/r(cid:75)

holds using

score(s/r, t) = x(cid:62)

s wrxt.

(5)

to motivate our compositionalization tech-
nique, take d = |e| and suppose wr is the ad-
jacency matrix for relation r and entity vector xe
is the indicator vector with a 1 in the entry corre-
sponding to entity e. then, to answer a path query
q = s/r1/ . . . /rk, we would then compute

score(q, t) = x(cid:62)

s wr1 . . . wrk xt.

(6)

it is easy to verify that the score counts the number
of unique paths between s and t following rela-
tions r1/ . . . /rk. hence, any t with positive score

is a correct answer ((cid:74)q(cid:75) = {t : score(q, t) > 0}).

let us interpret (6) recursively. the model be-
gins with an entity vector xs, and sequentially
applies traversal operators tri(v) = v(cid:62)wri for
each ri. each traversal operation results in a
new    set vector    representing the entities reached
at that point in traversal (corresponding to the
nonzero entries of the set vector). finally, it ap-
plies the membership operator m(v, xt) = v(cid:62)xt

to check if t     (cid:74)s/r1/ . . . /rk(cid:75). writing graph

traversal in this way immediately suggests a useful
generalization: take d much smaller than |e| and
learn the parameters wr and xe.

3.2 general technique
the strategy used to extend the bilinear model of
(5) to the compositional model in (6) can be ap-
plied to any composable model: namely, one that

(8)

(9)

has a scoring function of the form:

score(s/r, t) = m(tr(xs), xt)

(7)
for some choice of membership operator m : rd  
rd     r and traversal operator tr : rd     rd.

we can now de   ne the vector denotation of a

query (cid:74)q(cid:75)v analogous to the de   nition of (cid:74)q(cid:75) in

(1) and (2):

def= xs,

(cid:74)s(cid:75)v
(cid:74)q/r(cid:75)v
def= tr ((cid:74)q(cid:75)v) .
score(q, t) = m((cid:74)q(cid:75)v,(cid:74)t(cid:75)v).

the score function for a compositionalized

model is then

(10)

we would like (cid:74)q(cid:75)v to approximately represent
the set (cid:74)q(cid:75) in the sense that for every e     (cid:74)q(cid:75),
m ((cid:74)q(cid:75)v,(cid:74)e(cid:75)v) is larger than the values for e (cid:54)   
(cid:74)q(cid:75). of course it is not possible to represent all

sets perfectly, but in the next section, we present a
training objective that explicitly optimizes t and
m to preserve path information.
3.3 compositional training
the score function in (10) naturally suggests a new
compositional training objective. let {(qi, ti)}n
i=1
denote a set of path query training examples with
path lengths ranging from 1 to l. we minimize
the following max-margin objective:

n(cid:88)

(cid:88)

(cid:2)1     margin(qi, ti, t(cid:48))(cid:3)

j(  ) =

+ ,
margin(q, t, t(cid:48)) = score(q, t)     score(q, t(cid:48)),

t(cid:48)   n (qi)

i=1

where the parameters are the membership opera-
tor, the traversal operators, and the entity vectors:

   = {m}     {tr : r     r}    (cid:110)

xe     rd : e     e(cid:111)

.

this objective encourages the construction of
   set vectors   : because there are path queries of
different lengths and types, the model must learn

to produce an accurate set vector (cid:74)q(cid:75)v after any

sequence of traversals. another perspective is
that each traversal operator is trained such that
its transformation preserves information in the
set vector which might be needed in subsequent
traversal steps.

in contrast, previously proposed training objec-
tives for knowledge base completion only train on

queries of path length 1. we will refer to this spe-
cial case as single-edge training.

in section 5, we show that compositional train-
ing leads to substantially better results for both
path query answering and knowledge base com-
pletion. in section 6, we provide insight into why.

3.4 other composable models
there are many possible candidates for t and m.
for example, t could be one   s favorite neural net-
work mapping from rd to rd. here, we focus on
two composable models that were both recently
shown to achieve state-of-the-art performance on
knowledge base completion.

transe. the transe model of bordes et al.
(2013) uses the scoring function

score(s/r, t) =    (cid:107)xs + wr     xt(cid:107)2
2.

(11)

where xs, wr and xt are all d-dimensional vectors.
in this case, the model can be expressed using

membership operator

m(v, xt) =    (cid:107)v     xt(cid:107)2

2

(12)

and traversal operator tr(xs) = xs + wr.
hence, transe can handle a path query q =
s/r1/r2/       /rk using
score(q, t) =    (cid:107)xs + wr1 +        + wrk     xt(cid:107)2
2.

we visualize the compositional transe model in
figure 2.

bilinear-diag. the bilinear-diag model of
yang et al. (2015) is a special case of the bilinear
model with the relation matrices constrained to be
diagonal. alternatively, the model can be viewed
as a variant of transe with multiplicative interac-
tions between entity and relation vectors.

not all models can be compositionalized.
it
is important to point out that some models are
not naturally composable   for example, the latent
feature model of riedel et al. (2013) and the neu-
ral tensor network of socher et al. (2013). these
approaches have scoring functions which combine
s, r and t in a way that does not involve an inter-
mediate vector representing s/r alone without t,
so they do not decompose according to (7).

relations
entities

train
base
test
paths train
test

id138

freebase

11

38,696
112,581
10,544

13

75,043
316,232
23,733

2,129,539

46,577

6,266,058
109,557

table 1: id138 and freebase statistics for base
and path query datasets.

3.5

implementation

we use adagrad (duchi et al., 2010) to optimize
j(  ), which is in general non-convex.
initial-
ization scale, mini-batch size and step size were
cross-validated for all models. we initialize all
parameters with i.i.d. gaussians of variance 0.1 in
every entry, use a mini-batch size of 300 examples,
and a step size in [0.001, 0.1] (chosen via cross-
validation) for all of the models. for each exam-
ple q, we sample 10 negative entities t(cid:48)     n (q).
during training, all of the entity vectors are con-
strained to lie on the unit ball, and we clipped the
gradients to the median of the observed gradients
if the update exceeded 3 times the median.

we    rst train on path queries of length 1 until
convergence and then train on all path queries until
convergence. this guarantees that the model mas-
ters basic edges before composing them to form
paths. when training on path queries, we explic-
itly parameterize inverse relations. for the bilinear
model, we initialize wr   1 with w (cid:62)
r . for transe,
we initialize wr   1 with    wr. for bilinear-diag,
we found initializing wr   1 with the exact inverse
1/wr is numerically unstable, so we instead ran-
domly initialize wr   1 with i.i.d gaussians of vari-
ance 0.1 in every entry. additionally, for the bi-
linear model, we replaced the sum over n (qi) in
the objective with a max since it yielded slightly
higher accuracy. our models are implemented us-
ing theano (bastien et al., 2012; bergstra et al.,
2010).

4 datasets

in section 4.1, we describe two standard knowl-
edge base completion datasets. these consist of
single-edge queries, so we call them base datasets.
in section 4.2, we generate path query datasets
from these base datasets.

4.1 base datasets
our experiments are conducted using the sub-
sets of id138 and freebase from socher et al.
(2013). the statistics of these datasets and their
splits are given in table 1.

the id138 and freebase subsets exhibit sub-
stantial differences that can in   uence model per-
formance. the freebase subset is almost bipartite
with most of the edges taking the form (s, r, t) for
some person s, relation r and property t. in word-
net, both the source and target entities are arbi-
trary words.

both the raw id138 and freebase contain
many relations that are almost perfectly correlated
with an inverse relation. for example, id138
contains both has part and part of, and freebase
contains both parents and children. at test time,
a query on an edge (s, r, t) is easy to answer if the
inverse triple (t, r   1, s) was observed in the train-
ing set. following socher et al. (2013), we ac-
count for this by excluding such    trivial    queries
from the test set.

4.2 path query datasets
given a base id13, we generate path
queries by performing id93 on the graph.
if we view compositional training as a form of reg-
ularization, this approach allows us to generate ex-
tremely large amounts of auxiliary training data.
the procedure is given below.
let gtrain be the training graph, which consists
only of the edges in the training set of the base
dataset. we then repeatedly generate training ex-
amples with the following procedure:
1. uniformly sample a path length l    
{1, . . . , lmax}, and uniformly sample a start-
ing entity s     e.

2. perform a random walk beginning at entity s

and continuing l steps.

(a) at step i of the walk, choose a relation
ri uniformly from the set of relations in-
cident on the current entity e.

(b) choose the next entity uniformly from

the set of entities reachable via ri.

3. output a query-answer pair, (q, t), where q =
s/r1/       /rl and t is the    nal entity of the
random walk.

in practice, we do not sample paths of length 1 and
instead directly add all of the edges from gtrain to
the path query dataset.
to generate a path query test set, we repeat
the above procedure except using the graph gfull,
which is gtrain plus all of the test edges from the
base dataset. then we remove any queries from
the test set that also appeared in the training set.
the statistics for the path query datasets are pre-
sented in table 1.

5 main results

we evaluate the models derived in section 3 on
two tasks: path query answering and knowledge
base completion. on both tasks, we show that the
compositional training strategy proposed in sec-
tion 3.3 leads to substantial performance gains
over standard single-edge training. we also com-
pare directly against the kbc results of socher et
al. (2013), demonstrating that previously inferior
models now match or outperform state-of-the-art
models after compositional training.

evaluation metric. numerous metrics have
been used to evaluate knowledge base queries, in-
cluding hits at 10 (percentage of correct answers
ranked in the top 10) and mean rank. we evaluate
on hits at 10, as well as a normalized version of
mean rank, mean quantile, which better accounts
for the total number of candidates. for a query q,
the quantile of a correct answer t is the fraction of
incorrect answers ranked after t:

|{t(cid:48)     n (q) : score(q, t(cid:48)) < score(q, t)}|

|n (q)|

(13)

the quantile ranges from 0 to 1, with 1 being opti-
mal. mean quantile is then de   ned to be the aver-
age quantile score over all examples in the dataset.
to illustrate why id172 is important, con-
sider a set of queries on the relation gender. a
model that predicts the incorrect gender on ev-
ery query would receive a mean rank of 2 (since
there are only 2 candidate answers), which is fairly
good in absolute terms, whereas the mean quantile
would be 0, rightfully penalizing the model.

as a    nal note, several of the queries in the
freebase path dataset are    type-match trivial    in
the sense that all of the type matching candidates
c(q) are correct answers to the query. in this case,
mean quantile is unde   ned and we exclude such
queries from evaluation.

overview. the upper half of table 2 shows
that compositional training improves path query-
ing performance across all models and metrics on
both datasets, reducing error by up to 76.2%.

the lower half of table 2 shows that surpris-
ingly, compositional training also improves per-
formance on knowledge base completion across
almost all models, metrics and datasets. on word-
net, transe bene   ts the most, with a 43.3% re-
duction in error. on freebase, bilinear bene   ts
the most, with a 38.8% reduction in error.

in terms of mean quantile,

the best overall
model is transe (comp). in terms of hits at 10, the
best model on id138 is bilinear (comp), while
the best model on freebase is transe (comp).

get entities(cid:74)q(cid:75) are connected via relations p in the

deduction and induction. table 3 takes a
deeper look at performance on path query answer-
ing. we divided path queries into two subsets: de-
duction and induction. the deduction subset con-
sists of queries q = s/p where the source and tar-
training graph gtrain, but the speci   c query q was
never seen during training. such queries can be
answered by performing explicit traversal on the
training graph, so this subset tests a model   s abil-
ity to approximate the underlying training graph
and predict the existence of a path from a collec-
tion of single edges. the induction subset consists
of all other queries. this means that at least one
edge was missing on all paths following p from
source to target in the training graph. hence, this
subset tests a model   s generalization ability and its
robustness to missing edges.

performance on the deduction subset of the
dataset is disappointingly low for models trained
with single-edge training: they struggle to answer
path queries even when all edges in the path query
have been seen at training time. compositional
training dramatically reduces these errors, some-
times doubling mean quantile. in section 6, we
analyze how this might be possible. after com-
positional training, performance on the harder in-
duction subset is also much stronger. even when
edges are missing along a path, the models are able
to infer them.

interpretable queries. although
path
datasets consists of random queries, both datasets
contain a large number of useful, interpretable
queries. results on a few illustrative examples are
shown in table 4.

our

id138

path query task
mq
h@10
mq
h@10

freebase

kbc task

id138

freebase

mq
h@10
mq
h@10

bilinear
comp
89.4
54.3
83.5
42.1
comp
82.0
27.3
91.0
76.4

(%red)
30.7
19.0
60.7
21.9
(%red)
24.7
10.0
38.8
20.8

single
84.7
43.6
58.0
25.9
single
76.1
19.2
85.3
70.2

bilinear-diag

single
59.7
7.9
57.9
23.1
single
76.5
12.9
84.6
63.2

comp
90.4
31.1
84.8
38.6
comp
84.3
14.4
89.1
67.0

(%red)
76.2
25.4
63.9
20.2
(%red)
33.2
1.72
29.2
10.3

transe
comp
93.3
43.5
88
50.5
comp
86.1
16.5
92.8
78.6

(%red)
58.9
34.5
13.0
9.3

(%red)
43.3
12.5
1.37
-0.9

single
83.7
13.8
86.2
45.4
single
75.5
4.6
92.7
78.8

table 2: path query answering and knowledge base completion. we compare the performance of
single-edge training (single) vs compositional training (comp). mq: mean quantile, h@10: hits at 10,
%red: percentage reduction in error.

interpretable queries

x/institution/institution

   1/profession

x/parents/religion

x/nationality/nationality

   1/ethnicity

   1

   1

x/has part/has instance

x/type of/type of/type of

bilinear single bilinear comp

50.0
81.9
68.0
92.6
72.8

93.6
97.1
87.0
95.1
79.4

table 4: path query performance (mean quantile) on a selection of interpretable queries. we compare
bilinear single and bilinear comp. meanings of each query (descending):    what professions are there
at x   s institution?   ;    what is the religion of x   s parents?   ;    what are the ethnicities of people from the
same country as x?   ;    what types of parts does x have?   ; and the transitive    what is x a type of?   .
(note that a relation r and its inverse r   1 do not necessarily cancel out if r is not a one-to-one mapping.
   1 denotes the set of all people who work at the institution x
for example, x/institution/institution
works at, which is not just x.)

path query task id138

bilinear

bi-diag

single
comp
single
comp
transe single
comp

ded.
96.9
98.9
56.3
98.5
92.6
99.0

freebase
ind.
49.4
70.6
50.2
72.8
72.4
76.3

ind. ded.
49.3
66.0
75.6
82.1
49.3
51.6
84.5
78.2
85.3
71.7
87.4
87.5

table 3: deduction and induction. we compare
mean quantile performance of single-edge training
(single) vs compositional training (comp). length
1 queries are excluded.

comparison with socher et al. (2013). here,
we measure performance on the kbc task in terms
of the accuracy metric of socher et al. (2013).
this evaluation involves sampled negatives, and is
hence noisier than mean quantile, but makes our
results directly comparable to socher et al. (2013).
our results show that previously inferior models

such as the bilinear model can outperform state-
of-the-art models after compositional training.

socher et al. (2013) proposed parametrizing
each entity vector as the average of vectors of
words in the entity (wtad lincoln = 1
2 (wtad +
wlincoln), and pretraining these word vectors us-
ing the method of turian et al. (2010). table 5
reports results when using this approach in con-
junction with compositional training. we initial-
ized all models with word vectors from penning-
ton et al. (2014). we found that composition-
ally trained models outperform the neural tensor
network (ntn) on id138, while being only
slightly behind on freebase. (we did not use word
vectors in any of our other experiments.)

when the strategy of averaging word vectors to
form entity vectors is not applied, our composi-
tional models are signi   cantly better on id138
and slightly better on freebase. it is worth noting
that in many domains, entity names are not lexi-
cally meaningful, so word vector averaging is not

accuracy

ntn

bilinear comp
transe comp

id138
freebase
ev wv ev wv
90.0
70.6
89.4
77.6
80.3
89.6

86.2
87.6
84.9

87.2
86.1
87.6

table 5: model performance in terms of accu-
racy. ev: entity vectors are separate (initialized
randomly); wv: entity vectors are average of word
vectors (initialized with pretrained word vectors).

always meaningful.

training is effective.

6 analysis
in this section, we try to understand why com-
positional
for concrete-
ness, everything is described in terms of the bi-
linear model. we will refer to the compositionally
trained model as comp, and the model trained with
single-edge training as single.

6.1 why does compositional training
improve path query answering?

it is tempting to think that if single has accurately
modeled individual edges in a graph, it should ac-
curately model the paths that result from those
edges. this intuition turns out to be incorrect, as
revealed by single   s relatively weak performance
on the path query dataset. we hypothesize that this
is due to cascading errors along the path. for a
given edge (s, r, t) on the path, single-edge train-
ing encourages xt to be closer to x(cid:62)
s wr than any
other incorrect xt(cid:48). however, once this is achieved
by a margin of 1, it does not push xt any closer to
x(cid:62)
s wr. the remaining discrepancy is noise which
gets added at each step of path traversal. this is
illustrated schematically in figure 2.

to observe this phenomenon empirically, we
examine how well a model handles each interme-
diate step of a path query. we can do this by
measuring the reconstruction quality (rq) of the
set vector produced after each traversal operation.
since each intermediate stage is itself a valid path
query, we de   ne rq to be the average quantile

over all entities that belong in(cid:74)q(cid:75):

(cid:88)
t   (cid:74)q(cid:75) quantile (q, t)

1

|(cid:74)q(cid:75)|

when all entities in(cid:74)q(cid:75) are ranked above all in-

correct entities, rq is 1. in figure 3, we illustrate
how rq changes over the course of a query.

rq (q) =

(14)

figure 2: cascading errors visualized for
transe. each node represents the position of an
entity in vector space. the relation parent is
ideally a simple horizontal translation, but each
traversal introduces noise. the red circle is where
we expect tad   s parent to be. the red square is
where we expect tad   s grandparent to be. dotted
red lines show that error grows larger as we tra-
verse farther away from tad. compositional train-
ing pulls the entity vectors closer to the ideal ar-
rangement.

given the nature of cascading errors, it might
seem reasonable to address the problem by adding
a term to our objective which explicitly encour-
ages x(cid:62)
s wr to be as close as possible to xt. with
this motivation, we tried adding   (cid:107)x(cid:62)
s wr     xt(cid:107)2
2
term to the objective of the bilinear model and a
  (cid:107)xs + wr     xt(cid:107)2
2 term to the objective of transe.
we experimented with different settings of    over
the range [0.001, 100]. in no case did this addi-
tional (cid:96)2 term improve single   s performance on
the path query or single edge dataset. these re-
sults suggest that compositional training is a more
effective way to combat cascading errors.

6.2 why does compositional training

improve knowledge base completion?

table 2 reveals that comp also performs better on
the single-edge task of knowledge base comple-
tion. this is somewhat surprising, since single
is trained on a training set which distributionally
matches the test set, whereas comp is not. how-
ever, comp   s better performance on path queries
suggests that there must be another factor at play.
at a high level, training on paths must be provid-
ing some form of structural id173 which
reduces cascading errors.
indeed, paths in a
id13 have proven to be important fea-
tures for predicting the existence of single edges
(lao et al., 2011; neelakantan et al., 2015). for
example, consider the following horn clause:
parents (x, y)     location (y, z)     place of birth (x, z) ,

we have already seen empirically that single does
not meet criterion 1, because cascading errors
cause it to put incorrect entity vectors xt(cid:48) closer
to x(cid:62)
s wr1 . . . wrk than the correct entity. comp
mitigates these errors.

to empirically verify that comp also does a bet-
ter job of meeting criterion 2, we perform the
following: for a path type p and relation r, de-
   ne dist(p, r) to be the angle between their corre-
sponding matrices (treated as vectors in rd2). this
is a natural measure because x(cid:62)
s wrxt computes
the matrix inner product between wr and xsx(cid:62)
t .
hence, any matrix with small distance from wr
will produce nearly the same scores as wr for the
same entity pairs.

if comp is better at capturing the correlation be-
tween p and r, then we would expect that when
prec(p) is high, compositional
training should
shrink dist(p, r) more. to con   rm this hypothe-
sis, we enumerated over all 676 possible paths of
length 2 (including inverted relations), and exam-
ined the proportional reduction in dist(p, r) caused
by compositional training,

   dist(p, r) =

distcomp(p, r)     distsingle(p, r)

distsingle(p, r)

.
(16)

figure 4 shows that higher precision paths indeed
correspond to larger reductions in dist(p, r).

7 related work
knowledge base completion with vector space
models. many models have been proposed for
knowledge base completion, including those re-
viewed in section 3.4 (nickel et al., 2011; bor-
des et al., 2013; yang et al., 2015; socher et al.,
2013). dong et al. (2014) demonstrated that kbc
models can improve the quality of relation extrac-
tion by serving as graph-based priors. riedel et
al. (2013) showed that such models can be also be
directly used for open-domain id36.
our compositional training technique is an orthog-
onal improvement that could help any composable
model.
distributional id152. pre-
vious works have explored compositional vector
space representations in the context of logic and
sentence interpretation. in socher et al. (2012), a
matrix is associated with each word of a sentence,
and can be used to recursively modify the mean-
ing of nearby constituents. grefenstette (2013) ex-

figure 3: reconstruction quality (rq) at each step
of the query tad lincoln/parents/place of birth/
   1/profession. comp experiences
place of birth
signi   cantly less degradation in rq as path length
increases. correspondingly, the set of 5 highest
scoring entities computed at each step using comp
(green) is signi   ciantly more accurate than the set
given by single (blue). correct entities are bolded.

which states that if x has a parent with location
z, then x has place of birth z. the body of the
horn clause expresses a path from x to z. if comp
models the path better, then it should be better able
to use that knowledge to infer the head of the horn
clause.
more generally, consider horn clauses of the
form p     r, where p = r1/ . . . /rk is a path type
and r is the relation being predicted. let us focus
on horn clauses with high precision as de   ned by:

|(cid:74)p(cid:75)    (cid:74)r(cid:75)|
|(cid:74)p(cid:75)|

,

prec(p) =

(15)

where(cid:74)p(cid:75) is the set of entity pairs connected by p,
and similarly for(cid:74)r(cid:75).

intuitively, one way for the model to implicitly
learn and exploit such a horn clause would be to
satisfy the following two criteria:

1. the model should ensure a consistent spa-
tial relationship between entity pairs that are
related by the path type p; that is, keeping
x(cid:62)
s wr1 . . . wrk close to xt for all valid (s, t)
pairs.

2. the model   s representation of the path type p
and relation r should capture that spatial re-
s wr1 . . . wrk     xt im-
lationship; that is, x(cid:62)
s wr     xt, or simply wr1 . . . wrk    
plies x(cid:62)
wr.

(2014) sampled id93 on social networks
as training examples, with a different goal to clas-
sify nodes in the network. bordes et al. (2014) em-
bed paths as a sum of relation vectors for question
answering. our approach is unique in modeling
the denotation of each intermediate step of a path
query, and using this information to regularize the
spatial arrangement of entity vectors.

8 discussion

we introduced the task of answering path queries
on an incomplete knowledge base, and presented a
general technique for compositionalizing a broad
class of vector space models. our experiments
show that compositional training leads to state-of-
the-art performance on both path query answering
and knowledge base completion.

there are several key ideas from this paper: reg-
ularization by augmenting the dataset with paths,
representing sets as low-dimensional vectors in
a context-sensitive way, and performing function
composition using vectors. we believe these three
could all have greater applicability in the develop-
ment of vector space models for knowledge repre-
sentation and id136.

reproducibility our code, data, and exper-
iments are available on the codalab platform
at https://www.codalab.org/worksheets/
0xfcace41fdeec45f3bc6ddf31107b829f.

acknowledgments we would like to thank ga-
bor angeli for fruitful discussions and the anony-
mous reviewers for their valuable feedback. we
gratefully acknowledge the support of the google
natural language understanding focused pro-
gram and the national science foundation grad-
uate research fellowship under grant no. dge-
114747.

references
f. bastien, p. lamblin, r. pascanu, j. bergstra, i. j.
goodfellow, a. bergeron, n. bouchard, and y. ben-
gio. 2012. theano: new features and speed im-
provements. deep learning and unsupervised fea-
ture learning nips 2012 workshop.

j. bergstra, o. breuleux, f. bastien, p. lamblin, r. pas-
canu, g. desjardins, j. turian, d. warde-farley, and
y. bengio. 2010. theano: a cpu and gpu math
expression compiler. in proceedings of the python
for scienti   c computing conference (scipy).

figure 4: we divide paths of length 2 into high
precision (> 0.3), low precision (    0.3), and not
co-occuring with r. here r = nationality. each
box plot shows the min, max, and    rst and third
quartiles of    dist(p, r). as hypothesized, com-
positional training results in large decreases in
dist(p, r) for high precision paths p, modest de-
creases for low precision paths, and little to no de-
creases for irrelevant paths.

plored the ability of tensors to simulate logical cal-
culi. bowman et al. (2014) showed that recursive
neural networks can learn to distinguish impor-
tant semantic relations. socher et al. (2014) found
that compositional models were powerful enough
to describe and retrieve images.

we demonstrate that compositional representa-
tions are also useful in the context of knowledge
base querying and completion. in the aforemen-
tioned work, compositional models produce vec-
tors which represent truth values, sentiment or im-
age features.
in our approach, vectors represent
sets of entities constituting the denotation of a
knowledge base query.
path modeling. numerous methods have been
proposed to leverage path information for knowl-
edge base completion and id53.
nickel et al. (2014) proposed combining low-rank
models with sparse path features. lao and cohen
(2010) used id93 as features and gard-
ner et al. (2014) extended this approach by us-
ing vector space similarity to govern random walk
probabilities. neelakantan et al. (2015) addressed
the problem of path sparsity by embedding paths
using a recurrent neural network. perozzi et al.

k. bollacker, c. evans, p. paritosh, t. sturge, and
j. taylor. 2008. freebase: a collaboratively created
graph database for structuring human knowledge. in
international conference on management of data
(sigmod), pages 1247   1250.

a. bordes, n. usunier, a. garcia-duran, j. weston,
and o. yakhnenko. 2013. translating embeddings
in advances
for modeling multi-relational data.
in neural information processing systems (nips),
pages 2787   2795.

a. bordes, s. chopra, and j. weston. 2014. ques-
tion answering with subgraph embeddings. in em-
pirical methods in natural language processing
(emnlp).

m. nickel, v. tresp, and h. kriegel.

2011. a
three-way model for collective learning on multi-
relational data. in international conference on ma-
chine learning (icml), pages 809   816.

m. nickel, v. tresp, and h. kriegel. 2012. factorizing

yago. in world wide web (www).

m. nickel, x. jiang, and v. tresp. 2014. reducing the
rank in relational factorization models by including
observable patterns. in advances in neural informa-
tion processing systems (nips), pages 1179   1187.

j. pennington, r. socher, and c. d. manning. 2014.
glove: global vectors for word representation.
in
empirical methods in natural language processing
(emnlp).

s. r. bowman, c. potts, and c. d. manning. 2014.
can recursive neural tensor networks learn logical
reasoning? in international conference on learn-
ing representations (iclr).

b. perozzi, r. al-rfou, and s. skiena. 2014. deep-
walk: online learning of social representations. in
international conference on knowledge discovery
and data mining (kdd), pages 701   710.

x. dong, e. gabrilovich, g. heitz, w. horn, n. lao,
k. murphy, t. strohmann, s. sun, and w. zhang.
2014. knowledge vault: a web-scale approach
to probabilistic knowledge fusion. in international
conference on knowledge discovery and data min-
ing (kdd), pages 601   610.

j. duchi, e. hazan, and y. singer. 2010. adaptive sub-
gradient methods for online learning and stochastic
in conference on learning theory
optimization.
(colt).

m. gardner, p. talukdar,

j. krishnamurthy, and
t. mitchell. 2014. incorporating vector space sim-
ilarity in random walk id136 over knowledge
bases. in empirical methods in natural language
processing (emnlp).

e. grefenstette. 2013. towards a formal distributional
semantics: simulating logical calculi with tensors.
arxiv preprint arxiv:1304.5823.

n. lao and w. w. cohen.

2010. relational re-
trieval using a combination of path-constrained ran-
dom walks. machine learning, 81(1):53   67.

n. lao, t. mitchell, and w. w. cohen. 2011. random
walk id136 and learning in a large scale knowl-
in empirical methods in natural lan-
edge base.
guage processing (emnlp), pages 529   539.

b. min, r. grishman, l. wan, c. wang, and
d. gondek. 2013. distant supervision for relation
extraction with an incomplete knowledge base.
in
north american association for computational lin-
guistics (naacl), pages 777   782.

a. neelakantan, b. roth, and a. mccallum. 2015.
compositional vector space models for knowledge
base completion. in association for computational
linguistics (acl).

s. riedel, l. yao, and a. mccallum.

2013. re-
lation extraction with id105 and uni-
versal schemas. in north american association for
computational linguistics (naacl).

r. socher, b. huval, c. d. manning, and a. y. ng.
2012. semantic compositionality through recursive
matrix-vector spaces. in empirical methods in nat-
ural language processing and computational nat-
ural language learning (emnlp/conll), pages
1201   1211.

r. socher, d. chen, c. d. manning, and a. ng. 2013.
reasoning with neural tensor networks for knowl-
edge base completion. in advances in neural infor-
mation processing systems (nips), pages 926   934.

r. socher, a. karpathy, q. v. le, c. d. manning, and
a. y. ng. 2014. grounded id152
for    nding and describing images with sentences.
transactions of the association for computational
linguistics (tacl), 2:207   218.

j. turian, l. ratinov, and y. bengio. 2010. word rep-
resentations: a simple and general method for semi-
supervised learning. in proceedings of the 48th an-
nual meeting of the association for computational
linguistics, pages 384   394.

j. d. ullman. 1985. implementation of logical query
acm transactions on

languages for databases.
database systems (tods), 10(3):289   321.

b. yang, w. yih, x. he, j. gao, and l. deng.
2015. embedding entities and relations for learning
and id136 in knowledge bases. arxiv preprint
arxiv:1412.6575.

