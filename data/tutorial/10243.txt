5
1
0
2

 
l
u
j
 

8
2

 
 
]
l
c
.
s
c
[
 
 

1
v
6
3
6
7
0

.

7
0
5
1
:
v
i
x
r
a

reasoning about linguistic regularities in word

embeddings using matrix manifolds

sridhar mahadevan

university of massachusetts amherst

mahadeva@cs.umass.edu

sarath chandar

ibm research, usa

apsarathchandar@gmail.com

abstract

recent work has explored methods for learning continuous vector space word
representations re   ecting the underlying semantics of words. simple vector space
arithmetic using cosine distances has been shown to capture certain types of analo-
gies, such as reasoning about plurals from singulars, past tense from present tense,
etc. in this paper, we introduce a new approach to capture analogies in continu-
ous word representations, based on modeling not just individual word vectors, but
rather the subspaces spanned by groups of words. we exploit the property that the
set of subspaces in n-dimensional euclidean space form a curved manifold space
called the grassmannian, a quotient subgroup of the lie group of rotations in n-
dimensions. based on this mathematical model, we develop a modi   ed cosine
distance model based on geodesic kernels that captures relation-speci   c distances
across word categories. our experiments on analogy tasks show that our approach
performs signi   cantly better than the previous approaches for the given task.

1

introduction

in the past few decades,
there has been growing interest in machine learning of continuous
space representations of linguistic entities, such as words, sentences, paragraphs, and documents
[1, 2, 3, 4, 5, 6]. a recurrent neural network model was introduced in [7], and made widely avail-
able as the id97 program. it has been shown that continuous space representations learned
by id97 were fairly accurate in capturing certain syntactic and semantic regularities, which
could be revealed by relatively simple vector arithmetic [5]. in one well-known example, mikolov
et al. [5] showed that the vector representation of queen could be inferred by a simple linear com-
bination of the vectors representing king, man, and woman (king - man + woman). however, the
resulting vector might not correspond to vector representation of any of the words in the vocabulary.
cosine similarity was used between the resultant vector and all word vectors to    nd the word in the
voabulary that has maximum similarity with the resultant word. a more comprehensive study by
levy and goldberg [8] showed that a modi   ed similarity metric based on multiplicative combina-
tion of cosine terms resulted in improved performance. a recent study by levy et al. [9] veri   ed the
superiority of the modi   ed similarity metric with several word representations.
in this paper, we introduce a new approach to modeling word vector relationships. at the heart of
our approach is the distinction that we model not just the individual words vectors, but rather the
subspaces formed from groups of related words. for example, in inferring the plurals of words from
their singulars, such as apples from apple, or women from woman, we model the subspaces
of plural words as well as singular words. we exploit well-known mathematical properties of sub-
spaces, including principally the property that the set of k   dimensional subspaces of n-dimensional
euclidean space forms a curved manifold called the grassmannian [10]. it is well-known that the
grassmannian is a quotient subgroup of the lie group of rotations in n-dimensions. we use these
mathematical properties to derive a modi   ed cosine distance, using which we obtain remarkably
improved results in the same word analogy task studied previously [5, 8].

1

recent work has developed ef   cient algorithms for doing id136 on grassmannian manifolds, and
this area has been well explored in id161 [11, 12]. gopalan et al.[11] used the properties
of grassmannian manifolds to perform id20 in image classi   cation by sampling sub-
spaces between the source and target subspace on the geodesic    ow between them. geodesic    ow
is the shortest path between two points on curved manifolds. gong et al. [12] extended this idea by
integrating over all subspaces in the geodesic    ow from source to target subspace by computing the
geodesic flow kernel (gfk).
in this paper, we propose to develop a new approach to computing with word space embeddings
by constructing a distance function based on constructing the geodesic    ow kernel between sub-
spaces de   ned by various groups of words related by different relations, such as past-tense,
plural, capital-of, currency-of, and so on. the intuitive idea is that by explicitly com-
puting shortest-path geodesics between subspaces of word vectors, we can automatically determine
a customized distance function on the grassmannian manifold that speci   cally captures the way dif-
ferent relations map across word vectors, rather than assuming a simple vector translation model as
in past work. as we will see later, the signi   cant error reductions we achieve show that this intuition
appears to be correct.
the major contribution of this paper is the introduction of grassmannian manifold based approach
for reasoning in id27s. even though this has been previously applied in image classi   -
cation (a vision task), we demonstrate their success in learning analogies (an nlp task). this opens
up several interesting questions for further research which we will describe at the end of the paper.
here is a roadmap to the rest of the paper. we begin in section 2 with a brief review of continuous
space vector models of words. section 3 describes the analogical reasoning task. in section 4, we
describe the proposed approach for learning relations using matrix manifolds. section 5 describes
the experimental results in detail, comparing our approach with previous methods. section 6 con-
cludes the paper by discussing directions for further research.

2 vector space word models

continuous vector-space word models have a long and distinguished history [3, 2, 1, 4]. in recent
years, with the popularity of so-called    deep    learning    methods [13], the use of feedforward and
recurrent neural networks in learning continuous vector-space word models has increased. the work
of mikolov et al. [5, 6, 7] has done much to popularize this problem, and their id97 program
has been used quite widely in a number of related studies [8, 9]. recently, levy et al., [9], through a
series of experiments, showed that traditional count based methods for word representation are not
inferior to these neural based word representation algorithms.
in this paper, we consider two word representation learning algorithms: skip grams with negative
sampling (sgns) [6] and positive pointwise mutual information (ppmi) with svd approximation.
sgns is a neural based algorithm while ppmi is a count based algorithm. in the pointwise mutual
information (pmi) based approach, words are represented by a sparse matrix m, where the rows
corresponds to words in the vocabulary and the columns corresponds to the context. each entry in
the matrix corresponds to pmi between the word and the context. we use positive pmi (ppmi)
where all the negative values in the matrix are replaced by 0. ppmi matrices are sparse and high
dimensional. so we do truncated svd to come up with dense vector representation of ppmi which
is low dimensional. levy and goldberg [14] showed that sgns is implicitly factorizing a word
context matrix whose cell   s values are pmi, shifted by some global context.

3 analogical reasoning task

in the classic word analogy task studied in [5, 8], we are given two pairs of words that share a
relation, such as man:woman and king:queen, or run:running and walk:walking. typ-
ically, the identity of the fourth word is hidden, and we are required to infer it from the three given
words. assuming the problem is abstractly represented as a is to b as x is to y, we are required to
infer y given the known identities of a, b, and x.

2

mikolov et al. [5] proposed using a simple cosine similarity measure, whereby the missing word y
was    lled in by solving the optimization problem

(1)
where   i is the vector space d-dimensonal embedding of word i and    is the cosine similarity given
by

argmaxy   v   (  y,   x       a +   b)

  (i, j) =

  t

i   j

(cid:107)  i(cid:107)2(cid:107)  j(cid:107)2

(2)

let us call this method as cosadd. levy and goldberg [8] proposed an alternative similarity mea-
sure using the same cosine similarity as equation 2, but where the terms are used multiplicatively
rather than additively as in equation 1. speci   cally, they proposed using the following multiplicative
distance measure:

argmaxy   v

  (y, b)  (y, x)

  (y, a) +  

(3)

where   is some small constant (such as   = 0.001 in our experiments). let us call this method as
cosmul.
our original motivation for this work stemmed from noticing that the simple vector arithmetic ap-
proach described in earlier work appeared to work well for some relations, but rather poorly for
others. this suggested that the underlying space of vectors in the subspaces spanned by words that
   ll in x vs. y were rather non-homogeneous, and a simple universal rule such as vector subtraction
or addition that did not take into account the speci   c relationship would do less well than one that
exploited the knowledge of the speci   c relationship. of course, such an approach is only pragmatic
if the modi   ed distance measure could somehow be automatically learned from training samples. in
the next section, we propose one such approach.

4 reasoning on grassmannian manifolds

figure 1: this    gure illustrates the key idea underlying our subspace-based approach. a group of
related word vectors are combined into a low-dimensional subspace, visualized by a small circle
above, which represents a single point on the grassmannian manifold. the shortest-path geodesic
distances between subspaces are explicitly computed to generate a customized distance function for
each relation. this    gure illustrates the geodesic between countries and their currencies.

our approach builds on the key insight of explicitly representing the subspaces spanned by related
groups of word vectors (see figure 1). given word vectors are embedded in an ambient euclidean
space of dimension d, we construct a low-dimensional representation of subspaces of size d (cid:28) d,
each representing groups of vectors. given analogy tasks of the form a is to b as x is to
y, we construct subspaces from the list of sample training words comprising the categories de   ned
by a and b. for example, in the case of plurals, a sample word in the category a is woman, and a
sample word in the category b is women. we use principal components analysis (pca) to compute
low-dimensional subspaces of size d, although any id84 method could be used.
many of the methods for constructing low-dimensional representations, from classic methods such

3

as pca [15] to modern methods such as laplacian eigenmaps [16], search for orthogonal subspaces
of dimension d (cid:28) d, the ambient dimension in which the dataset reside. a fundamental property
of the set of all d-dimensional orthogonal subspaces in rd is that they form a manifold, where each
point is itself a subspace (see figure 1).
now, we need to compute the geodesic    ow kernel which integrates over the geodesic    ow between
head subspace and tail subspace, so that we can project the id27s onto this relation
speci   c kernel space. to compute the geodesic    ow kernel, we need to compute the shortest path
geodesic between two points on the grassmannian manifold. in our setting, this corresponds to
computing the shortest path geodesic between the points in the manifold which corresponds to the
head subspace and the tail subspace.
let the size of the id27s be d. let h denotes id27 matrix where each
row corresponds to id27 of corresponding word in a (head of analogy example) and t
denotes the id27 matrix where each row corresponds to id27 of correspond-
ing word in b (tail of analogy example). now we learn d-dimensional subspaces for both h and
t . let ph , pt     rd  d denote the two sets of basis vectors that span the subspaces for the    head   
and     tail    for a relation (for example, words and their plurals, or past and present tenses of verbs,
and so on). let rh     rd  (d   d) be the orthogonal complement to the subspace ph, such that
h rh = 0. the geodesic    ow shortest path between two points ph and pt of a grassmannian
p t
lie group can be parameterized by a one parameter exponential    ow   (t) = ph exp(tb)pt such
that   (0) = ph, and   (1) = pt and where b is a skew-symmetric matrix and exp refers to matrix
exponential. for any other point t other than 0 or 1, the    ow   (t) can be computed as:

(4)
where u1     rd  d and u2     r(d   d)  d are orthonormal (length-preserving rotation) matrices that
can be computed by a pair of singular value decompositions (svd) as follows:

  (t) = ph u1  (t)     rh u2  (t),

(5)
the d    d diagonal matrices    and    are particularly important since they represent cos(  i) and
sin(  i), i = 1, . . . , d, where   i are the so-called principal angles between the subspaces ph and
pt .

h pt = u1  v t , rt
p t

h pt =    u2  v t

figure 2: this    gure illustrates the principal angles between two pairs of subspaces involved in
family relationships. on left is a plot of principal angles (ranging from a maximum of 90 degrees
to a minimum of 0 degrees) between subspaces a and x, and on the right is plotted the principal
angles between the subspaces a and b in a training set of analogical family relationships of the form
a is to b as x is to y . the horizontal axis measures the dimension of the induced low-dimensional
subspace.

figure 2 illustrates a pair of subspaces involved in family relationships, and the principal angles be-
tween them. note that the maximum angle between two subspaces is 90 degrees, and the subspaces
get closer as the principal angles get closer to 0. what this intuitively means is that the principal
angles represent the degree of overlap between the subspaces, so that as the corresponding principal

4

(7)

(8)

(9)

vectors are added to each subspace the degree of overlap between the two subspaces increases. as
figure 2 shows, the degree of overlap between the subspaces a and x increases much more quickly
(causing the largest principal angle to shrink to 0) than that between a and b, as we would expect,
because both a and x represent the    head    in a family relationship.
now let us describe how to compute the geodesic    ow kernel gr speci   c to relation r. the basic
idea is as follows. each subspace   (t) along the curved path from the head ph to the tail pt
represents a possible concept that lies    in between    the subspace a and b (for example, a and b
could represent    singular    and    plural    forms of a noun ). to obtain the projection of a word vector
xi on a subspace   (t), we can just compute the dot product   (t)t xi. given two d-dimensional
word vectors xi and xj, we can simultaneously compute their projections on all the subspaces that
lie between the    head    and    tail    subspaces by forming the geodesic    ow kernel [12], de   ned as

(cid:104)zi, zj(cid:105)r =

(  (t)t

rxi)t (  (t)t

rxj)dt = xt

i grxj

(6)

(cid:90) 1

0

the geodesic kernel matrix gr can be computed in closed form from the above matrices computed
previously in equation 5 using singular value decomposition:

(cid:18)   1   2

(cid:19)(cid:18) u t

(cid:19)

1 p t
s
2 rt
s

u t

  2   3
where   i are diagonal matrices whose elements are given by:

gr = ( psu1 rsu2 )

  1i(  3i) = 1 + (   )

sin(2  i)

,   2i =

2  i

cos(2  i)     1

2  i

a more detailed discussion of geodesic    ow kernels can be found in [11, 12], which applies them
to problems in id161. this is the    rst application of these ideas to natural language
processing, to the best of our knowledge.
once we have the relation speci   c gfks computed, now we can perform our analogy task in the
kernel space. the modi   ed cosine distance would be,

  gr (i, k) =

(cid:107)   

  t

i gr  k

gr  i(cid:107)2(cid:107)   

gr  k(cid:107)2

here,   gr de   nes the modi   ed cosine distance between word vectors   i and   k corresponding to
words i and k for relation r using a kernel g, which captures the speci   c way in which the
standard distance between categories must be modi   ed for relation r. unlike the standard cosine
distance, which treats each dimension equivalently, our approach automatically learns to weight the
different dimensions adaptively from training data to customize it to different relations. the kernel
g is a positive de   nite matrix, which is learned from samples of word relationships.
now, similar to cosadd, we can de   ne gfkcosadd,

(10)
where   i is the vector space d-dimensonal embedding of word i and   gr is the modi   ed cosine
similarity given by 9. we can also compute gfkcosmul (cosmul in the kernel space) as:

argmaxy   v   gr (  y,   x       a +   b)

argmaxy   v

  gr (y, b)  gr (y, x)

  gr (y, a) +  

(11)

where   is some small constant.

5 experiments

in this section, we will describe the experimental results on google and msr analogy datasets. we
learn id27s using two different learning algorithms : sgns and svd approximation of
ppmi. we perform the analogy task using four distance metrics: two relation-independent metrics,
cosadd and cosmul, and two relation-speci   c metrics, gfkcosadd and gfkcosmul. our
primary goal is to investigate the potential reduction in error rate when we learn relation speci   c
kernels, as compared to using relation-independent metrics, cosadd and cosmul.

5

5.1 dataset

all word representation learning algorithms were trained on english wikipedia (august 2013
dump), following the preprocessing steps mentioned in [9]. words that appeared less than 100
times in the corpus were ignored. after preprocessing, we ended up with vocabulary of 189,533
terms. for sgns we learn 500 dimensional representations. ppmi learns a sparse high dimensional
representation which is projected to 500 dimensions using truncated svd.
for the analogy task, we used the google and msr datasets. the msr dataset contains 8000
analogy questions. they are broadly classifed as : adjective, noun, and verb based questions. the
google dataset contains 19544 questions. it contains 14 relations. out of vocabulary words were
removed from both datasets.

5.2 experimental setting

for all the three word representation algorithms, we consider two important hyperparameters that
might affect the quality of the representations learnt: window size of the context, and positional
context. we try both narrow and broad windows (2 and 5). when positional context is true, we
consider the position of the context words as well, while we ignore the position when this parameter
is set to false. this results in four possible settings. all the other hyperparameters of these two
algorithms where set to default values as suggested by levy et al. [9].
we report accuracy in google and msr datasets in table 1 and table 2, respectively. the results
are micro-averaged over all relations in the dataset.

con   g
win=2,
pos=true
win=5,
pos=true
win=2,
pos=false
win=5,
pos=false

model cosadd cosmul gfkcosadd gfkcosmul
sgns
svd
sgns
svd
sgns
svd
sgns
svd

62.35%
65.91%
71.70%
74.18%
76.01%
72.45%
84.64%
79.15%

45.15%
43.66%
53.17%
52.14%
49.41%
50.87%
56.14%
60.82%

54.27%
60.05%
62.19%
71.34%
63.21%
65.82%
74.43%
75.14%

57.62%
58.66%
67.68%
62.46%
71.17%
67.11%
81.06%
72.29%

table 1: accuracy obtained by various similarity measures in google dataset. win refers to the
window size. pos is true if position of the context is considered and false otherwise.

con   g
win=2,
pos=true
win=5,
pos=true
win=2,
pos=false
win=5,
pos=false

model cosadd cosmul gfkcosadd gfkcosmul
sgns
svd
sgns
svd
sgns
svd
sgns
svd

66.49%
66.76%
65.38%
59.11%
69.66%
71.42%
70.59%
60.84%
69.87%
72.70%
64.47%
61.99%
76.00%
78.81%
69.92% 62.25%

68.36%
69.00%
73.25%
72.18%
74.52%
66.25%
78.95%
67.05%

59.55%
50.59%
61.39%
53.59%
59.41%
51.68%
64.48%
52.50%

table 2: accuracy obtained by various similarity measures in msr dataset. win refers to the
window size. pos is true if position of the context is considered and false otherwise.

from the tables, it is clear that gfk based similarity measures perform much better than respective
non-gfk based similarity measures in most of the cases. we also report the relation-size accuracy
in both the datasets in table 3. except for captial-world relation (where cosmul performs
better), gfk based approaches perform signi   cantly better than euclidean cosine similarity based
methods.
table 4 and table 5 reports average rank of the of the correct answer in the ordered list of predictions
made by the models. ideally, this should be 1. these tables again demonstrate the superiority of

6

relation
capital-common-countries
capital-world
city-in-state
currency
family (gender in   ections)
gram1-adjective-to-adverb
gram2-opposite
gram3-comparative
gram4-superlative
gram5-present-participle
gram6-nationality-adjective
gram7-past-tense
gram8-plural (nouns)
gram9-pluran-verbs
adjectives
nouns
verbs

google

msr

cosadd cosmul gfkcosadd gfkcosmul
89.52%
51.25%
7.62%
18.57%
69.36%
30.54%
39.40%
73.49%
33.80%
80.01%
92.49%
84.29%
80.03%
82.52%
35.90%
69.91%
81.26%

100%
98.22%
80.43% 72.61%
46.00%
43.12%
33.43%
15.17%
94.26%
81.42%
89.31%
39.91%
75.00%
45.32%
92.71%
88.81%
67.61%
86.17%
99.81%
92.32%
98.93%
95.30%
99.80%
93.79%
98.19%
90.16%
97.81%
91.72%
59.55%
47.19%
84.10%
83.04%
89.03%
91.86%

100%
76.68%
69.59%
27.86%
93.67%
86.18%
73.02%
91.96%
90.43%
99.71%
98.43%
99.29%
97.67%
97.58
60.44%
83.90%
88.86%

table 3: relation wise accuracy in google and msr datasets. representations are learnt using
sgns with win=5 and pos=false. gfk-based methods perform better than their non-gfk based
counterparts in all but one relation type.

gfk based approaches. we can see average rank for gfk based methods are signi   cantly lower
that their non-gfk based counterparts in most of the cases.

con   g
win=2,
pos=true
win=5,
pos=true
win=2,
pos=false
win=5,
pos=false

model cosadd cosmul gfkcosadd gfkcosmul
sgns
svd
sgns
svd
sgns
svd
sgns
svd

149.42
108.53
86.46
64.35
53.19
76.38
28.05
53.00

262.81
332.73
165.69
255.38
110.74
196.47
60.03
116.65

178.46
128.01
116.81
74.71
74.94
98.14
41.61
61.53

214.28
279.41
124.67
225.87
83.36
149.58
39.25
101.03

table 4: average rank obtained by various similarity measures in google dataset. win refers to the
window size. pos is true if position of the context is considered and false otherwise.

con   g
win=2,
pos=true
win=5,
pos=true
win=2,
pos=false
win=5
pos=false

model cosadd cosmul gfkcosadd gfkcosmul
sgns
svd
sgns
svd
sgns
svd
sgns
svd

12.33
12.45
10.60
11.33
8.32
14.38
7.31
11.13

18.14
23.51
13.68
20.90
11.73
19.07
8.17
14.85

16.10
21.84
12.37
22.03
10.45
19.55
8.06
15.88

13.41
15.38
11.26
11.34
8.89
14.29
6.77
9.14

table 5: average rank obtained by various similarity measures in msr dataset. win refers to the
window size. pos is true if position of the context is considered and false otherwise.

an interesting question is how the performance of the gfk based methods varies with the dimen-
sionality of the subspace embedding. all the results in the above tables for our proposed gfk
method are based on reducing the dimensionality of id27 from the original d = 500 to

7

a subspace of dimension d = 40. figure 3 plots the performance of the gfk based methods and the
previous methods on the google dataset and msr dataset, showing how its performance varies as
the dimensionality of the subspace is varied. the best performance for the google dataset is with the
pca subspace dimension d = 60, whereas for the msr dataset, the best performance is achieved
with d = 100. in all these cases, this experiment shows that signi   cant reduction in the original
embedding dimension can be achieved without loss of performance (in fact, with signi   cant gains
in performance).

figure 3: this    gure explores the performance of the proposed gfk based methods on the google
dataset (left) and msr dataset (right), both with varying subspace dimension from d = 20 to d =
200 in (steps of 20) compared to the    xed performance of the non-gfk based methods.

the key difference between our approach and that proposed earlier [5, 8] is the use of a relationship-
speci   c distance metric, which is automatically learned from the given dataset, vs. using a universal
relationship independent rule. clearly, if generic rules performed extremely well across all cat-
egories, there would be no need for a relationship-speci   c method. our approach is speci   cally
designed to address the weaknesses in the    one size    ts all    philosophy underlying the earlier ap-
proaches.

6 future work

relational knowledge base completion: as discussed above, the methods tested are related to
ongoing work on relational knowledge base completion, such as transe [17], transh [18], and
tensor neural net methods [19]. the mathematical framework underlying gfk can be readily ex-
tended to relational knowledge base completion in a number of ways. first, many of these methods,
like transe and transh involve    nding embeddings of entities and relations that are of unit
norm. for example, if a relation is modeled abstractly by a triple (h, l, t), where h is the head of
relation l and t is its tail, then these embedding methods    nd a vector space representation for each
head h and tail t (denoted by   h and   t) such that (cid:107)  h(cid:107)2 = (cid:107)  t(cid:107)2 = 1. the space of unit norm
vectors de   nes a grassmannian manifold, and special types of gradient methods can be developed
that use the riemannian gradient instead of the euclidean gradient to    nd the suitable embedding
on the grassmannian.

choice of kernel: we selected one speci   c kernel based on geodesic    ows in this paper, but in
actuality, a large number of choices for grassmannian kernels are available for study [20]. these
include binet-cauchy metric, projection metric, maximum and minimum correlation metrics, and
related kernels. we are currently exploring several of these alternative choices of grassmannian
kernels for analyzing id27s.

compact kernel representations: to address the issue of scaling our approach to large datasets,
we could exploit the rich theory of representations of lie groups, to exploit more sophisticated
methods for compactly representing and ef   ciently computing with kernels on lie groups.

8

references
[1] g. hinton, d. mcclelland, and d. rumelhart. parallel distributed processing: explorations in
the microstructure of cognition. volume 1: foundations, chapter distributed representations.
mit press, 1986.

[2] j. elman. finding structure in time. cognitive science, pages 179   211, 1990.
[3] y. bengio, r. ducharme, p. vincent, and c jauvin. a neural probabilistic language model.

journal of machine learning research, 3:1137   1155, 2003.

[4] a. mnih and g. hinton. a scalable hierarchical distributed language model. in in proceedings
of the international conference on neural information processing systems (nips). mit press,
2008.

[5] t. mikolov, w. yih, and g. zweig. linguistic regularities in continuous space word representa-
tions. in proceedings of the 2013 conference of the north american chapter of the association
for computational linguistics: human language technologies, pages 746   751, 2013.

[6] t. mikolov, k. sutskever, g. corrado, and j. dean. distributed representations of words and
phrases and their compositionality. in advances in neural information processing systems 26:
27th annual conference on neural information processing systems (nips), pages 3111   3119,
2013.

[7] tomas mikolov, martin kara     at, luk  as burget, jan cernock  y, and sanjeev khudanpur. recur-
rent neural network based language model. in interspeech 2010, 11th annual conference
of the international speech communication association, makuhari, chiba, japan, september
26-30, 2010, pages 1045   1048, 2010.

[8] omer levy and yoav goldberg. linguistic regularities in sparse and explicit word repre-
in proceedings of the eighteenth conference on computational natural lan-
sentations.
guage learning, pages 171   180. association for computational linguistics, 2014. url
http://aclweb.org/anthology/w14-1618.

[9] omer levy and yoav goldberg. improving distributional similarity with lessons learned from

id27s. in transactions of acl. 2015.

[10] a. edelman, t. arias, and t. smith. the geometry of algorithms with orthogonality constraints.

siam journal of matrix analysis and applications, 20(2):303   353, 1998.

[11] r. gopalan, r. li, and r. chellappa. unsupervised adaptation across domain shifts by gener-

ating intermediate data representations. ieee pami, 12, 2013. to appear.

[12] b. gong, y. shi, f. sha, and k. grumman. geodesic    ow kernel for unsupervised domain

adaptation. ieee cvpr, 2012.

[13] g. hinton and r salakhutdinov. reducing the dimensionality of data with neural networks.

science, 313:504   507, 2006.

[14] omer levy and yoav goldberg. neural id27 as implicit id105. in
z. ghahramani, m. welling, c. cortes, n.d. lawrence, and k.q. weinberger, editors, ad-
vances in neural information processing systems 27, pages 2177   2185. 2014.

[15] t. jolliffe. principal components analysis. springer-verlag, 1986.
[16] m. belkin and p. niyogi. semi-supervised learning on riemannian manifolds. machine learn-

ing, 56:209   239, 2004.

[17] a. bordes, j. weston, r. collobert, and y. bengio. learning structured embeddings of knowl-

edge bases. in proceedings of aaai, 2011.

[18] z. wang, j. zhang, j. feng, and z. chen. id13 embedding by translating on

hyperplanes. in proceedings of aaai, 2014.

[19] richard socher, danqi chen, christopher d. manning, and andrew y. ng. reasoning with
tensor neural networks for knowledge base completion. in proceedings of the neural informa-
tion processing systems (nips) conference, 2013.

[20] j. hamm and d. lee. grassmannian discriminant analysis:a unifying view of subspace-based
learning. in proceedings of the 25th international conference on machine learning, icml    08,
new york, ny, usa, 2008. acm.

9

