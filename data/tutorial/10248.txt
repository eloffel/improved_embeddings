journal of machine learning research 14 (2013) 1801-1835

submitted 9/12; revised 4/13; published 7/13

random walk kernels and learning curves for gaussian process

regression on random graphs

matthew j. urry
peter sollich
department of mathematics

king   s college london

london, wc2r 2ls, u.k.

editor: manfred opper

matthew.urry@kcl.ac.uk

peter.sollich@kcl.ac.uk

abstract

we consider learning on graphs, guided by kernels that encode similarity between vertices. our fo-
cus is on random walk kernels, the analogues of squared exponential kernels in euclidean spaces.
we show that on large, locally treelike graphs these have some counter-intuitive properties, specif-
ically in the limit of large kernel lengthscales. we consider using these kernels as covariance func-
tions of gaussian processes. in this situation one typically scales the prior globally to normalise the
average of the prior variance across vertices. we demonstrate that, in contrast to the euclidean case,
this generically leads to signi   cant variation in the prior variance across vertices, which is undesir-
able from a probabilistic modelling point of view. we suggest the random walk kernel should be
normalised locally, so that each vertex has the same prior variance, and analyse the consequences of
this by studying learning curves for gaussian process regression. numerical calculations as well as
novel theoretical predictions for the learning curves using belief propagation show that one obtains
distinctly different probabilistic models depending on the choice of normalisation. our method for
predicting the learning curves using belief propagation is signi   cantly more accurate than previous
approximations and should become exact in the limit of large random graphs.

keywords: gaussian process, generalisation error, learning curve, cavity method, belief propaga-
tion, graph, random walk kernel

1. introduction

gaussian processes (gps) have become a workhorse for probabilistic id136 that has been de-
veloped in a wide range of research    elds under various guises (see for example kleijnen, 2009;
handcock and stein, 1993; neal, 1996; meinhold and singpurwalla, 1983). their success and wide
adoption can be attributed mainly to their intuitive nature and ease of use. they owe their intu-
itiveness to being one of a large family of kernel methods that implicitly map lower dimensional
spaces with non-linear relationships to higher dimensional spaces where (hopefully) relationships
are linear. this feat is achieved by using a kernel, which also encodes the types of functions that the
gp prefers a priori. the ease of use of gps is due to the simplicity of implementation, at least in the
basic setting, where prior and posterior distributions are both gaussian and can be written explicitly.
an important question for any machine learning method is how    quickly    the method can gen-
eralise its prediction of a rule to the entire domain of the rule (i.e., how many examples are required
to achieve a particular generalisation error). this is encapsulated in the learning curve, which traces
average error versus number of examples. learning curves have been studied for a variety of infer-

c(cid:13)2013 matthew j. urry and peter sollich.

urry and sollich

ence methods and are well understood for parametric models (seung et al., 1992; amari et al., 1992;
watkin et al., 1993; opper and haussler, 1995; haussler et al., 1996; freeman and saad, 1997) but
rather less is known for non-parametric models such as gps.
in the case of gp regression, re-
search has predominantly focused on leaning curves for input data from euclidean spaces (sollich,
1999a,b; opper and vivarelli, 1999; williams and vivarelli, 2000; malzahn and opper, 2003; sol-
lich, 2002; sollich and halees, 2002; sollich and williams, 2005), but there are many domains for
which the input data has a discrete structure. one of the simplest cases is the one where inputs are
vertices on a graph, with connections on the graph encoding similarity relations between different
inputs. examples could include the internet, social networks, protein networks and    nancial mar-
kets. such discrete input spaces with graph structure are becoming more important, and therefore
so is an understanding of gps, and machine learning techniques in general, on these spaces.

in this paper we expand on earlier work in sollich et al. (2009) and urry and sollich (2010)
and focus on predicting the learning curves of gps used for regression (where outputs are from the
whole real line) on large sparse graphs, using the random walk kernel (kondor and lafferty, 2002;
smola and kondor, 2003).

the rest of this paper will be structured as follows. in section 2 we begin by analysing the
random walk kernel, in particular with regard to the dependence on its lengthscale parameter, and
study the approach to the fully correlated limit. with a better understanding of the random walk
kernel in hand, we proceed in section 3 to an analysis of the use of the random walk kernel for
gp regression on graphs. we begin in section 3.2 by looking at how kernel normalisation affects
the prior id203 over functions. we show that the more frequently used global normalisation
of a kernel by its average prior variance is inappropriate for the highly location dependent random
walk kernel, and suggest normalisation to uniform local prior variance as a remedy. to understand
how this affects gp regression using random walk kernels quantitatively, we extend    rst in section
3.4 an existing approximation to the learning curve in terms of kernel eigenvalues (sollich, 1999a;
opper and malzahn, 2002) to the discrete input case, allowing for arbitrary normalisation. this
approximation turns out to be accurate only in the initial and asymptotic regimes of the learning
curve.

the core of our analysis begins in section 4 with the development of an improved learning
curve approximation based on belief propagation. we    rst apply this, in section 4.1, to the case
of globally normalised kernels as originally proposed. the belief propagation analysis for global
normalisation also acts as a useful warm-up for the extension to the prediction of learning curves for
the locally normalised kernel setting, which we present in section 4.3. in both sections we compare
our predictions to numerical simulations,    nding good agreement that improves signi   cantly on the
eigenvalue approximation. finally, to emphasise the distinction between the use of globally and
locally normalised kernels in gp regression, we study qualitatively the case of model mismatch,
with a gp with a globally normalised kernel as the teacher and a gp with a locally normalised
kernel as the student, or visa versa. the resulting learning curves show that the priors arising from
the two different normalisations are fundamentally different; the learning curve can become non-
monotonic and develop a maximum as a result of the mismatch. we conclude in section 6 by
summarising our results and discussing further potentially fruitful avenues of research.

1802

gaussian processes on random graphs

1.1 main results

in this paper we will derive three key results; that normalisation of a kernel by its average prior vari-
ance leads to a complicated relationship between the prior variances and the local graph structure;

that by    xing the scale to be equal everywhere using a local prescription ci j =   ci j/q   cii   c j j results

in a fundamentally different probabilistic model; and that we can derive accurate predictions of the
learning curves of gaussian processes on graphs with a random walk kernel for both normalisations
over a broad range of graphs and parameters. the last result is surprising since in continuous spaces
this is only possible for a few very restrictive cases.

2. the random walk kernel

a wide range of machine learning techniques like gaussian processes capture prior correlations
between points in an input space by mapping to a higher dimensional space, where correlations
can be represented by a linear combination of    features    (see, e.g., rasmussen and williams, 2005;
m  uller et al., 2001; cristianini and shawe-taylor, 2000). direct calculation of correlations in this
high dimensional space is avoided using the    kernel trick   , where the id81 implicitly
calculates inner products in feature space. the widespread use of, and therefore extensive research
in, kernel based machine learning has resulted in kernels being developed for a wide range of input
spaces (see genton, 2002, and references therein). we focus in this paper on the class of kernels
introduced in kondor and lafferty (2002). these make use of the normalised graph laplacian to
de   ne correlations between vertices of a graph.

we denote a generic graph by g(v , e ) with a vertex set v = {1, . . . ,v} and edge set e . we
encode the connection structure of g using an adjacency matrix a, where ai j = 1 if vertex i is
connected to j, and 0 otherwise; we exclude self-loops so that aii = 0. we denote the number of
edges connected to vertex i, known as the degree, by di =     j ai j and de   ne the degree matrix d as
a diagonal matrix of the vertex degrees, that is, di j = di  i j. the class of kernels created in kondor
and lafferty (2002) is constructed using the normalised laplacian, l = i     d   1/2ad   1/2 (see

chung, 1996) as a replacement for the laplacian in continuous spaces. of particular interest is the
diffusion kernel and its easier to calculate approximation, the random walk kernel. both of these
kernels can be viewed as an approximation to the ubiquitous squared exponential kernel that is used
in continuous spaces. the direct graph equivalent of the squared exponential kernel is given by the
diffusion kernel (kondor and lafferty, 2002). it is de   ned as

c = exp(cid:18)   

1

2

  2l(cid:19) ,    > 0,

(1)

where    sets the length-scale of the kernel. unlike in continuous spaces, the exponential in the
diffusion kernel is costly to calculate. to avoid this, smola and kondor (2003) proposed as a
cheaper approximation the random walk kernel

c =(cid:0)i     a   1l(cid:1)p

=(cid:16)(1    a   1)i + a   1d   1/2ad   1/2(cid:17)p

,

a > 2,

p     n.

(2)

this gives back the diffusion kernel in the limit a, p         whilst keeping p/a =   2/2    xed. the

random walk kernel derives its name from its use of id93 to express correlations between

1803

urry and sollich

vertices. explicitly, a binomial expansion of equation (2) gives

c =

p

   

q=0(cid:18)p

= d   1/2

q(cid:19)(1    a   1)p   q(a   1)q(d   1/2ad   1/2)q
q=0(cid:18)p

q(cid:19)(1    a   1)p   q(a   1)q(ad   1)qd1/2.

   

p

(3)

the matrix ad   1 is a random walk transition matrix: (ad   1)i j is the id203 of being at vertex
i after one random walk step starting from vertex j. apart from the pre- and post-multiplication by
d   1/2 and d1/2, the kernel c is therefore a q-step random walk transition matrix, averaged over the
number of steps q distributed as q     binomial(p, a   1). equivalently one can interpret the random
with id203 (1    a   1) and moves to a neighbouring vertex with id203 a   1.

walk kernel as a p-step lazy random walk, where at each step the walker stays at the current vertex

using either interpretation, one sees that p/a is the lengthscale over which the random walk
can diffuse along the graph, and hence the lengthscale describing the typical maximum range of the
random walk kernel. in the limit of large p, where this lengthscale diverges, the kernel should rep-
resent full correlation across all vertices. one can see that this is the case by observing that for large
p, a random walk on a graph will approach its stationary distribution, p        de, e = (1, . . . , 1)t .
the q-step transition matrix for large q is therefore (ad   1)q     p   et = deet , representing the
p        , the kernel c     d1/2eet d1/2, that is, ci j     d
. this corresponds to full correla-
tion across vertices as expected; explicitly, if f is a gaussian process on the graph with covariance
matrix d1/2eet d1/2, then f = vd1/2e with v a single gaussian degree of freedom.

fact that the random walk becomes stationary independently of the starting vertex. this gives, for

1/2
i d

1/2
j

we next consider how random walk kernels on graphs approach the fully correlated case, and
show that even for    simple    graphs the convergence to this limit is non-trivial. before we do so, we
note an additional peculiarity of random walk kernels compared to their euclidean counterparts: in
addition to the maximum range lengthscale p/a discussed so far, they have a diffusive lengthscale
   = (2p/a)1/2, which is suggested for large p and a by the lengthscale of the corresponding diffu-
sion kernel (1). this diffusive lengthscale will appear in our analysis of learning curves in the large
p-limit section 3.4.1.

2.1 the d-regular tree: a concrete example

to begin our discussion of the dependence of the random walk kernel on the lengthscale p/a, we
   rst look at how this kernel behaves on a d-regular graph sampled uniformly from the set of all
d-regular graphs. here d-regular means that all vertices have degree di = d. for a large enough
number of vertices v , typical cycles in such a d-regular graph are also large, of length o(logv ),
and can be neglected for calculation of the kernel when v        . we therefore begin by assuming
the graph is an in   nite tree, and assess later how the cycles that do exist on random d-regular graphs
cause departures from this picture.

a d-regular tree is a graph where each vertex has degree d with no cycles; it is unique up to
permutations of the vertices. since all vertices on the tree are equivalent, the random walk kernel
ci j can only depend on the distance between vertices i and j, that is, the smallest number of steps on
the graph required to get from one vertex to the other. denoting the value of a p-step lazy random
walk kernel for vertices a distance l apart by cl,p, we can determine these values by recursion over

1804

gaussian processes on random graphs

p as follows:

cl,p=0 =   l,0,

  p+1cl,p+1 =

1

ad

  p+1c0,p+1 =(cid:18)1   
a(cid:19)cl,p +

cl   1,p +(cid:18)1   

1

1

a(cid:19)c0,p +
d     1
ad

cl+1,p

1

a

c1,p,

l     1.

(4)

here   p is chosen to achieve the desired normalisation of the prior variance for every p. we will
normalise so that c0,p = 1.

figure 1 (left) shows the results obtained by iterating equation (4) numerically for a 3-regular
tree with a = 2. as expected the kernel becomes longer-ranged initially as p is increased, but seems
to approach a non-trivial limiting form. this can be calculated analytically and is given by (see
appendix a)

cl,p       =(cid:20)1 +

l(d     2)

d

(cid:21)

1

(d     1)l/2

.

(5)

equation (5) can be derived by taking the   2         limit of the integral expression for the diffusion

kernel from chung and yau (1999) whilst preserving normalisation of the kernel (see appendix
a.1 for further details). alternatively the result (5) can be obtained by rewriting the random walk
in terms of shells, that is, grouping vertices according to distance l from a chosen central vertex.

the number of vertices in the l-th shell, or shell volume, is vl = d(d     1)l   1 for l     1 and v0 = 1.
introducing rl,p = cl,p   vl, equation (4) can be written in the form
a(cid:19) r0,p +
   d     1

  p+1r0,p+1 =(cid:18)1   
a(cid:19) rl,p +
rl   1,p +(cid:18)1   

   d     1

rl,p=0 =   l,0,

  p+1rl,p+1 =

1
a   d

l     1.

rl+1,p

r1,p,

(6)

ad

ad

1

1

this is just the un-normalised diffusion equation for a biased random walk on a one dimensional
lattice with a re   ective boundary at 0. this has been solved in monthus and texier (1996), and
mapping this solution back to cl,p gives (5) (see appendix a.2 for further details).

to summarise thus far, the analysis on a d-regular tree shows that, for large p, the random walk
kernel does not approach the expected fully correlated limit: because all vertices have the same
degree this limit would correspond to cl,p       = 1. on the other hand, on a d-regular graph with any
   nite number v of vertices, the fully correlated limit must necessarily be approached as p        . as
a large regular graph is locally treelike, the difference must arise from the existence of long cycles
in a regular graph.

to estimate when the existence of cycles will start to affect the kernel, consider    rst a d-regular

tree truncated at depth l. this will have v = 1 +    l
d-regular graph with the same number of vertices, we therefore expect to encounter cycles after a
number of steps, taken along the graph, of order l. in the random walk kernel the typical number of
steps is p/a, so effects of cycles should appear once p/a becomes larger than

i=1 d(d     1)i   1 = o(d(d     1)l   1) vertices. on a

p
a    

log(v )
log(d     1)

.

(7)

figure 1 (right) shows a comparison between c1,p as calculated from equation (4) for a 3-regular
tree and its analogue on random 3-regular graphs of    nite size, which we call k1,p. we de   ne this

1805

urry and sollich

1

0.9

0.8

0.7

p
,
1
k

0.6

0.5

0.4

0.3

0.2

0.1

p = 1
p = 2
p = 3
p = 4
p = 5
p = 10
p = 20
p = 50
p = 100
p = 200
p = 500
p =    

1

0.8

0.6

p
,
l

c

0.4

0.2

0

log(v )/ log(d     1)
a = 2, v =    
a = 2, v = 500
a = 4, v =    
a = 4, v = 500

0

2

4

6

8

10 12 14

1

l

10
p/a

100

1000

figure 1: (left) random walk kernel cl,p on a 3-regular tree plotted against distance l for increasing
number of steps p and a = 2.
(right) comparison between numerical results for the
average nearest neighbour kernel k1,p on random 3-regular graphs with the result c1,p on
a 3-regular tree, calculated numerically by iteration of (4).

analogue as the average of ci j/pciic j j over all pairs of neighbouring vertices on a    xed graph,

averaged further over a number of randomly generated regular graphs. the square root accounts
for the fact that local kernel values cii can vary slightly on a regular graph because of cycles, while
they are the same for all vertices of a regular tree. looking at figure 1 (right) one sees that, as
expected from the arguments above, the nearest neighbour kernel value for the 3-regular graph,
k1,p, coincides with its analogue c1,p on the 3-regular tree for small p. when p/a crosses the
threshold (7), cycles in the regular graph become important and the two curves separate. for larger
p, the kernel value for neighbouring vertices approaches the fully correlated limit k1,p     1 on a
regular graph, while on a regular tree one has the non-trivial limit c1,p     2   d     1/d from (5).

in conclusion of our analysis of random walk kernels, we have seen that these kernels have
an unusual dependence on their lengthscale p/a. in particular, kernel values for vertices a short
distance apart can remain signi   cantly below the fully correlated limit, even if p/a is large. that
limit is approached only once p/a becomes larger than the graph size-dependent threshold (7), at
which point cycles become important. we have focused here on random regular graphs, but the
same qualitative behaviour should be observed also on graphs with a non-trivial distribution of
vertex degrees di.

3. learning curves for gaussian process regression

having reached a better understanding of the random walk kernel we now study its application
in machine learning. in particular we focus on the use of the random walk kernel for regression
with gaussian processes. we will begin, for completeness, with an introduction to gps for regres-

1806

gaussian processes on random graphs

sion. for a more comprehensive discussion of gps for machine learning we direct the reader to
rasmussen and williams (2005).

3.1 gaussian process regression: kernels as covariance functions

gaussian process regression is a bayesian id136 technique that constructs a posterior distribution

over a function space, p( f|x, y), given training input locations x = (x1, . . . , xn)t and corresponding
function value outputs y = (y1, . . . , yn)t. the posterior is constructed from a prior distribution p( f )
over the function space and the likelihood p(y| f , x) to generate the observed output values from
function f by using bayes    theorem

p( f|x, y) =

p(y| f , x)p( f )

r d f    p(y| f    , x)p( f    )

.

in the gp setting the prior is chosen to be a gaussian process, where any    nite number of function
values has a joint gaussian distribution, with a covariance matrix with entries given by a covariance
function or kernel c(x, x   ) and with a mean vector with entries given by a mean function   (x). for
simplicity we will focus on zero mean gps1 and a gaussian likelihood, which amounts to assuming
that training outputs are corrupted by independent and identically distributed gaussian noise. under
these assumptions all distributions are gaussian and can be calculated explicitly. if we assume we
are given training data {(x  , y  )|   = 1, . . . , n} where y   is the value of the target or    teacher    function
at input location x  , corrupted by additive gaussian noise with variance   2, the posterior distribution
is then given by another gaussian process with mean and covariance functions

  f (x) = k(x)tk   1y,

cov(x, x   ) = c(x, x   )    k(x)tk   1k(x   ),

(8)

(9)

where k(x) = (c(x1, x), . . . ,c(xn, x))t and k     = c(x  , x  ) +         2. with the posterior in the form
of a gaussian process, predictions are simple. assuming a squared id168, the optimal pre-
diction of the outputs is given by   f (x) and a measure of uncertainty in the prediction is provided by
cov(x, x)1/2.

equations (8) and (9) illustrate that, in the setting of gp regression, kernels are used to change
the type of function preferred by the gaussian process prior, and correspondingly the posterior. the
kernel can encode prior beliefs about smoothness properties, lengthscale and expected amplitude of
the function we are trying to predict. of particular importance for the discussion below, c(x, x) gives
the prior variance of the function f at input x, so that c(x, x)1/2 sets the typical function amplitude
or scale.

3.2 kernel normalisation

conventionally one    xes the desired scale of the kernel using a global normalisation: denoting
the unnormalised kernel by   c(x, x   ) one scales c(x, x   ) =   c(x, x   )/   to achieve a desired average
of c(x, x) across input locations x. in euclidean spaces one typically uses translationally invariant
kernels like the squared exponential kernel. for these, c(x, x) is the same for all input locations x
and so global normalisation is suf   cient to    x a spatially uniform scale for the prior amplitude. in
the case of kernels on graphs, on the other hand, the local connectivity structure around each vertex

1. in the discussion and analysis that follows, generalisation to non-zero mean gps is straightforward.

1807

urry and sollich

can be different. since information about correlations    propagates    only along graph edges, graph
kernels are not generally translation invariant. in particular, there can be large variation among the
prior variances at different vertices. this is usually undesirable in a probabilistic model, unless one
has strong prior knowledge to justify such variation. for the random walk kernel, the local prior
variances are the diagonal entries of equation (3). these are directly related to the id203 of
return of a lazy random walk on a graph, which depends sensitively on the local graph structure.
this dependence is in general non-trivial, and not just expressible through, for example, the degree
of the local vertex. it seems dif   cult to imagine a scenario where such a link between prior variances
and local graph structures could be justi   ed by prior knowledge.

to emphasise the issue, figure 2 shows examples of distributions of local prior variances cii
for random walk kernels globally normalised to an average prior variance of unity.2 the distribu-
tions are peaked around the desired value of unity but contain many    outliers    from vertices with
abnormally low or high prior variance. figure 2 (left) shows the distribution of cii for a large
single instance of an erd  os-r  enyi random graph (erd  os and r  enyi, 1959). in such graphs, each
edge is present independently of all others with some    xed id203, giving a poisson distribu-

tion of degrees p  (d) =   d exp(     )/d!; for the    gure we chose average degree    = 3. figure 2

(right) shows analogous results for a generalised random graph with power law mixing distribution
(britton et al., 2006). generalised random graphs are an extension of erd  os-r  enyi random graphs
where different edges are assigned different probabilities of being present. by appropriate choice
of these probabilities (britton et al., 2006), one can generate a degree distribution that is a superpo-

sition of poisson distributions, p(d) = r d   p  (d)p(  ). we have taken a shifted pareto distribution,
p(  ) =       
m/    +1 with exponent    = 2.5 and lower cutoff   m = 2 for the distribution of the means.
looking    rst at figure 2 (left), we know that large erd  os-r  enyi graphs are locally tree-like
and hence one might expect that this would lead to relatively uniform local prior variances. as
shown in the    gure, however, even for such tree-like graphs large variations can exist in the lo-
cal prior variances. to give some speci   c examples, the large spike near 0 is caused by single
disconnected vertices and the smaller spike at around 6.8 arises from two-vertex (single edge) dis-
connected subgraphs. single vertex subgraphs have an atypically small prior variance since, for a

single disconnected vertex i, before normalisation cii = (1    a   1)p which is the q = 0 contribution
from equation (3). other vertices in the graph will get additional contributions from q     1 and so
have a larger prior variance. this effect will become more pronounced as p is increased and the
binomial weights assign less weight to the q = 0 term.

q=0 (cid:0) p

2q(cid:1)a   2q(1    a   1)p   2q, which is an atypically large return id203: after any even

somewhat surprisingly at    rst sight, the opposite effect is seen for two-vertex disconnected
subgraphs as shown by the spike around cii = 6.8 in figure 2 (left). for vertices on such subgraphs,
cii =       p/2   
number of steps, the walker must always return to its starting vertex. a similar situation would
occur on vertices at the centre of a star. this illustrates that local properties of a vertex alone, like
its degree, do not constrain the prior variance. in a two-vertex disconnected subgraph both vertices
have degree 1. but there will generically be other vertices of degree 1 that are dangling ends of a
large connected graph component, and these will not have similarly elevated return probabilities.
thus, local graph structure is intertwined in a complex manner with local prior variance.

the black line in figure 2 (left) shows theoretical predictions (see section 4.2) for the prior
variance distribution in the large graph limit. there is signi   cant    ne structure in the various peaks,

2. we use cii again here, instead of c(i, i) as in our general discussion of gps; the subscript notation is more intuitive

because the covariance function on a graph is just a v    v matrix.

1808

gaussian processes on random graphs

on which theory and simulations agree well where the latter give reliable statistics. the decay from
the mean is roughly exponential (see linear-log plot in inset), emphasizing that the distribution of
local prior variances is not only rather broad but can also have large tails.

for the power law random graph, figure 2 (right), the broad features of the distribution of local
prior variances cii are similar: a peak at the desired value of unity, overlaid by spikes which again
come from single and two-vertex disconnected subgraphs. the inset shows that the tail beyond the
mean is roughly exponential again, but with a slower decay; this is to be expected since power law
graphs exhibit many more different local structures with a signi   cantly larger id203 than is
the case for erd  os-r  enyi graphs. accordingly, the distribution of the cii also has a larger standard
deviation than for the erd  os-r  enyi case. the maximum values of cii that we see in these two speci   c
graph instances follow the same trend, with maxi cii     40 for the power law graph and maxi cii     15
for the erd  os-r  enyi graph. such large values would constitute rather unrealistic prior assumptions
about the scaling of the target function at these vertices.

to summarise, figure 2 shows that after global normalisation a random walk kernel can retain a
large spread in the local prior variances, with the latter depending on the graph structure in a compli-
cated manner. we propose that to overcome this one should use a local normalisation. for a desired
prior variance c this means normalising according to ci j = c   ci j/(  i   j)1/2 with local normalisation
constants   i =   cii; here   ci j is the unnormalised kernel matrix as before. this guarantees that all
vertices have exactly equal prior variance as in the euclidean case, that is, all vertices have a prior
variance of c. no uncontrolled local variation in the scaling of the function prior then remains, and
the computational overhead of local over global normalisation is negligible. graphically, if we were
to normalise the kernel to unity according to the local prescription, a plot of prior variances like the
one in figure 2 would be a delta peak centred at 1.

the effect of this normalisation on the behaviour of gp regression is a key question for the
remainder of this paper; numerical simulation results are shown in section 3.3 below, while our
theoretical analysis is described in section 4.

3.3 predicting the learning curve

the performance of non-parametric methods such as gps can be characterised by studying the
learning curve,

  (n) =**** 1

v

v

   

i=1(cid:0)gi    h fiif|x,y(cid:1)2+y|g,x+g+x+g

,

de   ned as the average squared error between the student and teacher   s predictions f = ( f1, . . . , fv )t
and g = (g1, . . . , gv )t respectively, averaged over the student   s posterior distribution given the data
f|x, y, the outputs given the teacher y|g, x, the teacher functions g, and the input locations x.
this gives the average generalisation error as a function of the number of training examples. for
simplicity we will assume that the input distribution is uniform across the vertices of the graph.

because we are analysing gp regression on graphs, after the averages discussed so far the gen-
eralisation error will still depend on the structure of the speci   c graph considered. we therefore
include an additional average, over all graphs in a random graph ensemble g . we consider graph
ensembles de   ned by the distribution of degrees di: we specify a degree sequence {d1, . . . , dv}, or,
for large v , equivalently a degree distribution p(d), and pick uniformly at random any one of the
graphs that has this degree distribution. the actual shape of the degree distribution is left arbitrary,

1809

urry and sollich

3.5

3

2.5

)
i
i

c
(
p

2

1.5

2

3

4

1

0.5

0

100
10   1
10   2
10   3
10   4

)
i
i

c
(
p

3.5

3

2.5

2

1.5

1

0.5

0

100
10   1
10   2
10   3
10   4

1

2

3

4

0

1

2

3

4
cii

5

6

7

8

9

0

1

2

3

5

6

7

8

9

4
cii

figure 2: (left) grey: histogram of prior variances for the globally normalised random walk kernel
with a = 2, p = 10 on a single instance of an erd  os-r  enyi graph with mean degree    = 3
and v = 10000 vertices. black: prediction for this distribution in the large graph limit
(see section 4.2). inset: linear-log plot of the tail of the distribution. (right) as (left)
but for a power law generalised random graph with exponent 2.5 and cutoff 2.

as long as it has    nite mean. our analysis therefore has broad applicability, including in particu-
lar the graph types already mentioned above (d-regular graphs, where p(d   ) =   dd    , erd  os-r  enyi
graphs, power law generalised random graphs).

for this paper, as is typical for learning curve studies, we will assume that teacher and stu-
dent have the same prior distribution over functions, and likewise that the assumed gaussian noise
of variance   2 re   ects the actual noise process corrupting the training data. this is known as the
matched case.3 under this assumption the generalisation error becomes the bayes error, which
given that we are considering squared error simpli   es to the posterior variance of the student aver-
aged over data sets and graphs (rasmussen and williams, 2005). since we only need the posterior
variance we shift f so that the posterior mean is 0; fi is then just the deviation of the function value
at vertex i from the posterior mean. the bayes error can now be written as

  (n) =*** 1

v

v

   
i=1

f 2

i +f|x+x+g

.

(10)

note that by shifting the posterior distribution to zero mean, we have eliminated the dependence
on y in the above equation. that this should be so can also be seen from (9) for the posterior
(co-)variance, which only depends on training inputs x but not the corresponding outputs y.

3. the case of mismatch has been considered in malzahn and opper (2005) for    xed teacher functions, and for prior

and noise level mismatch in sollich (2002); sollich and williams (2005).

1810

gaussian processes on random graphs

the averages in equation (10) are in general dif   cult to calculate analytically, because the train-
ing input locations x enter in a highly nonlinear matter, see (9); only for very speci   c situations
can exact results be obtained (malzahn and opper, 2005; rasmussen and williams, 2005). approx-
imate learning curve predictions have been derived, for euclidean input spaces, with some degree
of success (sollich, 1999a,b; opper and vivarelli, 1999; williams and vivarelli, 2000; malzahn and
opper, 2003; sollich, 2002; sollich and halees, 2002; sollich and williams, 2005). we will show
that in the case of gp regression for functions de   ned on graphs, learning curves can be predicted
exactly in the limit of large random graphs. this prediction is broadly applicable because the degree
distribution that speci   es the graph ensemble is essentially arbitrary.

it is instructive to begin our analysis by extending a previous approximation seen in sollich
(1999a) and malzahn and opper (2005) to our discrete graph case. in so doing we will see explicitly
how one may improve this approximation to fully exploit the structure of random graphs, using
belief propagation or equivalently the cavity method (m  ezard and parisi, 2003). we will sketch the
derivation of the existing approximation following the method of malzahn and opper (2005); the
result given by sollich (1999a) is included in this as a somewhat more restricted approximation.
both the approximate treatment and our cavity method take a statistical mechanics approach, so we
begin by rewriting equation (10) in terms of a generating or partition function z

  (n) =* 1
v    

i

z df p(f|x) f 2

i +x,g

=     lim
     0

2

v

   
      hlog(z)ix,g ,

(11)

with

z = z df exp    

1

2

f tc   1f    

1
2  2

n

   
  =1

f 2
x      

  
2    

i

f 2

i ! .

in this representation the inputs x only enter z through the sum over   . we introduce   i to count the
number of examples at vertex i so that z becomes

z = z df exp(cid:18)   

1

2

f tc   1f    

1

2

f tdiag(cid:16)   i

  2 +   (cid:17) f(cid:19) .

(12)

the average in equation (11) of the logarithm of this partition function can still not be carried out
in closed form. the approximation given by malzahn and opper (2005) and our present cavity
approach diverge at this point. section 3.4 discusses the existing approximation for the learning
curve, applied to the case of regression on a graph. section 4 then improves on this using the cavity
method to fully exploit the graph structure.

3.4 kernel eigenvalue approximation

the approach of malzahn and opper (2005) is to average log(z) from (12) using the replica trick
n loghznix, performing the average hznix
(m  ezard et al., 1987). one writes hlog zix = limn   0
for integer n and assuming that a continuation to n     0 is possible. the required n-th power of
equation (12) is given by

1

hznix = z n
   
a=1

df a*exp    

1
2    

a

(f a)tc   1f a    

1
2  2    

i,a

  i( f a

i )2    

1811

  
2    

i,a

( f a

i )2!+x

,

urry and sollich

where the replica index a runs from 1 to n. assuming as before that examples are generated inde-
pendently and uniformly from v , the data set average over x will, for large v , become equivalent
to independent poisson averages over   i with mean    = n/v . explicitly performing these averages
gives

hznix = z n
   
a=1

df a exp    

1
2    

a

(f a)tc   1f a +      

i )2/2  2

i (cid:16)e       a( f a

  
2    

i,a

( f a

i )2! .

    1(cid:17)   

(13)

in order to evaluate (13) one has to    nd a way to deal with the exponential term in the exponent.
malzahn and opper (2005) do this using a variational approximation for the distribution of the f a,
of gaussian form. eventually this leads to the following eigenvalue learning curve approximation
(see also sollich, 1999a):

  (n) = g(cid:18)

n

  (n) +   2(cid:19) ,

g(h) =

v

   

  =1(cid:0)     1

   + h(cid:1)   1

.

(14)

the eigenvalues      of the kernel are de   ned here from the eigenvalue equation4 (1/v )     j ci j   j =
    i. the gaussian variational approach is evidently justi   ed for large   2, where a taylor expansion
of the exponential term in (13) can be truncated after the quadratic term. for small noise levels, on
the other hand, the gaussian variational approach will in general not capture all the details of the
   uctuations in the numbers of examples   i. this issue is expected to be most prominent for values
of    of order unity, where    uctuations in the number of examples are most relevant because some
vertices will not have seen examples locally or nearby and will have posterior variance close to the
prior variance, whereas those vertices with examples will have small posterior variance, of order   2.
this effect disappears again for large   , where the o(     )    uctuations in the number of examples at
each vertex becomes relatively small. mathematically this can be seen from the term proportional
i )2/2  2) close to 1
i with exp(       a( f a
to    in (13), which for large    ensures that only values of f a
will contribute. a quadratic approximation is then justi   ed even if   2 is not large.

learning curve predictions from equation (14) using numerically computed eigenvalues for the
globally normalised random walk kernel are shown in figure 3 as dotted lines for random regular
(left), erd  os-r  enyi (centre) and power law generalised random graphs (right). the predictions are
compared to numerically simulated learning curves shown as solid lines, for a range of noise levels.
consistent with the discussion above, the predictions of the eigenvalue approximation are accurate
where the gaussian variational approach is justi   ed, that is, for small and large   . figure 3 also
shows that the accuracy of the approximation improves as the noise level   2 becomes larger, again
as expected by the nature of the gaussian approximation.

3.4.1 learning curves for large p

before moving on to the more accurate cavity prediction of the learning curves, we now look at
how the learning curves for gp regression on graphs depend on the kernel lengthscale p/a. we
focus for this discussion on random regular graphs, where the distinction between global and local
normalisation is not important. in section 2.1, we saw that on a large regular graph the random walk
kernel approaches a non-trivial limiting form for large p, as long as one stays below the threshold

4. here and below we consider the case of a uniform distribution of inputs across vertices, though the results can be

generalised to the non-uniform case.

1812

gaussian processes on random graphs

100

10   1

10   2

  

10   3

10   4

  2 = 10   1
  2 = 10   2
  2 = 10   3
  2 = 10   4

10   5

10   2

10   1

100

10   2

10   1

100

10   2

10   1

100

101

   = n/v

figure 3: (left) learning curves for gp regression with globally normalised kernels with p = 10,
a = 2 on 3-regular random graphs for a range of noise levels   2. dotted lines: eigen-
value predictions (see section 3.4), solid lines: numerically simulated learning curves
for graphs of size v = 500, dashed lines: cavity predictions (see section 4.1); note these
are mostly visually indistinguishable from the simulation results. (centre) as (left) for
erd  os-r  enyi random graphs with mean degree 3. (right) as (left) for power law gener-
alised random graphs with exponent 2.5 and cutoff 2.

(7) for p where cycles become important. one might be tempted to conclude from this that also
the learning curves have a limiting form for large p. this is too naive however, as one can see by
considering, for example, the effect of the    rst example on the bayes error. if the example is at
i j/(cii +   2). as the prior variances
i j/(1 +   2). the reduction in
i j/(1 +   2). as long as cycles are unimportant
this is independent of the location of the example vertex i, and in the notation of section 2.1 can be
written as

vertex i, the posterior variance at vertex j is, from (9), c j j     c2
c j j are all equal, to unity for our chosen normalisation, this is 1    c2
the bayes error is therefore   (0)      (1) = (1/v )     j c2

  (0)      (1) =

1

1 +   2

p

   
l=0

vlc2

l,p,

(15)

where vl is, as before, the number of vertices a distance l away from vertex i, that is, v0 = 1,
vl = d(d     1)l   1 for l     1. to evaluate (15) for large p, one cannot directly plug in the limiting
kernel form (5): the    shell volume    vl just balances the l-dependence of the factor (d     1)   l/2 from
cl,p, so that one gets contributions from all distances l, proportional to l2 for large l. naively
summing up to l = p would give an initial decrease of the bayes error growing as p3. this is not
correct; the reason is that while cl,p approaches the large p-limit (5) for any    xed l, it does so more
and more slowly as l increases. a more detailed analysis, sketched in appendix a.2, shows that

1813

urry and sollich

for large l and p, cl,p is proportional to the large p-limit l(d     1)   l/2 up to a characteristic cutoff
distance l of order p1/2, and decays quickly beyond this. summing in (15) the contributions of order
l2 up to this distance predicts    nally that the initial error decay should scale, non-trivially, as p3/2.
we next show that this large p-scaling with p3/2 is also predicted, for the entire learning curve,
by the eigenvalue approximation (14). as before we consider d-regular random graphs. the re-
quired spectrum of kernel eigenvalues      becomes identical, for large v , to that on a d-regular tree
(mckay, 1981). explicitly, if   l
   are the eigenvalues of the normalised graph laplacian on a tree,
  /a)p. here the factor v    1 comes from the
then the kernel eigenvalues are      =      1v    1(1       l
same factor in the kernel eigenvalue de   nition after (14), and    is the overall normalisation constant
which enforces            = v    1     j c j j = 1. the spectrum of the tree laplacian is known (see mckay,
1981; chung, 1996) and is given by

q 4(d   1)
d2    (  l   1)2
(2  /d)  l(2     l)
0

                   +,
otherwise,

  (  l) =      
   

where      = 1    2

d (d     1)1/2. (there are also two isolated eigenvalues at 0 and 2, which do not

contribute for large v .)

we can now write down the function g from (14), converting the sum over kernel eigenvalues to
v times an integral over laplacian eigenvalues for large v . dropping the l superscript, the result is

g(h) = z   +
     

d     (  )[  (1      /a)   p + hv    1]   1.

(16)

the dependence on hv    1 here shows that in the approximate learning curve (14), the bayes error
will depend only on    = n/v as might have been expected. the condition for the normalisation

factor    becomes simply g(0) = 1, or      1 = r d     (  )(1      /a)p.

so far we have written down how one would evaluate the eigenvalue approximation to the learn-
ing curve on large d-regular random graphs, for arbitrary kernel parameters p and a. now we want
to consider the large p-limit. we show that there is then a master curve for the bayes error against
  p3/2. this is entirely consistent with the p3/2 scaling found above for the initial error decay. the
intuition for the large p analysis is that the factor (1      /a)p decays quickly as the laplacian eigen-
value    increases beyond      , so that only values of    near       contribute. one can then approximate

a          (cid:19) .
p(           )
similarly one can replace   (  ) by its leading square root behaviour near      ,

   (cid:18)1   

exp(cid:18)   

(cid:18)1   

     

  

a (cid:19)p

a(cid:19)p

  (  ) = (           )1/2 (d     1)1/4d5/2
  (d     2)2

.

substituting these approximations into (16) and introducing the rescaled integration variable y =
p(           )/(a         ) gives

g(h) = r     1(1         /a)p(cid:18) a         

p (cid:19)3/2

f(h     1v    1(1         /a)p),

1814

gaussian processes on random graphs

100

10   1

  

10   2

p = 50
p = 100
p = 200
p = 500
p = 1000
p = 1500
master curve

100

10   1

  

10   2

p = 5
p = 10
p = 15
p = 20

10   3

10   2

10   1

100

101

102

103

  

10   3

10   2

10   1

100

101

102

103

  

figure 4: (left) eigenvalue approximation for learning curves on a random 3-regular graph, using
a random walk kernel with a = 2,   2 = 0.1 and increasing values of p as shown. plotting
against   p3/2 shows that for large p these rescaled curves approach the master curve
predicted from (17), though this approach is slower in the tail of the curves. (right) as
(left), but for numerically simulated learning curves on graphs of size v = 500.

where r = (d     1)1/4d5/2/(  (d     2)2) and f(z) = r    
0 dy y1/2(exp(y) + z)   1. since g(0) = 1, the
prefactor must equal 1/f(0) = 2/     . this    xes the normalisation constant   , and we can simplify
to

g(h) =

f(hv    1c   1)

f(0)

,

c = rf(0)(cid:18) a         

p (cid:19)3/2

.

the learning curves for large p are then predicted from (14) by solving

   = f(  c   1/(   +   2))/f(0),

(17)

and depend clearly only on the combination   c   1. because c is proportional to p   3/2, this shows
that learning curves for different p should collapse onto a master curve when plotted against   p3/2.
a plot of the scaling of the eigenvalue learning curve approximations onto the master curve
is shown in figure 4 (left). as can be seen, large values of p are required in order to get a good
collapse in the tail of the learning curve prediction, whereas in the initial part the p3/2 scaling is
accurate already for relatively small p.

finally, figure 4 (right) shows that the predicted p3/2-scaling of the learning curves is present
not only within the eigenvalue approximation, but also in the actual learning curves. figure 4
(right) displays numerically simulated learning curves for p = 5, 10, 15 and 20, against the rescaled
number of examples   p3/2 as before. even for these comparatively small values of p one sees that
the rescaled learning curves approach a master curve.

1815

urry and sollich

4. exact learning curves: cavity method

so far we have discussed the eigenvalue approximation of gp learning curves, and how it deviates
from numerically exact simulated learning curves. as discussed in section 3.4, the de   ciencies
of the eigenvalue approximation can be traced back to the fact that the    uctuations in the number
of training examples seen at each vertex of the graph cannot be accounted for in detail. if in the
average over data sets these    uctuations could be treated exactly, one would hope to obtain exact,
or at least very accurate, learning curve predictions. in this section we show that this is indeed
possible in the case of a random walk kernel, for both global and local normalisations. we derive our
prediction using belief propagation or, equivalently, the cavity method (m  ezard and parisi, 2003).
the approach relies on the fact that the local structure of the graph on which we are learning is
tree-like. this local tree-like structure always occurs in large random graphs sampled uniformly
from an ensemble speci   ed by an arbitrary but    xed degree distribution, which is the scenario we
consider here. we will see that already for moderate graph sizes of v = 500, our predictions are
nearly indistinguishable from numerical simulations.

in order to apply the cavity method to the problem of predicting learning curves we must    rst
rewrite the partition function (12) in the form of a graphical model. this means that the function
being integrated over to obtain z must consist of factors relating only to individual vertices, or to
pairs of neighbouring vertices. the inverse of the covariance matrix in (12) creates factors linking
vertices at arbitrary distances along the graph, and so must be eliminated before the cavity method
can be applied. we begin by assuming a general form for the normalisation of   c that encompasses
both local and global normalisation and set c = k    1/2[(1    a   1)i + a   1d   1/2ad   1/2]pk    1/2
with ki j =   i  i j. to eliminate interactions across the entire graph we    rst fourier transform the prior
2 f tc   1f ) in (12), introduce fourier variables h, and then integrate out the remaining
term exp(    1
terms with respect to f to give

z        

  2 +   (cid:17)   1/2z dh exp(cid:18)   
i (cid:16)   i

1

2

htch   

1

2

htdiag(cid:18)(cid:16)   i

  2 +   (cid:17)   1/2(cid:19) h(cid:19) .

the coupling between different vertices in (4) is now through c so still links vertices up to distance
p. to reduce these remaining interactions to ones among nearest neighbours only, one exploits
the binomial expansion of the random walk kernel given in (3). de   ning p additional variables at

each vertex as hq = k 1/2(d   1/2ad   1/2)qk    1/2h, q = 1, . . . , p, and abbreviating cq =(cid:0)p

a   1)p   q(a   1)q, the interaction term htch turns into a local term    p
q=0 cq(h0)tk    1hq. (here we
have, for the sake of uniformity, written h0 instead of h.) of course the interactions have only
been    hidden    in the hq, but the key point is that the de   nition of these additional variables can be
enforced recursively, via hq = k 1/2d   1/2ad   1/2k    1/2hq   1. we represent this de   nition via a
dirac delta function (for each q = 1, . . . , p) and then fourier transform the latter, with conjugate
variables   hq, to get

q(cid:1)(1   

z        

  2 +   (cid:17)   1/2z
i (cid:16)   i
   

   
q=0

1

2

p

p

   
q=0

dhq

p

   
q=1

cq(h0)tk    1hq + i

2

1

(h0)tdiag(cid:18)(cid:16)   i

d  hq exp    
(  hq)t(cid:16)hq     k 1/2d   1/2ad   1/2k    1/2hq   1(cid:17)!.

  2 +   (cid:17)   1(cid:19) h0

   
q=1

p

(18)

1816

gaussian processes on random graphs

because the graph adjacency matrix a now appears at most linearly in the exponent, all interactions
are between nearest neighbours only. we have thus expressed our z as the partition function of a
(complex-valued) graphical model.

4.1 global normalisation

we can now apply belief propagation to the calculation of marginals for the above graphical model.
we focus    rst on the simpler case of a globally normalised kernel where   i =    for all i. rescaling
each hq

  hq
i /  1/2 we are left with

1/2
i   1/2hq

1/2
i to d
i

i and   hq

i to d

z        

  2 +   (cid:17)   1/2z
i (cid:16)   i

p

   
q=0

dhq

p

   
q=1

d  hq    

i

exp    

1

2

p

   
q=0

2

i hq

cqh0

i di    
exp    i
q=1(cid:16)  hq

   

p

   
(i, j)

1

i )2  di
(h0
  i/  2 +   

+ i

p

   
q=1

di   hq

i hq

i!

i hq   1

j +   hq

j hq   1

(19)

i (cid:17)! ,

where the interaction terms coming from the adjacency matrix, a, have been written explicitly as a
product over distinct graph edges (i, j).

to see how the bayes error (10) can be obtained from this partition function, we differentiate

log(z) with respect to    as prescribed by (11) to get

  (  ) = lim
     0

1
v    

i

1

  i/  2 +   (cid:18)1   

di  h(h0
i )2i
  i/  2 +    (cid:19) .

(20)

in order to calculate the bayes error we therefore require speci   cally the marginal distributions of
h0
i . these can be calculated using the cavity method: for a large random graph with arbitrary    xed
degree sequence the graph is locally tree-like, so that if vertex i were eliminated the corresponding
subgraphs (locally trees) rooted at the neighbours j     n (i) of i would become approximately in-
j (h j,   h j|x),
dependent. the resulting cavity marginals created by removing i, which we denote p
can then be calculated iteratively within these subgraphs using the update equations

(i)

j!
khq   1

j

p

(i)

j (h j,   h j|x)     exp    

1

2

p

   
q=0

cqd jh0

j hq

j    

d j  (h0
j)2
   j/  2 +   

1

2

+ i

p

   
q=1

d j   hq

j hq

z    
k   n ( j)\i

dhkd  hk exp    i

p

   
q=1

(  hq

j hq   1

k +   hq

)! p

( j)

k (hk,   hk|x).

(21)

j, . . . , hp

where h j = (h0
j )t. in terms of the sum-product formulation of belief
propagation, the cavity marginal on the left is the message that vertex j sends to the factor in z for
edge (i, j) (bishop, 2007).

j )t and   h j = (  h1

j, . . . ,   hp

one sees that the cavity update equations (21) are solved self-consistently by complex-valued

gaussian distributions with mean zero and covariance matrices v
. this gaussian character of
the solution was of course to be expected because in (19) we have a gaussian graphical model.
by performing the gaussian integrals in the cavity update equations explicitly, one    nds for the
corresponding updates of the covariance matrices the rather simple form

(i)
j

xv

( j)
k x)   1,

(22)

(i)

v

j = (o j        

k   n ( j)\i

1817

urry and sollich

where we have de   ned the (2p + 1)   (2p + 1) matrices
   

   j/  2+  
c1
2

c0+   

   

. . .

. . .

cp
2

c1
2

0

0
   i

                                    

...

cp
2
0
...
0

. . .

   i

   i

. . .

0p,p

   i

                                    

o j = d j

, x =

   

                                 

0p+1,p+1

i

. . .

0
...
0

i

i

. . .

i
0 . . . 0

0p,p

   

                                 

.

(23)

k=1 xv

at    rst glance (22) becomes singular for    j = 0; however this is easily avoided. we introduce
o j        d   1
0 = (1, 0, . . . , 0) so that m j contains
all the non-singular terms. we may then apply the woodbury identity (hager, 1989) to write the
matrix inverse in a form where the        0 limit can be taken without dif   culties:

( j)
k x = m j + [d j  /(   j/  2 +   )]e0et

0 with et

 o j    

d   1
   
k=1

xv

( j)

k x!   1

= m   1

j    

m   1

0 m   1
j
(   j/  2 +   )/(d j  ) + et

j e0et

0 m   1

j e0

.

(i)
j

in our derivation so far we have assumed a    xed graph, we therefore need to translate these
equations to the setting we ultimately want to study, that is, an ensemble of large random graphs.
this ensemble is characterised by the distribution p(d) of the degrees di, so that every graph that has
the desired degree distribution is assigned equal id203. instead of individual cavity covariance
, one must then consider their id203 distribution w (v ) across all edges of the
matrices v
graph. picking at random an edge (i, j) of a graph, the id203 that vertex j will have degree
d j is then p(d j)d j/   d, because such a vertex has d j    chances    of being picked. (the normalisation
factor is the average degree   d =    i p(di)di.) using again the locally treelike structure, the incoming
( j)
(to vertex j) cavity covariances v
k will be independent and identically distributed samples from
w (v ). thus a    xed point of the cavity update equations corresponds to a    xed point of an update
equation for w (v ):

w (v ) =    

d

p(d)d

  d *z d   1

   
k=1

dvk w (vk)      

   v     o    

d   1
   
k=1

xvkx!   1   
   +

  

.

(24)

since the vertex label is now arbitrary, we have omitted the index j. the average in (24) is over the
distribution of the number of examples           j at vertex j. as before we assume for simplicity that
examples are drawn with uniform input id203 across all vertices, so that the distribution of    is
simply        poisson(  ) in the limit of large n and v at    xed    = n/v .

in general equation (24)   which can also be formally derived using the replica approach (see
urry and sollich, 2012)   cannot be solved analytically, but we can tackle it numerically using
population dynamics (m  ezard and parisi, 2001). this is an iterative technique where one creates a
population of covariance matrices and for each iteration updates a random element of the population
according to the delta function in (24). the update is calculated by sampling from the degree
distribution p(d) of local degrees, the poisson distribution of the local number of examples    and

1818

gaussian processes on random graphs

from the distribution w (vk) of    incoming    covariance matrices, the latter being approximated by
uniform sampling from the current population.

once we have w (v ), the bayes error can be found from the graph ensemble version of equa-
i )2i in terms of the cavity

tion (11). this is obtained by inserting the explicit expression for h(h0

marginals of the neighbouring vertices, and replacing the average over vertices with an average over
degrees d:

  (  ) = lim
     0

   

d

p(d)*

1

  /  2 +    1   

d  

  /  2 +    z d
   
k=1

dvk w (vk) (o    

d

   
k=1

xvkx)   1

00!+  

.

(25)

the number of examples at the vertex    is once more to be averaged over        poisson(  ). the
subscript    00    indicates the top left element of the matrix, which determines the variance of h0.
to be able to use equation (25), we again need to rewrite it into a form that remains ex-
plicitly non-singular when    = 0 and        0. we separate the   -dependence of the matrix in-
verse again and write, in slightly modi   ed notation as appropriate for the graph ensemble case,
o        d
0 = (1, 0, . . . , 0). the 00 element of the
matrix inverse appearing above can then be expressed using the woodbury formula (hager, 1989)
as

k=1 xvkx = md + [d  /(  /  2 +   )]e0et

0 , where et

et

0 o    

d

   
k=1

xvkx!   1

e0 = et

0 m   1

d e0    

0 m   1
et

0 m   1
(  /  2 +   )/(d  ) + et

d e0et

d e0
0 m   1

d e0

.

the        0 limit can now be taken, with the result

  (  ) =*   

d

p(d)z d
   
k=1

dvk w (vk)

  /  2 + d  (m   1

1

d )00+  

.

(26)

this has a simple interpretation: the cavity marginals of the neighbours provide an effective gaus-
sian prior for each vertex, whose inverse variance is d  (m   1)00.

the self-consistency equation (24) for w (v ) and the expression (26) for the resulting bayes
error allow us to predict learning curves as a function of the number of examples per vertex,   , for
arbitrary degree distributions p(d) of our random graph ensemble. for large graphs the predictions
should become exact. it is worth stressing that such exact learning curve predictions have previously
only been available in very speci   c, noise-free, gp regression scenarios, while our result for gp
regression on graphs is applicable to a broad range of random graph ensembles, with arbitrary noise
levels and kernel parameters.

we note brie   y that for graphs with isolated vertices (d = 0), one has to be slightly careful:
already in the de   nition of the covariance function (2) one should replace d     d +   i to avoid
division by zero, taking        0 at the end. for d = 0 one then    nds in the expression (26) that
(m   1)00 = 1/(c0  ), where c0 is de   ned before (18). as a consequence,   (   + d)(m   1)00 =
    (m   1)00 =   /c0. this is to be expected since isolated vertices each have a separate gaussian
prior with variance c0/  .

equations (24) and (26) still require the normalisation constant,   . the simplest way to calculate
this is to run the population dynamics once for    = 1 and    = 0, that is, an unnormalised kernel and
no training data. the result for    then just gives the average (over vertices) prior variance. with
   set to this value, one can then run the population dynamics for any    to obtain the bayes error
prediction for gp regression with a globally normalised kernel.

1819

urry and sollich

comparisons between the cavity prediction for the learning curves, numerically exact simu-
lated learning curves and the results of the eigenvalue approximation are shown in figure 3 (left,
centre and right), for regular, erd  os-r  enyi and generalised random graphs with power law degree
distributions respectively. as can be seen the cavity predictions greatly outperform the eigenvalue
approximation and are accurate along the whole length of the curve. this con   rms our expecta-
tion that the cavity approach will become exact on large graphs, although it is remarkable that the
agreement is quantitatively so good already for graphs with only    ve hundred vertices.

4.2 predicting prior variances

as a by-product of the cavity analysis for globally normalised kernels we note that in the cavity form
of the bayes error in equation (26), the fraction (  /  2 + d  (m   1
d )00)   1 is the local bayes error,
that is, the local posterior variance. by keeping track of individual samples for this quantity from
the population dynamics approach, we can thus predict the distribution of local posterior variances.
if we set    = 0, then this becomes the distribution of prior variances. the cavity approach therefore
gives us, without additional effort, a prediction for this distribution.

we can now go back to section 3.2 and compare the cavity predictions to numerically simulated
distributions of prior variances. the cavity predictions for these distributions are shown by the black
lines in figure 2. the cavity approach provides, in particular, detailed information about the tail of
the distributions as shown in the insets. there is good agreement between the predictions and the
numerical simulations, both regarding the general shape of the variance distributions and the    ne
structure with a number of non-trivial peaks and troughs. the residual small shifts between the
predictions and the numerical results for a single instance of a large graph are most likely due to
   nite size effects: in a    nite graph, the assumption of a tree-like local structure is not exact because
there can be rare short cycles; also, the long cycles that the cavity method ignores because their
length diverges logarithmically with v will have an effect when v is    nite.

4.3 local normalisation

we now extend the cavity analysis for the learning curves to the case of locally normalised random
walk kernels, which, as argued above, provide more plausible probabilistic models. in this case the
diagonal entries of the normalisation matrix k are de   ned as

  i = z df f 2

i p(f ),

where p(f ) is the gp prior with the unnormalised kernel   c. this makes clear why the locally
normalised kernel case is more challenging technically: we cannot calculate the normalisation con-
stants once and for all for a given random graph ensemble and set of kernel parameters p and a as
we did for    in the globally normalised scenario. instead we have to account for the dependence of
the   i on the speci   c graph instance.

on a single graph instance, this stumbling block can be overcome as follows. one iterates the
cavity updates (22) for the unnormalised kernel and without the training data (i.e., setting    = 1 and
  i = 0). the local bayes error at vertex i, given by the i-th term in the sum from (20), then gives us
  i. because   i = 0, one has to use the woodbury trick to get well-behaved expressions in the limit
where the auxiliary parameter        0, as explained after (25).

1820

gaussian processes on random graphs

once the   i have been determined in this way, one can use them for predicting the bayes error
for the scenario we really want to study, that is, using a locally normalised kernel and incorporating
the training data. the relevant partition function is the analogue of (18) for local normalisation.
dropping the prefactors, the resulting z can be written as

z     z

p

   
q=0

dhq

p

   
q=1

d  hq    

i

exp    

1

2

p

   
q=0

cqdih0

i hq

i    

1

2

i )2
di  i(h0
  i/  2 +   

+ i

p

   
q=1

di   hq

i hq

i!

exp    i

   
(i, j)

p

   
q=1

(  hq

i hq   1

j +   hq

j hq   1

i

)! ,

where we have rescaled hq
  hq
i . given that the   i have already been
determined, this is a graphical model for which marginals can be calculated by iterating to a    xed
point the equations for the cavity marginals:

i and   hq

i hq

i to d

i to d

i      1/2

i   1/2

1/2

1/2

i

p

(i)

loc, j(h j,   h j|x)     exp    

1

2

p

   
q=0

cqd jh0

j hq

j    

d j   j(h0
j)2
   j/  2 +   

1

2

+ i

p

   
q=1

d j   hq

z    
k   n ( j)\i

dhkd  hk exp    i

p

   
q=1

(  hq

j hq   1

k +   hq

( j)

loc,k(hk,   hk|x).

(27)

j hq

j!
)! p
khq   1

j

as in section 4.1 these update equations are solved by cavity marginals of complex gaussian form,
and so we can simplify them to updates for the covariance matrices:

v

(i)

loc, j = oloc, j        

k   n ( j)\i

xv

( j)

k,locx!   1

.

(28)

here x is de   ned as in equation (23) and oloc, j is the obvious analogue of o j also de   ned in
equation (23); speci   cally,    is replaced by    j. once the update equations have converged, one can
calculate the bayes error from a similarly adapted version of (20).

the above procedure for a single    xed graph now has to be extended to the case of an ensemble
of large random graphs characterised by some degree distribution p(d). the outcome of the    rst
round of cavity updates, for the unnormalised kernel without training data, is then represented by a
distribution of cavity covariances v , while the second one gives a distribution of cavity covariances
vloc for the locally normalised kernel, with training data included.
importantly, these message
distributions are coupled to each other via the graph structure, so we need to look at the joint
distribution w (vloc, v ).

detailed analysis using the replica method (urry and sollich, 2012) shows that the correct    xed
point equation updates the v -messages as in the globally normalised case with    = 0. the second set
of local covariances, vloc, are then updated according to (28), with a normaliser calculated using the
marginals from the d     1 v -covariances and an additional    counter   ow    covariance generated from
w (v ) = r dvlocw (vloc, v ), subject to the constraint that the local marginals of the neighbours are
consistent. we    nd in practice that the consistency constraint can be dropped and the    xed point

1821

urry and sollich

equation for the distribution of the two sets of messages can be approximated by

w (vloc, v ) =*   

d

  d

p(d)d

z d   1
   
k=1
     
   vloc     oloc, j    

dvkdvloc,kdvd

d   1
   
k=1

w (vloc,k, vk)w (vd)

d   1
   
k=1

xv

( j)

k,locx!   1   
   

     
   v     o    

d   1
   
k=1

xvkx!   1   
   +

  

.

(29)

one sees that if one marginalises over vloc, then one obtains exactly the same condition on w (v )
as before in the globally normalised kernel case (but with    = 1 and    = 0), see (24). this re   ects
the fact that the cavity updates for the    rst set of messages on a single graph do not rely on any
information about the second set. the    rst delta function in (29) corresponds to the    xed point
condition for this second set of cavity updates. this condition depends, via the value of the local   ,
on the v -cavity covariances:

   =

1
d )00

d(m   1

.

(30)

it may seem unusual that d copies of v enter here; vd represents the cavity covariance from the
   rst set that is received from the vertex to which the new message vloc is being sent. while this
counter   ow appears to run against the basic construction of the cavity or belief propagation method,
it makes sense here because the    rst set of cavity messages (or equivalently the distribution w (v ))
reaches a    xed point that is independent of the second set, so the counter   ow of information is only
apparent. the reason why knowledge about vd is needed in the update is that    is the variance of a
full marginal rather than a cavity marginal.

similarly to the case of global normalisation, (29) can be solved by looking for a    xed point of
w (vloc, v ) using population dynamics. updates are made by    rst updating v using equation (21)
and then updating vloc using (27) with          i replaced by (30).
once a    xed point has been calculated for the covariance distribution we apply the woodbury
formula to (20) in a similar manner to section 4.1 to give the prediction for the learning curve for
gp regression with a locally normalised kernel. the result for the bayes error becomes

   =*   

d

p(d)z d
   
k=1

dvloc,kdvk

d

   
k=1

w (vloc,k, vk)

1
  /  2 + (m   1

d,loc)00/(m   1

d )00+  

.

learning curve predictions for gps with locally normalised kernels as they result from the cavity
approach described above are shown in figure 5. the    gure shows numerically simulated learning
curves and the cavity prediction, both for erd  os-r  enyi random graphs (left) and power law gener-
alised random graphs (centre) of size v = 500. as for the globally normalised case one sees that the
cavity predictions are quantitatively very accurate even with the simpli   ed update equation (29).
they capture all aspects of learning curve both qualitatively and quantitatively, including, for exam-
ple, the shoulder in the curves from disconnected single vertices, a feature discussed in more detail
below.

the fact that the cavity predictions of the learning curve for a locally normalised kernel are
indistinguishable from the numerically simulated learning curves in figure 5 leads us to believe that
the simpli   cation made by dropping the consistency requirement in (29) is in fact exact. this is
further substantiated by looking not just at the average of the posterior variance over vertices, which

1822

gaussian processes on random graphs

10   1
10   2
10   3
10   4

10   1
10   2
10   3
10   4

  2 = 10   1
  2 = 10   2
  2 = 10   3
  2 = 10   4

2

4

6 8 10

10   1

100

10   2

10   1

100

101

10   1

100

101

   = n/v

100

10   1

10   2

  

10   3

10   4

10   5

10   2

figure 5: (left) learning curves for gp regression with locally normalised kernels with p = 10,
a = 2 on erd  os-r  enyi random graphs with mean degree 3, for a range of noise levels   2.
solid lines: numerically simulated learning curves for graphs of size v = 500, dashed
lines: cavity predictions (see section 4.1); note these are mostly visually indistinguishable
from the simulation results. (left inset) dotted lines show single vertex contributions
to the learning curve (solid line). (centre) as (left) for power law generalised random
graphs with exponent 2.5 and cut off 2. (right top) comparison between learning curves
for locally (dashed line) and globally (solid line) normalised kernels for erd  os-r  enyi
random graphs. (right bottom) as (right top) for power law random graphs.

is the bayes error, but its distribution across vertices. as shown in figure 6, the cavity predictions
for this distribution are in very good agreement with the results of numerical simulations. this holds
not only for the two values of    shown, but along the entire learning curve.

5. a qualitative comparison of learning with locally and globally normalised

kernels

the cavity approach we have developed gives very accurate predictions for learning curves for gp
regression on graphs using random walk kernels. this is true for both global and local normalisa-
tions of the kernel. we argued in section 3.2 that the local normalisation is much more plausible as
a probabilistic model, because it avoids variability in the local prior variances that is non-trivially
related to the local graph structure and so dif   cult to justify from prior knowledge. we now compare
what the qualitative effects of the two different normalisations are on gp learning.

it is not a simple matter to say which kernel is    better   , the locally or globally normalised one.
since we have dealt with the matched case, where for each kernel the target functions are sampled
from a gp prior with that kernel as covariance function, it would not make sense to say the better
kernel is the one that gives the lower bayes error for given number of examples, as the bayes error

1823

urry and sollich

)
i
i

c
(
p

80

70

60

50

40

30

20

10

0

)
i
i

c
(
p

30

25

20

15

10

5

0

0

0.2

0.4

0.6

0.8

1

1.2

0

0.05

cii

0.1
cii

0.15

0.2

figure 6: (left) grey: histogram of posterior variances at    = 1.172 for the locally normalised ran-
dom walk kernel with a = 2, p = 10, averaged over ten samples each of teacher functions,
data and erd  os-r  enyi graphs with mean degree    = 3 and v = 1000 vertices. black:
cavity prediction for this distribution in the large graph limit. (right) as (left) but for
   = 6.210.

re   ects both the complexity of the target function and the success in learning it. a more de   nite
answer could be obtained only empirically, by running gp regression with local and global kernel
normalisation on the same data sets and comparing the prediction errors and also the marginal data
likelihood. the same approach could also be tried with synthetic data sets generated from gp
priors that are mismatched to both priors we have considered, de   ned by the globally and locally
normalised kernel, though justifying what is a reasonable choice for the prior of the target function
would not be easy.

while a detailed study along the lines above is outside the scope of this paper, we can neverthe-
less at least qualitatively study the effect of the kernel normalisation, to understand to what extent
the corresponding priors de   ne signi   cantly different probabilistic models. figure 5 (right top and
bottom) overlays the learning curves for global and local kernel normalisations, for an erd  os-r  enyi
and a power law generalised random graph respectively. there are qualitative differences in the
shapes of the learning curves, with the ones for the locally normalised kernel exhibiting a shoulder
around    = 2. this shoulder is due to the proper normalisation of isolated vertices to unit prior vari-
ance; by contrast, as shown earlier in figure 2 (left), global normalisation gives too small a prior
variance to such vertices. the inset in figure 5 (left) shows the expected learning curve contribu-
tions from all locally normalised isolated vertices (single vertex subgraphs) as dotted lines. after the
gp learns the rest of the graph to a suf   cient accuracy, the single vertex error dominates the learning
curve until these vertices have typically seen at least one example. once this point has been passed,
the dominant error comes once more from the giant connected component of the graph, and the gp

1824

gaussian processes on random graphs

learns in a similar manner to the globally normalised case. a similar effect, although not plotted, is
seen for the generalised random graph case.

we can extend the scope of this qualitative comparison by examining how a student gp with a
kernel with one normalisation performs when learning from a teacher with a kernel with the other
normalisation. this is a case of model mismatch; our theory so far does not extend to this scenario,
but we can obtain learning curves by numerical simulation. figure 7 (left) shows the case of gp
students with a globally normalised kernel learning from a teacher with a locally normalised kernel
on an erd  os-r  enyi graph. the learning curves for the mismatched scenario are very different from
those for the matched case (figures 3 and 5), showing an increase in error as    approaches unity.
the resulting maximum in the learning curve again emphasises that the two choices of normalisation
produce distinctly different probabilistic models. similar behaviour can be observed for the case of
power law generalised random graphs, shown in figure 7 (right) and for the case of gp students
with a locally normalised kernel learning from a teacher with a globally normalised kernel, shown
in figure 8. in all cases close inspection (see appendix b) shows that the error maximum is caused
by    dangling edges    of the graph, that is, chains of vertices (with degree two) extending away from
the giant graph component and terminating in a vertex of degree one.

as a    nal qualitative comparison between globally and locally normalised kernels, figure 9
shows the variance of local posterior variances. this measures how much the local bayes error
typically varies from vertex-to-vertex, as a function of the data set size. plausibly one would expect
that this error variance is low initially when prediction on all vertices is equally uncertain. for large
data sets the same should be true because errors on all vertices are then low. in an intermediate
regime the error variance should become larger because examples will have been received on or
near some vertices but not others. as figure 9 shows, for kernels with local normalisation we    nd
exactly this scenario, both for erd  os-r  enyi and power law random graphs. the error variance is low
for small    = n/v , increasing to a peak at        0.2 and    nally decreasing again.

these results can now be contrasted with those for globally normalised kernels, also displayed
in figure 9. here the error variance is largest at    = 0 and decays from there. this means that the
initial variance in the local prior variances is so large that any effects from the uneven distribution of
example locations in any given data set remain sub-dominant throughout. we regard this as another
indication of the probabilistically implausible character of the large spread of prior variances caused
by global normalisation.

6. conclusions and further work

in this paper we studied random walk kernels and their application to gp regression. we began, in
section 2, by studying the random walk kernel, with a focus on applying this to d-regular trees and
graphs. we showed that the kernel exhibits a rather subtle approach to the fully correlated limit;
this limit is reached only beyond a graph-size dependent threshold for the kernel range p/a, where
cycles become important. if p/a is large but below this threshold, the kernel reaches a non-trivial
limiting shape.

in section 3 we moved on to the application of random walk kernels to gp regression. we
showed, in section 3.2, that the more typical approach to normalisation, that is, scaling the kernel
globally to a desired average prior variance, results in a large spread of local prior variances that is
related in a complicated manner to the graph structure; this is undesirable in a prior. we suggested

1825

urry and sollich

100

  

10   1

100

  

10   1

  2 = 10   1
  2 = 10   2
  2 = 10   3
  2 = 10   4

10   2

10   2

10   1

  

100

10   2

10   2

10   1

  

100

101

figure 7: (left) numerically simulated learning curves for a gp with a globally normalised kernel
with p = 10 and a = 2, on erd  os-r  enyi random graphs with mean degree 3 for a range of
noise levels. the teacher gp has a locally normalised kernel with the same parameters.
(right) as (left) but for power law generalised random graphs with exponent 2.5 and
cutoff 2.

as a simple remedy to perform local normalisation, where the raw kernel is normalised by its local
prior variance so that the prior variance becomes the same at every vertex.

in order to get a deeper understanding of the performance of gps with random walk kernels
we then studied the learning curves, that is, the mean bayes error as a function of the number of
examples. we began in section 3.4 by applying a previous approximation due to sollich (1999a)
and malzahn and opper (2005) to the case of discrete inputs that are vertices on a graph. we
demonstrated numerically that this approximation is accurate only for small and large number of
training examples per vertex,    = n/v , while it fails in the crossover between these two regimes.
the outline derivation of this approximation suggested how one might improve it: one has to exploit
fully the structure of random graphs, using the cavity method, thus avoiding approximating the
average over data sets. in section 4 we implemented this programme, beginning in section 4.1 with
the case of global normalisation. we showed that by fourier transforming the prior and introducing
2p additional variables at each vertex one can rewrite the partition function in terms of a complex-
valued gaussian graphical model, where the marginals that are required to calculate the bayes
error can be found using the cavity method, or equivalently belief propagation.
in section 4.3
we tackled the more dif   cult scenario of a locally normalised kernel. this required two sets of
cavity equations. the    rst serves to calculate the local normalisation factors. the second one then
combines these with the information about the data set to    nd the local marginal distributions. one
might be tempted to consider applying our methods to a lattice so that one could make an estimate
of the learning curves for the continuous limit, that is, regression with a squared exponential kernel

1826

gaussian processes on random graphs

100

  

10   1

  2 = 10   1
  2 = 10   2
  2 = 10   3
  2 = 10   4

100

  

10   1

10   2

10   2

10   1

  

100

10   2

10   2

10   1

  

100

101

figure 8: (left) numerically simulated learning curves for a gp with a locally normalised kernel
with p = 10 and a = 2, on erd  os-r  enyi random graphs with mean degree 3 for a range of
noise levels. the teacher gp has a globally normalised kernel with the same parameters.
(right) as (left) but for power law generalised random graphs with exponent 2.5 and
cutoff 2.

1

0.8

0.6

  

0.4

0.2

global
local

1

0.8

0.6

  

0.4

0.2

0
10   2

10   1

  

100

0
10   2

10   1

  

100

101

figure 9: (left) error variance for gp regression with locally (stars) and globally (circles) nor-
malised kernels against   , on erd  os-r  enyi random graphs, and for matched learning with
p = 10, a = 2,   2 = 0.1. (right) as (left) but for power law generalised random graphs
with exponent 2.5 and cutoff 2.

1827

urry and sollich

for inputs in r2 or similar. sadly, however, since the cavity method requires graphs to be treelike
this is not possible.

finally in section 5 we qualitatively compared gps with kernels that are normalised globally
and locally. we showed that learning curves are indeed qualitatively different. in particular, local
normalisation leads to a shoulder in the learning curves owing to the correct normalisation of the
single vertex disconnected graph components. we also considered numerically calculated mismatch
curves. the mismatch caused a maximum to appear in the learning curves, as a result of the large
differences in the teacher and student priors. lastly we looked at the variance among the local bayes
errors, for gps with both globally and locally normalised kernels. plausibly, locally normalised
kernels lead to this error variance being maximal for intermediate data set sizes. this re   ects the
variation in number of examples seen at or near individual vertices. for globally normalised kernels,
the error variance inherited from the spread of local prior variances is always dominant, obscuring
any signatures from the changing    coverage    of the graph by examples.

in further work we intend to extend the cavity approximation of the learning curves to the case
of mismatch, where teacher and student have different kernel hyperparameters. it would also be
interesting to apply the cavity method to the learning curves of gps with random walk kernels
on more general random graphs, like those considered in rogers et al. (2010) and k  uhn and van
mourik (2011). this would enable us to consider graphs exhibiting some community structure.
looking further ahead, preliminary work has shown that it should be possible to extend the cavity
learning curve approximation to the problem of graph mismatch, where the student has incomplete
information about the graph structure of the teacher.

appendix a. random walk kernel on d-regular trees

we detail here how to derive the limiting kernel from (5) in section 2.1 and how to calculate the
scaling of the learning curve stated in section 3.4.1.

a.1 large-p limit from heat kernel results

in chung and yau (1999) the authors considered heat kernels on graphs. the heat kernel for a graph
with normalised laplacian l (see section 2 in the main text) is given by

h = exp (   tl) ,

t > 0,

this is exactly the diffusion kernel given in (1) with t = 1
2   2. chung and yau (1999) gave results
for heat kernels on d-regular trees. their method involves treating the tree as a covering of a path,
which is closely related to the mapping onto a one-dimensional lattice discussed in section 2.1. the
eigenvectors and eigenvalues of l are then found within this mapping. if we call the unnormalised
, then
we can directly use the results of chung and yau (1999), simply by modifying the function that is

form of the random walk kernels we considered in the main text again   c =(cid:0)i     a   1l(cid:1)p
applied to each eigenvalue    of l from exp(   t  ) to (1      /a)p. this gives, for the unnormalised

1828

gaussian processes on random graphs

random walk kernel element between two vertices on the tree at distance l,

  c0,p =

  

z   
2(d     1)d
  (d     1)l/2   1 z   

2

0

0

  cl   1,p =

dx

[1    (1    2 cos(x)   d     1/d)/a]p sin2(x)
d2     4(d     1) cos2(x)
dx [1    (1    2 cos(x)   d     1/d)/a]p
  

sin(x)[(d     1) sin((l + 1)x)    sin((l     1)x)]

d2     4(d     1) cos2(x)

,

(31)

.

in the limit p        , the factor in square brackets in both expressions means that only values of x
up to o(p   1/2) contribute signi   cantly to the integral. one can then expand sin(x)     x linearly
everywhere, and similarly use cos(x)     1. calculating   cl,p in this way for p         gives the result
(5) for cl,p =   cl,p/   c0,p.

a.2 scaling form for large p

in section 3.4.1 we derived the learning curve decay due to the    rst example, given in (15). as
explained there, to understand how this behaves for large p one needs to know how cl,p approaches
its limiting form (5). in the outline derivation in the previous subsection we saw that in order to
obtain this limiting form one expands not just sin(x), but also sin((l    1)x) and sin((l + 1)x) linearly
because x = o(p   1/2) is small. for the two sine factors this will break down once l p   1/2 is of order
one, that is, for large enough l. this suggests looking at cl,p as function of l    = l p   1/2. to focus
on the relevant part of the integral one can similarly transform the integration variable to x    = xp1/2.
for p         at constant l   , the integral for   cl,p in (31) then becomes proportional to

(d     1)   l/2z    

0

dx    e   x   2   d   1/(da)x   (d     2) sin(l   x   ).

integration by parts now gives cl,p       cl,p     (d     1)   l/2l    exp(    (l   )2d
). comparing with the large-l
2   d   1
limit of (5), cl,p     (d     1)   l/2l   , shows that the effect of    nite p is contained in the exponential
(gaussian) cutoff factor which becomes signi   cant for l    of order unity, that is, l = o(p1/2), as
stated in the main text.

visually, the cutoff effects for large l and p can be seen more clearly by plotting not cl,p directly
but rather   rl,p, the unormalised version of rl,p as de   ned in section 3.4.1. as vl = d(d     1)l   1
for l     1, the additional    vl factor just removes the decay with (d     1)   l/2 from cl,p. from the
above discussion, we then expect to    nd for large l that   rl,p     l    exp(    (l   )2d
). a plot of numerical
2   d   1
results for a suitably normalised version of   rl,p (see below), plotted against l p   1/2 for increasing p,
is shown in figure 10. there is a clear trend towards the predicted asymptotic behaviour for large p
(dashed line).

it may be somewhat surprising that the cutoff lengthscale that appears in the analysis above is
l = o(p1/2), which for large p is much smaller than the typical kernel range p/a. to understand this
intuitively, one can go back to (6) for rl,p and use that this is essentially unnormalised diffusion.
taking   rl,p as the unnormalised version of rl,p again and letting   l,p =   rl,p     p with    = (1    
1/a) + 2   d     1/(ad) to re-establish normalisation, one    nds that   l,p evolves almost according to a
diffusion process in    time    p, except that id203 conservation is broken at l = 0 and l = 1. in the
(leaky) diffusion process   l,p the typical lengthscale l should then scale with time as p1/2, exactly

1829

urry and sollich

p
,
l

  
p

2.2
2.0
1.8
1.6
1.4
1.2
1.0
0.8
0.6
0.4
0.2
0

  

  

  
  

  
  

  
  

  
  
  

  
  

  
  
  
  

  
  
  

  
  
  

                                                                       
                                                              
                                                     
                                               
                                      

  
  
  
  

  
  
  

  
  

  
  

  

  

  
  
  
  
  

  
  
  
  
  
  
  
  
  

  
  
  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

  
  
  
  
  
  
  
  
  
  
  
  
  
  

  
  
  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

    

  
  
  
  
  
  
  
  

  
  
  
  
  
  
  
  
  
  
  
  
  
  

  

  
  
  
  
  
  
  
  

  
  
  
  
  
  
  
  
  
  
  
  

  

  
  
  
  
  
  
  

  
  
  
  
  
  
  
  

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

  
  

  
  
  

  
  
  
  

  
  
  
  
  
  

  
  
  
  
  
  
  

                                                                                                  
                                                                                   
                                                                          
                                                              
                                                        

  
  
  
  

  
  
  

  
  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  

  
  

  
  

  
  

  
  

  

  

  

  

  

  

  

  
  

  
  
  
  

0

0.2

0.4

0.6

0.8

1.0

1.4

1.6

1.8

2.0

1.2
l/p1/2

figure 10: plot demonstrating the p         scaling from of p  l,p, the normalised version of   rl,p.

dashed line shows the analytically found functional from of the scaling.

as we found above. this diffusion interpretation can also be used to reproduce the quantitative
scaling form, by adapting the methods of monthus and texier (1996) where the authors study a
one dimensional walk with a re   ective boundary to compute the large-p scaling. the normalised
version of   rl,p shown in figure 10 is in fact p  l,p.

appendix b. analysis of the mismatch learning curves

in this appendix we suggest a way of understanding the mismatch maximum seen in the learning
curves plotted in figure 7 (left) and (right) and figure 8 (left) and (right). we begin by considering
the mean of the gp posterior, that is, the prediction function. if we arrange the predictive means at
each vertex into a vector   f , then from (8) this can be written as

  f = kv k   1y,

where (kv ) j   = c j,x   for    = 1, . . . , n and j = 1, . . . ,v and k     = cx  ,x   +   2    ,   for   ,    = 1, . . . , n.
(recall that x   is the location of the   -th training input, which on a graph is just a label in the range
{1, . . . ,v}.) we can rewrite this in the form   f = m z where
m = kv k   1/2,

z = k   1/2y.

one sees that m represents teacher-independent aspects of   f . the vector of    pseudo-training out-
puts    z has been de   ned so that in the matched case, it obeys hzzti = i. we think of z as pseudo-
training outputs because if its components z   are sampled as independent and identically distributed
unit variance gaussian random variables, then   f = m z will have the correct distribution of mean

prediction vectors. the columns m1, . . . , mn of m =(cid:0)m1

1830

       mn(cid:1) represent the    effective

gaussian processes on random graphs

104

103

102

101

t
z
z

100
10   1
10   2
10   3
10   4
10   5
10   6
10   7
10   8

0

50

100

150

200

example

1.0

0.8

0.6

0.4

0.2

0.0

-0.2

figure 11: (left) black lines show diagonal elements of zzt for a typical draw from the erd  os-
r  enyi random graph ensemble with average degree 3 in a matched scenario with 250
vertices, p = 10, a = 2 and   2 = 0.0001. red lines show zzt for the case where student
is globally normalised and the teacher is locally normalised. (right) a plot of the    pre-
diction vector    for the largest spike in zzt (indicated by a green circle in the left hand
plot) under mismatch.

prediction vectors    conjugate to the z  . in the matched case each   f 2
of squares of the corresponding entries in the effective prediction vectors.

i will then be, on average, a sum

for the case of mismatch, for example, because student and teacher use differently normalised

kernels, the only change is in the statistics of z. their covariance matrix hzzti will no longer simply
be the identity matrix, and so act as a re-weighting of the prediction vectors. to understand our
numerical simulations, we considered a value of    around the maximum in the mismatched learning
curve, listed the largest (diagonal) entries of zzt and plotted their corresponding prediction vectors.
these prediction vectors were generally localised around dangling ends of the graph, substantiating
our claim in the main text that it is from these graph regions that the major contribution to the
learning curve maximum arises. typical plots of zzt and the prediction vector corresponding to the
largest spike in zzt for an instance of an erd  os-r  enyi graph with 250 vertices are shown in figures
11 and 12 for both a locally normalised teacher and globally normalised student and a globally
normalised teacher and locally normalised student respectively. both these plots clearly show the
largest spike corresponding to a prediction vector localised on a dangling edge in the graph. similar
plots of the other large spikes give prediction vectors localised on other dangling edges.

1831

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
urry and sollich

104

103

102

101

t
z
z

100
10   1
10   2
10   3
10   4
10   5
10   6
10   7
10   8

0

50

100

150

200

example

1.0

0.8

0.6

0.4

0.2

0.0

-0.2

figure 12: analogue of figure 11 for a locally normalised student and globally normalised teacher

references

s. amari, n. fujita, and s. shinomoto. four types of learning curves. neural comput., 4(4):

605   618, 1992.

c. bishop. pattern recognition and machine learning. springer, new york, ny, usa., 2nd

edition, 2007.

t. britton, m. deijfen, and a. martin-loeff. generating simple random graphs with prescribed

degree distribution. j. stat. phys., 124(6):1377   1397, 2006.

f. k. chung. spectral id207, volume 92 of regional conference series in mathematics.

american mathematical society, dec 1996.

f. k. chung and s. t. yau. coverings, heat kernels and spanning trees. electronic journal of

combinatorics, 6:r12, 1999.

n. cristianini and j. shawe-taylor. an introduction to support vector machines and other kernel-

based learning methods. cambridge university press, 2000.

p. erd  os and a. r  enyi. on random graphs, i. publicationes mathematicae (debrecen), 6:290   297,

1959.

j. freeman and d. saad. dynamics of on-line learning in radial basis networks. phys. rev. e, 56

(1):907   918, jul 1997.

m. g. genton. classes of kernels for machine learning: a statistics perspective. journal of machine

learning research, 2(2):299   312, 2002.

1832

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
gaussian processes on random graphs

w.w. hager. updating the inverse of a matrix. siam rev., 31(2):221   239, 1989.

m. s. handcock and m. l. stein. a bayesian analysis of kriging. technometrics, 35(4):403   410,

1993.

d. haussler, m. kearns, h. s. seung, and n. tishby. rigorous learning curve bounds from statistical

mechanics. mach. learn., 25(2   3):195   236, 1996.

j. p. c. kleijnen. kriging metamodeling in simulation: a review. european journal of operational

research, 192(3):707     716, 2009.

r. kondor and j. lafferty. diffusion kernels on graphs and other discrete structures. in c. sammut
and hoffmann a. g., editors, proceedings of the 19th international conference on machine
learning (icml), pages 315   322, san francisco, ca, usa., 2002. morgan kaufmann.

r. k  uhn and j. van mourik. spectra of modular and small-world matrices. j. phys. a math. gen.,

44(16):165205, 2011.

d. malzahn and m. opper. learning curves and bootstrap estimates for id136 with gaussian

processes: a statistical mechanics study. complexity, 8(4):57   63, 2003.

d. malzahn and m. opper. a statistical physics approach for the analysis of machine learning

algorithms on real data. j. stat. mech. theor. exp., 2005(11):p11001, 2005.

b. d. mckay. the expected eigenvalue distribution of a large regular graph. id202 and

its applications, 40:203   216, 1981.

j. r. meinhold and n. d. singpurwalla. understanding the kalman    lter. the american statistician,

37(2):123   127, 1983.

m. m  ezard and g. parisi. the bethe lattice spin glass revisited. eur. phys. j. b, 20(2):217   233,

2001.

m. m  ezard and g. parisi. the cavity method at zero temperature. j. stat. phys., 111(1   2):1   34,

2003.

m. m  ezard, g. parisi, and m. virasoro. spin glass theory and beyond, volume 9 of world scienti   c

lecture notes in physics. world scienti   c, singapore, 1987.

c. monthus and c. texier. random walk on the bethe lattice and hyperbolic brownian motion. j.

phys. a math. gen., 29:2399   2409, 1996.

k. r. m  uller, s. mika, g. r  atsch, k. tsuda, and b. sch  olkopf. an introduction to kernel-based

learning algorithms. ieee transactions on neural networks, 12(2):181   201, 2001.

r. m. neal. bayesian learning for neural networks, volume 118 of lecture notes in statistics.

springer-verlag, berlin, germany, 1996.

m. opper and d. haussler. bounds for predictive errors in the statistical mechanics of supervised

learning. phys. rev. lett., 75(20):3772   3775, 1995.

1833

urry and sollich

m. opper and d. malzahn. a variational approach to learning curves. in t. g. dietterich, s. becker,
and z. ghahramani, editors, advances in neural information processing systems, volume 14,
pages 463   469, cambridge, ma, usa., 2002. the mit press.

m. opper and f. vivarelli. general bounds on bayes errors for regression with gaussian processes.
in m. s. kearns, s. a. solla, and d a cohn, editors, advances in neural information processing
systems, volume 11, pages 302   308, cambridge, ma, usa., 1999. the mit press.

c. e. rasmussen and c. k. i. williams. gaussian processes for machine learning. mit press,

nov 2005.

t. rogers, c. vicente, k. takeda, and i. castillo. spectral density of random graphs with topological

constraints. j. phys. a math. gen., 43(19):195002, 2010.

h. s. seung, h. sompolinsky, and n. tishby. statistical mechanics of learning from examples.

phys. rev. a, 45(8):6056   6091, 1992.

a. smola and r. kondor. kernels and id173 on graphs. in b sch  olkopf and m. k. warmuth,

editors, lect. notes artif. int., volume 2777, pages 144   158, 2003.

p. sollich. learning curves for gaussian processes. in m. s. kearns, s. a. solla, and d a cohn,
editors, advances in neural information processing systems, volume 11, pages 344   350, cam-
bridge, ma, usa., 1999a. the mit press.

p. sollich. approximate learning curves for gaussian processes. in ninth international conference
on arti   cial neural networks (icann99), pages 437   442, edison, nj, usa., 1999b. institute
of electrical engineers inspec inc.

p. sollich. gaussian process regression with mismatched models. in s becker t g dietterich and
z ghahramani, editors, advances in neural information processing systems, volume 14, pages
519   526, cambridge, ma, usa., 2002. the mit press.

p. sollich and a. halees. learning curves for gaussian process regression: approximations and

bounds. neural comput., 14(6):1393   1428, 2002.

p. sollich and c. k. i. williams. using the equivalent kernel to understand gaussian process regres-
sion. in l. k. saul, y. weiss, and l. bottou, editors, advances in neural information processing
systems, volume 17, pages 1313   1320, cambridge, ma, usa., 2005. the mit press.

p. sollich, m. j. urry, and c. coti. kernels and learning curves for gaussian process regression
on random graphs. in y. bengio, d. schuurmans, j. lafferty, c. k. i. williams, and a. culotta,
editors, advances in neural information processing systems, volume 22, pages 1723   1731, red
hook, ny, usa., 2009. curran associates inc.

m. j. urry and p. sollich. exact learning curves for gaussian process regression on large random
graphs. in j. lafferty, c. k. i. williams, j. shawe-taylor, r.s. zemel, and a. culotta, editors,
advances in neural information processing systems, volume 23, pages 2316   2324, red hook,
ny, usa., 2010. curran associates inc.

1834

gaussian processes on random graphs

m. j. urry and p. sollich. replica theory for learning curves for gaussian processes on random

graphs. j. phys. a math. gen., 45(42):425005, 2012.

t. l. h. watkin, a. rau, and m. biehl. the statistical mechanics of learning a rule. rev. mod.

phys., 65(2):499   556, 1993.

c. williams and f. vivarelli. upper and lower bounds on the learning curve for gaussian processes.

mach. learn., 40:77   102, 2000.

1835

