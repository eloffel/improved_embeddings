journal of machine learning research 14 (2013) 771-800

submitted 4/11; revised 9/12; published 3/13

semi-supervised learning using greedy max-cut

jun wang
ibm t.j. watson research center
1101 kitchawan road
yorktown heights, ny 10598, usa

tony jebara
department of computer science
columbia university
new york, ny 10027, usa

shih-fu chang
department of electrical engineering

columbia university

new york, ny 10027, usa

editor: mikhail belkin

wangjun@us.ibm.com

jebara@cs.columbia.edu

sfchang@ee.columbia.edu

abstract

graph-based semi-supervised learning (ssl) methods play an increasingly important role in prac-
tical machine learning systems, particularly in agnostic settings when no parametric information
or other prior knowledge is available about the data distribution. given the constructed graph rep-
resented by a weight matrix, transductive id136 is used to propagate known labels to predict
the values of all unlabeled vertices. designing a robust label diffusion algorithm for such graphs
is a widely studied problem and various methods have recently been suggested. many of these
can be formalized as regularized function estimation through the minimization of a quadratic cost.
however, most existing label diffusion methods minimize a univariate cost with the classi   cation
function as the only variable of interest. since the observed labels seed the diffusion process, such
univariate frameworks are extremely sensitive to the initial label choice and any label noise. to
alleviate the dependency on the initial observed labels, this article proposes a bivariate formulation
for graph-based ssl, where both the binary label information and a continuous classi   cation func-
tion are arguments of the optimization. this bivariate formulation is shown to be equivalent to a
linearly constrained max-cut problem. finally an ef   cient solution via greedy gradient max-cut
(ggmc) is derived which gradually assigns unlabeled vertices to each class with minimum con-
nectivity. once convergence guarantees are established, this greedy max-cut based ssl is applied
on both arti   cial and standard benchmark data sets where it obtains superior classi   cation accu-
racy compared to existing state-of-the-art ssl methods. moreover, ggmc shows robustness with
respect to the graph construction method and maintains high accuracy over extensive experiments
with various edge linking and weighting schemes.

keywords:
programming, greedy max-cut

graph transduction, semi-supervised learning, bivariate formulation, mixed integer

1. introduction

in many real applications, labeled samples are scarce but unlabeled samples are abundant. paradigms
that consider both labeled and unlabeled data, that is, semi-supervised learning (ssl) methods, have

c(cid:13)2013 jun wang, tony jebara and shih-fu chang.

wang, jebara and chang

been increasingly explored in practical machine learning systems. while many semi-supervised
learning approaches estimate a smooth function over labeled and unlabeled examples, this article
presents a novel approach which emphasizes a bivariate optimization problem over the classi   ca-
tion function and the labels. prior to describing the method in detail, we brie   y mention other ssl
methods and previous work to motivate this article   s contributions.

one of the earliest examples of the empirical advantages of ssl was co-training, a method    rst
developed for id111 problems (blum and mitchell, 1998) and later extended in various forms
to other applications (chawla and karakoulas, 2005; goldman and zhou, 2000). therein, multi-
ple classi   ers are    rst estimated using conditionally independent feature sets of training data. the
performance advantages of this method rely heavily on the existence of independent and comple-
mentary classi   ers. theoretical results show that some mild assumptions on the underlying data
distribution are suf   cient for co-training to work (balcan et al., 2005; wang and zhou, 2010). how-
ever, performance can dramatically degrade if the classi   ers do not complement each other or the
independence assumption does not hold (krogel and scheffer, 2004). though co-training is concep-
tually similar to semi-supervised learning due to the way it incorporates unlabeled data, the classi   er
training procedure itself is often supervised.

the extension of traditional supervised support vector machines (id166s) to the semi-supervised
scenario is another widely used ssl algorithm. instead of maximizing separation (via a maximum-
margin hyperplane) over training data as in standard id166s, semi-supervised id166s (s3vms) es-
timate a hyperplane to balance maximum-margin partitioning of labeled data while encouraging
a separation through low-density regions of the data (vapnik, 1998). for example, transductive
support vector machines (tid166s) were developed as one of the earliest incarnations of semi-
supervised id166s (joachims, 1999).1 various optimization techniques have been applied to solve
s3vms (chapelle et al., 2008), resulting in a wide range of methods, such as low density separation
(chapelle and zien, 2005), semi-de   nite programming based methods (bie and cristianini, 2004;
xu et al., 2008), and a branch-and-bound based approach (chapelle et al., 2007).

another family of ssl methods known as graph-based approaches have recently become popu-
lar due to their high accuracy and computational ef   ciency. graph-based semi-supervised learning
(gssl) treats both labeled and unlabeled samples from a data set as vertices in a graph and builds
pairwise edges between these vertices which are weighted by the af   nity between the corresponding
samples. the small portion of vertices with labels are then used by ssl methods to perform graph
partition or information propagation to predict labels for unlabeled vertices. for instance, the graph
mincuts approach formulates the label prediction as a graph cut problem (blum and chawla, 2001;
blum et al., 2004). other gssl methods, like graph transductive learning, formulate the problem
as regularized function estimation over an undirected weighted graph. these methods optimize a
trade-off between the accuracy of the classi   cation function on labeled samples and a regulariza-
tion term that favors a smooth function. the weighted graph and the optimal function ultimately
propagate label information from labeled data to unlabeled data to produce transductive predic-
tions. popular algorithms for gssl include graph cuts (blum and chawla, 2001; blum et al., 2004;
joachims, 2003; kveton et al., 2010), graph id93 (azran, 2007; szummer and jaakkola,
2002), manifold id173 (belkin et al., 2005, 2006; sindhwani et al., 2008, 2005), and graph
id173 (zhou et al., 2004; zhu et al., 2003). comprehensive survey articles have also been
disseminated (zhu, 2005).

1. it is actually more appropriate to call this method a semi-supervised id166 since the learned classi   er is indeed

inductive (zhu and goldberg, 2009).

772

semi-supervised learning using greedy max-cut

for some synthetic and real data problems, gssl approaches do achieve promising perfor-
mance. however, previous research has identi   ed several realistic settings and labeling situations
where this performance can be compromised (wang et al., 2008b). in particular, both the graph
construction methodology and the label initialization conditions can signi   cantly impact prediction
accuracy (jebara et al., 2009). for a well-constructed graph such as the one shown in figure 1(a),
many gssl methods produce satisfactory predictions. however, for graphs involving non-separable
manifold structure as shown in figure 1(b), prediction accuracy may deteriorate. even if one as-
sumes that the graph structures used in the above methods faithfully describe the data manifold,
gssl algorithms may still be misled by problems in the label information. figure 3 depicts several
cases where the label information leads to invalid graph transduction solutions for all the aforemen-
tioned algorithms.

in order to handle such challenging labeling conditions, we    rst extend the existing gssl for-
mulation by casting it as a bivariate optimization problem over the classi   cation function and the
labels. then we demonstrate that minimizing the mixed bivariate cost function can be reduced to
a pure integer programming problem that is equivalent to a constrained max-cut problem. though
semi-de   nite programming can be used to obtain approximate solutions, these are impractical due
to scalability issues. instead, an ef   cient greedy gradient max-cut (ggmc) solution is developed
which remedies the instability previous methods seem to have vis-a-vis the initial labeling condi-
tions on the graph. in the proposed greedy solution, initial labels simply act as initial values of
the graph cut which is incrementally re   ned until convergence. during each iteration of the greedy
search, the optimal unlabeled vertex is assigned to the labeled subset with minimum connectivity
to maximally preserve cross-subset edge weight. finally, an overall cut is produced after placing
the unlabeled vertices into one of the label sets. it is then straightforward to obtain the    nal label
prediction from the graph cut result. note that this greedy gradient max-cut solution is equivalent
to alternating between minimization of the cost over the label matrix and minimization of the cost
over the prediction function. moreover, to alleviate dependencies on the initialization of the cut (the
given labels), a re-weighting of the connectivity between unlabeled vertices and labeled subsets is
proposed. this re-weighting performs a within-class id172 using vertex degree as well as a
between-class id172 using class prior information. we demonstrate that the greedy gradient
max-cut based graph transduction produces signi   cantly better performance on both arti   cial and
real data sets.

the remainder of this paper is organized as the follows. section 2 provides a brief background
of graph-based ssl and discusses some open issues. in section 3, we present our bivariate graph
transduction framework, followed by the theoretical proof of its equivalence with the constrained
max-cut problem in section 4.
in addition, a greedy gradient max-cut algorithm is proposed.
section 5 provides experimental validation for the algorithm on both toy and real classi   cation
data sets. comparisons with leading semi-supervised methods are made. concluding remarks and
discussions are then provided in section 6.

2. background and open issues

in this section, we introduce some notation and then revisit two critical components of graph-based
ssl: graph construction and label propagation. subsequently, we discuss some challenging issues
such as ssl   s sensitivity to graph construction and label initialization.

773

wang, jebara and chang

1.2

1

0.8

0.6

0.4

0.2

0

   0.2

   0.4

   0.6

   0.8

   1.5

1.2

1

0.8

0.6

0.4

0.2

0

   0.2

   0.4

   0.6

   0.8

   1.5

   1

   0.5

0

0.5

1

1.5

2

2.5

3

   1

   0.5

0

0.5

1

1.5

2

2.5

3

(a)

(b)

figure 1: examples of constructed k-nearest-neighbors (knn) graphs with k = 5 on the arti   cial
two moon data set for a) the completely separable case; and b) the non-separable case
with noisy samples.

2.1 notations

we    rst summarize the notation used in this article. assume we are given iid (independent and iden-
labeled samples {(x1, z1), . . . , (xl, zl)} as well as unlabeled samples
tically distributed)
{xl+1, . . . , xl+u} drawn from a distribution p(x, z). de   ne the set of labeled inputs as xl ={x1, . . . , xl}
with cardinality |xl| = l and the set of unlabeled inputs xu ={xl+1, . . . , xl+u} with cardinality |xu| =
u. the labeled set xl is associated with labels zl = {z1,       , zl}, where zi     {1,       , c}, i = 1, 2,       , l.
the goal of semi-supervised learning is to infer the missing labels {zl+1,       , zn} corresponding to
the unlabeled data {xl+1,       , xn}, where typically l << n (l + u = n). a crucial component of
gssl is the estimation of a weighted sparse graph g from the input data x = xl     xu. subse-
quently, a labeling algorithm uses g and the known labels zl = {z1, . . . , zl} to provide estimates
  zu = {  zl+1, . . . ,   zl+u} which try to approximate the true labels zu = {zl+1, . . . , zl+u} as measured by
an appropriately chosen id168.

in this article, assume the undirected graph converted from the data x is represented by
g = {x, e}, where the set of vertices is x = {xi} and the set of edges is e = {ei j}. each sample xi
is treated as a vertex and the weight of edge ei j is wi j. typically, one uses a id81 k(  ) over
pairs of points to compute weights. the weights for edges are used to build a weight matrix which
is denoted by w = {wi j}. similarly, the vertex degree matrix d = diag ([d1,       , dn]) is de   ned as
wi j. the graph laplacian is de   ned as           = d    w and the normalized graph laplacian is
di =

n
   
j=1

l = d   1/2         d   1/2 = i    d   1/2wd   1/2.

the graph laplacian and its normalized version can be viewed as operators on the space of functions
f which can be used to de   ne a id173 measure of smoothness over strongly-connected
regions in a graph (chung and biggs, 1997). for example, the smoothness measurement of functions
f using l over a graph is de   ned as

h f , l fi =    

i

   

j

f (xi)
   di    

wi j(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

774

2

.

f (x j)

   d j (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

semi-supervised learning using greedy max-cut

finally, the label information is formulated as a label matrix y = {yi j}     bn  c, where yi j = 1
if sample xi is associated with label j for j     {1, 2,       , c}, that is, zi = j, and yi j = 0 otherwise.
yi j = 1 are also
for single label problems (as opposed to multi-label problems), the constraints

c
   
j=1

imposed. moreover, we will often refer to row and column vectors of such matrices, for instance,
the i   th row and j   th column vectors of y are denoted as yi   and y   j, respectively. let f = f (x)
be the values of classi   cation function over the data set x. most of the gssl methods then use
the graph quantity w as well as the known labels to recover a continuous classi   cation function

f     rn  c by minimizing a prede   ned cost on the graph.

2.2 graph construction for semi-supervised learning

to estimate   zu ={  zl+1, . . . ,   zl+u} using g and the known labels zl ={z1, . . . , zl}, we    rst convert the
data points x = xl     xu into a graph g = {x, e, w}. this section discusses the graph construction
method, x     g , in detail. given input data x with cardinality |x| = l + u, graph construction
produces a graph g consisting of n = l + u vertices where each vertex is associated with the sample
xi. the estimation of g from x usually proceeds in two steps.

the    rst step is to compute a score between all pairs of vertices using a similarity function. this

creates a full adjacency matrix k     rn  n, where ki j = k(xi, x j) is computed using id81
k(  ) to measure sample similarity. subsequently, in the second step of graph construction, the matrix
k is sparsi   ed and reweighted to produce the    nal matrix w. sparsi   cation is important since it
leads to improved ef   ciency, better accuracy, and robustness to noise in the label id136 stage.
furthermore, the id81 k(  ) is often only locally useful as a similarity and does not recover
reliable weights between pairs of samples that are relatively far apart.

2.2.1 graph sparsification

starting with the fully connected matrix k, sparsi   cation removes edges by recovering a binary

matrix b     bn  n where bi j = 1 indicates that an edge is present between sample xi and x j, and
bi j = 0 indicates the edge is absent (assume bii = 0 unless otherwise noted). here we will primarily
investigate two graph sparsi   cation algorithms: neighborhood approaches including the k-nearest
and    neighbors algorithms, and matching approaches such as b-matching (bm) (edmonds and
johnson, 2003). all such methods operate on the matrix k or, equivalently, the distance matrix

h     rn  n obtained from k element-wise as hi j =pkii + k j j     2ki j.

sparsi   cation via neighborhood methods: there are two typical ways to build a neighborhood
graph: the   -neighborhood graph connecting samples within a distance of   , and the knn (k-nearest-
neighbors) graph connecting k closest samples. recent studies show the dramatic in   uences that
different neighborhood methods have on id91 techniques (carreira-perpin  an and zemel, 2005;
maier et al., 2009). in practice, the knn graph remains a more common approach since it is more
adaptive to scale variation and data density anomalies while an improper threshold value in the   -
neighborhood graph may result in disconnected components or subgraphs in the data set or even
isolated singleton vertices, as shown in figure 2(b). in this article, we often use knn neighborhood
graphs since the   -neighborhood graphs provide consistently weaker performance. in the remainder
of this article, we will use neighborhood and knn neighborhood graph interchangeably without
speci   c declaration.

775

wang, jebara and chang

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.1

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

(a)

(b)

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.1

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

(c)

(d)

figure 2: the synthetic data set used for demonstrating different graph construction approaches. a)
the synthetic data; b) the   -nearest neighbor graph; c) the k-nearest neighbor graph; d)
the b-matched graph.

more speci   cally, the k-nearest neighbor graph is a graph in which two vertices xi and x j are
connected by an edge if the distance hi j between xi and x j is within or equal k-th smallest among
the distances from xi to other samples in x. roughly speaking, the k-nearest neighbors algorithm
starts with a matrix   b of all zeros and for each point, searches for the k closest points to it (without
considering itself). if a point j is one of the k closest neighbors to i, then we set   bi j = 1. it is
straightforward to show that k-nearest neighbors search solves the following optimization problem:

  bi jhi j

(1)

   i j

min
  b   b
s.t.     j

  bi j = k,   bii = 0,    i, j     1, . . . , n.

the    nal solution of equation (1) is produced by symmetrizing   b as follows bi j = max(   bi j,   b ji).2
this greedy algorithm is in fact not solving a well de   ned optimization problem over symmetric
binary matrices. in addition, since it produces a symmetric matrix only via the ad hoc maximization
over   b and its transpose, the solution b it produces does not satisfy the equality    k bi j = k, but,
rather, only satis   es the inequality     j bi j     k.
ironically, despite conventional wisdom and the
nomenclature, the k-nearest neighbors algorithm is producing an undirected subgraph with more

2. it is possible to replace the maximization operator with minimization to produce a symmetric matrix, yet in the setting

b = min(   b,   b   ) the solution b only satis   es the inequality     j bi j     k and not the desired equality.

776

semi-supervised learning using greedy max-cut

than k neighbors for each vertex. this motivates researchers to investigate the b-matching algorithm
which actually achieves the desired output.

sparsi   cation via b-matching: the b-matching problem generalizes maximum weight match-
ing, that is, the linear assignment problem, where the objective is to    nd the binary matrix to mini-
mize the optimization problem

bi jhi j

(2)

min

b   b    i j
s.t.     j

bi j = b, bii = 0, bi j = b ji,    i, j     1, . . . , n.

achieving symmetry directly without post-processing. here, the symmetric solution is recovered
up-front by enforcing the additional constraints bi j = b ji. the matrix then satis   es the equality
    j bi j =    i bi j = b strictly. the solution to equation (2) is not quite as straightforward or ef   cient
as the greedy k-nearest neighbors algorithm. a polynomial time o(bn3) solution has been known,
yet recent advances show that much faster alternatives are possible via (guaranteed) loopy belief
propagation (huang and jebara, 2007).

compared with the neighborhood graphs, the b-matching graph is balanced or b-regular. in
other words, each vertex in the b-matched graph has exactly b edges connecting it to other vertices.
this advantage plays a key role when conducting label propagation on typical samples x which are
unevenly and non-uniformly distributed. our previous work applied b-matching to construct graphs
for semi-supervised learning tasks and demonstrated the superior performance over some unevenly
sampled data (jebara et al., 2009). for example, in figure 2, this data set clearly contains two
clusters of points, a dense gaussian cluster surrounded by a ring cluster. furthermore, the cluster
data is unevenly sampled; one cluster is dense and the other is fairly sparse. in this example, the k-
nearest neighbor graph constantly generates many cross-cluster edges while b-matching ef   ciently
alleviates this problem by removing most of the improper edges. the example clearly shows that
the b-matching technique produces regular graphs which could overcome the drawback of cross-
structure linkages often generated by nearest neighbor methods. this intuitive study con   rms the
importance of graph construction methods and advocates b-matching as a valuable alternative to
k-nearest neighbors, a method that many practitioners expect to produce regular undirected graphs,
though in practice often generates irregular graphs.

2.2.2 graph edge re-weighting

once a graph has been sparsi   ed and a binary matrix b is computed and used to delete unwanted
edges, several procedures can then be used to update the weights in the matrix k to produce a    nal
set of edge weights w. speci   cally, whenever bi j = 0, the edge weight is also wi j = 0; however,
bi j = 1 implies that wi j     0. two popular approaches are considered here for estimating the non-
zero components of w.
binary weighting: the simplest approach for building the weighted graph is the binary weight-
ing approach, where all the linked edges in the graph are given the weight 1 and the edge weights
of disconnected vertices are given the weight 0. in other words, this setting simply uses w = b.
however, this uniform weight on graph edges can be sensitive, particularly if some of the graph
vertices were improperly connected by the sparsi   cation procedure (either the neighborhood based
procedures or the b-matching procedure).

gaussian kernel weighting: an alternative approach is gaussian kernel weighting which is
often applied to modulate sample similarity. therein, the edge weight between two connected

777

wang, jebara and chang

samples xi and x j is computed as:

wi j = bi j exp(cid:18)   

d2(xi, x j)

2  2 (cid:19) ,

where the function d(xi, x j) evaluates the dissimilarity of samples xi and x j, and    is the kernel
bandwidth parameter. there are many choices for the distance function d(  ) including any    p dis-
tance,   2 distance, and cosine distance (zhu, 2005; belkin et al., 2005; jebara et al., 2009).

this    nal step in the graph construction procedure ensures that the unlabeled data x has now
been converted into a graph g with a weighted sparse undirected adjacency matrix w. given this
graph and some initial label information yl, any of the current popular algorithms for graph based
ssl can be used to solve the labeling problem.

2.3 univariate graph id173 framework

given the constructed graph g = {x, e}, whose geometric structure is represented by the weight
matrix w, the label id136 task is to diffuse the known labels zl to all the unlabeled vertices
xu in the graph and estimate   zu. designing a robust label diffusion algorithm for such graphs is a
widely studied problem (chapelle et al., 2006; zhu, 2005; zhu and goldberg, 2009).

here we are particularly interested in a category of approaches, which estimate the prediction

function f    rn  c by minimizing a quadratic cost de   ned over the graph. the cost function typically

involves a trade-off between the smoothness of the function over the graph of both labeled and
unlabeled data (consistency of the predictions on closely connected vertices) and the accuracy of the
function at    tting the label information on the labeled vertices. approaches like the gaussian    elds
and id94 (gfhf) method (zhu et al., 2003) and the local and global consistency
(lgc) method (zhou et al., 2004) fall into this category, so does our previous method of graph
transduction via alternating minimization (wang et al., 2008b).

both lgc and gfhf de   ne a cost function q that involves the combined contribution of two
penalty terms: the global smoothness qsmooth and local    tting accuracy q f it . the    nal prediction
function f is obtained by minimizing the cost function as:

f    = arg min
f   rn  c

q (f) = arg min
f   rn  c

(qsmooth(f) + q f it(f)) .

(3)

a natural formulation of the above cost function is lgc (zhou et al., 2004) which uses an elastic
regularizer framework as follows

q (f) = kfk2

g +

  

2kf    yk2.

(4)

the    rst term kfk2

g represents function smoothness over graph g and kf    yk2 measures the em-

pirical loss on given labeled samples. speci   cally, in lgc, the function smoothness is de   ned using
the semi-inner product

qsmooth = kfk2

g =

1
2hf, lfi =

1

2

tr(f   lf).

note that the coef   cient    in equation (4) balances global smoothness and local    tting terms.
if we set    =     and use a standard graph laplacian quantity           for the smoothness term, the above

778

semi-supervised learning using greedy max-cut

framework reduces to the harmonic function formulation (zhu et al., 2003). more precisely, the cost
function only preserves the smoothness term as

q (f) = tr(f            f).

(5)

meanwhile, the harmonic function f minimizing the above cost also satis   es two conditions:

=          fu = 0,

   q
   f u
fl = yl,

where fl, fu are the function values of f (  ) over labeled and unlabeled vertices, that is, fl = f (xl),
fu = f (xu), and f = [fl fu]   . the    rst equation above denotes the zero derivative of the object
function on the unlabeled data and the second equation clamps the function value on the given label
value yl. both lgc and gfhf are univariate id173 frameworks where the continuous pre-
diction function is treated as the only variable in the optimization procedure. the optimal solutions
for equation (4) and equation (5) are easily obtained by solving a linear system.

2.4 open issues

existing graph-based ssl methods hinge on having good label information and an appropriately
constructed graph (wang et al., 2008b; liu et al., 2012). but the heuristic design of the graph may
result in suboptimal id136. in addition, the label propagation procedure can easily be misled if
there exist excessive noise or outliers in the initial labeled set. finally, in iid settings, the difference
between empirically estimated class proportions and their true expected value is bounded (huang
and jebara, 2010). however, practical annotation procedures are not necessarily iid and labeled
data may have empirical class frequencies that deviate signi   cantly from the expected class ratios.
these degenerate situations seem to plague real world problems and compromise the performance
of many state-of-the-art ssl algorithms. we next discuss some open issues which occur often in
graph construction and label propagation, two critical components of all gssl algorithms.

2.4.1 sensitivity to graph construction

as shown in figure 1(a), a well-built graph obtained from separable manifolds of data will achieve
good results with most existing gssl approaches. however, practical applications often produce
non-separable graphs as shown in figure 1(b). in addition to the widely used knn graph, we showed
that b-matching could be used successfully for graph construction (jebara et al., 2009). but both
knn graphs and b-matched graphs are heuristics and require the careful selection of the parameter
k or b which controls the number of links incident to each vertex in the graph. moreover, edge
reweighing on the sparse graph often also requires exploration forcing the user to select kernels and
various kernel parameters. all these heuristic steps in graph design require extra effort from the
user and demand some level of familiarity with the data domain.

2.4.2 sensitivity to label noise

most of the existing gssl methods are based on an univariate quadratic id173 framework
which relies heavily on the quality of the initially assigned labels. for certain synthetic and real
data problems, such graph transduction approaches achieve promising performance. however, sev-
eral realistic labeling conditions produce unsatisfactory performance (wang et al., 2008b). even if

779

wang, jebara and chang

prediction     positive
prediction     negative

 

 

   1

0

1

2

(b)

prediction     positive
prediction     negative

 

   1

0

1

2

(e)

prediction     positive
prediction     negative

3

 

3

 

 

   1

0

1

2

(h)

3

 

prediction     positive
prediction     negative

 

unlabeled samples
labeled     positve
labeled     negative

 

   1

0

1

2

(a)

prediction     positive
prediction     negative

 

   1

0

1

2

(d)

unlabeled samples
labeled     positve
labeled     negative

3

 

3

 

 

   1

0

1

2

(g)

3

 

prediction     positive
prediction     negative

1

0.5

0

   0.5

1

0.5

0

   0.5

1

0.5

0

   0.5

1

0.5

0

   0.5

1

0.5

0

   0.5

1

0.5

0

   0.5

1

0.5

0

   0.5

1

0.5

0

   0.5

1

0.5

0

   0.5

1

0.5

0

   0.5

1

0.5

0

   0.5

1

0.5

0

   0.5

prediction     positive
prediction     negative

 

 

   1

0

1

2

(c)

prediction     positive
prediction     negative

 

   1

0

1

2

(f)

prediction     positive
prediction     negative

3

 

3

 

 

   1

0

1

2

(i)

3

 

prediction     positive
prediction     negative

 

   1

0

1

2

3

 

   1

0

1

2

3

 

   1

0

1

2

3

(j)

(k)

(l)

figure 3: examples illustrating the sensitivity of graph-based ssl to adverse labeling conditions.
particularly challenging conditions are shown in (a) where an uninformative label on an
outlier sample is the only negative label (denoted by a black circle) and in (g) where
imbalanced labeling is involved. prediction results are shown for the gfhf method (zhu
et al., 2003) in (b) and (h), the lgc method (zhou et al., 2004) in (c) and (i), the lapid166
method (belkin et al., 2006) in (d) and (j), the tid166 method (joachims, 1999) in (e) and
(k); and our method in (f) and (l).

the graph is perfectly constructed from the data, problematic initial labels under practical situations
can easily deteriorate the performance of ssl prediction. figure 3 provides examples depicting
imbalanced and noisy labels that lead to invalid graph transduction solutions for all the aforemen-
tioned algorithms. the    rst labeling problem involves uninformative labels (figure 3(a)). the only

780

semi-supervised learning using greedy max-cut

negative label (dark circle) is located in an outlier region where the low density connectivity limits
its diffusion to the rest of the graph. the leading ssl methods classify the majority of unlabeled
nodes in the graph as positive (figure 3(b)-figure 3(e)). such conditions are frequent in real prob-
lems like content-based id162 (cbir) where the visual query example is not necessarily
representative of the class. another dif   cult case is due to imbalanced labeling. there, the ratio
of training labels is disproportionate to the underlying class proportions. for example, figure 3(g)
depicts two half-circles with an almost equal number of samples. however, since the training labels
contain three negative samples and only one positive example, the ssl predictions are strongly bi-
ased towards the negative class (see figures 3(h) to 3(k)). this imbalanced labeling situation occurs
frequently in realistic problems such as the annotation of microscopic images (wang et al., 2008a).
therein, the human labeler favors certain cellular phenotypes due to domain-speci   c biological hy-
potheses. to tackle these issues, we next propose a novel bivariate framework for graph-based ssl
and describe an ef   cient algorithm that achieves it via alternating minimization.

3. bivariate framework for graph-based ssl

we    rst propose an extension to the existing graph id173-based ssl formulations by casting
the problem as a bivariate optimization over both the classi   cation function and the unknown labels.
then we demonstrate that the minimization of this bivariate cost reduces to a linearly constrained
binary integer programming (bip) problem. this problem can be approximated via semi-de   nite
programming yet this approach is impractical due to scalability issues. we instead explore a fast
method which alternates minimization of the cost over the label matrix and the prediction function.

3.1 the cost function

recall the univariate id173 formulation for graph-based ssl in equation (3). also note that
the optimization problem in existing approaches such as lgc and gfhf can be broken up into
separate parallel problems since the cost function decomposes into additive terms that only depend
on individual columns of the prediction matrix f (wang et al., 2008a). such a decomposition reveals
that biases may arise if the input labels are disproportionately imbalanced. in addition, when the
graph contains background noise and makes class manifolds non-separable (as in figure 1(b)), these
existing graph transduction approaches fail to output reasonable classi   cation results.

since the univariate framework treats the initial label information as a constant, we propose a
novel bivariate optimization framework that explicitly optimizes over both the classi   cation function
f and the binary label matrix y:

(f   , y   )

= arg minf   rn  c,y   bn  cq (f, y)
s.t. yi j     {0, 1},
yi j = 1,

   c
yi j = 1, for zi = j,

j=1

j = 1,       , c.,

where bn  c is the set of all binary matrices y of size n   c. for a labeled sample xi     xl, yi j = 1
if zi = j, and the constraint    c
j=1 yi j = 1 indicates that this a single label prediction problem. we
specify the cost function as

q (f, y) =

1

2

tr(cid:16)f   lf +   (f    y)   (f    y)(cid:17) .

781

(6)

wang, jebara and chang

finally, rewriting the cost as a summation (zhou et al., 2004) reveals a more intuitive formulation
where

q (f, y) =

1

2

n

n

   
i=1

   
j=1

wi j(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

+

  

2

n

i=1kfi       yi  k2 .
   

fi     di    

2

f j  

pd j(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

3.2 reduction to a univariate problem

in the new graph id173 framework proposed above, the cost function involves two variables
to be optimized. simultaneously recovering both solutions is intractable due to the mixed integer
programming problem over binary y and continuous f. to solve the issue, we    rst show how to
reduce the original mixed problem to a univariate optimization problem with respect to the label
variable y.

f optimization step:

in each loop with y    xed, the classi   cation function f     rn  c is continuous and the cost function

is convex, allowing the minimum to be recovered by setting the partial derivative to zero:

where we denote the p matrix as

= 0 =    lf    +   (f        y) = 0

   q
   f   
=    f    = (l/   + i)   1y = py,

p = (l/   + i)   1,

(7)

and name it the propagation matrix since it is used to derived a prediction function f given a label
matrix y. because the graph is often symmetric, it is easy to show that the graph laplacian l and
the propagation matrix p are both symmetric.

y optimization step:

next replace f in equation (6) by its optimal value f    from the solution of equation (7). this yields

q (y) =

=

1

2
1

2

tr(y   p   lpy +   (py     y)   (py    y))
tr(cid:16)y   hp   lp +   (p        i)(p    i)i y(cid:17) =

1

2

tr(cid:16)y   ay(cid:17) ,

where we group all the constant parts in the above equation and de   ne

a = p   lp +   (p        i)(p    i) = p   lp +   (p    i)2.

the    nal optimization problem becomes

1

y    = arg min
s.t. yi j     {0, 1},
yi j = 1,

2

tr(cid:16)y   ay(cid:17)
j = 1,       , c

    j
yi j = 1, for zi = j,

j = 1,       , c.

782

(8)

semi-supervised learning using greedy max-cut

the    rst constraint produces a binary integer problem and the second one     j yi j = 1 produces a
single assignment constraint, that is, each vertex can only be assigned one class label. the third
group of constraints encodes the initial label information in the variable y. since the binary matrix

y     bn  c is subject to linear constraints of the form     j yi j = 1 and initial labeling conditions, the

optimization in equation (8) requires solving a linearly constrained binary integer programming
(bip) problem which is np hard (cook, 1971; karp, 1972).

3.3 incorporating label id172

a straightforward approach to solving the minimization problem in equation (8) is to use the gradi-
ent to greedily update the label variable y. however, this may produce biased classi   cation results
in practice since, at each iteration, the class with more labels will be preferred and will propagate
more quickly to the unlabeled examples. this arises in practice (as in figure 3) and is due to the
fact that y starts off sparse and contains many unknown entries. to compensate for this bias during
label propagation, we propose using a normalized label variable   y =       y for computing the cost
function in equation (6) as

q =

=

1

2
1

2

tr(cid:16)f   lf +   (f      y)   (f      y)(cid:17)
tr(cid:16)f   lf +   (f          y)   (f          y)(cid:17) .

(9)

the diagonal matrix        = diag(      ) = diag([  1,       ,   n]) is introduced to re-weight or re-balance the
in   uence of labels from different classes as it modulates the label importance based on node degree.
the value of   i (i = 1,       , n) is computed using the vertex degree di and label information

  i =( p j   

di

   k yk jdk

:

yi j = 1

0 : otherwise,

(10)

c
   
j=1

p j = 1. the value of p j can be

where p j is the prior of class j and is subject to the constraint

either estimated from the labeled training set or simply set to be uniform p j = 1/c ( j = 1,        , c)
in agnostic situations (when no better prior is available or if the labeled data is plagued by biased
sampling). using the normalized label matrix   y in the bivariate formulation allows labeled nodes
with high degrees to contribute more during the label propagation process. however, the total
diffusion of each class is kept equal (for agnostic settings with no priors available) or proportional
to the class prior (for the setting with prior information). therefore, the in   uence of different classes
is balanced even if the given class labels are imbalanced. if class proportion information is known,
it can be integrated by scaling the diffusion with the appropriate prior. in other words, the label
id172 attempts to enforce simple concentration inequalities which, in the iid case require
the predicted label results to concentrate around the underlying class ratios (huang and jebara,
2010). this intuition is in line with prior work that uses class proportion information in transductive
id136 where class proportion is enforced as a hard constraint (chapelle et al., 2007) or as a
regularizer (mann and mccallum, 2007).

3.4 alternating minimization procedure

to solve the above re   ned problem, we proposed an alternating minimization algorithm (wang et al.,
2008b). brie   y, starting with equation (9) and repeating the similar derivation as in section 3.2, we

783

(11)

(12)

wang, jebara and chang

obtain the optimal solution f    and the    nal cost function with respect to label variable y as

f    = p   y = p      y,

q =

1

2

tr(cid:16)   y   a   y(cid:17) =

1

2

tr(cid:16)y         a      y(cid:17) .

instead of    nding the global optimum y   , we only take an incremental step in each iteration to
modify a single entry in y. namely in each iteration, we    nd the optimal position (i   , j   ) in the
matrix y and change the binary value of yi    j    from 0 to 1. to do this, we    nd the direction with the
largest negative gradient guiding our choice of binary step on y. speci   cally, we evaluate k    qyk
and    nd the largest negative value to determine (i   , j   ).
note that the setting yi    j    = 1 is equivalent to modifying the normalized label matrix   y by setting
  yi   , j    =   i   , 0 <   i    < 1, and y,   y can be converted from each other componentwise. thus, the greedy
optimization of q with respect to y is equivalent to greedy minimization of q with respect to   y.
more formally, we derive the gradient of the above id168       yq =    q
and recover it with
      y
respect to y as:

   q
      y

= a   y = a      y.

(13)

as described earlier, we search the gradient matrix       yq to    nd the minimal element

(i   , j   ) = arg minxi   xu,1    j   c      yi j q .

because of the binary nature of y, we simply set yi    j    = 1 instead of making a continuous
update. accordingly, the node weight matrix       t+1 can be recalculated with the updated yt+1 in
the iteration t + 1. the update of y is greedy and thus it could backtrack from predicted labels in
previous iterations without convergence guarantees. we propose a straightforward way to guarantee
convergence and avoid backtracking or unstable oscillation in the greedy propagation process: once
an unlabeled point has been labeled, its labeling can no longer be changed. thus, we remove the
most recently labeled point (i   , j   ) from future consideration and conduct search over the remaining
unlabeled data only. in other words, to avoid retroactively changing predicted labels, the labeled
vertex xi    is removed from xu and added to xl.

note that although the optimal f    can be computed using equation (11), this need not be done
explicitly. instead, the new value is implicitly used in equation (14) only to update y. in the fol-
lowing, we summarize the update rules from step t to t + 1 in the alternating minimization scheme.

. compute gradient matrix:

(      yq )t = a   yt = a      t yt,       t = diag(      t).

. update one label:

(i   , j   ) = arg minxi   xu,1    j   c(      yi j q )t,

yt+1
i    j    = 1.

784

semi-supervised learning using greedy max-cut

. update label id172 matrix:

  t+1 = (

dii
   k yt+1
k j dkk

:

yt+1
i j = 1

0 : otherwise.

. update the list of labeled and unlabeled data:

xt+1
l        xt

l + xi    ; xt+1

u        xt

u     xi   .

starting with a few given labels, the method iteratively and greedily updates the label matrix y
to derive new labels in each iteration. the newly obtained labels are then use in the next iteration.
notice that the label id172 vector is re-computed for each iteration due to the change of
label set. although the original objective is formed in a bivariate manner in equation 6, the above
alternating optimization procedure drives the prediction of new labels without explicitly calculating
f    as is done in other graph transduction methods like lgc and gfhf. this unique feature makes
the proposed algorithm very ef   cient since we only update the gradient matrix       yq for prediction
new labels in each iteration.

due to the greedy assignment step and the lack of back-tracking, the algorithm can repeat the
alternating minimization (or the gradient computation) at most n     l times. each minimization
step over f and y requires o(n2) complexity and, thus, the total complexity of the above greedy
algorithm is o(n3). however, the update of the graph gradient can be done ef   ciently by modifying
only a single entry in y per iteration. this further reduces the computational cost down to o(n2).
empirically, the value of the id168 q decreases rapidly in the the    rst dozen iterations and
steadily converges afterward (wang et al., 2009). this phenomenon indicates that early stopping
strategy could be applied to speed up the training and prediction (melacci and belkin, 2011). once
the    rst few iterations are completed, the new labels are added and the standard propagation step can
be used to predict the optimal f    as indicated in equation (11) over the whole graph in one step. the
details of the algorithm, namely graph transduction via alternating minimization, can be referred to
wang et al. (2008b). in the following section, we provide a greedy max-cut based solution, which
essentially interprets the above alternating minimization procedure from a graph cut view.

4. greedy max-cut for semi-supervised learning

in this section, we introduce a connection between the proposed bivariate graph transduction frame-
work and the well-known maximum cut problem. then, a greedy gradient based max-cut solution
will be developed and related to the above alternating minimization algorithm.

4.1 equivalence to a constrained max-cut problem

recall the optimization problem de   ned in equation (8) which is exactly a linearly constrained
binary integer programming (bip) problem. in the case of a two-class problem, this optimization
will be reduced to a weighted max-cut problem over the graph ga = {x, a} subject to linear
constraints. the cost function in equation (8) can be rewritten as

q (y) =

1

2

tr(cid:16)y   ay(cid:17) =

1

2

tr(cid:16)ayy   (cid:17) =

1

2

tr (ar) ,

785

wang, jebara and chang

where a = {ai j} and r = yy   . considering the constraints     j yi j = 1 and y     bn  2 for a two-class

problem, we let

y = [y e   y],

where y     bn (i.e., y = {yi}, yi     {0, 1}, i = 1,       , n) and e = [1, 1,       , 1]    are column vectors. then

rewrite r as

r = yy    = [y e   y][y e   y]   

= ee        y(e        y   )    (e    y)y   .

(14)

now rewrite the cost function in equation (14) by replacing r with equation (14)

q (y) =

1

2

tr(cid:16)ahee        y(e        y   )    (e    y)y   i(cid:17) .

since ee    is the all-ones matrix, we obtain

1

2

tr(cid:16)aee   (cid:17) =

1
2    

i

   

j

ai j.

it is easy to show that a is symmetric and

next, simplify the cost function q as

y(e        y   ) = [(e    y)y   ]   .

q (y) =

=

1

2
1

2

tr(cid:16)aee   (cid:17)    trh(e        y   )ayi
tr(cid:16)aee   (cid:17)    y   a(e    y).

since the    rst part is a constant, the optimal value y    of the above minimization problem is the
argument of the maximization problem

y    = arg min
y

q (y) = arg max

y

y   a(e    y).

de   ne a new function f (y) as

f (y) = y   a(e    y).

again, the variable y     bn is a binary vector and e = [1, 1,       , 1]    is the unit column vector.
now we show that maximization of the above function maxy f (y) is exactly a max-cut problem if
we treat the symmetric matrix a as the weighted adjacency matrix of an undirected graph ga =
{va, a}. note that the diagonal elements of a could be non-zero aii 6= 0, i = 1, 2,       , n, which
indicates the undirected graph ga has self-connected nodes. assume a = a0 + a  , where a0
is the matrix obtained by zeroing the diagonal elements of a and a   is a diagonal matrix with
a  
i j = 0, i, j = 1, 2,       , n, i 6= j. it is straightforward to show that the the function f (y)
can be written as

ii = aii, a  

f (y) = y   (a0 + a  )(e    y) = y   a0(e    y).

786

semi-supervised learning using greedy max-cut

in other words, the non-zero elements in a do not affect the value of f (y). therefore, in the rest
of this article, we can assume that the matrix a has zero diagonal elements unless the text speci   es
otherwise.

since y = {y1, y2,       , yn} is a binary vector, each setting of y partitions the vertex set va in the
graph ga into two disjoint subsets (s1, s2). in other words, the two subsets s1 = {vi|yi = 1} and
s2 = {vi|yi = 0} satisfy s1    s2 = va and s1    s2 = /0. the maximization problem can then be written
as

max f (y) = max   
i, j

ai j    yi(1    y j) = max

ai j.

1
2    
vi   s1
v j   s2

because each binary vector y resulting in a partition (s1, s2) over the graph ga and f (y) is a corre-
sponding cut, the above maximization max f (y) is easily recognized as a max-cut problem (deza
and laurent, 2009). however, in graph based semi-supervised learning, the variable y is partially
speci   ed by the initial label values. this given label information can be interpreted as a set of linear
constraints on the max-cut problem. thus, the optimal solution can achieved by solving a linearly
constrained max-cut problem (karp, 1972). in addition, we also show that a multi-class problem
equals a max k-cut problem (k = c) (refer to appendix a). note that previous work used min-cut
over the original data graph g = {x, w} to perform semi-supervised learning (blum and chawla,
2001; blum et al., 2004). a key difference of the above formulation lies in the fact that we perform
max-cut over the transformed graph ga = {x, a}.
however, since there is no guarantee that the weights on the graph ga are non-negative, so-
lutions to the max-cut problem can be dif   cult to    nd (barahona et al., 1988). therefore, in the
following subsection, we will propose a gradient greedy solution to ef   ciently solve the above
max-cut problem, which can be treated as a different view of the previous alternating minimization
solution.

4.2 label propagation by gradient greedy max-cut

for the standard max-cut problem, many approximation techniques have been developed, including
the most remarkable goemans-williamson algorithm using semide   nite programming (goemans
and williamson, 1994, 1995). however, applying these guaranteed approximation schemes to solve
the constrained max-cut problem for y mentioned above is infeasible due to the constraints on
initial labels. furthermore, there is no guarantee that all edge weights ai j of the graph ga are
non-negative, a fundamental requirement in solving a standard max-cut problem (goemans and
williamson, 1995). instead, here we use a greedy gradient based strategy to    nd local optima by
assigning each unlabeled vertex to the label set with minimum connectivity to maximize cross-set
edge weights iteratively.

the greedy max-cut algorithm randomly selects unlabeled vertices and places each of them into
the appropriate class subset depending on the edges between this unlabeled vertex and the vertices
in the labeled subset. given the label information, the initial label set for class j can be constructed
as s j = {xi|yi j = 1} or s j = {xi|zi = j}, i = 1, 2,       , n; j = 1, 2,       , c. de   ne the following as the
connectivity between unlabeled vertex xi and labeled subset s j

ci j =

n

   
m=1

aimym j = ai.y. j,

787

(15)

wang, jebara and chang

algorithm 1 greedy max-cut for label propagation

input: the graph ga = {x, a}, the given labeled vertex xl, and initial labels y;
initialization:
obtain the initial cut {s j} by assigning the labeled vertex xl to each subset:

s j = {xi|yi j = 1}, j = 1, 2,       , c

unlabeled vertex set xu = x\ xl;
repeat

randomly select an unlabeled vertex xi     xu
compute the connectivity ci j, j = 1, 2,       , c
place the vertex to the labeled subject s j    :

j    = arg min j ci j

add xi to xl: xl        xl + xi;
remove xi from xu: xu        xu     xi;

until xu = /0
output: the    nal cut and the corresponding labeled subsets s j, j = 1, 2,       , c

where ai. is the i   th row vector of a and y. j is the j   th column vector of y. intuitively, ci j represents
the sum of edge weights between vertex xi and label set s j given the graph ga with edge weights a.
based on this de   nition, a straightforward local search for the maximum cut involves placing each
unlabeled vertex xi     xu in the labeled subset s j with minimum connectivity ci j to maximize the
cross-set edge weights as shown in algorithm (1). in order to achieve a good solution algorithm (1)
should be run multiple times with different random seeds after which the best cut overall is output
(mathieu and schudy, 2008).

while the above method is computationally cumbersome, it still does not resolve the issue of
undesired local optima and may generate biased cuts. according to the de   nition in equation (15),
the initialized labels determine the connectivity between unlabeled vertices and labeled subsets. if
the computed connectivity is negative, the above random search will prefer assigning unlabeled
vertices to the label set with the most labeled vertices which results in biased partitioning. such
biased partitioning also occurs in minimum cut problems over an undirected graph with positive
weights (shi and malik, 2000). other label initialization problems may also produce a poor cut.
for example, the numbers of labels from different classes may deviate from the underlying class
proportions. alternatively, the labeled vertices may be outliers and lie in regions of the graph
with low density. such labels often lead to weak label prediction results (wang et al., 2008a).
furthermore, the algorithm   s random selection of an unlabeled vertex results in unstable predictions
since the chosen unlabeled vertex xi could have equally low connectivity to multiple label subsets
s j.

to address the aforementioned issues, we    rst modify the original de   nition of connectivity to

alleviate label imbalance across different classes. a weighted connectivity is computed as

ci j = p j   

n

   
m=1

  maimym j = p j          ai.y. j.

(16)

the diagonal matrix        = diag([  1,   2,        ,   n] is called the label weight matrix as in equation (10)

  i =(cid:26) di/ds j

:
0 :

if xi     s j, j = 1,       , c
otherwise,

788

semi-supervised learning using greedy max-cut

algorithm 2 greedy gradient based max-cut for label propagation

input: the graph ga = {x, a} and the given labeled vertex xl, and initial label y;
initialization:
obtain the initial cut {s j} through assigning the labeled vertex xl to each subset:

s j = {xi|yi j = 1}, j = 1, 2,       , c

unlabeled vertex set xu = x\ xl;
repeat

for all j = 0 to |xu| do

compute weighted connectivity:

ci j =

n
   
k=1

  iaikyk j, xi     xu, j = 1,       , c

end for
update the cut {si} by placing the vertex xi    to the s j       th subset:

(i   , j   ) = arg min
i, j,xi   xu

ci j

add xi to xl: xl        xl + xi;
remove xi from xu: xu        xu     xi;

until xu = /0
output: the    nal cut and the corresponding labeled subsets s j, j = 1, 2,       , c

where ds j =    xm   s j dm is the sum of the degrees of the vertices in the label set s j. this heuristic
setting weights the importance of each label based on degree which alleviates the adverse impact of
outliers. if we ignore class priors, this de   nition of        coincides with the one in equation (10).

finally, to handle any instability due to the random search algorithm, we propose a greedy
gradient search approach where the most bene   cial vertex is assigned to the label set with minimum

connectivity. in other words, we    rst compute the connectivity matrix c = {ci j}     rn  c that gives

the connectivity between all unlabeled vertices to existing label sets

then we examine c to identify the element (i   , j   ) with minimum value as

c = a      y.

(i   , j   ) = arg min
i, j:xi   xu

ci j.

this means that the unlabeled vertex xi    has the least connectivity with label set s j    . then, we
update the labeled set s j    by adding vertex xi    as one greedy step to maximize the cross-set edge
weights. this greedy search can be repeated until all the unlabeled vertices are assigned to labeled
sets. in each iteration of the greedy cut process, the weighted connectivity of all unlabeled vertices
to labeled sets is re-computed. then the vertex with minimum connectivity is placed in the proper
labeled set. the algorithm is summarized in algorithm (2).

the connectivity matrix c can also be viewed as the gradient of the cost function q in equa-
tion (12) with respect to   y. this is precisely the same setting used in equation (13) of the alternating
minimization algorithm

c =

   q
      y

= a      y.

789

wang, jebara and chang

we name the algorithm greedy gradient max-cut (ggmc) since, in the greedy step, the unla-
beled vertices are assigned labels in a manner that reduces the value of q along the direction of
the steepest descent. consider both the variables y and f in the original bivariate formulation in
equation (9). the greedy max-cut method is equivalent to the alternating minimization procedure
discussed earlier. unlike graph-cut based ssl methods such as mincuts (blum and chawla, 2001;
blum et al., 2004), our ggmc algorithm tends to generate more natural graph cuts and avoid biased
solutions since it uses a weighted connectivity matrix. this allows it to effectively handle the issues
mentioned earlier and, in practice, achieve signi   cant gains in accuracy while retaining ef   ciency.

4.3 complexity and speed up
assume the graph has n = |x| vertices and a subset xl with l = |xl| labeled vertices (where l     n).
the greedy gradient algorithm terminates after at most n     l     n iterations. in each iteration of
the greedy gradient algorithm, the connectivity matrix c is updated by a id127 (an
n    n-matrix is multiplied by a n    c-matrix). hence, the complexity of the greedy algorithm is
o(cn3).
however, the greedy algorithm can be greatly accelerated in practice. for example, the com-
putation of the connectivity in equation (16) can be done incrementally after assigning each new
unlabeled vertex to a certain label set. this circumvents the re-calculation of all the entries in the
c matrix. assume in the t   th iteration the connectivity is ct and an unlabeled vertex xi with degree
di is assigned to the labeled set s j. clearly, for all remaining unlabeled vertices, the connectivity
to the labeled sets remains unchanged except for the j   th labeled set. in other words, only the j   th
column of c needs updating. this update is performed incrementally via

ct+1

. j =

j

ds t
ds t+1

j

ct

. j +

di
ds t+1

j

a.i,

j

= ds t

+di is the sum of the degrees of the labeled vertices after assigning xi to the labeled
where ds t+1
set s j. this incremental update reduces the complexity of the greedy gradient search algorithm to
o(n2).

j

5. experiments

in this section, we demonstrate the superiority of the proposed ggmc method over state-of-the-art
semi-supervised learning methods using both synthetic and real data. previous work showed that
lapid166 and laprls outperform other semi-supervised approaches such as transductive id166s
tid166 (joachims, 1999) and    tid166 (chapelle and zien, 2005). therefore, we limit our compar-
isons to only the laprls, lapid166 (sindhwani et al., 2005; belkin et al., 2005), lgc (zhou et al.,
2004) and gfhf (zhu et al., 2003). to set various hyper-parameters such as   i,   r in laprls and
lapid166, we followed the default con   gurations used in the literature. similarly, for ggmc and
lgc, we set the hyper-parameter    = 0.01 across all data sets. for the computational cost, ggmc,
lgc and gfhf required very similar run-times to output a prediction. however, laprls and
lapid166 need signi   cant longer training time, especially for multiple-class problems since multiple
rounds of one-vs-all training have to be performed.

as in section 2, any real implementation of graph-based ssl needs a graph construction method
algorithm that builds a graph from the training data x. this is then followed by a sparsi   cation

790

8

9

10

lgc
gfhf
lapid166
laprls
ggmc

semi-supervised learning using greedy max-cut

0.3

0.2

0.1

e
t
a
r
 
r
o
r
r
e

lgc
gfhf
lapid166
laprls
ggmc

0
2

3

4

0.3

0.2

0.1

e
t
a
r
 
r
o
r
r
e

8

9

10

0

2

3

4

lgc
gfhf
lapid166
laprls
ggmc

0.3

0.2

0.1

e
t
a
r
 
r
o
r
r
e

lgc
gfhf
lapid166
laprls
ggmc

8

9

10

0
2

3

4

5

6

7

the number of labels

5

6

7

the number of labels

5

6

7

the number of labels

(a)

(b)

(c)

0.4

0.3

0.2

0.1

e
t
a
r
 
r
o
r
r
e

0
4

5

6

0.5

0.4

0.3

0.2

0.1

e
t
a
r
 
r
o
r
r
e

0.3

0.2

0.1

e
t
a
r
 
r
o
r
r
e

lgc
gfhf
lapid166
laprls
ggmc

8

10

12

14

16

the number of neighborhoods

18

20

0

4

5 6

0.4

0.3

0.2

0.1

e
t
a
r
 
r
o
r
r
e

lgc
gfhf
lapid166
laprls
ggmc

18

20

0
4

5 6

8

10

12

14

16

the number of neighborhoods

(d)

(e)

(f)

8

10

12

14

16

18

20

the number of neighborhoods

lgc
gfhf
lapid166
laprls
ggmc

0.5

0.4

0.3

0.2

0.1

e
t
a
r
 
r
o
r
r
e

lgc
gfhf
lapid166
laprls
ggmc

16

18

20

0
1 2

4

6

0.5

0.4

0.3

0.2

0.1

e
t
a
r
 
r
o
r
r
e

16

18

20

0

1 2

4

6

10

8
imbalance ratio

12

14

lgc
gfhf
lapid166
laprls
ggmc

16

18

20

10

8
imbalance ratio

12

14

0
1 2

4

6

10

8
imbalance ratio

12

14

(g)

(h)

(i)

figure 4: experimental results on the noisy two-moon data set simulating different graph construc-
tion approaches and label conditions. figures a) d) g) use binary weighting. figures b)
e) h) use    xed gaussian kernel weighting. figures c) f) i) use adaptive gaussian kernel
weighting. figures a) b) c) vary the number of labels. figures d) e) f) vary the value of k
in the graph construction. figures g) h) i) vary the label imbalance ratio.

procedure to generate the sparse connectivity matrix b and a weighting procedure to obtain weights
on the edges in b. in these experiments, we used the same graph construction procedure for all the
ssl algorithms. the sparsi   cation was done using the standard k-nearest-neighbors approach and
the edge weighting involved either binary weighting or gaussian kernel weighting. in the latter case,
the    2 distance d   2(xi, x j) is used and the kernel bandwidth    is estimated in two different ways. the
   rst estimate uses a    xed    de   ned as the average distance between each selected sample and its k   th
nearest neighbor (chapelle et al., 2006). in addition, a second adaptive approach is also considered
which locally estimates the parameter    to the mean distance in the k-nearest neighborhoods of the
samples xi and x j (wang et al., 2008a).

791

wang, jebara and chang

0.25

0.2

0.15

0.1

0.05

e
t
a
r
 
r
o
r
r
e

0

20

30

40

50

60

70

the number of labels

lgc
gfhf
lapid166
laprls
ggmc

0.25

0.2

0.15

0.1

0.05

e
t
a
r
 
r
o
r
r
e

80

90

100

0
20

30

40

50

60

70

the number of labels

lgc
gfhf
lapid166
laprls
ggmc

0.25

0.2

0.15

0.1

0.05

e
t
a
r
 
r
o
r
r
e

80

90

100

0
20

30

40

lgc
gfhf
lapid166
laprls
ggmc

80

90

100

50

60

70

the number of labels

(a)

(b)

(c)

figure 5: experimental results on the usps digits data set under varying levels of labeling using:
a) binary weighting; b)    xed gaussian kernel weighting and c) adaptive gaussian kernel
weighting.

5.1 noisy two-moon data set

we    rst compared ggmc with several representative ssl algorithms using the noisy two-moon
data set shown in figure 3. despite the near-perfect classi   cation results reported on clean versions
of this data set (sindhwani et al., 2005; zhou et al., 2004), small amounts of noise quickly degrade
the performance of previous algorithms. in figure 3, two separable manifolds containing 600 two-
dimensional points are mixed with 100 noisy outlier samples. the noise foils previous methods
which are sensitive to the locations of the initial labels, disproportional sampling from the classes,
and outlier noise. all experiments are repeated with 100 independent folds with random sampling
to show the average error rate of each algorithm.

the    rst group of experiments varies the number of labels provided to the algorithms. we
uniformly used k = 6 in the k-nearest-neighbors graph construction and applied the aforementioned
three edge-weighting schemes. the average error rates of the predictions on the unlabeled points is
shown in figures 4(a), 4(b) and 4(c). these correspond to binary edge weighting,    xed gaussian
kernel edge weighting, and adaptive gaussian kernel edge weighting, respectively. the results
clearly show that ggmc is robust to the size of the label set and and generates perfect prediction
results for all three edge-weighting schemes.

the second group of experiments demonstrate the in   uence of the number of edges (i.e., the
value of k) in the graph construction method. we varied the value of k from 4 to 20 and figures 4(d),
4(e), and 4(f) show results for the different edge-weighting schemes. once again, ggmc achieves
signi   cantly better performance in most cases.

finally, we studied the effect of imbalanced labeling on the ssl algorithms. we    x one class
to have only one label and then randomly select r labels from the other classes. here, r indicates
the imbalance ratio and we study the range 1     r     20. figures 4(g), 4(h), and 4(i) show the
results with different edge-weighting schemes. clearly, ggmc is insensitive to the imbalance since
it computes a per-class label weight id172 which compensates directly for differences in
label proportions.

in summary, figure 4 depicted the performance advantages of ggmc relative to lgc, gfhf,
laprls, and lapid166 methods. we clearly see that the four previous algorithms are sensitive to
the initial labeling conditions and none of them produces perfect prediction. furthermore, the error
rates of lgc and gfhf increase signi   cantly when labeling becomes imbalanced, even if many

792

semi-supervised learning using greedy max-cut

0.2

0.15

0.1

0.05

e
t
a
r
 
r
o
r
r
e

0
6

9

12

0.2

e
t
a
r
 
r
o
r
r
e

0.15

0.1

0.05

0
6

9

12

lgc
gfhf
lapid166
laprls
ggmc

24

27

30

lgc
hfgf
lapid166
laprls
ggmc

e
t
a
r
 
r
o
r
r
e

0.35

0.25

0.15

0.05

0
6

e
t
a
r
 
r
o
r
r
e

0.25

0.2

0.15

0.1

0.05

9

12

24

27

30

0
6

9

12

lgc
gfhf
lapid166
laprls
ggmc

24

27

30

lgc
hfgf
lapid166
laprls
ggmc

e
t
a
r
 
r
o
r
r
e

0.25

0.15

0.05

0
4

0.25

0.2

0.15

0.1

e
t
a
r
 
r
o
r
r
e

6

8

24

27

30

0.05
4

6

8

15

18

21

the number of labels

(b)

15

18

21

the number of labels

15

18

21

the number of labels

(a)

15

18

21

the number of labels

lgc
gfhf
lapid166
laprls
ggmc

16

18

20

lgc
hfgf
lapid166
laprls
ggmc

16

18

20

10

12

14

the number of labels

(c)

10

12

14

the number of labels

(d)

(e)

(f)

figure 6: performance of lgc, gfhf, laprls, lapid166, and ggmc algorithms using the uci
data sets. the horizontal axis is the number of training labels provided while the vertical
axis is the average error rate achieved over 100 random folds. results are based on k-
nearest neighbor graphs shown in a) for the iris data set, in b) for the wine data set and
in c) for the breast cancer data set. results are based on b-matched graphs shown in d)
for the iris data set, in e) for the wine data set and in f) for the breast cancer data set.

labels are made available. however, ggmc achieves high accuracy regardless of the imbalance
ratio and the size of the label set. furthermore, ggmc remains robust to the graph construction
procedure and the edge-weighting strategy.

5.2 handwritten digit data set

we also evaluated the algorithms in an image recognition task where handwritten digits in the usps
database are to be annotated with the appropriate label {0, 1, . . . , 9}. the data set contains gray scale
handwritten digit images involving 16   16 pixels. we randomly sampled a subset of 4000 samples
from the data. for all the constructed graphs, we used the k-nearest-neighbors algorithm with k = 6
and tried the three different edge-weighting schemes above. we varied the total number of labels
from 20 to 100 while guaranteeing that each digit class had at least one label. for each setting, the
average error rate was computed over 20 random folds.

the experimental results are shown in figures 5(a), 5(b), and 5(c) which correspond to the three
different edge-weighting schemes. as usual, ggmc signi   cantly improves classi   cation accuracy
relative to other approaches, especially when few labeled training examples are available. the
average error rates of ggmc are consistently low with small standard deviations. this demonstrates
that the ggmc method is less sensitive to the number and locations of the initial training labels.

793

wang, jebara and chang

lgc
gfhf
lapid166
laprls
ggmc

e
t
a
r
 
r
o
r
r
e

0.55

0.5

lgc
gfhf
lapid166
laprls
ggmc

25

30

the number of labels

35

(a)

40

0.45
60

70

80

the number of labels

90

100

(b)

0.2

e
t
a
r
 
r
o
r
r
e

0.18

0.16

0.14
20

figure 7: performance of lgc, gfhf, laprls, lapid166, and ggmc algorithms using the coil-
20 and animal data sets. the horizontal axis is the number of training labels provided
while the vertical axis is the average error rate. results are shown in a) for the coil-210
object data set, and in b) for the animal data set.

5.3 uci data sets

we tested ggmc and the other algorithms on benchmark data sets from the uci machine learning
repository (frank and asuncion, 2010). speci   cally, we used the iris, wine, and breast cancer
data sets. the numerical attributes of the data sets are all normalized to span the range [0, 1]. for all
three data sets, we used a k-nearest-neighbors graph construction procedure with k = 6 and explored
the gaussian kernel with    xed bandwidth as the edge-weighting scheme, where the bandwidth is
set as the average distance between each selected sample and its k   th nearest neighbor.

figure 6 shows the performance of the various ssl algorithms. the vertical axis is the average
error rate computed over 100 random folds and the horizontal axis shows the number of labeled sam-
ples provided at training time. besides using the k-nearest neighbor graphs, we also evaluated the
perform using the b-matched graphs on this data set. the ggmc method signi   cantly outperforms
other algorithms in most test cases, especially when little labeled data is available.

5.4 coil-20 object images

we investigated the object recognition problem using the well-known columbia object image li-
brary (coil-20), which contains 1440 gray-scale images of 20 objects (nene et al., 1996). the
images sequences were obtained when the objects were placed on a turntable table with black
background, where one image was taken for each 5-degree interval. as with uci data sets, we
constructed knn graphs with k = 6 and used a    xed bandwidth for edge weighting. the number of
given labels from all object categories was varied from 20 to 40 with the guarantee that each object
class has at least one label. figure 7(a) shows the performance curves in terms of the average error
rate of 100 random tests, where ggmc outperformed all other methods.

794

semi-supervised learning using greedy max-cut

5.5 nec animal data set

the nec animal data set contains sequences of images of 60 toy animals and has been used as a
benchmark data set for image and video classi   cation (mobahi et al., 2009). each toy animal has
around 72 images taken at different poses. the data set contains a total of 4371 images, each of
size 580   480 pixels. in the experiments, the images were re-sized to 96   72 pixels and the grey
intensity was used as the feature representation. the previous graph construction methodology was
followed and algorithm performance was evaluated using the average error rate across 100 random
folds with the number of initial labels varying from 60 to 100. in the experiments, the ggmc
method again achieved the best performance among all tested methods. in particular, when given
very sparse labels, that is, one label per class, ggmc produced signi   cantly lower error rates.

6. conclusion and discussion

the performance of existing graph-based ssl methods depends heavily on the availability of accu-
rate initial labels and good connectivity structure. otherwise, performance can signi   cantly degrade
if labels are not distributed evenly across classes, if the initial label locations are biased, or if noise
and outliers corrupt the underlying manifold structure. these problems arise in many real world
data sets and limit the performance of state-of-the-art ssl algorithms. furthermore, several heuris-
tic choices in the ssl approach require considerable exploratory work by the practitioner before the
methods perform well in a speci   c problem domain.

this article addressed these shortcomings and proposed a novel graph-based semi-supervised

learning method named greedy gradient max-cut (ggmc). our main contributions include:

1. extending the existing univariate quadratic id173 framework to an optimization over
both label matrix and classi   cation function. such an extension allows us to treat input labels
as part of the optimization problem and thereby alleviate ssl   s sensitivity to initial labels.

2. demonstrating that the bivariate formulation is actually a mixed integer programming prob-
lem which can be reduced to a binary integer programming (bip) problem. in addition, we
show that an alternating minimization procedure can be used to derive a locally optimal solu-
tion.

3. proving that the proposed bivariate formulation is equivalent to a max-cut problem for the
two-class case and proving that it is equivalent to a maximum k-cut problem for the multi-
class case. in addition, we proposed an ef   cient solution with o(n2) complexity. this greedy
gradient max-cut (ggmc) solution presents a different interpretation for the alternating min-
imization procedure from a graph cut view.

unlike other graph-cut based ssl methods such as min-cut (blum and chawla, 2001; blum
et al., 2004), the proposed ggmc algorithm tends to generate more natural graph cuts and avoids
biased solutions. in addition, it uses a weighted connectivity matrix to normalize the label matrix.
the result is a solution that can cope with all the aforementioned degeneracies. it improves accuracy
in practice while remaining ef   cient. future work will extend the proposed methods to out-of-
sample settings where additional data points are added to the prediction problem without requiring a
full retraining procedure. another interesting extension of the bivariate framework is active learning
which can potentially reduce the amount of labels necessary for accurate prediction (goldberg et al.,
2011).

795

wang, jebara and chang

acknowledgments

this material is based upon work supported by the national science foundation under grant no.
1117631. any opinions,    ndings, and conclusions or recommendations expressed in this material
are those of the author and do not necessarily re   ect the views of the national science foundation.
this work was also partially supported by a google research award and by the department of
homeland security under grant no. n66001-09-c-0080.

appendix a. multi-class case as a max k-cut problem

here, we show that k-class bivariate graph transduction is equivalent to a max k-cut problem. if
the number of classes is k, the label variable y is a n   k matrix denoting the classi   cation result.
therein, yi j = 1 indicates that vertex xi is assigned the label j. we rewrite the cost function in
equation (8) as

q (y) =

1

2

tr(cid:16)y   ay(cid:17) =

1

2

k

   
k=1

y   .kay.k.

let yk = y.k be a column vector of y. let the non-zero elements in yk denote the vertices in subset
sk, where k = 1, 2,       , k, s1     s2              sk = va, and sm     sn = /0 if m 6= n. then the above cost
function is equivalent to

q (y1, y2,       , yk) = 1

2

k
   
k=1

y   k ayk =    
xi,x j   sk
i< j

ai j

=    
i< j

ai j    

k   1
   
m=1

k
   

n=m+1

ai j.

   
xi   sm
x j   sn

therefore the original minimization problem is equivalent to maximizing the sum of the weight of
the edges between the disjoint sets sk, that is, the maximum k-cut problem

max
s1,...,sk

k   1
   
m=1

k

   
n=m+1

ai j.

   
xi   sm
x j   sn

references

a. azran. the rendezvous algorithm: multiclass semi-supervised learning with markov random
walks. in proceedings of the international conference on machine learning, pages 49   56, cor-
valis, oregon, 2007. acm.

m.-f. balcan, a. blum, and k. yang. co-training and expansion: towards bridging theory and
practice. in advances in neural information processing systems, volume 17, pages 89   96. 2005.

f. barahona, m. gr  otschel, m. j  unger, and g. reinelt. an application of combinatorial optimization

to statistical physics and circuit layout design. operations research, 36(3):493   513, 1988.

796

semi-supervised learning using greedy max-cut

m. belkin, p. niyogi, and v. sindhwani. on manifold id173. in proceedings of interna-

tional conference on arti   cial intelligence and statistics, barbados, january 2005.

m. belkin, p. niyogi, and v. sindhwani. manifold id173: a geometric framework for
learning from labeled and unlabeled examples. journal of machine learning research, 7:2399   
2434, 2006.

t. d. bie and n. cristianini. convex methods for transduction. in advances in neural information

processing systems, volume 16. 2004.

a. blum and s. chawla. learning from labeled and unlabeled data using graph mincuts. in pro-
ceedings of international conference on machine learning, pages 19   26, san francisco, ca,
usa, 2001.

a. blum and t. mitchell. combining labeled and unlabeled data with co-training. in proceedings
of the eleventh annual conference on computational learning theory, pages 92   100, madison,
wisconsin, united states, 1998. acm.

a. blum, j. lafferty, m. r. rwebangira, and r. reddy. semi-supervised learning using randomized
mincuts. in proceedings of the twenty-   rst international conference on machine learning, pages
13   20, banff, alberta, canada, 2004.

m. carreira-perpin  an and r. s. zemel. proximity graphs for id91 and manifold learning. in

advances in neural information processing systems, volume 17, pages 225   232. 2005.

o. chapelle and a. zien. semi-supervised classi   cation by low density separation. in proceedings

of international conference on arti   cial intelligence and statistics, barbados, january 2005.

o. chapelle, b. sch  olkopf, and a. zien, editors. semi-supervised learning. mit press, cambridge,

ma, 2006. url http://www.kyb.tuebingen.mpg.de/ssl-book.

o. chapelle, v. sindhwani, and s. s. keerthi. branch and bound for semi-supervised support vector
machines. in advances in neural information processing systems, volume 19, pages 217   224.
cambridge, ma, 2007.

o. chapelle, v. sindhwani, and s. s. keerthi. optimization techniques for semi-supervised support

vector machines. the journal of machine learning research, 9:203   233, 2008.

n. v. chawla and g. karakoulas. learning from labeled and unlabeled data: an empirical study
across techniques and domains. journal of arti   cial intelligence research, 23(1):331   366, 2005.

f. r. k. chung and n. biggs. spectral id207. american mathematical society providence,

ri, 1997.

s. a. cook. the complexity of theorem-proving procedures. in proceedings of the third annual
acm symposium on theory of computing, pages 151   158, shaker heights, ohio, united states,
1971. acm.

m. m. deza and m. laurent. geometry of cuts and metrics. springer verlag, 2009.

797

wang, jebara and chang

j. edmonds and e. johnson. matching: a well-solved class of integer linear programs. combina-

torial optimizationeureka, you shrink!, pages 27   30, 2003.

a. frank and a. asuncion.

uci machine

learning repository,

2010.

url

http://archive.ics.uci.edu/ml.

m. x. goemans and d. p. williamson.

.879-approximation algorithms for max cut and max
2sat. in proceedings of the twenty-sixth annual acm symposium on theory of computing,
pages 422   431, montreal, quebec, canada, 1994.

m. x. goemans and d. p. williamson. improved approximation algorithms for maximum cut and
satis   ability problems using semide   nite programming. journal of the acm, 42(6):1115   1145,
1995.

a.b. goldberg, x. zhu, a. furger, and j.m. xu. oasis: online active semi-supervised learning. in

proceedings of the twenty-fifth aaai conference on arti   cial intelligence, 2011.

s. goldman and y. zhou. enhancing supervised learning with unlabeled data. in proceedings of

the 17th international conference on machine learning, pages 327   334, 2000.

b. huang and t. jebara. loopy belief propagation for bipartite maximum weight b-matching. in

int. workshop on arti   cial intelligence and statistics, 2007.

b. huang and t. jebara. collaborative    ltering via rating concentration. in y.w. teh and m. titter-
ington, editors, proceedings of the thirteenth international conference on arti   cial intelligence
and statistics, volume volume 9 of jmlr: w&cp, pages 334   341, may 13-15 2010.

t. jebara, j. wang, and s.-f. chang. graph construction and b-matching for semi-supervised learn-
ing. in proceedings of the 26th annual international conference on machine learning, pages
441   448, 2009.

t. joachims. transductive id136 for text classifcation using support vector machines. in pro-

ceedings of international conference on machine learning, pages 200   209, 1999.

t. joachims. transductive learning via spectral graph partitioning. in proceedings of international

conference on machine learning, pages 290   297, 2003.

r. m. karp. reducibility among combinatorial problems. complexity of computer computations,

43:85   103, 1972.

m.-a. krogel and t. scheffer. multi-relational learning, id111, and semi-supervised learning

for functional genomics. machine learning, 57(1):61   81, 2004.

b. kveton, m. valko, a. rahimi, and l. huang. semi-supervised learning with max-margin graph
cuts. in proceedings of the 13th international conference on arti   cial intelligence and statistics,
pages 421   428, 2010.

w. liu, j. wang, and s.-f. chang. robust and scalable graph-based semisupervised learning. pro-

ceedings of the ieee, 100(9):2624   2638, 2012.

798

semi-supervised learning using greedy max-cut

m. maier, u. von luxburg, and m. hein. in   uence of graph construction on graph-based id91
measures. in advances in neural information processing systems, volume 22, pages 1025   1032.
2009.

g. s. mann and a. mccallum. simple, robust, scalable semi-supervised learning via expectation
id173. in proceedings of international conference on machine learning, pages 593   
600, corvalis, oregon, 2007.

c. mathieu and w. schudy. yet another algorithm for dense max cut: go greedy. in proceedings of

the nineteenth annual acm-siam symposium on discrete algorithms, pages 176   182, 2008.

s. melacci and m. belkin. laplacian support vector machines trained in the primal. journal of

machine learning research, 12:1149   1184, 2011.

h. mobahi, r. collobert, and j. weston. deep learning from temporal coherence in video.

in
proceedings of the 26th annual international conference on machine learning, pages 737   744,
2009.

s.a. nene, s.k. nayar, and h. murase. columbia object image library (coil-20). dept. comput.
sci., columbia univ., new york.[online] http://www. cs. columbia. edu/cave/coil-20.html, 1996.

j. shi and j. malik. normalized cuts and image segmentation.

ieee transactions on pattern

analysis and machine intelligence, 22(8):888   905, 2000.

v. sindhwani, p. niyogi, and m. belkin. beyond the point cloud: from transductive to semi-
supervised learning. in proceedings of the 22nd international conference on machine learning,
pages 824   831, bonn, germany, 2005.

v. sindhwani, j. hu, and a. mojsilovic. regularized co-id91 with dual supervision.

in
d. koller, d. schuurmans, y. bengio, and l. bottou, editors, advances in neural information
processing systems, pages 976   983. mit press, 2008.

m. szummer and t. jaakkola. partially labeled classi   cation with markov id93. in ad-

vances in neural information processing systems, volume 14, pages 945   952. 2002.

v. vapnik. statistical learning theory. wiley, new york, 1998.

j. wang, s.-f. chang, x. zhou, and t. c. s. wong. active microscopic cellular image anno-
tation by superposable graph transduction with imbalanced labels. in proceedings of ieee
conference on id161 and pattern recognition, alaska, usa, june 2008a.

j. wang, t. jebara, and s.-f. chang. graph transduction via alternating minimization. in proceed-
ings of international conference on machine learning, pages 1144   1151, helsinki, finland,
2008b.

j. wang, y.-g. jiang, and s.-f. chang. label diagnosis through self tuning for web image search. in
ieee computer society conference on id161 and pattern recognition, miami beach,
florida, usa, june 2009.

w. wang and z.-h. zhou. a new analysis of co-training. in proceedings of the 27th international

conference on machine learning, pages 1135   1142, haifa, israel, june 2010.

799

wang, jebara and chang

z. xu, r. jin, j. zhu, i. king, and m. lyu. ef   cient convex relaxation for transductive support

vector machine. volume 21, pages 1641   1648. 2008.

d. zhou, o. bousquet, t. n. lal, j. weston, and b. sch  olkopf. learning with local and global
consistency. in s. thrun, l. saul, and b. sch  olkopf, editors, advances in neural information
processing systems, volume 16, pages 321   328. 2004.

x. zhu. semi-supervised learning literature survey. technical report 1530, computer sciences,

university of wisconsin-madison, 2005.

x. zhu and a. b. goldberg. introduction to semi-supervised learning. morgan & claypool pub-

lishers, 2009.

x. zhu, z. ghahramani, and j. lafferty. semi-supervised learning using gaussian    elds and har-
monic functions. in proceedings of international conference on machine learning, pages 912   
919, 2003.

800

