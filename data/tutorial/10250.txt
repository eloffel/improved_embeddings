on the universal structure of human lexical semantics

hyejin youn,1, 2, 3 logan sutton,4 eric smith,3 cristopher moore,3 jon f.

wilkins,3, 5 ian maddieson,4 william croft,4 and tanmoy bhattacharya3, 6

1institute for new economic thinking at the oxford martin school, oxford, ox2 6ed, uk

2mathematical institute, university of oxford, oxford, ox2 6gg, uk

3santa fe institute, 1399 hyde park road, santa fe, nm 87501, usa

4department of linguistics, university of new mexico, albuquerque, nm 87131, usa

5ronin institute, montclair, nj 07043

6ms b285, grp t-2, los alamos national laboratory, los alamos, nm 87545, usa.

(dated: april 30, 2015)

how universal is human conceptual structure? the way concepts are organized in the hu-

man brain may re   ect distinct features of cultural, historical, and environmental background

in addition to properties universal to human cognition. semantics, or meaning expressed

through language, provides direct access to the underlying conceptual structure, but mean-

ing is notoriously di   cult to measure, let alone parameterize. here we provide an empirical

measure of semantic proximity between concepts using cross-linguistic dictionaries. across

languages carefully selected from a phylogenetically and geographically strati   ed sample of

genera, translations of words reveal cases where a particular language uses a single pol-

ysemous word to express concepts represented by distinct words in another. we use the

frequency of polysemies linking two concepts as a measure of their semantic proximity, and

represent the pattern of such linkages by a weighted network. this network is highly uneven

and fragmented: certain concepts are far more prone to polysemy than others, and there

emerge naturally interpretable clusters loosely connected to each other. statistical analy-

sis shows such structural properties are consistent across di   erent language groups, largely

independent of geography, environment, and literacy.

it is therefore possible to conclude

the conceptual structure connecting basic vocabulary studied is primarily due to universal

features of human cognition and language use.

5
1
0
2

 
r
p
a
9
2

 

 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 

1
v
3
4
8
7
0

.

4
0
5
1
:
v
i
x
r
a

2

the space of concepts expressible in any language is vast. this space is covered by individual

words representing semantically tight neighborhoods of salient concepts. there has been much

debate about whether semantic similarity of concepts is shared across languages [1   8]. on the one

hand, all human beings belong to a single species characterized by, among other things, a shared set

of cognitive abilities. on the other hand, the 6000 or so extant human languages spoken by di   erent

societies in di   erent environments across the globe are extremely diverse [9   11] and may re   ect

accidents of history as well as adaptations to local environments. most psychological experiments

about this question have been conducted on members of    weird    (western, educated, industrial,

rich, democratic) societies, yet there is reason to question whether the results of such research

are valid across all types of societies [12]. thus, the question of the degree to which conceptual

structures expressed in language are due to universal properties of human cognition, the particulars

of cultural history, or the environment inhabited by a society, remains unresolved.

the search for an answer to this question has been hampered by a major methodological dif-

   culty. linguistic meaning is an abstract construct that needs to be inferred indirectly from

observations, and hence is extremely di   cult to measure; this is even more apparent in the    eld

of lexical semantics. meaning thus contrasts both with id102, in which instrumental measure-

ment of physical properties of articulation and acoustics is relatively straightforward, and with

grammatical structure, for which there is general agreement on a number of basic units of analysis

[13]. much lexical semantic analysis relies on linguists    introspection, and the multifaceted dimen-

sions of meaning currently lack a formal characterization. to address our primary question, it is

necessary to develop an empirical method to characterize the space of lexical meanings.

we arrive at such a measure by noting that translations uncover the alternate ways that lan-

guages partition meanings into words. many words have more than one meaning, or sense, to

the extent that word senses can be individuated [14]. words gain meanings when their use is

extended by speakers to similar meanings; words lose meanings when another word is extended to

the    rst word   s meaning, and the    rst word is replaced in that meaning. to the extent that words

in transition across similar, or possibly contiguous, meanings account for the polysemy (multiple

meanings of a single word form) revealed in cross-language translations, the frequency of poly-

semies found across an unbiased sample of languages can provide a measure of semantic similarity

among word meanings. the unbiased sample of languages is carefully chosen in a phylogenetically

and geographically strati   ed way, according to the methods of typology and universals research

[10, 11]. this large, diverse sample of languages allows us to avoid the pitfalls of research based

solely on    weird    societies and to separate contributions to the empirically attested patterns

3

fig. 1. schematic    gure of the construction of network representations. (a) tripartite polysemy network
constructed through translation (links from the    rst to the second layer) and back-translation (links from
the second to the third layer) for the cases of moon and sun in two american languages: coast tsimshian
(red links) and lakhota (blue links). (b) directed bipartite graph of two languages grouped, projected from
the tripartite graph above by aggregating links in the second layer. (c) directed and weighted unipartite
graph, projected from the bipartite graph by identifying and merging the same swadesh words (moon and
sun in this case).

in the linguistic data, arising from universal language cognition versus those from artifacts of the

speaker-groups    history or way of life.

there have been several cross-linguistic surveys of lexical polysemy, and its potential for under-

standing semantic shift [15], in the domains such as body parts [16, 17], cardinal directions [18],

perception verbs [19], concepts associated with    re [20], and color metaphors [21]. we add a new

dimension to the existing body of research by providing a comprehensive mathematical method

using a systematically strati   ed global sample of languages to measure degrees of similarity. our

cross-linguistic study takes the swadesh lists as basic concepts [22] as most languages have words

for them. among those concepts, we chose 22 meanings associated with two domains: celestial

objects (e.g. sun, moon, star) and landscape objects (e.g. fire, water, mountain, dust).

for each word expressing one of these meanings, we examined what other concepts were also ex-

pressed by the word. since the semantic structures of these two domains are very likely to be

in   uenced by the physical environment that human societies inhabit, any claim of universality of

lexical semantics needs to be demonstrated here.

gyemgm  atkgooypahgimgmdziwsmoonsunheatmonthsample level: swords level: wltranslation tsw    paw  red: coast_tsimshianblue: lakhotaw  meaning level: mha  h  pi_w  ha  w  sungyemkbacktranslation twmmwcwlmoonmonthheatmoonsunmoonsun64122122bipartite: {s} and {m}heatmoonsunmonth22211unipartite:  {m}(a)(b)(c)2results

4

we represent word-meaning and meaning-meaning relations uncovered by translation dictio-

naries between each language in the unbiased sample and major modern european languages by

constructing a network structure. two meanings (represented by a set of english words) are linked

if they are translated from one to another and then back, and the link is weighted by the number

of paths of the translation, or the number of words that represent both meanings (see methods

for detail). figure 1 illustrates the construction in the case of two languages, lakhota (primarily

spoken in north and south dakota) and coast tsimshian (mostly spoken in northwestern british

columbia and southeastern alaska). translation of sun in lakhota results w     and   a  paw    . while

the later picks up no other meaning, w     is a polysemy that possesses additional meanings of moon

and month, hence they are linked to sun. such polysemy is also observed in coast tsimshian where

gyemk, translated from sun, covers additional meanings including, thus additionally linking to,

heat.

each language has its own way of partitioning meanings by words, captured in a semantic net-

work of the language. it is conceivable, however, that a group of languages bear structural resem-

blance perhaps because the speakers share historical or environmental features. a link between sun

and moon, for example, reoccurs in both languages, but does not appear in many other languages.

sun is instead linked to divinity and time in japanese, and to thirst and day/daytime in !x  o  o.

the question then is the degree to which the observed polysemy patterns are general or sensitive

to the environment inhabited by the speech community, phylogenetic history of the languages,

and intrinsic linguistic factors such as literary tradition. we test such question by grouping the

individual networks in a number of ways according to properties of their corresponding languages.

we    rst analyze the networks of the entire languages, and then of sub-groups.

in fig. 2, we present the network of the entire languages exhibiting the broad topological

structure of polysemies observed in our data. it reveals three almost-disconnected clusters, groups

of concepts that are indeed more prone to polysemy within, that are associated with a natural

semantic interpretation. the semantically most uniform cluster, colored in blue, includes concepts

related to water. a second, smaller cluster, colored in yellow, associates solid natural substances

(centered around stone/rock) with their topographic manifestation (mountain). the third cluster,

in red, is more loosely connected, bridging a terrestrial cluster and a celestial cluster, including

less tangible substances such as wind, sky, and fire, and salient time intervals such as day and

year. in keeping with many traditional oppositions between earth and sky/heaven, or darkness,

5

fig. 2. connectance graph of concepts. concepts are connected through polysemous words that cover
the concepts. swadesh entries are capitalized. links whose weights are more than two are presented, and
direction is omitted for simplicity. the size of a node and the width of a link to another node are proportional
to the number of polysemies associated with the concept, and with the two connected concepts, respectively.
the thick link from sky to heaven denotes the large number of polysemies across languages. three distinct
clusters are identi   ed and coloured by red, blue, and yellow, which may imply a coherent set of relationship
among concepts that possibly re   ects human cognitive conceptualization of these semantic domains.

and light, the celestial, and terrestrial components form two sub-clusters connected most strongly

through cloud, which shares properties of both. the result reveals a coherent set of relationships

among concepts that possibly re   ects human cognitive conceptualization of these semantic domains

[8, 11, 23].

we test whether these relationships are universal rather than particular to properties of linguistic

groups such as physical environment that human societies inhabit. we    rst categorized languages

by nonlinguistic variables such as geography, topography, climate, and the existence or nonexis-

tence of a literary tradition (table ii in appendix) and constructed a network for each group. a

spectral algorithm then clusters swadesh entries into a hierarchical structure or dendrogram for

each language group. using standard metrics on trees [24   26], we    nd that the dendrograms of

language groups are much closer to each other than to dendrograms of randomly permuted leaves:

dustsawdustsootcloud_of_dustpollendust_stormcigarettealcoholbeveragesaltjuicemoisturetidewatertemper_(of_metal)interest_on_moneycharm   avorgun   re   ameblazeburning_objectelectricitycon   agrationfevermoldash(es)embersclaypowdercountrygravelmudgunpowderburned_objectsea/oceanstreamspringcoastriverliquidwaveearth/soil   oorbottomdebrisholeground   lth   eldhomeland   oodwatercourse   owing_watertorrentcurrentheat   rewoodpassionfireangerhearthmoney   intlumpgempebblehailweightbatteryseasonlong_period_of_timechristmasbirthdaysummertimepleiadeswinteryearskyatmosphereabovehightopairspacechambered_nautilusmenseslunar_monthmoonlightmenstruation_periodsatellitemoondivinitydatesunlightweatherheat_of_sunsunmonthcoldmoodbreezebodily_gasesbreathwindclimatedirectionstormstone/rockcalculusboulderseedgallstonecobblemountainkidneystonecliffhillmetalmilestonepileclearingtombstoneforestheight24hr_periodheavenceilingnoonthirstswamppondwaterholetankpuddlelakedawnafternoonclockday/daytimelifebody_of_waterrainsapbodily_   uidlagoonsoupteacolorworldsandbanknighteveningbeachdarknesslast_nightsandy_areasleepsandageplanetcelebrityasteriskstarlodestarluckfatestar   shheavenly_bodylighthighlandsvolcanoslopemountainous_regionmistfoghazesmellcloud(s)smokefumestobaccohouseholdmatchlampmeteorburningclandustsawdustsootcloud_of_dustpollendust_stormcigarettealcoholbeveragesaltjuicemoisturetidewatertemper_(of_metal)interest_on_moneycharmflavorgunfireflameblazeburning_objectelectricityconflagrationfevermoldash(es)embersclaypowdercountrygravelmudgunpowderburned_objectsea/oceanstreamspringcoastriverliquidwaveearth/soilfloorbottomdebrisholegroundfilthfieldhomelandfloodwatercourseflowing_watertorrentcurrentheatfirewoodpassionfireangerhearthmoneyflintlumpgempebblehailweightbatteryseasonlong_period_of_timechristmasbirthdaysummertimepleiadeswinteryearskyatmosphereabovehightopairspacechambered_nautilusmenseslunar_monthmoonlightmenstruation_periodsatellitemoondivinitydatesunlightweatherheat_of_sunsunmonthcoldmoodbreezebodily_gasesbreathwindclimatedirectionstormstone/rockcalculusboulderseedgallstonecobblemountainkidneystonecliffhillmetalmilestonepileclearingtombstoneforestheight24hr_periodheavenceilingnoonthirstswamppondwaterholetankpuddlelakedawnafternoonclockday/daytimelifebody_of_waterrainsapbodily_fluidlagoonsoupteacolorworldsandbanknighteveningbeachdarknesslast_nightsandy_areasleepsandageplanetcelebrityasteriskstarlodestarluckfatestarfishheavenly_bodylighthighlandsvolcanoslopemountainous_regionmistfoghazesmellcloud(s)smokefumestobaccohouseholdmatchlampmeteorburningclan6

thus the hypothesis that languages of di   erent subgroups share no semantic structure in common

is rejected (p < 0.05, see methods)   sea/ocean and salt are, for example, more related than

either is to sun in every group we tried. in addition, the distances between dendrograms of lan-

guage groups are statistically indistinguishable from the distances between bootstrapped languages

(p < 0.04). figure 3 shows a summary of the statistical tests of 11 di   erent groups. thus our data

analyses provide consistent evidences that all languages share semantic structure, the way concepts

are clustered in fig. 2, with no signi   cant in   uence from environmental or cultural factors.

fig. 3.
(a) an illustration of our bootstrap experiments. the triplet distance dtriplet between the
dendrograms of the americas and oceania is 0.56 (arrow). this number sits at the very low end of the
distribution of distances when leaves are randomly permuted (the red shaded pro   le on the right), but it
is well within the distribution that we obtain by randomly re-sampling from the set of languages (the blue
shaded pro   le on the left). this gives strong evidence that each pair of subgroups share an underlying
semantic network, and that the di   erences between them are no larger than would result from random
sampling. (b) comparing distances (the triplet dtriplet and the robinson-foulds drf) among dendrograms
of subgroups and two types of bootstrap experiments: permuting leaves of the dendrogram and replacing
the subgroups in question with bootstrapped samples of the same sizes. p1-values for the former bootstrap
(p2-values for the latter) are the fraction of 1000 bootstrap samples whose distances are smaller (larger)
than the observed distance. in either case 0    denotes a value below 0.001, i.e., no bootstrap sample satis   ed
the condition.

another structural feature apparent in fig. 2 is the heterogeneity of the node degrees and link

weights. the numbers of polysemies involving individual meanings are uneven, possibly toward

a heavy-tailed distribution (fig. 4). this indicates concepts not only form clusters within which

they are densely connected, but also exhibit di   erent levels of being polysemous. for example,

earth/soil has more than hundreds of polysemes while salt has only a few. having shown that

some aspects of the semantic network are universal, we next ask whether the observed heterogeneous

degrees of polysemy, possibly a manifestation of varying densities of near conceptual neighbors,

ab(a)0.20.30.40.50.60.70.050.100.150.200.250.30americasvs.oceania,tripletdistance(b)languagegroupsdtripletp1p2drfp1p2americasvs.eurasia0.590.060.20300   0.42americasvs.africa0.590.020.20300   0.46americasvs.oceania0.560.020.38260   0.87eurasiavs.africa0.520   0.62320.010.31eurasiavs.oceania0.600.060.16280   0.78africavs.oceania0.460   0.82340.010.18humidvs.cold0.580.040.17280   0.49humidvs.arid0.610.120.10340.010.13coldvs.arid0.500.010.71300   0.53inlandvs.coastal0.590.040.14280   0.39literaryvs.noliterary0.490.010.62280   0.45figure4:(a)anillustrationofourbootstrapexperiments.thetripletdistancedtripletbetweenthedendrogramsoftheamericasandoceaniais0.56(arrow).thisnumbersitsattheverylowendofthedistributionofdistanceswhenleavesarerandomlypermuted(theredshadedpro   leontheright),butitiswellwithinthedistributionthatweobtainbyrandomlyresamplingfromthesetoflanguages(theblueshadedpro   leontheleft).thisgivesstrongevidencethateachpairofsubgroupsshareanunderlyingsemanticnetwork,andthatthedifferencesbetweenthemarenolargerthanwouldresultfromrandomsampling.(b)comparingdistances(thetripletdtripletandtherobinson-fouldsdrf)amongdendrogramsofsubgroupsandtwotypesofbootstrapexperiments:permutingleavesofthedendrogram,andreplacingthesubgroupsinquestionwithbootstrappedsamplesofthesamesizes.0   denotesap-valuebelow0.001,i.e.,outsideall1000bootstrapsamples.15dtripletdistribution7

arise as artifacts of language family structure in our sample, or if they are inherent to the concepts

themselves. simply put, is it an intrinsic property of the concept, earth/soil, to be extensively

polysemous, or is it a few languages that happened to call the same concept in so many di   erent

ways.

suppose an underlying    universal space    relative to which each language l randomly draws a

subset of polysemies for each concept s. the number of polysemies nsl should then be linearly

proportional to both the tendency of the concept to be polysemous for being close to many other

concepts, and the tendency of the language to distinguish word senses in basic vocabulary. in our

network representation, a proxy for the former is the weighted degree ns of node s, and a proxy

for the latter is the total weight of links nl in language l. then the number of polysemies is

expected (see methods):

nmodel
sl     ns   

nl
n

.

(1)

this simple model indeed captures the gross features of the data very well (fig. 5 in the ap-

pendix). nevertheless, the id181 between the prediction nmodel

sl

and the

empirical data ndata
sl identi   es deviations beyond the sampling errors in three concepts   moon, sun
and ashes   that display nonlinear increase in the number of polysemies (p     0.01) with the ten-
dency of the language distinguish word senses as fig. 6 in the appendix shows. accommodating

saturation parameters (table iii in the appendix) enables the random sampling model to repro-

duce the empirical data in good agreement keeping the two parameters independent, hence retain

the universality over language groups.

discussion

the similarity relations between word meanings through common polysemies exhibit a uni-

versal structure, manifested as intrinsic closeness between concepts, that transcends cultural or

environmental factors. polysemy arises when two or more concepts are fundamental enough to

receive distinct vocabulary terms in some languages, yet similar enough to share a common term

in others. the highly variable degree of these polysemies indicates such salient concepts are not

homogeneously distributed in the conceptual space, and the intrinsic parameter that describes the

overall propensity of a word to participate in polysemies can then be interpreted as a measure of the

local density around such concept. our model suggests that given the overall semantic ambiguity

8

observed in the languages, such local density determines the degree of polysemies.

universal structures in lexical semantics would greatly aid another subject of broad interest,

namely reconstruction of human phylogeny using linguistic data [27, 28]. much progress has been

made in reconstructing the phylogenies of word forms from known cognates in various languages,

thanks to the ability to measure phonetic similarity and our knowledge of the processes of sound

change. however, the relationship between semantic similarity and semantic shift is still poorly

understood. the standard view in historical linguistics is that any meaning can change to any other

meaning [29, 30], and that no constraint is imposed on what meanings can be compared to detect

cognates [31]. it is, however, generally accepted among historical linguists that language change

is gradual, and that words in transition from having one meaning to being extended to another

meaning should be polysemous.

if this is true, then the weights on di   erent links re   ect the

probabilities that words in transition over these links will be captured in    snapshots    by language

translation at any time. such semantic shifts can be modeled as di   usion in the conceptual space,

or along a universal polysemy network where our constructed networks can serve an important

input to methods of inferring cognates.

the absence of signi   cant cladistic correlation with the patterns of polysemy suggests a possi-

bility to extend the constructed conceptual space by utilizing digitally archived dictionaries of the

major languages of the world with some con   dence that their expression of these features is not

strongly biased by correlations due to language family structure. large-corpus samples could be

used to construct the semantic space in as yet unexplored domains using automated means.

methods

polysemy data

high-quality bilingual dictionaries between the object language and the semantic metalanguage

for cross-linguistic comparison are used to identify polysemies. the 81 object languages were

selected from a phylogenetically and geographically strati   ed sample of low-level language families

or genera, listed in tab. i in the appendex [32]. translations into the object language of each of

the 22 word senses from the swadesh basic vocabulary list were    rst obtained (see appendix- a);

all translations (that is, all synonyms) were retained. polysemies were identi   ed by looking up the

metalanguage translations (back-translation) of each object-language term. the selected swadesh

word senses, and the selected languages are listed in the appendix.

9

fig. 4. rank plot of concepts in descending order of their strengths (summation of weighted links) and
degrees (summation of unweighted links) shown in fig. 2. entries from the initial swadesh list are distin-
guished with capital letters. (a) in-strengths of concepts: sum of weighted links to a node. (b) out-strengths
of swadesh entries: sum of weighted links from a swadesh entry. (c) degree of the concepts: sum of un-
weighted links to a node (d) degree of swadesh entries: sum of unweighted links to a node. a node strength
in this context indicates the total number of polysemies associated with the concept in 81 languages while
a node degree means the number of other concepts associated with the node regardless of the number of
synonymous polysemies associated with it. heaven, for example, has the largest number of polysemies, but
most of them are with sun, so that its degree is only three.

we use modern european languages as a semantic metalanguage, i.e., bilingual dictionaries

between such languages and the other languages in our sample. this could be problematic if these

languages themselves display polysemies; for example, english day expresses both daytime, and

24hr period. in many cases, however, the lexicographer is aware of these issues, and annotates

the translation of the object language word accordingly. in the lexical domain chosen for our study,

standard lexicographic practice was su   cient to overcome this problem.

comparing semantic networks between language groups

a hierarchical spectral algorithm clusters the swadesh word senses. each sense i is assigned to a
position in rn based on the ith components of the n eigenvectors of the weighted adjacency matrix.

each eigenvector is weighted by the square of its eigenvalue, and clustered by a greedy agglomerative

algorithm to merge the pair of clusters having the smallest squared euclidean distance between

their centers of mass, through which a binary tree or dendrogram is constructed we construct a

dendrogram for each subgroup of languages according to nonlinguistic variables such as geography,

heavengroundmonthdustairstreamearth/soilhillearth/soilskywaterfiredustday/daytimeworldwatersea/oceandustsootgroundstone/rockriverearth/soildustwatersunfiresky...windsunmoon...body_of_waterweatherday/daytime(a)(b)(c)(d)10

topography, climate, and the presence or absence of a literary tradition (table ii in appendix).

the structural distance between the dendrograms of each pair of language subgroups is measured

by two standard tree metrics. the triplet distance dtriplet [24, 25] is the fraction of the(cid:0)n

(cid:1) distinct

3

triplets of senses that are assigned a di   erent topology in the two trees: that is, those for which

the trees disagree as to which pair of senses are more closely related to each other than they are to

the third. the robinson-foulds distance drf [26] is the number of    cuts    on which the two trees

disagree, where a cut is a separation of the leaves into two sets resulting from removing an edge of

the tree.

for each pair of subgroups, we perform two types of bootstrap experiments. first, we compare

the distance between their dendrograms to the distribution of distances we would see under a

hypothesis that the two subgroups have no shared lexical structure. were this null hypothesis

true, the distribution of distances would be unchanged under the random permutation of the

senses at the leaves of each tree (for simplicity, the topology of the dendrograms are kept    xed.)

comparing the observed distance against the resulting distribution gives a p-value, called p1 in

figure 3. these p-values are small enough to decisively reject the null hypothesis.

indeed, for

most pairs of groups the robinson-foulds distance is smaller than that observed in any of the 1000
bootstrap trials (p < 0.001) marked as 0    in the table. this gives overwhelming evidence that
the semantic network has universal aspects that apply across language subgroups: for instance, in

every group we tried, sea/ocean, and salt are more related than either is to sun.

in the second bootstrap experiment, the null hypothesis is that the nonlinguistic variables have

no e   ect on the semantic network, and that the di   erences between language groups simply result

from random sampling: for instance, the similarity between the americas and eurasia is what one

would expect from any disjoint subgroups of the 81 languages of given sizes 29 and 20 respectively.

to test this null hypothesis, we generate random pairs of disjoint language subgroups with the

same sizes as the groups in question, and measure the distribution of their distances. the p-values,

called p2 in figure 3, are not small enough to reject this null hypothesis. thus, at least given the

current data set, there is no statistical distinction between random sampling and empirical data

   further supporting our thesis that it is, at least in part, universal.

null model

the model treats all concepts as independent members of an unbiased sample that the aggregate

summary statistics of the empirical data re   ects the underlying structure. the simplest model

perhaps then assumes no interaction between concept and languages: the number of polysemies of

concept s in language l, that is nmodel

sl , is linearly proportional to both the tendency of the concept
to be polysemous and the tendency of the language to distinguish word senses; and these tendencies

are estimated from the marginal distribution of the observed data as the fraction of polysemy

11

l /n , respectively. the model can, therefore, be expressed as, pmodel

sl = pdata

s

pdata
l , a

= ndata

s

/n , and the fraction of polysemy in the language,

associated with the concept, pdata
pdata
s = ndata
product of the two.

s

to test the model, we compare the kullback-leibler (kl) divergence of ensembles of the model

with the observation [33]. ensembles are generated by the multinominal distribution according

to the id203 pmodel

sl . the kl divergence is an appropriate measure for testing typicality of
this random process because it is the leading exponential approximation (by stirlings formula)

to the log of the multinomial distribution produced by poisson sampling (see appendix d). the

(cid:13)(cid:13) pmodel
(cid:1)
kl divergence of ensembles is d(cid:0)pensemble
(cid:13)(cid:13) pmodel
of the empirical observation is d(cid:0)pdata
(cid:1)
(cid:80)
(cid:13)(cid:13) pmodel
p-value is the cumulative id203 of d(cid:0)pensemble

log(cid:0)pensemble

(cid:80)
sl log(cid:0)pdata
(cid:1) to the right of d(cid:0)pdata

(cid:1) where
(cid:1). note that pdata
(cid:13)(cid:13) pmodel
(cid:1).

sl is
l /n 2. the

sl /pmodel
data/n and it is a di   erent value from an expected value of the model, ns

is the number of polysemies that the model generates divided by n , and the kl divergence

s,l pensemble

pensemble
sl

s,l pdata

datandata

/pmodel

nsl

   

   

sl

sl

sl

sl

sl

sl

sl

sl

sl

sl

sl

sl

acknowledgments

hy acknowledges support from cabdyn complexity centre, and the support of research

grants from the national science foundation (no. sma-1312294). wc and ls acknowledge

support from the university of new mexico resource allocation committee. tb, jw, es, cm,

and hy acknowledge the santa fe institute, and the evolution of human languages program.

authors thank ilia peiros, george starostin, and petter holme for helpful comments. w.c. and

t.b. conceived of the project and participated in all methodological decisions. l.s. and w.c.

collected the data, h.y., j.w., e.s., and t.b. did the modeling and statistical analysis. i.m. and

w.c. provided the cross-linguistic knowledge. h.y., e.s., and c.m. did the network analysis. the

manuscript was written mainly by h.y., e.s., w.c., c.m., and t.b., and all authors agreed on

the    nal version.

[1] whorf bl, language, thought and reality: selected writing. (mit press, cambridge, 1956).

12

[2] fodor ja, the language of thought. (harvard univ., new york, 1975).

[3] wierzbicka, a., semantics: primes and universals. (oxford university press. 1996)

[4] lucy ja, grammatical categories and cognition: a case study of the linguistic relativity hypothesis.

(cambridge university press, 1992).

[5] levinson sc, space in language and cognition: explorations in cognitive diversity. (cambridge univer-

sity press, 2003)

[6] choi s, bowerman m (1991) learning to express motion events in english and korean: the in   uence

of language-speci   c lexicalization patterns. cognition 41, 83-121.

[7] majid a, boster js, bowerman m (2008) cognition 109, 235-250.

[8] croft w (2010) relativity, linguistic variation and language universals. cognitextes 4, 303.

[9] evans n, levinson sc (2009) the myth of language universals: language diversity and its importance

for cognitive science. behav. brain sci. 21 429-492.

[10] comrie b language universals and linguistic typology, 2nd ed. (university of chicago press., 1989).

[11] croft w, typology and universals, 2nd ed. (cambridge university press. 2003).

[12] henrich j, heine sj, norenzayan a (2010) the weirdest people in the world? behav. brain sci. 33

1-75 (2010).

[13] shopen t (ed.), language typology and syntactic description, 2nd ed. (3 volumes) (cambridge univer-

sity press, cambridge, 2007).

[14] croft w, cruse da, cognitive linguistics. (cambridge university press. 2004).

[15] koptjevskaja-tamm m, vanhove m (2012) new directions in lexical typology. linguistics 50, 3.

[16] brown ch (1976) general principles of human anatomical partonomy and speculations on the growth

of partonomic nomenclature. am. ethnol. 3, 400-424 (1976).

[17] witkowski sr, brown ch (1978) lexical universals, ann. rev. of anthropol. 7 427-51.

[18] brown ch (1983) where do cardinal direction terms come from? anthropological linguistics 25, 121-

161.

[19] viberg   a (1983) the verbs of perception: a typological study. linguistics 21, 123-162.

[20] evans n, multiple semiotic systems, hyperpolysemy, and the reconstruction of semantic change in

australian languages. in diachrony within synchrony:

language history and cognition (peter lang.

frankfurt, 1992).

[21] derrig s (1978) metaphor in the color lexicon. chicago linguistic society, the parasession on the

lexicon 85-96.

[22] swadesh m (1952) lexico-statistical dating of prehistoric ethnic contacts. p. am. philos. soc. 96,

452-463.

[23] vygotsky l, thought and language. (mit press, cambridge, ma, 2002).

[24] critchlow de, pearl dk, qian cl (1996) the triples distance for rooted bifurcating phylogenetic trees.

syst. biol. 45, 323   334.

13

[25] dobson aj, comparing the shapes of trees, combinatorial mathematics iii, (springer-verlag, new

york 1975).

[26] robinson df, foulds lr (1981) comparison of phylogenetic trees. math. biosci. 53, 131   147.

[27] dunn m, et al. (2011) evolved structure of language shows lineage-speci   c trends in word-order uni-

versals. nature 473, 79-82.

[28] bouckaert r, et al. (2012) mapping the origins and expansion of the indo-european language family.

science 337, 957.

[29] fox a, linguistic reconstruction: an introduction to theory and method. (oxford university press.

1995).

[30] hock hh principles of historical linguistics. (mouton de gruyter, berlin, 1986).

[31] nichols j, the comparative method as heuristic. in the comparative method reviewed: regularity and

irregularity in language change (oxford university press, 1996).

[32] dryer ms (1989) large linguistic areas and language sampling. studies in language 13, 257-292.

[33] cover tm, and thomas ja, elements of id205, (wiley, new york, 1991).

[34] brown ch, a theory of lexical change (with examples from folk biology, human anatomical partonomy

and other domains). anthropol. linguist. 21, 257-276 (1979).

[35] brown ch & witkowski sr, figurative language in a universalist perspective. am. ethnol. 8 596-615

(1981).

appendix

a. criteria for selection of meanings

14

our translations use only lexical concepts as opposed to grammatical in   ections or function

words. for the purpose of universality and stability of meanings across cultures, we chose entries

from the swadesh 200-word list of basic vocabulary. among these, we have selected categories that

are likely to have single-word representation for meanings, and for which the referents are material

entities or natural settings rather than social or conceptual abstractions. we have selected 22 words

in domains concerning natural and geographic features, so that the web of polysemy will produce

a connected graph whose structure we can analyze, rather than having an excess of disconnected

singletons. we have omitted body parts   which by the same criteria would provide a similarly

appropriate connected domain   because these have been considered previously [16, 17, 34, 35].

the    nal set of 22 words are as follows:

    celestial phenomena and related time units:

star, sun, moon, year, day/daytime, night

    landscape features:

sky, cloud(s), sea/ocean, lake, river, mountain

    natural substances:

stone/rock, earth/soil, sand, ash(es), salt, smoke, dust, fire, water,

wind

b. language list

the languages included in our study are listed in tab. i. notes: oceania includes southeast

asia; the papuan languages do not form a single phylogenetic group in the view of most historical

linguists; other families in the table vary in their degree of acceptance by historical linguists. the

classi   cation at the genus level, which is of greater importance to our analysis, is generally agreed

upon.

region
africa

family
khoisan

genus
northern
central
southern

niger-kordofanian nw mande

language
ju|   hoan
khoekhoegowab
!x  o  o
bambara

15

nilo-saharan

afro-asiatic

eurasia basque

indo-european

uralic
altaic

japanese
chukotkan
caucasian

katvelian
dravidian
sino-tibetan

oceania hmong-mien
austroasiatic

daic
austronesian
papuan

australian

americas eskimo-aleut

na-dene

yor`ub  a
igbo
e   k
swahili
kanuri
ik
nandi

tumazabt
hausa
rendille
iraqi arabic
basque
armenian
hindi
albanian
spanish
russian
finnish
turkish
khalkha mongolian
japanese
itelmen (kamchadal)
kabardian
chechen
georgian
badaga
mandarin
karen (bwe)
mikir
hani
naxi
hmong njua
sora
minor mlabri
semai (sengoi)
thai
trukese
kwoma
yagaria
baruya

southern w. atlantic kisi
defoid
igboid
cross river
bantoid
saharan
kuliak
nilotic
bango-bagirmi-kresh kaba d  em  e
berber
west chadic
e cushitic
semitic
basque
armenian
indic
albanian
italic
slavic
finnic
turkic
mongolian
japanese
kamchatkan
nw caucasian
nax
kartvelian
dravidian proper
chinese
karen
kuki-chin-naga
burmese-lolo
naxi
hmong-mien
munda
palaung-khmuic
aslian
kam-tai
oceanic
middle sepik
e ng highlands
angan
c and se new guinea kolari
west bougainville
east bougainville
gunwinguan
maran
pama-nyungan
aleut
haida
athapaskan
algonquian
interior salish
wakashan
siouan
caddoan
iroquoian

rotokas
buin
nunggubuyu
mara
e and c arrernte
aleut
haida
koyukon
western abenaki
thompson salish
nootka (nuuchahnulth)
lakhota
pawnee
onondaga
coast tsimshian
klamath
wintu
northern sierra miwok
creek
itz  a maya
yana
cocopa
t  umpisa shoshone
hopi
quiavini zapotec
warao
mochica/chimu
huallaga quechua
mapudungun (mapuche)
guaran    
amarakaeri
piro
carib
yagua

algic
salishan
wakashan
siouan
caddoan
iroqoian
coastal penutian tsimshianic

gulf
mayan
hokan

uto-aztecan

otomanguean
paezan

klamath
wintuan
miwok
muskogean
mayan
yanan
yuman
numic
hopi
zapotecan
warao
chim  uan
quechua
araucanian
tup    -guaran    

quechuan
araucanian
tup    -guaran    
macro-arawakan har  akmbut

macro-carib

maipuran
carib
peba-yaguan

table i. the languages included in our study. the classi   cation at the genus level, which is of greater
importance to our analysis, is generally agreed upon.

16

variable

geography

climate

topography

literary tradition

size
subset
29
americas
20
eurasia
17
africa
15
oceania
38
humid
30
cold
13
arid
45
inland
coastal
36
some or long literary tradition 28
53
no literary tradition

table ii. various groups of languages based on nonlinguistic variables. for each variable we measured the
di   erence between the subsets    semantic networks, de   ned as the tree distance between the dendrograms of
swadesh words generated by spectral id91.

c. language groups

we performed several tests to see if the structure of the polysemy network (or whatever we   re

calling it) depends in a statistically signi   cant way on typological features, including the presence

or absence of a literary tradition, geography, topography, and climate. the typological features

tested, with the numbers of languages indicated for each feature shown in parentheses, are listed

in tab. ii

d. model for degree of polysemy

1. aggregation of language samples

17

we now consider more formally the reasons sample aggregates may not simply be presumed

as summary statistics, because they entail implicit generating processes that must be tested. by

demonstrating an explicit algorithm that assigns probabilities to samples of swadesh node degrees,

presenting signi   cance measures consistent with the aggregate graph and the sampling algorithm,

and showing that the languages in our dataset are typical by these measures, we justify the use

and interpretation of the aggregate graph (fig. 2 ).

we begin by introducing an error measure appropriate to independent sampling from a general

mean degree distribution. we then introduce calibrated forms for this distribution that reproduce

the correct sample means as functions of both swadesh-entry and language-weight properties.

the notion of consistency with random sampling is generally scale-dependent. in particular, the

existence of synonymous polysemy may cause individual languages to violate criteria of randomness,

but if the particular duplicated polysemes are not correlated across languages, even small groups of

languages may rapidly converge toward consistency with a random sample. therefore, we do not

present only a single acceptance/rejection criterion for our dataset, but rather show the smallest

groupings for which sampling is consistent with randomness, and then demonstrate a model that

reproduces the excess but uncorrelated synonymous polysemy within individual languages.

2.

independent sampling from the aggregate graph

figure 2 treats all words in all languages as independent members of an unbiased sample. to

test the appropriateness of the aggregate as a summary statistic, we ask: do random samples, with

link numbers equal to those in observed languages, and with link probabilities proportional to the

weights in the aggregate graph, yield ensembles of graphs within which the actual languages in

our data are typical?

statistical tests

the appropriate summary statistic to test for typicality, in ensembles produced by random

sampling (of links or link-ends) is the kullback-leibler (kl) divergence of the sample counts from

the probabilities with which the samples were drawn [33]. this is because the kl divergence is the

leading exponential approximation (by stirling   s formula) to the log of the multinomial distribution

18

produced by poisson sampling.

the appropriateness of a random-sampling model may be tested independently of how the

aggregate link numbers are used to generate an underlying id203 model. in this section, we

will    rst evaluate a variety of underlying id203 models under poisson sampling, and then we

will return to tests for deviations from independent poisson samples. we    rst introduce notation:

for a single language, the relative degree (link frequency), which is used as the id172

of a id203, is denoted as pdata

languages, the link frequency of a single entry relative to the total n will be denoted pdata

s|l     nl

s /nl, and for the joint con   guration of all words in all
sl    

s /n =(cid:0)nl

s /nl(cid:1)(cid:0)nl/n(cid:1)

nl

    pdata

s|l pdata
l .

corresponding to any of these, we may generate samples of links to de   ne the null model for

a random process, which we denote   nl

s ,   nl, etc. we will generally use samples with exactly the
same number of total links n as the data. the corresponding sample frequencies will be denoted
by psample

s|l       nl
finally, the calibrated model, which we de   ne from properties of aggregated graphs, will be

s /  nl(cid:1)(cid:0)  nl/n(cid:1)

s /n =(cid:0)  nl

s /  nl and psample

    psample

, respectively.

psample
l

      nl

s|l

sl

the prior id203 from which samples are drawn to produce p-values for the data. we denote

the model probabilities (which are used in sampling as    true    probabilities rather than sample

frequencies) by pmodel

s|l , pmodel

sl , and pmodel

l

.

s

s|l

order, as

for language l, the multinomial

for nl links sampled independently from the distribution psample

(cid:9) may be written, using stirling   s formula to leading exponential
id203 of a particular set(cid:8)nl
| nl(cid:1)
(cid:9)
p(cid:0)(cid:8)nl
(cid:13)(cid:13)(cid:13) pmodel
(cid:17)

(cid:13)(cid:13)(cid:13) pmodel
       psample

where the kullback-leibler (kl) divergence [33]

(cid:88)

psample
s|l

psample
s|l

sample
s|l

   nld

    e

(cid:16)

s|l

(2)

(3)

log

(cid:16)

(cid:17)

s|l

d

s

p

s|l
pmodel
s|l

   

s

for later reference, note that the leading quadratic approximation to eq. (3) is

(cid:16)

nld

psample
s|l

(cid:13)(cid:13)(cid:13) pmodel

s|l

(cid:17)

1
2

   

(cid:16)

(cid:88)

s

  nl
s     nlpmodel
s|l
nlpmodel

s|l

,

(4)

so that the variance of    uctuations in each word is proportional to its expected frequency.

as a null model for the joint con   guration over all languages in our set, if n links are drawn

       .
(cid:17)2

independently from the distribution psample

sl

, the multinomial id203 of a particular set(cid:8)nl

(cid:9)

19

s

is given by

where1

(cid:16)

d

s

(cid:9)

| n(cid:1)
p(cid:0)(cid:8)nl
(cid:13)(cid:13)(cid:13) pmodel
(cid:17)
(cid:13)(cid:13)(cid:13) pmodel

   

sl

l

(cid:17)

(cid:16)

   n d

sample
sl

p

(cid:13)(cid:13)(cid:13) pmodel

sl

(cid:17)

    e

(cid:32)
(cid:16)

psample
sl

log

psample
sl
pmodel
sl

psample
l

d

psample
s|l

(cid:88)
(cid:88)

s,l

+

l

(cid:33)
(cid:13)(cid:13)(cid:13) pmodel

s|l

(5)

(6)

(cid:17)

.

psample
sl

(cid:16)

= d

psample
l

multinomial samples of assignments   nl

n links total drawn from distribution pl
s

s to each of the 22    81 (swadesh, language) pairs, with
null, will produce kl divergences uniformly distributed in
the coordinate d       e   dklddkl, corresponding to the uniform increment of cumulative proba-
d(cid:0)pdata

(cid:1) (one-sided p-value), in the distribution of samples   nl

bility in the model distribution. we may therefore use the cumulative id203 to the right of

(cid:13)(cid:13) pmodel

s , as a test of consistency of

sl

sl

our data with the model of random sampling.

in the next two subsections we will generate and test candidates for pmodel which are di   erent

functions of the mean link numbers on swadesh concepts and the total links numbers in languages.

product model with intrinsic property of concepts

in general we wish to consider the consistency of joint con   gurations with random sampling, as

a function of an aggregation scale. to do this, we will rank-order languages by increasing nl, form

non-overlapping bins of 1, 3, or 9 languages, and test the resulting binned degree distributions

against di   erent mean-degree and sampling models. we denote by (cid:10)nl(cid:11) the average total link
number in a bin, and by(cid:10)nl
(cid:11) the average link number per swadesh entry in the bin. the simplest

s

model, which assumes no interaction between concept and language properties, makes the model

id203 pmodel

sl

as

a product of its marginals. it is estimated from data without regard to binning,

pproduct
sl

ns
n   

nl
n

.

   

(7)

1 as long as we calibrate pmodel

to agree with the per-language link frequencies nl/n in the data, the data will
always be counted as more typical than almost-all random samples, and its id203 will come entirely from the
kl divergences in the individual languages.

l

20

plots for the data nl

in accordance with fig. 4s (f). the colors denote
fig. 5.
corresponding numbers of the scale. the original data in the    rst panel with the sample in the last panel
seems to agree reasonably well.

s , n pproduct

, nsample

sl

sl

the 22    81 independent mean values are thereby speci   ed in terms of 22 + 81 sample estimators.
the kl divergence of the joint con   guration of links in the actual data from this model, under

whichever binning is used, becomes

(cid:16)

d

pdata
sl

(cid:13)(cid:13)(cid:13) pmodel

sl

(cid:17)

= d

(cid:11)

(cid:32)(cid:10)nl

s
n

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ns

n

(cid:33)

(cid:10)nl(cid:11)

n

(8)

as we show in fig. 7 below, even for 9-language bins which we expect to average over a large

amount of language-speci   c    uctuation, the product model is ruled out at the 1% level.

we now show that a richer model, describing interaction between word and language proper-

ties, accepts not only the 9-language aggregate, but also the 3-language aggregate with a small

adjustment of the language size to which words respond (to produce consistency with word-size

and language-size marginals). only    uctuation statistics at the level of the joint con   guration of

81 individual languages remains strongly excluded by the null model of random sampling.

product model with saturation

an inspection of the deviations of our data from the product model shows that the initial

propensity of a word to participate in polysemies, as inferred in languages where that word has

few links, in general overestimates the number of links (degree). put it di   erently, languages seem

languagesswadeshes21

fig. 6. plots of the saturating function (9) with the parameters given in table iii, compared to (cid:10)nl
(cid:11)
(ordinate) in 9-language bins (to increase sample size), versus bin-averages(cid:10)nl(cid:11) (abscissa). red line is drawn

s

through data values, blue is the product model, and green is the saturation model. water requires no
signi   cant deviation from the product model (bwater/n (cid:29) 20), while moon shows the lowest saturation
value among the swadesh entries, at bmoon     3.4.

to place limits on the weight of single polysemies, favoring distribution over distinct polysemies,

but the number of potential distinct polysemies is an independent parameter from the likelihood

that the available polysemies will be formed. interpreted in terms of our supposed semantic space,

the proximity of target words to a swadesh entry may determine the likelihood that they will be

polysemous with it, but the total number of proximal targets may vary independently of their

absolute proximity. these limits on the number of neighbors of each concept are captured by

additional 22 parameters.

to accommodate such characteristic, we revise the model eq. (7) to the following function:

where degree numbers (cid:10)nl

s

(cid:11) for each swadesh s is proportional to as and language size, but is

bounded by bs, the number of proximal concepts. the corresponding model id203 for each

language then becomes

psat
sl =

(as/bs)(nl/n )

1 + nl/bs

l
1 + pdata

  pspdata
l n/bs

.

   

(9)

as all bs/n         we recover the product model, with pdata

l     nl/n and   ps     ns/n .

a    rst-level approximation to    t parameters as and bs is given by minimizing the weighted

(cid:10)nl(cid:11)

as
bs + (cid:104)nl(cid:105)

.

020406001020304002040600510152025<nl>binnls=waternls=moon<nl>bin22

meaning category saturation: bs propensity   ps
0.025
star
0.126
sun
year
0.021
0.080
sky
0.026
sea/ocean
stone/rock
0.041
0.049
mountain
0.087
day/daytime
0.026
sand
ash(es)
0.068
0.007
salt
0.065
fire
smoke
0.031
0.034
night
0.065
dust
river
0.048
0.073
water
0.047
lake
moon
0.997
0.116
earth/soil
0.033
cloud(s)
wind
0.051

1234.2
25.0
1234.2
1234.2
1234.2
1234.2
1085.9
195.7
1234.2
13.8
1234.2
1234.2
1234.2
89.3
246.8
336.8
1234.2
1234.2
1.2
1234.2
53.4
1234.2

table iii. a table of    tted values of parameters bs and   ps for the saturation model of eq. (9) . the
saturation value bs is an asymtotic number of meanings associated with the entry s, and the propensity
  ps is a rate at which the number of polysemies increases with nl at low nl
s .

mean-square error

(cid:88)

l

1
(cid:104)nl(cid:105)

(cid:32)(cid:10)nl

s

(cid:11)

(cid:88)

s

   

(cid:33)2

.

(cid:10)nl(cid:11)

as
bs + (cid:104)nl(cid:105)

e    

(10)

(cid:10)nl(cid:11), propor-

the function (10) assigns equal penalty to squared error within each language bin    
tional to the variance expected from poisson sampling. the    t values obtained for as and bs do

not depend sensitively on the size of bins except for the swadesh entry moon in the case where all

81 single-language bins are used. moon has so few polysemies, but the moon/month polysemy

is so likely to be found, that the language itelman, with only one link, has this polysemy. this point

leads to instabilities in    tting bmoon in single-language bins. for bins of size 3   9 the instability

is removed. representative    t parameters across this range are shown in table iii. examples of

the saturation model for two words, plotted against the 9-language binned degree data in fig. 6,

23

id181 of link frequencies in our data, grouped into non-overlapping 9-
fig. 7.
language bins ordered by rank, from the product distribution (7) and the saturation model (9). parameters
as and bs have been adjusted (as explained in the text) to match the word- and language-marginals. from
10,000 random samples   nl
s , (green) histogram for the product model; (blue) histogram for the saturation
model; (red dots) data. the product model rejects the 9-language joint binned con   guration at the at
1% level (dark shading), while the saturation model is typical of the same con   guration at     59% (light
shading).

show the range of behaviors spanned by swadesh entries.

the least-squares    ts to as and bs do not directly yield a id203 model consisent with the

marginals for language size that, in our data, are    xed parameters rather than sample variables
sl     ns (deviations < 1 link
sl (cid:54)= nl. we corrected for this by altering the
saturation model to suppose that, rather than word properties    interacting with the exact value

to be explained. they closely approximate the marginal n (cid:80)
for every s) but lead to mild violations n(cid:80)
nl, they interact with a (word-independent but language-dependent) multiplier (cid:0)1 +   l(cid:1) nl, so

l psat

s psat

that the model for nl

s in each language becomes becomes

(cid:0)1 +   l(cid:1) nl

as
bs + (1 +   l) nl ,

newton   s method to produce n(cid:80)

in terms of the least-squares coe   cients as and bs of table iii. the values of   l are solved with
sl     ns
within small fractions of a link. the resulting adjustment parameters are plotted versus nl for

sl     nl, and we checked that they preserve n(cid:80)

l psat

s psat

individual languages in fig. 8. although they were computed individually for each l, they form

a smooth function of nl, possibly suggesting a re   nement of the product model, but also perhaps

re   ecting systematic interaction of small-language degree distributions with the error function (10).

0.040.0450.050.0550.060.0650.070.0750.08050100150200250300350400450500dkl(samples || saturation model)histogram counts24

fig. 8. plot of the correction factor   l versus nl for individual languages in the id203 model used
in text, with parameters bs and   ps shown in table iii. although   l values were individually solved with
newton   s method to ensure that the id203 model matched the whole-language link values, the resulting
correction factors are a smooth function of nl.

fig. 9. the same model parameters as in fig. 7 is now marginally plausible for the joint con   guration
of 27 three-language bins in the data, at the 7% level (light shading). for reference, this    ne-grained joint

con   guration rejects the null model of independent sampling from the product model at p     value     10   3

(dark shading in the extreme tail). 4000 samples were used to generate this test distribution. the blue
histogram is for the saturation model, the green histogram for the product model, and the red dots are
generated data.

with the resulting joint distribution psat

sl, tests of the joint degree counts in our dataset for
consistency with multinomial sampling in 9 nine-language bins are shown in fig. 7, and results of

tests using 27 three-language bins are shown in fig. 9. binning nine languages clearly averages

over enough language-speci   c variation to make the data strongly typical of a random sample
(p     59%), while the product model (which also preserves marginals) is excluded at the 1%

-0.5-0.4-0.3-0.2-0.100.10.2010203040506070lnl  0.150.160.170.180.190.20.210.220.23020406080100120140160180200dkl(samples || saturation model)histogram countslevel. the marginal acceptance of the data even for the joint con   guration of three-language bins
(p     7%) suggests that language size nl is an excellent explanatory variable and that residual
language variations cancel to good approximation even in small aggregations.

25

3. single instances as to aggregate representation

the preceding subsection showed intermediate scales of aggregation of our language data are

su   ciently random that they can be used to re   ne id203 models for mean degree as a func-

tion of parameters in the globally-aggregated graph. the saturation model, with data-consistent

marginals and multinomial sampling, is weakly plausible by bins of as few as three languages.

down to this scale, we have therefore not been able to show a requirement for deviations from the

independent sampling of links entailed by the use of the aggregate graph as a summary statistic.

however, we were unable to    nd a further re   nement of the mean distribution that would repro-

duce the properties of single language samples. in this section we characterize the nature of their

deviation from independent samples of the saturation model, show that it may be reproduced by

models of non-independent (clumpy) link sampling, and suggest that these re   ect excess synony-

mous polysemy.

power tests and uneven distribution of single-language p-values

to evaluate the contribution of individual languages versus language aggregates to the accep-

tance or rejection of random-sampling models, we computed p-values for individual languages or

language bins, using the kl-divergence (3). a plot of the single-language p-values for both the

null (product) model and the saturation model is shown in fig. 10. histograms for both single

languages (from the values in fig. 10) and aggregate samples formed by binning consecutive groups

of three languages are shown in fig. 11.

for samples from a random model, p-values would be uniformly distributed in the unit interval,

and histogram counts would have a multinomial distribution with single-bin    uctuations depending

on the total sample size and bin width. therefore, fig. 11 provides a power test of our summary

statistics. the variance of the multinomial may be estimated from the large-p-value body where

the distribution is roughly uniform, and the excess of counts in the small-p-value tail, more than

one standard deviation above the mean, provides an estimate of the number of languages that can

be con   dently said to violate the random-sampling model.

from the upper panel of fig. 11, with a total sample of 81 languages, we can estimate a number

26

of     0.05    81     4     5 excess languages at the lowest p-values of 0.05 and 0.1, with an additional
2   3 languages rejected by the product model in the range p-value     0.2. comparable plots in
fig. 11 (lower panel) for the 27 three-language aggregate distributions are marginally consistent

with random sampling for the saturation model, as expected from fig. 9 above. we will show in

the next section that a more systematic trend in language    uctuations with size provides evidence

that the cause for these rejections is excess variance due to repeated attachment of links to a subset

of nodes.

log10(p   value) by kl divergence, relative to 4000 random samples per language, plotted versus
fig. 10.
language rank in order of increasing nl. product model (green) shows equal or lower p-values for almost
all languages than the saturation model (blue). three languages     basque, haida, and yor`ub  a     had value
p = 0 consistently across samples in both models, and are removed from subsequent regression estimates.
a trend toward decreasing p is seen with increase in nl.

excess    uctuations in degree of polysemy

if we de   ne the size-weighted relative variance of a language analogously to the error term in

eq. (10), as

(cid:0)  2(cid:1)l

1
nl

   

(cid:88)

(cid:16)

nl
s     nlpmodel
s|l

(cid:17)2

,

s

fig. 12 shows that     log10(p   value) has high rank correlation with (cid:0)  2(cid:1)l and a roughly linear
2 recall from eq. (4) that the leading quadratic term in the kl-divergence di   ers from(cid:0)  2(cid:1)l in that it presumes

regression over most of the range.2 two languages (itelmen and hindi), which appear as large

outliers relative to the product model, are within the main dispersion in the saturation model,

poisson    uctuation with variance nlpmodel
all words in a language. the relative variance is thus a less speci   c error measure.

s|l

at the level of each word, rather than uniform variance     nl across

(11)

0102030405060708090   4   3.5   3   2.5   2   1.5   1   0.50log10(p)language (rank)27

fig. 11.
(upper panel) normalized histogram of p-values from the 81 languages plotted in fig. 10. the
saturation model (blue) produces a fraction     0.05    81     4     5 languages in the lowest p-values {0.05, 0.1}
above the roughly-uniform background for the rest of the interval (shaded area with dashed boundary). a
further excess of 2   3 languages with p-values in the range [0, 0.2] for the product model (green) re   ects the
part of the mismatch corrected through mean values in the saturation model. (lower panel) corresponding
histogram of p-values for 27 three-language aggregate degree distributions. saturation model (blue) is now
marginally consistent with a uniform distribution, while the product model (green) still shows slight excess
of low-p bins. coarse histogram bins have been used in both panels to compensate for small sample numbers
in the lower panel, while producing comparable histograms.

showing that their discrepency is corrected in the mean link number. we may therefore understand

a large fraction of the imid203 of languages as resulting from excess    uctuations of their degree

numbers relative to the expectation from poisson sampling.

fig. 13 then shows the relative variance from the saturation model, plotted versus total average

link number for both individual languages and three-language bins. the binned languages show no

signi   cant regression of relative variance away from the value unity for poisson sampling, whereas

00.10.20.30.40.50.60.70.80.9100.050.10.150.20.250.30.35p (bin center)fraction of languages with id203 punambiguous excesslow-p samples00.10.20.30.40.50.60.70.80.9100.050.10.150.20.250.30.35p (bin center)fraction of languages with id203 p28

(upper panel:)     log10(p ) plotted versus relative variance (cid:0)  2(cid:1)l

from eq. (11) for the 78
fig. 12.
(blue) saturation model; (green) product model. two
languages with non-zero p-values from fig. 10.
languages (circled) which appear as outliers with anomalously small relative variance in the product model
    itelman and hindi     disappear into the central tendency with the saturation model. (lower panel:) an
equivalent plot for 26 three-language bins. notably, the apparent separation of individual large-nl langauges

into two groups has vanished under binning, and a unimodal and smooth dependence of     log10(p ) on(cid:0)  2(cid:1)l

is seen.

single languages show a systematic trend toward larger variance in larger languages, a pattern that

we will show is consistent with    clumpy    sampling of a subset of nodes. the disappearance of

this clumping in binned distributions shows that the clumps are uncorrelated among languages at

similar nl.

00.511.522.5300.511.522.53(  2)l- log10(p)itelmenhindi0.40.60.811.21.41.61.8200.511.522.533.5(  2)l- log10(p)29

relative variance from the saturation model versus total link number nl for 78 languages
fig. 13.
excluding basque, haida, and yor`ub  a. least-squares regression are shown for three-language bins (green)
and individual languages (blue), with regression coe   cients inset. three-language bins are consistent with
poisson sampling at all nl, whereas single languages show systematic increase of relative variance with
increasing nl.

01020304050607000.511.522.53nl(  2)l(  2)l = 0.011938 nl + 0.87167(  2)l = 0.0023919 nl + 0.9417830

correlated link assignments

we may retain the mean degree distributions, while introducing a systematic trend of relative

variance with nl, by modifying our sampling model away from strict poisson sampling to introduce

   clumps    of links. to remain within the use of minimal models, we modify the sampling procedure

by a single parameter which is independent of word s, language-size nl, or particular language l.

we introduce the sampling model as a function of two parameters, and show that one function

of these is constrained by the regression of excess variance. (the other may take any interior value,
so we have an equivalence class of models.) in each language, select a number b of swadesh entries
randomly. let the swadesh indices be denoted {s  }     1,...b. we will take some fraction of the total
links in that language, and assign them only to the swadeshes whose indices are in this privileged

set. introduce a parameter q that will determine that fraction.

we require correlated link assignments be consistent with the mean determined by our model

   t, since binning of data has shown no systematic e   ect on mean parameters. therefore, for the
random choice {s  }     1,...b, introduce the normalized density on the privileged links

(12)

if s     {s  }     1,...b and   s|l = 0 otherwise. denote the aggregated weight of the links in the
priviledged set by

s|l(cid:80)b

pmodel
  =1 pmodel
s  |l

  s|l    

b(cid:88)

  =1

w    

ps  |l.

(13)

then introduce a modi   ed id203 distribution based on the randomly selected links, in the

form

  ps|l     (1     qw ) ps|l + qw   s|l.

(14)

multinomial sampling of nl links from the distribution   ps|l will produce a size-dependent variance
of the kind we see in the data. the expectated degrees given any particular set {s  } will not
agree with the means in the aggregate graph, but the ensemble mean over random samples of

languages will equal ps|l, and binned groups of languages will converge toward it according to the

central-limit theorem.

the proof that the relative variance increases linearly in nl comes from the expansion of the

expectation of eq. (11) for random samples, denoted

(cid:43)

(cid:17)2

   

1
nl

(cid:42)
(cid:68)(cid:0)    2(cid:1)l(cid:69)
(cid:42)
(cid:104)(cid:0)  nl
(cid:88)
(cid:42)
(cid:88)
(cid:0)  nl
(cid:42)(cid:88)

1
nl

1
nl

=

=

s

s

nl

s

(cid:88)

(cid:16)

s

s     nlpmodel
  nl
s|l
(cid:1) + nl(cid:16)
(cid:43)
s     nl   ps|l
(cid:1)2
(cid:43)
s     nl   ps|l
(cid:16)

(cid:17)2

+

  ps|l     pmodel
s|l

.

  ps|l     pmodel
s|l

(cid:43)

(cid:17)(cid:105)2

31

(15)

the    rst expectation over   nl

s is constant (of order unity) for poisson samples, and the second
expectation (over the sets {s  } that generate   ps|l) does not depend on nl except in the prefactor.
cross-terms vanish because link samples are not correlated with samples of {s  }. both terms in
the third line of eq. (15) scale under binning as (bin-size)0. the    rst term is invariant due to

poisson sampling, while in the second term, the central-limit theorem reduction of the variance in

samples over   ps|l cancels growth in the prefactor nl due to aggregation.

because the linear term in eq. (15) does not systematically change under binning, we interpret

the vanishing of the regression for three-language bins in fig. 13 as a consequence of    tting of the
mean value to binned data as sample estimators.3 therefore, we require to choose parameters b
and q so that regression coe   cients in the data are typical in the model of clumpy sampling, while

regressions including zero have non-vanishing weight in models of three-bin aggregations.

fig. 14 compares the range of regression coe   cients obtained for random samples of languages

with the values (cid:8)nl(cid:9) in our data, from either the original saturation model psat

s|l, or the clumpy
model   ps|l randomly re-sampled for each language in the joint con   guration. parameters used
were (b = 7, q = 0.975).4 with these parameters,     1/3 of links were assigned in excess to     1/3
of words, with the remaining 2/3 of links assigned according to the mean distribution.

the important features of the graph are: 1) binning does not change the mean regression
coe   cient, verifying that eq. (15) scales homogeneously as (bin-size)0. however, the variance for

binned data increases due to reduced number of sample points; 2) the observed regression slope

0.012 seen in the data is far out of the support of multinomial sampling from psat

these parameters, it becomes typical under(cid:8)  ps|l

(cid:9) while still leaving signi   cant id203 for the

s|l, whereas with

3 we have veri   ed this by generating random samples from the model (15),    tting a saturation model to binned
sample con   gurations using the same algorithms as we applied to our data, and then performing regressions
equivalent to those in fig. 13. in about 1/3 of cases the    tted model showed regression coe   cients consistent with
zero for three-language bins. the typical behavior when such models were    t to random sample data was that the
three-bin regression coe   cient decreased from the single-language regression by     1/3.
4 solutions consistent with the regression in the data may be found for b ranging from 3   17. b = 7 was chosen as
an intermediate value, consistent with the typical numbers of nodes appearing in our samples by inspection.

32

(cid:9) either generated by poisson

fig. 14. histograms of regression coe   cients for language link samples(cid:8)  nl

s

sampling from the saturation model pmodel
   tted to the data (blue), or drawn from clumped probabilities
  ps|l de   ned in eq. (14), with the set of privileged words {s  } independently drawn for each language
(green). solid lines refer to joint con   gurations of 78 individual languages with the nl values in fig. 13.
dashed lines are 26 non-overlapping three-language bins.

s|l

three-language binned regression around zero (even without ex-post    tting).

   0.02   0.0100.010.020.030.040.0501234567x 105