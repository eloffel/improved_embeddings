6
1
0
2

 
r
p
a
5
1

 

 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 

2
v
7
7
2
0
0

.

0
1
5
1
:
v
i
x
r
a

similarity of symbol frequency distributions with heavy tails

martin gerlach,1 francesc font-clos,2, 3 and eduardo g. altmann1

1max planck institute for the physics of complex systems, d-01187 dresden, germany
2centre de recerca matem`atica, edi   ci c, campus bellaterra, e-08193 barcelona, spain.

3departament de matem`atiques, facultat de ci`encies,

universitat aut`onoma de barcelona, e-08193 barcelona, spain

quantifying the similarity between symbolic sequences is a traditional problem in information
theory which requires comparing the frequencies of symbols in di   erent sequences. in numerous
modern applications, ranging from dna over music to texts, the distribution of symbol frequencies
is characterized by heavy-tailed distributions (e.g., zipf   s law). the large number of low-frequency
symbols in these distributions poses major di   culties to the estimation of the similarity between
sequences, e.g., they hinder an accurate    nite-size estimation of entropies. here we show analyti-
cally how the systematic (bias) and statistical (   uctuations) errors in these estimations depend on
the sample size n and on the exponent    of the heavy-tailed distribution. our results are valid
for the shannon id178 (   = 1), its corresponding similarity measures (e.g., the jensen-shanon
divergence), and also for measures based on the generalized id178 of order   . for small      s,
including    = 1, the errors decay slower than the 1/n -decay observed in short-tailed distributions.
for    larger than a critical value       = 1 + 1/       2, the 1/n -decay is recovered. we show the
practical signi   cance of our results by quantifying the evolution of the english language over the
last two centuries using a complete   -spectrum of measures. we    nd that frequent words change
more slowly than less frequent words and that    = 2 provides the most robust measure to quantify
language change.

i.

introduction

quantifying the similarity of symbolic sequences is a
classical problem in id205 [1] with modern
applications in linguistics [2], genetics [3], and image pro-
cessing [4]. the availability of large databases of texts
sparked a renewed interest in the problem of similarity of
the vocabulary of di   erent collections of texts [5   9]. for
instance, fig. 1 shows the word-frequency distribution in
three large collections of english texts: from 1850, 1900,
and 1950. we see that the distribution itself remains
essentially the same, a heavy-tailed zipf distribution [10]

p(r)     r     ,

(1)

where p is the frequency of the r-th most frequent word
and    (cid:39) 1. changes are seen in the frequency p (or
rank) of speci   c words, e.g., ship lost and genetic won
popularity. measures that quantify such changes are es-
sential to answer questions such as: is the vocabulary
from 1900 more similar to the one from 1850 or to the
one from 1950? how similar are two vocabularies (e.g.,
from di   erent years)? are the two    nite-size observations
compatible with a    nite sample of the same underlying
vocabulary? how similar are the vocabulary of di   erent
authors or disciplines? how fast is the lexical change
taking place?

heavy-tailed and broad distributions of

symbol-
frequencies such as eq. (1) are typical in natural lan-
guages [10, 12   16] and appear also in the dna (id165s
of base pairs for large n) [17], in gene expression [18], and
in music [19]. the slow decay observed in a broad range
of frequencies implies that there is no typical frequency
for words and therefore relevant changes can occur in
di   erent ranges of the p-spectrum, from the few large-

fig. 1. the english vocabulary in di   erent years. rank-
frequency distribution p(r) of individual years t for t = 1850,
1900, and 1950 of the google-ngram database [11], multi-
plied by a factor of 1, 2, and 4, respectively, for better vi-
sual comparison. the inset shows the original un-transformed
data (same axis), highlighting that the rank-frequency distri-
butions are almost indistinguishable. individual words (e.g.
   and   ,   see   ,   ship   ,   genetic   ) show changes in rank and fre-
quency (symbols), where words with larger ranks (i.e. smaller
frequencies) show larger change.

frequency words all the way to the many low-frequency
words. this imposes a challenge to de   ne similarity mea-
sures that are able to account for this variability and that
also yield accurate estimations based on    nite-size obser-
vations.

in this paper we quantify the vocabulary similarity us-
ing a spectrum of measures d   based on the general-
ized id178 of order    (d  =1 recovers the usual jensen-
shannon divergence). we show how varying    magni   es

100101102103104105106ranks:r10   810   710   610   510   410   310   210   1100frequency:p(r)t=1950t=1900t=1850   :   and   (cid:3):   see   o:   ship   m:   genetic   di   erences in the vocabulary at di   erent scales of the
(heavy-tailed) frequency spectrum, thus providing dif-
ferent information on the vocabulary change. we then
compute the accuracy (bias) and precision (variance) of
estimations of d   based on sequences of size n and
   nd that in heavy-tailed distributions the convergence
is much slower than in non-heavy-tailed distributions (it
often scales as 1/n    with    < 1). finally, we come back
to the problem of comparing the english vocabulary in
the last two centuries in order to illustrate the relevance
of our general results.

ii. definition

the

consider

id203

distribution

=
(p1, p2, . . . , ps) of a random variable over a discrete,
countable set of symbols i = 1, . . . , s (where later we
include the possibility for s        ). from an information
theory standpoint, a natural measure to quantify the
di   erence between two such id203 distributions p
and q is the jensen-shannon divergence (jsd) [20]

p

d(p, q) = h

    1
2

h(p)     1
2

h(q),

(2)

where h is the shannon id178 [21]

(cid:18) p + q
(cid:19)
h(p) =    (cid:88)

2

pi log pi.

(3)

i

(cid:112)

this de   nition has several properties which are useful in
the interpretation as a distance: i) d(p, q)     0 where the
equality holds if and only if p = q; ii) d(p, q) = d(q, p)
(it is a symmetrized kullback-leiber-divergence [20]); iii)
d(p, q) ful   lls the triangle inequality and thus is a
metric [22]; and iv) d(p, q) equals the mutual informa-
tion of variables sampled from p and q [3], i.e., d(p, q)
equals the average amount of information in one ran-
domly sampled word-token about which of the two dis-
tribution it was sampled from [23]. the jsd is widely
used in the statistical analysis of language [2], e.g. to au-
tomatically    nd individual documents that are (seman-
tically) related [5, 6] or to track the rate of evolution in
the lexical inventory of a language over historical time
scales [7, 8].

here we also consider the generalization of jsd in
which h in eq. (3) is replaced by the generalized id178
of order    [24]

(cid:32)(cid:88)

(cid:33)
i     1
p  

i

h  (p) =

1

1       

yielding a spectrum of divergence measures d   param-
eterized by   ,    rst introduced in ref. [25]. the usual
jsd is retrieved for    = 1. in (non-extensive) statistical
mechanics, eq. (4) has been    rst proposed in ref. [26]
and d   is sometimes called jensen-tsallis divergence.

2

while similar generalizations can be achieved with other
formulations of generalized entropies such as the renyi-
id178 [4, 27], the corresponding divergences can be-
come negative. in contrast, d   is strictly non-negative
and it was recently shown that
d  (p, q) is a metric
for any        (0, 2] [28]. for heavy-tailed distributions,
eq. (1), h   <     for    > 1/  .

(cid:112)

we de   ne a normalized version of d   as

  d  (p, q) =

d  (p, q)
dmax
   (p, q)

(5)

(cid:19)

2

1       

where

dmax

   (p, q) =

21          1

2

(cid:18)

h   (p) + h   (q) +

(6)
is the maximum possible d   between p and q obtained
assuming that the the set of symbols in each distribution
(i.e., the support of p and q) are disjoint. the main moti-
vation for using the measure (5) is that   d  (p, q)     [0, 1],
while the range of admissible values of d   depends on
  . this allows for a meaningful comparison of the diver-
gences   d  (p, q) and   d  (cid:48)(p, q) for    (cid:54)=   (cid:48) and therefore
also for the full spectrum of      s. in general, the metric
properties of d   are not preserved by   d  . an exception
is the case in which the rank-frequency distribution p(r)
underlying all p   s and q   s is invariant (see fig. 1). noting
that eq. (6) is independent of the symbols we obtain that
dmax
   (p, q) is a constant for all p   s and q   s and therefore
the metric property is preserved for   d  .

iii.

interpretation

in order to clarify the interpretation of d  , it is use-
ful to consider a toy model. as in fig. 1, we consider
two distributions p and q that have exactly the same
rank-frequency distribution p(r) but di   er in (a subset
of) the symbols they use. for simplicity, we consider
that symbols that di   er in the two cases appear only
in one of the distributions. more precisely, denoting by
ip = {a, b, c, d, e, . . .} the set of symbols in p we de-
   ne the set of replaced symbols as i        ip. the set of
symbols in q is chosen as iq = {i|i     ip \ i   }   {i   |i     i   }
with probabilities pi = qi for i     ip \ i    and pi = qi    for
i     i   , see fig. 2 for one example.
i   , we compute d  (p, i   )     d  (p, q) as

for a given distribution p and a set of replaced symbols

,

(4)

d  (p, i   ) = c  

p  
i ,

(7)

where c   = (2(1     )     1)/(1       ). the maximum is given
by

dmax

   (p, i   ) = c  

p  
i

(8)

(cid:88)

i   i   

(cid:88)

i   ip

such that

(cid:80)
(cid:80)

i   i    p  
i
p  
i   ip
i

  d  (p, i   ) =

.

(9)

this shows that each symbol i     i    that is replaced by
a new symbol contributes p  
it is thus clear
i
that varying   , the contribution of di   erent frequencies
become magni   ed (e.g. for    (cid:29) 1 large frequencies are
enhanced while for    < 0 low frequencies contribute more
to d   than large frequencies).

to d  .

in particular, for    = 0,   d  =0(p, i   ) = |i   |
|ip|

is the frac-
tion of symbols (types) that are di   erent in p and q. each
symbol i counts the same irrespective of their probabili-
(cid:28) 1,   d  =0(p, i   ) = 1     j(ip, iq), where
ties pi. for |i   |
|ip|
j(ip, iq) = |ip   iq|
is the jaccard-coe   cient between the
|ip   iq|
two sets ip and iq, an ad-hoc de   ned similarity mea-
sure widely used in information retrieval [2]. for    = 1,
i   i    pi showing that each replaced sym-
bol is weighted by its id203 pi and thus that   d  =1
measures the distance in terms of tokens.

  d  =1(p, i   ) =(cid:80)

the full spectrum   d   o   ers information on changes in
all frequencies, a point which is particularly important
for the case of heavy-tailed distributions because word-
frequencies vary over many orders of magnitude. figure 3
illustrates how di   erent values of    are able to capture
changes at di   erent regions in the frequency-spectrum.
in particular, it shows that   d   grows (decays) with   
when the modi   ed symbols have high (low) frequency.
furthermore, the comparison between two given changes
allow us to conclude about which change was more sig-
ni   cant at di   erent regions of the word-frequency spec-
trum.
in the example of the    gure, both changes (the
two lines) are equally signi   cant from the point of view
of the modi   ed tokens (   d1 are the same), the change in
the left a   ects more types (   d0 is larger), and the change
in the right a   ects more frequent words (   d   is larger for
   (cid:29) 1).

3

fig. 3. the spectrum   d  (p, i   ) for two di   erent changes.
the lines correspond to eq. (9) with pi     i   1 with i =
1, 2, . . . , 1000 and two di   erent sets of replaced symbols i   1 , i   2 .
right inset: i   1 = {1}, i.e., only the symbol with the high-
est id203, pi=1     0.13 is changed. left inset: i   2 =
{368, . . . , 1000}, i.e, the symbols with small id203 are re-
pi     pi=1

placed. the choice of i   2 was made such that(cid:80)

and therefore   d  =1(p, i   1 )       d  =1(p, i   2 ).

i   i   2

iv. finite-size estimation

in this section we turn to the estimation of   d   from
data. even if   d   is de   ned with respect to distributions
p and q, eq. (5), in practice these distributions are es-
timated from sequences with    nite-size n (total number
of symbols or word tokens) yielding    nite size estimates
of the distributions   p and   q. the main obstacle in ob-
taining accurate estimates of   d   is that it requires the
estimation of entropies for which, in general, unbiased
estimators do not exist [29]. accordingly, even if p = q,
in practice h  (   p) (cid:54)= h  (   q) and   d  (   p,   q) > 0 are mea-
sured not only in single realizations, but also on average
(the bias). besides the bias, we are also interested in
the expected    uctuation (standard deviation) of the es-
timations of h   and   d   and how both they depend on
the sequence size n for large n . in heavy-tailed distri-
butions such as eq. (1), these estimations are based on
an observed vocabulary v (number of di   erent symbols)
that grows sub-linearly with n as [30   32]

v (n )     n 1/  .

(10)

this implies that the entropies in eq. (4) are estimated
based on a sum of v         terms (for n        ).
in
practice,    and the precise functional form of the heavy-
tailed distribution are unknown and therefore, besides
  d  , the estimation of h   is also of interest (see ref. [33]
for the case in which a power-law form of p is assumed
to be known a priori).

fig. 2.
illustration of our toy model where p (left) and q
(right) have the same rank-frequency distribution, but di   er
in the id203 for individual symbols. in this example, p
and q are the same (pi = qi) for i     {a, c, d, e, f, g, h},
while the symbol i = b in p is replaced by i = b    in q with
pi=b = qi=b    and pi=b    = qi=b = 0.

a. analytical calculations

here we extend previous results [34   37] and general-
ize them to arbitrary   . given a id203 distribution

abcdefghsymbol:i0.00.10.20.30.40.5piab   cdefghsymbol:i0.00.10.20.30.40.5qi   2   101234  0.00.20.40.60.81.0  d  (p,i   )1103i10   51pi1103i10   51pip and the measured probabilities   p from a    nite sam-
ple of n word-tokens, we expand h  (   p) around the true
probabilities pi up to second order as
(  pi     pi)

h  (   p)     h  (p)+

p     1
i

  
1       

(  pi     pi)2  p     2

i

(11)

(cid:88)
(cid:88)

i:   pi>0

i:   pi>0

    1
2

i

i

=   /(1       )p     1

and    2h
   pi   pj

where we used that    h  
=
   pi
     p     2
  i,j. we then calculate e [h  (   p)] by averaging
over the di   erent realization of the random variables   pi
by assuming that the absolute frequency of each symbol i
is drawn from an independent binomial with id203
pi such that e [  pi] = pi and v [  pi] = pi(1    pi)/n     pi/n
yielding

e [h  (   p)]     h  (p)       
2n

p     1
i

= h  (p)       v (  )
2n

,

which de   nes the vocabulary size of order   

(cid:88)
v (  )    (cid:88)

i   v

p     1
i

.

(12)

(13)

i   v

from eq. (12) we see that the bias in the id178 estima-
tion |h  (p)     e [h  (   p)]| depends only on v (  ) and n .
similar calculations (see appendix b) show that the large
n behavior of the bias and the    uctuations (variance) of
h  , d  , and   d   can be written as simple functions of
v (  ) and n , as summarized in tab. i.

h  

d  ,   d  (p (cid:54)= q) d  ,   d  (p = q)

bias:

v (  )/n
fluctuations: v (2  )/n

v (  )/n
v (2  )/n

v (  )/n

v (2     1)/n 2

table i. scaling of the bias |e[   x]     x| and the    uctua-
tions v[x]     e[   x 2]     e[   x]2 of estimations   x. the results
are valid for large sequence sizes n and depend on the vo-
cabulary of order   , v (  ) as in eqs. (13) and (14). results
are shown for x = h   [order    id178, eq. (4)], d   [gen-
eralized divergence],   d   [normalized divergence, eq. (5)], see
appendix b for the derivations. for   d  , we approximate
  d       d  /e[dmax
   ].

sum (cid:80)

we now focus on the dependence of v (  ) on n . the
i   v in eq. (13) indicates that in n samples, on
average, v = v (n )     v (  =1) di   erent symbols are ob-
if for n         the vocabulary v converges to
served.
a    nite value, v (  ) in eq. (13) also converges and the
bias scales as 1/n . a more interesting scenario happens
when v grows with n . for the heavy-tailed case of in-
terest here, v grows as n 1/  , eq. (10), and we obtain
(in appendix c) that v (  ) scales for large n as

(cid:40)

v (  )    

n     +1+1/  ,    < 1 + 1/  ,
constant ,
   > 1 + 1/  ,

(14)

4

where    > 1 is the zipf exponent de   ned in eq. (1) and
   is the order of the id178 in eq. (4).

from the combination of eq. (14) and tab. i we ob-
tain the scalings with sequence size n of the estimators
of h  , d  , and   d   in a heavy-tailed distribution with
exponent   . these scalings are summarized in tab. ii.
three scaling regimes can be identi   ed for the bias and
for the    uctuations. (i) for large   , the decay is 1/n
(except when p = q, where the    uctuations decay even
faster as 1/n 2) as in the case of a    nite vocabulary and
short-tailed distributions. (ii) for intermediate   , a sub-
linear decay with n is observed. this regime appears
exclusively in heavy-tailed distributions and has impor-
tant consequences in real applications, as shown below.
from the exponents of the sub-linear decay we see that
the bias decays more slowly than the    uctuations. (iii)
for small   ,    < 1/  , h  (p) is not de   ned thus the esti-
mator for the mean of h   and d   diverge. the growth
of h   (and therefore dmax
   ) and d   with n have the
same scaling and therefore cancel each other for   d  , in
which case a convergence to a well de   ned value is found
(the    uctuation of   d   still decays in this regime).

b. numerical simulations

here we perform numerical estimations of the normal-
ized divergence spectrum   d   that illustrate the regimes
derived above, con   rm the validity of the approximations
used in their derivations, and show that the same scalings
are observed for di   erent id178 estimators. we sam-
ple twice n symbols (tokens) from the same distribution
(p = q), and therefore   d   = 0 and the expected value
e[   d  (   p,   q)] is the bias. (the fact that the bias shows a
slower decay with n than the    uctuations makes these
two e   ects distinguishable also in this   d   = 0 case be-
cause e[   d  (   p,   q)] (cid:29) v[   d  (   p,   q)] for large n ).

we start with the most important prediction of our
analytical calculations above:
the existence in heavy-
tailed distributions of a regime for which the bias and
   uctuations of   d   decay with n more slowly than 1/n .
this holds already for    = 1, i.e., for the usual jensen-
shannon divergence, previously shown for the bias of
h  =1 in ref. [37]. one potential limitation of our an-
alytical calculations is that they are based on the plugin-
estimator obtained from replacing the pi   s in the general-
ized entropies, eq. (4), by the measured frequencies (i.e.
pi (cid:55)      pi = ni/n , with ni being the number of observed
word tokens of type i). to test the generality of our re-
sults, in the numerical simulations we use four di   erent
estimators of the shannon id178 (i.e.,    = 1):
i) the
plugin-estimator; ii) miller    s-estimator [34], which takes
into account the approximation obtained from the expan-
sion in eq. (12); iii) grassberger    s estimator [38]; and iv)
a recently proposed bayesian estimator described in [39]
which is an extension of the approach by nemenman et
al. [40] to the case where the number of possible symbols
is unknown or even countably in   nite [41]. the numer-

e[h  (   p)]

1/  

1 + 1/  
cn     +1/  

e[d  (   p,   q)]

1/  

1 + 1/  
cn     +1/  

2 h  (p) + cn     +1/   d  (p, q) + cn     +1/  

h  (p) + cn   1

d  (p, q) + cn   1

  e
1
  e
2

   <   e
1
1 <    <   e
  e
   >   e
2

5

e[   d  (   p,   q)]

1/  

1 + 1/  

c

  d  (p, q) + cn     +1/  

  d  (p, q) + cn   1

v[h  (   p)]

1/(2  )

1
2 (1 + 1/  )
cn   2  +1/  
cn   2  +1/  

cn   1

  v
1
  v
2

   <   v
1
  v
1 <    <   v
   >   v
2

2

v[d  (   p,   q)]

v[   d  (   p,   q)]

p (cid:54)= q
1/(2  )

p = q

1/(2  )

p (cid:54)= q
1/  

p = q

1/  

1

2 (1 + 1/  ) 1 + 1/(2  ) 1
cn   2  +1/   cn   2  +1/  
cn   2  +1/   cn   2  +1/   cn   2  +1/   cn   2  +1/  

2 (1 + 1/  ) 1 + 1/(2  )
cn   1/  

cn   1/  

cn   1

cn   2

cn   1

cn   2

table ii. summary of    nite size scaling for distributions with heavy tails. mean (e) and variance (v) of the plug-in
estimator of h  , d  , and   d   for samples   p and   q each of size n drawn randomly from p and q with power-law rank-frequency
distributions with exponent    > 1, eq. (1). the results are obtained combining tab. i with eq. (14) (for details see appendix b,
c). the constant c depends on    and has a di   erent value in each case but is independent of n . the limit            corresponds
to the case in which both p and q have short tails.

fig. 4. finite-size estimation of the normalized jensen-shannon divergence   d =   d  =1. (a-c) estimation of e[   d(   p,   q)] between
two sequences of size n drawn from the same distribution (i.e. d(p, q) = 0) using four di   erent estimators of the id178 (see
text) for three representative distributions: (a) exponential (short-tailed) distribution pi     e   ai for i = 0, 1, . . . with a = 0.1;
(b) power-law (heavy-tailed) distribution pi     i      for i = 1, 2, . . . with    = 3/2; (c) empirical zipf-distribution of word
frequencies, i.e. rank-frequency distribution p(r) from the complete google-ngram data, pi = f (i = r) for i = 1, . . . , 4623568,
which is well described by a double power-law [10]. (d-f) show the same as (a-c) for the    uctuations v[   d(   p,   q)]. the dotted
lines show the expected scalings from tab. ii for short-tailed distributions, i.e. n   1 (n   2), and power-law distributions, i.e.
n   1+1/   (n   2+1/  ), for the bias (   uctuations). in (c) we show the expected scaling for the bias, vemp(n )/n , where vemp(n )
is the expected number of di   erent symbols in a random sample of size n from the empirical distribution [32]. averages are
taken over 1000 realizations.

10   410   310   210   1100biase[  d  =1(  p,  q)]   n   1(a)[exponential]100101102103104samplesize:n10   810   610   410   2100fluctuationsv[  d  =1(  p,  q)]   n   2(d)   n   1+1/     n   1(b)[powerlaw]100101102103104samplesize:n   n   2   n   2+1/  (e)   n   1(c)[empirical]pluginmillergrassbergerbayesian100101102103104samplesize:n   n   2(f)6

fig. 5. bias (a,b) and    uctuations (c,d) in    nite-size estimation of   d  . estimation of e[   d  (   p,   q)] between two sequences
each of size n drawn numerically from the same power-law distribution pi     i      for i = 1, 2, . . . , v         with    = 3/2 using
the plugin-estimator (pi (cid:55)      pi) for the entropies of order   . (a) scaling of the bias with n for di   erent   . (b) decrease of the
bias in   d   when sample size is doubled (n (cid:55)    2n ) for di   erent values of n as a function of   . (c) and (d) show the same as
(a) and (b) for the    uctuations v[   d  (   p,   q)]. red lines in all plots indicate the borders between the regimes,   e
1 = 1/   = 2/3,
  e
2 = 1 + 1/   = 5/3 (for the bias in a,b), and   v
2 = 1 + 1/(2  ) = 4/3 (for the    uctuations in c,d). dotted
lines indicate the predictions based on tab. i for    <   e
2 (in a,c) and all values of    (in b,d). averages are
taken over 1000 realizations.

1 = 1/   = 2/3,   v

1 and    >   e

1,   v

2,   v

ical results in fig. 4 show that the di   erent estimators
are indeed able to reduce the bias of the estimation, but
that the scaling of the bias with n remains the same.
in particular, the transition from short-tailed to heavy-
tailed distribution leads to the predicted transition from
n   1 (n   2) to the slower n   1+1/   (n   2+1/  ) decay for
the bias (   uctuations) for all estimators. the only ex-
ception is in the bias of the bayesian estimator for the
exact zipf   s law (1), but since this estimator shows a
bad performance for the    uctuation and for the real data
we conclude that the slower scaling should be expected
in general also for this elaborated estimator. these re-
sults con   rm the generality of our    nding that the bias
and    uctuation in   d  =1 decays more slowly than 1/n in
heavy-tailed distributions. the consequence of this result
to applications will be discussed in the next section.
we now consider the estimation of   d   for    (cid:54)= 1 in
the case of heavy-tailed distributions (1). the numer-
ical results in fig. 5 con   rm the existence of the three
scaling regimes discussed after eq. (14) and in tab. ii.
the panels (b) and (d) show the relative reduction in the
bias and    uctuations achieved when the sequence size
is doubled. for many di   erent      s the relative reduc-
tion is larger than 0.5 (0.25) for the bias (   uctuations),
a consequence of the slow decay with n that shows the
di   culty in obtaining a good estimation of   d  . in prac-
tice, the exponent    of the distribution is unknown such

1 ,   v

that the critical values of    that separates these regimes
(e.g.   e
1 = 1/   and   e
2 = 1 + 1/   for the bias) cannot
be determined a priori. yet, since    > 1, we know that:
1 < 1 and therefore d   for        1 is such that
(i)   e
d  (p, p) = 0 for n        ; and (ii)   e
2 < 2 and there-
fore the bias and    uctuations of d   for        2 decay as
1/n (or 1/n 2 for the    uctuations in the case of p = q).
this suggests d  =2 as a pragmatic choice for empirical
measurements because any further increase in    will not
lead to a faster convergence.

2 ,   v

v. application to real data

in this section, we show the signi   cance of the general

results of the previous section to speci   c problems.

a problem that appears in di   erent contexts is to test
whether two    nite-size n sequences, described by their
empirical distributions   p and   q, have a common source
(null hypothesis). this involves the computation of a
single divergence   d  (p, q), which is then compared to
the divergence   d  (p(cid:48), p(cid:48)) between two    nite-size (ran-
dom) samplings of a single (properly chosen) distribu-
tion p(cid:48) (e.g., p(cid:48) = 0.5p + 0.5q). the id203 of
observing   d  (p(cid:48), p(cid:48))       d  (p, q) is then reported as a
p-value [3]. besides applications in language, e.g. com-

10   610   510   410   310   210   1100biase[  d  (  p,  q)]   n   1   const.(a)100101102103104105106samplesize:n10   1210   1010   810   610   410   2100fluctuationsv[  d  (  p,  q)]   n   1/     n   2(c)0.00.51.01.52.0  0.50.60.70.80.91.0e[  d  ](2n)/e[  d  ](n)(b)  e1  e2n=25n=210n=215n=2190.00.51.01.52.0  0.20.30.40.50.60.7v[  d  ](2n)/v[  d  ](n)(d)  v1  v2n=25n=210n=215n=2197

fig. 6. measuring change in the usage of language on historical time-scales. (a)   d  (pt1 , pt2 ) as a function of    for pairs
of word-frequency distributions of the google-ngram database obtained from the yearly corpora t1 and t2 with (t1, t2)    
{(1850, 1900), (1900, 1950), (1850, 1950)} (solid lines). the dotted lines with the same colors show the results of a null model
in which samples of the same size of the ones in t1 and t2 are randomly drawn from the same distribution (obtained from
combining the corpora in t1 and t2) mimicking a minimum distance that can be observed due to    nite-size e   ects. the vertical
lines show the three regimes    < 1/  , 1/   <    < 1 + 1/  , and    > 1 + 1/   in the convergence of   d  (pt1 , pt2 ) with n
(see sec. iv), obtained using    = 1.77 [10]. inset: ratio   d  (pt12 , pt12 )/   d  (pt1 , pt2 ). (b) average divergence as a function of
   t     |t2     t1|, calculated as   d  (   t) = 1
  d  (pt1 , pt1+   t) for four di   erent    (solid lines). shaded areas represent
the standard deviation associated to the average   d  (   t). inset:
  d  (   t) as a function of    t highlighting the approximate
relationship   d  (   t)        t2 for    t (cid:29) 1.

(cid:80)2000      t

(cid:113)

t1=1805

n   t

paring the distribution of word-frequencies, this problem
appears in the identi   cation of coding- and non-coding
regions in dna [42]. the signi   cance of our results for
   nite-size estimations in sec. iv is that for the case of
heavy-tailed distribution the expected   d  (p, q) of the
null model may be much larger than the predicted value
based on a 1/n decay (as observed in short-tailed dis-
tributions).
if the slower convergence in n is ignored,
e.g., by applying standard tests [3] to heavy-tailed dis-
tributions, one rejects the null hypothesis (low p-value)
even if the data is drawn from the same source because
the measured distance will be much larger. the exam-
ple in fig. 4(c) shows that, even when the size of both
sequences is on the order of n     105, the expected   d1
(jsd) is e[   d  =1(   p,   q)]     10   1. this is two orders of
magnitude larger than for the exponential distribution
in fig. 4(a), where e[   d  =1(   p,   q)]     10   3.

a. temporal change. the change of english from
1850 to 1950 was larger than the change form 1850 to
1900 and from 1900 to 1950, as seen from the fact that
the curve of   d  (p1850, p1950) in fig. 6a lies above the
two other curves for all   . this intuitive result (evolu-
tionary dynamics show no recurrences) con   rms that the
divergence spectrum   d  (pt1, pt2) is a meaningful quan-
ti   cation of language change. the average dependency
of   d  (p1850, p1950) on    t = |t2     t1|, shown in fig. 6b,
can be thus used as a quanti   cation of the speed of lan-
guage change. we observe an approximate relationship
      t2 for    t (cid:29) 1 (see inset fig. 6b),
  d  (   t)       d(i)
where   d(i)
are constants and can be related to
words that change due to    uctuations (   nite sampling
or topical dependencies) which is independent of    t and
words that show a systematic increase or decrease over
   t, respectively (see appendix d for a detailed discus-
sion).

   +   d(ii)
   and   d(ii)

  

the next problems we consider appear in the anal-
ysis of historical data and in the quanti   cation of lan-
guage change [43]. these problems are representative of
problems that involve the comparison of two or more di-
vergences   d  (p, q), obtained from di   erent distributions
p (cid:54)= q and   (cid:48)
(cid:54)=   . as depicted in fig. 1, the di   er-
ent distributions are obtained based on individual years
(t     {1850, 1900, 1950}) and we calculate the normalized
spectrum   d  (pt1 , pt2) between pairs of years (t1, t2). as
argued in sec. ii and appendix a,   d  (p, q) is meaning-
ful even if the sequences used to estimate p and q have
di   erent sizes np (cid:54)= nq. we summarize our results in
fig. 6, from which di   erent conclusions can be drawn:

b. dependence on   . all observed divergences
  d  (pt1 , pt2) decay with    (e.g., the three curves in
fig. 6a). as discussed in sec. iii, this shows that
for words with a high (low) frequency the distribu-
tions are more (less) similar and thus the change is
slower (faster). this result is consistent with previ-
ous works on the evolution of individual words on his-
torical time scales reporting that frequent words tend
to be more stable [44, 45]. this dependence on   
is essential when comparing the change 1850 (cid:55)    1900
to the change 1900 (cid:55)    1950 (fig. 6a). while the
earlier change was smaller if counted on a token ba-
sis,
it be-

  d  =1(p1850, p1900) <   d  =1(p1900, p1950),

0.00.51.01.52.0  10   510   410   310   210   1  d  (pt1,pt2)(a)1850-19001900-19501850-195000.511.5210   410   310   210   11050100150   t10   410   310   210   1100  d  (   t)(b)  =0.0  =0.5  =1.0  =2.00501001500.00.20.40.60.8comes larger if one focus on the more frequent words
[   d  =2(p1850, p1900) >   d  =2(p1900, p1950)].

c. role of    nite-size scalings. our    nding that the
scalings (of the bias and of the    uctuations) in   d   with
sample size n depend on    allows for a deeper un-
derstanding of the   d  (pt1, pt2 ) measurements discussed
above. the expected   d     s for random sampling of the
same distribution (null model shown as dashed line in
fig. 6a) are of the same order as the empirical distance
  d  (pt12, pt12 )       d  (pt1, pt2)) and it
for small    (i.e.
is only for    > 1 that the null model divergence be-
comes negligible compared to the empirical divergence
(i.e.   d  (pt12, pt12 ) (cid:28)   d  (pt1, pt2 )). this implies that
even though the size of the individual corpora is of the
order of n     109 word-tokens, the empirically measured
  d   is still strongly in   uenced by    nite-size e   ects over a
wide range of values for   , in agreement with our analy-
sis in sec. iv. in particular, the bias for jensen-shanon
divergence (   = 1) is important even for the case of the
(extremely large) google-ngram database (e.g., the inset
of fig. 6a shows that the bias is     10%).

d.    = 2 as a pragmatic choice. the slow decay of
bias and    uctuations with database size suggests that
  d  =2 is a pragmatic choice in reducing such    nite-size
e   ects when the exponent    in the power-law distribu-
tion is not known. this conclusion is further corrobo-
rated in the analysis of the dependence of   d   with    t
(fig. 6b). while   d  (   t = 0) = 0 by construction,   d  
does not converge to zero for    t     0 when extrapolating
from   d  (   t > 0), but instead it seems to saturate, i.e.
  d  (   t     0)       d(i)
   is of
the same order of magnitude of the expected bias (e.g.,
shown as dashed line in fig. 6a) and even of the same
order of magnitude of the divergence   d  (   t = 100) be-
tween two corpora separated by 100 years. for small   
and    t, it is thus di   cult to distinguish between    nite-
size e   ects (   d(i)
   ) and actual language change. results for
   = 2 show the largest relative variation with    t and are
therefore statistically more suited to quantify language
change over time.

   > 0. for small values of   ,   d(i)

vi. conclusions

in summary, we investigated the use of generalized en-
tropies h   to quantify the di   erence between symbolic
sequences with heavy-tailed frequency distributions. in
particular, we introduced a normalized spectrum of a
generalized divergence,   d  (p, q) in eq. (5), that allows
for a comparison between the di   erent distributions p
increasing   ,   d   at-
and q and also for di   erent      s.
tributes higher weights to high-frequency symbols. the
more complete characterization given by the full spec-
trum   d   is particularly important in the case of heavy-
tailed distributions because in this case symbols do not
have a characteristic frequency but instead show frequen-
cies on a broad range of values.

8

our main analytical    nding is how the systematic
(bias) and statistical (   uctuations) errors of    nite-size
(n ) estimations of h   and   d   scales with n , see tab. ii.
the existence of regimes in which these scalings decay
slower than 1/n shows that large uncertainties should
be expected in h   and   d   estimated even for very large
databases. this should be taken into account when com-
paring two or more   d     s and when estimating the prob-
ability of two sequences having the same source. the
fact that for large    we recover the usual scaling (decay
with 1/n ) suggests   d  =2 as a pragmatic choice in ap-
plications involving heavy-tailed distributions. previous
works using information theoretic measures in language
used    = 1 [5   8] and did not take into account the ef-
fect of (   nite) database size. our results show that the
bias and    uctuations are signi   cant even in the extremely
large google-ngram database.
it is therefore essential
to clarify what is the role of    nite-size e   ects in the re-
ported conclusions, in particular in the (typical) case that
database sizes change over time.

our main empirical    ndings on language change are: i)
that least frequent words contribute more to the total vo-
cabulary change; ii) the answer to the question whether
the speed of language change is accelerating depends on
the emphasizes that is given to either low-frequency or
high-frequency words; and iii) the quanti   cation of the
speed of vocabulary change in time,    t, which shows
roughly a dependence   d  (   t)       d(i)
      t2, where
   (   d(ii)
  d(i)
   ) quanti   es the degree to which words change
due to    uctuations independent of time (systematic in-
crease/decrease of the frequency over time). more gener-
ally, our spectrum   d   opens the possibility of studying
language change at di   erent resolution, combining as-
pects from the analysis on the level of individual words
(e.g. refs. [44, 45]) and the full vocabulary of a language
(e.g. refs. [7, 9]).

   +   d(ii)

our results are also of interest beyond the cases treated
here. first, the    nite-size scaling we derive appear al-
ready in the id178 and therefore the same scalings are
expected in any id178-based measure, including those
based on conditional entropies such as the kullback-
leibler divergence [1]. second, the analysis is not nec-
essarily restricted to the word level, it can be straightfor-
ward extended also to id165s of words which also show
heavy-tailed distributions [46]. third, the spectrum of
divergences   d  (p, q) o   ers a unifying framework which
can be applied to problems involving di   erent partitions
of texts by varying the parameter   . for example, while
in document classi   cation [2] one tries to identify top-
ical words (suggesting the use of low values of   ), in
applications of authorship attribution [47] it has been
shown that the comparison of the most-frequent (func-
tion) words yields the best results (suggesting the use of
large values of   ). fourth, heavy-tailed distributions ap-
pear in di   erent problems involving symbolic sequences
(e.g., in the dna [17], in gene expression [18], and in
music [19]), and the signi   cance of our results is that they

can be applied in all these cases.

acknowledgments

we thank peter grassberger for insightful discussions.

appendix a: documents with di   erent lengths

here we discuss how to proceed if the jsd is computed
from    nite datasets with di   erent    nite lengths n , i.e.
when p (q) is estimated from a sequence of length np
(nq (cid:54)= np).

1. di   erent weights

a possible way to extend eq. (2) taking into ac-
count the unequal contribution np (cid:54)= nq is to consider
weights    as [3]

  (p, q) = h  (  pp +   qq)      ph  (p)      qh  (q). (a1)
d  
with   p = np/n and nq/n such that   p +   q = 1 with
n = np + nq (denoted as natural weights in the follow-
ing). obviously, if np = nq then   p =   q = 1/2 and d  
is recovered. the normalized distance (5) becomes

9

fig. 7.
jsd-   for sequences of di   erent lengths. measure-
ment of   d  (   p,   q) between sequences   p,   q of size n(cid:48)p = n(cid:48)q sam-
pled randomly from the empirical distribution of the google-
ngram of the years t     {1850, 1950} with di   erent sizes, i.e.
p = pt=1850 and q = pt=1950 with np (cid:54)= nq, as a function of
the sample size n(cid:48) = n(cid:48)p + n(cid:48)q for di   erent values of   . the
dotted (dashed) lines show   d  
  (p, q) between the full distribu-
tions p and q with equal (natural) weights, i.e.   p =   q = 1/2
(  p = np/(np + nq)     0.22 and   q = nq/(np + nq)     0.78
corresponding to the relative size of p and q)

2. equal weights

(p, q) =(cid:0)    

where

d  ,max

  

  d  

  (p, q) =

p       p
1       

1

+

d  

  (p, q)

,

  

d  ,max

(p, q)

(cid:1) h   (p) +(cid:0)    
q     1(cid:1) .
(cid:0)    

p +     

(a2)

(cid:1) h   (q)

q       q

(a3)
our main results for the    nite-size scaling of d   summa-
rized in tab. ii remain valid for the weighted divergences.
the approach above follows ref [3], which introduced
weights to the usual jsd (non-normalized,    = 1) and
showed that the natural weights   p = np/n and   q =
nq/n imply certain useful properties for the jsd, e.g.,
that the bias does not depend on the relative size of the
two samples. while their main motivation was to com-
pare the statistical signi   cance of a single measurement of
the jsd in the identi   cation of stationary subsequences
(of possibly di   erent lengths) in a non-stationary sym-
bolic sequence, here, we are mainly interested in com-
paring two (or more) measured distances. in this case,
choosing weights that depend on the size of the individ-
ual samples becomes problematic when the sequences are
of di   erent lengths. the demonstration that
d  (p, q)
is a metric for any        (0, 2] [28] is valid for    xed weights
  p =   q = 1/2. more generally, the measure d  
   itself
   and d  (cid:48)
depends on the weights    such that d  
   con-
stitute di   erent measures when    (cid:54)=   (cid:48). it is therefore
   (p(cid:48), q(cid:48)) if
not meaningful to compare d  
np/nq (cid:54)= np(cid:48)/nq(cid:48) because this would imply that   (cid:48) (cid:54)=   .

  (p, q) and d  (cid:48)

(cid:112)

in the previous section we argued that it is essential
to choose    xed weights    when comparing di   erent dis-
tances. the choice of equal weights   p =   q = 1/2 can,
however, still be interpreted in the framework of natural
weights (  p = np/n ,   q = nq/n ) as the distance be-
tween undersampled versions of the sequences. for given
p and q with np (cid:54)= nq we choose equal weights   p =
  q = 1/2 yielding a distance d1/2
   (p, q). if we randomly
draw samples p(cid:48) and q(cid:48) of size n(cid:48)p = n(cid:48)q from the distri-
butions p and q, (by construction) the natural weights
coincide with the equal weights, i.e.   (cid:48)p =   (cid:48)q = n(cid:48)p/n =
n(cid:48)q/n = 1/2, and

   (p(cid:48), q(cid:48)) = d1/2

   (p, q).

d  (cid:48)

lim

n(cid:48)p=n(cid:48)q      

in fig. 7 we show the di   erence in   d  

  (p, q) between
two empirical distributions from the google-ngram with
di   erent sizes (np (cid:54)= nq) when choosing equal and natu-
ral weights. using equal weights corresponds to the case
in which we draw samples   p and   q that are of equal length
(n(cid:48)p = n(cid:48)q) such that equal and natural weights coincide
and taking the limit n(cid:48)p, n(cid:48)q        .

appendix b: finite size estimation of h  , d  , and

  d  

in this section we present the calculations on the mean
(i.e. the bias) and the    uctuations in    nite-size estimates
of h  , d  , and   d  . the starting point is a    nite sample
  p = (n1/n, n2/n, . . . , nv /n ) of size n (where ni is the
number of times symbol i was observed) which we assume

100101102103104105106107108109n=np+nq10   310   210   1100  d  (  p,  q)  =0.0  =0.5  =1.0  =2.0is obtained from n identical and independent draws from
the distribution p giving an estimator for h  :

       (cid:88)

i:   pi>0

       .

i     1
  p  

h  (   p) =

1

1       

in order to take the corresponding expectation values we
expand   p  
i around the true probabilities pi up to second
order

i     p  
  p  

i +(  pi   pi)  p     1
i +

1
2

(  pi   pi)2  (     1)p     2

i

(b2)

and average over the realizations of the random variables
  p  
i by assuming that each symbol is drawn independently
from binomial with id203 pi such that (cid:104)(  pi   pi)(cid:105) = 0
and (cid:104)(  pi     pi)2(cid:105) = pi(1     pi)/n     pi/n yielding [37]

(cid:104)  p  
i (cid:105)     p  

i +

  (       1)p     1

i

.

1
2n

(b3)

1. h  

p     1
i

combining eqs. (b1) and (b3) we obtain for the mean

      
i (cid:105)     1
(cid:104)  p  
(cid:88)

1

1       

e[h  (   p)]     (cid:104)h  (   p)(cid:105) =

       (cid:88)
             
(cid:17)       
where we introduce the notation(cid:80)

       (cid:88)
(cid:16)

i   (cid:104)v   p(cid:105)
v (  +1)
  p

i     1
p  

1       

1       

i   (cid:104)v   p(cid:105)

    1

2n

=

=

1

1

2n

i   (cid:104)v   p(cid:105)

(b4)
indicating that
we average only over the expected number of observed
symbols (cid:104)v   p(cid:105) in samples   p.
for the variance we get
(cid:88)
v[h  (   p)]    e[h  (   p)2]     e[h  (   p)]2
(cid:88)

j (cid:105)     (cid:104)  p  
i (cid:105)(cid:104)  p  
(cid:88)

(cid:0)(cid:104)  p  

(1       )2

j (cid:105)(cid:1)

(cid:88)

j   (cid:104)v   p(cid:105)

i   (cid:104)v   p(cid:105)

i   p  

  2

=

1

v (  )
  p

i   (cid:104)v   p(cid:105)

p2     1
i

      2
4n 2

p2     2
i

=

(1       )2n

i   (cid:104)v   p(cid:105)

i   (cid:104)v   p(cid:105)

  2

=

(1       )2

v (2  )
  p
n

      2
4

v (2     1)
  p
n 2

(b5)
where we used that two di   erent symbols i (cid:54)= j are in-
j (cid:105) +

dependently drawn, thus(cid:80)
(cid:80)
i(cid:104)  p2  
i (cid:105).

j (cid:105) =(cid:80)

i(cid:54)=j(cid:104)  p  

i,j(cid:104)  p  

i (cid:105)(cid:104)  p  

i   p  

2. d  

for d   we have two samples   p and   q each of size n
randomly sampled from the distributions p and q such

that we can express the mean and the variance from the
expectation values of the corresponding individual en-
tropies.

10

(b1)

introducing the notation p     1

2 (p + q) we get for the

mean

e[d  (   p,   q)] =e[h  (   p )]     1
2
v (  +1)
  p

=

1

(cid:26)
(cid:26) 1

2

(cid:27)

e [h   (   p)]     1
e [h   (   q)]
2
    1
    1
2
2
1
2

v (  +1)
  p
  q     1
v (  )

v (  +1)
  q

v (  )
  p

(cid:27)

2

.

v (  )
  p +

1       
  
2n

+

(b6)
denotes the generalized vocabulary, eq. (13),
2 (   p +   q), which is of

where v (  )
for the combined sequence   p = 1
length 2n .

  p

for the variance we get

v[d   (   p,   q)]    e[d   (   p,   q)2]     e[d   (   p,   q)]2
1
v [h  (   p)] +
4

=v[h  (   p )] +

1
4

(cid:104)

    cov

h  (   p ), h  (   p) + h  (   q)

,

v [h  (   q)]

(cid:105)

(b7)
where cov [x, y ]     e[xy ]    e[x]e[y ]. we evaluate the
covariance-term in two di   erent ways, i.e.

(cid:104)

  p   
i

i:   pi+  qi>0

(1       )2cov

   

i:   pi+  qi>0

(cid:42) (cid:88)
(cid:42) (cid:88)
(cid:42) (cid:88)
(cid:42) (cid:88)
(cid:110)(cid:68)
(cid:88)
i   (cid:104)v   p(cid:105)

i:   pi+  qi>0

   

=

=

=

  p   
i

i:   pi+  qi>0
  p   
i (  p  

  p   
i

h  (   p ), h  (   p) + h  (   q)

  q  
j

j:  qj >0

j:   pj >0

  p  
j +

       (cid:88)
(cid:43)      (cid:42) (cid:88)
(cid:88)
(cid:0)  p  
(cid:43)(cid:42) (cid:88)
(cid:69)    (cid:68)

i +   q  
i )

j:   pj +  qj >0

j:   pj >0

  p  
j

+

(cid:88)
(cid:43)
(cid:1)(cid:43)
(cid:0)  p  
(cid:69)

  p   
i

j +   q  
j

j:   pj +  qj >0

  p   
i

(cid:105)
      (cid:43)
(cid:42) (cid:88)

j:  qj >0

(cid:1)(cid:43)

j +   q  
j

(cid:111)
i (cid:105))
i (cid:105) + (cid:104)  q  

((cid:104)  p  

(cid:43)      

  q  
j

(b8)

h  (   p ), h  (   p) + h  (   q)

11

(cid:43)

  p   
i

(cid:43)(cid:42) (cid:88)
(cid:43)(cid:42) (cid:88)

j:   pj >0

  p  
j

  p   
i

(cid:43)

  q  
j

(cid:105)

(cid:43)

   

i:   pi+  qi>0

(cid:42) (cid:88)
(cid:43)
(cid:42) (cid:88)
(cid:69)(cid:104)  p  
i (cid:105)(cid:111)
(cid:69)(cid:104)  q  
i (cid:105)(cid:111)

   

  p   
i

  q  
j

i:   pi+  qi>0

(cid:104)

and

i:   pi+  qi>0

(1       )2cov

(cid:42) (cid:88)
(cid:42) (cid:88)
(cid:110)(cid:68)
(cid:88)
(cid:110)(cid:68)
(cid:88)

i   (cid:104)v   p(cid:105)
+

+

i:   pi+  qi>0
  p   
i   p  
i

=

=

i   (cid:104)v   q(cid:105)

(cid:68)

  p   
i

  p   
i   p  
i

  p   
i   q  
i

(cid:68)
(cid:68)

j:  qj >0
  p   
i

  p  
j

  p   
i

  p   
i

j:   pj >0

  p   
i   q  
i

(cid:88)
(cid:88)
(cid:69)    (cid:68)
(cid:69)    (cid:68)
(cid:69)    p   
(cid:69)    p   
(cid:69)    p   

i p  
i +
  
2n
i q  
i +
  
2n

i +

+

+

similarly as in eq. (b3) we can approximate

  (       1)

4n
  
4n
(       1)p   

  
4n
(       1)p   

p      1

,

i

(3       1)p      1

i

i p     1

i

,

(3       1)p      1

i

p  
i

q  
i

i q     1

i

.

j:  qj >0

(b9)

(b10)

from this we get for the variance of d  

v[d  (   p,   q)] =

(1       )2

(cid:26)   2
(cid:26)   2
(cid:88)
(cid:26)   2
(cid:88)

i   (cid:104)v   p(cid:105)

(cid:88)

i   (cid:104)v   p (cid:105)

+

+

1
2

1
2

i   (cid:104)v   q(cid:105)

1
2n

p      1

i

(p  

i + q  
i )

      2

16n 2 p      1

i

i     1
p   
2

(cid:20)
(cid:2)p     1
(cid:2)q     1

i

i     p      1

i

    p      1

i

(cid:21)
(cid:3)       2
(cid:3)       2

8n 2 p     1

i

8n 2 q     1

i

(cid:2)p      1

i

(cid:2)p     1
(cid:2)q     1

i

i     p      1

i

    p      1

i

   (cid:0)p     1
(cid:3)(cid:27)
(cid:3)(cid:27)

.

(1       )2

(1       )2

1
2n

1
2n

p  
i

q  
i

i + q     1

i

(cid:1)(cid:3)(cid:27)

(b11)

now we can see that for p = q = p we get

3.

  d  

the    nite size estimation of   d   can be obtained ap-

proximately by

(cid:88)

v[d  (   p,   q)]p=q =

i   (cid:104)v   p (cid:105)

1
16n 2   2p2     2

i

=

  2
16n 2 v (2     1)

  p

.

  d  (   p,   q) =

d  (   p,   q)

d  (   p,   q)max

    d  (   p,   q)

e [dmax

   (   p,   q)]

(b12)
while for arbitrary p and q the variance of the d  
contains the variances of the individual entropies (e.g.
v (2  )
/n ) and a covariance term, (only) in the special
  p
case p = q all    rst-order terms (1/n ) vanish yielding a
qualitatively di   erent behaviour v (2     1)

/n 2.

  p

such that

e(cid:104)
v(cid:104)

  d  (   p,   q)

  d  (   p,   q)

(cid:105)     e [d  (   p,   q)]
(cid:105)     v [d  (   p,   q)]

e [dmax

   (   p,   q)]
   (   p,   q)]2 .

e [dmax

,

(b13)

(b14)

12
by setting    =        1 in eq. (13) and noting that for
pi     i     , eq. (1), the number of di   erent symbols scales
as v     n 1/  , eq. (10), we obtain eq. (14).

the mean of dmax
is given according to eq. (6) as a
linear combination of the individual entropies of   p and   q

  

   (   p,   q)]

e [dmax
21          1

(cid:18)

=

2

(cid:19)

e [h   (   p)] + e [h   (   q)] +

2

1       

(b15)

.

appendix d: temporal evolution of   d  (   t)

appendix c: derivation of eq. (14)

in this section we derive the scaling of the general-
ized vocabulary v (  ) de   ned in eq. (13) assuming that
p is a power-law of the form pi     i     , eq. (1).
in-
stead of looking at the id203 of individual symbols
i, we consider the distribution of frequencies n, which
in this case yields p(n)     n   1   1/   [48]. consider the
n sv (  ), where sv (  )
corresponds to the sum of v i.i.d. random variables ni
(i = 1, . . . , v ) drawn from the distribution p(n) it can be
shown that [49]

sum (cid:80)

i   v pi = 1

(cid:80)

n

i   v ni = 1
(cid:40)

sv (  )    

v   ,    > 1,
v,
   < 1.

(c1)

we can treat (cid:80)

the case    = 1 includes additional logarithmic correc-
(cid:80)
tions, but is not of relevance for the discussion, there-
fore, we leave it for sake of simplicity. in the same way,
i   v n  
n    sv (    ) by
noting that sv (    ) can be interpreted as the sum of v
i.i.d. random variables ni (i = 1, . . . , v ), where ni       p(n)
with   p(n)     n   1   1/(    ) such that we get

i   v p  

i = 1
n   

i = 1

(cid:40)

sv (    )    

v     ,    < 1/  ,
v,
   > 1/  .

(c2)

we are interested in understanding the dependence of
  d  (   t)       d  (t0, t0 +    t) on    t (we assume   d  (   t) is
the same for all t0). the triangle inequality implies that

(cid:18)(cid:113)

  d  (   t)    

  d  (   t     1) +
   (   t)2   d  (   t = 1).

(cid:113)

(cid:19)2

  d  (   t = 1)

(d1)

in order to consider the origin of di   erent    t dependen-
cies within the general bound given by eq. (d1), we con-
sider two classes of words subject to frequency change in
   t: (i) words which show    uctuations (e.g.,    nite sam-
pling or topical dependencies) that do not depend on    t;
and (ii) words which show a systematic increase or de-
crease over all t. if we assume that all words that change
fall in one of these classes we can use the fact that   d  
is de   ned as a sum of word types and decompose the
total change   d   as   d   =   d(i)
is
the divergence of all words falling in class (i) or (ii), re-
spectively. for category (ii), the changes between con-
secutive years are independent, thus, the equality case
  d  (t0, t2) =
  d  (t1, t2) for all t1 with t0     t1     t2.

of the triangle inequality is obtained:

   , where   d(i,ii)

  d  (t0, t1) +

   +   d(ii)

(cid:113)

(cid:113)

(cid:113)

  

therefore we obtain a quadratic dependence on    t as

  d  (   t) =   d(i)

   +   d(ii)

   (   t = 1)(   t)2

(d2)

[1] s. kullback, id205 and statistics (wiley,

new york, 1959).

[2] c. d. manning and h. sch  utze, foundations of statisti-
cal natural language processing (mit press, cambridge,
ma, 1999).

[3] i. grosse, p. bernaola-galv  an, p. carpena, r. rom  an-
rold  an, j. oliver, and h. e. stanley,    analysis of sym-
bolic sequences using the jensen-shannon divergence,   
phys. rev. e 65, 041905 (2002).

[4] y. he, a. ben hamza, and h. krim,    a generalized
divergence measure for robust image registration,    ieee
trans. signal proc. 51, 1211 (2003).

[5] k. w. boyack, d. newman, r. j. duhon, r. klavans,
m. patek, j. r. biberstine, b. schijvenaars, a. skupin,
n. ma, and k. b  orner,    id91 more than two mil-
lion biomedical publications: comparing the accuracies
of nine text-based similarity approaches,    plos one 6,
e18029 (2011).

[6] a. p. masucci, a. kalampokis, v. m. egu    luz,

and
e. hern  andez-garc    a,    wikipedia information    ow anal-

ysis reveals the scale-free architecture of the semantic
space,    plos one 6, e17333 (2011).

[7] v. bochkarev, v. solovyev, and s. wichmann,    univer-
sals versus historical contingencies in lexical evolution,   
j. r. soc. interface 11, 20140841 (2014).

[8] e. a. pechenick, c. m. danforth, and p. s. dodds,    is
language evolution grinding to a halt?: exploring the life
and death of words in english    ction,    arxiv:1503.03512
(2015).

[9] g. cocho, j. flores, c. gershenson, c. pineda,

and
s. s  anchez,    rank diversity of languages: generic be-
havior in computational linguistics,    plos one 10,
e0121898 (2015).

[10] m. gerlach and e. g. altmann,    stochastic model for the
vocabulary growth in natural languages,    phys. rev.
x 3, 021006 (2013).

[11] j.-b. michel, y. k. shen, a. p. aiden, a. veres, m. k.
gray, j. p. pickett, d. hoiberg, d. clancy, p. norvig,
j. orwant, s. pinker, m. a. nowak, and e. l. aiden,
   quantitative analysis of culture using millions of digi-

13

tized books,    science 331, 176 (2011).

[12] g. k. zipf, the psycho-biology of language (routledge,

london, 1936).

[13] r. h. baayen, word frequency distributions (kluwer

academic publishers, dordrecht, netherlands, 2001).

[14] r. ferrer i cancho,    the variation of zipf   s law in human

language,    eur. phys. j. b 44, 249 (2005).

[15] m. a. serrano, a. flammini, and f. menczer,    model-
ing statistical properties of written text,    plos one 4,
e5372 (2009).

[16] s. k. baek, s. bernhardsson, and p. minnhagen,    zipf   s

law unzipped,    new j. phys. 13, 043004 (2011).

[17] r. n. mantegna, s. v. buldyrev, a. l. goldberger,
s. havlin, c.-k. peng, m. simons,
and h. e. stan-
ley,    linguistic features of noncoding dna sequences,   
phys. rev. lett. 73, 3169 (1994).

[33] t. d. de wit,    when do    nite sample e   ects signi   cantly
a   ect id178 estimates?    eur. phys. j. b 11, 513 (1999).
[34] g. a. miller,    note on the bias of information esti-
mates,    in id205 in psychology: problems
and methods, edited by h. quastler (free press, glen-
coe, il, 1955).

[35] g. p. basharin,    on a statistical estimate for the en-
tropy of a sequence of independent random variables,   
theor. probab. appl. 4, 333 (1959).

[36] b. harris,    the statistical estimation of id178 in the
non-parametric case,    colloq. math. soc. j. b 16, 323
(1975).

[37] h. herzel, a. o. schmitt, and w. ebeling,    finite sam-
ple e   ects in sequence analysis,    chaos soliton. fract. 4,
97 (1994).

[38] p. grassberger,    id178 estimates from insu   cient

[18] c. furusawa and k. kaneko,    zipfs law in gene expres-

samplings,    arxiv:physics/0307138 (2008).

sion,    phys. rev. lett. 90, 088102 (2003).

[19] j. serr`a, a. corral, m. bogu  n  a, m. haro, and j. l. ar-
cos,    measuring the evolution of contemporary western
popular music,    sci. rep. 2, 521 (2012).

[20] j. lin,    divergence measures based on the shannon en-

tropy,    ieee trans. inf. theory 37, 145 (1991).

[21] t. m. cover and j. a. thomas, elements of information

theory (wiley-interscience, new york, 2006).

[22] d. m. endres and j. e. schindelin,    a new metric for
id203 distributions,    ieee trans. inf. theory 49,
1858 (2003).

[23] m. a. montemurro and d. h. zanette,    towards the
quanti   cation of the semantic information encoded in
written language,    adv. complex syst. 13, 135 (2010).
[24] j. havrda and f. charv  at,    quanti   cation method
of classi   cation processes: concept of structural a-
id178,    kybernetika 3, 30 (1967).

[25] j. burbea and c. rao,    on the convexity of some di-
vergence measures based on id178 functions,    ieee
trans. inf. theory 28, 489 (1982).

[26] c. tsallis,    possible generalization of boltzmann-gibbs

statistics,    j. stat. phys. 52, 479 (1988).

[27] a. r  enyi,    on measures of id178 and information,   
in berkeley symposium on mathematical statistics and
id203 (university of california press, berkeley, ca,
1961).

[28] j. bri  et and p. harremo  es,    properties of classical and
quantum jensen-shannon divergence,    phys. rev. a 79,
052311 (2009).

[29] t. sch  urmann,    bias analysis in id178 estimation,   

j. phys. a 37, l295 (2004).

[30] g. herdan, type-token mathematics (mouton, den haag,

1960).

[31] h. s. heaps, information retrieval (academic press,

new york, 1978).

[32] m. gerlach and e. g. altmann,    scaling laws and    uctu-
ations in the statistics of word frequencies,    new j. phys.
16, 113010 (2014).

[39] e. archer, i. m. park, and j. w. pillow,    bayesian en-
tropy estimation for countable discrete distributions,    j.
mach. learn. res. 15, 2833 (2014).

[40] i. nemenman, f. shafee, and w. bialek,    id178 and
id136, revisited,    in advances in neural informa-
tion processing systems 14 (mit press, cambridge, ma,
2002).

[41] for small values of n , the bayesian estimates of the in-
dividual entropies in some cases yield   d(   p,   q) < 0 such
that we consider averages over |   d(   p,   q)|.

[42] p. bernaola-galv  an, i. grosse, p. carpena, j. l. oliver,
r. rom  an-rold  an, and h. e. stanley,    finding borders
between coding and noncoding dna regions by an en-
tropic segmentation method,    phys. rev. lett. 85, 1342
(2000).

[43] we use the google-ngram corpus containing the fre-
quency of usage of millions of words from 4% of all books
ever published in the english language with a yearly res-
olution [11].

[44] m. pagel, q. d. atkinson, and a. meade,    frequency
of word-use predicts rates of lexical evolution through-
out indo-european history,    nature (london) 449, 717
(2007).

[45] e. lieberman, j.-b. michel, j. jackson, t. tang, and
m. a. nowak,    quantifying the evolutionary dynamics
of language,    nature (london) 449, 713 (2007).

[46] t. yasseri, a. kornai,

and j. kert  esz,    a practical
approach to language complexity: a wikipedia case
study,    plos one 7, e48386 (2012).

[47] e. stamatatos,    a survey of modern authorship attribu-
tion methods,    j. am. soc. inf. sci. tec. 60, 538 (2009).
[48] m. e. j. newman,    power laws, pareto distributions and

zipf   s law,    contemp. phys. 46, 323 (2005).

[49] j.-p. bouchaud and a. georges,    anomalous di   usion
in disordered media: statistical mechanisms, models and
physical applications,    rev. mod. phys. 195, 127 (1990).

