5
1
0
2

 
t
c
o
9

 

 
 
]
l
c
.
s
c
[
 
 

1
v
3
2
8
2
0

.

0
1
5
1
:
v
i
x
r
a

human languages order information ef   ciently

daniel gildea,1 and t. florian jaeger1,2,3

1department of computer science

2department of brain and cognitive sciences

3department of linguistics

university of rochester
rochester, ny 14627, usa

abstract

most languages use the relative order between words to encode meaning relations.
languages differ, however, in what orders they use and how these orders are mapped
onto different meanings. we test the hypothesis that    despite these differences    hu-
man languages might constitute different    solutions    to common pressures of lan-
guage use. using monte carlo simulations over data from    ve languages, we    nd
that their word orders are ef   cient for processing in terms of both dependency length
and local lexical id203. this suggests that biases originating in how the brain
understands language strongly constrain how human languages change over genera-
tions.

1

introduction

we test the hypothesis that language change is subject to small but persistent biases that
result, on average, in languages that are easier to process. biases for grammars with
higher processing ef   ciency could be the direct result of abstract learning biases [74, 21]
or they could result from the pressures of language use [11, 42, 43], such as preferences
that have been hypothesized to operate during language production [30, 48, 58] or biases
originating in comprehension [37, 68, 71, 81].

if language change is indeed subject to biases towards languages with higher process-
ing ef   ciency and if these biases are suf   ciently strong, these biases should over accumu-
late over historical time, leading natural languages that have existed for suf   ciently long
to have higher than expected processing ef   ciency. this is the primary hypothesis we set
out to test. some evidence suggests that the sound structure and lexicon of natural lan-
guages exhibit properties that are expected under this hypothesis [35, 64, 69, 70, 85]. at

1

those levels of linguistic organization, studies over the last couple of years have also pro-
vided more direct correlational evidence that language change is affected by processing
[82]. miniature language learning experiments have documented similar biases during
id146 and that these biases can accumulate over generations of learners
[50].

the level of linguistic organization that has remained elusive with regard to this ques-
tion, however, is also arguably the one that is the one that makes human languages most
unique compared to all other animal communication systems: syntax    or some aspects of
syntax (recursion)    give human languages in   nite expressivity with    nite means [45, 67]
and it is syntax that has been taken to be the de   ning property of human languages (e.g.,
[40]; but see [72]). whether at least some properties of the syntactic systems of languages
can be derived from the fact that languages need to be processed continues to be a heat-
edly debated question (for recent high impact reviews, see [21, 51, 50, 72]). one reason
why this question has not been directly addressed, as we detail below, is that until very
recently it has been impossible to directly test whether the syntax of natural languages
tends to facilitate processing ef   ciency. here we present the results of several large-scale
computational simulations that address these questions. for the purpose of presentation,
we group these simulations into studies 1 and 2.

study 1 tests and    nds con   rmed the hypothesis that natural languages have word
orders that makes them easier to process than expected by chance. from this is does not
follow that natural languages have optimal or even close to optimal processing ef   ciency.
processing ef   ciency is presumably just one of several factors that might bias language
change (the ease of acquisition of a grammar being another constraint). still, if biases
towards ef   cient processing are among the most in   uential factors in   uencing language
change, we would expect human languages to have word orders that are pretty close to
optimal in terms of processing ef   ciency. this hypothesis is tested in study 2. taken
together, studies 1 and 2 suggest that language processing exhibits a surprisingly strong
bias on language change.

the hypothesis we test is one that has long intrigued language researchers. the pres-
sures inherent to language processing have long been assumed to shape languages over
time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic
structure [11, 5, 6, 42, 74]. however, until relatively recently it has virtually been impossi-
ble to obtain reliable estimates of the processing ef   ciency of a language. imagine one was
to obtain such estimates experimentally (e.g., by obtaining estimates of the word-by-word
processing times a native speaker of that language experiences while reading sentences
from that language). a reliable estimate of the processing ef   ciency of an entire language
would require reading data for a representative sample of the language. ideally, this sam-
ple would be representative in terms of its lexical and grammatical distributions    i.e.,
it should contain both low and high frequency words, more and less complex syntactic
structures, and so on. further reliable estimates would require that individual differences
in, for instance, reading abilities are averaged out. in short, hundreds of readers would
likely have to read thousands of sentences. this alone is a daunting task. in one of the

2

two most commonly used methods to obtain word-by-word reading time estimates (self-
paced reading), it takes between .5-1 hour to obtain reading times for 100 sentences. so,
to obtain data from 100 readers on, say 1000 sentences from a language    which would
still not be a lot of sentences   , we would require about 500-1000 participant hours.

however, by far the biggest challenge lies in establishing a chance-level against which
to compare the processing ef   ciency of a language. this requires estimates of processing
ef   ciency from a large set of randomized variants of a language (see below). this further
increases the required experimental data by several orders of magnitude. assessing the
processing ef   ciency of a language based on human data is thus prohibitively expensive
and time-consuming. the smallest study we present below would correspond to 500,000
participant hours. at new york state minimum wage (as of 12/31/2014), this approach
of assessing processing ef   ciency would cost over 4 million us dollars per language, for a
total of 20 million dollars for the    ve languages we examine. it would also arguably pro-
vide an utterly anti-conservative estimate of chance (to say the least): without extensive
training on the new language variant, participants would experience massive interference
from their native language, making it appear as if human languages are highly ef   cient
simply because it is the one that participants are familiar with (it takes most learners of a
language years to have approximately native-like processing speeds).

here, we take an alternative approach. we take advantage of advances in compu-
tational psycholinguistics, natural language processing, and the availability of large lin-
guistic databases. rather than to obtain estimates of processing ef   ciency from human
readers, we automatically estimate the processing ef   ciency of a language from large lin-
guistically annotated collections of text (syntactically annotated corpora). this is now
possible, because psycholinguistic research has identi   ed grammar-dependent measures
of processing ef   ciency. here, we focus on two properties that are known to affect word-
by-word processing times: a word   s shannon information in context (i.e., its surprisal
[39, 56]) and the length of the dependencies that are integrated at the word (dependency
length, [28, 29]).

we describe and further motivate these two measures in more detail in below. for
now, it suf   ces to say that processing dif   culty (as assessed through, e.g., per-word read-
ing times) is positively correlated with surprisal and dependency length. if a bias for
processing ef   ciency affects the development of languages over time, it is thus expected
that natural languages have lower average surprisal and shorter average dependency
lengths than expected by chance.

we test these predictions against data from    ve languages: arabic (modern standard),
czech, english (american), german, and mandarin chinese. these    ve languages were
chosen for two reasons. first, we aimed for representative linguistic coverage. languages
often share linguistic properties simply because they are historically related or because
they have co-existed in geographic proximity over long periods of time, with the ensuing
language contact leading to lexical and grammatical borrowings. here, we are interested
in testing hypotheses that are assumed to apply universally across all languages. the
less historically and geographically related the languages in our sample are, the more

3

arabic (modern standard)
czech
english (american, written)
english (american, spoken)
german
mandarin chinese

sentences

6,776
72,703
39,832
17,968
45,422
28,289

sentence length

mean std dev min max
387
166
122
92
115
212

(26.9)
(9.6)
(10.1)
(8.2)
(9.6)
(16.4)

35.4
14.8
20.9
7.9
15.5
23.8

1
1
1
1
1
1

table 1: overview of corpora used in the current studies

likely any effect found on this sample is to generalize beyond the particular sample to any
language.

the    ve languages we investigated represent three major language families (sino-
tibetan, indo-european, and semitic) and four language subfamilies (chinese, balto-
slavic, germanic, and arabic). the    ve languages also differ in a variety of linguistic
properties that are known to be relevant to processing dif   culty. for example, three of the
languages in our sample have dominant subject-verb-object (svo) order, one of them has
dominant vso order (arabic), and one has no dominant word order (german). the lan-
guages also differ in whether they productively use morphological means to mark gram-
matical relations, such as using case (arabic, czech, german), or not (english, mandarin).
as a third and    nal example, the languages differ in whether and how they express cer-
tain arguments to the verb. for example, pronominal elements in subject position (e.g.,
i, you, he) can optionally be omitted in mandarin, are realized as suf   xes on the verb in
czech, but are more or less obligatorily realized as separate words in english and ger-
man. any of these properties could theoretically affect the measures we assess in our
studies.

second, as we describe next, suf   ciently large electronic corpora with the necessary
linguistic annotations are now available for these languages. corpus size is critical for
our purpose. the reliability of the estimates we derive below depends on the number
of words and sentences in the corpus. for example, the accuracy and reliability of the
processing ef   ciency estimates described below increases with the number of words in a
corpus. the corpora we employ in our studies are the largest available corpora for the
   ve languages with the required linguistic annotation. the methods we use to obtain
surprisal and dependency length estimates further increase robustness of estimates.

2 data

the data for all languages comes from newspaper corpora. for english, we also had ac-
cess to a corpus of conversational speech data with the required annotations. an overview
of the corpora is provided in table 1.

speci   cally, the arabic data consists of 6776 sentences from the penn arabic treebank,

4

sbar

whadvp

s

s

,

,

np

vp

prp

vbd

.

.

wrb

np

vp

i

left

when

dt

nn

vbd

the

man

arrived

figure 1: example syntax tree

in the dependency representation of the prague arabic dependency treebank version 1
[38]. the czech data consists of 72,703 sentences from the prague dependency treebank
version 1 [7], as used in the conll 2006 id33 evaluation [10]. the english
data comes from two sources. for written data, we use the 39,832 sentences from the
wall street journal portion of the id32 version 3 [65]. for spoken data, we
use 17,968 sentences from the switchboard corpus of spoken english [32]. the german
data consists of 45,422 sentences from the tiger corpus [9], which primarily consists
of articles from the german newspaper    frankfurter rundschau   . finally, the mandarin
chinese data consists of 28,289 sentences from the penn chinese treebank version 6.0 [83].
this includes newswire from xinhua news agency, articles from sinorama magazine,
news from the website of the hong kong special administrative region and transcripts
from various broadcast news programs.

all corpora consist of sentences that have been manually annotated with the syntactic
structure of each sentence. the annotations specify a syntactic structure for each sen-
tence. the annotation types differed somewhat between languages. an example, from
the english corpus is shown in figure 1.

we automatically converted the different syntactic annotations into a dependency rep-
resentation, as shown in figure 2. we use the dependency representation because depen-
dency length has been shown to be an important variable affecting human language pro-
cessing (see below). the dependency representation is a directed graph specifying, for
each word in the sentence, the head word (or    sender    [23]) that it modi   es. for example,
subjects and direct objects modify the main verb of the clause; determiners, adjectives,
and relative pronouns typically modify nouns; prepositions can modify nouns or verbs;
prepositions are modi   ed by the object nouns; and so on. we convert trees to dependency
representations using a set of rules which specify which child of each node in the tree is
the head child, i.e., the main component of the phrase [62, 16]. recursively choosing a
head child for each node from the top down, we    nd a head word for each node in the
tree. at each node in the tree, dependency relations are created indicating that the head

5

s>sbar
dt>nn

sbar>s

sbj>s

sbj>s

when

the

man

arrived

i

left

figure 2: dependency structure, converted from the syntactic tree in figure 1

word of the head child is modi   ed by the head word of each other child. the dependency
representation tends to be robust to the details of the syntactic annotation schemes used
by various corpora.

for english, german, and chinese, we extracted dependencies from constituent rep-
resentations, converting the representation of figure 1 to that of figure 2. speci   cally,
we extract dependencies using the head-   nding rules of collins [16]. our dependency
types consist of pairs of syntactic categories, with one element representing the category
of the maximal projection of the head, and one representing the category of the maximal
projection of the modi   er. additionally, we include a special subject type in order to dif-
ferentiate verb subjects and direct objects, by using the    sbj    function tag in the penn
treebank annotation (see figure 2).

for czech and arabic, our data was originally annotated in a dependency representa-
tion. we take advantage of relation labels provided, which included relations such sub-
ject, object, attribute, and so on. our dependency types consist of both the relation of a
word and the relation of its parent, in order to allow us to distinguish between, for exam-
ple, an attribute relation in a subject noun phase and an attribute relation mod   fying a
verb in a relative clause.

3 estimating the processing ef   ciency of languages

as outlined in the introduction, we focus on two measures of processing ef   ciency that
have received broad empirical support: surprisal [39, 56] and dependency length, [28, 29].

the surprisal of a word is identical to its shannon information (in bits) in context,

which is de   ned as the logarithm (to base 2) of the inverse of its id203 in context.

1

i(w) = log2

p(w|context)
=     log2 p(w|context)

(1)

(2)

a word   s surprisal (conditioned on all relevant preceding context) has been shown to
be identical to the relative id178 (or id181) between the distri-
bution over all possible parses prior to the word and the distribution over all possible
parses after processing the word [55]. surprisal can thus be understood as a measure of

6

the amount of syntactic belief-updating that is associated with processing the word. cru-
cially, a word   s surprisal has been found to be a good predictor of its reading times in con-
text [8, 18, 25, 66, 75]. for example, in a large-scale reading experiment, smith and levy
[75] found that per-word reading times were linear in the word   s surprisal. this relation
held over six orders of magnitude in the id203, from almost perfectly predictable
instances of words to barely predictable instances (1     p(word|context)     .000001). sur-
prisal has also been found to be re   ected in neural responses.

dependency length, too, has been found to affect processing dif   culty, with longer
dependencies leading to longer reading times at their integration point. consider the
word left in the example in figure 2. two dependencies end    and are thus assumed to be
integrated    at the word left. one is the dependency between the verb left and its subject
(i). this dependency is local. the other dependency is between the verb and its temporal
modi   er (when the man arrived). this dependency is non-local. psycholinguistic research
has found that non-local dependencies tend to cause processing dif   culty ([28, 29, 36];
though see [57, 79] for discussion). there is also evidence that cross-linguistically speak-
ers prefer shorter dependencies over longer ones when their language provides them
with two ways of encoding a message (e.g., for basque [73]; english: [2, 1, 59]; japanese
[84]; korean [14]; for reviews and discussion, see [41, 47]).

here, we estimate these two measures for entire languages. that is, unlike in psy-
cholinguistic work, which has focused on the word-by-word effects of surprisal and de-
pendency length on language processing, we are estimating surprisal and dependency
length at the system level. to us, these measures are of interest because they provide
an estimate of the average processing dif   culty a native speaker of a language experi-
ences while processing that language. this allows us to test whether natural languages
have lower average surprisal and shorter average dependency lengths than expected by
chance. an overview of the procedure is given in figure 3.

there are other factors that are known to contribute to processing ef   ciency. for exam-
ple, among the primary contributors to word-by-word processing are lexical properties.
to name just a few of these properties, a word   s length, frequency, neighborhood den-
sity, part-of-speech, and morphological structure are all correlated with the average time
it takes to comprehend or produce that word [3, 4, 8, 17, 18, 60, 63]. as expected under
the general hypothesis tested here, several studies have found the lexicon of languages
to exhibit properties that are consistent with the hypothesis that processing ef   ciency
over time shapes the phonology of words [12, 15, 35, 64, 69, 70, 85]. here, however, we
are interested in a grammatical property of languages    speci   cally, word order    and how
it affects processing ef   ciency.
it is grammatical properties that would differ between
grammatical systems, thus allowing processing preferences to affect    selection    of these
properties over time. the approach we present below therefore holds constant all context-
insensitive lexical properties, ruling these factors out as an explanation for hypothetical
preferences for certain grammatical systems.

7

figure 3: overview of procedure used to compare the processing ef   ciency of natural lan-
guages (measured in terms of their average information density and dependency length)
against the baseline ef   ciency expected by chance. weighted grammars are the random
reorderings of sets of dependencies (see text).

8

3.1 estimating processing ef   ciency
3.1.1 surprisal and information density

we estimate surprisal by means of a trigram model, which conditions a word   s id203
on the previous two words. for example, id203 of the sentence in figure 2 would
be modeled as:

p (when | (cid:104)s(cid:105))p (the | (cid:104)s(cid:105) when)p (man | when the)p (arrived | the man)      

where (cid:104)s(cid:105) indicates a sentence boundary.

id165 models of this type are widely used in id103 [49, 33] and ma-
chine translation [53]. id165 models like the ones used here are also known to provide
good approximations to computationally far more complex language models, such as
probabilistic phrase structure grammars (see e.g., [27]). one reason for this is presumably
that the local context of a word often captures many semantic phenomena through the co-
occurrence of related words (e.g., read and book in the trigram read the book). trigrams also
capture local syntactic patterns, such as the requirement of accusative case after certain
prepositions (e.g., to me) or subject-verb agreement (e.g., man arrives).

id165 models also have two properties that make them particularly appealing for
the current purpose. first, estimating id165 probabilities from corpora is far less compu-
tationally complex than estimating the same probabilities from structurally more complex
models (such as probabilistic phrase structure grammars). since, as we detail below, this
modeling needs to be repeated many times for each language, computational simplicity is
critical for the current study. second, id165 models have also been successfully used as
models of human language processing [8, 25, 48]. in fact, recent studies have argued that
models that primarily rely on the information captured by local context (such as the two
preceding words) fair better in explaining word-by-word variation in human processing
times than structurally more complex models ([25]; but see also [24, 77]).
indeed, the
   nding we mentioned above, that a word   s id203 in context is log-linearly related to
the processing dif   culty it causes, was based on a trigram estimate of the type employed
here [75]. in short, trigram models are well-suited for the current purpose of estimat-
ing processing ef   ciency. one reason for this might be that human language processing
preferably relies on more local information    for example, because non-local information
will tend to be less informative or because non-local information will be more costly or
less reliably retrieved from memory (consistent with the observation that non-local de-
pendencies are harder to process).

in order to obtain reliable estimates of a word   s trigram id203 even when the
preceding two words were rarely (or never) observed in the training corpus, we smooth
our trigram probabilities using the interpolated kneser-ney method [52, 13]. kneser-
ney is a technique that assigns id203 to unseen id165s according to a measure of
how likely the words in the trigram are to combine with new words. using kneser-ney
smoothed trigram probabilities have two advantages over alternative id165 models.
first, kneyser-ney smoothing perform well across a wide variety of tasks and is consid-
ered one of the most effective methods of dealing with unobserved trigrams. second, it is

9

speci   cally kneser-ney smoothed trigram estimates of surprisal that recent work found
to be linearly correlated with reaction times [75]. this makes this particular approach
well-suited for our purpose of estimating the average processing ef   ciency of a language.
surprisal and information density can be estimated at different levels of linguistic de-
scription. for example, in the psycholinguistic literature on sentence processing, surprisal
is usually calculated per word [25, 56, 75]. however, psycholinguistic research on pho-
netic production has also calculated information density at the sub-lexical level (e.g., the
information per sound in a word, [15, 78]). natural languages could theoretically be ef   -
cient at one level but not the other.

here, we consider two estimates of information density. the    rst estimate is the
by-word information density based on the unnormalized per-word information derived
from the trigram model. this is essentially the same measure that has found to correlate
linearly with word-by-word reading times in english [75].

the second estimate is a normalized by-character estimate of the amount of informa-
tion per sound or writing unit. for this second estimate, we    rst counted the number of
unique characters in the data base (see data above). speci   cally, we used the logarithm to
base 2 of that count, thereby measuring the number of bits one would need to encode all
unique characters observed in the databased for each language. for example, there were
48 unique characters (5.6 bits) in our english corpora (this includes special symbols like $)
and 4394 unique characters (12.1 bits) in our mandarin database. we then normalized the
information content of each word by the number of letters in that word multiplied by the
per-character bits for that language. this id172 has the advantage that it applies
the same standard across different writing systems. for example, mandarin chinese em-
ploys a logographic writing system, so that there are no letters. for spoken language, our
id172 approximates the number of phonemes in a word and its spoken duration,
while also taking into account the number of distinct sounds in the language. for writ-
ten language, our id172 corresponds directly to information per character, taking
into account the number of distinct symbols used in the database.

we note that our results are not sensitive to the choice of id172: all results
were qualitatively similar without any length id172. furthermore, the speci   c
id172 procedure chosen here only affects comparisons across languages (which
is not of theoretical interest here), as the id172 constant does not vary within
one language (see equation 5 below, where only |wi| varies by word, whereas the per-
character bits are a constant factor).

3.1.2 dependency length

our other measure of processing dif   culty is dependency length. this metric can be read
off the dependency trees, counting the number of words from each modi   er to its head
in the linear order of the sentence. for example, for the dependency structure in figure 2,
the word left is the    fth word from the word when. the length of the sbar>s dependency
between when and left is thus of length 5. the sbj>s dependency between the words i and
left, on the other hand, is of length 1. in our experiments, we compute the average length

10

of all the dependencies in all sentence. in figure 2, we have dependencies of length 1, 1,
1, 3, and 5, for an average length of 2.2.

a number of different metrics have been proposed to measure dependency length.
for example, dependency length is sometimes measured in terms of the number of inter-
vening non-discourse given referents [29], or in terms of the syntactic complexity of inter-
vening material. all of these measures tend to be highly correlated [76, 80]. for the current
purpose, we measure dependency length in words (following [23, 31, 43, 44, 59, 73]). this
measure has the advantage that it is easy to calculate and achieves broad-coverage (see
also [18]).

3.2 estimating chance
to obtain a chance baseline against which to compare the processing ef   ciency of each
language, we create 1000 variants for each language. speci   cally, we obtain 1000 pseudo-
grammars by randomly re-ordering the dependency structures described above, while
keeping the dependency relations between heads and their dependents intact. each pseudo-
grammar thus describes a theoretically possible reordering of the actual human language.
critically, this variant holds constant:

    all context-insensitive lexical properties, including all semantic and phonological

factors at the level of the word

    the number and identity of the sentences in the corpus
    the number of words in each sentence (which is known to affect estimates of the

per-word information) and in the corpus

    the identity of the words in each sentence (including their part of speech) and in the

corpus

    the number of heads, dependents, and dependencies in each sentence and in the

corpus

    the frequency of different types of dependencies in each sentence and in the corpus

we then measure the average information density and dependency length of each
variant of a language, allowing us to compare the information density and dependency
length of the actual languages against what is expected by chance (i.e., against the dis-
tribution of information density and dependency lengths observed for the 1000 pseudo-
grammars derived from that language).

for our representation of a possible    xed order, we use weighted grammars [31]. in
this representation, each dependency type (e.g., sbj>s in figure 2 is assigned a numeric
weight   i between -1 and 1. the head itself always has weight zero. dependencies with
negative weights appear to the left of the head, and dependencies with positive weights

11

s>sbar

sbar>s
sbj>s

dt>nn

sbj>s

when

arrived

the

man

left

i

figure 4: word order of a pseudo-grammar for the same sentence shown in figure 2.

to the right. for all studies reported below, these weights were held constant for each de-
pendency type. more speci   cally, we held orders constant within each set of dependencies,
where a set refers to all dependency types that end in the same head. one example of
a dependency set are all dependencies that end in a head noun (i.e., all noun phrase-
internal dependencies). weights thus de   ne a deterministic order over all dependents
of a head, from left to right in order of their numeric weights. for example, with re-
gard to the head of the sentence (s), a given pseudo-grammar might de   ne the order
sbj sbar s pp np. the relative order for the four dependency types in the rule above
(sbj>s, sbar>s, pp>s, and np>s), then also implies an order of sbj s np for sentences
in which only these two dependencies connect to s. as we show in control study 1, this
is a conservative assumption for the calculation of chance for both information density
and dependency length, i.e., it biases against the hypothesis tested here.

an example of a possible re-ordering of the example sentence of figure 2 is shown in

figure 4.

for each pseudo-grammar speci   ed by a set of weights   , we estimate the information

density and dependency length with the following procedure:

1. order the training portion of our corpus according to   .

2. estimate a kneser-ney trigram language model l(  ) from the training corpus.

3. order the test portion of our corpus according to   .

4.

(a) compute the average per-word information, hword, and normalized per-character

information, hcharacter, in the test data according to   , where n is number of
word tokens in the database, wi is the ith word token in the test data, wi   2, wi   1
are the two preceding word tokens, and pl(  ) is the id203 according to   :

hi(  ) = log2

pl(  )(wi | wi   2, wi   1)

1

n(cid:88)

i=1

hi(  )

  hword(  ) =

1
n

12

(3)

(4)

pearson   

by-character by-word
0.41
0.47
0.62
0.43
0.24

0.50
0.25
0.40
0.15
0.23

arabic
czech
english
german
mandarin

table 2: correlation of information density and dependency length in the random sam-
ples created for each language.

n(cid:88)

i=1

  hword is thus the average per-word information of a language sample, which
we refer to below as the by-word information density. and for the by-character
estimate of information density:

  hcharacter(  ) =

1
n

1

|wi| log2 k

hi(  )

(5)

where |wi| is the length of wi in characters, and k is the number of unique
characters in the database.

(b) compute the average dependency length of each word in the test data.

in all experiments, we use 9/10s of the available data as training data in step 1 above,
and the remaining 1/10 as test data in steps 4 and 5. this procedure takes several hours
(a few minutes per random order) of computer time, as it involves building a large table
of id165 counts for each new random order considered.

figure 6 shows the information density and dependency length of all 1000 samples
for the    ve languages. as indicated by the non-parametric smoother in figure 6, infor-
mation density and dependency length are positively correlated in the random pseudo-
grammars. although the strength of this correlation differs across languages (see table 2),
this correlation is signi   cant in all languages (pearson correlation ps < 10   6). this means
that shorter dependencies (i.e., keeping words that belong together adjacent to each other)
also tend to reduce information density. this correlation makes intuitive sense. recall
that we are using a trigram language model to estimate information density. to the ex-
tent that the syntactic dependencies annotated in the corpora we employed (see data
above) capture relevant statistical dependencies between words, it is thus expected that
trigram probabilities will be higher (and information density estimates lower) for word
orders that keep syntactic dependencies (and thus more often within the three word win-
dow). it is, however, an interesting question for future research whether the correlation
we observe here holds even when more computational more complex estimates of word
probabilities are used.

13

figure 5: average information density and dependency length of randomly generated
pseudo-grammars.
individual data points show the 1000 samples for each language.
contour lines show a 2d density estimation based on a bivariate normal kernel. this
summarizes the distribution of the random pseudo-grammars. panel (a) shows this for
the by-character estimate of information density and panel (b) for the by-word estimate
of information density.

14

arabicczechenglishgermanmandarin0.1460.1480.1500.1520.1540.1560.4300.4350.4400.4450.4500.400.410.420.3800.3850.3900.3950.1640.1660.1686784.04.55.03.03.54.04.53.03.54.0345dependency length (in words)information density (in bits / character)languagearabicczechenglishgermanmandarinarabicczechenglishgermanmandarin10.110.210.310.410.510.612.4012.4512.5012.5512.609.29.39.49.59.610.610.710.810.010.110.210.356784.04.55.03.03.54.04.55.03.03.54.04.53.03.54.04.55.0dependency length (in words)information density (in bits / word)languagearabicczechenglishgermanmandarinaverage information density

average dependency length

by-character

by-word

actual higher than

actual higher than

arabic
czech
english
german
mandarin

language
0.143
0.435
0.377
0.380
0.159

random language
0/1000
9.434
12.150
39/1000
8.781
0/1000
19/1000
10.395
9.760
0/1000

actual
random language
0/1000
3.21
2.94
0/1000
2.25
0/1000
0/1000
3.28
3.44
0/1000

higher than
random
0/1000
0/1000
0/1000
16/1000
220/1000

table 3: mean information density and dependency length of actual human language (on
test data) compared to 1000 random pseudo-grammars of that language when constituent
order is assumed to be    xed within each dependency type.

4 results

we    rst compare the actual information density and dependency length of    ve languages
in our sample against the pseudo-grammars derived from them. then we present four
control studies that serve to illustrate the robustness of our results.

4.1 study 1: comparing the information density and dependency length

of human languages to chance

figure 6 shows both the actual human languages and the 1000 random samples for each
of them on a plane de   ned by the two measures of processing ef   ciency considered here.
table 3 provides a numerical summary. as can be seen, the processing ef   ciency of ac-
tual arabic, czech, english, german, and mandarin chinese is considerably better than
expected by chance. speci   cally, applying a standard signi   cance criterion of    = .05,
all    ve languages have lower information density than expected by chance, and all lan-
guages but chinese have shorter dependency lengths than expected by chance.

next, we present three control studies that demonstrate the robustness of our    ndings.
since the studies we present here are computationally demanding, we limit our control
studies to one of the two information density estimates. we chose to focus on the per-
character estimate, as we take it to be less re   ective of properties speci   c to the writing
systems of the language (such as what constitutes a written word).1 additionally, this
is the more conservative approach given the results in table 3, which are stronger for
by-word information density.

1for example, whereas compounds are generally written as one word in german (e.g., rotwein), they
tend to be written as separate words in english (e.g., red wine). the per-character estimate of information
density is not affected by this orthographic decision.

15

figure 6:
illustration of the processing ef   ciency of actual human languages (solid
shapes) compared to their processing ef   ciency expected by chance (contour lines). pro-
cessing ef   ciency is measured in terms of information density (y-axis) and dependency
length (x-axis), which are both known to be positively correlated with processing times.
processing ef   ciency is thus higher, the lower the average information density and the
lower the average dependency length. contour lines show a 2-dimensional density esti-
mation based on a bivariate normal kernel, summarizing the distribution of the random
pseudo-grammar. panel (a) uses by-character information density. panel (b) uses by-word
information density.

16

larabicczechenglishgermanmandarin0.1440.1470.1500.1530.1560.4350.4400.4450.4500.380.390.400.410.420.3800.3850.3900.3950.1600.1620.1640.1660.168345673.03.54.04.55.02.53.03.54.04.53.23.43.63.84.04.23.03.54.04.5dependency length (in words)information density (in bits / character)languagelarabicczechenglishgermanmandarinlarabicczechenglishgermanmandarin9.49.69.810.010.210.412.212.312.412.512.68.89.09.29.410.410.510.610.710.89.89.910.010.110.2345673.03.54.04.55.02.53.03.54.04.53.43.63.84.03.03.54.04.5dependency length (in words)information density (in bits / word)languagelarabicczechenglishgermanmandarinarabic
czech
english
german
mandarin

percentage
92.7%
74.5%
92.8%
88.7%
90.1%

table 4: average percentage of most frequent ordering within a dependency set, averaged
across all dependency sets, for the languages in our sample.

4.2 control study 1: fixed vs.    exible constituent order
the results in table 3 are based on pseudo-grammars that were calculated under the
assumption that languages have    xed constituent orders within a dependency type. in-
terestingly, this assumption does approximate, but not quite match, what is observed for
human languages. table 4 provides a measure of the word order consistency of the lan-
guages in our sample.

for the calculation of chance for information density, the    xed-order assumption made
in study 1 is expected to be conservative, biasing against the hypothesis we are testing:
on average,    xed constituent orders increased the predictability of words, thereby low-
ering the average information density. this should give the pseudo-grammars derived
for study 1 a distinct advantage compared to the actual human languages, which often
do not have    xed constituent orders (or at least not entirely    xed orders). for example,
even english, which is considered a relatively    xed order language, allows constituent
order variation. most obviously this holds for alternations, such as the choice between
active and passive or heavy noun phrase shift (e.g.,he put the book on the table vs. he put on
the table the book, but he put the book he had gotten from a long lost friend on the table vs. he
put on the table the book he had gotten from a long lost friend). generally, the assumption of
   xed constituent orders in study 1 should thus be conservative with regard to informa-
tion density.

however, for the calculation of chance for dependency length, the consequences of
the assumption of    xed constituent order are less clear. it is possible that this assumption
made the dependency length results anti-conservative. we therefore repeated study 1
while allowing constituent order to vary absolutely freely. that is, rather than to use the
weighted grammar approach described above in creating random pseudo-grammars, we
randomly ordered all dependents for each instance of a dependency.

table 5 summarizes the results. for all languages in our sample, both the by-character
information density and dependency length of actual human languages were better than
that observed for any of the 1000 pseudo-grammars. control study 1 thus replicates the
results of study 1 and shows that the assumption of    xed constituent order made in study
1 biases against our hypothesis, relatively to allowing constituent order freedom.

we further note that the results of study 1 were also replicated when constituents
were allowed to order freely, but the position of dependents relative to the head was held

17

average information density average dependency length

by-character

actual
language
0.143
0.435
0.377
0.380
0.159

arabic
czech
english
german
mandarin

higher than

actual
random language
3.21
0/1000
2.94
0/1000
2.25
0/1000
0/1000
3.28
3.44
0/1000

higher than
random
0/1000
0/1000
0/1000
0/1000
0/1000

table 5: mean information density and dependency length of actual human language (on
test data) compared to 1000 random pseudo-grammars of that language when constituent
order is completely randomized.

constant (e.g., if all dependents occurred to the right of their head). taken together, this
suggests that the results obtained in study 1 are robust to assumptions about constituent
order freedom in the calculation of the chance baseline.

4.3 control study 2: sensitivity to genre and mode
while our primary datasets are taken from newspaper text, we wanted to test whether our
results were sensitive to the genre of the corpus, and in particular whether edited, written
text might have different properties than spontaneous, spoken text. unfortunately, large
syntactically annotated corpora are available only for very few languages. here we test
our hypothesis against conversational speech data from english.

repeating study 1 on the english switchboard corpus of conversational speech, we
again    nd that the processing ef   ciency of actual english is better than expected by
chance. actual conversational english had better per-character information density than
all of 1000 random word orders, and better dependency length than all of 1000 random
orders (both ps < .0001).

4.4 control study 3: sensitivity to corpus size
finally, we tested the sensitivity of our results to the amount of text available for esti-
mating the parameters of the kneser-ney trigram model. we thus repeated the analysis
reported above, using a much smaller data set of 1000 sentences randomly drawn from
the wall street journal (i.e., about 2.5% of the original corpus). unsurprisingly, informa-
tion density estimates were higher compared to the main study (reported in table 3)   this
is a direct consequence of the reduced data size: for smaller corpora, there will be more
id165s in the test data that were never observed in the training data; these id165s are
assigned low id203 (and thus high information). the estimates based on a smaller
corpus are also expected to be less reliable because a large porportion of the id165s in

18

the test data will be unseen in the training data, regardless of the word order used. to
quantify this effect, using actual english word order, we    nd that with 1000 training sen-
tences, only 10% of bigram tokens in test data have been observed in training data, even
when not predicting any single words in the test data that are unseen in training data. in
contrast, with our full training set for english, the corresponding    gure is 31%. despite
the low coverage of id165s in our 1000-sentence training set, we again    nd that actual
english has better per-character information density than all of 1000 random word orders,
and better dependency length than all of 1000 random orders (both ps < .05).

4.5 summary
we    nd that the    ve languages we investigated all have signi   cantly higher processing
ef   ciency than expected by chance. this holds for both measures of processing ef   ciency
considered here. for information density, all    ve languages fall into the top 95th per-
centile or better. for dependency length, four of the    ve languages fall into the 95th
percentile, and one language (mandarin) falls into approximately the 75th percentile of
the distribution de   ned by the random pseudo-grammars. our    ndings held regardless
of the size of the corpus and, more importantly, for both written and spoken language.
taken together, this suggests that language use    speci   cally, pressures that originate in
the incremental processing of language    shape the grammar of languages over time.

we note that information density and dependency length were not independent in our
random samples. it is thus possible that what we have    following the literature    treated
as two independent measures of processing ef   ciency is in reality due to one underly-
ing cause. this would not affect the conclusion that the processing ef   ciency of human
languages is better than expected by chance. further, it is worth noting that the correla-
tions in table 2 are mild. indeed, study 2    nds that information density and dependency
length can be optimized separately.

5 study 2: are natural languages optimal with regard to

information density and dependency length?

next, we tested whether an even stronger claim can be made. speci   cally, we won-
dered whether the pressures for ef   cient processing are suf   ciently strong to constrain
language change to the subspace of possible grammars that is optimal (or very close to
optimal) in terms of processing ef   ciency. as outlined in the introduction, many pres-
sures of language use have been hypothesized to bias and constrain language change,
thereby contributing to cross-linguistically observed properties of languages. we thus
did not expect languages to be optimal in terms of processing ef   ciency. we begin by
describing the procedure used to estimate the minimum possible information density for
each language. then we describe the procedure used to estimate the minimum possi-
ble dependency length for each language. finally, we present a procedure that jointly

19

optimizes both information density and dependency length for each language, allowing
us to compare human languages against pseudo-grammars that optimally trade-off the
two major contributors to processing ef   ciency. the results of these three calculations are
presented and discussed at the end of this section.

5.1 computing pseudo-grammars with optimal information density
we begin by describing the procedure used to calculate the minimum possible informa-
tion density h    for each language:

h    = min

  

h(  )

in order to    nd h   , we optimize one weight at a time, holding all others    xed, and it-
erating though the set of weights to be set. the objective function describing information
density is piecewise constant, as the objective function will not change until one weight
crosses some other, causing two dependents to reverse order, at which point the objec-
tive will discontinuously jump. this non-differentiability implies that methods based on
gradient ascent will not apply. however, because the objective function only changes at
points where one weight crosses another   s value, the set of segments of weight values
with different values of the objective function can be exhaustively enumerated. in fact,
the only signi   cant points are the values of other weights for dependency types which oc-
cur in the corpus attached the same head as the dependency being optimized. we build
a table of interacting dependencies as a preprocessing step on the data, and then when
optimizing a weight, consider the sequence of values between consecutive interacting
weights. for each value in this sequence, we evaluate the objective function h on the test
corpus, and choose the value yielding the minimum value of h.

this search procedure is similar to one previously used for    nding the grammar that
minimizes dependency length [31]. our present problem, however, is considerably more
computationally intensive, because when evaluating each possible value of each weight,
we must examine the entire test corpus, whereas, when optimizing dependency length,
one can take a shortcut in evaluation, by only considering sentences with the dependency
type whose weight has been modi   ed. in our case, since all the parameters of the id165
language model are subject to change at each step, we must evaluate the language model
on every word in the test corpus.

this optimization process is not guaranteed to    nd the global maximum, but is guar-
anteed to converge simply from the fact that there are a    nite number of objective func-
tion values, and the objective function must increase at each step at which weights are
adjusted. running the optimization procedure from different random initializations, we
   nd that, while    nal grammars are not identical, they are very close in terms of our ob-
jective function, which indicates that we are likely to be close to the global optimum. for
example, in ten runs optimizing the by-word information density of english, our    nal
values of the objective function have a variance of less than 10   5.

20

for our experiments, we    nd that the optimization procedure converges after several
days of computer time. however, the procedure reaches points very close to the eventual
optimum within several hours.

5.2 computing pseudo-grammars with optimal dependency length
we also optimize our pseudo-grammar in order to    nd the weights giving the lowest
dependency length:

d    = min

d(  )

  

where d(  ) is the average dependency length for the pseudo-grammar with weights   .
the search over weights uses the same algorithm described above.

5.3 joint optimization
there may be a trade-off between information density and dependency length. although
we found information density and dependency length to be positively correlated in the
random pseudo-grammars generated for study 1, these correlations were mild to mod-
erate. it is therefore possible that optimization of information density trades off against
optimization of dependency length (and vice versa). we therefore investigate the effect
on the dependency length of optimizing for information density, and vice versa. we also
experiment with a joint objective function j that combines information density and de-
pendency length:

j    = min

(1       )d(  ) +   h(  )

  

we then applied the same optimization algorithm described above to this joint mea-
sure of processing ef   ciency. the separate optimizations described in the previous two
sections correspond to   s of 1 and 0, respectively. we also considered   s of .5, .6, .7, .8,
and .9 (note that these weights are hard to interpret by themselves: information density
and dependency length are on different scales, as shown in table 3 above.

5.4 results
table 6 shows results when optimizing for different objective functions in each column.
rows show the by-character information density and dependency length for each lan-
guage. the left half of the table shows the information density and dependency length
for the optimal pseudo-grammars derived by optimizing information density (id), de-
pendency length (dl) or both jointly with    = 0.5 (id & dl). the right half of the table
shows the information density and dependency length of the actual human language, as
well as the mean of the random pseudo-grammars generated for study 1.

our    rst observation from table 6 is that optimizing either information density or de-
pendency length indeed comes at the expense of the other property (despite the overall
positive correlations between information density and dependency length in the random

21

figure 7: comparison of human languages to grammars that optimize information den-
sity and dependency length. gray-shaded lines show grammars that are optimal under a
given trade-off between information density and dependency length (bottom-most point:
grammar with optimal information density; left-most point: grammar with optimal de-
pendency length; in between: grammars that optimized weighted sums of information
density and dependency length). to facilitate cross-language comparison, information
density is normalized by character and both information density and dependency lengths
are standardized, so that axes represent z-values. contour lines and    ll show the distri-
bution of the random reorderings of all languages combined.

22

llllllllarabicczechenglishgermanmandarin   8   40   8   40   10   50   10   50dependency length (scaled within language)information density (by   character, scaled within language)languagellarabicczechenglishgermanmandarinoptimize for

id id & dl

dl

actual mean of random
language pseudo-grammars

arabic
information density 0.135
dependency length
4.42
czech
information density 0.413
dependency length
3.41
english
information density 0.372
dependency length
3.45
german
information density 0.358
dependency length
3.76
mandarin
information density 0.156
dependency length
3.06

0.141
2.73

0.143
2.73

0.431
2.45

0.432
2.45

0.386
2.01

0.391
2.00

0.380
2.18

0.383
2.18

0.163
2.55

0.162
2.55

0.143
3.21

0.435
2.94

0.377
2.25

0.380
3.28

0.159
3.44

0.146
4.75

0.436
3.27

0.394
2.79

0.386
3.91

0.164
4.56

table 6: results of separate and joint optimization of information density (id) and depen-
dency length (dl). for comparison, the two rightmost columns provide the information
density and dependency length of human languages as well as the mean of the random
pseudo-grammars (repeated from table 3).

pseudo-grammars, cf. figure 5). further, the average information density and depen-
dency length of all    ve natural languages is overall closer to the joint optimum, than to
either of the separate optima, suggesting that natural languages indeed trade-off infor-
mation density and dependency length.

this means that the jointly optimized pseudo-grammars provide the most relevant
point of comparison for natural languages, since we are interested in understanding
whether the grammars of natural languages are close to optimal in their overall processing
ef   ciency. one way to further illustrate this trade-off is to look at the equi-weighted joint
optimization (   = .5). these jointly optimized grammars do not unambiguously outper-
form actual natural languages. for two of the    ve languages, arabic, and czech, the equi-
weighted jointly optimized pseudo-grammar has lower information density and depen-
dency length. for the other three languages, however, the optimized pseudo-grammar
is better on one dimension of processing ef   ciency, but worse on the other (e.g., for the
jointly optimized pseudo-grammar of mandarin, a slight improvement in dependency
length results in a considerably worsening in information density, compared to actual
mandarin). this is visualized in figure 8.

the full results of the different joint optimizations, varying the weighting parameter
  , are shown in figure 7. if language processing is just one among many equally im-

23

figure 8: comparison of human languages and optimized languages derived from them,
in terms of by-character information density and dependency length. to put all languages
on a comparable scale, the two axes are standardized (i.e., they represent z-values). solid
shapes show the human languages. arrows point to optimal pseudo-grammars. contour
lines show a 2d density estimation based on a bivariate normal kernel. this summarizes
the distribution of the random pseudo-grammar.

portant factors that shape word order preferences over time, the processing ef   ciency of
optimized grammars should far outperform that of the actual human languages. the gray
lines in the figure 7 can be thought of as representing a    frontier    of optimality within the
space of possible grammars for each of the languages in our sample.

none of the languages in our sample lies on this frontier. that is, all languages could
theoretically change to have better information density and dependency length. how-
ever, we also    nd that the word orders of some languages (arabic and english) have
close to optimal processing ef   ciency. of the    ve languages investigated here, only the
word order of german clearly has non-optimal processing ef   ciency. one possible rea-
son for such striking differences between languages might be differences in how they use
other means than word order to convey the relations between words in a sentence (e.g.,
word-internal structure and morphosyntactic means). it is also possible that the histori-
cal development of some languages has been more strongly affected by other factors of
language use (such ease of production or learnability). the current computational simu-
lations cannot distinguish between these possibilities. the approach applied here does,
however, point a way forward: as better quantitative models of these other factors become
available, future simulations can investigate to what extent other aspects of language use
shape the cultural evolution of language.

we further note that our optimization procedure held constant the headedness of each
dependency type. as mentioned above, this is close to, but not identical to, what the

24

l   5.0   2.50.02.5   10   50dependency length (scaled within language)information density (by   word, scaled within language)languagelarabicczechenglishgermanmandarinhuman languages in our sample do. it is an open question how a relaxation of the the
constant-headedness constraint would affect our results. varying headedness arguably
makes a language harder to learn, so that the assumption of constant headedness can
be seen as holding constant what likely constitutes an additional (third) constraint that
languages need to balance.

6 discussion

the present results suggest that language processing affects language change: all natural
languages for which we could test the hypothesis have word orders that make them eas-
ier to process than expected by chance. speci   cally, the average information density and
dependency length of the natural languages in our sample is lower than would be ex-
pected if language change was not subject to a bias toward systems with high processing
ef   ciency.

to the best of our knowledge, this is the    rst cross-linguistic broad-coverage study
of the processing ef   ciency of natural languages. the measures of processing ef   ciency
we have employed here are two of the best documented correlates of processing com-
plexity. by calculating the average information density and dependency length of natural
languages based on large collections of text from these languages, we were able to side
step the insurmountable challenges that would be associated with a behavioral approach
to this question (see introduction). there are three directly related previous    ndings that
we are aware of [23, 31, 26]. gildea and temperley [31] investigated the average depen-
dency length of two closely related languages, english and german. ferrer i cancho [23]
investigates czech and romanian, while futrell et al. [26] study 37 languages spoken
worldwide. these studies found that the languages studied had shorter average depen-
dency length than expected by chance. our contribution is to    for the    rst time, to our
knowledge    assess the joint effect of two of the biggest contributors to the grammatical
processing ef   ciency of a language, information density and dependency length. as the
trade-off between these factors in study 2 shows, it is crucial to investigate the effect of
multiple contributors to processing ef   ciency simultaneously.

some other recent studies complement the approach taken here. these studies have
tested whether learners of miniature languages designed by experimenters prefer lan-
guages that increase processing ef   ciency [20, 21, 22]. in the most informative of these
studies, great care is taken to rule out native language biases as the source of the observed
preferences (cf. [34]). for example, [19]    nds that language learners prefer languages that
reduce unnecessary uncertainty about the syntactic structure of sentences. studies like
this provide evidence that processing preferences can bias the outcome of language learn-
ing and thus provide support for one causal pathway through which processing prefer-
ences could come to affect language change, thereby shaping languages over time.

there are several caveats that apply to our study. the most obvious perhaps is that we
have only considered syntactic dependencies that are annotated in the available syntactic
corpora. these dependencies constitute an impoverished subset of all the semantic and

25

syntactic dependencies that human comprehenders process when listening or reading.
for example, one obvious omission in our approach is that we did not consider the inter-
nal structure of words, the complexity of which varies starkly across languages. another
simplifying assumption we have implicitly made in our studies is the focus on informa-
tion density and dependency length. while these two measure of processing ef   ciency
are arguably the best documented ones, there are other properties of grammatical sys-
tems that are known to affect processing ef   ciency (e.g., interference in memory due to
similar words or referents, [57, 61]). as far as we can tell, neither of these simplifying
assumption is likely to have biased the results in favor of the hypothesis tested here (for
that to be the case, the measures of processing ef   ciency we have employed here would
have to be systematically inversely correlated with other measures or properties of the
languages under study).

acknowledgments the authors thank masha fedzechkina, chigusa kurumada, and
olga nikolayeva for feedback on earlier versions of this paper. this work was partially
funded by national science foundation award iis-1446996 to dg and national science
foundation career award iis-1150028 to tfj. the views expressed here at not neces-
sarily those of the funding agencies.

references

[1] jennifer e arnold, thomas wasow, ash asudeh, and peter alrenga. avoiding at-
tachment ambiguities : the role of constituent ordering. journal of memory and lan-
guage, 51:55   70, 2004.

[2] jennifer e arnold, thomas wasow, anthony losongco, and ryan ginstrom. heav-
iness vs. newness: the effects of structural complexity and discourse status on con-
stituent ordering. language, 76(1):28   55, 2000.

[3] r h baayen. morphological in   uences on the recognition of monosyllabic monomor-

phemic words. journal of memory and language, 55:290   313, 2006.

[4] david a balota, michael j cortese, susan d sergent-marshall, daniel h spieler, and
melvin j yap. visual word recognition of single-syllable words. journal of experi-
mental psychology: general, 133(2):283   316, 2004.

[5] elizabeth bates and brian macwhinney. functionalist approaches to grammar. in

id146: the state of the art, pages 173   218. 1982.

[6] elizabeth bates and brian macwhinney. competition, variation, and language
learning. in brian macwhinney, editor, mechanisms of id146, chap-
ter 6, pages 157   194. 1987.

26

[7] a. b  ohmov  a, j. haji  c, e. haji  cov  a, and b. hladk  a. the pdt: a 3-level annotation sce-
nario. in a. abeill  e, editor, treebanks: building and using parsed corpora, volume 20
of text, speech and language technology, chapter 7. kluwer academic publishers, dor-
drecht, 2003.

[8] marisa ferrara boston, john hale, reinhold kliegl, umesh patil, and shravan va-
sishth. parsing costs as predictors of reading dif   culty: an evaluation using the
potsdam sentence corpus. journal of eye movement research, 2(1):1   12, 2008.

[9] s. brants, s. dipper, s. hansen, w. lezius, and g. smith. the tiger treebank. in

proc. of the 1st workshop on treebanks and linguistic theories (tlt), 2002.

[10] sabine buchholz and erwin marsi. conll-x shared task on multilingual depen-
dency parsing. in proceedings of the tenth conference on computational natural lan-
guage learning, pages 149   164. association for computational linguistics, 2006.

[11] joan bybee and paul j hopper. frequency and the emergence of linguistic structure.

2001.

[12] joan bybee and joanne scheibman. the effect of usage on degrees of constituency:

the reduction of don  t in english. linguistics, 37(4):575   596, 1999.

[13] stanley f chen and joshua goodman. an empirical study of smoothing techniques

for id38. computer speech & language, 13(4):359   393, 1999.

[14] hye-won choi. length and order: a corpus study of korean dative-accusative

construction.

, 14(3):207   227, 2007.

[15] uriel cohen priva. using information content to predict phone deletion. in natasha
abner and jason bishop, editors, proceedings of the 27th west coast conference on for-
mal linguistics, pages 90   98, 2008.

[16] michael john collins. head-driven statistical models for natural language parsing. phd

thesis, university of pennsylvania, philadelphia, 1999.

[17] ferm    n moscoso del prado mart    n, aleksandar kosti  c, and r harald baayen. putting
the bits together: an information theoretical perspective on morphological process-
ing. cognition, 94(1):1   18, november 2004.

[18] vera demberg and frank keller. data from eye-tracking corpora as evidence for
theories of syntactic processing complexity. cognition, 109(2):193   210, november
2008.

[19] maryia fedzechkina. communicative ef   ciency, language learning, and language uni-

versals. phd thesis, university of rochester, 2014.

27

[20] maryia fedzechkina, t florian jaeger, and elissa l newport. functional biases
in language learning: evidence from word order and case-marking interaction.
in 33rd annual meeting of the cognitive science society, number 2004, pages 318   323,
2011.

[21] maryia fedzechkina, t. florian jaeger, and elissa l. newport. language learners
restructure their input to facilitate ef   cient communication. proceedings of the national
academy of sciences of the united states of america, pages 1   6, october 2012.

[22] maryia fedzechkina, t florian jaeger, and elissa l newport. communicative biases
shape structures of newly acquired languages. in m. knauff, n. pauen, n.sebanz,
and i. wachsmuth, editors, proceedings of the 35th annual meeting of the cognitive sci-
ence society (cogsci13), pages 430   435. cognitive science society, austin, tx, 2013.

[23] ramon ferrer i cancho. euclidean distance between syntactically linked words.

physical review e, 70(056135):1   5, 2004.

[24] victoria fossum and roger levy. sequential vs. hierarchical syntactic models of hu-
man incremental sentence processing. in proceedings of the 3rd workshop on cognitive
modeling and computational linguistics, pages 61   69. association for computational
linguistics, 2012.

[25] stefan l frank and rens bod. insensitivity of the human sentence-processing system

to hierarchical structure. psychological science, 22(6):829   834, 2011.

[26] richard futrell, kyle mahowald, and edward gibson. large-scale evidence of de-
pendency length minimization in 37 languages. proceedings of the national academy
of sciences, 112(33):10336   10341, 2015.

[27] dmitriy genzel and eugene charniak. id178 rate constancy in text. in proceed-
ings of the 40th annual meeting of the association for computational linguistics (acl),
pages 199   206, 2002.

[28] e gibson. linguistic complexity:

68(1):1   76, august 1998.

locality of syntactic dependencies. cognition,

[29] edward gibson. the dependency locality theory: a distance-based theory of
linguistic complexity. in alec marantz, yasushi miyashita, and wayne o   neil, edi-
tors, image, language, brain: papers from the    rst mind articulation symposium, chapter 5,
pages 95   126. 2000.

[30] edward gibson, steven t piantadosi, kimberly brink, leon bergen, eunice lim,
and rebecca saxe. a noisy-channel account of crosslinguistic word-order variation.
psychological science, 24(7):1079   88, july 2013.

[31] daniel gildea and david temperley. do grammars minimize dependency length?

cognitive science, 34(2):286   310, 2010.

28

[32] j. godfrey, e. holliman, and j. mcdaniel. switchboard: telephone speech cor-
pus for research and development. in ieee icassp-92, pages 517   520, san francisco,
1992. ieee.

[33] ben gold, nelson morgan, and dan ellis. speech and audio signal processing: processing

and perception of speech and music. john wiley & sons, 2011.

[34] adele e goldberg. substantive learning bias or an effect of familiarity? comment on.

cognition, 127(3):420   426, 2013.

[35] peter graff and t florian jaeger. locality and feature speci   city in ocp effects:
evidence from aymara, dutch, and javanese. in proceedings of the main session of the
45th meeting of the chicago linguistic society, pages 1   15, 2009.

[36] daniel grodner and edward gibson. consequences of the serial nature of linguistic

input for sentenial complexity. cognitive science, 29(2):261   90, march 2005.

[37] gegory r guy. form and function in linguistic variation. in gegory r guy, craw-
ford feagin, deborah schiffrin, and john baugh, editors, towards a social science of
language: papers in honor of william labov. volume 1: variation and change in language
and society, pages 221   252. benjamins publishing compagny, amsterdam, 1996.

[38] jan haji  c, otakar smr  z, petr zem  anek, jan   snaidauf, and emanuel be  ska. prague
arabic dependency treebank: development in data and tools. in proc. of the nemlar
intern. conf. on arabic language resources and tools, pages 110   117, 2004.

[39] john hale. a probabilistic earley parser as a psycholinguistic model.

in naacl
   01 proceedings of the second meeting of the north american chapter of the association for
computational linguistics on language technologies, pages 1   8, 2001.

[40] marc d hauser, noam chomsky, and w tecumseh fitch. the faculty of language:

what is it, who has it, and how did it evolve? science, 298(5598):1569   1579, 2002.

[41] j. a. hawkins. cross-linguistic variation and ef   ciency. oxford university press, ox-

ford, uk, 2014.

[42] john hawkins. a performance theory of order and constituency. cambridge university

press, cambridge, uk, 1994.

[43] john a hawkins. ef   ciency and complexity in grammars. oxford univ press, oxford,

2004.

[44] john a. hawkins. processing typology and why psychologists need to know about

it. new ideas in psychology, 25(2):87   107, august 2007.

[45] w. von humboldt. linguistic variability and intellectual development. university of

pennsylvania press, philadelphia, paadelphia, 1972.

29

[46] elizabeth hume and fr  ed  eric mailhot. the role of id178 and surprisal in phonol-
in alan c. l. yu, editor, origins of sound pat-
ogization and language change.
terns: approaches to phonologization, pages 29   50. oxford university press, oxford,
uk, 2013.

[47] t f jaeger and e j norcliffe. the cross-linguistic study of sentence production. lan-

guage and linguistics compass, 3:1   22, 2009.

[48] t florian jaeger. redundancy and reduction: speakers manage syntactic information

density. cognitive psychology, 61(1):23   62, august 2010.

[49] frederick jelinek. statistical methods for id103. mit press, cambridge,

ma, 1997.

[50] simon kirby, hannah cornish, and kenny smith. cumulative cultural evolution
in the laboratory: an experimental approach to the origins of structure in human
language. proceedings of the national academy of sciences of the united states of america,
105(31):10681   6, august 2008.

[51] simon kirby, mike dowman, and thomas l grif   ths. innateness and culture in the
evolution of language. proceedings of the national academy of sciences, 104(12):5241   
5245, 2007.

[52] reinhard kneser and hermann ney.

improved backing-off for m-gram language
in international conference on acoustics, speech, and signal processing

modeling.
(icassp), volume 1, pages 181   184, detroit, mi, 1995. ieee.

[53] philipp koehn. id151. cambridge university press, 2009.

[54] k j kohler. the id102/phonology issue in the study of articulatory reduction.

phonetica, 48(2-4):180   192, 1991.

[55] roger levy. probabilistic models of word order and syntactic discontinuity. phd thesis,

stanford university, 2005.

[56] roger levy. expectation-based syntactic comprehension. cognition, 106(3):1126   77,

march 2008.

[57] richard l lewis, shravan vasishth, and julie a van dyke. computational princi-
ples of working memory in sentence comprehension. trends in cognitive sciences,
10(10):447   54, october 2006.

[58] bj  orn lindblom. explaining phonetic variation: a sketch of the h&h theory.

in
w j hardcastle and a marchal, editors, speech production and speech modeling, pages
403   439. kluwer academic publishers, 1990.

30

[59] barbara lohse, john a hawkins, and thomas wasow. domain minimization in

english verb-particle constructions. language, 80(2):238   261, 2004.

[60] paul a luce and david b pisoni. recognizing spoken words: the neighborhood

activation model. ear and hearing, 19(1):1   36, 1998.

[61] maryellen c macdonald. how language production shapes language form and com-

prehension. frontiers in psychology, 4(april):226, january 2013.

[62] david magerman. natural language parsing as statistical pattern recognition. phd

thesis, stanford university, 1994.

[63] james s magnuson, james a dixon, michael k tanenhaus, and richard n aslin. the
dynamics of lexical competition during spoken word recognition. cognitive science,
31(1):133   56, february 2007.

[64] d yu manin. experiments on predictability of word in context and information rate

in natural language. pages 1   12, 2006.

[65] mitchell p. marcus, beatrice santorini, and mary ann marcinkiewicz. building a
large annotated corpus of english: the id32. computational linguistics,
19(2):313   330, june 1993.

[66] scott a. mcdonald and richard c. shillcock. low-level predictive id136 in read-
ing: the in   uence of transitional probabilities on eye movements. vision research,
43(16):1735   1751, july 2003.

[67] martin a nowak, joshua b plotkin, and vincent aa jansen. the evolution of syn-

tactic communication. nature, 404(6777):495   498, 2000.

[68] john j ohala. discussion of bjoern lindblom   s    phonetic invariance and the adaptive
nature of speech   . in working models of human perception. academic press, london,
uk, 1988.

[69] steven t piantadosi, harry tily, and edward gibson. word lengths are optimized

for ef   cient communication. pnas, 108(9):3526    3529, 2011.

[70] steven t piantadosi, harry tily, and edward gibson. the communicative function

of ambiguity in language. cognition, 122(3):280   91, march 2012.

[71] janet b pierrehumbert. word-speci   c id102.

in carlos gussenhoven and
natasha warner, editors, laboratory phonology 7, pages 101   139. mouton de gruyter,
berlin, 2002.

[72] steven pinker and ray jackendoff. the faculty of language: what   s special about it?

cognition, 95(2):201   36, march 2005.

31

[73] idoia ros, mike santesteban, kumiko fukumura, and itziar laka. aiming at shorter
dependencies: the role of agreement morphology. language, cognition, and neuro-
science, 2015.

[74] dan i slobin. language change in childhood and in history. working papers of the

language behavior research laboratory, 41:185   214, 1975.

[75] nathaniel j smith and roger levy. the effect of word predictability on reading time

is logarithmic. cognition, 128(3):302   19, september 2013.

[76] benedikt m szmrecs  anyi. on operationalizing syntactic complexity. in jadt 2004
: 7es journ  ees internationales d   analyse statistique des donn  ees textuelles, pages 1031   
1038, 2004.

[77] marten van schijndel and william schuler. hierarchic syntax improves reading time
prediction. in proceedings of the 2013 meeting of the north american chapter of the asso-
ciation for computational linguistics (naacl-15), 2015.

[78] r j j h van son and louis c w pols. how ef   cient is speech? proceedings institute of

phonetic sciences, university of amsterdam, 25:171   184, 2003.

[79] shravan vasishth and r l lewis. an activation-based model of sentence processing

as skilled memory retrieval. cognitive science, 29(3):375   419, 2005.

[80] thomas wasow. post-verbal behavior. csli publications, stanford, ca, 2002.

[81] andrew wedel. exemplar models, evolution and language change. the linguistic

review, 23:247   274, 2006.

[82] andrew wedel, scott jackson, and abby kaplan. functional load and the lexicon:
evidence that syntactic category and frequency relationships in minimal lemma
pairs predict the loss of phoneme contrasts in language change. language and
speech, 56(3):395   417, july 2013.

[83] nianwen xue, fei xia, fu-dong chiou, and martha palmer. the penn chinese tree-
bank: phrase structure annotation of a large corpus. natural language engineering,
11:207   238, 2005.

[84] hiroko yamashita and franklin chang.    long before short    preference in the pro-

duction of a head-   nal language. cognition, 81:45   55, 2001.

[85] george k. zipf. human behavior and the principle of least effort. addison-wesley, new

york, 1949.

32

