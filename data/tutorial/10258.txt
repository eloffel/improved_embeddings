5
1
0
2

 

p
e
s
8
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
8
0
8
5
0

.

9
0
5
1
:
v
i
x
r
a

word,graphandmanifoldembeddingfrommarkovprocessestatsunorib.hashimotomitcsailcambridge,ma02139thashim@csail.mit.edudavidalvarez-melismitcsailcambridge,ma02139davidam@csail.mit.edutommis.jaakkolamitcsailcambridge,ma02139tommi@mit.eduseptember18,2015abstractcontinuousvectorrepresentationsofwordsandobjectsappeartocarrysurprisinglyrichsemanticcontent.inthispaper,weadvanceboththecon-ceptualandtheoreticalunderstandingofwordembeddingsinthreeways.first,wegroundembeddingsinsemanticspacesstudiedincognitive-psychometricliteratureandintroducenewevaluationtasks.second,incontrasttopriorwork,wetakemetricrecoveryasthekeyobjectofstudy,unifyexistingalgorithmsasconsistentmetricrecoverymethodsbasedonco-occurrencecountsfromsimplemarkovrandomwalks,andproposeanewrecoveryalgorithm.third,wegeneralizemetricrecoverytographsandmanifolds,relatingco-occurencecountsonrandomwalksingraphsandrandomprocessesonmanifoldstotheunderlyingmetrictoberecov-ered,therebyreconcilingmanifoldestimationandembeddingalgorithms.wecompareembeddingalgorithmsacrossarangeoftasks,fromnonlin-eardimensionalityreductiontothreesemanticlanguagetasks,includinganalogies,sequencecompletion,andclassi   cation.1introductioncontinuousvectorrepresentationsofwords,objects,andsignalshavebeenwidelyadoptedacrossareas,fromnaturallanguageprocessingandcomputervisiontospeechrecognition.methodsforestimatingtheserepresentationssuchasneuralwordembeddings[3,14,12]aretypicallysimpleandscalableenoughtoberunonlargecorpora,yetresultinwordvectorsthatappeartocapturesyn-tacticallyandsemanticallymeaningfulproperties.indeed,analogytaskshave1analogyabcid1d2d3d4seriescompletionabid1d2d3d4classi   cationabcid1d2d3d41figure1:sternberg   smodelforinductivereasoningwithembeddings(a,b,caregiven,iistheidealpointanddarethechoices.thecorrectanswerisshadedgreen).becomedefactobenchmarkstoassessthesemanticrichnessofwordembeddings[8,12].however,theoreticalunderstandingofwhytheyworkhaslaggedbehindotherwiseintriguingempiricalsuccesses.severalrecentcontributionshaveaimedatbringingabetterunderstandingofwordembeddings,theirproperties,andassociatedalgorithms[8,5,9,1].forexample,[9]showedthattheglobalminimumoftheskip-grammethodwithnegativesampling[12]implicitlyfactorizesashiftedversionofthepoint-wisemutualinformation(pmi)matrixofword-contextpairs.aroraetal.[1]exploredlinksbetweenrandomwalksandwordembeddings,relatingthemtocontextual(id203ratio)analogies,underspeci   c(isotropic)assumptionsaboutwordvectors.inthispaper,weextendtheconceptualandtheoreticalunderstandingofwordembeddingsinthreeways.first,wegroundwordembeddingstosemanticspacesstudiedincognitive-psychometricliteratureandconsiderthreeinductivereasoningtasksforevaluatingthesemanticcontentinwordvectors,includinganalogies(previouslystudied)butalsotwonewtasks,sequencecompletionandclassi   cation.wedemonstrateexistingandproposedalgorithmsperformwellacrossthesetasks.second,incontrastto[1],wetakemetricrecoveryasthekeyobjectofstudyandunifyexistingalgorithmsasconsistentmetricrecoverymethodsbasedonlog(co-occurrence)countsarisingfromsimplemarkovran-domwalks.motivatedbymetricrecovery,wealsointroduceanddemonstrateadirectregressionmethodforestimatingwordembeddings.third,wegen-eralizemetricrecoverytographsandmanifolds,directlyrelatingco-occurencecountsforrandomwalksongraphsandrandomprocessesonmanifoldstotheunderlyingmetrictoberecovered.2wordvectorsandsemanticspacessemanticspaces,i.e.,vectorspaceswheresemanticallyrelatedwordsareclosetoeachother,havelongbeenanobjectofstudyinthepsychometricsandcognitivesciencecommunities[19,21].rumelhartandabrahamsonproposed2thatvectorwordrepresentationsderivedfromsemanticsimilarityalongwithvectoradditioncouldpredictresponsechoicesinanalogyquestions[19].thishypothesiswasveri   edinthreeways:bysolvinganalogiesusingembeddingsderivedfromsurveydata;observationsthathumanmistakeratesfollowedanexponentialdecayinembeddeddistancefromthetruesolution;andabilityofstudysubjectstoansweranalogiesconsistentwithanembeddingfornonexistentanimalsaftertraining[19].sternbergfurtherproposedthatgeneralinductivereasoningwasbaseduponmetricembeddingsandtestedtwoadditionallanguagetasks,seriescompletionandclassi   cation(seefigure1)[21].inthecompletiontask,thegoalistopredictwhichwordshouldcomenextinaseriesofwords(e.g.givenpenny,nickel,dime,theanswershouldbequarter).intheclassi   cationtask,thegoalistochoosethewordthatbest   tsasetofgivenwords.forexample,givenzebra,gira   eandgoat,andcandidatechoicesdog,mouse,catanddeer,theanswerwouldbedeersinceit   tsthe   rstthreetermsbest.sternbergproposedthat,givenwordembeddings,asubjectsolvestheseriescompletionproblemby   ndingthenextpointinthelinede   nedbythegivenwords,andsolvestheclassi   cationtaskby   ndingthecandidatewordclosesttothecentroidofthegivenwords.areproductionofsternberg   soriginalgraphicaldepictionofthethreeinductiontasksisgiveninfigure1.aswithanalogies,we   ndthatwordembeddingmethodsperformsurprisinglywellattheseadditionaltasks.forexample,intheseriescompletiontask,given   body,arm,hand   we   ndthecompletiontobe      ngers   .manyoftheembeddingalgorithmsaremotivatedbythedistributionalas-sumption[6]:wordsappearinginsimilarcontextsacrossalargecorpusshouldhavesimilarvectorrepresentations.goingbeyondthishypothesis,wefollowthepsychometricliteraturemorecloselyandtakemetricrecoveryasthekeyobjectofstudy,unifyingandextendingembeddingalgorithmsfromthisperspective.3recoveringsemanticdistanceswithwordem-beddingwebeginwithasimplemodelproposedintheliterature[2]wherewordco-occurencesoveradjacentwordsrepresentsemanticsimilarityandgeneralizethemodelinlatersections.ourcorpusconsistsofmtotalwordsacrossssentencesoveranwordvocabularywhereeachwordisgivenacoordinateinalatentwordvectorspace{x1,...,xn}   rd.foreachsentencesweconsideramarkovrandomwalk,x1,...,xms,withthefollowingtransitionfunctionp(xt=xj|xt   1=xi)=exp(   ||xi   xj||22/  2)pnk=1exp(   ||xi   xk||22/  2).(1)33.1logco-ocurrenceasametricsupposeweobservethegaussianrandomwalk(eq.1)overacorpuswithmtotalwordsandde   necijasthenumberoftimesforwhichxt=xjandxt   1=xi.1bythemarkovchainlawoflargenumbers,asm      ,   log cij/nxk=1cik!p      ||xi   xj||22/  2+log(zi)wherezi=pnk=1exp(   k|xi   xk||22/  2)(seesupplementarylemmas1.1).moregenerally,considerthefollowinglimitthatrelateslogco-occurencetowordembeddingslemma1.letcijbeaco-occurencematrixoveracorpusofsizemandxbecoordinatesofwordsinthelatentsemanticspace,thenthereexistsasequenceamiandbmjsuchthatasm      ,   log(cij)   amip      ||xi   xj||22+bmj.thegaussianrandomwalkaboveisaspecialcaseofthislimit;wewillshowthatrandomwalksongraphsandsometopicmodelsful   llthismetricrecoverylimit.3.2consistencyofwordembeddingsappyingthistothreewordembeddingalgorithms,weshowtheconditionsoflemma1aresu   cienttoensurethatthetrueembeddingxisaglobalminimum.glove:theglobalvectors(glove)[17]methodforwordembeddingoptimizestheobjectivefunctionminbx,bc,a,bxi,jf(cij)(2hbxi,bcji+ai+bj   log(cij))2withf(cij)=min(cij,10)3/4.ifwerewritethebiastermsasai=bai   ||bxi||22andbj=bbj   ||bcj||22,weobtaintheequivalentrepresentation:minbx,bc,ba,bbxi,jf(cij)(   log(cij)   ||bxi   bcj||22+bai+bbj))2.whencombinedwithlemma1,werecognizethisasaweightedmultidimen-sionalscalingobjectivewithweightsf(cij).splittingthewordvectorbxiandcontextvectorbciishelpfulinpracticetooptimizethisobjective,butnotneces-saryundertheassumptionsoflemma1sincethetrueembeddingbxi=bci=xi/  andbai,bbi=0isaglobalminimumwheneverdim(bx)=d.(seethms1.4fordetail)1inpractice,wordembeddingmethodsuseasymmetrizedwindowratherthancountingtransitions.thisdoesnotchangeanyoftheasymptoticanalysisinthepaper(supplementarysections2)4id97:theembeddingalgorithmid97approximatesasoftmaxob-jective:minbx,bcxi,jcijlog(cid:18)exp(hbxi,bcji)pnk=1exp(hbxi,bcki)(cid:19).ifdim(bx)=d+1wecansetoneofthedimensionsofbx=1asabiastermallowingustorewritetheobjectivewithaslackparameterbjanalogouslytoglove.afterreparametrizationweobtainthatforbb=bj   ||bcj||22,minbx,bc,bbxi,jcijlog exp(   ||bxi   bcj||22+bbj)pnk=1exp(   ||bxi   bck||22+bbk)!.sincecij/pnk=1cik   exp(   k|xi   xj||22/  2)pnk=1exp(   k|xi   xk||22/  2)thisisthestochasticneigh-borhoodembeddingobjectiveweightedbypnk=1cik.onceagain,thetrueembeddingbx=bc=x/  isaglobalminimum(theorems1.5).thenega-tivesamplingapproximationusedinpracticebehavesmuchlikethesvdap-proach[9]andthusapplyingthesamestationarypointanalysisas[9],thetrueembeddingisaglobalminimumundertheadditionalassumptionthat||xi||22/  =log(pjcij/qpijcij).svd:thesvdapproach[9]takesthelogpointwisemutualinformationmatrix:mij=log(cid:16)cij(cid:17)   log(cid:16)xkcik(cid:17)   log(cid:16)xkckj(cid:17)+log(cid:16)xijcij(cid:17)andappliesthesvdtotheshiftedandtruncatedmatrix:(mij+  )+.thisshiftandtruncationisdoneforcomputationalreasonsandtopreventmijfromdiverging.inthelimitwherem      andnotruncationisperformedthereexistsashift  asafunctionofmsuchthatthealgorithmrecoverstheunderlyingembeddingassuming||xi||22/  =log(pjcij/qpijcij)(lemmas1.3).thisassumptioncanberelaxedviaasmallmodi   cationtothealgorithm:assumewithoutlossofgeneralitythatthelatentwordvectorsaremean-centered.thenwecreatethecenteredinnerproductmatrixusingthecenteringmatrixv=i   11t/ncmij=vmijvt/2.thisisexactlyclassicalmultidimensionalscalingandcmij   hxi,xji/  2sincethecenteringremoveso   setsai,bjandnorms||xi||22makingsvdofcmijrecoverxiandxj(theorems1.2).3.3metricregressionfromlogco-occurenceswehavedemonstratedthatbyreparametrizingandtakingonadditionalas-sumptions,existingwordembeddingalgorithmscouldbecastasmetricrecoveryunderlemma1.however,itisnotknownifmetricrecoverywouldbee   ective5inpractice;forthisweproposeanewmodelwhichdirectlymodelslemma1andactsaslitmustestforourmetricrecoveryparadigm.lemma1describesalog-linearrelationshipbetweendistanceandco-occurences.thecanonicalwayto   tsucharelationshipwouldbetouseageneralizedlinearmodel,wheretheco-occurencescijfollowanegativebinomialdistribution:cij   negbin(cid:0)  ,  (  +exp(   ||xi   xj||22/2+ai+bj))   1(cid:1).underthisoverdispersedloglinearmodel,e[cij]=exp(   ||xi   xj||22/2+ai+bj),var(cij)=e[cij]2/  +e[cij].here,theparameter  controlsthecontributionoflargecijandactssimilarlytoglove   sf(cij)weightfunction,whichwecoverindetailbelow.fittingthismodelisstraightforward,aswecande   nethelog-likelihoodintermsoftheexpectedrate  ij=exp(   ||xi   xj||22/2+ai+bj)llh(x,a,b,  )=xi,j  log(  )     log(  ij+  )+cijlog(cid:18)1       ij+  (cid:19)+log(cid:18)  (cij+  )  (  )  (cij+1)(cid:19)andperformgradientdescentovertheparameters,givingasimpleupdatefor-mulaintermsoftheerroras  ij=(cij     ij)    ij+  dxi=xj(xj   xi)(  ij+  ji)dai=xj  ijdbj=xi  ij(2)optimizingthisobjectiveusingstocahsticgradientdescentwillrandomlyselectwordpairsi,jandattractorrepulsethevectorsbxandbcinordertoachievetherelationshipinlemma1.ourimplementationusestheglovecodebase(sections5.1fordetails).relationshiptoglove:theoverdispersionparameter  shedslightontheroleofglove   sweightfunctionf(cij).takingthetaylorexpansionofthelog-likelihoodatlog(  ij)      log(cij)we   ndthatforaconstantkij,llh(x,a,b,  )=xijkij   cij  2(cij+  )(log(  ij)   log(cij))2+o((log(  ij)   log(cij))3).notethesimilarityofthesecondordertermwiththegloveobjective.bothweightfunctionscij  2(cij+  )andf(cij)=max(c3/4ij,x3/4max)smoothlyasymptote,downweightinglargeco-occurences.however,theempiricalperformancesug-geststhatinpractice,optimizingthedistancesdirectlyandusingthenegativebinomiallossconsistentlyimprovesperformance.4metricrecoveryfrommarkovprocessesongraphsandmanifoldsmetricrecoveryfromrandomwalksispossiid7ndersubstantiallymoregeneralconditionsthanthesimplemarkovprocessineq1.wetakeanextremeview6hereandshowthatevenarandomwalkoveranunweighteddirectedgraphholdsenoughinformationformetricrecoveryprovidedthatthegraphitselfissuitablyconstructedinrelationtotheunderlyingmetric.2tothisend,weusealimitingargument(largevocabularylimit)withincreasingnumbersofpointsxn={x1,...,xn},wherexiaresampledi.i.d.fromadensityp(x)overacompactriemannianmanifold.forourpurposes,p(x)shouldhaveaboundedlog-gradientandastrictlowerboundp0overthemanifold.sincethepointsareassumedtolieonthemanifold,weusethesquaredgeodesicdistance  (xi,xj)2inplaceof||xi   xj||22usedearlier.therandomwalksweconsiderareoverunweightedspatialgraphsde   nedasde   nition2(spatialgraph).let  n:xn   r>0bealocalscalefunctionandh:r   0   [0,1]apiecewisecontinuousfunctionwithsub-gaussiantails.aspatialgraphgncorrespondingto  nandhisarandomgraphwithvertexsetxnandadirectededgefromxitoxjwithid203pij=h(  (xi,xj)2/  n(xi)2).simpleexamplesofspatialgraphswheretheconnectivityisnotrandom(pij=0,1)includethe  ballgraph(  n(x)=  )andthek-nearestneighborgraph(  n(x)=distancetok-thneighbor)asintheid92graph,  nismaydependonthesetofpointsxn.ourgoalistoshowthat,asn      ,wecanrecover  (xi,xj)fromco-occurrencecountsgeneratedfromsimplerandomwalksovergn.logco-occurencesandthegeodesicwillbeconnectedintwosteps.(1)weuseknownresultstoshowthatasimplerandomwalkoverthespatialgraph,properlyscaled,behavessimilarlytoadi   usionprocess;(2)thelog-transitionprobabil-ityofadi   usionprocesswillberelatedtothegeodesicmetriconamanifold.(1)thelimitingrandomwalkonagraph:justasthesimplerandomwalkovertheintegersconvergestoabrownianmotion,wemayexpectthatunderspeci   cconstraintsthesimplerandomwalkxntoverthegraphgnwillconvergetosomewell-de   nedcontinuousprocess.werequirethatthescalefunctionsconvergetoacontinuousfunction    (  n(x)a.s.         gn    (x));thesizeofasinglestepvanish(gn   0)butcontainatleastapolynomialnumberofpointswithin  n(x)(gnn1d+2log(n)   1d+2      ).underthislimit,ourassumptionsaboutthedensityp(x),andanadditionalregularitycondition,3theorem3(stroock-varadhanongraphs[7,22]).thesimplerandomwalkxntongnconvergesinskorokhodspaced([0,   ),d)afteratimescalingbt=tg2ntotheit  oprocessybtvaluedinc([0,   ),d)asxnbtg   2n   ybt.theprocessybtisde   nedoverthenormalcoordinatesofthemanifold(d,g)withre   ectingboundaryconditionsondasdybt=   log(p(ybt))  (ybt)2dbt+  (ybt)dwbt(3)2theweightedgraphcasefollowsidenticalarguments,replacingtheorem3with[22,the-orem3].3toensureconvergenceofdensities,requireinadditionthatfort=  (g   2n),therescaledmarginaldistributionnp(xt|x0)isa.s.uniformlyequicontinuous.forundirectedspatialgraphs,thisisknowntobetrue[4]forspatialgraphs,butfordirectedgraphsthisisanopenconjecturehighlightedin[7]7(2)logtransitionid203asametricwemaynowusethestochasticprocessybttoconnectthelogtransitionid203tothegeodesicdistanceusingvaradhan   slargedeviationformula.theorem4(varadhan[24,15]).letytbeait  oprocessde   nedoveracompleteriemannmanifold(d,g)withgeodesicdistance  (xi,xj)thenlimt   0   tlog(p(yt=xj|y0=xi))     (xi,xj)2.thisestimateholdsmoregenerallyforanyspaceadmittingadi   usivestochas-ticprocess[20].takentogether,we   nallyobtainvaradhan   sformulaovergraphs:corollary5(varadhan   sformulaongraphs).forany  ,  ,n0thereexistssomebt,n>n0,andsequencebnjsuchthatthefollowingholdsforthesimplerandomwalkxnt:p(cid:16)supxi,xj   xn0(cid:12)(cid:12)(cid:12)btlog(p(xnbtg   2n=xj|xn0=xi))   btbnj       (x)(xi,xj)2(cid:12)(cid:12)(cid:12)>  (cid:17)<  where    (x)isthegeodesicde   nedas    (x)(xi,xj)=minf   c1:f(0)=xi,f(1)=xjr10  (f(t))dtproof.sketch:fortheit  oprocess,varadhan   sformula(theorem4)impliesthatwecan   ndsometimebtsuchthatthelog-marginaldistributionofbyisclosetothegeodesic.toconvertthisstatementtoourgraphsetting,weusethecon-vergenceofstochasticprocesses(theorem3)withequicontinuityofmarginalstoensurethataftert=btg   2nsteps,thetransitionid203overthegraphconvergestothemarginaldistributionofby.finally,compactnessofthedomainimpliesthatlog-marginalsconvergeresultinginvaradhan   sformulaforgraphs(seecorollarys3.2fordetails).sincetheco-occurencecijhasthelimitlog(cij/pkcik)   p(xnt+1=xj|xn0=xi),thisresultsinananalogoflemma1inthemanifoldsetting.ourproofdemonstratesthatregardlessofthegraphweightsandmanifoldstructure,inthelarge-samplesmall-timelimit,logco-occurencesfaithfullycapturetheun-derlyingmetricstructureofthedata.whiletherehasbeenad-hocattemptstoapplywordembeddingstographrandomwalks[18],thistheoremdemonstratesthatembeddingthelogco-occurenceisaprincipledmethodforgraphmetricrecovery.generalizingthemarkovsentencemodel:thespatialmarkovrandomwalkde   nedabovehastwo   aws:   rst,cannotproperlyaccountforfunctionwordssuchasthesincewheneverthemarkovchaintransitionsfromatopictoafunctionword,itforgetstheoriginaltopic.second,sincetheunigramfrequencyofawordisthestationarydistribution,frequentwordsaregeomet-ricallyconstrainedtobeclosetoallotherwords.bothoftheseassumptionscanberelaxedbyassumingthatalatentspatialmarkovchain,whichwecallthetopicprocessyt,generatestheobservedsentenceprocessxt.thisideaof8googleanalogies(cos)googleanalogies(l2)satmethodsem.synt.totalsem.synt.totall2cosineregression78.470.873.775.570.972.639.237.8glove72.671.271.765.666.667.236.933.6svd57.450.853.453.748.250.327.125.8id9773.473.373.371.470.971.142.042.0table1:regressionandid97performwellongoogleandsatanalogies.alatenttopicmodelunderlyingwordembeddingshasbeenexplored[1];ourcontributionsarethreefold:wecontextualizethismodelaspartofametricem-beddingframework,provideaintuitiveproofthatdirectlyappliesvaradhan   sformula,andrelaxsomeconstraintsonthedistributionofybytakingthelargevocabularylimit(seesections4.1).thetopicprocessytisde   nedoverrdbylocaljumpsaccordingtoasmoothsubgaussiankernelhwithrx||x||22h(x)dx=  0,movementrate  2andalog-di   erentibletopicdistributionw(x)whichde   nesthestationarydistributionofthecurrenttopicoverthelatentsemanticspace.4p(yt+1|yt)=h(||yt+1   (yt+   log(w(yt))  2)||22/  2)(4)givenatopicyt,weassumetheid203ofobservingaparticularworddecaysexponentiallywiththesemanticdistancebetweenthecurrenttopicandwordscaledby  ,aswellasanon-metricfrequency  whichaccountsforthefrequencyoffunctionwordssuchastheandand.p(xt=xi|yt=y)     iexp(   ||xi   y||2/  2).underthisgeneralmodel,weobtainaheatkernelestimateanalogoustocor5,withconstraintsonthenewscaleparameter  (theorems4.1),p(xt=xj|x0=xi)     i  (xi)w(xi)exp(cid:18)   ||xj   xi||222(  2+t  2  20)(cid:19)(cid:0)1+o(  2  20t)+o(  2)(cid:1)+o(t   1/2).thisallowswordembeddingalgorithmstohandlelatentprocessesunderthesamesmallneighborhood(     0),largewindowt      limitassumingthat  0issmallrelativetothehessianofw(x)(seetheorems4.1fordetails).5empiricalvalidationweexperimentallyvalidatetwoaspectsofourwordembeddingtheory:thesemanticspacehypothesis,andthemanifoldvaradhan   sformula.ourgoalisnotto   ndtheabsolutebestmethodandevaluationmetricforwordembeddings,whichhasbeenstudiedatdetail[10].insteadwewilldemonstratethatwordembeddingsbasedonmetricrecoveryiscompetitivewithexistingstate-of-the-artinbothmanifoldlearningandsemanticinductiontasks.4weassumeeuclidean,ratherthanarbitrarymanifoldsincetheadditivityofvectorsim-pliedbyanalogicalreasoningtasksrequireeuclideanembeddings95.1semanticspacesinvectorwordrepresentationscorpusandtraining:wetrainedallmethodsonthreedi   erentcorpora:2.4btokensfromwikipedia,6.4btokensusedtotrainid97,and5.8btokenscombiningwikipediawithgigaword5emulatingglove   scorpus(sections5.2fordetails).weshowperformancefortheglovecorpusthroughoutbutincludeallcorporainsections7.wordembeddingsweregeneratedonthetop100kwordsforeachcorpususingfourmethods:id97,glove,randomizedsvd(referredtoassvd),andmetricregression(referredtoasregression).(seesections5.1).5forfairnesswe   xthehyperparameterformetricregressionat  =50,developingandtestingthecodeexclusivelyonthe   rst1gbsubsetofthewikidataset.vectorsusedinthispaperrepresentthe   rstrunofourmethodoneachfullcorpus.foropen-vocabularytasks,werestrictthesetofanswerstothetop30kwordswhichimprovesperformancewhilecoveringthemajorityofthequestions.solvinganalogiesusingsurveydataalone:wedemonstratethatem-beddingsemanticsimilarityderivedfromsurveydataissu   cientforsolvinganalogiesbyreplicatingastudybyrumelhartandabrahamson.inthisexperi-ment,shownintable2,wetakeafree-associationdataset[16]wherewordsareverticesonagraphandedgeweightswijrepresentthenumberoftimesthatwordjwasconsideredmostsimilartowordiinasurvey.wetakethisthelargestconnectedcomponentof4845wordsand61570weightsandembedthisweightedgraphusingstochasticneighborhoodembedding(sne)andisomapforwhichsquarededgedistancesarede   nedas   log(wij/maxkl(wkl)).solvingthegoogleanalogyquestions[11]coveredbythe4845wordsusingthesevec-torsshowsthatisomapcombinedwithsurveyscanoutperformthecorpusbasedmetricregressionvectorsonsemantic,butnotsyntactictasks;thisisduetothefactthatfree-associationsurveyscapturesemantic,butnotsyntacticsimilaritybetweenwords.theseresultssupportboththesemantic   eldhypothesis,andtheexponentialdecayofsemanticsimilaritywithembeddeddistance.analogies:theresultsonthegoogleanalogiesshownintable1demonstratethatourproposedframeworkofmetricregressionandnaivevectoraddition(l2)iscompetitivewiththebaselineofid97withcosinedistance.theperformancegapacrossmethodsissmalland   uctuatesacrosscorpora,butmetricregressionconsistentlyoutperformsgloveonmosttasksandoutperformsallmethodsonsemanticanalogies,whileid97doesbetteronsyntacticcategories.wealsoevaluatethemethodsonmoredi   cultsattypequestions[23]whereaprototypepaira:bisgivenandwemustchooseamongstasetofcan-didatepairs[c1:d1]...[c5:d5].inthisevaluation,cosinesimilaritybetweenvectordi   erencesisnolongertheoptimalchoiceandl2metricperformsslightlybetter.intermsofmethods,we   ndthatid97isbest,followedbymet-ricregression.theresultsonthesetwoanalogydatasetsshowthatdirectly5weusedrandomized,ratherthanfullsvdduetothedi   cultyofscalingsvdtothisproblemsize.forperfomanceoffullsvdfactorizationssee[10].10embeddingthelog-coocurrencemetricandtakingl2distancesbetweenvectorsiscompetitivewithcurrentapproachestoanalogicalreasoning.theconsis-tentimprovementofmetricembeddingoverglovedespitetheirsimilaritiesinimplementation(sections5.1),parameters,andstationarypoint(section3.3)suggestthatthemetricembeddingapproachtowordembeddingcanleadtoalgorithmicimprovements.sequenceandclassi   cationtasks:weproposetwonewdi   cultinductivereasoningtasksbaseduponthesemantic   eldhypothesis[21].thesequenceandclassi   cationdatasets,asdescribedinsection2aretasksthatrequireonetopickeitherasequencecompletion(hour,minute,...)or   ndanelementwithinthesamecategoryoutof   vepossiblechoices.thequestionsweregeneratedusingid138semanticrelations[13].thesedatasetswereconstructedbeforeanyembeddingstoavoidbiasingthemtowardsanyonemethod(sections5.3forfurtherdetails).aspredictedbythesemantic   eldhypothesis,wordembeddingssolvebothtaskse   ectively,withmetricembeddingconsistentlyperformingwellonthesemultiplechoicetasks(table3).themetricrecoveryapproachofmetricregressionmethodsandl2distancecanconsistentlyperformaswellasthecurrentstate-of-the-artonthethreesemantictasks:googlesemanticanalogies,sequence,andclassi   cation.5.2wordembeddingscanembedmanifoldsmnistdigits:weevaluatewhetherwordembeddingscanperformnonlin-eardimensionalityreductionbyembeddingthemnistdigitsdataset.usingafour-thousandpointsubset,wegeneratedak-nearestneighborgraph(k=20)andgenerated10simplerandomwalksoflength200fromeachpointresultingin40,000sentenceseachoflength200.wecomparedthefourwordembeddingmethodsagainststandarddimensionalityreductionmethods:pca,isomap,sneand,id167.thequalityofanembeddingwasmeasuredusingthepercent-ageof5-nearestneighborshavingthesameclusterlabel.thefourembeddingsshowninfig.2demonstratethatmetricregressionishighlye   ectiveatthistask,outperformingmetricsneandbeatenonlybyid167(91%),whichisavisualizationmethoddesignedforclusterseparation.allwordembeddingmeth-odsincludingsvd(68%)embedthemnistdigitswellandoutperformbase-linesofpca(48%)andisomap(49%)(suppplementaryfigures1).thisem-piricallyveri   esthetheoreticalpredictionsincorollary5thatlogco-occurencesofasimplerandomwalkconvergetothesquaredgeodesic.6discussionourworkfurtherjusti   eswordembeddingsbylinkingthemtosemanticspacesfrompsychometricliterature.thekeyconceptualglueismetricrecoveryfromco-occurrences.thenotionofsemanticspace,aswellasourtheoreticalrecoveryresults,suggestthel2distancecanserveasanaturalsemanticmetric.thisisreasonablysupportedbyourempiricalanalysis,includingtheconsistentperfor-11figure2:mnistdigitembeddingusingwordembeddingmethods(leftthree)andmetricembeddingonthesamegraph(right).performanceisquanti   edbypercentageof5-nearestneighborssharingthesameclusterlabel.manifoldlearningwordembeddinganalogyisomapsneregressionsemantic83.321.570.7syntactic8.21.576.9total51.413.173.4table2:wordembeddinggeneratedus-inghumansemanticsimilaritysurveysandmanifoldlearningoutperformswordembeddingsfromacorpus.classi   cationsequencemethodcosinel2cosinel2regression84.687.659.058.3glove80.173.159.048.8svd74.665.253.052.4id9784.676.456.254.4table3:regressionwithl2lossperformswellonsemanticclas-si   cationandsequencedatamanceoftheproposeddirectregressionmethodandtheutilityofl2distanceinselectinganalogies.ourframeworkhighlightsthestronginterplaybetweenmethodsforlearningwordembeddingsandmanifoldlearning,suggestingseveralavenuesforrecover-ingvectorrepresentationsofphrasesandsentencesviaproperlyde   nedmarkovprocessesandtheirgeneralizations.references[1]s.arora,y.li,y.liang,t.ma,anda.risteski.randomwalksoncontextspaces:towardsanexplanationofthemysteriesofsemanticwordembeddings.arxivpreprintarxiv:1502.03520,2015.[2]j.blitzer,a.globerson,andf.pereira.distributedlatentvariablemodelsoflexicalco-occurrences.inproc.aistats,pages25   32,2005.12[3]r.collobertandj.weston.auni   edarchitecturefornaturallanguageprocess-ing:deepneuralnetworkswithmultitasklearning.inproceedingsofthe25thinternationalconferenceonmachinelearning,pages160   167.acm,2008.[4]d.a.croydonandb.m.hambly.locallimittheoremsforsequencesofsimplerandomwalksongraphs.potentialanalysis,29(4):351   389,2008.[5]y.goldbergando.levy.id97explained:derivingmikolovetal.   snegative-samplingword-embeddingmethod.arxivprepr.arxiv1402.3722,pages1   5,2014.[6]z.s.harris.distributionalstructure.word,1954.[7]t.hashimoto,y.sun,andt.jaakkola.metricrecoveryfromdirectedunweightedgraphs.inarti   cialintelligenceandstatistics,pages342   350,2015.[8]o.levyandy.goldberg.linguisticregularitiesinsparseandexplicitwordrepresentations.proc.18thconf.comput.nat.lang.learn.(conll2014),2014.[9]o.levyandy.goldberg.neuralwordembeddingasimplicitmatrixfactoriza-tion.inadvancesinneuralinformationprocessingsystems,pages2177   2185,2014.[10]o.levy,y.goldberg,andi.dagan.improvingdistributionalsimilaritywithlessonslearnedfromwordembeddings.transactionsoftheassociationforcom-putationallinguistics,3:211   225,2015.[11]t.mikolov,k.chen,g.corrado,andj.dean.e   cientestimationofwordrepresentationsinvectorspace.arxivpreprintarxiv:1301.3781,2013.[12]t.mikolov,i.sutskever,k.chen,g.s.corrado,andj.dean.distributedrepresentationsofwordsandphrasesandtheircompositionality.inadvancesinneuralinformationprocessingsystems,pages3111   3119,2013.[13]g.millerandc.fellbaum.id138:anelectroniclexicaldatabase,1998.[14]a.mnihandg.e.hinton.ascalablehierarchicaldistributedlanguagemodel.inadvancesinneuralinformationprocessingsystems,pages1081   1088,2009.[15]s.molchanov.di   usionprocessesandriemanniangeometry.russianmathemat-icalsurveys,30(1):1,1975.[16]d.l.nelson,c.l.mcevoy,andt.a.schreiber.theuniversityofsouth   oridafreeassociation,rhyme,andwordfragmentnorms.behaviorresearchmethods,instruments,&computers,36(3):402   407,2004.[17]j.pennington,r.socher,andc.d.manning.glove:globalvectorsforwordrepresentation.proceedingsoftheempiricialmethodsinnaturallanguagepro-cessing(emnlp2014),12,2014.[18]b.perozzi,r.al-rfou,ands.skiena.deepwalk:onlinelearningofsocialrep-resentations.inproceedingsofthe20thacmsigkddinternationalconferenceonknowledgediscoveryanddatamining,pages701   710.acm,2014.13[19]d.e.rumelhartanda.a.abrahamson.amodelforanalogicalreasoning.cognitivepsychology,5(1):1   28,1973.[20]l.salo   -coste.theheatkernelanditsestimates.probabilisticapproachtogeometry,57:405   436,2010.[21]r.j.sternbergandm.k.gardner.unitiesininductivereasoning.journalofexperimentalpsychology:general,112(1):80,1983.[22]d.ting,l.huang,andm.jordan.ananalysisoftheconvergenceofgraphlaplacians.arxivpreprintarxiv:1101.5435,2011.[23]p.d.turneyandm.l.littman.corpus-basedlearningofanalogiesandsemanticrelations.machinelearning,60(1-3):251   278,2005.[24]s.r.varadhan.di   usionprocessesinasmalltimeinterval.communicationsonpureandappliedmathematics,20(4):659   685,1967.14supplementfor:word,graphandmanifoldembeddingfrommarkovprocessesseptember18,20151consistencyoftheglobalminimaofwordembeddingalgorithmslemmas1.1(lawoflargenumbersforlogcoocurrences).letxtbeamarkovchainde   nedbythetransitionp(xt=xj|xt   1=xi)=exp(   ||xi   xj||22/  2)pnk=1exp(   ||xi   xk||22/  2)(1)andcijbethenumberoftimesthatxt=xjandxt   1=xiovermstepsofthischain.thenforany  >0and  >0thereexistsomemandconstantsamiandbmjsuchthatp(cid:18)supi,j(cid:12)(cid:12)(cid:12)(cid:12)   log(cij)   ||xi   xj||22/  2+ami+bmj(cid:12)(cid:12)(cid:12)(cid:12)>  (cid:19)<  proof.bydetailedbalanceweobservethatthestationarydistribution  x(xi)existsandistheid172constantofthetransitionp(xt=xj|xt   1=xi)  x(xi)=exp(   ||xi   xj||22/  2)pnk=1exp(   ||xi   xk||22/  2)nxk=1exp(   ||xi   xk||22/  2)=p(xt=xi|xt   1=xj)  x(xj).de   nemiasthenumberoftimesthatxt=xiinamwordcorpus.applyingthemarkovchainlawoflargenumbers,weobtainthatforany  0>0and  0>0thereexistssomemsuchthatp(cid:16)supi(cid:12)(cid:12)(cid:12)  x(xi)   mi/m(cid:12)(cid:12)(cid:12)>  0(cid:17)<  0.thereforewithid203  0,mi>m(  x(xi)     0).nowgivenmi,cij   binom(p(xt=xj|xt   1=xi),mi)applyinghoe   ding   sinequalityandunionboundingforany  1>0and  1>0thereexistssomesetofmisuchthatp(cid:18)supi,j(cid:12)(cid:12)(cid:12)cij/mi   p(cid:16)xt=xj|xt   1=xi(cid:17)(cid:12)(cid:12)(cid:12)<  1!   (1   2exp(   2  21mi))n2=  1.since||xi   xj||2<   ,p(xt=xj|xt   1=xi)islowerboundedbysomestrictlypositiveconstantcandwemayapplythecontinuousmappingtheoremonlog(c)uniformlycontinuousover(c,   )toobtainthatforall  2and  2thereexistssomesetofmisuchthatp(cid:16)supi,j(cid:12)(cid:12)(cid:12)log(cij)   log(mi)   log(p(xt=xj|xt   1=xi))(cid:12)(cid:12)(cid:12)<  2(cid:17)     2.thereforegivenany  and  forthetheoremstatement,set  2=  and  2=     andde   nem0asthesmallestmirequired.sincesupij||xi   xj||<   ,themarkovchainlawoflargenumbersimplieswecanalways   ndsomemsuchthatinfimi>m0withid203atleast     whichcompletestheoriginalstatement.1theorems1.2(consistencyofsvd-mds).letcijbede   nedasaboveandmij=log(cij)andthecenteringmatrixv=i   11t/n.de   nethesvdbasedembeddingbxasbxbxt=cm=vmv/2.withoutlossofgenerality,alsoassumethatthelatentvectorsxhavezeromean,thenforany  >0and  >0,thereexistssomem,scalingconstant  ,andanorthogonalmatrixasuchthatp(xi||abxi/  2   xj||22>  )<  proof.bylemmas1.1wehavethatp(cid:18)supi,j(cid:12)(cid:12)(cid:12)(cid:12)   log(cij)   ||xi   xj||22/  2+ami+bmj(cid:12)(cid:12)(cid:12)(cid:12)>b  (cid:19)<b  sincemeanerrorcannotexceedentrywiseerrorwecanboundtherowaveragesoflog(cij),wherethedotproducttermiszerosincexiszeromean.p supi(cid:12)(cid:12)(cid:12)(cid:12)   pjlog(cij)n   ami   pjbmjn   ||xi||22     pj||xj||22  2n+2dxi,pjxjne(cid:12)(cid:12)(cid:12)(cid:12)>b  !<b  orinotherwords,   pjlog(cij)n   ami+pjbmjn   ||xi||22     pj||xj||22  2nde   nem0ij=   log(c)ij   pj   log(c)ijn;applyingthetriangleinequalityandcombiningbothboundsgivesp(cid:18)supj(cid:12)(cid:12)(cid:12)(cid:12)pim0ijn   (cid:18)bj   pkbmkn+||xj||   pk||xk||22  2n(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)>2b  (cid:19)<1   (1   b  )2.notethatm0ij   pim0ij/n=2cmijisthedoublycenteredmatrixasde   nedaboveandcombiningallaboveboundswehave,p(cid:18)supij(cid:12)(cid:12)(cid:12)(cid:12)cmij   hxi,xji(cid:12)(cid:12)(cid:12)(cid:12)>4b  (cid:19)<1   (1   b  )4.giventhatthedotproductmatrixhaserroratmost4  theresultingembeddingitknowntohaveatmostp4b  error[15].thiscompletestheproof,sincewecanpickb  =  2/4andb  =1   (1     )1/4lemmas1.3(consistencyofsvd).assumetheconditionsoftheorems1.2andadditionally,assumethenormofthelatentembeddingisproportionaltotheunigramfrequency||xi||/  2=pjcijqpijcij.undertheseconditions,letbxbetheembeddingderivedfromthesvdofmijas2bxbxt=mij=log(cij)   log(cid:16)xkcik(cid:17)   log(cid:16)xkckj(cid:17)+log(cid:16)xijcij(cid:17)+  .thenthereexistsa  suchthatthisembeddingisclosetothetrueembeddingunderthesameequivalenceclassaslemmas1.3p(cid:16)xi||abxi/  2   xj||22>  (cid:17)<  .2proof.bylemmas1.1,forany  0>0and  0>0thereexistsamsuchthatp supi,j(cid:12)(cid:12)(cid:12)(cid:12)   log(cij)   ||xi   xj||22/  2+log(cid:16)nxk=1exp(   ||xi   xk||22/  2)(cid:17)   log(cid:16)xkcik(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)>  0!<  0whichimpliesthatforany  1>0and  1>0thereexistsamsuchthatp(cid:18)supi,j(cid:12)(cid:12)(cid:12)(cid:12)   log(cij)   (cid:0)||xi   xj||22/  2(cid:1)   log(mc)(cid:12)(cid:12)(cid:12)(cid:12)>  1(cid:19)<  1.nowadditionally,ifpkcik/qpijcij=||xi||2/  2thenwecanrewritetheaboveboundasp      supi,j(cid:12)(cid:12)(cid:12)(cid:12)log(cij)   log(cid:16)xkcik(cid:17)   log(cid:16)xkckj(cid:17)+log(cid:16)xijcij(cid:17)   2hxi,xji/  2   log(mc)(cid:12)(cid:12)(cid:12)(cid:12)>  1      <  1.andtherefore,p(cid:18)supi,j(cid:12)(cid:12)(cid:12)(cid:12)mij   2hxi,xji/  2   log(mc)(cid:12)(cid:12)(cid:12)(cid:12)>  1(cid:19)<  1.giventhatthedotproductmatrixhaserroratmost  1,theresultingembeddingitknowntohaveatmost     1error[15].thiscompletestheproof,sincewecanpick  =   log(mc),  1=  2and  1=  .theorems1.4(consistencyofglove).de   nethegloveobjectivefunctionasg(bx,bc,ba,bb)=xi,jf(cij)(2bxibcj+ba+bb   log(cij))2de   nexm,cm,am,bmastheglobalminimaoftheaboveobjectivefunctionforacorpusofsizem.thentheparametersderivedfromthetrueembeddinginlemmas1.1,x0=x/  ,a0i=ami   ||xi||22/  2,b0i=bmi   ||xi||22/  2isarbitrarilyclosetotheglobalminimainthesensethatforany  >0and  >0thereexistssomemsuchthatp(|g(x0,x0,a0,b0)   g(xm,cm,am,bm)|>  )<  proof.usinglemmas1.1witherror  0andid203  0thereexistssomemsuchthatuniformlyoveriandj,(   ||xi   xj||22/  2+ami+bmi+log(cij))2     20.nowrecallthatf(cij)   103/4=cthereforep(g(x0,x0,a0,b0)>cn2  20)<  0.nowtheglobalminimag(xm,cm,am,bm)mustbelessthang(x0,x0,a0,b0)andwehave0<g(xm,cm,am,bm)<g(x0,x0,a0,b0).therefore,p(|g(x0,x0,a0,b0)   g(xm,cm,am,bm)|>cn2  0/2)<  0.pickingamsuchthat  0=2  /(cn2)and  0=  concludestheproof.theorems1.5(consistencyofsoftmax/id97).de   nethesoftmaxobjectivefunctionwithbiasasg(bx,bc,bb)=xijcijlog exp(   ||bxi   bcj||22+bbj)pnk=1exp(   ||bxi   bck||22+bbk)!de   nexm,cm,bmastheglobalminimaoftheaboveobjectivefunctionforacorpusofsizem.weclaimthatforany  >0and  >0thereexistssomemsuchthatp(|g(x/  ,x/  ,0)   g(x,c,b)|>  )<  3proof.bydi   erentiation,anyobjectiveoftheformmin  ijcijlog(cid:18)exp(     ij)pkexp(     ik)(cid:19)hastheminima  ij=   log(cij)+aiwithobjectivefunctionvaluecijlog(cij/pkcik).thisgivesaglobalfunctionlowerboundg(x,c,b)   xijcijlog(cid:18)cijpkcik(cid:19)nowconsiderthefunctionvalueofthetrueembeddingx/  ;g(x/  ,x/  ,0)=xijcijlog(cid:18)exp(   ||xi   xj||22/  2)pkexp(   ||xi   xk||22/  2)(cid:19)=xijcijlog(cid:18)exp(log(cij)+  ij+ai)pkexp(log(cik)+  ik+ai)(cid:19).wecanboundtheerrorvariables  ijusinglemmas1.1assupij|  ij|<  0withid203  0forsu   cientlylargemwithai=log(mi)   log(pnk=1exp(   ||xi   xk||22/  2)).takingthetaylorexpansionat  ij=0,wehaveg(x/  ,x/  ,0)=xijcijlog(cid:18)cijpkcik(cid:19)+nxl=1cilpkcik  il+o(||  ||22)applyinglemmas1.1weobtain:p      (cid:12)(cid:12)(cid:12)g(x/  ,x/  ,0)   xijcijlog(cid:16)cijpkcik(cid:17)(cid:12)(cid:12)(cid:12)>n  0      <  0combiningwiththeglobalfunctionlowerboundwehavethatp(cid:16)(cid:12)(cid:12)(cid:12)g(x/  ,x/  ,0)   g(x,c,b)(cid:12)(cid:12)(cid:12)>n  0(cid:17)<  0.toobtaintheoriginaltheoremstatement,takemtoful   l  0=  /nand  0=  .notethatfornegative-samplingbasedid97,applyingthestationarypointanalysisof[7]combinedwiththeanalysisinlemmas1.3showsthatthetrueembeddingisaglobalminima.theorems1.6(metricregressionconsistency).de   nethenegativebinomialobjectivefunction  (bx,bc,ba,bb)=exp(   ||bxi   bcj||22/2+ai+bj)g(bx,bc,ba,bb,  )=xi,j  log(  )     log(  (bxi,bcj,bai,bbj)+  )+cijlog 1       (bxi,bcj,bai,bbj)+  !+log(cid:18)  (cij+  )  (  )  (cij+1)(cid:19)thentheparametersderivedfromthetrueembeddinginlemmas1.1,x0=x/  ,a0i=am,b0i=bmiisarbitrarilyclosetotheglobalminimag(xm,cm,am,bm)inthesensethatforany  >0and  >0thereexistssomemsuchthatp(|g(x0,x0,a0,b0)   g(xm,cm,am,bm)|>  )<  proof.theproofproceedsidenticallytothatoftheorems1.5.firstobtaintheglobalminimaat  (bxi,bcj,bai,bbj)=cij)asg(xm,cm,am,bm)   xijkij4wherekij=cij(log(cij)   log(cij+  )+  (log(  )   log(cij+  ))+log(  (cij+  ))   log(  (  ))   log(cij+1)).aswiththeorems1.5,rewriting  (bxi,bcj,bai,bbj)=cijexp(  ij)allowsustotakethetaylorexpansionforexp(  ij)small,givingllh(x,a,b,  )=xijkij   cij  2(cij+  )(  ij)2+o(  3ij).applyinglemmas1.1weobtain:p(cid:16)(cid:12)(cid:12)(cid:12)g(x0,x0,a0,b0)   xijkij(cid:12)(cid:12)(cid:12)>n  0(cid:17)<  0whichwhencombinedwiththeglobalfunctionboundyieldsthattheglobalminimaisconsistent.2symmetryandwindowingco-occurencesexistingwordembeddingalgorithmsutilizeweighted,windowed,symmetrizedwordcounts.letctijde   nethet-stepco-occurencewhichcountsthenumberoftimesxt+t0=xjandxt0=xi.thenforsomeweightfunctionw(t)suchthatp   t=1w(t)=1,wede   nebcij=   xt=1w(t)(ctij+ctji).thisisdistinctfromourstochasticprocessapproachintwoways:   rst,thereissymmetrizationbycountingbothforwardandbackwardtransitionsofthemarkovchain.second,allwordswithinawindowofthecenterwordxt0areusedtoformtheco-occurences.symmetry:webeginbyconsideringasymmetryoftherandomwalk.ifthemarkovchainisreversibleasinthecasesofthegaussianrandomwalk,un-directedgraphs,andthetopicmodel,wecanapplydetailedbalancetoshowthatthejointdistributionsaresymmetricp(xt+1=xj|xt=xi)  x(xi)=p(xt+1=xi|xt=xj)  x(xj)thereforetheempiricalsumconvergestoctij+ctji   p(xt+t0=xj,xt0=xi)+p(xt+t0=xi,xt0=xj)=2p(xt+t0=xj,xt0=xi)inthecaseswheretherandomwalkisnon-reversible,suchasak-nearestneighborgraphthenthetwotermsarenotexactlyequal,howevernotethatifthenon-symmetrizedtransitionmatriciescijful   llvaradhan   sformulabothways:   tlog(cij)   ami   ||xi   xj||22+bmjand   tlog(cji)   amj   ||xj   xi||22+bmithesumbcijwillful   l(ctij+ctji)=exp(   ||xi   xj||22/t+o(1/t))(exp(ai/t+bj/t)+exp(bi/t+aj/t))and   tlog(ctij+ctji)=||xi   xj||22+log(exp(ai/t+bj/t)+exp(bi/t+aj/t))t+o(1)morespeci   cally,forthemanifoldcase,ai=log(  xn)   log(np(x)/  (xi)2)andbj=   log(np(x)),andsotheabovetermreducesto   tlog(ctij+ctji)=||xi   xj||22+log(cid:0)     2(xi)+     2(xj)(cid:1)t+o(1)sincethe  isindependentoft,ast   0,weareonceagainleftwithvaradhan   sformulainthesymmetrizedcase.5inpractice,thisdoesnotseemtoa   ectthemanifoldembeddingapproachesmuch;intheresultssectionweattemptembeddingthemnistdigitsdatasetusingthek-nearestneighborsimplerandomwalkwhichisnonreversible.windowing:nowweconsiderthee   ectofwindowing.wefocusonthemanifoldcaseforanalyticsimplicity,butthesamelimitsapplytotheothertwoexamplesofgaussianrandomwalksandtopicmodels.letqt(x,x0)=p(yt=x|y0=x0)andwhereytful   llsvaradhan   sformulasuchthatthereexistsametricfunction  ,limt   0   tlog(qt(x,x0))     (x,x0)2undertheseconditions,letbqt(x,x0)=rt0qt0(x,x0)/tdt0de   nethewindowedmarginaldistribution.weshowthisfollowsawindowedvaradhan   sformula.limt   0tbqt(x,x0)     (x,x0)2thiscanbedoneviaadirectargument.varadhan   sformulaimpliesthat,qt(x,x0)=exp(cid:18)     (x,x0)2t+o(cid:18)1t(cid:19)(cid:19).thuswecan   ndsomeboundingconstants0<c=o(1)suchthatzt01texp(cid:18)     (x,x0)2t0   ct0(cid:19)dt   bqt(x,x0)   zt01texp(cid:18)     (x,x0)2t0+ct0(cid:19)dt0performingtheboundingintegralforgeneralc   r,zt01texp(cid:18)     (x,x0)2t0+ct0(cid:19)dt0=1t(cid:18)exp(cid:18)     (x,x0)2   2c2t(cid:19)t+(c     (x,x0)2/2)  (cid:18)  (x,x0)2   2c2t(cid:19)(cid:19)=1t(cid:18)exp(cid:18)   ct     (x,x0)2t(cid:19)(cid:18)   2t2c     (x,x0)2+t2(cid:19)(cid:19)thereforewehavethatforanyc,limt   0   tlog(cid:18)zt01texp(cid:18)     (x,x0)2t0+ct0(cid:19)dt0(cid:19)     (x,x0)2   cbythetwo-sidedboundandc=o(1),limt   0tbqt(x,x0)     (x0,x)2.asdesired.3varadhan   sformulaongraphswe   rstprovetheconvergenceofmarginaldensitiesundertheassumptionofequicontinuity.lemmas3.1(convergenceofmarginaldensities).letx0besomepointinourdomainxnandde   nethemarginaldensitiesbqt(x)=p(yt=x|y0=x0)qtn(x)=p(xnt=x|xn0=x0)iftng2n=bt=  (1),thenundercondition(?)andtheresultsoftheorem3suchthatxnt   yntweakly,wehavelimn      nqtn(x)=bqbt(x)p(x).6proof.thea.s.weakconvergenceofprocessesoftheorem3impliesby[2,theorem4.9.12]thattheempiricalmarginaldistributiond  n=nxi=1qtn(xi)  xiconvergesweaklytoitscontinuousequivalentd  =bqbt(x)dxforybt.foranyx   xand  >0,weakconvergenceagainstthetestfunction1b(x,  )yieldsxy   xn,|y   x|<  qtn(y)   z|y   x|<  bqbt(y)dy.byuniformequicontinuityofnqt(x),forany  >0thereissmallenough  >0sothatforallnwehave(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)xy   xn,|y   x|<  qtn(y)   |xn   b(x,  )|bqt(x)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)   n   1|xn   b(x,  )|  ,whichimpliesthatlimn      qtn(x)p(x)n=lim     0limn      v   1d     dnqtn(x)z|y   x|<  p(y)dy=lim     0limn      v   1d     d|xn   b(x,  )|qtn(x)=lim     0v   1d     dz|y   x|<  bqbt(y)dy=bqbt(x).weconcludethedesiredlimn      nqt(x)=bqbt(x)p(x).giventhis,wecannowprovevaradhan   sformulaspecializedtothemanifoldgraphcase:corollarys3.2(heatkernelestimatesongraphs).forany  >0,  >0,n0>0thereexistssomebt,n>n0,andsequencebnjsuchthatthefollowingholdsforthesimplerandomwalkxnt:p supxi,xj   xn0(cid:12)(cid:12)(cid:12)btlog(p(xnbtg   2n=xj|xn0=xi))   btbnj       (x)(xi,xj)2(cid:12)(cid:12)(cid:12)>  !<  where    (x)isthegeodesicde   nedby  (x):    (x)(xi,xj)=minf   c1:f(0)=xi,f(1)=xjz10  (f(t))dtproof.theproofisintwoparts.first,byvaradhan   sformula(theorem4,[13,eq.1.7])forany  1>0thereexistssomebtsuchthat:supy,y0   d|   btlog(p(ybt=y0|y0=y))       (x)(y0,y)2|<  1nowuniformequicontinuityofmarginalsimpliesuniformconvergenceofmarginals(lemmas3.1)andthere-foreforany  2>0and  0,thereexistsansuchthat,p(supxj,xi   xn0|p(ybt=xj|y0=xi)   np(xj)p(xng   2nbt=xj|xn0=xi)|>  2)<  0bythelowerboundonpandcompactnessofthedomaind,p(ybt|y0)islowerboundedbysomestrictlypositiveconstantcandwecanapplyuniformcontinuityoflog(x)over(c,   )togetthatforsome  3and  ,p(supxj,xi   xn0|log(p(ybt=xj|y0=xi))   log(np(xj))   log(p(xng   2nbt=xj|xn0=xi))|>  3)<  .(2)7finallywehavethebound,p(supxi,xj   xn0(cid:12)(cid:12)(cid:12)(cid:12)   btlog(p(xng   2nbt=xj|xn0=xi))   btlog(np(xj))       (x)(xi,xj)2(cid:12)(cid:12)(cid:12)(cid:12)>  1+bt  3)<  tocombinethebounds,givensome  and  ,setbnj=log(np(xj)),pickbtsuchthat  1<  /2,thenpicknsuchthattheboundineq.2holdswithid203  anderror  3<  /(2bt).4heatkernelfortopicmodelstheorems4.1(heatkernelestimatesforthetopicmodel).lethhavesmoothsubgaussiantailsasde   nedbythefollowingconditions;thereexistssome  (x)suchthatsupx  (x)<   andrrd||x||2d+42  (x)dx      suchthat:1.(tailbound)forall|  |<6,|d  xh(x)|<  (x)2.(convolvedtailbound)forall|  |<6,forthek-foldconvolvedkernelh(k),|d  xh(k)(x)|<k    (k     x)for  >0.further,iflog(w(x))hasboundedgradientsoforderupto6,thenwehavethefollowing:let  2y=  20  2,where  0=r||x||22h(||x||22)dxthentherandomwalkytde   nedbyequation4admitsaheatkernelapproximationofthemarginaldistributionattimetintermsofconstants  1(x,y)andv1(x,y),supx,y(cid:12)(cid:12)(cid:12)(cid:12)p(yt=yt|y0=y0)   1(4  t  2y)d/2exp(cid:18)   ||yt   y0||224  2yt(cid:19)(cid:0)1+v1(yt,y0)  2yt+o(  2yt)(cid:1)(cid:12)(cid:12)(cid:12)(cid:12)   t   1/2  1(yt,y0)+o(t   1)proof.first,thestroock-varadhantheorem[17]impliesthataftert=bt     2stepsthereexistsalimitingprocesslim     0ybt     2   bybtdescribedbythesdedbybt=   log(w(bybt))  20dbt+  0dwbt.inourcase,wecandeterminetherateofconvergenceofthemarginaldistributionsofyttobytbyanedgeworthapproximationduetooursmoothtailconstraintsonh[5,theorem4.1].supyt,y0(cid:12)(cid:12)(cid:12)p(yt=yt|y0=y0)   p(byt=yt|by0=y0)   t   1/2  1(yt,y0)   t   1  2(yt,y0)(cid:12)(cid:12)(cid:12)   o(t   1     )thedetailsof  1(yt,y0)and  2(yt,y0)aregivenin[5,theorem4.1],wenotethatifthedriftisconstant   log(w(x))=c,themarginalofbytisexactlygaussianand  1(yt,y0)and  2(yt,y0)areexactlythetermsinanedgeworthapproximationwhenapplyingthecentrallimittheoremtoh(x).theaboveapproximationistightast      ;however,themarginaldistributionofbyt  2isonlygaussianast  2   0.weshowthatthisconvergenceisfastint  2suchthatif  2issu   cientlysmalltheheatkernelisstillanusefulapproximation.letbqt(y,x)=p(byt=y|by0=x)thenbythefokker-planckequation,thisful   lsthefollowingrelationship:      tbqt(yt,y0)=xi,j      yj   yi  20bqt(yt,y0)+xi      yi   log(w(yt))  20bqt(yt,y0)weuseshorttimeasymptoticsofsecond-orderellipticdi   erentialequationstoobtainthehigherorderexpansion[4]:bq(bt,x,y)=1(4    20bt)d/2exp(cid:18)   ||xi   yi||224  20bt(cid:19)         xj=0vj(x,y)tj      recallthatbt=  2t.substitutingintotheabovegivesthatsupx,y(cid:12)(cid:12)(cid:12)(cid:12)p(yt=yt|y0=y0)   1(4  t  2y)d/2exp(cid:18)   ||yt   y0||224  2yt(cid:19)         xj=0vj(yt,y0)tj      (cid:12)(cid:12)(cid:12)(cid:12)   t   1/2  1(yt,y0)+o(t   1)8finallyitsu   cestoshowthatv0(x,y)=1whichfollowsfromthefactthatourdatalieineuclideanspace[16].inmoregeneralmanifolds,therewillbesomecurvatureassociateddistortiontothedensity.thisproofgivestheintuitionbehindvaradhan   sformula.whilethereareconfounderssuchasthekernelh,driftw,andcurvaturehess(logw);theseissuesalldissapearwhent      (largewindowsize)and     2<<t(topicsremainlocal).combiningthiswiththeemissionid203ofxgivestheappropriateheatkernelestimatedirectlyontheobservedrandomwalkoverwords.applyingtheorems4.1,weobtainthefollowingapproximation:p(xt=xj|x0=xi)=zzp(xt=xj|yt=yt)p(yt=yt|y0=y0)p(y0=y0|x0=xi)dy0dyt=1  x(xi)zzp(xt=xj|yt=yt)p(xi=xi,y0=y0)p(yt=yt|y0=y0)dy0dytwhere  x(xi)istheunigramfrequency.dealingwiththeinnerintegral   rst,zp(xi=xi,y0=y0)p(yt=yt|y0=y0)dy0     izw(y0)exp(cid:18)   ||xi   y0||22  2(cid:19)1(4    2yt)d/2exp(cid:18)   ||yt   y0||224  2yt(cid:19)(1+o(t))dy0=  iw(xi)(2    )d/21(4    2yt)d/2exp(cid:18)   ||yt   xi||224  2yt(cid:19)(1+o(  yt)+o(  2)+o(t   1/2))wherethelastapproximationisalaplaceapproximationforsmall  takenatxi[3].nowapplyingtheintegraloverytp(xt=xj|x0=xi)     i  (xi)w(xi)exp    ||xj   xi||222(  2+t  2y)!(1+o(  2yt)+o(  2))+o(t   1/2)thishastheappropriateformofaheatkernelestimatewiththeai=log(  i)+log(w(xi))   log(  (xi))withtwosourcesoferror:toofewstepsresultinginnon-gaussiantransitionso(t   1/2)andtoomanystepsintroducingdistortionso(  2yt),o(  2).4.1relationshiptothetopicmodelofaroraetalthepreprint[1]suggestsalatenttopicmodelandconsiderthefollowingmodel.de   nectadiscrete-timecontinuousspacelatenttopicprocesswiththefollowingrestrictions:1.(stationarydistributionnearzero)thestationarydistributioncisaproductdistribution,andec   c[|ci|2]=1/dandalmostsurely|ci|   2/   d2.(incrementsofchavelighttailsandconvergetozeroforlargecorpora)ep(ct+1|ct)[exp(4  |ct+1   ct|1log(m))]   1+  2theobservedsentenceisthengeneratedbyp(w|c)=exp(hvw,ci)pwexp(hvw,ci)undertheseconditions,theyshowthatforwordsw,w0andforsu   cientlylargecorpora,log(p(w,w0)=12d||vw+v0w||2   2logz  o(1)9thismodelisqualitativelyquitesimilartoourtopicmodel.condition(1)onthestationarydistributionisanalogoustoourlimit     0,whichensuresthenoisetermissharpwithrespecttoourstationarydistributionw(x).condition(2)istheincrementsizeconstraint,gn   0.theconceptualdistinctionbetweenthesetwomethodsisthatourtopicmodelarisesasanaturalexten-sionofourshorttimeasymptoticmanifoldanalysis.theheatkernelargumentgivesdirectintuitionandjusti   cationforthegaussiandecayoftheresultingmarginaldistribution.examiningthemodelsindetail,thetwoconditionsof[1]onthelatenttopicmodelarestrongerthanoursinthesensethatwedonotrequirequantitativeboundsonthestationarydistributionortheincrementsize;theymaygotozeroatanyratewithrespecttothecorpussize.wegaintheseweakerconditionsbyassumingthatthevocabularysize(ninournotation)goestoin   nityandtakingmanystepst      .thistrade-o   betweenadditionalassumptionseitherasdirectconstraintsoradditionallimitsisunavoid-able.recallthatp(xt=xj|x0=xi)=1  x(xi)zzw(y0)p(xt=xj|yt=yt)p(xi=xi|y0=y0)p(yt=yt|y0=y0)dy0dyt.inordertoobtainexponentialdecayonthelhsassumingonlyexponentialdecayinthewordemissionsp(xi=xi|y0=y0),wemusteitherinvokeaguassianlimitforp(yt=yt|y0=y0)orconvergeittoapointmassrelativetop(xi=xi|y0=y0).ouruseofthelargevocabularylimitandtheheat-kernelapproximationallowsustotaketheformerlimit,ratherthanuseassumptionstoforcep(yt=yt|y0=y0)toapointmass.105empiricalevaluationdetails5.1implementationdetailsweusedo   -the-shelfavailableimplementationsofid97   andglove   .inthepaper,thesetwomethodsarerunwiththeirstandardsettings,withtwoexceptions:glove   scorpusweightingisdisabled,asthisgenerallyproducedsuperiorresults,andglove   sstepsizesarereducedasthedefaultstepsizedresultedinnan-valuedembeddings.forallthemodelsweused300-dimensionalvectors,withwindowsize5.forid97weusedtheskip-gramversionwith5negativesamples,10iterations,  =0.025andfrequentwordsub-samplingwithaparameterof10   3.forgloveweusedxmax=10,  =0.01and10iterations.thetwoothermethods(randomized)svdandregressionembeddingarebothimplementedontopoftheglovecodebase.forsvdwefactorizetheppmiwithnoshift(  =0inournotationfromthemaintext)using50,000vectorsintherandomizedprojectionapproximation.forregression,weuse  =50and  isline-searchedstartingat  =10.5.1.1regressionembeddingforregressionembedding,wedostandardstochasticgradientdescentwithtwodi   erences:   rst,anywordco-occurencepairscijwithcountsfewerthantenareskippedwithid203proportionalto1   cij/10,thisisdonetoachievedramaticspeedupsintrainingtimewithnodetectablelossinaccuracy.second,weavoidtheproblemofstepsizetuningbyusinganinitiallinesearchstepcomblateinedwithalinearstepsizedecaybyepoch.otherwise,initializationandotheroptimizerchoicesarekeptidenticaltoglove.5.1.2randomizedsvdduetothememoryandruntimerequirementsofrunningafullsvddecomposition,weperformedapproxi-matesvdsusingrandomizedprojections.forthesvdalgorithmof[7],weusethegloveco-occurencecountercombinedwithaparallelrandomizedprojectionbasedsvdfactorizerbasedupontheredsvdlibrary   .weimplementresonablebestpracticesof[8]ofusingthesquarerootfactorizationandnonegativeshifts.forthenumberofapproximationvectors,wetriedvarioussizesandfoundvectorcountspast50,000o   eredlittleimprovement.5.2wordembeddingcorporaweusedthreecorporatotrainthewordembeddings:thefullwikipediadumpof03/2015(about2.4btokens),alargercorpussimilartothatusedbyglove[14]:wikipedia2015+gigaword5(5.8btokensintotal)andtheoneusedid97[10],whichconsistsofamixtureofseveralcorporafromdi   erentsources(6.4btokensintotal).wepreprocessedallthecorporabyremovingpunctuation,numbersandlower-casingallthetext.finallywerantwopassesofid97   stokenizerword2phrase.asa   nalstep,weremovedfunctionwordsfromthevocabularyandkeptonlythe100kmostcommonwordsforallourexperiments.5.3datasetsforsemantictasksour   rstsetofexperimentsisontwostandardopen-vocabularyanalogytasks:google[9]andmsr[11].googleconsistsof19,544semanticandsyntacticanalogyquestions,whilemsr   s8,000questionsareallsyntactic.asanadditionalanalogytask,weusethesatanalogyquestions(version3)ofturney[18].thedatasetcontains374questionsfromactualsatexams,guidebooks,fromtheetswebsiteandothersources.eachquestionconsistsof5exemplarpairsofwordsword1:word2,whereallthepairsholdthesamerelation.thetaskistopickfromamonganother   vepairsofwordstheonethatbestrepresentstherelation   http://code.google.com/p/id97   http://nlp.stanford.edu/projects/glove   https://github.com/ntessore/redsvd-h11representedbytheexemplars.tothebestofourknowledge,thisisthe   rsttimewordembeddingsareusedtosolvethistask.giventhecurrentlackoffreelyavailabledatasetswithcategoryandsequencequestions,asdescribedinsection2,wedecidedtocreatethem.weusednltk   s  interfacetoid138[12]incombinationwithword-wordpmivaluescomputedonthewikicorpustocreatethesequencesandclasses.asa   rststep,wecollectedasetofrootwordsfromothersemantictaskstoinitializethemethods.fortheclassi   cationdata,wecreatedthein-categorywordsbyselectingwordsfromvariousid138relationsassociatedtotherootwords,afterwhichwepruneddowntofourwordsbasedonpmi-similaritytotherootwordandtheotherwordsintheclass.theadditionaloptionsforthemultiplechoicequestionwerecreatedsearchingoverwordsrelatedtotherootbyadi   erentrelationtype,andselectingthosemostsimilartotheroot.forthesequencedata,weobtainedfromid138treesofwordsgivenbyvariousrelationtypes,andthenprunedbasedonsimilaritytotherootword.forthemultiple-choiceversionofthedata,weselectedadditional(incorrect)optionsbysearchingoverotherwordsrelatedtotherootword,andpruning,asforsequences,basedonpmisimilarity.finally,wemanuallyprunedallthreesetsofquestions,keepingonlythemostcoherentquestions,inordertoincreasethequalityofthedatasets.afterpruning,thecategorydatasetwasleftwith215questionsandthesequencedatasetwith51questionsinitsopen-vocabularyversionand169initsmultiplechoiceversion.thetwodatasetswillbemadeavailableforotherstoexperimentwith.wehopethattheyhelpbroadenthetypeoftasksusedtoevaluatesemanticcontentofwordembeddings.5.4solvingclassi   cationandseriescompletiontasksineachtaskweobtainanidealpointviathefollowingvectoroperations.   analogies:givena:b::cformtheidealpointbyb   a+cfollowingtheparallelogramrule.   analogies(sat):givena:bandcandidatesc1:d1...cn:dnformtheidealpointbyb   aandrepresenttheoptionsasdi   ci.   categories:givenasetw1,...,wnde   ningacategory,wede   netheidealtobei=1npni=1wi.   sequence:givensequencew1:      :wnwecomputetheidealasi=wn+1n(wn   w1).5.5similaritymetricsforverbalreasoningtaskgiventheidealpointiofataskandoptions(possiblytheentirevocabulary)wepicktheanswerbyproximityoftheidealpointimeasuredinthreepossibleways.   cosine:we   rstunit-normalizeeachvectoraswi/||wi||2andusecosinesimilaritytochoosewhichvectorisclosesttotheideal.   l2:wedonotapplyanyid172,andpicktheclosestvectorbyl2distance.   di   -cosine(satonly):forthesat,thedi   erencesofthevectorsarenormalized,andsimilarityismasuredbycosinedistance.inourexperimentswefoundcosineandl2togivereasonableperformanceunderalltasks.thepre-id172ofcosinevectorsareconsistenttowhatwasdonein[10,6]).forthel2distanceweappliednoid172.  http://www.nltk.org/126mnist   gurefigure1:mnistdigitembeddingusingwordembeddingmethodandmetricembeddingonthesamegraph.7fulltableofanalogyresults137.1top-30kvocabularyresctrictiongoogleanalogiessatmsranalogiessemanticsyntactictotalcovered50228195132172174358total886910675195443748000table1:glovecorpusquestioncoveragegoogleanalogiessatmsranalogiessemanticsyntactictotalcovered47467679124251994340total886910675195443748000table2:wikicorpusquestioncoveragegoogleanalogiessatmsranalogiessemanticsyntactictotalcovered39658447124122574554total886910675195443748000table3:w2vcorpusquestioncoveragegoogleanalogies(cosine)googleanalogies(l2)satmsranalogiesmethodsemanticsyntactictotalsemanticsyntactictotall2di   -cosinecosinecosinel2regression78.470.573.574.170.071.238.741.733.767.264.0glove70.270.970.659.267.764.537.240.735.761.253.5svd55.846.450.049.141.244.332.732.228.133.530.3id9768.073.871.666.671.269.441.942.941.465.063.4table4:wikicorpusanalogyaccuracyclassi   cationsequencesequence(openvocab)sequence(openvocab,top5)methodcosinel2cosinel2cosinel2cosinel2regression86.185.658.055.67.85.972.560.8glove80.976.759.251.52.02.051.037.3svd74.964.746.246.22.02.021.625.5id9785.171.657.459.22.05.949.051.0table5:wikicorpusforclassi   cationandsequencegoogleanalogies(cosine)googleanalogies(l2)satmsranalogiesmethodsemanticsyntactictotalsemanticsyntactictotall2di   -cosinecosinecosinel2regression78.470.873.775.570.972.639.240.637.865.663.9glove72.671.271.765.666.667.236.942.833.662.055.6svd57.450.853.453.748.250.327.132.225.832.030.6id9773.473.373.371.470.971.142.049.242.067.966.5table6:glovecorpusanalogyaccuracy14classi   cationsequencesequence(openvocab)sequence(openvocab,top5)methodcosinel2cosinel2cosinel2cosinel2regression84.687.658.958.30.00.023.521.6glove80.173.158.948.80.00.027.523.5svd74.665.253.052.40.02.019.615.7id9784.676.456.254.40.03.953.058.8table7:glovecorpusforclassi   cationandsequencegoogleanalogies(cosine)googleanalogies(l2)satmsranalogiesmethodsemanticsyntactictotalsemanticsyntactictotall2di   -cosinecosinecosinel2regression80.173.075.277.373.174.438.143.036.969.468.4glove70.473.072.261.970.067.236.943.934.066.461.6svd55.243.654.152.850.651.327.937.329.135.635.4id9766.873.471.367.272.270.639.046.442.375.375.6table8:w2vcorpusanalogyaccuracyclassi   cationsequencesequence(openvocab)sequence(openvocab,top5)methodcosinel2cosinel2cosinel2cosinel2regression81.485.557.155.40.00.025.521.6glove78.270.057.750.62.00.031.431.4svd74.161.147.048.20.00.035.321.6id9787.075.052.750.93.95.949.045.1table9:w2vcorpusforclassi   cationandsequence7.2top-100kvocabularygoogleanalogiessatmsranalogiessemanticsyntactictotalcovered782910411182402175612total886910675195443748000table10:glovecorpusquestioncoveragegoogleanalogiessatmsranalogiessemanticsyntactictotalcovered766710231178981995186total886910675195443748000table11:wikicorpusquestioncoveragegoogleanalogiessatmsranalogiessemanticsyntactictotalcovered721310405176182445462total886910675195443748000table12:w2vcorpusquestioncoverage15googleanalogies(cosine)googleanalogies(l2)satmsranalogiesmethodsemanticsyntactictotalsemanticsyntactictotall2di   -cosinecosinecosinel2regression76.964.669.964.962.563.538.741.733.762.657.4glove69.066.067.353.562.158.437.240.735.758.650.2svd53.840.246.140.234.236.832.732.128.131.326.7id9767.970.469.367.467.267.341.743.241.262.461.5table13:wikicorpusanalogyaccuracyclassi   cationsequencesequence(openvocab,top5)sequence(openvocab)methodcosinel2cosinel2cosinel2cosinel2regression86.085.658.055.662.737.311.815.7glove80.976.759.251.551.037.33.93.9svd74.964.746.246.221.625.53.93.9id9785.171.645.143.143.145.13.911.8table14:wikicorpusforclassi   cationandsequencegoogleanalogies(cosine)googleanalogies(l2)satmsranalogiesmethodsemanticsyntactictotalsemanticsyntactictotall2di   -cosinecosinecosinel2regression75.066.470.170.066.167.739.240.637.862.258.9glove70.767.568.862.562.462.536.942.933.661.053.0svd57.044.250.347.942.044.527.232.325.830.627.4id9771.771.571.570.068.769.542.148.241.767.066.8table15:glovecorpusanalogyaccuracyclassi   cationsequencesequence(openvocabtop5)sequence(top1)methodcosinel2cosinel2cosinel2cosinel2regression84.687.658.958.323.517.60.00.0glove80.173.158.348.827.523.50.00.0svd74.665.155.654.419.611.80.03.9id9784.676.455.654.449.054.90.07.8table16:glovecorpusforclassi   cationandsequencegoogleanalogies(cosine)googleanalogies(l2)satmsranalogiesmethodsemanticsyntactictotalsemanticsyntactictotall2di   -cosinecosinecosinel2regression78.268.972.772.068.670.038.143.036.966.163.1glove70.669.870.161.265.763.936.953.934.065.359.0svd55.947.851.145.444.745.027.937.229.133.631.1id9767.171.669.868.070.469.439.247.142.873.874.6table17:w2vcorpusanalogyaccuracyclassi   cationsequencesequence(top5)sequence(top1)methodcosinel2cosinel2cosinel2cosinel2regression81.385.557.155.424.521.60.00.0glove78.270.058.350.631.431.43.90.0svd74.161.145.848.231.421.60.00.0id9787.075.053.350.943.135.37.8411.8table18:w2vcorpusforclassi   cationandsequence16references[1]s.arora,y.li,y.liang,t.ma,anda.risteski.randomwalksoncontextspaces:towardsanexplanationofthemysteriesofsemanticwordembeddings.arxivpreprintarxiv:1502.03520,2015.[2]s.n.ethierandt.g.kurtz.markovprocesses:characterizationandconvergence,volume282.johnwiley&sons,2009.[3]t.inglotandp.majerski.simpleupperandlowerboundsforthemultivariatelaplaceapproximation.journalofapproximationtheory,186:1   11,2014.[4]y.kannai.o   diagonalshorttimeasymptoticsforfundamentalsolutionofdi   usionequation.communicationsinpartialdi   erentialequations,2(8):781   830,1977.[5]v.konakov,e.mammen,etal.edgeworth-typeexpansionsfortransitiondensitiesofmarkovchainsconvergingtodi   usions.bernoulli,11(4):591   641,2005.[6]o.levyandy.goldberg.linguisticregularitiesinsparseandexplicitwordrepresentations.proc.18thconf.comput.nat.lang.learn.(conll2014),2014.[7]o.levyandy.goldberg.neuralwordembeddingasimplicitmatrixfactorization.inadvancesinneuralinformationprocessingsystems,pages2177   2185,2014.[8]o.levy,y.goldberg,andi.dagan.improvingdistributionalsimilaritywithlessonslearnedfromwordembeddings.transactionsoftheassociationforcomputationallinguistics,3:211   225,2015.[9]t.mikolov,k.chen,g.corrado,andj.dean.e   cientestimationofwordrepresentationsinvectorspace.arxivpreprintarxiv:1301.3781,2013.[10]t.mikolov,i.sutskever,k.chen,g.s.corrado,andj.dean.distributedrepresentationsofwordsandphrasesandtheircompositionality.inadvancesinneuralinformationprocessingsystems,pages3111   3119,2013.[11]t.mikolov,w.-t.yih,andg.zweig.linguisticregularitiesincontinuousspacewordrepresentations.inhlt-naacl,pages746   751,2013.[12]g.millerandc.fellbaum.id138:anelectroniclexicaldatabase,1998.[13]s.molchanov.di   usionprocessesandriemanniangeometry.russianmathematicalsurveys,30(1):1,1975.[14]j.pennington,r.socher,andc.d.manning.glove:globalvectorsforwordrepresentation.proceedingsoftheempiricialmethodsinnaturallanguageprocessing(emnlp2014),12,2014.[15]r.sibson.studiesintherobustnessofmultidimensionalscaling:perturbationalanalysisofclassicalscaling.journaloftheroyalstatisticalsociety.seriesb(methodological),pages217   229,1979.[16]s.a.stepin.parametrix,heatkernelasymptotics,andregularizedtraceofthedi   usionsemigroup.proceedingsofthesteklovinstituteofmathematics,271(1):228   245,2010.[17]d.w.stroockands.s.varadhan.di   usionprocesseswithboundaryconditions.communicationsonpureandappliedmathematics,24(2):147   225,1971.[18]p.d.turneyandm.l.littman.corpus-basedlearningofanalogiesandsemanticrelations.machinelearning,60(1-3):251   278,2005.17