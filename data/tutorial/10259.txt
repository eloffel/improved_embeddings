6
1
0
2

 
r
a

m
3

 

 
 
]
l
c
.
s
c
[
 
 

2
v
9
1
2
6
0

.

1
1
5
1
:
v
i
x
r
a

knowledge base population using semantic label propagation

lucas sterckx, thomas demeester, johannes deleu, chris develder

ghent university     iminds

technologiepark zwijnaarde 15, be-9052 ghent, belgium

abstract

a crucial aspect of a knowledge base population system that extracts new facts from text corpora, is the
generation of training data for its relation extractors. in this paper, we present a method that maximizes
the effectiveness of newly trained relation extractors at a minimal annotation cost. manual labeling can
be signi   cantly reduced by distant supervision (ds), which is a method to construct training data auto-
matically by aligning a large text corpus with an existing knowledge base of known facts. for example, all
sentences mentioning both    barack obama    and    us    may serve as positive training instances for the relation
born in(subject,object). however, distant supervision typically results in a highly noisy training set: many
training sentences containing the known entity pairs do not really express the intended relation. we explore
the idea of combining ds with (partial) human supervision to eliminate that noise. this idea is not novel
per se, but our key contributions are: (i) a novel method of    ltering the ds training set based on labeling
shortest dependency paths, (sdps), and (ii) the semantic label propagation (slp) model. we propose
to combine ds with minimal manual human supervision by annotating features (in particular sdps) rather
than (potential) relation instances. such so-called feature labeling is adopted to eliminate noise from the
large and noisy initial training set, resulting in a signi   cant increase of precision (at the expense of recall).
we further improve on this approach by introducing the semantic label propagation (slp) method, which
uses the similarity between low-dimensional representations of candidate training instances, to extend the
(   ltered) training set in order to increase recall while maintaining high precision.

our proposed strategy for generating training data is studied and evaluated on an established test collec-
tion designed for knowledge base population (kbp) tasks from the tac kbp english slot    lling task. the
experimental results show that the slp strategy leads to substantial performance gains when compared to

   corresponding author
email address: lucas.sterckx@intec.ugent.be (lucas sterckx)

preprint submitted to knowledge based systems

october 3, 2018

existing approaches, while requiring an almost negligible human annotation effort.

keywords: id36, knowledge base population, distant supervision, active learning,
semi-supervised learning

1. introduction

in recent years we have seen signi   cant advances in the creation of large-scale knowledge bases (kbs),
databases containing millions of facts about persons, organizations, events, products, etc. examples include
wikipedia-based kbs (e.g., yago [1], dbpedia [2], and freebase [3]), kbs generated from web documents
(e.g., nell [4], prospera[5]), or id10 approaches (e.g., textrunner [6], pris-
matic [7]). other knowledge bases like conceptnet [8] or senticnet [9] collect conceptual information
conveyed by natural language and store them in a form which makes them easily accessible to machines.
besides the academic projects, several commercial projects were initiated by major corporations like mi-
crosoft (satori1), google (id13 [10]), facebook2, walmart [11] and others. this is driven by a
wide variety of applications for which kbs are increasingly found to be essential, e.g., digital assistants, or
for enhancing search engine results with semantic search information.

because kbs are often manually constructed, they tend to be incomplete. for example, 78.5% of per-
sons in freebase have no known nationality [12]. to complete a kb, we need a knowledge base population
(kbp) system that extracts information from various sources, of which a large fraction comprises unstruc-
tured written text items [10]. a vital component of a kbp system is a relation extractor to populate a target
   eld of the kb with facts extracted from natural language. id36 (re) is the task of assigning
a semantic relationship between (pairs of) entities in text.

there are two categories of re systems: (i) closed-schema ie systems extract relations from a    xed
schema or for a closed set of relations, while (ii) open domain ie systems extract relations de   ned by
arbitrary phrases between arguments. we focus on the completion of kbs with a    xed schema, i.e., closed
ie systems.

effective approaches for closed schema re apply some form of supervised or semi-supervised learn-
ing [13, 14, 15, 16, 17, 18] and generally follow three steps: (i) sentences expressing relations are trans-
formed to a data representation, e.g., vectors are constructed to be used in feature-based methods, (ii) a

1https://blogs.bing.com/search/2013/03/21/understand-your-world-with-bing
2http://www.insidefacebook.com/2013/01/14/

2

relation (r)

born in

spouse

. . .

entity 2 (e2)

knowledge base
entity 1 (e1)
barrack obama u.s.
barrack obama michelle
. . .

. . .

mentions in free text
barack was born in honolulu, hawaii, u.s.
barrack obama ended u.s. military involvement in the iraq war.
michelle and barack are visiting cuba.
barack and his wife michelle are meeting with xi jinpeng

true +?

 

 

 

 

figure 1: illustration of the distant supervision paradigm and errors

binary or multi-class classi   er is trained from positive and negative instances, and (iii) the model is then
applied to new or unseen instances. to review the evolution of these and other natural language processing
(nlp) techniques, readers can refer to the article by cambria and white [19].

supervised systems are limited by the availability of expensive training data. to counter this problem,
the technique of iterative id64 has been proposed [20, 21], in which an initial seed set of known
facts is used to learn patterns, which in turn are used to learn new facts and incrementally extend the training
set. these id64 approaches suffer from semantic drift and are highly dependent on the initial seed
set.

when an existing kb is available, a much larger set of known facts can be used to bootstrap training
data, a procedure known as distant supervision (ds). ds automatically labels its own training data by
heuristically aligning facts from a kb with an unlabeled corpus. the kb, written as d, can be seen as a
collection of relational tables r(e1, e2), in which r     r (r is the set of relation labels), and < e1, e2 > is a
pair of entities that are known to have relation r. the corpus is written as c.

the intuition underlying ds is that any sentence in c which mentions the same pair of entities (e1 and
e2), expresses a particular relationship   r between them, which most likely corresponds to the known fact
from the kb,   r(e1, e2) = r(e1, e2), and thus forms a positive training example for an extractor of relation r.
ds has been successfully applied in many id36 tasks [22, 23] as it allows for the creation of
large training sets with little or no human effort.

equally apparent from the above intuition, however, is the danger of    nding incorrect examples for
the intended relation. the heuristic of accepting each co-occurrence of the entity pair < e1, e2 > as a
positive training item because of the kb entry r(e1, e2), is known to generate noisy training data or false
positives [24], i.e., two entities co-occurring in text are not guaranteed to express the same relation as the

3

   eld in the kb they were generated from. the same goes for the generation of negative examples: training
data consisting of facts missing from the kb are not guaranteed to be false since a kb in practice is highly
incomplete. an illustration of ds generating noisy training data is shown in figure 1.

several strategies have been proposed to reduce this noise. the most prominent is that of latent variable
models of the distantly supervised data that make the assumption that a known fact is expressed at least
once in the corpus [24, 25, 26]. these methods are cumbersome to train and are sensitive to initialization
parameters of the model.

an active research direction is the combination of ds with partial supervision, as was proposed in several
recent works which differ in the way this supervision is chosen and included. some focus on active learning,
selecting training instances to be labeled according to an uncertainty criterion [27, 22], while others focus
on annotations of surface patterns and de   ne rules or guidelines in a semi-supervised learning setting [28].
existing methods for fusion of distant and partial supervision require thousands of annotations and hours
of manual labor for minor improvements (4% in f1 for 23,425 annotations [27] or 2,500 labeled sentences
indicating true positives for a 3.9% gain in f1 [28]). in this work we start from a distantly supervised
training set and show that, using minimal supervision, we can reduce noise in the training data and boost
extraction performance. we will demonstrate how only a couple of minutes of annotation time per relation
suf   ces to strongly reduce noise, and obtain signi   cant improvements in precision and recall of the extracted
relations.

we de   ne the following research questions:

rq 1. how can we add supervision most effectively to reduce noise and optimize relation extractors?

rq 2. can we combine semi-supervised learning and dimension reduction techniques to further enhance
the quality of the training data and obtain state-of-the-art results using minimal manual supervision?

with the following contributions, we provide answers to these research questions:

1. in answer to rq 1, we demonstrate the effectiveness and ef   ciency of    ltering training data based on
high-precision trigger patterns. these are obtained by training initial weak classi   ers and manually
labeling a small amount of features chosen according to an active learning criterion.

2. we tackle rq 2 by proposing a semi-supervised learning technique that allows extending an initial
set of high-quality training instances with weakly supervised candidate training items by measuring
their similarity in a low-dimensional semantic vector space. this technique is called semantic label
propagation.

4

3. we evaluate our methodology on test data from the english slot filling (esf) task of the knowledge
base population at the 2014 text analysis conference (tac). we compare different methods by using
them in an existing kbp system. our relation extractors attain state-of-the-art effectiveness (a micro
averaged f1 value of 36%) while relying on a very low manual annotation cost (i.e., 5 minutes per
relation).

in section 2 we give an overview of existing supervised and semi-supervised re methods and highlight
their remaining shortcomings. section 3 describes our proposed methodology, with some details on the ds
starting point (section 3.1), the manual feature annotation approach (section 3.2), and the introduction of the
semantic label propagation method (section 3.3). the experimental results are given in section 4, followed
by our conclusions in section 5.

2. related work

the key idea of our proposed approach is to combine distant supervision with a minimal amount of
supervision, i.e., requiring as few (feature) annotations as possible. thus, our work is to be framed in the
context of supervised and semi-supervised id36 (re), and related to approaches designed to
minimize the annotation cost, e.g., active learning. furthermore, we use compact vector representations
carrying semantics, i.e., so-called id27s. below, we therefore brie   y summarize related work in
the areas of (i) supervised re, (ii) semi-supervised re, (iii) active learning, and (iv) id27s.

2.1. supervised id36

supervised re methods rely on training data in the form of sentences tagged with a label indicating the
presence or absence of the considered relation. there are three broad classes of supervised re: (i) methods
based on manual feature engineering, (ii) kernel based methods, and (iii) convolutional neural nets.

methods based on feature-engineering [16, 29] extract a rich list of manually designed structural, lexi-
cal, syntactic and semantic features to represent the given relation mentions. these features are cues for the
decision whether the relation is present or not. afterwards a classi   er is trained on positive and negative
examples. in contrast, kernel based methods [30, 31, 18] represent each relation mention as an object such
as an augmented token sequence or a parse tree, and use a carefully designed id81, e.g., sub-
sequence kernel or a convolution tree kernel, to calculate their similarity with test patterns. these objects
are usually augmented with extra features such as semantic information. with the recent success of deep

5

neural networks in nlp, convolutional neural networks (id98s) have emerged as effective relation extrac-
tors [32, 33]. id98s avoid the need for preprocessing and feature design by transforming tokens into dense
vectors using embeddings of words. lexical and sentence-level features are extracted using deep neural nets.
finally, the features are fed into a soft-max classi   er to predict the relationship between two marked nouns.
supervised approaches all share the need for training data, which is expensive to obtain. two common
methods have emerged for the generation of large quantities of training data, and both require an initial set of
known instances. when this number is initially small, the technique of id64 is used. when a very
large number of instances is available from an existing knowledge base, distant supervision is the preferred
technique. both are brie   y discussed below.

2.1.1. id64 models for id36

when a limited set of labeled instances is available, id64 methods have proven to be effective
methods to generate high-precision relation patterns [20, 21, 34, 35]. the objective of id64 is
to expand an initial    seed    set of instances with new relationship instances. documents are scanned for
entities from the seed instances and linguistic patterns connecting them are extracted. patterns are then
ranked according to coverage (recall) and low error rate (precision). using the top scoring patterns, new
seed instances are extracted and the cycle is repeated.

an important step in id64 methods is the calculation of similarity between new patterns and
the ones in the seed set. this measure decides whether a new pattern is relation oriented or not, based on
the existing set. systems use measures based on exact matches [34], cosine-similarity [20] or kernels [35].
a fundamental problem of these methods is semantic drift [36, 37]: id64, after several iterations,
deviates from the semantics of the seed relationship and extracts unrelated instances which in turn generate
faulty patterns. this phenomenon gets worse with the number of iterations of the id64 process.

recently, batista et al. [38] proposed the use of id27s for capturing semantic similarity
between patterns. contexts are modeled using linear combinations of the id27s and similarity
is measured in the resulting space. this approach has shown to reduce semantic drift compared to previous
similarity measures.

2.1.2. distant supervision

distant supervision (ds) was    rst proposed in [39], where labeled data was generated by aligning in-
stances from the yeast protein database into research articles to train an extractor. this approach was later
applied for training of relation extractors between entities in [12].

6

automatically gathering training data with ds is governed by the assumption that all sentences con-
taining both entities engaged in a reference instance of a particular relation, represent that relation. many
methods have been proposed to reduce the noise in training sets from ds. in a series of works the labels of ds
data are seen as latent variables. riedel et al. [24] relaxed the strong all sentences-assumption and relaxed
it to an at-least-one-sentence-assumption, creating a multi-instance learner. hoffman et al. [40] modi   ed
this model by allowing entity pairs to express multiple relations, resulting in a multi-instance multi-label
setting (miml-re). surdeanu et al. [26] further extended this approach and included a secondary classi   er,
which jointly modeled all the sentences in texts and all labels in knowledge bases for a given entity pair.

other methods apply heuristics [41], model the training data as a generative process [42, 43] or use a
low-rank representation of the feature-label matrix to exploit the underlying semantic correlated information.

2.2. semi-supervised id36

semi-supervised learning is situated between supervised and unsupervised learning.

in addition to
unlabeled data, algorithms are provided with some supervised information. the training data comprises
labeled instances xl = (x1 . . . xl) for which labels yl = (y1 . . . yl) are provided, and typically a large set of
unlabeled ones xu = (x1 . . . xu).

semi-supervised techniques have been applied to re on several occasions. chen et al. [44] apply label
propagation by representing labeled and unlabeled examples as nodes and their similarities as the weights
of edges in a graph. in the classi   cation process, the labels of unlabeled examples are then propagated from
the labeled to unlabeled instances according to similarity. experimental results demonstrate that this graph-
based algorithm can outperform id166 in terms of f1 when very few labeled examples are available. sun et
al. [17] show that several different word cluster-based features trained on large corpora can compensate for
the sparsity of lexical features and thus improve the re effectiveness.

zhang et al. [45] compare ds and complete supervision as training resources but do not attempt to fuse
them. they observe that distant supervision systems are often recall gated: to improve distant supervision
quality, large input collections are needed. they also report modest improvements by adding crowd-sourced
yes/no votes to the training instances. training instances were selected at random as labeling using active
learning criteria did not affect performance signi   cantly.

angeli et al. [27] show that providing a relatively small number of mention-level annotations can im-
prove the accuracy of miml-re. they introduce an active learning criterion for the selection of instances
incorporating both the uncertainty and the representativeness, and show that the choice of criterion is im-

7

portant. the miml-re model of surdeanu et al. [26] marginally outperforms the mintz++ baseline using
solely distant supervision: initialization of the latent variables using labeled data is needed for larger im-
provements. for this, a total of 10, 000 instances were labeled, resulting in a 3% increase on the micro-f1
.

guided distant supervision, proposed by pershina et al. [28], incorporates labeled patterns and trigger
words to guide miml-re during training. they make use of a labeled dataset from tac kbp to extract
training guidelines, which are intended to generalize across many examples.

2.3. tac kbp english slot filling

the knowledge base population (kbp) shared task is part of the nist text analysis conference and
aims to evaluate different approaches for discovering facts about entities and expansion of knowledge bases.
a selection of entities is distributed among participants for which missing facts need to be extracted from
a given large collection of news articles and internet fora.
important components of these systems are
id183, entity linking and relation extractors. over the years distant supervision has become
a regular feature of effective systems [22, 46]. other approaches use hand-coded rules or are based on
id53 systems [46]. the top performing 2014 kbp esf system [47] uses ds, the manual
labeling of 100, 000 features, and is built on deepdive, a database system allowing users to rapidly construct
sophisticated end-to-end knowledge base population techniques [48]. after initial ds, features are manually
labeled and only pairs associated with labeled features are used as positive examples. this approach has
proven to be very effective but further investigation is needed to reduce the amount of feature labeling.
here, we show how we can strongly reduce this effort while maintaining high precision.

2.4. active learning and feature labeling

active learning is used to reduce the amount of supervision required for effective learning. the most
popular form of active learning is based on iteratively requiring manual labels for the most informative
instances, an approach called uncertainty sampling. in id36, typical approaches include query-
by-committee [27, 49] and cluster-based sampling [50]. while the focus in re has been on labeling relation
instances, alternative methods have been proposed in other tasks in which features (e.g., patterns, or the
occurrence of terms) are labeled as opposed to instances [51, 52], resulting in a higher performance for less
supervision.

getting positive examples for certain relations can be hard, especially when training data is weakly
supervised. standard uncertainty sampling is ineffective in this case: it is likely that a feature or instance has

8

a low certainty score because it does not carry much discriminative information about the classes. assigning
labels to the most certain features has much greater impact on the classi   er and can remove the principle
sources of noise. this approach has been coined as feature certainty [52], and we show that this approach
is especially effective in ds for features that generalize across many training instances.

2.5. id65

the distributional hypothesis [53] states that words that tend to occur in similar contexts are likely
to have similar meanings. representations of words as dense vectors (as opposed to the standard one-hot
vectors), called id27s, exploit this hypothesis and are trained from large amounts of unlabeled
data on predicting their context. representations for words will be similar to those of related words, al-
lowing the model to generalize better to unseen events. the resulting vector space is also called a vector
model of meaning [54]. common methods for generating very dense, short vectors use dimensionality re-
duction techniques (e.g., singular value decomposition) or neural nets to create so-called id27s.
id27s have proven to be bene   cial for many natural language processing tasks including pos-
tagging, machine translation and id14. common unsupervised id27 algorithms
include id97 [55] and glove [56]. these models are inspired by neural networks and are trained using
stochastic gradient training.

while much research has been directed at ways of constructing distributional representations of indi-
vidual words, for example co-occurrence based representations and id27s, there has been far
less consensus regarding the representation of larger constructions such as phrases and sentences from these
representations. blacoe et al. [57] show that, for short phrases, a simple composition like addition or mul-
tiplication of the distributional word representations is competitive with more complex supervised models
such as id56s (id56s).

3. labeling strategy for noise reduction

in this section we introduce our strategy to combine distantly supervised training data with minimal
amounts of supervision. shortly stated, we designed our labeling strategy such as to minimize the amount of
false positive instances or noise while maintaining the diversity of relation expressions generated by ds.

we perform a highly selective form of noise reduction starting from a fully distantly supervised relation
extractor, described in section 3.1, and use the feature weights of this initial extractor to guide manual su-
pervision in the feature space. various questions arise from this. when do we over-constrain the original

9

training set generated by distant supervision? what is the trade-of between the application of distant su-
pervision with highly diverse labeled instances, and the constraining approach of labeling features, with a
highly accurate yet restricted set of training data? this is discussed in detail in sections 3.2 and 3.3.

more concretely, our approach is depicted in figure 2, and comprises the following steps:

(1) an existing kb is used to generate distantly supervised training instances by matching its facts with
sentences from a large text corpus. we discuss the characteristics of this weakly labeled training set
as well as the features extracted from each sentence (see section 3.1).

(2) an initial relation extractor is trained using the noisy training data generated in step (1).

(3) con   dent positive features learned by this initial classi   er are presented to an annotator with knowl-
edge of the semantics of the relation and labeled as true positive or false positive. feature con   dence
is quanti   ed with an active learning criterion.

(4) the collection of training instances is    ltered according to the labeled features and a second classi   er

is trained. this framework, in which we combine supervision and ds, is explained in section 3.2.

(5) in a semi-supervised step, the    ltered distantly supervised training data is added to training data by
propagating labels from labeled features to distantly supervised instances based on similarity in a
semantic vector space of reduced dimension. the technique is presented in 3.3 as semantic label
propagation.

(6) a    nal relation extractor is trained from the augmented training set. we evaluate and discuss results

of the proposed techniques in section 4.

3.1. distantly supervised training data

the english gigaword corpus [58] is used as unlabeled text collection to generate relation mentions.
the corpus consists of 1.8 million news articles published between january 1987 and june 2007. articles
are    rst preprocessed using different components of the stanford corenlp toolkit [59], including sentence
segmentation, tokenizing, pos-tagging, id39, and id91 noun phrases which refer
to the same entity.

as kb we use a snapshot of freebase (now wikidata) from may 2013. the relation schema of freebase
is mapped to that used for evaluation, the nist tac kbp esf task, which de   nes 41 relations, including

10

figure 2: work   ow overview. note that only step (3) involves human annotation effort.

25 relations with a person as subject entity and 16 with organizations as subject. 26 relations require objects
or    llers that are themselves named entities (e.g., scranton as place of birth of joe biden), whereas others
require string-values (e.g., profession (senator, teacher, . . . ), cause of death (cancer, car accident,. . . )).

we perform weak entity linking between freebase entities and textual mentions using simple string
matching. we reduce the effect of faulty entity links by thresholding the amount of training data per subject
entity [60]. most frequently occurring entities from the training data (e.g., john smith, robert johnson,
. . . ) are often most ambiguous, hard to link to a kb and thus result in noisy training data. thresholding
the amount of training data per entity also prevents the classi   er from over   tting on several, popular enti-
ties. the reason for that is our observation that training data was initially skewed towards several entities
frequently occurring in news articles, like barack obama or the united nations, resulting in over-classifying
professions of persons as president or seeing countries as members of the organization.

for each generated pair of mentions, we compute various lexical, syntactic and semantic features. table 1
shows an overview of all the features applied for the relation classi   cation. we use these features in a
binary id28 classi   er. features are illustrated for an example relation-instance <ray young,
general motors> and the sentence    ray young, the chief    nancial of   cer of general motors, said gm could
not bail out delphi   .

for each relation ri, we generate a set of (noisy) positive examples denoted as r+
i = { (m1, m2) | ri(e1, e2)     el(e1, m1)     el(e2, m2) }
r+

i and de   ned as

with e1 and e2 being subject and object entities from the kb and el(e1, m1) being the entity e1 linked to

11

(1) distantsupervisionknowledgebasedocumentsr(cid:31) <x(cid:31)(cid:31), y(cid:31)(cid:31)>r(cid:31) <x(cid:31)(cid:30), y(cid:31)(cid:30)>...r(cid:30) <x(cid:30)(cid:31), y(cid:30)(cid:31)>r(cid:30) <x(cid:30)(cid:30), y(cid:30)(cid:30)>...training set(labeled relation instances)(4) (cid:31)ltering based on       labeled featuresrelation extractorfor relation r(cid:31)relation extractorfor relation r(cid:30)...(2), (6)training(3) featureannotation(5) label propagation in       semantic feature spacetable 1: overview of different features used for classi   cation for the sentence    ray young, the chief    nan-
cial of   cer of general motors, said gm could not bail out delphi   .

feature

description

example feature value

dependency

tree

shortest path connecting the two names in
the id33 tree coupled with

entity types of the two names

the head word for name one

the head word for name two

whether 1dh is the same as e2dh

the dependent word for name one

the dependent word for name two

person   appos   of   cer

    prep of    organization

said

of   cer

false

of   cer

nil

the middle token sequence pattern

, the chief    nancial of   cer of

number of tokens between the two names

first token in between

last token in between

other tokens in between

first token before the    rst name

second token before the    rst name

first token after the second name

second token after the second name

6

,

of

{the, chief,    nancial, of   cer}

nil

nil

,

said

string of name one

string of name two

ray young

general motors

conjunction of e1 and e2

ray young   general motors

entity type of name one

person

entity type of name two

organization

conjunction of et1 and et2

person   organization

title in between

1 if name one comes before name two;

2 otherwise.

pos-tags on the path connecting

the two names

12

true

1

nnp   dt   jj   jj
   nn   in   nnp

token
sequence
features

entity
features

semantic
feature

order
feature

parse tree

mention m1 in the text. as in previous work [40, 61], we impose the constraint that both entity mentions
(m1, m2)     r+
i are contained in the same sentence. to generate negative examples for each relation, we
sample instances from co-occurring entities for which the relation is not present in the kb.

we measured the amount of noise, i.e., false positives, in the training set of positive ds instances,
for a selection of 15 relations: we manually veri   ed 2,000 randomly chosen instances (that ds found as
supposedly positive examples) for each of these relations. table 2 shows the percentage of true positives
among these 200 instances for each of these relations, which strongly varies among relations, ranging from
10% to 90%.

table 2: training data. fractions of true positives are estimated from the training data by manually labeling
a sample of 2,000 instances per relation that ds indicated as positive examples.

relation

per:title

org:top members employees

per:employee or member of

per:age

per:origin

per:countries of residence

per:charges

per:cities of residence

per:cause of death

per:spouse

per:city of death

org:country of headquarters

per:country of death

org:city of headquarters

org:founded by

estimated fraction
of true positives

positively

labeled sdps

remaining training
data after filtering

initial number of

true positives

26.2%

16.7%

16.5%

52.2%

11.9%

8.4%

21.5%

7.4%

29.4%

12.1%

5.6%

13.4%

16.5%

42.7%

22.7%

369,079

93,900

260,785

58,980

1,555,478

493,064

17,639

370,153

31,386

172,874

125,333

13,435

128,773

36,238

318,991

85.1%

71.7%

87.8%

62.4%

85.2%

55.6%

59.4%

11.7%

51.9%

63.2%

19.9%

10.8%

77.6%

56.5%

13.3%

157

236

256

79

116

65

122

96

97

124

92

92

70

67

85

13

relation: per:cities of residence
knowledge base entry: < sherman, greenwich >
dependency tree:

prep in

nsubj

amod

prt

case

det

nn

prep of

case

sherman

,

63

,

grew

up

in

a middle-class

neighborhood

of

greenwich

.

shortest dependency path:

person nsubj                grew

prep in

                   neighborhood

prep of

                   location

figure 3: dependency tree feature

3.2. labeling high con   dence shortest dependency paths

this section describes the manual feature labeling step that allows transforming a full ds training set
into a strongly reduced yet highly accurate training set, based on manual feature labeling. we focus on
a particular kind of feature, i.e., a relation   s shortests dependency path (sdp). dependency paths have
empirically been proven to be very informative for id36,: their capability in capturing a lot of
information is evidenced by a systematic comparison in effectiveness of different kernel methods [62] or as
features in feature-based systems [16]. this was originally proposed by bunescu et al. [18], who claimed
that the relation expressed by a sentence is often captured in the shortest path connecting the entities in the
dependency graph. figure 3 shows an example of an sdp for a sentence expressing a relation between a
person and a city of residence.

as shown in table 2, the fraction of false positive items among all weakly supervised instances can
be very large. labeling features based on the standard active learning approach of uncertainty sampling is
ineffective in our case since it is likely that a feature or instance has a low certainty score simply because
not much discriminative information about the classes is carried. annotating many such instances would
be a waste of effort. assigning labels to the most certain features has much greater impact on the classi   er
and can remove the principal sources of noise. this approach is called feature certainty sampling [52]. it

14

t
e
s

g
n
i
n
i
a
r
t
n
i

y
c
n
e
u
q
e
r
f

100
10   1
10   2
10   3
10   4
10   5

top member
date of death
employee of

10   2

10   3
10   1
normalized sorted rank

100

e
c
n
e
d
   
n
o
c

6

4

2

schools attended

top member
founded by

200

600
400
sorted rank

800 1,000

(a)

(b)

figure 4: illustration of frequency and con   dence of dependency paths for example relations. (a) occurrence
frequency, ranked from highest to lowest, and (b) con   dence c of dependency paths (eq. 1), ranked from
highest to lowest, with indication of true positives.

is intuitively an attractive method, as the goal is to reduce the most in   uential sources of noise as fast as
possible. for example for the relation founded by there are many persons that founded the company who are
also top members, leading to instances that we wish to remove when cleaning up the training data for the
relation founded by.

sdps offer all the information needed to assess the relationship validity of the training instances, are
easily labeled, and generalize over a considerable fraction of the training set as opposed to many of the
feature-unigrams which remain ambiguous in many cases. we implement the feature certainty idea by
ranking sdp features according to the odds that when a particular sdp occurs, it corresponds to a valid
relation instance. this corresponds to ranking by the following quantity, which we call the considered
sdp   s con   dence

con   dence(sdp) =

.

(1)

p (+|sdp)
p (   |sdp)

it can be directly estimated from the original distant supervision training set, based on each sdp feature   s
(smoothed) occurrence frequencies among the positive and negative distantly supervised instances. in partic-
ular, p (+|sdp) indicates the spd   s fraction of occurrences among the positive training data and p (   |sdp))
among the negative.

all dependency paths are ranked from most to least con   dent and the top-k are assigned to a human

15

annotator to select the true positive sdps. the annotator is asked to select only the patterns which un-
ambiguously express the relation. that is, a pattern is accepted only if the annotator judges it a suf   cient
condition for that relation. the annotator is provided with several complete sentences containing the depen-
dency path to this cause. when the sdp does not include any verbs, e.g., when entities are both part of the
same noun phrase like    microsoft ceo bill gates   , all words between the subject and object are included
and the complete path is added to the    lter set. in our experiments, we restrict the time of sdp annotations
to a limited effort of 5 minutes for each relation. on average our expert annotator was able to label around
250 sdps per relation this way. the ease of annotating sdps becomes apparent when comparing with an-
notating random relation instances, which they managed to do at a rate of only 100 in the same period of
time. section 4.3 provides further details on the different annotation methodologies for the experiments.

the motivation behind limiting the annotation time per relation to only a few hundred patterns comes
from the following analysis. first of all, a small subset of all different patterns are responsible for the
majority of relation instances in the ds training set. in fact, the sparsity of distantly supervised training data
becomes apparent when extracting all sdps for each fact in the kb in one pass over the corpus. figure 4a
shows the approximately zip   an distribution of the frequency of the dependency paths generated by ds in
the positively labeled training set for several example relations. the abscis shows the rank of dependency
paths for various relations, sorted from most to least frequent, normalized by the total number of paths for
the respective relations (to allow visualization on the same graph). in line with our goal of getting a highly
accurate training set with the largest sources of noise    ltered away at a low annotation cost, we need to
focus on capturing those top most frequent patterns. secondly, we noticed that beyond the    rst few hundred
most con   dent sdps, which take around 5 minutes to annotate, further true positives tend to occur less
frequently. annotating many more sdps would only marginally increase the diversity in the training set,
at a rapidly increasing annotation cost. figure 4b illustrates the occurrence of true positive patterns for
decreasing con   dence scores. for several example relations, the    gure shows the true positive patterns as
markers on the con   dence distribution of the 1, 000 most con   dent sdps.

finally, using the manually selected set of sdps, the complete training set is    ltered by enforcing that
one of these sdps be present in the feature set of the instance. we include all mention pairs associated with
that feature as positive examples of the considered relation. the classi   er trained on the resulting training
set is intuitively of high precision but doesn   t generalize well to unseen phrase constructions. note that the
classi   er is quite different from a regular pattern based relation extractor. although all training instances
satisfy at least one of the accepted sdps, the classi   er itself is trained on a set of features including, but not

16

table 3: examples of top-ranked patterns

relation

top members employees

children

city of birth

schools attended

(org:)parents

prep of

                   org
appos                org

prep of

                   per
                   per-1

prep of

prep of

                   per-2
                   per-1

prep of

prep in

                   loc
                   loc

top sdp
per appos                executive
per appos                chairman
org nn          f ounder
per-2 appos                son
per-1 appos                f ather
per-2 nn          grandson
per rcmod                born
per nsubj                mayor
per appos                historian
per nsubj                graduated
per dep          student
per appos                teacher
org-2 appos                subsidiary
org-1 appos                division
org-2 prep to                shareholder

prep of

prep at

prep at

                   org
                   org

prep f rom

                         loc
                         org

prep f rom

prep of

                   org-1

prep of

                   org-2
dep          org-1

assessment

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

restricted to, these sdps (see table 1). still, most of the bene   ts of ds are lost by having the selection of
training instances governed by a limited set of patterns.

the fourth column of table 2 lists the fraction of training data remaining after    ltering out all patterns
apart from those classi   ed as indicative of the relation at hand. the amount of training data remaining after
this    ltering step strongly depends on the speci   c relation, varying from 5% to more than half of the original
training set. yet on the whole, the    ltering results in a strong reduction of the purely ds-based training
data, often removing much more than the actual fraction of noise (column 2). for example, for the relation
per:employee or member of, we note only 100%     87.8% = 12.2% false positives, but the manual    ltering
leads to discarding 83.5% of the ds instances.

the strategy described in the previous paragraphs is related to the guidelines strategy from pershina et
al. [28] (without the miml model) in labeling features, but it differs in some essential aspects. instead of
needing a fully annotated corpus to do so, we rank and label features entirely based on distant supervision.

17

labeling features based on a fully labeled set ignores the variety of ds and risks being biased towards the
smaller set of labeled instances. also, no active learning criteria were applied when choosing which features
to label, making the process even more ef   cient.

3.3. noise reduction using semantic label propagation

if we strictly follow the approach proposed in section 3.2 and only retain ds training instances that
satisfy an accepted sdp, an important advantage of ds is lost, namely its potential of reaching high recall.
if we limit the feature annotation effort, we risk losing highly valuable sdps. to counteract this effect,
we introduce a second (re)labeling stage, adopting a semi-supervised learning (ssl) strategy to expand the
training set. this is done by again adding some instances from the set of previously discarded ds instances
with sdps not matching any of the manually labeled patterns. we rely on the basic ssl approach of self-
training by propagating labels from known instances to the nearest neighboring unlabeled instances. this
algorithm requires a method of determining the distance between labeled and unlabeled instances. dangers
of self-training include the failure to expand beyond the initial training data or the introduction of errors into
the labeled data. in order to avoid an overly strong focus on the    ltered training data, we use low-dimensional
vector representations of words, also called id27s.

id27s allow for a relaxed semantic matching between the labeled seed patterns and the
remaining weakly labeled patterns. as shown by sterckx et al. [50], representing small phrases by summing
each individual word   s embedding leads to semantic representations of small phrases that are meaningful for
the goal of id36.

we represent each relation instance by a single vector by    rst removing stop-words and averaging the

embeddings of the words on the dependency path. for example, consider the sentence:

geagea on friday for the    rst time addressed the court judging him for murder charges.

which has the following sdp,

per nsubj                addressed

dobj             court vmod             judging

prep f or

                   charges nn          criminal charge

its low-dimensional representation (cid:126)c is hence generated as

(cid:126)c =

e(   addressed   ) + e(   court   ) + e(   judging   ) + e(   charges   )

4

18

,

(2)

with e(x) the id27 of word x. the similarity between a labeled pattern (cid:126)ct and a weakly labeled
pattern (cid:126)cds is then measured using cosine similarity between the vector representations.

sim( (cid:126)ct, (cid:126)cds) =

(cid:126)ct. (cid:126)cds
| (cid:126)ct|.| (cid:126)cds|

(3)

in the special case that no verbs occur between two entities, all the words between the two entities are used
to build the representations for the context vector.

using these low-dimensional continuous representations of patterns, we can calculate similarities be-
tween longer, less frequently occurring patterns in the training data and the patterns from the initial seed
set which are the most frequently occurring ones. we can now increase recall by adding similar but less
frequent patterns.

more speci   cally, we calculate the similarity of the average vector of the labeled patterns (as in the
rocchio classi   er type of self-training) with each of the remaining patterns in the ds set and extend the
training data with the patterns that have a suf   ciently high similarity with the labeled ones. we call this
technique semantic label propagation.

4. experimental results

4.1. testing methodology

we evaluate the relation extractors in the context of an existing knowledge base population system
[60, 63] using the nist tac kbp english slot filling (esf) evaluation from 2012 to 2014. we choose
for this evaluation because of the diversity and dif   culty of entities in the queries. in the end-to-end esf
framework, the input to the system is a given entity (the    query   ), a set of relations, and a set of articles. the
output is a set of slot    lls, where each slot    ll is a triple consisting of two entities (including the query entity)
and a relation (from among the given relations) predicted to hold among these entities.

4.2. knowledge base population system

systems participating in the tac kbp esf need to handle each task of    lling missing slots in a kb.
participants are only provided with one surface-text occurrence of each query entity in a large collection of
text provided by the organizers. this means that an information retrieval component is needed to provide the
relation extractor with sentences containing candidate answers. our system performs id183 using
freebase aliases and wikipedia pages. each document containing one of the aliases is parsed and named

19

entities are automatically detected. persons, organizations, and locations are recognized, and locations are
further categorized as cities, states, or countries. non-entity    llers like titles or charges are tagged using lists
and table-lookups. for further details of the kbp system we refer to [60, 63].

4.3. methodologies for supervision

in this section we detail the different procedures for human supervision. supervision is obtained in two
forms: by labeling shortest dependency paths (sdps) and by labeling single training instances indicated
as positive by distant supervision, as either true positives or as false positives (noise). after a background
corpus is linked with a knowledge base, phrases containing facts are stored in a database for further feature
extraction, post processing, and calculation of feature con   dence values.

our annotators for the labeling of single training instances were undergraduate students from different
backgrounds with little or no experience in machine learning or natural language processing. first, they
were briefed on the semantics of the relation to be extracted using the of   cial tac kbp guidelines. they
were then presented with training instances, i.e., phrases from the database. each instance was shown with
entity and subject highlighted and colored. the average time needed to annotate a batch of 2,000 instances
was three hours, corresponding to about 5 seconds per instance, including the time needed to read and judge
the sentence. as this procedure was relatively expensive (annotators were paid $15 per hour), only the 15
most frequent relations, strongly in   uencing the optimal micro-f1 score shown in table 2, were selected.
other relations received between 200 and 1,000 annotations each.

in contrast, the time for annotation of the sdps was limited to merely 5 minutes per relation, during
which, on average, 200 sdps were judged. sdps were presented in a spreadsheet as a list, and true positives
were labeled using a simple checkbox. all sdp annotations were done by a single expert annotator. to
measure the degree of expertise needed for these annotations, we also assigned a novice annotator (student)
with the same task. we measured annotator agreement and time needed for a selection of the relations. for
this experiment the student was explained the meaning of dependency paths and the aim of choosing valid
sdps. several lists of sdps that the expert was able to label in 5 minutes were presented to the student.
for the    rst two relations the student needed more than 10 minutes to label, but for the subsequent relations,
annotation time dropped to 5 minutes per relation, equivalent to the time needed by an expert annotator. we
measured inter annotator agreement using cohen   s kappa coef   cient   . inter-annotator agreement between
student and expert was initially moderate (   = 0.65) and increased after the student completed lists of sdps
for two relations (   varies between 0.85 and 0.95), indicating a very good agreement.

20

4.4. pattern-based restriction vs. similarity-based extension

as table 2 shows, applying the manually annotated features as described in section 3.2 often leads to a
drastic reduction of training instances, compared to the original weakly labeled training set. using similarity
metrics described in section 3.3, we again add weakly supervised training data to the    ltered data. an
important question is therefore how to optimally combine initial reduction with subsequent expanding of the
training instances. intuitively, one would expect a high-precision-low-recall effect in the extreme case of
adding no similar patterns, and a low-precision-high-recall effect when adding all weakly labeled patterns,
both leading to a sub-optimal f1 measure. on the other hand, adding a limited amount of similar patterns
may increase recall without harming precision too much. in this section, we investigate for a selection of
relations, how the quality of the training set depends on the fraction of top similar patterns to extend it with.
in our experimental setup, we start from the training set that only contains the n   ltered instances that
match the manually labeled patterns, gradually adding weakly labeled data, and each time training binary
classi   ers on the corresponding training set. we chose to let the additional data grow exponentially, which
allows studying the effect of adding few extra instances initially, but extending towards the full weakly
supervised training set of size nds in a limited number of cases. more speci   cally, in k experiments of
adding additional instances, the intermediate training set size nk at step k is given by

(cid:18) nds

(cid:19)k/k

n   ltered

nk = n   ltered.

(4)

figure 5 illustrates how an initial training set containing only 5% of the amount of instances from the

full weakly labeled training set, is increased in k = 10 consecutive experiments.

apart from studying the addition of varying amounts of similar patterns, in this section we also inves-
tigate the in   uence of the type of similarity measure used. in section 3.2 we suggested the use of word
embeddings, but is there a difference between different types of embeddings? would embeddings work bet-
ter than traditional dimension reduction techniques? and would such techniques indeed perform better than
the original one-hot vector representations? these questions can be answered by considering several similar-
ity measures. as a classical baseline, we represent sdps using the average one-hot or bag-of-words (bow)
representations of the words contained in the sdps. we also transform the set of one-hot representations us-
ing singular value decomposition (svd) [64]    tted on the complete training set. for representations using
the summed average of id27s described in section 3.3, we use two sets of pre-trained id97

21

)

%

(
d
e
d
u
l
c
n
i

t
e
s
g
n
i
n
i
a
r
t
s
d

f
o

n
o
i
t
c
a
r
f

100

80

60

40

20

0

0

sdp

1

2

3

4

6

7

8

9

5
k

10
ds

figure 5: example of the proposed sampling strategy for training set sizes, with nf iltered = 0.05nds, and
in k = 10 steps.

embeddings1 (trained on news text) and glove embeddings2 (trained on wikipedia text).

figure 6 shows the effect of adding different amounts of weakly labeled data, for different values of k
as in eq. 4 (with k = 10 steps) and for similarity measures based on the different types of representations
described above. six frequently occurring relations were selected such that they give an idea of the various
forms of behavior that we observed during our investigation of all extracted relations. the chosen effec-
tiveness measure is the optimal f1 value of classi   cation on a development set, consisting of training data
from 2012 and 2013. (in the next section we will evaluate on a held-out test set, which consists of queries
from the 2014 tac esf task, whereby the optimal value of k and type of dimension reduction is selected
based on the development set.) also shown are standard deviations on these optimal f1 -values, obtained by
resampling different positive and negative instances for training the classi   er.

several insights can be gained from fig. 6:

    sdps vs full ds training set: we observe that the effect of expanding the initial training set is strongly
dependent on the speci   c relation and the quality of the initial training data.
in many cases training
data    ltered using only highly con   dent sdps (k = 0) generates a better relation extractor than pure ds
(k = k). this holds for all shown relations, except for the age relation. we have to be aware that wrongly
annotating an important pattern, or by chance missing any in the top most con   dent ones, can strongly
reduce recall when only using the accepted sdps. adding even a small amount of similar patterns may

1https://code.google.com/p/id97/
2http://nlp.stanford.edu/projects/glove/

22

figure 6: illustration of the behavior of semantic label propagation for different id84
techniques, and different amounts of added weakly labeled data, quanti   ed by k (as in eq. 4), with k = 10.
k = 0 corresponds to only accepting manually    ltered sdps, and k = 10 corresponds to using all weakly
labeled (ds) data for training.

23

hence result in a steep increase in effectiveness, such as for k = 1 in the age and country of headquarters
relations.

    effect of semantic label propagation: when relaxing the    ltering (i.e., increasing k) by adding unlabeled
data, the optimal f1 tends to increase until a certain point, and then again drops towards the behavior of
a fully ds training set, because the quality or similarity of the added training data declines and too many
false positives are re-introduced. the threshold on the amount of added ds instances is thus an important
parameter to tune on a development set. for some of the relations there is an optimal amount of added
unlabeled data, whereas other relations show no clear optimum and    uctuate between distant and    ltered
classi   ers    values.

    impact of id84: the use of id27s often leads to an improved maximum
f1 value with respect to the bow-representations or svd-based dimension reduction. this is for example
very clear for the charges, city of headquarters, or cities of residence relations, with a slight preference
of the glove embeddings with respect to id97 for this application. however, we also noticed that
id27s are not always better than the bow or svd based representations. for example, the
highest optimal f1 for the age relation is reached with the bow model.

4.5. end-to-end knowledge base population results

this section presents the results of training binary relation classi   ers according to our new strategy for
each of the 41 relations of the tac kbp schema. we tuned hyperparameters on data of the 2012 and 2013
tracks and now test on the last edition of the esf track of 2014.

next to the thresholds of choosing the amount of unlabeled data added as discussed previously (i.e., the
value of k), other parameters include id173 and the ratio between positive and negative instances,
which appeared to be an important parameter in   uencing the con   dence of an optimal f1 value greatly.
different ratios of negative to positive instances resulted in shifting the optimal trade-off between precision
and recall. the amount of available negative training data was on many occasions larger than the available
positive. more negative than positive training data overall appeared to result in lower positive classi   cation
probabilities assigned by the classi   er to test instances. negative instances had to be down-weighted multiple
times to prevent the classi   er from being too strict and rarely classify a relation as true. for each relation, this
parameter was tuned for optimal f1 value at the 0.5 id203 threshold of the id28 classi   er.

we use the of   cial tac kbp evaluation script which calculates the micro-average of all classi   cations.

24

all methods are evaluated while ignoring provenances (the character offsets in the documents which contain
the justi   cation for extraction of the relation), so as not to penalize any system for    nding a new provenance
not validated in the of   cial evaluation key. a listing of precision, recall and f1 for the top 20 most frequently
occurring relations in the test set is shown in table 4.

next to traditional distant supervision (also known as mintz++[29], indicated as    distant supervision   
in table 4), we compare our new semi-supervised approach (   semantic label propagation   ) to a fully su-
pervised classi   er trained by manually labeling 50, 000 instances (   fully supervised   ), and to the classi   ers
obtained by purely    ltering on manually labeled patterns (   sdp filtered   ). we also use the fully supervised
classi   ers in a traditional self-training scheme, classifying distantly supervised instances in the complete
feature space and adding con   dent instances to the training set (   self-training (instances)   ). the supervi-
sion needed for these classi   ers required far more annotation effort than the feature certainty sampling of
semantic label propagation.

the of   cial f1 value of 36.4% attained using semantic label propagation is equivalent to the second
best entry out of eighteen submissions to the 2014 esf track [22]. a relation extractor is but a part of a
kbp system and is in   uenced by each of the other modules (e.g., recognition and disambiguation of named
entities), which makes it hard to compare to other systems. this is the case for the absolute values of table 4,
but still, it demonstrates the overall quality of our relation extractors. especially, our system relying on
very limited annotations has a competitive place among systems that rely on many hours of manual feature
engineering [47]. comparing the results for semantic label propagation with the other approaches shows
that the proposed method that combines a small labeling effort based on feature certainty with the semantic
label propagation technique, outperforms the ds method, semi-supervision using instance labeling, and full
supervision methods. this is also con   rmed in fig. 7, which shows the trade-off between the precision and
recall averaged over all tac kbp relations for the different methods described above, using the tac kbp
evaluation script (varying the thresholds on classi   cation).

25

c
i
t
n
a
m
e
s

n
o
i
t
a
g
a
p
o
r
p

l
e
b
a
l

g
n
n

i

i
a
r
t

-
f
l
e
s

)
s
e
c
n
a
t
s
n
i
(

d
e
s
i
v
r
e
p
u
s
y
l
l

u
f

d
e
r
e
t
l
i

f
p
d
s

n
o
i
s
i
v
r
e
p
u
s
t
n
a
t
s
i
d

)
+
+
z
t
n
m

i

(

1
f

2

.

9
3

0

.

3
6

1

.

6
3

0

.

5
7

4

.

6
4

9

.

9
3

7

.

3
5

7

.

7
3

4

.

8
3

5

.

5
5

0

.

5
2

0

.

0
5

1

.

7
5

9

.

2
4

5

.

1
3

.

0
7
4

6

.

6
6

6

.

1
4

5

.

2
6

4

.

4
4

4

.

4
4

4

.

6
3

r

2

.

1
4

5

.

2
6

0

.

1
5

5

.

2
8

6

.

6
8

1

.

6
4

1

.

8
6

6

.

1
4

4

.

5
4

4

.

5
4

.

3
3
3

3

.

3
3

0

.

0
5

0

.

5
4

7

.

5
8

4

.

4
4

2

.

6
5

4

.

5
4

5

.

5
5

5

.

8
2

6

.

6
6

p

3

.

7
3

5

.

3
6

9

.

7
2

8

.

8
6

7

.

1
3

2

.

5
3

3

.

4
4

4

.

4
3

3

.

3
3

4

.

1
7

.

0
0
2

0

.

0
0
1

6

.

6
6

9

.

0
4

3

.

9
1

0

.

0
5

8

.

1
8

4

.

8
3

4

.

1
7

0

.

0
0
1

3

.

3
3

1
f

6

.

9
3

6

.

9
5

8

.

5
3

8

.

1
7

0

.

0
4

4

.

3
4

6

.

2
5

8

.

2
2

5

.

1
3

0

.

0
4

1

.

8
1

0

.

0
4

1

.

7
5

1

.

9
3

2

.

7
2

9

.

9
4

5

.

2
6

2

.

7
2

3

.

8
5

9

.

9
3

3

.

3
3

r

2

.

3
4

.

4
3
6

4

.

0
4

0

.

0
7

.

3
3
7

4

.

8
3

8

.

8
5

6

.

6
1

2

.

7
2

4

.

5
4

6

.

6
1

0

.

5
2

0

.

0
5

.

0
5
4

7

.

5
8

5

.

5
5

.

5
2
6

2

.

7
2

7

.

7
7

5

.

8
2

3

.

3
3

9

.

5
3

9

.

6
3

0

.

3
3

.

4
9
2

p

5

.

6
3

.

3
6
5

2

.

2
3

6

.

3
7

.

5
7
2

0

.

0
5

6

.

7
4

3

.

6
3

5

.

7
3

7

.

5
3

0

.

0
2

0

.

0
0
1

6

.

6
6

.

6
4
3

2

.

6
1

4

.

5
4

.

5
2
6

2

.

7
2

6

.

6
4

6

.

6
6

3

.

3
3

.

5
7
3

1
f

4

.

8
3

9

.

7
5

0

.

1
3

9

.

4
6

8

.

0
4

8

.

5
3

2

.

4
5

0

.

9
1

9

.

9
2

9

.

9
3

9

.

9

3

.

3
3

3

.

3
3

4

.

1
2

5

.

3
2

5

.

8
4

0

.

0
5

5

.

5
1

5

.

1
5

9

.

4
2

3

.

3
3

3

.

4
2

r

1

.

1
6

9

.

3
5

7

.

5
4

5

.

2
6

6

.

6
6

3

.

2
9

1

.

4
4

5

.

2
6

8

.

1
3

2

.

7
2

0

.

0
0
1

0

.

0
5

0

.

5
2

0

.

5
1

5

.

8
2

4

.

4
4

5

.

7
3

2

.

9

8

.

8
8

2

.

4
1

3

.

3
3

0

.

6
2

p

0

.

8
2

6

.

2
6

5

.

3
2

0

.

8
6

4

.

9
2

2

.

2
2

4

.

0
7

2

.

1
1

3

.

8
2

0

.

5
7

2

.

5

0

.

5
2

0

.

0
5

5

.

7
3

0

.

0
2

5

.

3
5

0

.

5
7

0

.

0
5

3

.

6
3

0

.

0
0
1

3

.

3
3

1
f

5

.

7
3

7

.

6
5

7

.

0
4

7

.

2
7

0

.

2
4

4

.

5
3

1

.

6
4

9

.

7
2

7

.

5
3

6

.

7
4

3

.

5
1

0

.

0
5

1

.

7
5

0

.

1
4

7

.

0
3

0

.

7
4

5

.

1
6

0

.

0
4

5

.

4
5

4

.

4
4

4

.

4
4

r

1

.

9
3

4

.

3
6

0

.

1
5

0

.

0
7

0

.

0
8

6

.

4
8

9

.

2
5

0

.

5
2

4

.

5
4

4

.

5
4

6

.

6
1

6

.

1
4

0

.

0
5

0

.

0
4

7

.

5
8

4

.

4
4

0

.

0
5

4

.

5
4

6

.

6
6

5

.

8
2

6

.

6
6

p

1

.

6
3

3

.

1
5

8

.

3
3

6

.

5
7

5

.

8
2

4

.

2
2

9

.

0
4

5

.

1
3

4

.

9
2

0

.

0
5

2

.

4
1

5

.

2
6

6

.

6
6

1

.

2
4

7

.

8
1

0

.

0
5

0

.

0
8

7

.

5
3

1

.

6
4

0

.

0
0
1

3

.

3
3

7

.

2
2

7

.

4
3

7

.

3
3

5

.

5
3

1
f

3

.

2
3

3

.

6
5

6

.

2
3

0

.

2
7

4

.

7
3

4

.

7
3

6

.

8
4

5

.

0
3

3

.

3
3

6

.

7
4

5

.

8
2

4

.

9
2

1

.

7
5

5

.

2
4

5

.

3
2

6

.

2
5

2

.

8
4

0

.

4
2

9

.

9
5

4

.

4
4

3

.

3
3

7

.

8
2

r

8

.

8
5

4

.

3
6

0

.

4
3

5

.

2
7

0

.

3
2

0

.

3
2

9

.

2
5

8

.

5
4

3

.

6
3

4

.

5
4

6

.

6
1

6

.

1
4

0

.

0
5

0

.

0
5

5

.

8
2

5

.

5
5

7

.

3
4

2

.

7
2

0

.

0
0
1

5

.

8
2

3

.

3
3

1

.

8
2

p

3

.

2
2

6

.

0
5

4

.

1
3

6

.

1
7

0

.

0
0
1

0

.

0
0
1

0

.

5
4

9

.

2
2

7

.

0
3

0

.

0
5

0

.

0
0
1

7

.

2
2

6

.

6
6

0

.

7
3

0

.

0
2

0

.

0
5

8

.

3
5

4

.

1
2

8

.

2
4

0

.

0
0
1

3

.

3
3

3

.

9
2

r
e
r
o
c
s
-
c
a
t

l
a
i
c
   
f
o

d
n
a

s
n
o
i
t
a
l
e
r

t
n
e
u
q
e
r
f
r
o
f

s
t
l
u
s
e
r

:
4

e
l
b
a
t

s
e
e
y
o
l
p
m
e

s
r
e
b
m
e
m
p
o
t

f
o

r
e
b
m
e
m
r
o

e
e
y
o
l
p
m
e

n
i
g
i
r
o

e
g
a

e
c
n
e
d
i
s
e
r

f
o

s
e
i
r
t
n
u
o
c

e
c
n
e
d
i
s
e
r

f
o

s
e
i
t
i
c

h
t
a
e
d

f
o

e
s
u
a
c

s
e
g
r
a
h
c

h
t
a
e
d

f
o

y
t
i
c

e
s
u
o
p
s

s
r
e
t
r
a
u
q
d
a
e
h

f
o

y
r
t
n
u
o
c

h
t
a
e
d

f
o

e
t
a
d

s
t
n
e
r
a
p
)
:
r
e
p
(

n
o
i
t
a
l
e
r

e
l
t
i
t

e
c
n
e
d
i
s
e
r

f
o

s
e
c
n
i
v
o
r
p
r
o
s
e
t
a
t
s

s
e
m
a
n

e
t
a
n
r
e
t
l
a
)
:
g
r
o
(

s
r
e
t
r
a
u
q
d
a
e
h

f
o

y
t
i
c

s
t
n
e
r
a
p
)
:
g
r
o
(

s
g
n
i
l
b
i
s

y
b

d
e
d
n
u
o
f

n
e
r
d
l
i
h
c

)

1
f
-
o
r
c
i

m

(

r
e
r
o
c
s
c
a
t

l
a
i
c
   
f
o

26

figure 7: precision-recall graph displaying the output of the tac kbp evaluation script on different sys-
tems, for varying classi   er decision thresholds.

one would expect the sdp    ltered and fully supervised extractors to attain high precision, but this is not
the case for some of the relations. for example, for relation countries of residence recall of these extractors
is higher than recall of the slp method. however, only those precision and recall scores are shown that
correspond to the maximum values for f1 and while precision could have been higher for these extractors
at the cost of lower recall, recall is equally important for this type of evaluation. the sdp    ltered and
fully supervised extractors are likely to attain high precision values, but this will not compensate for the
loss in recall when evaluating f1 scores. we conclude by noting that the results may also be in   uenced
to peculiarities of the data. entities chosen by tac may not always be representative for the majority of
persons or organizations in the training data: tac entities are in many cases more dif   cult than the average
entity from the training set and the most common way of expressing a relationship for these entities might
not be present in the test set.

4.6. 2015 tac kbp cold start slot filling

the slot    lling task in tac kbp in 2015 was organized as part of the cold start slot filling track,
where the goal is to search the same document collection to    ll in values for speci   c slots for speci   c

27

entities, and in a second stage    ll slots for answers of the    rst stage.
in the authors    tac kbp 2015
submission [63], some of the ideas presented in this paper were applied, leading to a second place in the slot
filling variant. the results showed the in   uence of a clean training set and the effectiveness of self-training.
a top-performing entry was based on the deepdive-like database system [48] and training set    ltering.
we note that the idea of self-training using a    rst stage high-precision classi   er was also included in this
deepdive system, independently of the work presented in this paper. neural architectures for relation
extractors were also included in ensembles of some systems and found to be important extractors but a
selection of our linear classi   ers in combination with a careful    ltering of distantly supervised training data
was shown to outperform ensembles of linear and neural classi   ers.

5. conclusions

the overall aim of our proposed strategy in building relation extractors in a closed ie setting (i.e., to ex-
tract a priori speci   ed relations) is to maximize their performance using minimal (human) annotation effort.
our key ideas to achieve that are: (i) distant supervision (ds): use known relation instances from a knowl-
edge base to automatically generate training data, (ii) feature annotation: rather than labeling instances,
annotate features (e.g., text patterns expressing a relationship), selected by means of an active learning
criterion, and (iii) semantic feature space representation: use compact semantic vector spaces to detect ad-
ditional, semantically related patterns that do not occur in the thus far selected training data, leaving useful
patterns undetected otherwise.

thus, we address the problem of noisy training data obtained through ds by expanding the key idea of
automatically    ltering of the training data to increase precision (see [50]). speci   cally, to improve recall,
we introduce the semi-supervised semantic label propagation method, that allows to relax the pattern-based
   ltering of the ds training data by again including weakly supervised items that are suf   ciently    similar   
to highly con   dent instances. we found that a simple linear combination of the embeddings of the words
contributing to the relation pattern is an effective dimension reduction technique to obtain representations
for propagating labels from supervised to weakly supervised instances. tuning a threshold parameter for
similarity creates an improved training set for id36.

the main contributions of this paper to the domain of closed id36, are (i) the novel method-
ology of    ltering an initial ds training set, where we motivated and demonstrated the effectiveness of an
almost negligible manual annotation effort, and (ii) the semantic label propagation model for again expand-
ing the    ltered set in order to increase diversity in the training data.

28

we evaluated our classi   ers on the knowledge base population task of tac kbp and show the competi-

tiveness with respect to established methods that rely on a much heavier annotation cost.

references

references

[1] f. m. suchanek, g. kasneci, g. weikum, yago: a core of semantic knowledge, in: proceedings of the

16th international conference on world wide web, acm, 2007, pp. 697   706.

[2] c. bizer, j. lehmann, g. kobilarov, s. auer, c. becker, r. cyganiak, s. hellmann, dbpedia-a crystal-
lization point for the web of data, web semantics: science, services and agents on the world wide web
7 (3) (2009) 154   165.

[3] k. bollacker, c. evans, p. paritosh, t. sturge, j. taylor, freebase: a collaboratively created graph
database for structuring human knowledge, in: proceedings of the 2008 acm sigmod international
conference on management of data, acm, 2008, pp. 1247   1250.

[4] t. mitchell, w. cohen, e. hruschka, p. p. talukdar, j. betteridge, a. carlson, b. dalvi mishra,
m. gardner, b. kisiel, j. krishnamurthy, et al., never-ending learning, in: aaai, aaai press, 2015,
pp. 2302   2310.

[5] n. nakashole, m. theobald, g. weikum, scalable knowledge harvesting with high precision and high
recall, in: proceedings of the fourth acm international conference on web search and data mining,
acm, 2011, pp. 227   236.

[6] a. yates, m. cafarella, m. banko, o. etzioni, m. broadhead, s. soderland, textrunner: open informa-
tion extraction on the web, in: proceedings of human language technologies: the annual conference
of the north american chapter of the association for computational linguistics: demonstrations, as-
sociation for computational linguistics, 2007, pp. 25   26.

[7] j. fan, d. ferrucci, d. gondek, a. kalyanpur, prismatic: inducing knowledge from a large scale
lexicalized relation resource, in: proceedings of the naacl hlt 2010    rst international workshop
on formalisms and methodology for learning by reading, association for computational linguistics,
2010, pp. 122   127.

29

[8] h. liu, p. singh, conceptnet   a practical commonsense reasoning tool-kit, bt technology journal

22 (4) (2004) 211   226.

[9] e. cambria, r. speer, c. havasi, a. hussain, senticnet: a publicly available semantic resource for

opinion mining, in: proceedings of aaai csk, arlington, 2010, pp. 14   18.

[10] x. dong, e. gabrilovich, g. heitz, w. horn, n. lao, k. murphy, t. strohmann, s. sun, w. zhang,
knowledge vault: a web-scale approach to probabilistic knowledge fusion, in: proceedings of the 20th
acm sigkdd international conference on knowledge discovery and data mining, acm, 2014, pp.
601   610.

[11] o. deshpande, d. s. lamba, m. tourn, s. das, s. subramaniam, a. rajaraman, v. harinarayan,

a. doan, building , maintaining , and using knowledge bases : a report from the trenches.

[12] b. min, r. grishman, l. wan, c. wang, d. gondek, distant supervision for id36 with an

incomplete knowledge base., in: hlt-naacl, 2013, pp. 777   782.

[13] s. miller, h. fox, l. ramshaw, r. weischedel, a novel use of statistical parsing to extract information
from text, in: proceedings of the 1st north american chapter of the association for computational
linguistics conference, association for computational linguistics, 2000, pp. 226   233.

[14] n. kambhatla, combining lexical, syntactic, and semantic features with maximum id178 models for
extracting relations, in: proceedings of the acl 2004 on interactive poster and demonstration sessions,
association for computational linguistics, 2004, p. 22.

[15] e. boschee, r. weischedel, a. zamanian, automatic information extraction, in: proceedings of the

2005 international conference on intelligence analysis, mclean, va, citeseer, 2005, pp. 2   4.

[16] j. jiang, c. zhai, a systematic exploration of the feature space for id36., in: hlt-

naacl, 2007, pp. 113   120.

[17] a. sun, r. grishman, s. sekine, semi-supervised id36 with large-scale word id91,
in: proceedings of the 49th annual meeting of the association for computational linguistics: human
language technologies-volume 1, association for computational linguistics, 2011, pp. 521   529.

[18] r. c. bunescu, r. j. mooney, a shortest path dependency kernel for id36, in: proceed-
ings of the conference on human language technology and empirical methods in natural language
processing, association for computational linguistics, 2005, pp. 724   731.

30

[19] e. cambria, b. white, jumping nlp curves: a review of natural language processing research [review

article], computational intelligence magazine, ieee 9 (2) (2014) 48   57.

[20] e. agichtein, l. gravano, snowball: extracting relations from large plain-text collections, in: pro-

ceedings of the    fth acm conference on digital libraries, acm, 2000, pp. 85   94.

[21] s. gupta, c. d. manning, spied: stanford pattern-based information extraction and diagnostics, pro-
ceedings of the acl 2014 workshop on interactive language learning, visualization, and interfaces
(acl-illvi).

[22] m. surdeanu, h. ji, overview of the english slot    lling track at the tac2014 knowledge base population

evaluation, proc. text analysis conference (tac2014).

[23] j. shin, s. wu, f. wang, c. de sa, c. zhang, c. r  e, incremental knowledge base construction using

deepdive, proceedings of the vldb endowment 8 (11) (2015) 1310   1321.

[24] s. riedel, l. yao, a. mccallum, modeling relations and their mentions without labeled text, in: ma-
chine learning and knowledge discovery in databases, springer berlin heidelberg, 2010, pp. 148   
163.

[25] r. hoffmann, c. zhang, x. ling, l. zettlemoyer, d. s. weld, knowledge-based weak supervision
for information extraction of overlapping relations, in: proceedings of the 49th annual meeting of the
association for computational linguistics: human language technologies-volume 1, association for
computational linguistics, 2011, pp. 541   550.

[26] m. surdeanu, j. tibshirani, r. nallapati, c. d. manning, multi-instance multi-label learning for re-
lation extraction, in: proceedings of the 2012 joint conference on empirical methods in natural
language processing and computational natural language learning, association for computational
linguistics, 2012, pp. 455   465.

[27] g. angeli, j. tibshirani, j. y. wu, c. d. manning, combining distant and partial supervision for
id36, in: proceedings of the 2014 conference on empirical methods in natural language
processing (emnlp), 2014.

[28] m. pershina, b. min, w. xu, r. grishman, infusion of labeled data into distant supervision for relation
extraction, in: proceedings of the 52nd annual meeting of the association for computational linguis-
tics (volume 2: short papers), association for computational linguistics, baltimore, maryland, 2014,

31

pp. 732   738.
url http://www.aclweb.org/anthology/p14-2119

[29] m. mintz, s. bills, r. snow, d. jurafsky, distant supervision for id36 without labeled
data, in: proceedings of the joint conference of the 47th annual meeting of the acl and the 4th
international joint conference on natural language processing of the afnlp: volume 2-volume 2,
association for computational linguistics, 2009, pp. 1003   1011.

[30] d. zelenko, c. aone, a. richardella, kernel methods for id36, in: proceedings of the
acl-02 conference on empirical methods in natural language processing - volume 10, emnlp
   02, association for computational linguistics, stroudsburg, pa, usa, 2002, pp. 71   78. doi:10.
3115/1118693.1118703.
url http://dx.doi.org/10.3115/1118693.1118703

[31] a. culotta, j. sorensen, dependency tree kernels for id36, in: proceedings of the 42nd
annual meeting on association for computational linguistics, association for computational lin-
guistics, 2004, p. 423.

[32] d. zeng, k. liu, s. lai, g. zhou, j. zhao, relation classi   cation via convolutional deep neural network,

in: proceedings of coling, 2014, pp. 2335   2344.

[33] k. xu, y. feng, s. huang, d. zhao, semantic relation classi   cation via convolutional neural networks

with simple negative sampling, arxiv preprint arxiv:1506.07650.

[34] s. brin, extracting patterns and relations from the world wide web., technical report 1999-65, stan-

ford infolab, previous number = sidl-wp-1999-0119 (november 1999).
url http://ilpubs.stanford.edu:8090/421/

[35] c. zhang, w. xu, z. ma, s. gao, q. li,

j. guo, construction of semantic bootstrap-
id36, knowledge-based systems 83 (2015) 128     137.

for

ping models
doi:http://dx.doi.org/10.1016/j.knosys.2015.03.017.
url

http://www.sciencedirect.com/science/article/pii/

s0950705115001112

[36] m. komachi, t. kudo, m. shimbo, y. matsumoto, graph-based analysis of semantic drift in espresso-
like id64 algorithms, in: proceedings of the conference on empirical methods in natural

32

language processing, emnlp    08, association for computational linguistics, stroudsburg, pa, usa,
2008, pp. 1011   1020.
url http://dl.acm.org/citation.cfm?id=1613715.1613847

[37] j. r. curran, t. murphy, b. scholz, minimising semantic drift with mutual exclusion id64,
proceedings of the conference of the paci   c association for computational linguistics (2007) 172   
180.

[38] d. s. batista, b. martins, m. j. silva, semi-supervised id64 of relationship extractors with
id65, in: proceedings of the 2015 conference on empirical methods in natural
language processing, association for computational linguistics, lisbon, portugal, 2015, pp. 499   
504.
url http://aclweb.org/anthology/d15-1056

[39] m. craven, j. kumlien, et al., constructing biological knowledge bases by extracting information from

text sources., in: ismb, vol. 1999, 1999, pp. 77   86.

[40] r. hoffmann, c. zhang, x. ling, knowledge-based weak supervision for information extraction of

overlapping relations, proc. 49th . . . .
url http://dl.acm.org/citation.cfm?id=2002541

[41] a. intxaurrondo, m. surdeanu, o. l. de lacalle, e. agirre, removing noisy mentions for distant

supervision, procesamiento del lenguaje natural 51 (2013) 41   48.

[42] e. alfonseca, k. filippova, j.-y. delort, g. garrido, pattern learning for id36 with a hier-
archical topic model, in: proceedings of the 50th annual meeting of the association for computational
linguistics: short papers-volume 2, association for computational linguistics, 2012, pp. 54   59.

[43] s. takamatsu, i. sato, h. nakagawa, reducing wrong labels in distant supervision for relation extrac-
tion, in: proceedings of the 50th annual meeting of the association for computational linguistics:
long papers-volume 1, association for computational linguistics, 2012, pp. 721   729.

[44] j. chen, d. ji, c. l. tan, z. niu, id36 using label propagation based semi-supervised
learning, in: proceedings of the 21st international conference on computational linguistics and the
44th annual meeting of the association for computational linguistics, association for computational
linguistics, 2006, pp. 129   136.

33

[45] c. zhang, f. niu, c. r  e, j. shavlik, big data versus the crowd: looking for relationships in all the right
places, in: proceedings of the 50th annual meeting of the association for computational linguistics:
long papers-volume 1, association for computational linguistics, 2012, pp. 825   834.

[46] h. ji, r. grishman, knowledge base population: successful approaches and challenges, in: proceed-
ings of the 49th annual meeting of the association for computational linguistics: human language
technologies-volume 1, association for computational linguistics, 2011, pp. 1148   1158.

[47] g. angeli, s. gupta, m. jose, c. d. manning, c. r  e, j. tibshirani, j. y. wu, s. wu, c. zhang, stan-

ford   s 2014 slot    lling systems, tac kbp.

[48] c. zhang, deepdive: a data management system for automatic knowledge base construction, ph.d.

thesis, uw-madison (2015).

[49] h. s. seung, m. opper, h. sompolinsky, query by committee, in: proceedings of the    fth annual

workshop on computational learning theory, acm, 1992, pp. 287   294.

[50] l. sterckx, t. demeester, j. deleu, c. develder, using active learning and semantic id91 for
noise reduction in distant supervision, in: 4e workshop on automated base construction at nips2014
(akbc-2014), 2014, pp. 1   6.

[51] g. druck, b. settles, a. mccallum, active learning by labeling features, in: proceedings of the 2009
conference on empirical methods in natural language processing: volume 1-volume 1, association
for computational linguistics, 2009, pp. 81   90.

[52] j. attenberg, p. melville, f. provost, a uni   ed approach to active dual supervision for labeling features
and examples, in: in european conference on machine learning and knowledge discovery in databases,
2010, pp. 40   55.

[53] z. harris, distributional structure, word 10 (23) (1954) 146   162.

[54] j. h. martin, d. jurafsky, speech and language processing, international edition.

[55] t. mikolov, k. chen, g. corrado, j. dean, ef   cient estimation of word representations in vector space,

corr abs/1301.3781.
url http://arxiv.org/abs/1301.3781

34

[56] j. pennington, r. socher, c. d. manning, glove: global vectors for word representation, in: proceed-
ings of the 2014 conference on empirical methods in natural language processing, emnlp 2014,
october 25-29, 2014, doha, qatar, a meeting of sigdat, a special interest group of the acl, 2014,
pp. 1532   1543.
url http://aclweb.org/anthology/d/d14/d14-1162.pdf

[57] w. blacoe, m. lapata, a comparison of vector-based representations for semantic composition, in:
proceedings of the 2012 joint conference on empirical methods in natural language processing and
computational natural language learning, association for computational linguistics, jeju island,
korea, 2012, pp. 546   556.
url http://www.aclweb.org/anthology/d12-1050

[58] d. graff, j. kong, k. chen, k. maeda, english gigaword, linguistic data consortium, philadelphia.

[59] c. d. manning, m. surdeanu, j. bauer, j. finkel, s. j. bethard, d. mcclosky, the stanford corenlp
natural language processing toolkit, in: proceedings of 52nd annual meeting of the association for
computational linguistics: system demonstrations, 2014, pp. 55   60.

[60] m. feys, l. sterckx, l. mertens, j. deleu, t. demeester, c. develder, ghent university-ibcn partic-
ipation in tac-kbp 2014 slot    lling and cold start tasks, in: 7th text analysis conference, proceed-
ings, 2014, pp. 1   10.

[61] m. mintz, s. bills, r. snow, d. jurafsky, distant supervision for id36 without labeled

data (august) (2009) 1003   1011.

[62] m. stevenson, m. a. greenwood, comparing information extraction pattern models, in: proceedings
of the workshop on information extraction beyond the document, association for computational
linguistics, 2006, pp. 12   19.

[63] l. sterckx, j. deleu, t. demeester, c. develder, ghent university-ibcn participation in tac-kbp

2015 cold start task, in: 8th text analysis conference, proceedings (to appear), 2015.

[64] s. c. deerwester, s. t. dumais, t. k. landauer, g. w. furnas, r. a. harshman, indexing by latent

semantic analysis, jasis 41 (6) (1990) 391   407.

35

