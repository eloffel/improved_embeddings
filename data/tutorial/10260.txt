transg : a generative model for id13 embedding

han xiao   , minlie huang   , xiaoyan zhu

state key lab. of intelligent technology and systems,
national lab. for information science and technology,

dept. of computer science and technology, tsinghua university, beijing 100084, pr china
   corresponding authors: http://www.ibookman.net, http://www.aihuang.org

bookman@vip.163.com; {aihuang,zxy-dcs}@tsinghua.edu.cn

7
1
0
2

 

p
e
s
8

 

 
 
]
l
c
.
s
c
[
 
 

7
v
8
8
4
5
0

.

9
0
5
1
:
v
i
x
r
a

abstract

recently, id13 embedding,
which projects symbolic entities and re-
lations into continuous vector space, has
become a new, hot topic in arti   cial in-
telligence. this paper proposes a novel
generative model (transg) to address
the issue of multiple relation semantics
that a relation may have multiple mean-
ings revealed by the entity pairs associ-
ated with the corresponding triples. the
new model can discover latent seman-
tics for a relation and leverage a mix-
ture of relation-speci   c component vec-
tors to embed a fact triple. to the best
of our knowledge, this is the    rst gen-
erative model for id13 em-
bedding, and at the    rst time, the issue
of multiple relation semantics is formally
discussed. extensive experiments show
that the proposed model achieves substan-
tial improvements against the state-of-the-
art baselines. all of the related poster,
slides, datasets and codes have been pub-
lished in http://www.ibookman.
net/conference.html.

introduction

1
abstract or real-world knowledge is always a ma-
jor topic in arti   cial intelligence. knowledge
bases such as id138 (miller, 1995) and free-
base (bollacker et al., 2008) have been shown very
useful to ai tasks including id53,
knowledge id136, and so on. however, tra-
ditional knowledge bases are symbolic and logic,
thus numerical machine learning methods can-
not be leveraged to support the computation over
the knowledge bases. to this end, knowledge
graph embedding has been proposed to project en-

tities and relations into continuous vector spaces.
among various embedding models, there is a line
of translation-based models such as transe (bor-
des et al., 2013), transh (wang et al., 2014),
transr (lin et al., 2015b), and other related mod-
els (he et al., 2015) (lin et al., 2015a).

figure 1: visualization of transe embedding vec-
tors with pca dimension reduction. four relations
(a     d) are chosen from freebase and id138.
a dot denotes a triple and its position is decided
by the difference vector between tail and head en-
tity (t     h). since transe adopts the principle of
t     h     r, there is supposed to be only one cluster
whose centre is the relation vector r. however, re-
sults show that there exist multiple clusters, which
justi   es our multiple relation semantics assump-
tion.

a fact of knowledge base can usually be rep-
resented by a triple (h, r, t) where h, r, t indicate
a head entity, a relation, and a tail entity, respec-

figure 2: visualization of multiple relation semantics. the data are selected from id138. the dots
are correct triples that belong to haspart relation, while the circles are incorrect ones. the point coor-
dinate is the difference vector between tail and head entity, which should be near to the centre. (a) the
correct triples are hard to be distinguished from the incorrect ones. (b) by applying multiple semantic
components, our proposed model could discriminate the correct triples from the wrong ones.

tively. all translation-based models almost follow
the same principle hr + r     tr where hr, r, tr in-
dicate the embedding vectors of triple (h, r, t),
with the head and tail entity vector projected with
respect to the relation space.

in spite of the success of these models, none
of the previous models has formally discussed
the issue of multiple relation semantics that a
relation may have multiple meanings revealed by
the entity pairs associated with the corresponding
triples. as can be seen from fig. 1, visualization
results on embedding vectors obtained from
transe (bordes et al., 2013) show that,
there
are different clusters for a speci   c relation,
and different clusters indicate different
latent
semantics. for example, the relation haspart has
at least two latent semantics: composition-related
as (table, haspart, leg) and location-related
as (atlantics, haspart, newyorkbay). as one
more example,
in freebase, (jon snow, birth
place, winter fall) and (george r. r. martin,
birth place, u.s.) are mapped to schema /   c-
tional universe/   ctional character/place of birth
and /people/person/place of birth respectively,
indicating that birth place has different meanings.
this phenomenon is quite common in knowledge
bases for two reasons: arti   cial simpli   cation and
nature of knowledge. on one hand, knowledge
base curators could not involve too many similar
relations, so abstracting multiple similar relations
into one speci   c relation is a common trick. on
the other hand, both language and knowledge
representations often involve ambiguous infor-
mation. the ambiguity of knowledge means a

semantic mixture. for example, when we mention
   expert   , we may refer to scientist, businessman
or writer, so the concept    expert    may be ambigu-
ous in a speci   c situation, or generally a semantic
mixture of these cases.
however, since previous translation-based mod-
els adopt hr + r     tr, they assign only one trans-
lation vector for one relation, and these models are
not able to deal with the issue of multiple relation
semantics. to illustrate more clearly, as showed
in fig.2, there is only one unique representation
for relation haspart in traditional models, thus
the models made more errors when embedding the
triples of the relation.
instead, in our proposed
model, we leverage a bayesian non-parametric in-
   nite mixture model to handle multiple relation se-
mantics by generating multiple translation compo-
nents for a relation. thus, different semantics are
characterized by different components in our em-
bedding model. for example, we can distinguish
the two clusters haspart.1 or haspart.2, where
the relation semantics are automatically clustered
to represent the meaning of associated entity pairs.
to summarize, our contributions are as follows:
    we propose a new issue in id13
embedding, multiple relation semantics that
a relation in id13 may have dif-
ferent meanings revealed by the associated
entity pairs, which has never been studied
previously.

    to address the above issue, we propose a
novel bayesian non-parametric in   nite mix-
ture embedding model, transg. the model

can automatically discover semantic clusters
of a relation, and leverage a mixture of multi-
ple relation components for translating an en-
tity pair. moreover, we present new insights
from the generative perspective.

    extensive experiments show that our pro-
improve-

posed model obtains substantial
ments against the state-of-the-art baselines.

2 related work

prior studies are classi   ed into two branches:
translation-based embedding methods and the oth-
ers.

2.1 translation-based embedding methods
existing translation-based embedding methods
share the same translation principle h + r     t and
the score function is designed as:

fr(h, t) = ||hr + r     tr||2

2

r hwr, tr = t     w(cid:62)

where hr, tr are entity embedding vectors pro-
jected in the relation-speci   c space. transe (bor-
des et al., 2013), lays the entities in the original en-
tity space: hr = h, tr = t. transh (wang et al.,
2014), projects entities into a hyperplane for ad-
dressing the issue of complex relation embedding:
hr = h     w(cid:62)
r twr. to address
the same issue, transr (lin et al., 2015b), trans-
forms the entity embeddings by the same relation-
speci   c matrix: hr = mrh, tr = mrt. transr
also proposes an ad-hoc id91-based method,
ctransr, where the entity pairs for a relation are
clustered into different groups, and the pairs in
the same group share the same relation vector. in
comparison, our model is more elegant to address
such an issue theoretically, and does not require a
pre-process of id91. furthermore, our model
has much better performance than ctransr, as
we expect. transm (fan et al., 2014) lever-
ages the structure of the id13 via pre-
calculating the distinct weight for each training
triple to enhance embedding. kg2e (he et al.,
2015) is a probabilistic embedding method for
modeling the uncertainty in id13.

2.2 pioneering embedding methods
there list the pioneering embedding approaches:
structured embedding (se). the se model
(bordes et al., 2011) transforms the entity space
with the head-speci   c and tail-speci   c matrices.

the score function is de   ned as fr(h, t) =
||mh,rh     mt,rt||. according to (socher et
al., 2013),
this model cannot capture the re-
lationship between entities. semantic match-
ing energy (sme). the sme model (bordes
et al., 2012) (bordes et al., 2014) can han-
dle the correlations between entities and rela-
tions by matrix product and hadamard prod-
uct.
in some recent work (bordes et al., 2014),
the score function is re-de   ned with 3-way ten-
sors instead of matrices. single layer model
(slm). slm applies neural network to knowl-
edge graph embedding. the score function is de-
   ned as fr(h, t) = u(cid:62)
r g(mr,1h + mr,2t) where
mr,1, mr,2 are relation-speci   c weight matrices.
collobert had applied a similar method into the
language model, (collobert and weston, 2008).
latent factor model (lfm). the lfm (jenat-
ton et al., 2012), (sutskever et al., 2009) attempts
to capture the second-order correlations between
entities by a quadratic form. the score func-
tion is as fr(h, t) = h(cid:62)wrt. neural ten-
sor network (ntn). the ntn model (socher et
al., 2013) de   nes a very expressive score func-
tion to combine the slm and lfm: fr(h, t) =
u(cid:62)
r g(h(cid:62)w    rt + mr,1h + mr,2t + br), where
ur is a relation-speci   c linear layer, g(  ) is the
tanh function, w     rd  d  k is a 3-way tensor.
unstructured model (um). the um (bordes et
al., 2012) may be a simpli   ed version of transe
without considering any relation-related informa-
tion. the score function is directly de   ned as
fr(h, t) = ||h     t||2
2. rescal. this is a col-
lective id105 model which is also
a common method in knowledge base embedding
(nickel et al., 2011), (nickel et al., 2012).

semantically smooth embedding (sse).
(guo et al., 2015) aims at further discovering the
geometric structure of the embedding space to
make it semantically smooth. (wang et al., 2014)
focuses on bridging the gap between knowledge
and texts, with a joint id168 for knowledge
graph and text corpus. (wang et al., 2015) incor-
porates the rules that are related with relation types
such as 1-n and n-1. ptranse. (lin et al., 2015a)
is a path-based embedding model, simultaneously
considering the information and con   dence level
of the path in id13.

3 methods
3.1 transg: a generative model for

embedding

as just mentioned, only one single translation vec-
tor for a relation may be insuf   cient to model mul-
tiple relation semantics.
in this paper, we pro-
pose to use bayesian non-parametric in   nite mix-
ture embedding model (grif   ths and ghahramani,
2011). the generative process of the model is as
follows:
1. for an entity e     e:

(a) draw each entity embedding mean vec-
tor from a standard normal distribution
as a prior: ue (cid:118) n (0, 1).

2. for a triple (h, r, t)        :

(a) draw a semantic component from chi-
nese restaurant process for this relation:
  r,m     crp (  ).

(b) draw a head entity embedding vec-
tor from a normal distribution: h (cid:118)
n (uh,   2

he).

(c) draw a tail entity embedding vec-
tor from a normal distribution: t (cid:118)
n (ut,   2
(d) draw a relation embedding vector for
this semantics: ur,m = t     h (cid:118)
n (ut     uh, (  2

t e).

t )e).

h +   2

where uh and ut indicate the mean embedding
vector for head and tail respectively,   h and   t
indicate the variance of corresponding entity dis-
tribution respectively, and ur,m is the m-th com-
ponent translation vector of relation r. chinese
restaurant process (crp) is a dirichlet process
and it can automatically detect semantic compo-
nents. in this setting, we obtain the score function
as below:

p{(h, r, t)}     mr(cid:88)
mr(cid:88)

m=1

=

  r,mp(ur,m|h, t)

    ||uh+ur,m   ut||2

2

  2
h

+  2
t

  r,me

(1)

m=1

where   r,m is the mixing factor, indicating the
weight of i-th component and mr is the number
of semantic components for the relation r, which
is learned from the data automatically by the crp.

inspired by fig.1, transg leverages a mixture
of relation component vectors for a speci   c re-
lation. each component represents a speci   c la-
tent meaning. by this way, transg could distin-
guish multiple relation semantics. notably, the
crp could generate multiple semantic compo-
nents when it is necessary and the relation seman-
tic component number mr is learned adaptively
from the data.

3.2 explanation from the geometry

perspective

similar to previous studies, transg has geometric
explanations. in the previous methods, when the
relation r of triple (h, r, t) is given, the geometric
representations are    xed, as h + r     t. however,
transg generalizes this geometric principle to:
    ||uh+ur,m   ut||2

(cid:32)

(cid:33)

2

  r,me

  2
h

+  2
t

m   

(h,r,t) = arg max
m=1...mr

h + ur,m   

(h,r,t)

    t

(2)

where m   
(h,r,t) is the index of primary compo-
nent. though all the components contribute to the
model, the primary one contributes the most due
to the exponential effect (exp(  )). when a triple
(h, r, t) is given, transg works out the index of
primary component then translates the head entity
to the tail one with the primary translation vector.
for most triples, there should be only one com-
ponent that have signi   cant non-zero value as

(cid:32)

(cid:33)

    ||uh+ur,m   ut||2

2

  2
h

+  2
t

  r,me

and the others would

be small enough, due to the exponential decay.
this property reduces the noise from the other
semantic components to better characterize mul-
tiple relation semantics. in detail, (t     h) is al-
most around only one translation vector ur,m   
in transg. under the condition m (cid:54)= m   

(h,r,t),
is very large so that the expo-
nential function value is very small. this is why
the primary component could represent the corre-
sponding semantics.

(cid:16)||uh+ur,m   ut||2

  2
h+  2
t

(cid:17)

(h,r,t)

2

to summarize, previous studies make transla-
tion identically for all the triples of the same re-
lation, but transg automatically selects the best
translation vector according to the speci   c seman-
tics of a triple. therefore, transg could focus on
the speci   c semantic embedding to avoid much
noise from the other unrelated semantic compo-
nents and result in promising improvements than

data
#rel
#ent
#train
#valid
#test

table 1: statistics of datasets

wn18

18

40,943
141,442
5,000
5,000

11

fb15k wn11
1,345
14,951
483,142
50,000
59,071

38,696
112,581
2,609
10,544

fb13
13

75,043
316,232
5,908
23,733

existing methods. note that, all the components in
transg have their own contributions, but the pri-
mary one makes the most.

3.3 training algorithm
the maximum data likelihood principle is applied
for training. as to the non-parametric part,   r,m
is generated from the crp with id150,
similar to (he et al., 2015) and (grif   ths and
ghahramani, 2011). a new component is sampled
for a triple (h,r,t) with the below id203:

p(mr,new) =

    ||h   t||2
2
+  2
  2
t +2
h

  e
    ||h   t||2
t +2 + p{(h, r, t)}

+  2

  2
h

2

  e

(3)

where p{(h, r, t)} is the current posterior prob-
ability. as to other parts, in order to better distin-
guish the true triples from the false ones, we max-
imize the ratio of likelihood of the true triples to
that of the false ones. notably, the embedding vec-
tors are initialized by (glorot and bengio, 2010).
putting all the other constraints together, the    nal
objective function is obtained, as follows:

ln

(cid:32) mr(cid:88)
       mr(cid:88)

m=1

l

(h,r,t)      

min

uh,ur,m,ut

l =     (cid:88)
(cid:88)
(cid:32)(cid:88)

(h(cid:48),r(cid:48),t(cid:48))      (cid:48)

+

mr(cid:88)

ln

+c

r   r

m=1

    ||uh+ur,m   ut||2

2

  2
h

+  2
t

  r,me

    ||uh(cid:48) +ur(cid:48),m   ut(cid:48)||2

2

h(cid:48) +  2
  2
t(cid:48)

  r(cid:48),me

m=1

||ur,m||2

2 +

(cid:88)

e   e

(cid:33)

||ue||2

2

where     is the set of golden triples and    (cid:48) is the
set of false triples. c controls the scaling degree.
e is the set of entities and r is the set of relations.
noted that the mixing factors    and the variances
   are also learned jointly in the optimization.

sgd is applied to solve this optimization prob-
lem. in addition, we apply a trick to control the

parameter updating process during training. for
those very impossible triples, the update process is
skipped. hence, we introduce a similar condition
as transe (bordes et al., 2013) adopts: the train-
ing algorithm will update the embedding vectors
only if the below condition is satis   ed:

p{(h, r, t)}
p{(h(cid:48), r(cid:48), t(cid:48))} =

(cid:80)mr
(cid:80)mr(cid:48)

m=1   r,me

m=1   r(cid:48),me

    mre  

    ||uh+ur,m   ut||2

2

  2
h

+  2
t

    ||uh(cid:48) +ur(cid:48),m   ut(cid:48)||2

2

h(cid:48) +  2
  2
t(cid:48)

(5)

where (h, r, t)         and (h(cid:48), r(cid:48), t(cid:48))        (cid:48).    con-
trols the updating condition.

as to the ef   ciency, in theory, the time com-
plexity of transg is bounded by a small constant
m compared to transe, that is o(transg) =
o(m    o(transe)) where m is the number of
semantic components in the model. note that
transe is the fastest method among translation-
based methods. the experiment of link predic-
tion shows that transg and transe would con-
verge at around 500 epochs, meaning there is also
no signi   cant difference in convergence speed. in
experiment, transg takes 4.8s for one iteration on
fb15k while transr costs 136.8s and ptranse
costs 1200.0s on the same computer for the same
dataset.

4 experiments

our experiments are conducted on four public
benchmark datasets that are the subsets of word-
net and freebase, respectively. the statistics of
these datasets are listed in tab.1. experiments
are conducted on two tasks : link prediction and
triple classi   cation. to further demonstrate how
the proposed model approaches multiple relation
semantics, we present semantic component analy-
sis at the end of this section.

(cid:33)
      

(4)

4.1 link prediction
link prediction concerns id13 com-
pletion: when given an entity and a relation, the
embedding models predict the other missing en-
tity. more speci   cally, in this task, we predict t
given (h, r,   ), or predict h given (   , r, t). the
wn18 and fb15k are two benchmark datasets for
this task. note that many ai tasks could be en-
hanced by link prediction such as relation extrac-
tion (hoffmann et al., 2011).

table 2: evaluation results on link prediction

wn18

fb15k

datasets

metric

unstructured (bordes et al., 2011)

rescal (nickel et al., 2012)

se(bordes et al., 2011)

sme(bilinear) (bordes et al., 2012)

lfm (jenatton et al., 2012)
transe (bordes et al., 2013)
transh (wang et al., 2014)
transr (lin et al., 2015b)
ctransr (lin et al., 2015b)

kg2e (he et al., 2015)

transg (this paper)

hits@10(%) mean rank

mean rank
raw filter raw filter
38.2
315
52.8
1,180
1,011
80.5
61.3
526
81.6
469
263
89.2
82.3
401
92.0
238
231
92.3
362
93.2
93.3
483

304
1,163
985
509
456
251
388
225
218
348
470

35.3
37.2
68.5
54.7
71.4
75.4
73.0
79.8
79.4
80.5
81.4

hits@10(%)
raw filter raw filter
6.3
1,074
44.1
828
273
39.8
41.3
284
33.1
283
243
47.1
64.4
212
68.7
198
70.2
199
183
71.5
79.8
203

4.5
28.4
28.8
31.3
26.0
34.9
45.7
48.2
48.4
47.5
52.8

979
683
162
158
164
125
87
77
75
69
98

table 3: evaluation results on fb15k by mapping properties of relations(%)

tasks

relation category

unstructured (bordes et al., 2011)

se(bordes et al., 2011)

sme(bilinear) (bordes et al., 2012)

transe (bordes et al., 2013)
transh (wang et al., 2014)
transr (lin et al., 2015b)
ctransr (lin et al., 2015b)

transg (this paper)

predicting head(hits@10) predicting tail(hits@10)
n-n
1-1
6.6
34.5
35.6
41.3
41.8
30.9
50.0
43.7
67.2
66.8
78.8
72.1
73.8
81.5
85.4
83.3

1-n n-1
6.1
2.5
62.6
17.2
19.9
69.6
18.2
65.7
28.7
87.6
89.2
34.1
34.7
89.0
95.7
44.7

1-n n-1
1.9
4.2
14.6
68.3
76.0
13.1
66.7
19.7
83.3
39.8
37.4
90.4
90.1
38.6
56.5
95.0

n-n
6.6
37.5
38.6
47.2
64.5
69.2
71.2
80.8

1-1
34.3
34.9
28.2
43.7
65.5
79.2
80.8
84.0

figure 3: semantic component number on wn18 (left) and fb13 (right).

relation

partof

religion

domainregion

table 4: different clusters in wn11 and fb13 relations.
cluster
triples (head, tail)
location

(capital of utah, beehive state), (hindustan, bharat) ...

composition
catholicism

others
abstract
speci   c
scientist

(monitor, television), (bush, adult body), (cell organ, cell)...

(cimabue, catholicism), (st.catald, catholicism) ...

(michal czajkowsk, islam), (honinbo sansa, buddhism) ...

(computer science, security system), (computer science, pl)..
(computer science, router), (computer science, disk file) ...

(michael woodruf, surgeon), (el lissitzky, architect)...
(enoch pratt, entrepreneur), (charles tennant, magnate)...

profession

businessman

writer

(vlad. gardin, screen writer), (john huston, screen writer) ...

evaluation protocol. we adopt the same proto-
col used in previous studies. for each testing triple
(h, r, t), we corrupt it by replacing the tail t (or the
head h) with every entity e in the id13
and calculate a probabilistic score of this corrupted
triple (h, r, e) (or (e, r, t)) with the score function
fr(h, e). after ranking these scores in descend-
ing order, we obtain the rank of the original triple.
there are two metrics for evaluation: the averaged
rank (mean rank) and the proportion of testing
triple whose rank is not larger than 10 (hits@10).
this is called    raw    setting. when we    lter out
the corrupted triples that exist in the training, val-
idation, or test datasets, this is the   filter    setting.
if a corrupted triple exists in the id13,
ranking it ahead the original triple is also accept-
able. to eliminate this case, the    filter    setting
is preferred. in both settings, a lower mean rank
and a higher hits@10 mean better performance.
implementation. as the datasets are the same,
we directly report the experimental results of sev-
eral baselines from the literature, as in (bordes
et al., 2013), (wang et al., 2014) and (lin et al.,
2015b). we have attempted several settings on
the validation dataset to get the best con   guration.
for example, we have tried the dimensions of 100,
200, 300, 400. under the    bern.    sampling strat-
egy, the optimal con   gurations are: learning rate
   = 0.001, the embedding dimension k = 100,
   = 2.5,    = 0.05 on wn18;    = 0.0015,
k = 400,    = 3.0,    = 0.1 on fb15k. note
that all the symbols are introduced in    methods   .
we train the model until it converges in previous
version (about 10,000 rounds), but we provide the
results of 2,000 rounds for comparison in current
version.

results.

evaluation results on wn18 and
fb15k are reported in tab.2 and tab.3. we ob-

serve that:

1. transg outperforms all the baselines obvi-
ously. compared to transr, transg makes
improvements by 2.9% on wn18 and 26.0%
on fb15k, and the averaged semantic com-
ponent number on wn18 is 5.67 and that on
fb15k is 8.77. this result demonstrates cap-
turing multiple relation semantics would ben-
e   t embedding.

2. the model has a bad mean rank score on
the wn18 and fb15k dataset. further anal-
ysis shows that there are 24 testing triples
(0.5% of the testing set) whose ranks are
more than 30,000, and these few cases would
lead to about 150 mean rank loss. among
these triples, there are 23 triples whose tail
or head entities have never been co-occurring
with the corresponding relations in the train-
ing set.
in one word, there is no suf   cient
training data for those relations and entities.

3. compared to ctransr, transg solves the
multiple relation semantics problem much
better for two reasons. firstly, ctransr clus-
ters the entity pairs for a speci   c relation and
then performs embedding for each cluster,
but transg deals with embedding and multi-
ple relation semantics simultaneously, where
the two processes can be enhanced by each
other. secondly, ctransr models a triple by
only one cluster, but transg applies a mix-
ture to re   ne the embedding.

our model is almost insensitive to the dimen-
sion if that is suf   cient.
for the dimensions
of 100, 200, 300, 400, the hits@10 of transg
on fb15 are 81.8%, 84.0%, 85.8%, 88.2%, while
those of transe are 47.1%, 48.5%, 51.3%, 49.2%.

on wn11, and    bern    sampling,    = 0.002,
k = 400,    = 3.0,    = 0.1 on fb13.

results. accuracies are reported in tab.5 and

fig.4. the following are our observations:

1. transg outperforms all the baselines remark-
ably. compared to transr, transg improves
by 1.7% on wn11 and 5.8% on fb13, and
the averaged semantic component number on
wn11 is 2.63 and that on fb13 is 4.53. this
result shows the bene   t of capturing multiple
relation semantics for a relation.

2. the relations, such as    synset domain    and
   type of   , which hold more semantic com-
ponents, are improved much more. in com-
parison, the relation    similar    holds only one
semantic component and is almost not pro-
moted. this further demonstrates that cap-
turing multiple relation semantics can bene   t
embedding.

4.3 semantic component analysis
in this subsection, we analyse the number of se-
mantic components for different relations and list
the component number on the dataset wn18 and
fb13 in fig.3.

results. as fig. 3 and tab. 4 show, we have the

following observations:

1. multiple semantic components are indeed
necessary for most relations. except for re-
lations such as    also see   ,    synset usage   
and    gender   , all other relations have more
than one semantic component.

2. different components

analysis

different

justifying

semantics,

and
example,

to
theoretical
of transg. for
has at
related
businessman-related
(enochpratt, entrepreneur)
related as (vlad.gardin, screenwriter).

indeed correspond
the
effectiveness
   profession   
scientist-
(ellissitzky, architect),
as
and writer-

three semantics:

least
as

figure 4: accuracies of each relations in wn11
for triple classi   cation. the right y-axis is the
number of semantic components, corresponding to
the lines.

4.2 triple classi   cation
in order to testify the discriminative capability be-
tween true and false facts, triple classi   cation is
conducted. this is a classical task in knowledge
base embedding, which aims at predicting whether
a given triple (h, r, t) is correct or not. wn11
and fb13 are the benchmark datasets for this task.
note that evaluation of classi   cation needs nega-
tive samples, and the datasets have already pro-
vided negative triples.

evaluation protocol. the decision process is
very simple as follows:
for a triple (h, r, t), if
fr(h, t) is below a threshold   r, then positive; oth-
erwise negative. the thresholds {  r} are deter-
mined on the validation dataset.

table 5: triple classi   cation: accuracy(%) for dif-
ferent embedding methods.

methods wn11 fb13 avg.
79.0
78.8
78.7
81.1
84.2
n/a
85.4
87.4

lfm
ntn
transe
transh
transr
ctransr
kg2e
transg

73.8
70.4
75.9
78.8
85.9
85.7
85.4
87.4

84.3
87.1
81.5
83.3
82.5
n/a
85.3
87.3

implementation. as all methods use the same
datasets, we directly re-use the results of different
methods from the literature. we have attempted
several settings on the validation dataset to    nd
the best con   guration. the optimal con   gurations
of transg are as follows:    bern    sampling, learn-
ing rate    = 0.001, k = 50,    = 6.0,    = 0.1

3. wn11 and wn18 are different subsets of
id138. as we know, the semantic compo-
nent number is decided on the triples in the
dataset. therefore, it   s reasonable that sim-
ilar relations, such as    synset domain    and
   synset usage    may hold different semantic
numbers for wn11 and wn18.

5 conclusion

in this paper, we propose a generative bayesian
non-parametric in   nite mixture embedding model,
transg, to address a new issue, multiple relation
semantics, which can be commonly seen in knowl-
edge graph. transg can discover the latent se-
mantics of a relation automatically and leverage
a mixture of relation components for embedding.
extensive experiments show our method achieves
substantial improvements against the state-of-the-
art baselines.

support materials.

the related
poster, slides, datasets and codes have been
already
http://www.
ibookman.net/conference.html.

published

all of

in

6 code tricks

1. the class    transg    is the experimental
version, rather than    transg hierarchical   .
note that, for numerical stabilization, we    x
the variance    as a constant.

2. when generating the new cluster, we assign
the crp factor as the mixture factor, which
is required by crp theoretically. but as to
the center of new cluster, we assign a random
vector rather than t     h. because in big data
scenario, theoretical center is far away from
the ground-truth.

3. regarding the learning methodology of pa-
rameter    and   , we applied the stochastic
gradient ascent (sgd) for ef   ciency, rather
than likelihood counting, which is naturally
suitable for crp but inef   cient. we suggest
the reader to implement both for experiments.

4. due to the slow convergence rate, in previ-
ous version, we train the model until conver-
gence, almost around 10,000 epos. but in
current version, we conduct the same rounds
with other baselines 2,000 rounds).

[bordes et al.2011] antoine bordes, jason weston, ro-
nan collobert, yoshua bengio, et al. 2011. learn-
ing structured embeddings of knowledge bases. in
proceedings of the twenty-   fth aaai conference on
arti   cial intelligence.

[bordes et al.2012] antoine bordes, xavier glorot, ja-
son weston, and yoshua bengio. 2012. joint learn-
ing of words and meaning representations for open-
text id29. in international conference
on arti   cial intelligence and statistics, pages 127   
135.

[bordes et al.2013] antoine bordes, nicolas usunier,
alberto garcia-duran, jason weston, and oksana
yakhnenko.
2013. translating embeddings for
modeling multi-relational data. in advances in neu-
ral information processing systems, pages 2787   
2795.

[bordes et al.2014] antoine bordes, xavier glorot, ja-
son weston, and yoshua bengio. 2014. a semantic
matching energy function for learning with multi-
relational data. machine learning, 94(2):233   259.

[collobert and weston2008] ronan collobert and ja-
son weston. 2008. a uni   ed architecture for natu-
ral language processing: deep neural networks with
in proceedings of the 25th in-
multitask learning.
ternational conference on machine learning, pages
160   167. acm.

[fan et al.2014] miao fan, qiang zhou, emily chang,
and thomas fang zheng. 2014. transition-based
id13 embedding with relational map-
in proceedings of the 28th pa-
ping properties.
ci   c asia conference on language, information,
and computation, pages 328   337.

[glorot and bengio2010] xavier glorot and yoshua
2010. understanding the dif   culty of
bengio.
in in-
training deep feedforward neural networks.
ternational conference on arti   cial intelligence and
statistics, pages 249   256.

[grif   ths and ghahramani2011] thomas l grif   ths
and zoubin ghahramani. 2011. the indian buffet
process: an introduction and review. the journal of
machine learning research, 12:1185   1224.

[guo et al.2015] shu guo, quan wang, bin wang, li-
semantically
in proceed-

hong wang, and li guo.
smooth id13 embedding.
ings of acl.

2015.

references
[bollacker et al.2008] kurt bollacker, colin evans,
praveen paritosh, tim sturge, and jamie taylor.
2008. freebase: a collaboratively created graph
database for structuring human knowledge. in pro-
ceedings of the 2008 acm sigmod international
conference on management of data, pages 1247   
1250. acm.

[he et al.2015] shizhu he, kang liu, guoliang ji, and
jun zhao. 2015. learning to represent knowledge
graphs with gaussian embedding. in proceedings of
the 24th acm international on conference on in-
formation and knowledge management, pages 623   
632. acm.

[hoffmann et al.2011] raphael hoffmann,

congle
zhang, xiao ling, luke zettlemoyer, and daniel s
weld. 2011. knowledge-based weak supervision
for information extraction of overlapping relations.

in proceedings of the 49th annual meeting of the
association for computational linguistics: human
language technologies-volume 1, pages 541   550.
association for computational linguistics.

[jenatton et al.2012] rodolphe jenatton, nicolas l
roux, antoine bordes, and guillaume r obozin-
ski. 2012. a latent factor model for highly multi-
relational data. in advances in neural information
processing systems, pages 3167   3175.

[lin et al.2015a] yankai lin, zhiyuan liu,

and
maosong sun.
2015a. modeling relation paths
for representation learning of knowledge bases.
proceedings of
the 2015 conference on empir-
ical methods in natural language processing
(emnlp). association for computational linguis-
tics.

[lin et al.2015b] yankai lin, zhiyuan liu, maosong
sun, yang liu, and xuan zhu. 2015b. learning
entity and relation embeddings for id13
in proceedings of the twenty-ninth
completion.
aaai conference on arti   cial intelligence.

[miller1995] george a miller. 1995. id138: a lex-
ical database for english. communications of the
acm, 38(11):39   41.

[nickel et al.2011] maximilian nickel, volker tresp,
and hans-peter kriegel. 2011. a three-way model
for collective learning on multi-relational data.
in
proceedings of the 28th international conference on
machine learning (icml-11), pages 809   816.

[nickel et al.2012] maximilian nickel, volker tresp,
and hans-peter kriegel. 2012. factorizing yago:
scalable machine learning for linked data. in pro-
ceedings of the 21st international conference on
world wide web, pages 271   280. acm.

[socher et al.2013] richard socher, danqi chen,
christopher d manning, and andrew ng. 2013.
reasoning with neural tensor networks for knowl-
in advances in neural
edge base completion.
information processing systems, pages 926   934.

[sutskever et al.2009] ilya sutskever, joshua b tenen-
baum, and ruslan salakhutdinov. 2009. modelling
relational data using bayesian clustered tensor fac-
torization. in advances in neural information pro-
cessing systems, pages 1821   1828.

[wang et al.2014] zhen wang, jianwen zhang, jianlin
feng, and zheng chen. 2014. id13
in pro-
embedding by translating on hyperplanes.
ceedings of the twenty-eighth aaai conference on
arti   cial intelligence, pages 1112   1119.

[wang et al.2015] quan wang, bin wang, and li guo.
2015. knowledge base completion using embed-
in proceedings of the 24th inter-
dings and rules.
national joint conference on arti   cial intelligence.

