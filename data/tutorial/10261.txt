transa: an adaptive approach for id13 embedding

han xiao1, minlie huang1, hao yu1, xiaoyan zhu1

1department of computer science and technology, state key lab on intelligent technology and systems,

national lab for information science and technology, tsinghua university, beijing, china

5
1
0
2

 

p
e
s
8
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
0
9
4
5
0

.

9
0
5
1
:
v
i
x
r
a

abstract

id99 is a major topic in ai, and many
studies attempt to represent entities and relations of knowl-
edge base in a continuous vector space. among these at-
tempts, translation-based methods build entity and relation
vectors by minimizing the translation loss from a head en-
tity to a tail one. in spite of the success of these methods,
translation-based methods also suffer from the oversimpli   ed
loss metric, and are not competitive enough to model vari-
ous and complex entities/relations in knowledge bases. to
address this issue, we propose transa, an adaptive metric
approach for embedding, utilizing the metric learning ideas
to provide a more    exible embedding method. experiments
are conducted on the benchmark datasets and our proposed
method makes signi   cant and consistent improvements over
the state-of-the-art baselines.

introduction

id13s such as id138 (miller 1995) and free-
base (bollacker et al. 2008) play an important role in ai re-
searches and applications. recent researches such as query
expansion prefer involving id13s (bao et al.
2014) while some industrial applications such as question
answering robots are also powered by id13s
(fader, zettlemoyer, and etzioni 2014). however, knowl-
edge graphs are symbolic and logical, where numerical ma-
chine learning methods could hardly be applied. this dis-
advantage is one of the most important challenges for the
usage of id13. to provide a general paradigm
to support computing on id13, various knowl-
edge graph embedding methods have been proposed, such
as transe (bordes et al. 2013), transh (wang et al. 2014)
and transr (lin et al. 2015).

embedding is a novel approach to address the represen-
tation and reasoning problem for id13. it trans-
forms entities and relations into continuous vector spaces,
where id13 completion and knowledge classi-
   cation can be done. most commonly, id13 is
composed by triples (h, r, t) where a head entity h, a rela-
tion r and a tail entity t are presented. among all the pro-
posed embedding approaches, geometry-based methods are
an important branch, yielding the state-of-the-art predictive
copyright c(cid:13) 2015, association for the advancement of arti   cial
intelligence (www.aaai.org). all rights reserved.

figure 1: visualization of transe embedding vectors for
freebase with pca dimension reduction. the navy crosses
are the matched tail entities for an actor   s award nominee,
while the red circles are the unmatched ones. transe ap-
plies euclidean metric and spherical equipotential surfaces,
so it must make seven mistakes as (a) shows. whilst transa
takes advantage of adaptive mahalanobis metric and ellipti-
cal equipotential surfaces in (b), four mistakes are avoided.

performance. more speci   cally, geometry-based embedding
methods represent an entity or a relation as k-dimensional
vector, then de   ne a score function fr(h, t) to measure the
plausibility of a triple (h, r, t). such approaches almost fol-
low the same geometric principle h + r     t and apply
the same loss metric ||h + r     t||2
2 but differ in the relation
space where a head entity h connects to a tail entity t.

however, the loss metric in translation-based models
is oversimpli   ed. this    aw makes the current embedding
methods incompetent to model various and complex enti-
ties/relations in knowledge base.

firstly, due to the in   exibility of loss metric, cur-
rent translation-based methods apply spherical equipoten-
tial hyper-surfaces with different plausibilities, where more
near to the centre, more plausible the triple is. as illustrated
in fig.1, spherical equipotential hyper-surfaces are applied
in (a), so it is dif   cult to identify the matched tail entities
from the unmatched ones. as a common sense in knowledge
graph, complex relations, such as one-to-many, many-to-one
and many-to-many relations, always lead to complex em-
bedding topologies. though complex embedding situation is

related work

we classify prior studies into two lines: one is the
translation-based embedding methods and the other includes
many other embedding methods.

translation-based embedding methods
all the translation-based methods share a common princi-
ple h + r     t, but differ in de   ning the relation-related
space where a head entity h connects to a tail entity t. this
principle indicates that t should be the nearest neighbour of
(h + r). hence, the translation-based methods all have the
same form of score function that applies euclidean distance
to measure the loss, as follows:

fr(h, t) = ||hr + r     tr||2

2

nal space, say hr = h, tr = t.

r hwr, tr = t     w(cid:62)

where hr, tr are the entity embedding vectors projected in
the relation-speci   c space. note that this branch of methods
keeps the state-of-the-art performance.
    transe (bordes et al. 2013) lays the entities in the origi-
    transh (wang et al. 2014) projects the entities into a hy-
perplane for addressing the issue of complex relation em-
bedding, say hr = h     w(cid:62)
    transr (lin et al. 2015) transforms the entities by the
same matrix to also address the issue of complex relation
embedding, as: hr = mrh, tr = mrt.
projecting entities into different hyperplanes or trans-
forming entities by different matrices allow entities to play
different roles under different embedding situations. how-
ever, as the    introduction    argues, these methods are incom-
petent to model complex id13s well and partic-
ularly perform unsatisfactorily in various and complex enti-
ties/relations situation, because of the oversimpli   ed metric.
transm (fan et al. 2014) pre-calculates the distinct

r twr.

weight for each training triple to perform better.

other embedding methods
there are also many other models for id13 em-
bedding.

unstructured model (um). the um (bordes et al.
2012) is a simpli   ed version of transe by setting all the re-
lation vectors to zero r = 0. obviously, relation is not con-
sidered in this model.

structured embedding (se). the se model (bordes
et al. 2011) applies two relation-related matrices, one for
head and the other for tail. the score function is de   ned as
fr(h, t) = ||mh,rh     mt,rt||2
2. according to (socher et al.
2013), this model cannot capture the relationship among en-
tities and relations.

single layer model (slm). slm applies neural net-
work to id13 embedding. the score function is
de   ned as

fr(h, t) = u(cid:62)

r g(mr,1h + mr,2t)

figure 2: speci   c illustration of weighting dimensions. the
data are selected from id138. the solid dots are cor-
rect matches while the circles are not. the arrows indicate
haspart relation. (a) the incorrect circles are matched, due
to the isotropic euclidean distance. (b) by weighting embed-
ding dimensions, we up-weighted y-axis component of loss
and down-weighted x-axis component of loss, thus the em-
beddings are re   ned because the correct ones have smaller
loss in x-axis direction.

an urgent challenge, spherical equipotential hyper-surfaces
are not    exible enough to characterise the topologies, mak-
ing current translation-based methods incompetent for this
task.

secondly, because of the oversimpli   ed loss metric, cur-
rent translation-based methods treat each dimension iden-
tically. this observation leads to a    aw illustrated in fig.2.
as each dimension is treated identically in (a)1, the incorrect
entities are matched, because they are closer than the correct
ones, measured by isotropic euclidean distance. therefore,
we have a good reason to conjecture that a relation could
only be affected by several speci   c dimensions while the
other unrelated dimensions would be noisy. treating all the
dimensions identically involves much noises and degrades
the performance.

motivated by these two issues, in this paper, we propose
transa, an embedding method by utilizing an adaptive and
   exible metric. first, transa applies elliptical surfaces in-
stead of spherical surfaces. by this mean, complex embed-
ding topologies induced by complex relations could be rep-
resented better. then, as analysed in    adaptive metric ap-
proach   , transa could be treated as weighting transformed
feature dimensions. thus, the noise from unrelated dimen-
sions is suppressed. we demonstrate our ideas in fig.1 (b)
and fig.2 (b).

to summarize, transa takes the adaptive metric ideas
for better id99. our method effectively
models various and complex entities/relations in knowledge
base, and outperforms all the state-of-the-art baselines with
signi   cant improvements in experiments.

the rest of the paper is organized as follows: we sur-
vey the related researches and then introduce our approach,
along with the theoretical analysis. next, the experiments
are present and at the    nal part, we summarize our paper.

1the dash lines indicate the x-axis component of the loss (hx +

rx     tx) and the y-axis component of the loss (hy + ry     ty).

note that slm is a special case of ntn when the zero ten-
sors are applied. (collobert and weston 2008) had proposed

a similar method but applied this approach into the language
model.

semantic matching energy (sme). the sme model
(bordes et al. 2012) (bordes et al. 2014) attempts to cap-
ture the correlations between entities and relations by ma-
trix product and hadamard product. the score functions are
de   ned as follows:

fr = (m1h + m2r + b1)(cid:62)(m3t + m4r + b2)
fr = (m1h     m2r + b1)(cid:62)(m3t     m4r + b2)

where m1, m2, m3 and m4 are weight matrices,     is the
hadamard product, b1 and b2 are bias vectors. in some re-
cent work (bordes et al. 2014), the second form of score
function is re-de   ned with 3-way tensors instead of matri-
ces.

latent factor model (lfm). the lfm (jenatton et al.
2012) uses the second-order correlations between entities by
a quadratic form, de   ned as fr(h, t) = h(cid:62)wrt.

neural tensor network (ntn). the ntn model
(socher et al. 2013) de   nes an expressive score function for
graph embedding to joint the slm and lfm.
fr(h, t) = u(cid:62)
where ur is a relation-speci   c linear layer, g(  ) is the tanh
function, wr     rd  d  k is a 3-way tensor. however, the
high complexity of ntn may degrade its applicability to
large-scale knowledge bases.

r g(h(cid:62)w    rt + mr,1h + mr,2t + br)

rescal. is a collective id105 model as
a common embedding method. (nickel, tresp, and kriegel
2011) (nickel, tresp, and kriegel 2012).

semantically smooth embedding (sse). (guo et al.
2015) aims at leveraging the geometric structure of em-
bedding space to make entity representations semantically
smooth.

(wang et al. 2014) jointly embeds knowledge and texts.
(wang, wang, and guo 2015) involves the rules into em-
bedding. (lin, liu, and sun 2015) considers the paths of
id13 into embedding.

adaptive metric approach

in this section, we would introduce the adaptive metric ap-
proach, transa, and present the theoretical analysis from
two perspectives.

adaptive metric score function
as mentioned in    introduction   , all the translation-based
methods obey the same principle h + r     t, but they differ
in the relation-speci   c spaces where entities are projected
into. thus, such methods share a similar score function.

fr(h, t) = ||h + r     t||2

2

= (h + r     t)(cid:62)(h + r     t)

(1)

this score function is actually euclidean metric. the disad-
vantages of the oversimpli   ed metric have been discussed
in    introduction   . as a consequence, the proposed transa

replaces in   exible euclidean distance with adaptive maha-
lanobis distance of absolute loss, because mahalanobis dis-
tance is more    exible and more adaptive (wang and sun
2014). thus, our score function is as follows:

fr(h, t) = (|h + r     t|)(cid:62)wr(|h + r     t|)

(2)
= (|h1 + r1    t1|,|h2 + r2    t2|, ...,|hn +
where |h + r     t| .
rn     tn|) and wr is a relation-speci   c symmetric non-
negative weight matrix that corresponds to the adaptive
metric. different from the traditional score functions, we
take the absolute value, since we want to measure the ab-
solute loss between (h + r) and t. furthermore, we would
list two main reasons for the applied absolute operator.

on one hand, the absolute operator makes the score
function as a well-de   ned norm only under the condi-
tion that all the entries of wr are non-negative. a well-
de   ned norm is necessary for most metric learning scenes
(kulis 2012), and the non-negative condition could be
achieved more easily than psd, so it generalises the com-
mon metric learning algebraic form for better render-
ing the knowledge topologies. expanding our score func-
= h + r     t. obviously, nr is non-negative, identical and
.
e
absolute homogeneous. besides with the easy-to-veri   ed

tion as an induced norm nr(e) = (cid:112)fr(h, t) where
inequality nr(e1 + e2) = (cid:112)|e1 + e2|(cid:62)wr|e1 + e2|    
(cid:112)|e1|(cid:62)wr|e1|+(cid:112)|e2|(cid:62)wr|e2| = nr(e1)+nr(e2), the

triangle inequality is hold. totally, absolute operators make
the metric a norm with an easy-to-achieve condition, helping
to generalise the representation ability.

on the other hand, in geometry, negative or positive val-
ues indicate the downward or upward direction, while in our
approach, we do not consider this factor. let   s see an in-
stance as shown in fig.2. for the entity goni   , the x-axis
component of its loss vector is negative, thus enlarging this
component would make the overall loss smaller, while this
case is supposed to make the overall loss larger. as a result,
absolute operator is critical to our approach. for a numerical
example without absolute operator, when the embedding di-
mension is two, weight matrix is [0 1; 1 0] and the loss vector
(h + r     t) = (e1, e2), the overall loss would be 2e1e2. if
e1     0 and e2     0, much absolute larger e2 would reduce
the overall loss and this is not desired.
perspective from equipotential surfaces
transa shares almost the same geometric explanations with
other translation-based methods, but they differ in the loss
metric. for other translation-based methods, the equipoten-
tial hyper-surfaces are spheres as the euclidean distance de-
   nes:

||(t     h)     r||2

2 = c

(3)
where c means the threshold or the equipotential value.
however, for transa, the equipotential hyper-surfaces are
elliptical surfaces as the mahalanobis distance of absolute
loss states (kulis 2012):

|(t     h)     r|(cid:62)wr|(t     h)     r| = c

(4)
note that the elliptical hyper-surfaces would be distorted a
bit as the absolute operator applied, but this makes no differ-
ence for analysing the performance of transa. as we know,

different equipotential hyper-surfaces correspond to differ-
ent thresholds and different thresholds decide whether the
triples are correct or not. due to the practical situation that
our knowledge base is large-scale and very complex, the
topologies of embedding cannot be distributed as uniform
as spheres, justi   ed by fig.1. thus, replacing the spherical
equipotential hyper-surfaces with the elliptical ones would
enhance the embedding.

as fig.1 illustrated, transa would perform better for one-
to-many relations. the metric of transa is symmetric, so
it is reasonable that transa would also perform better for
many-to-one relations. moreover, a many-to-many relation
could be treated as both a many-to-one and a one-to-many
relation. generally, transa would perform better for all the
complex relations.

perspective from feature weighting
transa could be regarded as weighting transformed fea-
tures. for weight matrix wr that is symmetric, we ob-
tain the equivalent unique form by ldl decomposition
(golub and van loan 2012) as follows:

wr = l(cid:62)

r drlr

fr = (lr|h + r     t|)(cid:62)dr(lr|h + r     t|)

(5)
(6)

be de   ned as follows:

min

(cid:88)

(cid:88)
(cid:32)(cid:88)

(h,r,t)      

r   r

  
[wr]ij     0

(cid:33)

(h(cid:48),r(cid:48),t(cid:48))      (cid:48)
||wr||2

f

+ c

(cid:32)(cid:88)

e   e

[fr(h, t) +        fr(cid:48)(h(cid:48), t(cid:48))]+ +

(cid:33)

(cid:88)

r   r

||e||2

2 +

||r||2

2

.
= max(0,

(7)
s.t.
where [    ]+
   ),     is the set of golden triples
and    (cid:48) is the set of incorrect ones,    is the margin that sepa-
rates the positive and negative triples. ||    ||f is the f-norm
of matrix. c controls the scaling degree, and    controls the
id173 of adaptive weight matrix. the e means the
set of entities and the r means the set of relations. at each
round of training process, wr could be worked out directly
by setting the derivation to zero. then, in order to ensure the
non-negative condition of wr, we set all the negative entries
of wr to zero.

    (cid:88)
(cid:88)

(h,r,t)      

(h(cid:48),r(cid:48),t(cid:48))      (cid:48)

(cid:16)|h + r     t||h + r     t|(cid:62)(cid:17)
(cid:16)|h(cid:48) + r(cid:48)     t(cid:48)||h(cid:48) + r(cid:48)     t(cid:48)|(cid:62)(cid:17)

(8)

wr =

+

in above equations, lr can be viewed as a transformation
matrix, which transforms the loss vector |h + r     t| to an-
other space. furthermore, dr = diag(w1, w2, w3....) is a
diagonal matrix and different embedding dimensions are
weighted by wi.

as analysed in    introduction   , a relation could only be af-
fected by several speci   c dimensions while the other dimen-
sions would be noisy. treating different dimensions identi-
cally in current translation-based methods can hardly sup-
press the noise, consequently working out an unsatisfactory
performance. we believe that different dimensions play dif-
ferent roles, particularly when entities are distributed di-
vergently. unlike existing methods, transa can automat-
ically learn the weights from the data. this may explain
why transa outperforms transr although both transa and
transr transform the entity space with matrices.

connection to previous works
regarding transr that rotates and scales the embedding
spaces, transa holds two advantages against it. firstly, we
weight feature dimensions to avoid the noise. secondly, we
loosen the psd condition for a    exible representation. re-
garding transm that weights feature dimensions using pre-
computed coef   cients, transa holds two advantages against
it. firstly, we learn the weights from the data, which makes
the score function more adaptive. secondly, we apply the
feature transformation that makes the embedding more ef-
fective.

training algorithm
to train the model, we use the margin-based ranking error.
taking other constraints into account, the target function can

as to the complexity of our model, the weight matrix is
completely calculated by the existing embedding vectors,
which means transa almost has the same free parameter
number as transe. as to the ef   ciency of our model, the
weight matrix has a closed solution, which speeds up the
training process to a large extent.

experiments

we evaluate the proposed model on two benchmark tasks:
link prediction and triples classi   cation. experiments are
conducted on four public datasets that are the subsets of
id138 and freebase. the statistics of these datasets are
listed in tab.1.

atpe is short for    averaged triple number per en-
tity   . this quantity measures the diversity and complexity
of datasets. commonly, more triples lead to more complex
structures of id13. to express the more com-
plex structures, entities would be distributed variously and
complexly. overall, embedding methods produce less satis-
factory results in the datasets with higher atpe, because a
large atpe means a various and complex entities/relations
embedding situation.

link prediction
link prediction aims to predict a missing entity given the
other entity and the relation. in this task, we predict t given
(h, r,   ), or predict h given (   , r, t). the wn18 and fb15k
datasets are the benchmark datasets for this task.

evaluation protocol. we follow the same protocol as
used in transe (bordes et al. 2013), transh (wang et al.
2014) and transr (lin et al. 2015). for each testing triple
(h, r, t), we replace the tail t by every entity e in the knowl-
edge graph and calculate a dissimilarity score with the score

datasets

metric

se(bordes et al. 2011)
sme (bordes et al. 2012)
lfm (jenatton et al. 2012)
transe (bordes et al. 2013)
transh (wang et al. 2014)
transr (lin et al. 2015)
adaptive metric (psd)

transa

table 2: evaluation results on link prediction

wn18

fb15k

hits@10(%) mean rank

mean rank
raw
1,011
545
469
263
401
238
289
405

filter raw
68.5
985
533
65.1
71.4
456
75.4
251
73.0
388
225
79.8
77.6
278
82.3
392

raw filter raw
28.8
273
274
30.7
26.0
283
34.9
243
45.7
212
198
48.2
52.4
172
155
56.1

162
154
164
125
87
77
88
74

hits@10(%)
filter
39.8
40.8
33.1
47.1
64.4
68.7
74.2
80.4

filter
80.5
74.1
81.6
89.2
82.3
92.0
89.6
94.3

data
#rel
#ent
#train
#valid
#test
atpe 2

table 1: statistics of datasets
wn18
wn11

fb15k
1,345
14,951
483,142
50,000
59,071
39.61

fb13
13

75,043
316,232
5,908
23,733
4.61

11

38,696
112,581
2,609
10,544
3.25

18

40,943
141,442
5,000
5,000
3.70

function fr(h, e) for the corrupted triple (h, r, e). ranking
these scores in ascending order, we then get the rank of the
original correct triple. there are two metrics for evaluation:
the averaged rank (mean rank) and the proportion of test-
ing triples, whose ranks are not larger than 10 (hits@10).
this is called    raw    setting. when we    lter out the cor-
rupted triples that exist in all the training, validation and test
datasets, this is the   filter    setting. if a corrupted triple exists
in the id13, ranking it before the original triple
is acceptable. to eliminate this issue, the    filter    setting is
more preferred. in both settings, a lower mean rank or a
higher hits@10 is better.

implementation. as the datasets are the same, we di-
rectly copy the experimental results of several baselines
from the literature, as in (bordes et al. 2013), (wang et al.
2014) and (lin et al. 2015). we have tried several settings
on the validation dataset to get the best con   guration for
both adaptive metric (psd) and transa. under the    bern.   
sampling strategy, the optimal con   gurations are: learning
rate    = 0.001, embedding dimension k = 50,    = 2.0,
c = 0.2 on wn18;    = 0.002, k = 200,    = 3.2, and
c = 0.2 on fb15k.

results. evaluation results on wn18 and fb15k are re-
ported in tab.2 and tab.3, respectively. we can conclude
that:
1. transa outperforms all the baselines signi   cantly and
justi   es the effectiveness of

consistently. this result
transa.
2atpe:averaged triple number per entity. triples are summed

up from all the #train, #valid and #test.

2. fb15k is a very various and complex entities/relations
embedding situation, because its atpe is absolutely high-
est among all the datasets. however, transa performs
better than other baselines on this dataset, indicating
that transa performs better in various and complex en-
tities/relations embedding situation. wn18 may be less
complex than fb15k because of a smaller atpe. com-
pared to transe, the relative improvement of transa on
wn18 is 5.7% while that on fb15k is 95.2%. this com-
parison shows transa has more advantages in the various
and complex embedding environment.

3. transa promotes the performance for 1-1 relations, which
means transa generally promotes the performance on
simple relations. transa also promotes the performance
for 1-n, n-1, n-n relations3, which demonstrates transa
works better for complex relation embedding.

4. compared to transr, better performance of transa
means the feature weighting and the generalised metric
form leaded by absolute operators, have signi   cant bene-
   ts, as analysed.

5. compared to adaptive metric (psd) which applies the
score function fr(h, t) = (h + r     t)(cid:62)wr(h + r     t)
and constrains wr as psd, transa is more competent,
because our score function with non-negative matrix con-
dition and absolute operator produces a more    exible rep-
resentation than that with psd matrix condition does, as
analysed in    adaptive metric approach   .

6. transa performs bad in mean rank on wn18 dataset.
digging into the detailed situation, we discover there are
27 testing triples (0.54% of the testing set) whose ranks
are more than 30,000, and these few cases would make
about 162 mean rank loss. the tail or head entity of all
these triples have never been co-occurring with the corre-
sponding relation in the training set. it is the insuf   cient
training data that leads to the over-distorted weight matrix
and the over-distorted weight matrix is responsible for the
bad mean rank.

3mapping properties of relations follow the same rules in (bor-

des et al. 2013).

table 3: evaluation results on fb15k by mapping properties of relations(%)

tasks

relation category

se(bordes et al. 2011)
sme (bordes et al. 2012)
transe (bordes et al. 2013)
transh (wang et al. 2014)
transr (lin et al. 2015)

transa

predicting head(hits@10)
n-n
1-1
37.5
35.6
40.3
35.1
43.7
47.2
64.5
66.8
69.2
78.8
86.8
77.8

1-n n-1
17.2
62.6
19.0
53.7
65.7
18.2
28.7
87.6
34.1
89.2
95.4
42.7

predicting tail(hits@10)
n-n
1-1
41.3
34.9
43.3
32.7
43.7
50.0
67.2
65.5
72.1
79.2
86.7
80.6

1-n n-1
68.3
14.6
61.6
14.9
19.7
66.7
83.3
39.8
90.4
37.4
54.3
94.4

table 4: triples classi   cation: accuracies(%) for different
embedding methods
methods

lfm
ntn
transe
transh
transr

adaptive metric (psd)

transa

wn11
73.8
70.4
75.9
78.8
85.9
81.4
83.2

fb13 avg.
79.0
84.3
78.8
87.1
78.7
81.5
83.3
81.1
84.2
82.5
84.3
87.1
87.3
85.3

triples classi   cation
triples classi   cation is a classical task in knowledge base
embedding, which aims at predicting whether a given triple
(h, r, t) is correct or not. our evaluation protocol is the same
as prior studies. besides, wn11 and fb13 are the bench-
mark datasets for this task. evaluation of classi   cation needs
negative labels. the datasets have already been built with
negative triples, where each correct triple is corrupted to get
one negative triple.

evaluation protocol. the decision rule is as follows: for
a triple (h, r, t), if fr(h, t) is below a threshold   r, then posi-
tive; otherwise negative. the thresholds {  r} are determined
on the validation dataset. the    nal accuracy is based on how
many triples are classi   ed correctly.

implementation. as all methods use the same datasets,
we directly copy the results of different methods from the
literature. we have tried several settings on the validation
dataset to get the best con   guration for both adaptive metric
(psd) and transa. the optimal con   gurations are:    bern   
sampling,    = 0.02, k = 50,    = 10.0, c = 0.2 on wn11,
and    bern    sampling,    = 0.002, k = 200,    = 3.0, c =
0.00002 on fb13.

results. accuracies are reported in tab.4 and fig.3. ac-
cording to    adaptive metric approach    section, we could
work out the weights by ldl decomposition for each
relation. because the minimal weight is too small to make
a signi   cant analysis, we choose the median one to rep-
resent relative small weight. thus,    weight difference    is
. bigger the
calculated by
weight difference is, more signi   cant effect, the feature
weighting makes. notably, scaling by the median weight

(cid:16) m aximalw eight   m edianw eight

m edianw eight

(cid:17)

figure 3: triples classi   cation accuracies for each relation
on wn11(left) and fb13(right). the    weight difference    is
worked out by the scaled difference between maximal and
median weight.

makes the weight differences comparable to each other. we
observe that:

1. overall, transa yields the best average accuracy, illus-

trating the effectiveness of transa.

2. accuracies vary with the weight difference, meaning the
feature weighting bene   ts the accuracies. this proves the
theoretical analysis and the effectiveness of transa.

3. compared to adaptive metric (psd) , transa performs
better, because our score function with non-negative ma-
trix condition and absolute operator leads to a more    ex-
ible representation than that with psd matrix condition
does.

conclusion

in this paper, we propose transa, a translation-based knowl-
edge graph embedding method with an adaptive and    ex-
ible metric. transa applies elliptical equipotential hyper-
surfaces to characterise the embedding topologies and
weights several speci   c feature dimensions for a relation to
avoid much noise. thus, our adaptive metric approach could
effectively model various and complex entities/relations
in knowledge base. experiments are conducted with two
benchmark tasks and the results show transa achieves con-
sistent and signi   cant improvements over the current state-
of-the-art baselines. to reproduce our results, our codes and
data will be published in github.

[lin, liu, and sun 2015] lin, y.; liu, z.; and sun, m. 2015.
modeling relation paths for representation learning of
knowledge bases. proceedings of the 2015 conference
on empirical methods in natural language processing
(emnlp). association for computational linguistics.
[miller 1995] miller, g. a.
1995. id138: a lexi-
cal database for english. communications of the acm
38(11):39   41.
[nickel, tresp, and kriegel 2011] nickel, m.; tresp, v.; and
kriegel, h.-p. 2011. a three-way model for collective learn-
ing on multi-relational data. in proceedings of the 28th inter-
national conference on machine learning (icml-11), 809   
816.
[nickel, tresp, and kriegel 2012] nickel, m.; tresp, v.; and
kriegel, h.-p. 2012. factorizing yago: scalable machine
learning for linked data. in proceedings of the 21st interna-
tional conference on world wide web, 271   280. acm.
[socher et al. 2013] socher, r.; chen, d.; manning, c. d.;
and ng, a. 2013. reasoning with neural tensor networks
for knowledge base completion. in advances in neural in-
formation processing systems, 926   934.
[wang and sun 2014] wang, f., and sun, j. 2014. survey
on distance metric learning and id84 in
data mining. data mining and knowledge discovery 1   31.
[wang et al. 2014] wang, z.; zhang, j.; feng, j.; and chen,
z. 2014. id13 embedding by translating on hy-
perplanes. in proceedings of the twenty-eighth aaai con-
ference on arti   cial intelligence, 1112   1119.
[wang, wang, and guo 2015] wang, q.; wang, b.; and guo,
l. 2015. knowledge base completion using embeddings
in proceedings of the 24th international joint
and rules.
conference on arti   cial intelligence.

references

[bao et al. 2014] bao, j.; duan, n.; zhou, m.; and zhao, t.
2014. knowledge-based id53 as machine
translation. cell 2:6.
[bollacker et al. 2008] bollacker, k.; evans, c.; paritosh, p.;
sturge, t.; and taylor, j. 2008. freebase: a collaboratively
created graph database for structuring human knowledge. in
proceedings of the 2008 acm sigmod international con-
ference on management of data, 1247   1250. acm.
[bordes et al. 2011] bordes, a.; weston, j.; collobert, r.;
bengio, y.; et al. 2011. learning structured embeddings of
knowledge bases. in proceedings of the twenty-   fth aaai
conference on arti   cial intelligence.
[bordes et al. 2012] bordes, a.; glorot, x.; weston, j.; and
bengio, y. 2012.
joint learning of words and meaning
representations for open-text id29. in interna-
tional conference on arti   cial intelligence and statistics,
127   135.
[bordes et al. 2013] bordes, a.; usunier, n.; garcia-duran,
a.; weston, j.; and yakhnenko, o. 2013. translating em-
beddings for modeling multi-relational data. in advances in
neural information processing systems, 2787   2795.
[bordes et al. 2014] bordes, a.; glorot, x.; weston, j.; and
bengio, y. 2014. a semantic matching energy function
for learning with multi-relational data. machine learning
94(2):233   259.
[collobert and weston 2008] collobert, r., and weston, j.
2008. a uni   ed architecture for natural language process-
ing: deep neural networks with multitask learning. in pro-
ceedings of the 25th international conference on machine
learning, 160   167. acm.
[fader, zettlemoyer, and etzioni 2014] fader, a.; zettle-
moyer, l.; and etzioni, o. 2014. open id53
in proceed-
over curated and extracted knowledge bases.
ings of the 20th acm sigkdd international conference on
knowledge discovery and data mining, 1156   1165. acm.
[fan et al. 2014] fan, m.; zhou, q.; chang, e.; and zheng,
t. f. 2014. transition-based id13 embedding
in proceedings of the
with relational mapping properties.
28th paci   c asia conference on language, information, and
computation, 328   337.
[golub and van loan 2012] golub, g. h., and van loan,
c. f. 2012. matrix computations, volume 3. jhu press.
[guo et al. 2015] guo, s.; wang, q.; wang, b.; wang, l.;
and guo, l. 2015. semantically smooth id13
embedding. in proceedings of acl.
[jenatton et al. 2012] jenatton, r.; roux, n. l.; bordes, a.;
and obozinski, g. r. 2012. a latent factor model for highly
in advances in neural information
multi-relational data.
processing systems, 3167   3175.
[kulis 2012] kulis, b. 2012. metric learning: a survey.
foundations & trends in machine learning 5(4):287   364.
[lin et al. 2015] lin, y.; liu, z.; sun, m.; liu, y.; and zhu,
x.
2015. learning entity and relation embeddings for
id13 completion. in proceedings of the twenty-
ninth aaai conference on arti   cial intelligence.

