6
1
0
2

 

p
e
s
7

 

 
 
]
l
c
.
s
c
[
 
 

5
v
1
3
5
0
0

.

2
1
5
1
:
v
i
x
r
a

benchmarking id31 methods for large-scale texts: a case for using

continuum-scored words and word shift graphs.

andrew j. reagan,1, 2 christopher m. danforth,1, 2 brian

tivnan,2, 3 jake ryland williams,4 and peter sheridan dodds1, 2
1department of mathematics & statistics, computational story lab,

& the vermont advanced computing core, university of vermont, burlington, vt, 05405

2vermont complex systems center, university of vermont, burlington, vt, 05405

3the mitre corporation, 7525 colshire drive, mclean, va, 22102

4school of information, university of california, berkeley, berkeley, ca, 94720

(dated: september 8, 2016)

the emergence and global adoption of social media has rendered possible the real-time estimation
of population-scale sentiment, issuing profound implications for our understanding of human behav-
ior. given the growing assortment of sentiment measuring instruments, comparisons between them
are evidently required. here, we perform detailed, quantitative tests and qualitative assessments of
6 dictionary-based methods applied to 4 di   erent corpora, and brie   y examine a further 20 methods.
we show that a dictionary-based method will only perform both reliably and meaningfully if (1)
the dictionary covers a su   ciently large enough portion of a given text   s lexicon when weighted by
word usage frequency; and (2) words are scored on a continuous scale.

i.

introduction

as we move further into what might be called the
sociotechnocene   with increasingly more interactions,
decisions, and impact being made by globally distributed
people and algorithms   the myriad human social dynam-
ics that have shaped our history have become far more
visible and measurable than ever before. driven by the
broad implications of being able to characterize social
systems in microscopic detail, sentiment detection for
populations at all scale has become a prominent research
arena. attempts to leverage online expression for senti-
ment mining include prediction of stock markets [1   4],
assessing responses to advertising, real-time monitoring
of global happiness [5], and measuring a health-related
quality of life [6]. the diverse set of instruments pro-
duced by this work now provide indicators that help
scientists understand collective behavior, inform public
policy makers, and in industry, gauge the sentiment of
public response to marketing campaigns. given their
widespread usage and potential to in   uence social sys-
tems, understanding how these instruments perform and
how they compare with each other has become an imper-
ative. benchmarking their performance both focuses
future development and provides practical advice to non-
experts in selecting a dictionary.

we identify sentiment detection methods as belonging
to one of three categories, each carrying their own advan-
tages and disadvantages:

1. dictionary-based methods [5, 7   11],

2. supervised learning methods [10], and

3. unsupervised (or deep) learning methods [12].

here, we focus on dictionary-based methods, which all
center around the determination of a text t    s average

n(cid:88)

(cid:80)n

(cid:80)n

happiness (sometimes referred to as valence) through the
equation:

i=1 havg(wi)    fi(t )

=

i=1

i=1 fi(t )

havg(t ) =

as pi(t ) = fi(t )/(cid:80)n

havg(wi)    pi(t ),
(1)
where we denote each of the n words in a given dic-
tionary as wi, word sentiment scores as havg(wi), word
frequency as fi(t ), and normalized frequency of wi in t
i=1 fi(t ). in this way, we measure
the happiness of a text in a manner analogous to taking
the temperature of a room. while other simple happi-
ness scores may be considered, we will see that analyzing
individual word contributions is important and that this
equation allows for a straightforward, meaningful inter-
pretation.

dictionary-based methods rely upon two distinct
advantages we will capitalize on: (1) they are in prin-
ciple corpus agnostic (including those without training
data available) and (2) in contrast to black box (high-
ly non-linear) methods, they o   er the ability to    look
under the hood    at words contributing to a particular
score through    word shifts    (de   ned fully later; see also
[13, 14]). indeed, if we are at all concerned with under-
standing why a particular scoring method varies   e.g,,
our undertaking is scienti   c   then word shifts are essen-
tial tools. in the absence of word shifts or similar, any
explanation of sentiment trends is missing crucial infor-
mation and rises only to the level of opinion or guess-
work [15   18].

as all methods must, dictionary-based    bag-of-words   
approaches su   er from various drawbacks, and three are
worth stating up front. first, they are only applicable
to corpora of su   cient size, well beyond that of a single
sentence (widespread usage in this misplaced fashion does
not su   ce as a counterargument). we directly verify this
assertion on individual tweets,    nding that some dictio-

naries perform admirably, however the average (medi-
an) f1-score on the sts-gold data set is 0.50 (0.54)
from all datasets (table s1), others having shown similar
results for dictionary methods with short text [19]. sec-
ond, state-of-the-art learning methods with a su   ciently
large training set for a speci   c corpus will outperform
dictionary-based methods on same corpus [20]. however,
in practice the domains and topics to which sentiment
analysis are applied are highly varied, such that training
to a high degree of speci   city for a single corpus may not
be practical and, from a scienti   c standpoint, will severe-
ly constrain attempts to detect and understand universal
patterns. third: words may be evaluated out of context
or with the wrong meaning. a simple example is the
word    new    occurring frequently when evaluating arti-
cles in the new york times. this kind of contextual
error is something we can readily identify and correct for
through word shift graphs, but would remain hidden to
nonlinear learning methods without new training.

we lay out our paper as follows. we list and describe
the dictionary-based methods we consider in sec. ii,
and outline the corpora we use for tests in sec. ii b.
we present our results in sec. iii, comparing all meth-
ods in how they perform for speci   c analyses of the
new york times (nyt) (sec. iii a), movie reviews
(sec. iii b), google books (sec. iii c), and twitter
(sec. iii d). in sec. iii e, we make some basic com-
parisons between dictionary-based methods and machine
learning approaches. we bolster our    ndings with    gures
in the supporting information, and provide concluding
remarks in sec. iv.

ii. dictionaries, corpora, and word

shift graphs

a. dictionaries

the words    dictionary,       lexicon,    and    corpus    are
often used interchangeably, and for clarity we de   ne our
usage as follows.

2

ol     opinion lexicon, developed by bing liu [10].

we also make note of 18 other dictionaries:

panas-x     the positive and negative a   ect sched-

ule     expanded [21].

pattern     a web mining module for the python pro-

gramming language, version 2.6 [22].

sentiid138     id138 synsets each assigned three
sentiment scores: positivity, negativity, and objec-
tivity [23].

afinn     words manually rated -5 to 5 with impact

scores by finn nielsen [24].

gi     general inquirer: database of words and manually
created semantic and cognitive categories, includ-
ing positive and negative connotations [25].

wdal     whissel   s dictionary of a   ective language:
words rated in terms of their pleasantness, activa-
tion, and imagery (concreteness) [26].

emolex     nrc word-emotion association lexicon:
emotions and sentiment evoked by common words
and phrases using mechanical turk [27].

maxdi        nrc maxdi    twitter sentiment lexicon:
crowdsourced real-valued scores using the maxdi   
method [28].

hashtagsent     nrc hashtag sentiment lexicon: cre-
ated from tweets using pairwise mutual informa-
tion with sentiment hashtags as positive and nega-
tive labels (here we use only the unigrams) [29].

sent140lex     nrc sentiment140 lexicon: created
from the    sentiment140    corpus of tweets, using
pairwise mutual information with emoticons as
positive and negative labels (here we use only the
unigrams) [30].

socal     manually constructed general-purpose sen-

timent dictionary [31].

senticnet     sentiment dataset labeled with semantics
and 5 dimensions of emotions by cambria et al.,
version 3 [32].

emoticons     commonly used emoticons with their

dictionary: set of words (possibly including word

positive, negative, or neutral emotion [33].

stems) with ratings.

corpus: collection of texts which we seek to analyze.
lexicon: the words contained within a corpus (often

said to be    tokenized   ).

we test the following six dictionaries in depth:

labmt     language assessment by mechanical turk [5].
anew     a   ective norms of english words [7].
wk     warriner and kuperman rated words from sub-

tlex by mechanical turk [11].

mpqa     the multi-perspective id53
liwc{01,07,15}     linguistic inquiry and word

(mpqa) subjectivity dictionary [9].

count, three versions [8].

sentistrength     an api and java program for general
purpose sentiment detection (here we use only the
sentiment dictionary) [34].

vader     method developed speci   cally for twitter

and social media analysis [35].

umigon     manually built

speci   cally to analyze

tweets from the sentiment140 corpus [36].

usent     set of emoticons and bad words that extend

mpqa [37].

emosenticnet     extends senticnet words with wna

labels [38].

all of these dictionaries were produced by academ-
ic groups, and with the exception of liwc, they are
provided free of charge.
in table i, we supply the

# pos # neg construction

license

ref.

3

survey: mt, 50 ratings
survey: fsu psych 101
manual

cc
free for research
paid, commercial
gnu gpl
free
survey: mt, at least 14 ratings cc
manual
manual
manual
unspeci   ed

[5]
[7]
[8]
[9]
[10]
[11]
[8]
[8]

dictionary

# fixed # stems total range

10222
1034
2145
5587
6782
13915
1232
4071
20
1528

labmt
anew
liwc07
mpqa
ol
wk
liwc01
liwc15
panas-x
pattern
sentiid138 147700
afinn
gi
wdal
emolex
maxdi   
hashtagsent
sent140lex
socal
senticnet
emoticons
sentistrength 1270
7502
vader
927
umigon
usent
592
emosenticnet 13188

2477
3629
8743
14182
1515
54129
62468
7494
30000
132

0
0
2338
1605
0
0
1090
2478
0
0
0
0
0
0
0
0
0
0
0
0
0
1345
0
0
0
0

1.3     8.5
10222
1.2     8.8
1034
4483
[-1,0,1]
7192
[-1,0,1]
6782
[-1,1]
1.3     8.5
13915
2322
[-1,0,1]
6549
[-1,0,1]
20
[-1,1]
-1.0     1.0
1528
147700 -1.0     1.0
2477
3629
8743
14182
1515
54129
62468
7494
30000
132
2615
7502
927
592
13188

[-5,-4, . . .,4,5]
[-1,1]
0.0     3.0
[-1,0,1]
-1.0     1.0
-6.9     7.5
-5.0     5.0
-30.2     30.7
-1.0     1.0
[-1,0,1]
[-5,-4, . . .,4,5]
-3.9     3.4
[-1,1]
[-1,1]
[-10,-2,-1,0,1,10] 9332

2977
449
500
4342 manual + ml
4779 dictionary propagation
5945
344
746
10
679

7152
584
406
2393
2003
7761
266
642
10
575
17677 20410 synset synonyms
878
1631
6517
2231
775
32048 22081 pmi with hashtags
38312 24156 pmi with emoticons
3325
16715 13285 label propogation
58
601
3333
334
63

48
2002
4169 mt survey, 10 ratings
593
529
1480 bootstrapped extension

1598 manual
1998 harvard-iv-4
1778
3243
726

manual
liwc+gi

manual
manual

4169 manual

survey: columbia students
survey: mt
survey: mt, maxdi   

paid, commercial
paid, commercial
copyrighted paper [21]
[22]
bsd
[23]
cc by-sa 3.0
[24]
odbl v1.0
[25]
unspeci   ed
[26]
unspeci   ed
[27]
free for research
free for research
[28]
[29]
free for research
[30]
free for research
gnu gpl
[31]
citation requested [32]
[33]
open source code
unknown
[34]
[35]
freely available
[36]
public domain
[37]
cc
non-commercial
[38]

table i: summary of dictionary attributes used in sentiment measurement instruments. we provide all acronyms and
abbreviations and further information regarding dictionaries in sec. ii a. we test the    rst 6 dictionaries extensively. # fixed,
# stems, # pos and # neg refer to the numbers of: terms in the dictionary that are    xed words, stems used to match words,
terms that are rated above neutral, and terms rated below neutral. the range indicates whether scores are continuous or binary
(we use the term binary for dictionaries for which words are scored as   1 and optionally 0).

main aspects   such as word count, score type (contin-
uum or binary), and license information   for the dictio-
naries listed above.
in the github repository associat-
ed with our paper, https://github.com/andyreagan/
sentiment-analysis-comparison, we include all of the
dictionaries but liwc.

the labmt, anew, and wk dictionaries have scores
ranging on a continuum from 1 (low happiness) to 9 (high
happiness) with 5 as neutral, whereas the others we test
in detail have scores of   1, and either explicitly or implic-
itly 0 (neutral). we will refer to the latter dictionaries
as being binary, even if neutral is included. other non-
binary ranges include a continuous scale from -1 to 1
(sentiid138), integers from -5 to 5 (afinn), contin-
uous from 1 to 3 (gi), and continuous from -5 to 5 (nrc).
for coverage tests, we include all available words, to gain
a full sense of the breadth of each dictionary. in scoring,
we do not include neutral words from any dictionary.

we test the labmt, anew, and wk dictionaries for
a range of stop words (starting with the removal of words
scoring within    h = 1 of the neutral score of 5) [14]. the
ability to remove stop words is one advantage of dictio-
naries that have a range of scores, allowing us to tune
the instrument for maximum performance, while retain-
ing all of the bene   ts of a dictionary method. we will
show that, in agreement with the original paper intro-
ducing labmt and looking at twitter data, a    h = 1 is
a pragmatic choice in general [14].

since we do not apply a part of speech tagger, when
using the mpqa dictionary we are obliged to exclude
words with scores of both +1 and -1. the words and
stems with both scores are: blood, boast* (we denote

stems with an asterisk), conscience, deep, destiny, keen,
large, and precious. we choose to match a text   s words
using the    xed word set from each dictionary before
stems, hence words with overlapping matches (a    xed
word that also matches a stem) are    rst matched by the
   xed word.

b. corpora tested

for each dictionary, we test both the coverage and the
ability to detect previously observed and/or known pat-
terns within each of the following corpora, noting the
pattern we hope to discern:

1. the new york times (nyt) [39]: goal of ranking

sections by sentiment (sec. iii a).

2. movie reviews [40]: goal of discerning positive and

negative reviews (sec. iii b).

3. google books [41]: goal of creating time series

(sec. iii c).

4. twitter: goal of creating time series (sec. iii d).

for the corpora other than the movie reviews and small
numbers of tagged tweets, there is no publicly available
ground truth sentiment, so we instead make comparisons
between methods and examine how words contribute to
scores. we note that comparison to societal measures of
well being would also be possible [42]. we o   er greater
detail on corpus processing below, and we also provide
the relevant scripts on github at https://github.com/
andyreagan/sentiment-analysis-comparison.

c. word shift graphs

id31 is applied in many circumstances in
which the goal of the analysis goes beyond simply cate-
gorizing text into positive or negative. indeed if this were
the only use case, the value added by sentiment analy-
sis would be severely limited. instead we use sentiment
analysis methods as a lens that allow us to see how the
emotive words in a text shape the overall content. this is
accomplished by analyzing each word for the individual
contribution to the sentiment score (or to the di   erence
in the sentiment score between two texts). in either case,
we need to consider the words ranked by this individual
contribution.

of the four corpora that we use as benchmarks, three
rely on this type of qualitative analysis: using the dictio-
nary as a tool to better understand the sentiment of the
corpora. for this case, we must    rst    nd the contribu-
tion of each word individually. starting with two texts,
we take the di   erence of their sentiment scores, rearrange
a few things, and arrive at

(cid:88)

w   l

(cid:104)
(cid:124)

(cid:105)
(cid:125)

(cid:104)
(cid:124)

(cid:105)
(cid:125)

(cid:123)(cid:122)
    p(ref)
   /   

w

(cid:123)(cid:122)

+/   

h(comp)
avg    h(ref)

avg =

havg(w)     h(ref)

avg

p(comp)
w

each word contributes to the word shift according to its
happiness relative to the reference text (+/    = more/less
emotive), and its change in frequency of usage (    /    
= more/less frequent). as a    rst step, it is possible to
visualize this sorted word list in a table, along with the
basic indicators of how its contribution is constituted.
we use word shift graphs to present this information in
the most accessible manner to advanced users.
the

detail,

refer

for

we

to

er
http://www.uvm.edu/storylab/2014/10/06/.

instructional

post

and

read-
at

video

further
our

iii. results

in fig 1, we show a direct comparison between word
scores for each pair of the 6 dictionaries tested. over-
all, we    nd strong agreement between all dictionaries
with exceptions we note below. as a guide, we will pro-
vide more detail on the individual comparison between
the labmt dictionary and the other    ve dictionaries by
examining the words whose scores disagree across dictio-
naries shown in fig 2. we refer the reader to the s2
appendix for the remaining individual comparisons.

to start with, consider the comparison the labmt and
anew on a word for word basis. because these dictio-
naries share the same range of values, a scatterplot is
the natural way to visualize the comparison. across the
top row of fig 1, which compares labmt to the other
5 dictionaries, we see in panel b for the labmt-anew
comparison that the rma best    t [43] is

hwlabmt = 0.92     hwanew + 0.40

.

4

for words wlabmt in labmt and words wanew in
anew. the 10 words with farthest from the line of
best    t shown in panel b of fig 2 are, with labmt
and anew scores:
lust (4.64, 7.12), bees (5.60, 3.20),
silly (5.30, 7.41), engaged (6.16, 8.00), book (7.24, 5.72),
hospital (3.50, 5.04), evil (1.90, 3.23), gloom (3.56, 1.88),
anxious (3.42, 4.81), and    ower (7.88, 6.64). these are
words whose individual ranges have high standard devi-
ations in labmt. while the overall agreement is very
good, we should expect some variation in the emotional
associations of words, due to chance, time of survey, and
demographic variability.
indeed, the mechanical turk
users who scored the words for the labmt set in 2011
are evidently di   erent from the university of florida stu-
dents who took the anew survey before 2000 as a class
requirement for introductory psychology.

comparing labmt with wk in panel c of fig 1,
we again    nd a    t with slope near 1, and a smaller
positive shift: hwlabmt = 0.96     hwwk + 0.26. the 10
words farthest from this line, shown in panel b of fig 2,
are (labmt, wk): sue (4.30, 2.18), boogie (5.86, 3.80),
exclusive (6.48, 4.50), wake (4.72, 6.57), federal (4.94,
3.06), stroke (2.58, 4.19), gay (4.44, 6.11), patient (5.04,
6.71), user (5.48, 3.67), and blow (4.48, 6.10). like
labmt, the wk dictionary used a mechanical turk
online survey to gather word ratings. we speculate that
the minor variation is due in part to the low number of
scores required for each word in the wk survey, with as
few as 14 ratings per words and 18 ratings for the major-
ity of the words. by contrast, labmt scores represent
50 ratings of each word. for an in depth comparison, see
reference [17].

next,

in comparing binary dictionaries with   1 or
  1, 0 scores to one with a 1   9 range, we can look at the
distribution of scores within the continuum score dictio-
nary for each score in the binary dictionary. looking at
the labmt-mpqa comparison in panel d of fig 1, we
see that most of the matches are between words without
stems (blue histograms), and that each score in -1, 0, +1
from mpqa corresponds to a distribution of scores in
labmt. to examine deviations, we take the words from
labmt sorted by happiest when mpqa is -1, both the
happiest and the least happy when mpqa is 0, and the
least happy when mpqa is 1 (fig 2 panels c-e). the 10
happiest words in labmt matched by mpqa words with
score -1 are: moonlight (7.50), cutest (7.62),    nest (7.66),
funniest (7.76), comedy (7.98), laughs (8.18), laughing
(8.20), laugh (8.22), laughed (8.26), laughter (8.50). this
is an immediately troubling list of evidently positive
words somehow rated as -1 in mpqa. we also see that
the top 5 are matched by the stem    laugh*    in mpqa.
the least happy 5 words and happiest 5 words in labmt
matched by words in mpqa with score 0 are: sorrows
(2.69), screaming (2.96), couldn   t (3.32), pressures (3.49),
couldnt (3.58), and baby (7.28), precious (7.34), strength
(7.40), surprise (7.42), song (7.58). again, we see mpqa
word scores are questionable. the least happy words in
labmt with score +1 in mpqa that are matched by

5

fig. 1: direct comparison of the words in each of the dictionaries tested. for the comparison of two dictionaries, we plot words
that are matched by the independent variable    x    in the dependent variable    y   . because of this, and cross stem matching,
the plots are not symmetric across the diagonal of the entire    gure. where the scores are continuous in both dictionaries, we
compute the rma linear    t. when a dictionary contains both    xed and stem words, we plot the matches by    xed words in
blue and by stem words in green. the axes in the bar plots are not of the same height, due to large mismatches in the number
of words in the dictionaries, and we note the maximum height of the bar in the upper left of such plots. detailed analysis of
panel c can be found in [17]. we provide a table for each o   -diagonal panel in the s2 appendix with the words whose scores
exhibit the greatest mismatch, and a subset of these tables in fig 2.

mpqa are: vulnerable (3.34), court (3.78), sanctions
(3.86), defendant (3.90), conviction (4.10), backwards
(4.22), courts (4.24), defendants (4.26), court   s (4.44),
and correction (4.44). clearly, these words are not posi-
tive words in most contexts.

while it would be simple to correct these ratings in the
mpqa dictionary going forward, we have are naturally
led to be concerned about existing work using mpqa.
we note again that the use of word shifts of some kind
would have exposed these problematic scores immediate-

2345678labmt  =1.00  =0.00  =0.92  =0.40  =0.96  =0.26nmax=80nmax=300nmax=602345678anew  =1.08  =-0.44  =1.00  =0.00  =1.07  =-0.30nmax=25nmax=20nmax=252345678wk  =1.04  =-0.27  =0.93  =0.28  =1.00  =0.00nmax=160nmax=250nmax=160   101mpqanmax=70nmax=25nmax=160nmax=4500nmax=1000nmax=3500   101liwcnmax=180nmax=14nmax=120nmax=250nmax=4000nmax=1002345678labmt   101liunmax=602345678anewnmax=252345678wknmax=160   101mpqanmax=4500   101liwcnmax=1200   101liunmax=5000abcdefghijklmnopqrstuvwxyzaabbccddeeffgghhiijjly.

for the labmt-liwc comparison in panel e of fig 1
we examine the same matched word lists as before. the
10 happiest words in labmt matched by words in liwc
with score -1 are: trick (5.22), shakin (5.29), number
(5.30), geek (5.34), tricks (5.38), defence (5.39), dwell
(5.47), doubtless (5.92), numbers (6.04), shakespeare
(6.88). from panel f of fig 2, the least happy 5 neutral
words and happiest 5 neutral words in liwc, matched
with liwc, are: negative (2.42), lack (3.16), couldn   t
(3.32), cannot (3.32), never (3.34), millions (7.26), cou-
ple (7.30), million (7.38), billion (7.56), millionaire (7.62).
the least happy words in labmt with score +1 in liwc
that are matched by liwc are: merrill (4.90), richardson
(5.02), dynamite (5.04), careful (5.10), richard (5.26), sil-
ly (5.30), gloria (5.36), securities (5.38), boldface (5.40),
treasury   s (5.42). the +1 and -1 words in liwc match
some neutral words in labmt, which is not alarming.
however, the problems with the    neutral    words in the
liwc set are immediate: these are not emotionally neu-
tral words. the range of scores in labmt for these 0-
score words in liwc formed the basis for garcia et al.   s
response to [5], and we point out here that the authors
must have not looked at the words, and all-too-common
problem in studies using id31 [16, 17].

for the labmt-ol comparison in panel e of fig 1
we again examine the same matched word lists as before,
except the neutral word list because ol has no explic-
it neutral words. the 10 happiest words in labmt
matched by ol   s negative list are: myth (5.90), puppet
(5.90), skinny (5.92), jam (6.02), challenging (6.10),    c-
tion (6.16), lemon (6.16), tenderness (7.06), joke (7.62),
funny (7.92). the least happy words in labmt with
score +1 in ol that are matched by ol are: defeat-
ed (2.74), defeat (3.20), envy (3.33), obsession (3.74),
tough (3.96), dominated (4.04), unreal (4.57), striking
(4.70), sharp (4.84), sensitive (4.86). despite nearly
twice as many negative words in ol as positive words
(at odds with the frequency-dependent positivity bias of
language [5]), these dictionaries generally agree.

a. new york times word shift analysis

the new york times corpus [39]

is split into 24
sections of the newspaper that are roughly contiguous
throughout the data from 1987   2008. with each dictio-
nary, we rate each section and then compute word shifts
(described below) against the baseline, and produce a
happiness ranked list of the sections.
in the    rst fig-
ure in s4 appendix we show scatterplots for each com-
parison, and compute the reduced major axes (rma)
regression    t [43]. in the second figure in s4 appendix
we show the sorted bar chart from each dictionary.

to gain understanding of the sentiment expressed by
any given text relative to another text, it is necessary to
inspect the words which contribute most signi   cantly by
their emotional strength and the change in frequency of

6

usage. we do this through the use of word shift graphs,
which plot the contribution of each word wi from the dic-
tionary (denoted   havg(wi)) to the shift in average hap-
piness between two texts, sorted by the absolute value of
the contribution. we use word shift graphs to both ana-
lyze a single text and to compare two texts, here focusing
on comparing text within corpora. for a derivation of the
algorithm used to make word shift graphs while separat-
ing the frequency and sentiment information, we refer the
reader to equations 2 and 3 in [14]. we consider both
the sentiment di   erence and frequency di   erence parts of
  havg(wi) by writing each term of eq. 1 as in [14]:

  havg(wi) =

100

havg(wi)     havg(tref)
havg(tcomp)     havg(tref)

[pi(tcomp)     pi(tref)] .

(2)

an in-depth explanation of how to interpret the word
shift graph can also be found at http://hedonometer.
org/instructions.html#wordshifts.

to both demonstrate the necessity of using word shift
graphs in carrying out id31, and to gain
understanding about the ranking of new york times
sections by each dictionary, we look at word shifts for
the    society    section of the newspaper from each dictio-
nary in fig 3, with the reference text being the whole of
the new york times. the    society    section happiness
ranks 1, 1, 1, 18, 1, and 11 within the happiness of each
of the 24 sections in the dictionaries labmt, anew,
wk, mpqa, liwc, and ol, respectively. these shifts
show only the very top of the distributions which range
in length from 1030 (anew) to 13915 words (wk).

first, using the labmt dictionary, we see that the
words 1.    graduated   , 2.    father   , and 3.    universi-
ty    top the list, which is dominated by positive words
that occur more frequently. these more frequent positive
words paint a clear picture of family life (relationships,
weddings, and divorces), as well as university accomplish-
ment (graduations and college). in general, we are able to
observe with only these words that the    society    section
is where we    nd the details of these positive events.

from the anew dictionary, we see that a few pos-
itive words are up, lead by 1.    mother   , 2.    father   ,
and 3.    bride   . looking at this shift in isolation, we
see only these words with three more (   graduate   ,    wed-
ding   , and    couple   ) that would lead us to suspect these
events are at least common in the    society    section.

the wk dictionary, with the most individual word
scores of any dictionary tested, agrees with labmt and
anew that the    society    section is number 1, with
somewhat similar set of words at the top: 1.    new   , 2.
   university   , and 3.    father   . less coverage of the new
york times corpus (see fig s3) results in the top of the
shift showing less of the character of the    society    section
than labmt, with more words that go down in frequen-
cy in the shift. with the words    bride    and    wedding   
up, as well as    university   ,    graduate   , and    college   , we
glean that the    society    section covers both graduations
and weddings, as we have seen so far.

7

fig. 2: we present the speci   c words from panels g, m, s and y of fig 1 with the greatest mismatch. only the center
histogram from panel y of fig 1 is included. we emphasize that the labmt dictionary scores generally agree well with
the other dictionaries, and we are looking at the marginal words with the strongest disagreement. within these words, we
detect di   erences in the creation of these dictionaries that carry through to these edge cases. panel a: the words with most
di   erent scores between the labmt and anew dictionaries are suggestive of the di   erent meanings that such words entail
for the di   erent demographic surveyed to score the words. panel b: both dictionaries use surveys from the same demographic
(mechanical turk), where the labmt dictionary required more individual ratings for each word (at least 50, compared to 14)
and appears to have dampened the e   ect of multiple meaning words. panels c   e: the words in labmt matched by mpqa
with scores of -1, 0, and +1 in mpqa show that there are at least a few words with negative rating in mpqa that are not
negative (including the happiest word in the labmt dictionary:    laughter   ), not all of the mpqa words with score 0 are
neutral, and that mpqa   s positive words are mostly positive according to the labmt score. panel f: the function words in
the expert-curated liwc dictionary are not emotionally neutral.

the mpqa dictionary ranks the    society    section
18th of the 24 nyt sections, a complete departure
from the other rankings, with the words 1.    mar*   , 2.
   retire*   , and 3.    yes*    the top three contributing words.
negative words increasing in frequency are the most com-
mon type near the top, and of these, the words with the
biggest contributions are being scored incorrectly in this
context (speci   cally words 1.    mar*   , 2.    retire*   , 6.
   bar*   , 12.    division   , and 14.    miss*   ). looking more
in depth at the problems created by the    rst of these,
we    nd 1211 unique words match    mar*    with the    ve
most frequent being married (36750), marriage (5977),
marketing (5382), mary (4403), and mark (2624). the
score for these words in, for example, labmt are 6.76,
6.7, 5.2, 5.88, and 5.48, con   rming our suspicion about
these words being categorized incorrectly with a broad
stem match. these problems plague the mpqa dictio-

nary for scoring the new york times corpus, and without
using word shifts would have gone completely unseen. in
an attempt to    x contextual issues by blocking corpus-
speci   c words, we block    mar*,retire*,vice,bar*,miss*   
and    nd that the mpqa dictionary ranks the society
section of the nyt at 15th of the 24 sections

the second   1 dictionary, liwc, agrees well with the
   rst three dictionaries and places the    society    section
at the top with the words 1.    rich*   , 2.    miss   , and
3.    engage*    at the head of the list. we immediately
notice that the word    miss    is being used frequently in
the    society    section in a di   erent context than was rated
liwc: it is used in the corpus to mean the title pre   xed
to the name of an unmarried woman, but is scored as
negative in liwc as meaning to fail to reach an target
or to acknowledge loss. we would remove this word from
liwc for further analysis of this corpus (we would also

a:labmtcomparisonwithanewb:labmtcomparisonwithwkc:labmtcomparisonwithmpqa   snega-tivewordswordhlabmthanewhdifflust4.647.121.72bees5.603.201.66silly5.307.411.43engaged6.168.001.20book7.245.721.15hospital3.505.041.15evil1.903.231.09gloom3.561.881.05anxious3.424.811.05   ower7.886.641.00wordhlabmthwkhdiffsue4.302.181.39boogie5.863.801.39exclusive6.484.501.36wake4.726.571.35federal4.943.061.25stroke2.584.191.24gay4.446.111.23patient5.046.711.22user5.483.671.21blow4.486.101.20wordhlabmthmpqa   ne6.74-1game6.92-1cartoon7.20-1eternal7.20-1moon7.28-1fun7.96-1comedy7.98-1laugh8.22-1laugh8.22-1laughter8.50-1d:labmtcomparisonwithmpqa   sneutralwordse:labmtcomparisonwithmpqa   spositivewordsf:labmtcomparisonwithliwc   sneutralwordswordhlabmthmpqascreaming2.960pressures3.490pressure3.660plead3.670mean3.680baby7.280precious7.340strength7.400surprise7.420surprise7.420wordhlabmthmpqavulnerable3.34+1court3.78+1conviction4.10+1craving4.46+1excuse4.58+1bull4.62+1striking4.70+1o   set4.72+1admit4.74+1repair4.76+1wordhlabmthliwclack3.160couldn   t3.320cannot3.320never3.340against3.400rest7.180greatest7.260couple7.300million7.380billion7.5608

fig. 3: new york times (nyt)    society    section shifted against the entire nyt corpus for each of the six dictionaries listed
in tiles a   f. we provide a detailed analysis in sec. iii a. generally, we are able to glean the greatest understanding of the
sentiment texture associated with this nyt section using the labmt dictionary. additionally we note the labmt dictionary
has the most coverage quanti   ed by word match count (figure in s3 appendix), we are able to identify and correct problematic
words scores in the ol dictionary, and we see that the mpqa dictionary disagrees entirely with the others because of an overly
broad stem match.

remove the word    trust    here). the words matched by
   miss*    aside, liwc    nds some positive words going up,
with    engage*    hinting at weddings. otherwise, without
words that capture the speci   c behavior happening in the
   society    section, we are unable to see anything about
college, graduations, or marriages, and there is much less
to be gained about the text from the words in liwc than
some of the other dictionaries we have seen. without
these words, it is con   rming that liwc still    nds the
   society    section to be the top section, due in large part
to a lack of negative words 18.    war    and 21.       ght*   .
the    nal dictionary from ol disagrees with the oth-
ers and puts the    society    section at 11th out of the 24
sections. the top three words, 1.    vice   , 2.    miss   , and

3.    concern   , contribute largely with respect to the rest
of distribution, of which two are clearly being used in an
inappropriate context. for a more reasonable analysis we
would remove both    vice    and    miss    from the ol dic-
tionary to score this text, making the    society    section
the second happiest of the 24 sections. with this    x, the
ol dictionary ranks the society section of the nyt as
the happiest section. focusing on the words, we see that
the ol dictionary    nds many positive words increasing
in frequency that are mostly generic. in the word shift
we do not    nd the wedding or university events as in
dictionaries with more coverage, but rather a variety of
positive language surrounding these events, for example
4.    works   , 5.    bene   t   , 6.    honor   , 7.    best   , 9.

1. graduated+   2. father+   3. university+   4. new+   5. not-   6. mother+   7. son+   8. daughter+   9. late-   10. bride+   11. married+   12. college+   13. no-   14. weddings+   15. bridegroom+   16. against-   17. old-   18. divorce-   19. million+   20. received+   21. graduate+   22. war-   23. like+   24. she+   25. last-   26. hospital-      +      -         +      -   a: labmt wordshiftnyt as a whole happiness: 5.91society section happiness: 6.42why society section is happier than nyt as a whole:per word average happiness shiftword rank1. mother+   2. father+   3. bride+   4. graduate+   5. divorce-   6. war-   7. people+   8. wedding+   9. death-   10. lost-   11. money+   12. cut-   13. beach+   14. good+   15. couple+   16. free+   17. prison-   18. dead-   19. debt-   20. win+   21. victory+   22. pressure-   23. fire-   24. game+   25. crime-   26. home+      +      -         +      -   b: anew wordshiftnyt as a whole happiness: 6.30society section happiness: 6.98why society section is happier than nyt as a whole:per word average happiness shiftword rank1. new+   2. university+   3. father+   4. mother+   5. late-   6. son+   7. vice-   8. daughter+   9. old-   10. bride+   11. million+   12. graduate+   13. like+   14. divorce-   15. corporation-   16. college+   17. government-   18. war-   19. hospital-   20. wedding+   21. federal-   22. partner+   23. evening+   24. first+   25. marriage+   26. ceremony+      +      -         +      -   c: wk wordshiftnyt as a whole happiness: 6.00society section happiness: 6.43why society section is happier than nyt as a whole:per word average happiness shiftword rank1. mar*-   2. retire*-   3. yes*+   4. vice-   5. laud*+   6. bar*-   7. profess*+   8. brook*+   9. divorce-   10. like*+   11. minister*+   12. division-   13. against-   14. miss*-   15. concern-   16. even+   17. woo*+   18. game-   19. back*+   20. real+   21. just+   22. heal*+   23. benefit+   24. want*+   25. force*-   26. down-      +      -         +      -   d: mpqa wordshiftnyt as a whole happiness: 0.06society section happiness: 0.04why society section is less happy than nyt as a whole:per word average happiness shiftword rank1. rich*+   2. miss-   3. engag*+   4. honor*+   5. friend*+   6. benefit+   7. trust*+   8. problem*-   9. share+   10. best+   11. loss*-   12. secur*+   13. attack*-   14. merr*+   15. special+   16. kill*-   17. great+   18. war-   19. love+   20. fail*-   21. fight*-   22. numb*-   23. argu*-   24. threat*-   25. gross*-   26. joy*+      +      -         +      -   e: liwc wordshiftnyt as a whole happiness: 0.21society section happiness: 0.52why society section is happier than nyt as a whole:per word average happiness shiftword rank1. vice-   2. miss-   3. concern-   4. works+   5. benefit+   6. honor+   7. best+   8. work+   9. great+   10. trust+   11. love+   12. providence+   13. master+   14. holy+   15. fine+   16. loss-   17. issue-   18. problems-   19. stern-   20. supreme+   21. grace+   22. right+   23. problem-   24. well+   25. victory+   26. win+      +      -         +      -   f: liu wordshiftnyt as a whole happiness: 0.03society section happiness: 0.17why society section is happier than nyt as a whole:per word average happiness shiftword rank9

fig. 4: coverage of the words in the movie reviews by each of the dictionaries. we observe that the labmt dictionary has
the highest coverage of words in the movie reviews corpus both across word rank and cumulatively. the liwc dictionary
has initially high coverage since it contains some high-frequency function words, but quickly drops o    across rank. the wk
dictionary coverage increases across word rank and cumulatively, indicating that it contains a large number of less common
words in the movie review corpus. the ol, anew, and mpqa have a cumulative coverage of less than 20% of the lexicon.

   great   , 10.    trust   , 11.    love   , etc.

in conclusion, we    nd that 4 of the 6 dictionaries score
the    society    section at number 1, and in these cases we
use the word shift to uncover the nuances of the language
used. we    nd, unsurprisingly, that the most matches
are found by the labmt dictionary, which is in part
built from the nyt corpus (see s3 appendix for cover-
age plots). without as much corpus-speci   c coverage, we
note that while the nuances of the text remain hidden,
the liwc and ol dictionaries still    nd the positivity
surrounding these unknown events. of the two that did
not score the    society    section at the top, we repair the
mpqa and the ol dictionaries by removing the words
   mar*,retire*,vice*,bar*,miss*    and    vice,miss   , respec-
tively. by identifying words used in the wrong context
using the word shift graph, we directly improve the sen-
timent score for the new york times corpus from both
mpqa and ol dictionaries. while the ol dictionary,
with two corrections, agrees with the other dictionaries,
the mpqa dictionary with    ve corrections still ranks the
society section of the nyt as the 15th happiest of the
24 sections.

b. movie reviews classi   cation and word shift

analysis

for the movie reviews, we test the ability to discern
positive and negative reviews. the entire dataset consists
of 1000 positive and 1000 negative reviews, as rated with
4 or 5 stars and 1 or 2 stars, respectively. we show
how well each dictionary covers the review database in
fig 4. the average review length is 650 words, and we
plot the distribution we average the sentiment of words
in each review individually, using each dictionary. we

also combine random samples of n positive or n negative
reviews for n varying from 2 to 900 on a logarithmic
scale, without replacement, and rate the combined text.
with an increase in the size of the text, we expect that
the dictionaries will be better able to distinguish positive
from negative. the simple statistic we use to describe
this ability is the percentage of distributions that overlap
the average.

in the lower right panel of fig 5, the percentage over-
lap of positive and negative review distributions presents
us with a simple summary of dictionary performance on
this tagged corpus. the anew dictionary stands out as
being considerably less capable of distinguishing positive
from negative. in order, we then see wk is slightly bet-
ter overall, labmt and liwc perform similarly better
than wk overall, and then mpqa and ol are each a
degree better again, across the review lengths (see below
for hard numbers at 1 review length). two figures in s5
appendix show the distributions for 1 review and for 15
combined reviews.

to analyze which words are being used by each dictio-
nary, we compute word shift graphs of the entire positive
corpus versus the entire negative corpus in fig 6. across
the board, we see that a decrease in negative words is
the most important word type for each dictionary, with
the word    bad    being the top word for every dictionary
in which it is scored (anew does not have it). oth-
er observations that we can make from the word shifts
include a few words that are potentially being used out
of context:    movie   ,    comedy   ,    plot   ,    horror   ,    war   ,
   just   .

classifying single reviews as positive or negative, the
f1-scores are: labmt .63, anew .36, liwc .53,
mpqa .66, ol .71, and wk .34 (see table s4). we
roughly con   rm the rule-of-thumb that 10,000 words are

050001000015000wordrank0.00.20.40.60.81.0percentageofindividualwordscovered050001000015000wordrank0.00.20.40.60.81.0percentageoftotalwordscoveredlabmtanewliwcmpqaliuwklabmtanewliwcmpqaliuwk0.00.20.40.60.81.0percentagetotalcoverageindividualwordcoverage10

fig. 5: the score assigned to increasing numbers of reviews drawn from the tagged positive and negative sets. for each
dictionary we show mean sentiment and 1 standard deviation over 100 samples for each distribution of reviews in panels a   
f. for comparison we compute the fraction of the distributions that overlap in panel g. at the single review level for each
dictionary this simple performance statistic (fraction of distribution overlap) ranks the ol dictionary in    rst place, the mpqa,
liwc, and labmt dictionaries in a second place tie, wk in    fth, and anew far behind. all dictionaries require on the order
of 1000 words to achieve 95% classi   cation accuracy.

enough to score with a dictionary con   dently, with all
dictionaries except mpqa and anew achieving 90%
accuracy with this many words. we sample the number
of reviews evenly in log space, generating sets of reviews
with average word counts of 4550, 6500, 9750, 16250, and
26000 words. speci   cally, the number of reviews neces-
sary to achieve 90% accuracy is 15 reviews (9750 words)
for labmt, 100 reviews (65000 words) for anew, 10
reviews (6500 words) for liwc, 10 reviews (6500 words)
for mpqa, 7 reviews (4550 words) for ol, and 25 reviews
(16250 words) for wk.

the ol dictionary, with the highest performance clas-
sifying individual movie reviews of the 6 dictionaries test-
ed in detail, performs worse than guessing at classify-
ing individual sentences in movie reviews. speci   cally,
76.9/74.2% of sentences in the positive/negative reviews
sets have words in the ol dictionary, and of these ol
achieves an f1-score of 0.44. the results for each dictio-
nary are included in table s5, with an average (median)
f1 score of 0.42 (0.45) across all dictionaries.

c. google books time series and word shift

analysis

we use the google books 2012 dataset with all english
books [41], from which we remove id52
and split into years. from this, we make time series by
year, and word shifts of decades versus the baseline. in
addition, to assess the similarity of each time series, we
produce correlations between each of the time series.

despite grand claims from research based on the
google books corpus [44], we keep in mind that there are
several deep problems with this beguiling data set [45].
leaving aside these issues, the google books corpus nev-
ertheless provides a substantive test of our six dictionar-
ies.

in fig 7, we plot the sentiment time series for google
books. three immediate trends stand out: a dip near
the great depression, a dip near world war ii, and a
general upswing in the 1990   s and 2000   s. from these
general trends, a few dictionaries waver: ol does not
dip very much for ww2, ol and liwc stay lower in
the 90   s and 2000   s, and labmt with    h = 0.5, 1.0 go
downward near the end of the 2000   s. we take a closer
look into the 1940   s to see what each dictionary is picking

5.55.65.75.85.96.06.16.26.3sentimenta:labmtpos.reviewsneg.reviews5.65.86.06.26.46.66.87.0b:anewpos.reviewsneg.reviews   0.2   0.10.00.10.20.30.40.50.6c:liwcpos.reviewsneg.reviews   0.15   0.10   0.050.000.050.100.150.200.25d:mpqapos.reviewsneg.reviews0.51.01.52.02.5log10(num.reviews)   0.4   0.3   0.2   0.10.00.10.20.30.4sentimente:liupos.reviewsneg.reviews0.51.01.52.02.5log10(num.reviews)5.65.86.06.26.4f:wkpos.reviewsneg.reviews0.00.51.01.52.02.53.0log10(num.reviews)0.000.050.100.150.200.250.300.350.400.45%scoreoverlapg:alllabmtanewliwcmpqaliuwk11

fig. 6: word shifts for the movie review corpus. by analyzing the words that contribute most signi   cantly to the sentiment
score produced by each dictionary we are able to identify words that are problematic for each dictionary at the word-level, and
generate an understanding of the emotional texture of the movie review corpus. again we    nd that coverage of the lexicon
is essential to produce meaningful word shifts, with the labmt dictionary providing the most coverage of this corpus and
producing the most detailed word shifts. all words on the left hand side of these word shifts are words that individually
made the positive reviews score more negatively than the negative reviews, and the removal of these words would improve the
accuracy of the ratings given by each dictionary. in particular, across each dictionary the word shifts show that domain-speci   c
words such as    war    and    movie    are used more frequently in the positive reviews and are not useful in determining the polarity
of a single review.

up in google books around world war 2 in figure in s6
appendix.

and each dictionary could be made more accurate if we
remove this word.

in each panel of the word shift figure in s6 appendix,
we see that the top word making the 1940   s less positive
than the the rest of google books is    war   , which is the
top contributor for every dictionary except ol. round-
ing out the top three contributing words are    no    and
   great   , and we infer that the word    great    is being seen
from mention of    the great depression    or    the great
war   , and is possibly being used out of context. all dic-
tionaries but anew have    great    in the top 3 words,

in panel a of the 1940   s word shift figure in s6
appendix, beyond the top words, increasing words are
mostly negative and war-related:    against   ,    enemy   ,
   operation   , which we could expect from this time peri-
od.

in panel b, the anew dictionary scores the 1940   s
of google books lower than the baseline as well,    nding
   war   ,    cancer   , and    cell    to be the most important
three words. with only 1030 words, there is not enough

1. bad-   2. no-   3. movie+   4. worst-   5. life+   6. war-   7. great+   8. stupid-   9. boring-   10. like+   11. nothing-   12. don't-   13. unfortunately-   14. love+   15. worse-   16. least-   17. poor-   18. waste-   19. doesn't-   20. fails-   21. wars-   22. family+   23. best+   24. can't-   25. problem-   26. terrible-      +      -         +      -   a: labmt wordshiftall negative reviews happiness: 5.82all positive reviews happiness: 5.99why all positive reviews are happier than all negativereviews:per word average happiness shiftword rank1. stupid-   2. movie+   3. war-   4. life+   5. terrible-   6. waste-   7. love+   8. family+   9. lost-   10. mother+   11. death-   12. pain-   13. hell-   14. comedy+   15. snake-   16. failure-   17. devil-   18. horror-   19. disaster-   20. dead-   21. pretty+   22. terrific+   23. tragedy-   24. insult-   25. beautiful+   26. anger-      +      -         +      -   b: anew wordshiftall negative reviews happiness: 6.21all positive reviews happiness: 6.35why all positive reviews are happier than all negativereviews:per word average happiness shiftword rank1. bad-   2. movie+   3. boring-   4. war-   5. stupid-   6. like+   7. great+   8. terrible-   9. waste-   10. awful-   11. life+   12. love+   13. mess-   14. dumb-   15. story+   16. kill-   17. problem-   18. dull-   19. ridiculous-   20. family+   21. lame-   22. poor-   23. annoying-   24. virus-   25. lost-   26. pointless-      +      -         +      -   c: wk wordshiftall negative reviews happiness: 5.94all positive reviews happiness: 6.11why all positive reviews are happier than all negativereviews:per word average happiness shiftword rank1. bad-   2. just+   3. plot*-   4. even+   5. like*+   6. great+   7. best*+   8. worst-   9. better+   10. will+   11. least-   12. boring-   13. stupid-   14. well+   15. too*-   16. kill*-   17. reason+   18. war-   19. love+   20. care*+   21. unfortunately*-   22. perfect+   23. quit*-   24. true+   25. although*-   26. differ*-      +      -         +      -   d: mpqa wordshiftall negative reviews happiness: -0.04all positive reviews happiness: 0.10why all positive reviews are happier than all negativereviews:per word average happiness shiftword rank1. bad-   2. stupid*-   3. worst-   4. great+   5. best+   6. boring-   7. perfect*+   8. kill*-   9. fail*-   10. war-   11. better+   12. beaut*+   13. well+   14. problem*-   15. unfortunate*-   16. wonderf*+   17. terribl*-   18. ridicul*-   19. love+   20. laugh*+   21. worse*-   22. joke*+   23. dull*-   24. mess-   25. true+   26. lame*-      +      -         +      -   e: liwc wordshiftall negative reviews happiness: 0.13all positive reviews happiness: 0.30why all positive reviews are happier than all negativereviews:per word average happiness shiftword rank1. bad-   2. great+   3. like+   4. plot-   5. best+   6. better+   7. well+   8. worst-   9. love+   10. boring-   11. stupid-   12. perfect+   13. wonderful+   14. unfortunately-   15. excellent+   16. enough+   17. beautiful+   18. effective+   19. perfectly+   20. hilarious+   21. strong+   22. worse-   23. waste-   24. memorable+   25. mess-   26. ridiculous-      +      -         +      -   f: liu wordshiftall negative reviews happiness: -0.13all positive reviews happiness: 0.09why all positive reviews are happier than all negativereviews:per word average happiness shiftword rank12

fig. 7: google books sentiment time series from each dictionary, with stop values of 0.5, 1.0, and 1.5 from the dictionaries
with word scores in the 1   9 range. to normalize the sentiment score, we subtract the mean and divide by the absolute range.
we observe that each time series is choppy, with a few pronounced negative time periods, and trending positive towards the
end of the corpus. the score of labmt varies substantially with di   erent stop values, although remaining highly correlated,
and    nds absolute lows near the world wars. the liwc and ol dictionaries trend down towards 1990, dipping as low as the
war periods.

coverage to see anything beyond the top word    war,   
and the shift is dominated by words that go down in
frequency.

the con   ict. the nature of the positive words that are
up is unclear, and could justify a more detailed analysis
of why the ol dictionary fails here.

in panel c, the wk dictionary    nds the the 1940   s
with slightly less happy than the baseline, with the top
three words being    war   ,    great   , and    old   . we see
many of the same war-related words as in labmt, and
in addition some positive words like    good    and    be    are
up in frequency. the word       rst    could be an artifact of
   rst aid.

in panel d, the mpqa dictionary rates the 1940   s with
slightly less happy than the baseline, with the top three
words being    war   ,    great   , and    di   er*   . beyond the
top word    war   , the score is dominated by words decreas-
ing in frequency, with only a few words up in frequency.
without speci   c words being up in frequency, it is dif-
   cult to obtain a good glance at the nature of the text
here.

in panel e, the liwc dictionary rates the 1940   s near-
ly the same as the baseline, with the top three words
being    war   ,    great   , and    argu*   . when the scores are
nearly the same, although the 1940   s are slightly high-
er happiness here, the word shift is a view into how the
words of the reference and comparison text vary. in addi-
tion to a few war related words being up and bringing the
score down (      ght   ,    enemy   ,    attack   ), we see some
positive words up that could also be war related:    cer-
tain   ,    interest   , and    de   nite   . although liwc does
not manage to    nd world war ii as a low point of the
20th century, the words that it generates are useful in
understanding the corpus.

in panel f, the ol dictionary rates the 1940   s as hap-
pier than the baseline, with the top three words being
   great   ,    support   , and    like   . with 7 positive words
up, and 1 negative word up, we see how the ol dictionary
misses the war without the word    war    itself and with
only    enemy    contributing from the words surrounding

d. twitter time series analysis

we store data on the vermont advanced computing
core (vacc), and process the text    rst into hash tables
(with approximately 8 million unique english words each
day) and then into word vectors for each 15 minutes, for
each dictionary tested. from this, we build sentiment
time series for time resolutions of 15 minutes, 1 hour, 3
hours, 12 hours, and 1 day. in addition to the raw time
series, we compute correlations between each time series
to assess the similarity of the ratings between dictionar-
ies.

in fig 8, we present a daily sentiment time series of
twitter processed using each of the dictionaries being
tested. with the exception of liwc and mpqa we
observe that the dictionaries generally track well together
across the entire range. a strong weekly cycle is present
in all, although muted for anew.

we plot the pearson   s correlation between all time
series in fig 9, and con   rm some of the general obser-
vations that we can make from the time series. namely,
the liwc and mpqa time series disagree the most from
the others, and even more so with each other. generally,
we see strong agreement within dictionaries with varying
stop values    h.

all of the dictionaries are choppy at the start of the
time frame, when twitter volume is low in 2008 and into
2009. as more people join twitter and the tweet vol-
ume increases through 2010, we see that liwc rates the
text as happier, while the rest start a slow decline in rat-
ing that is led by mpqa in the negative direction. in
2010, the liwc dictionary is more positive than the rest

1910192019301940195019601970198019902000years   0.8   0.6   0.4   0.20.00.20.40.6normalizedsentimentlabmt0.5labmt1.0labmt1.5anew0.5anew1.0anew1.5wk0.5wk1.0wk1.5liwcmpqaliu13

fig. 8: normalized sentiment time series on twitter using    h of 1.0 for all dictionaries. to normalize the sentiment score,
we subtract the mean and divide by the absolute range. the resolution is 1 day, and draws on the 10% gardenhose sample of
public tweets provided by twitter. all of the dictionaries exhibit wide variation for very early tweets, and from 2012 onward
generally track together strongly with the exception of mpqa and liwc. the liwc and mpqa dictionaries show opposite
trends: a rise until 2012 with a decline after 2012 from liwc, and a decline before 2012 with a rise afterwards from mpqa.
to analyze the trends we look at the words driving the movement across years using word shift figures in s7 appendix.

   nds many negative words going up in frequency (see
2010 word shift figure in appendix s7). all of the dic-
tionaries agree most strongly in 2012, all    nding a lot of
negative language and swearing that brings scores down
(see 2012 word shift figure in appendix s7). from the
bottom at 2012, liwc continues to go downward while
the others trend back up. the signal from mpqa jumps
to the most positive, and liwc does start trending back
up eventually. we analyze the words in 2014 with a word
shift against all 7 years of tweets for each dictionary in
each panel in the 2014 word shift figure in appendix s7:
a. labmt    nds 2014 with less happy with more negative
language. b. anew    nds it happier with a few positive
words up. c. wk    nds it happier with more negative
words (like labmt). d. mpqa    nds it more positive
with less negative words. e. liwc    nds it less positive
with more negative and less positive words. f. ol    nds
it to be of the same sentiment as the background with a
balance in positive and negative word usage. from these
word shifts, we can analyze which words cause mpqa
and liwc to disagree with the other dictionaries: the
disagreement of mpqa is again marred by broad stem
matches, and the disagreement of liwc is due to a lack
of coverage.

e. brief comparison to machine learning methods

we implement a naive bayes (nb) classi   er (some-
times harshly called idiot bayes [46]) on the tagged movie
review dataset, using 10% of the reviews for training and
then testing performance on the rest. following stan-
dard practice, we remove the top 30 ranked words (   stop
words   ) from the 5000 most frequent words, and use the
remaining 4970 words in our classi   er for maximum per-
formance (we observe a 0.5% improvement). our imple-
mentation is analogous to those found in common python

fig. 9:
pearson   s r correlation between daily resolution
twitter sentiment time series for each dictionary. we see
that there is strong agreement within dictionaries, with the
biggest di   erences coming from the stop value of    h = 0.5.
the labmt and ol dictionaries do not strongly disagree with
any of the others, while liwc is the least correlated overall
with other dictionaries. labmt and ol correlate strongly
with each other, and disagree most with the anew, liwc,
and mpqa dictionaries. the two least correlated dictionaries
are the liwc and mpqa dictionaries. again, since there
is no publicly accessible ground truth for twitter sentiment,
we compare dictionaries against the others, and look at the
words. with these criteria, we    nd the labmt dictionary to
be the most useful.

with words like    haha   ,    lol    and    hey    being used more
frequently and swearing being less frequent than the all
years of twitter put together. the other dictionaries
with more coverage    nd a decrease in positive words to
balance this increase, with the exception of mpqa which

2009201020112012201320142015years   0.6   0.4   0.20.00.20.40.60.8normalizedsentimentlabmt0.5labmt1.0labmt1.5anew0.5anew1.0anew1.5wk0.5wk1.0wk1.5liwcmpqaliulabmt0.5labmt1.0labmt1.5labmt2.0anew0.5anew1.0anew1.5anew2.0wk0.5wk1.0wk1.5wk2.0liwcmpqaliulabmt0.5labmt1.0labmt1.5labmt2.0anew0.5anew1.0anew1.5anew2.0wk0.5wk1.0wk1.5wk2.0liwcmpqaliu0.000.150.300.450.600.750.90natural language processing packages (see    nltk    or
   textblob    [47]).

as we should expect, at the level of single review, nb
outperforms the dictionary-based methods with a classi-
   cation accuracy of 75.7% averaged over 100 trials. as
the number of reviews is increased, the overlap from nb
diminishes, and using our simple    fraction overlapping   
metric, the error drops to 0 with more than 200 reviews.
interestingly, nb starts to do worse with more reviews
put together, and with more than 500 of the 1000 reviews
put together,
it rates both the positive and negative
reviews as positive (figure in s8 appendix).

the rating curves do not touch, and neither do the
error bars, but they both go very slightly above 0. over-
all, with naive bayes we are able to classify a higher
percentage of individual reviews correctly, but with more
variance.

in the two tables in s8 appendix we compute the
words which the nb classi   er uses to classify all of
the positive reviews as positive, and all of the negative
reviews as positive. the natural language toolkit [47]
implements a method to obtain the    most informative   
words, by taking the ratio of the likelihood of words
between all available classes, and looking for the largest
ratio:

max

all words w

p (w|ci)
p (w|cj)

(3)

for all combinations of classes ci, cj. this is possible
because of the    naive    assumption that feature (word)
likelihoods are independent, resulting in a classi   cation
metric that is linear for each feature. in s8 appendix,
we provide the derivation of this linearity structure.

we    nd that the trained nb classi   er relies heavily on
words that are very speci   c to the training set including
the names of actors of the movies themselves, making
them useful as classi   ers but not in understanding the
nature of the text. we report the top 10 words for both
positive and negative classes using both the ratio and
di   erence methods in table in s8 appendix. to classify
a document using nb, we use the frequency of each word
in the document in conjunction with the id203 that
that word occurred in each labeled class ci.

we next take the movie-review-trained nb classi   er
and use it to classify the new york times sections, both
by ranking them and by looking at the words (the above
ratio and di   erence weighted by the occurrence of the
words). we ranked the sections 5 di   erent times, and
among those    nd the    television    section both by far
the happiest, and by far the least happy in independent
tests. we show these rankings and report the top 10
words used to score the    society    section in table s3.

we thus see that the nb classi   er, a linear learning
method, may perform poorly when assessing sentiment

14

outside of the corpus on which it is trained.
in gen-
eral, performance will vary depending on the statistical
dissimilarity of the training and novel corpora. added to
this is the inscrutability of black box methods: while sus-
ceptible to the aforementioned di   culty, nonlinear learn-
ing methods (unlike nb) also render detailed examina-
tion of how individual words contribute to a text   s score
more di   cult.

iv. conclusion

we have shown that measuring sentiment in various
corpora presents unique challenges, and that dictionary
performance is situation dependent. across the board,
the anew dictionary performs poorly, and the contin-
ued use of this dictionary with clearly better alternatives
is a questionable choice. we have seen that the mpqa
dictionary does not agree with the other    ve dictionaries
on the nyt corpus and twitter corpus due to a vari-
ety of context and stem matching issues, and we would
not recommend using this dictionary. and in comparison
to labmt, the wk, liwc, and ol dictionaries fail to
provide much detail in corpora where their coverage is
lower, including all four corpora tested. su   cient cover-
age is essential or producing meaningful word shifts and
thereby enabling deeper understanding.

in each case, to analyze the output of the dictionary
method, we rely on the use of word shift graphs. with
this tool, we can produce a    ner grained analysis of the
lexical content, and we can also detect words that are
used out of context and can mask them directly.
it
should be clear that using any of the dictionary-based
sentiment detecting method without looking at how indi-
vidual words contribute is indefensible, and analyses that
do not use word shifts or similar tools cannot be trusted.
the poor word shift performance of binary dictionaries in
particular gravely limits their ability to reveal underlying
stories.

in sum, we believe that dictionary-based methods will
continue to play a powerful role   they are fast and well
suited for web-scale data sets   and that the best instru-
ments will be based on dictionaries with excellent cov-
erage and continuum scores. to this end, we urge
that all dictionaries should be regularly updated to cap-
ture changing lexicons, word usage, and demographics.
looking further ahead, a move from scoring words to
scoring both phrases and words should realize consider-
able improvement for many languages of interest. with
phrase dictionaries, the resulting phrase shift graphs will
allow for a more nuanced and detailed analysis of a cor-
pus   s sentiment score [6], ultimately a   ording clearer sto-
ries for sentiment dynamics.

[1] j. bollen, h. mao, and x. zeng. twitter mood pre-
dicts the stock market. journal of computational sci-

ence, 2(1):1   8, 2011.

[2] j. si, a. mukherjee, b. liu, q. li, h. li, and x. deng.
exploiting topic based twitter sentiment for stock pre-
diction. in acl (2), pages 24   29, 2013.

[3] s. chung and s. liu. predicting stock market    uctua-

tions from twitter. berkeley, california, 2011.

[4] e. j. ruiz, v. hristidis, c. castillo, a. gionis, and
a. jaimes. correlating    nancial time series with micro-
blogging activity. in proceedings of the    fth acm inter-
national conference on web search and data mining,
pages 513   522. acm, 2012.

[5] p. s. dodds, e. m. clark, s. desu, m. r. frank, a. j.
reagan, j. r. williams, l. mitchell, k. d. harris, i. m.
kloumann, j. p. bagrow, k. megerdoomian, m. t.
mcmahon, b. f. tivnan, and c. m. danforth. human
language reveals a universal positivity bias. pnas,
112(8):2389   2394, 2015.

[6] s. e. alajajian, j. r. williams, a. j. reagan, s. c. alaja-
jian, m. r. frank, l. mitchell, j. lahne, c. m. danforth,
and p. s. dodds. the lexicocalorimeter: gauging public
health through caloric input and output on social media.
available at http://arxiv.org/abs/1507.05098, 2016.

[7] m. m. bradley and p. j. lang. a   ective norms for english
words (anew): stimuli, instruction manual and a   ec-
tive ratings. technical report c-1, university of florida,
gainesville, fl, 1999.

[8] j. w. pennebaker, m. e. francis, and r. j. booth. lin-
guistic inquiry and word count: liwc 2001. mahway:
lawrence erlbaum associates, 71:2001, 2001.

[9] t. wilson, j. wiebe, and p. ho   mann. recognizing con-
textual polarity in phrase-level id31. pro-
ceedings of human language technologies conference/-
conference on empirical methods in natural language
processing (hlt/emnlp 2005), 2005.

[10] b. liu. id31 and subjectivity. handbook of

natural language processing, 2:627   666, 2010.

[11] a. b. warriner and v. kuperman. a   ective biases
in english are bi-dimensional. cognition and emotion,
pages 1   21, 2014.

[12] r. socher, a. perelygin, j. y. wu, j. chuang, c. d.
manning, a. y. ng, and c. potts. recursive deep models
for semantic compositionality over a sentiment treebank.
in proceedings of the conference on empirical methods
in natural language processing (emnlp), volume 1631,
page 1642. citeseer, 2013.

[13] p. s. dodds and c. m. danforth. measuring the happi-
ness of large-scale written expression: songs, blogs, and
presidents. journal of happiness studies, 11(4):441   456,
july 2009.

[14] p. s. dodds, k. d. harris, i. m. kloumann, c. a. bliss,
and c. m. danforth. temporal patterns of happiness and
information in a global social network: hedonometrics
and twitter. plos one, 6(12):e26752, 12 2011.

[15] s. a. golder and m. w. macy. diurnal and seasonal
mood vary with work, sleep, and daylength across diverse
cultures. science magazine, 333:1878   1881, 2011.

[16] d. garcia, a. garas, and f. schweitzer. the language-
dependent relationship between word happiness and fre-
quency. proceedings of the national academy of sciences,
112(23):e2983, 2015.

[17] p. s. dodds, e. m. clark, s. desu, m. r. frank,
a. j. reagan, j. r. williams, l. mitchell, k. d. har-
ris, i. m. kloumann, j. p. bagrow, k. megerdoomian,
m. t. mcmahon, b. f. tivnan, and c. m. danforth.
reply to garcia et al.: common mistakes in measuring

15

frequency-dependent word characteristics. proceedings of
the national academy of sciences, 112(23):e2984   e2985,
2015.

[18] s. p. wojcik, a. hovasapian, j. graham, m. motyl, and
p. h. ditto. conservatives report, but liberals display,
greater happiness. science, 347(6227):1243   1246, 2015.
[19] f. n. ribeiro, m. ara  ujo, p. gon  calves, m. a. gon  calves,
and f. benevenuto. sentibench - a benchmark compar-
ison of state-of-the-practice id31 methods.
epj data sci., 5(1), jul 2016.

[20] b. liu. id31 and opinion mining. synthe-
sis lectures on human language technologies, 5(1):1   167,
2012.

[21] d. watson and l. a. clark. the panas-x: manual for
the positive and negative a   ect schedule-expanded form:
manual for the positive and negative a   ect schedule-
expanded form. phd thesis, university of iowa, 1999.

[22] t. de smedt and w. daelemans. pattern for python.
the journal of machine learning research, 13(1):2063   
2067, 2012.

[23] s. baccianella, a. esuli, and f. sebastiani. sentiword-
net 3.0: an enhanced lexical resource for sentiment anal-
ysis and opinion mining.
in lrec, volume 10, pages
2200   2204, 2010.

[24] f.   a. nielsen. a new anew: evaluation of a word
list for id31 in microblogs.
in m. rowe,
m. stankovic, a.-s. dadzie, and m. hardey, editors,
ceur workshop proceedings, volume proceedings of the
eswc2011 workshop on    making sense of microposts   :
big things come in small packages 718, pages 93   98, may
2011.

[25] p. j. stone, d. c. dunphy, and m. s. smith. the general
inquirer: a computer approach to content analysis. mit
press, 1966.

[26] c. whissell, m. fournier, r. pelland, d. weir, and
k. makarec. a dictionary of a   ect in language: iv. reli-
ability, validity, and applications. perceptual and motor
skills, 62(3):875   888, 1986.

[27] s. m. mohammad and p. d. turney. id104 a
word   emotion association lexicon. computational intel-
ligence, 29(3):436   465, 2013.

[28] s. kiritchenko, x. zhu, and s. m. mohammad. senti-
ment analysis of short informal texts. journal of arti   -
cial intelligence research, 50:723   762, 2014.

[29] x. zhu, s. kiritchenko, and s. m. mohammad. nrc-
canada-2014: recent improvements in the sentiment
analysis of tweets. in proceedings of the 8th international
workshop on semantic evaluation (semeval 2014), pages
443   447. citeseer, 2014.

[30] s. m. mohammad, s. kiritchenko, and x. zhu. nrc-
canada: building the state-of-the-art in sentiment analy-
sis of tweets. in proceedings of the seventh international
workshop on semantic evaluation exercises (semeval-
2013), atlanta, georgia, usa, june 2013.

[31] m. taboada, j. brooke, m. to   loski, k. voll, and
m. stede. lexicon-based methods for id31.
computational linguistics, 37(2):267   307, 2011.

[32] e. cambria, d. olsher, and d. rajagopal. senticnet
3: a common and common-sense knowledge base for
cognition-driven id31. in proceedings of the
twenty-eighth aaai conference on arti   cial intelligence,
pages 1515   1521. aaai press, 2014.

[33] p. gon  calves, m. ara  ujo, f. benevenuto, and m. cha.
comparing and combining id31 methods.

16

in proceedings of the    rst acm conference on online
social networks, pages 27   38. acm, 2013.

[34] m. thelwall, k. buckley, g. paltoglou, d. cai, and
a. kappas. sentiment strength detection in short infor-
mal text. journal of the american society for informa-
tion science and technology, 61(12):2544   2558, 2010.

[35] c. j. hutto and e. gilbert. vader: a parsimonious rule-
based model for id31 of social media text.
in eighth international aaai conference on weblogs
and social media, 2014.

[36] c. levallois. umigon: id31 for tweets based
on terms lists and heuristics. in second joint conference
on lexical and computational semantics (* sem), vol-
ume 2, pages 414   417, 2013.

[37] n. pappas, g. katsimpras, and e. stamatatos. distin-
guishing the popularity between topics: a system for
up-to-date opinion retrieval and mining in the web. in
international conference on intelligent text processing
and computational linguistics, pages 197   209. springer,
2013.

[38] s. poria, a. gelbukh, a. hussain, n. howard, d. das,
and s. bandyopadhyay. enhanced senticnet with a   ec-
tive labels for concept-based opinion mining. ieee intel-
ligent systems, 28(2):31   38, 2013.

[39] e. sandhaus. the new york times annotated corpus.

linguistic data consortium, philadelphia, 2008.

[40] b. pang and l. lee. a sentimental education: sentiment
analysis using subjectivity summarization based on min-
imum cuts. in proceedings of the acl, 2004.

[41] y. lin, j.-b. michel, e. l. aiden, j. orwant, w. brock-
man, and s. petrov. syntactic annotations for the google
books ngram corpus.
in proceedings of the acl 2012
system demonstrations, pages 169   174. association for
computational linguistics, 2012.

[42] l. mitchell, m. r. frank, k. d. harris, p. s. dodds,
and c. m. danforth. the geography of happiness: con-
necting twitter sentiment and expression, demograph-
ics, and objective characteristics of place. plos one,
8(5):e64417, may 2013.

[43] j. m. v. rayner. linear relations in biomechanics:
the statistics of scaling functions. j. zool. lond. (a),
206:415   439, 1985.

[44] j.-b. michel, y. k. shen, a. p. aiden, a. veres, m. k.
gray, j. p. pickett, d. hoiberg, d. clancy, p. norvig,
j. orwant, et al. quantitative analysis of culture using
millions of digitized books. science, 331(6014):176   182,
2011.

[45] e. a. pechenick, c. m. danforth, and p. s. dodds. char-
acterizing the google books corpus: strong limits to infer-
ences of socio-cultural and linguistic evolution. arxiv
preprint arxiv:1501.00960, 2015.

[47] s. bird. nltk: the natural language toolkit.

[46] d. j. hand and k. yu. idiot   s bayes   not so stupid after
all? international statistical review, 69(3):385   398, 2001.
in pro-
ceedings of the coling/acl on interactive presenta-
tion sessions, pages 69   72. association for computation-
al linguistics, 2006.

s1 appendix: computational methods

s1

all of the code to perform these tests is available and document on github. the repository can be found here:

https://github.com/andyreagan/sentiment-analysis-comparison.

stem matching

of the dictionaries tested, both liwc and mpqa use    word stems   . here we quickly note some of the technical

di   culties with using word stems, and how we processed them, for future research to build upon and improve.

an example is abandon*, which is intended to the match words of the standard re form abandon[a-z]*. a
naive approach is to check each word against the regular expression, but this is prohibitively slow. we store each of
the dictionaries in a    trie    data structure with a record. we use the easily available    marisa-trie    python library,
which wraps the c++ counterpart. the speed of these libraries made the comparison possible over large corpora,
in particular for the dictionaries with stemmed words, where the prefix search is necessary. speci   cally, the    trie   
structure is 70 times faster than a regular expression based search for stem words. in particular, we construct two
tries for each dictionary: a    xed and stemmed trie. we    rst attempt to match words against the    xed list, and then
turn to the pre   x match on the stemmed list.

regular expression parsing

the    rst step in processing the text of each corpora is extracting the words from the raw text. here we rely on a
regular expression search, after    rst removing some punctuation. we choose to include a set of all characters that are
found within the words in each of the six dictionaries tested in detail, such that it respects the parse used to create
these dictionaries by retaining such characters. this takes the following form in python, for raw_text as a string
(note, pdflatex renders correctly locally, but arxiv seems to explode the link match group):

punctuation_to_replace = ["---","--","      "]
for punctuation in punctuation_to_replace:

raw_text = raw_text.replace(punctuation," ")

words = [x.lower() for x in re.findall(r"(?:[0-9][0-9,\.]*[0-9])|

(?:\protect\vrule width0pt\protect\href{http://[\w\string./\string-\string?}{http://[\w\./\-\?}\&\#]+)|
(?:[\w\@\#\   \&\]\[]+)|
(?:[b}/3d;p)|\-@x#^_0\\p(o:o{x$[=<>\]*b]+)",

raw_text,flags=re.unicode)]

s2 appendix: continued individual comparisons

s2

picking up right where we left o    in section iii, we next compare anew with the other dictionaries. the anew-
wk comparison in panel i of fig. 1 contains all 1030 words of anew, with a    t of hwanew = 1.07     hwwk     0.30,
making anew more positive and with increasing positivity for more positive words. the 20 most di   erent scores
are (anew,wk): fame (7.93,5.45), god (8.15,5.90), aggressive (5.10,3.08), casino (6.81,4.68), rancid (4.34,2.38),
bees (3.20,5.14), teacher (5.68,7.37), priest (6.42,4.50), aroused (7.97,5.95), skijump (7.06,5.11), noisy (5.02,3.21),
heroin (4.36,2.74), insolent (4.35,2.74), rain (5.08,6.58), patient (5.29,6.71), pancakes (6.08,7.43), hospital (5.04,3.52),
valentine (8.11,6.40), and book (5.72,7.05). we again see some of the same words from the labmt comparisons with
these dictionaries, and again can attribute some di   erences to small sample sizes and di   ering demographics.

for the anew-mpqa comparison in panel j of fig. 1 we show the same matched word lists as before. the
happiest 10 words in anew matched by mpqa are: clouds (6.18), bar (6.42), mind (6.68), game (6.98), sapphire
(7.00), silly (7.41),    irt (7.52), rollercoaster (8.02), comedy (8.37), laughter (8.45). the least happy 5 neutral words
and happiest 5 neutral words in mpqa, matched with mpqa, are: pressure (3.38), needle (3.82), quiet (5.58), key
(5.68), alert (6.20), surprised (7.47), memories (7.48), knowledge (7.58), nature (7.65), engaged (8.00), baby (8.22).
the least happy words in anew with score +1 in mpqa that are matched by mpqa are: terri   ed (1.72), meek
(3.87), plain (4.39), obey (4.52), contents (4.89), patient (5.29), reverent (5.35), basket (5.45), repentant (5.53),
trumpet (5.75). again we see some very questionable matches by the mpqa dictionary, with broad stems capturing
words with both positive and negative scores.

for the anew-liwc comparison in panel k of fig. 1 we show the same matched word lists as before. the
happiest 10 words in anew matched by liwc are:
lazy (4.38), neurotic (4.45), startled (4.50), obsession (4.52),
skeptical (4.52), shy (4.64), anxious (4.81), tease (4.84), serious (5.08), aggressive (5.10). there are only 5 words
in anew that are matched by liwc with liwc score of 0: part (5.11), item (5.26), quick (6.64), couple (7.41),
millionaire (8.03). the least happy words in anew with score +1 in liwc that are matched by liwc are: heroin
(4.36), virtue (6.22), save (6.45), favor (6.46), innocent (6.51), nice (6.55), trust (6.68), radiant (6.73), glamour (6.76),
charm (6.77).

for the anew-liu comparison in panel l of fig. 1 we show the same matched word lists as before, except
the neutral word list because liu has no explicit neutral words. the happiest 10 words in anew matched by liu
are: pig (5.07), aggressive (5.10), tank (5.16), busybody (5.17), hard (5.22), mischief (5.57), silly (7.41),    irt (7.52),
rollercoaster (8.02), joke (8.10). the least happy words in anew with score +1 in liu that are matched by liu are:
defeated (2.34), obsession (4.52), patient (5.29), reverent (5.35), quiet (5.58), trumpet (5.75), modest (5.76), humble
(5.86), salute (5.92), idol (6.12).

for the wk-mpqa comparison in panel p of fig. 1 we show the same matched word lists as before. the
happiest 10 words in wk matched by mpqa are: cutie (7.43), pancakes (7.43), panda (7.55), laugh (7.56), marriage
(7.56), lullaby (7.57), fudge (7.62), pancake (7.71), comedy (8.05), laughter (8.05). the least happy 5 neutral words
and happiest 5 neutral words in mpqa, matched with mpqa, are: sociopath (2.44), infectious (2.63), sob (2.65),
soulless (2.71), infertility (3.00), thinker (7.26), knowledge (7.28), legacy (7.38), surprise (7.44), song (7.59). the least
happy words in wk with score +1 in mpqa that are matched by mpqa are: kidnapper (1.77), kidnapping (2.05),
kidnap (2.19), discriminating (2.33), terri   ed (2.51), terrifying (2.63), terrify (2.84), courtroom (2.84), back   re (3.00),
indebted (3.21).

for the wk-liwc comparison in panel q of fig. 1 we show the same matched word lists as before. the happiest
10 words in wk matched by liwc are: geek (5.56), number (5.59),    ery (5.70), trivia (5.70), screwdriver (5.76),
foolproof (5.82), serious (5.88), yearn (5.95), dumpling (6.48), weeping willow (6.53). the least happy 5 neutral words
and happiest 5 neutral words in liwc, matched with liwc, are: negative (2.52), negativity (2.74), quicksand (3.62),
lack (3.68), wont (4.09), unique (7.32), millionaire (7.32),    rst (7.33), million (7.55), rest (7.86). the least happy
words in wk with score +1 in liwc that are matched by liwc are: heroin (2.74), friendless (3.15), promiscuous
(3.32), supremacy (3.48), faithless (3.57), laughingstock (3.77), promiscuity (3.95), tenderfoot (4.26), succession (4.52),
dynamite (4.79).

for the wk-liu comparison in panel r of fig. 1 we show the same matched word lists as before, except the neutral
word list because liu has no explicit neutral words. the happiest 10 words in wk matched by liu are: goofy (6.71),
silly (6.72),    irt (6.73), rollercoaster (6.75), tenderness (6.89), shimmer (6.95), comical (6.95), fanciful (7.05), funny
(7.59), fudge (7.62), joke (7.88). the least happy words in wk with score +1 in liu that are matched by liu are:
defeated (2.59), envy (3.05), indebted (3.21), supremacy (3.48), defeat (3.74), overtake (3.95), trump (4.18), obsession
(4.38), dominate (4.40), tough (4.45).

now we   ll focus our attention on the mpqa row, and    rst we see comparisons against the three full range dictionar-
ies. for the    rst match against labmt in panel d of fig. 1, the mpqa match catches 431 words with mpqa score
0, while labmt (without stems) matches 268 words in mpqa in panel s (1039/809 and 886/766 for the positive and
negative words of mpqa). since we   ve already highlighted most of these words, we move on and focus our attention

s3

on comparing the   1 dictionaries.
in panels v   x, bb   dd, and hh   jj of fig. 1 there are a total of 6 bins o    of the diagonal, and we focus out
attention on the bins that represent words that have opposite scores in each of the dictionaries. for example, consider
the matches made my mpqa in panel bb: the words in the top left corner and bottom right corner with are scored in
a opposite manner in liwc, and are of particular concern. looking at the words from panel w with a +1 in mpqa
and a -1 in liwc (matched by liwc) we see: stunned,    ery, terri   ed, terrifying, yearn, defense, doubtless, foolproof,
risk-free, exhaustively, exhaustive, blameless, low-risk, low-cost, lower-priced, guiltless, vulnerable, yearningly, and
yearning. the words with a -1 in mpqa that are +1 in liwc (matched by liwc) are: silly, madly,    irt, laugh,
keen, superiority, supremacy, sillily, dearth, comedy, challenge, challenging, cheerless, faithless, laughable, laughably,
laughingstock, laughter, laugh, grating, opportunistic, joker, challenge,    irty.

in panel w of 1, the words with a +1 in mpqa and a -1 in liu (matched by liu) are: solicitude,    air, funny,
resurgent, untouched, tenderness, giddy, vulnerable, and joke. the words with a -1 in mpqa that are +1 in liu,
matched by liu, are: superiority, supremacy, sharp, defeat, dumbfounded, a   ectation, charisma, formidable, envy,
empathy, trivially, obsessions, and obsession.

in panel bb of 1, the words with a +1 in liwc and a -1 in mqpa (matched by mpqa) are: silly, madly,    irt,
laugh, keen, determined, determina, funn, fearless, painl, cute, cutie, and gratef. the words with a -1 in liwc and
a +1 in mqpa, that are matched by mpqa, are: stunned, terri   ed, terrifying,    ery, yearn, terrify, aversi, pressur,
careless, helpless, and hopeless.

in panel dd of 1, the words with a -1 in liwc and a +1 in liu, that are matched by liu, are: silly, and madly.

the words with a +1 in liwc and a -1 in liu, that are matched by liu, are: stunned, and    ery.

in panel hh of 1, the words with a -1 in liu and a +1 in mpqa, that are matched by mpqa, are: superior-
ity, supremacy, sharp, defeat, dumbfounded, charisma, a   ectation, formidable, envy, empathy, trivially, obsessions,
obsession, stabilize, defeated, defeating, defeats, dominated, dominates, dominate, dumbfounding, cajole, cuteness,
faultless,    ashy,    ne-looking,    ner,    nest, panoramic, pain-free, retractable, believeable, blockbuster, empathize, err-
free, mind-blowing, marvelled, marveled, trouble-free, thumb-up, thumbs-up, long-lasting, and viewable. the words
with a +1 in liu and a -1 in mpqa, that are matched by mpqa, are: solicitude,    air, funny, resurgent, untouched,
tenderness, giddy, vulnerable, joke, shimmer, spurn, craven, aweful, backwoods, backwood, back-woods, back-wood,
back-logged, backaches, backache, backaching, backbite, tingled, glower, and gainsay.

in panel ii of 1, the words with a +1 in liu and a -1 in liwc, that are matched by liwc, are: stunned,
   ery, defeated, defeating, defeats, defeat, doubtless, dominated, dominates, dominate, dumbfounded, dumbfounding,
faultless, foolproof, problem-free, problem-solver, risk-free, blameless, envy, trivially, trouble-free, tougher, toughest,
tough, low-priced, low-price, low-risk, low-cost, lower-priced, geekier, geeky, guiltless, obsessions, and obsession. the
words with a -1 in liu and a +1 in liwc, that are matched by liwc, are: silly, madly, sillily, dearth, challenging,
cheerless, faithless,    irty,    irt, funnily, funny, tenderness, laughable, laughably, laughingstock, grating, opportunistic,
joker, and joke.

in the o   -diagonal bins for all of the   1 dictionaries, we see many of the same words. again mpqa stem matches
are disparagingly broad. we also    nd matches by liwc that are concerning, and should in all likelihood be removed
from the dictionary.

s3 appendix: coverage for all corpuses

we provide coverage plots for the other three corpuses.

s4

fig. s1: coverage of the words on twitter by each of the dictionaries.

fig. s2: coverage of the words in google books by each of the dictionaries.

050001000015000word rank0.00.20.40.60.81.0percentage of individual words coveredlabmtanewliwcmpqaliuwk050001000015000word rank0.00.20.40.60.81.0percentage of total words coveredlabmtanewliwcmpqaliuwklabmtanewliwcmpqaliuwk0.00.20.40.60.81.0percentagetotal coverageindividual word coverage050001000015000word rank0.00.20.40.60.81.0percentage of individual words coveredlabmtanewliwcmpqaliuwk050001000015000word rank0.00.20.40.60.81.0percentage of total words coveredlabmtanewliwcmpqaliuwklabmtanewliwcmpqaliuwk0.00.20.40.60.81.0percentagetotal coverageindividual word coverages5

fig. s3: coverage of the words in the new york times by each of the dictionaries.

050001000015000word rank0.00.20.40.60.81.0percentage of individual words coveredlabmtanewliwcmpqaliuwk050001000015000word rank0.00.20.40.60.81.0percentage of total words coveredlabmtanewliwcmpqaliuwklabmtanewliwcmpqaliuwk0.00.20.40.60.81.0percentagetotal coverageindividual word coverages4 appendix: sorted new york times rankings

s6

fig. s4: nyt sections scatterplot. the rma    t    and    for the formula y =    +   x. for the sake of comparison, we
normalized each dictionary to the range [-.5,.5] by subtracting the mean score (5 or 0) and dividing by the range (8 or 2).

0.200.250.300.350.400.450.500.55anewartsbooksclassi   edculturaleditorialeducation   nancialforeignhomeleisurelivingmagazinemetropolitanmoviesnationalregionalsciencesocietysportsstyletelevisiontravelweek-in-reviewweekendrma  =1.34  =0.010.100.150.200.250.300.350.400.45wkartsbooksclassi   edculturaleditorialeducation   nancialforeignhomeleisurelivingmagazinemetropolitanmoviesnationalregionalsciencesocietysportsstyletelevisiontravelweek-in-reviewweekendrma  =1.12  =-0.01   0.10.00.10.20.3mpqaartsbooksclassi   edculturaleditorialeducation   nancialforeignhomeleisurelivingmagazinemetropolitanmoviesnationalregionalsciencesocietysportsstyletelevisiontravelweek-in-reviewweekendrma  =1.90  =-0.39   0.10.00.10.20.30.40.50.6liwcartsbooksclassi   edculturaleditorialeducation   nancialforeignhomeleisurelivingmagazinemetropolitanmoviesnationalregionalsciencesocietysportsstyletelevisiontravelweek-in-reviewweekendrma  =2.93  =-0.490.100.150.200.250.300.350.40labmt   0.3   0.2   0.10.00.10.20.30.40.5liuartsbooksclassi   edculturaleditorialeducation   nancialforeignhomeleisurelivingmagazinemetropolitanmoviesnationalregionalsciencesocietysportsstyletelevisiontravelweek-in-reviewweekendrma  =2.98  =-0.66artsbooksclassi   edculturaleditorialeducation   nancialforeignhomeleisurelivingmagazinemetropolitanmoviesnationalregionalsciencesocietysportsstyletelevisiontravelweek-in-reviewweekendrma  =0.84  =-0.02artsbooksclassi   edculturaleditorialeducation   nancialforeignhomeleisurelivingmagazinemetropolitanmoviesnationalregionalsciencesocietysportsstyletelevisiontravelweek-in-reviewweekendrma  =1.22  =-0.34artsbooksclassi   edculturaleditorialeducation   nancialforeignhomeleisurelivingmagazinemetropolitanmoviesnationalregionalsciencesocietysportsstyletelevisiontravelweek-in-reviewweekendrma  =2.23  =-0.530.200.250.300.350.400.450.500.55anewartsbooksclassi   edculturaleditorialeducation   nancialforeignhomeleisurelivingmagazinemetropolitanmoviesnationalregionalsciencesocietysportsstyletelevisiontravelweek-in-reviewweekendrma  =2.21  =-0.68artsbooksclassi   edculturaleditorialeducation   nancialforeignhomeleisurelivingmagazinemetropolitanmoviesnationalregionalsciencesocietysportsstyletelevisiontravelweek-in-reviewweekendrma  =1.49  =-0.32artsbooksclassi   edculturaleditorialeducation   nancialforeignhomeleisurelivingmagazinemetropolitanmoviesnationalregionalsciencesocietysportsstyletelevisiontravelweek-in-reviewweekendrma  =2.71  =-0.500.100.150.200.250.300.350.40wkartsbooksclassi   edculturaleditorialeducation   nancialforeignhomeleisurelivingmagazinemetropolitanmoviesnationalregionalsciencesocietysportsstyletelevisiontravelweek-in-reviewweekendrma  =2.47  =-0.59artsbooksclassi   edculturaleditorialeducation   nancialforeignhomeleisurelivingmagazinemetropolitanmoviesnationalregionalsciencesocietysportsstyletelevisiontravelweek-in-reviewweekendrma  =2.80  =0.00   0.15   0.10   0.050.000.050.100.150.200.25mpqaartsbooksclassi   edculturaleditorialeducation   nancialforeignhomeleisurelivingmagazinemetropolitanmoviesnationalregionalsciencesocietysportsstyletelevisiontravelweek-in-reviewweekendrma  =2.01  =-0.08   0.10.00.10.20.30.40.50.6liwcartsbooksclassi   edculturaleditorialeducation   nancialforeignhomeleisurelivingmagazinemetropolitanmoviesnationalregionalsciencesocietysportsstyletelevisiontravelweek-in-reviewweekendrma  =0.95  =-0.14s7

fig. s5: sorted bar charts ranking each of the 24 new york times sections for each dictionary tested.

a:labmtb:anewc:warriner0.50.40.30.20.10.00.10.20.30.4happs diff from unweighted average1. society2. leisure3. weekend4. movies5. living6. home7. cultural8. style9. arts10. education11. regional12. classified13. travel14. television15. books16. magazine17. financial18. sports19. science20. editorial21. week_in_review22. metropolitan23. national24. foreign0.60.40.20.00.20.40.6happs diff from unweighted average1. society2. leisure3. education4. style5. classified6. home7. weekend8. movies9. regional10. arts11. cultural12. living13. sports14. television15. travel16. magazine17. financial18. books19. metropolitan20. national21. week_in_review22. science23. editorial24. foreign0.40.30.20.10.00.10.20.3happs diff from unweighted average1. society2. leisure3. weekend4. home5. living6. movies7. style8. cultural9. arts10. regional11. travel12. classified13. education14. sports15. television16. magazine17. books18. financial19. science20. metropolitan21. week_in_review22. editorial23. national24. foreignd:mpqae:liwcf:liu0.200.150.100.050.000.050.100.15happs diff from unweighted average1. travel2. style3. education4. home5. classified6. leisure7. living8. regional9. arts10. cultural11. movies12. weekend13. sports14. magazine15. financial16. editorial17. books18. society19. national20. metropolitan21. week_in_review22. science23. foreign24. television0.30.20.10.00.10.20.3happs diff from unweighted average1. society2. leisure3. weekend4. style5. movies6. cultural7. living8. regional9. arts10. home11. classified12. financial13. sports14. television15. education16. magazine17. books18. travel19. editorial20. national21. science22. metropolitan23. week_in_review24. foreign0.30.20.10.00.10.20.3happs diff from unweighted average1. leisure2. travel3. living4. style5. weekend6. home7. classified8. sports9. regional10. cultural11. society12. movies13. arts14. education15. magazine16. television17. financial18. books19. editorial20. science21. week_in_review22. national23. metropolitan24. foreigns5 appendix: movie review distributions

s8

here we examine the distributions of movie review scores. these distributions are each summarized by their mean
and standard deviation in panels of figure 2 for each dictionary. for example, the left most error bar of each panel in
figure 2 shows the standard deviation around the mean for the distribution of individual review scores (figure s6).

fig. s6: binned scores for each review by each corpus with a stop value of    h = 1.0.

a:labmtb:anewc:warrinerd:mpqae:liwcf:liuwordshifts9

fig. s7: binned scores for samples of 15 concatenated random reviews. each dictionary uses stop value of    h = 1.0.

fig. s8: binned length of positive reviews, in words.

a:labmtb:anewc:warriner5.65.75.85.96.06.16.2score020406080100number of reviewspositive reviewsnegative reviews5.65.86.06.26.46.66.87.0score020406080100number of reviewspositive reviewsnegative reviews5.75.85.96.06.16.26.36.4score0102030405060708090number of reviewspositive reviewsnegative reviewsd:mpqae:liwcf:liuwordshift0.150.100.050.000.050.100.150.20score0102030405060708090number of reviewspositive reviewsnegative reviews0.10.00.10.20.30.40.5score0102030405060708090number of reviewspositive reviewsnegative reviews0.30.20.10.00.10.20.3score020406080100number of reviewspositive reviewsnegative reviewss6 appendix: google books correlations and word shifts

s10

fig. s9: google books correlations. here we include correlations for the google books time series, and word shifts for selected
decades (1920   s,1940   s,1990   s,2000   s).

labmt 0.5labmt 1.0labmt 1.5anew 0.5anew 1.0anew 1.5wk 0.5wk 1.0wk 1.5liwcmpqaliulabmt 0.5labmt 1.0labmt 1.5anew 0.5anew 1.0anew 1.5wk 0.5wk 1.0wk 1.5liwcmpqaliu0.150.000.150.300.450.600.750.90s11

fig. s10: google books shifts in the 1920   s against the baseline of google books.

1. great+   2. no-   3. war-   4. old-   5. without-   6. never-   7. last-   8. you+   9. family+   10. all+   11. good+   12. nothing-   13. issues-   14. women+   15. risk-   16. life+   17. negative-   18. against-   19. doubt-   20. cancer-   21. relationship+   22. low-   23. information+   24. critical-   25. costs-   26. new+      +      -         +      -   a: labmt wordshiftgoogle books as a whole happiness: 5.871920's happiness: 5.87why 1920's are happier than google books as a whole:per word average happiness shiftword rank1. war-   2. family+   3. stress-   4. cell-   5. cancer-   6. man+   7. social+   8. failure-   9. good+   10. crisis-   11. abuse-   12. fire-   13. damage-   14. surgery-   15. danger-   16. mother+   17. sex+   18. dead-   19. depression-   20. illness-   21. alone-   22. rejected-   23. infection-   24. life+   25. gold+   26. broken-      +      -         +      -   b: anew wordshiftgoogle books as a whole happiness: 6.191920's happiness: 6.22why 1920's are happier than google books as a whole:per word average happiness shiftword rank1. great+   2. old-   3. war-   4. good+   5. can+   6. relationship+   7. new+   8. government-   9. give+   10. stress-   11. be+   12. doubt-   13. negative-   14. acid-   15. family+   16. economy-   17. cancer-   18. first+   19. disease-   20. federal-   21. enemy-   22. water+   23. difficulty-   24. create+   25. user-   26. mean-      +      -         +      -   c: wk wordshiftgoogle books as a whole happiness: 5.981920's happiness: 6.00why 1920's are happier than google books as a whole:per word average happiness shiftword rank1. great+   2. little-   3. differ*-   4. fun*-   5. need-   6. want*+   7. back*+   8. mind*-   9. will+   10. important+   11. rail*-   12. good+   13. like*+   14. just+   15. war-   16. support+   17. help+   18. significant+   19. basic+   20. values+   21. risk-   22. numb*-   23. heal*+   24. argue*-   25. complex-   26. mar*-      +      -         +      -   d: mpqa wordshiftgoogle books as a whole happiness: 0.091920's happiness: 0.10why 1920's are happier than google books as a whole:per word average happiness shiftword rank1. great+   2. argu*-   3. low*-   4. risk*-   5. numb*-   6. war-   7. importan*+   8. doubt*-   9. support+   10. create*+   11. certain*+   12. stress*-   13. values+   14. good+   15. critical-   16. domina*-   17. beaut*+   18. energ*+   19. threat*-   20. creati*+   21. challeng*+   22. interest*+   23. reject*-   24. damag*-   25. avoid*-   26. care+      +      -         +      -   e: liwc wordshiftgoogle books as a whole happiness: 0.221920's happiness: 0.26why 1920's are happier than google books as a whole:per word average happiness shiftword rank1. great+   2. important+   3. available+   4. support+   5. good+   6. significant+   7. issues-   8. like+   9. risk-   10. appropriate+   11. complex-   12. work+   13. critical-   14. issue-   15. effective+   16. fine+   17. doubt-   18. limited-   19. negative-   20. benefits+   21. gold+   22. best+   23. stress-   24. greatest+   25. concern-   26. impossible-      +      -         +      -   f: liu wordshiftgoogle books as a whole happiness: 0.041920's happiness: 0.07why 1920's are happier than google books as a whole:per word average happiness shiftword ranks12

fig. s11: google books shifts in the 1940   s against the baseline of google books.

1. war-   2. no-   3. great+   4. you+   5. against-   6. without-   7. old-   8. women+   9. risk-   10. issues-   11. acid-   12. last-   13. family+   14. enemy-   15. cancer-   16. good+   17. never-   18. information+   19. air+   20. all+   21. operation-   22. computer+   23. first+   24. drug-   25. nuclear-   26. relationship+      +      -         +      -   a: labmt wordshiftgoogle books as a whole happiness: 5.871940's happiness: 5.85why 1940's are less happy than google books as a whole:per word average happiness shiftword rank1. war-   2. cancer-   3. cell-   4. family+   5. stress-   6. abuse-   7. love+   8. man+   9. good+   10. mother+   11. social+   12. cut-   13. death-   14. failure-   15. surgery-   16. danger-   17. crisis-   18. fire-   19. crime-   20. sex+   21. anger-   22. damage-   23. trouble-   24. criminal-   25. victim-   26. illness-      +      -         +      -   b: anew wordshiftgoogle books as a whole happiness: 6.191940's happiness: 6.17why 1940's are less happy than google books as a whole:per word average happiness shiftword rank1. war-   2. great+   3. old-   4. good+   5. can+   6. acid-   7. be+   8. operation-   9. enemy-   10. relationship+   11. first+   12. cancer-   13. give+   14. water+   15. like+   16. user-   17. stress-   18. care+   19. family+   20. oil-   21. abuse-   22. attack-   23. air+   24. create+   25. issue-   26. support+      +      -         +      -   c: wk wordshiftgoogle books as a whole happiness: 5.981940's happiness: 5.97why 1940's are less happy than google books as a whole:per word average happiness shiftword rank1. war-   2. great+   3. differ*-   4. fun*-   5. little-   6. need-   7. want*+   8. mar*-   9. like*+   10. will+   11. just+   12. support+   13. risk-   14. back*+   15. against-   16. necessary+   17. good+   18. care*+   19. temper*-   20. rail*-   21. argue*-   22. object*-   23. allow*+   24. too*-   25. significant+   26. long*-      +      -         +      -   d: mpqa wordshiftgoogle books as a whole happiness: 0.091940's happiness: 0.08why 1940's are less happy than google books as a whole:per word average happiness shiftword rank1. war-   2. great+   3. argu*-   4. risk*-   5. support+   6. certain*+   7. create*+   8. good+   9. fight*-   10. care+   11. critical-   12. numb*-   13. enemy*-   14. doubt*-   15. interest*+   16. challeng*+   17. creati*+   18. stress*-   19. threat*-   20. definite+   21. values+   22. cut-   23. attack*-   24. satisf*+   25. energ*+   26. domina*-      +      -         +      -   e: liwc wordshiftgoogle books as a whole happiness: 0.221940's happiness: 0.22why 1940's are happier than google books as a whole:per word average happiness shiftword rank1. great+   2. support+   3. like+   4. issues-   5. risk-   6. good+   7. significant+   8. appropriate+   9. complex-   10. issue-   11. available+   12. important+   13. enemy-   14. critical-   15. greatest+   16. satisfactory+   17. benefits+   18. love+   19. cancer-   20. gold+   21. fine+   22. commitment+   23. work+   24. modern+   25. stress-   26. right+      +      -         +      -   f: liu wordshiftgoogle books as a whole happiness: 0.041940's happiness: 0.05why 1940's are happier than google books as a whole:per word average happiness shiftword ranks13

fig. s12: google books shifts in the 1990   s against the baseline of google books.

1. no-   2. great+   3. war-   4. women+   5. not-   6. without-   7. never-   8. old-   9. against-   10. last-   11. family+   12. you+   13. issues-   14. all+   15. good+   16. nothing-   17. risk-   18. abuse-   19. information+   20. we+   21. drug-   22. doubt-   23. children+   24. cancer-   25. critical-   26. enemy-      +      -         +      -   a: labmt wordshiftgoogle books as a whole happiness: 5.871990's happiness: 5.88why 1990's are happier than google books as a whole:per word average happiness shiftword rank1. war-   2. family+   3. abuse-   4. man+   5. stress-   6. cancer-   7. cell-   8. good+   9. social+   10. failure-   11. infection-   12. mother+   13. fire-   14. crisis-   15. alone-   16. damage-   17. danger-   18. surgery-   19. debt-   20. sex+   21. rape-   22. injury-   23. child+   24. dead-   25. anger-   26. death-      +      -         +      -   b: anew wordshiftgoogle books as a whole happiness: 6.191990's happiness: 6.18why 1990's are less happy than google books as a whole:per word average happiness shiftword rank1. great+   2. war-   3. old-   4. good+   5. aids-   6. can+   7. be+   8. abuse-   9. relationship+   10. give+   11. stress-   12. enemy-   13. hiv-   14. doubt-   15. family+   16. new+   17. care+   18. first+   19. issue-   20. death-   21. acid-   22. disease-   23. user-   24. cancer-   25. economy-   26. negative-      +      -         +      -   c: wk wordshiftgoogle books as a whole happiness: 5.981990's happiness: 5.97why 1990's are less happy than google books as a whole:per word average happiness shiftword rank1. great+   2. fun*-   3. differ*-   4. little-   5. mar*-   6. want*+   7. need-   8. war-   9. support+   10. good+   11. care*+   12. mind*-   13. back*+   14. important+   15. like*+   16. will+   17. argue*-   18. just+   19. heal*+   20. against-   21. risk-   22. object*-   23. numb*-   24. allow*+   25. significant+   26. help+      +      -         +      -   d: mpqa wordshiftgoogle books as a whole happiness: 0.091990's happiness: 0.08why 1990's are less happy than google books as a whole:per word average happiness shiftword rank1. great+   2. argu*-   3. risk*-   4. war-   5. numb*-   6. low*-   7. support+   8. doubt*-   9. create*+   10. importan*+   11. certain*+   12. stress*-   13. domina*-   14. care+   15. good+   16. critical-   17. values+   18. abuse*-   19. damag*-   20. creati*+   21. threat*-   22. challeng*+   23. attack*-   24. avoid*-   25. enemy*-   26. beaut*+      +      -         +      -   e: liwc wordshiftgoogle books as a whole happiness: 0.221990's happiness: 0.20why 1990's are less happy than google books as a whole:per word average happiness shiftword rank1. great+   2. issues-   3. support+   4. important+   5. good+   6. available+   7. risk-   8. significant+   9. like+   10. appropriate+   11. issue-   12. complex-   13. critical-   14. doubt-   15. benefits+   16. abuse-   17. stress-   18. limited-   19. negative-   20. enemy-   21. effective+   22. concerns-   23. right+   24. greatest+   25. commitment+   26. concern-      +      -         +      -   f: liu wordshiftgoogle books as a whole happiness: 0.041990's happiness: 0.03why 1990's are less happy than google books as a whole:per word average happiness shiftword ranks14

fig. s13: google books shifts in the 2000   s against the baseline of google books.

1. no-   2. you+   3. great+   4. war-   5. me+   6. like+   7. against-   8. risk-   9. without-   10. down-   11. she+   12. acid-   13. first+   14. issues-   15. my+   16. cancer-   17. operation-   18. old-   19. not-   20. violence-   21. force-   22. love+   23. last-   24. women+   25. special+   26. all+      +      -         +      -   a: labmt wordshiftgoogle books as a whole happiness: 5.872000's happiness: 5.88why 2000's are happier than google books as a whole:per word average happiness shiftword rank1. war-   2. cancer-   3. love+   4. home+   5. man+   6. abuse-   7. nature+   8. free+   9. mother+   10. death-   11. hurt-   12. danger-   13. car+   14. interest+   15. family+   16. terrorist-   17. cut-   18. hell-   19. crime-   20. alone-   21. loved+   22. heart+   23. anger-   24. surgery-   25. water+   26. baby+      +      -         +      -   b: anew wordshiftgoogle books as a whole happiness: 6.192000's happiness: 6.20why 2000's are happier than google books as a whole:per word average happiness shiftword rank1. war-   2. like+   3. great+   4. be+   5. first+   6. acid-   7. operation-   8. old-   9. know+   10. can+   11. create+   12. government-   13. cancer-   14. user-   15. care+   16. special+   17. love+   18. difficulty-   19. violence-   20. hiv-   21. water+   22. right+   23. home+   24. abuse-   25. help+   26. doubt-      +      -         +      -   c: wk wordshiftgoogle books as a whole happiness: 5.982000's happiness: 5.99why 2000's are happier than google books as a whole:per word average happiness shiftword rank1. like*+   2. just+   3. back*+   4. want*+   5. need-   6. great+   7. down-   8. risk-   9. less-   10. too*-   11. force*-   12. numb*-   13. necessary+   14. heal*+   15. war-   16. temper*-   17. help+   18. care*+   19. against-   20. right+   21. above+   22. allow*+   23. sure+   24. argue*-   25. interest+   26. rail*-      +      -         +      -   d: mpqa wordshiftgoogle books as a whole happiness: 0.092000's happiness: 0.09why 2000's are happier than google books as a whole:per word average happiness shiftword rank1. risk*-   2. great+   3. numb*-   4. certain*+   5. war-   6. interest*+   7. create*+   8. argu*-   9. difficult*-   10. low*-   11. doubt*-   12. challeng*+   13. care+   14. special+   15. sure*+   16. smil*+   17. terror*-   18. kill*-   19. support+   20. creati*+   21. love+   22. satisf*+   23. worr*-   24. importan*+   25. critical-   26. hit-      +      -         +      -   e: liwc wordshiftgoogle books as a whole happiness: 0.222000's happiness: 0.21why 2000's are less happy than google books as a whole:per word average happiness shiftword rank1. like+   2. great+   3. risk-   4. issues-   5. right+   6. support+   7. love+   8. concerned-   9. work+   10. hard-   11. cancer-   12. sufficient+   13. regard+   14. critical-   15. doubt-   16. top+   17. significant+   18. greatest+   19. difficulty-   20. benefits+   21. impossible-   22. satisfactory+   23. difficulties-   24. bad-   25. smile+   26. concerns-      +      -         +      -   f: liu wordshiftgoogle books as a whole happiness: 0.042000's happiness: 0.04why 2000's are less happy than google books as a whole:per word average happiness shiftword ranks7 appendix: additional twitter time series, correlations, and shifts

first, we present additional twitter time series:

s15

fig. s14: normalized time series on twitter using    h of 1.0 for all. for resolution of 3 hours. we do not include any of the
time series with resolution below 3 hours here because there are too many data points to see.

fig. s15: normalized time series on twitter using    h of 1.0 for all. for resolution of 12 hours.

next, we take a look at more correlations:
now we include word shift graphs that are absent from the manuscript itself.
finally, we include the results of each dictionary applied to a set of annotated twitter data. we apply sentiment
dictionaries to rate individual tweets and classify a tweet as positive (negative) if the tweet rating is greater (less)
than the average of all scores in dictionary.

20092010201120122013201420150.60.40.20.00.20.40.60.8labmtanewwarrinerliwcmpqaliu20092010201120122013201420150.60.40.20.00.20.40.60.8labmtanewwarrinerliwcmpqalius16

fig. s16: pearson   s r correlation between twitter time series for all resolutions below 1 day.

a:15minuteb:1hourlabmt 0.5labmt 1.0labmt 1.5labmt 2.0anew 0.5anew 1.0anew 1.5anew 2.0wk 0.5wk 1.0wk 1.5wk 2.0liwcmpqaliulabmt 0.5labmt 1.0labmt 1.5labmt 2.0anew 0.5anew 1.0anew 1.5anew 2.0wk 0.5wk 1.0wk 1.5wk 2.0liwcmpqaliu0.10.20.30.40.50.60.70.80.91.0labmt 0.5labmt 1.0labmt 1.5labmt 2.0anew 0.5anew 1.0anew 1.5anew 2.0wk 0.5wk 1.0wk 1.5wk 2.0liwcmpqaliulabmt 0.5labmt 1.0labmt 1.5labmt 2.0anew 0.5anew 1.0anew 1.5anew 2.0wk 0.5wk 1.0wk 1.5wk 2.0liwcmpqaliu0.10.20.30.40.50.60.70.80.91.0c:3hoursd:12hourslabmt 0.5labmt 1.0labmt 1.5labmt 2.0anew 0.5anew 1.0anew 1.5anew 2.0wk 0.5wk 1.0wk 1.5wk 2.0liwcmpqaliulabmt 0.5labmt 1.0labmt 1.5labmt 2.0anew 0.5anew 1.0anew 1.5anew 2.0wk 0.5wk 1.0wk 1.5wk 2.0liwcmpqaliu0.000.150.300.450.600.750.90labmt 0.5labmt 1.0labmt 1.5labmt 2.0anew 0.5anew 1.0anew 1.5anew 2.0wk 0.5wk 1.0wk 1.5wk 2.0liwcmpqaliulabmt 0.5labmt 1.0labmt 1.5labmt 2.0anew 0.5anew 1.0anew 1.5anew 2.0wk 0.5wk 1.0wk 1.5wk 2.0liwcmpqaliu0.000.150.300.450.600.750.90s17

fig. s17: word shifts for twitter in 2010. the reference word usage is all of twitter (the 10% gardenhose feed) from
september 2008 through april 2015, with the word usage normalized by year.

1. no-   2. don't-   3. haha+   4. not-   5. love+   6. hahaha+   7. can't-   8. never-   9. like+   10. shit-   11. lol+   12. you+   13. hate-   14. happy+   15. bitch-   16. youtube+   17. life+   18. bad-   19. miss-   20. ain't-   21. last-   22. down-   23. birthday+   24. die-   25. friends+   26. con-      +      -         +      -   a: labmt wordshifttwitter all years combined happiness: 6.10twitter 2010 happiness: 6.07why twitter 2010 is less happy than twitter all yearscombined:per word average happiness shiftword rank1. hate-   2. free+   3. people+   4. hell-   5. love+   6. happy+   7. bored-   8. ugly-   9. good+   10. life+   11. hurt-   12. birthday+   13. wit+   14. sad-   15. win+   16. sin-   17. lost-   18. mad-   19. christmas+   20. home+   21. death-   22. debt-   23. beautiful+   24. money+   25. alone-   26. proud+      +      -         +      -   b: anew wordshifttwitter all years combined happiness: 6.63twitter 2010 happiness: 6.64why twitter 2010 is happier than twitter all yearscombined:per word average happiness shiftword rank1. boa-   2. like+   3. hate-   4. love+   5. shit-   6. bitch-   7. live+   8. con-   9. happy+   10. die-   11. mean-   12. free+   13. awkward-   14. bout-   15. ugly-   16. old-   17. thank+   18. sad-   19. bad-   20. kill-   21. hurt-   22. christmas+   23. wrong-   24. wit+   25. fake-   26. one-      +      -         +      -   c: wk wordshifttwitter all years combined happiness: 6.34twitter 2010 happiness: 6.26why twitter 2010 is less happy than twitter all yearscombined:per word average happiness shiftword rank1. lag*-   2. mar*-   3. like*+   4. faze*-   5. bar*-   6. just+   7. want*+   8. love+   9. live+   10. bitch*-   11. jam-   12. pan*-   13. hate*-   14. back*+   15. will+   16. need-   17. even+   18. trying-   19. little-   20. happy+   21. pro+   22. gain+   23. dim*-   24. try*-   25. free+   26. please*+      +      -         +      -   d: mpqa wordshifttwitter all years combined happiness: 0.24twitter 2010 happiness: 0.18why twitter 2010 is less happy than twitter all yearscombined:per word average happiness shiftword rank1. haha*+   2. lol+   3. fuck-   4. shit*-   5. heh*+   6. love+   7. bitch*-   8. fuckin*-   9. hate-   10. friend*+   11. good+   12. happy+   13. miss-   14. please*+   15. wrong*-   16. amor*+   17. kill*-   18. ugl*-   19. worst-   20. lmao+   21. sad-   22. best+   23. thank+   24. stupid*-   25. ache*-   26. dumb*-      +      -         +      -   e: liwc wordshifttwitter all years combined happiness: 0.41twitter 2010 happiness: 0.45why twitter 2010 is happier than twitter all yearscombined:per word average happiness shiftword rank1. like+   2. fuck-   3. jam-   4. fucking-   5. shit-   6. free+   7. gain+   8. bitch-   9. hate-   10. love+   11. die-   12. happy+   13. bs-   14. damn-   15. super+   16. good+   17. well+   18. thank+   19. wow+   20. best+   21. perfect+   22. hard-   23. favor+   24. awkward-   25. better+   26. beautiful+      +      -         +      -   f: liu wordshifttwitter all years combined happiness: 0.18twitter 2010 happiness: 0.17why twitter 2010 is less happy than twitter all yearscombined:per word average happiness shiftword ranks18

fig. s18: word shifts for twitter in 2012. the reference word usage is all of twitter (the 10% gardenhose feed) from
september 2008 through april 2015, with the word usage normalized by year.

1. no-   2. shit-   3. not-   4. new+   5. haha+   6. hate-   7. great+   8. bitch-   9. don't-   10. free+   11. ass-   12. last-   13. bitches-   14. lol+   15. hahaha+   16. love+   17. good+   18. thanks+   19. hurt-   20. happy+   21. dont-   22. bad-   23. attack-   24. home+   25. fail-   26. war-      +      -         +      -   a: labmt wordshifttwitter all years combined happiness: 6.10twitter 2012 happiness: 5.98why twitter 2012 is less happy than twitter all yearscombined:per word average happiness shiftword rank1. hate-   2. love+   3. mad-   4. free+   5. hurt-   6. hell-   7. lie-   8. ugly-   9. stupid-   10. fat-   11. war-   12. fun+   13. alone-   14. home+   15. people+   16. win+   17. god+   18. death-   19. baby+   20. bored-   21. fire-   22. scared-   23. hungry-   24. crisis-   25. crash-   26. dead-      +      -         +      -   b: anew wordshifttwitter all years combined happiness: 6.63twitter 2012 happiness: 6.58why twitter 2012 is less happy than twitter all yearscombined:per word average happiness shiftword rank1. new+   2. bitch-   3. hate-   4. shit-   5. free+   6. great+   7. mad-   8. like+   9. awkward-   10. good+   11. lie-   12. live+   13. fun+   14. bout-   15. boa-   16. thanks+   17. hell-   18. old-   19. dick-   20. war-   21. hurt-   22. happy+   23. home+   24. awesome+   25. stupid-   26. ugly-      +      -         +      -   c: wk wordshifttwitter all years combined happiness: 6.34twitter 2012 happiness: 6.20why twitter 2012 is less happy than twitter all yearscombined:per word average happiness shiftword rank1. bitch*-   2. lag*-   3. mar*-   4. hate*-   5. like*+   6. great+   7. please*+   8. thank*+   9. miss*-   10. faze*-   11. free+   12. pan*-   13. will+   14. gain+   15. live+   16. awkward-   17. good+   18. damn-   19. help+   20. fun*-   21. mad-   22. want*+   23. trying-   24. just+   25. okay+   26. awesome+      +      -         +      -   d: mpqa wordshifttwitter all years combined happiness: 0.24twitter 2012 happiness: 0.18why twitter 2012 is less happy than twitter all yearscombined:per word average happiness shiftword rank1. haha*+   2. bitch*-   3. shit*-   4. fuck-   5. lol+   6. hate-   7. good+   8. great+   9. please*+   10. fuckin*-   11. miss-   12. free+   13. awkward*-   14. thanks+   15. love+   16. lmao+   17. nag*-   18. hope+   19. hurt*-   20. terror*-   21. slut*-   22. well+   23. grr*-   24. amaz*+   25. kill*-   26. thank+      +      -         +      -   e: liwc wordshifttwitter all years combined happiness: 0.41twitter 2012 happiness: 0.32why twitter 2012 is less happy than twitter all yearscombined:per word average happiness shiftword rank1. shit-   2. fuck-   3. bitch-   4. hate-   5. like+   6. great+   7. miss-   8. work+   9. free+   10. fucking-   11. good+   12. gain+   13. awkward-   14. awesome+   15. fun+   16. damn-   17. love+   18. mad-   19. win+   20. wow+   21. jam-   22. lie-   23. right+   24. top+   25. thank+   26. hurt-      +      -         +      -   f: liu wordshifttwitter all years combined happiness: 0.18twitter 2012 happiness: 0.09why twitter 2012 is less happy than twitter all yearscombined:per word average happiness shiftword ranks19

fig. s19: word shifts for twitter in 2014. the reference word usage is all of twitter (the 10% gardenhose feed) from
september 2008 through april 2015, with the word usage normalized by year.

1. no-   2. haha+   3. not-   4. lol+   5. hahaha+   6. die-   7. love+   8. good+   9. bad-   10. con-   11. ill-   12. down-   13. hell-   14. home+   15. thanks+   16. damn-   17. last-   18. sin-   19. sorry-   20. don't-   21. tired-   22. hahahaha+   23. great+   24. new+   25. bored-   26. sick-      +      -         +      -   a: labmt wordshifttwitter all years combined happiness: 6.10twitter 2014 happiness: 6.03why twitter 2014 is less happy than twitter all yearscombined:per word average happiness shiftword rank1. love+   2. happy+   3. good+   4. home+   5. people+   6. hell-   7. hate-   8. life+   9. sick-   10. fun+   11. birthday+   12. bored-   13. win+   14. sin-   15. ugly-   16. mad-   17. bed+   18. stupid-   19. sad-   20. cute+   21. party+   22. proud+   23. war-   24. person-   25. free+   26. gold+      +      -         +      -   b: anew wordshifttwitter all years combined happiness: 6.63twitter 2014 happiness: 6.68why twitter 2014 is happier than twitter all yearscombined:per word average happiness shiftword rank1. die-   2. love+   3. good+   4. con-   5. mean-   6. happy+   7. boa-   8. new+   9. live+   10. home+   11. fun+   12. bad-   13. bout-   14. old-   15. thanks+   16. hell-   17. bored-   18. great+   19. shit-   20. thank+   21. bitch-   22. sick-   23. late-   24. sin-   25. christmas+   26. awesome+      +      -         +      -   c: wk wordshifttwitter all years combined happiness: 6.34twitter 2014 happiness: 6.27why twitter 2014 is less happy than twitter all yearscombined:per word average happiness shiftword rank1. please*+   2. mar*-   3. bar*-   4. love+   5. too*-   6. gain+   7. lag*-   8. good+   9. faze*-   10. want*+   11. just+   12. pan*-   13. like*+   14. well+   15. happy+   16. mean-   17. live+   18. bitch*-   19. yes*+   20. long*-   21. jam-   22. woo*+   23. vie*-   24. fun*-   25. dim*-   26. dig*+      +      -         +      -   d: mpqa wordshifttwitter all years combined happiness: 0.24twitter 2014 happiness: 0.26why twitter 2014 is happier than twitter all yearscombined:per word average happiness shiftword rank1. haha*+   2. lol+   3. please*+   4. love+   5. good+   6. heh*+   7. bitch*-   8. fuckin*-   9. fuck-   10. damn*-   11. ok+   12. shit*-   13. happy+   14. well+   15. bore*-   16. ha+   17. hell-   18. best+   19. grr*-   20. sorry-   21. crying-   22. amor*+   23. ignor*-   24. hate-   25. ugh-   26. sin-      +      -         +      -   e: liwc wordshifttwitter all years combined happiness: 0.41twitter 2014 happiness: 0.33why twitter 2014 is less happy than twitter all yearscombined:per word average happiness shiftword rank1. gain+   2. love+   3. good+   4. work+   5. die-   6. well+   7. fuck-   8. happy+   9. fucking-   10. great+   11. damn-   12. shit-   13. jam-   14. like+   15. nice+   16. thank+   17. fun+   18. best+   19. cool+   20. wow+   21. awesome+   22. sorry-   23. bad-   24. yay+   25. cold-   26. hell-      +      -         +      -   f: liu wordshifttwitter all years combined happiness: 0.18twitter 2014 happiness: 0.18why twitter 2014 is less happy than twitter all yearscombined:per word average happiness shiftword rankrank dictionary
sent140lex
1.
2.
labmt
hashtagsent
3.
sentiid138
4.
vader
5.
sentistrength
6.
7.
senticnet
umigon
8.
9.
socal
10. wdal
11.
afinn
12. ol
13. maxdi   
14.
15. mpqa
16. wk
17.
18.
19. gi
20.
21.
22.
23.
24.
25.
26.

liwc07
liwc01
emolex
anew
usent
panas-x
emoticons

liwc15
pattern

emosenticnet

% tweets scored f1 of tweets scored calibrated f1 overall f1

s20

100.0
100.0
100.0
98.6
81.3
73.9
97.3
67.1
82.2
99.9
73.6
66.7
94.1
96.0
73.2
96.5
61.8
69.0
67.6
60.3
54.3
59.4
64.1
4.5
1.7
1.4

0.89
0.69
0.67
0.67
0.75
0.83
0.61
0.87
0.71
0.58
0.78
0.83
0.58
0.56
0.73
0.53
0.81
0.71
0.72
0.80
0.83
0.73
0.65
0.74
0.88
0.72

0.88
0.78
0.64
0.68
0.81
0.81
0.64
0.85
0.75
0.64
0.80
0.82
0.70
0.59
0.72
0.72
0.78
0.75
0.70
0.75
0.75
0.69
0.68
0.73

   

0.77

0.89
0.69
0.67
0.67
0.61
0.61
0.59
0.58
0.58
0.58
0.57
0.55
0.54
0.54
0.53
0.51
0.50
0.49
0.49
0.48
0.45
0.43
0.42
0.03
0.01
0.01

table s1: ranked results of sentiment dictionary performance on individual tweets from sts-gold dataset (saif, 2013).
we report the percentage of tweets for which each dictionary contains at least 1 entry, the f1 score on those tweets, and the
overall classi   cation f1 score. the calibrated f1 score tunes the decision threshold between positive and negative tweets with
a random 10% training sample.

s8 appendix: naive bayes results and derivation

s21

we now provide more details on the implementation of naive bayes, a derivation of the linearity structure, and

more results from the classi   cation of movie reviews.

first, to implement a binary naive bayes classi   er for a collection of documents, we denote each of the n words
in the given document t as wi, the word frequency as fi(t ), and class labels c1, c2. the id203 of a document
t belonging to class c1 can be written as

p (c1|t ) =

p (c1)p (t|c1)

p (t )

.

since we do not know p (t|c1) explicitly, we make the naive assumption that each word appears independently, and
thus write

p (c1|t ) =

p (c1)    [p (f1(t )|c1)    p (f2(t )|c1)       p (fn (t )|c1)]

p (t )

.

since we are only interested in comparing p (c1|t ) and p (c2|t ), we disregard the shared denominator and have

p (c1|t )     p (c1)    [p (f1(t )|c1)    p (f2(t )|c1)       p (fn (t )|c1)] .

finally we say that document t belongs to class c1 if p (c1|t ) > p (c2|t ). given that the probabilities of individual
words are small, to avoid machine truncation error we compute these probabilities in log space, such that the product
of individual word likelihoods becomes a sum

log p (c1|t )     log p (c1) +

p (fi(t )|c1).

n(cid:88)

i=1

assigning a classi   cation of class c1 if p (c1|t ) > p (c2|t ) is the same as saying that the di   erence between the two is
positive, i.e. p (c1|t )    p (c2|t ) > 0 and since the logarithm is monotonic, log p (c1|t )    log p (c2|t ) > 0. to examine
how individual words contribute to this di   erence, we can write

0 < log p (c1|t )     log p (c2|t )
    log p (c1) +

log p (fi(t )|c1)     log p (c2)    

n(cid:88)

i=1

log p (fi(t )|c2)

    log p (c1)     log p (c2) +

[log p (fi(t )|c1) log p (fi(t )|c2)]

n(cid:88)

i=1

    log

p (c1)
p (c2)

+

log

p (fi(t )|c1)
p (fi(t )|c2)

.

n(cid:88)

i=1

n(cid:88)

i=1

we can see from the above that the contribution of each word wi (or more accurately, the likelihood of frequency in
document t being of class c as p (fi(t )|c1)) is a linear constituent of the classi   cation.

next, we include the detailed results of the naive bayes classi   er on the movie review corpus.

s22

fig. s20: results of the nb classi   er on the movie reviews corpus.

fig. s21: nyt sections ranked by naive bayes in two of the    ve trials.

0.00.51.01.52.02.5log10(num reviews)1.51.00.50.00.51.01.52.0sentimentsentiment over many random samples for naive bayespositive reviewsnegative reviews0.00.51.01.52.02.5log10(num reviews)0.000.050.100.150.200.25fraction overlappingsentiment over many random samples for naive bayes0.0100.0050.0000.0050.0100.0150.0200.0250.0300.035happs diff from unweighted average1. television2. education3. society4. leisure5. movies6. weekend7. living8. cultural9. arts10. classified11. books12. style13. home14. regional15. sports16. week_in_review17. financial18. editorial19. magazine20. science21. metropolitan22. foreign23. national24. travelranking of nyt sections by nb0.050.040.030.020.010.000.010.02happs diff from unweighted average1. travel2. movies3. books4. weekend5. arts6. society7. cultural8. leisure9. regional10. week_in_review11. editorial12. foreign13. metropolitan14. national15. financial16. magazine17. sports18. style19. education20. classified21. living22. home23. science24. televisionranking of nyt sections by nbs23

most informative

positive

negative

value
   ynt
truman
charles
event
shrek
cusack
bulworth
robocop
jedi
gangster

word
20.21
15.95
13.83
13.83
13.83
13.83
12.76
12.76
11.70
11.70

value
godzilla
werewolf
gorilla
spice
memphis
sgt
jennifer
hill
max
200

nyt society

positive

negative

value
truman
charles
gangster
speech
melvin
wars
agents
dance
bleak
pitt

word
20.40
12.88
12.88
10.73
8.59
8.59
8.59
8.59
8.59
7.51

value
godzilla
hill
jennifer
fatal
freddie
=
mess
gene
apparent
travolta

word
27.27
26.33
20.68
15.04
14.10
13.16
13.16
13.16
12.22
12.22

word
26.08
20.49
12.11
10.25
9.32
8.85
7.45
6.52
6.52
6.52

table s2: trial 1 of naive bayes trained on a random 10% of the movie review corpus, and applied to the new york times
society section. we show the words which are used by the trained classi   er to classify individual reviews (in corpus), and on
the new york times (out of corpus). in addition, we report a second trial in table s3, since naive bayes is trained on a
random subset of data, to show the variation in individual words between trials (while performance is consistent).

s24

most informative

positive

negative

value
shrek
poker
shark
maggie
guido
outstanding
political
journey
bulworth
bacon

word
34.63
24.14
18.89
17.84
17.84
17.84
16.79
16.79
15.74
15.74

value
west
webb
jackal
travolta
woo
coach
awful
brenner
gabriel
general   s

nyt society

positive

negative

value
poker
journey
political
tribe
tony
price
threat
titanic
dicaprio
kate

word
33.39
17.20
17.20
15.18
12.14
9.44
8.09
7.59
7.42
7.08

value
west
coach
travolta
gabriel
pointless
stupid
screaming
mess
boring
=

word
18.11
17.15
15.25
14.29
13.34
13.34
13.34
13.34
13.34
12.39

word
17.79
13.84
13.84
8.90
7.91
7.91
7.91
7.12
6.92
6.92

table s3: trial 2 of naive bayes trained on a random 10% of the movie review corpus, and applied to the new york times
society section. we show the words which are used by the trained classi   er to classify individual reviews (in corpus), and on
the new york times (out of corpus). this second trial is in addition to the    rst trial in table s2, since naive bayes is trained
on a random subset of data, to show the variation in individual words between trials (while performance is consistent).

s9 appendix: movie review benchmark of additional dictionaries

here, we present the accuracy of each dictionary applied to binary classi   cation of movie reviews.

s25

rank title
ol
1.
hashtagsent
2.
mpqa
3.
sentiid138
4.
labmt
5.
afinn
6.
umigon
7.
gi
8.
socal
9.
10.
vader
11. wdal
sentistrength
12.
emolex
13.
liwc15
14.
liwc01
15.
liwc07
16.
pattern
17.
panas-x
18.
sent140lex
19.
senticnet
20.
21.
anew
22. maxdi   
23.
24. wk
25.
26.

emoticons
usent

emosenticnet

% scored f1 trained f1 untrained

100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
33
100
100
100
100
100
100

0
40

0.70
0.67
0.67
0.65
0.64
0.67
0.65
0.65
0.71
0.67
0.60
0.63
0.65
0.64
0.65
0.64
0.73
0.51
0.68
0.62
0.57
0.66
0.58
0.63

   
   

0.71
0.66
0.66
0.65
0.63
0.63
0.62
0.61
0.60
0.60
0.59
0.58
0.56
0.55
0.54
0.53
0.52
0.51
0.47
0.45
0.36
0.36
0.34
0.34

   
   

table s4: ranked performance of dictionaries on the movie review corpus.

senticnet
socal
emosenticnet
pattern

hashtagsent
liwc15
liwc07
liwc01
labmt
sent140lex
sentiid138

rank title
1.
2.
3.
4.
5.
6.
7.
8. wdal
9.
emolex
10. mpqa
11.
12.
13.
14.
15. gi
16. wk
17. ol
18.
19.
20. maxdi   
afinn
21.
22.
anew
umigon
23.
panas-x
24.
emoticons
25.
26.
usent

vader
sentistrength

% scored f1 trained of scored f1 untrained of scored f1 untrained, all

s26

100
99
99
99
99
100
99
99
95
93
97
88
98
81
80
97
76
79
77
83
70
63
53
1
0
2

0.55
0.53
0.53
0.52
0.54
0.55
0.54
0.53
0.54
0.54
0.53
0.56
0.52
0.55
0.55
0.54
0.56
0.56
0.54
0.54
0.56
0.52
0.56
0.53

   
   

0.55
0.55
0.55
0.55
0.54
0.54
0.53
0.53
0.55
0.55
0.52
0.55
0.46
0.55
0.55
0.45
0.57
0.55
0.54
0.49
0.56
0.48
0.56
0.53

   
   

0.55
0.55
0.54
0.54
0.54
0.54
0.53
0.52
0.52
0.52
0.50
0.49
0.45
0.45
0.44
0.44
0.44
0.43
0.41
0.41
0.39
0.30
0.30
0.01

   
   

table s5: ranked performance of dictionaries on the movie review corpus, broken into sentences.

s27

fig. s22: word shifts for the movie review corpus, with panel letters continuing from fig. 6. we again see many of the same
patterns, and refer the reader to fig. 6 for a more in depth analysis.

  1. guilty-   2. strong+   3. interested+   4. inspired+   5. afraid-   6. nervous-   7. ashamed-   8. upset-   9. scared-   10. determined+   11. attentive+   12. proud+   13. alert+   14. enthusiastic+   15. active+   16. jittery-   17. distressed-   18. irritable-   19. excited+   20. hostile-      +      -         +      -   g: panas-x wordshiftall negative reviews happiness: 0.32all positive reviews happiness: 0.46why all positive reviews are happier than all negativereviews:per word average happiness shiftword rank  1. bad-   2. worst-   3. best+   4. great+   5. boring-   6. stupid-   7. most+   8. perfect+   9. awful-   10. terrible-   11. wonderful+   12. many+   13. excellent+   14. better+   15. own+   16. least-   17. beautiful+   18. annoying-   19. love+   20. good+   21. brilliant+   22. more+   23. worse-   24. fails-   25. horrible-      +      -         +      -   h: pattern wordshiftall negative reviews happiness: 0.05all positive reviews happiness: 0.13why all positive reviews are happier than all negativereviews:per word average happiness shiftword rank  1. excellent+   2. wonderful-   3. superb+   4. kudos+   5. delightful-   6. mom+   7. courtesy-   8. deserve-   9. nifty+   10. momma+   11. greatest+   12. respectability-   13. gusto-   14. engaging+   15. first-class+   16. amusingly-   17. awesome+   18. congratulations+   19. admiration-   20. pivotal-   21. respected+   22. top-flight+   23. workmanlike-   24. lucky-   25. fab+      +      -         +      -   i: sentiid138 wordshiftall negative reviews happiness: 0.81all positive reviews happiness: 0.83why all positive reviews are happier than all negativereviews:per word average happiness shiftword rank  1. bad-   2. great+   3. best+   4. worst-   5. boring-   6. love+   7. wonderful+   8. funny+   9. perfect+   10. worse-   11. ridiculous-   12. excellent+   13. outstanding+   14. awful-   15. terrible-   16. brilliant+   17. beautiful+   18. superb+   19. terrific+   20. perfectly+   21. dumb-   22. fun+   23. fantastic+   24. good+   25. kill-      +      -         +      -   j: afinn wordshiftall negative reviews happiness: -0.03all positive reviews happiness: 1.15why all positive reviews are happier than all negativereviews:per word average happiness shiftword rank  1. bad-   2. plot-   3. just+   4. even-   5. great+   6. like+   7. best+   8. get-   9. worst-   10. well+   11. stupid-   12. better+   13. war-   14. love+   15. too-   16. perfect+   17. true+   18. know+   19. make-   20. worse-   21. waste-   22. mess-   23. point-   24. ridiculous-   25. wonderful+      +      -         +      -   k: gi wordshiftall negative reviews happiness: 0.03all positive reviews happiness: 0.18why all positive reviews are happier than all negativereviews:per word average happiness shiftword rank  1. bad-   2. movie+   3. i+   4. the-   5. no-   6. life+   7. like+   8. nothing-   9. off-   10. worst-   11. great+   12. this-   13. stupid-   14. be+   15. love+   16. war-   17. best+   18. to-   19. have+   20. better+   21. is-   22. well+   23. waste-   24. mess-   25. young+      +      -         +      -   l: wdal wordshiftall negative reviews happiness: 1.96all positive reviews happiness: 1.98why all positive reviews are happier than all negativereviews:per word average happiness shiftword rank  1. bad-   2. movie+   3. worst-   4. no-   5. great+   6. why-   7. unfortunately-   8. stupid-   9. tarantino+   10. poor-   11. supposed-   12. boring-   13. even-   14. best+   15. anaconda-   16. awful-   17. terrible-   18. excellent+   19. poorly-   20. wonderful+   21. love+   22. worse-   23. lifeless-   24. doesn't-   25. you+      +      -         +      -   m: nrc wordshiftall negative reviews happiness: 0.06all positive reviews happiness: 0.20why all positive reviews are happier than all negativereviews:per word average happiness shiftword rank6.686.986.686.98s28

s10 appendix: coverage removal and binarization tests of labmt dictionary

here, we perform a detailed analysis of the labmt dictionary to further isolate the e   ects of dictionary coverage and
scoring type. this analysis is motivated by ensuring that the our results are not confounded entirely by the quality
of the word scores across dictionaries, such that the e   ect of coverage and scoring type are isolated. we focus on
the movie review corpus for this analysis, and report the accuracy of the labmt dictionary with the aforementioned
modi   cations using the f1 score.

first, we gradually reduce the range of scores in the labmt dictionary from a centered -4     4, down to just the
integer scores    1 and 1. in figure s23, the f1 score is show across this gradual, linear change to a binary dictionary.
we observe that the direct binarization of the labmt dictionary results in a degradation of performance. second, to
test the e   ect of coverage alone, we systematically reduce the coverage of the labmt dictionary and again attempt
the binary classi   cation task of identifying movie review polarity. three possible strategies to reduce the coverage
are (1) removing the most frequent words, (2) removing the least frequent words, and (3) removing words randomly
(irrespective of their frequency of usage). in figures s24 and s25, we show the resulting f1 score of classi   cation
performance for each of these three strategies and the total coverage from each removal strategy. we observe that while
certain strategies are more e   ective at retaining performance, lower coverage scores are all lower despite substantial
variation, and the overall pattern for each strategy is a decrease in performance for decreasing coverage. in both cases
these results are consistent with those seen across dictionaries: integer scores and low coverage strongly reduce the
performance of the 2-class movie review classi   cation task, as measured by the f1-score.

fig. s23: the direct binarization of the labmt dictionary results in a degradation of performance. the binarization is
accomplished by linearly reducing the range of scores in the labmt dictionary from a centered -4     4 to the integer scores    1
and 1.

0.00.20.40.60.81.0percentagebinarized0.500.550.600.650.700.75f1scores29

fig. s24: the resulting f1 score of classi   cation performance for each of three coverage removal strategies. these strategies,
labeled in the above, are: (1) removing the most frequent words, (2) removing the least frequent words, and (3) removing words
randomly (irrespective of their frequency of usage).

fig. s25:
the resulting coverage for each of three coverage removal strategies. again, these strategies, labeled in the
above, are: (1) removing the most frequent words, (2) removing the least frequent words, and (3) removing words randomly
(irrespective of their frequency of usage).

0.00.20.40.60.81.0percentagewordsremoved0.400.450.500.550.600.65f1meanscorerandoid113astfreqmostfreq0.00.20.40.60.81.0percentagewordsremoved0.00.20.40.60.81.0totalcoveragerandoid113astfreqmostfreq