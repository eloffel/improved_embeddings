topic segmentation via community detection in complex networks

structures and organization in complex systems

henrique f. de arruda

institute of mathematics and computer sciences,

university of s  o paulo, s  o carlos, s  o paulo, brazil

luciano da f. costa

s  o carlos institute of physics,

university of s  o paulo, s  o carlos, s  o paulo, brazil

5
1
0
2
 
c
e
d
4

 

 
 
]
l
c
.
s
c
[
 
 

1
v
4
8
3
1
0

.

2
1
5
1
:
v
i
x
r
a

diego r. amancio   

institute of mathematics and computer sciences,

university of s  o paulo, s  o carlos, s  o paulo, brazil

many real systems have been modelled in terms of network concepts, and written texts are a
particular example of information networks. in recent years, the use of network methods to analyze
language has allowed the discovery of several interesting    ndings, including the proposition of novel
models to explain the emergence of fundamental universal patterns. while syntactical networks, one
of the most prevalent networked models of written texts, display both scale-free and small-world
properties, such representation fails in capturing other textual features, such as the organization
in topics or subjects.
in this context, we propose a novel network representation whose main
purpose is to capture the semantical relationships of words in a simple way. to do so, we link all
words co-occurring in the same semantic context, which is de   ned in a threefold way. we show
that the proposed representations favours the emergence of communities of semantically related
words, and this feature may be used to identify relevant topics. the proposed methodology to
detect topics was applied to segment selected wikipedia articles. we have found that, in general,
our methods outperform traditional bag-of-words representations, which suggests that a high-level
textual representation may be useful to study semantical features of texts.

pacs numbers: 89.75.fb

i.

introduction

with the ever-increasing amount of information avail-
able online, the classi   cation of texts has established it-
self as one of the most relevant applications supporting
the development of e   cient and informative search en-
gines [1]. as the correct categorization of texts is of chief
importance for the success of any search engine, this task
has been extensively studied, with signi   cant success in
some scenarios [2, 3]. however, the comprehension of
texts in a human-like fashion remains a challenge, as it
is the case of semantical analyses performed in disam-
biguating systems and id31 [4, 5]. while
humans are able to identify the content and semantics
conveyed in written texts in a artlessly manner, auto-
matic classi   cation methods fail in several aspects mainly
because the lack of a general representation of world
knowledge. currently, the automatic categorization of
texts stands as one of the most studied applications in
information sciences. in recent years, multidisciplinary
concepts have been applied to assist the classi   cation,
including those based on physical approaches [3, 6   9].

an important subarea of the research performed in
text classi   cation is the detection of subtopics in doc-
uments [10, 11]. this task is not only useful to an-

alyze the set of subjects approached in the document,
but also to assist computational applications, such as
automatic text summarization [12] and text recommen-
several approaches have been proposed
dation [13].
to tackle the subtopic identi   cation problem.
the
most common feature employed to cluster together re-
lated sentences/paragraphs is the frequency of shared
words [14, 15]. while most of the works on this area
considers only the presence/absence of speci   c words in
sentences/paragraphs, only a few studies have consid-
ered the organization of words to classify documents [16].
speci   cally, in this paper, we approach the text segmen-
tation problem using the organization of words as a rel-
evant feature for the task. the non-trivial relationships
between concepts are modelled via novel networked rep-
resentations.

complex networks have been used to analyze a myr-
iad of real systems [17],
including several features of
the human language [18]. a well-known model
for
text representation is the so-called word adjacency (co-
occurrence) model, where two words are linked if they
appear as neighbors in the text. though seemingly sim-
ple, this model is able to capture authors    styles [19   21],
textual complexity [8, 22] and many other textual as-
pects [18, 23, 24]. a similar model, referred to as syn-
tactical network, takes into account the syntactical rep-

resentation of texts by connecting words syntactically re-
lated. syntactical networks shared topological properties
of other real world networks, including both scale-free
and small-word behaviors. these networks have been
useful for example to capture language-dependent fea-
tures [25]. unlike traditional models, in this paper, we
extend traditional text representations to capture seman-
tical features so that words are connected if they are re-
lated semantically. here, we take the view that a given
subtopic is characterized by a set of words which are in-
ternally connected, with a few external relationships with
words belonging to other subjects. as we shall show,
this view can be straightforwardly translated into the
concept of community structure, where each semantical
structure can be regarded as a di   erent network com-
munity [26]. the proposed framework was evaluated in
a set of wikipedia articles, which are tagged according
to their subjects.
interestingly, the network approach
turned to be more accurate than traditional method in
several studied datasets, suggesting that the structure
of subjects can be used to improve the task. taken to-
gether, our proposed methods could be useful to analyse
written texts in a multilevel networked fashion, as re-
vealed by the topological properties obtained from tradi-
tional word adjacency models and networks of networks
in higher hierarchies via community analysis.

this manuscripts is organized as follows. in section ii,
we present the proposed network representation to tackle
the subtopic identi   cation task. in section iii we present
the dataset employed for the task. the results obtained
are then shown in section iv. finally, in section v, we
conclude this paper by presenting perspectives for further
research on related areas.

ii. complex network approach

the approach we propose here for segmenting texts
according to subjects relies upon a networked represen-
tation of texts. in this section, we also detail the method
employed for identifying network communities, for the
proposed methodology requires a prior segmentation of
text networks.

representing texts as networks

a well-known representation of texts as networks is
the word-adjacency model, where each di   erent word
is a node and links are established between adjacent
words [27, 28]. this model, which can be seen as a sim-
pli   cation of the so-called syntactical networks [25], has
been used with success to identify styles in applications
related to authorship attribution, language identi   cation
and authenticity veri   cation [29   31]. the application of
this model in id91 analysis is not widespread be-

2

cause traditional word adjacency networks are not or-
ganized in communities, if one considers large pieces of
texts. in this paper, we advocate that there is a strong re-
lationship between communities structures and subjects.
for this reason, we modify the traditional word adjacency
model to obtain an improved representation of words in-
teractions in written texts. more speci   cally, here we
present a threefold extension which considers di   erent
strategies of linking non-adjacent words.

prior to the creation of the model itself, some pre-
processing are applied. first, we remove stopwords, i.e.
the very frequent words conveying no semantic meaning
are removed (e.g. to, have and is). note that, such words
may play a pivotal role in style identi   cation, however in
this study they are not relevant because such words are
subject independent. the pre-processing step is also re-
sponsible for removing other non-relevant tokens, such
as punctuation marks. because we are interesting in
representing concepts as network nodes, it is natural to
consider that variant forms of the same word becomes
a unique node of the network. to do so, we lemmatize
the words so that nouns and verbs are mapped into their
singular and in   nitive forms [32]. to assist this process,
we label each word according to their respective parts of
speech in order to solve possible ambiguities. this part
of speech labeling is done with the high-accurate maxi-
mum id178 model de   ned in [33, 34]. finally, we con-
sider only nouns and verbs, which are the main classes of
words conveying semantical meanings [35, 36]. in all pro-
posed models, each remaining word (lemmatized nouns
and verbs) becomes a node. three variations concern-
ing the establishment of links between such words are
proposed:

    extended co-occurence (ec) model: in this model,
the edges are established between two words if they
are separated by d = (       1) or less intermediary
words in the pre-processed text. for example, if
   = 2 and the pre-processed text comprises the
sentence    w1 w2 w3 w4   , then the following set of
edges is created: e = {w1     w2, w2     w3, w3    
w4, w1     w3, w2     w4}. note that, if w = 1, the
traditional co-occcurrence (word adjacency) model
is recovered, because only adjacent words are con-
nected.
    paragraph-based (pb) model: in this model, we link
in a clique the words belonging to the same para-
graph. we disregard, however, edges linking words
separated by more than d intermediary words in
the pre-processing text. this method relies on
the premise that paragraphs represents the funda-
mental sub-structure in which a text is organized,
whose words are semantically related [37].
the pb
    adapted paragraph-based (apb) model:
model does not take into account the fact that

words may co-occur in the same paragraph just by
chance, as it is the case of very frequent words.
to consider only signi   cant links in the pb model,
we test the statistical signi   cance of co-occurrences
with regard to random, shu   ed texts [38]. given
two words vi and vj, an edge is created only if
the frequency of co-occurrences (k) (i.e. the num-
ber of paragraphs in which vi and vj co-occurs) is
much higher than the same value expected in a null
model. the signi   cance of the frequency k can be
computed using the quantity p(k), which quanti   es
the id203 of two words to appear in the same
paragraph considering the null model. to compute
the id203 p(k), let n1 and n2 be the number
of distinct partitions in which vi and vj occur, re-
spectively. the distribution of id203 for k,

p(k) = {n1}k{n2}k{n     n1}n2   k

(cid:34)

n2   k   1(cid:89)

=

j=0

{n}n2{k}k
n     j     n1

n     j

(cid:35) k   1(cid:89)

j=0

if in a given text the number of co-occurrences of
two words is r, the p-value p associated to r can be
computed as

p(k     r) =

p(r),

(3)

(cid:88)

k   r

where p(r) is computed according to equation 2.
now, using equation 3, the most signi   cant edges
can be selected.

figure 1 illustrates the topology obtained for the three
proposed models representing a text with four para-
graphs from wikipedia [39]. note that the structure of
communities depends on the method chosen to create the
netswork. especially, a lower modularity has been found
for the apb model in this example and for this reason
the organization in communities is not so clear. as we
shall show, the identi   cation of communities of extreme
importance for identifying accurately the subjects.

from network communities to text subjects

the    rst step for id91 texts according to subjects
concerns the computation of network communities, i.e. a
region of nodes with several intra-edges (i.e. edges inside
the community) and a few inter-edges (i.e. edges leaving

3

the number of co-occurrences between vi and vj,
can be computed as

p(k) =

(n ; k, n1     k, n2     k)

(n ; n1)(n ; n2)

, where

x!

1

x1! . . . xn!

(x; y1, . . . , yn)    
the above expression for p(k) can be rewritten in
a more convenient way, using the notation

(x     y1     . . . yn)!

.

b   1(cid:89)

i=0

{a}b    

(a     i),

(1)

which is adopted for a     b. in this case, the likeli-
hood p(k) can be written as

= {n1}k{n2}k{n     n1}n2   k
{n}n2   k{n     n2 + k}k{k}k
(n1     j)(n2     j)

.

(n     n2 + k     j)(k     j)

(2)

(cid:19)

(cid:88)

(cid:88)

(cid:18)

i

j

the community). methods for    nding network commu-
nities have been applied in several applications [41   43],
including in text analysis [44]. the quality of partitions
obtained from community structure detection methods
can be obtained from the modularity q, which is de   ned
as

q =

1
2m

aij    

kikj
2m

  (ci, cj),

(4)

i

(cid:80)

represents the membership of the i-th node, ki =(cid:80)
is the node degree, m = 1/2(cid:80)

where aij denotes an element of the adjacent matrix (i.e.
aij = 1 if i and j are linked and aij = 0, otherwise), ci
j aij
j aij is the total num-
ber of edges in the network and   (x, y) is the kronecker   s
delta. several algorithms use the modularity to assist the
identi   cation of partitions in networks. note that the
modularity de   ned in equation 4 quanti   es the di   erence
between the actual number of intra-links (i.e. the links in
the same community, aij  (ci, cj)) and the expected num-
ber of intra-links (i.e. the number of links in the same
community of a random network, (kikj/2m )  (ci, cj)).
here we use a simple yet e   cient approach devised in [40],
where the authors de   ne a greedy optimization strategy
for q. more speci   cally, to devise a greedy optimization
the authors de   ne the quantities

avw  (cv, i)  (cw, j),

(5)

(cid:88)

(cid:88)

v

w

eij =

1
2m

4

fig. 1. example of networks obtained from the following models: (a) ec, (b) pb and (c) apb. the colours represent in this
case the community to which each node belongs. to obtain the partitions, we used the fast greedy algorithm [40]. note that
the organization of nodes in communities varies from model to model. while in (a) and (b) the organization in communities is
clear, in (c) the modular organization is much more weak.

(cid:88)

v

  i =

1
2m

which represents the fraction of edges that link nodes in
community i and j, and

kv  (cv, i),

(6)

which denotes the fraction of ends of edges that are linked
to nodes in community i. the authors rede   ne q in

equation 4 by replacing   (cv, cw) to(cid:80)
(cid:35)(cid:88)

i   (cv, i)  (cw, i):

  (cv, i)  (cw, i)

avw    

(cid:88)

w

kvkw
2m

i

(cid:88)

1
2m

avw  (cv, i)  (cw, i)

kv  (cv, i)

kw  (cw, i)

(cid:35)

w

(7)

q =

1
2m

(cid:88)

(cid:88)
(cid:34)

v

(cid:34)
(cid:88)
(cid:88)
(cid:88)

w

v

1
2m

i

i

   

1
(cid:88)
2m
(eii       2
i ).

v

=

=

using the modularity obtained in equation 7, it is possible
to detect communities using an agglomerative approach.
first, each node is associated to a distinct community.
in other words, the partition initially comprises only sin-
gleton clusters. then, nodes are joined to optimize the
variation of modularity,    qij. thus, after the    rst ag-
glomeration, a multi-graph is de   ned so that communi-
ties are represented as a single node in a new adjacency
matrix with elements a(cid:48)
ij = 2m eij. more speci   cally,
the operation of joining nodes is optimized by noting
that    qij > 0 only if communities i and j are adja-
cent. the implementation details are provided in [40].
note that our approach does not rely on any speci   c

community identi   cation method. we have speci   cally
chosen the fast-greedy method because, in preliminary
experiments, it outperformed other similar algorithms,
such as the multi-level approach devised in [45] (result
not shown).

given a partition of the network established by the
community detection method, we devised the following
approach for id91 the text in ns distinct subjects.
let c = {c1, c2, . . .} and    = {  1,   2, . . .} be the set of
communities in the network and the set of paragraphs in
the text, respectively. if the obtained number of commu-
nities (nc) is equal to the expect number of subjects ns,
then we assign the label ci to the word that corresponds
to node i in the text. as a consequence, each paragraph
is represented by a set of labels l(  j) = {l
, . . .},
    c is the label associated to the i-th word of
where l
the j-th paragraph (  j). the number of occurrences of
each label in each paragraph is de   ned as

(  j )
, l
2

(  j )
i

(  j )
1

(cid:88)

l   l(  j )

f (ci,   j) =

  (ci, l).

thus the subject associated to the paragraph   j is

  s(  j) = arg max

ci   c f (ci,   j),

i.e.
the most frequent label is chosen to represent the
subject of each paragraph. to illustrate the application
of our method, we show in figure 2 the communities
obtained in a text about cars, whose    rst paragraph ap-
proaches the de   nition and invention of cars, and the
remaining paragraphs present their parts.

the expression in equation 9 is suitable to be applied
only when nc = ns. whenever the number of expected

(8)

(9)

(a)(b)(c)5

fig. 2. example of community obtained with the following models: (a) ec; and (b) pb. as expected, most of the words
belong to a unique subject, while a few words lies at the overlapping region. the visualization of communities obtained with
the apb model is not as good as the ones obtained with both ec and pb methods.

subjects is lower than the number of network commu-
nities, we adopt a strategy to reduce the quantity of
communities found. to do so, the following algorithm
is applied:

data: nc, the number of communities and ns, the

number of subjects.

result: a match between communities and

subjects is established.

while (nc > ns) do

ck = label of the community with the largest
overlapping region;
erase all nodes from ck;
detect again the community structure of the
network formed of the remaining nodes;
update nc;

end
according to the above algorithm, if the most overlap-
ping community is to be estimated, a measure to quantify
the degree of overlapping must be de   ned. in the present
work, we de   ned an overlapping index,   , which is com-
puted for each community. to compute   (ci), we    rst
recover all paragraphs whose most frequent label is the
one associated to community ci. these paragraphs are
represented by the subset   d(ci) = {          |   s(  ) = ci}.
next, we count how many words in   d(ci) are associ-
ated with the community ci. the overlapping is then

de   ned as the the amount of words in   d(ci) which are
associated to a community cj (cid:54)= ci. mathematically, the
overlapping is de   ned as

  (ci) = 1    

  (ci, l).

(10)

(cid:88)

(cid:88)

     

  d(ci)

l   
l(  )

to evaluate the performance of the methods, we em-
ployed the following methodology. let s(  i) be the
subject associated to the i-th paragraph according to
wikipedia. here, we represent each di   erent subtopic
as a integer number, so that s(  i)     [1, ns].
let
  s(  i)     [1, ns] be the label associated to the i-th para-
graph according to a speci   c id91 method, as de-
   ned in equation 9. to quantify the similarity of two
sets s = {s(  1), s(  2), . . .} and   s = {  s(  1),   s(  2), . . .}, it
is necessary consider all combinations of labels permuta-
tions either on s or   s, because the same partition may
be de   ned with distinct labels. for example, if ns = 2,
s = {1, 1, 2, 2} and   s = {2, 2, 1, 1}, both partitions are
the same, even if a straightforward comparison (element
by element) yields a zero similarity. to account for dis-
tinct labelings in the evaluation, we de   ne the operator
p, which maps a sequence of labels to all possible com-
binations. thus, if s = {1, 1, 2, 2}, then

p(s) = {{1, 1, 2, 2},{2, 2, 1, 1}}.

(b)passengerrepairreplacemotorparkingdrivecomfortusagemanufactureautoparkcartfordpartcaramericanroadmoveindianumbercartravelbenefitproductionvehiclefuelincludenaturechinaplaceusageroadauto(a)modelequivalently, the application of the p to s yields two
components:

p(s, 1) = {1, 1, 2, 2} and p(s, 2) = (2, 2, 1, 1).

the accuracy rate,   , is then de   ned as

   = max

i h(s,p(   s, i)),

(11)

where h(x, y ) is the operator that compares the sim-
ilarity between two subsets x = {x1, x2, . . .} and y =
{y1, y2, . . .} and is mathematically de   ned as:

h(x, y ) =

  (xi, yi).

(12)

(cid:88)

i

iii. database

the arti   cial dataset we created to evaluate the meth-
ods is formed from paragraphs extracted from wikipedia
articles. the selected articles can be classi   ed in 8 dis-
tinct topics and 5 distinct subtopics:

1. actors: jack nicholson, johnny depp, robert de

niro, robert downey jr. and tom hanks.

2. cities: barcelona (spain), budapest (hungary),
london (united kingdom), prague (czech repub-
lic) and rome (italy).

3. soccer players: diego maradona (argentina),
(brazil),

leonel messi (argentina), neymar jr.
pel   (brazil) and robben (netherlands).

4. animals: bird, cat, dog, lion and snake.

5. food: bean, cake, ice cream, pasta, and rice.

6. music: classical, funk, jazz, rock and pop.

7. scientists: albert einstein, gottfried leibniz, li-

nus pauling, santos dumont and alan turing.

8. sports: football, basketball, golf, soccer and swim-

ming.

to construct the arti   cial texts, we randomly selected
paragraphs from the articles using two parameters: ns,
the total number of subtopics addressed in the arti   cial
text; and np, the number of paragraphs per subtopic.
for each pair (ns, np) we have compiled a total of 200
documents. to create a dataset with distinct granular-
ity of subtopics, the random selection of subtopics was
performed in a two-fold way. in the dataset comprising
coarse-grained subtopics, hereafter referred to as cgs
dataset, each arti   cial text comprises ns subtopics of
distinct topics. di   erently, in the dataset encompass-
ing    ne-grained subtopics, hereafter referred to as fgs
dataset, the texts are made up of distinct subtopics of
the same major topic.

6

iv. results

in this section, we analyze the statistical properties
of the proposed models in terms of their organization in
communities. we also evaluate the performance of our
model in the arti   cial dataset and compare with more
simple models that do not rely on any networked infor-
mation.

modularity analysis

the models we propose to cluster topics in texts re-
lies on the ability of a network to organize itself in a
modular way. for this reason, we study how the mod-
ularity of networks depends on the models parameters.
the unique parameter that may a   ect networks modular-
ity in our models is   , which amounts to the distance of
links in texts (see de   nition in the description of the ec
model in section ii). note that the distance    controls
the total number of edges of the network, so a compar-
ison of networks modularity for distinct values of    is
equivalent to comparing networks with distinct average
degrees. because the average degree plays an important
role on the computation of the modularity [26], we de-
   ned here a normalized modularity qn that only takes
into account the organization of the network in modules
and is not biased towards dense networks. the normal-
ized modularity qn of a given network with modularity
q is computed as

qn = q     (cid:104)qs(cid:105),

(13)
where qs is the average modularity obtained in 30 equiv-
alent random networks with the same number of nodes
and edges of the original network.

in figure 3, we show the values of q, qn and qs in
the dataset fgs. in the ec model, high values of modu-
larity q are found for low-values of   . to the best of our
knowledge, this is the    rst time that a modular struc-
ture is found in a traditional word adjacency network
(i.e. the ec model with    = 1) in a relatively small net-
work. as    takes higher values and the network becomes
more dense, the modularity q decreases.
in a similar
way, the average modularity (cid:104)qs(cid:105) obtained in equiva-
lent random networks also decreases when    increases.
a di   erent behavior arises for the normalized modular-
ity qn, as shown in figure 3(a). the modularity (cid:104)qs(cid:105)
initially takes a low value, reaching its maximum when
   =   max = 20. note that when    >   max, there is no
signi   cant gain in modularity qn. a similar behavior ap-
pears in the other two considered models (pb and apb)
as the normalized modularity becomes almost constant
for    > 20. because in all models the normalized modu-
larity takes high values for    = 20, we have chosen this
value to construct the networks for the purpose of clus-
tering topics. even though a higher value of normalized

modularity was found for    > 20 in figures 3(b)-(c), we
have decided not to use larger values of    in these mod-
els because only a minor improvement in the quality is
obtained for    > 20.

performance analysis

to evaluate the performance of the proposed network-
based methods, we evaluate the accuracy of the generated
partitions in the cgs and fds datasets presented in sec-
tion iii. we have created two additional methods based
on linguistic attributes to compare the performance of
networked and non-networked methods. both methods
are based on a bag-of-words strategy [1], where each para-
graph is represented by the frequency of appearance of
its words. to cluster the paragraphs in distinct subjects,
we used two traditional id91 methods: id116 [46]
and expectation maximization [47]. in our results, the
bag-of-words strategy based on the id116 and expec-
tation mazimization algorithms are referred to as bow-
k and bow-em, respectively.

in figure 4, we show the results obtained in the dataset
comprising texts whose subtopics belong to distinct ma-
jor topics (cgs dataset). we classi   ed the dataset in
terms of the number of distinct subtopics in each text
(ns) and the total number of paragraphs per subtopic
(np) (see section iii). we    rst note that, in all cases, at
least one networked approach outperformed both bow-
k and bow-em approaches.
in some cases, the im-
provement in performance is much clear (see figure 4
(d), (e) and (f)), while in others the di   erence in per-
formance is lower. when comparing all three networked
approaches, the apb strategy, in average, performs bet-
ter than others when the number of subtopics is ns     3.
for ns = 4, both ec and pb strategies outperforms the
apb method.

in figure 5, we show the performance obtained in the
dataset comprising texts with subtopics belonging to the
same major topic (fds dataset). when one compares
the networked approaches with bow-k and bow-em,
we note again that at least one networked approach out-
performed both non-networked strategies. this result
con   rms that the networked method seems to be use-
ful especially when the subtopics are not very di   erent
from each other, which corresponds to the scenario found
in most real documents. the relevance of our proposed
methods is specially clear when ns = 3. a systematic
comparison of all three networked methods reveals that
the average performance depends on speci   c parameters
of the dataset. in most of the cases, however, the best
average performance was obtained with the ec method.
the apb method also displayed high levels of accuracy,
with no signi   cant di   erence of performance in compar-
ison with ec in most of the studied cases.

all in all, we have shown that networked approaches

7

performs better than traditional techniques that do not
rely on a networked representation of texts. interestingly,
a larger gain in performance was obtained in the tss
dataset, where the di   erence in subtopics is much more
subtle. this result suggests that the proposed networked
approaches are more suitable to analyze real texts, as
one expects that the changes in subjects in a given text
is much more subtle than variations of subjects across
distinct texts. another interesting    nding concerns the
variations of performance in distinct datasets. our re-
sults revealed that there is no unique networked approach
able to outperforms strategies in all studied cases. how-
ever, we have observed that, in general, both ec and
apb methods performs better than the ap approach
and, for this reason, they should be tried in real applica-
tions.

v. conclusion

in this paper, we have proposed a method to    nd
subtopics in written texts using the structure of com-
munities obtained in word networks. even though texts
are written in a modular and hierarchical manner [48],
such a feature is hidden in traditional networked text
models, as it is the case of syntactical and word adja-
cency networks. in order to capture the topic structure
of a text, we devised three methods to link words ap-
pearing in the same semantical context, whose length is
established via parametrization. in preliminary experi-
ments, we have found that the modular organization is
optimized when one considers a context comprising 20
words. above this threshold, the gain in modularity was
found to be not signi   cant. we applied our methods in
a subset of articles from wikipedia with two granularity
levels. interestingly, in all considered datasets, at least
one of the proposed networked methods outperformed
traditional bag-of-word methods. as a proof of princi-
ple, we have shown that the information of network con-
nectivity hidden in texts might be used to unveil their
semantical organization, which in turn might be useful
to improve the applications relying on the accurate char-
acterization of textual semantics.

in future works, we intend to use the proposed char-
acterization to study related problems in text analysis.
the high-level representation could be employed in a
straightforwardly manner to recognize styles in texts, as
one expects that distinct writing styles may approach
di   erent subjects in a very particular way. as a con-
sequence, the proposed networked representations could
be used for identifying authors in disputed documents.
we also intend to extend our methods to make it suit-
able to analyze very large documents.
in this context,
an important issue concerns the accurate choice of val-
ues for the context length, which plays an important role
in the performance. our methods could also be com-

8

(a)

(b)

(c)

fig. 3. example of modularity evolution using the proposed text representations (ec, pb and apb). the results were obtained
in both cgs and fgs datasets (see section iii). note that the normalized modularity qn does not display a signi   cant increase
for    > 20.

(a) ns = 2 and np = 3

(b) ns = 2 and np = 4

(c) ns = 2 and np = 5

(d) ns = 3 and np = 3

(e) ns = 3 and np = 4

(f) ns = 3 and np = 5

(g) ns = 4 and np = 3

(h) ns = 4 and np = 4

(i) ns = 4 and np = 5

fig. 4. performance in segmenting subjects in texts with subtopics on the same major topic. the parameters employed to
generate the dataset (ns, the number of subtopics and np , the number of paragraphs per subtopic) are shown in the    gure.
note that, in most of the cases, the best performance is obtained with the apbm approach.

26101418222630343842numberofconnectedwords0.00.20.40.60.81.0modularityecqhqsiqn26101418222630343842numberofconnectedwords0.00.20.40.60.81.0modularitypbqhqsiqn26101418222630343842numberofconnectedwords0.00.20.40.60.81.0modularityapbqhqsiqnecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy rateecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy rateecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy rateecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy rateecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy rateecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy rateecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy rateecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy rateecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy rate9

(a) ns = 2 and np = 3

(b) ns = 2 and np = 4

(c) ns = 2 and np = 5

(d) ns = 3 and np = 3

(e) ns = 3 and np = 4

(f) ns = 3 and np = 5

(g) ns = 4 and np = 3

(h) ns = 4 and np = 4

(i) ns = 4 and np = 5

fig. 5. performance obtained in the fds dataset. the parameters employed to generate the dataset (ns, the number of
subtopics and np , the number of paragraphs per subtopic) are shown in the    gure. in most of the cases, the best performance
was obtained with the ec approach.

ecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy rateecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy rateecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy rateecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy rateecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy rateecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy rateecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy rateecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy rateecpbapbbow-kbow-em0.00.20.40.60.81.0accuracy ratebined with other traditional networked representation to
improve the characterization of related systems. thus, a
general framework could be created to study the prop-
erties of written texts in a multi-level way, in order to
capture the topological properties of networks formed of
simple (e.g. words) and more complex structures (e.g.
communities).

vi. acknowledgments

henrique ferraz de arruda thanks federal agency
for support and evaluation of graduate education
(capes-brazil) for    nancial support. diego raphael
amancio acknowledges s  o paulo research founda-
tion (fapesp) (grant no.
2014/20830-0) for    nan-
cial support. luciano da fontoura costa is grateful to
cnpq (grant no. 307333/2013-2), fapesp (grant no.
11/50761-2), and nap-prp-usp for sponsorship.

10

    diego@icmc.usp.br
[1] c. d. manning and h. sch  tze, foundations of statisti-
cal natural language processing (mit press, cambridge,
ma, usa, 1999).

[2] c. aggarwal and c. zhai, in mining text data (springer

[3] j. p. herrera and p. a. pury, the european physical

us, 2012) pp. 163   222.

journal b 63, 135 (2008).

[4] s. a. golder and m. w. macy, science 333, 1878 (2011).
[5] r. navigli, acm comput. surv. 41, 10:1 (2009).
[6] d. g. hern  ndez, d. h. zanette, and i. samengo, phys.

rev. e 92, 022813 (2015).

[7] d. r. amancio, o. n. oliveira jr., and l. d. f. costa,

journal of informetrics 6, 427 (2012).

[8] h. liu and c. xu, epl 93, 28005 (2011).
[9] a. delanoe and s. galam, physica a 402, 93 (2014).
[10] j. c. reynar, in proceedings of the 37th annual meet-
ing of the association for computational linguistics
on computational linguistics, acl    99 (association
for computational linguistics, stroudsburg, pa, usa,
1999) pp. 357   364.

[11] a. lancichinetti, m. i. sirer, j. wang, d. acuna, k. ko-
rding, and l. a. n. amaral, phys. rev. x 5, 011007
(2015).

[12] s. gong, y. qu, and s. tian, in third international joint
conference on computational science and optimization,
vol. 2 (2010) pp. 382   386.

[13] g. adomavicius, r. sankaranarayanan, s. sen,
a. tuzhilin, acm trans. inf. syst. 23, 103 (2005).

[14] c. clifton, r. cooley, and j. rennie, ieee transactions

on knowledge and data engineering 16, 949 (2004).

[15] d. m. blei, a. y. ng, and m. i. jordan, j. mach. learn.

and

res. 3, 993 (2003).

[16] d. r. amancio, plos one 10, e0136076 (2015).
[17] l. d. f. costa, o. n. oliveira jr., g. travieso, f. a.
rodrigues, p. r. v. boas, l. antiqueira, m. p. viana,
and l. e. c. rocha, advances in physics 60, 329 (2011).

[18] j. cong and h. liu, phys. life rev. 11, 598 (2014).
[19] d. r. amancio, e. g. altmann, o. n. oliveira jr., and

l. d. f. costa, new j. phys. 13, 123024 (2011).

[20] d. r. amancio, journal of statistical mechanics 2015,

p03005 (2015).

391, 2429 (2012).

[21] a. mehri, a. h. darooneh, and a. shariati, physica a

[22] d. r. amancio, s. m. aluisio, o. n. oliveira jr., and

l. d. f. costa, epl 100, 58002 (2012).

[23] a. p. masucci, a. kalampokis, v. m. egu  luz,

e. hern  ndez-garcia, plos one 6, e17333 (2011).

[24] a. p. masucci and g. j. rodgers, advances in complex

and

[25] r. f. cancho, r. v. sol  , and r. k  hler, phys. rev. e

systems 12, 113 (2009).

69, 051915 (2004).

[26] s. fortunato, physics reports 486, 75 (2010).
[27] r. f. cancho and r. v. sol  , proceedings of the royal

society of london b 268, 2261 (2001).

[28] d. r. amancio, l. antiqueira, t. a. s. pardo, l. d. f.
costa, o. n. oliveira jr., and m. g. v. nunes, interna-
tional journal of modern physics c 19, 583 (2008).

[29] d. r. amancio, scientometrics 105, 1763 (2015).
[30] m. a. montemurro and d. h. zanette, plos one 8,

e66344 (2013).

[31] d. r. amancio, e. g. altmann, d. rybski, o. n.
oliveira jr., and l. d. f. costa, plos one 8, e67310
(2013).

[32] g. a. miller, communications of the acm 38, 39 (1995).
[33] a. ratnaparkhi et al., in proceedings of the conference on
empirical methods in natural language processing, vol. 1
(philadelphia, usa, 1996) pp. 133   142.

[34] g. malecha and i. smith, unpublished course-related re-

port (2010).

[35] d. hindle, in proceedings of the 28th annual meeting on
association for computational linguistics (association
for computational linguistics, 1990) pp. 268   275.

[36] n. b  chet, j. chauch  , v. prince, and m. roche, com-

puter science and information systems , 133 (2014).

[37] j. v  ronis, computer speech & language 18, 223

(2004).

[38] j. martinez-romo, l. araujo, j. borge-holthoefer,
a. arenas, j. a. capit  n, and j. a. cuesta, physical
review e 84, 046108 (2011).
[39] en.wikipedia.org/wiki/car.
[40] a. clauset, m. e. newman,
review e 70, 066111 (2004).

and c. moore, physical

11

[41] a. arenas, l. danon, a. diaz-guilera, p. m. gleiser,
and r. guimera, the european physical journal b-
condensed matter and complex systems 38, 373 (2004).
[42] r. guimera, s. mossa, a. turtschi, and l. n. amaral,
proceedings of the national academy of sciences 102,
7794 (2005).

[43] g. palla, i. der  nyi, i. farkas, and t. vicsek, nature

435, 814 (2005).

[44] n. londhe, v. gopalakrishnan, a. zhang, h. q. ngo,

and r. srihari, proc. vldb endow. 7, 1167 (2014).

[45] v. d. blondel, j.-l. guillaume, r. lambiotte,

and
e. lefebvre, journal of statistical mechanics: theory
and experiment 2008, p10008 (2008).

[46] t. kanungo, d. m. mount, n. s. netanyahu, c. d. pi-
atko, r. silverman, and a. y. wu, ieee trans. pattern
anal. mach. intell. 24, 881 (2002).

[47] b. chai, c. jia, and j. yu, physica a 438, 454 (2015).
[48] e. alvarez-lacalle, b. dorow, j.-p. eckmann,
and
e. moses, proceedings of the national academy of sci-
ences 103, 7956 (2006).

