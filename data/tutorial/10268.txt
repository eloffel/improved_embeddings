6
1
0
2

 

b
e
f
9
1

 

 
 
]

g
l
.
s
c
[
 
 

1
v
5
2
0
6
0

.

2
0
6
1
:
v
i
x
r
a

journal of latex class files, vol. 6, no. 1, january 2007

1

spectral learning for supervised

topic models

yong ren   , yining wang   , jun zhu, member, ieee

abstract   supervised topic models simultaneously model the latent topic structure of large collections of documents and a
response variable associated with each document. existing id136 methods are based on variational approximation or monte
carlo sampling, which often suffers from the local minimum defect. id106 have been applied to learn unsupervised
topic models, such as id44 (lda), with provable guarantees. this paper investigates the possibility of
applying id106 to recover the parameters of supervised lda (slda). we    rst present a two-stage spectral method,
which recovers the parameters of lda followed by a power update method to recover the regression model parameters. then,
we further present a single-phase spectral algorithm to jointly recover the topic distribution matrix as well as the regression
weights. our spectral algorithms are provably correct and computationally ef   cient. we prove a sample complexity bound for
each algorithm and subsequently derive a suf   cient condition for the identi   ability of slda. thorough experiments on synthetic
and real-world datasets verify the theory and demonstrate the practical effectiveness of the spectral algorithms. in fact, our
results on a large-scale review rating dataset demonstrate that our single-phase spectral algorithm alone gets comparable
or even better performance than state-of-the-art methods, while previous work on id106 has rarely reported such
promising performance.

index terms   id106, supervised topic models, methods of moments

!

1 introduction

t opic modeling offers a suite of useful tools that

automatically learn the latent semantic structure
of a large collection of documents or images, with
id44 (lda) [11] as one of the
most popular examples. the vanilla lda is an unsu-
pervised model built on input contents of documents
or images. in many applications side information is
often available apart from raw contents, e.g., user-
provided rating scores of an online review text or
user-generated tags for an image. such side signal
usually provides additional information to reveal the
underlying structures of the data in study. there have
been extensive studies on developing topic models
that incorporate various side information, e.g., by
treating it as supervision. some representative models
are supervised lda (slda) [10] that captures a real-
valued regression response for each document, multi-
class slda [28] that learns with discrete classi   cation
responses, discriminative lda (disclda) [18] that
incorporates classi   cation response via discriminative
linear transformations on topic mixing vectors, and
medlda [32], [33] that employs a max-margin cri-

       y.r. and y.w. contributed equally.
    y. ren and j. zhu are with department of computer science
and technology; tnlist lab; state key laboratory for intelli-
gent technology and systems; center for bio-inspired comput-
ing research, tsinghua university, beijing, 100084 china. email:
reny11@mails.tsinghua.edu.cn; dcszj@tsinghua.edu.cn.
    y. wang is with machine learning department, carnegie mellon

university, pittsburgh, usa. email: yiningwa@andrew.cmu.edu

terion to learn discriminative latent topic representa-
tions for accurate prediction.

topic models are typically learned by    nding maxi-
mum likelihood estimates (id113) through local search
or sampling methods [15], [24], [25], which may get
trapped in local optima. much recent progress has
been made on developing spectral decomposition [1],
[3], [5] and nonnegative id105 (nmf)
[6], [7], [8], [9] methods to estimate the topic-word dis-
tributions. instead of    nding id113 estimates, which is
a known np-hard problem [8], these methods assume
that the documents are i.i.d. sampled from a topic
model, and attempt to recover the underlying model
parameters. compared to local search and sampling
algorithms, these methods enjoy the advantage of
being provably effective. in fact, sample complexity
bounds have been proved to show that given a suf   -
ciently large collection of documents, these algorithms
can recover the model parameters accurately with a
high id203.

recently, some attention has been paid on super-
vised topic models with nmf methods. for example,
nguyen et al. [23] present an extension of the anchor-
word methods [6] for lda to capture categorical
information in a supervised lda for sentiment clas-
si   cation. however, for id106, previous
work has mainly focused on unsupervised latent vari-
able models, leaving the broad family of supervised
models (e.g., slda) largely unexplored. the only
exception is [12] which presents a spectral method
for mixtures of regression models, quite different from
slda. such ignorance is not a coincidence as super-

journal of latex class files, vol. 6, no. 1, january 2007

2

vised models impose new technical challenges. for in-
stance, a direct application of previous techniques [1],
[3] on slda cannot handle regression models with
duplicate entries. in addition, the sample complexity
bound gets much worse if we try to match entries
in regression models with their corresponding topic
vectors.

in this paper, we extend the applicability of spec-
tral learning methods by presenting novel spectral
decomposition algorithms to recover the parameters
of slda models from low-order empirical moments
estimated from the data. we present two variants of
id106. the    rst algorithm is an extension
of the id106 for lda, with an extra power
update step of recovering the regression model in
slda, including the variance parameter. the power-
update step uses a newly designed empirical mo-
ment to recover regression model entries directly from
the data and reconstructed topic distributions. it is
free from making any constraints on the underlying
regression model. we provide a sample complexity
bound and analyze the identi   ability conditions. in
fact, the two-stage method does not increase the sam-
ple complexity much compared to that of the vanilla
lda.

however, the two-stage algorithm could have some
disadvantages because of its separation that the topic
distribution matrix is recovered in an unsupervised
manner without considering supervision and the re-
gression parameters are recovered by assuming a
   xed topic matrix. such an unwarranted separation
often leads to inferior performance compared to gibbs
sampling methods in practice (see section 7). to ad-
dress this problem, we further present a novel single-
phase spectral method for supervised topic models,
which jointly recovers all the model parameters (ex-
cept the noise variance) by doing a single-step of
robust tensor decomposition of a newly designed
empirical moment that takes both input data and
supervision signal into consideration. therefore, the
joint method can use supervision information in re-
covering both the topic distribution matrix and regres-
sion parameters. the joint method is also provably
correct and we provide a sample complexity bound
to achieve the   error rate in a high id203.

finally, we provide thorough experiments on both
synthetic and real datasets to demonstrate the practi-
cal effectiveness of our id106. for the two-
stage method, by combining with a id150
procedure, we show superior performance in terms of
id38, prediction accuracy and running
time compared to traditional
id136 algorithms.
furthermore, we demonstrate that on a large-scale
review rating dataset our single-phase method alone
can achieve comparable or even better results than
the state-of-the-art methods (e.g., slda with gibbs
sampling and medlda). such promising results are
signi   cant to the literature of id106, which

were often observed to be inferior to the id113-based
methods; and a common heuristic was to use the
outputs of a spectral method to initialize an em
algorithm, which sometimes improves the perfor-
mance [31].

the highly non-convex property of

the rest of the paper is organized as follows. section
2 reviews basics of supervised topic models. section 3
introduces the background knowledge and notations
for slda and high-order tensor decomposition. sec-
tion 4 presents the two-stage spectral method, with a
rigorous theoretical analysis, and section 5 presents
the joint spectral method together with a sample
complexity bound. section 6 presents some implemen-
tation details to scale up the computation. section 7
presents experimental results on both synthetic and
real datasets. finally, we conclude in section 8.
2 related work
based on different principles, there are various meth-
ods to learn supervised topic models. the most nat-
ural one is maximum-likelihood estimation (id113).
however,
the
learning objective makes the optimization problem
very hard. in the original paper [10], where the id113
is used, the authors choose variational approximation
to handle intractable posterior expectations. such a
method tries to maximize a lower bound which is
built on some variational distributions and a mean-
   eld assumption is usually imposed for tractability.
although the method works well
in practice, we
do not have any guarantee that the distribution we
learned is close to the true one. under bayesian frame-
work, id150 is an attractive method which
enjoys the property that the stationary distribution of
the chain is the target posterior distribution. however,
this does not mean that we can really get accurate
samples from posterior distribution in practice. the
slow mixing rate often makes the sampler trapped in a
local minimum which is far from the true distribution
if we only run a    nite number of iterations.

max-margin learning is another principle on learn-
ing supervised topic models, with maximum id178
discrimination lda (medlda) [32] as a popular ex-
ample. medlda explores the max-margin principle
to learn sparse and discriminative topic representa-
tions. the learning problem can be de   ned under the
regularized bayesian id136 (regbayes) [34] frame-
work, where the max-margin posterior id173
is introduced to ensure that the topic representations
are good at predicting response variables. though
both carefully designed variational id136 [32] and
id150 methods [33] are given, we still can-
not guarantee the quality of the learnt model
in
general.

recently, increasing efforts have been made to re-
cover the parameters directly with provable correct-
ness for topic models, with the main focus on unsu-
pervised models such as lda. such methods adopt

journal of latex class files, vol. 6, no. 1, january 2007

3

either nmf or spectral decomposition approaches.
for nmf, the basic idea is that even nmf is an np-
hard problem in general, the topic distribution matrix
can be recovered under some separable condition,
e.g., each topic has at least one anchor word. precisely,
for each topic, the method    rst    nds an anchor word
that has non-zero id203 only in that topic. then a
recovery step reconstructs the topic distribution given
such anchor words and a second-order moment ma-
trix of word-word co-occurrence [8], [6]. the original
reconstruction step only needs a part of the matrix
which is not robust in practice. thus in [6], the author
recovers the topic distribution based on a probabilistic
framework. the nmf methods produce good empiri-
cal results on real-world data. recently, the work [23]
extends the anchor word methods to handle super-
vised topic models. the method augments the word
co-occurrence matrix with additional dimensions for
metadata such as sentiment, and shows better perfor-
mance in sentiment classi   cation.

id106 start from computing some low-
order moments based on the samples and then relate
them with the model parameters. for lda, tensors
up to three order are suf   cient to recover its pa-
rameters [1]. after centralization, the moments can
be expressed as a mixture of the parameters we
are interested in. after that whitening and robust
tensor decomposition steps are adopted to recover
model parameters. the whitening step makes that
the third-order tensor can be decomposed as a set
of orthogonal eigenvectors and their corresponding
eigenvalues after some operations based on its output
and the robust tensor decomposition step then    nds
them. previous work only focuses on unsupervised
lda models and we aim to extend the ability for
id106 to handle response variables. finally,
some preliminary results of the two-stage recovery
algorithm have been reported in [29]. this paper
presents a systematical analysis with a novel one-
stage spectral method, which yields promising results
on a large-scale dataset.

3 preliminaries
we    rst overview the basics of slda, orthogonal
tensor decomposition and the notations to be used.

3.1 supervised lda
id44 (lda) [11] is a hierarchical
generative model for id96 of text docu-
ments or images represented in a bag-of-visual-words
format [20]. it assumes k different topics with topic-
word distributions   1,       ,   k        v    1, where v is
the vocabulary size and    v    1 denotes the id203
simplex of a v -dimensional random vector. for a
document, lda models a topic mixing vector h    
   k   1 as a id203 distribution over the k topics. a
conjugate dirichlet prior with parameter    is imposed

  

  

h

y

  

z

x

m

y =   (cid:62)h +  

n

fig. 1. a graphical illustration of supervised lda. see
text for details.

on the topic mixing vectors. a bag-of-words model is
then adopted, which    rst generates a topic indicator
for each word zj     multi(h) and then generates the
word itself as wj     multi(  zj ). supervised latent
dirichlet allocation (slda) [10] incorporates an extra
response variable y     r for each document. the
response variable is modeled by a id75
model        rk on either the topic mixing vector h
or the averaging topic assignment vector   z, where
  zi = 1
j=1 1[zj =i] with m being the number of
m
words in the document and 1[  ] being the indicator
function (i.e., equals to 1 if the predicate holds; oth-
erwise 0). the noise is assumed to be gaussian with
zero mean and   2 variance.

(cid:80)m

fig. 1 shows the graph structure of the slda model
using h for regression. although previous work has
mainly focused on the model using averaging topic
assignment vector   z, which is convenient for col-
lapsed id150 and variational id136 with
h integrated out due to the conjugacy between a
dirichlet prior and a multinomial likelihood, we con-
sider using the topic mixing vector h as the features
for regression because it will considerably simplify
our spectral algorithm and analysis. one may assume
that whenever a document is not too short, the empir-
ical distribution of its word topic assignments should
be close to the document   s topic mixing vector. such
a scheme was adopted to learn sparse topic coding
models [35], and has demonstrated promising results
in practice. our results also prove that this is an
effective strategy.
3.2 high-order tensor product and orthogonal
tensor decomposition
here we brie   y introduce something about tensors,
which mostly follow the same as in [3]. a real p-
rni belongs to the tensor
product of euclidean spaces rni. without loss of
generality, we assume n1 = n2 =        = np = n. we can
identify each coordinate of a by a p-tuple (i1,       , ip),
where i1,       , ip     [n]. for instance, a p-th order tensor
is a vector when p = 1 and a matrix when p = 2. we
can also consider a p-th order tensor a as a multilinear
the mapping a(x1,       , xp) is a p-th or-
rn  m,
(cid:44)
j1,       ,jp   [n] aj1,       ,jp [x1]j1,i1[x2]j2,i2        [xp]jp,ip. con-
sider some concrete examples of such a multilinear

mapping. for a    (cid:78)p rn and matrices x1,       , xp    
der tensor in (cid:78)p rm, with [a(x1,       , xp)]i1,       ,ip
(cid:80)

th order tensor a     (cid:78)p

i=1

journal of latex class files, vol. 6, no. 1, january 2007

4

i=1 such that a =(cid:80)k

mapping. when a, x1 and x2 are matrices, we have
a(x1, x2) = x(cid:62)
1 ax2. similarly, when a is a matrix
and x is a vector, we have a(i, x) = ax.

a     (cid:78)p rn is a collection of orthonormal vectors

an orthogonal tensor decomposition of a tensor
   p
{vi}k
i=1 and scalars {  i}k
,
i=1   iv
where we use v   p (cid:44) v     v              v to denote the p-th
i
order tensor generated by a vector v. without loss of
generality, we assume   i are nonnegative when p is
odd since we can change the sign of vi otherwise. al-
though orthogonal tensor decomposition in the matrix
case can be done ef   ciently by singular value decom-
position (svd), it has several delicate issues in higher
order tensor spaces [3]. for instance, tensors may
not have unique decompositions, and an orthogonal
decomposition may not exist for every symmetric ten-
sor [3]. such issues are further complicated when only
noisy estimates of the desired tensors are available.
for these reasons, we need more advanced techniques
to handle high-order tensors. in this paper, we will
apply robust tensor power methods [3] to recover robust
eigenvalues and eigenvectors of an (estimated) third-
order tensor. the algorithm recovers eigenvalues and
eigenvectors up to an absolute error   , while running
in polynomial time w.r.t the tensor dimension and
log(1/  ). further details and analysis of the robust
tensor power method are in appendix a.2 and [3].

3.3 notations

we use (cid:107)v(cid:107) =(cid:112)(cid:80)

i v2

i,j m 2

(cid:113)(cid:80)

i to denote the euclidean norm
of vector v, (cid:107)m(cid:107) to denote the spectral norm of
matrix m, (cid:107)t(cid:107) to denote the operator norm of a high-
order tensor, and (cid:107)m(cid:107)f =
ij to denote the
frobenious norm of m. we use an one-hot vector
x     rv to represent a word in a document (i.e., for
the i-th word in a vocabulary, only xi = 1 all other
elements are 0). in the two-stage spectral method,
we use o (cid:44) (  1,   2,       ,   k)     rv   k to denote the

topic distribution matrix, and (cid:101)o (cid:44) ((cid:101)  1,(cid:101)  2,       ,(cid:101)  k)
to denote the canonical version of o, where (cid:101)  i =
(cid:113)   i
  0(  0+1)    with   0 (cid:44) (cid:80)k

i=1   i. for the joint spec-
tral method, we combine the topic distribution   i
with its regression parameter to form a joint topic
distribution vector vi (cid:44) [  (cid:48)
i,   i](cid:48). we use notation
o    (cid:44) (v1, v2, ..., vk)     r(v +1)  k to denote the joint

topic distribution matrix and (cid:101)o    (cid:44) [(cid:101)v1,(cid:101)v2, ...,(cid:101)vk] to
denote its canonical version where (cid:101)vi (cid:44)(cid:113)   i

  0(  0+1) vi.

4 a two-stage spectral method
we    rst present a two-stage spectral method to re-
cover the parameters of slda. the algorithm consists
of two key components   an orthogonal tensor decom-
position of observable moments to recover the topic
distribution matrix o and a power update method
to recover the regression model   . we present these
techniques and a rigorous theoretical analysis below.

4.1 moments of observable variables
our spectral decomposition methods recover the topic
distribution matrix o and the id75 model
   by manipulating moments of observable variables.
in de   nition 1, we de   ne a list of moments on random
variables from the underlying slda model.
de   nition 1. we de   ne the following moments of observ-
able variables:
m1 = e[x1], m2 = e[x1     x2]       0
m3 = e[x1     x2     x3]       0

m1     m1,
(e[x1     x2     m1]

  0 + 1

(1)

+e[x1     m1     x2] + e[m1     x1     x2])

  0 + 2

2  2
0

+

(  0 + 1)(  0 + 2)
my = e[yx1     x2]       0
  0 + 2
+e[yx1]     e[x2]) +

m1     m1     m1,

(2)
(e[y]e[x1     x2] + e[x1]     e[yx2]
e[y]m1     m1. (3)

2  2
0

(  0 + 1)(  0 + 2)

note that the moments m1, m2 and m3 were also
de   ned in [1], [3] for recovering the parameters of
lda models. for slda, we need to de   ne a new
moment my in order to recover the id75
model   . the moments are based on observable vari-
ables in the sense that they can be estimated from
i.i.d. sampled documents. for instance, m1 can be
estimated by computing the empirical distribution of
all words, and m2 can be estimated using m1 and
word co-occurrence frequencies. though the moments
in the above forms look complicated, we can apply
elementary calculations based on the conditional inde-
pendence structure of slda to signi   cantly simplify
them and more importantly to get them connected
with the model parameters to be recovered, as sum-
marized in proposition 1, whose proof is elementary
and deferred to appendix c for clarity.
proposition 1. the moments can be expressed using the
model parameters as:

k(cid:88)

m2 =

m3 =

my =

1

  i  i       i,

  0(  0 + 1)

i=1

2

  0(  0 + 1)(  0 + 2)

2

  0(  0 + 1)(  0 + 2)

i=1

k(cid:88)
k(cid:88)

i=1

  i  i       i       i,

  i  i  i       i.

(4)

(5)

(6)

4.2 simultaneous diagonalization
proposition 1 shows that the moments in de   nition
1 are all the weighted sums of tensor products of
{  i}k
i=1 from the underlying slda model. one idea
to reconstruct {  i}k
i=1 is to perform simultaneous di-
agonalization on tensors of different orders. the idea
has been used in a number of recent developments
of id106 for latent variable models [1],
[3], [12]. speci   cally, we    rst whiten the second-order
tensor m2 by    nding a matrix w     rv   k such that

journal of latex class files, vol. 6, no. 1, january 2007

5

algorithm 1 spectral parameter recovery algorithm
for slda. input parameters:   0, l, t .

1: compute empirical moments and obtain (cid:99)m2,(cid:99)m3
and (cid:99)my.
2: find (cid:99)w     rn  k such that (cid:99)m2((cid:99)w ,(cid:99)w ) = ik.
3: find robust eigenvalues and eigenvectors ((cid:98)  i,(cid:98)vi)
of (cid:99)m3((cid:99)w ,(cid:99)w ,(cid:99)w ) using the robust tensor power
4: recover prior parameters: (cid:98)  i     4  0(  0+1)
(  0+2)2(cid:98)  2
(cid:98)  i((cid:99)w +)(cid:62)(cid:98)vi.
(cid:98)  i       0 + 2
i (cid:99)my((cid:99)w ,(cid:99)w )(cid:98)vi.
(cid:98)v(cid:62)
(cid:98)  i       0 + 2
7: output: (cid:98)  , (cid:98)   and {(cid:98)  i}k

6: recover the id75 model:

method [3] with parameters l and t .

5: recover topic distributions:

i=1.

2

2

.

i

w (cid:62)m2w = ik. this whitening procedure is possible
whenever the topic distribuction vectors {  i}k
i=1 are
linearly independent (and hence m2 has rank k). this
is not always correct since in the    overcomplete   
case [4], it is possible that the topic number k is
larger than vocabulary size v . however, the linear
independent assumption gives us a more compact
representation for the topic model and works well in
practice. hence we simply assume that the whiten-
ing procedure is possible. the whitening procedure
and the linear independence assumption also imply
that {w   i}k
i=1 are orthogonal vectors (see appendix
a.2 for details), and can be subsequently recovered
by performing an orthogonal tensor decomposition
on the simultaneously whitened third-order tensor
m3(w, w, w ). finally, by multiplying the pseudo-
inverse of the whitening matrix w + we obtain the
topic distribution vectors {  i}k
it should be noted that jennrich   s algorithm [17],
[19], [22] could recover {  i}k
i=1 directly from the 3-
rd order tensor m3 alone when {  i}k
i=1 is linearly
independent. however, we still adopt the above si-
multaneous diagonalization framework because the
intermediate vectors {w   i}k
i=1 play a vital role in the
recovery procedure of the id75 model   .

i=1.

4.3 the power update method
although the id75 model    can be recov-
ered in a similar manner by performing simultaneous
diagonalization on m2 and my, such a method has
several disadvantages, thereby calling for novel so-
lutions. first, after obtaining entry values {  i}k
i=1 we
need to match them to the topic distributions {  i}k
i=1
previously recovered. this can be easily done when
we have access to the true moments, but becomes
dif   cult when only estimates of observable tensors
are available because the estimated moments may not
share the same singular vectors due to sampling noise.
a more serious problem is that when    has duplicate

entries the orthogonal decomposition of my is no
longer unique. though a randomized strategy similar
to the one used in [1] might solve the problem, it could
substantially increase the sample complexity [3] and
render the algorithm impractical.

in [5], the authors provide a method for the match-
ing problem by reusing eigenvectors. we here de-
velop a power update method to resolve the above
dif   culties with a similar spirit. speci   cally, after ob-
taining the whitened (orthonormal) vectors {vi} (cid:44)
ci    w (cid:62)  i
1 we recover the entry   i of the linear re-
gression model directly by computing a power update
v(cid:62)
i my(w, w )vi. in this way, the matching problem
is automatically solved because we know what topic
distribution vector   i is used when recovering   i. fur-
thermore, the singular values (corresponding to the
entries of   ) do not need to be distinct because we are
not using any unique svd properties of my(w, w ).
as a result, our proposed algorithm works for any
linear model   .

4.4 parameter recovery algorithm
alg. 1 outlines our parameter recovery algorithm for
slda (spectral-slda). first, empirical estimations of
the observable moments in de   nition 1 are computed
from the given documents. the simultaneous diag-
onalization method is then used to reconstruct the
topic distribution matrix o and its prior parameter   .
after obtaining o = (  1,       ,   k), we use the power
update method introduced in the previous section to
recover the id75 model   . we can also
recover the noise level parameter   2 with the other
parameters in hand by estimating e[y] and e[y2] since
e[y] = e[e[y|h]] =   (cid:62)   and e[y2] = e[e[y2|h]] =
  (cid:62)e[h     h]   +   2, where the term e[h     h] can
be computed in an analytical form using the model
parameters, as detailed in appendix c.1.

alg. 1 admits three hyper-parameters   0, l and t .
  0 is de   ned as the sum of all entries in the prior
parameter   . following the conventions in [1], [3],
we assume that   0 is known a priori and use this
value to perform parameter estimation. it should be
noted that this is a mild assumption, as in practice
usually a homogeneous vector    is assumed and the
entire vector is known [27]. the l and t param-
eters are used to control the number of iterations
in the robust tensor power method. in general, the
robust tensor power method runs in o(k3lt ) time.
to ensure suf   cient recovery accuracy, l should be
at least a linear function of k and t should be set
as t =    (log(k) + log log(  max/  )), where   max =
and    is an error tolerance parameter.
  0+2
appendix a.2 and [3] provide a deeper analysis into
the choice of l and t parameters.

(cid:113)   0(  0+1)
1. ci (cid:44)(cid:113)   i

  0(  0+1) is a scalar coef   cient that depends on   0 and

  min

2

  i. see appendix a.2 for details.

journal of latex class files, vol. 6, no. 1, january 2007

6

4.5 sample complexity analysis
we now analyze the sample complexity of alg. 1 in
order to achieve   -error with a high id203. for
clarity, we focus on presenting the main results, while
deferring the proof details to appendix a, including
the proofs of important lemmas that are needed for
the main theorem.

theorem 1. let   1((cid:101)o) and   k((cid:101)o) be the largest and
tribution matrix (cid:101)o. de   ne   min (cid:44) 2
and the smallest entries of   . suppose (cid:98)  , (cid:98)   and (cid:98)   are

the smallest singular values of the canonical topic dis-
and
? with   max and   min the largest

(cid:113)   0(  0+1)

(cid:113)   0(  0+1)

the outputs of algorithm 1, and l is at least a linear
function of k. fix        (0, 1). for any small error-
tolerance parameter    > 0, if algorithm 1 is run with
parameter t =    (log(k) + log log(  max/  )) on n i.i.d.
sampled documents (each containing at least 3 words) with
n     max(n1, n2, n3), where

  max (cid:44) 2

  0+2

  0+2

  max

  min

(cid:17)2      2
n1 = c1   (cid:16)
1 +(cid:112)log(6/  )
n2 = c2    (1 +(cid:112)log(15/  ))2
  2  k((cid:101)o)4
(cid:32)(cid:18)
(cid:19)(cid:19)2
(cid:18)   
n3 = c3    (1 +(cid:112)log(9/  ))2
  k((cid:101)o)10

60
   max

(cid:107)  (cid:107)         

   max

   1

0(  0 + 1)2

,

  min

(cid:33)

,

,   2

max  1((cid:101)o)2
(cid:18) 1
(cid:19)

k2
  2

min

  2 ,

,

and c1, c2 and c3 are universal constants, then with
id203 at least 1       , there exists a permutation    :
[k]     [k] such that for every topic i, the following holds:
|  i    (cid:98)    (i)|    4  0(  0 +1)(  max +5  )
(cid:18)
(cid:18) 8  max
min(  min   5  )2    5  , if   min > 5  
3  1((cid:101)o)
(cid:107)  i    (cid:98)    (i)(cid:107)   
(cid:19)
(cid:18) (cid:107)  (cid:107)
|  i    (cid:98)    (i)|   

(  0 +2)2  2

+ (  0 + 2)

5(  0 + 2)

(cid:19)

(cid:19)

  min

+ 1

+

  .

2

  

  min

in brevity, the proof is based on matrix perturba-
tion lemmas (see appendix a.1) and analysis to the
orthogonal tensor decomposition methods (including
svd and robust tensor power method) performed
on inaccurate tensor estimations (see appendix a.2).
the sample complexity lower bound consists of three
terms, from n1 to n3. the n3 term comes from the
sample complexity bound for the robust tensor power
method [3]; the ((cid:107)  (cid:107)          1(  /60))2 term in n2 charac-
terizes the recovery accuracy for the id75
model   , and the   2
to recover the topic distribution vectors   ;    nally, the
term n1 is required so that some technical conditions
are met. the n1 term does not depend on either k or

max  1((cid:101)o)2 term arises when we try
  k((cid:101)o), and could be largely neglected in practice.

remark 1. an important implication of theorem 1 is
that it provides a suf   cient condition for a supervised
lda model to be identi   able, as shown in remark 2. to
some extent, remark 2 is the best identi   ability result
possible under our id136 framework, because it makes no

restriction on the id75 model   , and the linear
independence assumption is unavoidable without making
further assumptions on the topic distribution matrix o.

i.i.d.
remark 2. given a suf   ciently large number of
sampled documents with at least 3 words per document,
a supervised lda model m = (  ,   ,   ) is identi   able
i=1 are linearly

if   0 = (cid:80)k

is known and {  i}k

i=1   i

independent.

we now take a close look at the sample complexity
bound in theorem 1. it is evident that n2 can be
neglected when the number of topics k gets large,
because in practice the norm of the id75
model    is usually assumed to be small in order to
avoid over   tting. moreover, as mentioned before, the
prior parameter    is often assumed to be homoge-
neous with   i = 1/k [27]. with these observations,
the sample complexity bound in theorem 1 can be
greatly simpli   ed.
remark 3. assume (cid:107)  (cid:107) and    are small and    =
(1/k,       , 1/k). as the number of topics k gets large, the
sample complexity bound in theorem 1 can be simpli   ed
as

(cid:33)

(cid:32)

n        

   max(     2, k3)

.

(7)

log(1/  )

  k((cid:101)o)10

look formidable as it depends on   k((cid:101)o)10. however,

the sample complexity bound in remark 3 may

such dependency is somewhat necessary because we
are using third-order tensors to recover the underly-
ing model parameters.

5 joint parameter recovery

the above two-stage procedure has one possible dis-
advantages, that is, the recovery of the topic distribu-
tion matrix does not use any supervision signal, and
thus the recovered topics are often not good enough
for prediction tasks, as shown in experiments. the
disadvantage motivates us to develop a joint spectral
method with theoretical guarantees. we now describe
our single-phase algorithm.

5.1 moments of observable variables

we    rst de   ne some moments based on the observ-
able variables including the information that we need
to recover the model parameters. since we aim to
recover the joint topic distribution matrix o   , we
combine the word vector x with the response variable
y to form a joint vector z = [x(cid:48), y](cid:48) and de   ne the
following moments:

journal of latex class files, vol. 6, no. 1, january 2007

7

interaction between response variable y and

fig. 2.
word vector x in the 3rd-order tensor m3.

de   nition 2. (centerized moments)

n1 = e[z1]
n2 = e[z1     z2]       0
n3 = e[z1     z2     z3]       0

  0 + 1

n1     n1       2e     e

(e[z1     z2     n1]

+ e[z1     n1     z2] + e[n1     z1     z2])

  0 + 1

2  2
0

n1     n1     n1

(  0 + 1)(  0 + 2)

+
+ 3  2n1v +1 e     e     e +
1
+ e     n1     e + n1     e     e),

  0 + 1

  2(e     e     n1

(8)

where e is the (v +1)-dimensional vector with the last
element equaling to 1 and all others zero, n1v +1 is the
v + 1-th element of n1.

e[y|h] = (cid:80)k
for x (i.e., e[x|h] = (cid:80)k

the intuition for such de   nitions is derived from
an important observation that once the latent vari-
able h is given, the mean value of y is a weighted
combination of the regression parameters and h (i.e.,
i=1   ihi), which has the same form as
i=1   ihi). therefore, it is nat-
ural to regard y as an additional dimension of the
word vector x, which gives the new vector z. this
combination leads to some other terms involving the
high-order moments of y, which introduce the vari-
ance parameter    when we centerize the moments.
although we can recover    in the two-stage method,
recovering it jointly with the other parameters seems
to be hard. thus we treat    as a hyper-parameter. one
can determine it via a cross-validation procedure.

(cid:44) e[x1     x2     x3], n(cid:48)

as illustrated in fig. 2, our 3rd-order moment can
be viewed as a centerized version of the combination
(cid:44) e[yx2     x2] and
of n(cid:48)
some high-order statistics of the response variables.
note that this combination has already aligned the
regression parameters with the corresponding topics.
hence, we do not need an extra matching step.

in practice, we cannot get the exact values of those
moments. instead, we estimate them from the i.i.d.
sampled documents. note that we only need the
moments up to the third order, which means any
document consisting of at least three words can be
used in this estimation. furthermore, although these
moments seem to be complex, they can be expressed
via the model parameters in a graceful manner, as
summarized in proposition 2 which can be proved by
expanding the terms by de   nition, similar as in the
proof of proposition 1.

y

3

algorithm 2 a joint spectral method to recover slda
parameters. input parameters:   0, l, t ,   

1: compute empirical moments and obtain (cid:98)n2,(cid:99)n3.
2: find (cid:99)w     rv +1  k such that (cid:98)n2((cid:99)w ,(cid:99)w ) = ik.
3: find robust eigenvalues and eigenvectors ((cid:98)  i,(cid:98)vi)
of (cid:98)n3((cid:99)w ,(cid:99)w ,(cid:99)w ) using the robust tensor power
4: recover prior parameters: (cid:98)  i     4  0(  0+1)
(  0+2)2(cid:98)  2
(cid:98)  i(w +)(cid:62)(cid:98)  i.

6: output: model parameters (cid:98)  i, (cid:98)vi i = 1, ..., k

(cid:98)vi       0 + 2

method with parameters l and t .

5: recover topic distribution:

2

.

i

proposition 2. the moments in de   nition 2 can be
expressed by using the model parameters as follows:

k(cid:88)

n2 =

n3 =

1

  ivi     vi

  0(  0 + 1)

i=1

2

  0(  0 + 1)(  0 + 2)

k(cid:88)

i=1

(9)

  ivi     vi     vi,

where vi is the concatenation of the i-th word-topic distri-
bution   i and regression parameter   i.

5.2 robust tensor decomposition
proposition 2 shows that the centerized tensors are
weighted sums of the tensor products of the param-
eters {vi}k
i=1 to be recovered. a similar procedure as
in the two-stage method can be followed in order
to develop our joint spectral method, which consists
of whitening
and robust tensor decomposition steps.
first, we whiten the 2nd-order tensor n2 by    nding
a matrix w     r(v +1)  k such that w (cid:62)n2w = ik. this
whitening procedure is possible whenever the joint
topic distribution vectors {vi}k
i=1 are linearly indepen-
dent, that is, the matrix has rank k. the whitening
procedure and the linear independence assumption
also imply that {w (cid:62)vi}k
i=1 are orthogonal vectors
and can be subsequently recovered by performing
an orthogonal tensor decomposition on the simulta-
neously whitened third-order tensor n3(w, w, w ) as
summarized in the following proposition.

proposition 3. de   ne   i =

w (cid:62)vi. then:

  0(  0 + 1)

    {  }k
    n3(w, w, w ) has pairs of robust eigenvalue and

i=1 is an orthonormal basis.

(cid:114)   0(  0 + 1)

eigenvector (  i, vi) with    =

2

  0 + 2

  i

finally, by multiplying the pseudo-inverse of the
whitening matrix w + we obtain the joint topic distri-
bution vectors {vi}k

we outline our single-phase spectral method in
alg. 2. here we assume that the noise variance    is
given. note that in the two-stage spectral method, it

i=1.

(cid:114)   i

journal of latex class files, vol. 6, no. 1, january 2007

8

does not need the parameter    because it does not use
the information of the variance of prediction error.
although there is a disadvantage that we need to
tune it, the introduction of    sometimes increases the
   exibility of our methods on incorporating some prior
knowledge (if exists).

we additionally need three hyper-parameters   0, l
and t , similar as in the two-stage method. the pa-
rameter   0 is de   ned as the summation of all the
entries of the prior parameter   . l and t are used
to control the number of iterations in robust tensor
decomposition. to ensure a suf   ciently high recovery
accuracy, l should be at least a linear function of k,
and t should be set as t =    (log(k)+log log(  max/ )),
where   max = 2
and   is the error rate.

(cid:113)   0(  0+1)

  0+2

  min

(cid:113)   0(  0+1)

5.3 sample complexity analysis
we now analyze the sample complexity in order to
achieve  -error with a high id203. for clarity, we
defer proof details to appendix b.

theorem 2. let   1((cid:101)o   ) and   k((cid:101)o   ) be the largest
(cid:113)   0(  0+1)
distribution matrix (cid:101)o   . let   max (cid:44)

and smallest singular values of the joint canonical topic

where   min is the smallest element of   ;   min (cid:44)
where   max is the largest element of   .
2
for any error-tolerance parameter   > 0, if algorithm 2
runs at least t =    (log(k)+log log(  max/ )) iterations on
n i.i.d. sampled documents with n     (n(cid:48)
3), where:
1 = k1      2
(cid:48)
n
(cid:48)
n

0(  0 + 1)2c 2(  /36n )    (2 +(cid:112)2 log(18/  ))2

2 = k2    c 2(  /144n )(2 +(cid:112)2 log(72/  ))2
3 = k3    c 2(  /36n )(2 +(cid:112)2 log(18/  ))2

1, n(cid:48)

2, n(cid:48)

  0+2

  0+2

  max

  min

(cid:48)
n

  2

min

2

   max(

1
 2 ,

k2
  2

min

),

  k((cid:101)o   )10

c(x) is a polynomial of inverse cdf of normal distribution
and the norm of regression parameters (cid:107)  (cid:107); k1, k2, k3 are
some universal constants. then with id203 at least
1      , there exist a permutation    : [k]     [k] such that the
following holds for every i     [k]:

|  i    (cid:98)    (i)|    
(cid:107)vi    (cid:98)v  (i)(cid:107)    

(  0 + 2)2  2

4  0(  0 + 1)

(cid:18)
  1((cid:101)o   )(  0 + 2)(

(cid:19)
min(  min     5 )2    5 

+

8

)

    .

7
2

  min

similar to theorem 1, the sample complexity bound
consists of three terms. the    rst and second terms do
not depend on the error rate  , which are required
so that some technical conditions are met. thus they
could be largely neglected in practice. the third n(cid:48)
3
term comes from the sample complexity bound for
the robust tensor power method [3].
remark 4. note the rhs of the requirements of n
includes a function of n (i.e., c(1/n )). as mentioned
above, c(x) is polynomial of
inverse cdf of normal
distribution with low degree. since the inverse cdf grows

(cid:32)

very slowly (i.e., |     1(1/n )| = o(log(n ))). we can omit
it safely.
remark 5. following the above remark and assume that
(cid:107)  (cid:107) and    are small and    are homogeneous, the sample
complexity can be simpli   ed as (a function of k):

   max(

1
 2 , k3)

.

n = o

the factor 1/  k((cid:101)o   )10 is large, however, such a fac-
better methods to decompose (cid:98)n3.

tor is necessary since we use the third order tensors.
this factor roots in the tensor decomposition methods
and one can expect to improve it if we have other

log(1/  )

  k((cid:101)o   )10

(cid:33)

5.4 sample complexity comparison
as mentioned in remark 3 and remark 5, the joint
spectral method shares the same sample complexity as
the two-stage algorithm in order to achieve   accuracy,
except two minor differences.

est singular value of (joint) topic distribution   k((cid:101)o).

first, the sample complexity depends on the small-

for the joint method, the joint topic distribution ma-
trix consists of the original topic distribution matrix
and one extra row of the regression parameters. thus
from weyl   s inequality [30], the smallest singular
value of the joint topic distribution matrix is larger
than that of the original topic distribution matrix, and
then the sample complexity of the joint method is
a bit lower than that of the two-stage method, as
empirically justi   ed in experiments.

second, different from the two-stage method, the er-
rors of topic distribution    and regression parameters
   are estimated together in the joint method (i.e., v),
which can potentially give more accurate estimation
of regression parameters considering that the number
of regression parameters is much less than the topic
distribution.

6 speeding up moment computation
we now analyze the computational complexity and
present some implementation details to make the
algorithms more ef   cient.

6.1 two-stage method
in alg. 1, a straightforward computation of the third-

order tensor (cid:99)m3 requires o(n m 3) time and o(v 3)

storage, where n is corpus size, m is the number
of words per document and v is the vocabulary
size. such time and space complexities are clearly
prohibitive for real applications, where the vocab-
ulary usually contains tens of thousands of terms.
however, we can employ a trick similar as in [13]
to speed up the moment computation. we    rst note

that only the whitened tensor(cid:99)m3((cid:99)w ,(cid:99)w ,(cid:99)w ) is needed

in our algorithm, which only takes o(k3) storage.
another observation is that the most dif   cult term

journal of latex class files, vol. 6, no. 1, january 2007

in (cid:99)m3 can be written as (cid:80)r
i=1 ciui,1     ui,2     ui,3,
compute (cid:99)m3((cid:99)w ,(cid:99)w ,(cid:99)w ) in o(n m k) time by com-
where r is proportional
to n and ui,   contains
puting(cid:80)r
at most m non-zero entries. this allows us to
i=1 ci(w (cid:62)ui,1)     (w (cid:62)ui,2)     (w (cid:62)ui,3). ap-
pendix c.2 provides more details about this speed-up
trick. the overall time complexity is o(n m (m +k2)+
v 2 + k3lt ) and the space complexity is o(v 2 + k3).

6.2 joint method
for the single-phase algorithm, a straightforward

complexity of o(n m 3) as in the two-stage method.
and a much higher time complexity is needed for

computation of the third-order tensor (cid:98)n3 has the same
computing (cid:98)n3((cid:99)w ,(cid:99)w ,(cid:99)w ), which is prohibitive. sim-
(cid:98)n3((cid:99)w ,(cid:99)w ,(cid:99)w ) in alg. 2, we turn to compute this term

ilar as in the two-stage method, since we only need

directly. we can then use the trick mentioned above to
do this. the key idea is to decompose the third-order
tensor into different parts based on the occurrence
of words and compute them respectively. the same
time coid113xity and space complexity is needed for
the single-phase method.

sometimes n2 and n3 are not    balanced    (i.e.,
the value of some elements are much larger than
the others). this situation happens when either the
vocabulary size is too large or the range of    is too
large. one can image that if we have a vocabulary
consisting of one million words while min   i = 1,
then the energy of the matrix n2 concentrates on
(n2)v +1,v +1. as a consequence, the svd performs
badly when the matrix is ill-conditioned. a practical
solution to this problem is that we scale the word
vector x by a constant, that is, for the i-th word in
the dictionary, we set xi = c, xj = 0,   i (cid:54)= j, where c
is a constant. the main effect is that we can make the
matrix more stable after this manipulation. note that
when we    x c, this makes no effect on the recovery
accuracy. such a trick is primarily for computational
stability. in our experiments, c is set to be 100.

6.3 dealing with large vocabulary size v
one key step in the whitening procedure of both
methods is to perform svd on the second order
moment m2     rv   v (or n2     r(v +1)  (v +1)). a
straightforward implementation of svd has complex-
ity o(kv 2),2 which is unbearable when the vocabu-
lary size v is large. we follow the method in [14] and
perform random projection to reduce dimensionality.
matrix and then de   ne c = m2s and     = s(cid:62)m2s.
then a low rank approximation of m2 is given by

more precisely, let s     r(cid:101)k where(cid:101)k < v be a random
(cid:102)m2 = c   +c(cid:62). now we can obtain the whitening

matrix without directly performing an svd on m2
by appoximating c   1 and     separately. the overall

2. it is not o(v 3) as we only need top-k truncated svd.

9

2: compute the matrices c and    :

algorithm 3 randomized whitening procedure. input
parameters: second order moment m2 (or n2).

1: generate a random projection matrix s     rv   (cid:101)k.
c = m2s     rv   (cid:101)k, and     = s(cid:62)m2s     r(cid:101)k  (cid:101)k.
3: do svd for both c and    :
4: take the rank-k approximation: uc     uc( : , 1 : k)

c , and     = u        d(cid:62)

c = uc  cd(cid:62)

   

  c       c(1 : k, 1 : k), dc     dc( : , 1 : k)
d        d   (1 : k, 1 : k),                (1 : k, 1 : k)

5: whiten the approximated matrix:

c d(cid:62)
6: output: whitening matrix w .

w = uc     1

c d     1/2
    .

algorithm is provided in alg. 3. in practice, we set

(cid:101)k = 10k to get a suf   ciently accurate approximation.

7 experiments
we now present experimental results on both syn-
thetic and two real-world datasets. for our spectral
methods, the hyper-parameters l and t are set to
be 100, which is suf   ciently large for our experiment
settings. since id106 can only recover the
underlying parameters, we    rst run them to recover
those parameters in training and then use gibbs sam-
pling to infer the topic mixing vectors h and topic
assignments for each word ti for testing.

our main competitor is slda with a gibbs sampler
(gibbs-slda), which is asymptotically accurate and
often outperforms variational methods. we imple-
ment an uncollapsed gibbs sampler, which alternately
draws samples from the local conditionals of   , z, h,
or   , when the rest variables are given. we monitor
the behavior of the gibbs sampler by observing the
relative change of the training data log-likelihood, and
terminate when the average change is less than a
given threshold (e.g., 1e   3) in the last 10 iterations.
the hyper-parameters of the gibbs sampler are set to
be the same as our methods, including topic numbers
and   . we evaluate a hybrid method that uses the
parameters   ,   ,    recovered by our joint spectral
method as initialization for a gibbs sampler. this
strategy is similar to that in [31], where the estimation
of a spectral method is used to initialize an em
method for further re   ning. in our hybrid method,
the gibbs sampler plays the similar role of re   ning.
we also compare with medlda [32], a state-of-the-
art topic model for classi   cation and regression, on
real datasets. we use the gibbs sampler with data
augmentation [33], which is more accurate than the
original variational methods, and adopts the same
stopping condition as above.

on the synthetic data, we    rst use l1-norm to
measure the difference between the reconstructed pa-
rameters and the underlying true parameters. then
we compare the prediction accuracy and per-word

journal of latex class files, vol. 6, no. 1, january 2007

10

likelihood on both synthetic and real-world datasets.
the quality of the prediction on the synthetic dataset
is measured by mean squared error (mse) while
the quality on the real-word dataset is assessed by
which is de   ned as pr2 = 1   (cid:80)
i(yi   (cid:98)yi)2
predictive r2 (pr2), a normalized version of mse,
(cid:80)
mean of testing data and(cid:98)yi is the estimation of yi. the
i(yi     y)2 ,where   y is the
log(cid:80)k
per-word log-likelihood is de   ned as log p(  |h, o) =

j=1 p(  |t = j, o)p(t = j|h).

7.1 synthetic dataset

we generate our synthetic dataset following the gen-
erative process of slda, with a vocabulary of size
v = 500 and topic number k = 20. we generate the
topic distribution matrix o by    rst sampling each en-
try from a uniform distribution and then normalizing
every column of it. the id75 model    is
sampled from a standard gaussian distribution. the
prior parameter    is assumed to be homogeneous, i.e.,
   = (1/k, ..., 1/k). documents and response variables
are then generated from the slda model speci   ed in
section 3.1. we consider two cases where the length
of each document is set to be 250 and 500 repectively.
the hyper-parameters are set to be the same as the
ones that used to generate the dataset 3.

7.1.1 convergence of estimated model parameters

fig. 3 and fig. 4 show the l1-norm reconstruction er-
rors of   ,    and    when each document contains dif-
ferent number of words. note that due to the uniden-
ti   ability of topic models, we only get a permutated
estimation of the underlying parameters. thus we run
a bipartite graph matching to    nd a permutation that
minimizes the reconstruction error. we can    nd that
as the sample size increases, the reconstruction errors
for all parameters decrease consistently to zero in both
methods, which veri   es the correctness of our theory.
taking a closer look at the    gures, we can see that the
empirical convergence rates for    and    are almost
the same for the two id106. however, the
convergence rate for regression parameters    in the
joint method is much higher than the one in the two-
stage method, as mentioned in the comparison of the
sample complexity in section 5.4 , due to the fact that
the joint method can bound the estimation error of   
and    together.

furthermore, though theorem 1 and theorem 2 do
not involve the number of words per document, the
simulation results demonstrate a signi   cant improve-
ment when more words are observed in each docu-
ment, which is a nice complement for the theoretical
analysis.

fig. 5. mean square errors and negative per-word log-
likelihood of alg. 1 and gibbs slda. each document
contains m = 500 words. the x axis denotes the
training size (  103). the    ref. model    denotes the one
with the underlying true parameters.

7.1.2 prediction accuracy and per-word likelihood

fig. 5 shows that both id106 consistently
outperform gibbs-slda. our methods also enjoy the
advantage of being less variable, as indicated by the
curve and error bars. moreover, when the number
of training documents is suf   ciently large, the per-
formance of the reconstructed model is very close
to the true model4, which implies that our spectral
methods can correctly identify an slda model from
its observations, therefore supporting our theory.

the performances of the two-stage spectral method
and the joint one are comparable this time, which is
largely because of the fact the when giving enough
training data, the recovered model is accurate enough.
the gibbs method is easily caught in a local minimum
so we can    nd as the sample size increases, the
prediction errors do not decrease monotonously.

fig. 6.
pr2 scores and negative per-word log-
likelihood on the hotel review dataset. the x axis
indicates the number of topics. error bars indicate the
standard deviation of 5-fold cross-validation.

3. the methods are insensitive to the hyper-parameters in a wide
range. e.g., we still get high accuracy even we set the hyper-
parameter   0 to be twice as large as the true value.

4. due to the randomness in the data generating process, the true

model has a non-zero prediction error.

journal of latex class files, vol. 6, no. 1, january 2007

11

fig. 3. reconstruction errors of two id106 when each document contains 250 words. x axis denotes
the training size n in log domain with base 2 (i.e., n = 2k, k     {8, ..., 15}). error bars denote the standard
deviations measured on 3 independent trials under each setting.

fig. 4. reconstruction errors of two id106 when each document contains 500 words. x axis denotes
the training size n in log domain with base 2 (i.e., n = 2k, k     {8, ..., 15}). error bars denote the standard
deviations measured on 3 independent trials under each setting.

7.2 hotel reviews dataset
for real-world datasets, we    rst test on a relatively
small hotel review dataset, which consists of 15, 000
documents for training and 3, 000 documents for
testing that are randomly sampled from tripadvisor
website. each document is associated with a rating
score from 1 to 5 and our task is to predict it. we pre-
process the dataset by shifting the review scores so
that they have zero mean and unit variance as in [33].
fig. 6 shows the prediction accuracy and per-
word likelihood when the vocabulary size is 5, 000
and the mean level of    is      = 0.1. as medlda
adopts a quite different objective from slda, we
only compare on the prediction accuracy. comparing
with traditional gibbs-slda and medlda, the two-
stage spectral method is much worse, while the joint
spectral method is comparable at its optimal value.
this result is not surprising since the convergence
rate of regression parameters for the joint method
is faster than that of the two-stage one. the hybrid
method (i.e., id150 initialized with the joint
spectral method) performs as well as the state-of-
the-art medlda. these results show that spectral
methods are good ways to avoid stuck in relatively
bad local optimal solution.

7.3 amazon movie reviews dataset
finally, we report the results on a large-scale real
dataset, which is built on amazon movie reviews [21],
to demonstrate the effectiveness of our spectral meth-
ods on improving the prediction accuracy as well as
   nding discriminative topics. the dataset consists of
7, 911, 684 movie reviews written by 889, 176 users
from aug 1997 to oct 2012. each review is accom-
panied with a rating score from 1 to 5 indicating how

a user likes a particular movie. the median number of
words per review is 101. we consider two cases where
a vocabulary with v = 5, 000 terms or v = 10, 000 is
built by selecting high frequency words and deleting
the most common words and some names of charac-
ters in movies. when the vocabulary size v is small
(i.e., 5, 000), we run exact svd for the whitening step;
when v is large (i.e., 10, 000), we run the randomized
svd to approximate the result. as before, we also pre-
process the dataset by shifting the review scores so
that they have zero mean and unit variance.

7.3.1 prediction performance
fig. 7 shows the prediction accuracy and per-word
log-likelihood when      takes different values and the
vocabulary size v = 5, 000, where      =   0/k denotes
the mean level for   . we can see that comparing to the
classical id150 method, our spectral method
is a bit more sensitive to the hyper-parameter   . but
in both cases, our joint method alone outperforms
the gibbs sampler and the two-stage spectral method.
medlda is also sensitive to the hyper-parameter   .
when    is set properly, medlda achieves the best
result comparing with the other methods, however,
the gap between our joint method and medlda is
small. this result is signi   cant for id106,
whose practical performance was often much inferior
to likelihood-based estimators. we also note that if
     is not set properly (e.g.,      = 0.01), a hybrid
method that initializes a gibbs sampler by the results
of our id106 can lead to high accuracy,
outperforming the gibbs sampler and medlda with
a random initialization. we use the results of the
joint method for initialization because this gives better
performance compared with the two-stage method.

fig. 8 shows the results when the vocabulary size

journal of latex class files, vol. 6, no. 1, january 2007

12

fig. 7. pr2 scores and negative per-word log-likelihood on amazon dataset. the x axis indicates the number
of topics. error bars indicate the standard deviation of 5-fold cross-validation. vocabulary size v = 5, 000

fig. 8. pr2 scores and negative per-word log-likelihood on amazon dataset. the x axis indicates the number
of topics. error bars indicate the standard deviation of 5-fold cross-validation. vocabulary size v = 10, 000

v = 10, 000. this time the joint spectral method
gets the best result while the two-stage method is
comparable with id150 but worse than
medlda. the hybrid method is comparable with the
joint method, demonstrating that this strategy works
well in practice again. an interesting phenomenon is
that the spectral method gets good results when the
topic number is only 3 or 4, which means the spectral
method can    t the data using fewer topics. although
there is a rising trend on prediction accuracy for the
hybrid method, we cannot verify this because we can-
not get the results of id106 when k is large.
the reason is that when k > 9, the spectral method
fails in the robust tensor decomposition step, as we
get some negative eigenvalues. this phenomenon can
be explained by the nature of our methods     one

crucial step in alg. 1 and alg. 2 is to whiten (cid:99)m2 which
joint topic matrix (cid:98)o   ) is of full rank. for the amazon

can be done when the underlying topic matrix o ( or

review dataset, it is impossible to whiten it with more
than 9 topics. this fact can be used for model selection
to avoid using too many extra topics. there is also a
rising trend in the id150 when v = 10, 000
as we measure the pr2 indicator, it reaches peak when

topic size k = 40 which is about 0.22 no matter   0 is
0.1 or 0.01. the results may indicate that with a good
initialization, the id150 method could get
much better performance.

finally, note that id150 and the hybrid
id150 methods get better log-likelihood
values. this result is not surprising because gibbs
sampling is based on id113 while id106
do not. fig. 7 shows that the hybrid id150
achieves the best per-word likelihood. thus if one   s
main goal is to maximize likelihood, a hybrid tech-
nique is desirable.
7.3.2 parameter recovery
we now take a closer investigation of the recovered
parameters for our id106. table 1 shows
the estimated regression parameters of both methods,
with k = 8 and v = 5, 000. we can see that the
two methods have different ranges of the possible
predictions     due to the id172 of h, the

range of the predictions by a model with estimate (cid:98)   is
[min((cid:98)  ), max((cid:98)  )]. therefore, compared with the range

provided by the two-stage method (i.e., [   0.75, 0.83]),
the joint method gives a larger one (i.e. [   2.00, 1.12])
which better matches the range of the true labels

journal of latex class files, vol. 6, no. 1, january 2007

13

table 2

probabilities and ranks (in brackets) of some

non-neutral words. n/a means that the word does not
appear in the top 200 ones with highest probabilities.

disappointed

words
bad

boring
stupid
horrible
terrible
waste

good
great
love
funny
enjoy

awesome
amazing

two-stage spec-slda
0.006905 (14)
0.002163 (101)
0.001513 (114)
0.001255 (121)
0.001184 (136)
0.001157 (140)
0.000926 (171)
0.012549 (7)
0.012549 (11)
0.007513 (12)
0.004334 (26)
0.002163 (77)
0.001208 (132)
0.001199 (133)

joint spec-slda
0.009864 (8)
0.002433 (63)
0.001841 (93)
0.001868 (89)
0.001868 (88)
0.001896 (84)
0.001282 (127)
0.012033 (5)
0.003568 (37)
0.003137 (46)
0.003346 (39)
0.001317 (123)
n/a
n/a

[   3, 1]) and therefore leads to more accurate

(i.e.,
predictions as shown in fig. 7.

table 1

estimated    by the two
id106 (sorted
for ease of comparison).

we also examine the estimated topics by both meth-
ods. for the topics with the large value of   , positive
words (e.g.,    great   ) dominate the topics in both
id106 because the frequencies for them
are much higher than negative ones (e.g.,    bad   ).
thus we mainly focus on the    negative    topics where
the difference can be found more expressly. table 2
shows the topics correspond to the smallest value of
   by each method. to save space, for the topic in
each method we show the non-neutral words from
the top 200 with highest probabilities. for each word,
we show its id203 as well as the rank (i.e., the
number in bracket) in the topic distribution vector.
we can see that the neg-
ative words (e.g.,    bad   ,
   boring   ) have a higher
rank (on average) in the
topic by the joint spec-
tral method than in the
topic by the two-stage
method, while the posi-
tive words (e.g.,    good   ,
   great   ) have a lower
rank (on average) in the
topic by the joint method
than in the topic by the
two-stage method. this
result suggests that this
topic in the joint method is more strongly associated
with the negative reviews, therefore yielding a better
   t of the negative review scores when combined with
the estimated   . therefore, considering the supervi-
sion information can lead to improved topics. finally,
we also observe that in both topics some positive
words (e.g.,    good   ) have a rather high rank. this
is because the occurrences of such positive words are
much frequent than the negative ones.

joint
-1.998
-0.762
-0.212
-0.098
0.437
0.946
1.143
1.122

-0.754
-0.385
-0.178
-0.022
0.321
0.522
0.712
0.833

two-stage

table 3

running time (seconds) of our spectral learning

methods and id150.

n(  5    103)
id150
joint spec-slda
two-stage spec-slda

1
47
11
10

2
92
15
13

4
167
17
15

8
340
28
22

16
671
45
39

32
1313
90
81

7.4 time ef   ciency
finally, we compare the time ef   ciency with gibbs
sampling. all algorithms are implemented in c++.

our methods are very time ef   cient because they
avoid the time-consuming iterative steps in tradi-
tional variational id136 and id150 meth-
ods. furthermore, the empirical moment computa-
tion, which is the most time-consuming part in alg. 1
and alg. 2 when dealing with large-scale datasets,
consists of only elementary operations and can be
easily optimized. table 3 shows the running time on
the synthetic dataset with various sizes in the setting
where the topic number is k = 10, vocabulary size
is v = 500 and document length is 100. we can see
that both id106 are much faster than gibbs
sampling, especially when the data size is large.

another advantage of our id106 is that
we can easily parallelize the computation of the low-
order moments over multiple compute nodes, fol-
lowed by a single step of synchronizing the local
moments. therefore, the communication cost will be
very low, as compared to the distributed algorithms
for topic models [26] which often involve intensive
communications in order to synchronize the messages
for (approximately) accurate id136.

as a small k is suf   cient for the amazon review
dataset, we report the results with different k values
on a synthetic dataset where the vocabulary size
v = 500, the document length m = 100 and the
document size n = 1, 000, 000. as shown in fig. 9, the
distributed implementation of our id106
(both two-stage and joint) has almost ideal (i.e., linear)
speedup with respect to the number of threads for mo-
ments computing. the computational complexity of
the tensor decomposition step is o(k5+  ) for a third-
order tensor t     rk  k  k, where    is small [3]. when
the topic number k is large (e.g., as may be needed
in applications with much larger datasets), one can
follow the recent developed stochastic tensor gradient
descent (stgd) method to compute the eigenvalues
and eigenvectors [16], which can signi   cantly reduce
the running time in the tensor decomposition stage.

8 conclusions and discussions
we propose two novel spectral decomposition meth-
ods to recover the parameters of supervised lda
models from labeled documents. the proposed meth-
ods enjoy a provable guarantee of model reconstruc-
tion accuracy and are highly ef   cient and effective.
experimental results on real datasets demonstrate that

journal of latex class files, vol. 6, no. 1, january 2007

14

[14] a. gittens and m. w. mahoney. revisiting the nystrom method
for improved large-scale machine learning. international con-
ference on machine learning (icml), 2013.

[15] m. hoffman, f. bach, and d. blei. online learning for latent
dirichlet allocation. advances in neural information processing
systems (nips), 2010.

[16] f. huang, u. n. niranjan, m. u. hakeem, and a. anandkumar.
fast detection of overlapping communities via online tensor
methods. arxiv:1309.00787, 2014.

[17] j. kruskal. three-way arrays: rank and uniqueness of trilinear
decompositions, with applications to arithmetic complexity
and statistics. id202 and its applications, 18(2):95   138,
1977.

[18] s. lacoste-julien, f. sha, and m. jordan. disclda: discrimi-
native learning for id84 and classi   cation.
advances in neural information processing systems (nips), 2008.
[19] s. leurgans, r. ross, and r. abel. a decomposition for three-
way arrays. siam journal on matrix analysis and applications,
14(4):1064   1083, 1993.

[20] f. li and p. perona. a bayesian hierarchical model for learning
natural scene categories. conference on id161 and
pattern recognition (cvpr), 2005.

[21] j. mcauley and j. leskovec. from amateurs to connoisseus:
modeling the evolution of user expertise through online re-
in international world wide web comference (www),
views.
2013.

[22] a. moitra. algorithmic aspects of machine learning. 2014.
[23] t. nguyen, j. boyd-graber, j. lund, k. seppi, and e. ringger. is
your anchor going up or down? fast and accurate supervised
topic models. the north american chapter of the association for
computational linguistics (naacl), 2015.

[24] i. porteous, d. newman, a. ihler, a. asuncion, p. smyth, and
m. welling. fast collapsed id150 for latent dirichlet
allocation. in sigkdd, 2008.

[25] r. redner and h. walker. mixture densities, maximum like-
lihood and the em algorithm. siam review, 26(2):195   239,
1984.

[26] a. smola and s. narayanamurthy. an architecture for parallel

topic models. proceedings of the vldb endowment, 2010.

[27] m. steyvers and t. grif   ths. latent semantic analysis: a road
laurence

to meaning, chapter probabilistic topic models.
erlbaum, 2007.

[28] c. wang, d. blei, and f. li. simultaneous image classi   cation
and annotation. conference on id161 and pattern
recognition (cvpr), 2009.

[29] y. wang and j. zhu. id106 for supervised topic
models. advances in neural information processing systems
(nips), 2014.

[30] h. weyl. das asymptotische verteilungsgesetz der eigenwerte

linearer partieller differentialgleichungen. math. ann., 1912.

[31] y. zhang, x. chen, d. zhou, and m. jordan. id106
meet em: a provably optimal algorithm for id104.
advances in neural information processing systems (nips), 2014.
[32] j. zhu, a. ahmed, and e. xing. medlda: maximum margin
supervised topic models. journal of machine learning research
(jmlr), (13):2237   2278, 2012.

[33] j. zhu, n. chen, h. perkins, and b. zhang. gibbs max-margin
journal of machine

topic models with data augmentation.
learning research (jmlr), 2014.

[34] j. zhu, n. chen, and e.p. xing. bayesian id136 with pos-
terior id173 and in   nite latent support vector ma-
chines. journal of machine learning research (jmlr), (15):1799   
1847, 2014.

[35] j. zhu and e. xing. sparse topic coding. the conference on

uncertainty in arti   cial intelligence (uai), 2011.

fig. 9. running time of our method w.r.t the number of
threads. both x and y axes are plotted in log scale with
base e.
the proposed methods, especially the joint one, are
superior to existing methods. this result is signi   cant
for id106, which were often inferior to
id113-based methods in practice. for further work, it is
interesting to recover parameters when the regression
model is non-linear.

acknowledgements
this work is supported by the national 973 basic
research program of china (nos. 2013cb329403,
2012cb316301), national nsf of china
(nos.
61322308, 61332007), and tsinghua initiative scienti   c
research program (no. 20141080934).

references
[1] a. anandkumar, d. foster, d. hsu, s. kakade, and y. liu. a
spectral algorithm for id44. advances in
neural information processing systems (nips), 2012.

[2] a. anandkumar, d. foster, d. hsu, s. kakade, and y. liu.
two svds suf   ce: spectral decompositions for probabilistic
id96 and latent dirichlet allocatoin. arxiv:1204.6703,
2012.

[3] a. anandkumar, r. ge, d. hsu, s. kakade, and m. telgarsky.
tensor decompositions for learning latent variable models.
journal of machine learning research (jmlr), 2014.

[4] a. anandkumar, r. ge, and m.

analyzing
tensor power method dynamics in overcomplete regime.
arxiv:1411.1488v2, 2015.

janzamin.

[5] a. anandkumar, d. hsu, and s. kakade. a method of
moments for mixture models and id48.
conference of learning theory (colt), 2012.
s. arora, r. ge, y. halpern, d. mimno, a. moitra, d. sontag,
y. wu, and m. zhu. a practical algorithm for id96
with provable guarantees. international conference on machine
learning (icml), 2013.
s. arora, r. ge, r. kannan, and a. moitra. computing a
nonnegative id105 - provably. symposium on
theory of computing (stoc), 2012.
s. arora, r. ge, and a. moitra. learning topic models-going
beyond svd. 2012.

[9] v. bittorf, b. recht, c. re, and j. tropp. factoring nonnegative
matrices with linear programs. advances in neural information
processing systems (nips), 2012.

[10] d. blei and j. mcauliffe. supervised topic models. advances

[6]

[7]

[8]

in neural information processing systems (nips), 2007.

[11] d. blei, a. ng, and m. jordan. id44.
journal of machine learning research (jmlr), (3):993   1022, 2003.
[12] a. chaganty and p. liang. spectral experts for estimating
international conference on

mixtures of id75s.
machine learning (icml), 2013.

[13] s. cohen and m. collins. tensor decomposition for fast pars-
ing with latent-variable pid18s. advances in neural information
processing systems (nips), 2012.

journal of latex class files, vol. 6, no. 1, january 2007

15

appendix a
proof to theorem 1
in this section, we prove the sample complexity bound given in theorem 1. the proof consists of three main
parts. in appendix a.1, we prove perturbation lemmas that bound the estimation error of the whitened
tensors m2(w, w ), my(w, w ) and m3(w, w, w ) in terms of the estimation error of the tensors themselves.
in appendix a.2, we cite results on the accuracy of svd and robust tensor power method when performed
on estimated tensors, and prove the effectiveness of the power update method used in recovering the linear
regression model   . finally, we give tail bounds for the estimation error of m2, my and m3 in appendix a.3

and complete the proof in appendix a.4. we also make some remarks on the indirect quantities (e.g.   k((cid:101)o))

used in theorem 1 and simpli   ed bounds for some special cases in appendix a.4.

all norms in the following analysis, if not explicitly speci   ed, are 2 norms in the vector and matrix cases

and the operator norm in the high-order tensor case.

a.1 perturbation lemmas

simplify the notations that arise in subsequent analysis.

we    rst de   ne the canonical topic distribution vectors (cid:101)   and estimation error of observable tensors, which
de   nition 3 (canonical topic distribution). de   ne the canonical version of topic distribution vector   i,(cid:101)  i, as follows:
we also de   ne o,(cid:101)o     rn  k by o = [  1,       ,   k] and (cid:101)o = [(cid:102)  1,       ,(cid:102)  k].

(cid:114)   i

(cid:101)  i (cid:44)

  0(  0 + 1)

(10)

  i.

de   nition 4 (estimation error). assume

(cid:107)m2    (cid:99)m2(cid:107)     ep ,
(cid:107)my    (cid:99)my(cid:107)     ey,
(cid:107)m3    (cid:99)m3(cid:107)     et .

(11)
(12)
(13)

for some real values ep , ey and et , which we will set later.

,

1

(cid:107)w(cid:107) =

the following lemma analyzes the whitening matrix w of m2. many conclusions are directly from [1].

lemma 1 (lemma c.1, [2]). let w,(cid:99)w     rn  k be the whitening matrices such that m2(w, w ) = (cid:99)m2((cid:99)w ,(cid:99)w ) = ik.
let a = w (cid:62)(cid:101)o and (cid:98)a =(cid:99)w (cid:62)(cid:101)o. suppose ep       k(m2)/2. we have
  k((cid:101)o)
(cid:107)(cid:99)w(cid:107)    
  k((cid:101)o)
(cid:107)w    (cid:99)w(cid:107)    
  k((cid:101)o)3
(cid:107)w +(cid:107)     3  1((cid:101)o),
(cid:107)(cid:99)w +(cid:107)     2  1((cid:101)o),
(cid:107)w +    (cid:99)w +(cid:107)     6  1((cid:101)o)
  k((cid:101)o)2
(cid:107)(cid:98)a(cid:107)     2,
(cid:107)a     (cid:98)a(cid:107)    

(cid:107)a(cid:107) = 1,

(17)
(18)

(20)
(21)

(15)

(19)

(14)

(16)

(22)

4ep

ep ,

2

,

,

,

proof: proof to eq. (16): let (cid:99)w (cid:62)(cid:99)m2(cid:99)w = i and (cid:99)w (cid:62)m2(cid:99)w = bdb(cid:62), where b is orthogonal and d is a
positive de   nite diagonal matrix. we then see that w = (cid:99)w bd   1/2b(cid:62) satis   es the condition w m2w (cid:62) = i.

.

(23)

  k((cid:101)o)2
(cid:107)aa(cid:62)     (cid:98)a(cid:98)a(cid:62)(cid:107)     12ep
  k((cid:101)o)2

4ep

journal of latex class files, vol. 6, no. 1, january 2007

subsequently, (cid:99)w = w bd1/2b(cid:62). we then can bound (cid:107)w    (cid:99)w(cid:107) as follows

(cid:107)w    (cid:99)w(cid:107)     (cid:107)w(cid:107)    (cid:107)i     d1/2(cid:107)     (cid:107)w(cid:107)    (cid:107)i     d(cid:107)     4ep
  k((cid:101)o)3

,

where the inequality (cid:107)i     d(cid:107)     4ep

proof to eq. (23): (cid:107)aa(cid:62)     (cid:98)a(cid:98)a(cid:62)(cid:107)     (cid:107)aa(cid:62)     a(cid:98)a(cid:62)(cid:107) + (cid:107)a(cid:98)a(cid:62)     (cid:98)a(cid:98)a(cid:62)(cid:107)     (cid:107)a     (cid:98)a(cid:107)    ((cid:107)a(cid:107) + (cid:107)(cid:98)a(cid:107))     12ep
  k((cid:101)o)2 .

  k((cid:101)o)2 was proved in [2].

all the other inequalities come from lemma c.1, [2].
we are now able to provide perturbation bounds for estimation error of whitened moments.

de   nition 5 (estimation error of whitened moments). de   ne

  p,w (cid:44) (cid:107)m2(w, w )    (cid:99)m2((cid:99)w ,(cid:99)w )(cid:107),
  y,w (cid:44) (cid:107)my(w, w )    (cid:99)my((cid:99)w ,(cid:99)w )(cid:107),
  t,w (cid:44) (cid:107)m3(w, w, w )    (cid:99)m3((cid:99)w ,(cid:99)w ,(cid:99)w )(cid:107).

lemma 2 (perturbation lemma of whitened moments). suppose ep       k(m2)/2. we have

  p,w     16ep

  y,w    

  t,w    

,

24(cid:107)  (cid:107)ep

  k((cid:101)o)2
(  0 + 2)  k((cid:101)o)2
  k((cid:101)o)2
(  0 + 1)(  0 + 2)  k((cid:101)o)5

54ep

4ey

+

+

,

8et

  k((cid:101)o)3

.

proof: using the idea in the proof of lemma c.2 in [2], we can split   p,w as

  p,w =(cid:107)m2(w, w )     m2((cid:99)w ,(cid:99)w ) + m2((cid:99)w ,(cid:99)w )    (cid:99)m2((cid:99)w ,(cid:99)w )(cid:107)
   (cid:107)m2(w, w )     m2((cid:99)w ,(cid:99)w )(cid:107) + (cid:107)m2((cid:99)w ,(cid:99)w )    (cid:99)m2((cid:99)w ,(cid:99)w )(cid:107).
(cid:107)m2(w, w )     m2((cid:99)w ,(cid:99)w )(cid:107) = (cid:107)w (cid:62)m2w    (cid:99)w (cid:62)(cid:99)m2(cid:99)w(cid:107)

we can the bound the two terms seperately, as follows.
for the    rst term, we have

= (cid:107)aa(cid:62)     (cid:98)a(cid:98)a(cid:62)(cid:107)
  k((cid:101)o)2

    12ep

.

16

(24)
(25)
(26)

(27)

(28)

(29)

where the last inequality comes from eq. (23).

for the second term, we have

(cid:107)m2((cid:99)w ,(cid:99)w )    (cid:99)m2((cid:99)w ,(cid:99)w )(cid:107)     (cid:107)(cid:99)w(cid:107)2    (cid:107)m2    (cid:99)m2(cid:107)     4ep
  k((cid:101)o)2

,

where the last inequality comes from eq. (15).

similarly,   y,w can be splitted as (cid:107)my(w, w )     my((cid:99)w ,(cid:99)w )(cid:107) and (cid:107)my((cid:99)w ,(cid:99)w )     (cid:99)my((cid:99)w ,(cid:99)w )(cid:107), which can be

bounded separately. for the    rst term, we have

(cid:107)my(w, w )     my((cid:99)w ,(cid:99)w )(cid:107) =(cid:107)w (cid:62)myw    (cid:99)w (cid:62)my(cid:99)w(cid:107)
(cid:107)adiag(  )a(cid:62)     (cid:98)adiag(  )(cid:98)a(cid:62)(cid:107)
   (cid:107)aa(cid:62)     (cid:98)a(cid:98)a(cid:62)(cid:107)

  0 + 2

=

2

  0 + 2

    2(cid:107)  (cid:107)
(  0 + 2)  k((cid:101)o)2
   

24(cid:107)  (cid:107)

   ep .

for the second term, we have

(cid:107)my((cid:99)w ,(cid:99)w )    (cid:99)my((cid:99)w ,(cid:99)w )(cid:107)     (cid:107)(cid:99)w(cid:107)2    (cid:107)my    (cid:99)my(cid:107)     4ey
  k((cid:101)o)2

.

journal of latex class files, vol. 6, no. 1, january 2007

17

finally, we bound   t,w as below, following the work [12].

  t,w = (cid:107)m3(w, w, w )    (cid:99)m3((cid:99)w ,(cid:99)w ,(cid:99)w )(cid:107)
    (cid:107)m3(cid:107)    (cid:107)w    (cid:99)w(cid:107)    ((cid:107)w(cid:107)2 + (cid:107)w(cid:107)    (cid:107)(cid:99)w(cid:107) + (cid:107)(cid:99)w(cid:107)2) + (cid:107)(cid:99)w(cid:107)3    (cid:107)m3    (cid:99)m3(cid:107)
  k((cid:101)o)3
(  0 + 1)(  0 + 2)  k((cid:101)o)5

54ep

8et

   

+

,

where we have used the fact that

(cid:107)m3(cid:107)     k(cid:88)

i=1

2  i

  0(  0 + 1)(  0 + 2)

=

2

(  0 + 1)(  0 + 2)

.

a.2 svd accuracy
the key idea for spectral recovery of lda id96 is the simultaneous diagonalization trick, which
asserts that we can recover lda model parameters by performing orthogonal tensor decomposition on a pair
of simultaneously whitened moments, for example, (m2, m3) and (m2, my). the following proposition details
this insight, as we derive orthogonal tensor decompositions for the whitened tensor product my(w, w ) and
m3(w, w, w ).

proposition 4. de   ne vi (cid:44) w (cid:62)(cid:101)  i =

(cid:113)   i
  0(  0+1) w (cid:62)  i. then

i=1 is an orthonormal basis.

1) {vi}k
2) my has a pair of singular value and singular vector (  y
3) m3 has a pair of robust eigenvalue and eigenvector [3] (  i, vi) with   i = 2

i , vi) with   y

i = 2

  0+2   j for some j     [k].

(cid:113)   0(  0+1)

i=1 follows from the fact that w (cid:62)m2w =(cid:80)k

  0+2

  j(cid:48)
i=1 viv(cid:62)

proof: the orthonormality of {vi}k

for some j(cid:48)     [k].

i = ik. subsequently,

we have

k(cid:88)
k(cid:88)

i=1

my(w, w ) =

m3(w, w, w ) =

2

  0 + 2

2

  iviv(cid:62)
i ,

(cid:115)

  0(  0 + 1)

  0 + 2

i=1

  i

vi     vi     vi.

2

  0 + 2

then have

i my(w, w )vi = 2

proof: first, note that v(cid:62)

the following lemmas (lemma 3 and lemma 4) give upper bounds on the estimation error of    and    in

terms of |(cid:98)  i       i|, |(cid:98)vi     vi| and the estimation errors of whitened moments de   ned in de   nition 5.
i (cid:99)my((cid:99)w ,(cid:99)w )(cid:98)vi, where (cid:98)vi is some estimation of vi. we
2 (cid:98)v(cid:62)
lemma 3 (  i estimation error bound). de   ne (cid:98)  i (cid:44)   0+2
(1 + 2(cid:107)(cid:98)vi     vi(cid:107))      y,w.
|  i    (cid:98)  i|     2(cid:107)  (cid:107)(cid:107)(cid:98)vi     vi(cid:107) +
(cid:12)(cid:12)(cid:12)(cid:98)v(cid:62)
(cid:12)(cid:12)(cid:12)
i (cid:99)my((cid:99)w ,(cid:99)w )(cid:98)vi     v(cid:62)
   (cid:12)(cid:12)(cid:12)((cid:98)vi     vi)(cid:62)(cid:99)my((cid:99)w ,(cid:99)w )(cid:98)vi
(cid:12)(cid:12)(cid:12) +
(cid:17)(cid:12)(cid:12)(cid:12)
(cid:16)(cid:99)my((cid:99)w ,(cid:99)w )(cid:98)vi     my(w, w )vi
   (cid:107)(cid:98)vi     vi(cid:107)(cid:107)(cid:99)my((cid:99)w ,(cid:99)w )(cid:107)(cid:107)(cid:98)vi(cid:107) + (cid:107)vi(cid:107)(cid:107)(cid:99)my((cid:99)w ,(cid:99)w )(cid:98)vi     my(w, w )vi(cid:107).
note that both vi and (cid:98)vi are unit vectors. therefore,
(cid:19)

   (cid:107)(cid:99)my((cid:99)w ,(cid:99)w )(cid:107)(cid:107)(cid:98)vi     vi(cid:107) + (cid:107)(cid:99)my((cid:99)w ,(cid:99)w )(cid:107)(cid:107)(cid:98)vi     vi(cid:107) + (cid:107)(cid:99)my((cid:99)w ,(cid:99)w )     my(w, w )(cid:107)(cid:107)vi(cid:107)
   2(cid:107)(cid:98)vi     vi(cid:107)

|  i    (cid:98)  i|    (cid:107)(cid:99)my((cid:99)w ,(cid:99)w )(cid:107)(cid:107)(cid:98)vi     vi(cid:107) + (cid:107)(cid:99)my((cid:99)w ,(cid:99)w )(cid:98)vi     my(w, w )vi(cid:107)

i=1 are orthonormal. subsequently, we have

  0+2   i because {vi}k

|  i    (cid:98)  i| =

(cid:18) 2

(cid:107)  (cid:107) +   y,w

(cid:12)(cid:12)(cid:12)v(cid:62)

i my(w, w )vi

+   y,w.

  0 + 2

  0 + 2

(30)

2

2

i

  0 + 2

journal of latex class files, vol. 6, no. 1, january 2007

18

the last inequality is due to the fact that (cid:107)my(w, w )(cid:107) = 2

lemma 4 (  i estimation error bound). de   ne (cid:98)  i (cid:44)   0+2

value pairs (  i, vi) of m3(w, w, w ). we then have

  0+2(cid:107)  (cid:107).

2 (cid:98)  i((cid:99)w +)(cid:62)(cid:98)vi, where (cid:98)  i,(cid:98)vi are some estimates of singular

(cid:107)(cid:98)  i       i(cid:107)     3(  0 + 2)

2

  1((cid:101)o)|(cid:98)  i       i| + 3  max  1((cid:101)o)(cid:107)(cid:98)vi     vi(cid:107) +

6  max  1((cid:101)o)ep
  k((cid:101)o)2

.

(31)

proof: first note that   i =   0+2

2   i(w +)(cid:62)vi. subsequently,

(cid:107)  i    (cid:98)  i(cid:107) =(cid:107)(cid:98)  i((cid:99)w +)(cid:62)(cid:98)vi       i(w +)(cid:62)vi(cid:107)

2

  0 + 2

   (cid:107)(cid:98)  i(cid:99)w +       iw +(cid:107)(cid:107)(cid:98)vi(cid:107) + (cid:107)  iw +(cid:107)(cid:107)(cid:98)vi     vi(cid:107)
   |(cid:98)  i       i|(cid:107)(cid:99)w +(cid:107) + |  i|(cid:107)(cid:99)w +     w +(cid:107) + |  i|(cid:107)w +(cid:107)(cid:107)(cid:98)vi     vi(cid:107)
   3  1((cid:101)o)|(cid:98)  i       i| +

   6  1((cid:101)o)ep
  k((cid:101)o)2

2  max
  0 + 2

2  max
  0 + 2

+

   3  1((cid:101)o)    (cid:107)(cid:98)vi     vi(cid:107).

to bound the error of orthogonal tensor decomposition performed on the estimated tensors (cid:99)m3((cid:99)w ,(cid:99)w ,(cid:99)w ),
recovering(cid:98)  i and (cid:98)vi.

we cite theorem 5.1 [3], a sample complexity analysis on the robust tensor power method we used for

(cid:113)   0(  0+1)

(cid:113)   0(  0+1)

lemma 5 (theorem 5.1, [3]). let   max = 2
, where   min = min   i and
  max = max   i. then there exist universal constants c1, c2 > 0 such that the following holds: fix   (cid:48)     (0, 1). suppose
  t,w        and

,   min = 2

  0+2

  0+2

  max

  min

  t,w     c1      min
k

suppose {((cid:98)  i,(cid:98)vi)}k
(cid:99)m3((cid:99)w ,(cid:99)w ,(cid:99)w ) for l = poly(k) log(1/  (cid:48)) and n     c2    (log(k) + log log(   max
|(cid:98)  i         (cid:48)(i)|     5  .

than 1       (cid:48), there exists a permutation   (cid:48) : [k]     [k] such that for all i,

(cid:107)(cid:98)vi     v  (cid:48)(i)(cid:107)     8  /  min,

,

  

i=1 are eigenvalue and eigenvector pairs returned by running algorithm 1 in [3] with input
)) iterations. with id203 greater

(32)

a.3 tail inequalities
lemma 6 (lemma 5, [12]). let x1,       , xn     rd be i.i.d. samples from some distribution with bounded support (i.e.,
(cid:107)x(cid:107)2     b with id203 1 for some constant b). then with id203 at least 1       ,

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1
id203 at least 1     n   (cid:48)       , (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

n

n(cid:88)

i=1

n(cid:88)

i=1

xi     e[x]

    2b   
n

1 +

log(1/  )

2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:32)

(cid:32)

(cid:114)

(cid:114)

(cid:33)

(cid:33)

.

.

xi     e[x]

    2b   
n

1 +

log(1/  )

2

corollary 1. let x1,       , xn     rd be i.i.d. samples from some distributions with pr[(cid:107)x(cid:107)2     b]     1       (cid:48). then with

proof: use union bound.

lemma 7 (concentration of moment norms). suppose we obtain n i.i.d. samples (i.e., documents with at least three
words each and their regression variables in slda models). de   ne r(  ) (cid:44) (cid:107)  (cid:107)           1(  ), where      1(  ) is the inverse
function of the cdf of a standard gaussian distribution. let e[  ] denote the mean of the true underlying distribution

and (cid:98)e[  ] denote the empirical mean. then

journal of latex class files, vol. 6, no. 1, january 2007

(cid:34)
(cid:34)
(cid:34)
(cid:34)
(cid:34)
(cid:34)

2 +(cid:112)2 log(1/  )

(cid:35)

n

n

   

   

(cid:35)

    1       ,

    1       ,

(cid:107)e[x1]    (cid:98)e[x1](cid:107)f <
2 +(cid:112)2 log(1/  )
(cid:107)e[x1     x2]    (cid:98)e[x1     x2](cid:107)f <
2 +(cid:112)2 log(1/  )
(cid:107)e[x1     x2     x3]    (cid:98)e[x1     x2     x3](cid:107)f <
(cid:35)
(cid:107)e[y]    (cid:98)e[y](cid:107) < r(  /4n )    2 +(cid:112)2 log(2/  )
(cid:107)e[yx1]    (cid:98)e[yx1](cid:107)f < r(  /4n )    2 +(cid:112)2 log(2/  )
(cid:107)e[yx1     x2]    (cid:98)e[yx1     x2](cid:107)f < r(  /4n )    2 +(cid:112)2 log(2/  )

n
    1       ,

    1       ,

(cid:35)

(cid:35)

   

   

   

   

n

n

pr

pr

pr

pr

pr

pr

n

    1       ,

(cid:35)

    1       .

19

(33)

proof: use lemma 6 and corrolary 1 for concentration bounds involving the regression variable y.

corollary 2. with id203 1        the following holds:

   

1) ep = (cid:107)m2    (cid:99)m2(cid:107)     3    2+
2) ey = (cid:107)my    (cid:99)my(cid:107)     10r(  /60n )    2+
3) et = (cid:107)m3    (cid:99)m3(cid:107)     10    2+

   
2 log(6/  )

   
2 log(9/  )

   

n

.

.

   

   
2 log(15/  )

n

.

n

proof: corrolary 2 can be proved by expanding the terms by de   nition and then using tail inequality in

lemma 7 and union bound. also note that (cid:107)    (cid:107)     (cid:107)    (cid:107)f for all matrices.

a.4 completing the proof
we are now ready to give a complete proof to theorem 1.

proof: (proof of theorem 1) first, the assumption ep       k(m2) is required for error bounds on   p,w,   y,w

and   t,w. noting corrolary 2 and the fact that   k(m2) =   min

(cid:32)

0(  0 + 1)2(1 +(cid:112)log(6/  ))2

  0(  0+1), we have

(cid:33)

  2

.

n =    

min

  2

54ep

for lemma 5 to hold, we need the assumptions that   t,w     min(  , o(   min

k )). these imply n3, as we expand
(  0+1)(  0+2)  k((cid:101)o)5 dominates the second
  t,w according to de   nition 5 and note the fact that the    rst term
one. the   0 is missing in the third requirment n3 because   0 + 1     1,   0 + 2     2 and we discard them both.
to bound the estimation error for the linear classi   er   , we need to further bound   y,w. we assume   y,w       .
by expanding   y,w according to de   nition 5 in a similar manner we obtain the ((cid:107)  (cid:107) +      1(  /60  ))2 term in

note that this lower bound does not depend on k,    and   k((cid:101)o).
2 (cid:98)  i.
the |  i    (cid:98)    (i)| bound follows immediately by lemma 5 and the recovery rule (cid:98)  i =   0+2
the requirment of n2. the bound on |  i    (cid:98)    (i)| follows immediately by lemma 3.
finally, we bound (cid:107)  i    (cid:98)    (i)(cid:107) using lemma 4. we need to assume that 6  max  1((cid:101)o)ep
  k((cid:101)o)2
max  1((cid:101)o)2 term. the (cid:107)  i    (cid:98)    (i)(cid:107) bound then follows by lemma 4 and lemma 5.
appeared in theorem 1 (e.g.,   k((cid:101)o)) and the functions of original model parameters (e.g.,   k(o)). these
remark 6. the indirect quantities   1((cid:101)o) and   k((cid:101)o) can be related to   1(o),   k(o) and    in the following way:

we make some remarks for the main theorem. in remark 6, we establish links between indirect quantities

connections are straightforward following their de   nitions.

      , which gives the

  2

(cid:114)   min

  0(  0 + 1)

  k(o)       k((cid:101)o)    

(cid:114)   max

  0(  0 + 1)

  k(o);

journal of latex class files, vol. 6, no. 1, january 2007

  1((cid:101)o)    

(cid:114)   max

  0(  0 + 1)

  1(o)    

1   
  0 + 1

.

20

we now take a close look at the sample complexity bound in theorem 1. it is evident that n2 can be neglected
when the number of topics k gets large, because in practice the norm of the id75 model    is usually
assumed to be small in order to avoid over   tting. moreover, as mentioned before, the prior parameter    is
often assumed to be homogeneous with   i = 1/k [27]. with these observations, the sample complexity bound
in theorem 1 can be greatly simpli   ed.
remark 7. assume (cid:107)  (cid:107) and    are small and    = (1/k,       , 1/k). as the number of topics k gets large, the sample
complexity bound in theorem 1 can be simpli   ed as

   max(     2, k3)

n =    

such dependency is somewhat necessary because we are using third-order tensors to recover the underlying

the sample complexity bound in remark 7 may look formidable as it depends on   k((cid:101)o)10. however,
model parameters. furthermore, the dependence on   k((cid:101)o)10 is introduced by the robust tensor power
method to recover lda parameters, and the reconstruction accuracy of    only depends on   k((cid:101)o)4 and
algorithms that have milder dependence on the singular value   k((cid:101)o), we might be able to get an algorithm

((cid:107)  (cid:107)+     1(  /60  ))2. as a consequence, if we can combine our power update method for    with lda id136

(34)

.

with a better sample complexity.

(cid:32)

log(1/  )

  k((cid:101)o)10

(cid:33)

journal of latex class files, vol. 6, no. 1, january 2007

21

appendix b
proof of theorem 2
in this section we give the proof of theorem 2, following the similar line of the proof of theorem 1. we bound
the estimation errors and the errors introduced by the tensor decomposition step respectively.

b.1 de   nitions
we    rst recall the de   nition of joint canconical topic distribution.

de   nition 6. (joint canconical topic distribution) de   ne the canconical version of joint topic distribution vector vi,(cid:101)vi

as follows:

(cid:114)   i

(cid:101)vi =

  0(  0 + 1)

vi,

where vi = [  (cid:48)

we also de   ne o   ,(cid:102)o        r(v +1)  k by o    = [v1, ..., vk] and (cid:102)o    = [(cid:101)v1, ...,(cid:101)vk].

i,   i](cid:48) is the topic dictribution vector extended by its regression parameter.

b.2 estimation errors
the tail inequatlities in last section 6,1 gives the following estimation:
lemma 8. suppose we obtain n i.i.d. samples. let e[  ] denote the mean of the true underlying distribution and   e[  ]
denote the empirical mean. de   ne

r1(  ) = (cid:107)  (cid:107)            1(  ),
r2(  ) = 2(cid:107)  (cid:107)2 + 2  2[     1(  )]2,
r3(  ) = 4(cid:107)  (cid:107)3     4  3[     1(  )]3,

where      1(  ) is the inverse function of the cdf of a standard gaussian distribution. then:

pr

pr

pr

pr

n

   

   

   

(cid:35)

(cid:35)

n
    1       

2 +(cid:112)2 log(1/  )

(cid:107)e[x1     x2     x3]       e[x1     x2     x3](cid:107)f <

(cid:34)
(cid:34)
(cid:35)
(cid:107)e[yi]       e[yi](cid:107)f < ri(  /4n )    2 +(cid:112)2 log(2/  )
(cid:34)
(cid:107)e[yix1]       e[yix1](cid:107)f < ri(  /4n )    2 +(cid:112)2 log(2/  )
(cid:34)
(cid:107)e[yix1     x2]       e[yix1     x2](cid:107)f < ri(  /4n )    2 +(cid:112)2 log(2/  )
(cid:34)
(cid:34)
(cid:34)

n
(cid:107)e[z1     z2]       e[z1     z2](cid:107) < c2(  /12n )

(cid:35)
2 +(cid:112)2 log(6/  )

n
(cid:107)e[z1     z2     z3]       e[z1     z2     z3](cid:107)f < c3(  /16n )

2 +(cid:112)2 log(4/  )

(cid:107)e[z1]       e[z1](cid:107) < c1(  /8n )

    1       ,

    1       

   

   

   

   

n

n

p r

p r

proof: this lemma is a direct application of lemma 6 and corollary 1.

corollary 3. with id203 1       , the following holds:

    1       ;

i = 1, 2, 3;

(cid:35)

i = 1, 2;

    1       

i = 1, 2.

(cid:35)
2 +(cid:112)2 log(8/  )

    1       ,

n

(cid:35)

    1       ,

p r

where

proof: use lemma 8 as well as the fact that

p r(x + y     t1 + t2).

b,   a, b     0 and p r(x     t1, y     t2)    

c1(  ) = r1(  ) + 1,
c2(  ) = r1(  ) + r2(  ) + 1,
c3(  ) = r1(  ) + r2(  ) + r3(  ) + 1.

   

a + b        

a +

   

(35)

(36)

(37)

(38)

journal of latex class files, vol. 6, no. 1, january 2007

2 +(cid:112)2 log(72/  )

   

n

22

   

lemma 9. (concentration of moment norms) using notations in lemma 8 and suppose c3(  /144n )
1 we have:

ep = (cid:107)n2       n2(cid:107)     3c2(  /36n ))    2 +(cid:112)2 log(18/  )
et = (cid:107)n3       n3(cid:107)     10c3(  /144n ))    2 +(cid:112)2 log(72/  )

n
   

   

,

.

(39)

n

proof: this lemma can be proved by expanding the terms by de   nition and use lemma 8 , corollary 3 as

well as union bound. also note that (cid:107)    (cid:107)     (cid:107)    (cid:107)f .
lemma 10. (estimation error of whitened moments) de   ne

 p,w = (cid:107)n2(w, w )       n2(   w ,   w )(cid:107),
 t,w = (cid:107)n3(w, w, w )       n3(   w ,   w ,   w )(cid:107).

 p,w     16ep

,

  k((cid:102)o   )2
(  0 + 1)(  0 + 2)  k((cid:102)o   )5

54ep

 t,w    

8et

  k((cid:102)o   )3

.

+

(40)

(41)

then we have:

b.3 svd accuracy

we rewrite a part of lemma 1 which we will use in the following.
lemma 11. let w,   w     r(v +1)  k be the whitening matrices such that n2(w, w ) =   w2(   w ,   w ) = ik. further more,
we suppose ep       k(n2)/2. then we have:

,

(cid:107)w       w(cid:107)     4ep

  k((cid:102)o   )3
(cid:107)w +(cid:107)     3  1((cid:102)o   ),
(cid:107)(cid:102)w +(cid:107)     2  1((cid:102)o   ),
(cid:107)w +       w +(cid:107)     6  1((cid:102)o   )ep
  k((cid:102)o   )2

(42)

.

using the above lemma, we now can estimate the error introduced by svd.

lemma 12. (vi estimation error) de   ne   vi =   0+2
of n3(w, w, w ). we then have:

2

(cid:107)  vi     vi(cid:107)     (  0 + 1)6  1((cid:102)o   )ep

  k((cid:102)o   )2

+

(  0 + 2)  1((cid:102)o   )|    i       i|

2

+ 3(  0 + 1)  1((cid:102)o   )(cid:107)     i       i(cid:107).

    i(   w +)(cid:62)     i where (    i,     i) are some estimations of svd pairs (  i,   i)

proof: note that vi =   i(w +)(cid:62)  i. thus we have:

2

  0 + 2

(cid:107)  vi     vi(cid:107) =(cid:107)    i(   w +)(cid:62)     i       i(w +)(cid:62)  i(cid:107)

=(cid:107)    i(   w +)(cid:62)     i       i(w +)(cid:62)     i +   i(w +)(cid:62)     i       i(w +)(cid:62)  i(cid:107)
   (cid:107)    i(   w +)(cid:62)       i(w +)(cid:62)(cid:107)(cid:107)     i(cid:107) + (cid:107)  i(w +)(cid:62)(cid:107)(cid:107)     i       i(cid:107)
   |  i|(cid:107)   w +     w +(cid:107)(cid:107)     i(cid:107) + |    i       i|(cid:107)   w +(cid:107)(cid:107)     i(cid:107) + |  i|(cid:107)w +(cid:107)(cid:107)     i       i(cid:107)
    2(  0 + 1)
  0 + 2

+ 3  1((cid:102)o   )|    i       i| +

6ep   1((cid:102)o   )
  k((cid:102)o   )2

2(  0 + 1)

  0 + 2

   3  1((cid:102)o   )    (cid:107)     i       i(cid:107).

(43)

(44)

in lemma 9, we need that c3(  /144n )

for lemma 5 to hold, we need the assumption that  t,w     c1      min

journal of latex class files, vol. 6, no. 1, january 2007

23

b.4 completeing the proof
we are now ready to complete the proof of theorem 2.

  k(n2)/2 is required in lemma 11. noting that   k(n2) =

proof: (proof of theorem 2) we check out the conditions that must be satis   ed for   accuracy. first ep    

(cid:32)

n     o

n     o

(cid:32)

n     o

c 2

(cid:33)

.

  min

.

n

min

  2

c 2

   

2 log(72/  )

  0(  0 + 1)

, we have:

  2
0(  0 + 1)2c 2

(cid:16)
3 (  /36n )(2 +(cid:112)2 log(18/  ))2

    1, which means:

2 (  /36n )    (2 +(cid:112)2 log(18/  ))2
3 (  /144n )(2 +(cid:112)2 log(72/  ))2(cid:17)
  k((cid:102)o   )10
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:18)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)    

2 , we have:

(  0 + 2)2  2

4  0(  0 + 1)

    1
  2
i

k2
  2

  1((cid:102)o   )(  0 + 2)(

1
 2 ,

(cid:19)

    .

+

8

)

7
2

  min

k , which implies:
   max(

)

.

(cid:33)

min

from the recovery rule     i =

4  0(  0 + 1)
(  0 + 2)2     i
|    i       i|     4  0(  0 + 1)
(  0 + 2)2

(45)
finally, we bound (cid:107)  vi     vi(cid:107), which is a direct consequence of lemma 12 and lemma 5. we additional assume
     . in fact, this condition is already satis   ed if the above requirements
the following condition holds:

    2
i

min(  min     5 )2    5 .

6ep

  k((cid:102)o   )2

for n hold. under this condition, we have:
(cid:107)  vi     vi(cid:107)    

journal of latex class files, vol. 6, no. 1, january 2007

24

appendix c
moments of observable variables
c.1 proof to proposition 1
the equations on m2 and m3 have already been proved in [2] and [3]. here we only give the proof to the
equation on my. in fact, all the three equations can be proved in a similar manner.

in slda the topic mixing vector h follows a dirichlet prior distribution with parameter   . therefore, we

have

,

(cid:40)

e[hi] =

  i
  0
e[hihj] =

e[hihjhk] =

                     

  2
i

  0(  0+1) ,
  i  j
  2
0

,

if i = j,
if i (cid:54)= j

,

  3
i

i   k

  0(  0+1)(  0+2) ,
  2
0(  0+1) ,
  2
  i  j   k
  3
0

,

if i = j = k,
if i = j (cid:54)= k,
if i (cid:54)= j, j (cid:54)= k, i (cid:54)= k

.

e[y|h] =   (cid:62)h,
e[x1|h] =

k(cid:88)

i=1

e[x1     x2|h] =

hi  i,

k(cid:88)
k(cid:88)

i,j=1

hihj  i       j,

e[yx1     x2|h] =

hihjhk      k  j       k.

(46)

(47)

next, note that

proposition 1 can then be proved easily by taking expectation over the topic mixing vector h.

i,j,k=1

space. they do not need to be accelerated in most practical applications. this time and space complexity also

c.2 details of the speeding-up trick
in this section we provide details of the trick mentioned in the main paper to speed up empirical moments

computations. first, note that the computation of (cid:99)m1, (cid:99)m2 and (cid:99)my only requires o(n m 2) time and o(v 2)
applies to all terms in (cid:99)m3 except the (cid:98)e[x1     x2     x3] term, which requires o(n m 3) time and o(v 3) space if
using naive implementations. therefore, this section is devoted to speed-up the computation of(cid:98)e[x1   x2   x3].
(cid:98)e[x1     x2     x3]((cid:99)w ,(cid:99)w ,(cid:99)w )     rk  k  k.
fix a document d with m words. let t (cid:44)(cid:98)e[x1   x2   x3|d] be the empirical tensor demanded. by de   nition,

more precisely, as mentioned in the main paper, what we want to compute is the whitened empirical moment

we have

ti,j,k =

1

m(m     1)(m     2)

ni(nj     1)(nk     2),
ni(ni     1)nk,
ninj(nj     1),
ninj(ni     1),
ninjnk,

i = j = k;
i = j, j (cid:54)= k;
j = k, i (cid:54)= j;
i = k, i (cid:54)= j;
otherwise;

(48)

                                 

where ni is the number of occurrences of the i-th word in document d. if ti,j,k =
i, j and k, then we only need to compute

ninj nk

m(m   1)(m   2) for all indices

t (w, w, w ) =

1

m(m     1)(m     2)

   (w n)   3,

where n (cid:44) (n1, n2,       , nv ). this takes o(m k +k3) computational time because n contains at most m non-zero
entries, and the total time complexity is reduced from o(n m 3) to o(n (m k + k3)).

we now consider the remaining values, where at least two indices are identical. we    rst consider those
values with two indices the same, for example, i = j. for these indices, we need to subtract an nink term, as
shown in eq. (48). that is, we need to compute the whitened tensor    (w, w, w ), where         rv   v   v and

   i,j,k =

1

m(m     1)(m     2)

  

i = j;
otherwise.

(49)

(cid:26) nink,

0,

journal of latex class files, vol. 6, no. 1, january 2007

25

1

note that     can be written as

m(m   1)(m   2)    a     n, where a = diag(n1, n2,       , nv ) is a v    v matrix
and n = (n1, n2,       , nv ) is de   ned previously. as a result,    (w, w, w ) =
m(m   1)(m   2)    (w (cid:62)aw )     n. so
the computational complexity of    (w, w, w ) depends on how we compute w (cid:62)aw . since a is a diagonal
matrix with at most m non-zero entries, w (cid:62)aw can be computed in o(m k2) operations. therefore, the time
complexity of computing    (w, w, w ) is o(m k2) per document.

(48), we need to add a

finally we handle those values with three indices the same, that is, i = j = k. as indicated by eq.
m(m   1)(m   2) term for compensation. this can be done ef   ciently by    rst computing
m(m   1)(m   2) ) for all the documents (requiring o(n v ) time), and then add them up, which takes o(v k3)

(cid:98)e(

2ni

1

2ni
operations.

