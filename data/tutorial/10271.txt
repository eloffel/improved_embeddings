4
1
0
2

 

y
a
m
1
2

 

 
 
]

g
l
.
s
c
[
 
 

3
v
3
0
2
6

.

2
1
3
1
:
v
i
x
r
a

spectral networks and deep locally connected

networks on graphs

joan bruna

new york university

bruna@cims.nyu.edu

wojciech zaremba
new york university

woj.zaremba@gmail.com

arthur szlam

the city college of new york
aszlam@ccny.cuny.edu

yann lecun

new york university
yann@cs.nyu.edu

abstract

convolutional neural networks are extremely ef   cient architectures in image and
audio recognition tasks, thanks to their ability to exploit the local translational
invariance of signal classes over their domain. in this paper we consider possi-
ble generalizations of id98s to signals de   ned on more general domains without
the action of a translation group.
in particular, we propose two constructions,
one based upon a hierarchical id91 of the domain, and another based on the
spectrum of the graph laplacian. we show through experiments that for low-
dimensional graphs it is possible to learn convolutional layers with a number of
parameters independent of the input size, resulting in ef   cient deep architectures.

1

introduction

convolutional neural networks (id98s) have been extremely succesful in machine learning prob-
lems where the coordinates of the underlying data representation have a grid structure (in 1, 2 and 3
dimensions), and the data to be studied in those coordinates has translational equivariance/invariance
with respect to this grid. speech [11], images [14, 20, 22] or video [23, 18] are prominent examples
that fall into this category.
on a regular grid, a id98 is able to exploit several structures that play nicely together to greatly
reduce the number of parameters in the system:

1. the translation structure, allowing the use of    lters instead of generic linear maps and

hence weight sharing.

2. the metric on the grid, allowing compactly supported    lters, whose support is typically

much smaller than the size of the input signals.

3. the multiscale dyadic id91 of the grid, allowing subsampling, implemented through

stride convolutions and pooling.

if there are n input coordinates on a grid in d dimensions, a fully connected layer with m outputs
requires n    m parameters, which in typical operating regimes amounts to a complexity of o(n2)
parameters. using arbitrary    lters instead of generic fully connected layers reduces the complexity
to o(n) parameters per feature map, as does using the metric structure by building a    locally con-
nected    net [8, 17]. using the two together gives o(k    s) parameters, where k is the number of
feature maps and s is the support of the    lters, and as a result the learning complexity is independent
of n. finally, using the multiscale dyadic id91 allows each succesive layer to use a factor of 2d
less (spatial) coordinates per    lter.

1

in many contexts, however, one may be faced with data de   ned over coordinates which lack some,
or all, of the above geometrical properties. for instance, data de   ned on 3-d meshes, such as
surface tension or temperature, measurements from a network of meteorological stations, or data
coming from social networks or collaborative    ltering, are all examples of structured inputs on which
one cannot apply standard convolutional networks. another relevant example is the intermediate
representation arising from deep neural networks. although the spatial convolutional structure can
be exploited at several layers, typical id98 architectures do not assume any geometry in the    feature   
dimension, resulting in 4-d tensors which are only convolutional along their spatial coordinates.
graphs offer a natural framework to generalize the low-dimensional grid structure, and by extension
the notion of convolution. in this work, we will discuss constructions of deep neural networks on
graphs other than regular grids. we propose two different constructions. in the    rst one, we show
that one can extend properties (2) and (3) to general graphs, and use them to de   ne    locally    con-
nected and pooling layers, which require o(n) parameters instead of o(n2). we term this the spatial
construction. the other construction, which we call spectral construction, draws on the properties
of convolutions in the fourier domain. in rd, convolutions are linear operators diagonalised by the
fourier basis exp(i    t),   , t     rd. one may then extend convolutions to general graphs by    nding
the corresponding    fourier    basis. this equivalence is given through the graph laplacian, an opera-
tor which provides an harmonic analysis on the graphs [1]. the spectral construction needs at most
o(n) paramters per feature map, and also enables a construction where the number of parameters is
independent of the input dimension n. these constructions allow ef   cient forward propagation and
can be applied to datasets with very large number of coordinates.

1.1 contributions

our main contributions are summarized as follows:

    we show that from a weak geometric structure in the input domain it is possible to obtain
ef   cient architectures using o(n) parameters, that we validate on low-dimensional graph
datasets.
    we introduce a construction using o(1) parameters which we empirically verify, and we

discuss its connections with an harmonic analysis problem on graphs.

2 spatial construction

the most immediate generalisation of id98 to general graphs is to consider multiscale, hierarchical,
local receptive    elds, as suggested in [3]. for that purpose, the grid will be replaced by a weighted
graph g = (   , w ), where     is a discrete set of size m and w is a m  m symmetric and nonnegative
matrix.

2.1 locality via w

the notion of locality can be generalized easily in the context of a graph. indeed, the weights in a
graph determine a notion of locality. for example, a straightforward way to de   ne neighborhoods
on w is to set a threshold    > 0 and take neighborhoods

n  (j) = {i         : wij >   } .

we can restrict attention to sparse       lters    with receptive    elds given by these neighborhoods to get
locally connected networks, thus reducing the number of parameters in a    lter layer to o(s    n),
where s is the average neighborhood size.

2.2 multiresolution analysis on graphs

id98s reduce the size of the grid via pooling and subsampling layers. these layers are possible
because of the natural multiscale id91 of the grid: they input all the feature maps over a cluster,
and output a single feature for that cluster. on the grid, the dyadic id91 behaves nicely with
respect to the metric and the laplacian (and so with the translation structure). there is a large
literature on forming multiscale id91s on graphs, see for example [16, 25, 6, 13]. finding

2

multiscale id91s that are provably guaranteed to behave well w.r.t. laplacian on the graph is
still an open area of research. in this work we will use a naive agglomerative method.
figure 1 illustrates a multiresolution id91 of a graph with the corresponding neighborhoods.

figure 1: undirected graph g = (   0, w ) with two levels of id91. the original points are
drawn in gray.

2.3 deep locally connected networks

the spatial construction starts with a multiscale id91 of the graph, similarly as in [3] we
consider k scales. we set    0 =    , and for each k = 1 . . . k, we de   ne    k, a partition of    k   1
into dk clusters; and a collection of neighborhoods around each element of    k   1:

nk = {nk,i ; i = 1 . . . dk   1} .

with these in hand, we can now de   ne the k-th layer of the network. we assume without loss of
generality that the input signal is a real signal de   ned in    0, and we denote by fk the number of
      lters    created at each layer k. each layer of the network will transform a fk   1-dimensional signal
indexed by    k   1 into a fk-dimensional signal indexed by    k, thus trading-off spatial resolution
with newly created feature coordinates.
more formally, if xk = (xk,i ; i = 1 . . . fk   1) is the dk   1    fk   1 is the input to layer k, its the
output xk+1 is de   ned as

      fk   1(cid:88)

i=1

       (j = 1 . . . fk) ,

xk+1,j = lkh

fk,i,jxk,i

(2.1)

where fk,i,j is a dk   1    dk   1 sparse matrix with nonzero entries in the locations given by nk, and
lk outputs the result of a pooling operation over each cluster in    k. this construcion is illustrated
in figure 2.
in the current code, to build    k and nk we use the following construction:

(cid:88)

(cid:88)

w0 = w

ak(i, j) =

wk   1(s, t) , (k     k)

s      k(i)

t      k(j)

wk = rownormalize(ak) , (k     k)
nk = supp(wk) . (k     k)

3

figure 2: spatial construction as described by (2.1), with k = 2. for illustration purposes, the
pooling operation is assimilated with the    ltering stage. each layer of the transformation loses
spatial resolution but increases the number of    lters.

1. this is just one amongst many strategies to perform
and    k is found as an   covering for wk
hierarchicial agglomerative id91. for a larger account of the problem, we refer the reader to
[10].
if sk is the average support of the neighborhoods nk, we verify from (2.1) that the number of
parameters to learn at layer k is

o(sk    |   k|    fk    fk   1) = o(n) .

in practice, we have sk   |   k|         |   k   1|, where    is the oversampling factor, typically        (1, 4).
the spatial construction might appear na    ve, but it has the advantage that it requires relatively weak
regularity assumptions on the graph. graphs having low intrinsic dimension have localized neigh-
borhoods, even if no nice global embedding exists. however, under this construction there is no
easy way to induce weight sharing across different locations of the graph. one possible option is to
consider a global embedding of the graph into a low dimensional space, which is rare in practice for
high-dimensional data.

3 spectral construction

the global structure of the graph can be exploited with the spectrum of its graph-laplacian to gen-
eralize the convolution operator.

3.1 harmonic analysis on weighted graphs
the combinatorial laplacian l = d     w or graph laplacian l = i     d   1/2w d   1/2 are gener-
alizations of the laplacian on the grid; and frequency and smoothness relative to w are interrelated
through these operators [2, 25]. for simplicity, here we use the combinatorial laplacian. if x is an
m-dimensional vector, a natural de   nition of the smoothness functional ||   x||2

w at a node i is

(cid:107)   x(cid:107)2

w (i) =

(cid:88)
(cid:88)

j

(cid:88)

wij[x(i)     x(j)]2,

wij[x(i)     x(j)]2 ,

and

(cid:107)   x(cid:107)2

w =

(3.1)

with this de   nition, the smoothest vector is a constant:
(cid:107)   x(cid:107)2

v0 = arg min

   
w = (1/

x   rm (cid:107)x(cid:107)=1

m)1m.

i

j

1an  -covering of a set     using a similarity kernel k is a partition p = {p1, . . . ,pn} such that

supn supx,x(cid:48)   pn k(x, x(cid:48))      .

4

each succesive

vi =

arg min

x   rm (cid:107)x(cid:107)=1 x   {v0,...,vi   1}

(cid:107)   x(cid:107)2

w

is an eigenvector of l, and the eigenvalues   i allow the smoothness of a vector x to be read off
from the coef   cients of x in [v0, ...vm   1], equivalently as the fourier coef   cients of a signal de   ned
in a grid. thus, just an in the case of the grid, where the eigenvectors of the laplacian are the
fourier vectors, diagonal operators on the spectrum of the laplacian modulate the smoothness of
their operands. moreover, using these diagonal operators reduces the number of parameters of a
   lter from m2 to m.
these three structures above are all tied together through the laplacian operator on the d-

dimensional grid    x =(cid:80)d

   2x
   u2
i

:

i=1

1. filters are multipliers on the eigenvalues of the laplacian    .
2. functions that are smooth relative to the grid metric have coef   cients with quick decay in

the basis of eigenvectors of    .

3. the eigenvectors of the subsampled laplacian are the low frequency eigenvectors of    .

3.2 extending convolutions via the laplacian spectrum

as in section 2.3, let w be a weighted graph with index set denoted by    , and let v be the eigen-
vectors of the graph laplacian l, ordered by eigenvalue. given a weighted graph, we can try to
generalize a convolutional net by operating on the spectrum of the weights, given by the eigenvec-
tors of its graph laplacian.
for simplicity, let us    rst describe a construction where each layer k = 1 . . . k transforms an input
vector xk of size |   |    fk   1 into an output xk+1 of dimensions |   |    fk, that is, without spatial
subsampling:

      v

fk   1(cid:88)

       (j = 1 . . . fk) ,

xk+1,j = h

fk,i,jv t xk,i

(3.2)

i=1

where fk,i,j is a diagonal matrix and, as before, h is a real valued nonlinearity.
often, only the    rst d eigenvectors of the laplacian are useful in practice, which carry the smooth
geometry of the graph. the cutoff frequency d depends upon the intrinsic regularity of the graph
and also the sample size. in that case, we can replace in (3.2) v by vd, obtained by keeping the    rst
d columns of v .
if the graph has an underlying group invariance this construction can discover it; the best example
being the standard id98; see 3.3. however, in many cases the graph does not have a group structure,
or the group structure does not commute with the laplacian, and so we cannot think of each    lter as
passing a template across     and recording the correlation of the template with that location.     may
not be homogenous in a way that allows this to make sense, as we shall see in the example from
section 5.1.
assuming only d eigenvectors of the laplacian are kept, equation (3.2) shows that each layer re-
quires fk   1    fk    d = o(|   |) paramters to train. we shall see in section 3.4 how the global and local
regularity of the graph can be combined to produce layers with o(1) parameters, i.e. such that the
number of learnable parameters does not depend upon the size of the input.
this construction can suffer from the fact that most graphs have meaningful eigenvectors only for
the very top of the spectrum. even when the individual high frequency eigenvectors are not mean-
ingful, a cohort of high frequency eigenvectors may contain meaningful information. however this
construction may not be able to access this information because it is nearly diagonal at the highest
frequencies.
finally, it is not obvious how to do either the forwardprop or the backprop ef   ciently while applying
the nonlinearity on the space side, as we have to make the expensive multiplications by v and v t ;
and it is not obvious how to do standard nonlinearities on the spectral side. however, see 4.1.

5

3.3 rediscovering standard id98   s

a simple, and in some sense universal, choice of weight matrix in this construction is the covariance
of the data. let x = (xk)k be the input data distribution, with xk     rn.
if each coordinate
j = 1 . . . n has the same variance,

j = e(cid:0)|x(j)     e(x(j))|2(cid:1) ,

  2

then diagonal operators on the laplacian simply scale the principal components of x. while this
may seem trivial, it is well known that the principal components of the set of images of a    xed size
are (experimentally) correspond to the discrete cosine transform basis, organized by frequency.
this can be explained by noticing that images are translation invariant, and hence the covariance
operator

  (j, j) = e ((x(j)     e(x(j)))(x(j(cid:48))     e(x(j(cid:48)))))

satis   es   (j, j(cid:48)) =   (j     j(cid:48)), hence it is diagonalized by the fourier basis. moreover, it is well

known that natural images exhibit a power spectrum e(|(cid:98)x(  )|2)          2, since nearby pixels are more

correlated than far away pixels. it results that principal components of the covariance are essentially
ordered from low to high frequencies, which is consistent with the standard group structure of the
fourier basis.
the upshot is that, when applied to natural images, the construction in 3.2 using the covariance as the
similarity kernel recovers a standard convolutional network, without any prior knowledge. indeed,
the linear operators v fi,jv t from eq (3.2) are by the previous argument diagonal in the fourier
basis, hence translation invariant, hence    classic    convolutions. moreover, section 4.1 explains how
spatial subsampling can also be obtained via dropping the last part of the spectrum of the laplacian,
leading to max-pooling, and ultimately to deep convolutonal networks.

3.4 o(1) construction with smooth spectral multipliers

in the standard grid, we do not need a parameter for each fourier function because the    lters are
compactly supported in space, but in (3.2), each    lter requires one parameter for each eigenvector
on which it acts. even if the    lters were compactly supported in space in this construction, we still
would not get less than o(n) parameters per    lter because the spatial response would be different at
each location.
one possibility for getting around this is to generalize the duality of the grid. on the euclidian grid,
the decay of a function in the spatial domain is translated into smoothness in the fourier domain, and
viceversa. it results that a funtion x which is spatially localized has a smooth frequency response
  x = v t x. in that case, the eigenvectors of the laplacian can be thought of as being arranged on a
grid isomorphic to the original spatial grid.
this suggests that, in order to learn a layer in which features will be not only shared across locations
but also well localized in the original domain, one can learn spectral multipliers which are smooth.
smoothness can be prescribed by learning only a subsampled set of frequency multipliers and using
an interpolation kernel to obtain the rest, such as cubic splines. however, the notion of smoothness
requires a geometry in the domain of spectral coordinates, which can be obtained by de   ning a dual

graph(cid:102)w as shown by (3.1). as previously discussed, on regular grids this geometry is given by the

notion of frequency, but this cannot be directly generalized to other graphs.
a particularly simple and navie choice consists in choosing a 1-dimensional arrangement, obtained
by ordering the eigenvectors according to their eigenvalues. in this setting, the diagonal of each    lter
fk,i,j (of size at most |   |) is parametrized by

diag(fk,i,j) = k   k,i,j ,

where k is a d    qk    xed cubic spline kernel and   k,i,j are the qk spline coef   cients. if one seeks
to have    lters with constant spatial support (ie, whose support is independent of the input size |   |),
it follows that one can choose a sampling step        |   | in the spectral domain, which results in a
constant number qk     |   |         1 = o(1) of coef   cients   k,i,j per    lter.
although results from section 5 seem to indicate that the 1-d arrangement given by the spectrum
of the laplacian is ef   cient at creating spatially localized    lters, a fundamental question is how to

6

a dual graph(cid:99)w by measuring the similarity of in the spectral domain: (cid:98)x = v t x. the similarity

de   ne a dual graph capturing the geometry of spectral coordinates. a possible algorithmic stategy is
to consider an input distribution x = (xk)k consisting on spatially localized signals and to construct
could be measured for instance with e((|  x|     e(|  x)|))t (|  x|     e(|  x|)).

4 relationship with previous work

there is a large literature on building wavelets on graphs, see for example [21, 7, 4, 5, 9]. a wavelet
basis on a grid, in the language of neural networks, is a linear autoencoder with certain provable
regularity properties (in particular, when encoding various classes of smooth functions, sparsity
is guaranteed). the forward propagation in a classical wavelet transform strongly resembles the
forward propagation in a neural network, except that there is only one    lter map at each layer (and
it is usually the same    lter at each layer), and the output of each layer is kept, rather than just
the output of the    nal layer. classically, the    lter is not learned, but constructed to facilitate the
regularity proofs.
in the graph case, the goal is the same; except that the smoothness on the grid is replaced by smooth-
ness on the graph. as in the classical case, most works have tried to construct the wavelets explicitly
(that is, without learning), based on the graph, so that the corresponding autencoder has the correct
sparsity properties. in this work, and the recent work [21], the       lters    are constrained by con-
struction to have some of the regularity properties of wavelets, but are also trained so that they are
appropriate for a task separate from (but perhaps related to) the smoothness on the graph. whereas
[21] still builds a (sparse) linear autoencoder that keeps the basic wavelet transform setup, this work
focuses on nonlinear constructions; and in particular, tries to build analogues of id98   s.
another line of work which is rellevant to the present work is that of discovering grid topologies
from data. in [19], the authors empirically con   rm the statements of section 3.3, by showing that
one can recover the 2-d grid structure via second order statistics. in [3, 12] the authors estimate
similarities between features to construct locally connected networks.

4.1 multigrid

we could improve both constructions, and to some extent unify them, with a multiscale id91
of the graph that plays nicely with the laplacian. as mentioned before, in the case of the grid,
the standard dyadic cubes have the property that subsampling the fourier functions on the grid to a
coarser grid is the same as    nding the fourier functions on the coarser grid. this property would
eliminate the annoying necessity of mapping the spectral construction to the    nest grid at each layer
to do the nonlinearity; and would allow us to interpret (via interpolation) the local    lters at deeper
layers in the spatial construction to be low frequency.
this kind of id91 is the underpinning of the multigrid method for solving discretized pde   s
(and linear systems in general) [24]. there have been several papers extending the multigrid method,
and in particular, the multiscale id91(s) associated to the multigrid method, in settings more
general than regular grids, see for example [16, 15] for situations as in this paper, and see [24] for the
algebraic multigrid method in general. in this work, for simplicity, we use a naive multiscale clus-
tering on the space side construction that is not guaranteed to respect the original graph   s laplacian,
and no explicit spatial id91 in the spectral construction.

5 numerical experiments

the previous constructions are tested on two variations of the mnist data set.
in the    rst, we
subsample the normal 28    28 grid to get 400 coordinates. these coordinates still have a 2-d
structure, but it is not possible to use standard convolutions. we then make a dataset by placing
d = 4096 points on the 3-d unit sphere and project random mnist images onto this set of points,
as described in section 5.2.
in all the experiments, we use recti   ed linear units as nonlinearities and max-pooling. we train
the models with cross-id178 loss, using a    xed learning rate of 0.1 with momentum 0.9.

7

(a)

(b)

figure 3: subsampled mnist examples.

5.1 subsampled mnist

we    rst apply the constructions from sections 3.2 and 2.3 to the subsampled mnist dataset. figure
3 shows examples of the resulting input signals, and figures 4, 5 show the hierarchical id91
constructed from the graph and some eigenfunctions of the graph laplacian, respectively. the per-
formance of various graph architectures is reported in table 1. to serve as a baseline, we compute
the standard nearest neighbor classi   er, which performs slightly worse than in the full mnist
dataset (2.8%). a two-layer fully connected neural network reduces the error to 1.8%. the geo-
metrical structure of the data can be exploited with the id98 graph architectures. local receptive
fields adapted to the graph structure outperform the fully connected network. in particular, two
layers of    ltering and max-pooling de   ne a network which ef   ciently aggregates information to
the    nal classi   er. the spectral construction performs slightly worse on this dataset. we consid-
ered a frequency cutoff of n/2 = 200. however, the frequency smoothing architecture described
in section 3.4, which contains the smallest number of parameters, outperforms the regular spectral
construction.
these results can be interpreted as follows. mnist digits are characterized by localized oriented
strokes, which require measurements with good spatial localization. locally receptive    elds are
constructed to explicitly satisfy this constraint, whereas in the spectral construction the measure-
ments are not enforced to become spatially localized. adding the smoothness constraint on the
spectrum of the    lters improves classi   cation results, since the    lters are enforced to have better
spatial localization.
this fact is illustrated in figure 6. we verify that locally receptive    elds encode different templates
across different spatial neighborhoods, since there is no global strucutre tying them together. on the
other hand, spectral constructions have the capacity to generate local measurements that generalize
across the graph. when the spectral multipliers are not constrained, the resulting    lters tend to be
spatially delocalized, as shown in panels (c)-(d). this corresponds to the fundamental limitation of
fourier analysis to encode local phenomena. however, we observe in panels (e)-(f) that a simple
smoothing across the spectrum of the graph laplacian restores some form of spatial localization
and creates    lters which generalize across different spatial positions, as should be expected for
convolution operators.

5.2 mnist on the sphere

we test in this section the graph id98 constructions on another low-dimensional graph.
in this
case, we lift the mnist digits to the sphere. the dataset is constructed as follows. we    rst sample
4096 random points s = {sj}j   4096 from the unit sphere s2     r3. we then consider an orthogonal
basis e = (e1, e2, e3) of r3 with (cid:107)e1(cid:107) = 1 , (cid:107)e2(cid:107) = 2 , (cid:107)e3(cid:107) = 3 and a random covariance operator
   = (e+w )t (e+w ), where w is a gaussian iid matrix with variance   2 < 1. for each signal xi
from the original mnist dataset, we sample a covariance operator   i from the former distribution
and consider its pca basis ui. this basis de   nes a point of view and in-plane rotation which we use

8

(a)

(b)

figure 4: clusters obtained with the agglomerative id91. (a) clusters corresponding to the
   nest scale k = 1, (b) clusters for k = 3 .

(a)

(b)

figure 5: examples of eigenfunctions of the graph laplacian v2, v20.

table 1: classi   cation results on mnist subsampled on 400 random locations, for different ar-
chitectures. fcn stands for a fully connected layer with n outputs, lrfn denotes the locally
connected construction from section 2.3 with n outputs, mpn is a max-pooling layer with n
outputs, and spn stands for the spectral layer from section 3.2.

method

nearest neighbors
400-fc800-fc50-10

400-lrf1600-mp800-10

400-lrf3200-mp800-lrf800-mp400-10

400-sp1600-10 (d1 = 300, q = n)
400-sp1600-10 (d1 = 300, q = 32)
400-sp4800-10 (d1 = 300, q = 20)

parameters error
n/a
4.11
3.6    105
1.8
7.2    104
1.8
1.6    105
1.3
3.2    103
2.6
1.6    103
2.3
5    103
1.8

9

(a)

(c)

(e)

(b)

(d)

(f)

figure 6: subsampled mnist learnt    lters using spatial and spectral construction. (a)-(b) two dif-
ferent receptive    elds encoding the same feature in two different clusters. (c)-(d) example of a    lter
obtained with the spectral construction. (e)-(f) filters obtained with smooth spectral construction.

to project xi onto s using bicubic interpolation. figure 7 shows examples of the resulting projected
digits. since the digits    6    and    9    are equivalent modulo rotations, we remove the    9    from the
dataset. figure 8 shows two eigenvectors of the graph laplacian.
we    rst consider    mild    rotations with   2 = 0.2. the effect of such rotations is however not
negligible. indeed, table 2 shows that the nearest neighbor classifer performs considerably worse
than in the previous example. all the neural network architectures we considered signi   catively
improve over this basic classi   er. furthermore, we observe that both convolutional constructions
match the fully connected constructions with far less parameters (but in this case, do not improve
its performance). figure 9 displays the    lters learnt using different constructions. again, we verify

10

table 2: classi   cation results on the mnist-sphere dataset generated using partial rotations, for
different architectures

method

nearest neighbors

4096-fc2048-fc512-9

4096-lrf4620-mp2000-fc300-9

4096-lrf4620-mp2000-lrf500-mp250-9

4096-sp32k-mp3000-fc300-9 (d1 = 2048, q = n)
4096-sp32k-mp3000-fc300-9 (d1 = 2048, q = 64)

parameters error
19
5.6
6
6.5
7
6

n/a
107
8    105
2    105
9    105
9    105

that the smooth spectral construction consistently improves the performance, and learns spatially
localized    lters, even using the naive 1-d organization of eigenvectors, which detect similar features
across different locations of the graph (panels (e)-(f)).
finally, we consider the uniform rotation case, where now the basis ui is a random basis of r3. in
that case, the intra-class variability is much more severe, as seen by inspecting the performance of the
nearest neighbor classi   er. all the previously described neural network architectures signi   cantly
improve over this classi   er, although the performance is notably worse than in the mild rotation
scenario. in this case, an ef   cient representation needs to be fully roto-translation invariant. since
this is a non-commutative group, it is likely that deeper architectures perform better than the models
considered here.

(a)

(b)

figure 7: examples of some mnist digits on the sphere.

(a)

(b)

figure 8: examples of eigenfunctions of the graph laplacian v20, v100

11

(a)

(c)

(e)

(b)

(d)

(f)

figure 9: filters learnt on the mnist-sphere dataset, using spatial and spectral construction. (a)-(b)
two different receptive    elds encoding the same feature in two different clusters. (c)-(d) example
of a    lter obtained with the spectral construction.
(e)-(f) filters obtained with smooth spectral
construction.

6 conclusion

using graph-based analogues of convolutional architectures can greatly reduce the number of pa-
rameters in a neural network without worsening (and often improving) the test error, while simul-
taneously giving a faster forward propagation. these methods can be scaled to data with a large
number of coordinates that have a notion of locality.
there is much to be done here. we suspect with more careful training and deeper networks we can
consistently improve on fully connected networks on    manifold like    graphs like the sampled sphere.

12

table 3: classi   cation results on the mnist-sphere dataset generated using uniformly random ro-
tations, for different architectures

method

nearest neighbors

4096-fc2048-fc512-9

4096-lrf4620-mp2000-fc300-9

4096-lrf4620-mp2000-lrf500-mp250-9

4096-sp32k-mp3000-fc300-9 (d1 = 2048, q = n)
4096-sp32k-mp3000-fc300-9 (d1 = 2048, q = 64)

parameters error
80
52
61
63
56
50

na
107
8    105
2    105
9    105
9    105

furthermore, we intend to apply these techniques to less arti   cal problems, for example, on net   ix
like recommendation problems where there is a biid91 of the data and coordinates. finally,
the fact that smoothness on the naive ordering of the eigenvectors leads to improved results and
localized    lters suggests that it may be possible to make    dual    constructions with o(1) parameters
per    lter in much more generality than the grid.

references
[1] mikhail belkin and partha niyogi. laplacian eigenmaps and spectral techniques for embed-

ding and id91. in nips, volume 14, pages 585   591, 2001.

[2] f. r. k. chung. spectral id207. american mathematical society.
[3] adam coates and andrew y ng. selecting receptive    elds in deep networks. in advances in

neural information processing systems, 2011.

[4] r.r. coifman and m. maggioni. diffusion wavelets. appl. comp. harm. anal., 21(1):53   94,

july 2006.

[5] mark crovella and eric d. kolaczyk. graph wavelets for spatial traf   c analysis. in infocom,

2003.

[6] inderjit s. dhillon, yuqiang guan, and brian kulis. weighted graph cuts without eigenvectors
a multilevel approach. ieee trans. pattern anal. mach. intell., 29(11):1944   1957, november
2007.

[7] matan gavish, boaz nadler, and ronald r. coifman. multiscale wavelets on trees, graphs
and high dimensional data: theory and applications to semi supervised learning. in johannes
frankranz and thorsten joachims, editors, icml, pages 367   374, 2010.

[8] karol gregor and yann lecun. emergence of complex-like cells in a temporal product net-

work with local receptive    elds. corr, abs/1006.0448, 2010.

[9] i. guskov, w. sweldens, and p. schr  oder. multiresolution signal processing for meshes. com-

puter graphics proceedings (siggraph 99), pages 325   334, 1999.

[10] trevor hastie, robert tibshirani, and jerome friedman. the elements of statistical learning:

data mining, id136 and prediction. springer, 2 edition, 2009.

[11] geoffrey e. hinton, li deng, dong yu, george e. dahl, abdel rahman mohamed, navdeep
jaitly, andrew senior, vincent vanhoucke, patrick nguyen, tara n. sainath, and brian kings-
bury. deep neural networks for acoustic modeling in id103: the shared views of
four research groups. ieee signal process. mag., 29(6):82   97, 2012.

[12] yangqing jia, chang huang, and trevor darrell. beyond spatial pyramids: receptive    eld
in id161 and pattern recognition (cvpr),

learning for pooled image features.
2012 ieee conference on, pages 3370   3377. ieee, 2012.

[13] george karypis and vipin kumar. metis - unstructured graph partitioning and sparse matrix

ordering system, version 2.0. technical report, 1995.

[14] alex krizhevsky, ilya sutskever, and geoff hinton. id163 classi   cation with deep con-
volutional neural networks. in advances in neural information processing systems 25, pages
1106   1114, 2012.

13

[15] d. kushnir, m. galun, and a. brandt. ef   cient multilevel eigensolvers with applications
to data analysis tasks. pattern analysis and machine intelligence, ieee transactions on,
32(8):1377   1391, 2010.

[16] dan kushnir, meirav galun, and achi brandt. fast multiscale id91 and manifold iden-
ti   cation. pattern recognition, 39(10):1876     1891, 2006.   ce:title  similarity-based pattern
recognition  /ce:title  .

[17] quoc v. le, jiquan ngiam, zhenghao chen, daniel chia, pang wei koh, and andrew y. ng.

tiled convolutional neural networks. in in nips, 2010.

[18] quoc v le, will y zou, serena y yeung, and andrew y ng. learning hierarchical invariant
spatio-temporal features for action recognition with independent subspace analysis. in com-
puter vision and pattern recognition (cvpr), 2011 ieee conference on, pages 3361   3368.
ieee, 2011.

[19] nicolas le roux, yoshua bengio, pascal lamblin, marc joliveau, bal  azs k  egl, et al. learning

the 2-d topology of images. in nips, 2007.

[20] y. lecun, l. bottou, y. bengio, and p. haffner. gradient-based learning applied to document

recognition. in intelligent signal processing, pages 306   351. ieee press, 2001.

[21] raif m. rustamov and leonidas guibas. wavelets on graphs via deep learning. in nips, 2013.
[22] pierre sermanet, soumith chintala, and yann lecun. convolutional neural networks applied
in international conference on pattern recognition

to house numbers digit classi   cation.
(icpr 2012), 2012.

[23] graham, w. taylor, rob fergus, yann lecun, and christoph bregler. convolutional learning
of spatio-temporal features. in proc. european conference on id161 (eccv   10),
2010.

[24] ulrich trottenberg and anton schuller. multigrid. academic press, inc., orlando, fl, usa,

2001.

[25] u. von luxburg. a tutorial on spectral id91. technical report 149, 08 2006.

14

