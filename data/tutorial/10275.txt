6
1
0
2

 
r
p
a
1
2

 

 
 
]

y
c
.
s
c
[
 
 

2
v
1
8
7
5
0

.

4
0
6
1
:
v
i
x
r
a

what we write about when we write about

causality: features of causal statements across

large-scale social discourse

thomas c. mcandrew      , joshua c. bongard      , christopher m. danforth      , peter s. dodds      ,

   department of mathematics and statistics, university of vermont, burlington, vt, united states

paul d. h. hines      and james p. bagrow      

   vermont complex systems center, university of vermont
   department of computer science, university of vermont

  school of engineering, university of vermont

abstract   identifying and communicating relationships be-
tween causes and effects is important for understanding our
world, but is affected by language structure, cognitive and
emotional biases, and the properties of
the communication
medium. despite the increasing importance of social media,
much remains unknown about causal statements made online.
to study real-world causal attribution, we extract a large-scale
corpus of causal statements made on the twitter social network
platform as well as a comparable random control corpus. we
compare causal and control statements using statistical language
and id31 tools. we    nd that causal statements
have a number of signi   cant lexical and grammatical differences
compared with controls and tend to be more negative in sentiment
than controls. causal statements made online tend to focus on
news and current events, medicine and health, or interpersonal
relationships, as shown by topic models. by quantifying the
features and potential biases of causality communication, this
study improves our understanding of the accuracy of information
and opinions found online.

keywords   social media; online social network; causal attri-

bution; natural language processing

i. introduction

social media and online social networks now provide vast
amounts of data on human online discourse and other activi-
ties [1], [2], [3], [4], [5], [6], [7]. with so much communication
taking place online and with social media being capable of
hosting powerful misinformation campaigns [8] such as those
claiming vaccines cause autism [9], [10], it is more important
than ever to better understand the discourse of causality and
the interplay between online communication and the statement
of cause and effect.

causal id136 is a crucial way that humans comprehend
the world, and it has been a major focus of philosophy,
statistics, mathematics, psychology, and the cognitive sciences.
philosophers such as hume and kant have long argued
whether causality is a human-centric illusion or the discovery
of a priori truth [11], [12]. causal id136 in science is in-
credibly important, and researchers have developed statistical
measures such as granger causality [13], mathematical and
probabilistic frameworks [14], [15], [16], [17], and id111
procedures [18], [19], [20] to better infer causal in   uence

from data. in the cognitive sciences, the famous perception
experiments of michotte et al. led to a long line of research
exploring the cognitive biases that humans possess when
attempting to link cause and effect [21], [22], [23].

how humans understand and communicate cause and effect
relationships is complicated, and is in   uenced by language
structure [24], [25], [26], [27] and sentiment or valence [28].
a key    nding is that the perceived emphasis or causal weight
changes between the agent (the grammatical construct respon-
sible for a cause) and the patient (the construct effected by
the cause) depending on the types of verbs used to describe
the cause and effect. researchers have hypothesized [29] that
this is because of the innate weighting property of the verbs
in the english language that humans use to attribute causes
and effects. another    nding is the role of a valence bias: the
volume and intensity of causal reasoning may increase due to
negative feedback or negative events [28].

despite these long lines of research, causal attributions
made via social media or online social networks have not been
well studied. the goal of this paper is to explore the language
and topics of causal statements in a large corpus of social
media taken from twitter. we hypothesize that language and
sentiment biases play a signi   cant role in these statements, and
that tools from natural language processing and computational
linguistics can be used to study them. we do not attempt to
study the factual correctness of these statements or offer any
degree of veri   cation, nor do we exhaustively identify and
extract all causal statements from these data. instead, here
we focus on statements that are with high certainty causal
statements, with the goal to better understand key characteris-
tics about causal statements that differ from everyday online
communication.

the rest of this paper is organized as follows: in sec. ii
we discuss our materials and methods, including the dataset
we studied, how we preprocessed that data and extracted a
   causal    corpus and a corresponding    control    corpus, and the
details of the statistical and language analysis tools we studied
these corpora with. in sec. iii we present results using these
tools to compare the causal statements to control statements.

we conclude with a discussion in sec. iv.

ii. materials and methods

dataset,    ltering, and corpus selection

data was collected from a 10% uniform sample of twitter
posts made during 2013, speci   cally the gardenhose api.
twitter activity consists of short posts called tweets which
are limited to 140 characters. retweets, where users repost a
tweet to spread its content, were not considered. (the spread
of causal statements will be considered in future work.) we
considered only english-language tweets for this study. to
avoid cross-language effects, we kept only tweets with a user-
reported language of    english    and, as a second constraint,
individual tweets needed to match more english stopwords
than any other language   s set of stopwords. stopwords con-
sidered for each language were determined using nltk   s
database [30]. a tweet will be referred to as a    document   
for the rest of this work.

all document text was processed the same way. punctua-
tion, xml characters, and hyperlinks were removed, as were
twitter-speci   c    at-mentions    and    hashtags    (see also the
appendix). there is useful information here, but it is either
not natural language text, or it is twitter-speci   c, or both.
documents were broken into individual words (unigrams) on
whitespace. casing information was retained, as we will use
it for our named entity analysis, but otherwise all words
were considered lowercase only (see also the appendix).
id30 [31] and lemmatization [32] were not performed.
causal documents were chosen to contain one occurrence
only of the exact unigrams:    caused   ,    causing   , or    causes   .
the word    cause    was not included due to its use as a popular
contraction for    because   . one    cause-word    per document
restricted the analysis to single relationships between two re-
lata. documents that contain bidirectional words (   associate   ,
   relate   ,    connect   ,    correlate   , and any of their stems) were
also not selected for analysis. this is because our focus is
on causality, an inherently one-sided relationship between
two objects. we also did not consider additional synonyms
of these cause words, although that could be pursued for
future work. control documents were also selected. these
documents did not contain any of    caused   ,    causing   , or
   causes   , nor any bidirectional words, and are further matched
temporally to obtain the same number of control documents as
causal documents in each    fteen-minute period during 2013.
control documents were otherwise selected randomly; causal
synonyms may be present. the end result of this procedure
identi   ed 965,560 causal and 965,560 control documents.
each of the three    cause-words   ,    caused   ,    causes   , and
   causing    appeared in 38.2%, 35.0%, and 26.8% of causal
documents, respectively.

tagging and corpus comparison

documents were further studied by annotating their uni-
grams with parts-of-speech (pos) and named entities (ne)
tags. id52 was done using nltk v3.1 [30] which im-
plements an averaged id88 classi   er [33] trained on the

brown corpus [34]. (id52 is affected by punctuation;
we show in the appendix that our results are relatively robust
to the removal of punctuation.) pos tags denote the nouns,
verbs, and other grammatical constructs present in a document.
id39 (ner) was performed using the 4-
class, distributional similarity tagger provided as part of the
stanford corenlp v3.6.0 toolkit [35]. ner aims to identify
and classify proper words in a text. the ne classi   cations
considered were: organization, location, person, and misc.
the stanford ner tagger uses a conditional random    eld
model [36] trained on diverse sets of manually-tagged english-
language data (conll-2003) [35]. conditional random    elds
allow dependencies between words so that    new york    and
   new york times   , for example, are classi   ed separately as
a location and organization, respectively. these taggers are
commonly used and often provide reasonably accurate results,
but there is always potential ambiguity in written text and
improving upon these methods remains an active area of
research.

comparing corpora: unigrams, pos, and nes were
compared between the cause and control corpora using odds
ratios (ors):

or(x) =

pc(x)/(1     pc(x))
pn (x)/(1     pn (x))

,

(1)

separately as p(x) = f (x)/(cid:80)

where pc(x) and pn (x) are the probabilities that a unigram,
pos, or ne x occurs in the causal and control corpus, re-
spectively. these probabilities were computed for each corpus
x(cid:48)   v f (x(cid:48)), where f (x) is the
total number of occurrences of x in the corpus and v is the
relevant set of unigrams, pos, or nes. con   dence intervals
for the ors were computed using wald   s methodology [37].
as there are many unique unigrams in the text, when
computing unigram ors we focused on the most meaningful
unigrams within each corpus by using the following    ltering
criteria: we considered only the ors of the 1500 most frequent
unigrams in that corpus that also have a term-frequency-
inverse-document-frequency (tf-idf) score above the 90th per-
centile for that corpus [38]. the tf-idf was computed as

tf-idf(w) = log f (w)    log

,

(2)

(cid:18) d

(cid:19)

df (w)

where d is the total number of documents in the corpus, and
df (w) is the number of documents in the corpus containing
unigram w. intuitively, unigrams with higher tf-idf scores ap-
pear frequently, but are not so frequent that they are ubiquitous
through all documents. filtering via tf-idf is standard practice
in the information retrieval and data mining    elds.

cause-trees

for a better understanding of the higher-order language
structure present in text phrases, cause-trees were constructed.
a cause-tree starts with a root cause word (either    caused   ,
   causing    or    causes   ),
then the two most probable words
following (preceding) the root are identi   ed. next, the root
word plus one of the top probable words is combined into

a bigram and the top two most probable words following
(preceding) this bigram are found. repeatedly applying this
process builds a binary tree representing the id165s that
begin with (terminate at) the root word. this process can
continue until a certain id165 length is reached or until there
are no more documents long enough to search.

id31

sentimental analysis was applied to estimate the emotional
content of documents. two levels of analysis were used: a
method where individual unigrams were given crowdsourced
numeric sentiment scores, and a second method involving a
trained classi   er that can incorporate document-level phrase
information.

for the    rst id31, each unigram w was as-
signed a crowdsourced    labmt    sentiment score s(w) [6].
(unlike [6], scores were recentered by subtracting the mean,
s(w)     s(w)     (cid:104)s(cid:105).) unigrams determined by volunteer
raters to have a negative emotional sentiment (   hate   ,   death   ,
etc.) have s(w) < 0, while unigrams determined to have
a positive emotional sentiment (   love   ,    happy   , etc.) tend
to have s(w) > 0. unigrams that have labmt scores and
are above the 90th percentile of tf-idf for the corpus form
the set   v . (unigrams in   v need not be among the 1500
most frequent unigrams.) the set   v captures 87.9% (91.5%)
of total unigrams in the causal (control) corpus. crucially,
the tf-idf    ltering ensures that the words    caused   ,    causes   ,
and    causing   , which have a slight negative sentiment, are
not included and do not introduce a systematic bias when
comparing the two corpora.

this sentiment measure works on a per-unigram basis, and
is therefore best suited for large bodies of text, not short
documents [6]. instead of considering individual documents,
the distributions of labmt scores over all unigrams for each
corpus was used to compare the corpora. in addition, a single
sentiment score for each corpus was computed as the average
sentiment score over all unigrams in that corpus, weighed by

unigram frequency:(cid:80)

(cid:46)(cid:80)

w(cid:48)      v f (w(cid:48)).

w      v f (w)s(w)

to supplement this id31 method, we applied a
second method capable of estimating with reasonable accuracy
the sentiment of individual documents. we used the sentiment
classi   er [39] included in the stanford corenlp v3.6.0 toolkit
to documents in each corpus. documents were individually
classi   ed into one of    ve categories: very negative, negative,
neutral, positive, very positive. the data used to train this
classi   er is taken from positive and negative reviews of movies
(stanford sentiment treebank v1.0) [39].

id96

lastly, we applied id96 to the causal corpus to
determine what are the topical foci most discussed in causal
statements. topics were built from the causal corpus using
id44 (lda) [40]. under lda each doc-
ument is modeled as a bag-of-words or unordered collection of
unigrams. topics are considered as mixtures of unigrams by
estimating conditional distributions over unigrams: p (w|t ),

the id203 of unigram w given topic t and documents are
considered as mixtures of topics via p (t|d), the id203
of topic t given document d. these distributions are then
found via statistical id136 given the observed distributions
of unigrams across documents. the total number of topics
is a parameter chosen by the practitioner. for this study we
used the mallet v2.0.8rc3 id96 toolkit [41] for
model id136. by inspecting the most probable unigrams
per topic (according to p (w|t )), we found 10 topics provided
meaningful and distinct topics.

iii. results

we have collected approximately 1m causal statements
made on twitter over the course of 2013, and for a control we
gathered the same number of statements selected at random but
controlling for time of year (see methods). we applied parts-
of-speech (pos) and named entity (ne) taggers to all these
texts. some post-processed and tagged example documents,
both causal and control, are shown in fig. 1a. we also applied
id31 methods to these documents (methods) and
we have highlighted very positive and very negative words
throughout fig. 1.

in fig. 1b we present odds ratios for how frequently
unigrams (words), pos, or ne appear in causal documents rel-
ative to control documents. the three unigrams most strongly
skewed towards causal documents were    stress   ,    problems   ,
and    trouble   , while the three most skewed towards control
documents were    photo   ,    ready   , and    cute   . while these are
only a small number of the unigrams present, this does imply
a negative sentiment bias among causal statements (we return
to this point shortly).

figure 1b also presents odds ratios for pos tags,

to
help us measure the differences in grammatical structure
between causal and control documents (see also the ap-
pendix for the effects of punctuation and casing on these
odds ratios). the causal corpus showed greater odds for
plural nouns (id32 tag: nns), plural proper nouns
(nnps), wh-determiners/pronouns (wdt, wp$) such as
   whichever   ,   whatever   ,    whose   , or    whosever   , and predeter-
miners (pdt) such as    all    or    both   . predeterminers quantify
noun phrases such as    all    in    after all the events that caused
you tears   , showing that many causal statements, despite the
potential brevity of social media, can encompass or delineate
classes of agents and/or patients. on the other hand, the causal
corpus has lower odds than the control corpus for list items
(ls), proper singular nouns (nnp), and interjections (uh).

lastly, fig. 1b contains odds ratios for ne tags, allowing
us to quantify the types of proper nouns that are more or less
likely to appear in causal statements. of the four tags, only the
   person    tag is less likely in the causal corpus than the control.
(this matches the odds ratio for the proper singular noun
discussed above.) perhaps surprisingly, these results together
imply that causal statements are less likely to involve individ-
ual persons than non-causal statements. there is considerable
celebrity news and gossip on social media [5]; discussions of
celebrities may not be especially focused on attributing causes

fig. 1. measuring the differences between causal and control documents. (a) examples of processed documents tagged by parts-of-speech (pos) or named
entities (nes). unigrams highlighted in red (yellow) are in the bottom 10% (top 10%) of the labmt sentiment scores. (b) log odds ratios with 95% wald
con   dence intervals for the most heavily skewed unigrams, pos, and all nes between the causal and control corpus. pos tags that are plural and use
wh-pronouns (that, what, which, ...) are more common in the causal corpus, while singular nouns and list items are more common in the controls. finally,
the    person    tag is the only ne less likely in the causal corpus. certain unigrams were censored for presentation only, not analysis. all shown odds ratios
were signi   cant at the    = 0.05 level except ls (list item markers). see also the appendix.

to these celebrities. all other ne tags, organization, location,
and miscellaneous, occur more frequently in the causal corpus
than the control. all the odds ratios in fig. 1b were signi   cant
at the    = 0.05 level except the list item marker (ls) pos
tag.

the unigram analysis in fig. 1 does not incorporate higher-
order phrase structure present in written language. to ex-
plore these structures speci   cally in the causal corpus, we
constructed    cause-trees   , shown in fig. 2. inspired by as-
sociation mining [42], a cause-tree is a binary tree rooted at
either    caused   ,    causes   , or    causing   , that illustrates the most
frequently occurring id165s that either begin or end with that
root cause word (see methods for details).

the    causes    tree shows the focused writing (sentence seg-
ments) that many people use to express either the relationship
between their own actions and a cause-and-effect (   even if it
causes   ), or the uncontrollable effect a cause may have on
themselves:    causes me to have    shows a person   s inability
to control a causal event (   [. . . ] i have central heterochromia
which causes me to have dual colors in both eyes   ). the
   causing    tree reveals our ability to con   ne causal patterns to
speci   c areas, and also our ability to be affected by others
causal decisions. phrases like    causing a scene in/at    and

   causing a ruckus in/at    (from documents like    causing a
ruckus in the hotel lobby typical [. . . ]   ) show people com-
monly associate bounds on where causal actions take place.
the causing tree also shows people   s tendency to emphasize
current negativity: phrases like    pain this is causing    coming
from documents like    cant you see the pain you are causing
her    supports the sentiment bias that causal attribution is
more likely for negative cause-effect associations. finally, the
   caused    tree focuses heavily on negative events and indicates
people are more likely to remember negative causal events.
documents with phrases from the caused tree (   [. . . ] appalling
tragedy [. . . ] that caused the death   ,    [. . . ] live with this pain
that you caused when i was so young [. . . ]   ) exemplify the
negative events that are focused on are large-scale tragedies
or very personal negative events in one   s life.

taken together, the popularity of negative sentiment uni-
grams (fig. 1) and id165s (fig. 2) among causal documents
shows that emotional sentiment or    valence    may play a role
in how people perform causal attribution [28]. the    if it
bleeds, it leads    mentality among news media, where violent
and negative news are more heavily reported, may appeal
to this innate causal association mechanism. (on the other
hand, many news media themselves use social media for

controlcauselog or(95% c.i.)unigramsstress3.43 ( 3.35, 3.51)problems3.29 ( 3.23, 3.35)trouble3.14 ( 3.06, 3.21)drama2.78 ( 2.70, 2.85)weight2.45 ( 2.38, 2.51)cancer2.40 ( 2.32, 2.47)brain2.25 ( 2.17, 2.33)death2.06 ( 2.00, 2.11)living1.98 ( 1.93, 2.04)major1.98 ( 1.89, 2.06)lose1.94 ( 1.89, 1.98)special1.90 ( 1.85, 1.96)which1.75 ( 1.72, 1.79)gonna-1.39 ( -1.43, -1.36)amazing-1.43 ( -1.49, -1.37)aint-1.45 ( -1.50, -1.41)omg-1.45 ( -1.50, -1.41)gotta-1.58 ( -1.63, -1.52)n******-1.63 ( -1.70, -1.57)wanna-1.68 ( -1.71, -1.63)bout-1.72 ( -1.79, -1.65)tomorrow-1.73 ( -1.78, -1.68)n****-1.73 ( -1.79, -1.68)cute-1.75 ( -1.81, -1.68)ready-1.89 ( -1.96, -1.82)photo-2.15 ( -2.21, -2.11)p.o.s.nnps1.21 ( 0.89, 1.53)wdt1.12 ( 1.10, 1.13)wp$0.70 ( 0.56, 0.84)pdt0.42 ( 0.40, 0.44)rbs0.38 ( 0.34, 0.41)nns0.32 ( 0.31, 0.32)vbp-0.37 ( -0.37, -0.36)fw-0.66 ( -0.70, -0.62)''-0.76 ( -1.08, -0.42)uh-0.88 ( -0.92, -0.84)nnp-0.93 ( -0.98, -0.89)ls-1.83 ( -4.02, 0.36)n.e.organization1.09 ( 1.09, 1.10)location0.54 ( 0.53, 0.55)misc0.38 ( 0.37, 0.39)person-0.11 ( -0.11, -0.10)thedtbiebeid56pfamilynnakavbzthedtcutestjjsfamilynnthisdtonennproblemnnhasvbzcausedvbnsorbmuchjjshurtnnandccpainnn[...]freezingvbgrainnncausesnnsthousandsnnstotolosevbpoweid56acrossin southernjjontarionnheprpgonmdplayvbuntilinheprpwinsvbzmorerbrorccuntilinheprpcantvbnomojj[...]innthinkvbpthedtpsnnisvbzadtamazingjjproductnnitsprp$worthjjbuyingnnlondonsoappolootheatreocollapsescausinginjurieseyewitnesseshavedescribedthechaosandpanickevinpwehavehadtonsofsnowinbowermanvillewecouldhelptheslopesatbluelmountainlwdt/ wp: wh-determiner/pronoun(n/nn)+(s/p): noun plural/properpdt:predeterminerjjs: superlative adjectivemd: modalfw:foreign wordls:list item markero:organizationl: locationp: personinnsactuallyrbhatevbpdramannitprpcausesvbzsorbmuchjjunnecessaryjjstressnnapart of speech (p.o.s)named entities (n.e.)m: miscellaneous012-1-2bfig. 2.
   cause-trees    containing the most probable id165s terminating at (left) or beginning with (right) a chosen root cause-word (see methods). line
widths are log proportional to their corresponding id165 frequency and bar plots measure the 4-gram per-document rate n (4-gram)/d. most trees express
negative sentiment consistent with the unigram analysis (fig. 1). the    causes    tree shows (i) people think in terms of causal id203 (   you know what
causes [. . . ]   ), and (ii) people use causal language when they are directly affected or being affected by another (   causes you   ,    causes me   ). the    causing    tree
is more global (   causing a ruckus/scene   ) and ego-centric (   pain you are causing   ). the    caused    tree focuses on negative sentiment and alludes to humans
retaining negative causal thoughts in the past.

reporting.) the prevalence of negative sentiment also contrasts
with the    better angels of our nature    evidence of pinker [43],
illustrating one bias that shows why many    nd the results of
ref. [43] surprising.

given this apparent sentiment skew, we further studied
sentiment (fig. 3). we compared the sentiment between the
corpora in four different ways to investigate the observation
(figs. 1b and 2) that people focus more about negative
concepts when they discuss causality. first, we computed the
mean sentiment score of each corpus using crowdsourced
   labmt    scores weighted by unigram frequency (see meth-
ods). we also applied tf-idf    ltering (methods) to exclude
very common words, including the three cause-words, from
the mean sentiment score. the causal corpus text was slightly
negative on average while the control corpus was slightly
positive (fig. 3a). the difference in mean sentiment score
was signi   cant (t-test: p < 0.01).

second, we moved from the mean score to the distribution
of sentiment across all (scored) unigrams in the causal and
control corpora (fig. 3b). the causal corpus contained a large
group of negative sentiment unigrams, with labmt scores in
the approximate range    3 < s <    1/2; the control corpus
had signi   cantly fewer unigrams in this score range.

third, in fig. 3c we used pos tags to categorize scored
unigrams into nouns, verbs, and adjectives. studying the
distributions for each, we found that nouns explain much
of the overall difference observed in fig. 3b, with verbs
showing a similar but smaller difference between the two

corpora. adjectives showed little difference. the distributions
in fig. 3c account for 87.8% of scored text in the causal
corpus and 77.2% of the control corpus. the difference in
sentiment between corpora was signi   cant for all distributions
(t-test: p < 0.01).

fourth, to further con   rm that the causal documents tend
toward negative sentiment, we applied a separate, indepen-
dent id31 using the stanford nlp sentiment
toolkit [39] to classify the sentiment of individual documents
not unigrams (see methods). instead of a numeric sentiment
score, this classi   er assigns documents to one of    ve cat-
egories ranging from very negative to very positive. the
classi   er showed that the causal corpus contains more negative
and very negative documents than the control corpus, while
the control corpus contains more neutral, positive, and very
positive documents (fig. 3d).

we have found language (figs. 1 and 2) and sentiment
(fig. 3) differences between causal statements made on social
media compared with other social media statements. but
what is being discussed? what are the topical foci of causal
statements? to study this, for our last analysis we applied
topic models to the causal statements. id96    nds
groups of related terms (unigrams) by considering similarities
between how those terms co-occur across a set of documents.
we used the popular id96 method latent dirichlet
allocation (lda) [40]. we ranked unigrams by how strongly
associated they were with the topic. inspecting these unigrams
we found that a 10-topic model discovered meaningful topics.

totocausespickifyouthewhattoloseespeciallyhaveitbutlovepainknowsomethingiseventosoanditbethisyoubemewhataretooutthatismuchinthisatruckussceneispaininbemanycausingthingsahavesopeoplepainthatandyoumeatpeoplethedealthelotthatgoodinpainoneandcausedthatthestrongmoreaofofproblemdeathpaininhomicidemuchyouwomantheonlycausescausedcausing10-610-510-410-310-610-510-410-310-610-510-410-310-610-510-410-310-610-510-410-310-610-510-410-3id31 revealed differences between the causal and control corpora. (a) the mean unigram sentiment score (see methods), computed
fig. 3.
from crowdsourced    labmt    scores [6], was more negative for the causal corpus than for the control. this held whether or not tf-idf    ltering was applied. (b)
the distribution of unigram sentiment scores for the two corpora showed more negative unigrams (with scores in the approximate range    3 < s <    1/2)
in the causal corpus compared with the control corpus. (c) breaking the sentiment distribution down by parts-of-speech, nouns show the most pronounced
difference in sentiment between cause and control; verbs and adjectives are also more negative in the causal corpus than the control but with less of a difference
than nouns. pos tags corresponding to nouns, verbs, and adjectives together account for 87.8% and 77.2% of the causal and control corpus text, respectively.
(d) applying a different id31 tool   a trained sentiment classi   er [39] that assigns individual documents to one of    ve categories   the causal
corpus had an overabundance of negative sentiment documents and fewer positive sentiment documents than the control. this shift from very positive to very
negative documents further supports the tendency for causal statements to be negative.

see methods for full details. the top unigrams for each topic
are shown in tab. i.

topics in the causal corpus tend to fall into three main
categories: (i) news, covering current events, weather, etc.;
(ii) medicine and health, covering cancer, obesity, stress, etc.;
and (iii) relationships, covering problems, stress, crisis, drama,
sorry, etc.

while the topics are quite different, they are all similar in
their use of negative sentiment words. the negative/global fea-
tures in the    news    topic are captured in the most representative
words: damage,    re, power, etc. similar to news, the    accident   
topic balances the more frequent day-to-day minor frustrations
with the less frequent but more severe impacts of car accidents.
the words    traf   c    and    delays    are the most probable words
for this topic, and are common, low-impact occurrences. on
the contrary,    crash   ,    car   ,    accident    and    death    are the next
most probable words for the accident topic, and generally show
a focus on less-common but higher-impact events.

the    medical    topic also focused on negative words; highly
probable words for this topic included    cancer   ,    break   ,    dis-
ease   ,    blood   , etc. meanwhile, the    body    topic contained
words like:    stress   ,    lose   , and    weight   , giving a focus on on
our more personal struggles with body image. besides body
image, the    injuries    topic uses speci   c pronouns (   his   ,    him   ,
   her   ) in references to a person   s own injuries or the injuries
of others such as athletes.

aside from more factual information, social information is
well represented in causal statements. the    problems    topic
shows people attribute their problems to many others with
terms like:    dont   ,    people   ,    they   ,    them   . the    stress    topic
also uses general words such as    more   ,    than   , or    people   
to link stress to all people, and in the same vein, the    crisis   
topic focuses on problems within organizations such as gov-
ernments. the    drama    and    sorry    topics tend towards more
speci   c causal statements. drama used the words:    like   ,    she   ,
and    her    while documents in the sorry topic tended to address
other people.

the topics of causal documents discovered by lda showed
that both general and speci   c statements are made regarding
news, medicine, and relationships when individuals make
causal attributions online.

iv. discussion

the power of online communication is the speed and ease
with which information can be propagated by potentially any
connected users. yet these strengths come at a cost: rumors
and misinformation also spread easily. causal misattribution
is at
the heart of many rumors, conspiracy theories, and
misinformation campaigns.

given the central role of causal statements, further studies
of the interplay of information propagation and online causal
attributions are crucial. are causal statements more likely to

5432101234sentiment,0.00.20.40.60.81.0controlcause10-310-210-1100proportion of documentsvery neg.negativeneutralpositivevery pos.4202noun0.00.20.40.60.81.04202verb4202adjectivecausecontrolfilterednot filteredabdcmeanse-0.1054.562   10-4meanse-0.1572.819   10-40.1114.180   10-43.357   10-40.116 symptoms

problems
government

   accident   

   problems   

   medical   

   crisis   

   sorry   

   stress   

   drama   

   injuries   

   news   
damage

   re
power

via
new
news
from
says
after
video
global
rain

warming

water

explosion

outage
storm
change
house
may

   ooding

gas
air
say
stir
heavy
weather
collapse
climate
death
deaths
home
oil

massive
attack
blast
two
city
into
state

traf   c
delays
crash
car

accident
death
between

after
year
down
there
man
due
from
snow
road
old
over

problems

chaos
morning

two

driving
major
today

disruption

train

accidents
almost
into
driver
police
until
delay

congestion

school
late

weather

been
earlier

dont
people
they

problems

why
about
when
know
them
like
drama
who
one
youre
get
stop
think
how
sh*t
want
cant

too
hate
need
only
really
many
even
then

because

someone

their
away
always

feel
thats
say
thing

something
yourself

cancer
break
some
men
can

disease
from
most
our
others
loss
heart
health
food

hair

women
blood
how
skin

records
adversity

high
helium
which
eating
may
body

smoking

own
acne
death
brain
alcohol
common
deaths
news

treatment
unknown
damage

problems

can
now
trouble

any
never
been
sorry
there
know
will
ive
they
out

see
one
how
would
could
were
had
ever
whats
again
did
time
think
well

ill
still
about
sure
hope
get

youve
thats
day
some
good

something

more
than
being
over
person
sleep
which
people

one
stress

someone
makes
think
when
most

thinking

brain

depression

anxiety

lack
night
without

love
mental
them
mind
fact

insomnia

hand
even
feel

physical
emotional
become

can
too
less
same
often
keeps

   body   
stress
lose
weight
stuff
living
quickly
special
proof
diets

excercise

f*ck
who
giving
love
god
people
will
our
those
one

around

life
his
thats
work
out
good
sex
come
great
say
back
when
give
their
them
things
goes
comes

too

their
our
from
how
about
social
crisis
via
great
money
many
issues
should
war

true
new
world
they
media
other
obama
   nancial
change
violence

will
also

shutdown

issue
support

kids

problem

poor
free
says
pay

against
party

confusion

table i

like
she
her
lol
out

trouble
good
now
sh*t
life

twitter

got
scene
get
too
girl
haha
needs
see
walk
drama
hes

woman
some
last
strong
shes
always
him
ways
little

because

said
really
thats
here
man
ass
night
ego

his
him
out
back
her
when
head
into
off
well
which
from
down
game
fall
face
then
get

had
sports
stick
over
while
eyes
only
hit

injuries

famous
hockey
right
left
injury
got
room

involvement
innumerable

time
play
run

because

topical foci of causal documents. each column lists the unigrams most highly associated (in descending order) with a topic,
computed from a 10-topic id44 model. the topics generally fall into three broad categories: news,

medicine, and relationships. many topics place an emphasis on negative sentiment terms. topic names were determined manually.

words are highlighted according to sentiment score as in fig. 1.

spread online and, if so, in which ways? what types of social
media users are more or less likely to make causal statements?
will a user be more likely to make a causal statement if they
have recently been exposed to one or more causal statements
from other users?

the topics of causal statements also bring forth important
questions to be addressed: how timely are causal statements?
are certain topics always being discussed in causal state-
ments? are there causal topics that are very popular for only
brief periods and then forgotten? temporal dynamics of causal
statements are also interesting: do time-of-day or time-of-year
factors play a role in how causal statements are made?

our work here focused on a limited subset of causal
statements, but more generally, these results may inform new
methods for automatically detecting causal statements from
unstructured, natural language text [18]. better computational
tools focused on causal statements are an important step
towards further understanding misinformation campaigns and
other online activities. lastly, an important but deeply chal-
lenging open question is how, if it is even possible, to validate
the accuracy of causal statements. can causal statements be
ranked by some con   dence metric(s)? we hope to pursue these

and other questions in future research.

appendix

parts-of-speech tagging depends on punctuation and casing,
which we    ltered in our data, so a study of how robust
the pos algorithm is to punctuation and casing removal is
important. we computed pos tags for the corpora with and
without casing as well as with and without punctuation (which
includes hashtags, links and at-symbols). two tags mentioned
in fig. 1b, nnps and ls (which was not signi   cant), were
affected by punctuation removal. otherwise, there is a strong
correlation (fig. 4) between odds ratios (causal vs. control)
with punctuation and without punctuation, including casing
and without casing (   = 0.71 and 0.80, respectively), indicat-
ing the pos differences between the corpora were primarily
not due to the removal of punctuation or casing.

acknowledgments

we thank r. gallagher for useful comments and gratefully
acknowledge the resources provided by the vermont advanced
computing core. this material is based upon work supported
by the national science foundation under grant no. iss-
1447634.

[19] c. pechsiri and a. kawtrakul,    mining causality from texts for
id53 system,    ieice transactions on information
and systems, vol. 90, no. 10, pp. 1523   1533, 2007.

[20] h. d. kim, m. castellanos, m. hsu, c. zhai, t. rietz, and d. diermeier,
   mining causal topics in text data: iterative id96 with
time series feedback,    in proceedings of the 22nd acm international
conference on conference on information & knowledge management.
acm, 2013, pp. 885   890.

[21] m. rolfs, m. dambacher, and p. cavanagh,    visual adaptation of the
perception of causality,    current biology, vol. 23, no. 3, pp. 250   254,
2013.

[22] b. j. scholl and p. d. tremoulet,    perceptual causality and animacy,   

trends in cognitive sciences, vol. 4, no. 8, pp. 299   309, 2000.

[23] r. joynson,    michotte   s experimental methods,    british journal of

psychology, vol. 62, no. 3, pp. 293   302, 1971.

[24] h. h. kelley,    attribution theory in social psychology.    in nebraska

symposium on motivation. university of nebraska press, 1967.

[25] s. e. taylor and s. t. fiske,    point of view and perceptions of
causality.    journal of personality and social psychology, vol. 32, no. 3,
p. 439, 1975.

[26] h. h. kelley and j. l. michela,    attribution theory and research,   

annual review of psychology, vol. 31, no. 1, pp. 457   501, 1980.

[27] d. j. hilton,    conversational processes and causal explanation,    psy-

chological bulletin, vol. 107, no. 1, p. 65, 1990.

[28] g. bohner, h. bless, n. schwarz, and f. strack,    what triggers causal
attributions? the impact of valence and subjective id203,    european
journal of social psychology, vol. 18, no. 4, pp. 335   345, 1988.

[29] r. brown and d. fish,    the psychological causality implicit in lan-

guage,    cognition, vol. 14, no. 3, pp. 237   273, 1983.

[30] s. bird,    nltk: the natural language toolkit,    in proceedings of the
coling/acl on interactive presentation sessions. association for
computational linguistics, 2006, pp. 69   72.

[31] j. b. lovins, development of a id30 algorithm. mit information

processing group, electronic systems laboratory cambridge, 1968.

[32] j. plisson, n. lavrac, d. mladenic et al.,    a rule based approach to

word lemmatization,    proceedings of is-2004, pp. 83   86, 2004.

[33] d. m. tax, m. van breukelen, r. p. w. duin, and j. kittler,    combining
multiple classi   ers by averaging or by multiplying?    pattern recognition,
vol. 33, no. 9, pp. 1475   1485, 2000.

[34] w. n. francis and h. kucera,    brown corpus manual,    brown univer-

sity, 1979.

[35] c. d. manning, m. surdeanu, j. bauer, j. finkel, s. j. bethard, and
d. mcclosky,    the stanford corenlp natural language processing
toolkit,    in acl (system demonstrations), 2014, pp. 55   60.

[36] j. r. finkel, t. grenager, and c. manning,    incorporating non-local
information into information extraction systems by id150,   
in proceedings of the 43rd annual meeting on association for compu-
tational linguistics. association for computational linguistics, 2005,
pp. 363   370.

[37] a. agresti and m. kateri, categorical data analysis. springer, 2011.
[38] d. klein and c. d. manning,    fast exact id136 with a factored
model for natural language parsing,    in advances in neural infor-
mation processing systems 15: proceedings of the 2002 conference,
vol. 15. mit press, 2003, p. 3.

[39] r. socher, a. perelygin, j. y. wu, j. chuang, c. d. manning, a. y. ng,
and c. potts,    recursive deep models for semantic compositionality
over a sentiment treebank,    in proceedings of
the conference on
empirical methods in natural language processing (emnlp), vol. 1631.
citeseer, 2013, p. 1642.

[40] d. m. blei, a. y. ng, and m. i. jordan,    id44,   
the journal of machine learning research, vol. 3, pp. 993   1022, 2003.
[41] a. k. mccallum,    mallett: a machine learning for language

[42] r. agrawal, t. imieli  nski, and a. swami,    mining association rules
between sets of items in large databases,    acm sigmod record,
vol. 22, no. 2, pp. 207   216, 1993.

[43] s. pinker, the better angels of our nature: why violence has declined.

toolkit,    2002.

penguin, 2011.

fig. 4. comparison of odds ratios for all parts-of-speech (pos) tags with
punctuation retained and removed for documents with and without casing.
tags cardinal number (cd), list item marker (ls), and proper noun plural
(nnps) were most affected by removing punctuation.

references

[1] d. lazer, a. s. pentland, l. adamic, s. aral, a. l. barabasi, d. brewer,
n. christakis, n. contractor, j. fowler, m. gutmann et al.,    life in the
network: the coming age of computational social science,    science (new
york, ny), vol. 323, no. 5915, p. 721, 2009.

[2] s. asur and b. a. huberman,    predicting the future with social media,   
in web intelligence and intelligent agent technology (wi-iat), 2010
ieee/wic/acm international conference on, vol. 1.
ieee, 2010, pp.
492   499.

[3] a. m. kaplan and m. haenlein,    users of the world, unite! the chal-
lenges and opportunities of social media,    business horizons, vol. 53,
no. 1, pp. 59   68, 2010.

[4] a. pak and p. paroubek,    twitter as a corpus for id31

and opinion mining.    in lrec, vol. 10, 2010, pp. 1320   1326.

[5] s. wu, j. m. hofman, w. a. mason, and d. j. watts,    who says what to
whom on twitter,    in proceedings of the 20th international conference
on world wide web. acm, 2011, pp. 705   714.

[6] p. s. dodds, k. d. harris, i. m. kloumann, c. a. bliss, and c. m.
danforth,    temporal patterns of happiness and information in a global
social network: hedonometrics and twitter,    plos one, vol. 6, no. 12,
p. e26752, 2011.

[7] l. mitchell, m. r. frank, k. d. harris, p. s. dodds, and c. m.
danforth,    the geography of happiness: connecting twitter sentiment
and expression, demographics, and objective characteristics of place,   
plos one, vol. 8, no. 5, p. e64417, 2013.

[8] j. ratkiewicz, m. conover, m. meiss, b. gonc  alves, a. flammini, and
f. menczer,    detecting and tracking political abuse in social media,   
in icwsm, 2011.

[9] m. salath  e and s. khandelwal,    assessing vaccination sentiments with
online social media: implications for infectious disease dynamics and
control,    plos comput biol, vol. 7, no. 10, p. e1002199, 2011.

[10] h. j. larson, l. z. cooper, j. eskola, s. l. katz, and s. ratzan,
   addressing the vaccine con   dence gap,    the lancet, vol. 378, no. 9790,
pp. 526   535, 2011.

[11] d. hume, a treatise of human nature. courier corporation, 2012.
[12] i. kant and p. guyer, critique of pure reason. cambridge university

press, 1998.

[13] c. w. j. granger,    investigating causal relations by econometric
models and cross-id106,    econometrica: journal of the
econometric society, pp. 424   438, 1969.

[14] d. b. rubin,    causal id136 using potential outcomes: design,
modeling, decisions,    journal of the american statistical association,
2011.

[15] j. s. sekhon,    the neyman-rubin model of causal id136 and
estimation via matching methods,    the oxford handbook of political
methodology, pp. 271   299, 2008.

[16] c. e. frangakis and d. b. rubin,    principal strati   cation in causal

id136,    biometrics, vol. 58, no. 1, pp. 21   29, 2002.
[17] j. pearl, causality. cambridge university press, 2009.
[18] r. girju, d. moldovan et al.,    id111 for causal relations,    in

flairs conference, 2002, pp. 360   364.

3210123log or (no punc.)3210123log or (punc.)lscdnnpspearson    = 0.60without casing3210123log or (no punc.)3210123log or (punc.)lscdnnpspearson    = 0.71with casing