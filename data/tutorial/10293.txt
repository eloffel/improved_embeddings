   iframe: [1]https://www.googletagmanager.com/ns.html?id=gtm-tdgjhk

   [2]skip to content

   advertisement
   [3]advertisement
   [4]bmc
   part of springer nature
   (button) menu
     * [5]explore journals
     * [6]get published
     * [7]about bmc

     * (button) search
     * [8]login my account

   search all bmc articles
   ____________________
   (button) search

   [9]bmc bioinformatics

   (button) menu
     * [10]home
     * [11]about
     * [12]articles
     * [13]submission guidelines

   we'd love your feedback. please complete this 3 question [14]survey.

table of contents

    1. [15]abstract
    2. [16]background
    3. [17]implementation
    4. [18]results and discussion
    5. [19]conclusions
    6. [20]availability and requirements
    7. [21]declarations
    8. [22]references

     * software
     * open access

gimli: open source and high-performance biomedical name recognition

     * david campos^[23]1[24]email author,
     * s  rgio matos^[25]1 and
     * jos   lu  s oliveira^[26]1

   bmc bioinformatics201314:54

   [27]https://doi.org/10.1186/1471-2105-14-54

       campos et al.; licensee biomed central ltd. 2013
     * received: 11 january 2012
     * accepted: 29 december 2012
     * published: 15 february 2013

[28]abstract

background

   automatic recognition of biomedical names is an essential task in
   biomedical information extraction, presenting several complex and
   unsolved challenges. in recent years, various solutions have been
   implemented to tackle this problem. however, limitations regarding
   system characteristics, customization and usability still hinder their
   wider application outside id111 research.

results

   we present gimli, an open-source, state-of-the-art tool for automatic
   recognition of biomedical names. gimli includes an extended set of
   implemented and user-selectable features, such as orthographic,
   morphological, linguistic-based, conjunctions and dictionary-based. a
   simple and fast method to combine different trained models is also
   provided. gimli achieves an f-measure of 87.17% on genetag and 72.23%
   on jnlpba corpus, significantly outperforming existing open-source
   solutions.

conclusions

   gimli is an off-the-shelf, ready to use tool for named-entity
   recognition, providing trained and optimized models for recognition of
   biomedical entities from scientific text. it can be used as a command
   line tool, offering full functionality, including training of new
   models and customization of the feature set and model parameters
   through a configuration file. advanced users can integrate gimli in
   their id111 workflows through the provided library, and extend or
   adapt its functionalities. based on the underlying system
   characteristics and functionality, both for final users and developers,
   and on the reported performance results, we believe that gimli is a
   state-of-the-art solution for biomedical ner, contributing to faster
   and better research in the field. gimli is freely available at
   [29]http://bioinformatics.ua.pt/gimli.

[30]keywords

     * conditional random field
     * name entity recognition
     * biomedical domain
     * command line interface
     * id33

[31]background

   in the biomedical field, a growing amount of data is continuously being
   produced, resulting largely from the widespread application of
   high-throughput techniques, such as gene and protein analysis. this
   growth is accompanied by a corresponding increase of textual
   information, in the form of articles, books and technical reports. in
   order to organize and manage these data, several manual curation
   efforts have been set up to identify, in texts, information regarding
   entities (e.g., genes and proteins) and their interactions (e.g.,
   protein-protein). the extracted information is stored in structured
   knowledge resources, such as medline, swiss-prot and genbank. however,
   manual annotation of large quantities of data is a very demanding and
   expensive task, making it difficult to keep these databases up-to-date.
   these factors have naturally led to increasing interest in the
   application of id111 (tm) systems to help perform those tasks.

   one major focus of tm research has been on id39
   (ner), a crucial initial step in information extraction, aimed at
   identifying chunks of text that refer to specific entities of interest.
   several ner systems have been developed for the biomedical domain,
   using different approaches and techniques that can generally be
   categorized as being based on rules, dictionary matching or machine
   learning (ml). in this study we follow an ml approach, the goal being
   to train statistical models focused on recognizing specific entity
   names, using a feature-based representation of the observed data. this
   presents various advantages over other approaches, such as the
   recognition of new and short entity names. moreover, ml solutions have
   been shown to achieve the best results for this specific domain.

   various techniques for adapting and optimizing ml-based solutions have
   been proposed in recent years. these can be categorized into one of the
   following sub-tasks: pre-processing, feature extraction, modelling, and
   post-processing. in the initial step, the input data is pre-processed
   to make it readable by computers and to simplify the recognition
   process. this sub-task is one of the most important, since every single
   action will affect the entire system behaviour. tokenisation is a
   mandatory step, in order to divide natural language texts into discrete
   and meaningful units. there are several approaches to implement it,
   depending on the input data and desired output. for instance, tsuruoka
   et al. [[32]1] keep words that contain a dash as a single token, while
   leaman and gonzalez [[33]2] create multiple tokens for the same word.

   in the feature extraction step, it is important to obtain features that
   reflect the different characteristics of the sentences and tokens. at
   the token level, orthographic [[34]2-[35]5] and morphological [[36]1,
   [37]4, [38]6, [39]7] features are commonly used in order to extract
   token formation patterns. it is also common to encode domain knowledge
   as features [[40]2, [41]8] using external resources, such as lexicons
   of gene and protein names. at the sentence level, linguistic [[42]2,
   [43]6, [44]9] and local context features [[45]1, [46]3, [47]5, [48]10],
   such as windows and conjunctions of features, are used to model the
   links between tokens.

   the ultimate goal is to model the observed data using the features
   extracted in the previous step, thus creating a probabilistic
   description of the data classes. this task is accomplished using ml
   models, which can be classified as being supervised or semi-supervised,
   depending on unannotated data being used or not. supervised learning,
   which only uses annotated data, has received most research interest in
   recent years. consequently, different supervised models have been used
   on biomedical ner systems, such as id49 (crfs)
   [[49]2, [50]3, [51]9, [52]10], support vector machines (id166s) [[53]8]
   and maximum id178 markov models (memms) [[54]1, [55]5].

   finally, the post-processing stage aims to improve the recognition
   results, cleaning annotation errors or refining incomplete annotations.
   the most common methods consist of removing annotations with unmatched
   parentheses [[56]7, [57]10], adding the results of abbreviation
   resolution tools [[58]2, [59]8], and extending names using a domain
   dictionary [[60]10].

   although several open source solutions aimed at recognizing biomedical
   names have been proposed in recent years, most present one or more of
   the following limitations:
     * are focused on a specific corpus and/or biomedical domain;
     * do not take advantage of state-of-the-art techniques;
     * achieved performance results are deprecated and/or not according to
       similar closed source solutions;
     * not configurable and/or easy to use;
     * not easily extensible to new features and/or scalable.

   in this article we present gimli, a new open source solution for
   automatic recognition of biomedical names, namely gene/protein, dna,
   rna, cell type and cell line names. it extends and optimizes the most
   advanced state-of-the-art techniques in a simple and easy-to-use tool.
   by default, gimli already provides high-performance trained models,
   supporting several known corpora formats. moreover, it also allows easy
   and flexible development of new solutions focused on different semantic
   types, as well as training new ml models with different feature sets
   and characteristics.

[61]implementation

   this section presents a detailed description of the resources used and
   methods implemented, following the workflow of ml-based ner solutions.
   figure [62]1 illustrates gimli   s architecture, presenting the
   connections between the various steps.
   [63]figure 1 figure 1
   figure 1

   overall architecture of gimli. overview of gimli   s architecture,
   presenting the workflow of required steps, tools and external
   resources.

tools and resources

   gimli takes advantage of various publicly available tools and
   resources. the implementation of id49 for
   statistical natural language processing is provided by mallet [[64]11].
   gdep [[65]12] is used for id121 and linguistic processing,
   namely lemmatization, part-of-speech tagging, chunking and dependency
   parsing. in terms of lexical resources, we use biothesaurus [[66]13]
   for gene and protein names, and biolexicon [[67]14] as the resource for
   biomedical domain terms.

corpora

   there are several publicly available corpora that can be used for
   training and evaluation of ner systems. to allow direct comparison with
   other tools, we selected two of the most used corpora: genetag and
   jnlpba. genetag [[68]15] is composed of 20000 sentences extracted from
   medline abstracts, not being focused on any specific domain. it
   contains annotations of proteins, dnas and rnas (grouped in only one
   semantic type), which were performed by experts in biochemistry,
   genetics and molecular biology. this corpus was used in the biocreative
   ii challenge [[69]16], providing 15000 sentences for training and 5000
   sentences for testing. on the other hand, the jnlpba corpus [[70]17]
   contains 2404 abstracts extracted from medline using the mesh terms
      human   ,    bloodcell    and    transcription factor   . the manual annotation
   of these abstracts was based on five classes of the genia ontology
   [[71]18], namely protein, dna, rna, cell line, and cell type. this
   corpus was used in the bio-entity recognition task in bionlp/nlpba 2004
   [[72]17], providing 2000 abstracts for training and the remaining 404
   abstracts for testing.

   since genetag is not focused on any specific biomedical domain, its
   annotations are more heterogeneous than those of jnlpba. a brief
   analysis, considering protein, dna and rna classes, shows that genetag
   contains almost 65% of unique entity names, as opposed to the 36% found
   in jnlpba.

pre-processing

   in recent years, various tokenisation solutions have been developed for
   several domains and languages. gimli uses the tokeniser from genia
   tagger [[73]1] (included in gdep) which is developed for biomedical
   documents and presents state-of-the-art results in this domain.
   however, words containing the symbols    /   ,    -    or    .    are not always
   split into multiple tokens. when working at the token level, this may
   create inconsistencies with the human provided annotations,
   constraining the model learning process and the recognition of some
   entity names. for instance, consider that    brca-1/2    is taken as one
   token and that in the gold standard only    brca-1    is tagged as an
   entity name. in the model training phase, the token    brca-1/2    as well
   as its local and contextual features will be considered as a
      negative   , which will directly affect the final model. thus, we
   decided to make the tokenizer behaviour more consistent, by breaking
   words containing the symbols    /   ,    -    or    .    into multiple tokens.

   to train ml models, each token in the training data must be identified
   as being part, or not, of an entity name. we use the bio encoding
   scheme, which is the de facto standard. in this scheme tokens are
   tagged as being at the beginning (tag   b   ), inside (tag    i   ) or outside
   (tag    o   ) of an entity name.

features

   feature extraction is a crucial ner task, since the predictions will be
   performed based on the information that they encode. nadeau and sekine
   [[74]19] present a complete survey on features used in general ner
   solutions. gimli implements a rich set of features, including
   orthographic, morphological, linguistic parsing, external resources and
   local context features. we also propose improvements on various
   features, in order to optimize their behaviour and performance results.
   the purpose of orthographic features is to capture knowledge about word
   formation. for example, a word that starts with a capital letter could
   indicate the occurrence of an entity name (e.g., in the protein name
      myod   ). figure [75]2 lists the formation patterns used by gimli to
   extract orthographic features from tokens.
   [76]figure 2 figure 2
   figure 2

   orthographic features. list of orthographic features organized by
   category.

   morphological features, on the other hand, reflect common structures
   and/or sub-sequences of characters among several entity names, thus
   identifying similarities between distinct tokens. to accomplish this
   goal, three distinct types of morphological features are considered:
   suffixes and prefixes, char id165s and word shape patterns. particular
   prefixes and suffixes could be used to distinguish entity names. for
   instance, suffixes like    ase   ,    ome    and    gen    frequently occur in
   gene/protein names [[77]20]. a char id165 is a subsequence of n
   characters from a given token. this feature type has an identical role
   to prefixes and suffixes, however it also finds common sub-sequences of
   characters in the middle of tokens. finally, it is also important to
   extract the token   s structure. collins [[78]21] proposed a method to
   generate a sequence of characters to reflect how letters and digits are
   organized in the token. we extended this idea to support symbols too.
   thus, three distinct types of word shapes are used by gimli:
     * word shape type i: replace sequence of digits by    *    (e.g., the
       structure of    abc1234    is expressed as    abc*   );
     * word shape type ii: replace each letter, digit and symbol by a
       morphological symbol (e.g., the structure of    abc:1234    is
       expressed as    aaa#1111   ).
     * word shape type iii: replace each sequence of letters, digits and
       symbols by a morphological symbol (e.g., the structure of
          abc:1234    is expressed as    a#1   ).

   the most basic internal feature is the token itself. however, in most
   cases, morphological variants of words have similar semantic
   interpretations, which can be considered as equivalent. for this
   reason, lemmatisation is commonly used to group together all inflected
   forms of a word, so that they can be analysed as a single item. on the
   other hand, it is also possible to associate each token with a
   particular grammatical category based on its definition and context, a
   procedure called part-of-speech (pos) tagging. moreover, we also use
   chunking, dividing the text into syntactically correlated chunks of
   words (e.g., noun or verb phrases). the bio encoding format is used to
   properly indicate the beginning and end of each chunk. for instance,
   considering two consecutive tokens that make part of a noun phrase
   chunk, the tag    b-np    is associated with the first token and the tag
      i-np    with the second one. in the end, each tag is used as a feature
   of the respective token.

   the previous features provide a local analysis of the sentence. to
   complement these with information about relations between the tokens of
   a sentence, we use features derived from id33. namely, we
   follow a strategy similar to the one presented by vlachos [[79]22],
   considering only those dependencies that could indicate the presence of
   an entity name. thus, we add as features of each token, the lemmas
   corresponding to each of the following: verbs for which the token acts
   as subject; verbs for which the token acts as object; nouns for which
   the token acts as modifier; and the modifiers of that token.
   gimli is further optimized by adding biomedical knowledge to its
   features. to provide this knowledge, dictionaries of specific domain
   terms and entity names are matched in the text and the resulting tags
   are used as features. thus, the tokens that make part of a matched term
   contain a feature that reflect such information. for instance, if the
   term    brca    is matched, the feature    lexicon=prge    is added to the
   token. two different types of dictionaries are used in gimli:
     * gene and protein names: biothesaurus is the most complete and
       up-to-date lexical resource for gene and protein names, containing
       almost 17 million unique names. due to its size, we decided to
       filter this lexicon considering only human genes and proteins,
       obtaining almost 400 thousand unique names. in the end, this
       lexicon is used to indicate the presence of curated gene and
       protein names. since these names could be present in text with
       small orthographic variations, the matching is performed according
       the following variation rules, adapted from [[80]23]:
          + replace white spaces per hyphens, and vice-versa;
          + remove white spaces and hyphens;
          + insert an hyphen on letter-digit sequences;
          + replace roman by arabic numbers, and arabic numbers by greek
            letters;
          + add theprefix    h    and the suffix    p    to acronyms
     * trigger words: specific domain terms may indicate the presence of
       biomedical names in the surrounding tokens. instead of using words
       from training data as proposed in [[81]20], we apply a more general
       solution, by matching the terms in biolexicon. this lexical
       resource contains more than two million relevant biomedical terms,
       including nouns, verbs, adjectives and adverbs (e.g.,
          stimulation   , and    activation   ).

   higher level relations between tokens and extracted features can be
   established through windows or conjunctions of features, reflecting the
   local context of each token. the application of windows consists of
   adding selected features from preceding and succeeding tokens as
   features of each token. on the other hand, conjunction of features
   consists of creating new features by grouping together features of the
   surrounding tokens. for instance, considering the sentence
      pharmacologic aspects of neonatal hyperbilirubinemia.    and a {-1,1}
   range of tokens, the following features are added to the token
      neonatal   :
     * windows: the tokens    of    and    hyperbilirubinemia   ;
     * conjunctions: the new conjunction feature
          of@-1_&_hyperbilirubinemia@1   .

   our tests showed that the best results were obtained using
   conjunctions. however, gimli does not use all of the features to
   generate conjunctions, since this would become impracticable,
   generating millions of new features. tsai et al. [[82]9] proposed the
   use of tokens of the following windows to generate the conjunctions:
   {-3,-1}, {-2,-1}, {-1,0}, {-1,1} and {0,1}. to improve the context
   knowledge, we propose a different approach, using lemmas and pos tags
   instead of tokens, since lemma conjunctions better reflect the pairwise
   patterns of words, and the pos tags conjunctions provide grammar-based
   relations and patterns. following the previous example, instead of the
   simple token-based conjunction feature, the token    neonatal    now has
   two conjunction features: pos=in@-1_&_pos=nn@1 and
   lemma=of@-1_&_lemma=hyperbilirubinemia@1. the benefits of these choices
   were confirmed through various experiments.

model

   when ml techniques are applied to ner, an algorithm must build a
   feature and statistical-based representation of target entity names
   from training data, in order to develop an appropriate response to
   unseen data. such methodologies are commonly categorized as being
   supervised or semi-supervised. semi-supervised solutions use both
   annotated and unannotated data, in order to obtain features of the
   entity names that are not present in the annotated data. specifically
   for this task, the usage of unannotated data could contribute to a
   better abstract learning of the named entities. however, the
   application of such techniques is computationally heavy and could be
   implemented as an extension to an equivalent supervised solution. thus,
   we decided to follow a supervised training approach, through the
   application of id49 (crfs) [[83]24]. such
   technique present various advantages over other methods. firstly, crfs
   avoid the label bias problem [[84]24], a weakness of maximum id178
   markov models (memms). additionally, the conditional nature of crfs (a
   discriminative model) relaxes strong independence assumptions required
   to learn the parameters of a generative model, such as hidden markov
   models (id48s) [[85]25]. finally, support vector machines (id166s) follow
   a different approach and have been shown to deliver comparable results
   to crfs [[86]26]. however, training complex id166 models for ner may take
   more time [[87]27, [88]28].
   id49 (crfs) were first introduced by lafferty et
   al. [[89]24]. assuming that we have an input sequence of observations
   (represented by x), and a state variable that needs to be inferred from
   the given observations (represented by y), a    crf is a form of
   undirected graphical model that defines a single log-linear
   distribution over label sequences (y) given a particular observation
   sequence (x)    [[90]25]. this layout makes it possible to have efficient
   algorithms to train models, in order to learn conditional distributions
   between y[ j ]and feature functions from the observable data. to
   accomplish this, it is necessary to determine the id203 of a
   given label sequence y given x. first, the model assigns a numerical
   weight to each feature, and then those weights are combined to
   determine the id203 of y[ j ]. such id203 is calculated as
   follows:
   [math: <semantics><mrow> <mi>p</mi> <mo>(</mo> <mi>y</mi> <mo>|</mo>
   <mi>x</mi> <mo>,</mo> <mi>  </mi> <mo>)</mo> <mo>=</mo> <mfrac> <mrow>
   <mn>1</mn> </mrow> <mrow> <mi>z</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo>
   </mrow> </mfrac> <mtext mathvariant="italic">exp</mtext> <mo>(</mo>
   <munder> <mrow> <mo mathsize="big">   </mo> </mrow> <mrow> <mi>j</mi>
   </mrow> </munder> <msub> <mrow> <mi>  </mi> </mrow> <mrow> <mi>j</mi>
   </mrow> </msub> <msub> <mrow> <mi>f</mi> </mrow> <mrow> <mi>j</mi>
   </mrow> </msub> <mo>(</mo> <mi>y</mi> <mo>,</mo> <mi>x</mi> <mo>)</mo>
   <mo>)</mo> <mo>,</mo> </mrow></semantics> :math]
   (1)

   where   [ j ]is a parameter to be estimated from training data and
   indicates the informativeness of the respective feature, z(x) is a
   id172 factor and
   [math: <semantics><mrow> <msub> <mrow> <mi>f</mi> </mrow> <mrow>
   <mi>j</mi> </mrow> </msub> <mo>(</mo> <mi>y</mi> <mo>,</mo> <mi>x</mi>
   <mo>)</mo> <mo>=</mo> <munderover> <mrow> <mo mathsize="big">   </mo>
   </mrow> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mrow>
   <mi>n</mi> </mrow> </munderover> <msub> <mrow> <mi>f</mi> </mrow>
   <mrow> <mi>j</mi> </mrow> </msub> <mo>(</mo> <msub> <mrow> <mi>y</mi>
   </mrow> <mrow> <mi>i</mi> <mo>-</mo> <mn>1</mn> </mrow> </msub>
   <mo>,</mo> <msub> <mrow> <mi>y</mi> </mrow> <mrow> <mi>i</mi> </mrow>
   </msub> <mo>,</mo> <mi>x</mi> <mo>,</mo> <mi>i</mi> <mo>)</mo>
   </mrow></semantics> :math]
   , where each f[ j ](y[i-1],y[ i ],x,i) is either a state function
   s(y[i-1],y[ i ],x,i) or a transition function t(y[i-1],y[ i ],x,i)
   [[91]25].

   when considering higher-order models, each label depends on a specific
   number of o previous labels. thus, the id203 will consider not
   only the previous observation and its features, but o-previous
   observations and features, which better models dependencies and may
   provide improved results, depending on the target data and task.
   however, the training complexity of higher-order models increases
   exponentially with the pre-defined order o[[92]29].

model combination

   the most recent results on biomedical ner clearly indicate that better
   performance results can be achieved by combining several systems with
   different characteristics. as an example, the top five systems of the
   biocreative ii gene mention challenge [[93]16] used ensembles of ner
   systems, combining distinct models or combining models with dictionary
   and/or rule-based systems. additionally, the application of machine
   learning-based harmonization solutions have been shown to deliver high
   improvements in terms of performance results [[94]30].

   we propose a new and simple combination strategy based on confidence
   scores. to achieve this, each model provides a confidence value for the
   annotations predicted for a given sentence. if the models that produced
   the overlapping annotations predict the same entity class, we follow a
   straightforward strategy, selecting the annotations from the model that
   has the highest confidence score and rejecting the predictions of other
   model(s). on the other hand, if we need to combine annotations of
   models that predict different entity classes (e.g., as in the jnlpba
   corpus), this strategy is extended in order to allow distinct entity
   types in the same sentence. thus, instead of selecting a single model
   to provide the predictions for the entire sentence, this choice is made
   for each annotation in the sentence. when two or more models provide
   different annotations for the same chunk of text, we select the
   annotation given by the model with the highest confidence score. if
   only one model provides an annotation for a chunk of text, that
   annotation is accepted.

post-processing

   in order to solve some errors generated by the crf model, gimli
   integrates a post-processing module that implements parentheses
   correction and abbreviation resolution. to perform parentheses
   correction, the number of parentheses (round, square and curly) on each
   annotation is verified and the annotation is removed if this is an odd
   number, since it clearly indicates a mistake by the ml model. we also
   tried to correct the annotations by removing or adding tokens up to the
   next or previous parenthesis. however, this solution provided worse
   results than simply removing the annotations.

   regarding abbreviation resolution, we adapt a simple but effective
   abbreviation definition recognizer [[95]31], which is based on a set of
   pattern-matching rules to identify abbreviations and their full forms.
   such patterns consider some constraints, namely: a)the first character
   of the acronym has to be the first character of the first word in the
   corresponding long form; b) the long form should be longer than the
   corresponding acronym; and c) the long form should not contain the
   candidate acronym. in the end, we are able to extract both short and
   long forms of each abbreviation in text. thus, if one of the forms is
   annotated as an entity name, the other one is added as a new
   annotation. additionally, if one of the forms is not completely
   annotated, gimli expands the annotation boundaries using the result
   from the abbreviation extraction tool.

[96]results and discussion

   to analyse the impact of various techniques and compare the final
   results with other existing solutions, we use common evaluation
   metrics: precision (i.e., positive predictive value) the ability of a
   system to present only relevant items; recall (i.e., sensitivity) the
   ability of a system to present all relevant items; and f-measure, the
   harmonic mean of precision and recall. these measures are formulated as
   follows:
   [math: <semantics><mrow> <mspace width="-12.0pt"></mspace> <mi
   mathvariant="bold">p</mi> <mo>=</mo> <mfrac> <mrow> <mtext
   mathvariant="italic">tp</mtext> </mrow> <mrow> <mtext
   mathvariant="italic">tp</mtext> <mo>+</mo> <mtext
   mathvariant="italic">fp</mtext> </mrow> </mfrac> <mo>,</mo> <mspace
   width="2em"></mspace> <mi mathvariant="bold">r</mi> <mo>=</mo> <mfrac>
   <mrow> <mtext mathvariant="italic">tp</mtext> </mrow> <mrow> <mtext
   mathvariant="italic">tp</mtext> <mo>+</mo> <mtext
   mathvariant="italic">fn</mtext> </mrow> </mfrac> <mo>,</mo> <mspace
   width="2em"></mspace> <mstyle> <mi mathvariant="bold">f</mi> <mn
   mathvariant="bold">1</mn> </mstyle> <mo>=</mo> <mn>2</mn> <mfrac>
   <mrow> <mi>p.r</mi> </mrow> <mrow> <mi>p</mi> <mo>+</mo> <mi>r</mi>
   </mrow> </mfrac> <mo>,</mo> </mrow></semantics> :math]
   (2)

   where tp is the amount of true positives, fp the number of false
   positives and fn the amount of false negatives.

preliminary experiments

   during the development of gimli, various optimizations and decisions
   had to be performed to achieve the best possible results. in order to
   run such experiments, we randomly split the training part of each
   corpus into training and development sets, using 80% of the data for
   training and the remaining 20% for development testing. accordingly,
   from the 15000 sentences of the training part of genetag, 12000
   sentences are used for training and 3000 sentences for development
   testing. regarding jnlpba, considering the 2000 training abstracts, we
   now use 1600 abstracts for training and the remaining 400 abstracts for
   development testing. most experiments on the development stage, namely
   id121 and feature set optimization, were performed using
   first-order crf models with forward (left to right) text parsing.

id121

   to evaluate the impact of the id121 changes introduced in gimli,
   we compared the results achieved against the use of the original
   id121. this analysis only applies to the genetag corpus, since
   jnlpba is provided as tokenized text. using the development set, an
   improvement of 8.28% in f-measure was achieved when applying a model
   trained on tokens provided by our proposed id121 as compared to
   using the original version of genia tagger. when applied to the final
   test set, and considering the alternative annotations provided, the
   improvement in f-measure is 2.53%. such results clearly show the
   positive contribution of our id121 approach on gimli.

feature set

   each feature encodes specific characteristics of target annotations,
   providing a different contribution in the learning process. in order to
   evaluate their impact in the recognition performance, we initially
   grouped features that encode similar information into logical
   sub-classes for each feature type, as shown in figure [97]3. we then
   followed a backward elimination approach to find the best feature set
   for each entity type, by removing each sub-class from the complete
   feature set and analysing its impact in the results. although small
   improvements or drops may not be significant regarding performance
   improvements, they indicate that adding or removing a specific feature
   may have an impact on the final performance results, which is relevant
   when considering the inclusion (or not) of that feature. when such
   cases occurred, we decided to keep the feature when a small improvement
   occurred and remove it when a small drop is present. in the end, the
   features that presented a negative impact when removed from the initial
   set were included in the final feature set, as indicated in figure
   [98]3. for instance, our trigger words approach provides a slight
   positive impact in the recognition of gene and protein names in
   genetag, resulting in an f-measure improvement of 0.11%. however, a
   negative impact is observed on jnlpba, with a 0.39% decrease of
   f-measure. we believe that the obtained results are a consequence of
   the corpus specificity, since biolexicon terms may point to the
   presence of entity names that were not considered in the specific
   corpus and/or entity type.
   [99]figure 3 figure 3
   figure 3

   feature set per corpus and entity. feature set applied to each corpus
   and entity type. features marked with an    x    are used in the final
   feature set for that entity type.

   this approach helps experts to better understand the linguistic
   characteristics of each entity type on each corpus. thus, the final
   feature sets seem to reflect the complexity and heterogeneity
   associated with each entity type and corpus. for instance, the absence
   of the original tokens for protein, cell line and cell type on jnlpba
   may indicate less heterogeneity, as the use of lemmatization appears to
   better reflect and generalize the target names. overall, the feature
   set required by genetag is more complex than the ones used on jnlpba,
   discarding the original tokens and some orthographic and morphological
   features. this is consistent with the idea that the entity names
   present on genetag are more heterogeneous than those present on jnlpba,
   as suggested before.

conjunctions vs windows

   local context, as encoded in windows or conjunctions of features, has a
   great impact in recognition performance. we therefore analysed in
   detail the impact of using these two alternatives, considering basic
   and improved solutions. thus, four different configurations are
   considered in our analysis:
     * token conjunctions: form conjunctions as the concatenation of
       tokens taken from the following windows {-3,-1}, {-2,-1}, {-1,0},
       {-1,1} and {0,1};
     * optimized conjunctions: the same windows as the previous
       configuration but using lemmas and pos tags for the conjunctions,
       instead of tokens;
     * windows tokens: use each token from the window {-2,2};
     * windows optimized: use lemmas, lexicon matching, biomedical
       concepts matching and tokens in the window {-3,3}, and all the
       features in the window {-1,1}.

   figure [100]4 presents the performance (f-measure) achieved with the
   four approaches. results are shown for crf models of order 1 and 2 with
   forward and backward parsing directions, as explained in the next
   section. optimized conjunctions present the best results on both
   corpora, considerably outperforming conjunctions with tokens.
   conjunctions of features seem to perform better than windows for this
   task, as indicated by the fact that using simple token conjunctions
   provided better results than even the optimized windows of features.
   interestingly, while the optimized windows present better results than
   windows with tokens on genetag, in the case of jnlpba using just the
   tokens provides better results for the models trained with backward
   parsing direction. overall, optimized conjunctions present the most
   constant behaviour, presenting the best results and less deviation. on
   the other hand, using tokens resulted in higher deviation on both
   approaches.
   [101]figure 4 figure 4
   figure 4

   comparison of windows and conjunctions results on development sets.
   comparison of f-measure results achieved by token-based and optimized
   windows and conjunctions in the development sets of both corpora,
   considering exact matching evaluation, different model orders and text
   parsing directions. results for the jnlpba corpus indicate the overall
   performance, i.e. across entity types. fw: forward, and bw:backward.

   this analysis indicates that choosing the right method to encode local
   context is fundamental, since an untidy decision may deliver
   considerably worse results. as we can see, the average f-measure
   differences between the best and worst solutions on genetag and jnlpba
   are of 2.13% and 1.73%, respectively.

model combination analysis

   the usual direction to parse a text is from left to right (forward).
   however, previous studies [[102]10, [103]32] have shown that parsing
   the text from right to left (backward) may provide better results,
   which has been shown to be a consequence of the asymmetric
   implementation of crf models in mallet [[104]32]. additionally, we
   believe that using crfs with different orders will extract different
   context based characteristics from text. thus, we decided to train
   first and second order crf models, considering both forward and
   backward text parsing.
   initial evaluation results on genetag and jnlpba are presented in
   figure [105]5, using the previously selected feature set (figure
   [106]3). as we can see, the application of different crf orders and
   parsing directions provides significant performance differences. for
   instance, considering rna on jnlpba, the difference between different
   parsing directions is above 3%, and the difference between different
   crf orders is approximately 2%. overall, backward models present the
   best results, which confirms the benefit of using backward text
   parsing. moreover, due to the different entity names    heterogeneity
   existent in both corpora, different model orders are required. on
   genetag, the best results are achieved using second order models. on
   the other hand, the best results for protein and cell type on jnlpba
   are achieved using first order models.
   [107]figure 5 figure 5
   figure 5

   preliminary results on development sets. preliminary f-measure results
   achieved by gimli in the development sets of both corpora, considering
   exact matching evaluation, different model orders and text parsing
   directions. the best combination of model order and parsing direction
   for each entity type is highlighted. fw: forward, and bw:backward.
   to combine the various models for each class on each corpus, we
   performed a sequential analysis of the combination results. thus, we
   first combined the two best models for each class and, if the
   performance was better than the best model alone, we kept adding models
   to the two best, in order to find the best set. if the combination
   result of the two best models was not better than the best model, we
   tried combining the best model with others, until a better combination
   was obtained. if the combination did not improve the results, only the
   model with the best result was used. figure [108]6 presents the results
   of our analysis. even with the simple combination approach used by
   gimli, the harmonisation strategy improves the best model results. an
   average improvement of 0.5% is verified. overall, the best combination
   results are achieved by combining the two best performing models.
   moreover, models with low performance results also contribute to a
   better model combination, by providing heterogeneity that is not
   present in other models. for instance, on cell line the best model
   combination is achieved by adding the worst performing model.
   [109]figure 6 figure 6
   figure 6

   combination results on development sets. f-measure results achieved for
   each class with the combination of several models in the development
   sets of both corpora, considering exact matching evaluation. the
   combination results are compared with the best performing model
   obtained in previous experiments (figure [110]5). fw: forward,
   bw:backward, and f1: f-measure.
   figure [111]7 presents the final results achieved on both corpora,
   considering the final and unseen test data of both corpora. note that
   the evaluation strategies of the two challenges are slightly different.
   on jnlpba only full matches are considered correct, requiring both left
   and right boundaries to match exactly. on the other hand, genetag
   evaluation allows minor mistakes, based on alternative names that were
   previously accepted by human annotators during the preparation of the
   corpus.
   [112]figure 7 figure 7
   figure 7

   final results. final precision (p), recall (r) and f-measure (f1)
   results achieved by gimli on test data of both corpora.

feature contributions

   in order to evaluate the overall contribution of some high-end features
   implemented by gimli, we performed an analysis on both corpora,
   considering the removal of such features from the best feature set for
   each entity type. figure [113]8 presents the observed differences,
   reflecting the features    contribution. overall, removing conjunctions
   causes the highest negative impact, considerably reducing the
   performance results. id33 also contributes positively to
   the final results, namely on dna and cell line. on the other hand,
   removing id33 features from rna improves the results.
   however, this is a consequence of the algorithm to combine the models
   of different entity types. when evaluated alone, rna recognition
   presents an f-measure of 68.97%. removing id33 features,
   this value drops slightly to 68.91%, reflecting the positive
   contribution of such features. as expected, lexicons also provide a
   positive contribution, increasing the models    precision.
   post-processing, on the other hand, introduces just a small positive
   contribution. for instance, on rna, the absence of post-processing
   methods does not affect the performance in any way.
   [114]figure 8 figure 8
   figure 8

   key features    contribution. f-measure contribution of key features on
   genetag and jnlpba considering all semantic types.

performance analysis

   to evaluate gimli and understand its behaviour in comparison with
   existing solutions, we collected the best open and closed source
   systems for biomedical id39. figure [115]9presents
   a summary description of the systems    characteristics, comparing them
   against gimli. overall, we collected a total of 12 systems, where seven
   are open source and five closed source. our study of these systems
   allowed to identify some current trends of biomedical ner systems:
     * the most used ml model is crf (6 systems);
     * almost all the discriminative ml models use orthographic,
       morphological and basic-linguistic (pos tags and lemmas) features;
     * only 3 systems use model combination, all of which are closed
       source;
     * only 5 systems use post-processing techniques, where 4 are closed
       source.
     * 8 systems support genetag and 6 systems support jnlpba;
     * only 3 systems support both corpora, where 2 are open source;

   [116]figure 9 figure 9
   figure 9

   biomedical ner systems overview. summary of the open and closed source
   systems    characteristics, presenting the used programming languages,
   features, models and post-processing techniques. cbr-tagger [[117]33]
   and lingpipe [[118]34] were also included in this analysis.

   based on these facts, we can argue that closed source solutions are
   commonly developed for a specific corpus, being focused on only one
   specific goal. however, those solutions present the most advanced
   techniques. on the contrary, open source solutions do not always take
   advantage of high-end techniques.
   figures [119]10 and [120]11 present the results obtained on genetag and
   jnlpba corpus respectively, comparing gimli against open and closed
   source systems. on the genetag corpus, gimli outperforms all the open
   source solutions, achieving an f-measure of 87.17%. it presents a
   significant improvement of 0.74% over the second best system, banner.
   in comparison with nersuite, gimli presents an improvement of 1.72%.
   overall, it presents the best results both on precision and recall.
   regarding closed source solutions, gimli presents the third best
   result, with a similar performance as the winner of the biocreative ii
   gene mention challenge [[121]16] (ibm watson), which uses
   semi-supervised ml and forward and backward model combination. overall,
   aiiagmt presents the best result on this corpus (with 88.30% of
   f-measure). however, the presented solution was prepared specifically
   for this corpus, applying a complex combination strategy that requires
   eight different crf models using two different crf frameworks.
   [122]figure 10 figure 10
   figure 10

   results comparison on genetag corpus. comparison of the precision (p),
   recall (r) and f-measure (f1) results achieved by gimli on genetag
   corpus, comparing with both open and closed source solutions. results
   of closed source solutions are shown with a shaded background.
   [123]figure 11 figure 11
   figure 11

   results comparison on jnlpba corpus. comparison of the f-measure
   results achieved by gimli on jnlpba corpus, comparing with both open
   and closed source solutions. the overall result reflects the achieved
   performance considering the five entity types, and the results of
   closed source solutions are shown with a shaded background.

   considering the jnlpba corpus, gimli outperforms all the open source
   solutions, achieving an overall f-measure of 72.23%. it presents a
   significant improvement of f-measure in comparison with the second best
   system, genia tagger. compared to the best java-based solution (abner),
   gimli presents an improvement of 1.73% of f-measure. it considerably
   outperforms open source systems in recognition of protein, dna, rna and
   cell line names. however, it is outperformed in the recognition of cell
   types. regarding closed source solutions, gimli presents the third best
   result, with similar results as the winner of the nlpba shared task
   [[124]17] (zho04). when compared with the second best participant of
   this challenge (fin04), gimli presents an overall improvement of 2.17%
   of f-measure. nerbio, the best system on this corpus, implements a
   rule-based post-processing method that was prepared specifically for
   this corpus. moreover, nerbio presents a very low performance result
   (79.05% of f-measure) on genetag, which could indicate some limitations
   in adapting this solution to different corpora.

   considering a non-blind model combination strategy, as taken by hsu et
   al. [[125]10], gimli presents slightly better results, achieving an
   f-measure of 87.36% on genetag and 72.69% on jnlpba. such results
   outperform all the systems that participated on both challenges.

   overall, gimli significantly outperforms all the existent open source
   solutions on both genetag and jnlpba, by simply adapting the feature
   set used for each corpora and entity type. moreover, it also presents
   competitive results when compared with similar closed source solutions
   for both corpora.

speed analysis

   the various experiments to check training and tagging speed were
   performed in a machine with 8 processing cores @ 2.67 ghz and 16gb of
   ram.

   the training speed varies with the corpus size, feature set complexity
   and model order. considering the training parts of both corpora and the
   final feature set, a second-order crf model takes on average one hour
   to be trained. on the other hand, a first-order crf model requires on
   average 30 minutes.

   in order to check tagging speed and gimli tractability, we developed a
   simple algorithm to annotate medline abstracts using multi-threading
   processing. this solution includes input xml parsing, sentence
   splitting, gimli integration and output generation in xml. it uses a
   single second-order crf model, but model combination can be easily
   integrated with reduced speed impact, taking advantage of
   multi-threaded processing. during this analysis, we considered various
   configurations of gimli, enabling and disabling the most resource
   expensive techniques. thus, if users prioritize annotation speed over
   high performance results, windows can be used instead of conjunctions
   and id33 can be removed from the workflow. moreover, in
   order to use the available resources as much as possible, the number of
   running threads must be inversely proportional to the complexity of the
   used techniques, since complex techniques require more processing
   resources. the following results were obtained:
     * conjunctions with id33: 4 threads, 20
       sentences/second;
     * conjunctions without id33: 6 threads, 86
       sentences/second;
     * windows without id33: 8 threads, 232
       sentences/second.

[126]conclusions

   this article presents gimli, a new open source and high-performance
   solution for biomedical id39 on scientific
   documents, supporting the automatic recognition of gene/protein, dna,
   rna, cell line and cell type names. gimli implements a machine
   learning-based solution, taking advantage of id49.
   moreover, it supports a rich set of features, including orthographic,
   morphological, linguistic-based and also domain knowledge features,
   through the implementation of a lexicon matching technique.
   additionally, gimli implements advanced conjunctions of features,
   creating new features based on windows of lemmas and part-of-speech
   tags. in order to correct mistakes generated by the crf models, gimli
   also integrates a post-processing module, implementing parentheses
   correction and abbreviation resolution, aimed at extending incompletely
   tagged names. finally, gimli also combines several forward and backward
   models to achieve the best results.

   in order to evaluate gimli and compare it against existing systems, we
   used two well-known corpora: genetag and jnlpba. in the end, it
   achieved f-measure results of 87.17% and 72.23% on each corpora,
   respectively. these results were compared to the systems that
   participated in the challenges where the corpora were used, biocreative
   ii gene mention and nlpba shared task. gimli outperforms all existing
   open source solutions on both corpora, presenting significant
   improvements both in results and techniques used.

   gimli is an off-the-shelf solution that can be used through two
   different endpoints, thinking on users with different goals and
   expertise:
     * command line interface (cli): automatic scripts with easy access to
       main functionalities, allowing the annotation of documents using
       provided models, and training new models focused on different
       entity types, using a configuration file to customize the feature
       set and model parameters;
     * application programming interface (api): provides complete access
       to implemented features and associated infrastructure, allowing the
       easy integration of gimli in complex id111 workflows, by
       using, extending and/or adapting the provided functionalities.

   overall, we believe that gimli provides various characteristics that
   make it a state-of-the-art solution for biomedical ner:
     * high-end techniques: gimli applies various state-of-the-art
       techniques and proposes optimizations on various methods,
       presenting innovative and high-performance alternatives. moreover,
       it integrates various solutions that are only present on closed
       source solutions, such as id33, chunking and model
       combination;
     * flexible: gimli was built thinking on flexibility, founded on a
       strong infrastructure that allows adding new features and extending
       or changing existing ones. moreover, gimli offers the only cli that
       allows feature set and model parameters definition;
     * scalable: the internal infrastructure is ready to scale, supporting
       the development of more complex solutions. moreover, gimli is ready
       to be used on multi-threaded applications, in order to process
       millions of documents;
     * documentation: we provide complete and detailed documentation of
       gimli, in order to use both cli and api. together with the
       associated simplicity and self-explanatory code, we believe that
       gimli is easy to use, change and extend.

   developers and researchers of the biomedical domain, especially text
   mining experts, can take advantage of the presented characteristics to
   develop their own ner and/or post-ner applications. gimli reduces the
   required effort to develop innovative ner solutions, increasing the
   users    time to focus on their main goals. thus, it can be used to
   support the development of various multi-disciplinary solutions: a) ner
   using different corpora and target entity names, such as disorders and
   chemicals; b) id172; c) id36, such as
   protein-protein interactions; and d) information retrieval.

   with the results achieved and the characteristics presented by our
   system, we strongly believe that gimli is a state-of-the art solution
   for biomedical id39, contributing to faster and
   better research in the field.

future work

   although gimli already incorporates various improvements on existing
   tools, some aspects can be further explored. we are currently
   investigating other approaches for model combination, considering for
   example the introduction of domain knowledge information and/or context
   based harmonisation through the use of dictionaries or machine
   learning-based solutions [[127]30]. as for the recognition of
   particular entity types such as dna, rna, cell type and cell line, we
   are working on improving the lexicons in order to achieve better
   precision. an interesting area to explore is the use of feature
   induction [[128]35] to automatically extract informative features from
   texts, in order to improve the feature set and obtain    hidden   
   characteristics of the tokens. the second technique that could be
   studied is semi-supervised learning [[129]36], using both annotated and
   unannotated data in order to extract characteristics of the unlabelled
   data that could contribute to better recognition of entity name
   boundaries. regarding the use of gimli, it could be interesting to
   implement a set of web services to streamline its integration in other
   tools and disseminate the simple and fast annotation of scientific
   documents. furthermore, although gimli offers a simple to use
   command-line application, developing a gui interface could simplify the
   analysis of the generated annotations.

   there are already various solutions being developed on top of gimli,
   such as: a) a framework for biomedical information extraction
   supporting ml and dictionary-based methodologies for id172 of
   biomedical concepts; b) a solution based on semi-supervised ner for
   gene and protein names recognition; and c) an information retrieval
   solution for knowledge discovery focused on degenerative diseases.

[130]availability and requirements

     * project name: gimli
     * project home page: [131]http://bioinformatics.ua.pt/gimli
     * operating system(s): platform independent
     * programming language: java
     * other requirements: java 1.6 or higher
     * license: creative commons attribution-noncommercial-sharealike 3.0
       unported license
     * any restrictions to use by non-academics: non-commercial use

[132]abbreviations

   crfs:
          id49

   id48s:
          id48

   memms:
          maximum id178 markov models

   ml:
          machine learning

   ner:
          id39

   pos:
          part-of-speech

   id166s:
          support vector machines

[133]declarations

acknowledgements

   this research work was funded by feder through the compete programme
   and by national funds through fct -    funda    o para a ci  ncia e a
   tecnologia    under the project number ptdc/eia-cco/100541/2008. s. matos
   is funded by fct under the ci  ncia2007 programme.

authors' original submitted files for images

   below are the links to the authors    original submitted files for
   images.
   [134]12859_2012_5873_moesm1_esm.pdf authors    original file for figure 1
   [135]12859_2012_5873_moesm2_esm.pdf authors    original file for figure 2
   [136]12859_2012_5873_moesm3_esm.pdf authors    original file for figure 3
   [137]12859_2012_5873_moesm4_esm.pdf authors    original file for figure 4
   [138]12859_2012_5873_moesm5_esm.pdf authors    original file for figure 5
   [139]12859_2012_5873_moesm6_esm.pdf authors    original file for figure 6
   [140]12859_2012_5873_moesm7_esm.pdf authors    original file for figure 7
   [141]12859_2012_5873_moesm8_esm.pdf authors    original file for figure 8
   [142]12859_2012_5873_moesm9_esm.pdf authors    original file for figure 9
   [143]12859_2012_5873_moesm10_esm.pdf authors    original file for figure
   10
   [144]12859_2012_5873_moesm11_esm.pdf authors    original file for figure
   11

competing interests

   the authors declare that they have no competing interests.

authors    contributions

   dc participated in the design and implementation of the system and
   drafted the manuscript. sm and jlo conceived the study, participated in
   its design and coordination and helped to draft the manuscript. all
   authors read and approved the final manuscript.

[145]authors    affiliations

   (1)
   ieeta/deti, university of aveiro, campus universit  rio de santiago,
   aveiro, 3810-193, portugal

[146]references

    1. tsuruoka y, tateishi y, kim j, ohta t, mcnaught j, ananiadou s,
       tsujii j: developing a robust part-of-speech tagger for biomedical
       text. advances in informatics. 2005, 3746: 382-392.
       10.1007/11573036_36.[147]view article[148]google scholar
    2. leaman r, gonzalez g: banner: an executable survey of advances in
       biomedical id39. pacific, symposium on
       biocomputing, volume 13. 2008, big island, hawaii: ,
       652-663.[149]google scholar
    3. settles b: abner: an open source tool for automatically tagging
       genes, proteins and other entity names in text. bioinformatics.
       2005, 21 (14): 3191-10.1093/bioinformatics/bti475.[150]view
       article[151]pubmed[152]google scholar
    4. song y, kim e, lee g, yi b: posbiotm-ner in the shared task of
       bionlp/nlpba 2004. proceedings of the international joint workshop
       on natural language processing in biomedicine and its applications.
       2004, stroudsburg, pa, usa: association for computational
       linguistics, 100-103.[153]google scholar
    5. finkel j, dingare s, nguyen h, nissim m, manning c, sinclair g:
       exploiting context for biomedical entity recognition: from syntax
       to the web. proceedings of the international joint workshop on
       natural language processing in biomedicine and its applications.
       2004, stroudsburg, pa, usa: association for computational
       linguistics, 88-91.[154]google scholar
    6. cho hc: nersuite: a id39 toolkit. tsujii
       laboratory, department of information science, university of tokyo,
       tokyo, japan 2010, [[155]http://nersuite.nlplab.org]
    7. ando r: biocreative ii gene mention tagging system at ibm watson.
       proceedings of the second biocreative challenge evaluation
       workshop. 2007, madrid, spain: , 101-103.[156]google scholar
    8. zhou g, zhang j, su j, shen d, tan c: recognizing names in
       biomedical texts: a machine learning approach. bioinformatics.
       2004, 20 (7): 1178-90. 10.1093/bioinformatics/bth060.[157]view
       article[158]pubmed[159]google scholar
    9. tsai r, sung c, dai h, hung h, sung t, hsu w: nerbio: using
       selected word conjunctions, term id172, and global patterns
       to improve biomedical id39. bmc bioinformatics.
       2006, 7 (suppl 5): s11-10.1186/1471-2105-7-s5-s11.[160]pubmed
       central[161]view article[162]pubmed[163]google scholar
   10. hsu c, chang y, kuo c, lin y, huang h, chung i: integrating high
       dimensional bi-directional parsing models for gene mention tagging.
       bioinformatics. 2008, 24 (13):
       i286-10.1093/bioinformatics/btn183.[164]pubmed central[165]view
       article[166]pubmed[167]google scholar
   11. mccallum ak: mallet: a machine learning for language toolkit.
       amherst, ma, usa 2002, [[168]http://mallet.cs.umass.edu]
   12. sagae k: id33 and id20 with, lr models
       and parser ensembles. proceedings of the eleventh conference on
       computational natural language learning. 2007, prague, czech
       republic: , 1044-1050.[169]google scholar
   13. liu h, hu zz, zhang j, wu ch: biothesaurus: a web-based thesaurus
       of protein and gene names. bioinformatics. 2006, 22: 103-105.
       10.1093/bioinformatics/bti749.[170]view
       article[171]pubmed[172]google scholar
   14. sasaki y, montemagni s, pezik p, rebholz-schuhmann d, mcnaught j,
       ananiadou s: biolexicon: a lexical resource for the biology domain.
       proceedings of the, third international symposium on semantic
       mining in biomedicine (smbm 2008), volume 3. 2008, jena, germany: ,
       109-116.[173]google scholar
   15. tanabe l, xie n, thom l, matten w, wilbur w: genetag: a tagged
       corpus for gene/protein id39. bmc
       bioinformatics. 2005, 6 (suppl 1):
       s3-10.1186/1471-2105-6-s1-s3.[174]pubmed central[175]view
       article[176]pubmed[177]google scholar
   16. smith l, tanabe l, ando r, kuo c, chung i, hsu c, lin y, klinger r,
       friedrich c, ganchev k: overview of biocreative ii gene mention
       recognition. genome biology. 2008, 9 (suppl 2):
       s2-10.1186/gb-2008-9-s2-s2.[178]pubmed central[179]view
       article[180]pubmed[181]google scholar
   17. kim j, ohta t, tsuruoka y, tateisi y, collier n: introduction to
       the bio-entity recognition task at jnlpba. proceedings of the
       international joint workshop on natural language processing in
       biomedicine and its applications. 2004, stroudsburg, pa, usa:
       association for computational linguistics, 70-75.[182]google
       scholar
   18. kim j, ohta t, tateisi y, tsujii j: genia corpus-a semantically
       annotated corpus for bio-textmining. bioinformatics. 2003, 19:
       180-182. 10.1093/bioinformatics/btg1023.[183]view
       article[184]google scholar
   19. nadeau d, sekine s: a survey of id39 and
       classification. lingvisticae investigationes. 2007, 30: 3-26.
       10.1075/li.30.1.03nad.[185]view article[186]google scholar
   20. zhou g, shen d, zhang j, su j, tan s: recognition of protein/gene
       names from text using an ensemble of classifiers. bmc
       bioinformatics. 2005, 6 (suppl 1):
       s7-10.1186/1471-2105-6-s1-s7.[187]pubmed central[188]view
       article[189]pubmed[190]google scholar
   21. collins m: ranking algorithms for named-entity extraction: boosting
       and the voted id88. proceedings of the 40th annual meeting on
       association for computational linguistics. 2002, philadelphia, pa,
       usa: association for computational linguistics, 489-496.[191]google
       scholar
   22. vlachos a: tackling the, biocreative2 gene mention task with
       id49 and syntactic parsing. proceedings of the
       second biocreative challenge evaluation workshop; 23 to 25 april
       2007. 2007, madrid, spain: , 85-87.[192]google scholar
   23. schuemie m, mons b, weeber m, kors j: evaluation of techniques for
       increasing recall in a dictionary approach to gene and protein name
       identification. journal of biomedical informatics. 2007, 40 (3):
       316-324. 10.1016/j.jbi.2006.09.002.[193]view
       article[194]pubmed[195]google scholar
   24. lafferty j, mccallum a, pereira f: id49:
       probabilistic models for segmenting and labeling sequence data.
       proceedings of the eighteenth international conference on machine
       learning (icml-2001). 2001, williamstown, ma, usa: ,
       282-289.[196]google scholar
   25. wallach h: id49: an introduction. tech. rep.,
       university of pennsylvania, philadelphia, pa, usa 2004[197]google
       scholar
   26. keerthi s, sundararajan s: crf versus id166-struct for sequence
       labeling. tech. rep., yahoo research 2007[198]google scholar
   27. lee c, jang m: fast training of structured id166 using
       fixed-threshold sequential minimal optimization. etri journal.
       2009, 31 (2): 121-128. 10.4218/etrij.09.0108.0276.[199]view
       article[200]google scholar
   28. hoefel g, elkan c: learning a two-stage id166/crf sequence
       classifier. proceedings of the 17th acm conference on information
       and knowledge management, cikm    08. 2008, new york, ny, usa: acm,
       271-278.[201]google scholar
   29. sarawagi s, cohen w: semi-markov id49 for
       information extraction. advances in neural information processing
       systems. 2004, 17: 1185-1192.[202]google scholar
   30. campos d, matos s, lewin i, oliveira j, rebholz-schuhmann d:
       harmonization of gene/protein annotations: towards a gold standard
       medline. bioinformatics. 2012, 28 (9): 1253-1261.
       10.1093/bioinformatics/bts125.[203]view
       article[204]pubmed[205]google scholar
   31. schwartz a, hearst m: a simple algorithm for identifying
       abbreviation definitions in biomedical text. pacific symposium on
       biocomputing, volume 8. 2003, lihue, hawaii: , 451-462.[206]google
       scholar
   32. kuo c, chang y, huang h, lin k, yang b, lin y, hsu c, chung i: rich
       feature set, unification of bidirectional parsing and dictionary
       filtering for high f-score gene mention tagging. proceedings of the
       second biocreative challenge evaluation workshop. 2007, madrid,
       spain: , 105-107.[207]google scholar
   33. neves m, chagoyen m, carazo j, pascual-montano a: cbr-tagger: a
       case-based reasoning approach to the gene/protein mention problem.
       proceedings of the workshop on current trends in biomedical natural
       language processing. 2008, stroudsburg, pa, usa: association for
       computational linguistics, 108-109.[208]view article[209]google
       scholar
   34. alias-i i: lingpipe. alias-i, inc., brooklyn, ny, usa 2011,
       [[210]http://alias-i.com/lingpipe/index.html]
   35. mccallum a: efficiently inducing features of conditional random
       fields. proceedings of the nineteenth conference on uncertainty in
       artificial intelligence. 2003, acapulco, mexico: morgan kaufmann
       publishers inc., 403-410.[211]google scholar
   36. mann g, mccallum a: generalized expectation criteria for
       semi-supervised learning of id49. proc. acl.
       2008, columbus, ohio, usa: , 870-878.[212]google scholar

[213]copyright

      campos et al.; licensee biomed central ltd. 2013

   this article is published under license to biomed central ltd. this is
   an open access article distributed under the terms of the creative
   commons attribution license
   ([214]http://creativecommons.org/licenses/by/2.0), which permits
   unrestricted use, distribution, and reproduction in any medium,
   provided the original work is properly cited.

   [215]download pdf
   (button) export citations

papers, zotero, reference manager, refworks (.ris)

     * [216]download citations
     * [217]download references
     * [218]download both

endnote (.enw)

     * [219]download citations
     * [220]download references
     * [221]download both

mendeley, jabref (.bib)

     * [222]download citations
     * [223]download references
     * [224]download both

section

     * [225]knowledge-based analysis

metrics

share this article

     * (button) share on twitter
     * (button) share on facebook
     * (button) share on linkedin
     * (button) share on weibo
     * (button) share on reddit

   [226]get shareable link

other actions

     * [227]order reprint

   advertisement

bmc bioinformatics

   issn: 1471-2105

contact us

     * submission enquiries: [228]bmcbioinformatics@biomedcentral.com
     * general enquiries: [229]info@biomedcentral.com

     * [230]read more on our blogs
     * [231]receive bmc newsletters
     * [232]manage article alerts
     * [233]language editing for authors
     * [234]scientific editing for authors

     * [235]policies
     * [236]accessibility
     * [237]press center

     * [238]support and contact
     * [239]leave feedback
     * [240]careers

follow bmc

     * [241]bmc twitter page
     * [242]bmc facebook page
     * [243]bmc weibo page

   by using this website, you agree to our [244]terms and conditions,
   [245]privacy statement and [246]cookies policy. [247]manage the cookies
   we use in the preference centre.

   springer nature logo

      2019 biomed central ltd unless otherwise stated. part of
   [248]springer nature.

   [nature.png]

references

   1. https://www.googletagmanager.com/ns.html?id=gtm-tdgjhk
   2. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#main-content
   3. https://pubads.g.doubleclick.net/gampad/jump?iu=/270604982/bmc/bmcbioinformatics/articles&sz=728x90,970x90&pos=lb1&doi=10.1186/1471-2105-14-54&kwrd=conditional random field,name entity recognition,biomedical domain,command line interface,id33&pmc=l15001,b12050,i23050,l17004,m14018,m14018&
   4. https://www.biomedcentral.com/
   5. https://www.biomedcentral.com/journals
   6. https://www.biomedcentral.com/getpublished
   7. https://www.biomedcentral.com/about
   8. https://www.biomedcentral.com/login
   9. https://bmcbioinformatics.biomedcentral.com/
  10. https://bmcbioinformatics.biomedcentral.com/
  11. https://bmcbioinformatics.biomedcentral.com/about
  12. https://bmcbioinformatics.biomedcentral.com/articles
  13. https://bmcbioinformatics.biomedcentral.com/submission-guidelines
  14. https://forms.gle/7attl74qdp2be1nb6
  15. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#abs1
  16. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#sec1
  17. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#sec2
  18. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#sec10
  19. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#sec19
  20. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#sec21
  21. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#decs
  22. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#bib1
  23. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#aff1
  24. mailto:david.campos@ua.pt
  25. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#aff1
  26. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#aff1
  27. https://doi.org/10.1186/1471-2105-14-54
  28. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#abstract
  29. http://bioinformatics.ua.pt/gimli
  30. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#keywords
  31. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#background
  32. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr1
  33. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr2
  34. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr2
  35. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr5
  36. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr1
  37. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr4
  38. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr6
  39. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr7
  40. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr2
  41. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr8
  42. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr2
  43. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr6
  44. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr9
  45. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr1
  46. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr3
  47. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr5
  48. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr10
  49. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr2
  50. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr3
  51. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr9
  52. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr10
  53. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr8
  54. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr1
  55. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr5
  56. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr7
  57. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr10
  58. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr2
  59. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr8
  60. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr10
  61. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#implementation
  62. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#fig1
  63. https://media.springernature.com/full/springer-static/image/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_article_5873_fig1_html.jpg
  64. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr11
  65. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr12
  66. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr13
  67. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr14
  68. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr15
  69. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr16
  70. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr17
  71. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr18
  72. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr17
  73. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr1
  74. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr19
  75. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#fig2
  76. https://media.springernature.com/full/springer-static/image/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_article_5873_fig2_html.jpg
  77. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr20
  78. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr21
  79. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr22
  80. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr23
  81. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr20
  82. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr9
  83. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr24
  84. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr24
  85. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr25
  86. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr26
  87. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr27
  88. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr28
  89. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr24
  90. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr25
  91. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr25
  92. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr29
  93. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr16
  94. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr30
  95. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr31
  96. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#resultsanddiscussion
  97. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#fig3
  98. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#fig3
  99. https://media.springernature.com/full/springer-static/image/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_article_5873_fig3_html.jpg
 100. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#fig4
 101. https://media.springernature.com/full/springer-static/image/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_article_5873_fig4_html.jpg
 102. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr10
 103. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr32
 104. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr32
 105. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#fig5
 106. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#fig3
 107. https://media.springernature.com/full/springer-static/image/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_article_5873_fig5_html.jpg
 108. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#fig6
 109. https://media.springernature.com/full/springer-static/image/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_article_5873_fig6_html.jpg
 110. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#fig5
 111. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#fig7
 112. https://media.springernature.com/full/springer-static/image/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_article_5873_fig7_html.jpg
 113. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#fig8
 114. https://media.springernature.com/full/springer-static/image/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_article_5873_fig8_html.jpg
 115. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#fig9
 116. https://media.springernature.com/full/springer-static/image/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_article_5873_fig9_html.jpg
 117. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr33
 118. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr34
 119. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#fig10
 120. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#fig11
 121. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr16
 122. https://media.springernature.com/full/springer-static/image/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_article_5873_fig10_html.jpg
 123. https://media.springernature.com/full/springer-static/image/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_article_5873_fig11_html.jpg
 124. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr17
 125. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr10
 126. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#conclusions
 127. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr30
 128. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr35
 129. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#cr36
 130. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#availabilityandrequirements
 131. http://bioinformatics.ua.pt/gimli
 132. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#abbreviations
 133. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#declarations
 134. https://static-content.springer.com/esm/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_5873_moesm1_esm.pdf
 135. https://static-content.springer.com/esm/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_5873_moesm2_esm.pdf
 136. https://static-content.springer.com/esm/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_5873_moesm3_esm.pdf
 137. https://static-content.springer.com/esm/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_5873_moesm4_esm.pdf
 138. https://static-content.springer.com/esm/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_5873_moesm5_esm.pdf
 139. https://static-content.springer.com/esm/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_5873_moesm6_esm.pdf
 140. https://static-content.springer.com/esm/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_5873_moesm7_esm.pdf
 141. https://static-content.springer.com/esm/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_5873_moesm8_esm.pdf
 142. https://static-content.springer.com/esm/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_5873_moesm9_esm.pdf
 143. https://static-content.springer.com/esm/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_5873_moesm10_esm.pdf
 144. https://static-content.springer.com/esm/art:10.1186/1471-2105-14-54/mediaobjects/12859_2012_5873_moesm11_esm.pdf
 145. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#author-affiliations
 146. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#references
 147. https://doi.org/10.1007/11573036_36
 148. http://scholar.google.com/scholar_lookup?title=developing a robust part-of-speech tagger for biomedical text&author=y. tsuruoka&author=y. tateishi&author=j. kim&author=t. ohta&author=j. mcnaught&author=s. ananiadou&author=j. tsujii&journal=advances in informatics&volume=3746&pages=382-392&publication_year=2005&doi=10.1007/11573036_36
 149. http://scholar.google.com/scholar_lookup?title=banner: an executable survey of advances in biomedical id39&author=r. leaman&author=g. gonzalez&pages=652-663&publication_year=2008
 150. https://doi.org/10.1093/bioinformatics/bti475
 151. http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=retrieve&db=pubmed&dopt=abstract&list_uids=15860559
 152. http://scholar.google.com/scholar_lookup?title=abner: an open source tool for automatically tagging genes, proteins and other entity names in text&author=b. settles&journal=bioinformatics&volume=21&issue=14&pages=3191&publication_year=2005&doi=10.1093/bioinformatics/bti475
 153. http://scholar.google.com/scholar_lookup?title=posbiotm-ner in the shared task of bionlp/nlpba 2004&author=y. song&author=e. kim&author=g. lee&author=b. yi&pages=100-103&publication_year=2004
 154. http://scholar.google.com/scholar_lookup?title=exploiting context for biomedical entity recognition: from syntax to the web&author=j. finkel&author=s. dingare&author=h. nguyen&author=m. nissim&author=c. manning&author=g. sinclair&pages=88-91&publication_year=2004
 155. http://nersuite.nlplab.org/
 156. http://scholar.google.com/scholar_lookup?title=biocreative ii gene mention tagging system at ibm watson&author=r. ando&pages=101-103&publication_year=2007
 157. https://doi.org/10.1093/bioinformatics/bth060
 158. http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=retrieve&db=pubmed&dopt=abstract&list_uids=14871877
 159. http://scholar.google.com/scholar_lookup?title=recognizing names in biomedical texts: a machine learning approach&author=g. zhou&author=j. zhang&author=j. su&author=d. shen&author=c. tan&journal=bioinformatics&volume=20&issue=7&pages=1178-90&publication_year=2004&doi=10.1093/bioinformatics/bth060
 160. http://www.ncbi.nlm.nih.gov/pmc/articles/pmc1764467
 161. https://doi.org/10.1186/1471-2105-7-s5-s11
 162. http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=retrieve&db=pubmed&dopt=abstract&list_uids=17254295
 163. http://scholar.google.com/scholar_lookup?title=nerbio: using selected word conjunctions, term id172, and global patterns to improve biomedical id39&author=r. tsai&author=c. sung&author=h. dai&author=h. hung&author=t. sung&author=w. hsu&journal=bmc bioinformatics&volume=7&issue=suppl 5&pages=s11&publication_year=2006&doi=10.1186/1471-2105-7-s5-s11
 164. http://www.ncbi.nlm.nih.gov/pmc/articles/pmc2718659
 165. https://doi.org/10.1093/bioinformatics/btn183
 166. http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=retrieve&db=pubmed&dopt=abstract&list_uids=18586726
 167. http://scholar.google.com/scholar_lookup?title=integrating high dimensional bi-directional parsing models for gene mention tagging&author=c. hsu&author=y. chang&author=c. kuo&author=y. lin&author=h. huang&author=i. chung&journal=bioinformatics&volume=24&issue=13&pages=i286&publication_year=2008&doi=10.1093/bioinformatics/btn183
 168. http://mallet.cs.umass.edu/
 169. http://scholar.google.com/scholar_lookup?title=id33 and id20 with, lr models and parser ensembles&author=k. sagae&pages=1044-1050&publication_year=2007
 170. https://doi.org/10.1093/bioinformatics/bti749
 171. http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=retrieve&db=pubmed&dopt=abstract&list_uids=16267085
 172. http://scholar.google.com/scholar_lookup?title=biothesaurus: a web-based thesaurus of protein and gene names&author=h. liu&author=zz. hu&author=j. zhang&author=ch. wu&journal=bioinformatics&volume=22&pages=103-105&publication_year=2006&doi=10.1093/bioinformatics/bti749
 173. http://scholar.google.com/scholar_lookup?title=biolexicon: a lexical resource for the biology domain&author=y. sasaki&author=s. montemagni&author=p. pezik&author=d. rebholz-schuhmann&author=j. mcnaught&author=s. ananiadou&pages=109-116&publication_year=2008
 174. http://www.ncbi.nlm.nih.gov/pmc/articles/pmc1869017
 175. https://doi.org/10.1186/1471-2105-6-s1-s3
 176. http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=retrieve&db=pubmed&dopt=abstract&list_uids=15960837
 177. http://scholar.google.com/scholar_lookup?title=genetag: a tagged corpus for gene/protein id39&author=l. tanabe&author=n. xie&author=l. thom&author=w. matten&author=w. wilbur&journal=bmc bioinformatics&volume=6&issue=suppl 1&pages=s3&publication_year=2005&doi=10.1186/1471-2105-6-s1-s3
 178. http://www.ncbi.nlm.nih.gov/pmc/articles/pmc2559986
 179. https://doi.org/10.1186/gb-2008-9-s2-s2
 180. http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=retrieve&db=pubmed&dopt=abstract&list_uids=18834493
 181. http://scholar.google.com/scholar_lookup?title=overview of biocreative ii gene mention recognition&author=l. smith&author=l. tanabe&author=r. ando&author=c. kuo&author=i. chung&author=c. hsu&author=y. lin&author=r. klinger&author=c. friedrich&author=k. ganchev&journal=genome biology&volume=9&issue=suppl 2&pages=s2&publication_year=2008&doi=10.1186/gb-2008-9-s2-s2
 182. http://scholar.google.com/scholar_lookup?title=introduction to the bio-entity recognition task at jnlpba&author=j. kim&author=t. ohta&author=y. tsuruoka&author=y. tateisi&author=n. collier&pages=70-75&publication_year=2004
 183. https://doi.org/10.1093/bioinformatics/btg1023
 184. http://scholar.google.com/scholar_lookup?title=genia corpus-a semantically annotated corpus for bio-textmining&author=j. kim&author=t. ohta&author=y. tateisi&author=j. tsujii&journal=bioinformatics&volume=19&pages=180-182&publication_year=2003&doi=10.1093/bioinformatics/btg1023
 185. https://doi.org/10.1075/li.30.1.03nad
 186. http://scholar.google.com/scholar_lookup?title=a survey of id39 and classification&author=d. nadeau&author=s. sekine&journal=lingvisticae investigationes&volume=30&pages=3-26&publication_year=2007&doi=10.1075/li.30.1.03nad
 187. http://www.ncbi.nlm.nih.gov/pmc/articles/pmc1869021
 188. https://doi.org/10.1186/1471-2105-6-s1-s7
 189. http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=retrieve&db=pubmed&dopt=abstract&list_uids=15960841
 190. http://scholar.google.com/scholar_lookup?title=recognition of protein/gene names from text using an ensemble of classifiers&author=g. zhou&author=d. shen&author=j. zhang&author=j. su&author=s. tan&journal=bmc bioinformatics&volume=6&issue=suppl 1&pages=s7&publication_year=2005&doi=10.1186/1471-2105-6-s1-s7
 191. http://scholar.google.com/scholar_lookup?title=ranking algorithms for named-entity extraction: boosting and the voted id88&author=m. collins&pages=489-496&publication_year=2002
 192. http://scholar.google.com/scholar_lookup?title=tackling the, biocreative2 gene mention task with id49 and syntactic parsing&author=a. vlachos&pages=85-87&publication_year=2007
 193. https://doi.org/10.1016/j.jbi.2006.09.002
 194. http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=retrieve&db=pubmed&dopt=abstract&list_uids=17079192
 195. http://scholar.google.com/scholar_lookup?title=evaluation of techniques for increasing recall in a dictionary approach to gene and protein name identification&author=m. schuemie&author=b. mons&author=m. weeber&author=j. kors&journal=journal of biomedical informatics&volume=40&issue=3&pages=316-324&publication_year=2007&doi=10.1016/j.jbi.2006.09.002
 196. http://scholar.google.com/scholar_lookup?title=id49: probabilistic models for segmenting and labeling sequence data&author=j. lafferty&author=a. mccallum&author=f. pereira&pages=282-289&publication_year=2001
 197. https://scholar.google.com/scholar?hl=en&q=wallach h: id49: an introduction. tech. rep., university of pennsylvania, philadelphia, pa, usa 2004
 198. https://scholar.google.com/scholar?hl=en&q=keerthi s, sundararajan s: crf versus id166-struct for sequence labeling. tech. rep., yahoo research 2007
 199. https://doi.org/10.4218/etrij.09.0108.0276
 200. http://scholar.google.com/scholar_lookup?title=fast training of structured id166 using fixed-threshold sequential minimal optimization&author=c. lee&author=m. jang&journal=etri journal&volume=31&issue=2&pages=121-128&publication_year=2009&doi=10.4218/etrij.09.0108.0276
 201. http://scholar.google.com/scholar_lookup?title=learning a two-stage id166/crf sequence classifier&author=g. hoefel&author=c. elkan&pages=271-278&publication_year=2008
 202. http://scholar.google.com/scholar_lookup?title=semi-markov id49 for information extraction&author=s. sarawagi&author=w. cohen&journal=advances in neural information processing systems&volume=17&pages=1185-1192&publication_year=2004
 203. https://doi.org/10.1093/bioinformatics/bts125
 204. http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=retrieve&db=pubmed&dopt=abstract&list_uids=22419783
 205. http://scholar.google.com/scholar_lookup?title=harmonization of gene/protein annotations: towards a gold standard medline&author=d. campos&author=s. matos&author=i. lewin&author=j. oliveira&author=d. rebholz-schuhmann&journal=bioinformatics&volume=28&issue=9&pages=1253-1261&publication_year=2012&doi=10.1093/bioinformatics/bts125
 206. http://scholar.google.com/scholar_lookup?title=a simple algorithm for identifying abbreviation definitions in biomedical text&author=a. schwartz&author=m. hearst&pages=451-462&publication_year=2003
 207. http://scholar.google.com/scholar_lookup?title=rich feature set, unification of bidirectional parsing and dictionary filtering for high f-score gene mention tagging&author=c. kuo&author=y. chang&author=h. huang&author=k. lin&author=b. yang&author=y. lin&author=c. hsu&author=i. chung&pages=105-107&publication_year=2007
 208. https://doi.org/10.3115/1572306.1572333
 209. http://scholar.google.com/scholar_lookup?title=cbr-tagger: a case-based reasoning approach to the gene/protein mention problem&author=m. neves&author=m. chagoyen&author=j. carazo&author=a. pascual-montano&pages=108-109&publication_year=2008
 210. http://alias-i.com/lingpipe/index.html
 211. http://scholar.google.com/scholar_lookup?title=efficiently inducing features of id49&author=a. mccallum&pages=403-410&publication_year=2003
 212. http://scholar.google.com/scholar_lookup?title=generalized expectation criteria for semi-supervised learning of id49&author=g. mann&author=a. mccallum&pages=870-878&publication_year=2008
 213. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54#camposetallicenseebiomedcentralltd
 214. http://creativecommons.org/licenses/by/2.0
 215. https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-14-54
 216. http://citation-needed.springer.com/v2/references/10.1186/1471-2105-14-54?format=refman&flavour=citation
 217. http://citation-needed.springer.com/v2/references/10.1186/1471-2105-14-54?format=refman&flavour=references
 218. http://citation-needed.springer.com/v2/references/10.1186/1471-2105-14-54?format=refman&flavour=full
 219. http://citation-needed.springer.com/v2/references/10.1186/1471-2105-14-54?format=endnote&flavour=citation
 220. http://citation-needed.springer.com/v2/references/10.1186/1471-2105-14-54?format=endnote&flavour=references
 221. http://citation-needed.springer.com/v2/references/10.1186/1471-2105-14-54?format=endnote&flavour=full
 222. http://citation-needed.springer.com/v2/references/10.1186/1471-2105-14-54?format=bibtex&flavour=citation
 223. http://citation-needed.springer.com/v2/references/10.1186/1471-2105-14-54?format=bibtex&flavour=references
 224. http://citation-needed.springer.com/v2/references/10.1186/1471-2105-14-54?format=bibtex&flavour=full
 225. https://bmcbioinformatics.biomedcentral.com/articles/sections/knowledge-based-analysis
 226. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-54/sharedit
 227. https://www.odysseypress.com/onlinehost/reprint_order.php?type=a&page=0&journal=738&doi=10.1186/1471-2105-14-54&volume=14&issue=1&title=gimli:+open+source+and+high-performance+biomedical+name+recognition&author_name=david+campos&start_page=1&end_page=14
 228. mailto:bmcbioinformatics@biomedcentral.com
 229. mailto:info@biomedcentral.com
 230. http://blogs.biomedcentral.com/
 231. https://www.biomedcentral.com/login
 232. https://www.biomedcentral.com/account
 233. https://authorservices.springernature.com/go/10bmc
 234. http://authorservices.springernature.com/scientific-editing/
 235. https://www.biomedcentral.com/about/policies
 236. https://www.biomedcentral.com/accessibility
 237. https://www.biomedcentral.com/about/press-centre
 238. https://www.biomedcentral.com/about/contact-us
 239. https://biomedcentral.typeform.com/to/vlxboo
 240. https://www.biomedcentral.com/about/jobs
 241. https://twitter.com/biomedcentral
 242. https://www.facebook.com/biomedcentral
 243. http://www.weibo.com/biomedcentral
 244. https://www.biomedcentral.com/terms-and-conditions
 245. https://www.biomedcentral.com/privacy-statement
 246. https://www.biomedcentral.com/cookies
 247. javascript:void(0);
 248. http://www.springernature.com/
