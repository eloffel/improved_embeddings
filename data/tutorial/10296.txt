5
1
0
2
 
c
e
d
4

 

 
 
]
l
c
.
s
c
[
 
 

1
v
7
8
5
1
0

.

2
1
5
1
:
v
i
x
r
a

extracting biomolecular interactions using

id29 of biomedical text

sahil garg, aram galstyan, ulf hermjakob, and daniel marcu

usc information sciences institute
{sahil, galstyan, ulf, marcu}@isi.edu

marina del rey, ca 90292

abstract

we advance the state of the art in biomolecular interaction extraction with three contributions: (i) we show that deep, abstract
meaning representations (amr) signi   cantly improve the accuracy of a biomolecular interaction extraction system when compared
to a baseline that relies solely on surface- and syntax-based features; (ii) in contrast with previous approaches that infer relations
on a sentence-by-sentence basis, we expand our framework to enable consistent predictions over sets of sentences (documents);
(iii) we further modify and expand a graph kernel learning framework to enable concurrent exploitation of automatically induced
amr (semantic) and dependency structure (syntactic) representations. our experiments show that our approach yields interaction
extraction systems that are more robust in environments where there is a signi   cant mismatch between training and test conditions.

i. introduction

1 recent advances in genomics and proteomics have signi   cantly accelerated the rate of uncovering and accumulating new
biomedical knowledge. most of this knowledge is available only via scienti   c publications, which necessitates the development
of automated and semi-automated tools for extracting useful biomedical information from unstructured text. in particular,
there has been a signi   cant body of research on identifying biological entities (proteins, genes, chemical compounds) and
interactions between those entities from bio-medical papers [22, 19, 35, 6]. despite the recent progress, current methods for
biomedical knowledge extraction suffer from a number of important shortcomings. first of all, existing methods rely heavily
on shallow analysis techniques that severely limit their scope. for instance, most existing approaches focus on whether there is
an interaction between a pair of proteins while ignoring the interaction types [1, 27], whereas other more advanced approaches
cover only a small subset of all possible interaction types [20, 24, 12]. second, most existing methods focus on single-sentence
extraction, which makes them very susceptible to noise. and    nally, owing to the enormous diversity of research topics in
biomedical literature and the high cost of data annotation, there is often signi   cant mismatch between training and testing
corpora, which re   ects poorly on generalization ability of existing methods [35].

in this paper, we present a novel algorithm for extracting biomolecular interactions from unstructured text that addresses the
above challenges. contrary to the previous works, the extraction task considered here is less restricted and spans a much more
diverse corpus of biomedical articles. these more realistic settings present some important technical problems for which we
provide explicit solutions.

our speci   c contributions are as follows:
    we propose a graph-kernel based algorithm for extracting biomolecular interactions from id15,
or amr. to the best of our knowledge, this is the    rst attempt of using deep id29 for biomedical knowledge
extraction task.

    we provide a multi-sentence generalization of the algorithm by de   ning graph distribution kernels (gdk), which enables

us to perform document-level extraction.

    we suggest a hybrid extraction method that utilizes both amrs and syntactic parses given by stanford dependency
graphs (sdgs). toward this goal, we develop a id202ic formulation for learning vector space embedding of edge
labels in amrs and sdgs to de   ne similarity measures between amrs and sdgs.

we conduct an exhaustive empirical evaluation of the proposed extraction system on 45+ research articles on cancer (ap-
proximately 3k sentences), containing approximately 20,000 positive-negative labeled biomolecular interactions2. our results
indicate that the joint extraction method that leverages both amrs and sdgs parses signi   cantly improves the extraction
accuracy, and is more robust to mismatch between training and test conditions.

1this work has been published previously [15].
2the code and the data are available at https://github.com/sgarg87/big mech isi gg

state
change

bind

inhibit, phosphorylate, signal, activate, transcript, regulate, apoptose, express,
translocate, degrade, carboxymethylate, depalmitoylate, acetylate, nitrosylate,
farnesylate, methylate, glycosylate, hydroxylate, ribosylate, sumoylate, ubiquitinate.
bind, heterodimerize, homodimerize, dissociate.

table i

interaction type examples

fig. 1. amr of text    as a result, mutant ras proteins accumulate with elevated gtp-bound proportion.   ; interaction    ras binds to gtp    is extracted from
the colored sub-graph.

ii. problem statement

consider the sentence    as a result, mutant ras proteins accumulate with elevated gtp-bound proportion   , which describes

a    binding    interaction between a protein    ras    and a small-molecule    gtp   . we want to extract this interaction.

in our representation, which is motivated by biopax [12], an interaction refers to either i) an entity effecting state change
of another entity; or ii) an entity binding/dissociating with another entity to form/break a complex while, optionally, also
in   uenced by a third entity. an entity can be of any type existent in a bio pathway, such as protein, complex, enzyme, etc,
although here we refer to an entity of all valid types simply as a protein. the change in state of an entity or binding type is
simply termed as    interaction type    in this work. in some cases, entities are capable of changing their state on their own or
bind to an instance of its own (self-interaction). such special cases are also included. some examples of interaction types are
shown in table i.

below we describe our approach for extracting above-de   ned interactions from natural language parses of sentences in a

research document.

a. amr biomedical corpus

iii. extracting interactions from an amr

id15, or amr, is a semantic annotation of single/multiple sentences [3]. in contrast to syntactic
parses, in amr, entities are identi   ed, typed and their semantic roles are annotated. amr maps different syntactic constructs
to same conceptual term. for instance,    binding   ,    bound   ,    bond    correspond to the same concept    bind-01   . because one
amr representation subsumes multiple syntactic representations, we hypothesize that amrs have higher utility for extracting
biomedical interactions.

we trained an english-to-amr parser [31] on two manually annotated corpora: i) a corpus of 17k general domain sentences
including newswire and web text as published by the linguistic data consortium; and ii) 3.4k systems biology sentences,
including in-domain pubmedcentral papers and the bel biocreative corpus. as part of building the bio-speci   c amr corpus,
we extended the propbank-based framesets used in amr by 45 bio-speci   c frames such as    phosphorylate-01   ,    immunoblot-
01    and extended the list of amr standard named entities by 15 types such as    enzyme   ,    pathway   . it is important to note
that these extensions are not speci   c to biomolecular interactions, and cover more general cancer biology concepts.

result-01accumulate-01arg2include-91ras enzymeproportionarg3arg1topbind-01modmutate-01arg1gtp small-moleculearg1arg2elevate-01arg1arg2ras enzymearg1b. extracting interactions

fig. 1 depicts a manual amr annotation of a sentence, which has two highlighted entity nodes with labels    ras    and
   gtp   . these nodes also have entity type annotations,    enzyme    and    small-molecule    respectively; the concept node with
a node label    bind-01    corresponds to an interaction type    binding    (from the    gtp-bound    in the text). the interaction
   ras-binds-gtp    is extracted from the highlighted subgraph under the    bind    node. in the subgraph, relationship between
the interaction node    bind-01    and the entity nodes,    ras    and    gtp   , is de   ned through two edges with edge labels    arg1   
and    arg2    respectively. additionally, in the subgraph, we assign roles    interaction-type   ,    protein   ,    protein    to the nodes
   bind-01   ,    ras   ,    gtp    respectively (roles presented with different colors in the subgraph).

given an amr graph, as in fig. 1, we    rst identify potential entity nodes (proteins, molecules, etc) and interaction
nodes (bind, activate, etc). next, we consider all permutations to generate a set of potential interactions according to the
format de   ned above. for each candidate interaction, we extract the corresponding shortest path subgraph. we then project the
subgraph to a tree structure 3 with the interaction node as root and also possibly the protein nodes (entities involved in the
interaction) as leaves.
is an amr subgraph constructed such that it can represent
an extracted candidate interaction ii with interaction node as root and proteins nodes as leaves typically; and l = {0, 1} is a
binary label indicating whether this subgraph contains ii or not. given a training set, and a new sample amr subgraph ga   
for interaction i   , we would like to infer whether i    is valid or not. we address this problem by developing a graph-kernel
based approach.

our training set consists of tuples {ga

i=1, where ga
i

i , ii, li}n

c. semantic embedding based graph kernel

we propose an extension of the contiguous subtree kernel [38, 11] for mapping the extracted subgraphs (tree structure) to
an implicit feature space. originally, this kernel uses an identity function on two node labels when calculating the similarity
between those two nodes. we instead propose to use vector space embedding of the node labels [10, 26], and then de   ne a sparse
rbf kernel on the node label vectors. similar extensions of convolution kernels have been been suggested previously[25, 33].
consider two graphs gi and gj rooted at nodes gi.r and gj.r, respectively, and let gi.c and gj.c be the children nodes

of the corresponding root nodes. then the kernel between gi and gj is de   ned as follows:

(cid:40)

k(gi, gj) =

0
k(i, j) + kc(gi.c, gj.c) otherwise

if k(i, j) = 0

,

where k(i, j)     k(gi.r, gj.r) is the similarity between the root nodes, whereas kc(gi.c, gj.c) is the recursive part of the
kernel that measures the similarity of the children subgraphs. furthermore, the similarity between root nodes x and y is de   ned
as follows:

(cid:18) (wt

(cid:19)(cid:18) (wt

k(x, y) = kw(x, y)2(kw(x, y)2 + ke(x, y) + kr(x, y))

x wy       )
(1       )
kw(x, y) = exp
ke(x, y) = i(ex = ey), kr(x, y) = i(rx = ry) .

x wy     1)

  

+

(cid:19)

(1)

here (  )+ denotes the positive part; i(  ) is the indicator function; wx, wy are unit vector embeddings of node labels 4; ex, ey
represent edge labels (label of an edge from a node   s parent to it is the node   s edge label); rx, ry are roles of nodes (such as
protein, catalyst, concept, interaction-type);    is a threshold parameter on the cosine similarity (wt
x wy) to control sparsity [16];
and    is the bandwidth.

the recursive part of the kernel, kc, is de   ned as follows:

(cid:88)

  l(i) (cid:88)

(cid:89)

kc(gi.c, gj.c) =

k(gi[i[s]], gj[j[s]])

k(gi[i[s]].r, gj[j[s]].r),

i,j:l(i)=l(j)

s=1,       ,l(i)

s=1,       ,l(i)

where i, j are contiguous children subsequences under the respective root nodes gi.r, gj.r;        (0, 1) is a tuning parameter;
and l(i) is the length of sequence i = i1,       , il; gi[i[s]] is a sub-tree rooted at i[s] index child node of gi.r. here, we
propose to sort children of a node based on the corresponding edge labels. this helps in distinguishing between two mirror
image trees.

this extension is a valid id81 (zelenko et al.). next, we generalize the id145 approach of zelenko

et al. for ef   cient calculation of this extended kernel.

3this can be done via so called inverse edge labels; see [3, section 3].
4learned using id97 software [26] on over one million pubmed articles.

(2)

(3)

(4)

1) id145 for computing convolution graph kernel: in the convolution kernel presented above, the main
computational cost is due to comparison of children sub-sequences. since different children sub-sequences of a given root
node partially overlap with each other, one can use id145 to avoid redundant computations, thus reducing the
cost. toward this goal, we use the following decomposition of the kernel kc :

kc(gi.c, gj.c) =

cp,q ,

(cid:88)

p,q

where cp,q refers to the similarity between sub-sequences starting at indices p, q respectively in gi.c and gj.c.

to calculate cp,q via id145, let us introduce

lp,q = max

l

k(gi[i[p + s]].r, gj[j[q + s]].r) (cid:54)= 0

.

(cid:19)

furthermore, let us denote kp,q = k(gi[i[p]].r, gj[j[q]].r), and kp,q = k(gi[i[p]], gj[j[q]]). we then evaluate cp,q in a
recursive manner using the following equations.

(cid:18) l(cid:89)

s=0

(cid:40)

(cid:40)

cp,q =

lp,q =

0
lp+1,q+1 + 1

if kp,q = 0
otherwise

0
  (1     l(p,q))

1      kp,qkp,q +   cp+1,q+1

if kp,q = 0
otherwise

lm+1,n+1 = 0, lm+1,n = 0, lm,n+1 = 0
cm+1,n+1 = 0, cm+1,n = 0, cm,n+1 = 0 ,
where m, n are number of children under the root nodes gi.r and gj.r respectively.

note that for graphs with cycles, the above dynamic program can be transformed into a linear program.
there are a couple of practical considerations during the kernel computations. first of all, the kernel depends on two tunable
parameters    and   . intuitively, decreasing    discounts the contributions of longer child sub-sequences. the parameter   , on
the other hand, controls the tradeoff between computational cost and accuracy. based on some prior tuning we found that
our results are not very sensitive to the parameters. in the experiments below we set    = 0.99 and    = 0.4. also, consistent
with previous studies, we normalize the graph kernel (e.g., kernel similarity k(gi, gj) is divided by the id172 term

(cid:112)k(gi, gi)k(gj, gj)) to increase accuracy.

iv. graph distribution kernel- gdk

often an interaction is mentioned more than once in the same research paper, which justi   es a document-level extraction,
where one combines evidence from multiple sentences. the prevailing approach to document-level extraction is to    rst perform
id136 at sentence level, and then combine those id136s using some type of an aggregation function for a    nal document-
level id136 [32, 7]. for instance, in [7], the id136 with the maximum score is chosen. we term this baseline approach
as    maximum score id136   , or msi. here we advocate a different approach, where one uses the evidences from multiple
sentences jointly, for a collective id136.
let us assume an interaction im is supported by km sentences, and let {gm1,       , gmkm} be the set of relevant amr
subgraphs extracted from those sentences. we can view the elements of this set as samples from some distribution over the
graphs, which, with a slight abuse of notation, we denote as gm. consider now interactions i1,       , ip, and let g1,       ,gp be
graph distributions representing these interactions.

the graph distribution kernel (gdk), k(gi,gj), for a pair gi,gj is de   ned as follows:

k(gi,gj) = exp(   dmm(gi,gj));
dmm(gi,gj) =
k(gir, gis)

ki(cid:88)

r,s=1

k2
i

kj(cid:88)

r,s=1

+

k(gjr, gjs)

k2
j

ki,kj(cid:88)

r,s=1

    2

k(gir, gjs)

kikj

here dmm is the maximum mean discrepancy (mmd), a valid l2 norm, between a pair of distributions gi,gj [17]; k(., .) is
the graph kernel de   ned in section iii-c (though, not restricted to this speci   c kernel). as the term suggests, maximum mean
discrepancy represents the discrepancy between the mean of graph kernel features (features implied by kernels) in samples of
distributions gi and gj. now, since dmm is the l2 norm on the mean feature vectors, k(gp,gq) is a valid id81.

we note that mmd metric has attracted a considerable attention in the machine learning community recently [17, 21, 30, 5].
for our purpose, we prefer using this divergence metric over others (such as kl-d divergence) for the following reasons:
i) dmm(., .) is a    kernel trick    based formulation, nicely    tting with our settings since we do not have explicit features
representation of the graphs but only kernel density on the graph samples. same is true for kl-d estimation with kernel
density method. ii) empirical estimate of dmm(., .) is a valid l2 norm distance. therefore, it is straightforward to derive the
graph distribution kernel k(gi,gj) from dmm(gi,gj) using a function such as rbf. this is not true for divergence metrics
such as kl-d, renyi [34]; iii) it is suitable for compactly supported distributions (small number of samples) whereas methods,
such as k-nearest neighbor estimation of kl-d, are not suitable if the number of samples in a distribution is too small [36];
iv) we have seen the most consistent results in our extraction experiments using this metric as opposed to the others.

for the above mentioned reasons, here we focus on mmd as our primary metric for computing similarities between graph
distributions. the proposed gdk framework, however, is very general and not limited to a speci   c metric. next, we brie   y
describe two other metrics that can be used with gdk.

a) gdk with id181: while mmd represents maximum discrepancy between the mean features of
two distributions, the id181 (kl-d) is a more comprehensive (and fundamental) measure of distance
between two distributions5. for de   ning kernel kkl in terms of kl-d, however, we have two challenges. first of all, kl-d
is not a symmetric function. this problem can be addressed by using a symmetric version of the distance in the rbf kernel,

kkl(gi,gj) = exp(   [dkl(gi||gj) + dkl(gj||gi)])

where dkl(gi||gj) is the kl distance of the distribution gi w.r.t. the distribution gj. and second, even the symmetric
combination of the divergences is not a valid euclidian distance. hence, kkl is not guaranteed to be a positive semi-de   nite
function. this issue can be dealt in a practical manner as nicely discussed in [34]. namely, having computed the gram matrix
using kkl, we can project it onto a positive semi-de   nite one by using id202ic techniques, e.g., by discarding negative
eigenvalues from the spectrum.
since we do not know the true divergence, we approximate it with its empirical estimate from the data,
dkl(gi||gj)       dkl(gi||gj). while there are different approaches for estimating divergences from samples [36], here
we use kernel density estimator as shown below:

b) gdk with cross kernels: another simple way to evaluate similarity between two distributions is to take the mean of

cross-kernel similarities between the corresponding two sample sets:

  dkl(gi||gj) =

1
ki

log

1
ki
1
kj

s=1 k(gir, gis)
s=1 k(gir, gjs)

(cid:80)ki
(cid:80)kj

ki(cid:88)

r=1

ki,kj(cid:88)

k(gi,gj) =

k(gir, gjs)

r,s=1

kikj

note that this metric looks quite similar to the mmd. as demonstrated in our experiments, however, mmd does better,
presumably because it accounts for the mean kernel similarity between samples of the same distribution.

having de   ned the graph distribution kernel-gdk, k(., .), our revised training set consists of tuples {gi, ii, li}n
i=1 with
i1,       , ga
sample sub-graphs in gi. for inferring an interaction i   , we evaluate gdk between a test distribution g    and
ga
the train distributions {g1,       ,gn}, from their corresponding sample sets. then, one can apply any    kernel trick    based
classi   er.

iki

v. cross representation similarity

in the previous section, we proposed a novel algorithm for document-level extraction of interactions from amrs. looking
forward, we will see in our experiments (section vi) that amrs yield better extraction accuracy compared to sdgs. this
result suggests that using deep semantic features is very useful for the extraction task. on the other hand, the accuracy of
semantic (amr) parsing is not as good as the accuracy of shallow parsers like sdgs [31, 14, 37, 2, 9]. thus, one can ask
whether the joint use of semantic (amrs) and syntactic (sdgs) parses can improve extraction accuracy further.

there are some intuitive observations that justify the joint approach: i) shallow syntactic parses may be suf   cient for correctly
extracting a subset of interactions; ii) semantic parsers might make mistakes that are avoidable in syntactic ones. for instance,
in machine translation based semantic parsers [31, 2], hallucinating phrasal translations may introduce an interaction/protein
in a parse that is non-existent in true semantics; iii) over    t of syntactic/semantic parsers can vary from each other in a test
corpus depending upon the data used in their independent trainings.

5recall that the kl divergence between distributions p and q is de   ned as dkl(p||q) = ep(x)[log p(x)
q(x) ]

id15

(a / activate-01

:arg0 (s / protein :name (n1 / name :op1 "ras"))
:arg1 (s / protein :name (n2 / name :op1 "b-raf"))

stanford typed dependency

nsubj(activates-2, ras-1)
root(root-0, activates-2)
acomp(activates-2, b-raf-3)

fig. 2. amr and sdg parses of    ras activates b-raf.   

in this setting, in each evidence sentence, a candidate interaction ii is represented by a tuple   i = {ga
i and gs

i} of sub-graphs
i which are constructed from amr and sdg parses of a sentence respectively. our problem is to classify the
ga
interaction jointly on features of both sub-graphs. this can be further extended for the use of multiple evidence sentences. we
now argue that the graph-kernel framework outlined above can be applied to this setting as well, with some modi   cations.

i , gs

let   i and   j be two sets of points. to apply the framework above, we need a valid kernel k(  i,   j) de   ned on the
joint space. one way of de   ning this kernel would be using similarity measures between amrs and sdgs separately, and
then combining them e.g., via linear combination. however, here we advocate a different approach, where we    atten the joint
representation. each candidate interaction is represented as a set of two points in the same space. this projection is a valid
operation as long as we have a similarity measure between ga
i (correlation between the two original dimensions).
this is rather problematic since amrs and sdgs have non-overlapping edge labels (although the space of node labels of
both representations coincide). to address this issue, for inducing this similarity measure, we next develop our approach for
edge-label vector space embedding.

i and gs

let us understand what we mean by vector space embedding of edge-labels. in fig. 2, we have an amr and a sdg parse
of    ras activates b-raf   .    arg0    in the amr and    nsubj    in sdg are conveying that    ras    is a catalyst of the interaction
   activation   ;    arg1    and    acomp    are meaning that    b-raf    is activated. in this sentence,    arg0    and    nsubj    are playing
the same role though their higher dimensional roles, across a diversity set of sentences, would vary. along these lines, we
propose to embed these high dimensional roles in a vector space, termed as    edge label vectors   .

a. consistency equations for edge vectors

we now describe our unsupervised algorithm that learns vector space embedding of edge labels. the algorithm works by
imposing linear consistency conditions on the word vector embeddings of node labels. while we describe the algorithm using
amrs, it is directly applicable to sdgs as well.

1) id202ic formulation: in our formulation, we    rst learn subspace embedding of edge labels (edge label matrices)
and then transform it into vectors by    attening. let us see the amr in fig. 2 again. we already have word vectors embedding
for terms    activate   ,    ras   ,    b-raf   , denoted as wactivate, wras, wbraf respectively; a word vector wi     rm  1. let
embedding for edge labels    arg0    and    arg1    be aarg0, aarg1; ai     rm  m. in this amr, we de   ne following linear
algebraic equations.

wactivate = at
at

arg0wras = at

arg1wbraf

arg0wras, wactivate = at

arg1wbraf

the edge label matrices at
arg1 are linear transformations on the word vectors wras, wbraf , establishing linear
consistencies between the word vectors along the edges. one can de   ne such a set of equations in each parent-children
nodes sub-graph in a given set of manually annotated amrs (and so applies to sdgs independent of amrs). along these
lines, for a pair of edge labels i, j in amrs, we have generalized equations as below.

arg0, at

y i = x iai, y j = x jaj, zij

i ai = zij

j aj

here ai, aj are edge labels matrices. considering ni occurrences of edge labels i, we correspondingly have word vectors from
the ni child node labels stacked as rows in matrix x i     rni  m; and y i     rni  m from the parent node labels. there would
be a subset of instances, nij <= ni, nj where edge labels i and j has same parent node (occurrence of pairwise relationship
j     rnij  m, subsets of word vectors in x i and x j respectively (along
between i and j). this gives zij
rows). along these lines, neighborhood of edge label i is de   ned to be: n (i) : j     n (i) s.t. nij > 0. from the above pairwise

i     rnij  m and zij

linear consistencies, we derive linear dependencies of an ai with its neighbors aj : j     n (i), while also applying least square
approximation.

(cid:88)

j   n (i)

(cid:88)

j   n (i)

x t

i y i +

t

zij
i

zij

j aj = (x t

i x i +

t

zij
i

zij

i )ai

exploiting the block structure in the linear program, we propose an algorithm that is a variant of    gauss-seidel    method [13, 28].
algorithm 1. (a) initialize:

(b) iteratively update a(t+1)

i

until convergence:

a(0)

i = (x t

i x i)   1x t

i y i.

      x t

(cid:88)

j   n (i)

         1      x t

      

zij

j a(t)

j

(cid:88)

j   n (i)

t

zij
i

a(t+1)

i

=

i x i +

t

zij
i

zij
i

i y i +

(c) set the inverse edge label matrices:

aiinv = a   1

i

.

theorem 6.2 in [13][p. 287, chapter 6] states that the gauss-seidel method converges if the linear transformation matrix in
a linear program is strictly row diagonal dominant [28]. in our formulation, diagonal blocks dominate the non-diagonal ones
row-wise. thus, algorithm 1 should converge to an optimum.

using algorithm 1, we learned edge label matrices in amrs and sdgs independently on corresponding amrs and sdgs
annotations from 2500 bio-sentences (high accuracy auto-parse for sdgs). convergence was fast for both amrs and sdgs
(log error drops from 10.14 to 10.02 for amrs, and from 30 to approx. 10 for sdgs).
next, we    atten an edge label matrix ai     rm  m to a corresponding edge label vector 6 ei     rm2  1, and then rede   ne

ke(x, y) in (1) using the sparse rbf kernel.

(cid:18) et

(cid:19)(cid:18) et

x ey     1

  

x ey       
1       

(cid:19)

+

ke(x, y) = exp

this rede   nition enables to de   ne kernel similarity between amrs and sdgs. one can either use our original formulation
where a single amr/sdg sub-graph is classi   ed using training sub-graphs from both amrs and sdgs, and then the id136
i} as samples of a graph
[7] is chosen. another option, preferable, is to consider the set {ga
with maximum score-msi
},
distribution gi representing an interaction ii. generalizing it further, gi has samples set {ga
containing ka
i number of sub-graphs in amrs and sdgs respectively from multiple sentences in a document, all for
classifying ii. with this graph distribution representation, we can apply our gdk from section iv and then infer using a
   kernel trick    based classi   er. this    nal formulation gives the best results in our experiments discussed next.

i , gs
i1,       , ga

i1,       , gs

i , ks

, gs

ika
i

iks
i

we evaluated the proposed algorithm on two data sets.

a. data sets

vi. experimental evaluation

1) pubmed45: this dataset has 400 manual and 3k auto parses of amrs (and 3.4k auto parses of sdgs)7; amrs
auto-parses are from 45 pubmed articles on cancer. from the 3.4k amrs, we extract 25k subgraphs representing 20k
interactions (valid/invalid); same applies to sdgs. this is our primary data for the evaluation.

we found that for both amr and sgd based methods, a part of the extraction error can be attributed to poor recognition of
named entities. to minimize this effect, and to isolate errors that are speci   c to the extraction methods themselves, we follow
the footsteps of the previous studies, and take a    ltered subset of the interactions (approx. 10k out of 20k). we refer to this
data subset as    pubmed45    and the super set as    pubmed45-ern    (for entity recognition noise).

2) aimed: this is a publicly available dataset8, which contains about 2000 sentences from 225 abstracts. in contrast to
pubmed45, this dataset is very limited as it describes only whether a given pair of proteins interact or not, without specifying
the interaction type. nevertheless, we    nd it useful to include this dataset in our evaluation since it enables us to compare our
results with other reported methods.

6alternatives for kernel directly on the matrices instead of the    attening can be more accurate, that we plan to explore in the future
7not the same 2.5k sentences used in learning edge label vectors
8http://corpora.informatik.hu-berlin.de

methods
sdg (sli)

amr (sli)

sdg (msi)

amr (msi)

sdg (gdk)

amr (gdk)

amr-sdg (msi)

amr-sdg (gdk)

data statistics
positive ratio
train-test div.

pubmed45-ern
0.25    0.16
(0.42, 0.29)
0.33    0.16
(0.33, 0.45)
0.24    0.14
(0.39, 0.28)
0.32    0.14
(0.30, 0.45)
0.25    0.16
(0.33, 0.31)
0.35    0.16
(0.31, 0.51)
0.33    0.18
(0.29, 0.54)
0.38    0.16
(0.33, 0.55)

pubmed45
0.32    0.18
(0.50, 0.35)
0.45    0.25
(0.58, 0.43)
0.33    0.17
(0.50, 0.34)
0.45    0.24
(0.56, 0.44)
0.38    0.15
(0.32, 0.61)
0.51    0.23
(0.59, 0.49)
0.47    0.24
(0.50, 0.53)
0.57    0.23
(0.63, 0.54)

aimed
0.27    0.12
(0.54, 0.22)
0.39    0.05
(0.53, 0.33)
0.39    0.09
(0.51, 0.38)
0.51    0.11
(0.49, 0.56)
0.47    0.08
(0.41, 0.58)
0.51    0.11
(0.43, 0.65)
0.55    0.09
(0.46, 0.73)
0.52    0.09
(0.43, 0.67)

0.07    0.04
0.014    0.019

0.19    0.14
0.041    0.069

0.37    0.11
0.005    0.002

f1 score statistics.    sli    is sentence level id136;    msi    refers to maximum score id136 at document level;    gdk   

denotes graph distribution kernel based id136 at document level. precision, recall statistics are presented as

(mean-precision, mean-recall) tuples.

table ii

b. evaluation settings

in a typical evaluation scenario, validation is performed by random sub-sampling of labeled interactions (at sentence level)
for a test subset, and using the rest as a training set. this sentence-level validation approach is not always appropriate for
extracting protein interactions [35], since interactions from a single/multiple sentences in a document can be correlated. such
correlations can lead to information leakage between training and test sets (arti   cial match, not encountered in real settings).
for instance, in [27], the reported f1 score from the random validation in the aimed data is approx. 0.5. our algorithm, even
using sdgs, gives 0.66 f1 score in those settings. however, the performance drops signi   cantly when an independent test
document is processed. therefore, for a realistic evaluation, we divide data sets at documents level into approx. 10 subsets such
that there is minimal match between a subset, chosen as test set, and the rest of sub sets used for training a kernel classi   er.
in the pubmed45 data sets, the 45 articles are clustered into 11 subsets by id91 pubmed-ids (training data also includes
gold annotations). in aimed, abstracts are clustered into 10 subsets on abstract-ids. in each of 25 independent test runs (5 for
aimed data) on a single test subset, 80% interactions are randomly sub sampled from the test subset and same percent from
the train data.
for the classi   cation, we use the libid166 implementation of kernel support vector machines [8] with the sklearn python
wrapper 9. speci   cally, we used settings { id203 = t rue, c = 1, class weight = auto}. in our data, we have a class
   swap    in addition to the two binary classes (   valid   ,    invalid   ). the    swap    class means that an interaction is invalid as such
but swapping of entity roles in the interaction makes it valid. for the analysis purpose however, we focus on f1 scores only
for the positive class, i.e. class    valid   .

c. evaluation results

we categorize all methods evaluated below as follows: i) sentence level id136-sli 10; ii) document level using maximum
score id136-msi [7]; and iii) document-level id136 on all the subgraphs using our graph distribution kernel (gdk).
in each of the categories, amrs, sdgs are used independently, and then jointly. edge label vectors are used only when amrs
and sdgs are jointly used, referred as    amr-sdg   .

table ii shows the f1 score statistics for all the experiments. in addition, the mean of precision and recall values are
presented as (precision, recall) tuples in the same table. for most of the following discussion, we focus on f1 scores only to
keep the exposition simple.

9http://scikit-learn.org/stable/modules/generated/sklearn.id166.svc.html
10note that even for the sentence level id136, the training/test division is done on document level.

sdg

amr

mmd
0.25    0.16
(0.33, 0.31)
0.35    0.16
(0.31, 0.51)

kl-d
0.21    0.17
(0.59, 0.21)
0.37    0.17
(0.50, 0.41)

ck
0.26    0.13
(0.29, 0.38)
0.29    0.13
(0.28, 0.39)

comparison of f1 scores for different divergence metrics used with gdk. the evaluation is on pubmed45-ern dataset.    kl-d   

and    ck    stand for id181 and cross kernels, respectively.

table iii

before going into detailed discussion of the results, we make the following two observations. first, we can see that, in
all methods (including our gdk and baselines), we obtain much better accuracy using amrs compared to sdgs. this
result is remarkable, especially taking into account the fact that the accuracy of id29 is still signi   cantly lower
when compared to syntactic parsing. and second, observe that the overall accuracy numbers are considerably lower for the
pubmed45-ern data, compared to the    ltered data pubmed45.

let us focus on document-level extraction using msi. we do not see much improvement in numbers compared to sli for
our pubmed45 data. on the other hand, even this simple msi technique works for the restricted extraction settings in the
aimed data. msi works for aimed data probably because there are multiple sub-graph evidences with varying interaction
types (root node in subgraphs), even in a single sentence, all representing same protein-protein pair interaction. this high
number of evidences at document level, should give a boost in performance even using msi.

next, we consider document-level extraction using the proposed gdk method with the mmd metric. comparing against
the baseline sli, we see a signi   cant improvement for all data sets and in both amrs and sdgs (although the improvement
in pubmed45-ern is relatively small). the effect of the noise in entity recognition can be a possible reason why gdk does
not work so well in this data compared to the other two data sets. here, we also see that: a) gdk method performs better
than the document level baseline msi; and b) amrs perform better than sdgs with gdk method also.

let us now consider the results of extraction using both amrs and sdgs jointly. here we evaluate msi and gdk, both
using our edge label vectors. our primary observation here is that the joint id136 using both amrs and sdgs improves the
extraction accuracy across all datasets. furthremore, in both pubmed45 datasets, the proposed gdk method is a more suitable
choice for the joint id136 on amrs and sdgs. as we can see, comparing to gdk for amrs only, f1 points increment
from 0.35 to 0.38 for the pubmed45-ern data, and from 0.51 to 0.57 for the pubmed45 data. for the aimed dataset, on the
other hand, the best result (f1 score of 0.55) is obtained when one uses the baseline msi for the joint id136 on amrs
and sdgs.

to get more insights, we now consider (mean-precision, mean-recall) tuples shown in the table ii. the general trend is
that the amrs lead to higher recall compared to the sdgs. in the pubmed45-ern data set, this increase in the recall is at
cost of a drop in the precision values. since the entity types are noisy in this data set, this drop in the precision numbers
is not completely surprising (note that the f1 scores still increase). with the use of the gdk method in the same data set,
however, the precision drop (sdgs to amrs) becomes negligible, while the recall still increases signi   cantly. in the data
set pubmed45 (the one without noise in the entity types), both the precision and recall are generally higher for the amrs
compared to the sdgs. again, there is an exception for the gdk approach, for which the recall decreases slightly. however,
the corresponding precision almost doubles.

for a more    ne-grained comparison between the methods, we plot f1 score for each individual test set in fig. 3. here, we
compare the baselines,    amr (msi)   ,    sdg (msi)    against the    amr-sdg (gdk)    in our data (and,    amr-sdg (msi)   
for    aimed   ). we see a general trend, across all test subsets, of amrs being more accurate than sdgs and the joint use of
two improving even upon amrs. though, there are some exceptions where the difference is marginal between the three. in
our cross checking, we    nd that such exceptions are when there is relatively more information leakage between train-test, i.e.
less train-test divergence. we use maximum mean discrepancy-mmd for evaluating this train-test divergence (originally used
for de   ning gdk in section iv. we    nd that our gdk technique is more suitable when m m d > 0.01 (mmd is normalized
metric for a normalized graph kernel).

the results for the gdk method described above are speci   c to the mmd metric. we also evaluated gdk using two other
metrics (kl-d and cross kernels), speci   cally on    pubmed45-ern    dataset, as presented in table iii. here, as in table ii,
we also present (mean-precision, mean-recall) tuples. we can see that mmd and kl-d metrics, both, perform equally well
for amr whereas mmd does better in case of sdg. ck (cross kernels), which is a relatively naive approach, also performs
reasonably well, although for the amrs it performs worse compared to mmd and kl-d. for the precision and recall numbers
in the table iii, we see similar trends as reported in table ii. we observe that the recall numbers increase for the amrs
compared to the sdgs (the metric ck is an exception with negligible increase). also, comparing kl-d against mmd, we
see the former favors (signi   cantly) higher precision, albeit at the expense of lower recall values.

(a) pubmed45

(b) aimed

fig. 3.

comparison of extraction accuracy (f1 score)

vii. related work

there have been different lines of work for extracting protein extractions. pattern-matching based systems (either manual or
semi-automated) usually yield high precision but low recall [20, 22, 19, 18]. kernel-based methods based on various convolution
kernels have also been developed for the extraction task [35, 1, 27]. some approaches work on string rather than parses [27].
the above mentioned works either rely on text or its shallow parses, none using id29 for the extraction task.
also, most works consider only protein-protein interactions while ignoring interaction types. some recent works used distant
supervision to obtain a large data set of protein-protein pairs for their experiments [23].

document-level extraction has been explored in the past [32, 7]. these works classify at sentence level and then combine

the id136s whereas we propose to infer jointly on all the sentences at document level.

previously, the idea of linear relational embedding has been explored in [29], where triples of concepts and relation types
between those concepts are (jointly) embedded in some latent space. neural networks have also been employed for joint
embedding [4]. here we advocate for a factored embedding where concepts (node labels) are embedded    rst using plain text,
and then relations (edge labels) are embedded in a linear sub-space.

in summary, we have developed and validated a method for extracting biomolecular interactions that, for the    rst time, uses
deep semantic parses of biomedical text (amrs). we have presented a novel algorithm, which relies on graph distribution
kernels (gdk) for document-level extraction of interactions from a set of amrs in a document. gdk can operate on both

viii. conclusion

1234567891011pubmedarticlesetsoncancerresearch0.00.20.40.60.81.0f1scoresdg(msi)amr(msi)amr-sdg(gdk)12345678910aimedpapersabstractsets0.00.20.40.60.81.0f1scoresdg(msi)amr(msi)amr-sdg(msi)amr and sdg parses of sentences jointly. the rationale behind this hybrid approach is that while neither parsing is perfect,
their combination can yield superior results. indeed, our experimental results suggest that the proposed approach outperforms
the baselines, especially in the practically relevant scenario when there is a noticeable mismatch between the training and test
sets.

to facilitate the joint approach, we have proposed a novel edge vector space embedding method to assess similarity between
different types of parses. we believe this notion of edge-similarly is quite general and will have applicability for a wider class
of problems involving graph kernels. as a future work, we intend to validate this framework on a number of problems such
as improving accuracy in amrs parsing with sdgs.

this work was sponsored by the darpa big mechanism program (w911nf-14-1-0364). it is our pleasure to acknowledge
fruitful discussions with michael pust, kevin knight, shuyang gao, linhong zhu, neal lawton, emilio ferrara, and greg ver
steeg. we are also grateful to anonymous reviewers for their valuable feedback.

ix. acknowledgements

references

[1] antti airola, sampo pyysalo, jari bj  orne, tapio pahikkala, filip ginter, and tapio salakoski. all-paths graph kernel for

protein-protein interaction extraction with evaluation of cross-corpus learning. bmc bioinformatics, 9:s2, 2008.

[2] jacob andreas, andreas vlachos, and stephen clark. id29 as machine translation. in proc. of acl, pages

47   52, 2013.

[3] laura banarescu, claire bonial, shu cai, madalina georgescu, kira grif   tt, ulf hermjakob, kevin knight, philipp
koehn, martha palmer, and nathan schneider. id15 for sembanking. in proceedings of the
7th linguistic annotation workshop and interoperability with discourse, 2013.

[4] antoine bordes, xavier glorot, jason weston, and yoshua bengio. a semantic matching energy function for learning

with multi-relational data. machine learning, 94:233   259, 2014.

[5] karsten m borgwardt, arthur gretton, malte j rasch, hans-peter kriegel, bernhard sch  olkopf, and alex j smola.

integrating structured biological data by kernel maximum mean discrepancy. bioinformatics, 22:e49   e57, 2006.

[6] razvan bunescu, ruifang ge, rohit j kate, edward m marcotte, raymond j mooney, arun k ramani, and yuk wah
wong. comparative experiments on learning information extractors for proteins and their interactions. arti   cial
intelligence in medicine, pages 139   155, 2005.

[7] razvan bunescu, raymond mooney, arun ramani, and edward marcotte.

information extraction for robust retrieval of protein interactions from medline.
natural language processing and biology: towards deeper biological literature analysis, 2006.

integrating co-occurrence statistics with
in proc. of the workshop on linking

[8] chih-chung chang and chih-jen lin. libid166: a library for support vector machines. acm transactions on intelligent

[9] danqi chen and christopher d manning. a fast and accurate dependency parser using neural networks.

in proc. of

systems and technology (tist), 2:27, 2011.

emnlp, pages 740   750, 2014.

[10] stephen clark. vector space models of lexical meaning. handbook of contemporary semantics, 2nd ed. blackwell, 2014.
[11] aron culotta and jeffrey sorensen. dependency tree kernels for id36. in proc. of acl, page 423, 2004.
[12] emek demir, michael p cary, suzanne paley, ken fukuda, christian lemer, imre vastrik, guanming wu, peter
d   eustachio, carl schaefer, joanne luciano, et al. the biopax community standard for pathway data sharing. nature
biotechnology, 28:935   942, 2010.

[13] james w demmel. applied numerical id202. siam, 1997.
[14] j. flanigan, s. thomson, j. carbonell, c. dyer, and n. a. smith. a discriminative graph-based parser for the abstract

[15] sahil garg, aram galstyan, ulf hermjakob, and daniel marcu. extracting biomolecular interactions using semantic

meaning representation. in proc. of acl, 2014.

parsing of biomedical text. in proc. of aaai, 2016.

[16] tilmann gneiting. compactly supported correlation functions. journal of multivariate analysis, 83:493   508, 2002.
[17] arthur gretton, karsten m borgwardt, malte j rasch, bernhard sch  olkopf, and alexander smola. a kernel two-sample

test. jmlr, 13:723   773, 2012.

[18] marco a valenzuela-esc  arcega gus hahn and powell thomas hicks mihai surdeanu. a domain-independent rule-based

framework for event extraction. acl-ijcnlp 2015, page 127, 2015.

[19] j  org hakenberg, conrad plake, loic royer, hendrik strobelt, ulf leser, and michael schroeder. gene mention

id172 and interaction extraction with context models and sentence motifs. genome biol, 9:s14, 2008.

[20] lawrence hunter, zhiyong lu, james firby, william a baumgartner, helen l johnson, philip v ogren, and k bretonnel
cohen. opendmap: an open source, ontology-driven concept analysis engine, with applications to capturing knowledge
regarding protein transport, protein interactions and cell-type-speci   c gene expression. bmc bioinformatics, 9:78, 2008.

[21] beomjoon kim and joelle pineau. maximum mean discrepancy imitation learning. in proc. of rss, 2013.
[22] martin krallinger, florian leitner, carlos rodriguez-penagos, and alfonso valencia. overview of the protein-protein

interaction annotation extraction task of biocreative ii. genome biology, 9:s4, 2008.

[23] emily k mallory, ce zhang, christopher r  e, and russ b altman. large-scale extraction of gene interactions from

full-text literature using deepdive. bioinformatics, page btv476, 2015.

[24] ryan mcdonald, fernando pereira, seth kulick, scott winters, yang jin, and pete white. simple algorithms for complex

id36 with applications to biomedical ie. in proc. of acl, 2005.

[25] yashar mehdad, alessandro moschitti, and fabio massimo zanzotto. syntactic/semantic structures for id123
in human language technologies: the 2010 annual conference of the north american chapter of the

recognition.
association for computational linguistics, pages 1020   1028, 2010.

[26] tomas mikolov, ilya sutskever, kai chen, greg s corrado, and jeff dean. distributed representations of words and

phrases and their compositionality. in proc. of nips, 2013.

[27] raymond j mooney and razvan c bunescu. subsequence kernels for id36.

in proc. of nips, pages

[28] w niethammer, j de pillis, and rs varga. convergence of block iterative methods applied to sparse least-squares

problems. id202 and its applications, 58:327   341, 1984.

[29] alberto paccanaro and geoffrey e hinton. learning distributed representations by mapping concepts and relations into

a linear space. in proc. of icml, pages 711   718, 2000.

[30] sinno jialin pan, james t kwok, and qiang yang. id21 via id84. in proc. of aaai,

[31] michael pust, ulf hermjakob, kevin knight, daniel marcu, and jonathan may. using syntax-based machine translation

to parse english into id15. in proc. of emnlp, 2015.

[32] marios skounakis and mark craven. evidence combination in biomedical natural-language processing.

in proc. of

[33] shashank srivastava, dirk hovy, and eduard h hovy. a walk-based semantically enriched tree kernel over distributed

word representations. in proc. of emnlp, pages 1411   1416, 2013.

[34] dougal j sutherland, liang xiong, barnab  as p  oczos, and jeff schneider. kernels on sample sets via nonparametric

divergence estimates. arxiv preprint arxiv:1202.0302, 2012.

[35] domonkos tikk, philippe thomas, peter palaga, j  org hakenberg, and ulf leser. a comprehensive benchmark of kernel

methods to extract protein   protein interactions from literature. plos comput biol, 2010.

[36] qing wang, sanjeev r kulkarni, and sergio verd  u. divergence estimation for multidimensional densities via-nearest-

neighbor distances. ieee transactions on id205, pages 2392   2405, 2009.

[37] yushi wang, jonathan berant, and percy liang. building a semantic parser overnight. in proc. of acl, 2015.
[38] dmitry zelenko, chinatsu aone, and anthony richardella. kernel methods for id36. jmlr, 3:1083   1106,

2003.

171   178, 2005.

2008.

biokdd, 2003.

