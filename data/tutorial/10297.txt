6
1
0
2

 

n
u
j
 

8

 
 
]
l
c
.
s
c
[
 
 

5
v
6
1
8
6
0

.

5
0
5
1
:
v
i
x
r
a

representing meaning with a combination
of logical and distributional models

i. beltagy
computer science department
the university of texas at austin

pengxiang cheng
computer science department
the university of texas at austin

raymond j. mooney
computer science department
the university of texas at austin

stephen roller
computer science department
the university of texas at austin

katrin erk
linguistics department
the university of texas at austin

nlp tasks differ in the semantic information they require, and at this time no single se-
mantic representation ful   lls all requirements. logic-based representations characterize sentence
structure, but do not capture the graded aspect of meaning. distributional models give graded
similarity ratings for words and phrases, but do not capture sentence structure in the same detail
as logic-based approaches. so it has been argued that the two are complementary.

we adopt a hybrid approach that combines logical and id65 using
probabilistic logic, speci   cally markov logic networks (mlns). in this paper, we focus on the
three components of a practical system:1 1) logical representation focuses on representing
the input problems in probabilistic logic. 2) knowledge base construction creates weighted
id136 rules by integrating distributional information with other sources. 3) probabilistic
id136 involves solving the resulting mln id136 problems ef   ciently. to evaluate our
approach, we use the task of id123 (rte), which can utilize the strengths of both
logic-based and distributional representations. in particular we focus on the sick dataset, where
we achieve state-of-the-art results. we also release a lexical entailment dataset of 10,213 rules
extracted from the sick dataset, which is a valuable resource for evaluating lexical entailment
systems.2

1. introduction

computational semantics studies mechanisms for encoding the meaning of natural
language in a machine-friendly representation that supports automated reasoning and
that, ideally, can be automatically acquired from large text corpora. effective semantic
representations and reasoning tools give computers the power to perform complex
applications like id53. but applications of computational semantics are
very diverse and pose differing requirements on the underlying representational for-

1 system is available for download at: https://github.com/ibeltagy/pl-semantics
2 available at: https://github.com/ibeltagy/rrr

   2005 association for computational linguistics

computational linguistics

volume xx, number xx

malism. some applications bene   t from a detailed representation of the structure of
complex sentences. some applications require the ability to recognize near-paraphrases
or degrees of similarity between sentences. some applications require id136, either
exact or approximate. often it is necessary to handle ambiguity and vagueness in
meaning. finally, we frequently want to learn knowledge relevant to these applications
automatically from corpus data.

there is no single representation for natural language meaning at this time that ful-
   lls all of the above requirements, but there are representations that ful   ll some of them.
logic-based representations (montague 1970; dowty, wall, and peters 1981; kamp and
reyle 1993) like    rst-order logic represent many linguistic phenomena like negation,
quanti   ers, or discourse entities. some of these phenomena (especially negation scope
and discourse entities over paragraphs) can not be easily represented in syntax-based
representations like natural logic (maccartney and manning 2009). in addition,    rst-
order logic has standardized id136 mechanisms. consequently, logical approaches
have been widely used in id29 where it supports answering complex
natural language queries requiring reasoning and data aggregation (zelle and mooney
1996; kwiatkowski et al. 2013; pasupat and liang 2015). but logic-based representations
often rely on manually constructed dictionaries for lexical semantics, which can result
in coverage problems. and    rst-order logic, being binary in nature, does not capture
the graded aspect of meaning (although there are combinations of logic and proba-
bilities). distributional models (turney and pantel 2010) use contextual similarity to
predict the graded semantic similarity of words and phrases (landauer and dumais
1997; mitchell and lapata 2010), and to model polysemy (sch  tze 1998; erk and pad  
2008; thater, f  rstenau, and pinkal 2010). but at this point, fully representing structure
and logical form using distributional models of phrases and sentences is still an open
problem. also, current distributional representations do not support logical id136
that captures the semantics of negation, logical connectives, and quanti   ers. therefore,
distributional models and logical representations of natural language meaning are com-
plementary in their strengths, as has frequently been remarked (coecke, sadrzadeh, and
clark 2011; garrette, erk, and mooney 2011; grefenstette and sadrzadeh 2011; baroni,
bernardi, and zamparelli 2014).

our aim has been to construct a general-purpose natural language understanding
system that provides in-depth representations of sentence meaning amenable to au-
tomated id136, but that also allows for    exible and graded id136s involving
word meaning. therefore, our approach combines logical and distributional methods.
speci   cally, we use    rst-order logic as a basic representation, providing a sentence
representation that can be easily interpreted and manipulated. however, we also use
distributional information for a more graded representation of words and short phrases,
providing information on near-synonymy and lexical entailment. uncertainty and grad-
edness at the lexical and phrasal level should inform id136 at all levels, so we
rely on probabilistic id136 to integrate logical and id65. thus,
our system has three main components, all of which present interesting challenges.
for logic-based semantics, one of the challenges is to adapt the representation to the
assumptions of the probabilistic logic (beltagy and erk 2015). for distributional lexical
and phrasal semantics, one challenge is to obtain appropriate weights for id136
rules (roller, erk, and boleda 2014). in probabilistic id136, the core challenge is
formulating the problems to allow for ef   cient mln id136 (beltagy and mooney
2014).

our approach has previously been described in garrette, erk, and mooney (2011)
and beltagy et al. (2013). we have demonstrated the generality of the system by ap-

2

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

plying it to both id123 (rte-1 in beltagy et al. (2013), sick (preliminary
results) and fracas in beltagy and erk (2015)) and semantic textual similarity (sts)
similarity (beltagy, erk, and mooney 2014), and we are investigating applications to
id53. we have demonstrated the modularity of the system by testing
both markov logic networks (richardson and domingos 2006) and probabilistic soft
logic (broecheler, mihalkova, and getoor 2010) as probabilistic id136 engines (belt-
agy et al. 2013; beltagy, erk, and mooney 2014).

the primary aim of the current paper is to describe our complete system in detail,
all the nuts and bolts necessary to bring together the three distinct components of our
approach, and to showcase some of the dif   cult problems that we face in all three areas
along with our current solutions.

the secondary aim of this paper is to show that it is possible to take this general
approach and apply it to a speci   c task, here id123 (dagan et al. 2013),
adding task-speci   c aspects to the general framework in such a way that the model
achieves state-of-the-art performance. we chose the task of id123 because
it utilizes the strengths of both logical and distributional representations. we speci   cally
use the sick dataset (marelli et al. 2014b) because it was designed to focus on lexical
knowledge rather than world knowledge, matching the focus of our system.

our system is    exible with respect to the sources of lexical and phrasal knowledge it
uses, and in this paper we utilize ppdb (ganitkevitch, van durme, and callison-burch
2013) and id138 along with distributional models. but we are speci   cally interested
in distributional models, in particular in how well they can predict lexical and phrasal
entailment. our system provides a unique framework for evaluating distributional
models on rte because the overall sentence representation is handled by the logic, so
we can zoom in on the performance of distributional models at predicting lexical (geffet
and dagan 2005) and phrasal entailment. the evaluation of distributional models on
rte is the third aim of our paper. we build a lexical entailment classi   er that exploits
both task-speci   c features as well as distributional information, and present an in-depth
evaluation of the distributional components.

we now provide a brief sketch of our framework (garrette, erk, and mooney 2011;
beltagy et al. 2013). our framework is three components, the    rst is the logical form
which is the primary meaning representation for a sentence. the second is the distri-
butional information which is encoded in the form of weighted logical rules (   rst-order
formulas). for example, in its simplest form, our approach can use the distributional
similarity of the words grumpy and sad as the weight on a rule that says if x is grumpy
then there is a chance that x is also sad:

   x.grumpy(x)     sad(x) | f (sim(

(cid:126)grumpy, (cid:126)sad))

(cid:126)grumpy and (cid:126)sad are the vector representations of the words grumpy and sad,
where
sim is a distributional similarity measure, like cosine, and f is a function that maps the
similarity score to an mln weight. a more principled, and in fact superior, choice is to
use an asymmetric similarity measure to compute the weight, as we discuss below.

the third component is id136. we draw id136s over the weighted rules
using markov logic networks (mln) (richardson and domingos 2006), a statistical
relational learning (srl) technique (getoor and taskar 2007) that combines logical
and statistical knowledge in one uniform framework, and provides a mechanism for
coherent probabilistic id136. mlns represent uncertainty in terms of weights on the

3

computational linguistics

volume xx, number xx

logical rules as in the example below:

   x. ogre(x)     grumpy(x) | 1.5
   x, y. (f riend(x, y)     ogre(x))     ogre(y) | 1.1

(1)

which states that there is a chance that ogres are grumpy, and friends of ogres tend to
be ogres too. markov logic uses such weighted rules to derive a id203 distribution
over possible worlds through an undirected graphical model. this id203 distribu-
tion over possible worlds is then used to draw id136s.

we publish a dataset of the lexical and phrasal rules that our system queries when
running on sick, along with gold standard annotations. the training and testing sets
are extracted from the sick training and testing sets respectively. the total number of
rules (training + testing) is 12,510, only 10,213 are unique with 3,106 entailing rules,
177 contradictions and 6,928 neutral. this is a valuable resource for testing lexical en-
tailment systems, containing a variety of entailment relations (hypernymy, synonymy,
antonymy, etc.) that are actually useful in an end-to-end rte system.

in addition to providing further details on the approach introduced in garrette, erk,
and mooney (2011) and beltagy et al. (2013) (including improvements that improve the
scalability of mln id136 (beltagy and mooney 2014) and adapt logical constructs
for probabilistic id136 (beltagy and erk 2015)) this paper makes the following new
contributions:
    we show how to represent the rte task as an id136 problem in probabilistic logic

(sections 4.1, 4.2), arguing for the use of a closed-word assumption (section 4.3).

    contradictory rte sentence pairs are often only contradictory given some assump-
tion about entity coreference. for example, an ogre is not snoring and an ogre is snoring
are not contradictory unless we assume that the two ogres are the same. handling
such coreferences is important to detecting many cases of contradiction (section 4.4).

    we use multiple parses to reduce the impact of misparsing (section 4.5).
    in addition to distributional rules, we add rules from existing databases, in particular
id138 (princeton university 2010) and the paraphrase collection ppdb (ganitke-
vitch, van durme, and callison-burch 2013) (section 5.3).

    a logic-based alignment to guide generation of distributional rules (section 5.1).
    a dataset of all lexical and phrasal rules needed for the sick dataset (10,213 rules).
this is a valuable resource for testing lexical entailment systems on entailment rela-
tions that are actually useful in an end-to-end rte system (section 5.1).

    evaluate a state-of-the-art compositional distributional approach (paperno, pham,

and baroni 2014) on the task of phrasal entailment (section 5.2.5).

    a simple weight learning approach to map rule weights to mln weights (section 6.3).
    the question    do supervised distributional methods really learn lexical id136
relations?    (levy et al. 2015) has been studied before on a variety of lexical entailment
datasets. for the    rst time, we study it on data from an actual rte dataset and show
that distributional information is useful for lexical entailment (section 7.1).

    marelli et al. (2014a) report that for the sick dataset used in semeval 2014, the
best result was achieved by systems that did not compute a sentence representation
in a compositional manner. we present a model that performs deep compositional
semantic analysis and achieves state-of-the-art performance (section 7.2).

4

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

2. background

logical semantics. logical representations of meaning have a long tradition in lin-
guistic semantics (montague 1970; dowty, wall, and peters 1981; kamp and reyle 1993;
alshawi 1992) and computational semantics (blackburn and bos 2005; van eijck and
unger 2010), and commonly used in id29 (zelle and mooney 1996; berant
et al. 2013; kwiatkowski et al. 2013). they handle many complex semantic phenomena
such as negation and quanti   ers, they identify discourse referents along with the pred-
icates that apply to them and the relations that hold between them. however, standard
   rst-order logic and theorem provers are binary in nature, which prevents them from
capturing the graded aspects of meaning in language: synonymy seems to come in
degrees (edmonds and hirst 2000), as does the difference between senses in polysemous
words (brown 2008). van eijck and lappin (2012) write:    the case for abandoning the
categorical view of competence and adopting a probabilistic model is at least as strong
in semantics as it is in syntax.   

recent wide-coverage tools that use logic-based sentence representations include
copestake and flickinger (2000), bos (2008), and lewis and steedman (2013). we use
boxer (bos 2008), a wide-coverage semantic analysis tool that produces logical forms
using discourse representation structures (kamp and reyle 1993). it builds on the c&c
id35 (id35) parser (clark and curran 2004) and maps
sentences into a lexically-based logical form, in which the predicates are mostly words
in the sentence. for example, the sentence an ogre loves a princess is mapped to:
   x, y, z. ogre(x)     agent(y, x)     love(y)     patient(y, z)     princess(z)

(2)
as can be seen, boxer uses a neo-davidsonian framework (parsons 1990): y is an event
variable, and the semantic roles agent and patient are turned into predicates linking y
to the agent x and patient z.

as we discuss below, we combine boxer   s logical form with weighted rules and
perform probabilistic id136. lewis and steedman (2013) also integrate logical and
distributional approaches, but use distributional information to create predicates for a
standard binary logic and do not use probabilistic id136. much earlier, hobbs et al.
(1988) combined logical form with weights in an abductive framework. there, the aim
was to model the interpretation of a passage as its best possible explanation.

id65. distributional models (turney and pantel 2010) use statistics
on contextual data from large corpora to predict semantic similarity of words and
phrases (landauer and dumais 1997; mitchell and lapata 2010). they are motivated
by the observation that semantically similar words occur in similar contexts, so words
can be represented as vectors in high dimensional spaces generated from the contexts
in which they occur (landauer and dumais 1997; lund and burgess 1996). therefore,
distributional models are relatively easier to build than logical representations, auto-
matically acquire knowledge from    big data   , and capture the graded nature of linguistic
meaning, but they do not adequately capture logical structure (grefenstette 2013).

distributional models have also been extended to compute vector representations
for larger phrases, e.g. by adding the vectors for the individual words (landauer and
dumais 1997) or by a component-wise product of word vectors (mitchell and lapata
2008, 2010), or through more complex methods that compute phrase vectors from word
vectors and tensors (baroni and zamparelli 2010; grefenstette and sadrzadeh 2011).

5

computational linguistics

volume xx, number xx

integrating logic-based and id65. it does not seem particularly useful
at this point to speculate about phenomena that either a distributional approach or a
logic-based approach would not be able to handle in principle, as both frameworks are
continually evolving. however, logical and distributional approaches clearly differ in
the strengths that they currently possess (coecke, sadrzadeh, and clark 2011; garrette,
erk, and mooney 2011; baroni, bernardi, and zamparelli 2014). logical form excels at
in-depth representations of sentence structure and provides an explicit representation
of discourse referents. distributional approaches are particularly good at representing
the meaning of words and short phrases in a way that allows for modeling degrees of
similarity and entailment and for modeling word meaning in context. this suggests that
it may be useful to combine the two frameworks.

another argument for combining both representations is that it makes sense from
a theoretical point of view to address meaning, a complex and multifaceted phe-
nomenon, through a combination of representations. meaning is about truth, and logical
approaches with a model-theoretic semantics nicely address this facet of meaning.
meaning is also about a community of speakers and how they use language, and
distributional models aggregate observed uses from many speakers.

there are few hybrid systems that integrate logical and distributional information,

and we discuss some of them below.

beltagy et al. (2013) transform distributional similarity to weighted distributional
id136 rules that are combined with logic-based sentence representations, and use
probabilistic id136 over both. this is the approach that we build on in this paper.
lewis and steedman (2013), on the other hand, use id91 on distributional data to
infer word senses, and perform standard    rst-order id136 on the resulting logical
forms. the main difference between the two approaches lies in the role of gradience.
lewis and steedman view weights and probabilities as a problem to be avoided. we
believe that the uncertainty inherent in both language processing and world knowl-
edge should be front and center in all inferential processes. tian, miyao, and takuya
(2014) represent sentences using dependency-based id152 (liang,
jordan, and klein 2011). they construct phrasal entailment rules based on a logic-based
alignment, and use distributional similarity of aligned words to    lter rules that do not
surpass a given threshold.

also related are distributional models where the dimensions of the vectors encode
model-theoretic structures rather than observed co-occurrences (clark 2012; sadrzadeh,
clark, and coecke 2013; grefenstette 2013; herbelot and vecchi 2015), even though
they are not strictly hybrid systems as they do not include contextual distributional
information. grefenstette (2013) represents logical constructs using vectors and tensors,
but concludes that they do not adequately capture logical structure, in particular quan-
ti   ers.

if like andrews, vigliocco, and vinson (2009), silberer and lapata (2012) and bruni
et al. (2012) (among others) we also consider perceptual context as part of distributional
models, then cooper et al. (2015) also quali   es as a hybrid logical/distributional ap-
proach. they envision a classi   er that labels feature-based representations of situations
(which can be viewed as perceptual distributional representations) as having a certain
id203 of making a proposition true, for example smile(sandy). these propositions
function as types of situations in a type-theoretic semantics.

probabilistic logic with markov logic networks. to combine logical and probabilistic
information, we utilize markov logic networks (mlns) (richardson and domingos
2006). mlns are well suited for our approach since they provide an elegant framework

6

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

figure 1: a sample ground network for a markov logic network

for assigning weights to    rst-order logical rules, combining a diverse set of id136
rules and performing sound probabilistic id136.

a weighted rule allows truth assignments in which not all instances of the rule hold.
equation 1 above shows sample weighted rules: friends of ogres tend to be ogres and
ogres tend to be grumpy. suppose we have two constants, anna (a) and bob (b). using
these two constants and the predicate symbols in equation 1, the set of all ground atoms
we can construct is:

la,b = {ogre(a), ogre(b), grumpy(a), grumpy(b), f riend(a, a),

f riend(a, b), f riend(b, a), f riend(b, b)}

if we only consider models over a domain with these two constants as entities, then each
truth assignment to la,b corresponds to a model. mlns make the assumption of a one-
to-one correspondence between constants in the system and entities in the domain. we
discuss the effects of this domain closure assumption below.

markov networks or undirected id114 (pearl 1988) compute the prob-
ability p (x = x) of an assignment x of values to the sequence x of all variables
in the model based on clique potentials, where a clique potential is a function that
assigns a value to each clique (maximally connected subgraph) in the graph. markov
logic networks construct markov networks (hence their name) based on weighted
   rst order logic formulas, like the ones in equation 1. figure 1 shows the network
for equation 1 with two constants. every ground atom becomes a node in the graph,
and two nodes are connected if they co-occur in a grounding of an input formula.
in this graph, each clique corresponds to a grounding of a rule. for example, the
clique including f riend(a, b), ogre(a), and ogre(b) corresponds to the ground rule
f riend(a, b)     ogre(a)     ogre(b). a variable assignment x in this graph assigns to
each node a value of either true or false, so it is a truth assignment (a world). the
clique potential for the clique involving f riend(a, b), ogre(a), and ogre(b) is exp(1.1)
if x makes the ground rule true, and 0 otherwise. this allows for nonzero id203
for worlds x in which not all friends of ogres are also ogres, but it assigns exponentially
more id203 to a world for each ground rule that it satis   es.

more generally, an mln takes as input a set of weighted    rst-order formulas f =
f1, . . . , fn and a set c of constants, and constructs an undirected graphical model in
which the set of nodes is the set of ground atoms constructed from f and c. it computes
the id203 distribution p (x = x) over worlds based on this undirected graphical
model. the id203 of a world (a truth assignment) x is de   ned as:

p (x = x) =

1
z

exp

wini (x)

(3)

where i ranges over all formulas fi in f , wi is the weight of fi, ni(x) is the number
of groundings of fi that are true in the world x, and z is the partition function

7

(cid:33)

(cid:32)(cid:88)

i

ogre(a)ogre(b)friend(a, b)friend(b, a)friend(b, b)friend(a, a)grumpy(a)grumpy(b)computational linguistics

volume xx, number xx

(i.e., it normalizes the values to probabilities). so the id203 of a world increases
exponentially with the total weight of the ground clauses that it satis   es.

below, we use r (for rules) to denote the input set of weighted formulas. in addition,
an mln takes as input an evidence set e asserting truth values for some ground
clauses. for example, ogre(a) means that anna is an ogre. marginal id136 for mlns
calculates the id203 p (q|e, r) for a query formula q.

alchemy (kok et al. 2005) is the most widely used mln implementation. it is a
software package that contains implementations of a variety of mln id136 and
learning algorithms. however, developing a scalable, general-purpose, accurate infer-
ence method for complex mlns is an open problem. mlns have been used for various
nlp applications including unsupervised coreference resolution (poon and domingos
2008), id14 (riedel and meza-ruiz 2008) and event extraction (riedel
et al. 2009).

recognizing id123. the task that we focus on in this paper is recognizing
id123 (rte) (dagan et al. 2013), the task of determining whether one
natural language text, the text t , entails, contradicts, or is not related (neutral) to another,
the hypothesis h.    entailment    here does not mean logical entailment: the hypothesis
is entailed if a human annotator judges that it plausibly follows from the text. when
using naturally occurring sentences, this is a very challenging task that should be able
to utilize the unique strengths of both logic-based and id65. here
are examples from the sick dataset (marelli et al. 2014b):

    entailment
t: a man and a woman are walking together through the woods.
h: a man and a woman are walking through a wooded area.
    contradiction
t: nobody is playing the guitar
h: a man is playing the guitar
    neutral
t: a young girl is dancing
h: a young girl is standing on one leg

the sick (   sentences involving compositional knowledge   ) dataset, which we
use for evaluation in this paper, was designed to foreground particular linguistic phe-
nomena but to eliminate the need for world knowledge beyond linguistic knowledge.
it was constructed from sentences from two image description datasets, imageflickr3
and the semeval 2012 sts msr-video description data.4 randomly selected sentences
from these two sources were    rst simpli   ed to remove some linguistic phenomena
that the dataset was not aiming to cover. then additional sentences were created as
variations over these sentences, by id141, negation, and reordering. rte pairs
were then created that consisted of a simpli   ed original sentence paired with one of the
transformed sentences (generated from either the same or a different original sentence).
we would like to mention two particular systems that were evaluated on sick.
the    rst is lai and hockenmaier (2014) which was the top performing system at the
original shared task. they use a linear classi   er with many hand crafted features,
including alignments, word forms, pos tags, distributional similarity, id138, and

3 http://nlp.cs.illinois.edu/hockenmaiergroup/data.html
4 http://www.cs.york.ac.uk/semeval-2012/task6/index.php?id=data

8

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

figure 2: system architecture

a unique feature called denotational similarity. many of these hand crafted features
are later incorporated in our lexical entailment classi   er, described in section 5.2. the
denotational similarity uses a large database of human- and machine-generated image
captions to cleverly capture some world knowledge of entailments.

the second system is bjerva et al. (2014) which also participated in the original
sick shared task, and achieved 81.6% accuracy. the rte system uses boxer to parse
input sentences to logical form, then uses a theorem prover and a model builder to
check for entailment and contradiction. the knowledge bases used are id138 and
ppdb. in contrast with our work, ppdb paraphrases are not translated to logical rules
(section 5.3). instead, in case a ppdb paraphrase rule applies to a pair of sentences,
the rule is applied at the text level before parsing the sentence. theorem provers and
model builders have high precision detecting entailments and contradictions, but low
recall. to improve recall, neutral pairs are reclassi   ed using a set of textual, syntactic
and semantic features.

3. system overview

this section provides an overview of our system   s architecture, using the following rte
example to demonstrate the role of each component:

t : a grumpy ogre is not smiling.
h: a monster with a bad temper is not laughing.

which in logic are:
t :    x. ogre(x)     grumpy(x)          y. agent(y, x)     smile(y)
h:    x, y. monster(x)     with(x, y)     bad(y)     temper(y)          z. agent(z, x)     laugh(z).
this example needs the following rules in the knowledge base kb:
r1: laugh     smile
r2: ogre     monster
r3: grumpy     with a bad temper

9

parsing using boxer (section 4.1)task representation, dca and quantifiers(sections 4.2-4.5)mln id136p(h|t, kb, wt,h)p(  h|t, kb, wt,  h)(sections 6.1, 6.2)modified robinson resolution(section 5.1)entailment rules classifier (section 5.2)weight learning(section 6.3)t, hppdb rulesid138 ruleskbt, ht, hweighted rulesresultt, h,   h wt,h, ,wt,  h2) knowledge base construction3) id1361) logical representationdistributional modelproposed rulescomputational linguistics

volume xx, number xx

figure 2 shows the high-level architecture of our system and figure 3 shows the

mlns constructed by our system for the given rte example.
d : {o, l, co}
g : {ogre(o), grumpy(o), monster(o), agent(l, o), smile(l), laugh(l),

skolemf (o, co), with(o, co), bad(co), temper(co)}

t : ogre(o)     grumpy(o)          y. agent(y, o)     smile(y) |    
r1 :    x. laugh(x)     smile(x) | w1    wppdb
r2 :    x. ogre(x)     monster(x) | wwn =    
r3 :    x. grumpy(x)        y. skolemf (x, y)     with(x, y)     bad(y)     temper(y) |

w3    weclassif

sk : skolemf (o, co) |    
a :    x. agent(l, x)     laugh(l) | 1.5
h :    x, y. monster(x)     with(x, y)     bad(y)     temper(y)          z. agent(z, x)     laugh(z)

(a) mln to calculate p (h|t, kb, wt,h )

d : {o, co, m, t}
g : {ogre(o), grumpy(o), monster(o), skolemf (o, co), with(o, co), bad(co),

temper(co), monster(m ), with(m, t ), bad(t ), temper(t )}

t : ogre(o)     grumpy(o)          y. agent(y, o)     smile(y) |    
r1 :    x. laugh(x)     smile(x) | w1    wppdb
r2 :    x. ogre(x)     monster(x) | wwn =    
r3 :    x. grumpy(x)        y. skolemf (x, y)     with(x, y)     bad(y)     temper(y) |

w3    weclassif

sk : skolemf (o, co) |    
a : monster(m )     with(m, t )     bad(t )     temper(t ) | 1.5
  h :      x, y. monster(x)     with(x, y)     bad(y)     temper(y)          z. agent(z, x)     laugh(z)

(b) mln to calculate p (  h|t, kb, wt,  h )

figure 3: mlns for the given rte example. the rte task is represented as two in-
ferences p (h|t, kb, wt,h ) and p (  h|t, kb, wt,  h ) (section 4.1). d is the set of
constants in the domain. t and r3 are skolemized and sk is the skolem function of r3
(section 4.2). g is the set of non-false (true or unknown) ground atoms as determined
by the cwa (section 4.3, 6.2). a is the cwa for the negated part of h (section 4.3).
d, g, a are the world assumptions wt,h ( or wt,  h). r1, r2, r3 are the kb. r1 and
its weight w1 are from ppdb (section 5.3). r2 is from id138 (section 5.3). r3 is
constructed using the modi   ed robinson resolution (section 5.1), and its weight w3
is calculated using the entailment rules classi   er (section 5.2). the resource speci   c
weights wppdb, weclassif are learned using weight learning (section 6.3). finally the two
probabilities are calculated using mln id136 where h (or   h) is the query formula
(section 6.1)

our system has three main components:

10

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

1. logical representation (section 4), where input natural sentences t and h are
mapped into logic then used to represent the rte task as a probabilistic id136
problem.

2. knowledge base construction kb (section 5), where the background knowledge
is collected from different sources, encoded as    rst-order logic rules, weighted
and added to the id136 problem. this is where distributional information is
integrated into our system.

3. id136 (section 6), which uses mlns to solve the resulting id136 problem.
one powerful advantage of using a general-purpose probabilistic logic as a se-
mantic representation is that it allows for a highly modular system. therefore, the
most recent advancements in any of the system components, in parsing, in knowledge
base resources and id65, and in id136 algorithms, can be easily
incorporated into the system.

in the logical representation step (section 4), we map input sentences t and h to
logic. then, we show how to map the three-way rte classi   cation (entailing, neutral,
or contradicting) to probabilistic id136 problems. the mapping of sentences to logic
differs from standard    rst order logic in several respects because of properties of the
probabilistic id136 system. first, mlns make the domain closure assumption
(dca), which states that there are no objects in the universe other than the named
constants (richardson and domingos 2006). this means that constants need to be
explicitly introduced in the domain in order to make probabilistic logic produce the
expected id136s. another representational issue that we discuss is why we should
make the closed-world assumption, and its implications on the task representation.

in the knowledge base construction step kb (section 5), we collect id136 rules
from a variety of sources. we add rules from existing databases, in particular word-
net (princeton university 2010) and ppdb (ganitkevitch, van durme, and callison-
burch 2013). to integrate id65, we use a variant of robinson res-
olution to align the text t and the hypothesis h, and to    nd the difference between
them, which we formulate as an entailment rule. we then train a lexical and phrasal
entailment classi   er to assess this rule. ideally, rules need be contextualized to handle
polysemy, but we leave that to future work.

in the id136 step (section 6), automated reasoning for mlns is used to perform
the rte task. we implement an mln id136 algorithm that directly supports query-
ing complex logical formula, which is not supported in the available mln tools (beltagy
and mooney 2014). we exploit the closed-world assumption to help reduce the size of
the id136 problem in order to make it tractable (beltagy and mooney 2014). we also
discuss weight learning for the rules in the knowledge base.

4. logical representation

the    rst component of our system parses sentences into logical form and uses this to
represent the rte problem as mln id136. we start with boxer (bos 2008), a rule-
based semantic analysis system that translates a id35 parse into a logical form. the
formula

   x, y, z. ogre(x)     agent(y, x)     love(y)     patient(y, z)     princess(z)

(4)
is an example of boxer producing discourse representation structures using a neo-
davidsonian framework. we call boxer   s output alone an    uninterpreted logical form   
because the predicate symbols are simply words and do not have meaning by them-

11

computational linguistics

volume xx, number xx

selves. their semantics derives from the knowledge base kb we build in section 5. the
rest of this section discusses how we adapt boxer output for mln id136.

4.1 representing tasks as text and query

representing natural language understanding tasks. in our framework, a language
understanding task consists of a text and a query, along with a knowledge base. the text
describes some situation or setting, and the query in the simplest case asks whether
a particular statement is true of the situation described in the text. the knowledge
base encodes relevant background knowledge: lexical knowledge, world knowledge,
or both. in the id123 task, the text is the text t , and the query is the
hypothesis h. the sentence similarity (sts) task can be described as two text/query
pairs. in the    rst pair, the    rst sentence is the text and the second is the query, and
in the second pair the roles are reversed (beltagy, erk, and mooney 2014). in question
answering, the input documents constitute the text and the query has the form h(x) for
a variable x; and the answer is the entity e such that h(e) has the highest id203
given the information in t .

in this paper, we focus on the simplest form of text/query id136, which applies
to both rte and sts: given a text t and query h, does the text entail the query given the
knowledge base kb? in standard logic, we determine entailment by checking whether
t     kb     h. (unless we need to make the distinction explicitly, we overload notation
and use the symbol t for the logical form computed for the text, and h for the logical
form computed for the query.) the probabilistic version is to calculate the id203
p (h|t, kb, wt,h ), where wt,h is a world con   guration, which includes the size of
the domain. we discuss wt,h in sections 4.2 and 4.3. while we focus on the simplest
form of text/query id136, more complex tasks such as id53 still have
the id203 p (h|t, kb, wt,h ) as part of their calculations.

representing id123. rte asks for a categorical decision between three
categories, entailment, contradiction, and neutral. a decision about entailment can
be made by learning a threshold on the id203 p (h|t, kb, wt,h ). to differen-
tiate between contradiction and neutral, we additionally calculate the id203
p (  h|t, kb, wt,  h ). if p (h|t, kb, wt,h ) is high while p (  h|t, kb, wt,  h ) is low,
this indicates entailment. the opposite case indicates contradiction. if the two prob-
abilities values are close, this means t does not signi   cantly affect the id203 of
h, indicating a neutral case. to learn the thresholds for these decisions, we train an
id166 classi   er with libid166   s default parameters (chang and lin 2001) to map the two
probabilities to the    nal decision. the learned mapping is always simple and re   ects
the intuition described above.

4.2 using a fixed domain size

mlns compute a id203 distribution over possible worlds, as described in sec-
tion 2. when we describe a task as a text t and a query h, the worlds over which
the mln computes a id203 distribution are    mini-worlds   , just large enough to
describe the situation or setting given by t . the id203 p (h|t, kb, wt,h ) then
describes the id203 that h would hold given the id203 distribution over the

12

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

worlds that possibly describe t . 5 the use of    mini-worlds    is by necessity, as mlns
can only handle worlds with a    xed domain size, where    domain size    is the number
of constants in the domain. (in fact, this same restriction holds for all current practical
probabilistic id136 methods, including psl (bach et al. 2013).)

formally, the in   uence of the set of constants on the worlds considered by an
mln can be described by the domain closure assumption (dca, (genesereth and
nilsson 1987; richardson and domingos 2006)): the only models considered for a set
f of formulas are those for which the following three conditions hold: (a) different
constants refer to different objects in the domain, (b) the only objects in the domain are
those that can be represented using the constant and function symbols in f , and (c)
for each function f appearing in f , the value of f applied to every possible tuple of
arguments is known, and is a constant appearing in f . together, these three conditions
entail that there is a one-to-one relation between objects in the domain and the named constants
of f . when the set of all constants is known, it can be used to ground predicates to
generate the set of all ground atoms, which then become the nodes in the graphical
model. different constant sets result in different id114. if no constants are
explicitly introduced, the graphical model is empty (no random variables).

this means that to obtain an adequate representation of an id136 problem
consisting of a text t and query h, we need to introduce a suf   cient number of constants
explicitly into the formula: the worlds that the mln considers need to have enough
constants to faithfully represent the situation in t and not give the wrong entailment
for the query h. in what follows, we explain how we determine an appropriate set
of constants for the logical-form representations of t and h. the domain size that we
determine is one of the two components of the parameter wt,h.

skolemization. we introduce some of the necessary constants through the well-known
technique of skolemization [skolem 1920]. it transforms a formula    x1 . . . xn   y.f to
   x1 . . . xn.f    , where f     is formed from f by replacing all free occurrences of y in f by
a term f (x1, . . . , xn) for a new function symbol f. if n = 0, f is called a skolem constant,
otherwise a skolem function. although skolemization is a widely used technique in    rst-
order logic, it is not frequently employed in probabilistic logic since many applications
do not require existential quanti   ers.

we use skolemization on the text t (but not the query h, as we cannot assume a
priori that it is true). for example, the logical expression in equation 4, which represents
the sentence t: an ogre loves a princess, will be skolemized to:

ogre(o)     agent(l, o)     love(l)     patient(l, n )     princess(n )

(5)

where o, l, n are skolem constants introduced into the domain.

standard skolemization transforms existential quanti   ers embedded under uni-
versal quanti   ers to skolem functions. for example, for the text t: all ogres snore and
its logical form    x. ogre(x)        y. agent(y, x)     snore(y) the standard skolemization is
   x. ogre(x)     agent(f (x), x)     snore(f (x)). per condition (c) of the dca above, if a
skolem function appeared in a formula, we would have to know its value for any
constant in the domain, and this value would have to be another constant. to achieve
this, we introduce a new predicate skolemf instead of each skolem function f, and

5 cooper et al. (2015) criticize probabilistic id136 frameworks based on a id203 distribution over
worlds as not feasible. but what they mean by a world is a maximally consistent set of propositions. so
because we use mlns only to handle    mini-worlds    describing individual situations or settings, this
criticism does not apply to our approach.

13

computational linguistics

volume xx, number xx

for every constant that is an ogre, we add an extra constant that is a loving event. the
example above then becomes:

t :    x. ogre(x)        y. skolemf (x, y)     agent(y, x)     snore(y)

if the domain contains a single ogre o1, then we introduce a new constant c1 and an
atom skolemf (o1, c1) to state that the skolem function f maps the constant o1 to the
constant c1.

existence. but how would the domain contain an ogre o1 in the case of the text t: all
ogres snore,    x.ogre(x)        y.agent(y, x)     snore(y)? skolemization does not introduce
any variables for the universally quanti   ed x. we still introduce a constant o1 that is
an ogre. this can be justi   ed by pragmatics since the sentence presupposes that there
are, in fact, ogres [strawson 1950; geurts 2007]. we use the sentence   s parse to identify
the universal quanti   er   s restrictor and body, then introduce entities representing the
restrictor of the quanti   er [beltagy and erk 2015]. the sentence t: all ogres snore ef-
fectively changes to t: all ogres snore, and there is an ogre. at this point, skolemization
takes over to generate a constant that is an ogre. sentences like t: there are no ogres is a
special case: for such sentences, we do not generate evidence of an ogre. in this case, the
non-emptiness of the domain is not assumed because the sentence explicitly negates it.

universal quanti   ers in the query. the most serious problem with the dca is that
it affects the behavior of universal quanti   ers in the query. suppose we know that t:
shrek is a green ogre, represented with skolemization as ogre(sh)     green(sh). then
we can conclude that h: all ogres are green, because by the dca we are only considering
models with this single constant which we know is both an ogre and green. to address
this problem, we again introduce new constants.

we want a query h: all ogres are green to be judged true iff there is evidence that
all ogres will be green, no matter how many ogres there are in the domain. so h should
follow from t2: all ogres are green but not from t1: there is a green ogre. therefore we
introduce a new constant d for the query and assert ogre(d) to test if we can then
conclude that green(d). the new evidence ogre(d) prevents the query from being
judged true given t1. given t2, the new ogre d will be inferred to be green, in which
case we take the query to be true. again, with a query such as h: there are no ogres, we
do not generate any evidence for the existence of an ogre.

4.3 setting prior probabilities

suppose we have an empty text t , and the query h: a is an ogre, where a is a constant
in the system. without any additional information, the worlds in which ogre(a) is true
are going to be as likely as the worlds in which the ground atom is false, so ogre(a) will
have a id203 of 0.5. so without any text t , ground atoms have a prior id203
in mlns that is not zero. this prior id203 depends mostly on the size of the set f
of input formulas. the prior id203 of an individual ground atom can be in   uenced
by a weighted rule, for example ogre(a) |    3, with a negative weight, sets a low prior
id203 on a being an ogre. this is the second group of parameters that we encode
in wt,h: weights on ground atoms to be used to set prior probabilities.

prior probabilities are problematic for our probabilistic encoding of natural lan-
guage understanding problems. as a reminder, we probabilistically test for entail-
ment by computing the id203 of the query given the text, or more precisely
p (h|t, kb, wt,h ). however, how useful this id155 is as an indication

14

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

of entailment depends on the prior id203 of h, p (h|kb, wt,h ). for example, if
h has a high prior id203, then a high id155 p (h|t, kb, wt,h )
does not add much information because it is not clear if the id203 is high because
t really entails h, or because of the high prior id203 of h. in practical terms, we
would not want to say that we can conclude from t: all princesses snore that h: there is
an ogre just because of a high prior id203 for the existence of ogres.
to solve this problem and make the id203 p (h|t, kb, wt,h ) less sensitive
to p (h|kb, wt,h ), we pick a particular wt,h such that the prior id203 of h is
approximately zero, p (h|kb, wt,h )     0, so that we know that any increase in the
id155 is an effect of adding t . for the task of rte, where we need
to distinguish entailment, neutral, and contradiction, this id136 alone does not
account for contradictions, which is why an additional id136 p (  h|t, kb, wt,  h )
is needed.
for the rest of this section, we show how to set the world con   gurations wt,h
such that p (h|kb, wt,h )     0 by enforcing the closed-world assumption (cwa). this
is the assumption that all ground atoms have very low prior id203 (or are false by
default).

using the cwa to set the prior id203 of the query to zero. the closed-world
assumption (cwa) is the assumption that everything is false unless stated otherwise.
we translate it to our probabilistic setting as saying that all ground atoms have very low
prior id203. for most queries h, setting the world con   guration wt,h such that all
ground atoms have low prior id203 is enough to achieve that p (h|kb, wt,h )     0
(not for negated hs, and this case is discussed below). for example, h: an ogre loves a
princess, in logic is:

h :    x, y, z. ogre(x)     agent(y, x)     love(y)     patient(y, z)     princess(z)

having low prior id203 on all ground atoms means that the prior id203 of
this existentially quanti   ed h is close to zero.

we believe that this setup is more appropriate for probabilistic natural language
entailment for the following reasons. first, this aligns with our intuition of what it
means for a query to follow from a text: that h should be entailed by t not because of
general world knowledge. for example, if t: an ogre loves a princess, and h: texas is in the
usa, then although h is true in the real world, t does not entail h. another example:
t: an ogre loves a princess, h: an ogre loves a green princess, again, t does not entail h
because there is no evidence that the princess is green, in other words, the ground atom
green(n ) has very low prior id203.

the second reason is that with the cwa, the id136 result is less sensitive to the
domain size (number of constants in the domain). in logical forms for typical natural
language sentences, most variables in the query are existentially quanti   ed. without
the cwa, the id203 of an existentially quanti   ed query increases as the domain
size increases, regardless of the evidence. this makes sense in the mln setting, because
in larger domains the id203 that something exists increases. however, this is not
what we need for testing natural language queries, as the id203 of the query
should depend on t and kb, not the domain size. with the cwa, what affects the
id203 of h is the non-zero evidence that t provides and kb, regardless of the
domain size.

the third reason is computational ef   ciency. as discussed in section 2, markov
logic networks    rst compute all possible groundings of a given set of weighted formu-
las which can require signi   cant amounts of memory. this is particularly striking for

15

computational linguistics

volume xx, number xx

problems in natural language semantics because of long formulas. beltagy and mooney
(2014) show how to utilize the cwa to address this problem by reducing the number of
ground atoms that the system generates. we discuss the details in section 6.2.

setting the prior id203 of negated h to zero. while using the cwa is enough to set
p (h|kb, wt,h )     0 for most hs, it does not work for negated h (negation is part of
h). assuming that everything is false by default and that all ground atoms have very
low prior id203 (cwa) means that all negated queries h are true by default. the
result is that all negated h are judged entailed regardless of t . for example, t: an ogre
loves a princess would entail h: no ogre snores. this h in logic is:

h :    x, y. ogre(x)       (agent(y, x)     snore(y))

as both x and y are universally quanti   ed variables in h, we generate evidence of an
ogre ogre(o) as described in section 4.2. because of the cwa, o is assumed to be does
not snore, and h ends up being true regardless of t .
to set the prior id203 of h to     0 and prevent it from being assumed true
when t is just uninformative, we construct a new rule a that implements a kind of anti-
cwa. a is formed as a conjunction of all the predicates that were not used to generate
evidence before, and are negated in h. this rule a gets a positive weight indicating that
its ground atoms have high prior id203. as the rule a together with the evidence
generated from h states the opposite of the negated parts of h, the prior id203 of
h is low, and h cannot become true unless t explicitly negates a. t is translated into
unweighted rule, which are taken to have in   nite weight, and which thus can overcome
the    nite positive weight of a. here is a neutral rte example, t: an ogre loves a princess,
and h: no ogre snores. their representations are:
t :    x, y, z. ogre(x)     agent(y, x)     love(y)     patient(y, z)     princess(z)
h:    x, y. ogre(x)       (agent(y, x)     snore(y))
e: ogre(o)
a: agent(s, o)     snore(s)|w = 1.5

e is the evidence generated for the universally quanti   ed variables in h, and a is
the weighted rule for the remaining negated predicates. the relation between t and
h is neutral, as t does not entail h. this means, we want p (h|t, kb, wt,h )     0,
but because of the cwa, p (h|t, kb, wt,h )     1. adding a solves this problem and
p (h|t, a, kb, wt,h )     0 because h is not explicitly entailed by t .

in case h contains existentially quanti   ed variables that occur in negated predi-
cates, they need to be universally quanti   ed in a for h to have a low prior id203.
for example, h: there is an ogre that is not green:

h :    x. ogre(x)       green(x)
a :    x. green(x)|w = 1.5

if one variable is universally quanti   ed and the other is existentially quanti   ed, we need
to do something more complex. here is an example, h: an ogre does not snore:

h :    x. ogre(x)       (    y. agent(y, x)     snore(y) )

a :    v. agent(s, v)     snore(s)|w = 1.5

notes about how id136 proceeds with the rule a added. if h is a negated formula that
is entailed by t , then t (which has in   nite weight) will contradict a, allowing h to
be true. any weighted id136 rules in the knowledge base kb will need weights

16

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

high enough to overcome a. so the weight of a is taken into account when computing
id136 rule weights.

in addition, adding the rule a introduces constants in the domain that are necessary
for making the id136. for example, take t: no monster snores, and h: no ogre snores,
which in logic are:
t :      x, y. monster(x)     agent(y, x)     snore(y)
h:      x, y. ogre(x)     agent(y, x)     snore(y)
a: ogre(o)     agent(s, o)     snore(s)|w = 1.5
kb:    x. ogre(x)     monster(x)
without the constants o and s added by the rule a, the domain would have been empty
and the id136 output would have been wrong. the rule a prevents this problem. in
addition, the introduced evidence in a    t the idea of    evidence propagation    mentioned
above, (detailed in section 6.2). for entailing sentences that are negated, like in the
example above, the evidence propagates from h to t (not from t to h as in non-
negated examples). in the example, the rule a introduces an evidence for ogre(o) that
then propagates from the lhs to the rhs of the kb rule.

4.4 id123 and coreference

the adaptations of logical form that we have discussed so far apply to any natural
language understanding problem that can be formulated as text/query pairs. the
adaptation that we discuss now is speci   c to id123. it concerns coreference
between text and query.

for example, if we have t: an ogre does not snore and h: an ogre snores, then strictly
speaking t and h are not contradictory because it is possible that the two sentences are
referring to different ogres. although the sentence uses an ogre not the ogre, the annota-
tors make the assumption that the ogre in h refers to the ogre in t . in the sick textual
entailment dataset, many of the pairs that annotators have labeled as contradictions are
only contradictions if we assume that some expressions corefer across t and h.
updated   h:

for the above examples, here are the logical formulas with coreference in the

t :    x. ogre(x)       (   y. agent(y, x)     snore(y))

skolemized t : ogre(o)       (   y. agent(y, o)     snore(y))
h :    x, y. ogre(x)     agent(y, x)     snore(y)
  h :      x, y. ogre(x)     agent(y, x)     snore(y)
updated   h :      y. ogre(o)     agent(y, o)     snore(y)

notice how the constant o representing the ogre in t is used in the updated   h instead
of the quanti   ed variable x.

we use a rule-based approach to determining coreference between t and h, con-
sidering both coreference between entities and coreference of events. two items (entities
or events) corefer if they 1) have different polarities, and 2) share the same lemma or
share an id136 rule. two items have different polarities in t and h if one of them is
embedded under a negation and the other is not. for the example above, ogre in t is not
negated, and ogre in   h is negated, and both words are the same, so they corefer.

a pair of items in t and h under different polarities can also corefer if they share
an id136 rule. in the example of t: a monster does not snore and h: an ogre snores, we
need monster and ogre to corefer. for cases like this, we rely on the id136 rules found

17

computational linguistics

volume xx, number xx

using the modi   ed robinson resolution method discussed in section 5.1. in this case, it
determines that monster and ogre should be aligned, so they are marked as coreferring.
here is another example: t: an ogre loves a princess, h: an ogre hates a princess. in this
case, loves and hates are marked as coreferring.

4.5 using multiple parses

in our framework that uses probabilistic id136 followed by a classi   er that learns
thresholds, we can easily incorporate multiple parses to reduce errors due to mispars-
ing. parsing errors lead to errors in the logical form representation, which in turn can
lead to erroneous entailments. if we can obtain multiple parses for a text t and query
h, and hence multiple logical forms, this should increase our chances of getting a good
estimate of the id203 of h given t .

the default id35 parser that boxer uses is c&c (clark and curran 2004). this
parser can be con   gured to produce multiple ranked parses (ng and curran 2012);
however, we found that the top parses we get from c&c are usually not diverse enough
and map to the same logical form. therefore, in addition to the top c&c parse, we use
the top parse from another recent id35 parser, easyid35 (lewis and steedman 2014).

so for a natural language text nt and query nh, we obtain two parses each,
say st 1 and st 2 for t and sh1 and sh2 for h, which are transformed to logical
forms t1, t2, h1, h2. we now compute probabilities for all possible combinations of
representations of nt and nh: the id203 of h1 given t1, the id203 of h1
given t2, and conversely also the probabilities of h2 given either t1 or t2. if the task
is id123 with three categories entailment, neutral, and contradiction, then
as described in section 4.1 we also compute the id203 of   h1 given either t1 or t2,
and the id203 of   h12 given either t1 or t2. when we use multiple parses in this
manner, the thresholding classi   er is simply trained to take in all of these probabilities
as features. in section 7, we evaluate using c&c alone and using both parsers.

5. knowledge base construction

this section discusses the automated construction of the knowledge base, which in-
cludes the use of distributional information to predict lexical and phrasal entailment.
this section integrates two aims that are con   icting to some extent, as alluded to in
the introduction. the    rst is to show that a general-purpose in-depth natural language
understanding system based on both logical form and distributional representations
can be adapted to perform the rte task well enough to achieve state of the art
results. to achieve this aim, we build a classi   er for lexical and phrasal entailment
that includes many task-speci   c features that have proven effective in state-of-the-
art systems (marelli et al. 2014a; bjerva et al. 2014; lai and hockenmaier 2014). the
second aim is to provide a framework in which we can test different distributional
approaches on the task of lexical and phrasal entailment as a building block in a general
id123 system. to achieve this second aim, in section 7) we provide an in-
depth ablation study and error analysis for the effect of different types of distributional
information within the lexical and phrasal entailment classi   er.

since the biggest computational bottleneck for mlns is the creation of the network,
we do not want to add a large number of id136 rules blindly to a given text/query
pair. instead, we    rst examine the text and query to determine id136 rules that are
potentially useful for this particular entailment problem. for pre-existing rule collec-
tions, we add all possibly matching rules to the id136 problem (section 5.3). for

18

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

more    exible lexical and phrasal entailment, we use the text/query pair to determine
additionally useful id136 rules, then automatically create and weight these rules.
we use a variant of robinson resolution (robinson 1965) to compute the list of useful
rules (section 5.1), then apply a lexical and phrasal entailment classi   er (section 5.2) to
weight them.

ideally, the weights that we compute for id136 rules should depend on the
context in which the words appear. after all, the ability to take context into account
in a    exible fashion is one of the biggest advantages of distributional models. unfor-
tunately the id123 data that we use in this paper does not lend itself to
contextualization     polysemy just does not play a large role in any of the existing rte
datasets that we have used so far. therefore, we leave this issue to future work.

5.1 robinson resolution for alignment and rule extraction

to avoid undo complexity in the mln, we only want to add id136 rules speci   c
to a given text t and query h. earlier versions of our system generated distributional
rules matching any word or short phrase in t with any word or short phrase in h. this
includes many unnecessary rules, for example for t: an ogre loves a princess and h: a
monster likes a lady, the system generates rules linking ogre to lady. in this paper, we use
a novel method to generate only rules directly relevant to t and h: we assume that t
entails h, and ask what missing rule set kb is necessary to prove this entailment. we
use a variant of robinson resolution (robinson 1965) to generate this kb. another way
of viewing this technique is that it generates an alignment between words and phrases
in t and words or phrases in h guided by the logic.

modi   ed robinson resolution. robinson resolution is a theorem proving method for
testing unsatis   ability that has been used in some previous rte systems bos (2009). it
assumes a formula in conjunctive normal form (cnf), a conjunction of clauses, where
a clause is a disjunction of literals, and a literal is a negated or non-negated atom. more
formally, the formula has the form    x1, . . . , xn
it has the form l1     . . .     lk where li is a literal, which is an atom ai or a negated
atom   ai. the resolution rule takes two clauses containing complementary literals, and
produces a new clause implied by them. writing a clause c as the set of its literals, we
can formulate the rule as:

(cid:0)c1     . . .     cm), where cj is a clause and

c1     {l1}

c2     {l2}

(c1     c2)  
where    is a most general uni   er of l1 and   l2.

in our case, we use a variant of robinson resolution to remove the parts of text
t and query h that the two sentences have in common. instead of one set of clauses,
we use two: one is the cnf of t , the other is the cnf of   h. the resolution rule is
only applied to pairs of clauses where one clause is from t , the other from h. when
no further applications of the resolution rule are possible, we are left with remainder
formulas rt and rh. if rh contains the empty clause, then h follows from t without
id136 rules. otherwise, id136 rules need to be generated. in the simplest case,
we form a single id136 rule as follows. all variables occurring in rt or rh are
existentially quanti   ed, all constants occurring in rt or rh are un-skolemized to new
universally quanti   ed variables, and we infer the negation of rh from rt . that is, we
form the id136 rule

   x1 . . . xn   y1 . . . ym. rt          rh  

19

computational linguistics

volume xx, number xx

where {y1 . . . ym} is the set of all variables occurring in rt or rh, {a1, . . . an} is the set
of all constants occurring in rt or rh and    is the inverse of a substitution    : {a1    
x1, . . . , an     xn} for distinct variables x1, . . . , xn.

for example, consider t: an ogre loves a princess and h: a monster loves a princess.
this gives us the following two clause sets. note that all existential quanti   ers have
been eliminated through skolemization. the query is negated, so we get    ve clauses for
t but only one for h.

t : {ogre(a)},{princess(b)},{love(c},{agent(c, a)},{patient(c, b)}
  h : {  monster(x),  princess(y),  love(z),  agent(z, x),  patient(z, y)}

the resolution rule can be applied 4 times. after that, c has been uni   ed with z
(because we have resolved love(c) with love(z)), b with y (because we have resolved
princess(b) with princess(y)), and a with x (because we have resolved agent(c, a)
with agent(z, x)). the formula rt is ogre(a), and rh is   monster(a). so the rule that
we generate is:

   x.ogre(x)     monster(x)

the modi   ed robinson resolution thus does two things at once: it removes words that
t and h have in common, leaving the words for which id136 rules are needed, and
it aligns words and phrases in t with words and phrases in h through uni   cation.

one important re   nement to this general idea is that we need to distinguish con-
tent predicates that correspond to content words (nouns, verb and adjectives) in the
sentences from non-content predicates such as boxer   s meta-predicates agent(x, y ).
resolving on non-content predicates can result in incorrect rules, for example in the
case of t: a person solves a problem and h: a person    nds a solution to a problem, in cnf:
t : {person(a)},{solve(b)},{problem(c)},{agent(b, a)},{patient(b, c)}
  h : {  person(x),  f ind(y),  solution(z),  problem(u),  agent(y, x),  patient(y, z),

  to(z, u)}

if we resolve patient(b, c) with patient(y, z), we unify the problem c with the solution
z, leading to a wrong alignment. we avoid this problem by resolving on non-content
predicates only when they are fully grounded (that is, when the substitution of variables
with constants has already been done by some other resolution step involving content
predicates).

in this variant of robinson resolution, we currently do not perform any search, but
unify two literals only if they are fully grounded or if the literal in t has a unique literal
in h that it can be resolved with, and vice versa. this works for most pairs in the sick
dataset. in future work, we would like to add search to our algorithm, which will help
produce better rules for sentences with duplicate words.

rule re   nement. the modi   ed robinson resolution algorithm gives us one rule per
text/query pair. this rule needs postprocessing, as it is sometimes too short (omitting
relevant context), and often it combines what should be several id136 rules.

in many cases, a rule needs to be extended. this is the case when it only shows the
difference between text and query is too short and needs context to be usable as a distri-
butional rule, for example: t: a dog is running in the snow, h: a dog is running through the
snow, the rule we get is    x, y. in(x, y)     through(x, y). although this rule is correct, it
does not carry enough information to compute a meaningful vector representation for
each side. what we would like instead is a rule that infers    run through snow    from
   run in snow   .

remember that the variables x and y were skolem constants in rt and rh, for
example rt : in(r, s) and rh : through(r, s). we extend the rule by adding the content

20

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

words that contain the constants r and s. in this case, we add the running event
and the snow back in. the    nal rule is:    x, y. run(x)     in(x, y)     snow(y)     run(x)    
through(x, y)     snow(y).

in some cases however, extending the rule adds unnecessary complexity. however,
we have no general algorithm for when to extend a rule, which would have to take
context into account. at this time, we extend all rules as described above. as discussed
below, the entailment rules subsystem can itself choose to split long rules, and it may
choose to split these extended rules again.

sometimes, long rules need to be split. a single pair t and h gives rise to one
single pair rt and rh, which often conceptually represents multiple id136 rules. so
we split rt and rh as follows. first, we split each formula into disconnected sets of
predicates. for example, consider t: the doctors are healing a man, h: the doctor is helping
the patient which leads to the rule    x, y. heal(x)     man(y)     help(x)     patient(y). the
formula rt is split into heal(x) and man(y) because the two literals do not have any
variable in common and there is no relation (such as agent()) to link them. similarly, rh
is split into help(x) and patient(y). if any of the splits has more than one verb, we split
it again, where each new split contains one verb and its arguments.
after that, we create new rules that link any part of rt to any part of rh with which
it has at least one variable in common. so for our example we get    x heal(x)     help(x)
and    y man(y)     patient(y).
there are cases where splitting the rule does not work, for example with a person,
who is riding a bike     a biker . here, splitting the rule and using person     biker loses
crucial context information. so we do not perform those additional splits at the level of
the logical form, though the entailment rules subsystem may choose to do further splits.
rules as training data. the output from the previous steps is a set of rules {r1, ..., rn}
for each pair t and h. one use of these rules is to test whether t probabilistically
entails h. but there is a second use too: the lexical and phrasal entailment classi   er that
we describe below is a supervised classi   er, which needs training data. so we use the
training part of the sick dataset to create rules through modi   ed robinson resolution,
which we then use to train the lexical and phrasal entailment classi   er. for simplicity,
we translate the robinson resolution rules into textual rules by replacing each boxer
predicate with its corresponding word.

computing id136-rule training data from rte data requires deriving labels for
individual rules from the labels on rte pairs (entailment, contradiction and neutral).
the entailment cases are the most straightforward. knowing that t     r1     ...     rn     h,
then it must be that all ri are entailing. we automatically label all ri of the entailing pairs
as entailing rules.
for neutral pairs, we know that t     r1     ...     rn (cid:59) h, so at least one of the ri is non-
entailing. we experimented with automatically labeling all ri as non-entailing, but that
adds a lot of noise to the training data. for example, if t: a man is eating an apple and h: a
guy is eating an orange, then the rule man     guy is entailing, but the rule apple     orange is
non-entailing. so we automatically compare the ri from a neutral pair to the entailing
rules derived from entailing pairs. all rules ri found among the entailing rules from
entailing pairs are assumed to be entailing (unless n = 1, that is, unless we only have
one rule), and all other rules are assumed to be non-entailing. we found that this step
improved the accuracy of our system. to further improve the accuracy, we performed a
manual annotation of rules derived from neutral pairs, focusing only on the rules that
do not appear in entailing. we labeled rules as either entailing or non-entailing. from
around 5,900 unique rules, we found 737 to be entailing. in future work, we plan to use

21

computational linguistics

volume xx, number xx

multiple instance learning (dietterich, lathrop, and lozano-perez 1997; bunescu and
mooney 2007) to avoid manual annotation; we discuss this further in section 8.

for contradicting pairs, we make a few simplifying assumptions that    t almost all
such pairs in the sick dataset. in most of the contradiction pairs in sick, one of the two
sentences t or h is negated. for pairs where t or h has a negation, we assume that this
negation is negating the whole sentence, not just a part of it. we    rst consider the case
where t is not negated, and h =   sh. as t contradicts h, it must hold that t       h,
so t         sh, and hence t     sh. this means that we just need to run our modi   ed
robinson resolution with the sentences t and sh and label all resulting ri as entailing.
next we consider the case where t =   st while h is not negated. as t contradicts
h, it must hold that   st       h, so h     st. again, this means that we just need to run
the modi   ed robinson resolution with h as the    text    and st as the    hypothesis    and
label all resulting ri as entailing.

the last case of contradiction is when both t and h are not negated, for example: t:
a man is jumping into an empty pool, h: a man is jumping into a full pool, where empty
and full are antonyms. as before, we run the modi   ed robinson resolution with t
and h and get the resulting ri. similar to the neutral pairs, at least one of the ri is a
contradictory rule, while the rest could be entailing or contradictory rules. as for the
neutral pairs, we take a rule ri to be entailing if it is among the entailing rules derived
so far. all other rules are taken to be contradictory rules. we did not do the manual
annotation for these rules because they are few.

5.2 the lexical and phrasal entailment rule classi   er

after extracting lexical and phrasal rules using our modi   ed robinson resolution
(section 5.1), we use several combinations of distributional information and lexical
resources to build a lexical and phrasal entailment rule classi   er (entailment rule classi   er for
short) for weighting the rules appropriately. these extracted rules create an especially
valuable resource for testing lexical entailment systems, as they contain a variety of
entailment relations (hypernymy, synonymy, antonymy, etc.), and are actually useful in
an end-to-end rte system.

we describe the entailment rule classi   er in multiple parts. in section 5.2.1, we
overview a lexical entailment rule classi   er, which only handles single words. sec-
tion 5.2.2 describes the lexical resources used. in section 5.2.3, we describe how our pre-
vious work in supervised hypernymy detection is used in the system. in section 5.2.4,
we describe the approaches for extending the classi   er to handle phrases.

5.2.1 lexical entailment rule classi   er. we begin by describing the lexical entailment
rule classi   er, which only predicts entailment between single words, treating the task
as a supervised classi   cation problem given the lexical rules constructed from the
modi   ed robinson resolution as input. we use numerous features which we expect
to be predictive of lexical entailment. many were previously shown to be successful for
the semeval 2014 shared task on lexical entailment (marelli et al. 2014a; bjerva et al.
2014; lai and hockenmaier 2014). altogether, we use four major groups of features as
summarized in table 1 and described in detail below.

wordform features we extract a number of simple features based on the usage of the
lhs and rhs in their original sentences. we extract features for whether the lhs and
rhs have the same lemma, same surface form, same pos, which pos tags they have,
and whether they are singular or plural. plurality is determined from the pos tags.

22

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

description

same lemma, surface form
pos of lhs, pos of rhs, same pos
whether lhs/rhs/both are singular/plural

true if a lemma is not in id138, or no path exists
true if lhs is hypernym of rhs
true if rhs is hypernym of lhs
true if lhs and rhs is in same synset
true if lhs and rhs are antonyms
path similarity (nltk)
bins of path similarity (nltk)

name
wordform
same word
pos
sg/pl
id138
oov
hyper
hypo
syn
ant
path sim
path sim hist
distributional features (lexical)
true if either lemma not in dist space
oov
cosine between lhs and rhs in bow space
bow cosine
cosine between lhs and rhs in dep space
dep cosine
bins of bow cosine
bow hist
bins of dep cosine
dep hist
asymmetric features (roller, erk, and boleda 2014)
lhs dep vector     rhs dep vector
diff
rhs dep vector     rhs dep vector, squared
diffsq

type

binary
binary
binary

binary
binary
binary
binary
binary
real
binary

binary
real
real
binary
binary

real
real

#
18
2
10
6
18
1
1
1
1
1
1
12
28
2
1
1
12
12
600
300
300

table 1: list of features in the lexical entailment classi   er, along with types and counts

id138 features we use id138 3.0 to determine whether the lhs and rhs have
known synonymy, antonymy, hypernymy, or hyponymy relations. we disambiguate be-
tween multiple synsets for a lemma by selecting the synsets for the lhs and rhs which
minimize their path distance. if no path exists, we choose the most common synset
for the lemma. path similarity, as implemented in the natural language toolkit (bird,
klein, and loper 2009), is also used as a feature.

distributional features we measure distributional similarity in two distributional
spaces, one which models topical similarity (bow), and one which models syntactic
similarity (dep). we use cosine similarity of the lhs and rhs in both spaces as features.
one very important feature set used from distributional similarity is the histogram
binning of the cosines. we create 12 additional binary, mutually-exclusive features,
which mark whether the distributional similarity is within a given range. we use the
ranges of exactly 0, exactly 1, 0.01-0.09, 0.10-0.19, . . . , 0.90-0.99. figure 4 shows the
importance of these histogram features: words that are very similar (0.90-0.99) are much
less likely to be entailing than words which are moderately similar (0.70-0.89). this is
because the most highly similar words are likely to be cohyponyms.

5.2.2 preparing distributional spaces. as described in the previous section, we use
distributional semantic similarity as features for the entailment rules classi   er. here we
describe the preprocessing steps to create these distributional resources.

corpus and preprocessing: we use the bnc, ukwac and a 2014-01-07 copy of
wikipedia. all corpora are preprocessed using stanford corenlp. we collapse particle
verbs into a single token, and all tokens are annotated with a (short) pos tag so that the
same lemma with a different pos is modeled separately. we keep only content words
(nn, vb, rb, jj) appearing at least 1000 times in the corpus. the    nal corpus contains
50,984 types and roughly 1.5b tokens.

bag-of-words vectors: we    lter all but the 51k chosen lemmas from the corpus, and
create one sentence per line. we use skip-gram negative sampling to create vectors
(mikolov et al. 2013). we use 300 latent dimensions, a window size of 20, and 15 negative

23

computational linguistics

volume xx, number xx

figure 4: distribution of entailment relations on lexical items by cosine. highly similar
pairs (0.90-0.99) are less likely entailing than moderately similar pairs (0.70-0.89).

samples. these parameters were not tuned, but chosen as reasonable defaults for the
task. we use the large window size to ensure the bow vectors captured more topical
similarity, rather than syntactic similarity, which is modeled by the dependency vectors.
dependency vectors: we extract (lemma/pos, relation, context/pos) tuples from each
of the stanford collapsed cc dependency graphs. we    lter tuples with lemmas not in
our 51k chosen types. following baroni and lenci (2010), we model inverse relations
and mark them separately. for example,    red/jj car/nn    will generate tuples for both
(car/nn, amod, red/jj) and (red/jj, amod   1, car/nn). after extracting tuples, we discard all
but the top 100k (relation, context/pos) pairs and build a vector space using lemma/pos
as rows, and (relation, context/pos) as columns. the matrix is transformed with positive
pointwise mutual information (ppmi), and reduced to 300 dimensions using singular
value decomposition (svd). we do not vary these parameters, but chose them as they
performed best in prior work (roller, erk, and boleda 2014).

5.2.3 asymmetric entailment features. as an additional set of features, we also
use the representation previously employed by the asymmetric, supervised hypernymy
classi   er described by roller, erk, and boleda (2014). previously, this classi   er was only
used on arti   cial datasets, which encoded speci   c lexical relations, like hypernymy, co-
hyponymy, and meronymy. here, we use its representation to encode just the three
general relations: entailment, neutral, and contradiction.

the asymmetric features take inspiration from mikolov, yih, and zweig (2013), who
found that differences between distributional vectors often encode certain linguistic
regularities, like (cid:126)king     (cid:126)man + (cid:126)woman     (cid:126)queen. in particular the asymmetric classi   er
uses two sets of features, < f, g >, where:

fi(lhs, rhs) = (cid:126)lhsi     (cid:126)rhsi
gi(lhs, rhs) = f 2
i ,

that is, the vector difference between the lhs and the rhs, and this difference vector
squared. both feature sets are extremely important to strong performance.

for these asymmetric features, we use the dependency space described earlier. we
choose the dep space because we previously found that spaces reduced using svd
outperform id27s generated by the skip-gram procedure. we do not use
both spaces, because of the large number of features this creates.

24

01002003000.00.10.20.30.40.50.60.70.80.91.01.1cosinefrequencygoldcontradictneutralentailinghistogram of cosine by entailment classbeltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

description

length of rules
length of lhs - length of rhs
number of alignments
number of unaligned words on lhs, rhs
percentage of words aligned
percentage of words unaligned on lhs, rhs

name
base
length
length diff
aligned
unaligned
pct aligned
pct unaligned
distributional features (paperno, pham, and baroni 2014)
cosine between mean constituent vectors
cosine
bins of cosine between mean constituent vectors
hist
stats
min/mean/max between constituent vectors
lexical features of aligned words
wordform
id138
distributional min/mean/max of each distributional feature

min/mean/max of each wordform feature
min/mean/max of each id138 feature

type

real
real
real
real
real
real

real
binary
real

#
9
2
1
1
2
1
2
16
1
12
3
192
54
54
84

table 2: features used in phrasal entailment classi   er, along with types and counts.

recently, there have been considerable work in detecting lexical entailments using
only distributional vectors. the classi   ers proposed by fu et al. (2014); levy et al. (2015);
and kruszewski, paperno, and baroni (2015) could have also been used in place of these
asymmetric features, but we reserve evaluations of these models for future work.

5.2.4 extending lexical entailment to phrases. the lexical entailment rule classi   er
described in previous sections is limited to only simple rules, where the lhs and rhs
are both single words. many of the rules generated by the modi   ed robinson resolution
are actually phrasal rules, such as little boy     child, or running     moving quickly. in order
to model these phrases, we use two general approaches:    rst, we use a state-of-the-
art compositional model, in order to create vector representations of phrases, and then
include the same similarity features described in the previous section. the full details
of the compositional distributional model are described in section 5.2.5.

in addition to a compositional distributional model, we also used a simple, greedy
word aligner, similar to the one described by lai and hockenmaier (2014). this aligner
works by    nding the pair of words on the lhs and rhs which are most similar in a
distributional space, and marking them as    aligned   . the process is repeated until at
least one side is completely exhausted. for example,    red truck     big blue car   , we
would align    truck    with    car       rst, then    red    with    blue   , leaving    big    unaligned.
after performing the phrasal alignment, we compute a number of base features,
based on the results of the alignment procedure. these include values like the length of
the rule, the percent of words unaligned, etc. we also compute all of the same features
used in the lexical entailment rule classi   er (wordform, id138, distributional) and
compute their min/mean/max across all the alignments. we do not include the asym-
metric entailment features as the feature space then becomes extremely large. table 2
contains a listing of all phrasal features used.

5.2.5 phrasal id65. we build phrasal distributional space based
on the practical lexical function model of paperno, pham, and baroni (2014). we again
use as the corpus a concatenation of bnc, ukwac and english wikipedia, parsed with
the stanford corenlp parser. we focus on 5 types of dependency labels,    amod   ,
   nsubj   ,    dobj   ,    pobj   ,    acomp   , and combine the governor and dependent words
of these dependencies to form adjective-noun, subject-verb, verb-object, preposition-
noun and verb-complement phrases respectively. we only retain phrases where both

25

computational linguistics

volume xx, number xx

the governor and the dependent are among the 50k most frequent words in the corpus,
resulting in 1.9 million unique phrases. the co-occurrence counts of the 1.9 million
phrases with the 20k most frequent neighbor words in a 2-word window are converted
to a ppmi matrix, and reduced to 300 dimensions by performing svd on a lexical space
and applying the resulting representation to the phrase vectors, normalized to length 1.
paperno et al. represent a word as a vector, which represents the contexts in which
the word can appear, along with a number of matrices, one for each type of dependent
that the word can take. for a transitive verb like chase, this would be one matrix for
subjects, and one for direct objects. the representation of the phrase chases dog is then

(cid:126)chase +

(cid:3)o
chase   (cid:126)dog

where    is id127, and when the phrase is extended with cat to form cat
chases dog, the representation is

(cid:126)chase +

(cid:3)s
chase   (cid:126)cat + ( (cid:126)chase +

(cid:3)o
chase   (cid:126)dog)

for verbs, the practical lexical function model trains a matrix for each of the relations
nsubj, dobj and acomp, for adjectives a matrix for amod, and for prepositions a matrix for
pobj. for example, the amod matrix of the adjective    red/jj    is trained as follows. we
collect all phrases in which    red/jj    serves as adjective modi   er (assuming the number
of such phrases is n), like    red/jj car/nn   ,    red/jj house/nn    etc., and construct
two 300    n matrices marg and mph, where the ith column of marg is the vector of the
            
noun modi   ed by    red/jj    in the ith phrase (      car,
house, etc.), and the ith column of mph
               
red car           
is vector of phrase i minus the vector of    red/jj    (
red, etc.),
(cid:3)(amod)     r300  300 of    red/jj    can be
normalized to length 1. then the amod matrix red
computed via ridge regression. given trained matrices, we compute the composition
vectors by applying the functions recursively starting from the lowest dependency.

                     
red house           

red,

as discussed above, some of the logical rules from section 5.1 need to be split
into multiple rules. we use the dependency parse to split long rules by iteratively
searching for the highest nodes in the dependency tree that occur in the logical rule, and
identifying the logical rule words that are its descendants in phrases that the practical
lexical functional model can handle. after splitting, we perform greedy alignment on
phrasal vectors to pair up rule parts. similar to section 5.2.4, we iteratively identify the
pair of phrasal vectors on the lhs and rhs which have the highest cosine similarity
until one side has no more phrases.

5.3 precompiled rules

the second group of rules is collected from existing databases. we collect rules from
id138 (princeton university 2010) and the paraphrase collection ppdb (ganitke-
vitch, van durme, and callison-burch 2013). we use simple string matching to    nd
the set of rules that are relevant to a given text/query pair t and h. if the left-hand side
of a rule is a substring of t and the right-hand is a substring of h, the rule is added, and
likewise for rules with lhs in h and rhs in t . rules that go from h to t are important
in case t and h are negated, e.g. t : no ogre likes a princess, h: no ogre loves a princess.
the rule needed is love     like which goes from h to t .

id138. id138 (princeton university 2010) is a lexical database of words grouped
into sets of synonyms. in addition to grouping synonyms, it lists semantic relations

26

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

connecting groups. we represent the information on id138 as    hard    logical rules.
the semantic relations we use are:

    synonymy:    x. man(x)     guy(x)
    hypernymy:    x. car(x)     vehicle(x)
    antonymy:    x. tall(x)       short(x)

one advantage of using logic is that it is a powerful representation that can effectively
represent these different semantic relations.

paraphrase collections. paraphrase collections are precompiled sets of rules, e.g: a person
riding a bike     a biker. we translate paraphrase collections, in this case ppdb (ganitke-
vitch, van durme, and callison-burch 2013), to logical rules. we use the lexical, one-
to-many and phrasal sections of the xl version of ppdb.

we use a simple rule-based approach to translate natural-language rules to logic.
first, we can make the assumption that the translation of a ppdb rule is going to be a
conjunction of positive atoms. ppdb does contain some rules that are centrally about
negation, such as deselected     not selected, but we skip those as the logical form analysis
already handles negation. as always, we want to include in kb only rules pertaining
to a particular text/query pair t and h. say lhs     rhs is a rule such that lhs is a
substring of t and rhs is a substring of h. then each word in lhs gets represented
by a unary predicate applied to a variable, and likewise for rhs     note that we can
expect the same predicates to appear in the logical forms l(t ) and l(h) of the text
and query. for example, if the rule is a person riding a bike     a biker, then we get the
atoms person(p), riding(r) and bike(b) for the lhs, with variables p, r, b. we then add
boxer meta-predicates to the logical form for lhs, and likewise for rhs. say that l(t )
includes person(a)     ride(b)     bike(c)     agent(b, a)     patient(b, c) for constants a,
b, and c, then we extend the logical form for lhs with agent(r, p)     patient(r, b). we
proceed analogously for rhs. this gives us the logical forms: l(lhs) = person(p)    
agent(r, p)     riding(r)     patient(r, b)     bike(b) and l(rhs) = biker(k).

the next step is to bind the variables in l(lhs) to those in l(rhs). in the example
above, the variable k in the rhs should be matched with the variable p in the lhs.
we determine these bindings using a simple rule-based approach: we manually de   ne
paraphrase rule templates for ppdb, which specify variable bindings. a rule template
is conditioned on the part-of-speech tags of the words involved. in our example it is
n1v2n3     n1, which binds the variables of the    rst n on the left to the    rst n on the
right, unifying the variables p and k. the    nal paraphrase rule is:    p, r, b. person(p)    
agent(r, p)     riding(r)     patient(r, b)     bike(b)     biker(p). in case some variables in
the rhs remain unbound, they are existentially quanti   ed, e.g.:    p. pizza(p)    
   q. slice(p)     of (p, q)     pizza(q).

each ppdb rule comes with a set of similarity scores which we need to map to a
single mln weight. we use the simple log-linear equation suggested by ganitkevitch,
van durme, and callison-burch (2013) to map the scores into a single value:

  i log   i

(6)

where, r is the rule, n is number of the similarity scores provided for the rule r,   i is the
value of the ith score, and   i is its scaling factor. for simplicity, following ganitkevitch,
van durme, and callison-burch (2013), we set all   i to 1. to map this weight to a    nal
mln rule weight, we use the weight-learning method discussed in section 6.3.

i=1

27

weight(r) =     n(cid:88)

computational linguistics

volume xx, number xx

handcoded rules. we also add a few handcoded rules to the kb that we do not get
from other resources. for the sick dataset, we only add several lexical rules where one
side of the rule is the word nobody, e.g: nobody        somebody and nobody        person.

6. probabilistic logical id136

we now turn to the last of the three main components of our system, probabilistic logical
id136. mln id136 is usually intractable, and using mln implementations    out
of the box    does not work for our application. this section discusses an mln imple-
mentation that supports complex queries and uses the closed world assumption (cwa)
to decrease problem size, hence making id136 more ef   cient. finally, this section
discusses a simple weight learning scheme to learn global scaling factors for weighted
rules in kb from different sources.

6.1 complex formulas as queries

current implementations of mlns like alchemy (kok et al. 2005) do not allow queries
to be complex formulas, they can only calculate probabilities of ground atoms. this
section discusses an id136 algorithm for arbitrary query formulas.

h     result(d) |    

the standard work-around. although current mln implementations can only calculate
probabilities of ground atoms, they can be used to calculate the id203 of a complex
formula through a simple work-around. the complex query formula h is added to the
mln using the hard formula:

(7)
where result(d) is a new ground atom that is not used anywhere else in the mln.
then, id136 is run to calculate the id203 of result(d), which is equal to the
id203 of the formula h. however, this approach can be very inef   cient for the
most common form of queries, which are existentially quanti   ed queries, e.g:

h :    x, y, z. ogre(x)     agent(y, x)     love(y)     patient(y, z)     princess(z)

(8)
grounding of the backward direction of the double-implication is very problematic
because the existentially quanti   ed formula is replaced with a large disjunction over
all possible combinations of constants for variables x, y and z (gogate and domingos
2011). converting this disjunction to clausal form becomes increasingly intractable as
the number of variables and constants grow.

new id136 method. instead, we propose an id136 algorithm to directly calculate
the id203 of complex query formulas. in mlns, the id203 of a formula is the
sum of the probabilities of the possible worlds that satisfy it. gogate and domingos
(2011) show that to calculate the id203 of a formula h given a probabilistic
knowledge base kb, it is enough to compute the partition function z of kb with and
without h added as a hard formula:

p (h | kb) =

z(kb     {(h,   )})

z(kb)

(9)

therefore, all we need is an appropriate algorithm to estimate the partition function z
of a markov network. then, we construct two ground networks, one with the query and
one without, and estimate their zs using that estimator. the ratio between the two zs
is the id203 of h.

28

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

we use samplesearch (gogate and dechter 2011) to estimate the partition function.
samplesearch is an importance sampling algorithm that has been shown to be effective
when there is a mix of probabilistic and deterministic (hard) constraints, a fundamental
property of the id136 problems we address. importance sampling in general is
problematic in the presence of determinism, because many of the generated samples
violate the deterministic constraints, and they get rejected. instead, samplesearch uses
a base sampler to generate samples then uses backtracking search with a sat solver
to modify the generated sample if it violates the deterministic constraints. we use an
implementation of samplesearch that uses a generalized belief propagation algorithm
called iterative join-graph propagation (ijgp) (dechter, kask, and mateescu 2002) as a
base sampler. this version is available online (gogate 2014).

for cases like the example h in equation 8, we need to avoid generating a large
disjunction because of the existentially quanti   ed variables. so we replace h with its
negation   h, replacing the existential quanti   ers with universals, which are easier
to ground and perform id136 upon. finally, we compute the id203 of the
query p (h) = 1     p (  h). note that replacing h with   h cannot make id136
with the standard work-around faster, because with   h, the direction   h     result(d)
suffers from the same problem of existential quanti   ers that we previously had with
h     result(d).

6.2 id136 optimization using the closed-world assumption

this section explains why our mln id136 problems are computationally dif   cult,
then explains how the closed-world assumption (cwa) can be used to reduce the
problem size and speed up id136. for more details, see beltagy and mooney (2014).
in the id136 problems we address, formulas are typically long, especially the
query h. the number of ground clauses of a    rst-order formula is exponential in the
number of variables in the formula, it is o(cv), where c is number of constants in the
domain and v is number of variables in the formula. for a moderately long formula, the
number of resulting ground clauses is infeasible to process.

we have argued above (section 4.3) that for probabilistic id136 problems based
on natural language text/query pairs, it makes sense to make the closed world assump-
tion: if we want to know if the query is true in the situation or setting laid out in the
text, we should take as false anything not said in the text. in our probabilistic setting,
the cwa amounts to giving low prior probabilities to all ground atoms unless they
can be inferred from the text and knowledge base. however, we found that a large
fraction of the ground atoms cannot be inferred from the text and knowledge base,
and their probabilities remain very low. as an approximation, we can assume that this
small id203 is exactly zero and these ground atoms are false, without signi   cantly
affecting the id203 of the query. this will remove a large number of the ground
atoms, which will dramatically decrease the size of the ground network and speed up
id136.
from the text and the knowledge base t     kb. for example:
t : ogre(o)     agent(s, o)     snore(s)

we assume that all ground atoms are false by default unless they are can be inferred

kb :    x. ogre(x)     monster(x)
h :    x, y. monster(x)     agent(y, x)     snore(y)

29

computational linguistics

volume xx, number xx

ground atoms {ogre(o), snore(s), agent(s, o)} are not false because they can be in-
ferred from t . ground atom monster(o) is also not false because it can be inferred
from t     kb. all other ground atoms are false.

all

setting

ground

the inferred ones,

here is an example of how this simpli   es the query h. h is equivalent
its possible groundings: h : (monster(o)     agent(s, o)    

to a disjunction of all
snore(s))     (monster(o)     agent(o, o)     snore(o))     (monster(s)     agent(o, s)    
snore(o))     (monster(s)     agent(s, s)     snore(s)).
atoms
then simplifying the expression, we get:
to false except
h : monster(o)     agent(s, o)     snore(s). notice that most ground clauses of h
are removed because they are false. we are left just with the ground clauses that
potentially have a non-zero id203. dropping all false ground clauses leaves an
exponentially smaller number of ground clauses in the ground network. even though
the id136 problem remains exponential in principle, the problem is much smaller
in practice, such that id136 becomes feasible. in our experiments with the sick
dataset, the number of ground clauses for the query ranges from 0 to 19,209 with mean
6. this shows that the cwa effectively reduces the number of ground clauses for the
query from millions (or even billions) to a manageable number. with the cwa, the
average number of inferrable ground atoms (ignoring ground atoms from the text)
ranges from 0 to 245 with an average of 18.

6.3 weight learning

we use weighted rules from different sources, both ppdb weights (section 5.3) and
the con   dence of the entailments rule classi   er (section 5.2). these weights are not
necessarily on the same scale, for example one source could produce systematically
larger weights than the other. to map them into uniform weights that can be used within
an mln, we use weight learning. similar to the work of zirn et al. (2011), we learn a
single mapping parameter for each source of rules that functions as a scaling factor:

m ln weight = scalingf actor    rulew eight

(10)
we use a simple grid search to learn the scaling factors that optimize performance on
the rte training data.

assuming that all rule weights are in [0, 1] (this is the case for classi   cation con   -

dence scores, and ppdb weights can be scaled), we also try the following mapping:

m ln weight = scalingf actor    log(

(11)
this function assures that for an mln with a single rule lhs     rhs | m ln weight,
it is the case that p (rhs|lhs) = rulew eight, given that scalingf actor = 1.

rulew eight
1     rulew eight

)

7. evaluation

this section evaluates our system. first, we evaluate several lexical and phrasal distribu-
tional systems on the rules that we collected using modi   ed robinson resolution. this
includes an in-depth analysis of different types of distributional information within the
entailment rule classi   er. second, we use the best con   guration we    nd in the    rst step
as a knowledge base and evaluate our system on the rte task using the sick dataset.
dataset: the sick dataset, which is described in section 2, consists of 5,000 pairs for
training and 4,927 for testing. pairs are annotated for rte and sts (semantic textual
similarity) tasks. we use the rte annotations of the dataset.

30

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

7.1 evaluating the entailment rule classi   er

the entailment rule classi   er described in section 5.2 constitutes a large portion of the
full system   s end-to-end performance, but consists of many different feature sets pro-
viding different kinds of information. in this section, we thoroughly evaluate the entail-
ment rule classi   er, both quantitatively and qualitatively, to identify the individual and
holistic value of each feature set and systematic patterns. however, this evaluation may
also be used as a framework by future lexical semantics research to see its value in end-
to-end id123 systems. for example, we could have also included features
corresponding to the many measures of distributional inclusion which were developed
to predict hypernymy (weeds, weir, and mccarthy 2004; kotlerman et al. 2010; lenci
and benotto 2012; santus 2013), or other supervised lexical entailment classi   ers (baroni
et al. 2012; fu et al. 2014; weeds et al. 2014; levy et al. 2015; kruszewski, paperno, and
baroni 2015).

evaluation is broken into four parts:    rst, we overview performance of the entire
entailment rule classi   er on all rules, both lexical and phrasal. we then break down
these results into performance on only lexical rules and only phrasal rules. finally, we
look at only the asymmetric features to address concerns raised by levy et al. (2015).
in all sections, we evaluate the lexical rule classi   er on its ability to generalize to new
word pairs, as well as the full system   s performance when the entailment rule classi   er
is used as the only source of knowledge.

overall, we    nd that id65 is of vital importance to the lexical
rule classi   er and the end-to-end system, especially when word relations are not explic-
itly found in id138. the introduction of syntactic distributional spaces and cosine
binning are especially valuable, and greatly improve performance over our own prior
work. contrary to levy et al. (2015), we    nd the asymmetric features provide better
detection of hypernymy over memorizing of prototypical hypernyms, but the prototype
vectors better capture examples which occur very often in the data; explicitly using both
does best. finally, we    nd, to our surprise, that a state-of-the-art compositional distri-
butional method (paperno, pham, and baroni 2014) yields disappointing performance
on phrasal entailment detection, though it does successfully identify non-entailments
deriving from changing prepositions or semantic roles.

7.1.1 experimental setup. we use the gold standard annotations described in
section 5.1. we perform 10 fold cross-validation on the annotated training set, using
the same folds in all settings. since some rte sentence pairs require multiple lexical
rules, we ensure that cross-validation folds are strati   ed across the sentences, so that the
same sentence cannot appear in both training and testing. we use a id28
classi   er with an l2 regularizer.6 since we perform three-way classi   cation, we train
models using one-vs-all.

performance is measured in two main metrics. intrinsic accuracy measures how the
classi   er performs in the cross-validation setting on the training data. this corresponds
to treating lexical and phrasal entailment as a basic supervised learning problem. rte
accuracy is accuracy on the end task of id123 using the predictions of the
entailment rule classi   er. for rte accuracy, the predictions of the entailment rule clas-

6 we experimented with multiple classi   ers, including id28, id90, and id166s

(with polynomial, rbf, and linear kernels). we found that linear classi   ers, and chose logistic
regression, since it was used in roller, erk, and boleda (2014) and lai and hockenmaier (2014).

31

computational linguistics

volume xx, number xx

feature set
always guess neutral
gold standard annotations
base only
wordform only
id138 only
dist (lexical) only
dist (phrasal) only
asym only
all features

intrinsic
64.3
100.0
64.3
67.3
75.1
71.5
66.9
70.1
79.9

rte train
73.9
95.0
73.8
77.0
81.9
78.7
75.9
77.3
84.0

rte test
73.3
95.5
73.4
76.7
81.3
77.7
75.1
77.2
83.0

table 3: cross-validation accuracy on entailment on all rules

si   er were used as the only knowledge base in the rte system. rte training accuracy
uses the predictions from the cross-validation experiment, and for rte test accuracy the
entailment rule classi   er was trained on the whole training set.

7.1.2 overall lexical and phrasal entailment evaluation. table 3 shows the results
of the entailment experiments on all rules, both lexical and phrasal. in order to give
bounds on our system   s performance, we present baseline score (entailment rule clas-
si   er always predicts non-entailing) and ceiling score (entailment rule classi   er always
predicts gold standard annotation).

the ceiling score (entailment rule classi   er always predicts gold standard annota-
tion) does not achieve perfect performance. this is due to a number of different issues
including misparses, imperfect rules generated by the modi   ed robinson resolution, a
few system id136 timeouts, and various idiosyncrasies of the sick dataset.

another point to note is that id138 is by far the strongest set of features for the
task. this is unsurprising, as synonymy and hypernymy information from id138
gives nearly perfect information for much of the task. there are some exceptions, such
as woman (cid:57) man, or black (cid:57) white, which id138 lists as antonyms, but which are
not considered contradictions in the sick dataset (e.g:    t: a man is cutting a tomato   
and    h: a woman is cutting a tomato    is not a contradiction). however, even though
id138 has extremely high coverage on this particular dataset, it still is far from
exhaustive: about a quarter of the rules have at least one pair of words for which
id138 relations could not be determined.

the lexical distributional features do surprisingly well on the task, obtaining a test
accuracy of 77.7 (table 3). this indicates that, even with only distributional similarity,
we do well enough to score in the upper half of systems in the original semeval shared
task, where the median test accuracy of all teams was 77.1 (marelli et al. 2014a). two
components were critical to the increased performance over our own prior work:    rst,
the use of multiple distributional spaces (one topical, one syntactic); second, the binning
of cosine values. while using only the bow cosine similarity as a feature, the classi   er
actually performs below baseline (50.0 intrinsic accuracy; compare to table 4). similarly,
only using syntactic cosine similarity as a feature also performs poorly (47.2 ia). how-
ever adding binning to either improves performance (64.3 and 64.7 for bow and dep),
and adding binning to both improves it further (68.8 ia, as reported in table 4).

the phrasal distributional similarity features, which are based on the state-of-the-
art paperno, pham, and baroni (2014) compositional vector space, perform somewhat
disappointingly on the task. we discuss possible reasons for this below in section 7.1.4.
we also note that the basic alignment features and wordform features (described
in tables 1 and 2) do not do particularly well on their own. this is encouraging, as it
means the dataset cannot be handled by simply expecting the same words to appear on

32

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

feature set
always guess neutral
gold standard annotations
wordform only
id138 only
dist (lexical) only
asym only
all features

intrinsic
56.6
100.0
57.4
79.1
68.8
76.8
84.6

rte train
69.4
93.2
70.4
83.1
76.3
78.3
82.7

rte test
69.3
94.6
70.9
84.2
76.7
79.2
83.8

table 4: cross-validation accuracy on entailment on lexical rules only

the lhs and rhs. finally, we note that the features are highly complementary, and the
combination of all features gives a substantial boost to performance.

7.1.3 evaluating the lexical entailment rule classi   er. table 4 shows performance of
the classi   er on only the lexical rules, which have single words on the lhs and rhs. in
these experiments we use the same procedure as before, but omit the phrasal rules from
the dataset. on the rte tasks, we compute accuracy over only the sick pairs which
require at least one lexical rule. note that a new ceiling score is needed, as some rules
require both lexical and phrasal predictions, but we do not predict any phrasal rules.

again we see that id138 features have the highest contribution. distributional
rules still perform better than the baseline, but the gap between distributional features
and id138 is much more apparent. perhaps most encouraging is the very high per-
formance of the asymmetric features: by themselves, they perform substantially better
than just the distributional features. we investigate this further below in section 7.1.5.

as with the entire dataset, we once again see that all the features are highly
complementary, and intrinsic accuracy is greatly improved by using all the features
together. it may be surprising that these signi   cant gains in intrinsic accuracy do not
translate to improvements on the rte tasks; in fact, there is a minor drop from using
all features compared to only using id138. this most likely depends on which pairs
the system gets right or wrong. for sentences involving multiple lexical rules, errors
become disproportionately costly. as such, the high-precision id138 predictions are
slightly better on the rte task.

in a qualitative analysis comparing a classi   er with only cosine distributional fea-
tures to a classi   er with the full feature set, we found that, as expected, the distributional
features miss many hypernyms and falsely classify many co-hyponyms as entailing:
we manually analyzed a sample of 170 pairs that the distributional classi   er falsely
classi   es as entailing. of these, 67 were co-hyponyms (39%), 33 were antonyms (19%),
and 32 were context-speci   c pairs like stir/fry. on the other hand, most (87%) cases of
entailment that the distributional classi   er detects but the all-features classi   er misses
are word pairs that have no link in id138. these pairs include note     paper, swimmer
    racer, eat     bite, and stand     wait.

7.1.4 evaluating the phrasal entailment rule classi   er. table 5 shows performance
when looking at only the phrasal rules. as with the evaluation of lexical rules, we
evaluate the rte tasks only on sentence pairs that use phrasal rules, and do not provide
any lexical id136s. as such, the ceiling score must again be recomputed.

we    rst notice that the phrasal subset is generally harder than the lexical subset:
none of the features sets on their own provide dramatic improvements over the baseline,
or come particularly close to the ceiling score. on the other hand, using all features
together does better than any of the feature groups by themselves, indicating again that
the feature groups are highly complementary.

33

computational linguistics

volume xx, number xx

feature set
always guess neutral
gold standard annotations
base only
wordform only
id138 only
dist (lexical) only
dist (phrasal) only
all features

intrinsic
67.8
100.0
68.3
72.5
73.9
72.9
71.9
77.8

rte train
72.5
91.9
73.3
77.1
78.3
77.0
75.7
79.7

rte test
72.7
92.8
73.6
77.1
77.7
76.5
75.3
78.8

table 5: cross-validation accuracy on entailment on phrasal rules only

distributional features perform rather close to the wordform features, suggesting
that possibly the distributional features may simply be proxies for the same lemma
and same pos features. a qualitative analysis comparing the predictions of wordform
and distributional features shows otherwise though: the wordform features are best at
correctly identifying non-entailing phrases (higher precision), while the distributional
features are best at correctly identifying entailing phrases (higher recall).

as with the full dataset, we see that the features based on paperno, pham, and
baroni (2014) do not perform as well as just the alignment-based distributional lexical
features; in fact, they do not perform even as well as features which make predictions
using only wordform features. we qualitatively compare the paperno et al. features (or
phrasal features for short) to the features based on word similarity of greedily aligned
words (or alignment features). we generally    nd the phrase features are much more
likely to predict neutral, while the alignment-based features are much more likely to
predict entailing. in particular, the phrasal vectors seem to be much better at capturing
non-entailment based on differences in prepositions (walk inside building (cid:57) walk outside
building), additional modi   ers on the rhs (man (cid:57) old man, room (cid:57) darkened room), and
changing semantic roles (man eats near kitten (cid:57) kitten eats). surprisingly, we found the
lexical distributional features were better at capturing complex paraphrases, such as
teenage     in teens, ride bike     biker, or young lady     teenage girl.

7.1.5 evaluating the asymmetric classi   er. levy et al. (2015) show several ex-
periments suggesting that asymmetric classi   ers do not perform better at the task of
identifying hypernyms than when the rhs vectors alone are used as features. that
is, they    nd that the asymmetric classi   er and variants frequently learn to identify
prototypical hypernyms rather than the hypernymy relation itself. we look at our data
in the light of the levy et al. study, in particular as none of the entailment problem sets
used by levy et al. were derived from an existing rte dataset.

in a qualitative analysis comparing the predictions of a classi   er using only
asymmetric features with a classi   er using only cosine similarity, we found that the
asymmetric classi   er does substantially better at distinguishing hypernymy from co-
hyponymy. this is what we had hoped to    nd, as we had previously found an asym-
metric classi   er to perform well at identifying hypernymy in other data (roller, erk,
and boleda 2014), and cosine is known to heavily favor co-hyponymy (baroni and lenci
2011). however, we also    nd that cosine features are better at discovering synonymy,
and that asymmetric frequently mistakes antonymy as an entailing. we did a quantita-
tive analysis comparing the predictions of a classi   er using only asymmetric features
to a classi   er that tries to learn typical hyponyms or hypernyms by using only the lhs
vectors, or the rhs vectors, or both. table 6 shows the results of these experiments.

counter to the main    ndings of levy et al. (2015), we    nd that there is at least
some learning of the entailment relationship by the asymmetric classi   er (in particular

34

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

feature set
always guess neutral
gold standard annotations
asym only
lhs only
rhs only
lhs + rhs
asym + lhs + rhs

intrinsic
56.6
100.0
76.8
65.4
73.2
76.4
81.4

rte train
69.4
93.2
78.3
73.8
78.6
79.8
81.4

rte test
69.3
94.6
79.2
73.5
79.9
80.6
82.6

table 6: cross-validation accuracy on entailment on lexical rules for asym evaluation

on the intrinsic evaluation), as opposed to the prototypical hypernym hypothesis. we
believe this is because the dataset is too varied to allow the classi   er to learn what
an entailing rhs looks like. indeed, a qualitative analysis shows that the asymmetric
features successfully predict many hypernyms that rhs vectors miss. on the other
hand, the rhs do manage to capture particular semantic classes, especially on words
that appear many times in the dataset, like cut, slice, man, cliff, and weight.

the classi   er given both the lhs and rhs vectors dramatically outperforms its
components: it is given freedom to nearly memorize rules that appear commonly in the
data. still, using all three sets of features (asym + lhs + rhs) is most powerful by a
substantial margin. this feature set is able to capture the frequently occurring items,
while also allowing some power to generalize to novel entailments. for example, by
using all three we are able to capture some additional hypernyms (beer     drink, pistol
    gun) and synonyms (couch     sofa, throw    , hurl), as well as some more dif   cult
entailments (hand     arm, young     little).

still, there are many ways our lexical classi   er could be improved, even using all
of the features in the system. in particular, it seems to do particularly bad on antonyms
(strike (cid:57) miss), and items that require additional world knowledge (surfer     man). it also
occasionally misclassi   es some co-hyponyms (trumpet (cid:57) guitar) or gets the entailment
direction wrong (toy (cid:57) ball).

7.2 rte task evaluation

this section evaluates different components of the system, and    nds a con   guration of
our system that achieves state-of-the-art results on the sick rte dataset.

we evaluate the following system components. the component logic is our basic
mln-based logic system that computes two id136 probabilities (section 4.1). this
includes the changes to the logical form to handle the domain closure assumption (sec-
tion 4.2), the id136 algorithm for query formulas (section 6.1), and the id136 opti-
mization (section 6.2). the component cws deals with the problem that the closed-world
assumption raises for negation in the hypothesis (section 4.3), and coref is coreference
resolution to identify contradictions (section 4.4). the component multiparse signals
the use of two parsers, the top c&c parse and the top easyid35 parse (section 4.5).

the remaining components add entailment rules. the component eclassif adds
the rules from the best performing entailment rule classi   er trained in section 7.1.
this is the system with all features included. the ppdb component adds rules from
ppdb paraphrase collection (section 5.3). the wlearn component learns a scaling factor
for ppdb rules, and another scaling factor for the eclassif rules that maps the clas-
si   cation con   dence scores to mln weights (section 6.3). without weight learning,
the scaling factor for ppdb is set to 1, and all eclassif rules are used as hard rules
(in   nite weight). the wlearn_log component is similar to wlearn but uses equation
11, which    rst transforms a rule weight to its log odds. the wn component adds rules

35

computational linguistics

volume xx, number xx

components enabled
logic
+ cwa
+ cwa + coref
+ cwa + coref + ppdb
+ cwa + coref + ppdb + wlearn
+ cwa + coref + ppdb + wlearn + wn
+ cwa + coref + ppdb + wlearn + wn + handcoded
+ cwa + coref + ppdb + wlearn + wn + handcoded + multiparse

63.2
72.1
73.8
75.3
76.5
78.8
79.2
80.8

63.5
71.7
73.4
74.8
76.3
78.4
78.8
80.4

train acc. test acc.

table 7: ablation experiment for the system components without eclassif

from id138 (section 5.3). in addition, we have a few handcoded rules (section 5.3).
like wn, the components hyp and mem repeat information that is used as features
for entailment rules classi   cation but is not always picked up by the classi   er. as the
classi   er sometimes misses hypernyms, hyp marks all hypernymy rules as entailing (so
this component is subsumed by wn), as well as all rules where the left-hand side and
the right-hand side are the same. (the latter step becomes necessary after splitting long
rules derived by our modi   ed robinson resolution; some of the pieces may have equal
left-hand and right-hand sides.) the mem component memorizes all entailing rules seen
in the training set of eclassif.

sometimes id136 takes a long time, so we set a 2 minute timeout for each
id136 run. if id136 does not    nish processing within the time limit, we terminate
the process and return an error code. about 1% of the dataset times out.

7.2.1 ablation experiment without eclassif. because eclassif has the most impact
on the system   s accuracy, and when enabled suppresses the contribution of the other
components, we evaluate the other components    rst without eclassif. in the following
section, we add the eclassif rules. table 7 summarizes the results of this experiment.
the results show that each component plays a role in improving the system accuracy.
our best accuracy without eclassif is 80.4%. without handling the problem of negated
hypotheses (logic alone), p (  h|t ) is almost always 1 and this additional id136
becomes useless, resulting in an inability to distinguish between neutral and contra-
diction. adding cwa signi   cantly improves accuracy because the resulting system has
p (  h|t ) equal to 1 only for contradictions.

each rule set (ppdb, wn, handcoded) improves accuracy by reducing the number
of false negatives. we also note that applying weight learning (wlearn) to    nd a global
scaling factor for ppdb rules makes them more useful. the learned scaling factor is
3.0. when the knowledge base is lacking other sources, weight learning assigns a high
scaling factor to ppdb, giving it more in   uence throughout. when eclassif is added
in the following section, weight learning assigns ppdb a low scaling factor because
eclassif already includes a large set of useful rules, such that only the highest weighted
ppdb rules contribute signi   cantly to the    nal id136.

the last component tested is the use of multiple parses (multiparse). many of the
false negatives are due to misparses. using two different parses reduces the impact of
the misparses, improving the system accuracy.

7.2.2 ablation experiment with eclassif. in this experiment, we    rst use eclassif
as a knowledge base, then incrementally add the other system components. table 8
summarizes the results. first, we note that adding eclassif to the knowledge base kb
signi   cantly improves the accuracy from 73.4% to 83.0%. this is higher than what

36

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

train acc. test acc.

components enabled
logic + cwa + coref
logic + cwa + coref + eclassif
+ handcoded
+ handcoded + multiparse
+ handcoded + multiparse + hyp
+ handcoded + multiparse + hyp + wlearn
+ handcoded + multiparse + hyp + wlearn_log
+ handcoded + multiparse + hyp + wlearn_log + mem
+ handcoded + multiparse + hyp + wlearn_log + mem + ppdb
current state of the art (lai and hockenmaier 2014)

73.8
84.0
84.6
85.0
85.6
85.7
85.9
93.4
93.4

   

73.4
83.0
83.2
83.9
83.9
84.1
84.3
85.1
84.9
84.6

table 8: ablation experiment for the system components with eclassif, and the best
performing con   guration

ppdb and wn achieved without eclassif. adding handcoded still improves the accuracy
somewhat.

adding multiparse improves accuracy, but interestingly, not as much as in the
previous experiment (without eclassif). the improvement on the test set decreases from
1.6% to 0.7%. therefore, the rules in eclassif help reduce the impact of misparses. here
is an example to show how: t: an ogre is jumping over a wall, h: an ogre is jumping over
the fence which in logic are:
t :    x, y, z. ogre(x)     agent(y, x)     jump(y)     over(y, z)     wall(z)
h:    x, y, z. ogre(x)     agent(y, x)     jump(y)     over(y)     patient(y, z)     wall(z)
t should entail h (strictly speaking, wall is not a fence but this is a positive entailment
example in sick). the modi   ed robinson resolution yields the following rule:

f :    x, y. jump(x)     over(x, y)     wall(y)     jump(x)     over(x)     patient(x, y)    

wall(y)

note that in t , the parser treats over as a preposition, while in h, jump over is treated
as a particle verb. a lexical rule wall     fence is not enough to get the right id136
because of this inconsistency in the parsing. the rule f re   ects this parsing inconsis-
tency. when f is translated to text for the entailment classi   er, we obtain jump over
wall     jump over fence, which is a simple phrase that the entailment classi   er addresses
without dealing with the complexities of the logic. without the modi   ed robinson
resolution, we would have had to resort to collecting    structural    id136 rules like
   x, y. over(x, y)     over(x)     patient(x, y).

table 8 also shows the impact of hyp and mem, two components that in principle
should not add anything over eclassif, but they do add some accuracy due to noise in
the training data of eclassif.

weight learning results are the rows wlearn and wlearn_log. both weight learning
components help improve the system   s accuracy. it is interesting to see that even though
the sick dataset is not designed to evaluate "degree of entailment", it is still useful to
keep the rules uncertain (as opposed to using hard rules) and use probabilistic id136.
results also show that wlearn_log performs slightly better than wlearn.

finally, adding ppdb does not improve the accuracy. apparently, eclassif already
captures all the useful rules that we were getting from ppdb. it is interesting to see that
simple distributional information can subsume a large paraphrase database like ppdb.
adding wn (not shown in the table) leads to a slight decrease in accuracy.

the system comprising logic, cwa, coref, multiparse, eclassif, handcoded, hyp,
wlearn_log, and mem achieves a state-of-the-art accuracy score of 85.1% on the sick
test set. the entailment rule classi   er eclassif plays a vital role in this result.

37

computational linguistics

volume xx, number xx

8. future work

one area to explore is contextualization. the evaluation of the entailment rule classi   er
showed that some of the entailments are context-speci   c, like put/pour (which are
entailing only for liquids) or push/knock (which is entailing in the context of    pushing
a toddler into a puddle   ). cosine-based distributional features were able to identify
some of these cases when all other features did not. we would like to explore whether
contextualized distributional word representations, which take the sentence context into
account (erk and pad   2008; thater, f  rstenau, and pinkal 2010; dinu, thater, and laue
2012), can identify such context-speci   c lexical entailments more reliably.

we would also like to explore new ways of measuring lexical entailment. it is well-
known that cosine similarity gives high ratings to co-hyponyms (baroni and lenci 2011),
and our evaluation con   rmed that this is a problem for lexical entailment judgments, as
co-hyponyms are usually not entailing. however, co-hyponymy judgments can be used
to position unknown terms in the id138 hierarchy (snow, jurafsky, and ng 2006).
this could be a new way of using distributional information in lexical entailment: using
cosine similarity to position a term in an existing hierarchy, and then using the relations
in the hierarchy for lexical entailment. while distributional similarity is usually used
only on individual word pairs, this technique would use distributional similarity to
learn the meaning of unknown terms given that many other terms are already known.
while this paper has focused on the rte task, we are interested in applying our
system to other tasks, in particular id53 task. this task is interesting
because it may offer a wider variety of tasks to the distributional subsystem. existing
logic-based systems are usually applied to limited domains, such as querying a speci   c
database (kwiatkowski et al. 2013; berant et al. 2013), but with our system, we have the
potential to query a large corpus because we are using boxer for wide-coverage seman-
tic analysis. the general system architecture discussed in this paper can be applied to
the id53 task with some modi   cations. for knowledge base construction,
the general idea of using theorem proving to infer rules still applies, but the details of
the technique would be a lot different from the modi   ed robinson resolution in section
5.1. for the probabilistic logic id136, scaling becomes a major challenge.

another important extension to this work is to support generalized quanti   ers in
probabilistic logic. some determiners, such as    few    and    most   , cannot be represented
in standard    rst-order logic, and are usually addressed using higher-order logics. but it
could be possible to represent them using the probabilistic aspect of probabilistic logic,
sidestepping the need for higher-order logic.

9. conclusion

being able to effectively represent natural language semantics is important and has
many important applications. we have introduced an approach that uses probabilistic
logic to combine the expressivity and automated id136 provided by logical represen-
tations, with the ability to capture graded aspects of natural language captured by dis-
tributional semantics. we evaluated this semantic representation on the rte task which
requires deep semantic understanding. our system maps natural-language sentences
to logical formulas, uses them to build probabilistic logic id136 problems, builds
a knowledge base from precompiled resources and on-the-   y distributional resources,
then performs id136 using markov logic. experiments demonstrated state-of-the-
art performance on the recently introduced sick rte task.

38

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

acknowledgments
this research was supported by the darpa deft program under afrl grant
fa8750-13-2-0026, by the nsf career grant iis 0845925 and by the nsf grant iis 1523637. any
opinions,    ndings, and conclusions or recommendations expressed in this material are those of
the author and do not necessarily re   ect the view of darpa, dod or the us government. some
experiments were run on the mastodon cluster supported by nsf grant eia-0303609, and the
texas advanced computing center (tacc) at the university of texas at austin.

references
[alshawi1992]alshawi, hiyan. 1992. the core

language engine. mit press.

[andrews, vigliocco, and vinson2009]

andrews, mark, gabriella vigliocco, and
david vinson. 2009. integrating
experiential and distributional data to
learn semantic representations.
psychological review, 116(3):463   498.
[bach et al.2013]bach, stephen h., bert

huang, ben london, and lise getoor.
2013. hinge-loss markov random    elds:
convex id136 for structured
prediction. in uai 2013.

[baroni et al.2012]baroni, marco, raffaella

bernardi, ngoc-quynh do, and
chung-chieh shan. 2012. entailment above
the word level in id65.
in eacl 2012.

[baroni, bernardi, and zamparelli2014]

baroni, marco, raffaella bernardi, and
roberto zamparelli. 2014. frege in space:
a program for compositional
id65. linguistic issues
in language technology, 9(6):5   110.

[baroni and lenci2010]baroni, marco and
alessandro lenci. 2010. distributional
memory: a general framework for
corpus-based semantics. computational
linguistics, 36(4):673   721.

[baroni and lenci2011]baroni, marco and

alessandro lenci. 2011. how we blessed
distributional semantic evaluation. in
gems 2011 workshop on geometrical
models of natural language semantics.

[baroni and zamparelli2010]baroni, marco
and roberto zamparelli. 2010. nouns are
vectors, adjectives are matrices:
representing adjective-noun constructions
in semantic space. in emnlp 2010.

[beltagy et al.2013]beltagy, i., cuong chau,
gemma boleda, dan garrette, katrin erk,
and raymond mooney. 2013. montague
meets markov: deep semantics with
probabilistic logical form. in *sem 2013.
[beltagy and erk2015]beltagy, i. and katrin

erk. 2015. on the proper treatment of
quanti   ers in probabilistic logic semantics.
in iwcs 2015.

[beltagy, erk, and mooney2014]beltagy, i.,
katrin erk, and raymond mooney. 2014.
probabilistic soft logic for semantic textual
similarity. in acl 2014.

[beltagy and mooney2014]beltagy, i. and

raymond j. mooney. 2014. ef   cient
markov logic id136 for natural
language semantics. in starai 2014.

[berant et al.2013]berant, jonathan, andrew
chou, roy frostig, and percy liang. 2013.
id29 on freebase from
question-answer pairs. in emnlp 2013.
[bird, klein, and loper2009]bird, steven,
ewan klein, and edward loper. 2009.
natural language processing with python.
"o   reilly media, inc.".

[bjerva et al.2014]bjerva, johannes, johan

bos, rob van der goot, and malvina
nissim. 2014. the meaning factory: formal
semantics for recognizing textual
entailment and determining semantic
similarity. in semeval 2014.

[blackburn and bos2005]blackburn, patrick

and johan bos. 2005. representation and
id136 for natural language: a first
course in computational semantics. csli
publications.

[bos2008]bos, johan. 2008. wide-coverage

semantic analysis with boxer. in
proceedings of semantics in text processing
(step-2008).

[bos2009]bos, johan. 2009. applying

automated deduction to natural language
understanding. journal of applied logic,
7:100   112.

[broecheler, mihalkova, and getoor2010]

broecheler, matthias, lilyana mihalkova,
and lise getoor. 2010. probabilistic
similarity logic. in uai 2010.

[brown2008]brown, susan windisch. 2008.

choosing sense distinctions for wsd:
psycholinguistic evidence. in acl 2008.

[bruni et al.2012]bruni, elia, gemma boleda,
marco baroni, and nam-khanh tran. 2012.
id65 in technicolor. in
acl 2012.

[bunescu and mooney2007]bunescu, razvan

and ray mooney. 2007. multiple instance
learning for sparse positive bags. in icml
2007.

39

computational linguistics

volume xx, number xx

[chang and lin2001]chang, chih-chung

and chih-jen lin. 2001. libid166: a library
for support vector machines. software
available at http://www.csie.ntu.
edu.tw/~cjlin/libid166.

[clark2012]clark, stephen. 2012. vector

space models of lexical meaning. in
handbook of contemporary semantics.
wiley-blackwell, 2 edition.

[clark and curran2004]clark, stephen and

james r. curran. 2004. parsing the wsj
using id35 and id148. in acl
2004.

[coecke, sadrzadeh, and clark2011]coecke,
bob, mehrnoosh sadrzadeh, and stephen
clark. 2011. mathematical foundations for
a compositional distributed model of
meaning. linguistic analysis,
36(1-4):345   384.

[cooper et al.2015]cooper, robin, simon

dobnik, shalom lappin, and staffan
larsson. 2015. probabilistic type theory
and natural language semantics. linguistic
issues in language technology, 10(4).

[copestake and flickinger2000]copestake,

ann and dan flickinger. 2000. an
open-source grammar development
environment and broad-coverage english
grammar using hpsg. in lrec 2000.
[dagan et al.2013]dagan, ido, dan roth,

mark sammons, and fabio massimo
zanzotto. 2013. recognizing textual
entailment: models and applications.
synthesis lectures on human language
technologies, 6(4):1   220.

[dechter, kask, and mateescu2002]dechter,
rina, kalev kask, and robert mateescu.
2002. iterative join-graph propagation. in
uai 2002.

[dietterich, lathrop, and lozano-perez1997]
dietterich, thomas g., richard h. lathrop,
and tomas lozano-perez. 1997. solving
the multiple instance problem with
axis-parallel rectangles. arti   cial
intelligence, 89(1-2):31   71.

[dinu, thater, and laue2012]dinu,

georgiana, stefan thater, and s  ren laue.
2012. a comparison of models of word
meaning in context. in hlt-naacl 2012.

[dowty, wall, and peters1981]dowty,

david r., robert e. wall, and stanley
peters. 1981. introduction to montague
semantics. d. reidel, dordrecht, holland.
[edmonds and hirst2000]edmonds, philip

and graeme hirst. 2000. reconciling
   ne-grained lexical knowledge and
coarse-grained ontologies in the
representation of near-synonyms. in
proceedings of the workshop on semantic

40

[ganitkevitch, van durme, and callison-burch2013]

approximation, granularity, and vagueness.

[erk and pad  2008]erk, katrin and sebastian

pad  . 2008. a structured vector space
model for word meaning in context. in
emnlp 2008.

[fu et al.2014]fu, ruiji, jiang guo, bing qin,

wanxiang che, haifeng wang, and ting
liu. 2014. learning semantic hierarchies
via id27s. in acl 2014.

ganitkevitch, juri, benjamin van durme,
and chris callison-burch. 2013. ppdb: the
paraphrase database. in naacl-hlt 2013.

[garrette, erk, and mooney2011]garrette,
dan, katrin erk, and raymond mooney.
2011. integrating logical representations
with probabilistic information using
markov logic. in iwcs 2011.

[geffet and dagan2005]geffet, maayan and

ido dagan. 2005. the distributional
inclusion hypotheses and lexical
entailment. in acl 2005.

[genesereth and nilsson1987]genesereth,

m. r. and n. j. nilsson. 1987. logical
foundations of arti   cial intelligence. morgan
kaufman.

[getoor and taskar2007]getoor, l. and

b. taskar. 2007. introduction to statistical
relational learning. mit press.

[geurts2007]geurts, bart. 2007. existential

import. in i. comorovski and k. van
heusinger, editors, existence: syntax and
semantics. springer, dordrecht, pages
253   271.

[gogate2014]gogate, vibhav. 2014.

ijgp-sampling and samplesearch.
software available at
http://www.hlt.utdallas.edu/
~vgogate/ijgp-samplesearch.html.

[gogate and dechter2011]gogate, vibhav
and rina dechter. 2011. samplesearch:
importance sampling in presence of
determinism. arti   cial intelligence,
175(2):694   729.

[gogate and domingos2011]gogate, vibhav
and pedro domingos. 2011. probabilistic
theorem proving. in uai 2011.

[grefenstette2013]grefenstette, edward.
2013. towards a formal distributional
semantics: simulating logical calculi with
tensors. in *sem 2013.

[grefenstette and sadrzadeh2011]

grefenstette, edward and mehrnoosh
sadrzadeh. 2011. experimental support for
a categorical compositional distributional
model of meaning. in emnlp 2011.

[herbelot and vecchi2015]herbelot, aur  lie

and eva maria vecchi. 2015. building a
shared world: mapping distributional to

beltagy, roller, cheng, erk and mooney

meaning using logical and distributional models

model-theoretic semantic spaces.

[hobbs et al.1988]hobbs, jerry r., mark e.

stickel, paul martin, and douglas
edwards. 1988. interpretation as
abduction. in acl 1988.

[kamp and reyle1993]kamp, hans and uwe

reyle. 1993. from discourse to logic.
kluwer.

[kok et al.2005]kok, stanley, parag singla,

matthew richardson, and pedro
domingos. 2005. the alchemy system for
statistical relational ai. http://www.cs.
washington.edu/ai/alchemy.

[kotlerman et al.2010]kotlerman, lili, ido

dagan, idan szpektor, and maayan
zhitomirsky-geffet. 2010. directional
distributional similarity for lexical
id136. natural language engineering,
16(4):359   389.

[kruszewski, paperno, and baroni2015]

kruszewski, german, denis paperno, and
marco baroni. 2015. deriving boolean
structures from distributional vectors.
tacl 2015, 3:375   388.

[kwiatkowski et al.2013]kwiatkowski, tom,

eunsol choi, yoav artzi, and luke
zettlemoyer. 2013. scaling semantic
parsers with on-the-   y ontology matching.
in emnlp 2013.

[lai and hockenmaier2014]lai, alice and

julia hockenmaier. 2014. illinois-lh: a
denotational and distributional approach
to semantics. in semeval 2014.

[landauer and dumais1997]landauer,
thomas and susan dumais. 1997. a
solution to plato   s problem: the latent
semantic analysis theory of the
acquisition, induction, and representation
of knowledge. psychological review,
104(2):211.

[lenci and benotto2012]lenci, alessandro

and giulia benotto. 2012. identifying
hypernyms in distributional semantic
spaces. in *sem 2012.

[levy et al.2015]levy, omer, steffen remus,
chris biemann, and ido dagan. 2015. do
supervised distributional methods really
learn lexical id136 relations? in naacl
hlt 2015.

[lewis and steedman2013]lewis, mike and

mark steedman. 2013. combined
distributional and logical semantics. tacl
2013, 1:179   192.

[lewis and steedman2014]lewis, mike and

mark steedman. 2014. a* id35 parsing
with a supertag-factored model. in
emnlp 2014.

[liang, jordan, and klein2011]liang, percy,

michael jordan, and dan klein. 2011.

learning dependency-based
id152. in acl-hlt
2011.

[lund and burgess1996]lund, kevin and

curt burgess. 1996. producing
high-dimensional semantic spaces from
lexical co-occurrence. behavior research
methods, instruments, and computers,
28(2):203   208.

[maccartney and manning2009]maccartney,
bill and christopher d manning. 2009. an
extended model of natural logic. in iwcs
2009.

[marelli et al.2014a]marelli, marco, luisa

bentivogli, marco baroni, raffaella
bernardi, stefano menini, and roberto
zamparelli. 2014a. semeval-2014 task 1:
evaluation of compositional distributional
semantic models on full sentences through
semantic relatedness and textual
entailment. in semeval 2014.

[marelli et al.2014b]marelli, marco, stefano
menini, marco baroni, luisa bentivogli,
raffaella bernardi, and roberto
zamparelli. 2014b. a sick cure for the
evaluation of compositional distributional
semantic models. in lrec 2014.

[mikolov et al.2013]mikolov, tomas, kai
chen, greg corrado, and jeffrey dean.
2013. ef   cient estimation of word
representations in vector space. in iclr
2013.

[mikolov, yih, and zweig2013]mikolov,

tomas, wentau yih, and geoffrey zweig.
2013. linguistic regularities in continuous
space word representations. in
naacl-hlt 2013.

[mitchell and lapata2008]mitchell, jeff and
mirella lapata. 2008. vector-based models
of semantic composition. in acl 2008.

[mitchell and lapata2010]mitchell, jeff and

mirella lapata. 2010. composition in
distributional models of semantics.
cognitive science, 34(3):1388   1429.

[montague1970]montague, richard. 1970.
universal grammar. theoria, 36:373   398.

[ng and curran2012]ng, dominick and

james r curran. 2012. dependency
hashing for n-best id35 parsing. in acl
2012.

[paperno, pham, and baroni2014]paperno,

denis, nghia the pham, and marco
baroni. 2014. a practical and
linguistically-motivated approach to
compositional id65. in
acl 2014.

[parsons1990]parsons, terry. 1990. events in

the semantics of english. mit press.

41

computational linguistics

volume xx, number xx

semantic taxonomy induction from
heterogenous evidence. in coling/acl
2006.

[strawson1950]strawson, p. f. 1950. on

referring. mind, 59:320   344.

[thater, f  rstenau, and pinkal2010]thater,
stefan, hagen f  rstenau, and manfred
pinkal. 2010. contextualizing semantic
representations using syntactically
enriched vector models. in acl 2010.
[tian, miyao, and takuya2014]tian, ran,
yusuke miyao, and matsuzaki takuya.
2014. logical id136 on
dependency-based compositional
semantics. in acl 2014.

[turney and pantel2010]turney, peter and
patrick pantel. 2010. from frequency to
meaning: vector space models of
semantics. journal of arti   cial intelligence
research, 37(1):141   188.

[van eijck and lappin2012]van eijck, jan and

shalom lappin. 2012. probabilistic
semantics for natural language. in logic
and interactive rationality (lira) yearbook.
amsterdam dynamics group.

[van eijck and unger2010]van eijck, jan and

christina unger. 2010. computational
semantics with functional programming.
cambridge university press.

[weeds et al.2014]weeds, julie, daoud

clarke, jeremy ref   n, david weir, and bill
keller. 2014. learning to distinguish
hypernyms and co-hyponyms. in coling
2014.

[weeds, weir, and mccarthy2004]weeds,
julie, david weir, and diana mccarthy.
2004. characterising measures of lexical
distributional similarity. in coling 2004.
[zelle and mooney1996]zelle, john m. and

raymond j. mooney. 1996. learning to
parse database queries using inductive
logic programming. in aaai 1996.
[zirn et al.2011]zirn, c  cilia, mathias

niepert, heiner stuckenschmidt, and
michael strube. 2011. fine-grained
id31 with structural
features. in ijcnlp 2011.

[pasupat and liang2015]pasupat, panupong

and percy liang. 2015. compositional
id29 on semi-structured
tables. in acl 2015.

[pearl1988]pearl, judea. 1988. probabilistic

reasoning in intelligent systems: networks of
plausible id136. morgan kaufmann.
[poon and domingos2008]poon, hoifung

and pedro domingos. 2008. joint
unsupervised coreference resolution with
markov logic. in emnlp 2008.

[princeton university2010]princeton
university. 2010. about id138.
http://id138.princeton.edu.

[richardson and domingos2006]richardson,

matthew and pedro domingos. 2006.
markov logic networks. machine learning,
62:107   136.

[riedel et al.2009]riedel, sebastian,

hong-woo chun, toshihisa takagi, and
jun   ichi tsujii. 2009. a markov logic
approach to bio-molecular event
extraction. in bionlp 2009.

[riedel and meza-ruiz2008]riedel, sebastian

and ivan meza-ruiz. 2008. collective
semantic role labelling with markov logic.
in conll   08.

[robinson1965]robinson, j. a. 1965. a
machine-oriented logic based on the
resolution principle. journal of the acm,
12(1):23   41.

[roller, erk, and boleda2014]roller, stephen,

katrin erk, and gemma boleda. 2014.
inclusive yet selective: supervised
distributional hypernymy detection. in
coling 2014.

[sadrzadeh, clark, and coecke2013]

sadrzadeh, mehrnoosh, stephen clark,
and bob coecke. 2013. the frobenius
anatomy of word meanings i: subject and
object relative pronouns. journal of logic
and computation, 23(6):1293   1317.

[santus2013]santus, enrico. 2013. slqs: an

id178 measure. master   s thesis,
university of pisa.

[sch  tze1998]sch  tze, hinrich. 1998.

automatic word sense discrimination.
computational linguistics, 24(1):97   123.
[silberer and lapata2012]silberer, carina

and mirella lapata. 2012. grounded
models of semantic representation. in
emnlp-conll 2012.

[skolem1920]skolem, thoralf. 1920.

logisch-kombinatorische untersuchungen
  ber die erf  llbarkeit oder beweisbarkeit
mathematischer s  tze. skrifter utgit av
videnskapselskapet i kristiania, 4:4   36.

[snow, jurafsky, and ng2006]snow, rion,

daniel jurafsky, and andrew y. ng. 2006.

42

