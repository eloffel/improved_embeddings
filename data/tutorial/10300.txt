5
1
0
2

 

v
o
n
9

 

 
 
]
l
c
.
s
c
[
 
 

3
v
4
3
8
4
0

.

6
0
5
1
:
v
i
x
r
a

tree-structured composition in neural networks

without tree-structured architectures

samuel r. bowman, christopher d. manning, and christopher potts

stanford university

{sbowman, manning, cgpotts}@stanford.edu

stanford, ca 94305-2150

abstract

tree-structured neural networks encode a particular tree geometry for a sentence
in the network design. however, these models have at best only slightly out-
performed simpler sequence-based models. we hypothesize that neural sequence
models like lstms are in fact able to discover and implicitly use recursive com-
positional structure, at least for tasks with clear cues to that structure in the data.
we demonstrate this possibility using an arti   cial data task for which recursive
compositional structure is crucial, and    nd an lstm-based sequence model can
indeed learn to exploit the underlying tree structure. however, its performance
consistently lags behind that of tree models, even on large training sets, suggest-
ing that tree-structured models are more effective at exploiting recursive structure.

1

introduction

neural networks that encode sentences as real-valued vectors have been successfully used in a wide
array of nlp tasks, including translation [1], parsing [2], and id31 [3]. these models
are generally either sequence models based on recurrent neural networks, which build representa-
tions incrementally from left to right [4, 1], or tree-structured models based on recursive neural
networks, which build representations incrementally according to the hierarchical structure of lin-
guistic phrases [5, 6].
while both model classes perform well on many tasks, and both are under active development, tree
models are often presented as the more principled choice, since they align with standard linguis-
tic assumptions about constituent structure and the compositional derivation of complex meanings.
nevertheless, tree models have not shown the kinds of dramatic performance improvements over se-
quence models that their billing would lead one to expect: head-to-head comparisons with sequence
models show either modest improvements [3] or none at all [7].
we propose a possible explanation for these results: standard sequence models can learn to exploit
recursive syntactic structure in generating representations of sentence meaning, thereby learning to
use the structure that tree models are explicitly designed around. this requires that sequence models
be able to identify syntactic structure in natural language. we believe this is plausible on the basis
of other recent research [8, 9]. in this paper, we evaluate whether lstm sequence models are able
to use such structure to guide interpretation, focusing on cases where syntactic structure is clearly
indicated in the data.
we compare standard tree and sequence models on their handling of recursive structure by training
the models on sentences whose length and recursion depth are limited, and then testing them on
longer and more complex sentences, such that only models that exploit the recursive structure will
be able to generalize in a way that yields correct interpretations for these test sentences. our methods
extend those of our earlier work in [10], which introduces an experiment and corresponding arti   cial
dataset to test this ability in two tree models. we adapt that experiment to sequence models by

1

not p3

p3

p3 (cid:64) p3 or p2

   

|

p4 or (not ((p1 or p6) or p4)) (cid:64) not ((((not p6) or (not p4)) and (not p5)) and (p6 and p6))

(not p2) and p6

not (p6 or (p5 or p3))

table 1: examples of short to moderate length pairs from the arti   cial data introduced in [10]. we
only show the parentheses that are needed to disambiguate the sentences rather than the full binary
bracketings that the models use.

decorating the statements with an explicit bracketing, and we use this design to compare an lstm
sequence model with three tree models, with a focus on what data each model needs in order to
generalize well.
as in [10], we    nd that standard tree neural networks are able to make the necessary generalizations,
with their performance decaying gradually as the structures in the test set grow in size. we addi-
tionally    nd that extending the training set to include larger structures mitigates this decay. then
considering sequence models, we    nd that a single-layer lstm is also able to generalize to unseen
large structures, but that it does this only when trained on a larger and more complex training set
than is needed by the tree models to reach the same generalization performance.
our results engage with those of [8] and [2], who    nd that sequence models can learn to recognize
syntactic structure in natural language, at least when trained on explicitly syntactic tasks. the sim-
plest model presented in [8] uses an lstm sequence model to encode each sentence as a vector,
and then generates a linearized parse (a sequence of brackets and constituent labels) with high accu-
racy using only the information present in the vector. this shows that the lstm is able to identify
the correct syntactic structures and also hints that it is able to develop a generalizable method for
encoding these structures in vectors. however, the massive size of the dataset needed to train that
model, 250m tokens, leaves open the possibility that it primarily learns to generate only tree struc-
tures that it has already seen, representing them as simple hashes   which would not capture unseen
tree structures   rather than as structured objects. our experiments, though, show that lstms can
learn to understand tree structures when given enough data, suggesting that there is no fundamental
obstacle to learning this kind of structured representation. we also    nd, though, that sequence mod-
els lag behind tree models across the board, even on training corpora that are quite large relative to
the complexity of the underlying grammar, suggesting that tree models can play a valuable role in
tasks that require recursive interpretation.

2 recursive structure in arti   cial data

reasoning about entailment the data that we use de   ne a version of the recognizing textual
entailment task, in which the goal is to determine what kind of logical consequence relation holds
between two sentences, drawing on a small    xed vocabulary of relations such as entailment, con-
tradiction, and synonymy. this task is well suited to evaluating neural network models for sentence
interpretation: models must develop comprehensive representations of the meanings of each sen-
tence to do well at the task, but the data do not force these representations to take a speci   c form,
allowing the model to learn whatever kind of representations it can use most effectively.
the data we use are labeled with the seven mutually exclusive logical relations of [11], which dis-
tinguish entailment in two directions ((cid:64), (cid:65)), equivalence (   ), exhaustive and non-exhaustive con-
tradiction (   , |), and two types of semantic independence (#, (cid:96)).
the arti   cial language the language described in [10] (  4) is designed to highlight the use of re-
cursive structure with minimal additional complexity. its vocabulary consists only of six unanalyzed
word types (p1, p2, p3, p4, p5, p6), and, or, and not. sentences of the language can be straightfor-
wardly interpreted as statements of id118 (where the six unanalyzed words types are
variable names), and labeled sentence pairs can be interpreted as theorems of that logic. some
example pairs are provided in table 1.
crucially, the language is de   ned such that any sentence can be embedded under negation or con-
junction to create a new sentence, allowing for arbitrary-depth recursion, and such that the scope of

2

negation and conjunction are determined only by bracketing with parentheses (rather than bare word
order). the compositional structure of each sentence can thus be an arbitrary tree, and interpreting
a sentence correctly requires using that structure.
the data come with parentheses representing a complete binary bracketing. our models use this
information in two ways. for the tree models, the parentheses are not word tokens, but rather are
used in the expected way to build the tree. for the sequence model, the parentheses are word tokens
with associated learned embeddings. this approach provides the models with equivalent data, so
their ability to handle unseen structures can be reasonably compared.

the data our sentence pairs are divided into thirteen bins according to the number of logical
connectives (and, or, not) in the longer of the two sentences in each pair. we test each model on
each bin separately (58k total examples, using an 80/20% train/test split) in order to evaluate how
each model   s performance depends on the complexity of the sentences. in three experiments, we
train our models on the training portions of bins 0   3 (62k examples), 0   4 (90k), and 0   6 (160k), and
test on every bin but the trivial bin 0. capping the size of the training sentences allows us to evaluate
how the models interpret the sentences: if a model   s performance falls off abruptly above the cutoff,
it is reasonable to conclude that it relies heavily on speci   c sentence structures and cannot generalize
to new structures. if a model   s performance decays gradually1 with no such abrupt change, then it
must have learned a more generally valid interpretation function for the language which respects its
recursive structure.

3 testing sentence models on entailment

we use the architecture depicted in figure 1a, which builds on the one used in [10]. the model
architecture uses two copies of a single sentence model (a tree or sequence model) to encode the
premise and hypothesis (left and right side) expressions, and then uses those encodings as the fea-
tures for a multilayer classi   er which predicts one of the seven relations. since the encodings are
computed separately, the sentence models must encode complete representations of the meanings of
the two sentences for the downstream model to be able to succeed.

classi   er the classi   er component of the model consists of a combining layer which takes the two
sentence representations as inputs, followed by two neural network layers, then a softmax classi   er.
for the combining layer, we use a neural tensor network (ntn, [12]) layer, which sums the output
of a plain recursive/recurrent neural network layer with a vector computed using two multiplications
with a learned (full rank) third-order tensor parameter:

(cid:20)(cid:126)x(l)

(cid:21)

(cid:126)x(r)

(1)

(2)

(cid:126)ynn = tanh(m

+ (cid:126)b )

(cid:126)yntn = (cid:126)ynn + tanh((cid:126)x(l)t t[1...n](cid:126)x(r))

our model is largely identical to the model from [10], but adds the two additional tanh nn layers,
which we found help performance across the board, and also uses the ntn combination layer when
evaluating all four models, rather than just the treerntn model, so as to ensure that the sentence
models are compared in as similar a setting as possible.
we only study models that encode entire sentences in    xed length vectors, and we set aside models
with attention [13], a technique which gives the downstream model (here, the classi   er) the potential
to access each input token individually through a soft content addressing system. while attention
simpli   es the problem of learning complex correspondences between input and output, there is
no apparent reason to believe that it should improve or harm a model   s ability to track structural
information like a given token   s position in a tree. as such, we expect our results to re   ect the same
basic behaviors that would be seen in attention-based models.

1since sentences are    xed-dimensional vectors of    xed-precision    oating point numbers, all models will
make errors on sentences above some length, and l2 id173 (which helps overall performance) ex-
acerbates this by discouraging the model from using the kind of numerically precise, nonlinearity-saturating
functions that generalize best.

3

7-way softmax classi   er

100d tanh nn layer

100d tanh nn layer

100d tanh ntn layer

50d premise

50d hypothesis

sentence model

with premise input

sentence model

with hypothesis input

(a) the general architecture shared across models.

...

a or b

a

or b

or

b

a

a

a or

or

a or b

...

b

(b) the architecture for the tree-structured sen-
tence models. terminal nodes are learned em-
beddings and nonterminal nodes are nn, ntn, or
treelstm layers.

(c) the architecture for the sequence sentence
model. nodes in the lower row are learned em-
beddings and nodes in the upper row are lstm
layers.

figure 1: in our model, two copies of a sentence model   based on either tree (b) or sequence
(c) models   encode the two input sentences. a multilayer classi   er component (a) then uses the
resulting vectors to predict a label that re   ects the logical relationship between the two sentences.

sentence models the sentence encoding component of the model transforms the (learned) em-
beddings of the input words for each sentence into a single vector representing that sentence. we
experiment with tree-structured models (figure 1b) with treeid56 (eqn. 1), treerntn (eqn. 2),
and treelstm [3] id180. in addition, we use a sequence model (figure 1c) with an
lstm activation function [14] implemented as in [15]. in experiments with a simpler non-lstm
id56 sequence model, the model tended to badly under   t the training data, and those results are not
included here.

training we randomly initialize all embeddings and layer parameters, and train them using mini-
batch stochastic id119 with adadelta [16] learning rates. our objective is the standard
negative log likelihood classi   cation objective with l2 id173 (tuned on a separate train/test
split). all models were trained for 100 epochs, after which all had largely converged without signif-
icantly declining from their peak performances.

4 results and discussion

the results are shown in figure 2. the tree models    t the training data well, reaching 98.9, 98.8,
and 98.4% overall accuracy respectively in the    6 setting, with the lstm under   tting slightly at
94.8%. in that setting, all models generalized well to structures of familiar length, with the tree
models all surpassing 97% on examples in bin 4, and the lstm reaching 94.8%. on the longer
test sentences, the tree models decay smoothly in performance across the board, while the lstm
decays more quickly and more abruptly, with a striking difference in the    4 setting, where lstm
performance falls 10% from bin 4 to bin 5, compared to 4.4% for the next worse model. however,
the lstm improves considerably with more ample training data in the    6 condition, showing only
a 3% drop and generalization results better than the best model   s in the    3 setting.
all four models robustly beat the simple baselines reported in [10]: the most frequent class occurs
just over 50% of the time and a neural id159 does reasonably on the shortest examples
but falls below 60% by bin 4.

4

(a) training on sz.    3.

(b) training on sz.    4.

(c) training on sz.    6.

figure 2: test accuracy on three experiments with increasingly rich training sets. the horizontal
axis on each graph divides the test set expression pairs into bins by the number of logical operators
in the more complex of the two expressions in the pair. the dotted line shows the size of the largest
examples in the training set in each experiment.

figure 3: learning curve for the    6 experiment.

the learning curve (figure 3) suggests that additional data is unlikely to change these basic results.
the lstm lags behind the tree models across the curve, but appears to gain accuracy at a similar
rate.

5 conclusion

we    nd that all four models are able to effectively exploit a recursively de   ned language to interpret
sentences with complex unseen structures. we    nd that tree models    biases allow them to do this
with greater ef   ciency, outperforming sequence-based models substantially in every experiment.
however, our sequence model is nonetheless able to generalize smoothly from seen sentence struc-
tures to unseen ones, showing that its lack of explicit recursive structure does not prevent it from
recognizing recursive structure in our arti   cial language.
we interpret these results as evidence that both tree and sequence architectures can play valuable
roles in the construction of sentence models over data with recursive syntactic structure. tree ar-
chitectures provide an explicit bias that makes it possible to ef   ciently learn to compositional in-
terpretation, which is dif   cult for sequence models. sequence models, on the other hand, lack this
bias, but have other advantages. since they use a consistent graph structure across examples, it is
easy to accelerate minibatch training in ways that yield substantially faster training times than are
possible with tree models, especially with gpus. in addition, when sequence models integrate each
word into a partial sentence representation, they have access to the entire sentence representation up
to that point, which may provide valuable cues for the resolution of lexical ambiguity, which is not
present in our arti   cial language, but is a serious concern in natural language text.
finally, we suggest that, because of the well-supported linguistic claim that the kind of recursive
structure that we study here is key to the understanding of real natural languages, there is likely to
be value in developing sequence models that can more ef   ciently exploit this structure without fully
sacri   cing the    exibility that makes them succeed.

acknowledgments

we gratefully acknowledge a google faculty research award, a gift from bloomberg l.p., and
support from the defense advanced research projects agency (darpa) deep exploration and fil-

5

deep$350d(treeid5650d(treerntn50d(lstm1110.935920.992420.984850.9848530.951320.978950.9368440.863420.931230.7516750.791250.890250.6554160.748520.861280.5712270.69440.80730.5076480.672530.782420.4725390.618830.757720.45988100.557990.750550.4267110.569490.705080.4203120.504590.666670.3639deep$450d(treeid5650d(treerntn50d(lstm1110.9871820.98990.992420.9848530.989470.986840.9394740.96180.960840.8777550.93860.947810.7613260.910980.913950.6869470.88710.903230.6035780.873630.870330.5549590.805560.868830.50463100.77680.846830.44639110.77280.803390.48814120.74610.788990.43119deep$650d(treeid5650d(treerntn50d(lstm110.987180.97436210.997470.9848530.996050.982890.9486840.978030.965620.9121350.977740.950880.8733760.974040.944360.8249370.955860.918510.7826880.943960.89670.7307790.898150.882720.6682140%50%60%70%80%90%100%123456789101112accuracysize of longer expression40%50%60%70%80%90%100%123456789101112accuracysize of longer expression40%50%60%70%80%90%100%123456789101112accuracysize of longer expression50d treeid5650d treerntn50d lstmdeep$350d(treeid5650d(treerntn50d(treelstm50d(lstmpost$submission,(epoch(100110.9871810.9871820.98990.982320.997470.9873730.9250.948680.961840.9421140.854820.908310.893030.7717350.795090.836530.832690.6607860.731450.788580.783380.5652870.716470.754670.777590.538280.642860.686810.70330.5087990.620370.657410.668210.42438100.57980.634570.64550.431110.5050.616950.60330.4372120.5290.581040.59020.4189drop(0.070180.040370.068810.17038deep$450d(treeid5650d(treerntn50d(treelstm50d(lstm11110.9743620.98990.997470.994950.9924230.978950.988160.988160.9657940.936960.955110.960840.906450.907140.925560.932460.8234860.858310.889470.896880.6832370.84550.876910.876060.6196980.80220.823080.824180.5626490.770060.782410.766980.50309100.75270.785560.778990.49234110.70160.715250.738980.44407120.75840.761470.697250.39755drop(0.029820.029550.028380.08292deep$650d(treeid5650d(treerntn50d(treelstm50d(lstm10.97436110.9743620.997470.997410.9848530.996050.98150.998680.9802640.985670.97420.982810.961850.976980.95850.968530.9194260.965130.92730.947330.8746370.957560.93880.936330.8429580.945050.90980.906590.7791290.918210.8750.878090.7083340%50%60%70%80%90%100%123456789101112accuracysize of longer expression50d treeid5650d treerntn50d treelstm50d lstm40%50%60%70%80%90%100%123456789101112accuracysize of longer expression50d treeid5650d treerntn50d treelstm50d lstm40%50%60%70%80%90%100%123456789101112accuracysize of longer expression50d treeid5650d treerntn50d treelstm50d lstmdeep$350d(treeid5650d(treerntn50d(treelstm50d(lstmpost$submission,(epoch(100110.9871810.9871820.98990.982320.997470.9873730.9250.948680.961840.9421140.854820.908310.893030.7717350.795090.836530.832690.6607860.731450.788580.783380.5652870.716470.754670.777590.538280.642860.686810.70330.5087990.620370.657410.668210.42438100.57980.634570.64550.431110.5050.616950.60330.4372120.5290.581040.59020.4189drop(0.070180.040370.068810.17038deep$450d(treeid5650d(treerntn50d(treelstm50d(lstm11110.9743620.98990.997470.994950.9924230.978950.988160.988160.9657940.936960.955110.960840.906450.907140.925560.932460.8234860.858310.889470.896880.6832370.84550.876910.876060.6196980.80220.823080.824180.5626490.770060.782410.766980.50309100.75270.785560.778990.49234110.70160.715250.738980.44407120.75840.761470.697250.39755drop(0.029820.029550.028380.08292deep$650d(treeid5650d(treerntn50d(treelstm50d(lstm10.97436110.9743620.997470.997410.9848530.996050.98150.998680.9802640.985670.97420.982810.961850.976980.95850.968530.9194260.965130.92730.947330.8746370.957560.93880.936330.8429580.945050.90980.906590.7791290.918210.8750.878090.7083340%50%60%70%80%90%100%123456789101112accuracysize of longer expression50d treeid5650d treerntn50d treelstm50d lstm40%50%60%70%80%90%100%123456789101112accuracysize of longer expression50d treeid5650d treerntn50d treelstm50d lstm40%50%60%70%80%90%100%123456789101112accuracysize of longer expression50d treeid5650d treerntn50d treelstm50d lstmdeep$350d(treeid5650d(treerntn50d(treelstm50d(lstmpost$submission,(epoch(100110.9871810.9871820.98990.982320.997470.9873730.9250.948680.961840.9421140.854820.908310.893030.7717350.795090.836530.832690.6607860.731450.788580.783380.5652870.716470.754670.777590.538280.642860.686810.70330.5087990.620370.657410.668210.42438100.57980.634570.64550.431110.5050.616950.60330.4372120.5290.581040.59020.4189drop(0.070180.040370.068810.17038deep$450d(treeid5650d(treerntn50d(treelstm50d(lstm11110.9743620.98990.997470.994950.9924230.978950.988160.988160.9657940.936960.955110.960840.906450.907140.925560.932460.8234860.858310.889470.896880.6832370.84550.876910.876060.6196980.80220.823080.824180.5626490.770060.782410.766980.50309100.75270.785560.778990.49234110.70160.715250.738980.44407120.75840.761470.697250.39755drop(0.029820.029550.028380.08292deep$650d(treeid5650d(treerntn50d(treelstm50d(lstm10.97436110.9743620.997470.997410.9848530.996050.98150.998680.9802640.985670.97420.982810.961850.976980.95850.968530.9194260.965130.92730.947330.8746370.957560.93880.936330.8429580.945050.90980.906590.7791290.918210.8750.878090.7083340%50%60%70%80%90%100%123456789101112accuracysize of longer expression50d treeid5650d treerntn50d treelstm50d lstm40%50%60%70%80%90%100%123456789101112accuracysize of longer expression50d treeid5650d treerntn50d treelstm50d lstm40%50%60%70%80%90%100%123456789101112accuracysize of longer expression50d treeid5650d treerntn50d treelstm50d lstmdeep$350d(treeid5650d(treerntn50d(treelstm50d(lstmfrom(sat(morn,(pass(10000010.9871811120.987370.97980.98990.9924230.914470.952630.959210.9460540.843360.890160.889210.7774650.785110.826550.832690.6492760.727740.774480.783380.5445170.716470.737690.76910.5314180.652750.660440.706590.498990.629630.632720.666670.42901100.5930.606130.66520.4004110.49830.579660.60.4711120.51980.593270.59630.425deep$450d(treeid5650d(treerntn50d(treelstm50d(lstm111120.992420.98232130.961840.964470.9815840.918820.916910.9541550.885650.879510.9155860.84570.837540.8642470.815790.81070.8531480.764840.762640.8164890.736110.736110.75154100.71330.73960.78993110.67790.67110.71525120.67580.69110.72171deep$650d(treeid5650d(treerntn50d(treelstm50d(lstm10.987180.8846210.9871820.987370.906570.997470.9722230.978950.835530.996050.9618440.951290.810890.986630.9178650.926320.795860.967770.8672360.905790.773740.942140.8100970.890490.76910.940580.7818380.853850.725270.905490.7252790.830250.708330.88580.6527840%50%60%70%80%90%100%123456789101112accuracysize of longer expression50d treeid5650d treerntn50d treelstm50d lstm40%50%60%70%80%90%100%123456789101112accuracysize of longer expression50d treeid5650d treerntn50d treelstm50d lstm40%50%60%70%80%90%100%123456789101112accuracysize of longer expression50d treeid5650d treerntn50d treelstm50d lstm10111250d(treeid5650d(treerntn50d(treelstm50d(lstm00.52520.52520.52520.52520.030.646260.614270.660630.582240.10.72820.763210.76610.665480.30.84470.917310.896320.765210.977320.956340.964510.90911size of longer expression50%60%70%80%90%100%0%3%10%30%100%accuracy across all binsamount of training data used (nonlinear scale)50d treeid5650d treerntn50d treelstm50d lstmtering of text (deft) program under air force research laboratory (afrl) contract no. fa8750-
13-2-0040, the national science foundation under grant no. iis 1159679, and the department of
the navy, of   ce of naval research, under grant no. n00014-13-1-0287. any opinions,    ndings,
and conclusions or recommendations expressed in this material are those of the authors and do not
necessarily re   ect the views of google, bloomberg l.p., darpa, afrl, nsf, onr, or the us
government.

references
[1] ilya sutskever, oriol vinyals, and quoc v. le. sequence to sequence learning with neural

networks. in proc. nips, 2014.

[2] chris dyer, miguel ballesteros, wang ling, austin matthews, and noah a. smith. transition-

based id33 with stack long short-term memory. in proc. acl, 2015.

[3] kai sheng tai, richard socher, and christopher d. manning. improved semantic representa-

tions from tree-structured id137. in proc. acl, 2015.

[4] jeffrey l. elman. finding structure in time. cognitive science, 14(2), 1990.
[5] christoph goller and andreas kuchler. learning task-dependent distributed representations
in proc. ieee international conference on neural

by id26 through structure.
networks, 1996.

[6] richard socher, jeffrey pennington, eric h. huang, andrew y. ng, and christopher d. man-
ning. semi-supervised recursive autoencoders for predicting sentiment distributions. in proc.
emnlp, 2011.

[7] minh-thang luong li, jiwei, dan jurafsky, and eudard hovy. when are tree structures nec-

essary for deep learning of representations? proc. emnlp, 2015.

[8] oriol vinyals, lukasz kaiser, terry koo, slav petrov, ilya sutskever, and geoffrey hinton.

grammar as a foreign language. in proc. nips, 2015.

[9] andrej karpathy, justin johnson, and fei-fei li. visualizing and understanding recurrent

networks. arxiv:1506.02078, 2015.

[10] samuel r. bowman, christopher potts, and christopher d. manning. recursive neural net-
works can learn logical semantics. in proc. of the 3rd workshop on continuous vector space
models and their compositionality, 2015.

[11] bill maccartney and christopher d. manning. an extended model of natural logic. in proc.

of the eighth international conference on computational semantics, 2009.

[12] danqi chen, richard socher, christopher d. manning, and andrew y. ng. learning new facts
from knowledge bases with neural tensor networks and semantic word vectors. in proc. iclr,
2013.

[13] dzmitry bahdanau, kyunghyun cho, and yoshua bengio. id4 by

jointly learning to align and translate. in proc. iclr, 2015.

[14] sepp hochreiter and j  urgen schmidhuber. long short-term memory. neural computation, 9

(8), 1997.

[15] wojciech zaremba, ilya sutskever, and oriol vinyals. recurrent neural network id173.

in proc. iclr, 2015.

[16] matthew d. zeiler. adadelta: an adaptive learning rate method. arxiv:1212.5701, 2012.

6

