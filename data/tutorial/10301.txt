document embedding with paragraph vectors

andrew m. dai

google

christopher olah

google

adai@google.com

colah@google.com

quoc v. le

google

qvl@google.com

5
1
0
2

 
l
u
j
 

9
2

 
 
]
l
c
.
s
c
[
 
 

1
v
8
9
9
7
0

.

7
0
5
1
:
v
i
x
r
a

abstract

paragraph vectors has been recently proposed as an unsupervised method for
learning distributed representations for pieces of texts. in their work, the authors
showed that the method can learn an embedding of movie review texts which can
be leveraged for id31. that proof of concept, while encouraging, was
rather narrow. here we consider tasks other than id31, provide a more
thorough comparison of paragraph vectors to other document modelling algorithms
such as id44, and evaluate performance of the method as
we vary the dimensionality of the learned representation. we benchmarked the
models on two document similarity data sets, one from wikipedia, one from arxiv.
we observe that the paragraph vector method performs signi   cantly better than
other methods, and propose a simple improvement to enhance embedding quality.
somewhat surprisingly, we also show that much like id27s, vector
operations on paragraph vectors can perform useful semantic results.

1

introduction

central to many language understanding problems is the question of id99: how
to capture the essential meaning of a document in a machine-understandable format (or    represen-
tation   ). despite much work going on in this area, the most established format is perhaps the bag
of words (or bag of id165) representations [2]. id44 (lda) [1] is another
widely adopted representation.
a recent paradigm in machine intelligence is to use a distributed representation for words [4]
and documents [3]. the interesting part is that even though these representations are less human-
interpretable than previous representations, they seem to work well in practice. in particular, le and
mikolov [3] show that their method, paragraph vectors, capture many document semantics in dense
vectors and that they can be used in classifying movie reviews or retrieving web pages.
despite their success, little is known about how well the model works compared to bag-of-words
or lda for other unsupervised applications and how sensitive the model is when we change the
hyperparameters.
in this paper, we make an attempt to compare paragraph vectors with other baselines in two tasks
that have signi   cant practical implications. first, we benchmark paragraph vectors on the task of
wikipedia browsing: given a wikipedia article, what are the nearest articles that the audience should
browse next. we also test paragraph vectors on the task of    nding related articles on arxiv. in both
of these tasks, we    nd that paragraph vectors allow for    nding documents of interest via simple and
intuitive vector operations. for example, we can    nd the japanese equivalence of    lady gaga.   .
the goal of the paper is beyond benchmarking: the positive results on wikipedia and arxiv datasets
con   rm that having good representations for texts can be powerful when it comes to language
understanding. the success in these tasks shows that it is possible to use paragraph vectors for local
and non-local browsing of large corpora.

1

we also show a simple yet effective trick to improve paragraph vector. in particular, we observe that
by jointly training id27s, as in the skip gram model, the quality of the paragraph vectors
is improved.

2 model

the paragraph vector model is    rst proposed in [3]. the model inserts a memory vector to the
standard language model which aims at capturing the topics of the document. the authors named this
model    distributed memory   :

figure 1: the distributed memory model of paragraph vector for an input sentence.

as suggested by the    gure above, the paragraph vector is concatenated or averaged with local context
word vectors to predict the next word. the prediction task changes the word vectors and the paragraph
vector.
the paragraph vector can be further simpli   ed when we use no local context in the prediction task.
we can arrive at the following    distributed bag of words    model:

figure 2: the distributed id159 of paragraph vector.

at id136 time, the parameters of the classi   er and the word vectors are not needed and backpropa-
gation is used to tune the paragraph vectors.
as the distributed id159 is more ef   cient, the experiments in this paper focuses on
this implementation of the paragraph vector. in the following sections, we will explore the use of
paragraph vectors in different applications in document understanding.

3 experiments

we conducted experiments with two different publicly available corpora: a corpus from the repository
of electronic preprints (arxiv), and a corpus from the online encyclopaedia (wikipedia).
in each case, all words were lower-cased before the datasets were used. we also jointly trained word
embeddings with the paragraph vectors since preliminary experiments showed that this can improve

2

the quality of the paragraph vectors. preliminary results also showed that training with both unigrams
and bigrams does not improve the quality of the    nal vectors. we present a range of qualitative
and quantitative results. we give some examples of nearest neighbours to some wikipedia articles
and arxiv papers as well as a visualisation of the space of wikipedia articles. we also show some
examples of nearest neighbours after performing vector operations.
for the quantitative evaluation, we attempt to measure how well paragraph vectors represent semantic
similarity of related articles. we do this by constructing (both automatically and by hand) triplets,
where each triplet consists of a pair of items that are close to each other and one item that is unrelated.
for the publicly available corpora we trained paragraph vectors over at least 10 epochs of the data and
use a hierarchical softmax constructed as a huffman tree as the classi   er. we use cosine similarity
as the metric. we also applied lda with id150 and 500 iterations with varying numbers
of topics. for lda, we set    to 0.1 and used values of    between 0.01 and 0.000001. we used the
posterior topic proportions for each paper with hellinger distance to compute the similarity between
pairs of documents. for completeness, we also include the results of averaging the id27s
for each word in a paper and using that as the paragraph vector. finally, we consider the classical
id159 where each word is represented as a one-hot vector weighted by tf-idf and the
document is represented by that vector, with comparisons done using cosine similarity.

3.1 performance of paragraph vectors on wikipedia entries

we extracted the main body text of 4,490,000 wikipedia articles from the english site. we removed
all links and applied a frequency cutoff to obtain a vocabulary of 915,715 words. we trained paragraph
vectors on these wikipedia articles and visualized them in figure 3 using id167 [5]. the visualization
con   rms that articles having the same category are grouped together. there is a wide range of sport
descriptions on wikipedia, which explains why the sports are less concentrated.

figure 3: visualization of wikipedia paragraph vectors using id167.

we also qualitatively look at the nearest neighbours of wikipedia articles and compare paragraph
vectors and lda. for example, the nearest neighbours for the wikipedia article    machine learning   
are shown in table 1. we    nd that overall paragraph vectors have better nearest neighbours than
lda.

3

table 1: nearest neighbours to    machine learning.    bold face texts are articles we found unrelated to
   machine learning.    we use hellinger distance for lda and cosine distance for paragraph vectors as
they work the best for each model.

lda
arti   cial neural network
predictive analytics
id170
mathematical geophysics
supervised learning
constrained conditional model
sensitivity analysis
sxml
feature scaling
boosting (machine learning)
prior id203
curse of dimensionality
scienti   c evidence
online machine learning
id165
cluster analysis
id84
functional decomposition
id110

paragraph vectors
arti   cial neural network
types of arti   cial neural networks
unsupervised learning
id171
predictive analytics
pattern recognition
statistical classi   cation
id170
training set
meta learning (computer science)
kernel method
supervised learning
generalization error
over   tting
id72
generative model
computational learning theory
inductive bias
semi-supervised learning

we can perform vector operations on paragraph vectors for local and non-local browsing of wikipedia.
in table 2a and table 2b, we show results of two experiments. the    rst experiment is to    nd related
articles to    lady gaga.    the second experiment is to    nd the japanese equivalence of    lady gaga.   
this can be achieved by vector operations: pv(   lady gaga   ) - wv(   american   ) + wv(   japanese   )
where pv is paragraph vectors and wv is word vectors. both sets of results show that paragraph
vectors can achieve the same kind of analogies like word vectors [4].

table 2: wikipedia nearest neighbours

(a) wikipedia nearest neighbours to    lady
gaga    using paragraph vectors. all articles
are relevant.

(b) wikipedia nearest neighbours to    lady
gaga    -    american    +    japanese    using para-
graph vectors. note that ayumi hamasaki is
one of the most famous singers, and one of the
best selling artists in japan. she also has an
album called    poker face    in 1998.

article

cosine

similarity

article

cosine

similarity

christina aguilera
beyonce
madonna (entertainer)
artpop
britney spears
cyndi lauper
rihanna
pink (singer)
born this way
the monster ball tour

0.674
0.645
0.643
0.640
0.640
0.632
0.631
0.628
0.627
0.620

ayumi hamasaki
shoko nakagawa
izumi sakai
urbangarde
ringo sheena
toshiaki kasuga
chihiro onitsuka
namie amuro
yakuza (video game)
nozomi sasaki (model)

0.539
0.531
0.512
0.505
0.503
0.492
0.487
0.485
0.485
0.485

4

to quantitatively compare these methods, we constructed two datasets for triplet evaluation. the    rst
consists of 172 triplets of articles we knew were related because of our domain knowledge. some
examples are:    deep learning    is closer to    machine learning    than    computer network    or    google   
is closer to    facebook    than    walmart    etc. some examples are hard and probably require some
deep understanding of the content such as    san diego    is closer to    los angeles    than    san jose.   
the second dataset consists of 19,876 triplets in which two articles are closer because they are listed
in the same category by wikipedia, and the last article is not in the same category, but may be in a
sibling category. for example, the articles for    barack obama    are closer to    britney spears    than
   china.    these triplets are generated randomly.1
we will benchmark document embedding methods, such as lda, bag of words, paragraph vector, to
see how well these models capture the semantic of the documents. the results are reported in table 3
and table 4. for each of the methods, we also vary the number of embedding dimensions.

figure 4: results of experiments on the hand-built wikipedia triplet dataset.

table 3: performances of different methods on hand-built triplets of wikipedia articles on the best
performing dimensionality.

model

embedding

dimensions/topics

accuracy

paragraph vectors
lda
averaged id27s
bag of words

10000
5000
3000

93.0%
82%
84.9%
86.0%

from the results in table 3 and 4, it can be seen that paragraph vectors perform better than lda. we
also see a peak in paragraph vector performance at 10,000 dimensions. both paragraph vectors and
averaging id27s perform better than the lda model. for lda, we found that tf-idf
weighting of words and their inferred topic allocations did not affect the performance. from these
results, we can also see that joint training of word vectors improves the    nal quality of the paragraph
vectors.

1the datasets are available at http://cs.stanford.edu/  quocle/triplets-data.tar.gz

5

llll657075808590100100010000dimensionalityaccuracymodellavg. id27sldapvpv w/o word trainingfigure 5: results of experiments on the generated wikipedia triplet dataset.

table 4: performances of different methods on dataset with generated wikipedia triplets on the best
performing dimensionality.

model

embedding

dimensions/topics

accuracy

paragraph vectors
lda
averaged id27s
bag of words

10000
5000
3000

78.8%
67.7%
74%
78.3%

3.2 performance of paragraph vectors on arxiv articles

we extracted text from the pdf versions of 886,000 full arxiv papers. in each case we only used the
latest revision available. we applied a minimum frequency cutoff to the vocabulary so that our    nal
vocabulary was 969,894 words.
we performed experiments to    nd related articles using paragraph vectors. in table 5 and table 6,
we show the nearest neighbours of the original paragraph vector paper    distributed representations
of sentences and documents    and the current paper. in table 7, we want to    nd the bayesian
equivalence of the paragraph vector paper. this can be achieved by vector operations: pv(   distributed
representations of sentences and documents   ) - wv(   neural   ) + wv(   bayesian   ) where pv are
paragraph vectors and wv are word vectors learned during the training of paragraph vectors. the
results suggest that paragraph vector works well in these two tasks.
to measure the performance of different models on this task, we picked pairs of papers that had
at least one shared subject, the unrelated paper was chosen at random from papers with no shared
subjects with the    rst paper. we produced a dataset of 20,000 triplets by this method.
from the results in table 8, it can be seen that paragraph vectors perform on par than the best
performing number of topics for lda. paragraph vectors are also less sensitive to differences in
embedding size than lda is to the number of topics. we also see a peak in paragraph vector
performance at 100 dimensions. both models perform better than the vector space model. for
lda, we found that tf-idf weighting of words and their inferred topic allocations did not affect
performance.

6

ll657075100100010000dimensionalityaccuracymodellavg. id27sldapvpv w/o word trainingtable 5: arxiv nearest neighbours to    distributed representations of sentences and documents   
using paragraph vectors.

title

evaluating neural word representations in tensor-based compositional settings
polyglot: distributed word representations for multilingual nlp
lexicon infused phrase embeddings for named entity resolution
a convolutional neural network for modelling sentences
distributed representations of words and phrases and their compositionality
convolutional neural networks for sentence classi   cation
siid113x-999: evaluating semantic models with (genuine) similarity estimation
exploiting similarities among languages for machine translation
ef   cient estimation of word representations in vector space
multilingual distributed representations without word alignment

cosine

similarity

0.771
0.764
0.757
0.747
0.740
0.735
0.735
0.731
0.727
0.721

table 6: arxiv nearest neighbours to the current paper using paragraph vectors.

title

distributed representations of sentences and documents
ef   cient estimation of word representations in vector space
thumbs up? sentiment classi   cation using machine learning techniques
distributed representations of words and phrases and their compositionality
knet: a general framework for learning id27 using

morphological knowledge

japanese-spanish thesaurus construction using english as a pivot
multilingual distributed representations without word alignment
catching the drift: probabilistic content models, with applications

to generation and summarization

exploiting similarities among languages for machine translation
a survey on information retrieval, text categorization, and web crawling

cosine

similarity

0.681
0.680
0.642
0.624
0.622

0.614
0.614
0.613

0.612
0.610

table 7: arxiv nearest neighbours to    distributed representations of sentences and documents    -
   neural    +    bayesian   . i.e., the bayesian equivalence of the paragraph vector paper.

title

content modeling using latent permutations
siid113x-999: evaluating semantic models with (genuine) similarity estimation
probabilistic topic and syntax modeling with part-of-speech lda
evaluating neural word representations in tensor-based compositional settings
syntactic topic models
training restricted id82s on word observations
discrete component analysis
resolving lexical ambiguity in tensor regression models of meaning
measuring political sentiment on twitter: factor-optimal design for

multinomial inverse regression

scalable probabilistic entity-id96

cosine

similarity

0.629
0.611
0.579
0.572
0.548
0.548
0.547
0.546
0.544

0.541

7

figure 6: results of experiments on the arxiv triplet dataset.

table 8: performances of different methods at the best dimensionality on the arxiv article triplets.

model
paragraph vectors
lda
averaged id27s
bag of words

embedding dimensions/topics accuracy

100
100
300

85.0%
85.0%
81.1%
80.4%

4 discussion

we described a new set of results on paragraph vectors showing they can effectively be used for
measuring semantic similarity between long pieces of texts. our experiments show that paragraph
vectors are superior to lda for measuring semantic similarity on wikipedia articles across all sizes
of paragraph vectors. paragraph vectors also perform on par with lda   s best performing number of
topics on arxiv papers and perform consistently relative to the embedding size. also surprisingly,
vector operations can be performed on them similarly to word vectors. this can provide interesting
new techniques for a wide range of applications: local and nonlocal corpus navigation, dataset
exploration, book recommendation and reviewer allocation.

references
[1] d. blei, a. y. ng, and m. i. jordan. id44. journal of machine learning

research, 2003.

[2] z. harris. distributional structure. word, 1954.
[3] q. v. le and t. mikolov. distributed representations of sentences and documents. in international

conference on machine learning, 2014.

[4] t. mikolov, k. chen, g. corrado, and j. dean. ef   cient estimation of word representations in

vector space. arxiv preprint arxiv:1301.3781, 2013.

[5] l. j. p. van der maaten and g. e. hinton. visualizing high-dimensional data using id167. journal

of machine learning research, 2008.

8

l7779818385100100010000dimensionalityaccuracymodellavg. id27sldapv