improved id36 with

feature-rich compositional embedding models

matthew r. gorid113y1    and mo yu2    and mark dredze1

1human language technology center of excellence

center for language and speech processing

johns hopkins university, baltimore, md, 21218

2machine intelligence and translation lab

harbin institute of technology, harbin, china

5
1
0
2

 

p
e
s
5
1

 

 
 
]
l
c
.
s
c
[
 
 

3
v
9
1
4
2
0

.

5
0
5
1
:
v
i
x
r
a

gflfof@gmail.com, {mrg, mdredze}@cs.jhu.edu

abstract

compositional embedding models build
a representation (or embedding) for a
linguistic structure based on its compo-
nent id27s. we propose
a feature-rich compositional embedding
model (fcm) for id36 that
is expressive, generalizes to new domains,
and is easy-to-implement. the key idea
is to combine both (unlexicalized) hand-
crafted features with learned word em-
beddings. the model is able to directly
tackle the dif   culties met by traditional
compositional embeddings models, such
as handling arbitrary types of sentence an-
notations and utilizing global information
for composition. we test the proposed
model on two id36 tasks,
and demonstrate that our model outper-
forms both previous compositional models
and traditional feature rich models on the
ace 2005 id36 task, and the
semeval 2010 relation classi   cation task.
the combination of our model and a log-
linear classi   er with hand-crafted features
gives state-of-the-art results. we made our
implementation available for general use1.

introduction

1
two common nlp feature types are lexical
properties of words and unlexicalized linguis-
tic/structural interactions between words. prior
work on id36 has extensively stud-
ied how to design such features by combining dis-
crete lexical properties (e.g. the identity of a word,

      gorid113y and yu contributed equally.
1https://github.com/mgorid113y/pacaya

its lemma, its morphological features) with as-
pects of a word   s linguistic context (e.g. whether it
lies between two entities or on a dependency path
between them). while these help learning, they
make generalization to unseen words dif   cult. an
alternative approach to capturing lexical informa-
tion relies on continuous id27s2 as
representative of words but generalizable to new
words. embedding features have improved many
tasks, including ner, chunking, dependency pars-
ing, id14, and relation extrac-
tion (miller et al., 2004; turian et al., 2010; koo
et al., 2008; roth and woodsend, 2014; sun et
al., 2011; plank and moschitti, 2013; nguyen and
grishman, 2014). embeddings can capture lexi-
cal information, but alone they are insuf   cient: in
state-of-the-art systems, they are used alongside
features of the broader linguistic context.

in this paper, we introduce a compositional
model that combines unlexicalized linguistic con-
text and id27s for id36,
a task in which contextual feature construction
plays a major role in generalizing to unseen data.
our model allows for the composition of embed-
dings with arbitrary linguistic structure, as ex-
pressed by hand crafted features.
in the follow-
ing sections, we begin with a precise construction
of compositional embeddings using word embed-
dings in conjunction with unlexicalized features.
various feature sets used in prior work (turian et
al., 2010; nguyen and grishman, 2014; hermann
et al., 2014; roth and woodsend, 2014) are cap-

2such embeddings have a long history in nlp,

in-
cluding term-document frequency matrices and their low-
dimensional counterparts obtained by id202 tools
(lsa, pca, cca, nnmf), brown clusters, random projec-
tions and vector space models. recently, neural networks /
deep learning have provided several popular methods for ob-
taining such embeddings.

class

(1) art(m1,m2)
(2) part-whole(m1,m2)
(3) physical(m2,m1)

m1
a man

the southern suburbs

the united states

m2

a taxicab
baghdad
284 people

sentence snippet

a man driving what appeared to be a taxicab
direction of the southern suburbs of baghdad

in the united states , 284 people died

table 1: examples from ace 2005. in (1) the word    driving    is a strong indicator of the relation art3 between m1 and m2.
a feature that depends on the embedding for this context word could generalize to other lexical indicators of the same relation
(e.g.    operating   ) that don   t appear with art during training. but lexical information alone is insuf   cient; id36
requires the identi   cation of lexical roles: where a word appears structurally in the sentence. in (2), the word    of    between
   suburbs    and    baghdad    suggests that the    rst entity is part of the second, yet the earlier occurrence after    direction    is of no
signi   cance to the relation. even    ner information can be expressed by a word   s role on the dependency path between entities.
in (3) we can distinguish the word    died    from other irrelevant words that don   t appear between the entities.

tured as special cases of this construction. adding
these compositional embeddings directly to a stan-
dard log-linear model yields a special case of our
full model. we then treat the id27s
as parameters giving rise to our powerful, ef   cient,
and easy-to-implement log-bilinear model. the
model capitalizes on arbitrary types of linguistic
annotations by better utilizing features associated
with substructures of those annotations, including
global information. we choose features to pro-
mote different properties and to distinguish differ-
ent functions of the input words.

the full model involves three stages. first, it
decomposes the annotated sentence into substruc-
tures (i.e.
a word and associated annotations).
second, it extracts features for each substructure
(word), and combines them with the word   s em-
bedding to form a substructure embedding. third,
we sum over substructure embeddings to form a
composed annotated sentence embedding, which
is used by a    nal softmax layer to predict the out-
put label (relation).

the result is a state-of-the-art relation extractor
for unseen domains from ace 2005 (walker et al.,
2006) and the relation classi   cation dataset from
semeval-2010 task 8 (hendrickx et al., 2010).
contributions this paper makes several contri-
butions, including:

1. we introduce the fcm, a new compositional

embedding model for id36.

2. we obtain the best reported results on ace-
2005 for coarse-grained id36 in
the cross-domain setting, by combining fcm
with a log-linear model.

3. we obtain results on on semeval-2010 task
8 competitive with the best reported results.
note that other work has already been published
that builds on the fcm, such as hashimoto et al.
(2015), nguyen and grishman (2015), dos santos
3in ace 2005, art refers to a relation between a person
and an artifact; such as a user, owner, inventor, or manufac-
turer relationship

et al. (2015), yu and dredze (2015) and yu et al.
(2015). additionally, we have extended fcm to
incorporate a low-rank embedding of the features
(yu et al., 2015), which focuses on    ne-grained
id36 for ace and ere. this paper
obtains better results than the low-rank extension
on ace coarse-grained id36.

2 id36

in id36 we are given a sentence as in-
put with the goal of identifying, for all pairs of en-
tity mentions, what relation exists between them,
if any. for each pair of entity mentions in a sen-
tence s, we construct an instance (y, x), where
x = (m1, m2, s, a). s = {w1, w2, ..., wn} is
a sentence of length n that expresses a relation
of type y between two entity mentions m1 and
m2, where m1 and m2 are sequences of words in
s. a is the associated annotations of sentence s,
such as part-of-speech tags, a dependency parse,
and named entities. we consider directed rela-
tions: for a relation type rel, y=rel(m1, m2)
and y(cid:48)=rel(m2, m1) are different relations. ta-
ble 1 shows ace 2005 relations, and has a strong
label bias towards negative examples. we also
consider the task of relation classi   cation (se-
meval), where the number of negative examples
is arti   cially reduced.

embedding models id27s and
compositional embedding models have been suc-
cessfully applied to a range of nlp tasks, however
the applications of these embedding models to re-
lation extraction are still limited. prior work on
relation classi   cation (e.g. semeval 2010 task 8)
has focused on short sentences with at most one
relation per sentence (socher et al., 2012; zeng
et al., 2014). for id36, where neg-
ative examples abound, prior work has assumed
that only the named entity boundaries and not
their types were available (plank and moschitti,
2013; nguyen et al., 2015). other work has as-

sumed that the order of two entities in a relation
are given while the relation type itself is unknown
(nguyen and grishman, 2014; nguyen and grish-
man, 2015). the standard id36 task,
as adopted by ace 2005 (walker et al., 2006),
uses long sentences containing multiple named en-
tities with known types4 and unknown relation di-
rections. we are the    rst to apply neural language
model embeddings to this task.

motivation and examples whether a word is
indicative of a relation depends on multiple prop-
erties, which may relate to its context within the
sentence. for example, whether the word is in-
between the entities, on the dependency path be-
tween them, or to their left or right may provide
additional complementary information.
illustra-
tive examples are given in table 1 and provide
the motivation for our model. in the next section,
we will show how we develop informative repre-
sentations capturing both the semantic information
in id27s and the contextual informa-
tion expressing a word   s role relative to the entity
mentions. we are the    rst to incorporate all of
this information at once. the closest work is that
of nguyen and grishman (2014), who use a log-
linear model for id36 with embed-
dings as features for only the entity heads. such
embedding features are insensitive to the broader
contextual information and, as we show, are not
suf   cient to elicit the word   s role in a relation.

3 a feature-rich compositional

embedding model for relations

we propose a general framework to construct an
embedding of a sentence with annotations on its
component words. while we focus on the rela-
tion extraction task, the framework applies to any
task that bene   ts from both embeddings and typi-
cal hand-engineered lexical features.

3.1 combining features with embeddings
we begin by describing a precise method for con-
structing substructure embeddings and annotated
sentence embeddings from existing (usually un-
lexicalized) features and embeddings. note that
these embeddings can be included directly in a
log-linear model as features   doing so results in

4since the focus of this paper is id36, we
adopt the evaluation setting of prior work which uses gold
named entities to better facilitate comparison.

a special case of our full model presented in the
next subsection.

an annotated sentence is    rst decomposed into
substructures. the type of substructures can vary
by task; for id36 we consider one
substructure per word5. for each substructure in
the sentence we have a hand-crafted feature vec-
tor fwi and a dense embedding vector ewi. we
represent each substructure as the outer product
    between these two vectors to produce a matrix,
herein called a substructure embedding: hwi =
fwi     ewi. the features fwi are based on the local
context in s and annotations in a, which can in-
clude global information about the annotated sen-
tence. these features allow the model to pro-
mote different properties and to distinguish differ-
ent functions of the words. feature engineering
can be task speci   c, as relevant annotations can
change with regards to each task.
in this work
we utilize unlexicalized binary features common
in id36. figure 1 depicts the con-
struction of a sentence   s substructure embeddings.
we further sum over the substructure embed-
dings to form an annotated sentence embedding:

ex =

n(cid:88)i=1

fwi     ewi

(1)

when both the hand-crafted features and word em-
beddings are treated as inputs, as has previously
been the case in id36, this anno-
tated sentence embedding can be used directly as
the features of a log-linear model.
in fact, we
   nd that the feature sets used in prior work for
many other nlp tasks are special cases of this
simple construction (turian et al., 2010; nguyen
and grishman, 2014; hermann et al., 2014; roth
and woodsend, 2014). this highlights an im-
portant connection: when the id27s
are constant, our constructions of substructure and
annotated sentence embeddings are just speci   c
forms of polynomial (speci   cally quadratic) fea-
ture combination   hence their commonality in the
literature. our experimental results suggest that
such a construction is more powerful than directly
including embeddings into the model.

3.2 the log-bilinear model
our full log-bilinear model    rst forms the sub-
structure and annotated sentence embeddings from

5we use words as substructures for id36, but

use the general terminology to maintain model generality.

figure 1: example construction of substructure embeddings. each substructure is a word wi in s, augmented by the target
entity information and related information from annotation a (e.g. a dependency tree). we show the factorization of the
annotated sentence into substructures (left), the concatenation of the substructure embeddings for the sentence (middle), and a
single substructure embedding from that concatenation (right). the annotated sentence embedding (not shown) would be the
sum of the substructure embeddings, as opposed to their concatenation.

the previous subsection. the model uses its pa-
rameters to score the annotated sentence embed-
ding and uses a softmax to produce an output la-
bel. we call the entire model the feature-rich
compositional embedding model (fcm).

our task is to determine the label y (relation)
given the instance x = (m1, m2, s, a). we for-
mulate this as a id203.

exp ((cid:80)n

z(x)

p (y|x; t, e) =

i=1 ty (cid:12) (fwi     ewi))
(2)
where (cid:12) is the    matrix dot product    or frobe-
nious inner product of the two matrices. the
normalizing constant which sums over all possi-
ble output labels y(cid:48)     l is given by z(x) =
i=1 ty(cid:48) (cid:12) (fwi     ewi)(cid:1). the pa-
(cid:80)y(cid:48)
rameters of the model are the id27s
e for each word type and a list of weight matrix
t = [ty]y   l which is used to score each label
y. the model is log-bilinear 6 (i.e. log-quadratic)
since we recover a log-linear model by    xing ei-
ther e or t . we study both the full log-bilinear and
the log-linear model obtained by    xing the word
embeddings.

   l exp(cid:0)(cid:80)n

3.3 discussion of the model
substructure embeddings similar words (i.e.
those with similar embeddings) with similar func-
tions in the sentence (i.e. those with similar fea-
tures) will have similar matrix representations. to
understand our selection of the outer product, con-
sider the example in fig. 1. the word    driving   
can indicate the art relation if it appears on the

6other popular log-bilinear models are the log-bilinear
language models (mnih and hinton, 2007; mikolov et al.,
2013).

dependency path between m1 and m2. suppose
the third feature in fwi indicates this on-path
feature. our model can now learn parameters
which give the third row a high weight for the
art label. other words with embeddings similar
to    driving    that appear on the dependency path
between the mentions will similarly receive high
weight for the art label. on the other hand, if the
embedding is similar but is not on the dependency
path, it will have 0 weight. thus, our model gen-
eralizes its model parameters across words with
similar embeddings only when they share similar
functions in the sentence.
smoothed lexical features another intuition
about the selection of outer product is that it is
actually a smoothed version of traditional lexical
features used in classical nlp systems. consider
a lexical feature f = u     w, which is a conjunc-
tion (logic-and) between non-lexical property u
and lexical part (word) w. if we represent w as
a one-hot vector, then the outer product exactly re-
covers the original feature f. then if we replace
the one-hot representation with its word embed-
ding, we get the current form of our fcm. there-
fore, our model can be viewed as a smoothed ver-
sion of lexical features, which keeps the expres-
sive strength, and uses embeddings to generalize
to low frequency features.
time complexity id136 in fcm is much
faster than both id98s (collobert et al., 2011) and
id56s (socher et al., 2013b; bordes et al., 2012).
fcm requires o(snd) products on average with
sparse features, where s is the average number of
per-word non-zero feature values, n is the length
of the sentence, and d is the dimension of word
embedding. in contrast, id98s and id56s usually

054055056057058059060061062063064065066067068069070071072073074075076077078079080081082083084085086087088089090091092093094095096097098099100101102103104105106107basedonaboveideas,weachieveageneralmodelandcaneasilyapplytomodeltoannlptaskwithouttheneedfordesigningmodelstructuresorselectingfeaturesfromscratch.speci   cally,ifwedenoteainstanceas(y,s),wheresisanarbitrarylanguagestructureandyisthelabelforthestructure.thenwedecomposethestructuretosomefactorsfollowings={f}.foreachfactorf,thereisalistofmassociatedfeaturesg=g1,g2,...,gm,andalistoftassociatedwordswf,1,wf,2,...,wf,t2f.herewesupposethateachfactorhasthesamenumberofwords,andthereisatransformationfromthewordsinafactortoahiddenlayerasfollows:hf=     ewf,1:ewf,2:...:ewf,t     w ,(1)whereewiisthewordembeddingforwordwi.supposethewordembeddingshavededimensionsandthehiddenlayerhasdhdimensions.herew=[w1w2...wt],eachwjisade   dhmatrix,isatransformationfromtheconcatenationofwordembeddingstotheinputsofthehiddenlayer.thenthesigmoidtransformation willbeusedtogetthevaluesofhiddenlayerfromitsinputs.(cid:13)(cid:5)(cid:6)(cid:9)(cid:13)(cid:18)(cid:1)(cid:10)(cid:9)(cid:5)(cid:19)(cid:20)(cid:17)(cid:9)(cid:18)(cid:1)(cid:4)(cid:14)(cid:6)(cid:9)(cid:8)(cid:8)(cid:12)(cid:15)(cid:11)(cid:18)(cid:1)(cid:1)(cid:16)(cid:10)(cid:1)(cid:21)(cid:16)(cid:17)(cid:8)(cid:18)(cid:1)(cid:12)(cid:15)(cid:1)(cid:5)(cid:1)(cid:10)(cid:5)(cid:7)(cid:19)(cid:16)(cid:17)(cid:1)devmrrtestmrrmodelfine-tuningsupervison1,00010,000100,0001,00010,000100,000sum--46.9535.2930.6952.6341.1937.32yppdb50.8136.8132.9257.2345.0141.23id56(d=50)yppdb45.6730.8627.0554.8439.2535.49id56(d=200)yppdb48.9733.5031.1353.5940.5038.57fctnppdb47.5335.5831.3154.3341.9639.10yppdb51.2236.7633.5961.1146.9944.31fct-lm49.4337.4632.2253.5642.6339.44ylm+ppdb53.8237.4834.4365.4749.4445.65jointlm+ppdb56.5341.4136.4568.5251.6546.53table9:performanceonthesemanticsimilaritytaskwithppdbdata.@`@t=nxi=1@`@r   fwi   ewi,(13)@`@ew=nxi=1i[wi=w]t @`@r fwi(14)t:nxi=1fi   ewi representation(15)t  nxi=1fi   ewi (16)t  nxi=1fi   ewi =nxi=1t fi     ewiminxi=1fi   ewi    (17)    (18)+(19)(wi,fi=f(wi,s))(20)wi2s(21)t (   fwi   ewi),(22)   (23)   2,0 0(24)   2,1,   2,2,   2,3 0(25)054055056057058059060061062063064065066067068069070071072073074075076077078079080081082083084085086087088089090091092093094095096097098099100101102103104105106107basedonaboveideas,weachieveageneralmodelandcaneasilyapplytomodeltoannlptaskwithouttheneedfordesigningmodelstructuresorselectingfeaturesfromscratch.speci   cally,ifwedenoteainstanceas(y,s),wheresisanarbitrarylanguagestructureandyisthelabelforthestructure.thenwedecomposethestructuretosomefactorsfollowings={f}.foreachfactorf,thereisalistofmassociatedfeaturesg=g1,g2,...,gm,andalistoftassociatedwordswf,1,wf,2,...,wf,t2f.herewesupposethateachfactorhasthesamenumberofwords,andthereisatransformationfromthewordsinafactortoahiddenlayerasfollows:hf=     ewf,1:ewf,2:...:ewf,t     w ,(1)whereewiisthewordembeddingforwordwi.supposethewordembeddingshavededimensionsandthehiddenlayerhasdhdimensions.herew=[w1w2...wt],eachwjisade   dhmatrix,isatransformationfromtheconcatenationofwordembeddingstotheinputsofthehiddenlayer.thenthesigmoidtransformation willbeusedtogetthevaluesofhiddenlayerfromitsinputs.figure1:tensorrepresentationofthefctmodel.(a)representationofaninputsentence.(b)representationfortheparameterspace.basedonabovenotations,wecanrepresenteachfactorastheouterproductbetweenthefeaturevectorandthehiddenlayeroftransformedembeddinggf   hf.theweuseatensort=l   e   fasinfigrure1(b)totransformthisinputmatrixtothelabels.herelisthesetoflabels,ereferstoalldimensionsofhiddenlayer(|e|=200)andfisthesetoffeatures.inordertopredicttheconditionalid203ofalabelygiventhestructures,wehavep(y|s;t)=exp{s(y,s;t)}py02lexp{s(y0,s;t)},(2)wheres(y,s;t)isthescoreoflabelycomputedwithourmodel.sincewedecomposethestruc-turestofactors,eachfactorfi2swillcontributetothescorebasedonthemodelparameters.speci   cally,eachlabelycorrespondstoasliceofthetensorty,whichisamatrix   (y,  ,  ).theneachfactorfiwillcontributeascores(y,fi)=ty gf hf,(3)where correspondtotensorproduct,whileinthecaseofeq.(3),ithastheequivalentform:ty gf hf=ty (gf   hf)=(   (y,  ,  )  gf)thf.inthisway,thetargetscoreoflabelygivenaninstancesandparametertensortcanbewrittenas:s(y,s;t)=nxi=1s(y,fi;t)=nxi=1ty gfi hfi.(4)thefcmmodelonlyperformslineartransformationsoneachviewofthetensor,makingthemodelef   cientandeasytoimplement.learninginordertotraintheparametersweoptimizethefollowingcross-id178objective:`(d;t,w)=x(y,s)2dlogp(y|s;t,w)wheredisthesetofalltrainingdata.weusedadagrad[9]tooptimizeaboveobjective.thereforeweareperformingstochastictraining;andforeachin-stance(y,s)thelossfunction`=`(y,s;t,w)=logp(y|s;t,w).then2162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215classi   erfeaturesf1id166[]pos,pre   xes,morphological,id138,dependencyparse,82.2(bestinsemeval2010)levinclassed,probank,framenet,noid113x-plus,googleid165,paraphrases,textrunnerid56wordembeddings,syntacticparse74.8wordembeddings,syntacticparse,pos,ner,id13877.6mvid56wordembeddings,syntacticparse79.1wordembedding,syntacticparse,pos,ner,id13882.4fcm(   xed-embedding)wordembeddings,dependencyparse,id13882.0fcm(   ne-tuning)wordembeddings,dependencyparse,id13882.3fcm+linearwordembeddings,dependencyparse,id138table2:featuresetsusedinfcm.references[1]yoshuabengio,holgerschwenk,jean-s  ebastiensen  ecal,fr  edericmorin,andjean-lucgauvain.neuralprobabilisticlanguagemodels.ininnovationsinmachinelearning,pages137   186.springer,2006.[2]ronancollobertandjasonweston.auni   edarchitecturefornaturallanguageprocessing:deepneuralnetworkswithmultitasklearning.ininternationalconferenceonmachinelearning,pages160   167.acm,2008.[3]tomasmikolov,ilyasutskever,kaichen,gregcorrado,andjeffreydean.distributedrepresentationsofwordsandphrasesandtheircompositionality.arxivpreprintarxiv:1310.4546,2013.[4]josephturian,levratinov,andyoshuabengio.wordrepresentations:asimpleandgeneralmethodforsemi-supervisedlearning.inassociationforcomputationallinguistics,pages384   394,2010.[5]ronancollobert.deeplearningforef   cientdiscriminativeparsing.ininternationalconferenceonarti   cialintelligenceandstatistics,2011.[6]erichhuang,richardsocher,christopherdmanning,andandrewyng.improvingwordrepresen-tationsviaglobalcontextandmultiplewordprototypes.inassociationforcomputationallinguistics,pages873   882,2012.[7]richardsocher,alexperelygin,jeanwu,jasonchuang,christopherd.manning,andrewng,andchristopherpotts.recursivedeepmodelsforsemanticcompositionalityoverasentimenttreebank.inempiricalmethodsinnaturallanguageprocessing,pages1631   1642,2013.[8]karlmoritzhermann,dipanjandas,jasonweston,andkuzmanganchev.semanticframeidenti   cationwithdistributedwordrepresentations.inproceedingsofacl.associationforcomputationallinguistics,june2014.[9]johnduchi,eladhazan,andyoramsinger.adaptivesubgradientmethodsforonlinelearningandstochasticoptimization.thejournalofmachinelearningresearch,12:2121   2159,2011.[10]irishendrickx,sunamkim,zornitsakozareva,preslavnakov,diarmuid  os  eaghdha,sebastianpad  o,marcopennacchiotti,lorenzaromano,andstanszpakowicz.semeval-2010task8:multi-wayclas-si   cationofsemanticrelationsbetweenpairsofnominals.inproceedingsofthesemeval-2workshop,uppsala,sweden,2010.[11]richardsocher,brodyhuval,christopherd.manning,andandrewy.ng.semanticcompositionalitythroughrecursivematrix-vectorspaces.inproceedingsofthe2012jointconferenceonempiricalmeth-odsinnaturallanguageprocessingandcomputationalnaturallanguagelearning,pages1201   1211,jejuisland,korea,july2012.associationforcomputationallinguistics.[12]robertparker,davidgraff,junbokong,kechen,andkazuakimaeda.englishgigaword   fthedition,june.linguisticdataconsortium,ldc2011t07,2011.ewf,1ewf,2...ewf,t(9)4(cid:2)(cid:6)(cid:3)(cid:1)(cid:13)(cid:5)(cid:6)(cid:9)(cid:13)(cid:18)(cid:1)(cid:10)(cid:9)(cid:5)(cid:19)(cid:20)(cid:17)(cid:9)(cid:18)(cid:1)(cid:4)(cid:14)(cid:6)(cid:9)(cid:8)(cid:8)(cid:12)(cid:15)(cid:11)(cid:18)(cid:1)(cid:1)(cid:16)(cid:10)(cid:1)(cid:21)(cid:16)(cid:17)(cid:8)(cid:18)(cid:1)(cid:12)(cid:15)(cid:1)(cid:5)(cid:1)(cid:10)(cid:5)(cid:7)(cid:19)(cid:16)(cid:17)(cid:1)devmrrtestmrrmodelfine-tuningsupervison1,00010,000100,0001,00010,000100,000sum--46.9535.2930.6952.6341.1937.32yppdb50.8136.8132.9257.2345.0141.23id56(d=50)yppdb45.6730.8627.0554.8439.2535.49id56(d=200)yppdb48.9733.5031.1353.5940.5038.57fctnppdb47.5335.5831.3154.3341.9639.10yppdb51.2236.7633.5961.1146.9944.31fct-lm49.4337.4632.2253.5642.6339.44ylm+ppdb53.8237.4834.4365.4749.4445.65jointlm+ppdb56.5341.4136.4568.5251.6546.53table9:performanceonthesemanticsimilaritytaskwithppdbdata.@`@t=nxi=1@`@r   fwi   ewi,(13)@`@ew=nxi=1i[wi=w]t @`@r fwi(14)t:nxi=1fi   ewi representation(15)t  nxi=1fi   ewi (16)t  nxi=1fi   ewi =nxi=1t fi     ewiminxi=1fi   ewi    (17)    (18)+(19)(wi,fi=f(wi,s))(20)wi2s(21)t (   fwi   ewi),(22)   (23)   2,0 0(24)   2,1,   2,2,   2,3 0(25)054055056057058059060061062063064065066067068069070071072073074075076077078079080081082083084085086087088089090091092093094095096097098099100101102103104105106107basedonaboveideas,weachieveageneralmodelandcaneasilyapplytomodeltoannlptaskwithouttheneedfordesigningmodelstructuresorselectingfeaturesfromscratch.speci   cally,ifwedenoteainstanceas(y,s),wheresisanarbitrarylanguagestructureandyisthelabelforthestructure.thenwedecomposethestructuretosomefactorsfollowings={f}.foreachfactorf,thereisalistofmassociatedfeaturesg=g1,g2,...,gm,andalistoftassociatedwordswf,1,wf,2,...,wf,t2f.herewesupposethateachfactorhasthesamenumberofwords,andthereisatransformationfromthewordsinafactortoahiddenlayerasfollows:hf=     ewf,1:ewf,2:...:ewf,t     w ,(1)whereewiisthewordembeddingforwordwi.supposethewordembeddingshavededimensionsandthehiddenlayerhasdhdimensions.herew=[w1w2...wt],eachwjisade   dhmatrix,isatransformationfromtheconcatenationofwordembeddingstotheinputsofthehiddenlayer.thenthesigmoidtransformation willbeusedtogetthevaluesofhiddenlayerfromitsinputs.figure1:tensorrepresentationofthefctmodel.(a)representationofaninputsentence.(b)representationfortheparameterspace.basedonabovenotations,wecanrepresenteachfactorastheouterproductbetweenthefeaturevectorandthehiddenlayeroftransformedembeddinggf   hf.theweuseatensort=l   e   fasinfigrure1(b)totransformthisinputmatrixtothelabels.herelisthesetoflabels,ereferstoalldimensionsofhiddenlayer(|e|=200)andfisthesetoffeatures.inordertopredicttheconditionalid203ofalabelygiventhestructures,wehavep(y|s;t)=exp{s(y,s;t)}py02lexp{s(y0,s;t)},(2)wheres(y,s;t)isthescoreoflabelycomputedwithourmodel.sincewedecomposethestruc-turestofactors,eachfactorfi2swillcontributetothescorebasedonthemodelparameters.speci   cally,eachlabelycorrespondstoasliceofthetensorty,whichisamatrix   (y,  ,  ).theneachfactorfiwillcontributeascores(y,fi)=ty gf hf,(3)where correspondtotensorproduct,whileinthecaseofeq.(3),ithastheequivalentform:ty gf hf=ty (gf   hf)=(   (y,  ,  )  gf)thf.inthisway,thetargetscoreoflabelygivenaninstancesandparametertensortcanbewrittenas:s(y,s;t)=nxi=1s(y,fi;t)=nxi=1ty gfi hfi.(4)thefcmmodelonlyperformslineartransformationsoneachviewofthetensor,makingthemodelef   cientandeasytoimplement.learninginordertotraintheparametersweoptimizethefollowingcross-id178objective:`(d;t,w)=x(y,s)2dlogp(y|s;t,w)wheredisthesetofalltrainingdata.weusedadagrad[9]tooptimizeaboveobjective.thereforeweareperformingstochastictraining;andforeachin-stance(y,s)thelossfunction`=`(y,s;t,w)=logp(y|s;t,w).then2162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215classi   erfeaturesf1id166[]pos,pre   xes,morphological,id138,dependencyparse,82.2(bestinsemeval2010)levinclassed,probank,framenet,noid113x-plus,googleid165,paraphrases,textrunnerid56wordembeddings,syntacticparse74.8wordembeddings,syntacticparse,pos,ner,id13877.6mvid56wordembeddings,syntacticparse79.1wordembedding,syntacticparse,pos,ner,id13882.4fcm(   xed-embedding)wordembeddings,dependencyparse,id13882.0fcm(   ne-tuning)wordembeddings,dependencyparse,id13882.3fcm+linearwordembeddings,dependencyparse,id138table2:featuresetsusedinfcm.references[1]yoshuabengio,holgerschwenk,jean-s  ebastiensen  ecal,fr  edericmorin,andjean-lucgauvain.neuralprobabilisticlanguagemodels.ininnovationsinmachinelearning,pages137   186.springer,2006.[2]ronancollobertandjasonweston.auni   edarchitecturefornaturallanguageprocessing:deepneuralnetworkswithmultitasklearning.ininternationalconferenceonmachinelearning,pages160   167.acm,2008.[3]tomasmikolov,ilyasutskever,kaichen,gregcorrado,andjeffreydean.distributedrepresentationsofwordsandphrasesandtheircompositionality.arxivpreprintarxiv:1310.4546,2013.[4]josephturian,levratinov,andyoshuabengio.wordrepresentations:asimpleandgeneralmethodforsemi-supervisedlearning.inassociationforcomputationallinguistics,pages384   394,2010.[5]ronancollobert.deeplearningforef   cientdiscriminativeparsing.ininternationalconferenceonarti   cialintelligenceandstatistics,2011.[6]erichhuang,richardsocher,christopherdmanning,andandrewyng.improvingwordrepresen-tationsviaglobalcontextandmultiplewordprototypes.inassociationforcomputationallinguistics,pages873   882,2012.[7]richardsocher,alexperelygin,jeanwu,jasonchuang,christopherd.manning,andrewng,andchristopherpotts.recursivedeepmodelsforsemanticcompositionalityoverasentimenttreebank.inempiricalmethodsinnaturallanguageprocessing,pages1631   1642,2013.[8]karlmoritzhermann,dipanjandas,jasonweston,andkuzmanganchev.semanticframeidenti   cationwithdistributedwordrepresentations.inproceedingsofacl.associationforcomputationallinguistics,june2014.[9]johnduchi,eladhazan,andyoramsinger.adaptivesubgradientmethodsforonlinelearningandstochasticoptimization.thejournalofmachinelearningresearch,12:2121   2159,2011.[10]irishendrickx,sunamkim,zornitsakozareva,preslavnakov,diarmuid  os  eaghdha,sebastianpad  o,marcopennacchiotti,lorenzaromano,andstanszpakowicz.semeval-2010task8:multi-wayclas-si   cationofsemanticrelationsbetweenpairsofnominals.inproceedingsofthesemeval-2workshop,uppsala,sweden,2010.[11]richardsocher,brodyhuval,christopherd.manning,andandrewy.ng.semanticcompositionalitythroughrecursivematrix-vectorspaces.inproceedingsofthe2012jointconferenceonempiricalmeth-odsinnaturallanguageprocessingandcomputationalnaturallanguagelearning,pages1201   1211,jejuisland,korea,july2012.associationforcomputationallinguistics.[12]robertparker,davidgraff,junbokong,kechen,andkazuakimaeda.englishgigaword   fthedition,june.linguisticdataconsortium,ldc2011t07,2011.ewf,1ewf,2...ewf,t(9)4(cid:2)(cid:6)(cid:3)(cid:1)figure1:tensorrepresentationofthefctmodel.(a)representationofaninputstructure.(b)representationfortheparameterspace.basedonabovenotations,wecanrepresenteachfactorastheouterproductbetweenthefeaturevectorandthehiddenlayeroftransformedembeddinggf   hf.theweuseatensort=l   e   fasinfigrure1(b)totransformthisinputmatrixtothelabels.herelisthesetoflabels,ereferstoalldimensionsofhiddenlayer(|e|=200)andfisthesetoffeatures.inordertopredicttheconditionalid203ofalabelygiventhestructures,wehavep(y|s;t)=exp{s(y,s;t)}py02lexp{s(y0,s;t)},(2)wheres(y,s;t)isthescoreoflabelycomputedwithourmodel.sincewedecomposethestruc-turestofactors,eachfactorfi2swillcontributetothescorebasedonthemodelparameters.speci   cally,eachlabelycorrespondstoasliceofthetensorty,whichisamatrix   (y,  ,  ).theneachfactorfiwillcontributeascores(y,fi)=ty gf hf,(3)where correspondtotensorproduct,whileinthecaseofeq.(3),ithastheequivalentform:ty gf hf=ty (gf   hf)=(   (y,  ,  )  gf)thf.inthisway,thetargetscoreoflabelygivenaninstancesandparametertensortcanbewrittenas:s(y,s;t)=nxi=1s(y,fi;t)=nxi=1ty gfi hfi.(4)thefcmmodelonlyperformslineartransformationsoneachviewofthetensor,makingthemodelef   cientandeasytoimplement.learninginordertotraintheparametersweoptimizethefollowingcross-id178objective:`(d;t,w)=x(y,s)2dlogp(y|s;t,w)wheredisthesetofalltrainingdata.weusedadagrad[9]tooptimizeaboveobjective.thereforeweareperformingstochastictraining;andforeachin-stance(y,s)thelossfunction`=`(y,s;t,w)=logp(y|s;t,w).then2bcctswlmodelprf1prf1prf1headembid98(wsize=1)+localfeaturesid98(wsize=3)+localfeaturesfctlocalonlyfctglobal60.6942.3949.9256.4134.4542.7841.9531.7736.16fctglobal(brown)63.1539.5848.6662.4536.4746.0554.9529.9338.75fctglobal(id138)59.0044.7950.9260.2039.6047.7750.9534.1840.92pet(plankandmoschitti,2013)51.240.645.351.037.843.435.432.834.0bow(plankandmoschitti,2013)57.237.145.057.531.841.041.127.232.7best(plankandmoschitti,2013)55.343.148.554.138.144.739.935.837.8table7:performanceonace2005testsets.the   rstpartofthetableshowstheperformanceofdifferentmodelsondifferentsourcesofentitytypes,where   g   meansthatthegoldtypesareusedand   p   meansthatweareusingthepredictedtypes.thesecondpartofthetableshowstheresultsunderthelow-resourcesetting,wheretheentitytypesareunknown.devmrrtestmrrmodelfine-tuning1,00010,000100,0001,00010,000100,000sum-46.9535.2930.6952.6341.1937.32sumy50.8136.8132.9257.2345.0141.23bestrecursivenn(d=50)y45.6730.8627.0554.8439.2535.49bestrecursivenn(d=200)y48.9733.5031.1353.5940.5038.57fctn47.5335.5831.3154.3341.9639.10fcty51.2236.7633.5961.1146.9944.31fct+lm-49.4337.4632.2253.5642.6339.44fct+lm+supervisedy53.8237.4834.4365.4749.4445.65joint56.5341.4136.4568.5251.6546.53table8:performanceonthesemanticsimilaritytaskwithppdbdata.appendix1:featuresusedinfct7.1overallperformancesonace2005sum(ab)6=sum(ba)(7)2n2|v|n(8)aa0ofb0b(9)aba0ofb0(10)t f e)relations(11)f   e[f:e]fctid98@`@r@`@t=@`@r@r@tl1,l2@l@r=@l1@r+@l2@rs(l,e1,e2,s;t)=nxi=1s(l,ewi,fwi)=nxi=1tl fwi ewi(12)@`@t=nxi=1@`@r   fwi   ewi,(13)-.5.3.8.70000-.5.3.8.700000000-.5.3.8.7-.5.3.8.7-.5.3.8.7101001~      0~ ( 2 0)-.5.3.8.7101001-.5.3.8.70000-.5.3.8.700000000-.5.3.8.7-.5.3.8.7-.5.3.8.7101001~      0~ ( 2 0)-.5.3.8.7101001bcctswlmodelprf1prf1prf1headembid98(wsize=1)+localfeaturesid98(wsize=3)+localfeaturesfctlocalonlyfctglobal60.6942.3949.9256.4134.4542.7841.9531.7736.16fctglobal(brown)63.1539.5848.6662.4536.4746.0554.9529.9338.75fctglobal(id138)59.0044.7950.9260.2039.6047.7750.9534.1840.92pet(plankandmoschitti,2013)51.240.645.351.037.843.435.432.834.0bow(plankandmoschitti,2013)57.237.145.057.531.841.041.127.232.7best(plankandmoschitti,2013)55.343.148.554.138.144.739.935.837.8table7:performanceonace2005testsets.the   rstpartofthetableshowstheperformanceofdifferentmodelsondifferentsourcesofentitytypes,where   g   meansthatthegoldtypesareusedand   p   meansthatweareusingthepredictedtypes.thesecondpartofthetableshowstheresultsunderthelow-resourcesetting,wheretheentitytypesareunknown.devmrrtestmrrmodelfine-tuning1,00010,000100,0001,00010,000100,000sum-46.9535.2930.6952.6341.1937.32sumy50.8136.8132.9257.2345.0141.23bestrecursivenn(d=50)y45.6730.8627.0554.8439.2535.49bestrecursivenn(d=200)y48.9733.5031.1353.5940.5038.57fctn47.5335.5831.3154.3341.9639.10fcty51.2236.7633.5961.1146.9944.31fct+lm-49.4337.4632.2253.5642.6339.44fct+lm+supervisedy53.8237.4834.4365.4749.4445.65joint56.5341.4136.4568.5251.6546.53table8:performanceonthesemanticsimilaritytaskwithppdbdata.appendix1:featuresusedinfct7.1overallperformancesonace2005sum(ab)6=sum(ba)(7)2n2|v|n(8)aa0ofb0b(9)aba0ofb0(10)t f e)relations(11)f   e[f:e]fctid98@`@r@`@t=@`@r@r@tl1,l2@l@r=@l1@r+@l2@rs(l,e1,e2,s;t)=nxi=1s(l,ewi,fwi)=nxi=1tl fwi ewi(12)@`@t=nxi=1@`@r   fwi   ewi,(13)w1,w2,   ,wn ew fw fwi ewi ewi fwi (wi=   driving   ) (wi is on path?)     y m1=man m2=taxicab w1=   a    wi=   driving    a     fwi fw1 [a man]m1 driving what appeared to be [a taxicab]m2 have complexity o(c    nd2), where c is a model
dependent constant.

4 hybrid model
we present a hybrid model which combines the
fcm with an existing log-linear model. we do so
by de   ning a new model:

pfcm+loglin(y|x) =

1
z

pfcm(y|x)ploglin(y|x)

(3)

the usual

the log-linear model has
form:
ploglin(y|x)     exp(      f (x, y)), where    are the
model parameters and f (x, y) is a vector of fea-
tures. the integration treats each model as a pro-
viding a score which we multiply together. the
constant z ensures a normalized distribution.

5 training
fcm training optimizes a cross-id178 objective:

(cid:96)(d; t, e) = (cid:88)(x,y)   d

log p (y|x; t, e)

   (cid:96)
   s

where d is the set of all training data and e
is the set of id27s. to optimize
the objective, for each instance (y, x) we per-
form stochastic training on the id168 (cid:96) =
the gradi-
(cid:96)(y, x; t, e) = log p (y|x; t, e).
ents of the model parameters are obtained by
id26 (i.e.
repeated application of
the chain rule). we de   ne the vector s =

[(cid:80)i ty (cid:12) (fwi     ewi)]1   y   l, which yields
=(cid:104)(cid:0)i[y(cid:48) = y]     p (y(cid:48)|x; t, e)(cid:1)1   y   l(cid:105)t

   s    (cid:80)n

where the indicator function i[x] equals 1 if x is
true and 0 otherwise. we have the following gradi-
ents:    (cid:96)
i=1 fwi     ewi, which is equiv-
alent to:
   (cid:96)
   ty(cid:48)
when we treat the id27s as parameters
(i.e. the log-bilinear model), we also    ne-tune the
id27s with the fcm model:

=(cid:0)i[y = y(cid:48)]     p (y(cid:48)|x; t, e)(cid:1)   

n(cid:88)i=1

   t =    (cid:96)

,

   (cid:96)
   ew

=

n(cid:88)i=1(cid:32)(cid:32)(cid:88)y

ty(cid:33)    fi(cid:33)    i[wi = w].

   (cid:96)
   sy

as is common in deep learning, we initialize
these embeddings from an neural language model
and then    ne-tune them for our supervised task.
the training process for the hybrid model (   4)
is also easily done by id26 since each
sub-model has separate parameters.

fwi     ewi.

set
heademb

context

in-between

template

{i[i = h1], i[i = h2]}

(wi is head of m1/m2)   {  , th1 , th2 , th1     th2}

i[i = h1    1] (left/right token of wh1 )
i[i = h2    1] (left/right token of wh2 )
i[i > h1]&i[i < h2] (in between )

on-path

  {  , th1 , th2 , th1     th2}
  {  , th1 , th2 , th1     th2}
table 2: feature sets used in fcm.

i[wi     p ] (on path)

6 experimental settings
features our fcm features (table 2) use a fea-
ture vector fwi over the word wi, the two tar-
get entities m1, m2, and their dependency path.
here h1, h2 are the indices of the two head words
of m1, m2,    refers to the cartesian product be-
tween two sets, th1 and th2 are entity types (named
entity tags for ace 2005 or id138 supertags for
semeval 2010) of the head words of two entities,
and    stands for the empty feature.     refers to the
conjunction of two elements. the in-between
features indicate whether a word wi is in between
two target entities, and the on-path features in-
dicate whether the word is on the dependency
path, on which there is a set of words p , between
the two entities.

all

linguistic annotations needed for

we also use the target entity type as a feature.
combining this with the basic features results in
more powerful compound features, which can help
us better distinguish the functions of word embed-
dings for predicting certain relations. for exam-
ple, if we have a person and a vehicle, we know
it will be more likely that they have an art rela-
tion. for the art relation, we introduce a corre-
sponding weight vector, which is closer to lexical
embeddings similar to the embedding of    drive   .
fea-
tures (pos, chunks7, parses) are from stanford
corenlp (manning et al., 2014). since semeval
does not have gold entity types we obtained word-
net and named entity tags using ciaramita and
altun (2006). for all experiments we use 200-
d id27s trained on the nyt portion
of the gigaword 5.0 corpus (parker et al., 2011),
with id97 (mikolov et al., 2013). we use the
cbow model with negative sampling (15 nega-
tive words). we set a window size c=5, and re-
move types occurring less than 5 times.

models we consider several methods. (1) fcm
in isolation without    ne-tuning. (2) fcm in isola-
tion with    ne-tuning (i.e. trained as a log-bilinear
7obtained from the constituency parse using the conll

2000 chunking converter (perl script).

model). (3) a log-linear model with a rich binary
feature set from sun et al. (2011) (baseline)   
this consists of all the baseline features of zhou et
al. (2005) plus several additional carefully-chosen
features that have been highly tuned for ace-style
id36 over years of research. we ex-
clude the country gazetteer and id138 features
from zhou et al. (2005). the two remaining meth-
ods are hybrid models that integrate fcm as a sub-
model within the log-linear model (   4). we con-
sider two combinations.
(4) the feature set of
nguyen and grishman (2014) obtained by using
the embeddings of heads of two entity mentions
(+headonly).
(5) our full fcm model (+fcm).
all models use l2 id173 tuned on dev
data.

6.1 datasets and evaluation
ace 2005 we evaluate our id36
system on the english portion of the ace 2005
corpus (walker et al., 2006).8 there are 6 do-
mains: newswire (nw), broadcast conversation
(bc), broadcast news (bn), telephone speech
(cts), usenet newsgroups (un), and weblogs
(wl). following prior work we focus on the do-
main adaptation setting, where we train on one set
(the union of the news domains (bn+nw), tune
hyperparameters on a dev domain (half of bc)
and evaluate on the remainder (cts, wl, and
the remainder of bc) (plank and moschitti, 2013;
nguyen and grishman, 2014). we assume that
gold entity spans and types are available for train
and test. we use all pairs of entity mentions to
yield 43,518 total relations in the training set. we
report precision, recall, and f1 for relation extrac-
tion. while it is not our focus, for completeness
we include results with unknown entity types fol-
lowing plank and moschitti (2013) (appendix 1).

semeval 2010 task 8 we evaluate on the se-
meval 2010 task 8 dataset9 (hendrickx et al.,
2010) to compare with other compositional mod-
els and highlight the advantages of fcm. this task
is to determine the relation type (or no relation)
between two entities in a sentence. we adopt the
setting of socher et al. (2012). we use 10-fold

8many id36 systems evaluate on the ace
2004 corpus (mitchell et al., 2005). unfortunately, the most
common convention is to use 5-fold cross validation, treating
the entirety of the dataset as both train and evaluation data.
rather than continuing to over   t this data by perpetuating the
cross-validation convention, we instead focus on ace 2005.

9

http://docs.google.com/view?docid=dfvxd49s_36c28v9pmw

cross validation on the training data to select hy-
perparameters and do id173 by early stop-
ping. the learning rates for fcm with/without
   ne-tuning are 5e-3 and 5e-2 respectively. we
report macro-f1 and compare to previously pub-
lished results.

7 results
ace 2005 despite fcm   s (1) simple feature set,
it is competitive with the log-linear baseline (3)
on out-of-domain test sets (table 3). in the typi-
cal gold entity spans and types setting, both plank
and moschitti (2013) and nguyen and grishman
(2014) found that they were unable to obtain im-
provements by adding embeddings to baseline fea-
ture sets. by contrast, we    nd that on all do-
mains the combination baseline + fcm (5) obtains
the highest f1 and signi   cantly outperforms the
other baselines, yielding the best reported results
for this task. we found that    ne-tuning of em-
beddings (2) did not yield improvements on our
out-of-domain development set, in contrast to our
results below for semeval. we suspect this is be-
cause    ne-tuning allows the model to over   t the
training domain, which then hurts performance on
the unseen ace test domains. accordingly, ta-
ble 3 shows only the log-linear model.

finally, we highlight an important contrast be-
tween fcm (1) and the log-linear model (3): the
latter uses over 50 feature templates based on a
pos tagger, dependency parser, chunker, and con-
stituency parser.
fcm uses only a dependency
parse but still obtains better results (avg. f1).

semeval 2010 task 8 table 4 shows fcm
compared to the best reported results from the
semeval-2010 task 8 shared task and several
other compositional models.

for the fcm we considered two feature sets. we
found that using ne tags instead of id138 tags
helps with    ne-tuning but hurts without. this may
be because the set of id138 tags is larger mak-
ing the model more expressive, but also introduces
more parameters. when the embeddings are    xed,
they can help to better distinguish different func-
tions of embeddings. but when    ne-tuning, it be-
comes easier to over-   t. alleviating over-   tting is
a subject for future work (   9).
with either id138 or ner features, fcm
achieves better performance than the id56 and
mvid56. with ner features and    ne-tuning, it
outperforms a id98 (zeng et al., 2014) and also

model

(1) fcm only (st)
(3) baseline (st)
(4) + headonly (st)
(5) + fcm (st)

p

66.56
74.89
70.87
74.39

bc
r

57.86
48.54
50.76
55.35

f1
61.90
58.90
59.16
63.48

p

65.62
74.32
71.16
74.53

cts
r

44.35
40.26
43.21
45.01

f1
52.93
52.23
53.77
56.12

p

57.80
63.41
57.71
65.63

wl
r

44.62
43.20
42.92
47.59

f1
50.36
51.39
49.23
55.17

avg.
f1
55.06
54.17
54.05
58.26

table 3: comparison of models on ace 2005 out-of-domain test sets. baseline + headonly is our
reimplementation of the features of nguyen and grishman (2014).

classi   er
id166 (rink and harabagiu, 2010)
(best in semeval2010)

features
pos, pre   xes, morphological, id138, dependency parse,
levin classed, probank, framenet, noid113x-plus,
google id165, paraphrases, textrunner
id27, syntactic parse
id27, syntactic parse, pos, ner, id138
id27, syntactic parse
id27, syntactic parse, pos, ner, id138
id27, id138
id27
id27
id27
id27

id56
id56 + linear
mvid56
mvid56 + linear
id98 (zeng et al., 2014)
cr-id98 (log-loss)
cr-id98 (ranking-loss)
relemb (id97 embedding)
relemb (task-spec embedding)
relemb (task-spec embedding) + linear id27, dependency paths, id138, ne
depnn
depnn + linear
(1) fcm (log-linear)

id27, dependency paths
id27, dependency paths, id138, ner
id27, dependency parse, id138
id27, dependency parse, ner
id27, dependency parse, id138
id27, dependency parse, ner
id27, dependency parse, id138
id27, dependency parse, ner

(2) fcm (log-bilinear)

(5) fcm (log-linear) + linear (hybrid)

f1

82.2

74.8
77.6
79.1
82.4
82.7
82.7
84.1
81.8
82.8
83.5
82.8
83.6
82.0
81.4
82.5
83.0
83.1
83.4

table 4: comparison of fcm with previously published results for semeval 2010 task 8.

the combination of an embedding model and a tra-
ditional log-linear model (id56/mvid56 + lin-
ear) (socher et al., 2012). as with ace, fcm uses
less linguistic resources than many close competi-
tors (rink and harabagiu, 2010).

we also compared to concurrent work on en-
hancing the compositional models with task-
speci   c information for relation classi   cation, in-
cluding hashimoto et al. (2015) (relemb), which
trained task-speci   c id27s, and dos
santos et al. (2015) (cr-id98), which proposed
a task-speci   c ranking-based id168. our
hybrid methods (fcm + linear) get comparable re-
sults to theirs. note that their base compositional
model results without any task-speci   c enhance-
ments, i.e. relemb with id97 embeddings
and cr-id98 with log-loss, are still lower than the
best fcm result. we believe that fcm can be also
improved with these task-speci   c enhancements,
e.g. replacing the id27s to the task-
speci   c ones from (hashimoto et al., 2015) in-
creases the result to 83.7% (see   7.2 for details).
we leave the application of ranking-based loss to
future work.

finally, a concurrent work (liu et al., 2015)
proposes depnn, which builds representations for
the dependency path (and its attached subtrees)
between two entities by applying recursive and
convolutional neural networks successively. com-
pared to their model, our fcm achieves compa-
rable results. of note, our fcm and the relemb
are also the most ef   cient models among all above
compositional models since they have linear time
complexity with respect to the dimension of em-
beddings.

7.1 effects of the embedding sub-models
we next investigate the effects of different types of
features on fcm using ablation tests on ace 2005
(table 5.) we focus on fcm alone with the fea-
ture templates of table 2. additionally, we show
results of using only the head embedding features
from nguyen and grishman (2014) (headonly).
not surprisingly, the headonly model performs
poorly (f1 score = 14.30%), showing the impor-
tance of our rich binary feature set. among all the
features templates, removing heademb results in
the largest degradation. the second most im-

feature set
headonly
fcm

-heademb
-context
-in-between
-on-path

fcm-entitytypes

prec
31.67
69.17
66.06
70.89
66.39
69.23
71.33

rec
9.24
56.73
47.00
55.27
51.86
53.97
34.68

f1
14.30
62.33
54.92
62.11
58.23
60.66
46.67

table 5: ablation test of fcm on development set.

portant feature template is in-between, while
context features have little impact. remov-
ing all entity type features (thi) does signi   cantly
worse than the full model, showing the value of
our entity type features.

7.2 effects of the id27s
good id27s are critical for both fcm
and other compositional models. in this section,
we show the results of fcm with embeddings
used to initialize other recent state-of-the-art mod-
els. those embeddings include the 300-d baseline
embeddings trained on english wikipedia (w2v-
enwiki-d300) and the 100-d task-speci   c embed-
dings (task-speci   c-d100)10 from the relemb pa-
per (hashimoto et al., 2015), the 400-d embed-
dings from the cr-id98 paper (dos santos et al.,
2015). moreover, we list the best result (depnn)
in liu et al. (2015), which uses the same embed-
dings as ours. table 6 shows the effects of word
embeddings on fcm and provides relative compar-
isons between fcm and the other state-of-the-art
models. we use the same hyperparameters and
number of iterations in table 4.

the results show that using different embed-
dings to initialize fcm can improve f1 beyond
our previous results. we also    nd that increas-
ing the dimension of the id27s does
not necessarily lead to better results due to the
problem of over-   tting (e.g.w2v-enwiki-d400 vs.
w2v-enwiki-d300). with the same initial embed-
dings, fcm usually gets better results without any
changes to the hyperparameters than the compet-
ing model, further con   rming the advantage of
fcm at the model-level as discussed under ta-
ble 4. the only exception is the depnn model,
which gets better result than fcm on the same
embeddings. the task-speci   c embeddings from
(hashimoto et al., 2015) leads to the best perfor-
mance (an improvement of 0.7%). this observa-

10in the task-speci   c setting, fcm will represent entity
words and context words with separate sets of embeddings.

embeddings
w2v-enwiki-d300

task-speci   c-d100

w2v-enwiki-d400

w2v-nyt-d200

model
relemb
(2) fcm (log-bilinear)
relemb
relemb+linear
(2) fcm (log-bilinear)
cr-id98
(2) fcm (log-bilinear)
depnn
(2) fcm (log-bilinear)

f1
81.8
83.4
82.8
83.5
83.7
82.7
83.0
83.6
83.0

table 6: evaluation of fcms with different word
embeddings on semeval 2010 task 8.

tion suggests that the other compositional models
may also bene   t from the work of hashimoto et
al. (2015).

8 related work
compositional models for sentences
in order
to build a representation (embedding) for a sen-
tence based on its component id27s
and structural information, recent work on compo-
sitional models (id30 from the deep learning
community) has designed model structures that
mimic the structure of the input. for example,
these models could take into account the order of
the words (as in convolutional neural networks
(id98s)) (collobert et al., 2011) or build off of
an input tree (as in id56s
(id56s) or the semantic matching energy func-
tion) (socher et al., 2013b; bordes et al., 2012).

while these models work well on sentence-level
representations, the nature of their designs also
limits them to    xed types of substructures from the
annotated sentence, such as chains for id98s and
trees for id56s. such models cannot capture arbi-
trary combinations of linguistic annotations avail-
able for a given task, such as word order, depen-
dency tree, and named entities used for relation
extraction. moreover, these approaches ignore the
differences in functions between words appearing
in different roles. this does not suit more general
substructure labeling tasks in nlp, e.g. these mod-
els cannot be directly applied to id36
since they will output the same result for any pair
of entities in a same sentence.

compositional models with annotation fea-
tures to tackle the problem of traditional com-
positional models, socher et al. (2012) made the
id56 model speci   c to id36 tasks by
working on the minimal sub-tree which spans the
two target entities. however, these specializations

to id36 does not generalize easily to
other tasks in nlp. there are two ways to achieve
such specialization in a more general fashion:

1. enhancing compositional models with fea-
tures. a recent trend enhances compositional
models with annotation features. such an ap-
proach has been shown to signi   cantly improve
over pure compositional models. for example,
hermann et al. (2014) and nguyen and grishman
(2014) gave different weights to words with dif-
ferent syntactic context types or to entity head
words with different argument ids. zeng et al.
(2014) use concatenations of embeddings as fea-
tures in a id98 model, according to their posi-
tions relative to the target entity mentions. be-
linkov et al. (2014) enrich embeddings with lin-
guistic features before feeding them forward to a
id56 model. socher et al. (2013a) and hermann
and blunsom (2013) enhanced id56 models by
re   ning the transformation matrices with phrase
types and id35 super tags.

2. engineering of embedding features. a dif-
ferent approach to combining traditional linguistic
features and embeddings is hand-engineering fea-
tures with id27s and adding them to
id148. such approaches have achieved
state-of-the-art results in many tasks including
ner, chunking, id33, semantic
role labeling, and id36 (miller et al.,
2004; turian et al., 2010; koo et al., 2008; roth
and woodsend, 2014; sun et al., 2011; plank and
moschitti, 2013). roth and woodsend (2014) con-
sidered features similar to ours for semantic role
labeling.

however,

in prior work both of above ap-
proaches are only able to utilize limited informa-
tion, usually one property for each word. yet there
may be different useful properties of a word which
can contribute to the performances of the task. by
contrast, our fcm can easily utilize these features
without changing the model structures.

in order to better utilize the dependency anno-
tations, recently work built their models according
to the dependency paths (ma et al., 2015; liu et
al., 2015), which share similar motivations to the
usage of on-path features in our work.

task-speci   c enhancements for relation clas-
si   cation an orthogonal direction of improving
compositional models for relation classi   cation is
to enhance the models with task-speci   c informa-
tion. for example, hashimoto et al. (2015) trained

task-speci   c id27s, and dos santos et
al. (2015) proposed a ranking-based id168
for relation classi   cation.

9 conclusion

we have presented fcm, a new compositional
model for deriving sentence-level and substruc-
ture embeddings from id27s. com-
pared to existing compositional models, fcm can
easily handle arbitrary types of input and handle
global information for composition, while remain-
ing easy to implement. we have demonstrated
that fcm alone attains near state-of-the-art perfor-
mances on several id36 tasks, and
in combination with traditional feature based log-
linear models it obtains state-of-the-art results.

our next steps in improving fcm focus on en-
hancements based on task-speci   c embeddings or
id168s as in hashimoto et al. (2015; dos
santos et al. (2015). moreover, as the model pro-
vides a general idea for representing both sen-
tences and sub-structures in language, it has the
potential to contribute useful components to vari-
ous tasks, such as id33, srl and
id141. also as kindly pointed out by one
anonymous reviewer, our fcm can be applied to
the tac-kbp (ji et al., 2010) tasks, by replac-
ing the training objective to a multi-instance multi-
label one (e.g. surdeanu et al. (2012)). we plan to
explore the above applications of fcm in the fu-
ture.

acknowledgments

we thank the anonymous reviewers for their com-
ments, and nicholas andrews, francis ferraro,
and benjamin van durme for their input. we
thank kazuma hashimoto, c    cero nogueira dos
santos, bing xiang and bowen zhou for sharing
their id27s and many helpful discus-
sions. mo yu is supported by the china scholar-
ship council and by nsfc 61173073.

references
yonatan belinkov, tao lei, regina barzilay, and amir
globerson. 2014. exploring compositional archi-
tectures and word vector representations for prepo-
sitional phrase attachment. transactions of the as-
sociation for computational linguistics, 2:561   572.

antoine bordes, xavier glorot, jason weston, and
yoshua bengio. 2012. a semantic matching en-

ergy function for learning with multi-relational data.
machine learning, pages 1   27.

massimiliano ciaramita and yasemin altun.

2006.
broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. in
emnlp2006, pages 594   602, july.

ronan collobert, jason weston, l  eon bottou, michael
karlen, koray kavukcuoglu, and pavel kuksa.
2011. natural language processing (almost) from
scratch. jmlr, 12:2493   2537.

cicero dos santos, bing xiang, and bowen zhou.
2015. classifying relations by ranking with con-
volutional neural networks.
in proceedings of the
53rd annual meeting of the association for compu-
tational linguistics and the 7th international joint
conference on natural language processing (vol-
ume 1: long papers), pages 626   634, beijing,
china, july. association for computational linguis-
tics.

kazuma hashimoto, pontus stenetorp, makoto miwa,
and yoshimasa tsuruoka.
2015. task-oriented
learning of id27s for semantic relation
classi   cation. arxiv preprint arxiv:1503.00095.

iris hendrickx, su nam kim, zornitsa kozareva,
preslav nakov, diarmuid   o s  eaghdha, sebastian
pad  o, marco pennacchiotti, lorenza romano, and
stan szpakowicz.
semeval-2010 task
8: multi-way classi   cation of semantic relations
between pairs of nominals.
in proceedings of
semeval-2 workshop.

2010.

karl moritz hermann and phil blunsom. 2013. the
role of syntax in vector space models of composi-
tional semantics. in association for computational
linguistics, pages 894   904.

karl moritz hermann, dipanjan das, jason weston,
and kuzman ganchev. 2014. semantic frame iden-
ti   cation with distributed word representations. in
proceedings of the 52nd annual meeting of the as-
sociation for computational linguistics (volume 1:
long papers), pages 1448   1458, baltimore, mary-
land, june. association for computational linguis-
tics.

heng ji, ralph grishman, hoa trang dang, kira grif-
   tt, and joe ellis. 2010. overview of the tac 2010
knowledge base population track.
in third text
analysis conference (tac 2010).

terry koo, xavier carreras, and michael collins.
2008. simple semi-supervised id33.
in proceedings of acl-08: hlt, pages 595   603,
columbus, ohio, june. association for computa-
tional linguistics.

qi li and heng ji. 2014.

incremental joint extrac-
tion of entity mentions and relations.
in proceed-
ings of the 52nd annual meeting of the association
for computational linguistics (volume 1: long pa-
pers), pages 402   412, baltimore, maryland, june.
association for computational linguistics.

yang liu, furu wei, sujian li, heng ji, ming zhou,
and houfeng wang. 2015. a dependency-based
neural network for relation classi   cation.
in pro-
ceedings of the 53rd annual meeting of the associ-
ation for computational linguistics and the 7th in-
ternational joint conference on natural language
processing (volume 2: short papers), pages 285   
290, beijing, china, july. association for computa-
tional linguistics.

mingbo ma, liang huang, bowen zhou, and bing xi-
ang. 2015. dependency-based convolutional neural
networks for sentence embedding.
in proceedings
of the 53rd annual meeting of the association for
computational linguistics and the 7th international
joint conference on natural language processing
(volume 2: short papers), pages 174   179, beijing,
china, july. association for computational linguis-
tics.

christopher d. manning, mihai surdeanu, john bauer,
jenny finkel, steven j. bethard, and david mc-
closky. 2014. the stanford corenlp natural lan-
guage processing toolkit.
in proceedings of 52nd
annual meeting of the association for computa-
tional linguistics: system demonstrations, pages
55   60.

tomas mikolov, ilya sutskever, kai chen, greg cor-
rado, and jeffrey dean. 2013. distributed represen-
tations of words and phrases and their composition-
ality. arxiv preprint arxiv:1310.4546.

scott miller, jethran guinness, and alex zamanian.
2004. name tagging with word clusters and dis-
criminative training.
in susan dumais, daniel
marcu, and salim roukos, editors, hlt-naacl
2004: main proceedings. association for compu-
tational linguistics.

alexis mitchell, stephanie strassel, shudong huang,
and ramez zakhary. 2005. ace 2004 multilin-
gual training corpus. linguistic data consortium,
philadelphia.

andriy mnih and geoffrey hinton. 2007. three new
id114 for statistical language modelling.
in proceedings of the 24th international conference
on machine learning, pages 641   648. acm.

thien huu nguyen and ralph grishman. 2014. em-
ploying word representations and id173 for
id20 of id36.
in pro-
ceedings of the 52nd annual meeting of the associa-
tion for computational linguistics (volume 2: short
papers), pages 68   74, baltimore, maryland, june.
association for computational linguistics.

thien huu nguyen and ralph grishman. 2015. rela-
tion extraction: perspective from convolutional neu-
ral networks. in proceedings of naacl workshop
on vector space modeling for nlp.

thien huu nguyen, barbara plank, and ralph gr-
ishman. 2015. semantic representations for do-

methods in natural language processing and com-
putational natural language learning, pages 455   
465. association for computational linguistics.

joseph turian, lev ratinov, and yoshua bengio. 2010.
word representations: a simple and general method
for semi-supervised learning.
in association for
computational linguistics, pages 384   394.

christopher walker, stephanie strassel, julie medero,
and kazuaki maeda. 2006. ace 2005 multilin-
gual training corpus. linguistic data consortium,
philadelphia.

mo yu and mark dredze. 2015. learning composition
models for phrase embeddings. transactions of the
association for computational linguistics, 3:227   
242.

mo yu, matthew r. gorid113y, and mark dredze. 2015.
combining id27s and feature embed-
dings for    ne-grained id36.
in pro-
ceedings of naacl.

daojian zeng, kang liu, siwei lai, guangyou zhou,
and jun zhao. 2014. relation classi   cation via con-
volutional deep neural network. in proceedings of
coling 2014, the 25th international conference
on computational linguistics: technical papers,
pages 2335   2344, dublin, ireland, august. dublin
city university and association for computational
linguistics.

guodong zhou, jian su, jie zhang, and min zhang.
2005. exploring various knowledge in relation ex-
traction. in association for computational linguis-
tics, pages 427   434.

main adaptation: a case study on the tree kernel-
based method for id36.
in proceed-
ings of the 53rd annual meeting of the association
for computational linguistics and the 7th interna-
tional joint conference on natural language pro-
cessing (volume 1: long papers), pages 635   644,
beijing, china, july. association for computational
linguistics.

robert parker, david graff, junbo kong, ke chen,
english gigaword
linguistic data consortium,

and kazuaki maeda.
   fth edition, june.
ldc2011t07.

2011.

barbara plank and alessandro moschitti. 2013. em-
bedding semantic similarity in tree kernels for do-
main adaptation of id36. in proceed-
ings of the 51st annual meeting of the association
for computational linguistics (volume 1: long pa-
pers), pages 1498   1507, so   a, bulgaria, august.
association for computational linguistics.

bryan rink and sanda harabagiu. 2010. utd: clas-
sifying semantic relations by combining lexical and
semantic resources. in proceedings of the 5th inter-
national workshop on semantic evaluation, pages
256   259, uppsala, sweden, july. association for
computational linguistics.

michael roth and kristian woodsend. 2014. com-
position of word representations improves semantic
role labelling. in emnlp.

richard socher, brody huval, christopher d. man-
ning, and andrew y. ng. 2012. semantic composi-
tionality through recursive matrix-vector spaces. in
proceedings of the 2012 joint conference on empir-
ical methods in natural language processing and
computational natural language learning, pages
1201   1211, jeju island, korea, july. association for
computational linguistics.

richard socher, john bauer, christopher d manning,
and andrew y ng. 2013a. parsing with compo-
sitional vector grammars. in in proceedings of the
acl conference. citeseer.

richard socher, alex perelygin, jean wu, jason
chuang, christopher d. manning, andrew ng, and
christopher potts. 2013b. recursive deep models
for semantic compositionality over a sentiment tree-
bank.
in empirical methods in natural language
processing, pages 1631   1642.

ang sun, ralph grishman, and satoshi sekine. 2011.
semi-supervised id36 with large-scale
word id91.
in proceedings of the 49th an-
nual meeting of the association for computational
linguistics: human language technologies, pages
521   529, portland, oregon, usa, june. association
for computational linguistics.

mihai surdeanu, julie tibshirani, ramesh nallapati,
and christopher d manning. 2012. multi-instance
multi-label learning for id36. in pro-
ceedings of the 2012 joint conference on empirical

appendix 1: experiments on ace 2005
where gold entity types are unknown

experimental settings: for comparison with
prior work (plank and moschitti, 2013), we (1)
generate relation instances from all pairs of enti-
ties within each sentence with three or fewer inter-
vening entity mentions   labeling those pairs with
no relation as negative instances, (2) use gold en-
tity spans (but not types) at train and test time, and
(3) evaluate on the 7 coarse relation types, ignor-
ing the subtypes.
in the training set, 35,990 to-
tal relations are annotated of which only 3,658 are
non-nil relations. we did not match the number of
tokens they reported in the cts and wl domains.
therefore, in this section we only report the re-
sults on the test set of bc domain. we will leave
experiments on additional domains in future work.
we run the same models as in   7 on this task.
here the fcm does not use entity type features.
plank and moschitti (2013) also use brown clus-
ters and word vectors learned by latent-semantic
analysis (lsa). in order to make a fair compar-
ison with their method, we also report the fcm
result using brown clusters (pre   xes of length
5) of entity heads as entity types. furthermore,
we report non-comparable settings using word-
net super-sense tags of entity heads as types. the
id138 features were also used in their paper
but not as substitution of entity types. we use the
same toolkit to get the id138 tags as in   6. the
brown clusters are from (koo et al., 2008)11.

results: table 7 shows the results under the
low-resource setting. when no entity types are
available, the performance of our fcm only model
greatly decreases to 48.15%, which is consis-
tent with our observation in the ablation tests.
the baseline model also relies heavily on the
entity types. after we remove all
the hand-
engineering features that contain entity type in-
formation, the performance of our baseline model
drop to 40.62%, even lower than the reduced fcm
only model.

the combination of baseline model and head
embeddings (baseline + headonly) greatly im-
prove the results. this is consistent with the ob-
servation in nguyen and grishman (2014) that
when the gold entity types are unknown, informa-
tion of the entity heads provided by their embed-

11http://people.csail.mit.edu/maestro/

papers/bllip-clusters.gz

dings will play a more important role. combina-
tion of the baseline and fcm (baseline + fcm) also
achieves improvement but not signi   cantly better
than baseline + headonly. a possible explana-
tion is that fcm becomes less ef   cient on using
context id27s when the entity type in-
formation is unavailable. in this situation the head
embeddings provided by fcm become the domi-
nating contribution to the baseline model, making
the model have similar behavior as the baseline +
headonly method.

finally, we    nd brown clusters can help fcm
when entity types are unknown. although the per-
formance is still not signi   cantly better than base-
line + headonly, it outperforms all the results in
plank and moschitti (2013) as a single model, and
with the same source of features. id138 super-
sense tags further improves fcm, and achieves the
best reported results on this low-resource setting.
these results are encouraging since it shows fcm
may be more useful under the end-to-end setting
where predictions of both entity mentions and re-
lation mentions are required in place of predicting
relation based on gold tags (li and ji, 2014).

recently nguyen et al. (2015) proposed a novel
way of applying embeddings to tree-kernels. from
the results, our best single model achieves com-
parable result with their best single system, while
their combination method is slightly better than
ours. this suggests that we may bene   t more from
combining the usages of multiple word represen-
tations; and we will investigate it in future work.

model
pm   13 (brown)
pm   13 (lsa)
pm   13 (combination)
(1) fcm only
(3) baseline
(4) + headonly
(5) + fcm
(1) fcm only w/ brown
(1) fcm only w/id138
linear+emb
tree-kernel+emb (single)
tree-kernel+emb (combination)

p
54.4
53.9
55.3
53.7
59.4
64.9
65.5
64.6
64.0
46.5
57.6
58.5

bc
r
43.4
45.2
43.1
43.7
30.9
41.3
41.5
40.2
43.2
49.3
46.6
47.3

f1
48.3
49.2
48.5
48.2
40.6
50.5
50.8
49.6
51.6
47.8
51.5
52.3

table 7: comparison of models on ace 2005 out-of-
domain test sets for the low-resource setting, where the
gold entity spans are known but entity types are unknown.
pm   13 is the results reported in plank and moschitti (2013).
   linear+emb    is the implementation of our method (4) in
(nguyen et al., 2015). the    tree-kernel+emb    methods are
the enrichments of tree-kernels with embeddings proposed by
nguyen et al. (2015).

