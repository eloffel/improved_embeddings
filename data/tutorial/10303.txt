on using monolingual corpora in id4
caglar gulcehre(cid:63)

orhan firat(cid:63),   

kelvin xu

universit  e de montr  eal

middle east technical university

universit  e de montr  eal

5
1
0
2

 

n
u
j
 

2
1

 
 
]
l
c
.
s
c
[
 
 

2
v
5
3
5
3
0

.

3
0
5
1
:
v
i
x
r
a

kyunghyun cho

universit  e de montr  eal

lo    c barrault

universit  e du maine

huei-chi lin

universit  e du maine

fethi bougares

universit  e du maine

holger schwenk
universit  e du maine

yoshua bengio

universit  e de montr  eal
cifar senior fellow

abstract

on

end-to-end

neural
recent work
network-based architectures for machine
translation has shown promising results
for en-fr and en-de translation. ar-
guably, one of the major factors behind
this success has been the availability of
high quality parallel corpora. in this work,
we investigate how to leverage abundant
monolingual corpora for neural machine
translation. compared to a phrase-based
and hierarchical baseline, we obtain up
to 1.96 id7 improvement on the low-
resource language pair turkish-english,
and 1.59 id7 on the focused domain
task of chinese-english chat messages.
while our method was initially targeted
toward such tasks with less parallel data,
we show that
it also extends to high
resource languages such as cs-en and
de-en where we obtain an improvement
of 0.39 and 0.47 id7 scores over the
neural machine
translation baselines,
respectively.

1

introduction

id4 (id4) is a novel
approach to machine translation that has shown
promising results (kalchbrenner and blunsom,
2013; sutskever et al., 2014; cho et al., 2014;
bahdanau et al., 2014). until recently, the applica-
tion of neural networks to machine translation was
restricted to extending standard machine transla-
tion tools for rescoring translation hypotheses or

coin    ip.

(cid:63) equal contribution. order has been determined with a
    work done while author was at universit  e de montr  eal

re-ranking n-best lists (see, e.g., (schwenk, 2012;
schwenk, 2007a). in contrast, it has been shown
that, it is possible to build a competitive trans-
lation system for english-french and english-
german using an end-to-end neural network archi-
tecture (sutskever et al., 2014; jean et al., 2014)
(also see sec. 2).

arguably, a large part of the recent success of
these methods has been due to the availability of
large amounts of high quality, sentence aligned
corpora.
in the case of low resource language
pairs or in a task with heavy domain restrictions,
there can be a lack of such sentence aligned cor-
pora. in contrast, monolingual corpora is almost
always universally available. despite being    unla-
beled   , monolingual corpora still exhibit rich lin-
guistic structure that may be useful for translation
tasks. this presents an opportunity to leverage
such corpora to give hints to an id4 system.

in this work, we present a way to effectively
integrate a language model (lm) trained only on
monolingual data (target language) into an id4
system. we provide experimental results that in-
corporating monolingual corpora can improve a
translation system on a low-resource language pair
(turkish-english) and a domain restricted trans-
lation problem (chinese-english sms chat).
in
addition, we show that these methods improve
the performance on the relatively high-resource
german-english (de-en) and czech-english (cs-
en) translation tasks.

in the following section (sec. 2), we review
recent work in id4. we
present our basic model architecture in sec. 3 and
describe our shallow and deep fusion approaches
in sec. 4. next, we describe our datasets in sec. 5.
finally, we describe our main experimental results
in sec. 6.

2 background: neural machine

translation

id151 (smt) systems
maximize the id155 p(y | x) of
a correct target translation y given a source sen-
tence x. this is done by maximizing separately a
language model p(y) and the (inverse) translation
model p(x | y) component by using bayes    rule:

p(y | x)     p(x | y)p(y).

this decomposition into a language model and
translation model is meant to make full use of
available corpora: monolingual corpora for    tting
the language model and parallel corpora for the
translation model. in reality, however, smt sys-
tems tend to model log p(y | x) directly by lin-
early combining multiple features by using a so-
called log-linear model:
log p(y | x) =

(cid:88)

fj(x, y) + c,

(1)

j

where fj is the j-th feature based on both or either
of the source and target sentences, and c is a nor-
malization constant which is often ignored. these
features include, for instance, pair-wise statistics
between two sentences/phrases. the log-linear
model is    tted to data, in most cases, by maximiz-
ing an automatic evaluation metric other than an
actual id155, such as id7.
id4, on the other hand,
aims at directly optimizing log p(y | x) includ-
ing the feature extraction as well as the normaliza-
tion constant by a single neural network. this is
typically done under the encoder-decoder frame-
work (kalchbrenner and blunsom, 2013; cho et
al., 2014; sutskever et al., 2014) consisting of neu-
ral networks. the    rst network encodes the source
sentence x into a continuous-space representa-
tion from which the decoder produces the target
translation sentence. by using id56 architectures
equipped to learn long term dependencies such
as id149 (gru) or long short-
term memory (lstm), the whole system can be
trained end-to-end (cho et al., 2014; sutskever et
al., 2014).

once the model learns the conditional distribu-
tion or translation model, given a source sentence
we can    nd a translation that approximately max-
imizes the id155 using, for in-
stance, a id125 algorithm.

3 model description
we use the model recently proposed by (bahdanau
et al., 2014) that learns to jointly (soft-)align and
translate as the baseline id4
system in this paper. here we describe in detail
this model to which we refer as    id4   .

the encoder of the id4 is a bidirectional
id56 which consists of forward and backward
id56s (schuster and paliwal, 1997). the for-
ward id56 reads the input sequence/sentence x =
(x1, . . . , xt ) in a forward direction, resulting in
      
h t ). the
a sequence of hidden states (
backward id56 reads x in an opposite direction
      
and outputs (
h t ). we concatenate a pair
of hidden states at each time step to build a se-
quence of annotation vectors (h1, . . . , ht ), where

      
h 1, . . . ,

      
h 1, . . . ,

(cid:104)      

h(cid:62)
j =

      
h (cid:62)

j

h (cid:62)
j ;

(cid:105)

.

each annotation vector hj encodes information
about the j-th word with respect to all the other
surrounding words in the sentence.

in our decoder, which we construct with a sin-
gle layer id56, at each timestep t a soft-alignment
mechanism    rst decides on which annotation vec-
tors are most relevant. the relevance weight   tj of
the j-th annotation vector for the t-th target word
is computed by a feedforward neural network f
that takes as input hj, the previous decoder   s hid-
den state st   1 and the previous output yt   1:

etj = f(st   1, hj, yt   1).

the outputs etj are normalized over the sequence
of the annotation vectors so that the they sum to 1:

(cid:80)t

exp(etj)
k=1 exp(etk)

  tj =

,

(2)

and we call   tj a relevance score, or an alignment
weight, of the j-th annotation vector.

the relevance scores are used to get the context

vector ct of the t-th word in the translation:

t(cid:88)

ct =

  tjhj ,

j=1

then, the decoder   s hidden state stm

at time t is
computed based on the previous hidden state stm
t   1,
the context vector ct and the previously translated
word yt   1:

t

stm
t = fr(stm

t   1, yt   1, ct),

(3)

where fr is the gated recurrent unit (cho et al.,
2014).

we use a deep output layer (pascanu et al.,
2014) to compute the conditional distribution over
words:

p(yt|y<t, x)    

exp(y(cid:62)

t (wofo(stm

t

, yt   1, ct) + bo)),

(4)

where yt is a one-hot encoded vector indicating
one of the words in the target vocabulary. wo is
a learned weight matrix and bo is a bias.
fo is
a single-layer feedforward neural network with a
two-way maxout non-linearity (goodfellow et al.,
2013).

the whole model, including both the encoder
and decoder, is jointly trained to maximize the
(conditional) log-likelihood of the bilingual train-
ing corpus:

n(cid:88)

n=1

max

  

1
n

log p  (y(n)|x(n)),

where the training corpus is a set of (x(n), y(n))   s,
and    denotes a set of all the tunable parameters.

4

integrating language model into the
decoder

in this paper, we propose two alternatives to in-
tegrating a language model into a neural machine
translation system which we refer as shallow fu-
sion (sec. 4.1) and deep fusion (sec. 4.2). without
loss of generality, we use a language model based
on recurrent neural networks (id56lm, (mikolov
et al., 2011)) which is equivalent to the decoder de-
scribed in the previous section except that it is not
biased by a context vector (i.e., ct = 0 in eqs. (3)   
(4)).

in the sections that follow, we assume that
both an id4 model (on parallel corpora) as well
as a recurrent neural network language model
(id56lm, on larger monolingual corpora) have
been pre-trained separately before being inte-
grated. we denoted the hidden state at time t of
the id56lm with slm

.

t

weighted sum of the scores given by the transla-
tion model and the language model.

(cid:111)

(cid:110)

y(i)   t   1

more speci   cally, at each time step t, the trans-
lation model (in this case, the id4) computes the
score of every possible next word for each hypoth-
. each score is the
esis all of hypotheses
summation of the score of the hypothesis and the
score given by the id4 to the next word. all
these new hypotheses (a hypothesis from the pre-
vious timestep with a next word appended at the
end) are then sorted according to their respective
scores, and the top k ones are selected as candi-
dates

(cid:110)

(cid:111)

.

  y(i)   t

we then rescore these hypotheses with the
weighted sum of the scores by the id4 and
id56lm, where we only need to recompute the
score of the    new word    at the end of each can-
didate hypothesis. the score of the new word is
computed by

i=1,...,k

log p(yt = k) = log ptm(yt = k)

+    log plm(yt = k),

(5)

where    is a hyper-parameter that needs to be
tuned to maximize the translation performance on
a development set.

see fig. 1 (a) for illustration.

4.2 deep fusion
in deep fusion, we integrate the id56lm and the
decoder of the id4 by concatenating their hid-
den states next to each other (see fig. 1 (b)).
the model is then    netuned to use the hidden
states from both of these models when comput-
ing the output id203 of the next word (see
eq. (4)). unlike the vanilla id4 (without any
language model component), the hidden layer of
the deep output takes as input the hidden state of
the id56lm in addition to that of the id4, the
previous word and the context such that

p(yt|y<t, x)    

exp(y(cid:62)

t (wofo(slm

t

, stm

t

, yt   1, ct) + bo)),

(6)

4.1 shallow fusion
shallow fusion is analogous to how language mod-
els are used in the decoder of a usual smt sys-
tem (koehn, 2010). at each time step, the trans-
lation model proposes a set of candidate words.
the candidates are then scored according to the

where again we use the superscripts lm and tm
to denote the hidden states of the id56lm and
id4 respectively.

during the    netuning of the model, we tune
only the parameters that were used to parameterize
the output (6). this is to ensure that the structure

(a) shallow fusion (sec. 4.1)

(b) deep fusion (sec. 4.2)

figure 1: graphical illustrations of the proposed fusion methods.

learned by the lm from monolingual corpora is
not overwritten. it is possible to use monolingual
corpora as well while    netuning all the parame-
ters, but in this paper, we alter only the output pa-
rameters in the stage of    netuning.
4.2.1 balancing the lm and tm
in order for the decoder to    exibly balance the in-
put from the lm and tm, we augment the decoder
with a    controller    mechanism. the need to    ex-
ibly balance the signals arises depending on the
work being translated. for instance, in the case
of zh-en, there are no chinese words that corre-
spond to articles in english, in which case the lm
may be more informative. on the other hand, if
a noun is to be translated, it may be better to ig-
nore any signal from the lm, as it may prevent the
decoder from choosing the correct translation. in-
tuitively, this mechanism helps the model dynami-
cally weight the different models depending on the
word being translated.

the controller mechanism is implemented as a
function taking the hidden state of the lm as input
and computing

(cid:16)

(cid:17)

gt =   

v(cid:62)
g slm

t + bg

,

(7)

where    is a logistic sigmoid function. vg and bg
are learned parameters.

the output of the controller is then multiplied
with the hidden state of the lm. this lets the de-

coder use the signal from the tm fully, while the
controller controls the magnitude of the lm sig-
nal.

in our experiments, we empirically found that it
was better to initialize the bias bg to a small, neg-
ative number. this allows the decoder to decide
the importance of the lm only when it is deemed
necessary.

5 datasets

we evaluate the proposed approaches on four di-
verse tasks: chinese to english (zh-en), turkish
to english (tr-en), german to english (de-en)
and czech to english (cs-en). we describe each
of these datasets in more detail below.

5.1 parallel corpora
5.1.1 zh-en: opeid4   15
we use the parallel corpora made available
as a part of the nist opeid4   15 challenge.
sentence-aligned pairs from three domains are
combined to form a training set: (1) sms/chat
and (2) conversational telephone speech (cts)
from darpa bolt project, and (3) newsgroup-
s/weblogs from darpa gale project. in total,
the training set consists of 430k sentence pairs
(see table 1 for the detailed statistics). we train
in all our experiments, we set bg =    1 to ensure that

gt is initially 0.26 on average.

models with this training set and the development
set (the concatenation of the provided develop-
ment and tune sets from the challenge), and evalu-
ate them on the test set. the domain of the devel-
opment and test sets is restricted to cts.
preprocessing importantly, we did    not seg-
ment    the chinese sentences and considered each
character as a symbol, unlike other approaches
which use a separate segmentation tool to segment
the chinese characters into words (devlin et al.,
2014). any consecutive non-chinese characters
such as latin alphabets were, however, considered
as an individual word. lastly, we removed any
html/xml tags from the corpus, chose only the
intended meaning word if both intended and literal
translations are available, and ignored any indica-
tor of, e.g., typos. the only preprocessing we did
on the english side of the corpus was a simple to-
kenization using the tokenizer from moses. .
5.1.2 tr-en: iwslt   14
we used the wit parallel corpus (cettolo et al.,
2012) and setimes parallel corpus made available
as a part of iwslt   14 (machine translation track).
the corpus consists of the sentence-aligned subti-
tles of ted and tedx talks, and we concatenated
dev2010 and tst2010 to form a development set,
and tst2011, tst2012, tst2013 and tst2014 to form
a test set. see table 1 for the detailed statistics of
the parallel corpora.
preprocessing as done with the case of zh-en,
initially we removed all special symbols from the
corpora and tokenized the turkish side with the to-
kenizer provided by moses. to overcome the ex-
ploding vocabulary due to the rich in   ections and
derivations in turkish, we segmented each turk-
ish sentence into a sequence of sub-word units
using zemberek followed by morphological dis-
ambiguation on the morphological analysis (sak
et al., 2007). we removed any non-surface mor-
phemes corresponding to, for instance, part-of-
speech tags.

5.2 cs-en and de-en: wmt   15
for the training of our models, we used all the
available training data provided for cs-en and de-
en in the wmt   15 competition. we used new-

https://github.com/moses-smt/

mosesdecoder/blob/master/scripts/
tokenizer/tokenizer.perl

https://github.com/ahmetaa/

zemberek-nlp

stest2013 as a development set and newstest2014
for a test set. the detailed statistics of the parallel
corpora is provided in table 1.

preprocessing we tokenized the datasets with
moses tokenizer    rst.
sentences longer than
eighty words and those that have large mismatch
between lengths of the source and target sentences
were removed from the training set. then, we    l-
tered the training data by removing sentence pairs
in which one sentence (or both) was written in
the wrong language by using a language detection
toolkit (shuyo, 2010), unless the sentence had 5
words or less. for de-en, we also split the com-
pounds in the german side by using moses. fi-
nally we shuf   ed the training corpora seven times
and concatenated its outputs.

5.3 monolingual corpora
the english gigaword corpus by the linguis-
tic data consortium (ldc), which mainly con-
sists of newswire documents, was allowed in both
opeid4   15 and iwslt-15 challenges for lan-
guage modelling. we used the tokenized gi-
gaword corpus without any other preprocessing
step to train three different id56lm   s to fuse into
id4 for zh-en, tr-en and the wmt   15 transla-
tion tasks (de-en and cs-en.)

6 settings
6.1 training procedure
6.1.1 id4
the input and output of the network were se-
quences of one-hot vectors whose dimensionality
correspond to the sizes of the source and target vo-
cabularies, respectively. we constructed the vo-
cabularies with the most common words in the
parallel corpora. the sizes of the vocabularies
for chinese, turkish and english were 10k, 30k
and 40k, respectively, for the tr-en and zh-en
tasks. each word was projected into the contin-
uous space of 620-dimensional euclidean space
   rst to reduce the dimensionality, on both the en-
coder and the decoder. we chose the size of the
recurrent units for zh-en and tr-en to be 1, 200
and 1, 000 respectively.

in cs-en and de-en experiments, we were able
to use larger vocabularies. we trained our id4
model for cs-en and de-en with large vocabular-
ies using the importance sampling based technique
introduced in (jean et al., 2014) and with this tech-

# of sentences

# of unique words
# of total words

avg. length

(a) zh-en

# of sentences

# of unique words
# of total words

avg. length

(c) cs-en

chinese english

436k

21k
8.4m
19.3

150k
5.9m
13.5

czech english

12.1m

1.5m 911k
151m 172m
12.5
14.2

# of sentences

# of unique words
# of total words

avg. length

turkish english

160k

96k(cid:63)
11.4m(cid:63)

31.6

95k
8.1m
22.6

(b) tr-en

german english

4.1m

# of sentences

# of unique words
# of total words

avg. length

1.16m   
11.4m   
24.2

de-en

(d)

742k
8.1m
25.1

table 1: statistics of the parallel corpora. (cid:63): after segmentation,    : after compound splitting.

nique we were able to use large vocabulary of size
200k.

was

each

model

optimized

using
adadelta (zeiler, 2012) with minibatches of
80 sentence pairs. at each update, we normalized
the gradient such that
if the l2 norm of the
gradient exceeds 5, gradient is renormalized back
to 5 (pascanu et al., 2013). for the non-recurrent
layers (see eq. (4)), we used dropout (hinton
et al., 2012) and additive gaussian noise (mean
0 and std. dev. 0.001) on each parameter to
prevent over   tting (graves, 2011). training was
early-stopped to maximize the performance on
the development set measured by id7. we
initialized all recurrent weight matrices as random
orthonormal matrices.

6.1.2 language model
we trained three id56lm   s with 2, 400 long
short-term memory (lstm)
(hochreiter and
schmidhuber, 1997) units on english gigaword
corpus using respectively the vocabularies con-
structed separately from the english sides of zh-
en and tr-en corpora. the third language model
was trained using 2, 000 lstm units on the en-
glish gigaword corpus again but with a vocabu-
lary constructed from the intersection the english
sides of cs-en and de-en. the parameters of the
former two language models were optimized us-
ing rmsprop (tieleman and hinton, 2012), and
adam optimizer (kingma and ba, 2014) was used
for the latter one. any sentence with more than
ten percent of its words out of vocabulary was
discarded from the training set. we did early-

we compute the id7 score using the multi-blue.perl

script from moses on tokenized sentence pairs.

stopping using the perplexity of development set.

6.2 shallow and deep fusion
6.2.1 shallow fusion
the hyperparameter    in eq. (5) was selected to
maximize the translation performance on the de-
velopment set, from the range 0.001 and 0.1. in
preliminary experiments, we found it important to
renormalize the softmax of the lm without the
end-of-sequence and out-of-vocabulary symbols
(   = 0 in eq. (5)). this may be due to the differ-
ence in the domains of tm and lm.

6.2.2 deep fusion
we    netuned the parameters of the deep output
layer (eq. (6)) as well as the controller (see eq. (7)
using the adam optimizer for zh-en, and rm-
sprop with momentum for tr-en. during the    ne-
tuning, the dropout id203 and the standard
deviation of the weight noise were set to 0.56 and
0.005, respectively. based on our preliminary ex-
periments, we reduced the level of id173
after the    rst 10k updates. in cs-en and de-en
tasks with large vocabularies, the model parame-
ters were    netuned using adadelta while scaling
down the magnitude of the update steps by 0.01.

6.2.3 handling rare words
on the de-en and cs-en translation tasks, we re-
placed the unknown words generated by the id4
with the words the id4 assigned to which the
highest score in the source sentence (eq.
(2)).
we copied the selected source word in the place
of the corresponding unknown token in the target
sentence. this method is similar to the technique
proposed by (luong et al., 2014) for addressing

rare words. but instead of relying on an exter-
nal alignment tool, we used the attention mech-
anism of the id4 model to extract alignments.
this method consistently improved the results by
approximately 1.0 id7 score.

7 results and analysis
7.1 zh-en: opeid4   15
in addition to id4-based systems, we also trained
a phrase-based as well as hierarchical phrase-
based smt systems (koehn et al., 2003; chiang,
2005) with/without re-scoring by an external neu-
ral language model (cslm) (schwenk, 2007b).
we present the results in table 2.

we observed that integrating an additional lm
by deep fusion (see sec. 4.2) helped the models
achieving better performance in general, except
in the case of the cts task. we noticed that the
id4-based models, regardless of whether the lm
was integrated or not, outperformed the more tra-
ditional phrase-based smt systems.

sms/chat
dev
test
14.73
pb
15.5
15.25
+ cslm 16.02
hpb
15.33
14.71
+ cslm 15.93
15.8
17.36
id4
17.32
16.42
shallow 16.59
17.58
17.64
deep

cts

dev
21.94
23.05
21.45
22.61
23.4
22.7
23.78

test
21.68
22.79
21.43
22.17
23.59
22.83
23.5

table 2: results on the task of zh-en. pb and
hpb stand for the phrase-based and hierarchical
phrase-based smt systems, respectively.

7.2 tr-en: iwslt   14
in table 3, we present our results on tr-en. com-
pared to zh-en, we saw a greater performance im-
provement up to +1.19 id7 points from the ba-
sic id4 to the id4 integrated with the lm un-
der the proposed method of deep fusion. further-
more, by incorporating the lm using deep fusion,
the id4 systems were able to outperform the best
previously reported result (y  lmaz et al., 2013) by
up to +1.96 id7 points on all of the separate
test sets.

7.3 cs-en and de-en: wmt-15
we provide the results of cs-en and de-en on
table 4. shallow fusion achieved 0.09 and 0.29

de-en

cs-en

dev
25.51
25.53
25.88

test
23.61
23.69
24.00

dev
21.47
21.95
22.49

test
21.89
22.18
22.36

id4 baseline
shallow fusion
deep fusion

table 4: results for de-en and cs-en translation
tasks on wmt   15 dataset.

id7 score improvements respectively on de-en
and cs-en over the baseline id4 model. with
deep fusion the improvements of 0.39 and 0.47
id7 score were observed again over the id4
baseline.

7.4 analysis: effect of language model
the performance improvements we report in this
paper re   ect a heavy dependency on the degree of
similarity between the domain of monolingual cor-
pora and the target domain of translation.

in the case of zh-en, intuitively, we can tell that
the style of writing in both sms/chat as well
as the conversational speech will be different from
that of news articles (which constitutes the major-
ity of the english gigaword corpus). empirically,
this is supported by the high perplexity on the de-
velopment set with our lm (see the column zh-en
of table 5). this explains the marginal improve-
ment we observed in sec. 7.1.

on the other hand, in the case of tr-en, the sim-
ilarity between the domains of the monolingual
corpus and parallel corpora is higher (see the col-
umn tr-en of table 5). this led to a signi   cantly
larger improvement in translation performance by
integrating the external language model than the
case of zh-en. similarly, we observed the im-
provement by both shallow and deep fusion in the
case of de-en and cs-en, where the perplexity on
the development set was much lower.

unlike shallow fusion, deep fusion allows a
model to selectively incorporate the information
from the additional lm by the controller mech-
anism from sec. 4.2.1. although this controller
mechanism works on per-word basis, it can be ex-
pected that if the additional lm models the target
domain better, the controller mechanism will be
more frequently active on average, i.e., e[gt] (cid:29) 0.
from table 5, we can see that, on average, the con-
troller mechanism is most active with de-en and
cs-en, where the additional lm was able to model
the target sentences best. this effectively means

previous best (single)
previous best (combination)
id4
id4+lm (shallow)
id4+lm (deep)

development set
dev2010
tst2010
17.14
15.33
17.34
18.01
17.99
19.34

14.50
14.44
15.69

-

tst2011
18.77
18.83
18.40
18.48
20.17

test set

tst2012
18.62
18.93
18.77
18.80
20.23

-
-

tst2013 test 2014
18.88
18.70
19.86
19.87
21.34

18.64
18.66
20.56

table 3: results on tr-en. we show for each set separately to make it easier to compare to previously
reported scores.

that deep fusion allows the model to be more ro-
bust to the domain mismatch between the tm and
lm, thus suggests why deep fusion was more suc-
cessful than shallow fusion in the experiments.

perplexity
average g
std dev g

zh-en
223.68
0.23
0.0009

tr-en de-en cs-en
78.20
163.73
0.31
0.12
0.02
0.008

78.20
0.28
0.003

table 5: perplexity of id56lm   s on develop-
ment sets and the statistics of the controller gating
mechanism g.

8 conclusion and future work
in this paper, we propose and compare two meth-
ods for incorporating monolingual corpora into
an existing id4 system. we empirically eval-
uate these approaches (shallow fusion and deep
fusion) on low-resource en-tr (ted/tedx sub-
titles), focused domain for en-zh (sms/chat and
conversational speech) and two high-resource lan-
guage pairs: cs-en and de-en. we show that with
our approach on the tr-en and zh-en language
pairs, the id4 models trained with deep fusion
were able to achieve better results than the ex-
isting phrase-based id151
systems (up to a +1.96 id7 points on en-tr).
we also observed up to a 0.47 id7 score im-
provement for high resource language pairs such
as de-en and cs-en on the datasets provided in
wmt   15 competition over our id4 baseline.
this provides an evidence that our method can
also improve the translation performance regard-
less of the amount of available parallel corpora.

our analysis also revealed that the performance
improvement from incorporating an external lm
was highly dependent on the domain similarity be-
tween the monolingual corpus and the target task.
in the case where the domain of the bilingual and

monolingual corpora were similar (de-en, cs-
en), we observed improvement with both deep
and shallow fusion methods.
in the case where
they were dissimilar (zh-en), the improvement us-
ing shallow fusion were much smaller. this trend
might also explain why deep fusion, which im-
plements an adaptive mechanism for modulating
information from the integrated lm, works better
than shallow fusion. this analysis also suggests
that future work on domain adaption of the lan-
guage model may further improve translations.

references
[bahdanau et al.2014] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2014. neural machine
translation by jointly learning to align and translate.
arxiv preprint arxiv:1409.0473.

[cettolo et al.2012] mauro cettolo, christian girardi,
and marcello federico. 2012. wit3: web inven-
tory of transcribed and translated talks. proceedings
of the 16th conference of the european association
for machine translation (eamt), pages 261   268.

[chiang2005] david chiang.

2005. a hierarchical
phrase-based model for statistical machine transla-
in proceedings of the 43rd annual meeting
tion.
on association for computational linguistics, pages
263   270. association for computational linguis-
tics.

[cho et al.2014] kyunghyun cho, bart van merrien-
boer, caglar gulcehre, fethi bougares, holger
schwenk, and yoshua bengio.
2014. learning
phrase representations using id56 encoder-decoder
for id151. in proceedings of
the empiricial methods in natural language pro-
cessing (emnlp 2014), october. to appear.

rabih

[devlin et al.2014] jacob devlin,

zbib,
zhongqiang huang, thomas lamar, richard
schwartz, and john makhoul.
fast and
robust neural network joint models for statistical
in association for computa-
machine translation.
tional linguistics.

2014.

[goodfellow et al.2013] ian goodfellow, david warde-
farley, mehdi mirza, aaron courville, and yoshua
bengio. 2013. maxout networks. in proceedings
of the 30th international conference on machine
learning, pages 1319   1327.

[pascanu et al.2014] r. pascanu, c. gulcehre, k. cho,
and y. bengio. 2014. how to construct deep recur-
rent neural networks. in proceedings of the second
international conference on learning representa-
tions (iclr 2014), april.

[sak et al.2007] has  im sak, tunga g  ung  or, and murat
sarac  lar. 2007. id60 of
turkish text with id88 algorithm. in compu-
tational linguistics and intelligent text processing,
pages 107   118. springer.

[schuster and paliwal1997] mike

kuldip k paliwal.
rent neural networks.
transactions on, 45(11):2673   2681.

schuster

and
1997. bidirectional recur-
signal processing, ieee

[schwenk2007a] holger schwenk. 2007a. continu-
ous space language models. comput. speech lang.,
21(3):492   518, july.

[schwenk2007b] holger schwenk. 2007b. continuous
space language models. computer speech & lan-
guage, 21(3):492   518.

[schwenk2012] holger schwenk. 2012. continuous
space translation models for phrase-based statisti-
cal machine translation. in martin kay and chris-
tian boitet, editors, proceedings of the 24th in-
ternational conference on computational linguis-
tics (colin), pages 1071   1080. indian institute of
technology bombay.

[shuyo2010] nakatani shuyo. 2010. language detec-

tion library for java.

[sutskever et al.2014] ilya sutskever, oriol vinyals,
and quoc le. 2014. sequence to sequence learning
with neural networks. in advances in neural infor-
mation processing systems (nips 2014), december.

[tieleman and hinton2012] tijmen tieleman and ge-
offrey hinton. 2012. lecture 6.5-rmsprop: divide
the gradient by a running average of its recent mag-
nitude. coursera: neural networks for machine
learning, 4.

[y  lmaz et al.2013] ertugrul y  lmaz, ilknur durgar el-
kahlout, burak ayd  n, zisan s  la ozil, and coskun
mermer. 2013. tubitak turkish-english submissions
for iwslt 2013. proceedings of the 10th interna-
tional workshop on spoken language translation
(iwslt), pages 152   159.

[zeiler2012] matthew

2012.
adadelta: an adaptive learning rate method.
arxiv:1212.5701 [cs.lg].

zeiler.

d

[graves2011] alex graves. 2011. practical variational
id136 for neural networks. in advances in neu-
ral information processing systems, pages 2348   
2356.

[hinton et al.2012] geoffrey e hinton, nitish srivas-
tava, alex krizhevsky, ilya sutskever, and rus-
lan r salakhutdinov. 2012. improving neural net-
works by preventing co-adaptation of feature detec-
tors. arxiv preprint arxiv:1207.0580.

[hochreiter and schmidhuber1997] sepp hochreiter
and j  urgen schmidhuber. 1997. long short-term
memory. neural computation, 9(8):1735   1780.

[jean et al.2014] s  ebastien jean, kyunghyun cho,
roland memisevic, and yoshua bengio. 2014. on
using very large target vocabulary for neural ma-
chine translation. arxiv preprint arxiv:1412.2007.

[kalchbrenner and blunsom2013] nal kalchbrenner
and phil blunsom. 2013. recurrent continuous
in proceedings of the acl
translation models.
conference on empirical methods
in natural
language processing (emnlp), pages 1700   1709.
association for computational linguistics.

[kingma and ba2014] diederik p. kingma and jimmy
ba. 2014. adam: a method for stochastic opti-
mization. arxiv:1412.6980 [cs.lg], decem-
ber.

[koehn et al.2003] philipp koehn, franz josef och,
and daniel marcu. 2003. statistical phrase-based
translation. in proceedings of the 2003 conference
of the north american chapter of the association
for computational linguistics on human language
technology-volume 1, pages 48   54. association for
computational linguistics.

[koehn2010] philipp koehn.

statistical ma-
chine translation. cambridge university press,
new york, ny, usa.

2010.

[luong et al.2014] thang luong,

ilya sutskever,
quoc v le, oriol vinyals, and wojciech zaremba.
addressing the rare word problem in
2014.
arxiv preprint
id4.
arxiv:1410.8206.

[mikolov et al.2011] tomas mikolov, stefan kom-
brink, anoop deoras, lukar burget, and jan cer-
nocky. 2011. id56lm-recurrent neural network lan-
guage modeling toolkit. in proc. of the 2011 asru
workshop, pages 196   201.

[pascanu et al.2013] razvan pascanu, tomas mikolov,
and yoshua bengio. 2013. on the dif   culty of train-
ing recurrent neural networks. in proceedings of the
30th international conference on machine learning
(icml 2013).

