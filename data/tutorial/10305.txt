6
1
0
2

 
r
a

 

m
2
2

 
 
]
l
c
.
s
c
[
 
 

4
v
8
4
5
0
0

.

4
0
5
1
:
v
i
x
r
a

learning to understand phrases by embedding the dictionary

felix hill

computer laboratory

university of cambridge

felix.hill@cl.cam.ac.uk

kyunghyun cho   

courant institute of mathematical sciences

and centre for data science

new york university

kyunghyun.cho@nyu.edu

anna korhonen

department of theoretical and applied linguistics

yoshua bengio

cifar senior fellow
universit  e de montr  eal

university of cambridge
alk23@cam.ac.uk

abstract

distributional models that learn rich seman-
tic word representations are a success story
of recent nlp research. however, develop-
ing models that learn useful representations of
phrases and sentences has proved far harder.
we propose using the de   nitions found in
everyday dictionaries as a means of bridg-
ing this gap between lexical and phrasal se-
mantics. neural language embedding mod-
els can be effectively trained to map dictio-
nary de   nitions (phrases) to (lexical) repre-
sentations of the words de   ned by those de   -
nitions. we present two applications of these
architectures: reverse dictionaries that return
the name of a concept given a de   nition or
description and general-knowledge crossword
question answerers. on both tasks, neural lan-
guage embedding models trained on de   ni-
tions from a handful of freely-available lex-
ical resources perform as well or better than
existing commercial systems that rely on sig-
ni   cant task-speci   c engineering. the re-
sults highlight the effectiveness of both neu-
ral embedding architectures and de   nition-
based training for developing models that un-
derstand phrases and sentences.

1 introduction

much recent
research in computational seman-
tics has focussed on learning representations of
arbitrary-length phrases and sentences. this task is
challenging partly because there is no obvious gold
standard of phrasal representation that could be used

    work mainly done at the university of montreal.

yoshua.bengio@umontreal.ca

in training and evaluation. consequently, it is dif   -
cult to design approaches that could learn from such
a gold standard, and also hard to evaluate or compare
different models.

in this work, we use dictionary de   nitions to ad-
dress this issue. the composed meaning of the
words in a dictionary de   nition (a tall, long-necked,
spotted ruminant of africa) should correspond to
the meaning of the word they de   ne (giraffe). this
bridge between lexical and phrasal semantics is use-
ful because high quality vector representations of
single words can be used as a target when learning
to combine the words into a coherent phrasal repre-
sentation.

this approach still requires a model capable of
learning to map between arbitrary-length phrases
and    xed-length continuous-valued word vectors.
for this purpose we experiment with two broad
classes of neural language models (nlms): recur-
rent neural networks (id56s), which naturally en-
code the order of input words, and simpler (feed-
forward) bag-of-words (bow) embedding models.
prior to training these nlms, we learn target lexi-
cal representations by training the id97 soft-
ware (mikolov et al., 2013) on billions of words of
raw text.

we demonstrate the usefulness of our approach
by building and releasing two applications. the
   rst is a reverse dictionary or concept    nder: a sys-
tem that returns words based on user descriptions
or de   nitions (zock and bilac, 2004). reverse dic-
tionaries are used by copywriters, novelists, trans-
lators and other professional writers to    nd words
for notions or ideas that might be on the tip of their

tongue. for instance, a travel-writer might look to
enhance her prose by searching for examples of a
country that people associate with warm weather or
an activity that is mentally or physically demand-
ing. we show that an nlm-based reverse dictionary
trained on only a handful of dictionaries identi   es
novel de   nitions and concept descriptions compara-
bly or better than commercial systems, which rely
on signi   cant task-speci   c engineering and access
to much more dictionary data. moreover, by ex-
ploiting models that learn bilingual word represen-
tations (vulic et al., 2011; klementiev et al., 2012;
hermann and blunsom, 2013; gouws et al., 2014),
we show that the nlm approach can be easily ex-
tended to produce a potentially useful cross-lingual
reverse dictionary.

the second application of our models is as a
general-knowledge crossword question answerer.
when trained on both dictionary de   nitions and the
opening sentences of wikipedia articles, nlms pro-
duce plausible answers to (non-cryptic) crossword
clues, even those that apparently require detailed
world knowledge. both bow and id56 models can
outperform bespoke commercial crossword solvers,
particularly when clues contain a greater number of
words. qualitative analysis reveals that nlms can
learn to relate concepts that are not directly con-
nected in the training data and can thus generalise
well to unseen input. to facilitate further research,
all of our code, training and evaluation sets (together
with a system demo) are published online with this
paper.1

2 neural language model architectures

in our case, natural

the    rst model we apply to the dictionary-based
learning task is a recurrent neural network (id56).
id56s operate on variable-length sequences of in-
puts;
language de   nitions,
descriptions or sentences. id56s (with lstms)
have achieved state-of-the-art performance in lan-
guage modelling (mikolov et al., 2010), image cap-
tion generation (kiros et al., 2015) and approach
state-of-the-art performance in machine transla-
tion (bahdanau et al., 2015).

during training, the input to the id56 is a dic-
tionary de   nition or sentence from an encyclope-

1 https://www.cl.cam.ac.uk/  fh295/

dia. the objective of the model is to map these
de   ning phrases or sentences to an embedding of
the word that
the tar-
get id27s are learned independently
of the id56 weights, using the id97 soft-
ware (mikolov et al., 2013).

the de   nition de   nes.

the set of all words in the training data consti-
tutes the vocabulary of the id56. for each word in
this vocabulary we randomly initialise a real-valued
vector (input embedding) of model parameters. the
id56    reads    the    rst word in the input by applying
a non-linear projection of its embedding v1 parame-
terised by input weight matrix w and b, a vector of
biases.

a1 =   (w v1 + b)

yielding the    rst internal activation state a1. in our
implementation, we use   (x) = tanh(x), though in
theory    can be any differentiable non-linear func-
tion. subsequent internal activations (after time-step
t) are computed by projecting the embedding of the
tth word and using this information to    update    the
internal activation state.

at =   (u at   1 + w vt + b).

as such, the values of the    nal internal activation
state units an are a weighted function of all input
id27s, and constitute a    summary    of the
information in the sentence.

2.1 long short term memory
a known limitation when training id56s to read lan-
guage using id119 is that the error sig-
nal (gradient) on the training examples either van-
ishes or explodes as the number of time steps (sen-
tence length) increases (bengio et al., 1994). con-
sequently, after reading longer sentences the    nal
internal activation an typically retains useful in-
formation about the most recently read (sentence-
   nal) words, but can neglect important informa-
tion near the start of the input sentence. lstms
(hochreiter and schmidhuber, 1997) were designed
to mitigate this long-term dependency problem.

at each time step t, in place of the single inter-
nal layer of units a, the lstm id56 computes six
internal layers iw, gi, gf , go, h and m. the    rst, gw,
represents the core information passed to the lstm

unit by the latest input word at t. it is computed as
a simple linear projection of the input embedding
vt (by input weights ww) and the output state of
the lstm at the previous time step ht   1 (by update
weights uw):

iw
t = wwvt + uwht   1 + bw

the layers gi, gf and go are computed as weighted
sigmoid functions of the input embeddings, again
parameterised by layer-speci   c weight matrices w
and u:

gs
t =

1

1 + exp(   (wsvt + usht   1 + bs))

where s stands for one of i, f or o. these vectors
take values on [0, 1] and are often referred to as gat-
ing activations. finally, the internal memory state,
mt and new output state ht, of the lstm at t are
computed as

mt =iw
ht =go

t     gi
t       (mt),

t + mt   1     gf

t

where     indicates elementwise vector multiplica-
tion and    is, as before, some non-linear function
(we use tanh). thus, gi determines to what extent
the new input word is considered at each time step,
gf determines to what extent the existing state of
the internal memory is retained or forgotten in com-
puting the new internal memory, and go determines
how much this memory is considered when comput-
ing the output state at t.

the sentence-   nal memory state of the lstm,
mn , a    summary    of all the information in the sen-
tence, is then projected via an extra non-linear pro-
jection (parameterised by a further weight matrix)
to a target embedding space. this layer enables the
target (de   ned) id27 space to take a dif-
ferent dimension to the activation layers of the id56,
and in principle enables a more complex de   nition-
reading function to be learned.

the bow model simply maps an input de   nition
with id27s v1 . . . vn to the sum of the
projected embeddings pn
i=1 w vi. this model can
also be considered a special case of an id56 in
which the update function u and nonlinearity    are
both the identity, so that    reading    the next word in
the input phrase updates the current representation
more simply:

at = at   1 + w vt.

2.3 pre-trained input representations

we experiment with variants of these models in
which the input de   nition embeddings are pre-
learned and    xed (rather than randomly-initialised
and updated) during training. there are several po-
tential advantages to taking this approach. first, the
id27s are trained on massive corpora
and may therefore introduce additional linguistic or
conceptual knowledge to the models. second, at test
time, the models will have a larger effective vocab-
ulary, since the pre-trained id27s typi-
cally span a larger vocabulary than the union of all
dictionary de   nitions used to train the model. fi-
nally, the models will then map to and from the same
space of embeddings (the embedding space will be
closed under the operation of the model), so con-
ceivably could be more easily applied as a general-
purpose    composition engine   .

2.4 training objective

we train all neural language models m to map the
input de   nition phrase sc de   ning word c to a lo-
cation close to the the pre-trained embedding vc of
c. we experiment with two different cost functions
for the word-phrase pair (c, sc) from the training
data. the    rst is simply the cosine distance between
m (sc) and vc. the second is the rank loss

max(0, m     cos(m (sc), vc)     cos(m (sc), vr))

2.2 bag-of-words nlms
we implement a simpler linear bag-of-words (bow)
architecture for encoding the de   nition phrases. as
with the id56, this architecture learns an embedding
vi for each word in the model vocabulary, together
with a single matrix of input projection weights w .

where vr is the embedding of a randomly-selected
word from the vocabulary other than c. this loss
function was used for language models, for example,
in (huang et al., 2012). in all experiments we apply
a margin m = 0.1, which has been shown to work
well on word-retrieval tasks (bordes et al., 2015).

2.5

implementation details

3 reverse dictionaries

since training on the dictionary data took 6-10
hours, we did not conduct a hyper-parameter search
on any validation sets over the space of possible
model con   gurations such as embedding dimension,
or size of hidden layers.
instead, we chose these
parameters to be as standard as possible based on
previous research. for fair comparison, any aspects
of model design that are not speci   c to a particu-
lar class of model were kept constant across experi-
ments.

the pre-trained id27s used in all of
our models (either as input or target) were learned
by a continuous bag-of-words (cbow) model using
the id97 software on approximately 8 billion
words of running text.2 when training such models
on massive corpora, a large embedding length of up
to 700 have been shown to yield best performance
(see e.g. (faruqui et al., 2014)). the pre-trained em-
beddings used in our models were of length 500,
as a compromise between quality and memory con-
straints.

in cases where the id27s are learned
during training on the dictionary objective, we make
these embeddings shorter (256), since they must
be learned from much less language data.
in the
id56 models, and at each time step each of the
four lstm id56 internal layers (gating and activa-
tion states) had length 512     another standard choice
(see e.g. (cho et al., 2014)). the    nal hidden state
was mapped linearly to length 500, the dimension
of the target embedding. in the bow models, the
projection matrix projects input embeddings (either
learned, of length 256, or pre-trained, of length 500)
to length 500 for summing.
were

with
theano (bergstra et al., 2010) and trained with
minibatch sgd on gpus. the batch size was
   xed at 16 and the learning rate was controlled by
adadelta (zeiler, 2012).

implemented

models

all

2the

further

known;

id97

embedding

are
well
found
at https://code.google.com/p/id97/ the
training data for this pre-training was compiled from various
online text sources using the script demo-train-big-model-v1.sh
from the same page.

models
be

details

can

the most immediate application of our trained mod-
els is as a reverse dictionary or concept    nder.
it
is simple to look up a de   nition in a dictionary
given a word, but professional writers often also re-
quire suitable words for a given idea, concept or
de   nition.3 reverse dictionaries satisfy this need
by returning candidate words given a phrase, de-
scription or de   nition. for instance, when queried
with the phrase an activity that requires strength
and determination, the onelook.com reverse dictio-
nary returns the concepts exercise and work. our
trained id56 model can perform a similar func-
tion, simply by mapping a phrase to a point in the
target (id97) embedding space, and returning
the words corresponding to the embeddings that are
closest to that point.

several other academic studies have proposed
reverse dictionary models. these generally rely
on common techniques from information retrieval,
comparing de   nitions in their internal database to
the input query, and returning the word whose def-
inition is    closest    to that query (bilac et al., 2003;
bilac et al., 2004; zock and bilac, 2004). proxim-
ity is quanti   ed differently in each case, but is gen-
erally a function of hand-engineered features of the
two sentences. for instance, shaw et al. (2013) pro-
pose a method in which the candidates for a given
input query are all words in the model   s database
whose de   nitions contain one or more words from
the query. this candidate list is then ranked accord-
ing to a query-de   nition similarity metric based on
the hypernym and hyponym relations in id138,
features commonly used in ir such as tf-idf and a
parser.

there are, in addition, at least two commercial
online reverse dictionary applications, whose ar-
chitecture is proprietary knowledge. the    rst is
the dictionary.com reverse dictionary 4, which re-
trieves candidate words from the dictionary.com
dictionary based on user de   nitions or descrip-
tions. the second is onelook.com, whose algo-
rithm searches 1061 indexed dictionaries, including
all major freely-available online dictionaries and re-

3see

the

testimony

from professional writers

at

http://www.onelook.com/?c=awards

4available at http://dictionary.reference.com/reverse/

sources such as wikipedia and id138.

3.2 comparisons

3.1 data collection and training

to compile a bank of dictionary de   nitions for train-
ing the model, we started with all words in the tar-
get embedding space. for each of these words, we
extracted dictionary-style de   nitions from    ve elec-
tronic resources: id138, the american heritage
dictionary, the collaborative international dictio-
nary of english, wiktionary and webster   s. we
chose these    ve dictionaries because they are freely-
available via the wordnik api,5 but in theory any
dictionary could be chosen. most words in our train-
ing data had multiple de   nitions. for each word
w with de   nitions {d1 . . . dn} we included all pairs
(w, d1) . . . (w, dn) as training examples.

to allow models access to more factual knowl-
edge than might be present in a dictionary (for in-
stance, information about speci   c entities, places or
people, we supplemented this training data with in-
formation extracted from simple wikipedia. 6 for
every word in the model   s target embedding space
that is also the title of a wikipedia article, we treat
the sentences in the    rst paragraph of the article as
if they were (independent) de   nitions of that word.
when a word in wikipedia also occurs in one (or
more) of the    ve training dictionaries, we simply
add these pseudo-de   nitions to the training set of
de   nitions for the word. combining wikipedia and
dictionaries in this way resulted in     900, 000 word-
   de   nition    pairs of     100, 000 unique words.

to explore the effect of the quantity of training
data on the performance of the models, we also
trained models on subsets of this data. the    rst sub-
set comprised only de   nitions from id138 (ap-
proximately 150,000 de   nitions of 75,000 words).
the second subset comprised only words in word-
net and their    rst de   nitions (approximately 75,000
word, de   nition pairs).7. for all variants of id56
and bow models, however, reducing the training
data in this way resulted in a clear reduction in per-
formance on all tasks. for brevity, we therefore do
not present these results in what follows.

5see http://developer.wordnik.com
6https://simple.wikipedia.org/wiki/main_page
7as with other dictionaries, the    rst de   nition in id138
generally corresponds to the most typical or common sense of a
word.

as a baseline, we also implemented two entirely
unsupervised methods using the neural (id97)
id27s from the target word space. in the
   rst (w2v add), we compose the embeddings for
each word in the input query by pointwise addition,
and return as candidates the nearest word embed-
dings to the resulting composed vector.8 the sec-
ond baseline, (w2v mult), is identical except that
the embeddings are composed by elementwise mul-
tiplication. both methods are established ways of
building phrase representations from word embed-
dings (mitchell and lapata, 2010).

none of the models or evaluations from previous
academic research on reverse dictionaries is pub-
licly available, so direct comparison is not possi-
ble. however, we do compare performance with
the commercial systems. the dictionary.com sys-
tem returned no candidates for over 96% of our in-
put de   nitions. we therefore conduct detailed com-
parison with onelook.com, which is the    rst re-
verse dictionary tool returned by a google search
and seems to be the most popular among writers.

3.3 reverse dictionary evaluation

to our knowledge there are no established means of
measuring reverse dictionary performance.
in the
only previous academic research on english reverse
dictionaries that we are aware of, evaluation was
conducted on 300 word-de   nition pairs written by
lexicographers (shaw et al., 2013). since these are
not publicly available we developed new evaluation
sets and make them freely available for future eval-
uations.

the evaluation items are of three types, designed
to test different properties of the models. to cre-
ate the seen evaluation, we randomly selected 500
words from the id138 training data (seen by all
models), and then randomly selected a de   nition for
each word. testing models on the resulting 500
word-de   nition pairs assesses their ability to recall
or decode previously encoded information. for the
unseen evaluation, we randomly selected 500 words
from id138 and excluded all de   nitions of these

8since we retrieve all answers from embedding spaces by
cosine similarity, addition of id27s is equivalent to
taking the mean.

dictionary de   nitions

test set

unsup.
models

w2v add
w2v mult
onelook
id56 cosine
id56 w2v cosine
id56 ranking
nlms id56 w2v ranking
bow cosine
bow w2v cosine
bow ranking
bow w2v rankng

-

-
-

163
10*

923
1000

.04/.16
.00/.00

seen (500 wn defs) unseen (500 wn defs) concept descriptions (200)
-
-
0
12
19
18
54
22
15
17
55

.07/.30
.00/.00
.38/.58
.28/.54
.38/.66
.34/.66
.33/.69
.34/.60
.36/.66
.35/.69
.33/.72

.89/.91
.48/.73
.44/.70
.45/.67
.32/.56
.44/.65
.46/.71
.45/.68
.32/.56

.41/.70
.44/.69
.43/.69
.36/.65
.43/.69
.46/ .71
.42/.70
.35/.66

339
1000
18.5
69
26
25
30
50
28
32
38

150
27*
153
157
111
102
77
99
99
101
85

-
-
67
103
111
128
155
129
124
115
155

116
126
103
137
103
104
95
138

-
22
19
24
33
19
14
22
36

-

table 1: performance of different reverse dictionary models in different evaluation settings. *low variance in mult
models is due to consistently poor scores, so not highlighted.

median rank

accuracy@10/100

rank variance

words from the training data of all models.

finally, for a fair comparison with onelook,
which has both the seen and unseen pairs in its in-
ternal database, we built a new dataset of concept
descriptions that do not appear in the training data
for any model. to do so, we randomly selected 200
adjectives, nouns or verbs from among the top 3000
most frequent tokens in the british national cor-
pus (leech et al., 1994) (but outside the top 100).
we then asked ten native english speakers to write
a single-sentence    description    of these words. to
ensure the resulting descriptions were good qual-
ity, for each description we asked two participants
who did not produce that description to list any
words that    tted the description (up to a maximum
of three).
if the target word was not produced by
one of the two checkers, the original participant was
asked to re-write the description until the validation
was passed.9 these concept descriptions, together
with other evaluation sets, can be downloaded from
our website for future comparisons.

test set word description

dictionary
de   nition
concept
description

valve

prefer

   control consisting of a mechanical
device for controlling    uid    ow   
   when you like one thing
more than another thing   

table 2: style difference between dictionary de   nitions
and concept descriptions in the evaluation.

9re-writing was required in 6 of the 200 cases.

given a test description, de   nition, or question,
all models produce a ranking of possible word an-
swers based on the proximity of their representations
of the input phrase and all possible output words.
to quantify the quality of a given ranking, we re-
port three statistics: the median rank of the correct
answer (over the whole test set, lower better), the
proportion of training cases in which the correct an-
swer appears in the top 10/100 in this ranking (accu-
racy@10/100 - higher better) and the variance of the
rank of the correct answer across the test set (rank
variance - lower better).

3.4 results

table 1 shows the performance of the different mod-
els in the three evaluation settings. of the unsu-
pervised composition models, elementwise addition
is clearly more effective than multiplication, which
almost never returns the correct word as the near-
est neighbour of the composition. overall, however,
the supervised models (id56, bow and onelook)
clearly outperform these baselines.

the results indicate interesting differences be-
tween the nlms and the onelook dictionary search
engine. the seen (wn    rst) de   nitions in table 1
occur in both the training data for the nlms and
the lookup data for the onelook model. clearly the
onelook algorithm is better than nlms at retriev-
ing already available information (returning 89% of
correct words among the top-ten candidates on this

set). however, this is likely to come at the cost of a
greater memory footprint, since the model requires
access to its database of dictionaries at query time.10
the performance of the nlm embedding models
on the (unseen) concept descriptions task shows that
these models can generalise well to novel, unseen
queries. while the median rank for onelook on
this evaluation is lower, the nlms retrieve the cor-
rect answer in the top ten candidates approximately
as frequently, within the top 100 candidates more
frequently and with lower variance in ranking over
the test set. thus, nlms seem to generalise more
   consistenly    than onelook on this dataset, in that
they generally assign a reasonably high ranking to
the correct word. in contrast, as can also be veri   ed
by querying our we demo, onelook tends to per-
form either very well or poorly on a given query.11
when comparing between nlms, perhaps the
most striking observation is that the id56 models
do not signi   cantly outperform the bow models,
even though the bow model output is invariant to
changes in the order of words in the de   nition. users
of the online demo can verify that the bow models
recover concepts from descriptions strikingly well,
even when the words in the description are per-
muted. this observation underlines the importance
of lexical semantics in the interpretation of language
by nlms, and is consistent with some other recent
work on embedding sentences (iyyer et al., 2015).

it is dif   cult to observe clear trends in the dif-
ferences between nlms that learn input word em-
beddings and those with pre-trained (id97) in-
put embeddings. both types of input yield good
performance in some situations and weaker perfor-
mance in others. in general, pre-training input em-
beddings seems to help most on the concept de-
scriptions, which are furthest from the training data
in terms of linguistic style. this is perhaps unsur-
prising, since models that learn input embeddings
from the dictionary data acquire all of their concep-
tual knowledge from this data (and thus may over-
   t to this setting), whereas models with pre-trained

10the trained neural language models are approximately half
the size of the six training dictionaries stored as plain text, so
would be hundreds of times smaller than the onelook database
of 1061 dictionaries if stored this way.

11we also observed that the mean ranking for nlms was

lower than for onelook on the concept descriptions task.

embeddings have some semantic memory acquired
from general running-text language data and other
knowledge acquired from the dictionaries.

3.5 qualitative analysis
some example output from the various models is
presented in table 3. the differences illustrated
here are also evident from querying the web demo.
the    rst example shows how the nlms (bow and
id56) generalise beyond their training data. four
of the top    ve responses could be classed as ap-
propriate in that they refer to inhabitants of cold
countries. however, inspecting the wordnik train-
ing data, there is no mention of cold or anything to
do with climate in the de   nitions of eskimo, scandi-
navian, scandinavia etc. therefore, the embedding
models must have learned that coldness is a char-
acteristic of scandinavia, siberia, russia, relates to
eskimos etc. via connections with other concepts
that are described or de   ned as cold.
in contrast,
the candidates produced by the onelook and (unsu-
pervised) w2v baseline models have nothing to do
with coldness.

the second example demonstrates how the nlms
generally return candidates whose linguistic or con-
ceptual function is appropriate to the query. for a
query referring explicitly to a means, method or pro-
cess, the id56 and bow models produce verbs in
different forms or an appropriate deverbal noun. in
contrast, onelook returns words of all types (aero-
dynamics, draught) that are arbitrarily related to the
words in the query. a similar effect is apparent in
the third example. while the candidates produced
by the onelook model are the correct part of speech
(noun), and related to the query topic, they are not
semantically appropriate. the dictionary embedding
models are the only ones that return a list of plausi-
ble habits, the class of noun requested by the input.

3.6 cross-lingual reverse dictionaries
we now show how the id56 architecture can be eas-
ily modi   ed to create a bilingual reverse dictionary
- a system that returns candidate words in one lan-
guage given a description or de   nition in another.
a bilingual reverse dictionary could have clear ap-
plications for translators or transcribers. indeed, the
problem of attaching appropriate words to concepts
may be more common when searching for words in

input
description

   a native of
a cold
country   

   a way of
moving
through
the air   

   a habit that
might annoy
your spouse   

onelook

w2v add

id56

1:country 2:citizen

3:foreign 4:naturalize

5:cisco

1:drag 2:whiz

3:aerodynamics 4:draught

5:coef   cient of drag

1:a 2.the

1:eskimo 2:scandinavian

3:another 4:of

5:whole

1:the 2:through
3:a 4:moving

5:in

3:arctic 4:indian

5:siberian

1:glide 2:scooting
3:glides 4:gliding

5:   ight

bow

1:frigid 2:cold
3:icy 4:russian

5:indian

1:   ying 2:gliding

3:glide 4:   y
5:scooting

1:sisterinlaw 2:fatherinlaw 1:annoy 2:your
3:motherinlaw 4:stepson
3:might 4:that

1:bossiness 2:jealousy
3:annoyance 4:rudeness

1:in   delity 2:bossiness
3:foible 4:unfaithfulness

5:stepchild

5:either

5:boorishness

5:adulterous

table 3: the top-   ve candidates for example queries (invented by the authors) from different reverse dictionary mod-
els. both the id56 and bow models are without id97 input and use the cosine loss.

input description
   an emotion that you might feel
after being rejected   

id56 en-fr
triste, pitoyable

w2v add

id56 + google

insister, effectivement

sentiment, regretter

r  epugnante,   epouvantable

pourquoi, nous

peur, aversion

   a small black    ying insect that
transmits disease and likes horses   

mouche, canard
hirondelle, pigeon

attentivement, pouvions
pourrons, naturellement

voler, faucon
mouches, volant

table 4: responses from cross-lingual reverse dictionary models to selected queries. underlined responses are    cor-
rect    or potentially useful for a native french speaker.

a second language than in a monolingual context.

together

such that words

to create the bilingual variant, we simply
replace the id97 target embeddings with
those from a bilingual embedding space. bilin-
gual embedding models use bilingual corpora
to learn a space of representations of the words
in two languages,
from ei-
language that have similar meanings are
ther
close
(hermann and blunsom, 2013;
chandar et al., 2014; gouws et al., 2014).
for
a test-of-concept experiment, we used english-
french embeddings learned by the state-of-the-art
bilbowa model
from the
wikipedia (monolingual) and europarl (bilingual)
corpora.12 we trained the id56 model to map
from english de   nitions to english words in the
bilingual space. at test
time, after reading an
english de   nition, we then simply return the nearest
french word neighbours to that de   nition.

(gouws et al., 2014)

because no benchmarks exist for quantitative
evaluation of bilingual reverse dictionaries, we com-

12the approach should work with any bilingual embeddings.

we thank stephan gouws for doing the training.

pare this approach qualitatively with two alternative
methods for mapping de   nitions to words across
languages. the    rst is analogous to the w2v add
model of the previous section: in the bilingual em-
bedding space, we    rst compose the embeddings of
the english words in the query de   nition with ele-
mentwise addition, and then return the french word
whose embedding is nearest to this vector sum. the
second uses the id56 monolingual reverse dictio-
nary model to identify an english word from an en-
glish de   nition, and then translates that word using
google translate.

table 4 shows that the id56 model can be ef-
fectively modi   ed to create a cross-lingual reverse
dictionary. it is perhaps unsurprising that the w2v
add model candidates are generally the lowest in
quality given the performance of the method in the
monolingual setting.
in comparing the two id56-
based methods, the id56 (embedding space) model
appears to have two advantages over the id56 +
google approach. first, it does not require on-
line access to a bilingual word-word mapping as
de   ned e.g. by google translate. second, it less

prone to errors caused by word sense ambiguity.
for example, in response to the query an emotion
you feel after being rejected, the bilingual embed-
ding id56 returns emotions or adjectives describing
mental states. in contrast, the monolingual+google
model incorrectly maps the plausible english re-
sponse regret to the verbal in   nitive regretter. the
model makes the same error when responding to a
description of a    y, returning the verb voler (to    y).

3.7 discussion

we have shown that simply training id56 or bow
nlms on six dictionaries yields a reverse dictionary
that performs comparably to the leading commer-
cial system, even with access to much less dictio-
nary data.
indeed, the embedding models consis-
tently return syntactically and semantically plausi-
ble responses, which are generally part of a more
coherent and homogeneous set of candidates than
those produced by the commercial systems. we also
showed how the architecture can be easily extended
to produce bilingual versions of the same model.

in the analyses performed thus far, we only test
the dictionary embedding approach on tasks that it
was trained to accomplish (mapping de   nitions or
descriptions to words). in the next section, we ex-
plore whether the knowledge learned by dictionary
embedding models can be effectively transferred to
a novel task.

4 general knowledge (crossword)

id53

the automatic answering of questions posed in nat-
ural language is a central problem of arti   cial in-
telligence. although web search and ir techniques
provide a means to    nd sites or documents related to
language queries, at present, internet users requiring
a speci   c fact must still sift through pages to locate
the desired information.

systems that attempt

to overcome this, via
fully open-domain or general knowledge question-
answering (open qa), generally require large
teams of researchers, modular design and pow-
erful
infrastructure, exempli   ed by ibm   s wat-
son (ferrucci et al., 2010). for this reason, much
academic research focuses on settings in which
the scope of
this has

the task is reduced.

been achieved by restricting questions to a spe-
ci   c topic or domain (moll  a and vicedo, 2007),
allowing systems access
to pre-speci   ed pas-
sages of text from which the answer can be in-
ferred (iyyer et al., 2014; weston et al., 2015), or
centering both questions and answers on a par-
ticular knowledge base (berant and liang, 2014;
bordes et al., 2014).

in what follows, we show that the dictionary em-
bedding models introduced in the previous sections
may form a useful component of an open qa sys-
tem. given the absence of a knowledge base or
web-scale information in our architecture, we nar-
row the scope of the task by focusing on general
knowledge crossword questions. general knowl-
edge (non-cryptic, or quick) crosswords appear in
national newspapers in many countries. crossword
id53 is more tractable than general
open qa for two reasons. first, models know the
length of the correct answer (in letters), reducing
the search space. second, some crossword questions
mirror de   nitions, in that they refer to fundamental
properties of concepts (a twelve-sided shape) or re-
quest a category member (a city in egypt).13

4.1 evaluation
general knowledge crossword questions come in
different styles and forms. we used the eddie james
crossword website to compile a bank of sentence-
like general-knowledge questions.14 eddie james is
one of the uk   s leading crossword compilers, work-
ing for several national newspapers. our long ques-
tion set consists of the    rst 150 questions (starting
from puzzle #1) from his general-knowledge cross-
words, excluding clues of fewer than four words
and those whose answer was not a single word (e.g.
kingjames).

to evaluate models on a different type of clue, we
also compiled a set of shorter questions based on
the guardian quick crossword. guardian questions
still require general factual or linguistic knowledge,
but are generally shorter and somewhat more cryptic
than the longer eddie james clues. we again formed

13as our interest

is in the language understanding, we
do not address the question of    tting answers into a grid,
which is the main concern of end-to-end automated crossword
solvers (littman et al., 2002).

14http://www.eddiejames.co.uk/

a list of 150 questions, beginning on 1 january 2015
and excluding any questions with multiple-word an-
swers. for clear contrast, we excluded those few
questions of length greater than four words. of these
150 clues, a subset of 30 were single-word clues.
all evaluation datasets are available online with the
paper.

as with the reverse dictionary experiments, can-
didates are extracted from models by inputting def-
initions and returning words corresponding to the
closest embeddings in the target space. in this case,
however, we only consider candidate words whose
length matches the length speci   ed in the clue.

test set

word

long baudelaire
(150)

description
   french poet
and key    gure
in the development
of symbolism.   

short (120)

satanist

   devil devotee   

single-word (30)

guilt

   culpability   

table 5: examples of the different question types in the
crossword question evaluation dataset.

4.2 benchmarks and comparisons

as with the reverse dictionary experiments, we
compare id56 and bow nlms with a simple
unsupervised baseline of elementwise addition of
id97 vectors in the embedding space (we
discard the ineffective w2v mult baseline), again
restricting candidates to words of the pre-speci   ed
length. we also compare to two bespoke online
crossword-solving engines. the    rst, one across
(http://www.oneacross.com/) is the can-
didate generation module of the award-winning
proverb crossword system (littman et al., 2002).
proverb, which was produced by academic re-
searchers, has featured in national media such
as new scientist,
and beaten expert humans
in crossword solving tournaments.
the sec-
ond comparison is with crossword maestro
(http://www.crosswordmaestro.com/), a
commercial crossword solving system that handles
both cryptic and non-cryptic crossword clues (we
focus only on the non-cryptic setting), and has also

been featured in national media.15 we are unable
to compare against a third well-known automatic
crossword solver, dr fill (ginsberg, 2011), because
code for dr fill   s candidate-generation module
is not readily available. as with the id56 and
baseline models, when evaluating existing systems
we discard candidates whose length does not match
the length speci   ed in the clue.

certain principles connect the design of the ex-
isting commercial systems and differentiate them
from our approach. unlike the nlms, they each re-
quire query-time access to large databases contain-
ing common crossword clues, dictionary de   nitions,
the frequency with which words typically appear
as crossword solutions and other hand-engineered
and task-speci   c components (littman et al., 2002;
ginsberg, 2011).

4.3 results

the performance of models on the various question
types is presented in table 6. when evaluating the
two commercial systems, one across and cross-
word maestro, we have access to web interfaces that
return up to approximately 100 candidates for each
query, so can only reliably record membership of the
top ten (accuracy@10).

on the long questions, we observe a clear advan-
tage for all dictionary embedding models over the
commercial systems and the simple unsupervised
baseline. here, the best performing nlm (id56
with id97 input embeddings and ranking loss)
ranks the correct answer third on average, and in the
top-ten candidates over 60% of the time.

as the questions get shorter, the advantage of
the embedding models diminishes. both the unsu-
pervised baseline and one across answer the short
questions with comparable accuracy to the id56 and
bow models. one reason for this may be the differ-
ence in form and style between the shorter clues and
the full de   nitions or encyclopedia sentences in the
dictionary training data. as the length of the clue de-
creases,    nding the answer often reduces to generat-
ing synonyms (culpability - guilt), or category mem-
bers (tall animal - giraffe). the commercial systems
can retrieve good candidates for such clues among
their databases of entities, relationships and com-

15 see e.g. http://www.theguardian.com/crosswords/crossword

question type

avg rank -accuracy@10/100 - rank variance

long (150)

short (120)

single-word (30)

one across
crossword maestro
w2v add
id56 cosine
id56 w2v cosine
id56 ranking
id56 w2v ranking
bow cosine
bow w2v cosine
bow ranking
bow w2v ranking

42
15
4
6
3
4
4
5
5

.39 /
.27 /
.31/.63
.43/.69
.61/.82
.58/.84
.62/.80
.60/.82
.60/.83
.62/.87
.60/.86

92
108
60
48
61
54
56
50
48

11
22
7
10
8
7
7
8
8

.68 /
.43 /
.50/.78
.39/.67
.56/.79
.51/.73
.57/.78
.56/.78
.54/.80
.58/.83
.56/.83

.70 /
.73 /
.79/.90
.31/.52
.48/.72
.48/.69
.48/.69
.45/.72
.59/.79
.55/.79
.55/.83

66
117
60
57
49
51
48
37
35

2
72
12
12
12
12
3
8
4

45
187
116
67
114
137
111
39
43

table 6: performance of different models on crossword questions of different length. the two commercial systems
are evaluated via their web interface so only accuracy@10 can be reported in those cases.

mon crossword answers. unsupervised id97
representations are also known to encode these sorts
of relationships (even after elementwise addition for
short sequences of words) (mikolov et al., 2013).
this would also explain why the dictionary embed-
ding models with pre-trained (id97) input em-
beddings outperfom those with learned embeddings,
particularly for the shortest questions.

4.4 qualitative analysis

a better understanding of how the different models
arrive at their answers can be gained from consider-
ing speci   c examples, as presented in table 7. the
   rst three examples show that, despite the apparently
super   cial nature of its training data (de   nitions and
introductory sentences) embedding models can an-
swer questions that require factual knowledge about
people and places. another notable characteristic of
these model is the consistent semantic appropriate-
ness of the candidate set. in the    rst case, the top
   ve candidates are all mountains, valleys or places in
the alps; in the second, they are all biblical names.
in the third, the id56 model retrieves currencies, in
this case performing better than the bow model,
which retrieves entities of various type associated
with the netherlands. generally speaking (as can
be observed by the web demo), the    smoothness    or
consistency in candidate generation of the dictionary
embedding models is greater than that of the com-
mercial systems. despite its simplicity, the unsuper-
vised w2v addition method is at times also surpris-

ingly effective, as shown by the fact that it returns
joshua in its top candidates for the third query.

the    nal example in table 7 illustrates the sur-
prising power of the bow model.
in the training
data there is a single de   nition for the correct an-
swer schoenberg: united states composer and musi-
cal theorist (born in austria) who developed atonal
composition. the only word common to both the
query and the de   nition is    composer    (there is no
id121 that allows the bow model to directly
connect atonal and atonality). nevertheless,
the
model is able to infer the necessary connections be-
tween the concepts in the query and the de   nition to
return schoenberg as the top candidate.

despite such cases,

it remains an open ques-
tion whether, with more diverse training data,
the world knowledge required for full open qa
(e.g.
secondary facts about schoenberg, such
as his family) could be encoded and retained as
weights in a (larger) dynamic network, or whether
it will be necessary to combine the id56 with
an external memory that
is less frequently (or
never) updated. this latter approach has begun to
achieve impressive results on certain qa and entail-
ment tasks (bordes et al., 2014; graves et al., 2014;
weston et al., 2015).

5 conclusion

dictionaries exist in many of the world   s languages.
we have shown how these lexical resources can con-
stitute valuable data for training the latest neural lan-

input description

one across

crossword maestro

bow

id56

   swiss mountain
peak famed for its
north face (5)   

1:noted 2:front
3:eiger 4:crown

5:fount

1:after 2:favor
3:ahead 4:along

5:being

1:joshua 2:exodus
3:hebrew 4:person

1:devise 2:daniel
3:haggai 4: isaiah

5:across

5:joseph

1:eiger 2.crags
3:teton 4:cerro

5:jebel

1:isaiah 2:elijah
3:joshua 4:elisha

5:yahweh

1:eiger 2:aosta
3:cuneo 4:lecco

5:tyrol

1:joshua 2:isaiah
3:gideon 4:elijah

5:yahweh

1:holland 2:general

3:lesotho

1:holland 2:ancient
3:earlier 4:onetime

1:guilder 2:holland
3:drenthe 4:utrecht

1:guilder 2:escudos
3:pesetas 4:someren

5:qondam

5:naarden

5:florins

1:surrealism
2:laborparty
3:tonemusics
4:introduced
5:schoenberg

1:disharmony
2:dissonance
3:bringabout
4:constitute
5:triggeroff

1:schoenberg
2:christleib
3:stravinsky
4:elder   eld
5:mendelsohn

1:mendelsohn
2:williamson
3:huddleston
4:mandelbaum
5:zimmerman

   old testament
successor to
moses (6)   

   the former
currency of the
netherlands
(7)   

   arnold, 20th
century composer
pioneer of
atonality
(10)   

table 7: responses from different models to example crossword clues. in each case the model output is    ltered to
exclude any candidates that are not of the same length as the correct answer. bow and id56 models are trained
without id97 input embeddings and cosine loss.

guage models to interpret and represent the mean-
ing of phrases and sentences. while humans use
the phrasal de   nitions in dictionaries to better un-
derstand the meaning of words, machines can use
the words to better understand the phrases. we used
two dictionary embedding architectures - a recurrent
neural network architecture with a long-short-term
memory, and a simpler linear bag-of-words model -
to explicitly exploit this idea.

on the reverse dictionary task that mirrors its
training setting, nlms that embed all known con-
cepts in a continuous-valued vector space perform
comparably to the best known commercial applica-
tions despite having access to many fewer de   ni-
tions. moreover, they generate smoother sets of can-
didates and require no linguistic pre-processing or
task-speci   c engineering. we also showed how the
description-to-word objective can be used to train
models useful for other tasks. nlms trained on the
same data can answer general-knowledge crossword
questions, and indeed outperform commercial sys-
tems on questions containing more than four words.
while our qa experiments focused on crosswords,
the results suggest that a similar embedding-based
approach may ultimately lead to improved output
from more general qa and id71 and in-
formation retrieval engines in general.

we make all code, training data, evaluation sets
and both of our linguistic tools publicly available on-
line for future research. in particular, we propose the
reverse dictionary task as a comparatively general-
purpose and objective way of evaluating how well
models compose lexical meaning into phrase or sen-
tence representations (whether or not they involve
training on de   nitions directly).

in the next stage of this research, we will ex-
plore ways to enhance the nlms described here,
especially in the question-answering context. the
models are currently not trained on any question-
like language, and would conceivably improve on
exposure to such linguistic forms. we would also
like to understand better how bow models can per-
form so well with no    awareness    of word order,
and whether there are speci   c linguistic contexts in
which models like id56s or others with the power
to encode word order are indeed necessary. finally,
we intend to explore ways to endow the model with
richer world knowledge. this may require the in-
tegration of an external memory module, similar to
the promising approaches proposed in several recent
papers (graves et al., 2014; weston et al., 2015).

acknowledgments

kc and yb acknowledge the support of the follow-
ing organizations: nserc, calcul qu  ebec, com-
pute canada, the canada research chairs and ci-
far. fh and ak were supported by google faculty
research award, and fh further by google euro-
pean doctoral fellowship.

references

dzmitry bahdanau, kyunghyun cho, and yoshua ben-
gio.
2015. id4 by jointly
learning to align and translate. in proceeding of iclr.
yoshua bengio, patrice simard, and paolo frasconi.
1994. learning long-term dependencies with gradient
descent is dif   cult. neural networks, ieee transac-
tions on, 5(2):157   166.

jonathan berant and percy liang. 2014. semantic pars-
ing via id141. in proceedings of the associa-
tion for computational linguistics.

james bergstra, olivier breuleux, fr  ed  eric bastien, pas-
cal lamblin, razvan pascanu, guillaume desjardins,
joseph turian, david warde-farley, and yoshua ben-
gio. 2010. theano: a cpu and gpu math expression
compiler. in proceedings of the python for scienti   c
computing conference (scipy).

slaven bilac, timothy baldwin, and hozumi tanaka.
2003. improving dictionary accessibility by maximiz-
ing use of available knowledge. traitement automa-
tique des langues, 44(2):199   224.

slaven bilac, wataru watanabe, taiichi hashimoto,
takenobu tokunaga, and hozumi tanaka. 2004. dic-
tionary search based on the target word description. in
proceedings of nlp 2014.

antoine bordes, sumit chopra, and jason weston. 2014.
id53 with subgraph embeddings. pro-
ceedings of emnlp.

antoine bordes, nicolas usunier, sumit chopra, and
jason weston. 2015. large-scale simple question
answering with memory networks.
arxiv preprint
arxiv:1506.02075.

sarath chandar, stanislas lauly, hugo larochelle,
mitesh khapra, balaraman ravindran, vikas c.
raykar, and amrita saha. 2014. an autoencoder ap-
proach to learning bilingual word representations. in
advances in neural information processing systems,
pages 1853   1861.

kyunghyun cho, bart van merri  enboer, caglar gul-
cehre, dzmitry bahdanau, fethi bougares, holger
schwenk, and yoshua bengio. 2014. learning phrase
representations using id56 encoder-decoder for statis-
tical machine translation. in proceedings of emnlp.

manaal faruqui, jesse dodge, sujay k. jauhar, chris
dyer, eduard hovy, and noah a. smith.
2014.
retro   tting word vectors to semantic lexicons. pro-
ceedings of the north american chapter of the asso-
ciation for computational linguistics.

david ferrucci, eric brown, jennifer chu-carroll, james
fan, david gondek, aditya a. kalyanpur, adam
lally, j. william murdock, eric nyberg, john prager,
nico schlaefer, and chris welty. 2010. building wat-
son: an overview of the deepqa project. in ai mag-
azine, volume 31(3), pages 59   79.

matthew l. ginsberg. 2011. dr. fill: crosswords and
an implemented solver for singly weighted csps. in
journal of arti   cial intelligence research, pages 851   
886.

stephan gouws, yoshua bengio, and greg corrado.
2014. bilbowa: fast bilingual distributed represen-
tations without word alignments.
in proceedings of
nips deep learning workshop.

alex graves, greg wayne,

turing machines.

and ivo danihelka.
arxiv preprint

2014.
arxiv:1410.5401.

neural

karl moritz hermann and phil blunsom. 2013. multi-
lingual distributed representations without word align-
ment. in proceedings of iclr.

sepp hochreiter and j  urgen schmidhuber. 1997. long
short-term memory. neural computation, 9(8):1735   
1780.

eric h. huang, richard socher, christopher d. manning,
and andrew y. ng. 2012. improving word representa-
tions via global context and multiple word prototypes.
in proceedings of the association for computational
linguistics.

mohit iyyer, jordan boyd-graber, leonardo claudino,
richard socher, and hal daum  e iii. 2014. a neu-
ral network for factoid id53 over para-
graphs. in proceedings of emnlp.

mohit iyyer, varun manjunatha, jordan boyd-graber,
and hal daum  e iii. 2015. deep unordered compo-
sition rivals syntactic methods for text classi   cation.
in proceedings of the association for computational
linguistics.

ryan kiros, ruslan salakhutdinov, and richard s.
zemel. 2015. unifying visual-semantic embeddings
with multimodal neural language models. transac-
tions of the association for computational linguistics.
to appear.

alexandre klementiev, ivan titov, and binod bhattarai.
inducing crosslingual distributed representa-

2012.
tions of words. proceedings of coling.

geoffrey leech, roger garside, and michael bryant.
1994. claws4: the tagging of the british national
corpus. in proceedings of coling.

michael l. littman, greg a. keim, and noam shazeer.
2002. a probabilistic approach to solving crossword
puzzles. arti   cial intelligence, 134(1):23   55.

tomas mikolov, martin kara     at, lukas burget, jan cer-
nock`y, and sanjeev khudanpur. 2010. recurrent neu-
ral network based language model. in proceedings of
interspeech 2010.

tomas mikolov, ilya sutskever, kai chen, greg s. cor-
rado, and jeff dean. 2013. distributed representations
of words and phrases and their compositionality.
in
advances in neural information processing systems.
jeff mitchell and mirella lapata. 2010. composition in
distributional models of semantics. cognitive science,
34(8):1388   1429.

diego moll  a and jos  e luis vicedo. 2007. question an-
swering in restricted domains: an overview. compu-
tational linguistics, 33(1):41   61.

ryan shaw, anindya datta, debra vandermeer, and
kaushik dutta. 2013. building a scalable database-
driven reverse dictionary. knowledge and data engi-
neering, ieee transactions on, 25(3):528   540.

ivan vulic, wim de smet, and marie-francine moens.
2011. identifying word translations from comparable
corpora using latent topic models. in proceedings of
the association for computational linguistics.

jason weston, antoine bordes, sumit chopra, and
tomas mikolov. 2015. towards ai-complete question
answering: a set of prerequisite toy tasks.
in arxiv
preprint arxiv:1502.05698.

matthew d. zeiler. 2012. adadelta: an adaptive learn-

ing rate method. in arxiv preprint arxiv:1212.5701.

michael zock and slaven bilac. 2004. word lookup on
the basis of associations: from an idea to a roadmap.
in proceedings of the acl workshop on enhancing
and using electronic dictionaries.

