5
1
0
2

 

y
a
m
2

 

 
 
]

g
l
.
s
c
[
 
 

3
v
7
7
5
6

.

2
1
4
1
:
v
i
x
r
a

published as a conference paper at iclr 2015

modeling compositionality with
multiplicative recurrent neural networks

ozan   irsoy & claire cardie
department of computer science
cornell university
ithaca, ny
{oirsoy,cardie}@cs.cornell.edu

abstract

we present the multiplicative recurrent neural network as a general model for com-
positional meaning in language, and evaluate it on the task of    ne-grained senti-
ment analysis. we establish a connection to the previously investigated matrix-
space models for compositionality, and show they are special cases of the mul-
tiplicative recurrent net. our experiments show that these models perform com-
parably or better than elman-type additive recurrent neural networks and outper-
form matrix-space models on a standard    ne-grained id31 corpus.
furthermore, they yield comparable results to structural deep models on the re-
cently published stanford sentiment treebank without the need for generating
parse trees.

1

introduction

recent advancements in neural networks and deep learning have provided fruitful applications for
natural language processing (nlp) tasks. one important such advancement was the invention of
id27s that represent a single word as a dense, low-dimensional vector in a meaning
space (bengio et al., 2001) from which numerous problems in nlp have bene   ted (collobert &
weston, 2008; collobert et al., 2011). the natural next question, then, was how to properly map
larger phrases into such dense representations for nlp tasks that require properly capturing their
meaning. most existing methods take a compositional approach by de   ning a function that com-
poses multiple word vector representations into a phrase representation (e.g. mikolov et al. (2013b),
socher et al. (2013), yessenalina & cardie (2011)).
compositional matrix-space models (rudolph & giesbrecht, 2010; yessenalina & cardie, 2011),
for example, represent phrase-level meanings in a vector space and represent words as matrices that
act on this vector space. therefore, a matrix assigned to a word should capture how it transforms
the meaning space (e.g. negation or intensi   cation). meaning representations for longer phrases are
simply computed as a multiplication of word matrices in sequential order (left-to-right, for english).
their representational power, however, is accompanied by a large number of parameters     a matrix
for every word in the vocabulary. thus, learning can be dif   cult.
but sequential composition of words into phrases is not the only mechanism for tackling semantic
composition. id56s (pollack, 1990), for example, employ a structural ap-
proach to compositionality: the composition function for a phrase operates on its two children in a
binary parse tree of the sentence. single words are represented in a vector-space. different ways
of de   ning the composition function lead to different variants of the id56. in
socher et al. (2011), a simple additive af   ne function with an additional nonlinearity is used. the
matrix-vector id56 of socher et al. (2012) extends this by assigning an addi-
tional matrix to each word, similar to the aforementioned matrix-space models; and the composition
function involves a matrix-vector multiplication of sibling representations. more recently, socher
et al. (2013) de   nes a bilinear tensor multiplication as the composition function     to capture mul-
tiplicative interactions between siblings.
on the other hand, recurrent neural networks (id56s), a neural network architecture with se-
quential prediction capabilities, implicitly model compositionality when applied to natural language

1

published as a conference paper at iclr 2015

sentences. representation of a phrase can be conceptualized as a nonlinear function that acts on
the network   s hidden layer (memory), which results from repeated function composition over the
hidden layer and the next word in the phrase/sentence (see section 3.2). unfortunately, it is possible
that conventional additive recurrent networks are not powerful enough to accommodate some of the
more complex effects in language, as suggested in previous work on (multiplicative and additive
variants of) id56s (e.g. socher et al. (2013)). more speci   cally, even though ad-
ditive models can theoretically model arbitrary functions when combined with a nonlinearity, they
might require a very large number of hidden units, and learnability of large parameter sets from data
might pose an issue.
to this end we investigate the multiplicative recurrent neural network as a model for composi-
tional semantic effects in language. previously, this type of multiplicative sequential approach has
been applied to a character-level text generation task (sutskever et al., 2011). in this work, we inves-
tigate its capacity for recognizing the sentiment of a sentence or a phrase represented as a sequence
of dense word vectors. like the matrix-space models, multiplicative id56s are sequential models
of language; and as a type of recurrent nn, they implicitly model compositionality. like the very
successful multiplicative id56s, multiplicative id56s can capture the same types
of sibling interactions, but are much simpler. in particular, no parse trees are required, so sequential
computations replace the associated recursive computations and performance does not depend on
the accuracy of the parser.
we also show a connection between the multiplicative id56 and compositional matrix-space mod-
els, which have also been applied to id31 (rudolph & giesbrecht, 2010; yessenalina
& cardie, 2011). in particular, matrix-space models are effectively a special case of multiplicative
id56s in which a word is represented as a large    one-hot    vector instead of a dense small one. thus,
these networks carry over the idea of matrix-space models from a one-hot sparse representation
to dense word vectors. they can directly employ word vector representations, which makes them
better suited for semi-supervised learning given the plethora of word vector training schemes. mul-
tiplicative recurrent networks can be considered to unify these two views of distributed language
processing     the operator semantics view of matrix-space models in which a word is interpreted
as an operator acting on the meaning representation, and the sequential memory processing view of
recurrent neural networks.
our experiments show that multiplicative id56s provide comparable or better performance than
conventional additive recurrent nets and matrix-space models in terms of    ne-grained sentiment
detection accuracy. furthermore, although the absence of parse tree information puts an additional
learning burden on multiplicative id56s, we    nd that they can reach comparable performance to the
id56 variants that require parse tree annotations for each sentence.

2 related work

vector space models. in natural language processing, a common way of representing a single token
as a vector is to use a    one-hot    vector per token, with a dimensionality of the vocabulary size. this
results in a very high dimensional, sparse representation. additionally, every word is put at an
equal distance to one another, disregarding their syntactic or semantic similarities. alternatively,
a distributed representation maps a token to a real-valued dense vector of smaller size (usually
on the order of 100 dimensions). generally, these representations are learned in an unsupervised
manner from a large corpus, e.g. wikipedia. various architectures have been explored to learn these
embeddings (bengio et al., 2001; collobert & weston, 2008; mnih & hinton, 2007; mikolov et al.,
2013a) which might have different generalization capabilities depending on the task (turian et al.,
2010). the geometry of the induced word vector space might have interesting semantic properties
(king - man + woman     queen) (mikolov et al., 2013a;b). in this work, we employ such word vector
representations as the initial input representation when training neural networks.
matrix space models. an alternative approach is to embed words into a matrix space, by assigning
matrices to words. intuitively, a matrix embedding of a word is desired in order to capture operator
semantics: the embedding should model how a word transforms meaning when it is applied to a
context. baroni & zamparelli (2010) partially apply this idea to model adjectives as matrices that
act on noun vectors. in their theoretical work, rudolph & giesbrecht (2010) de   ne a proper matrix
space model by assigning every word to a matrix; representations for longer phrases are computed

2

published as a conference paper at iclr 2015

by id127. they show that matrix space models generalize vector space models and
argue that they are neurologically and psychologically plausible. yessenalina & cardie (2011) apply
this model to    ne-grained sentiment detection. socher et al. (2012) use a structural approach in
which every word is assigned a matrix-vector pair, where the vector captures the meaning of the
word in isolation and the matrix captures how it transforms meaning when applied to a vector.
compositionality in vector and matrix spaces. commutative vector operations such as addition
(e.g. bag-of-words) or element-wise multiplication along with negation (widdows, 2003) provide
simple composition schemes (mitchell & lapata, 2010; zanzotto et al., 2010). even though they
ignore the order of the words, they might prove effective depending on the length of the phrases,
and on the task (mikolov et al., 2013b). other models for compositional id65
emulate formal semantics by representing functions as tensors and arguments as vectors (e.g. (clark,
2008; coecke et al., 2010; grefenstette et al., 2013)) for which (grefenstette et al., 2013) generalise
the tensor-learning approach of (baroni & zamparelli, 2010). more complex non-commutative
composition functions can be modeled via sequential or structural models of the sentence. in par-
ticular, compositionality in recurrent neural networks can be considered as tranformations on the
memory (hidden layer) applied by successive word vectors in order. id56s
employ a structural setting where compositions of smaller phrases into larger ones are determined
by their parent-children relationship in the associated binary parse tree (socher et al., 2011; 2012;
2013). in matrix space models, compositionality is naturally modeled via function composition in
sequence (rudolph & giesbrecht, 2010; yessenalina & cardie, 2011).
id31. id31 has been a very active area among nlp researchers, at
various granularities such as the word-, phrase-, sentence- or document-level (pang & lee, 2008).
besides preexisting work that tried to formulate the problem as binary classi   cation, recently    ne-
grained approaches were explored (yessenalina & cardie, 2011; socher et al., 2013). ultimately, the
vast majority of approaches do not tackle the task compositionally, and in addition to bag-of-words
features, they incorporate engineered features to account for negators, intensi   ers and contextual
valence shifters (polanyi & zaenen, 2006; wilson et al., 2005; kennedy & inkpen, 2006; shaikh
et al., 2007).

3 preliminaries

3.1 matrix-space models

a matrix-space model models a single word as a square matrix that transforms a meaning (state)
vector to another vector in the same meaning space. intuitively, a word is viewed as a function, or an
operator (in this particular case, linear) that acts on the meaning representation. therefore, a phrase
(or any sequence of words) is represented as successive application of the individual operators inside
the phrase.
let s = w1, w2, . . . , wt be a sequence of words of length t and let mw     rm  m denote the matrix
representation of a word w     v where v is the vocabulary. then, the representation of s is simply
(1)

m (s) = mw1mw2 . . . mwt

which yields another linear transformation in the same space. observe that this representation re-
spects word order (unlike, e.g. a bag of words). note that even though m (s) is modeled as a linear
operator on the meaning space, m (s) as a function of {mwi}i=1..t is not linear, since it constitutes
multiplications of those terms.
applying this representation to a task is simply applying the function to an initial empty meaning
vector h0, which results in a transformed,    nal meaning vector h that then is used to make a decision
on the phrase s. in the case of sentiment detection, a sentiment score y(s) can be assigned to s as
follows:

y(s) = h(cid:62)u = h(cid:62)

(2)
in such a supervised task, matrix-space model parameters {mw}w   v , h0, u are learned from data.
h0 and u can be    xed (without reducing the representative power of the model) to reduce the degrees
of freedom during training.

0 m (s)u

3

published as a conference paper at iclr 2015

figure 1: vector x (blue) and tensor a (red) sliced along the dimension of x. left. dense word
vector x computes a weighted sum over base matrices to get a square matrix, which then is used to
transform the meaning vector. right. one-hot word vector x with the same computation, which is
equivalent to selecting one of the base matrices and falls back to a matrix-space model.

3.2 recurrent neural networks

a recurrent neural network (id56) is a class of neural network that has recurrent connections, which
allow a form of memory. this makes them applicable for sequential prediction tasks of arbitrary
spatio-temporal dimension. they model the conditional distribution of a set (or a sequence) of
output variables, given an input sequence. in this work, we focus our attention on only elman-type
networks (elman, 1990).
in the elman-type network, the hidden layer ht at time step t is computed from a nonlinear trans-
formation of the current input layer xt and the previous hidden layer ht   1. then, the    nal output
yt is computed using the hidden layer ht. one can interpret ht as an intermediate representation
summarizing the past so far.
more formally, given a sequence of vectors {xt}t=1..t , an elman-type id56 operates by computing
the following memory and output sequences:

ht = f (w xt + v ht   1 + b)
yt = g(u ht + c)

(3)
(4)

where f is a nonlinearity, such as the element-wise sigmoid function, g is the output nonlinearity,
such as the softmax function, w and v are weight matrices between the input and hidden layer,
and among the hidden units themselves (connecting the previous intermediate representation to the
current one), respectively, while u is the output weight matrix, and b and c are bias vectors connected
to hidden and output units, respectively. when yt is a scalar (hence, u is a row vector) and g is the
sigmoid function, yt is simply the id203 of a positive label, conditioned on {x  }   =1..t.
for tasks requiring a single label per sequence (e.g. single sentiment score per sentence), we can
discard intermediate outputs {yt}t=1..(t   1) and use the output of the last time step yt , where t is
the length of the sequence. this also means that during training, external error is only incurred at
the    nal time step. in general, supervision can be applied at any intermediate time step whenever
there are labels available in the dataset, even if intermediate time step labels are not to be used at the
testing phase, since this makes training easier.

4 methodology

4.1 multiplicative recurrent neural network

a property of recurrent neural networks is that input layer activations and the hidden layer activations
of the previous time step interact additively to make up the activations for hidden layers at the current

4

published as a conference paper at iclr 2015

time step. this might be rather restrictive for some applications, or dif   cult to learn for modeling
more complex input interactions. on the other hand, a multiplicative interaction of those layers
might provide a better representation for some semantic analysis tasks. for sentiment detection,
for example,    not    might be considered as a negation of the sentiment that comes after it, which
might be more effectively modeled with multiplicative interactions. to this end, we investigate the
multiplicative recurrent neural network (or the recurrent neural tensor network) for the sentiment
analysis task that is the main focus of this paper (sutskever et al., 2011).
mid56s retain the same interpretation of memory as id56s, the only difference being the recursive
de   nition of h:

ht = f (x(cid:62)
yt = g(u ht + c)

t a[1..dh]ht   1 + w xt + v ht   1 + b)

(5)
(6)
where a is a dh    dx    dh tensor, and the bilinear operation x(cid:62)ay de   nes another vector as
(x(cid:62)ay)i = x(cid:62)a[i]y where the right-hand side represents the standard vector id127s
and a[i] is a single slice (matrix) of the tensor a. this means that a single entry of ht,i is not only
a linear combination of entries xt,j and ht   1,k, but also includes multiplicative terms in the form of
jkxt,jht   1,k.
ai
we can simplify equation 5 and 6 by adding bias units to x and h:

(7)
(8)
where x(cid:48) = [x; 1] and h(cid:48) = [h; 1]. with this notation, w , v and b become part of the tensor a(cid:48) and
c becomes part of the matrix u(cid:48).

t   1)

ht = f (x(cid:48)(cid:62)
yt = g(u(cid:48)h(cid:48)
t)

t a(cid:48)[1..dh]h(cid:48)

4.2 ordinal regression with neural networks

since    ne-grained sentiment labels denote intensity in addition to polarity, our class labels are or-
dinal in nature. therefore, we use an ordinal regression scheme for neural networks, as described
in cheng et al. (2008). intuitively, each sentiment class denotes a threshold for which the instances
belonging to the class have sentiment values less than or equal to. if an instance s belongs to class k,
it automatically belongs to the lower order classes 1, . . . , k     1, as well. therefore, the target vector
for instance s is r = [1, . . . , 1, 0, . . . , 0](cid:62) where ri = 1 if i < k and ri = 0 otherwise. this way, we
can consider the output vector as a cumulative id203 distribution on classes.
because of the way class labels are de   ned, output response is not subject to id172. there-
fore, output layer nonlinearity in this case is the elementwise sigmoid function (
1+exp(   xi) ) instead
of the softmax function (

(cid:80)
j exp(xj ) ) which is traditionally used for multiclass classi   cation.

exp(xi)

1

note that with this scheme, output of the network is not necessarily consistent. to decode an output
vector, we    rstly binarize each entry, by assigning 0 if the entry is less than 0.5 and 1 otherwise, as
in conventional binary classi   cation. then we simply start from the entry with the lowest index, and
whenever we observe a 0, we assume all of the entries with higher indices are also 0, which ensures
that the resulting target vector has the proper ordinal form. as an example, [1, 0, 1, 0](cid:62) is mapped
to [1, 0, 0, 0](cid:62). then    nally, we assign the corresponding integer label.

4.3 relationship to matrix-space model

in this section we will show the connection between mid56s and matrix space model.
let us assume a purely multiplicative mid56, without the bias units in the input and hidden layers
(equivalently, w = v = b = 0). in such an mid56, we compute the hidden layer (memory) as
follows:

ht = f (x(cid:62)

t aht   1)

(9)

furthermore, assume f = i is the identity mapping, rather than a nonlinearity function. we can
view the tensor multiplication in two parts: a vector xt multiplied by a tensor a, resulting in a

5

published as a conference paper at iclr 2015

figure 2: hidden layer vectors reduced to 2 dimensions for various phrases. left. recurrent neural
network. right. purely multiplicative recurrent neural tensor network.
in mid56, handling of
negation is more nonlinear and correctly shifts the sentiment.

matrix which we will denote as m (wt), to make the dependence of the resulting matrix on the word
wt explicit. then the matrix-vector multiplication m (wt)ht   1 resulting in the vector ht. therefore,
we can write the same equation as:

ht = (x(cid:62)

t a)ht   1 = m (wt)ht   1

(10)

and unfolding the recursion, we have

(11)
if we are interested in a scalar response for the whole sequence, we apply the output layer to the
hidden layer at the    nal time step:

ht = m (wt)m (wt   1) . . . m (w1)h0

yt = u(cid:62)ht = u(cid:62)m (wt ) . . . m (w1)h0

(12)
which is the matrix space model if individual m (wt) were to be associated with the matrices of their
corresponding words (equation 2). therefore, we can view mid56s as a simpli   cation to matrix-
space models in which we have a tensor a to extract a matrix for a word w from its associated
word vector, rather than associating a matrix with every word. this can be viewed as learning a
matrix-space model with parameter sharing. this reduces the number of parameters greatly: instead
of having a matrix for every word in the vocabulary, we have a vector per word, and a tensor to
extract matrices.
another interpretation of this is the following: instead of learning an individual linear operator mw
per word as in matrix-space models, mid56 learns dx number of base linear operators. mid56, then,
represents each word as a weighted sum of these base operators (weights given by the word vector
x). note that if x is a one-hot vector representation of a word instead of a dense id27
(which means dx = |v|), then we have |v| matrices as the base set of operators, and x simply
selects one of these matrices, essentially falling back to an exact matrix-space model (see figure 1).
therefore mid56s provide a natural transition of the matrix-space model from a one-hot sparse
word representation to a low dimensional dense id27.
besides a reduction in the number of parameters, another potential advantage of mid56s over
matrix-space models is that the matrix-space model is task-dependent: for each task, one has to
learn one matrix per word in the whole vocabulary. on the other hand, mid56s can make use of
task-independent word vectors (which can be learned in an unsupervised manner) and only the pa-
rameters for the network would have to be task-dependent. this allows easier extension to multitask
learning or id21 settings.

5 experiments

5.1 setting

data. for experimental evaluation of the models, we use the manually annotated mpqa corpus
(wiebe et al., 2005) that contains 535 newswire documents annotated with phrase-level subjectivity

6

published as a conference paper at iclr 2015

table 1: average ranking losses (mpqa)

table 2: average accuracies (sst)

method
prank
bag-of-words logreg
matrix-spacerand (dh = 3)
matrix-spacebow (dh = 3)
id56+
vec (dh = 315)
mid56i
rand (dh = 2)
mid56i
vec (dh = 25)
mid56+
vec (dh = 25)
mid56tanh
vec

(dh = 25)

loss
0.7808
0.6665
0.7417
0.6375
0.5265
0.6799
0.5278
0.5232
0.5147

method
bag-of-words nb
bag-of-words id166
bigram nb
vecavg
recursivetanh
mv-recursivetanh
mrecursivetanh
recurrent+
mrecurrent+

vec (dh = 315)
vec (dh = 20)

acc (%)

41.0
40.7
41.9
32.7
43.2
44.4
45.7
43.1
43.5

and intensity. we use the same scheme as yessenalina & cardie (2011) to preprocess and extract
individual phrases from the annotated documents, and convert the annotations to an integer ordinal
label {0, 1, 2, 3, 4} denoting a sentiment score from negative to positive. after preprocessing, we
have 8022 phrases in total with an average length of 2.83. we use the training-validation-test set
partitions provided by the authors to apply 10-fold cv and report average performance over ten
folds.
additionally, we use the recently published stanford sentiment treebank (sst) (socher et al.,
2013), which includes labels for 215,154 phrases in the parse trees of 11,855 sentences, with an
average sentence length of 19.1. similarly, real-valued sentiment labels are converted to an integer
ordinal label in {0, . . . , 4} by simple thresholding. we use the single training-validation-test set
partition provided by the authors. we do not make use of the parse trees in the treebank since our
approach is not structural; however, we include the phrase-level supervised labels (at the internal
nodes of the parse trees) as labels for partial sentences.
problem formulation. for experiments on the mpqa corpus, we employ an ordinal regression
setting. for experiments on sst, we employ a simple multiclass classi   cation setting, to make the
models directly comparable to previous work.
in the classi   cation setting, output nonlinearity g is the softmax function, and the output y is a vector
(cid:80)
valued response with the class probabilities. ordinal regression setting is as described in section 4.2.
id74. for experiments using the mpqa corpus, we use the ranking loss as in yesse-
i |yi     ri| where y and r are predicted and true scores
nalina & cardie (2011), de   ned as 1
n
i 1(yi = ri) as in socher et al.
respectively. for experiments using sst, we use accuracy, 1
n
(2013).
word vectors. we experiment with both randomly initialized word vectors (rand) and pretrained
word vector representations (vec). for pretrained word vectors, we use publicly available 300
dimensional word vectors by mikolov et al. (2013b), trained on part of google news dataset (   100b
words). when using pretrained word vectors, we do not    netune them to reduce the degree of
freedom of our models.
additionally, matrix-space models are initialized with random matrices (rand) or a bag-of-words
regression model weights (bow) as described in yessenalina & cardie (2011).

(cid:80)

5.2 results

quantitative results on the mpqa corpus are reported in table 1. the top group shows previous
results from yessenalina & cardie (2011) and the bottom group shows our results.
we observe that mid56 does slightly better that id56 with approximately the same number of pa-
rameters (0.5232 vs. 0.5265). this suggests that multiplicative interactions improve the model over
additive interactions. even though the difference is not signi   cant in the test set, it is signi   cant in
the development set. we partially attribute this effect to the test set variance. this also suggests that
multiplicative models are indeed more powerful, but require more careful id173, because
early stopping with a high model variance might tend to over   t to the development set.

7

published as a conference paper at iclr 2015

the randomly initialized mid56 outperforms its equivalent randomly initialized matrix-space model
(0.6799 vs. 0.7417), which suggests that more compact representations with shared parameters
learned by mid56 indeed generalize better.
the mid56 and id56 that use pretrained word vectors get the best results, which suggests the
importance of good pretraining schemes, especially when supervised data is limited. this is also
con   rmed by our preliminary experiments (which are not shown here) using other word vector
training methods such as cw embeddings (collobert & weston, 2008) or hlbl (mnih & hinton,
2007), which yielded a signi   cant difference (about 0.1     0.2) in ranking loss.
to test the effect of different nonlinearities, we experiment with the identity, recti   er and tanh func-
tions with mid56s. experiments show that there is small but consistent improvement as we use
recti   er or tanh over not using extra nonlinearity. the differences between recti   er and identity,
and tanh and recti   er are not signi   cant; however, the difference between tanh and identity is sig-
ni   cant, suggesting a performance boost from using a nonlinear squashing function. nonetheless,
not using any nonlinearity is only marginally worse. a possible explanation is that since the squash-
ing function is not the only source of nonlinearity in mid56s (multiplicativeness is another source
of nonlinearity), it is not as crucial.
results on the stanford sentiment treebank are shown in table 2. again, the top group shows
baselines from socher et al. (2013) and the bottom group shows our results.
both id56 and mid56 outperform the conventional id166 and naive bayes baselines. we observe
that id56 can get very close to the performance of id56, which can be con-
sidered its structural counterpart. mid56 further improves over id56 and performs better than the
recursive net and worse than the matrix-vector recursive net. note that none of the id56-based
methods employ parse trees of sentences, unlike their id56 variants.

6 conclusion and discussion

in this work, we explore multiplicative recurrent neural networks as a model for the compositional
interpretation of language. we evaluate on the task of    ne-grained id31, in an ordinal
regression setting and show that mid56s outperform previous work on mpqa, and get compara-
ble results to previous work on stanford sentiment treebank without using parse trees. we also
describe how mid56s effectively generalize matrix-space models from a sparse 1-hot word vector
representation to a distributed, dense representation.
one bene   t of mid56s over matrix-space models is their separation of task-independent word rep-
resentations (vectors) from task-dependent classi   ers (tensor), making them very easy to extend for
semi-supervised learning or id21 settings. slices of the tensor can be interpreted as base
matrices of a simpli   ed matrix-space model. intuitively, every meaning factor (a dimension of the
dense word vector) of a word has a separate operator acting on the meaning representation which
we combine to get the operator of the word itself.
from a parameter sharing perspective, mid56s provide better models. for matrix-space models, an
update over a sentence affects only the word matrices that occur in that particular sentence. on the
other hand, in an mid56, an update over a sentence affects the global tensor as well. with such an
update, the network alters its operation for similar words towards a similar direction.
one drawback of mid56s over conventional additive id56s is their increased model variance, re-
sulting from multiplicative interactions. this can be tackled by a stricter id173. another
future direction is to explore sparsity constraints on word vectors, which would mean that every
word would select only a few base operators to act on the meaning representation.

acknowledgments

this work was supported in part by nsf grant iis-1314778 and darpa deft grant fa8750-
13-2-0015. the views and conclusions contained herein are those of the authors and should not
be interpreted as necessarily representing the of   cial policies or endorsements, either expressed or
implied, of nsf, darpa or the u.s. government.

8

published as a conference paper at iclr 2015

references
baroni, marco and zamparelli, roberto. nouns are vectors, adjectives are matrices: representing adjective-
in proceedings of the 2010 conference on empirical methods in

noun constructions in semantic space.
natural language processing, pp. 1183   1193. association for computational linguistics, 2010.

bengio, yoshua, ducharme, rjean, vincent, pascal, jauvin, christian, k, jaz, hofmann, thomas, poggio,
tomaso, and shawe-taylor, john. a neural probabilistic language model. in in advances in neural infor-
mation processing systems, 2001.

cheng, jianlin, wang, zheng, and pollastri, gianluca. a neural network approach to ordinal regression. in
neural networks, 2008. ijid98 2008.(ieee world congress on computational intelligence). ieee interna-
tional joint conference on, pp. 1279   1284. ieee, 2008.

clark, stephen. a compositional distributional model of meaning. in proceedings of the second quantum

interaction symposium (qi-2008), 2008.

coecke, b., sadrzadeh, m., and s., clark. mathematical foundations for a compositional distributional model

of meaning. linguistic analysis, 36:345   384, 2010.

collobert, ronan and weston, jason. a uni   ed architecture for natural language processing: deep neural
networks with multitask learning. in proceedings of the 25th international conference on machine learning,
pp. 160   167. acm, 2008.

collobert, ronan, weston, jason, bottou, l  eon, karlen, michael, kavukcuoglu, koray, and kuksa, pavel.
natural language processing (almost) from scratch. j. mach. learn. res., 12:2493   2537, november 2011.
issn 1532-4435. url http://dl.acm.org/citation.cfm?id=1953048.2078186.

elman, jeffrey l. finding structure in time. cognitive science, 14(2):179   211, 1990.

grefenstette, e., dinu, g., zhang, y., sadrzadeh, m., and baroni, m. multi-step regression learning for com-
positional id65. in proceedings of the 10th international conference on computational
semantics (iwcs 2013)     long papers, pp. 131   142, potsdam, germany, march 2013. association for
computational linguistics. url http://www.aclweb.org/anthology/w13-0112.

kennedy, alistair and inkpen, diana. sentiment classi   cation of movie reviews using contextual valence

shifters. computational intelligence, 22(2):110   125, 2006.

mikolov, tomas, chen, kai, corrado, greg, and dean, jeffrey. ef   cient estimation of word representations in

vector space. arxiv preprint arxiv:1301.3781, 2013a.

mikolov, tomas, sutskever, ilya, chen, kai, corrado, greg s, and dean, jeff. distributed representations of
words and phrases and their compositionality. in advances in neural information processing systems, pp.
3111   3119, 2013b.

mitchell, jeff and lapata, mirella. composition in distributional models of semantics. cognitive science, 34

(8):1388   1439, 2010.

mnih, andriy and hinton, geoffrey. three new id114 for statistical language modelling. in pro-

ceedings of the 24th international conference on machine learning, pp. 641   648. acm, 2007.

pang, bo and lee, lillian. opinion mining and id31. foundations and trends in information

retrieval, 2(1-2):1   135, 2008.

polanyi, livia and zaenen, annie. contextual valence shifters. in computing attitude and affect in text: theory

and applications, pp. 1   10. springer, 2006.

pollack, j. b. recursive distributed representations. arti   cial intelligence, 1:77   105, 1990.

rudolph, sebastian and giesbrecht, eugenie. compositional matrix-space models of language. in proceedings
of the 48th annual meeting of the association for computational linguistics, pp. 907   916. association for
computational linguistics, 2010.

shaikh, mostafa al masum, prendinger, helmut, and mitsuru, ishizuka. assessing sentiment of text by se-
mantic dependency and contextual valence analysis. in affective computing and intelligent interaction, pp.
191   202. springer, 2007.

socher, richard, lin, cliff c, ng, andrew, and manning, chris. parsing natural scenes and natural language
with id56s. in proceedings of the 28th international conference on machine learning
(icml-11), pp. 129   136, 2011.

9

published as a conference paper at iclr 2015

socher, richard, huval, brody, manning, christopher d, and ng, andrew y. semantic compositionality
through recursive matrix-vector spaces. in proceedings of the 2012 joint conference on empirical methods
in natural language processing and computational natural language learning, pp. 1201   1211. associa-
tion for computational linguistics, 2012.

socher, richard, perelygin, alex, wu, jean y, chuang, jason, manning, christopher d, ng, andrew y, and
potts, christopher. recursive deep models for semantic compositionality over a sentiment treebank.
in
proceedings of the conference on empirical methods in natural language processing, emnlp    13, 2013.

sutskever, ilya, martens, james, and hinton, geoffrey e. generating text with recurrent neural networks. in
proceedings of the 28th international conference on machine learning (icml-11), pp. 1017   1024, 2011.

turian, joseph, ratinov, lev, and bengio, yoshua. word representations: a simple and general method for
semi-supervised learning. in proceedings of the 48th annual meeting of the association for computational
linguistics, pp. 384   394. association for computational linguistics, 2010.

widdows, dominic. orthogonal negation in vector spaces for modelling word-meanings and document re-
trieval. in proceedings of the 41st annual meeting of the association for computational linguistics, pp. 136   
143, sapporo, japan, july 2003. association for computational linguistics. doi: 10.3115/1075096.1075114.
url http://www.aclweb.org/anthology/p03-1018.

wiebe, janyce, wilson, theresa, and cardie, claire. annotating expressions of opinions and emotions in

language. language resources and evaluation, 39(2-3):165   210, 2005.

wilson, theresa, wiebe, janyce, and hoffmann, paul. recognizing contextual polarity in phrase-level senti-
ment analysis. in proceedings of the conference on human language technology and empirical methods
in natural language processing, pp. 347   354. association for computational linguistics, 2005.

yessenalina, ainur and cardie, claire. compositional matrix-space models for id31. in proceed-
ings of the conference on empirical methods in natural language processing, pp. 172   182. association
for computational linguistics, 2011.

zanzotto, fabio massimo, korkontzelos, ioannis, fallucchi, francesca, and manandhar, suresh. estimating
in proceedings of the 23rd international con-
linear models for compositional id65.
ference on computational linguistics (coling 2010), pp. 1263   1271, beijing, china, august 2010. coling
2010 organizing committee. url http://www.aclweb.org/anthology/c10-1142.

10

