effective use of word order for text categorization

with convolutional neural networks

rie johnson

rj research consulting
tarrytown, ny, usa

tong zhang

baidu inc., beijing, china

rutgers university, piscataway, nj, usa

5
1
0
2

 
r
a

 

m
6
2

 
 
]
l
c
.
s
c
[
 
 

2
v
8
5
0
1

.

2
1
4
1
:
v
i
x
r
a

riejohnson@gmail.com

tzhang@stat.rutgers.edu

abstract

convolutional neural network (id98) is a neu-
ral network that can make use of the inter-
nal structure of data such as the 2d structure
of image data. this paper studies id98 on
text categorization to exploit the 1d structure
(namely, word order) of text data for accurate
prediction. instead of using low-dimensional
word vectors as input as is often done, we
directly apply id98 to high-dimensional text
data, which leads to directly learning embed-
ding of small text regions for use in classi   -
cation. in addition to a straightforward adap-
tation of id98 from image to text, a sim-
ple but new variation which employs bag-of-
word conversion in the convolution layer is
proposed. an extension to combine multiple
convolution layers is also explored for higher
accuracy. the experiments demonstrate the
effectiveness of our approach in comparison
with state-of-the-art methods.

1

introduction

text categorization is the task of automatically as-
signing pre-de   ned categories to documents writ-
ten in natural languages. several types of text cat-
egorization have been studied, each of which deals
with different types of documents and categories,
such as topic categorization to detect discussed top-
ics (e.g., sports, politics), spam detection (sahami et
al., 1998), and sentiment classi   cation (pang et al.,
2002; pang and lee, 2008; maas et al., 2011) to de-
termine the sentiment typically in product or movie
reviews. a standard approach to text categorization
is to represent documents by bag-of-word vectors,

to appear in naacl hlt 2015.

namely, vectors that indicate which words appear in
the documents but do not preserve word order, and
use classi   cation models such as id166.

it has been noted that loss of word order caused
by bag-of-word vectors (bow vectors) is particularly
problematic on sentiment classi   cation. a simple
remedy is to use word bi-grams in addition to uni-
grams (blitzer et al., 2007; glorot et al., 2011; wang
and manning, 2012). however, use of word id165s
with n > 1 on text categorization in general is not
always effective; e.g., on topic categorization, sim-
ply adding phrases or id165s is not effective (see,
e.g., references in (tan et al., 2002)).

to bene   t from word order on text categoriza-
tion, we take a different approach, which employs
convolutional neural networks (id98) (lecun et al.,
1986). id98 is a neural network that can make use
of the internal structure of data such as the 2d struc-
ture of image data through convolution layers, where
each computation unit responds to a small region of
input data (e.g., a small square of a large image).
we apply id98 to text categorization to make use of
the 1d structure (word order) of document data so
that each unit in the convolution layer responds to a
small region of a document (a sequence of words).
id98 has been very successful on image clas-
si   cation; see e.g., the winning solutions of im-
agenet large scale visual recognition challenge
(krizhevsky et al., 2012; szegedy et al., 2014; rus-
sakovsky et al., 2014).

on text, since the work on token-level applica-
tions (e.g., id52) by collobert et al. (2011),
id98 has been used in systems for entity search, sen-
tence modeling, id27 learning, product
feature mining, and so on (xu and sarikaya, 2013;
gao et al., 2014; shen et al., 2014; kalchbrenner et

al., 2014; xu et al., 2014; tang et al., 2014; weston
et al., 2014; kim, 2014). notably, in many of these
id98 studies on text, the    rst layer of the network
converts words in sentences to word vectors by ta-
ble lookup. the word vectors are either trained as
part of id98 training, or    xed to those learned by
some other method (e.g., id97 (mikolov et al.,
2013)) from an additional large corpus. the latter is
a form of semi-supervised learning, which we study
elsewhere. we are interested in the effectiveness
of id98 itself without aid of additional resources;
therefore, word vectors should be trained as part of
network training if word vector lookup is to be done.
a question arises, however, whether word vector
lookup in a purely supervised setting is really useful
for text categorization. the essence of convolution
layers is to convert text regions of a    xed size (e.g.,
   am so happy    with size 3) to feature vectors, as de-
scribed later. in that sense, a word vector learning
layer is a special (and unusual) case of convolution
layer with region size one. why is size one appro-
priate if bi-grams are more discriminating than uni-
grams? hence, we take a different approach. we di-
rectly apply id98 to high-dimensional one-hot vec-
tors; i.e., we directly learn embedding1 of text re-
gions without going through id27 learn-
ing. this approach is made possible by solving the
computational issue2 through ef   cient handling of
high-dimensional sparse data on gpu, and it turned
out to have the merits of improving accuracy with
fast training/prediction and simplifying the system
(fewer hyper-parameters to tune). our id98 code
for text is publicly available on the internet3.

we study the effectiveness of id98 on text cate-
gorization and explain why id98 is suitable for the
task. two types of id98 are tested: seq-id98 is a
straightforward adaptation of id98 from image to
text, and bow-id98 is a simple but new variation of
id98 that employs bag-of-word conversion in the
convolution layer. the experiments show that seq-

1we use the term    embedding    loosely to mean a structure-
preserving function, in particular, a function that generates low-
dimensional features that preserve the predictive structure.

2id98 implemented for image would not handle sparse data
ef   ciently, and without ef   cient handling of sparse data, convo-
lution over high-dimensional one-hot vectors would be compu-
tationally infeasible.

3riejohnson.com/id98_download.html

figure 1: convolutional neural network.

figure 2: convolution layer for image. each computation
unit (oval) computes a non-linear function   (w   r(cid:96)(x) + b) of
a small region r(cid:96)(x) of input image x, where weight matrix w
and bias vector b are shared by all the units in the same layer.

id98 outperforms bow-id98 on sentiment classi-
   cation, vice versa on topic classi   cation, and the
winner generally outperforms the conventional bag-
of-id165 vector-based methods, as well as previ-
ous id98 models for text which are more complex.
in particular, to our knowledge, this is the    rst work
that has successfully used word order to improve
topic classi   cation performance. a simple exten-
sion that combines multiple convolution layers (thus
combining multiple types of text region embedding)
leads to further improvement. through empirical
analysis, we will show that id98 can make effec-
tive use of high-order id165s when conventional
methods fail.

2 id98 for document classi   cation

we    rst review id98 applied to image data and then
discuss the application of id98 to document classi-
   cation tasks to introduce seq-id98 and bow-id98.

2.1 preliminary: id98 for image
id98 is a feed-forward neural network with convo-
lution layers interleaved with pooling layers, as il-
lustrated in figure 1, where the top layer performs
classi   cation using the features generated by the lay-
ers below. a convolution layer consists of several
computation units, each of which takes as input a
region vector that represents a small region of the
input image and applies a non-linear function to it.
typically, the region vector is a concatenation of

outputinput[ 0 0 1 0 0 0 0 0 0 0 ]convolution layerpooling layerconvolution layerpooling layeroutput layer(linear classifier)[ 0 0 1 0 0 0 0 0 0 0 ]pixels in the region, which would be, for example,
75-dimensional if the region is 5   5 and the number
of channels is three (red, green, and blue). concep-
tually, computation units are placed over the input
image so that the entire image is collectively cov-
ered, as illustrated in figure 2. the region stride
(distance between the region centers) is often set to
a small value such as 1 so that regions overlap with
each other, though the stride in figure 2 is set larger
than the region size for illustration.

a distinguishing feature of convolution layers
is weight sharing. given input x, a unit associ-
ated with the (cid:96)-th region computes   (w    r(cid:96)(x) +
b), where r(cid:96)(x) is a region vector representing
the region of x at location (cid:96), and    is a pre-
de   ned component-wise non-linear activation func-
tion, (e.g., applying   (x) = max(x, 0) to each vec-
tor component). the matrix of weights w and the
vector of biases b are learned through training, and
they are shared by the computation units in the same
layer. this weight sharing enables learning useful
features irrespective of their location, while preserv-
ing the location where the useful features appeared.
we regard the output of a convolution layer as an
   image    so that the output of each computation unit
is considered to be a    pixel    of m channels where
m is the number of weight vectors (i.e., the number
of rows of w) or the number of neurons. in other
words, a convolution layer converts image regions
to m-dim vectors, and the locations of the regions
are inherited through this conversion.

the output image of the convolution layer is
passed to a pooling layer, which essentially shrinks
the image by merging neighboring pixels, so that
higher layers can deal with more abstract/global in-
formation. a pooling layer consists of pooling units,
each of which is associated with a small region
of the image. commonly-used merging methods
are average-pooling and max-pooling, which respec-
tively compute the channel-wise average/maximum
of each region.

2.2 id98 for text
now we consider application of id98 to text data.
suppose that we are given a document d =
(w1, w2, . . .) with vocabulary v . id98 requires vec-
tor representation of data that preserves internal lo-
cations (word order in this case) as input. a straight-

forward representation would be to treat each word
as a pixel, treat d as if it were an image of |d|    1
pixels with |v | channels, and to represent each pixel
(i.e., each word) as a |v |-dimensional one-hot vec-
tor4. as a running toy example, suppose that vocab-
ulary v = {    don   t   ,    hate   ,    i   ,    it   ,    love    } and
we associate the words with dimensions of vector
in alphabetical order (as shown), and that document
d=   i love it   . then, we have a document vector:

(cid:62)
x = [ 0 0 1 0 0 | 0 0 0 0 1 | 0 0 0 1 0 ]

.

2.2.1

seq-id98 for text

as in the convolution layer for image, we repre-
sent each region (which each computation unit re-
sponds to) by a concatenation of the pixels, which
makes p|v |-dimensional region vectors where p is
the region size    xed in advance. for example, on
the example document vector x above, with p = 2
and stride 1, we would have two regions    i love    and
   love it    represented by the following vectors:

                                                

                                                

don(cid:48)t
hate

i
it

love
don(cid:48)t
hate

i
it

love

0
0
1
0
0
   
0
0
0
0
1

                                                

                                                

don(cid:48)t
hate

i
it

love
don(cid:48)t
hate

i
it
love

0
0
0
0
1
   
0
0
0
1
0

r0(x) =

r1(x) =

the rest is the same as image; the text region vec-
tors are converted to feature vectors, i.e., the con-
volution layer learns to embed text regions into low-
dimensional vector space. we call a neural net with
a convolution layer with this region representation
seq-id98 (   seq    for keeping sequences of words) to
distinguish it from bow-id98, described next.
2.2.2 bow-id98 for text

a potential problem of seq-id98 however, is that
unlike image data with 3 rgb channels, the number
of    channels    |v | (size of vocabulary) may be very
large (e.g., 100k), which could make each region
vector r(cid:96)(x) very high-dimensional if the region size
4alternatively, one could use bag-of-letter-id165 vectors
as in (shen et al., 2014; gao et al., 2014) to cope with out-of-
vocabulary words and typos.

p is large. since the dimensionality of region vec-
tors determines the dimensionality of weight vec-
tors, having high-dimensional region vectors means
more parameters to learn. if p|v | is too large, the
model becomes too complex (w.r.t.
the amount of
training data available) and/or training becomes un-
affordably expensive even with ef   cient handling of
sparse data; therefore, one has to lower the dimen-
sionality by lowering the vocabulary size |v | and/or
the region size p, which may or may not be desir-
able, depending on the nature of the task.
an alternative we provide is to perform bag-
of-word conversion to make region vectors |v |-
dimensional instead of p|v |-dimensional; e.g., the
example region vectors above would be converted
to:

               

               

0
0
1
0
1

don(cid:48)t
hate

r0(x) =

r1(x) =

i
it

love

               

               

don(cid:48)t
hate

i
it

love

0
0
0
1
1

with this representation, we have fewer param-
eters to learn.
the expressiveness
of bow-convolution (which loses word order only
within small regions) is somewhere between seq-
convolution and bow vectors.

essentially,

2.2.3 pooling for text

whereas the size of images is    xed in image ap-
plications, documents are naturally variable-sized,
and therefore, with a    xed stride, the output of a con-
volution layer is also variable-sized as shown in fig-
ure 3. given the variable-sized output of the convo-
lution layer, standard pooling for image (which uses
a    xed pooling region size and a    xed stride) would
produce variable-sized output, which can be passed
to another convolution layer. to produce    xed-sized
output, which is required by the fully-connected top
layer5, we    x the number of pooling units and dy-
namically determine the pooling region size on each
data point so that the entire data is covered without
overlapping.

in the previous id98 work on text, pooling is
typically max-pooling over the entire data (i.e., one

5in this work, the top layer is fully-connected (i.e., each neu-
ron responds to the entire data) as in id98 for image. alterna-
tively, the top layer could be convolutional so that it can receive
variable-sized input, but such id98 would be more complex.

figure 3: convolution layer for variable-sized text.

pooling unit associated with the whole text). the dy-
namic k-max pooling of (kalchbrenner et al., 2014)
for sentence modeling extends it to take the k largest
values where k is a function of the sentence length,
but it is again over the entire data, and the operation
is limited to max-pooling. our pooling differs in that
it is a natural extension of standard pooling for im-
age, in which not only max-pooling but other types
can be applied. with multiple pooling units associ-
ated with different regions, the top layer can receive
locational information (e.g., if there are two pooling
units, the features from the    rst half and last half of
a document are distinguished). this turned out to be
useful (along with average-pooling) on topic classi-
   cation, as shown later.

2.3 id98 vs. bag-of-id165s
traditional methods represent each document en-
tirely with one bag-of-id165 vector and then ap-
ply a classi   er model such as id166. however, since
high-order id165s are susceptible to data sparsity,
use of a large n such as 20 is not only infeasible
but also ineffective. also note that a bag-of-id165
represents each id165 by a one-hot vector and ig-
nores the fact that some id165s share constituent
words. by contrast, id98 internally learns embed-
ding of text regions (given the consituent words as
input) useful for the intended task. consequently,
a large n such as 20 can be used especially with the
bow-convolution layer, which turned out to be useful
on topic classi   cation. a neuron trained to assign a
large value to, e.g.,    i love    (and a small value to    i
hate   ) is likely to assign a large value to    we love   
(and a small value to    we hate   ) as well, even though
   we love    was never seen during training. we will
con   rm these points empirically later.

2.4 extension: parallel id98
we have described id98 with the simplest network
architecture that has one pair of convolution and
pooling layers. while this can be extended in sev-
eral ways (e.g., with deeper layers), in our experi-
ments, we explored parallel id98, which has two or

i  love  it this  isn   t   what   i  expected  ! (a)                                                       (b)this  isn   t   what   i  expected  ! (a)                                                       (b)figure 4: id98 with two convolution layers in parallel.

more convolution layers in parallel6, as illustrated in
figure 4. the idea is to learn multiple types of em-
bedding of small text regions so that they can com-
plement each other to improve model accuracy. in
this architecture, multiple convolution-pooling pairs
with different region sizes (and possibly different re-
gion vector representations) are given one-hot vec-
tors as input and produce feature vectors for each
region; the top layer takes the concatenation of the
produced feature vectors as input.

3 experiments

we experimented with id98 on two tasks, topic clas-
si   cation and sentiment classi   cation. detailed in-
formation for reproducing the results is available on
the internet along with our code.

3.1 id98
we    xed the activation function to recti   er   (x) =
max(x, 0) and minimized square loss with l2 reg-
ularization by stochastic id119 (sgd).
we only used the 30k words that appeared most
frequently in the training set; thus, for example, in
seq-id98 with region size 3, a region vector is 90k
dimensional. out-of-vocabulary words were repre-
sented by a zero vector. on bow-id98, to speed up
computation, we used variable region stride so that a
larger stride was taken where repetition7 of the same
region vectors can be avoided by doing so. padding8
size was    xed to p     1 where p is the region size.

6similar architectures have been used for image. kim
(2014) used it for text, but it was on top of a word vector con-
version layer.

7for example, if we slide a window of size 3 over    * * foo
* *    where    *    is out of vocabulary, a bag of    foo    will be
repeated three times with stride    xed to 1.

8as is commonly done, to the beginning and the end of each
document, special words that are treated as unknown words
(and converted to zero vectors instead of one-hot vectors) were
added as    padding   . the purpose is to equally treat the words at
the edge and words in the middle.

we used two techniques commonly used with
id98 on image, which typically led to small per-
formance improvements. one is dropout (hinton
et al., 2012) optionally applied to the input to the
top layer. the other is response id172 as in
(krizhevsky et al., 2012), which in our case scales
the output of the pooling layer z at each location by
multiplying (1 + |z|2)   1/2.
3.2 baseline methods
for comparison, we tested id166 with the linear ker-
nel and fully-connected neural networks (see e.g.,
bishop (1995)) with bag-of-id165 vectors as in-
put. to experiment with fully-connected neural nets,
as in id98, we minimized square loss with l2 reg-
ularization and optional dropout by sgd, and ac-
tivation was    xed to recti   er. to generate bag-of-
id165 vectors, on topic classi   cation, we    rst set
each component to log(x + 1) where x is the word
frequency in the document and then scaled them to
unit vectors, which we found always improved per-
formance over raw frequency. on sentiment classi-
   cation, as is often done, we generated binary vec-
tors and scaled them to unit vectors. we tested three
types of bag-of-id165: bow1 with n     {1}, bow2
with n     {1, 2}, and bow3 with n     {1, 2, 3};
that is, bow1 is the traditional bow vectors, and with
bow3, each component of the vectors corresponds to
either uni-gram, bi-gram, or tri-gram of words.

we used id166light9 for the id166 experiments.

nb-lm we also tested nb-lm, which    rst ap-
peared (but without performance report10 ) as nb-
id166 in wm12 (wang and manning, 2012) and
later with a small modi   cation produced perfor-
mance that exceeds state-of-the-art supervised meth-
ods on imdb (which we experimented with) in
mmrb14 (mesnil et al., 2014). we experimented
with the mmrb14 version, which generates bi-
nary bag-of-id165 vectors, multiplies the com-
ponent for each id165 fi with log(p (fi|y =
1)/p (fi|y =    1)) (nb-weight) where the prob-
abilities are estimated using the training data,
and does id28 training. we used
mmrb14   s software11 with a modi   cation so that

9http://id166light.joachims.org/
10wm12 instead reported the performance of an ensemble of

nb and id166 as it performed better.

11https://github.com/mesnilgr/nbid166

output1 (positive)input:    i really love it !                   oneregion size s1region size convolution layerspooling layersoutput layer1 (positive)input:    i really love it !                   one-hot vectorsregion size s2the id173 parameter can be tuned on devel-
opment data.

3.3 model selection
for all
the methods,
the hyper-parameters such
as net con   gurations and id173 parameters
were chosen based on the performance on the devel-
opment data (held-out portion of the training data),
and using the chosen hyper-parameters, the models
were re-trained using all the training data.

3.4 data, tasks, and id174
imdb: movie reviews the imdb dataset (maas
et al., 2011) is a benchmark dataset for sentiment
classi   cation. the task is to determine if the movie
reviews are positive or negative. both the training
and test sets consist of 25k reviews. for preprocess-
ing, we tokenized the text so that emoticons such as
   :-)    are treated as tokens and converted all the char-
acters to lower case.
elec: electronics product reviews elec consists
of electronic product reviews. it is part of a large
amazon review dataset (mcauley and leskovec,
2013). we chose electronics as it seemed to be very
different from movies. following the generation of
imdb (maas et al., 2011), we chose the training set
and the test set so that one half of each set consists
of positive reviews and the other half is negative, re-
garding rating 1 and 2 as negative and 4 and 5 as
positive, and that the reviewed products are disjoint
between the training set and test set. note that to
extract text from the original data, we only used the
text section, and we did not use the summary sec-
tion. this way, we obtained a test set of 25k reviews
(same as imdb) and training sets of various sizes.
the training and test sets are available on the inter-
net12. id174 was the same as imdb.
rcv1: topic categorization rcv1 is a corpus
of reuters news articles as described in lyrl04
(lewis et al., 2004). rcv1 has 103 topic categories
in a hierarchy, and one document may be associated
with more than one topic. performance on this task
(multi-label categorization) is known to be sensitive
to thresholding strategies, which are algorithms ad-
ditional to the models we would like to test. there-
fore, we also experimented with single-label cate-

12riejohnson.com/id98_data.html

label
single
table 2
fig. 6
single
table 4 multi

#train
15,564
varies
23,149

#test
49,838
49,838
781,265

#class
55
55
103

table 1: rcv1 data summary.

gorization to assign one of 55 second-level topics
to each document to directly evaluate models. for
this task, we used the documents from a one-month
period as the test set and generated various sizes of
training sets from the documents with earlier dates.
data sizes are shown in table 1. as in lyrl04, we
used the concatenation of the headline and text ele-
ments. id174 was the same as imdb
except that we used the stopword list provided by
lyrl04 and regarded numbers as stopwords.

3.5 performance results
table 2 shows the error rates of id98 in comparison
with the baseline methods. the    rst thing to note
is that on all the datasets, the best-performing id98
outperforms the baseline methods, which demon-
strates the effectiveness of our approach.

to look into the details, let us    rst focus on id98
with one convolution layer (seq- and bow-id98 in
the table). on sentiment classi   cation (imdb and
elec), the con   guration chosen by model selection
was: region size 3, stride 1, 1000 weight vectors, and
max-pooling with one pooling unit, for both types
of id98; seq-id98 outperforms bow-id98, as well
as all the baseline methods except for one. note
that with a small region size and max-pooling, if a
review contains a short phrase that conveys strong
sentiment (e.g.,    a great movie!   ), the review could
receive a high score irrespective of the rest of the re-
view. it is sensible that this type of con   guration is
effective on sentiment classi   cation.

by contrast, on topic categorization (rcv1), the
con   guration chosen for bow-id98 by model selec-
tion was: region size 20, variable-stride   2, average-
pooling with 10 pooling units, and 1000 weight vec-
tors, which is very different from sentiment classi   -
cation. this is presumably because on topic clas-
si   cation, a larger context would be more predic-
tive than short fragments (    larger region size),
the entire document matters (    the effectiveness of
average-pooling), and the location of predictive text
also matters (    multiple pooling units). the last

point may be because news documents tend to have
crucial sentences (as well as the headline) at the be-
ginning. on this task, while both seq and bow-id98
outperform the baseline methods, bow-id98 outper-
forms seq-id98, which indicates that in this setting
the merit of having fewer parameters is larger than
the bene   t of keeping word order in each region.

now we turn to parallel id98. on imdb, seq2-
id98, which has two seq-convolution layers (region
size 2 and 3; 1000 neurons each; followed by one
unit of max-pooling each), outperforms seq-id98.
with more neurons (3000 neurons each; table 3) it
further exceeds the best-performing baseline, which
is also the best previous supervised result. we pre-
sume the effectiveness of seq2-id98 indicates that
the length of predictive text regions is variable.

the best performance 7.67 on imdb was ob-
tained by    seq2-bown-id98   , equipped with three
layers in parallel: two seq-convolution layers (1000
neurons each) as in seq2-id98 above and one layer
(20 neurons) that regards the entire document as one
region and represents the region (document) by a
bag-of-id165 vector (bow3) as input to the compu-
tation unit; in particular, we generated bow3 vectors
by multiplying the nb-weights with binary vectors,
motivated by the good performance of nb-lm. this
third layer is a bow-convolution layer13 with one re-
gion of variable size that takes one-hot vectors with
id165 vocabulary as input to learn document em-
bedding. the seq2-bown-id98 for elec in the ta-
ble is the same except that the regions sizes of seq-
convolution layers are 3 and 4. on both datasets,
performance is improved over seq2-id98. the re-
sults suggest that what can be learned through these
three layers are distinct enough to complement each
other. the effectiveness of the third layer indicates
that not only short word sequences but also global
context in a large window may be useful on this task;
thus, inclusion of a bow-convolution layer with n-
gram vocabulary with a large    xed region size might
be even more effective, providing more focused con-
text, but we did not pursue it in this work.
baseline methods comparing the baseline meth-
ods with each other, on sentiment classi   cation, re-
ducing the vocabulary to the most frequent id165s

13it can also be regarded as a fully-connected layer that takes

bow3 vectors as input.

methods
id166 bow3 (30k)
id166 bow1 (all)
id166 bow2 (all)
id166 bow3 (all)
nn bow3 (all)
nb-lm bow3 (all)
bow-id98
seq-id98
seq2-id98
seq2-bown-id98

imdb
10.14
11.36
9.74
9.42
9.17
8.13
8.66
8.39
8.04
7.67

elec rcv1
10.68
9.16
10.76
11.71
10.59
9.05
8.71
10.69
10.67
8.48
13.97
8.11
9.33
8.39
7.64
9.96
   
7.48
7.14
   

table 2: error rate (%) comparison with bag-of-id165-
based methods. sentiment classi   cation on imdb and
elec (25k training documents) and 55-way topic cate-
gorization on rcv1 (16k training documents).    (30k)   
indicates that the 30k most frequent id165s were used,
and    (all)    indicates that all the id165s (up to 5m) were
used. id98 used the 30k most frequent words.

id166 bow2 [wm12]
wrrbm+bow [dal12]
nb+id166 bow2 [wm12]
nb-lm bow3 [mmrb14]
paragraph vectors [lm14]
seq2-id98 (3k  2) [ours]
seq2-bown-id98 [ours]

10.84
10.77
8.78
8.13
7.46
7.94
7.67

   
   

   

ensemble

unlabeled data

   
   

table 3: error rate (%) comparison with previous best
methods on imdb.

notably hurt performance (also observed on nb-lm
and nn) even though some reduction is a common
practice. error rates were clearly improved by ad-
dition of bi- and tri-grams. by contrast, on topic
categorization, bi-grams only slightly improved ac-
curacy, and reduction of vocabulary did not hurt per-
formance. nb-lm is very strong on imdb and
poor on rcv1; its effectiveness appears to be data-
dependent, as also observed by wm12.
comparison with state-of-the-art results as
shown in table 3, the previous best supervised result
on imdb is 8.13 by nb-lm with bow3 (mmrb14),
and our best error rate 7.67 is better by nearly 0.5%.
(le and mikolov, 2014) reports 7.46 with the semi-
supervised method that learns low-dimensional vec-
tor representations of documents from unlabeled
data. their result is not directly comparable with our
supervised results due to use of additional resource.
nevertheless, our best result rivals their result.
we tested bow-id98 on the multi-label

topic
categorization task on rcv1 to compare with

models

micro-f macro-f

lyrl04   s best id166

bow-id98

81.6
84.0

60.7
64.8

table 4: rcv1 micro-averaged and macro-averaged f-
measure results on multi-label task with lyrl04 split.

lyrl04. we used the same thresholding strategy as
lyrl04. as shown in table 4, bow-id98 outper-
forms lyrl04   s best results even though our data
preprocessing is much simpler (no id30 and no
tf-idf weighting).
previous id98 we focus on the sentence classi   -
cation studies due to its relation to text categoriza-
tion. kim (2014) studied    ne-tuning of pre-trained
word vectors to produce input to parallel id98. he
reported that performance was poor when word vec-
tors were trained as part of id98 training (i.e., no ad-
ditional method/corpus). on our tasks, we were also
unable to outperform the baselines with this type of
model. also, with our approach, a system is sim-
pler with one fewer layer     no need to tune the di-
mensionality of word vectors or meta-parameters for
word vector learning.

kalchbrenner et al. (2014) proposed complex
modi   cations of id98 for sentence modeling. no-
tably, given word vectors     rd, their convolution
with m feature maps produces for each region a ma-
trix     rd  m (instead of a vector     rm as in stan-
dard id98). using the provided code, we found that
their model is too resource-demanding for our tasks.
on imdb and elec14 the best error rates we ob-
tained by training with various con   gurations that
   t in memory for 24 hours each on gpu (cf. fig 5)
were 10.13 and 9.37, respectively, which is no bet-
ter than id166 bow2. since excellent performances
were reported on short sentence classi   cation, we
presume that their model is optimized for short sen-
tences, but not for text categorization in general.
performance
dependency id98 training
is
known to be expensive, compared with, e.g., linear
models     linear id166 with bow3 on imdb only
takes 9 minutes using id166light (single-core) on a
high-end intel cpu. nevertheless, with our code on
gpu, id98 training only takes minutes (to a few
hours) on these datasets shown in figure 5.

14we could not train adequate models on rcv1 on either

tesla k20 or m2070 due to memory shortage.

figure 5: training time (minutes) on tesla k20. the
horizontal lines are the best-performing baselines.

figure 6: error rate in relation to training data size. for
readability, only representative methods are shown.

finally, the results with training sets of various

sizes on elec and rcv1 are shown in figure 6.

3.6 why is id98 effective?
in this section we explain the effectiveness of id98
through looking into what it learns from training.

first, for comparison, we show the id165s that
id166 with bow3 found to be the most predictive;
i.e., the following id165s were assigned the 10
largest weights by id166 with binary features on elec
for the negative and positive class, respectively:

    poor, useless, returned, not worth, return, worse,
    great, excellent, perfect, love, easy, amazing, awe-

disappointed, terrible, worst, horrible

some, no problems, perfectly, beat

note that, even though id166 was also given bi- and
tri-grams, the top 10 features chosen by id166 with
binary features are mostly uni-grams; furthermore,
the top 100 features (50 for each class) include 28
bi-grams but only four tri-grams. this means that,
with the given size of training data, id166 still heav-
ily counts on uni-grams, which could be ambiguous,
and cannot fully take advantage of higher-order n-
grams. by contrast, nb-weights tend to promote n-
grams with a larger n; the 100 features that were as-
signed the largest nb-weights are 7 uni-, 33 bi-, and
60 tri-grams. however, as seen above, nb-weights
do not always lead to the best performance.

77.588.50204060error rate (%)minuteselec (25k)seq2-id98seq2-bown7.588.599.5050100error rate (%)minutesimdb (25k)seq-id98seq2-bown99.51010.511010203040error rate (%)minutesrcv1 (16k)bow-id9880id98bown56789101112500050000error rate (%)training data size (log-scale)elecid166 bow2 (all)id166 bow3 (all)nb-lm bow3 (all)seq2-id98error rate (%)67891011121314151617200020000error rate (%)training data size (log-scale)rcv1id166 bow1 (all)id166 bow2 (all)id166 bow3 (all)bow-id98completely useless ., return policy .
it won   t even, but doesn   t work
product is defective, very disappointing !
is totally unacceptable, is so bad

n1
n2
n3
n4
n5 was very poor, it has failed
p1 works perfectly !, love this product
p2
p3
p4
p5

very pleased !, super easy to, i am pleased
   m so happy, it works perfect, is awesome !
highly recommend it, highly recommended !
am extremely satis   ed, is super fast

table 5: examples of predictive text regions in the train-
ing set.

in table 5, we show some of text regions learned
by seq-id98 to be predictive on elec. this net has
one convolution layer with region size 3 and 1000
neurons; thus, embedding by the convolution layer
produces a 1000-dim vector for each region, which
(after pooling) serves as features in the top layer
where weights are assigned to the 1000 vector com-
ponents. in the table, ni/pi indicates the component
that received the i-th highest weight in the top layer
for the negative/positive class, respectively. the ta-
ble shows the text regions (in the training set) whose
embedded vectors have a large value in the corre-
sponding component, i.e., predictive text regions.

note that the embedded vectors for the text re-
gions listed in the same row are close to each other
as they have a large value in the same component.
that is, table 5 also shows that the proximity of
the embedded vectors tends to re   ect the proximity
in terms of the relations to the target classes (pos-
itive/negative sentiment). this is the effect of em-
bedding, which helps classi   cation by the top layer.
with the bag-of-id165 representation, only the
id165s that appear in the training data can partici-
pate in prediction. by contrast, one strength of id98
is that id165s (or text regions of size n) can con-
tribute to accurate prediction even if they did not
appear in the training data, as long as (some of)
their constituent words did, because input of embed-
ding is the constituent words of the region. to see
this point, in table 6 we show the text regions from
the test set, which did not appear in the training
data, either entirely or partially as bi-grams, and yet
whose embedded features have large values in the
heavily-weighted (predictive) component thus con-
tributing to the prediction. there are many more of
these, and we only show a small part of them that

were unacceptably bad, is abysmally bad, were uni-
versally poor, was hugely disappointed, was enor-
mously disappointed, is monumentally frustrating,
are endlessly frustrating
best concept ever, best ideas ever, best hub ever,
am wholly satis   ed, am entirely satis   ed, am in-
credicbly satis   ed,    m overall impressed, am aw-
fully pleased, am exceptionally pleased,    m entirely
happy, are acoustically good, is blindingly fast,
table 6: examples of text regions that contribute to
prediction. they are from the test set, and they did not
appear in the training set, either entirely or partially as
bi-grams.

   t certain patterns. one noticeable pattern is (be-
verb, adverb, sentiment adjective) such as    am en-
tirely satis   ed    and       m overall impressed   . these
adjectives alone could be ambiguous as they may be
negated. to know that the writer is indeed    satis-
   ed   , we need to see the sequence    am satis   ed   ,
but the insertion of adverb such as    entirely    is very
common.    best x ever    is another pattern that a dis-
criminating pair of words are not adjacent to each
other. these patterns require tri-grams for disam-
biguation, and seq-id98 successfully makes use of
them even though the exact tri-grams were not seen
during training, as a result of learning, e.g.,    am x
satis   ed    with non-negative x (e.g.,    am very satis-
   ed   ,    am so satis   ed   ) to be predictive of the pos-
itive class through training. that is, id98 can ef-
fectively use word order when bag-of-id165-based
approaches fail.

4 conclusion

this paper showed that id98 provides an alternative
mechanism for effective use of word order for text
categorization through direct embedding of small
text regions, different from the traditional bag-of-n-
gram approach or word-vector id98. with the paral-
lel id98 framework, several types of embedding can
be learned and combined so that they can comple-
ment each other for higher accuracy. state-of-the-art
performances on sentiment classi   cation and topic
classi   cation were achieved using this approach.

acknowledgements

we thank the anonymous reviewers for useful sug-
gestions. the second author was supported by nsf
iis-1250985 and nsf iis-1407939.

references
christopher bishop. 1995. neural networks for pattern

recognition. oxford university press.

john blitzer, mark dredze, and fernando pereira. 2007.
biographies, bollywood, boom-boxes, and blenders:
id20 for sentiment classi   cation.
in
proceedings of acl.

ronan collobert, jason weston, l  eon bottou, michael
karlen, koray kavukcuoglu, and pavel kuksa. 2011.
natural language processing (almost) from scratch.
journal of machine learning research, 12:2493   
2537.

jianfeng gao, patric pantel, michael gamon, xiaodong
he, and li dent. 2014. modeling interestingness with
deep neural networks. in proceedings of emnlp.

xavier glorot, antoine bordes, and yoshua bengio.
2011. id20 for large-scale sentiment
classi   cation: a deep learning approach. in proceed-
ings of icml.

geoffrey e. hinton, nitish srivastava, alex krizhevsky,
and ruslan r. salakhutdinov.
ilya sutskever,
2012.
improving neural networks by preventing
co-adaptation of feature detectors. arxiv:1207.0580.
nal kalchbrenner, edward grefenstette, and phil blun-
som. 2014. a convolutional neural network for mod-
eling sentences. in proceedings of acl.

yoon kim. 2014. convolutional neural networks for sen-
tence classi   cation. in proceedings of emnlp, pages
1746   1751.

alex krizhevsky, ilya sutskever, and geoffrey e. hinton.
2012. id163 classi   cation with deep convolutional
neural networks. in proceedings of nips.

quoc le and tomas mikolov. 2014. distributed repre-
in proceed-

sentations of sentences and documents.
ings of icml.

yann lecun, le  on bottou, yoshua bengio, and patrick
haffner. 1986. gradient-based learning applied to
in proceedings of the ieee,
document recognition.
pages 2278   2324.

david d. lewis, yiming yang, tony g. rose, and fan li.
2004. rcv1: a new benchmark collection for text
categorization research. journal of marchine learn-
ing research, 5:361   397.

andrew l. maas, raymond e. daly, peter t. pham, dan
huang, andrew y. ng, and christopher potts. 2011.
learning word vectors for id31. in pro-
ceedings of acl.

julian mcauley and jure leskovec. 2013. hidden fac-
tors and hidden topics: understanding rating dimen-
sions with review text. in recsys.

gr  egoire mesnil, tomas mikolov, marc   aurelio ran-
zato, and yoshua bengio. 2014. ensemble of genera-
tive and discriminative techniques for sentiment analy-

sis of movie reviews. arxiv:1412.5335v5 (4 feb 2015
version).

tomas mikolov, ilya sutskever, kai chen, greg corrado,
and jeffrey dean. 2013. distributed representations
of words and phrases and their compositionality.
in
proceedings of nips.

bo pang and lillian lee. 2008. opinion mining and
id31. foundations and trends in infor-
mation retrieval, 2(1   2):1   135.

bo pang, lillian lee, and shivakumar vaithyanathan.
2002. thumbs up? sentiment classi   cation using ma-
chine learning techniques. in proceedings of confer-
ence on empirical methods in natural language pro-
cessing (emnlp), pages 79   86.

olga russakovsky, jia deng, hao su, jonathan krause,
sanjeev satheesh, sean ma, zhiheng huang, an-
drej karpathy, aditya khosla, michael bernstein,
alexander c. berg, and li fei-fei.
im-
agenet large scale visual recognition challenge.
arxiv:1409.0575.

2014.

mehran sahami, susan dumais, david heckerman, and
eric horvitz. 1998. a bayesian approach to    ltering
junk e-mail. in proceedings of aaai   98 workshop on
learning for text categorization.

yelong shen, xiaodong he, jianfeng gao, li deng, and
gr  egoire mensnil. 2014. a latent semantic model
with convolutional-pooling structure for information
retrieval. in proceedings of cikm.

christian szegedy, wei liu, yangqing jia, pierre
sermanet, scott reed, dragomir anguelov, du-
mitru erhan, vincent vanhoucke, and andrew ra-
binovich.
2014. going deeper with convolutions.
arxiv:1409.4842.

chade-meng tan, yuan-fang wang, and chan-do lee.
2002. the use of bigrams to enhance text catego-
information processing and management,
rization.
38:529   546.

duyu tang, furu wei, nan yang, ming zhou, ting liu,
and bing qin.
2014. learning sentiment-speci   c
id27 for twitter sentiment classi   cation.
in proceedings of acl, pages 1555   1565.

sida wang and christopher d. manning. 2012. base-
lines and bigrams: simple, good sentiment and topic
classi   cation. in proceedings of acl (short paper).

jason weston, sumit chopra, and keith adams. 2014.
in

#tagspace: semantic embeddings from hashtags.
proceedings of emnlp, pages 1822   1827.

puyang xu and ruhi sarikaya. 2013. convolutional neu-
ral network based triangular crf for joint intent detec-
tion and slot    lling. in asru.

liheng xu, kang liu, siwei lai, and jun zhao. 2014.
product feature mining: semantic clues versus syntac-
tic constituents. in proceedings of acl.

