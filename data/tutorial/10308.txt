5
1
0
2

 

v
o
n
1

 

 
 
]
l
m

.
t
a
t
s
[
 
 

3
v
5
5
2
1
0

.

4
0
5
1
:
v
i
x
r
a

semi-supervised convolutional neural networks for

text categorization via region embedding

rie johnson

rj research consulting
tarrytown, ny, usa

riejohnson@gmail.com

tong zhang   

baidu inc., beijing, china

rutgers university, piscataway, nj, usa

tzhang@stat.rutgers.edu

abstract

this paper presents a new semi-supervised framework with convolutional neural
networks (id98s) for text categorization. unlike the previous approaches that rely
on id27s, our method learns embeddings of small text regions from
unlabeled data for integration into a supervised id98. the proposed scheme for
embedding learning is based on the idea of two-view semi-supervised learning,
which is intended to be useful for the task of interest even though the training
is done on unlabeled data. our models achieve better results than previous ap-
proaches on sentiment classi   cation and topic classi   cation tasks.

1

introduction

convolutional neural networks (id98s) [15] are neural networks that can make use of the internal
structure of data such as the 2d structure of image data through convolution layers, where each
computation unit responds to a small region of input data (e.g., a small square of a large image). on
text, id98 has been gaining attention, used in systems for tagging, entity search, sentence modeling,
and so on [4, 5, 26, 7, 21, 12, 25, 22, 24, 13], to make use of the 1d structure (word order) of text
data. since id98 was originally developed for image data, which is    xed-sized, low-dimensional and
dense, without modi   cation it cannot be applied to text documents, which are variable-sized, high-
dimensional and sparse if represented by sequences of one-hot vectors. in many of the id98 studies
on text, therefore, words in sentences are    rst converted to low-dimensional word vectors. the word
vectors are often obtained by some other method from an additional large corpus, which is typically
done in a fashion similar to id38 though there are many variations [3, 4, 20, 23, 6, 19].
use of word vectors obtained this way is a form of semi-supervised learning and leaves us with
the following questions. q1. how effective is id98 on text in a purely supervised setting without
the aid of unlabeled data? q2. can we use unlabeled data with id98 more effectively than using
general word vector learning methods? our recent study [11] addressed q1 on text categorization
and showed that id98 without a word vector layer is not only feasible but also bene   cial when
not aided by unlabeled data. here we address q2 also on text categorization: building on [11], we
propose a new semi-supervised framework that learns embeddings of small text regions (instead of
words) from unlabeled data, for use in a supervised id98.
the essence of id98, as described later, is to convert small regions of data (e.g.,    love it    in a docu-
ment) to feature vectors for use in the upper layers; in other words, through training, a convolution
layer learns an embedding of small regions of data. here we use the term    embedding    loosely to
mean a structure-preserving function, in particular, a function that generates low-dimensional fea-
tures that preserve the predictive structure. [11] applies id98 directly to high-dimensional one-hot
vectors, which leads to directly learning an embedding of small text regions (e.g., regions of size 3
   tong zhang would like to acknowledge nsf iis-1250985, nsf iis-1407939, and nih r01ai116744 for

supporting his research.

1

like phrases, or regions of size 20 like sentences), eliminating the extra layer for word vector con-
version. this direct learning of region embedding was noted to have the merit of higher accuracy
with a simpler system (no need to tune hyper-parameters for word vectors) than supervised word
vector-based id98 in which word vectors are randomly initialized and trained as part of id98 train-
ing. moreover, the performance of [11]   s best id98 rivaled or exceeded the previous best results on
the benchmark datasets.
motivated by this    nding, we seek effective use of unlabeled data for text categorization through
direct learning of embeddings of text regions. our new semi-supervised framework learns a re-
gion embedding from unlabeled data and uses it to produce additional input (additional to one-hot
vectors) to supervised id98, where a region embedding is trained with labeled data. speci   cally,
from unlabeled data, we learn tv-embeddings (   tv    stands for    two-view   ; de   ned later) of a text
region through the task of predicting its surrounding context. according to our theoretical    nding,
a tv-embedding has desirable properties under ideal conditions on the relations between two views
and the labels. while in reality the ideal conditions may not be perfectly met, we consider them as
guidance in designing the tasks for tv-embedding learning.
we consider several types of tv-embedding learning task trained on unlabeled data; e.g., one task
is to predict the presence of the concepts relevant to the intended task (e.g.,    desire to recommend
the product   ) in the context, and we indirectly use labeled data to set up this task. thus, we seek
to learn tv-embeddings useful speci   cally for the task of interest. this is in contrast to the previous
word vector/embedding learning methods, which typically produce a id27 for general
purposes so that all aspects (e.g., either syntactic or semantic) of words are captured. in a sense,
the goal of our region embedding learning is to map text regions to high-level concepts relevant
to the task. this cannot be done by id27 learning since individual words in isolation
are too primitive to correspond to high-level concepts. for example,    easy to use    conveys positive
sentiment, but    use    in isolation does not. we show that our models with tv-embeddings outper-
form the previous best results on sentiment classi   cation and topic classi   cation. moreover, a more
direct comparison con   rms that our region tv-embeddings provide more compact and effective rep-
resentations of regions for the task of interest than what can be obtained by manipulation of a word
embedding.

1.1 preliminary: one-hot id98 for text categorization [11]

a id98 is a feed-forward network equipped with convolution layers interleaved with pooling layers.
a convolution layer consists of computation units, each of which responds to a small region of
input (e.g., a small square of an image), and the small regions collectively cover the entire data. a
computation unit associated with the (cid:96)-th region of input x computes:

  (w    r(cid:96)(x) + b) ,

(1)
where r(cid:96)(x)     rq is the input region vector that represents the (cid:96)-th region. weight matrix w    
rm  q and bias vector b     rm are shared by all the units in the same layer, and they are learned
through training. in [11], input x is a document represented by one-hot vectors (figure 1); therefore,
we call [11]   s id98 one-hot id98; r(cid:96)(x) can be either a concatenation of one-hot vectors, a bag-of-
word vector (bow), or a bag-of-id165 vector: e.g., for a region    love it   

i
r(cid:96)(x) =[ 0
i
r(cid:96)(x) =[ 0

it
0
it
1

love
1
love
1

i
| 0
](cid:62)

it
1

love
0

](cid:62)

(concatenation)

(bow)

(2)

(3)

the bow representation (3) loses word order within the region but is more robust to data sparsity,
enables a large region size such as 20, and speeds up training by having fewer parameters. this is
what we mainly use for embedding learning from unlabeled data. id98 with (2) is called seq-id98
and id98 with (3) bow-id98. the region size and stride (distance between the region centers) are
meta-parameters. note that we used a tiny three-word vocabulary for the vector examples above to
save space, but a vocabulary of typical applications could be much larger.    in (1) is a component-
wise non-linear function (e.g., applying   (x) = max(x, 0) to each vector component). thus, each
computation unit generates an m-dimensional vector where m is the number of weight vectors (w   s
rows) or neurons. in other words, a convolution layer embodies an embedding of text regions, which
produces an m-dim vector for each text region. in essence, a region embedding uses co-presence
and absence of words in a region as input to produce predictive features, e.g., if presence of    easy

2

figure 1: one-hot id98 example. re-
gion size 2, stride 1.

figure 2: tv-embedding learning by training
to predict adjacent regions.

to use    with absence of    not    is a predictive indicator, it can be turned into a large feature value
by having a negative weight on    not    (to penalize its presence) and positive weights on the other
three words in one row of w. a more formal argument can be found in the appendix. the m-dim
vectors from all the text regions of each document are aggregated by the pooling layer, by either
component-wise maximum (max-pooling) or average (average-pooling), and used by the top layer
(a linear classi   er) as features for classi   cation. here we focused on the convolution layer; for other
details, [11] should be consulted.

2 semi-supervised id98 with tv-embeddings for text categorization

it was shown in [11] that one-hot id98 is effective on text categorization, where the essence is direct
learning of an embedding of text regions aided by new options of input region vector representation.
we go further along this line and propose a semi-supervised learning framework that learns an em-
bedding of text regions from unlabeled data and then integrates the learned embedding in supervised
training. the    rst step is to learn an embedding with the following property.
de   nition 1 (tv-embedding). a function f1 is a tv-embedding of x1 w.r.t. x2 if there exists a
function g1 such that p (x2|x1) = g1(f1(x1), x2) for any (x1, x2)     x1    x2.
a tv-embedding (   tv    stands for two-view) of a view (x1), by de   nition, preserves everything re-
quired to predict another view (x2), and it can be trained on unlabeled data. the motivation of tv-
embedding is our theoretical    nding (formalized in the appendix) that, essentially, a tv-embedded
feature vector f1(x1) is as useful as x1 for the purpose of classi   cation under ideal conditions.
the conditions essentially state that there exists a set h of hidden concepts such that two views
and labels of the classi   cation task are related to each other only through the concepts in h. the
concepts in h might be, for example,    pricey   ,    handy   ,    hard to use   , and so on for sentiment
classi   cation of product reviews. while in reality the ideal conditions may not be completely met,
we consider them as guidance and design tv-embedding learning accordingly.
tv-embedding learning is related to two-view id171 [2] and aso [1], which learn a linear
embedding from unlabeled data through tasks such as predicting a word (or predicted labels) from
the features associated with its surrounding words. these studies were, however, limited to a linear
embedding. a related method in [6] learns a id27 so that left context and right context
maximally correlate in terms of canonical correlation analysis. while we share with these studies
the general idea of using the relations of two views, we focus on nonlinear learning of region em-
beddings useful for the task of interest, and the resulting methods are very different. an important
difference of tv-embedding learning from co-training is that it does not involve label guessing, thus
avoiding risk of label contamination. [8] used a stacked denoising auto-encoder to extract features
invariant across domains for sentiment classi   cation from unlabeled data. it is for fully-connected
neural networks, which underperformed id98s in [11].
now let b be the base id98 model for the task of interest, and assume that b has one convolution
layer with region size p. note, however, that the restriction of having only one convolution layer is
merely for simplifying the description. we propose a semi-supervised framework with the following
two steps.
1. tv-embedding learning: train a neural network u to predict the context from each region
of size p so that u   s convolution layer generates feature vectors for each text region of size
p for use in the classi   er in the top layer. it is this convolution layer, which embodies the
tv-embedding, that we transfer to the supervised learning model in the next step. (note that
u differs from id98 in that each small region is associated with its own target/output.)

3

i  really    love    it    !    output1 (positive)convolution layer (size 2)top layerreally    love    it    !    pooling layer1 (positive)input: one-hot vectorsgood    goodacting   fun plot :)   plot :)     :)good   acting   , fun    plot     :)    convolution layer  f1top layer g1acting   outputx2fun    plot     :) input  x1   2. final supervised learning: integrate the learned tv-embedding (the convolution layer of u)
into b, so that the tv-embedded regions (the output of u   s convolution layer) are used as an
additional input to b   s convolution layer. train this    nal model with labeled data.

these two steps are described in more detail in the next two sections.

2.1 learning tv-embeddings from unlabeled data

we create a task on unlabeled data to predict the context (adjacent text regions) from each region of
size p de   ned in b   s convolution layer. to see the correspondence to the de   nition of tv-embeddings,
it helps to consider a sub-task that assigns a label (e.g., positive/negative) to each text region (e.g.,    ,
fun plot   ) instead of the ultimate task of categorizing the entire document. this is sensible because
id98 makes predictions by building up from these small regions. in a document    good acting, fun
plot :)    as in figure 2, the clues for predicting a label of    , fun plot    are    , fun plot    itself (view-
1: x1) and its context    good acting    and    :)    (view-2: x2). u is trained to predict x2 from x1,
i.e., to approximate p (x2|x1) by g1(f1(x1), x2)) as in de   nition 1, and functions f1 and g1 are
embodied by the convolution layer and the top layer, respectively.
given a document x, for each text region indexed by (cid:96), u   s convolution layer computes:

u(cid:96)(x) =   (u )(cid:16)

(x) + b(u )(cid:17)

w(u )    r(u )

(cid:96)

,

(x) can be either sequential, bow, or bag-of-id165, independent of r(cid:96)(x) in b.

(4)
which is the same as (1) except for the superscript    (u)    to indicate that these entities belong to u.
the top layer (a linear model for classi   cation) uses u(cid:96)(x) as features for prediction. w(u ) and b(u )
(and the top-layer parameters) are learned through training. the input region vector representation
r(u )
(cid:96)
the goal here is to learn an embedding of text regions (x1), shared with all the text regions at
every location. context (x2) is used only in tv-embedding learning as prediction target (i.e., not
transferred to the    nal model); thus, the representation of context should be determined to optimize
the    nal outcome without worrying about the cost at prediction time. our guidance is the conditions
on the relationships between the two views mentioned above; ideally, the two views should be
related to each other only through the relevant concepts. we consider the following two types of
target/context representation.
unsupervised target a straightforward vector encoding of context/target x2 is bow vectors of
the text regions on the left and right to x1. if we distinguish the left and right, the target vector is
2|v |-dimensional with vocabulary v , and if not, |v |-dimensional. one potential problem of this
encoding is that adjacent regions often have syntactic relations (e.g.,    the    is often followed by an
adjective or a noun), which are typically irrelevant to the task (e.g., to identify positive/negative
sentiment) and therefore undesirable. a simple remedy we found effective is vocabulary control
of context to remove function words (or stop-words if available) from (and only from) the target
vocabulary.
partially-supervised target another context representation that we consider is partially super-
vised in the sense that it uses labeled data. first, we train a id98 with the labeled data for the
intended task and apply it to the unlabeled data. then we discard the predictions and only retain
the internal output of the convolution layer, which is an m-dimensional vector for each text region
where m is the number of neurons. we use these m-dimensional vectors to represent the context.
[11] has shown, by examples, that each dimension of these vectors roughly represents concepts rel-
evant to the task, e.g.,    desire to recommend the product   ,    report of a faulty product   , and so on.
therefore, an advantage of this representation is that there is no obvious noise between x1 and x2
since context x2 is represented only by the concepts relevant to the task. a disadvantage is that it
is only as good as the supervised id98 that produced it, which is not perfect and in particular, some
relevant concepts would be missed if they did not appear in the labeled data.

2.2 final supervised learning: integration of tv-embeddings into supervised id98
we use the tv-embedding obtained from unlabeled data to produce additional input to b   s convolu-
tion layer, by replacing    (w    r(cid:96)(x) + b) (1) with:

   (w    r(cid:96)(x) + v    u(cid:96)(x) + b) ,

(5)

4

where u(cid:96)(x) is de   ned by (4), i.e., u(cid:96)(x) is the output of the tv-embedding applied to the (cid:96)-th
region. we train this model with the labeled data of the task; that is, we update the weights w, v,
bias b, and the top-layer parameters so that the designated id168 is minimized on the labeled
training data. w(u ) and b(u ) can be either    xed or updated for    ne-tuning, and in this work we    x
them for simplicity.
note that while (5) takes a tv-embedded region as input, (5) itself is also an embedding of text
regions; let us call it (and also (1)) a supervised embedding, as it is trained with labeled data, to
distinguish it from tv-embeddings. that is, we use tv-embeddings to improve the supervised em-
bedding. note that (5) can be naturally extended to accommodate multiple tv-embeddings by

(cid:32)

k(cid:88)

(cid:33)

w    r(cid:96)(x) +

  

v(i)    u(i)

(cid:96) (x) + b

,

(6)

so that, for example, two types of tv-embedding (i.e., k = 2) obtained with the unsupervised target
and the partially-supervised target can be used at once, which can lead to performance improvement
as they complement each other, as shown later.

i=1

3 experiments

our code and the experimental settings are available at riejohnson.com/id98 download.html.
data we used the three datasets used in [11]: imdb, elec, and rcv1, as summarized in table
1. imdb (movie reviews) [17] comes with an unlabeled set. to facilitate comparison with previous
studies, we used a union of this set and the training set as unlabeled data. elec consists of amazon
reviews of electronics products. to use as unlabeled data, we chose 200k reviews from the same
data source so that they are disjoint from the training and test sets, and that the reviewed products
are disjoint from the test set. on the 55-way classi   cation of the second-level topics on rcv1
(news), unlabeled data was chosen to be disjoint from the training and test sets. on the multi-label
categorization of 103 topics on rcv1, since the of   cial lyrl04 split for this task divides the entire
corpus into a training set and a test set, we used the entire test set as unlabeled data (the transductive
learning setting).

#unlabeled

#train
imdb 25,000
25,000
elec
15,564
rcv1
23,149
table 1: datasets.    the multi-label rcv1 is used only in table 6.

75k (20m words)
200k (24m words)
669k (183m words)
781k (214m words)

#test
25,000
25,000
49,838
781,265

55 (single)
103 (multi)   

#class

2
2

sentiment
topic(s)

output

positive/negative

mized weighted square loss(cid:80)

implementation we used the one-layer id98 models found to be effective in [11] as our base
models b, namely, seq-id98 on imdb/elec and bow-id98 on rcv1. tv-embedding training mini-
i,j   i,j(zi[j]    pi[j])2 where i goes through the regions, z represents
the target regions, and p is the model output. the weights   i,j were set to balance the loss originat-
ing from the presence and absence of words (or concepts in case of the partially-supervised target)
and to speed up training by eliminating some negative examples, similar to negative sampling of
[19]. to experiment with the unsupervised target, we set z to be bow vectors of adjacent regions
on the left and right, while only retaining the 30k most frequent words with vocabulary control;
on sentiment classi   cation, function words were removed, and on topic classi   cation, numbers and
stop-words provided by [16] were removed. note that these words were removed from (and only
from) the target vocabulary. to produce the partially-supervised target, we    rst trained the super-
vised id98 models with 1000 neurons and applied the trained convolution layer to unlabeled data
to generate 1000-dimensional vectors for each region. the rest of implementation follows [11]; i.e.,
supervised models minimized square loss with l2 id173 and optional dropout [9];    and
  (u ) were the recti   er; response id172 was performed; optimization was done by sgd.
model selection on all the tested methods, tuning of meta-parameters was done by testing the
models on the held-out portion of the training data, and then the models were re-trained with the
chosen meta-parameters using the entire training data.

5

3.1 performance results

overview after con   rming the effectiveness of our new models in comparison with the supervised
id98, we report the performances of [13]   s id98, which relies on word vectors pre-trained with a
very large corpus (table 3). besides comparing the performance of approaches as a whole, it is
also of interest to compare the usefulness of what was learned from unlabeled data; therefore, we
show how it performs if we integrate the word vectors into our base model one-hot id98s (figure
3). in these experiments we also test word vectors trained by id97 [19] on our unlabeled data
(figure 4). we then compare our models with two standard semi-supervised methods, transductive
id166 (tid166) [10] and co-training (table 3), and with the previous best results in the literature
(tables 4   6). in all comparisons, our models outperform the others. in particular, our region tv-
embeddings are shown to be more compact and effective than region embeddings obtained by simple
manipulation of id27s, which supports our approach of using region embedding instead
of id27.

names in table 3 x1: r(u )
bow vector
unsup-tv.
parsup-tv.
bow vector
bag-of-{1,2,3}-gram vector
unsup3-tv.

(x)

(cid:96)

x2: target of u training
bow vector
output of supervised embedding
bow vector

table 2: tested tv-embeddings.

1
2
3
4
5
6
7
8
9
10
11
12

linear id166 with 1-3grams [11]

linear tid166 with 1-3grams

[13]   s id98

one-hot id98 (simple) [11]

one-hot id98 (simple) co-training best

our id98

unsup-tv.

parsup-tv.

unsup3-tv.
all three

100-dim
200-dim
100-dim
200-dim
100-dim
200-dim
100  3

imdb
10.14
9.99
9.17
8.39
(8.06)
7.12
6.81
7.12
7.13
7.05
6.96
6.51

elec rcv1
10.68
9.16
16.41
10.77
10.44
8.03
9.17
7.64
(8.73)
(7.63)
6.96
8.10
7.97
6.69
8.19
6.58
7.99
6.57
8.13
6.66
6.84
8.02
7.71
6.27

table 3: error rates (%). for comparison, all the id98 models were constrained to have 1000 neurons. the
parentheses around the error rates indicate that co-training meta-parameters were tuned on test data.

our id98 with tv-embeddings we tested three types of tv-embedding as summarized in table
2. the    rst thing to note is that all of our id98s (table 3, row 6   12) outperform their supervised
counterpart in row 4. this con   rms the effectiveness of the framework we propose. in table 3, for
meaningful comparison, all the id98s are constrained to have exactly one convolution layer (except
for [13]   s id98) with 1000 neurons. the best-performing supervised id98s within these constraints
(row 4) are: seq-id98 (region size 3) on imdb and elec and bow-id98 (region size 20) on rcv11.
they also served as our base models b (with region size parameterized on imdb/elec). more
complex supervised id98s from [11] will be reviewed later. on sentiment classi   cation (imdb
and elec), the region size chosen by model selection for our models was 5, larger than 3 for the
supervised id98. this indicates that unlabeled data enabled effective use of larger regions which are
more predictive but might suffer from data sparsity in supervised settings.
   unsup3-tv.    (rows 10   11) uses a bag-of-id165 vector to initially represent each region, thus, re-
tains word order partially within the region. when used individually, unsup3-tv. did not outperform
the other tv-embeddings, which use bow instead (rows 6   9). but we found that it contributed to
error reduction when combined with the others (not shown in the table). this implies that it learned
from unlabeled data predictive information that the other two embeddings missed. the best perfor-
mances (row 12) were obtained by using all the three types of tv-embeddings at once according to
(6). by doing so, the error rates were improved by nearly 1.9% (imdb) and 1.4% (elec and rcv1)
compared with the supervised id98 (row 4), as a result of the three tv-embeddings with different
strengths complementing each other.

1 the error rate on rcv1 in row 4 slightly differs from [11] because here we did not use the stopword list.

6

imdb
elec
rcv1

concat
8.31
7.37
8.70

avg
7.83
7.24
8.62

figure 3: gn word vec-
tors integrated into our base
models. better than [13]   s
id98 (table 3, row 3).

figure 4: region tv-embeddings vs. id97 id27s. trained
on our unlabeled data. x-axis: dimensionality of the additional input to
supervised region embedding.    r:   : region,    w:   : word.

[13]   s id98 it was shown in [13] that id98 that uses the google news word vectors as input is
competitive on a number of sentence classi   cation tasks. these vectors (300-dimensional) were
trained by the authors of id97 [19] on a very large google news (gn) corpus (100 billion
words; 500   5k times larger than our unlabeled data). [13] argued that these vectors can be useful
for various tasks, serving as    universal feature extractors   . we tested [13]   s id98, which is equipped
with three convolution layers with different region sizes (3, 4, and 5) and max-pooling, using the
gn vectors as input. although [13] used only 100 neurons for each layer, we changed it to 400,
300, and 300 to match the other models, which use 1000 neurons. our models clearly outperform
these models (table 3, row 3) with relatively large differences.
comparison of embeddings besides comparing the performance of the approaches as a whole,
it is also of interest to compare the usefulness of what was learned from unlabeled data. for this
purpose, we experimented with integration of a id27 into our base models using two
methods; one takes the concatenation, and the other takes the average, of word vectors for the words
in the region. these provide additional input to the supervised embedding of regions in place of
u(cid:96)(x) in (5). that is, for comparison, we produce a region embedding from a id27
to replace a region tv-embedding. we show the results with two types of id27s: the
gn id27 above (figure 3), and id27s that we trained with the id97
software on our unlabeled data, i.e., the same data as used for tv-embedding learning and all others
(figure 4). note that figure 4 plots error rates in relation to the dimensionality of the produced
additional input; a smaller dimensionality has an advantage of faster training/prediction.
on the results,    rst, the region tv-embedding is more useful for these tasks than the tested word
embeddings since the models with a tv-embedding clearly outperform all the models with a word
embedding. word vector concatenations of much higher dimensionality than those shown in the
   gure still underperformed 100-dim region tv-embedding. second, since our region tv-embedding
takes the form of   (w    r(cid:96)(x) + b) with r(cid:96)(x) being a bow vector, the columns of w correspond
to words, and therefore, w    r(cid:96)(x) is the sum of w   s columns whose corresponding words are
in the (cid:96)-th region. based on that, one might wonder why we should not simply use the sum or
average of word vectors obtained by an existing tool such as id97 instead. the suboptimal
performances of    w: average    (figure 4) tells us that this is a bad idea. we attribute it to the fact that
region embeddings learn predictiveness of co-presence and absence of words in a region; a region
embedding can be more expressive than averaging of word vectors. thus, an effective and compact
region embedding cannot be trivially obtained from a id27. in particular, effectiveness
of the combination of three tv-embeddings (   r: 3 tv-embed.    in figure 4) stands out.
additionally, our mechanism of using information from unlabeled data is more effective than [13]   s
id98 since our id98s with gn (figure 3) outperform [13]   s id98s with gn (table 3, row 3). this
is because in our model, one-hot vectors (the original features) compensate for potential information
loss in the embedding learned from unlabeled data. this, as well as region-vs-id27, is a
major difference between our model and [13]   s model.
standard semi-supervised methods many of the standard semi-supervised methods are not ap-
plicable to id98 as they require bow vectors as input. we tested tid166 with bag-of-{1,2,3}-gram
vectors using id166light. tid166 underperformed the supervised id1662 on two of the three datasets

2 note that for feasibility, we only used the 30k most frequent id165s in the tid166 experiments, thus,
showing the id166 results also with 30k vocabulary for comparison, though on some datasets id166 performance
can be improved by use of all the id165s (e.g., 5 million id165s on imdb) [11]. this is because the
computational cost of tid166 (single-core) turned out to be high, taking several days even with 30k vocabulary.

7

6.577.588.50150300error rate (%)additional dimimdb66.577.580150300error rate (%)additional dimelec7.58.59.5error rate (%)7.588.599.50150300additional dimrcv1supervisedw: concatw: averager: unsup-tvr: 3 tv-embed.nb-lm 1-3grams [18]
[11]   s best id98
paragraph vectors [14]
ensemble of 3 models [18]
our best
table 4: imdb: previous error rates (%).

8.13
7.67
7.46
7.43
6.51

   
   

unlab.data
ens.+unlab.
unlab.data

id166 1-3grams [11]
dense nn 1-3grams [11]
nb-lm 1-3grams [11]
[11]   s best id98
our best
table 5: elec: previous error rates (%).

8.71
8.48
8.11
7.14
6.27 unlab.data

   
   
   
   

models
id166 [16]
bow-id98 [11]
bow-id98 w/ three tv-embed.

micro-f macro-f

extra resource

81.6
84.0
85.7

60.7
64.8
67.1

   
   

unlabeled data

table 6: rcv1 micro- and macro-averaged f on the multi-label task (103 topics) with the lyrl04 split.

(table 3, rows 1   2). since co-training is a meta-learner, it can be used with id98. random split of
vocabulary and split into the    rst and last half of each document were tested. to reduce the computa-
tional burden, we report the best (and unrealistic) co-training performances obtained by optimizing
the meta-parameters including when to stop on the test data. even with this unfair advantage to co-
training, co-training (table 3, row 5) clearly underperformed our models. the results demonstrate
the dif   culty of effectively using unlabeled data on these tasks, given that the size of the labeled data
is relatively large.
comparison with the previous best results we compare our models with the previous best re-
sults on imdb (table 4). our best model with three tv-embeddings outperforms the previous best
results by nearly 0.9%. all of our models with a single tv-embed. (table 3, row 6   11) also perform
better than the previous results. since elec is a relatively new dataset, we are not aware of any previ-
ous semi-supervised results. our performance is better than [11]   s best supervised id98, which has
a complex network architecture of three convolution-pooling pairs in parallel (table 5). to compare
with the benchmark results in [16], we tested our model on the multi-label task with the lyrl04
split [16] on rcv1, in which more than one out of 103 categories can be assigned to each document.
our model outperforms the best id166 of [16] and the best supervised id98 of [11] (table 6).

4 conclusion

this paper proposed a new semi-supervised id98 framework for text categorization that learns em-
beddings of text regions with unlabeled data and then labeled data. as discussed in section 1.1,
a region embedding is trained to learn the predictiveness of co-presence and absence of words in
a region. in contrast, a id27 is trained to only represent individual words in isolation.
thus, a region embedding can be more expressive than simple averaging of word vectors in spite
of their seeming similarity. our comparison of embeddings con   rmed its advantage; our region tv-
embeddings, which are trained speci   cally for the task of interest, are more effective than the tested
id27s. using our new models, we were able to achieve higher performances than the
previous studies on sentiment classi   cation and topic classi   cation.

appendix a theory of tv-embedding
suppose that we observe two views (x1, x2)     x1    x2 of the input, and a target label y     y of
interest, where x1 and x2 are    nite discrete sets.
assumption 1. assume that there exists a set of hidden states h such that x1, x2, and y are
conditionally independent given h in h, and that the rank of matrix [p (x1, x2)] is |h|.
theorem 1. consider a tv-embedding f1 of x1 w.r.t. x2. under assumption 1, there exists
a function q1 such that p (y |x1) = q1(f1(x1), y ). further consider a tv-embedding f2 of
x2 w.r.t. x1. then, under assumption 1, there exists a function q such that p (y |x1, x2) =
q(f1(x1), f2(x2), y ).

8

proof. first, assume that x1 contains d1 elements, and x2 contains d2 elements, and |h| = k. the
independence and rank condition in assumption 1 implies the decomposition

p (x2|x1) =

p (x2|h)p (h|x1)

is of rank k if we consider p (x2|x1) as a d2    d1 matrix (which we denote by a). now we may
also regard p (x2|h) as a d2    k matrix (which we denote by b), and p (h|x1) as a k    d1 matrix
(which we denote by c). from the matrix equation a = bc, we obtain c = (b(cid:62)b)   1b(cid:62)a.
consider the k    d2 matrix u = (b(cid:62)b)   1b(cid:62). then we know that its elements correspond to
a function of (h, x2)     h    x2. therefore the relationship c = ua implies that there exists a
function u(h, x2) such that

   h     h : p (h|x1) =

p (x2|x1)u(h, x2).

(cid:88)

h   h

(cid:88)

x2   x2

(cid:88)

using the de   nition of embedding in de   nition 1, we obtain

de   ne t1(a1, h) =(cid:80)

x2

p (h|x1) =
g1(a1, x2)u(h, x2), then for any h     h we have

g1(f1(x1), x2)u(h, x2).

x2   x2

similarly, there exists a function t2(a2, h) such that for any h     h

p (h|x1) = t1(f1(x1), h).

p (h|x2) = t2(f2(x2), h).

(cid:88)

p (h|x1)p (y |h, x1)

p (y, h|x1) =

h   h
p (h|x1)p (y |h) =

(cid:88)

t1(f1(x1), h)p (y |h)

(cid:88)
(cid:88)

h   h

=

observe that

p (y |x1) =

equation has used (7). by de   ning q1(a1, y ) = (cid:80)

where the third equation has used the assumption that y is independent of x1 given h and the last
h   h t1(a1, h)p (y |h), we obtain p (y |x1) =

h   h

h   h

(7)

(8)

q1(f1(x1), y ), as desired.
further observe that

p (y |x1, x2) =

(cid:88)
(cid:88)
(cid:88)

h   h

h   h

h   h

=

=

p (y, h|x1, x2)

p (h|x1, x2)p (y |h, x1, x2)

p (h|x1, x2)p (y |h),

(9)

where the last equation has used the assumption that y is independent of x1 and x2 given h.
note that

(cid:80)

p (h|x1, x2) =

=

=

=

=

p (h, x1, x2)
h(cid:48)   h p (h(cid:48), x1, x2)

p (h, x1, x2)
p (x1, x2)

=

(cid:80)
p (h)p (x1|h)p (x2|h)
h(cid:48)   h p (h(cid:48))p (x1|h(cid:48))p (x2|h(cid:48))
(cid:80)
p (h, x1)p (h, x2)/p (h)
h(cid:48)   h p (h(cid:48), x1)p (h(cid:48), x2)/p (h(cid:48))
(cid:80)
p (h|x1)p (h|x2)/p (h)
h(cid:48)   h p (h(cid:48)|x1)p (h(cid:48)|x2)/p (h(cid:48))
(cid:80)
t1(f1(x1), h)t2(f2(x2), h)/p (h)

h(cid:48)   h t1(f1(x1), h(cid:48))t2(f2(x2), h(cid:48))/p (h(cid:48))

,

9

where the third equation has used the assumption that x1 is independent of x2 given h, and
the last equation has used (7) and (8). the last equation means that p (h|x1, x2) is a func-
there exists a function   t such that p (h|x1, x2) =
tion of (f1(x1), f2(x2), h). that is,
  t(f1(x1), f2(x2), h). from (9), this implies that

p (y |x1, x2) =

now the theorem follows by de   ning q(a1, a2, y ) =(cid:80)

h   h

  t(f1(x1), f2(x2), h)p (y |h).

h   h   t(a1, a2, h)p (y |h).

(cid:88)

appendix b representation power of region embedding

we provide some formal de   nitions and theoretical arguments to support the effectiveness of the
type of region embedding experimented with in the main text.
a text region embedding is a function that maps a region of text (a sequence of two or more words)
into a numerical vector. the particular form of region embedding we consider takes either sequential
or bow representation of the text region as input. more precisely, consider a language with vocabu-
lary v . each word w in the language is taken from v , and can be represented as a |v | dimensional
vector referred to as one-hot-vector representation. each of the |v | vector components represents
a vocabulary entry. the vector representation of w has value one for the component correspond-
ing to the word, and value zeros elsewhere. a text region of size m is a sequence of m words
(w1, w2, . . . , wm), where each word wi     v . it can be represented as a m|v | dimensional vector,
which is a concatenation of vector representations of the words, as in (2) in section 1.1 of the main
text. here we call this representation seq-representation. an alternative is the bow-representation
as in (3) of the main text.
let rm be the set of all possible text regions of size m in the seq-representation (or alternatively,
bow-representation). we consider embeddings of a text region x     rm in the form of

(wx + b)+ = max(0, wx + b) .

the embedding matrix w and bias vector b are learned by training, and the training objective
depends on the task. in the following, this particular form of region embedding is referred to as
retex (region embedding of text), and the vectors produced by retex or the results of retex
are referred to as retex vectors.
the goal of region embedding learning is to map high-level concepts (relevant to the task of interest)
to low-dimensional vectors. as said in the main text, this cannot be done by id27
learning since a id27 embeds individual words in isolation (i.e., word-i is mapped to
vector-i irrespective of its context), which are too primitive to correspond to high-level concepts.
for example,    easy to use    conveys positive sentiment, but    use    in isolation does not. through
the analysis of the representation power of retex, we show that unlike id27s, retex
can model high-level concepts by using co-presence and absence of words in the region, which is
similar to the traditional use of m-grams but more ef   cient/robust.
first we show that for any (possibly nonlinear) real-valued function f (  ) de   ned on rm, there
exists a retex so that this function can be expressed in terms of a linear function of retex
vectors. this property is often referred to as universal approximation in the literature (e.g., see
https://en.wikipedia.org/wiki/universal_approximation_theorem).
proposition 1. consider a real-valued function f (  ) de   ned on rm. there exists an embedding
matrix w, bias vector b, and vector v such that f (x) = v(cid:62)(wx + b)+ for all x     rm.

proof. denote by wi,j the entry of w corresponding to the i-th row and j-th column. assume
each element in rm can be represented as a d dimensional vector with no more than m ones (and
the remaining entries are zeros). given a speci   c xi     rm, let si be a set of indexes j     {1, . . . , d}
such that the j-th component of xi is one. we create a row wi,   in w such that wi,j = 2i(j    
si)    1 for 1     j     d, where i(  ) is the set indicator function. let bi =    |si| + 1 where bi denotes
the i-th component of b. it follows that wi,  x + bi = 1 if x = xi, and wi,  x + bi     0 otherwise.
in this manner we create one row of w per every member of rm. let vi = f (xi). then it follows
that f (x) = v(cid:62)(wx + b)+.

10

the proof essentially constructs the indicator functions of all the m-grams (text regions of size m) in
rm and maps them to the corresponding function values. thus, the representation power of retex
is at least as good as m-grams, and more powerful than the sum of id27s in spite of the
seeming similarity in form. however, it is well known that the traditional m-gram-based approaches,
which assign one vector dimension per m-gram, can suffer from the data sparsity problem because
an m-gram is useful only if it is seen in the training data.
this is where retex can have clear advantages. we show below that it can map similar m-grams
(similar w.r.t. the training objective) to similar lower-dimensional vectors, which helps learning the
task of interest. it is also more expressive than the traditional m-gram-based approaches because it
can map not only co-presence but also absence of words (which m-gram cannot express concisely)
into a single dimension. these properties lead to robustness to data sparsity.
we    rst introduce a de   nition of a simple concept.
de   nition 2. consider rm of the seq-representation. a high level semantic concept c     rm is
called simple if it can be de   ned as follows. let v1, . . . , vm     v be m word groups (each word
group may either represent a set of similar words or the absent of certain words), and s1, . . . , sm    
{  1} be signs. de   ne c such that x     c if and only if the i-th word in x either belongs to vi (if
si = 1) or   vi (if si =    1).
the next proposition illustrates the points above by stating that retex has the ability to represent
a simple concept (de   ned above via the notion of similar words) by a single dimension. this is in
contrast to the construction in the proof of proposition 1, where one dimension could represent only
one m-gram.
proposition 2. the indicator function of any simple concept c can be embedded into one dimension
using retex.
proof. consider a text region vector x     rm in seq-representation that contains m of |v |-
dimensional segments, where the i-th segment represents the i-th position in the text region. let
the i-th segment of w be a vector of zeros except for those components in vi being si. let

i=1(si + 1)/2. then it is not dif   cult to check that i(x     c) = (w(cid:62)x + b)+.

b = 1    (cid:80)m

the following proposition shows that retex can embed concepts that are unions of simple con-
cepts into low-dimensional vectors.
proposition 3. if c     rm is the union of q simple concepts c1, . . . , cq, then there exists a function
f (x) that is the linear function of q-dimensional retex vectors so that x     c if and only if
f (x) > 0.
proof. let b     rq, and let w have q rows, so that i(x     ci) = (wi,  x + bi)+ for each row i, as
constructed in the proof of proposition 2. let v = [1, . . . , 1](cid:62)     rq. then f (x) = v(cid:62)(wx + b)+
is a function of the desired property.

note that q can be much smaller than the number of m-grams in concept c. proposition 3 shows
that retex has the ability to simultaneously make use of word similarity (via word groups) and the
fact that words occur in the context, to reduce the embedding dimension. a id27 can
model word similarity but does not model context. m-gram-based approaches can model context but
cannot model word similarity     which means a concept/context has to be expressed with a large
number of individual m-grams, leading to the data sparsity problem. thus, the representation power
of retex exceeds that of single-id27 and traditional m-gram-based approaches.

references
[1] rie k. ando and tong zhang. a framework for learning predictive structures from multiple tasks and

unlabeled data. journal of machine learning research, 6:1817   1853, 2005.

[2] rie k. ando and tong zhang. two-view feature generation model for semi-supervised learning.

proceedings of icml, 2007.

in

[3] yoshua bengio, r  ejean ducharme, pascal vincent, and christian jauvin. a neural probabilistic language

model. journal of marchine learning research, 3:1137   1155, 2003.

11

[4] ronan collobert and jason weston. a uni   ed architecture for natural language processing: deep neural

networks with multitask learning. in proceedings of icml, 2008.

[5] ronan collobert, jason weston, l  eon bottou, michael karlen, koray kavukcuoglu, and pavel kuksa.
natural language processing (almost) from scratch. journal of machine learning research, 12:2493   
2537, 2011.

[6] paramveer s. dhillon, dean foster, and lyle ungar. multi-view learning of id27s via cca.

in proceedings of nips, 2011.

[7] jianfeng gao, patric pantel, michael gamon, xiaodong he, and li dent. modeling interestingness with

deep neural networks. in proceedings of emnlp, 2014.

[8] xavier glorot, antoine bordes, and yoshua bengio. id20 for large-scale sentiment classi-

   cation: a deep learning approach. in proceedings of icml, 2011.

[9] geoffrey e. hinton, nitish srivastava, alex krizhevsky, ilya sutskever, and ruslan r. salakhutdinov.

improving neural networks by preventing co-adaptation of feature detectors. arxiv:1207.0580, 2012.
[10] thorsten joachims. transductive id136 for text classi   cation using support vector machines.

in

proceedings of icml, 1999.

[11] rie johnson and tong zhang. effective use of word order for text categorization with convolutional

neural networks. in proceedings of naacl hlt, 2015.

[12] nal kalchbrenner, edward grefenstette, and phil blunsom. a convolutional neural network for modeling

sentences. in proceedings of acl, pages 655   665, 2014.

[13] yoon kim. convolutional neural networks for sentence classi   cation. in proceedings of emnlp, pages

1746   1751, 2014.

[14] quoc le and tomas mikolov. distributed representations of sentences and documents. in proceedings of

icml, 2014.

[15] yann lecun, le  on bottou, yoshua bengio, and patrick haffner. gradient-based learning applied to

document recognition. in proceedings of the ieee.

[16] david d. lewis, yiming yang, tony g. rose, and fan li. rcv1: a new benchmark collection for text

categorization research. journal of marchine learning research, 5:361   397, 2004.

[17] andrew l. maas, raymond e. daly, peter t. pham, dan huang, andrew y. ng, and christopher potts.

learning word vectors for id31. in proceedings of acl, 2011.

[18] gr  egoire mesnil, tomas mikolov, marc   aurelio ranzato, and yoshua bengio. ensemble of generative
and discriminative techniques for id31 of movie reviews. arxiv:1412.5335v5 (4 feb 2015
version), 2014.

[19] tomas mikolov, ilya sutskever, kai chen, greg corrado, and jeffrey dean. distributed representations

of words and phrases and their compositionality. in proceedings of nips, 2013.

[20] andriy mnih and geoffrey e. hinton. a scalable hierarchical distributed language model. in nips, 2008.
[21] yelong shen, xiaodong he, jianfeng gao, li deng, and gr  egoire mensnil. a latent semantic model with

convolutional-pooling structure for information retrieval. in proceedings of cikm, 2014.

[22] duyu tang, furu wei, nan yang, ming zhou, ting liu, and bing qin. learning sentiment-speci   c word

embedding for twitter sentiment classi   cation. in proceedings of acl, pages 1555   1565, 2014.

[23] joseph turian, lev rainov, and yoshua bengio. word representations: a simple and general method for

semi-supervised learning. in proceedings of acl, pages 384   394, 2010.

[24] jason weston, sumit chopra, and keith adams. #tagspace: semantic embeddings from hashtags. in

proceedings of emnlp, pages 1822   1827, 2014.

[25] liheng xu, kang liu, siwei lai, and jun zhao. product feature mining: semantic clues versus syntactic

constituents. in proceedings of acl, pages 336   346, 2014.

[26] puyang xu and ruhi sarikaya. convolutional neural network based triangular crf for joint intent detec-

tion and slot    lling. in asru, 2013.

12

