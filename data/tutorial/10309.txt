deep visual-semantic alignments for generating image descriptions

andrej karpathy

li fei-fei

department of computer science, stanford university

{karpathy,feifeili}@cs.stanford.edu

5
1
0
2

 
r
p
a
4
1

 

 
 
]

v
c
.
s
c
[
 
 

2
v
6
0
3
2

.

2
1
4
1
:
v
i
x
r
a

abstract

we present a model that generates natural language de-
scriptions of images and their regions. our approach lever-
ages datasets of images and their sentence descriptions to
learn about the inter-modal correspondences between lan-
guage and visual data. our alignment model is based on a
novel combination of convolutional neural networks over
image regions, id182
over sentences, and a structured objective that aligns the
two modalities through a multimodal embedding. we then
describe a multimodal recurrent neural network architec-
ture that uses the inferred alignments to learn to generate
novel descriptions of image regions. we demonstrate that
our alignment model produces state of the art results in re-
trieval experiments on flickr8k, flickr30k and mscoco
datasets. we then show that the generated descriptions sig-
ni   cantly outperform retrieval baselines on both full images
and on a new dataset of region-level annotations.

1. introduction
a quick glance at an image is suf   cient for a human to
point out and describe an immense amount of details about
the visual scene [14]. however, this remarkable ability has
proven to be an elusive task for our visual recognition mod-
els. the majority of previous work in visual recognition
has focused on labeling images with a    xed set of visual
categories and great progress has been achieved in these en-
deavors [45, 11]. however, while closed vocabularies of vi-
sual concepts constitute a convenient modeling assumption,
they are vastly restrictive when compared to the enormous
amount of rich descriptions that a human can compose.

some pioneering approaches that address the challenge of
generating image descriptions have been developed [29,
13]. however, these models often rely on hard-coded visual
concepts and sentence templates, which imposes limits on
their variety. moreover, the focus of these works has been
on reducing complex visual scenes into a single sentence,
which we consider to be an unnecessary restriction.

in this work, we strive to take a step towards the goal of

figure 1. motivation/concept figure: our model treats language
as a rich label space and generates descriptions of image regions.

generating dense descriptions of images (figure 1). the
primary challenge towards this goal is in the design of a
model that is rich enough to simultaneously reason about
contents of images and their representation in the domain
of natural language. additionally, the model should be free
of assumptions about speci   c hard-coded templates, rules
or categories and instead rely on learning from the training
data. the second, practical challenge is that datasets of im-
age captions are available in large quantities on the internet
[21, 58, 37], but these descriptions multiplex mentions of
several entities whose locations in the images are unknown.

our core insight is that we can leverage these large image-
sentence datasets by treating the sentences as weak labels,
in which contiguous segments of words correspond to some
particular, but unknown location in the image. our ap-
proach is to infer these alignments and use them to learn
a generative model of descriptions. concretely, our contri-
butions are twofold:

    we develop a deep neural network model that in-
fers the latent alignment between segments of sen-
tences and the region of the image that they describe.

our model associates the two modalities through a
common, multimodal embedding space and a struc-
tured objective. we validate the effectiveness of this
approach on image-sentence retrieval experiments in
which we surpass the state-of-the-art.

    we introduce a multimodal recurrent neural network
architecture that takes an input image and generates
its description in text. our experiments show that the
generated sentences signi   cantly outperform retrieval-
based baselines, and produce sensible qualitative pre-
dictions. we then train the model on the inferred cor-
respondences and evaluate its performance on a new
dataset of region-level annotations.

we make code, data and annotations publicly available. 1
2. related work
dense image annotations. our work shares the high-level
goal of densely annotating the contents of images with
many works before us. barnard et al.
[2] and socher et
al.
[48] studied the multimodal correspondence between
words and images to annotate segments of images. sev-
eral works [34, 18, 15, 33] studied the problem of holistic
scene understanding in which the scene type, objects and
their spatial support in the image is inferred. however, the
focus of these works is on correctly labeling scenes, objects
and regions with a    xed set of categories, while our focus is
on richer and higher-level descriptions of regions.
generating descriptions. the task of describing images
with sentences has also been explored. a number of ap-
proaches pose the task as a retrieval problem, where the
most compatible annotation in the training set is transferred
to a test image [21, 49, 13, 43, 23], or where training an-
notations are broken up and stitched together [30, 35, 31].
several approaches generate image captions based on    xed
templates that are    lled based on the content of the image
[19, 29, 13, 55, 56, 9, 1] or generative grammars [42, 57],
but this approach limits the variety of possible outputs.
most closely related to us, kiros et al. [26] developed a log-
bilinear model that can generate full sentence descriptions
for images, but their model uses a    xed window context
while our recurrent neural network (id56) model condi-
tions the id203 distribution over the next word in a sen-
tence on all previously generated words. multiple closely
related preprints appeared on arxiv during the submission
of this work, some of which also use id56s to generate im-
age descriptions [38, 54, 8, 25, 12, 5]. our id56 is simpler
than most of these approaches but also suffers in perfor-
mance. we quantify this comparison in our experiments.
grounding natural language in images. a number of ap-
proaches have been developed for grounding text in the vi-

1cs.stanford.edu/people/karpathy/deepimagesent

sual domain [27, 39, 60, 36]. our approach is inspired by
frome et al. [16] who associate words and images through
a semantic embedding. more closely related is the work of
karpathy et al. [24], who decompose images and sentences
into fragments and infer their inter-modal alignment using a
ranking objective. in contrast to their model which is based
on grounding dependency tree relations, our model aligns
contiguous segments of sentences which are more mean-
ingful, interpretable, and not    xed in length.
neural networks in visual and language domains. mul-
tiple approaches have been developed for representing im-
ages and words in higher-level representations. on the im-
age side, convolutional neural networks (id98s) [32, 28]
have recently emerged as a powerful class of models for
image classi   cation and id164 [45]. on the sen-
tence side, our work takes advantage of pretrained word
vectors [41, 22, 3] to obtain low-dimensional representa-
tions of words. finally, recurrent neural networks have
been previously used in id38 [40, 50], but we
additionally condition these models on images.
3. our model
overview. the ultimate goal of our model is to generate
descriptions of image regions. during training, the input
to our model is a set of images and their corresponding
sentence descriptions (figure 2). we    rst present a model
that aligns sentence snippets to the visual regions that they
describe through a multimodal embedding. we then treat
these correspondences as training data for a second, multi-
modal recurrent neural network model that learns to gen-
erate the snippets.
3.1. learning to align visual and language data
our alignment model assumes an input dataset of images
and their sentence descriptions. our key insight is that sen-
tences written by people make frequent references to some
particular, but unknown location in the image. for exam-
ple, in figure 2, the words    tabby cat is leaning    refer to
the cat, the words    wooden table    refer to the table, etc.
we would like to infer these latent correspondences, with
the eventual goal of later learning to generate these snippets
from image regions. we build on the approach of karpathy
et al. [24], who learn to ground dependency tree relations
to image regions with a ranking objective. our contribu-
tion is in the use of bidirectional recurrent neural network
to compute word representations in the sentence, dispens-
ing of the need to compute dependency trees and allowing
unbounded interactions of words and their context in the
sentence. we also substantially simplify their objective and
show that both modi   cations improve ranking performance.
we    rst describe neural networks that map words and image
regions into a common, multimodal embedding. then we
introduce our novel objective, which learns the embedding

figure 2. overview of our approach. a dataset of images and their sentence descriptions is the input to our model (left). our model    rst
infers the correspondences (middle, section 3.1) and then learns to generate novel descriptions (right, section 3.2).

representations so that semantically similar concepts across
the two modalities occupy nearby regions of the space.

3.1.1 representing images
following prior work [29, 24], we observe that sentence de-
scriptions make frequent references to objects and their at-
tributes. thus, we follow the method of girshick et al. [17]
to detect objects in every image with a region convolu-
tional neural network (rid98). the id98 is pre-trained on
id163 [6] and    netuned on the 200 classes of the ima-
genet detection challenge [45]. following karpathy et al.
[24], we use the top 19 detected locations in addition to the
whole image and compute the representations based on the
pixels ib inside each bounding box as follows:

v = wm[id98  c(ib)] + bm,

(1)

where id98(ib) transforms the pixels inside bounding box
ib into 4096-dimensional activations of the fully connected
layer immediately before the classi   er. the id98 parame-
ters   c contain approximately 60 million parameters. the
matrix wm has dimensions h    4096, where h is the size
of the multimodal embedding space (h ranges from 1000-
1600 in our experiments). every image is thus represented
as a set of h-dimensional vectors {vi | i = 1 . . . 20}.

3.1.2 representing sentences
to establish the inter-modal relationships, we would like
to represent the words in the sentence in the same h-
dimensional embedding space that the image regions oc-
cupy. the simplest approach might be to project every in-
dividual word directly into this embedding. however, this
approach does not consider any ordering and word context
information in the sentence. an extension to this idea is
to use word bigrams, or dependency tree relations as pre-
viously proposed [24]. however, this still imposes an ar-
bitrary maximum size of the context window and requires
the use of dependency tree parsers that might be trained on
unrelated text corpora.

to address these concerns, we propose to use a bidirec-
tional recurrent neural network (bid56) [46] to compute
the word representations. the bid56 takes a sequence of

n words (encoded in a 1-of-k representation) and trans-
forms each one into an h-dimensional vector. however, the
representation of each word is enriched by a variably-sized
context around that word. using the index t = 1 . . . n to
denote the position of a word in a sentence, the precise form
of the bid56 is as follows:

xt = ww it
et = f (wext + be)
t = f (et + wf hf
hf
t = f (et + wbhb
hb
st = f (wd(hf
t + hb

t   1 + bf )
t+1 + bb)
t) + bd).

(2)
(3)
(4)
(5)
(6)

here, it is an indicator column vector that has a single one
at the index of the t-th word in a word vocabulary. the
weights ww specify a id27 matrix that we ini-
tialize with 300-dimensional id97 [41] weights and
keep    xed due to over   tting concerns. however, in prac-
tice we    nd little change in    nal performance when these
vectors are trained, even from random initialization. note
that the bid56 consists of two independent streams of pro-
cessing, one moving left to right (hf
t ) and the other right to
t) (see figure 3 for diagram). the    nal h-dimensional
left (hb
representation st for the t-th word is a function of both the
word at that location and also its surrounding context in the
sentence. technically, every st is a function of all words in
the entire sentence, but our empirical    nding is that the    nal
word representations (st) align most strongly to the visual
concept of the word at that location (it).

we learn the parameters we, wf , wb, wd and the respec-
tive biases be, bf , bb, bd. a typical size of the hidden rep-
resentation in our experiments ranges between 300-600 di-
mensions. we set the activation function f to the recti   ed
linear unit (relu), which computes f : x (cid:55)    max(0, x).

3.1.3 alignment objective

we have described the transformations that map every im-
age and sentence into a set of vectors in a common h-
dimensional space. since the supervision is at the level of
entire images and sentences, our strategy is to formulate an

image-sentence score as a function of the individual region-
word scores. intuitively, a sentence-image pair should have
a high matching score if its words have a con   dent support
in the image. the model of karpathy et a. [24] interprets the
i st between the i-th region and t-th word as a
dot product vt
measure of similarity and use it to de   ne the score between
image k and sentence l as:

skl =

max(0, vt

i st).

(7)

(cid:88)

(cid:88)

t   gl

i   gk

here, gk is the set of image fragments in image k and gl
is the set of sentence fragments in sentence l. the indices
k, l range over the images and sentences in the training set.
together with their additional multiple instance learning
objective, this score carries the interpretation that a sentence
fragment aligns to a subset of the image regions whenever
the dot product is positive. we found that the following
reformulation simpli   es the model and alleviates the need
for additional objectives and their hyperparameters:

skl =

maxi   gk vt

i st.

(8)

(cid:88)

t   gl

here, every word st aligns to the single best image region.
as we show in the experiments, this simpli   ed model also
leads to improvements in the    nal ranking performance.
assuming that k = l denotes a corresponding image and
sentence pair, the    nal max-margin, structured loss remains:

c(  ) =

max(0, skl     skk + 1)

k

(cid:104)(cid:88)
(cid:88)
(cid:124)
(cid:88)
(cid:124)

+

l

l

(cid:123)(cid:122)
(cid:123)(cid:122)

rank sentences

rank images

max(0, slk     skk + 1)

(cid:125)

(cid:105)

.

(cid:125)

this objective encourages aligned image-sentences pairs to
have a higher score than misaligned pairs, by a margin.

3.1.4 decoding text segment alignments to images
consider an image from the training set and its correspond-
ing sentence. we can interpret the quantity vt
i st as the un-
normalized log id203 of the t-th word describing any
of the bounding boxes in the image. however, since we are
ultimately interested in generating snippets of text instead
of single words, we would like to align extended, contigu-
ous sequences of words to a single bounding box. note that
the na    ve solution that assigns each word independently to
the highest-scoring region is insuf   cient because it leads to
words getting scattered inconsistently to different regions.
to address this issue, we treat the true alignments as latent
variables in a markov random field (mrf) where the bi-
nary interactions between neighboring words encourage an

figure 3. diagram for evaluating the image-sentence score skl.
object regions are embedded with a id98 (left). words (enriched
by their context) are embedded in the same multimodal space with
a bid56 (right). pairwise similarities are computed with inner
products (magnitudes shown in grayscale) and    nally reduced to
image-sentence score with equation 8.

alignment to the same region. concretely, given a sentence
with n words and an image with m bounding boxes, we
introduce the latent alignment variables aj     {1 . . . m} for
j = 1 . . . n and formulate an mrf in a chain structure
along the sentence as follows:

(cid:88)

(cid:88)

e(a) =

  u

j (aj) +

j=1...n

j=1...n   1

  b

j (aj, aj+1)

(10)

(9)

j (aj = t) = vt
  u
  b
j (aj, aj+1) =   1[aj = aj+1].

i st

(11)
(12)

here,    is a hyperparameter that controls the af   nity to-
wards longer word phrases. this parameter allows us to
interpolate between single-word alignments (   = 0) and
aligning the entire sentence to a single, maximally scoring
region when    is large. we minimize the energy to    nd the
best alignments a using id145. the output
of this process is a set of image regions annotated with seg-
ments of text. we now describe an approach for generating
novel phrases based on these correspondences.
3.2. multimodal recurrent neural network for

generating descriptions

in this section we assume an input set of images and their
textual descriptions. these could be full images and their
sentence descriptions, or regions and text snippets, as in-
ferred in the previous section. the key challenge is in the
design of a model that can predict a variable-sized sequence
of outputs given an image.
in previously developed lan-
guage models based on recurrent neural networks (id56s)
[40, 50, 10], this is achieved by de   ning a id203 distri-
bution of the next word in a sequence given the current word
and context from previous time steps. we explore a simple

but effective extension that additionally conditions the gen-
erative process on the content of an input image. more for-
mally, during training our multimodal id56 takes the image
pixels i and a sequence of input vectors (x1, . . . , xt ). it
then computes a sequence of hidden states (h1, . . . , ht) and
a sequence of outputs (y1, . . . , yt) by iterating the following
recurrence relation for t = 1 to t :

bv = whi[id98  c (i)]
(13)
ht = f (whxxt + whhht   1 + bh + 1(t = 1) (cid:12) bv) (14)
(15)
yt = sof tmax(wohht + bo).
in the equations above, whi, whx, whh, woh, xi and bh, bo
are learnable parameters, and id98  c(i) is the last layer of
a id98. the output vector yt holds the (unnormalized) log
probabilities of words in the dictionary and one additional
dimension for a special end token. note that we provide
the image context vector bv to the id56 only at the    rst
iteration, which we found to work better than at each time
step. in practice we also found that it can help to also pass
both bv, (whxxt) through the activation function. a typical
size of the hidden layer of the id56 is 512 neurons.
id56 training. the id56 is trained to combine a word (xt),
the previous context (ht   1) to predict the next word (yt).
we condition the id56   s predictions on the image informa-
tion (bv) via bias interactions on the    rst step. the training
proceeds as follows (refer to figure 4): we set h0 = (cid:126)0, x1 to
a special start vector, and the desired label y1 as the    rst
word in the sequence. analogously, we set x2 to the word
vector of the    rst word and expect the network to predict
the second word, etc. finally, on the last step when xt rep-
resents the last word, the target label is set to a special end
token. the cost function is to maximize the log id203
assigned to the target labels (i.e. softmax classi   er).
id56 at test time. to predict a sentence, we compute the
image representation bv, set h0 = 0, x1 to the start vec-
tor and compute the distribution over the    rst word y1. we
sample a word from the distribution (or pick the argmax),
set its embedding vector as x2, and repeat this process until
the end token is generated. in practice we found that beam
search (e.g. beam size 7) can improve results.

3.3. optimization
we use sgd with mini-batches of 100 image-sentence pairs
and momentum of 0.9 to optimize the alignment model. we
cross-validate the learning rate and the weight decay. we
also use dropout id173 in all layers except in the
recurrent layers [59] and clip gradients elementwise at 5
(important). the generative id56 is more dif   cult to op-
timize, party due to the word frequency disparity between
rare words and common words (e.g.    a    or the end token).
we achieved the best results using rmsprop [52], which is
an adaptive step size method that scales the update of each
weight by a running average of its gradient norm.

figure 4. diagram of our multimodal recurrent neural network
generative model. the id56 takes a word, the context from previ-
ous time steps and de   nes a distribution over the next word in the
sentence. the id56 is conditioned on the image information at the
   rst time step. start and end are special tokens.

4. experiments
datasets. we use the flickr8k [21], flickr30k [58] and
mscoco [37] datasets in our experiments. these datasets
contain 8,000, 31,000 and 123,000 images respectively
and each is annotated with 5 sentences using amazon
mechanical turk. for flickr8k and flickr30k, we use
1,000 images for validation, 1,000 for testing and the rest
for training (consistent with [21, 24]). for mscoco we
use 5,000 images for both validation and testing.
id174. we convert all sentences to lower-
case, discard non-alphanumeric characters. we    lter words
to those that occur at least 5 times in the training set,
which results in 2538, 7414, and 8791 words for flickr8k,
flickr30k, and mscoco datasets respectively.
4.1. image-sentence alignment evaluation
we    rst investigate the quality of the inferred text and image
alignments with ranking experiments. we consider a with-
held set of images and sentences and retrieve items in one
modality given a query from the other by sorting based on
the image-sentence score skl (section 3.1.3). we report the
median rank of the closest ground truth result in the list and
recall@k, which measures the fraction of times a correct
item was found among the top k results. the result of these
experiments can be found in table 1, and example retrievals
in figure 5. we now highlight some of the takeaways.
our full model outperforms previous work. first, our
full model (   our model: bid56   ) outperforms socher et
al. [49] who trained with a similar loss but used a single
image representation and a id56 over
the sentence. a similar loss was adopted by kiros et al.
[25], who use an lstm [20] to encode sentences. we list
their performance with a id98 that is equivalent in power
(alexnet [28]) to the one used in this work, though simi-
lar to [54] they outperform our model with a more powerful
id98 (vggnet [47], googlenet [51]).    defrag    are the
results reported by karpathy et al. [24]. since we use dif-
ferent word vectors, dropout for id173 and different
cross-validation ranges and larger embedding sizes, we re-
implemented their loss for a fair comparison (   our imple-

model

image annotation

image search

r@1 r@5 r@10 med r r@1 r@5 r@10 med r

sdt-id56 (socher et al. [49])
kiros et al. [25]
mao et al. [38]
donahue et al. [8]
defrag (karpathy et al. [24])
our implementation of defrag [24]
our model: deptree edges
our model: bid56
vinyals et al. [54] (more powerful id98)

our model: 1k test images
our model: 5k test images

9.6
14.8
18.4
17.5
14.2
19.2
20.0
22.2
23

38.4
16.5

flickr30k
29.8
39.2
40.2
40.3
37.7
44.5
46.6
48.2

-

41.1
50.9
50.9
50.8
51.3
58.0
59.4
61.4
63

mscoco
69.9
39.2

80.5
52.0

16
10
10
9
10
6.0
5.4
4.8
5

1.0
9.0

8.9
11.8
12.6

-

10.2
12.9
15.0
15.2
17

27.4
10.7

29.8
34.0
31.2

-

30.8
35.4
36.5
37.7

-

60.2
29.6

41.1
46.3
41.5

-

44.2
47.5
48.2
50.5
57

74.8
42.2

16
13
16
-
14
10.8
10.4
9.2
8

3.0
14.0

table 1. image-sentence ranking experiment results. r@k is recall@k (high is good). med r is the median rank (low is good). in the
results for our models, we take the top 5 validation set models, evaluate each independently on the test set and then report the average
performance. the standard deviations on the recall values range from approximately 0.5 to 1.0.

figure 5. example alignments predicted by our model. for every test image above, we retrieve the most compatible test sentence and
visualize the highest-scoring region for each word (before mrf smoothing described in section 3.1.4) and the associated scores (vt
i st).
we hide the alignments of low-scoring words to reduce clutter. we assign each region an arbitrary color.

mentation of defrag   ). compared to other work that uses
alexnets, our full model shows consistent improvement.
our simpler cost function improves performance. we
strive to better understand the source of our performance.
first, we removed the bid56 and used dependency tree re-
lations exactly as described in karpathy et al. [24] (   our
model: deptree edges   ). the only difference between this
model and    our reimplementation of defrag    is the new,
simpler cost function introduced in section 3.1.3. we see
that our formulation shows consistent improvements.
bid56 outperforms dependency tree relations. further-
more, when we replace the dependency tree relations with
the bid56 we observe additional performance improve-
ments. since the dependency relations were shown to work
better than single words and bigrams [24], this suggests that
the bid56 is taking advantage of contexts longer than two
words. furthermore, our method does not rely on extracting
a dependency tree and instead uses the raw words directly.
mscoco results for future comparisons. we are not
aware of other published ranking results on mscoco.

therefore, we report results on a subset of 1,000 images
and the full set of 5,000 test images for future comparisons.
note that the 5000 images numbers are lower since re-
call@k is a function of test set size.
qualitative. as can be seen from example groundings in
figure 5, the model discovers interpretable visual-semantic
correspondences, even for small or relatively rare objects
such as an    accordion   . these would be likely missed by
models that only reason about full images.
learned region and word vector magnitudes. an ap-
pealing feature of our model is that it learns to modulate
the magnitude of the region and id27s. due
to their inner product interaction, we observe that repre-
sentations of visually discriminative words such as    kayak-
ing, pumpkins    have embedding vectors with higher mag-
nitudes, which in turn translates to a higher in   uence on
the image-sentence score. conversely, stop words such as
   now, simply, actually, but    are mapped near the origin,
which reduces their in   uence. see more analysis in supple-
mentary material.

model
b-1
   
nearest neighbor
mao et al. [38]
58
63
google nic [54]
   
lrcn [8]
ms research [12]
   
chen and zitnick [5]    
our model
57.9

cider
38.3
   
   
   
   
   
66.0
table 2. evaluation of full image predictions on 1,000 test images. b-n is id7 score that uses up to id165s. high is good in all columns.
for future comparisons, our meteor/cider flickr8k scores are 16.7/31.8 and the flickr30k scores are 15.3/24.7.

flickr30k
b-4
b-1
b-3
b-2
    48.0
   
   
   
   
20
24
66.6
18.3
27.7
42.3
16.5
62.8
39.1
25.1
   
   
   
   
    12.6    
   
36.9
24.0
62.5

flickr8k
b-1
b-4
b-3
b-2
   
   
   
   
   
55
23
28
    66.3
27
41
    58.8
   
   
   
   
   
   
    14.1    
   
24.5
38.3
57.3

mscoco 2014
b-4 meteor
b-3
10.0
16.6
   
   
32.9
24.6
30.4    
    21.1
    19.0
32.1
23.0

b-2
28.1
   
46.1
44.2
   
   
45.0

15.7
   
   
   
20.7
20.4
19.5

16.0

15.7

figure 6. example sentences generated by the multimodal id56 for test images. we provide many more examples on our project page.

4.2. generated descriptions: fulframe evaluation
we now evaluate the ability of our id56 model to describe
images and regions. we    rst trained our multimodal id56
to generate sentences on full images with the goal of veri-
fying that the model is rich enough to support the mapping
from image data to sequences of words. for these full im-
age experiments we use the more powerful vggnet image
features [47]. we report the id7 [44], meteor [7] and
cider [53] scores computed with the coco-caption
code [4] 2. each method evaluates a candidate sentence
by measuring how well it matches a set of    ve reference
sentences written by humans.
qualitative. the model generates sensible descriptions of
images (see figure 6), although we consider the last two
images failure cases. the    rst prediction    man in black
shirt is playing a guitar    does not appear in the training set.
however, there are 20 occurrences of    man in black shirt   
and 60 occurrences of    is paying guitar   , which the model
may have composed to describe the    rst image. in general,
we    nd that a relatively large portion of generated sentences
(60% with beam size 7) can be found in the training data.
this fraction decreases with lower beam size; for instance,
with beam size 1 this falls to 25%, but the performance also
deteriorates (e.g. from 0.66 to 0.61 cider).
multimodal id56 outperforms retrieval baseline. our
   rst comparison is to a nearest neighbor retrieval baseline.

2https://github.com/tylin/coco-caption

here, we annotate each test image with a sentence of the
most similar training set image as determined by l2 norm
over vggnet [47] fc7 features. table 2 shows that the mul-
timodal id56 con   dently outperforms this retrieval method.
hence, even with 113,000 train set images in mscoco
the retrieval approach is inadequate. additionally, the id56
takes only a fraction of a second to evaluate per image.

comparison to other work. several related models have
been proposed in arxiv preprints since the original submis-
sion of this work. we also include these in table 2 for com-
parison. most similar to our model is vinyals et al. [54].
unlike this work where the image information is commu-
nicated through a bias term on the    rst step, they incorpo-
rate it as a    rst word, they use a more powerful but more
complex sequence learner (lstm [20]), a different id98
(googlenet [51]), and report results of a model ensemble.
donahue et al. [8] use a 2-layer factored lstm (similar
in structure to the id56 in mao et al. [38]). both models
appear to work worse than ours, but this is likely in large
part due to their use of the less powerful alexnet [28] fea-
tures. compared to these approaches, our model prioritizes
simplicity and speed at a slight cost in performance.

4.3. generated descriptions: region evaluation
we now train the multimodal id56 on the correspondences
between image regions and snippets of text, as inferred by
the alignment model. to support the evaluation, we used
amazon mechanical turk (amt) to collect a new dataset

figure 7. example region predictions. we use our region-level multimodal id56 to generate text (shown on the right of each image) for
some of the bounding boxes in each image. the lines are grounded to centers of bounding boxes and the colors are chosen arbitrarily.

of region-level annotations that we only use at test time. the
labeling interface displayed a single image and asked anno-
tators (we used nine per image) to draw    ve bounding boxes
and annotate each with text. in total, we collected 9,000 text
snippets for 200 images in our mscoco test split (i.e. 45
snippets per image). the snippets have an average length of
2.3 words. example annotations include    sports car   ,    el-
derly couple sitting   ,    construction site   ,    three dogs on
leashes   ,    chocolate cake   . we noticed that asking anno-
tators for grounded text snippets induces language statistics
different from those in full image captions. our region an-
notations are more comprehensive and feature elements of
scenes that would rarely be considered salient enough to be
included in a single sentence sentence about the full image,
such as    heating vent   ,    belt buckle   , and    chimney   .
qualitative. we show example region model predictions
in figure 7. to reiterate the dif   culty of the task, consider
for example the phrase    table with wine glasses    that is
generated on the image on the right in figure 7. this phrase
only occurs in the training set 30 times. each time it may
have a different appearance and each time it may occupy a
few (or none) of our object bounding boxes. to generate
this string for the region, the model had to    rst correctly
learn to ground the string and then also learn to generate it.
region model outperforms full frame model and rank-
ing baseline. similar to the full image description task, we
evaluate this data as a prediction task from a 2d array of
pixels (one image region) to a sequence of words and record
the id7 score. the ranking baseline retrieves training
sentence substrings most compatible with each region as
judged by the bid56 model. table 3 shows that the region
id56 model produces descriptions most consistent with our
collected data. note that the fullframe model was trained
only on full images, so feeding it smaller image regions
deteriorates its performance. however, its sentences are
also longer than the region model sentences, which likely
negatively impacts the id7 score. the sentence length
is non-trivial to control for with an id56, but we note that
the region model also outperforms the fullframe model on
all other metrics: cider 61.6/20.3, meteor 15.8/13.3,
id8 35.1/21.0 for region/fullframe respectively.

b-2
45.2
10.5
6.0
23.0

b-3
30.1
0.0
2.2
16.1

b-1
61.5
22.9
14.2
35.2

model
human agreement
nearest neighbor
id56: fullframe model
id56: region level model

b-4
22.0
0.0
0.0
14.8
table 3. id7 score evaluation of image region annotations.
4.4. limitations
although our results are encouraging, the multimodal id56
model is subject to multiple limitations. first, the model can
only generate a description of one input array of pixels at a
   xed resolution. a more sensible approach might be to use
multiple saccades around the image to identify all entities,
their mutual interactions and wider context before generat-
ing a description. additionally, the id56 receives the image
information only through additive bias interactions, which
are known to be less expressive than more complicated mul-
tiplicative interactions [50, 20]. lastly, our approach con-
sists of two separate models. going directly from an image-
sentence dataset to region-level annotations as part of a sin-
gle model trained end-to-end remains an open problem.
5. conclusions
we introduced a model that generates natural language de-
scriptions of image regions based on weak labels in form of
a dataset of images and sentences, and with very few hard-
coded assumptions. our approach features a novel ranking
model that aligned parts of visual and language modalities
through a common, multimodal embedding. we showed
that this model provides state of the art performance on
image-sentence ranking experiments. second, we described
a multimodal recurrent neural network architecture that
generates descriptions of visual data. we evaluated its per-
formance on both fullframe and region-level experiments
and showed that in both cases the multimodal id56 outper-
forms retrieval baselines.
acknowledgements.
we thank justin johnson and jon krause for helpful com-
ments and discussions. we gratefully acknowledge the sup-
port of nvidia corporation with the donation of the gpus
used for this research. this research is partially supported
by an onr muri grant, and nsf iss-1115313.

references

[1] a. barbu, a. bridge, z. burchill, d. coroian, s. dickin-
son, s. fidler, a. michaux, s. mussman, s. narayanaswamy,
d. salvi, et al. video in sentences out. arxiv preprint
arxiv:1204.2742, 2012.

[2] k. barnard, p. duygulu, d. forsyth, n. de freitas, d. m.
blei, and m. i. jordan. matching words and pictures. jmlr,
2003.

[3] y. bengio, h. schwenk, j.-s. sen  ecal, f. morin, and j.-l.
gauvain. neural probabilistic language models. in innova-
tions in machine learning. springer, 2006.

[4] x. chen, h. fang, t.-y. lin, r. vedantam, s. gupta, p. dol-
lar, and c. l. zitnick. microsoft coco captions: data collec-
tion and evaluation server. arxiv preprint arxiv:1504.00325,
2015.

[5] x. chen and c. l. zitnick.

learning a recurrent vi-
sual representation for image id134. corr,
abs/1411.5654, 2014.

[6] j. deng, w. dong, r. socher, l.-j. li, k. li, and l. fei-
fei. id163: a large-scale hierarchical image database. in
cvpr, 2009.

[7] m. denkowski and a. lavie. meteor universal: language
in
speci   c translation evaluation for any target language.
proceedings of the eacl 2014 workshop on statistical ma-
chine translation, 2014.

[8] j. donahue, l. a. hendricks, s. guadarrama, m. rohrbach,
s. venugopalan, k. saenko, and t. darrell. long-term recur-
rent convolutional networks for visual recognition and de-
scription. arxiv preprint arxiv:1411.4389, 2014.

[9] d. elliott and f. keller. image description using visual de-
in emnlp, pages 1292   1302,

pendency representations.
2013.

[10] j. l. elman. finding structure in time. cognitive science,

14(2):179   211, 1990.

[11] m. everingham, l. van gool, c. k. i. williams, j. winn, and
a. zisserman. the pascal visual object classes (voc) chal-
lenge. international journal of id161, 88(2):303   
338, june 2010.

[12] h. fang, s. gupta, f. iandola, r. srivastava, l. deng,
p. doll  ar, j. gao, x. he, m. mitchell, j. platt, et al.
from captions to visual concepts and back. arxiv preprint
arxiv:1411.4952, 2014.

[13] a. farhadi, m. hejrati, m. a. sadeghi, p. young,
c. rashtchian, j. hockenmaier, and d. forsyth. every pic-
ture tells a story: generating sentences from images.
in
eccv. 2010.

[14] l. fei-fei, a. iyer, c. koch, and p. perona. what do we
perceive in a glance of a real-world scene? journal of vision,
7(1):10, 2007.

[15] s. fidler, a. sharma, and r. urtasun. a sentence is worth a

thousand pixels. in cvpr, 2013.

[16] a. frome, g. s. corrado, j. shlens, s. bengio, j. dean,
t. mikolov, et al. devise: a deep visual-semantic embed-
ding model. in nips, 2013.

[17] r. girshick, j. donahue, t. darrell, and j. malik. rich fea-
ture hierarchies for accurate id164 and semantic
segmentation. in cvpr, 2014.

[18] s. gould, r. fulton, and d. koller. decomposing a scene
into geometric and semantically consistent regions. in com-
puter vision, 2009 ieee 12th international conference on,
pages 1   8. ieee, 2009.

[19] a. gupta and p. mannem. from image annotation to im-
age description. in neural information processing. springer,
2012.

[20] s. hochreiter and j. schmidhuber. long short-term memory.

neural computation, 9(8):1735   1780, 1997.

[21] m. hodosh, p. young, and j. hockenmaier. framing image
description as a ranking task: data, models and evaluation
metrics. journal of arti   cial intelligence research, 2013.

[22] r. jeffreypennington and c. manning. glove: global vec-

tors for word representation. 2014.

[23] y. jia, m. salzmann, and t. darrell. learning cross-modality

similarity for multinomial data. in iccv, 2011.

[24] a. karpathy, a. joulin, and l. fei-fei. deep fragment em-
beddings for bidirectional image sentence mapping. arxiv
preprint arxiv:1406.5679, 2014.

[25] r. kiros, r. salakhutdinov, and r. s. zemel. unifying
visual-semantic embeddings with multimodal neural lan-
guage models. arxiv preprint arxiv:1411.2539, 2014.

[26] r. kiros, r. s. zemel, and r. salakhutdinov. multimodal

neural language models. icml, 2014.

[27] c. kong, d. lin, m. bansal, r. urtasun, and s. fidler. what
are you talking about? text-to-image coreference. in cvpr,
2014.

[28] a. krizhevsky, i. sutskever, and g. e. hinton.

classi   cation with deep convolutional neural networks.
nips, 2012.

id163
in

[29] g. kulkarni, v. premraj, s. dhar, s. li, y. choi, a. c. berg,
and t. l. berg. baby talk: understanding and generating
simple image descriptions. in cvpr, 2011.

[30] p. kuznetsova, v. ordonez, a. c. berg, t. l. berg, and
y. choi. collective generation of natural image descriptions.
in acl, 2012.

[31] p. kuznetsova, v. ordonez, t. l. berg, u. c. hill, and
y. choi. treetalk: composition and compression of trees
for image descriptions. transactions of the association for
computational linguistics, 2(10):351   362, 2014.

[32] y. lecun, l. bottou, y. bengio, and p. haffner. gradient-
based learning applied to document recognition. proceed-
ings of the ieee, 86(11):2278   2324, 1998.

[33] l.-j. li and l. fei-fei. what, where and who? classifying

events by scene and object recognition. in iccv, 2007.

[34] l.-j. li, r. socher, and l. fei-fei. towards total scene un-
derstanding: classi   cation, annotation and segmentation in
in id161 and pattern
an automatic framework.
recognition, 2009. cvpr 2009. ieee conference on, pages
2036   2043. ieee, 2009.

[35] s. li, g. kulkarni, t. l. berg, a. c. berg, and y. choi. com-
posing simple image descriptions using web-scale id165s.
in conll, 2011.

[36] d. lin, s. fidler, c. kong, and r. urtasun. visual semantic
search: retrieving videos via complex textual queries. 2014.

[54] o. vinyals, a. toshev, s. bengio, and d. erhan. show
and tell: a neural image caption generator. arxiv preprint
arxiv:1411.4555, 2014.

[55] y. yang, c. l. teo, h. daum  e iii, and y. aloimonos.
in

corpus-guided sentence generation of natural images.
emnlp, 2011.

[56] b. z. yao, x. yang, l. lin, m. w. lee, and s.-c. zhu. i2t:
image parsing to text description. proceedings of the ieee,
98(8):1485   1508, 2010.

[57] m. yatskar, l. vanderwende, and l. zettlemoyer. see no
evil, say no evil: description generation from densely la-
beled images. lexical and computational semantics, 2014.

[58] p. young, a. lai, m. hodosh, and j. hockenmaier. from im-
age descriptions to visual denotations: new similarity met-
rics for semantic id136 over event descriptions. tacl,
2014.

[59] w. zaremba, i. sutskever, and o. vinyals. recurrent neu-
ral network id173. arxiv preprint arxiv:1409.2329,
2014.

[60] c. l. zitnick, d. parikh, and l. vanderwende. learning the

visual interpretation of sentences. iccv, 2013.

[37] t.-y. lin, m. maire, s. belongie, j. hays, p. perona, d. ra-
manan, p. doll  ar, and c. l. zitnick. microsoft coco: com-
mon objects in context. arxiv preprint arxiv:1405.0312,
2014.

[38] j. mao, w. xu, y. yang, j. wang, and a. l. yuille. explain
images with multimodal recurrent neural networks. arxiv
preprint arxiv:1410.1090, 2014.

[39] c. matuszek*, n. fitzgerald*, l. zettlemoyer, l. bo, and
d. fox. a joint model of language and perception for
grounded attribute learning. in proc. of the 2012 interna-
tional conference on machine learning, edinburgh, scot-
land, june 2012.

[40] t. mikolov, m. kara     at, l. burget, j. cernock`y, and s. khu-
danpur. recurrent neural network based language model. in
interspeech, 2010.

[41] t. mikolov, i. sutskever, k. chen, g. s. corrado, and
j. dean. distributed representations of words and phrases
and their compositionality. in nips, 2013.

[42] m. mitchell, x. han, j. dodge, a. mensch, a. goyal,
a. berg, k. yamaguchi, t. berg, k. stratos, and h. daum  e,
iii. midge: generating image descriptions from computer
vision detections. in eacl, 2012.

[43] v. ordonez, g. kulkarni, and t. l. berg. im2text: describ-
ing images using 1 million captioned photographs. in nips,
2011.

[44] k. papineni, s. roukos, t. ward, and w.-j. zhu. id7: a
method for automatic evaluation of machine translation. in
proceedings of the 40th annual meeting on association for
computational linguistics, pages 311   318. association for
computational linguistics, 2002.

[45] o. russakovsky, j. deng, h. su, j. krause, s. satheesh,
s. ma, z. huang, a. karpathy, a. khosla, m. bernstein,
a. c. berg, and l. fei-fei. id163 large scale visual recog-
nition challenge, 2014.

[46] m. schuster and k. k. paliwal. bidirectional recurrent neural
networks. signal processing, ieee transactions on, 1997.
[47] k. simonyan and a. zisserman. very deep convolutional
networks for large-scale image recognition. arxiv preprint
arxiv:1409.1556, 2014.

[48] r. socher and l. fei-fei. connecting modalities: semi-
supervised segmentation and annotation of images using un-
aligned text corpora. in cvpr, 2010.

[49] r. socher, a. karpathy, q. v. le, c. d. manning, and a. y.
ng. grounded id152 for    nding and de-
scribing images with sentences. tacl, 2014.

[50] i. sutskever, j. martens, and g. e. hinton. generating text

with recurrent neural networks. in icml, 2011.

[51] c. szegedy, w. liu, y. jia, p. sermanet, s. reed,
d. anguelov, d. erhan, v. vanhoucke, and a. rabi-
novich. going deeper with convolutions. arxiv preprint
arxiv:1409.4842, 2014.

[52] t. tieleman and g. e. hinton. lecture 6.5-rmsprop: divide
the gradient by a running average of its recent magnitude.,
2012.

[53] r. vedantam, c. l. zitnick, and d. parikh.
consensus-based image description evaluation.
abs/1411.5726, 2014.

cider:
corr,

6. supplementary material

6.1. magnitude modulation
an appealing feature of our alignment model is that it learns
to modulate the importance of words and regions by scaling
the magnitude of their corresponding embedding vectors.
to see this, recall that we compute the image-sentence sim-
ilarity between image k and sentence l as follows:

skl =

maxi   gk vt

i st.

(16)

(cid:88)

t   gl

disciminative words. as a result of this formulation,
we observe that representations of visually discriminative
words such as    kayaking, pumpkins    tend to have higher
magnitude in the embedding space, which translates to a
higher in   uence on the    nal image-sentence scores due to
the inner product. conversely, the model learns to map stop
words such as    now, simply, actually, but    near the ori-
gin, which reduces their in   uence. table 4 show the top
40 words with highest and lowest magnitudes (cid:107)st(cid:107).
disciminative regions. similarly, image regions that con-
tain discriminative entities are assigned vectors of higher
magnitudes by our model. this can be be interpreted as a
measure of visual saliency, since these regions would pro-
duced large scores if their textual description was present in
a corresponding sentence. we show the regions with high
magnitudes in figure 8. notice the common occurrence of
often described regions such as balls, bikes, helmets.

figure 8. flickr30k test set regions with high vector magnitude.

magnitude word
now
simply
actually
but
neither
then
still
obviously
that

felt
not

because
appeared
therefore
been
if
also
only
so

0.42
0.42
0.43
0.44
0.44
0.45
0.45
0.46
0.47
0.47 which
0.47
0.47
0.47 might
0.47
0.48
0.48
0.48
0.48
0.48
0.48
0.48
0.49 would
0.49
0.50
0.50
0.50
0.50
0.50
0.50 without
0.51
0.51
0.51
0.51
0.51
0.51 when
0.51
0.51
0.51
0.52
0.52

already
being
else
just
ones

yet
be
had
revealed
never
very

they
either
could
feel
otherwise

magnitude word

kayaking
trampoline
pumpkins

2.61
2.59
2.59
2.58 windsur   ng
2.56 wakeboard
acrobatics
2.54
2.54
sousaphone
2.54
skydivers
2.52 wakeboarders
skateboard
2.52
2.51
snowboarder
2.51 wakeboarder
skydiving
2.50
guitar
2.50
snowboard
2.50
kitchen
2.48
paraglider
2.48
ollie
2.48
   retruck
2.47
2.47
gymnastics
2.46 waterfalls
2.46 motorboat
fryer
2.46
skateboarding
2.46
2.46
dulcimer
2.46 waterfall
back   ips
2.46
unicyclist
2.46
kayak
2.45
2.43
costumes
2.43 wakeboarding
2.43
2.42
2.42
2.42
2.42
2.41
2.41
2.40
2.40

trike
dancers
cupcakes
tuba
skijoring
   rewood
elevators
cranes
bassoon

table 4. this table shows the top magnitudes of vectors ((cid:107)st(cid:107)) for
words in flickr30k. since the magnitude of individual words in
our model is also a function of their surrounding context in the
sentence, we report the average magnitude.

6.2. alignment model

learned appearance of text snippets. we can query our
alignment model with a piece of text and retrieve individual
image regions that have the highest score with that snip-
pet. we show examples of such queries in figure 9 and
figure 10. notice that the model is sensitive to compound
words and modi   ers. for example,    red bus    and    yel-
low bus    give very different results. similarly,    bird    ying
in the sky    and    bird on a tree branch    give different re-
sults. additionally, it can be seen that the quality of the
results deteriorates for less frequently occurring concepts,
such as    roof    or    straw hat   . however, we emphasize that
the model learned these visual appearances of text snippets
from raw data of full images and sentences, without any ex-
plicit correspondences.
additional alignment visualizations. see additional ex-
amples of inferred alignments between image regions and
words in figure 11. note that one limitation of our model is
that it does not explicitly handle or support counting. for in-
stance, the last example we show contains the phrase    three
people   . these words should align to the three people in
the image, but our model puts the bounding box around two
of the people. in doing so, the model may be taking advan-
tage of the bid56 structure to modify the    people    vector
to preferentially align to regions that contain multiple peo-
ple. however, this is still unsatisfying because such spuri-
ous detections only exist as a result of an error in the rid98
id136 process, which presumably failed to localize the
individual people.
web demo. we have published a web demo that displays
our alignments for all images in the test set 3.
additional flickr8k experiments. we omitted ranking
experiment results from our paper due to space constraints,
but these can be found in table 5
counting. we experimented with losses that perform prob-
abilistic id136 in the forward pass that explicitly tried
to localize exactly three distinct people in the image. how-
ever, this worked poorly because while the rid98 is good
at    nding people, it is not very good at localizing them. for
instance, a single person can easily yield multiple detections
(the head, the torso, or the full body, for example). we were
not able to come up with a simple approach to collapsing
these into a single detection (non-maxim suppression by it-
self was not suf   cient in our experiments). note that this
ambiguity is partly an artifact of the training data. for ex-
ample, torsos of people can often be labeled alone if the
body is occluded. we are therefore lead to believe that this
additional modeling step is highly non-trivial and a worthy
subject of future work.

3http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/

plug and play use of natural language processing
toolkits. before adopting the bid56 approach, we also
tried to use natural language processing toolkits to process
the input sentences into graphs of noun phrases and their bi-
nary relations. for instance, in the sentence    a brown dog is
chasing a young child   , the toolkit would infer that there are
two noun phrases (   a brown dog   ,    young child   ), joined
by a binary relationship of    chasing   . we then developed
a crf that inferred the grounding of these noun phrases to
the detection bounding boxes in the image with a unary ap-
pearance model and a spatial binary model. however, this
endeavor proved fruitless. first, performing crf-like infer-
ence during the forward pass of a neural network proved
to be extremely slow. second, we found that there is sur-
prisingly little information in the relative spatial positions
between bounding boxes. for instance, almost any two
bounding boxes in the image could correspond to the ac-
tion of    chasing    due to huge amount of possibly camera
views of a scene. hence, we were unable to extract enough
signal from the binary relations in the coordinate system
of the image and suspect that more complex 3-dimensional
reasoning may be required. lastly, we found that nlp tools
(when used out of the box) introduce a large amount of mis-
takes in the extracted parse trees, dependency trees and parts
of speech tags. we tried to    x these with complex rules and
exceptions, but ultimately decided to abandon the idea. we
believe that part of the problem is that these tools are usually
trained on different text corpora (e.g. news articles), so im-
age captions are outside of their domain of competence. in
our experience, adopting the bid56 model instead of this
approach provided immediate performance improvements
and produced signi   cant reductions in code complexity.

6.3. additional examples: image annotation

additional examples of generated captions on the full im-
age level can be found in figure 12 (and our website). the
model often gets the right gist of the scene, but sometimes
guesses speci   c    ne-grained words incorrectly. we expect
that reasoning not only the global level of the image but also
on the level of objects will signi   cantly improve these re-
sults. we    nd the last example (   woman in bikini is jumping
over hurdle   ) to be especially illuminating. this sentence
does not occur in the training data. our general qualitative
impression of the model is that it learns certain templates,
e.g.    <noun>in <noun>is <verb>in <noun>   , and then
   lls these in based on textures in the image. in this partic-
ular case, the volleyball net has the visual appearance of a
hurdle, which may have caused the model to insert it as a
noun (along with the woman) into one of its learned sen-
tence templates.

6.4. additional examples: region annotation

additional examples of region annotations can be found
in figure 13. note that we annotate regions based on the
content of each image region alone, which can cause erro-
neous predictions when not enough context is available in
the bounding box (e.g. a generated description that says
   container    detected on the back of a dog   s head in the im-
age on the right, in the second row). we found that one ef-
fective way of using the contextual information and improv-
ing the predictions is to concatenate the fullframe feature
id98 vector to the vector of the region of interest, giving
8192-dimensional input vector the to id56. however, we
chose to omit these experiments in our paper to preserve the
simplicity of the mode, and because we believe that cleaner
and more principled approaches to this challenge can be de-
veloped.

6.5. training the multimodal id56

there are a few tricks needed to get the multimodal id56 to
train ef   ciently. we found that clipping the gradients (we
only experimented with simple per-element clipping) at an
appropriate value consistently gave better results and helped
on the validation data. as mentioned in our paper, we exper-
imented with sgd, sgd+momentum, adadelta, adagrad,
but found rmsprop to give best results. however, some
sgd checkpoints usually also converged to nearby valida-
tion performance vicinity. moreover, the distribution of the
words in english language are highly non-uniform. there-
fore, the model spends the    rst few iterations mostly learn-
ing the biases for the softmax classi   er such that it is pre-
dicting every word at random with the appropriate dataset
frequency. we found that we could obtain faster conver-
gence early in the training (and nicer loss curves) by explic-
itly initializing the biases of all words in the dictionary (in
the softmax classi   er) to log id203 of their occurrence
in the training data. therefore, with small weights and bi-
ases set appropriately the model right away predicts word
at random according to their chance distribution. after sub-
mission of our original paper we performed additional ex-
periments with comparing an id56 to an lstm and found
that lstms consistently produced better results, but took
longer to train. lastly, we initially used id97 vectors
as our word representations xi, but found that it was suf   -
cient to train these vectors from random initialization with-
out changes in the    nal performance. moreover, we found
that the id97 vectors have some unappealing properties
when used in multimodal language-visual tasks. for exam-
ple, all colors (e.g. red, blue, green) are clustered nearby
in the id97 representation because they are relatively
interchangeable in most language contexts. however, their
visual instantiations are very different.

   chocolate cake   

   glass of wine   

   red bus   

   yellow bus   

   closeup of zebra   

   sprinkled donut   

   wooden chair   

   wooden of   ce desk   

   shiny laptop   

figure 9. examples of highest scoring regions for queried snippets of text, on 5,000 images of our mscoco test set.

   bird    ying in the sky   

   bird on a tree branch   

   bird sitting on roof   

   closeup of fruit   

   bowl of fruit   

   man riding a horse   

   straw hat   

figure 10. examples of highest scoring regions for queried snippets of text, on 5,000 images of our mscoco test set.

model

image annotation

image search

r@1 r@5 r@10 med r r@1 r@5 r@10 med r

flickr8k

devise (frome et al. [16])
sdt-id56 (socher et al. [49])
kiros et al. [25]
mao et al. [38]
defrag (karpathy et al. [24])
our implementation of defrag [24]
our model: deptree edges
our model: bid56

4.5
9.6
13.5
14.5
12.6
13.8
14.8
16.5

18.1
29.8
36.2
37.2
32.9
35.8
37.9
40.6

29.2
41.1
45.7
48.5
44.0
48.2
50.0
54.2

26
16
13
11
14
10.4
9.4
7.6

6.7
8.9
10.4
11.5
9.7
9.5
11.6
11.8

21.9
29.8
31.0
31.0
29.6
28.2
31.4
32.1

32.7
41.1
43.7
42.4
42.5
40.3
43.8
44.7

25
16
14
15
15
15.6
13.2
12.4

table 5. ranking experiment results for the flickr8k dataset.

figure 11. additional examples of alignments. for each query test image above we retrieve the most compatible sentence from the test set
and show the alignments.

figure 12. additional examples of captions on the level of full images. green: human ground truth. red: top-scoring sentence from
training set. blue: generated sentence.

figure 13. additional examples of region captions on the test set of flickr30k.

