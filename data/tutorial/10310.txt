under review as a conference paper at iclr 2016

visualizing and understanding recurrent
networks

andrej karpathy   
department of computer science, stanford university
{karpathy,jcjohns,feifeili}@cs.stanford.edu

justin johnson   

li fei-fei

5
1
0
2

 

v
o
n
7
1

 

 
 
]

g
l
.
s
c
[
 
 

2
v
8
7
0
2
0

.

6
0
5
1
:
v
i
x
r
a

abstract

recurrent neural networks (id56s), and speci   cally a variant with long short-
term memory (lstm), are enjoying renewed interest as a result of successful
applications in a wide range of machine learning problems that involve sequential
data. however, while lstms provide exceptional results in practice, the source
of their performance and their limitations remain rather poorly understood. us-
ing character-level language models as an interpretable testbed, we aim to bridge
this gap by providing an analysis of their representations, predictions and error
types. in particular, our experiments reveal the existence of interpretable cells that
keep track of long-range dependencies such as line lengths, quotes and brackets.
moreover, our comparative analysis with    nite horizon id165 models traces the
source of the lstm improvements to long-range structural dependencies. finally,
we provide analysis of the remaining errors and suggests areas for further study.

1

introduction

recurrent neural networks, and speci   cally a variant with long short-term memory (lstm)
hochreiter & schmidhuber (1997), have recently emerged as an effective model in a wide vari-
ety of applications that involve sequential data. these include id38 mikolov et al.
(2010), handwriting recognition and generation graves (2013), machine translation sutskever et al.
(2014); bahdanau et al. (2014), id103 graves et al. (2013), video analysis donahue
et al. (2015) and image captioning vinyals et al. (2015); karpathy & fei-fei (2015).
however, both the source of their impressive performance and their shortcomings remain poorly
understood. this raises concerns of the lack of interpretability and limits our ability design better
architectures. a few recent ablation studies analyzed the effects on performance as various gates
and connections are removed greff et al. (2015); chung et al. (2014). however, while this analysis
illuminates the performance-critical pieces of the architecture, it is still limited to examining the
effects only on the global level of the    nal test set perplexity alone. similarly, an often cited ad-
vantage of the lstm architecture is that it can store and retrieve information over long time scales
using its gating mechanisms, and this ability has been carefully studied in toy settings hochreiter
& schmidhuber (1997). however, it is not immediately clear that similar mechanisms can be ef-
fectively discovered and utilized by these networks in real-world data, and with the common use of
simple stochastic id119 and truncated id26 through time.
to our knowledge, our work provides the    rst empirical exploration of the predictions of lstms
and their learned representations on real-world data. concretely, we use character-level language
models as an interpretable testbed for illuminating the long-range dependencies learned by lstms.
our analysis reveals the existence of cells that robustly identify interpretable, high-level patterns
such as line lengths, brackets and quotes. we further quantify the lstm predictions with com-
prehensive comparison to id165 models, where we    nd that lstms perform signi   cantly better
on characters that require long-range reasoning. finally, we conduct an error analysis in which we
   peel the onion    of errors with a sequence of oracles. these results allow us to quantify the extent
of remaining errors in several categories and to suggest speci   c areas for further study.

   both authors contributed equally to this work.

1

under review as a conference paper at iclr 2016

2 related work
recurrent networks. recurrent neural networks (id56s) have a long history of applications in
various sequence learning tasks werbos (1988); schmidhuber (2015); rumelhart et al. (1985). de-
spite their early successes, the dif   culty of training simple recurrent networks bengio et al. (1994);
pascanu et al. (2012) has encouraged various proposals for improvements to their basic architec-
ture. among the most successful variants are the long short term memory networks hochreiter &
schmidhuber (1997), which can in principle store and retrieve information over long time periods
with explicit gating mechanisms and a built-in constant error carousel. in the recent years there has
been a renewed interest in further improving on the basic architecture by modifying the functional
form as seen with id149 cho et al. (2014), incorporating content-based soft atten-
tion mechanisms bahdanau et al. (2014); weston et al. (2014), push-pop stacks joulin & mikolov
(2015), or more generally external memory arrays with both content-based and relative addressing
mechanisms graves et al. (2014). in this work we focus the majority of our analysis on the lstm
due to its widespread popularity and a proven track record.
understanding recurrent networks. while there is an abundance of work that modi   es or extends
the basic lstm architecture, relatively little attention has been paid to understanding the properties
of its representations and predictions. greff et al. (2015) recently conducted a comprehensive study
of lstm components. chung et al. evaluated gru compared to lstms chung et al. (2014). joze-
fowicz et al. (2015) conduct an automated architecture search of thousands of id56 architectures.
pascanu et al. (2013) examined the effects of depth . these approaches study recurrent network
based only on the variations in the    nal test set cross id178, while we break down the performance
into interpretable categories and study individual error types. most related to our work is hermans
& schrauwen (2013), who also study the long-term interactions learned by recurrent networks in
the context of character-level language models, speci   cally in the context of parenthesis closing and
time-scales analysis. our work complements their results and provides additional types of analysis.
lastly, we are heavily in   uenced by work on in-depth analysis of errors in id164 hoiem
et al. (2012), where the    nal mean average precision is similarly broken down and studied in detail.

3 experimental setup
we    rst describe three commonly used recurrent network architectures (id56, lstm and the gru),
then describe their used in sequence learning and    nally discuss the optimization.
3.1 recurrent neural network models
the simplest instantiation of a deep recurrent network arranges hidden state vectors hl
t in a two-
dimensional grid, where t = 1 . . . t is thought of as time and l = 1 . . . l is the depth. the bottom
t = xt at depth zero holds the input vectors xt and each vector in the top row {hl
t }
row of vectors h0
t are computed with a recurrence
is used to predict an output vector yt. all intermediate vectors hl
. through these hidden vectors, each output yt at time step t
formula based on hl
becomes a function of all input vectors up to t, {x1, . . . , xt}. the precise mathematical form of the
recurrence (hl
vanilla recurrent neural network (id56) has a recurrence of the form

t varies from model to model and we describe these details next.

t   1 and hl   1

t   1 , hl   1

)     hl

t

t

(cid:19)

(cid:18)hl   1

t
hl
t   1

hl
t = tanh w l

t

) and before in time (hl

where we assume that all h     rn. the parameter matrix w l on each layer has dimensions [n   2n]
and tanh is applied elementwise. note that w l varies between layers but is shared through time.
we omit the bias vectors for brevity. interpreting the equation above, the inputs from the layer below
in depth (hl   1
t   1) are transformed and interact through additive interaction
before being squashed by tanh. this is known to be a weak form of coupling sutskever et al. (2011).
both the lstm and the gru (discussed next) include more powerful multiplicative interactions.
long short-term memory (lstm) hochreiter & schmidhuber (1997) was designed to address
the dif   culties of training id56s bengio et al. (1994). in particular, it was observed that the back-
propagation dynamics caused the gradients in an id56 to either vanish or explode.
it was later
found that the exploding gradient concern can be alleviated with a heuristic of clipping the gradients
at some maximum value pascanu et al. (2012). on the other hand, lstms were designed to miti-
gate the vanishing gradient problem. in addition to a hidden state vector hl
t, lstms also maintain a

2

under review as a conference paper at iclr 2016

memory vector cl
using explicit gating mechanisms. the precise form of the update is as follows:

t. at each time step the lstm can choose to read from, write to, or reset the cell

          =

          i

f
o
g

         sigm

sigm
sigm
tanh

          w l

(cid:19)

(cid:18)hl   1

t
hl
t   1

t   1 + i (cid:12) g

t = f (cid:12) cl
cl
t = o (cid:12) tanh(cl
hl
t)

here, the sigmoid function sigm and tanh are applied element-wise, and w l is a [4n    2n] matrix.
the three vectors i, f, o     rn are thought of as binary gates that control whether each memory cell
is updated, whether it is reset to zero, and whether its local state is revealed in the hidden vector,
respectively. the activations of these gates are based on the sigmoid function and hence allowed to
range smoothly between zero and one to keep the model differentiable. the vector g     rn ranges
between -1 and 1 and is used to additively modify the memory contents. this additive interaction
is a critical feature of the lstm   s design, because during id26 a sum operation merely
distributes gradients. this allows gradients on the memory cells c to    ow backwards through time
uninterrupted for long time periods, or at least until the    ow is disrupted with the multiplicative
interaction of an active forget gate. lastly, note that an implementation of the lstm requires one
to maintain two vectors (hl
gated recurrent unit (gru) cho et al. (2014) recently proposed as a simpler alternative to the
lstm that takes the form:

t) at every point in the network.

t and cl

(cid:18)hl   1

t
hl
t   1

(cid:19)
x are [n    n]. the gru has the interpretation of computing

xhl   1
  hl
t = tanh(w l
t = (1     z) (cid:12) hl
hl

t + w l
t   1 + z (cid:12)   hl

g(r (cid:12) hl

t   1))

t

g and w l

(cid:18)sigm

(cid:19)

z

=

w l
r
r are [2n    2n] and w l

sigm

here, w l
a candidate hidden vector   hl

t and then smoothly interpolating towards it gated by z.

(cid:19)

(cid:18)r

3.2 character-level id38
we use character-level id38 as an interpretable testbed for sequence learning. in this
setting, the input to the network is a sequence of characters and the network is trained to predict
the next character in the sequence with a softmax classi   er at each time step. concretely, assuming
a    xed vocabulary of k characters we encode all characters with k-dimensional 1-of-k vectors
{xt}, t = 1, . . . , t , and feed these to the recurrent network to obtain a sequence of d-dimensional
t }, t = 1, . . . , t . to obtain predictions for the next
hidden vectors at the last layer of the network {hl
character in the sequence we project this top layer of activations to a sequence of vectors {yt}, where
t and wy is a [k    d] parameter matrix. these vectors are interpreted as holding the
yt = wyhl
(unnormalized) log id203 of the next character in the sequence and the objective is to minimize
the average cross-id178 loss over all targets.

3.3 optimization
following previous work sutskever et al. (2014) we initialize all parameters uniformly in range
[   0.08, 0.08]. we use mini-batch stochastic id119 with batch size 100 and rmsprop
dauphin et al. (2015) per-parameter adaptive update with base learning rate 2    10   3 and decay
0.95. these settings work robustly with all of our models. the network is unrolled for 100 time
steps. we train each model for 50 epochs and decay the learning rate after 10 epochs by multiplying
it with a factor of 0.95 after each additional epoch. we use early stopping based on validation
performance and cross-validate the amount of dropout for each model individually.

4 experiments
datasets. two datasets previously used in the context of character-level language models are the
id32 dataset marcus et al. (1993) and the hutter prize 100mb of wikipedia dataset hutter
(2012) . however, both datasets contain a mix of common language and special markup. our goal is
not to compete with previous work but rather to study recurrent networks in a controlled setting and
on both ends on the spectrum of degree of structure. therefore, we chose to use leo tolstoy   s war
and peace (wp) novel, which consists of 3,258,246 characters of almost entirely english text with
minimal markup, and at the other end of the spectrum the source code of the linux kernel (lk). we
shuf   ed all header and source    les randomly and concatenated them into a single    le to form the
6,206,996 character long dataset. we split the data into train/val/test splits as 80/10/10 for wp and

3

under review as a conference paper at iclr 2016

lstm

2

1

3

1

id56

2

3

1

gru

2

3

war and peace dataset

1.449 1.442 1.540 1.446 1.401 1.396 1.398 1.373 1.472
1.277 1.227 1.279 1.417 1.286 1.277 1.230 1.226 1.253
1.189 1.137 1.141 1.342 1.256 1.239 1.198 1.164 1.138
1.170 1.201 1.077
1.161 1.092 1.082

-

-

-

linux kernel dataset

1.355 1.331 1.366 1.407 1.371 1.383 1.335 1.298 1.357
1.149 1.128 1.177 1.241 1.120 1.220 1.154 1.125 1.150
1.026 0.972 0.998 1.171 1.116 1.116 1.039 0.991 1.026
0.943 0.861 0.829
0.952 0.840 0.846

-

-

-

layers
size
64
128
256
512

64
128
256
512

figure 1: left: the test set cross-id178 loss for all models and datasets (low is good). models
in each row have nearly equal number of parameters. the test set has 300,000 characters. the
standard deviation, estimated with 100 bootstrap samples, is less than 4    10   3 in all cases. right:
a id167 embedding based on the probabilities assigned to test set characters by each model on war
and peace. the color, size, and marker correspond to model type, model size, and number of layers.

90/5/5 for lk. therefore, there are approximately 300,000 characters in the validation/test splits in
each case. the total number of characters in the vocabulary is 87 for wp and 101 for lk.

4.1 comparing recurrent networks
we    rst train several recurrent network models to support further analysis and to compare their
performance in a controlled setting.
in particular, we train models in the cross product of type
(lstm/id56/gru), number of layers (1/2/3), number of parameters (4 settings), and both datasets
(wp/kl). for a 1-layer lstm we used hidden size vectors of 64,128,256, and 512 cells, which with
our character vocabulary sizes translates to approximately 50k, 130k, 400k, and 1.3m parameters
respectively. the sizes of hidden layers of the other models were carefully chosen so that the total
number of parameters in each case is as close as possible to these 4 settings.
the test set results are shown in figure 1. our consistent    nding is that depth of at least two is
bene   cial. however, between two and three layers our results are mixed. additionally, the results
are mixed between the lstm and the gru, but both signi   cantly outperform the id56. we also
computed the fraction of times that each pair of models agree on the most likely character and use
it to render a id167 van der maaten & hinton (2008) embedding (we found this more stable and
robust than the kl divergence). the plot (figure 1, right) further supports the claim that the lstm
and the gru make similar predictions while the id56s form their own cluster.

internal mechanisms of an lstm

4.2
interpretable, long-range lstm cells. an lstms can in principle use its memory cells to remem-
ber long-range information and keep track of various attributes of text it is currently processing. for
instance, it is a simple exercise to write down toy cell weights that would allow the cell to keep track
of whether it is inside a quoted string. however, to our knowledge, the existence of such cells has
never been experimentally demonstrated on real-world data. in particular, it could be argued that
even if the lstm is in principle capable of using these operations, practical optimization challenges
(i.e. sgd dynamics, or approximate gradients due to truncated id26 through time) might
prevent it from discovering these solutions. in this experiment we verify that multiple interpretable
cells do in fact exist in these networks (see figure 2). for instance, one cell is clearly acting as a
line length counter, starting with a high value and then slowly decaying with each character until the
next newline. other cells turn on inside quotes, the parenthesis after if statements, inside strings or
comments, or with increasing strength as the indentation of a block of code increases. in particular,
note that truncated id26 with our hyperparameters prevents the gradient signal from di-
rectly noticing dependencies longer than 100 characters, but we still observe cells that reliably keep
track of quotes or comment blocks much longer than 100 characters (e.g.     230 characters in the
quote detection cell example in figure 2). we hypothesize that these cells    rst develop on patterns
shorter than 100 characters but then also appropriately generalize to longer sequences.
gate activation statistics. we can gain some insight into the internal mechanisms of the lstm
by studying the gate activations in the networks as they process test set data. we were particularly
interested in looking at the distributions of saturation regimes in the networks, where we de   ne a

4

gru  2  (512)gru  3  (256)gru  1  (128)lstm  2  (512)lstm  1  (64)gru  1  (64)lstm  2  (256)gru  3  (128)id56  2  (256)id56  3  (128)gru  1  (512)id56  3  (64)id56  1  (64)gru  2  (128)lstm  2  (64)id56  1  (128)lstm  3  (512)id56  2  (64)gru  2  (256)gru  3  (64)lstm  1  (256)lstm  1  (128)lstm  3  (128)gru  2  (64)lstm  3  (256)lstm  1  (512)lstm  3  (64)gru  1  (256)id56  2  (128)id56  1  (256)id56  3  (256)lstm  2  (128)gru  3  (512)under review as a conference paper at iclr 2016

figure 2: several examples of cells with interpretable activations discovered in our best linux ker-
nel and war and peace lstms. text color corresponds to tanh(c), where -1 is red and +1 is blue.

figure 3: left three: saturation plots for an lstm. each circle is a gate in the lstm and its
position is determined by the fraction of time it is left or right-saturated. these fractions must add to
at most one (indicated by the diagonal line). right two: saturation plot for a 3-layer gru model.

gate to be left or right-saturated if its activation is less than 0.1 or more than 0.9, respectively, or
unsaturated otherwise. we then compute the fraction of times that each lstm gate spends left or
right saturated, and plot the results in figure 3. for instance, the number of often right-saturated
forget gates is particularly interesting, since this corresponds to cells that remember their values
for very long time periods. note that there are multiple cells that are almost always right-saturated
(showing up on bottom, right of the forget gate scatter plot), and hence function as nearly perfect
integrators. conversely, there are no cells that function in purely feed-forward fashion, since their
forget gates would show up as consistently left-saturated (in top, left of the forget gate scatter plot).
the output gate statistics also reveal that there are no cells that get consistently revealed or blocked
to the hidden state. lastly, a surprising    nding is that unlike the other two layers that contain gates
with nearly binary regime of operation (frequently either left or right saturated), the activations in
the    rst layer are much more diffuse (near the origin in our scatter plots). we struggle to explain this
   nding but note that it is present across all of our models. a similar effect is present in our gru
model, where the    rst layer reset gates r are nearly never right-saturated and the update gates z are
rarely ever left-saturated. this points towards a purely feed-forward mode of operation on this layer,
where the previous hidden state is barely used.

4.3 understanding long-range interactions
good performance of lstms is frequently attributed to their ability to store long-range information.
in this section we test this hypothesis by comparing an lstm with baseline models that can only
utilize information from a    xed number of previous steps. in particular, we consider two baselines:
1. n-nn: a fully-connected neural network with one hidden layer and tanh nonlinearities. the
input to the network is a sparse binary vector of dimension nk that concatenates the one-of-k

5

under review as a conference paper at iclr 2016

pppppp

model

n

1

2

3

4

5

6

7

8

9

20

war and peace dataset

id165
n-nn

id165
n-nn

2.399
2.399

2.702
2.707

1.928
1.931

1.954
1.974

1.521
1.553

1.440
1.505

1.314
1.451

1.213
1.395

1.232
1.339

1.097
1.256

linux kernel dataset

1.203
1.321

1.027
1.376

1.194

-

0.982

-

1.194

-

0.953

-

1.194

-

0.933

-

1.195

-

0.889

-

table 2: the test set cross-id178 loss on both datasets for id165 models (low is good). the
standard deviation estimate using 100 bootstrap samples is below 4    10   3 in all cases.

figure 4: left: overlap between test-set errors between our best 3-layer lstm and the id165
models (low area is good). middle/right: mean probabilities assigned to a correct character (higher
is better), broken down by the character, and then sorted by the difference between two models.
   <s>    is the space character. lstm (red) outperforms the 20-gram model (blue) on special char-
acters that require long-range reasoning. middle: lk dataset, right: wp dataset.

encodings of n consecutive characters. we optimize the model as described in section 3.3 and
cross-validate the size of the hidden layer.

2. id165: an unpruned (n + 1)-gram language model using modi   ed kneser-ney smoothing
chen & goodman (1999). this is a standard smoothing method for language models huang et al.
(2001). all models were trained using the popular kenlm software package hea   eld et al. (2013).
performance comparisons. the performance of both id165 models is shown in table 2. the
id165 and n-nn models perform nearly identically for small values of n, but for larger values
the n-nn models start to over   t and the id165 model performs better. moreover, we see that on
both datasets our best recurrent network outperforms the 20-gram model (1.077 vs. 1.195 on wp
and 0.84 vs.0.889). it is dif   cult to make a direct model size comparison, but the 20-gram model
   le has 3gb, while our largest checkpoints are 11mb. however, the assumptions encoded in the
kneser-ney smoothing model are intended for word-level modeling of natural language and may
not be optimal for character-level data. despite this concern, these results already provide weak
evidence that the recurrent networks are effectively utilizing information beyond 20 characters.
error analysis. it is instructive to delve deeper into the errors made by both recurrent networks
and id165 models. in particular, we de   ne a character to be an error if the id203 assigned
to it by a model on the previous time step is below 0.5. figure 4 (left) shows the overlap between
the test-set errors for the 3-layer lstm, and the best n-nn and id165 models. we see that the
majority of errors are shared by all three models, but each model also has its own unique errors.
to gain deeper insight into the errors that are unique to the lstm or the 20-gram model, we compute
the mean id203 assigned to each character in the vocabulary across the test set. in figure 4
(middle,right) we display the 10 characters where each model has the largest advantage over the
other. on the linux kernel dataset, the lstm displays a large advantage on special characters that
are used to structure c programs, including whitespace and brackets. the war and peace dataset
features an interesting long-term dependency with the carriage return, which occurs approximately
every 70 characters. figure 4 (right) shows that the lstm has a distinct advantage on this character.
to accurately predict the presence of the carriage return the model likely needs to keep track of
its distance since the last carriage return. the cell example we   ve highlighted in figure 2 (top,
left) seems particularly well-tuned for this speci   c task. similarly, to predict a closing bracket
or quotation mark, the model must be aware of the corresponding open bracket, which may have
appeared many time steps ago. the fact that the lstm performs signi   cantly better than the 20-

6

@=  }\t){]:<s>wmjs973jhv0.00.20.40.60.81.0mean  id203lstm20  gram\r<s>"7d8?ykn(g:vqj29500.00.20.40.60.81.0mean  id203lstm20  gramunder review as a conference paper at iclr 2016

figure 5: left: mean probabilities that the lstm and 20-gram model assign to the    }    character,
bucketed by the distance to the matching    {   . right: comparison of the similarity between 3-layer
lstm and the n-nn baselines over the    rst 3 epochs of training, as measured by the symmetric kl-
divergence (middle) and the test set loss (right). low kl indicates similar predictions, and positive
   loss indicates that the lstm outperforms the baseline.

gram model on these characters provides strong evidence that the model is capable of effectively
keeping track of long-range interactions.
case study: closing brace. of these structural characters, the one that requires the longest-term
reasoning is the closing brace (   }   ) on the linux kernel dataset. braces are used to denote blocks
of code, and may be nested; as such, the distance between an opening brace and its corresponding
closing brace can range from tens to hundreds of characters. this feature makes the closing brace
an ideal test case for studying the ability of the lstm to reason over various time scales. we group
closing brace characters on the test set by the distance to their corresponding open brace and compute
the mean id203 assigned by the lstm and the 20-gram model to closing braces within each
group. the results are shown in figure 5 (left). first, note that the lstm only slightly outperforms
the 20-gram model in the    rst bin, where the distance between braces is only up to 20 characters.
after this point the performance of the 20-gram model stays relatively constant, re   ecting a baseline
id203 of predicting the closing brace without seeing its matching opening brace. compared
to this baseline, we see that the lstm gains signi   cant boosts up to 60 characters, and then its
performance delta slowly decays over time as it becomes dif   cult to keep track of the dependence.
training dynamics. it is also instructive to examine the training dynamics of the lstm by com-
paring it with trained n-nn models during training using the (symmetric) kl divergence between
the predictive distributions on the test set. we plot the divergence and the difference in the mean loss
in figure 5 (right). notably, we see that in the    rst few iterations the lstm behaves like the 1-nn
model but then diverges from it soon after. the lstm then behaves most like the 2-nn, 3-nn,
and 4-nn models in turn. this experiment suggests that the lstm    grows    its competence over
increasingly longer dependencies during training. this insight might be related to why sutskever
et al. sutskever et al. (2014) observe improvements when they reverse the source sentences in their
encoder-decoder architecture for machine translation. the inversion introduces short-term depen-
dencies that the lstm can model    rst, and then longer dependencies are learned over time.

4.4 error analysis: breaking down the failure cases
in this section we break down lstm   s errors into categories to study the remaining limitations, the
relative severity of each error type, and to suggest areas for further study. we focus on the war
and peace dataset where it is easier to categorize the errors. our approach is to    peel the onion   
by iteratively removing the errors with a series of constructed oracles. as in the last section, we
consider a character to be an error if the id203 it was assigned by the model in the previous
time step is below 0.5. note that the order in which the oracles are applied in   uences the results.
we tried to apply the oracles in order of increasing dif   culty of removing each error category and
believe that the    nal results are instructive despite this downside. the oracles we use are, in order:
id165 oracle. first, we construct optimistic id165 oracles that eliminate errors that might be
   xed with better modeling of short dependencies.
in particular, we evaluate the id165 model
(n = 1, . . . , 9) and remove a character error if it is correctly classi   ed (id203 assigned to that
character greater than 0.5) by any of these models. this gives us an approximate idea of the amount
of signal present only in the last 9 characters, and how many errors we could optimistically hope to
eliminate without needing to reason over long time horizons.
dynamic n-long memory oracle. to motivate the next oracle, consider the string    jon yelled at
mary but mary couldn   t hear him.    one interesting and consistent failure mode that we noticed in

7

0  2020  4040  6060  8080  100100  125125  150150  200200  250250  300distance  between  "{"  and  "}"0.00.10.20.30.40.50.60.70.8mean  p("}")lstm20  gram0.00.51.01.52.02.53.03.5lstm  epoch12345678mean  kl1  nn2  nn3  nn4  nn0.00.51.01.52.02.53.03.5lstm  epoch   2.5   2.0   1.5   1.0   0.50.00.51.01.52.0   lossunder review as a conference paper at iclr 2016

figure 6: left: lstm errors removed one by one with oracles, starting from top of the pie chart
and going counter-clockwise. the area of each slice corresponds to fraction of errors contributed.
   n-memory    refers to dynamic memory oracle with context of n previous characters.    word t-train   
refers to the rare words oracle with word count threshold of t. right: concrete examples of text from
the test set for each error type. blue color highlights the relevant characters with the associated error.
for the memory category we also highlight the repeated substrings with red bounding rectangles.

the predictions is that if the lstm fails to predict the characters of the    rst occurrence of    mary   
then it will almost always also fail to predict the same characters of the second occurrence, with
a nearly identical pattern of errors. however, in principle the presence of the    rst mention should
make the second much more likely. the lstm could conceivably store a summary of previously
seen characters in the data and fall back on this memory when it is uncertain. however, this does
not appear to take place in practice. this limitation is related to the improvements seen in    dynamic
evaluation    mikolov (2012); jelinek et al. (1991) of recurrent language models, where an id56 is
allowed to train on the test set characters during evaluation as long as it sees them only once. in this
mode of operation when the id56 trains on the    rst occurrence of    mary   , the log probabilities on
the second occurrence are signi   cantly better. we hypothesize that this dynamic aspect is a common
feature of sequence data, where certain subsequences that might not frequently occur in the training
data should still be more likely if they were present in the immediate history. however, this general
algorithm does not seem to be learned by the lstm. our dynamic memory oracle quanti   es the
severity of this limitation by removing errors in all words (starting with the second character) that
can found as a substring in the last n characters (we use n     {100, 500, 1000, 5000}).
rare words oracle. next, we construct an oracle that eliminates errors for rare words that occur
only up to n times in the training data (n = 0, . . . , 5). this estimates the severity of errors that could
optimistically be eliminated by increasing the size of the training data, or with pretraining.
word model oracle. we noticed that a large portion of the errors occur on the    rst character of each
word. intuitively, the task of selecting the next word in the sequence is harder than completing the
last few characters of a known word. motivated by this observation we constructed an oracle that
eliminated all errors after a space, quote or a newline. interestingly, a high portion of errors can be
found after a newline, since the models have to learn that newline has semantics similar to a space.
punctuation oracle. the remaining errors become dif   cult to blame on one particular, interpretable
aspect of the modeling. at this point we construct an oracle that removes errors on all punctuation.
boost oracles. the remaining errors that do not show salient structures or patterns are removed by
an oracle that boosts the id203 of the correct letter by a    xed amount. these oracles allow us
to understand the distribution of the dif   culty of the remaining errors.
we now subject two lstm models to the error analysis: first, our best lstm model and second,
the best lstm model in the smallest model category (50k parameters). the small and large models

8

1-gram to 5-grampunctuationup to 500 memoryless than 3 training examples of wordafter space or quoteafter newline0.4 to 0.5 boostlstm-3 (512)lstm-2 (64)under review as a conference paper at iclr 2016

allow us to understand how the error break down changes as we scale up the model. the error
breakdown after applying each oracle for both models can be found in figure 6.
the error breakdown. in total, our best lstm model made a total of 140k errors out of 330k
test set characters (42%). of these, the id165 oracle eliminates 18%, suggesting that the model
is not taking full advantage of the last 9 characters. the dynamic memory oracle eliminates 6% of
the errors. in principle, a dynamic evaluation scheme could be used to mitigate this error, but we
believe that a more principled approach could involve an approach similar to memory networks
weston et al. (2014), where the model is allowed to attend to a recent history of the sequence while
making its next prediction. finally, the rare words oracle accounts for 9% of the errors. this
error type might be mitigated with unsupervised pretraining dai & le (2015), or by increasing the
size of the training set. the majority of the remaining errors (37%) follow a space, a quote, or a
newline, indicating the model   s dif   culty with word-level predictions. this suggests that longer time
horizons in id26 through time, or possibly hierarchical context models, could provide
improvements. see figure 6 (right) for examples of each error type. we believe that this type of error
breakdown is a valuable tool for isolating and understanding the source of improvements provided
by new proposed models.
errors eliminated by scaling up. in contrast, the smaller lstm model makes a total of 184k
errors (or 56% of the test set), or approximately 44k more than the large model. surprisingly, 36k
of these errors (81%) are id165 errors, 5k come from the boost category, and the remaining 3k are
distributed across the other categories relatively evenly. that is, scaling the model up by a factor 26
in the number of parameters has almost entirely provided gains in the local, id165 error rate and
has left the other error categories untouched in comparison. this analysis provides some evidence
that it might be necessary to develop new architectural improvements instead of simply scaling up
the basic model.

5 conclusion
we have used character-level language models as an interpretable test bed for analyzing the predic-
tions, representations training dynamics, and error types present in recurrent neural networks. in
particular, our qualitative visualization experiments, cell activation statistics and comparisons to    -
nite horizon id165 models demonstrate that these networks learn powerful, and often interpretable
long-range interactions on real-world data. our error analysis broke down cross id178 loss into
several interpretable categories, and allowed us to illuminate the sources of remaining limitations
and to suggest further areas for study. in particular, we found that scaling up the model almost
entirely eliminates errors in the id165 category, which provides some evidence that further archi-
tectural innovations may be needed to address the remaining errors.

acknowledgments

we gratefully acknowledge the support of nvidia corporation with the donation of the gpus used
for this research.

references
bahdanau, dzmitry, cho, kyunghyun, and bengio, yoshua. id4 by jointly

learning to align and translate. arxiv preprint arxiv:1409.0473, 2014.

bengio, yoshua, simard, patrice, and frasconi, paolo. learning long-term dependencies with gra-

dient descent is dif   cult. neural networks, ieee transactions on, 5(2):157   166, 1994.

chen, stanley f and goodman, joshua. an empirical study of smoothing techniques for language

modeling. computer speech & language, 13(4):359   393, 1999.

cho, kyunghyun, van merri  enboer, bart, bahdanau, dzmitry, and bengio, yoshua. on the proper-
ties of id4: encoder-decoder approaches. arxiv preprint arxiv:1409.1259,
2014.

9

under review as a conference paper at iclr 2016

chung, junyoung, gulcehre, caglar, cho, kyunghyun, and bengio, yoshua. empirical evaluation
of gated recurrent neural networks on sequence modeling. arxiv preprint arxiv:1412.3555, 2014.

dai, andrew m and le, quoc v.

arxiv:1511.01432, 2015.

semi-supervised sequence learning.

arxiv preprint

dauphin, yann n, de vries, harm, chung, junyoung, and bengio, yoshua. rmsprop and equi-
librated adaptive learning rates for non-id76. arxiv preprint arxiv:1502.04390,
2015.

donahue, jeff, hendricks, lisa anne, guadarrama, sergio, rohrbach, marcus, venugopalan, sub-
hashini, saenko, kate, and darrell, trevor. long-term recurrent convolutional networks for visual
recognition and description. cvpr, 2015.

graves, alex.

generating sequences with recurrent neural networks.

arxiv:1308.0850, 2013.

arxiv preprint

graves, alex, mohamed, a-r, and hinton, geoffrey. id103 with deep recurrent neu-
ral networks. in acoustics, speech and signal processing (icassp), 2013 ieee international
conference on, pp. 6645   6649. ieee, 2013.

graves, alex, wayne, greg, and danihelka, ivo. id63s.

arxiv:1410.5401, 2014.

arxiv preprint

greff, klaus, srivastava, rupesh kumar, koutn    k, jan, steunebrink, bas r., and schmidhuber,
j  urgen. lstm: a search space odyssey. corr, abs/1503.04069, 2015. url http://arxiv.
org/abs/1503.04069.

hea   eld, kenneth, pouzyrevsky, ivan, clark, jonathan h., and koehn, philipp. scalable modi-
   ed kneser-ney language model estimation. in proceedings of the 51st annual meeting of the
association for computational linguistics, pp. 690   696, so   a, bulgaria, august 2013. url
http://kheafield.com/professional/edinburgh/estimate_paper.pdf.

hermans, michiel and schrauwen, benjamin. training and analysing deep recurrent neural net-

works. in advances in neural information processing systems, pp. 190   198, 2013.

hochreiter, sepp and schmidhuber, j  urgen. long short-term memory. neural computation, 9(8):

1735   1780, 1997.

hoiem, derek, chodpathumwan, yodsawalai, and dai, qieyun. diagnosing error in object detectors.

in id161   eccv 2012, pp. 340   353. springer, 2012.

huang, xuedong, acero, alex, hon, hsiao-wuen, and foreword by-reddy, raj. spoken language

processing: a guide to theory, algorithm, and system development. prentice hall ptr, 2001.

hutter, marcus. the human knowledge compression contest. 2012.

jelinek, frederick, merialdo, bernard, roukos, salim, and strauss, martin. a dynamic language

model for id103. in hlt, volume 91, pp. 293   295, 1991.

10

under review as a conference paper at iclr 2016

joulin, armand and mikolov, tomas. inferring algorithmic patterns with stack-augmented recurrent

nets. corr, abs/1503.01007, 2015. url http://arxiv.org/abs/1503.01007.

jozefowicz, rafal, zaremba, wojciech, and sutskever, ilya. an empirical exploration of recurrent
network architectures. in proceedings of the 32nd international conference on machine learning
(icml-15), pp. 2342   2350, 2015.

karpathy, andrej and fei-fei, li. deep visual-semantic alignments for generating image descrip-

tions. cvpr, 2015.

marcus, mitchell p, marcinkiewicz, mary ann, and santorini, beatrice. building a large annotated

corpus of english: the id32. computational linguistics, 19(2):313   330, 1993.

mikolov, tom  a  s. statistical language models based on neural networks. presentation at google,

mountain view, 2nd april, 2012.

mikolov, tomas, kara     at, martin, burget, lukas, cernock`y, jan, and khudanpur, sanjeev. recur-
rent neural network based language model. in interspeech 2010, 11th annual conference
of the international speech communication association, makuhari, chiba, japan, september
26-30, 2010, pp. 1045   1048, 2010.

pascanu, razvan, mikolov, tomas, and bengio, yoshua. on the dif   culty of training recurrent

neural networks. arxiv preprint arxiv:1211.5063, 2012.

pascanu, razvan, g  ulc  ehre, c   aglar, cho, kyunghyun, and bengio, yoshua. how to construct deep
recurrent neural networks. corr, abs/1312.6026, 2013. url http://arxiv.org/abs/
1312.6026.

rumelhart, david e, hinton, geoffrey e, and williams, ronald j. learning internal representations

by error propagation. technical report, dtic document, 1985.

schmidhuber, j. deep learning in neural networks: an overview. neural networks, 61:85   117,
2015. doi: 10.1016/j.neunet.2014.09.003. published online 2014; based on tr arxiv:1404.7828
[cs.ne].

sutskever, ilya, martens, james, and hinton, geoffrey e. generating text with recurrent neural
networks. in proceedings of the 28th international conference on machine learning (icml-11),
pp. 1017   1024, 2011.

sutskever, ilya, vinyals, oriol, and le, quoc vv. sequence to sequence learning with neural net-

works. in advances in neural information processing systems, pp. 3104   3112, 2014.

van der maaten, laurens and hinton, geoffrey. visualizing data using id167. journal of machine

learning research, 9(2579-2605):85, 2008.

vinyals, oriol, toshev, alexander, bengio, samy, and erhan, dumitru. show and tell: a neural

image caption generator. cvpr, 2015.

werbos, paul j. generalization of id26 with application to a recurrent gas market model.

neural networks, 1(4):339   356, 1988.

11

under review as a conference paper at iclr 2016

weston, jason, chopra, sumit, and bordes, antoine. memory networks. corr, abs/1410.3916,

2014. url http://arxiv.org/abs/1410.3916.

12

