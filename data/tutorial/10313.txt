5
1
0
2

 

v
o
n
3
2

 

 
 
]

v
c
.
s
c
[
 
 

2
v
0
0
5
3
0

.

6
0
5
1
:
v
i
x
r
a

unveiling the dreams of id27s:
towards language-driven image generation

angeliki lazaridou

center for mind/brain sciences

university of trento

angeliki.lazaridou@unitn.it

dat tien nguyen   

institute of formal and applied linguistics

charles university of prague

dtnguyen88@outlook.com

disi and center for mind/brain sciences

raffaella bernardi

university of trento

raffaella.bernardi@unitn.it

marco baroni

center for mind/brain sciences

university of trento

marco.baroni@unitn.it

abstract

we introduce language-driven image generation, the task of generating an im-
age visualizing the semantic contents of a id27, e.g., given the word
embedding of grasshopper, we generate a natural image of a grasshopper. we
implement a simple method based on two mapping functions. the    rst takes as
input a id27 (as produced, e.g., by the id97 toolkit) and maps it
onto a high-level visual space (e.g., the space de   ned by one of the top layers of
a convolutional neural network). the second function maps this abstract visual
representation to pixel space, in order to generate the target image. several user
studies suggest that the current system produces images that capture general vi-
sual properties of the concepts encoded in the id27, such as color or
typical environment, and are suf   cient to discriminate between general categories
of objects.

1

introduction

imagination, creating new images in the mind, is a fundamental capability of humans, studies of
which date back to plato   s ideas about memory and perception. through imagery, we form mental
images, picture-like representations in our mind, that encode and extend our perceptual and linguistic
experience of the world. recent work in neuroscience attempts to generate reconstructions of these
mental images, as encoded in vector-based representations of fmri patterns [15]. in this work, we
take the    rst steps towards implementing the same paradigm in a computational setup, by generating
images that re   ect the imagery of distributed word representations.
we introduce language-driven image generation, the task of visualizing the contents of a linguis-
tic message, as encoded in id27s, by generating a real image. language-driven image
generation can serve as evaluation tool providing intuitive visualization of what computational repre-
sentations of word meaning encode. more ambitiously, effective language-driven image generation
could complement image search and retrieval, producing images for words that are not associated
to images in a certain collection, either for sparsity, or due to their inherent properties (e.g., artists
and psychologists might be interested in images of abstract or novel words). in this work, we focus
on generating images for distributed representations encoding the meaning of single words. how-
ever, given recent advances in compositional distributed semantics [20] that produce embeddings

   research carried out in center for mind/brain sciences, university of trento

1

figure 1: generated images of 10 concepts per category for 20 basic categories, grouped my macro-
category. see supplementary materials for the answer key.

for arbitrarily long linguistic units, we also see our contribution as the    rst step towards generating
images depicting the meaning of phrases (e.g., blue car) and sentences. after all, language-driven
image generation can be seen as the symmetric goal of recent research (e.g., [7, 8]) that introduced
effective methods to generate linguistic descriptions of the contents of a given image.
to perform language-driven image generation, we combine various recent strands of research. tools
such as id97 [12] and glove [16] have been shown to produce extremely high-quality vector-
based id27s. at the same time, in id161, images are effectively represented
by vectors of abstract visual features, such as those extracted by convolutional neural networks
(id98s) [9]. consequently, the problem of translating between linguistic and visual representations
has been coached in terms of learning a cross-modal mapping function between vector spaces [4, 19].
finally, recent work in id161, motivated by the desire to achieve a better understanding
of what the layers of id98s and other deep architectures have really learned, has proposed feature
inversion techniques that map a representation in abstract visual feature space (e.g., from the top
layer of a id98) back onto pixel space, to produce a real image [24, 10].
our language-driven image generation system takes a id27 as input (e.g., the id97
vector for grasshopper), projects it with a cross-modal function onto visual space (e.g., onto a rep-
resentation in the space de   ned by a id98 layer), and then applies feature inversion to it (using the
method hoggles method of [22]) to generate an actual image (cell a18 in figure 3). we test our
system in a rigorous zero-shot setup, in which words and images of tested concepts are neither used
to train cross-modal mapping, nor employed to induce the feature inversion function. so, for ex-
ample, our system mapped grasshopper onto visual and then pixel space without having ever been
exposed to grasshopper pictures.
figure 3 illustrates our results (   answer key    for the    gure provided as supplementary material).
while it is dif   cult to discriminate among similar objects based on these images, the    gure shows
that our language-driven image generation method already captures the broad gist of different do-
mains (food looks like food, animals are blobs in a natural environment, and so on).

2

appliancecontainerfurniture/large toolinstrumentgarmenttoolweapontoy sportsvehiclebuildingstructurefoodfruit/vegetablenatural objectplantbird   shinsectreptile/amphibianmammalman-madeorganicanimalsabcdefhijg12345678910111213141516171819202 language-driven image generation

2.1 from word to visual vectors

up to now, feature inversion algorithms [10, 22, 24] have been applied to visual representations
directly extracted from images (hence the    inversion    name). we aim instead at generating an image
conveying the semantics of a concept as encoded in a word representation. thus, we need a way to
   translate    the word representation into a visual representation, i.e., a representation laying on the
visual space that conveys the corresponding visual semantics of the word.
cross-modal mapping has been    rst introduced in the context of zero-shot learning as a way to
address the manual annotation bottleneck in domains where other vector-based representations (e.g.,
images or brain signals) must be associated to word labels [13, 19]. this is achieved by using training
data to learn a mapping function from vectors in the domain of interest to vector representations of
word labels. in our case, we are interested in the general ability of cross-modal mapping to translate
a representation between different spaces, and speci   cally from a word to a visual feature space.
the mapping is performed by inducing a function f : rd1     rd2 from data points (wi, vi), where
wi     rd1 is a word representation and vi     rd2 the corresponding visual representation. the
mapping function can then be applied to any given word vector wj to obtain its projection   vj =
f (wj) onto visual space. following previous work [13, 4], we assume that the mapping is linear.
to estimate its parameters m     rd1  d2, given word vectors w paired with visual vectors v, we
use elastic-net-penalized least squares regression, that linearly combines the l1 and l2 weight
penalties of lasso and ridge id173:

  m = argmin
m   rd1  d2

(cid:107)wm     v(cid:107)f +   1(cid:107)m(cid:107)1 +   2(cid:107)m(cid:107)f

(1)

by modifying the weights of the l1 and l2 penalties,   1 and   2, we can derive different regression
methods. speci   cally, we experiment with plain regression (  1 = 0,   2 = 0), ridge regression
(  1 = 0,   2 (cid:54)= 0), lasso regression (  1 (cid:54)= 0 and   2 = 0) and symmetric elastic net (  1 =   2,   1 (cid:54)= 0).

2.2 from visual vectors to images

convolutional neural networks have recently surpassed human performance on object recogni-
tion [17]. nevertheless, these models exhibit    intriguing properties   , that are somewhat surprising
given their state-of-the-art performance [21], prompting an effort to reach a deeper understanding of
how they really work. given that these models consist of millions of parameters, there is ongoing
research on feature inversion of different id98 layers to attain an intuitive visualization of what each
of them learned.
several methods have been proposed for inverting id98 visual features, however, the exact nature
of the task imposes certain constraints on the inversion method. for example, the original work of
zeiler and fergus [24] cannot be straightforwardly adapted to our task of generating images from
id27s, since their deconvnet method requires information related to the activations of
the network in several layers. in this work, we adopt the framework of vondrick et al. [22] that casts
the problem of inversion as paired dictionary learning.1
speci   cally, given an image x0     rd and its visual representation y =   (x0)     rd, the goal is to
   nd an image x    that minimizes the reconstruction error:

x    = argmin
x   rd

(cid:107)  (x)     y(cid:107)2

2

(2)

given that there are no guarantees regarding the convexity of   , both images and visual represen-
tations are approximated by paired, over-complete bases, u     rd  k and v     rd  k, respec-
tively. enforcing u and v to have paired representations through shared coef   cients        rk, i.e.,
x0 = u    and y = v   , allows the feature inversion to be done by estimating such coef   cients   
that minimize the reconstruction error. practically, the algorithm proceeds by    nding u, v and   

1originally, the hoggles method of [22] was introduced for visualizing hog features. however, the
method does not make feature-speci   c assumptions and it has also recently been used to invert id98 fea-
tures [23].

3

through a standard sparse coding method. for learning the parameters, the algorithm is presented
with training data of the form (xi, yi), where xi is an image patch and yi the corresponding visual
vector associated with that patch.

3 experimental setup

3.1 materials

dreamed concepts we refer to the words we generate images for as dreamed concepts. the
dreamed word set comes from the concepts studied by mcrae et al. [11], in the context of property
norm generation. this set contains 541 base-level concrete concepts (e.g., cat, apple, car etc.) that
span across 20 general and broad categories (e.g., animal, fruit/vegetable, vehicle etc). for the
purposes of the current experiments, 69 mcrae concepts were excluded (either because of high
ambiguity or for technical reasons), resulting in 472 dreamed words we test on.

seen concepts we refer to the set of words associated to real pictures that are used for training
purposes as seen concepts. the real picture set contains approximately 480k images extracted from
id163 [3] representing 5k distinct concepts. the seen concepts are used for training the cross-
modal mapping. importantly, the dreamed and seen concept sets do emphnot overlap.

word representations for all seen and dreamed concepts, we build 300-dimensional word vec-
tors with the id97 toolkit,2 choosing the cbow method.3 cbow, which learns to predict a tar-
get word from the ones surrounding it, produces state-of-the-art results in many linguistic tasks [1].
word vectors are induced from a language corpus (e.g., wikipedia) of 2.8 billion words.4

visual representations the visual representations, for the set of 480k seen concept images, are
extracted with the pre-trained id98 model of [9] through the caffe toolkit [6]. id98s trained on
natural images learn a hierarchy of increasingly more abstract properties: the features in the bottom
layers resemble gabor    lters, while features in the top layers capture more abstract properties of
the dataset or tasks the id98 is trained for (see [24]) (e.g., the topmost layer captures a distribution
over training labels). in this work, we experiment with feature representations extracted from two
levels, pool-5, extracted from the 5th layer (6x6x256=9216 dimensions), and fc-7, extracted from the
7th layer (1x4096 dimensions). pool-5 is an intermediate pooling layer that should capture object
commonalities. fc-7 is a fully-connected layer just below the topmost one, and as such it is expected
to capture high-level discriminative features of different object classes.
since each seen concept is associated with many images, we experiment with two ways to derive
a unique visual representation.
inspired from categorization schemes in cognitive science [14],
we will refer to them as the prototype and exemplar methods. the prototype visual vector of a
concept is constructed by averaging the visual representations (either pool-5 or fc-7) of images tagged
in id163 with the concept. the averaging method should smooth out noise and emphasize
invariances in images associated to a concept. on the other hand, the constructed prototype does not
correspond to an actual depiction of the concept. the exemplar visual vector, on the other hand, is a
single visual vector that is picked as a good representative of the set, as it is the one with the highest
average cosine similarity to all other vectors extracted from images labeled with the same concept.

3.2 model selection and parameter estimation

visual feature type and concept representations
in order to determine the optimal visual feature
type (between pool-5 and fc-7) and concept representation method (between prototype and exemplar),
we set up a human study through crowdflower.5 for 50 randomly chosen test concepts, we generate
4 images, each obtained by inverting the visual vector computed by combining a feature type with

2https://code.google.com/p/id97/
3other hyperparameters, adopted without tuning, include a context window size of 5 words to either side of
the target, setting the sub-sampling option to 1e-05 and estimating the id203 of target words by negative
sampling, drawing 10 samples from the noise distribution [12].

4corpus sources: http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk
5http://www.crowdflower.com/

4

a concept representation method, e.g., for pool-5+prototype, we generate an image by inverting the
visual vector resulting from averaging the pool-5 feature vectors extracted from images labeled with
the test concept (details on our implementation of feature inversion below). participants are then
asked to judge which of the 4 images is more likely to denote the test concept. for each test con-
cept, we collect 20 judgments. overall, participants showed a strong signi   cant preference for the
images generated from inverting pool-5 feature vectors (28/50), and in particular for those that were
generated from pool-5 by inverting feature vectors constructed with the exemplar protocol (18/50).6
the following experiments were thus carried out using the pool-5+exemplar visual space.

cross-modal mapping to learn the mapping m of equation 1, we use 5k training pairs (wc, vc)
= {wc     r300, vc     r9216}, where wc is the word vector and vc is the visual vector for the
(seen) concept c, based on pool-5 features and exemplar representation. speci   cally, we estimate the
weights m by training the 4 regression methods described in section 2.1 above, cross-validating
the values of   1 and   2 on the training data. model selection is performed by conducting a human
study on the language-driven image generation task. for the same test of 50 concepts as above, we
obtain estimates of their visual vectors   v by mapping their word vectors into visual space through
the different mapping functions m. we then generate an image by inverting the visual features   v.
participants are again asked to judge which of the 4 images is more likely to denote the test concept.
for each concept we collected 20 judgments. participants showed a preference for plain regression
(9/50 signi   cant tests in favor of this model), which we adopt in rest of the paper.

feature inversion training data for feature inversion (section 2.2 above) are created by using
the pascal voc 2011 dataset, that contains 15k images of 20 distinct objects. note that the 20
pascal objects are not part of our dreamed concepts, and thus the feature inversion is performed
in a zero-shot way (the inversion will be asked to generate an image for a concept that it has never
encountered before). in order to increase the size of the training data, from each image we de-
rived several image patches xi associated with different parts of the image and paired them with
their equivalent visual representations yi. both paired dictionary learning and feature inversion are
conducted using the hoggles software [22] with default hyperparameters.7

4 experiments

figure 3 provides a snapshot of our results; we randomly picked 10 dreamed concepts from each
of the 20 mcrae categories, and we show the image we generated for them from the corresponding
id27s, as described in section 2. we stress again that the images of dreamed concepts
were never used in any step of the pipeline, neither to train cross-modal mapping, nor to train fea-
ture inversion, so they are genuinely generated in a zero-shot manner, by leveraging their linguistic
associations to seen concepts.
not surprisingly, the images we generate are not as clear as those one would get by retrieving existing
images. however, we see in the    gure that concepts belonging to different categories are clearly
distinguished, with the exception of food and fruit/vegetable (columns 12 and 13), that look very
much the same (on the other hand, fruit and vegetable are also food, and word vectors extracted
from corpora will likely emphasize this    functional    role of theirs).
we next present a series of user studies providing quantitative and qualitative insights into the infor-
mation that subjects can extract from the visual properties of the generated images.

4.1 experiment 1: correct word vs. random confounder

the    rst experiment is a sanity check, evaluating whether the visual properties in the generated
images are informative enough for subjects to guess the correct label against a random alternative.

experiment description participants are presented with the generated image of a dreamed con-
cept and are asked to judge if it is more likely to denote the correct word or a confounder randomly

6throughout this paper, statistical signi   cance is assessed with two-tailed exact binomial tests with thresh-

old    < 0.05, corrected for multiple comparisons with the false discovery rate method.

7https://github.com/csailvision/ihog

5

picked from the seen word set. given that the confounder is a randomly picked item, the task is
relatively easy. however, both confounders and dreamed concepts are concrete, basic-level con-
cepts, so they are sometimes related just by chance. moreover, the confounders were used to train
the mapping and inversion functions, which could have introduced a systematic bias in their favour.
we test the 472 dreamed concepts, collecting 20 ratings for each via crowdflower. word order is
randomized both across and within trials (the same setup is used in the following experiments, with
image order also randomized).

results participants show a consistent preference for the correct word (dreamed concept) (median
proportion of votes in favor: 75%). preference for the correct word is signi   cantly different from
chance in 211/472 cases. participants expressed a signi   cant preference for the confounder in 10
cases only, and in the majority of those, dreamed concepts and their confounders shared similar
properties, e.g., cape-tabletop (both made of textile), zebra-baboon (both mammals), oak-boathouse
(existing in similar natural environments).
the experiment con   rms that our method can generally capture at least those visual properties of
dreamed concepts that can distinguish them from visually dissimilar random items.

4.2 experiment 2: correct image vs. image of similar concept

the second experiment ascertains to what extent subjects can pick the right generated image for a
dreamed concept over a closely related alternative.

experiment description for each dreamed concept, we pick as confounder the closest seman-
tic neighbor according to the subject-based conceptual distance statistics provided by mcrae et
al. [11]. in 379/472 cases, the confounder belongs to the category of the dreamed concept; hence,
distinguishing the two concepts is quite challenging (e.g., mandarin vs. pumpkin). participants were
presented with the images generated from the dreamed concept and the confounder, and they were
asked which of the two images is more likely to denote the dreamed concept.

results results and examples are provided in table 1. in the vast majority of cases (409/472) the
participants did not show a signi   cant preference for either the correct image or the confounder. this
shows that the current image generation pipeline does not capture, yet,    ne-grained properties that
would allow within-category discrimination. still, within the subset of 63 cases for which subjects
did express a signi   cant preferences, we observe a clear trend in favour of the correct image (41
vs. 22). color and environment seem to be the    ne-grained properties that determined many of
the subjects    right or wrong choices within this subset. of the 63 pairs, 14 involve concepts from
different categories, and 49 same-category pairs. of the former, in 11/14 the preference was for the
right image. in 2 of the 3 wrong cases, the dreamed concept vs. intruder pairs have similar color
(emerald vs. parsley, bowl vs. dish), while neither concept has a typical discriminative color in the
third case (thermometer vs. marble). even in the challenging same-category group, 30/49 pairs
display the right preference. in particular, subjects distinguished objects that typically have different
colors (e.g.,    amingo vs. partridge), or live in different environments (e.g., turtle vs. tortoise). in
the remaining 19 within-category cases in which the confounder was preferred, color seems again
to play a crucial role in the confusion (e.g., alligator vs. crocodile, asparagus vs. spinach).
we next ran a follow-up experiment to    nd out to what extent the lack of precision of our algorithm
should be attributed to noise in image generation from abstract visual features, independently of the
linguistic origin of the signal. for these purposes, we replaced the visual feature vector produced by
cross-modal mapping with the    gold-standard    visual vector for each dreamed/confounder concept
(e.g., instead of mapping the partridge word vector onto visual space, we generated a partridge
image by inverting a pool-5+exemplar vector directly extracted from a set of images labeled with this
word obtained from id163). we repeated the experiment 2 setup using the images generated in
this way. in this case, the number of pairs for which no signi   cant preference emerged was 75.4%
(356/472), in 17.6% (83/472) of the cases there was a signi   cant preference for the correct image,
and in 7% (33/472) for the confounder. the results in this setting are better than when visual features
are derived from word representations, but not dramatically so. since feature inversion is an active
area of research in id161, we can thus expect that the quality of language-driven image
generation will greatly improve simply in virtue of general advances in image generation methods.

6

in favor of dreamed concept

8.6% (41/472)

in favor of confounder

4.6% (22/472)

same category

different category

same category

different category

   amingo partridge

helicopter shotgun

alligator crocodile

bowl dish

turtle tortoise

barn cabinet

sailboat boat

emerald parsley

pumpkin mandarin

whale bison

asparagus spinach

thermometer marble

table 1: proportion and examples of cases where subjects signi   cantly preferred the dreamed con-
cept image (left) or the confounder (right). pairs of images depict dreamed and confounder concepts.
bold marks the dreamed concept that subjects were asked to pick the image for. e.g., subjects were
presented with the bowl and dish images, were asked to decide which one contains a bowl, and
preferred the image of the dish.

xxxxxxxxx

predicted

gold
man-made
organic
animal

man-made
128
0
9

organic
9
48
14

animal
5
1
33

pref. no pref. total
266
142
68
49
56
128

124
19
72

table 2: confusion matrix for experiment 3: rows report gold categories, columns report subjects   
responses (the    rst 3 columns count cases with signi   cant preferences for one macro-category only).

4.3 experiment 3: judging macro-categories of objects

the previous experiments have shown that our language-driven image generation system visualizes
properties that are salient and relevant enough to distinguish unrelated concepts (experiment 1) but
not closely related ones (experiment 2). the last experiment takes high-level category structure
explicitly into account in the design.

experiment description we group the mcrae categories into three macro-categories, namely
animal vs. organic vs. man-made, that are widely recognized in cognitive science as funda-
mental and unambiguous [11]. participants are given a generated image and are asked to pick the
macro-category that best describes the object in it.

results again, the number of images for which participants    preferences are not signi   cant is
high: 28% of the organic images, 47% of the man-made images and 56% of the animal
images. however, when participants do show signi   cant preference, in the large majority of cases
it is in favor of the correct macro-category: this is so for 98% of the organic images (70.5% of
total), 90% of the man-made images (48% of total), and 59% of the animal ones (25.7% of total).
table 2 reports the confusion matrix across the macro-categories. confusions arise where one would
expect them: both man-made and animal images are more often confused with organic things
than with each other.
again, color (either of the object itself or of the environment) is the leading property, distinguishing
objects among the three macro-categories. as figure 3 shows, orange, green and a darker mixture of
colors characterize organic things, animals, and man-made objects respectively. images that
do not typically have these colors are harder to be recognized. for instance, the few mistakes for
organic images belong to the natural object category (e.g., rocks); all the other categories within

7

figure 2: distribution of macro-category preferences across the gold concepts of the man-made
(left), organic (middle) and animal (right) categories.

this macro-category are in the vast majority of the cases judged correctly. in the man-made macro-
category (figure 2, left), the images of buildings are those more easily recognizable; as one can see in
figure 3 those images share the same pattern: two horizontal layers (land/dark and sky/blue) with a
vertical structure cutting across them (the building itself). similarly, vehicles display two layers with
a small horizontal structure crossing them, and they are almost always correctly classi   ed. finally,
within the animal macro-category (figure 2, right), birds and    sh are more often misclassi   ed than
other animals , with their typical environment probably playing a role in the confusion.

5 discussion

we introduced the new task of generating pictures visualizing the semantic content of linguistic
expressions as encoded in id27s, proposing more speci   cally a method we dubbed
language-driven image generation.
the current system seems capable to visualize the typical color of object classes and aspects of their
characteristic environment. interestingly, vector-based word representations are notoriously bad at
capturing color [2], and we do not expect them to be much better at characterizing environments, so
our results suggest that, already in its current form, our system could also be used to enrich word
representations, by highlighting aspects of concepts that are not salient in language but are probably
learned by similarity-based generalization from the cross-modal mapping training examples. in this
sense, language-driven image generation is more than a simple id27 evaluation tool. at
the same time, our system completely ignores visual properties related to shape. shapes are not often
expressed by linguistic means (although we all recognize the typical    gestalt    of, say, a mammal,
it is very dif   cult to describe it in words), but in the same way in which we can capture color and
environment, better visual representations or feature inversion methods might lead us in the future
to associate, by means of images, typical shapes to shape-blind linguistic representations.
currently we approach language-based image generation as a two-step process. inspired from recent
work in id134 that conditions word production on visual vectors, we plan to explore an
end-to-end model that conditions the generation process on information encoded in the word em-
beddings of the word/phrase that we wish to produce an image for, building upon classic generative
models of image generation [18, 5].

references

[1] marco baroni, georgiana dinu, and germ  an kruszewski. don   t count, predict! a systematic
in proceedings of

comparison of context-counting vs. context-predicting semantic vectors.
acl, 2014.

[2] elia bruni, gemma boleda, marco baroni, and nam khanh tran. id65 in

technicolor. in proceedings of acl, pages 136   145, jeju island, korea, 2012.

8

applianceno   preferenceanimalorganicman   made010203040506070buildingcontainerfurniture/large toogarmentinstrumentstructuretooltoy/sportsvehicleweaponno   preferenceman   madeanimalorganic01020304050foodfruit/vegetablenatural objectplantno   preferenceman   madeorganicanimal01020304050bird   sh/sea animalinsectmammalreptile/amphibian[3] jia deng, wei dong, richard socher, lia-ji li, and li fei-fei.

id163: a large-scale
in proceedings of cvpr, pages 248   255, miami beach, fl,

hierarchical image database.
2009.

[4] andrea frome, greg corrado, jon shlens, samy bengio, jeff dean, marc   aurelio ranzato,
and tomas mikolov. devise: a deep visual-semantic embedding model. in proceedings of
nips, pages 2121   2129, lake tahoe, nv, 2013.

[5] karol gregor, ivo danihelka, alex graves, and daan wierstra. draw: a recurrent neural

network for image generation. arxiv preprint arxiv:1502.04623, 2015.

[6] yangqing jia, evan shelhamer, jeff donahue, sergey karayev, jonathan long, ross girshick,
sergio guadarrama, and trevor darrell. caffe: convolutional architecture for fast feature
embedding. arxiv preprint arxiv:1408.5093, 2014.

[7] andrej karpathy and li fei-fei. deep visual-semantic alignments for generating image de-

scriptions. in proceedings of cvpr, boston, ma, 2015. in press.

[8] ryan kiros, ruslan salakhutdinov, and richard zemel. unifying visual-semantic embeddings
in proceedings of the nips deep learning and

with multimodal neural language models.
representation learning workshop, 2014.

[9] alex krizhevsky, ilya sutskever, and geoffrey hinton.

id163 classi   cation with deep

convolutional neural networks. in proceedings of nips, pages 1097   1105.

[10] aravindh mahendran and andrea vedaldi. understanding deep image representations by in-

verting them. in proceedings of cvpr, 2015.

[11] ken mcrae, george cree, mark seidenberg, and chris mcnorgan. semantic feature pro-
duction norms for a large set of living and nonliving things. behavior research methods,
37(4):547   559, 2005.

[12] tomas mikolov, kai chen, greg corrado, and jeffrey dean. ef   cient estimation of word

representations in vector space. http://arxiv.org/abs/1301.3781/, 2013.

[13] tom mitchell, svetlana shinkareva, andrew carlson, kai-min chang, vincente malave,
robert mason, and marcel just. predicting human brain activity associated with the mean-
ings of nouns. science, 320:1191   1195, 2008.

[14] gregory murphy. the big book of concepts. mit press, cambridge, ma, 2002.
[15] shinji nishimoto, an t vu, thomas naselaris, yuval benjamini, bin yu, and jack l gal-
lant. reconstructing visual experiences from brain activity evoked by natural movies. current
biology, 21(19):1641   1646, 2011.

[16] jeffrey pennington, richard socher, and christopher manning. glove: global vectors for word

representation. in proceedings of emnlp, pages 1532   1543, doha, qatar, 2014.

[17] olga russakovsky, jia deng, hao su, jonathan krause, sanjeev satheesh, sean ma, zhiheng
huang, andrej karpathy, aditya khosla, michael bernstein, alexander berg, and li fei-fei.
id163 large scale visual recognition challenge, 2014.

[18] ruslan salakhutdinov and geoffrey e hinton. deep id82s. in in proceedings

of aistats, pages 448   455, 2009.

[19] richard socher, milind ganjoo, christopher manning, and andrew ng. zero-shot learning
through cross-modal transfer. in proceedings of nips, pages 935   943, lake tahoe, nv, 2013.
[20] richard socher, alex perelygin, jean wu, jason chuang, christopher manning, andrew ng,
and christopher potts. recursive deep models for semantic compositionality over a sentiment
treebank. in proceedings of emnlp, pages 1631   1642, seattle, wa, 2013.

[21] christian szegedy, wojciech zaremba, ilya sutskever, joan bruna, dumitru erhan, ian
arxiv preprint

intriguing properties of neural networks.

goodfellow, and rob fergus.
arxiv:1312.6199, 2013.

[22] c. vondrick, a. khosla, t. malisiewicz, and a. torralba. hoggles: visualizing object de-

tection features. in in proceedings of iccv, 2013.

[23] carl vondrick, hamed pirsiavash, aude oliva, and antonio torralba. acquiring visual classi-

   ers from human imagination. arxiv preprint arxiv:1410.4627, 2014.

[24] matthew zeiler and rob fergus. visualizing and understanding convolutional networks. in

proceedings of eccv (part 1), pages 818   833, zurich, switzerland, 2014.

9

a answer keys to figure 1

we provide the concept names of the id27s used to generate the images of figure 1 (we
provide again figure 1 in this document to facilitate the readers). due to lack of space, we split the
concept names into 3 tables, table 1-3, where each table provides the concept names of the word
embeddings used to generate the man-made, organic and animal images respectively.

figure 3: generated images of 10 concepts per category for 20 basic categories, grouped my macro-
category.

10

appliancecontainerfurniture/large toolinstrumentgarmenttoolweapontoy sportsvehiclebuildingstructurefoodfruit/vegetablenatural objectplantbird   shinsectreptile/amphibianmammalman-madeorganicanimalsabcdefhijg12345678910111213141516171819201
dishwasher
freezer
fridge

a
b
c
d microwave
e
f
g radio
h sink
i
j

oven
projector

stereo
stove

2
ashtray
bag
barrel
basket
bathtub
bottle
bowl
box
bucket
cup

3
bed
bench
bookcase
bureau
cabinet
cage
carpet
catapult
chair
sofa

4
accordion
bagpipe
banjo
cello
clarinet
drum
   ute
guitar
harmonica
harp

5
apron
armour
belt
blouse
boots
bracelet
buckle
camisole
cape
cloak

6
anchor
banner
blender
bolts
book
brick
broom
brush
candle
crayon

8
balloon
ball
doll
football
kite
marble
racquet

7
axe
baton
bayonet
bazooka
bomb
bullet
cannon
crossbow rattle
skis
dagger
shotgun
toy

9
airplane
ambulance
bike
boat
buggy
bus
canoe
cart
car
helicopter

11
10
apartment
barn
building
basement
bungalow bedroom
cabin
cathedral
chapel
church
cottage
house
hut

bridge
cellar
elevator
escalator
garage
pier
bridge

table 3: concept names of id27s used to generate man-made images.

1
1

12
biscuit
bread
cake
cheese
pickle
pie

a
b
c
d
e
f
g raisin
h rice
i
cake
j
biscuit

13
apple
asparagus
avocado
banana
beans
beets
blueberry
broccoli
cabbage
cantaloupe muzzle

14
beehive
bouquet
emerald
muzzle
pearl
rock
seaweed
shell
stone

15
birch
cedar
dandelion
oak
pine
prune
vine
willow
birch
pine

17

16
a
blackbird whale
b
bluejay
c
budgie
d
buzzard
e
canary
f
chickadee
g    amingo
h partridge
i
j

octopus
clam
cod
crab
dolphin
eel
gold   sh
guppy
mackerel

dove
duck

18
grasshopper
hornet
moth
snail
ant
beetle
butter   y
caterpillar
cockroach
   ea

19
alligator
crocodile
frog
iguana
python
rattlesnake
salamander
toad
tortoise
cheetah

20
bear
beaver
bison
buffalo
bull
calf
camel
caribou
cat
cheetah

table 4: concept names of id27s used to generate
organic images.

table 5: concept names of id27s used to generate
animal images.

