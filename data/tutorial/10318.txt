learning to answer questions from image using convolutional neural network

forest.linma@gmail.com

hangli,hl@huawei.com

lin ma

zhengdong lu

hang li

noah   s ark lab, huawei technologies
lu.zhengdong@huawei.com

5
1
0
2

 

v
o
n
3
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
3
3
3
0
0

.

6
0
5
1
:
v
i
x
r
a

abstract

in this paper, we propose to employ the convolutional neural
network (id98) for the image id53 (qa).
our proposed id98 provides an end-to-end framework with
convolutional architectures for learning not only the image
and question representations, but also their inter-modal inter-
actions to produce the answer. more speci   cally, our model
consists of three id98s: one image id98 to encode the
image content, one sentence id98 to compose the words of
the question, and one multimodal convolution layer to learn
their joint representation for the classi   cation in the space of
candidate answer words. we demonstrate the ef   cacy of our
proposed model on the daquar and coco-qa datasets,
which are two benchmark datasets for the image qa, with the
performances signi   cantly outperforming the state-of-the-art.

introduction

recently, the multimodal learning between image and lan-
guage (ma et al. 2015; makamura et al. 2013; xu et
al. 2015b) has become an increasingly popular research
area of arti   cial intelligence (ai). in particular, there have
been rapid progresses on the tasks of bidirectional image
and sentence retrieval (frome et al. 2013; socher et al.
2014; klein et al. 2015; karpathy, joulin, and li 2014;
ordonez, kulkarni, and berg 2011), and automatic image
captioning (chen and zitnick 2014; karpathy and li 2014;
donahue et al. 2014; fang et al. 2014; kiros, salakhutdinov,
and zemel 2014a; kiros, salakhutdinov, and zemel 2014b;
klein et al. 2015; mao et al. 2014a; mao et al. 2014b;
vinyals et al. 2014; xu et al. 2015a). in order to further
advance the multimodal learning and push the boundary of
ai research, a new    ai-complete    task, namely the visual
id53 (vqa) (antol et al. 2015) or image
id53 (qa) (malinowski and fritz 2014a;
malinowski and fritz 2014b; malinowski and fritz 2015;
malinowski, rohrbach, and fritz 2015; ren, kiros, and
zemel 2015), is recently proposed. generally, it takes an
image and a free-form, natural-language like question about
the image as the input and produces an answer to the image
and question.

image qa differs with the other multimodal learning
tasks between image and sentence, such as the automatic
copyright c(cid:13) 2015, association for the advancement of arti   cial
intelligence (www.aaai.org). all rights reserved.

figure 1: samples of the image, the related question, and the
ground truth answer, as well as the answer produced by our
proposed id98 model.

image captioning. the answer produced by the image qa
needs to be conditioned on both the image and question.
as such, the image qa involves more interactions be-
tween image and language. as illustrated in figure 1,
the image contents are complicated, containing multiple
different objects. the questions about the images are very
speci   c, which requires a detailed understanding of the
image content. for the question    what is the largest
blue object in this picture?   , we need not only
identify the blue objects in the image but also compare their
sizes to generate the correct answer. for the question    how
many pieces does the curtain have?   , we need to
identify the object    curtain    in the non-salient region of
the image and    gure out its quantity.

a successful image qa model needs to be built upon
good representations of the image and question. recently,
deep neural networks have been used to learn image and
sentence representations. in particular, convolutional neural
networks (id98s) are extensively used to learn the image
representation for image recognition (simonyan and zisser-
man 2014; szegedy et al. 2015). id98s (hu et al. 2014;
kim 2014; kalchbrenner, grefenstette, and blunsom 2014)
also demonstrate their powerful abilities on the sentence
representation for paraphrase, id31, and so
on. moreover, deep neural networks (mao et al. 2014a;
karpathy, joulin, and li 2014; karpathy and li 2014;
vinyals et al. 2014) are used to capture the relations between
image and sentence for image captioning and retrieval.

question: how  many  pieces  does  the curtain have?ground truth: 2proposed id98: 2question: what is the largest blue object in this picture?ground truth: water carboyproposed id98: water carboyhowever, for the image qa task, the ability of id98 has
not been studied.

in this paper, we employ id98 to address the image qa
problem. our proposed id98 model, trained on a set of
triplets consisting of (image, question, answer), can answer
free-form, natural-language like questions about the image.
our main contributions are:
1. we propose an end-to-end id98 model for learning to
answer questions about the image. experimental results
on public image qa datasets show that our proposed
id98 model surpasses the state-of-the-art.

2. we employ convolutional architectures to encode the
image content, represent the question, and learn the inter-
actions between the image and question representations,
which are jointly learned to produce the answer condi-
tioning on the image and question.

related work

recently, the visual turing test, an open domain task of
id53 based on real-world images, has been
proposed to resemble the famous turing test. in (gao
et al. 2015) a human judge will be presented with an
image, a question, and the answer to the question by the
computational models or human annotators. based on the
answer, the human judge needs to determine whether the
answer is given by a human (i.e. pass the test) or a machine
(i.e. fail the test). geman et al. (geman et al. 2015) proposed
to produce a stochastic sequence of binary questions from a
given test image, where the answer to the question is limited
to yes/no. malinowski et al. (malinowski and fritz 2014b;
malinowski and fritz 2015) further discussed the associated
challenges and issues with regard to visual turing test, such
as the vision and language representations, the common
sense knowledge, as well as the evaluation.

the image qa task, resembling the visual turing test,
is then proposed. malinowski et al. (malinowski and fritz
2014a) proposed a multi-world approach that conducts the
id29 of question and segmentation of image
to produce the answer. deep neural networks are also em-
ployed for the image qa task, which is more related to our
research work. the work by (malinowski, rohrbach, and
fritz 2015; gao et al. 2015) formulates the image qa task
as a generation problem. malinowski et al.   s model (ma-
linowski, rohrbach, and fritz 2015), namely the neural-
image-qa, feeds the image representation from id98 and
the question into the long-short
term memory (lstm)
to produce the answer. this model ignores the different
characteristics of questions and answers. compared with
the questions, the answers tend to be short, such as one
single word denoting the object category, color, number,
and so on. the deep neural network in (gao et al. 2015),
inspired by the multimodal recurrent neural networks model
(mao et al. 2014b; mao et al. 2014a), used two lstms
for the representations of question and answer, respec-
tively. in (ren, kiros, and zemel 2015), the image qa
task is formulated as a classi   cation problem, and the so-
called visual semantic embedding (vse) model is proposed.
lstm is employed to jointly model the image and ques-

figure 2: the proposed id98 model for image qa.

tion by treating the image as an independent word, and
appending it to the question at the beginning or ending
position. as such, the joint representation of image and
question is learned, which is further used for classi   cation.
however, simply treating the image as an individual word
cannot help effectively exploit the complicated relations
between the image and question. thus, the accuracy of the
answer prediction may not be ensured. in order to cope
with these drawbacks, we proposed to employ an end-to-
end convolutional architectures for the image qa to capture
the complicated inter-modal relationships as well as the
representations of image and question. experimental results
demonstrate that the convolutional architectures can can
achieve better performance for the image qa task.

proposed id98 for image qa

for image qa, the problem is to predict the answer a given
the question q and the related image i:

a = argmax
a      

p(a|q, i;   ),

(1)

where     is the set containing all the answers.    denotes
all the parameters for performing image qa. in order to
make a reliable prediction of the answer, the question q and
image i need to be adequately represented. based on their
representations, the relations between the two multimodal
inputs are further learned to produce the answer. in this
paper, the ability of id98 is exploited for not only modeling
image and sentence individually, but also capturing the
relations and interactions between them.

as illustrated in figure 2, our proposed id98 framwork
for image qa consists of three individual id98s: one im-
age id98 encoding the image content, one sentence id98
generating the question representation, one multimodal con-
volution layer fusing the image and question representations
together and generate the joint representation. finally, the
joint representation is fed into a softmax layer to produce the
answer. the three id98s and softmax layer are fully coupled
for our proposed end-to-end image qa framework, with all
the parameters (three id98s and softmax) jointly learned in
an end-to-end fashion.

how many leftover donuts   is      the    red  bicycle holdingimage id98sentence id98multimodalconvolution. . .redgreenchairlaptopsoftmax. . .image id98
there are many research papers employing id98s to gen-
erate image representations, which achieve the state-of-
the-art performances on image recognition (simonyan and
zisserman 2014; szegedy et al. 2015). in this paper, we
employ the work (simonyan and zisserman 2014) to encode
the image content for our image qa model:

  im =   (wim(cn nim(i)) + bim),

(2)

figure 3: the sentence id98 for the question representation.

where    is a nonlinear activation function, such as sigmoid
and relu (dahl, sainath, and hinton 2013). cn nim takes
the image as the input and outputs a    xed length vector
as the image representation. in this paper, by chopping
out the top softmax layer and the last relu layer of the
id98 (simonyan and zisserman 2014), the output of the last
fully-connected layer is deemed as the image representation,
which is a    xed length vector with dimension as 4096. note
that wim is a mapping matrix of the dimension d    4096,
with d much smaller than 4096. on one hand, the dimension
of the image representation is reduced from 4096 to d. as
such, the total number of parameters for further fusing image
and question, speci   cally the multimodal convolution pro-
cess, is signi   cantly reduced. consequently, fewer samples
are needed for adequately training our id98 model. on the
other hand, the image representation is projected to a new
space, with the nonlinear activation function    increasing
the nonlinear modeling property of the image id98. thus
its capability for learning complicated representations is
enhanced. as a result, the multimodal convolution layer
(introduced in the following section) can better fuse the
question and image representations together and further ex-
ploit their complicated relations and interactions to produce
the answer.

sentence id98
in this paper, id98 is employed to model the question for
image qa. as most convolution models (lecun and bengio
1995; kalchbrenner, grefenstette, and blunsom 2014), we
consider the convolution unit with a local    receptive    eld   
and shared weights to capture the rich structures and compo-
sition properties between consecutive words. the sentence
id98 for generating the question representation is illustrated
in figure 3. for a given question with each word represented
as the id27 (mikolov et al. 2013), the sentence
id98 with several layers of convolution and max-pooling is
performed to generate the question representation   qt.
convolution for a sequential input   , the convolution unit
for feature map of type-f on the (cid:96)th layer is

  i
((cid:96),f )

def
=   (w((cid:96),f )(cid:126)  i

((cid:96)   1) + b((cid:96),f )),

(3)

where w((cid:96),f ) are the parameters for the f feature map on the
(cid:96)th layer,    is the nonlinear activation function, and (cid:126)  i
((cid:96)   1)
denotes the segment of ((cid:96)   1)th layer for the convolution at
location i , which is de   ned as follows.

(cid:126)  i
((cid:96)   1)

def
=   i

((cid:96)   1) (cid:107)   i+1

((cid:96)   1) (cid:107)        (cid:107)   i+srp   1

((cid:96)   1)

,

(4)

where srp de   nes the size of local    receptive    eld    for
convolution.    (cid:107)    concatenates the srp vectors into a long
vector. in this paper, srp is chosen as 3 for the convolution
process. the parameters within the convolution unit are
shared for the whole question with a window covering 3
semantic components sliding from the beginning to the end.
the input of the    rst convolution layer for the sentence id98
is the id27s of the question:

(cid:126)  i
(0)

def
=   i

wd (cid:107)   i+1

wd (cid:107)        (cid:107)   i+srp   1

wd

,

(5)

wd is the id27 of the ith word in the

where   i
question.
max-pooling with the convolution process, the sequential
srp semantic components are composed to a higher semantic
representation. however, these compositions may not be the
meaningful representations, such as    is on the    of the
question in figure 3. the max-pooling process following
each convolution process is performed:

((cid:96),f ) ).

((cid:96),f ),   2i+1

  i
((cid:96)+1,f ) = max(  2i

(6)
firstly, together with the stride as two, the max-pooling
process shrinks half of the representation, which can quickly
make the sentence representation. most importantly, the
max-pooling process can select the meaningful composi-
tions while    lter out the unreliable ones. as such, the
meaningful composition    of the chair    is more likely to
be pooled out, compared with the composition    front of
the   .

the convolution and max-pooling processes exploit and
summarize the local relation signals between consecutive
words. more layers of convolution and max-pooling can
help to summarize the local interactions between words
at larger scales and    nally reach the whole representation
of the question. in this paper, we employ three layers
of convolution and max-pooling to generate the question
representation   qt.
multimodal convolution layer
the image representation   im and question representation
  qt are obtained by the image and sentence id98s, respec-
tively. we design a new multimodal convolution layer on
top of them, as shown in figure 4, which fuses the multi-
modal inputs together to generate their joint representation
for further answer prediction. the image representation is
treated as an individual semantic component. based on the
image representation and the two consecutive semantic com-
ponents from the question side, the mulitmodal convolution

. . .convolutionmax-poolingmore convolution & max-pooling  what           is            on            the        table          in           front         of            the         chair...figure 4: the multimodal convolution layer to fuse the
image and question representations.

is performed, which is expected to capture the interactions
and relations between the two multimodal inputs.

(cid:126)  in
mm

def
=   i

qt (cid:107)   im (cid:107)   i+1

qt

,

  i
(mm,f )

def
=   (w(mm,f )(cid:126)  in

mm + b(mm,f )),

(7)

(8)

where (cid:126)  in
mm is the input of the multimodal convolution unit.
qt is the segment of the question representation at location
  i
i. w(mm,f ) and b(mm,f ) are the parameters for the type-f
feature map of the multimodal convolution layer.

alternatively, lstm could be used to fuse the image
and question representations, as in (malinowski, rohrbach,
and fritz 2015; ren, kiros, and zemel 2015). for ex-
ample,
in the latter work, a bidirectional lstm (ren,
kiros, and zemel 2015) is employed by appending the
image representation to the beginning or ending position
of the question. we argue that it is better to employ id98
than lstm for the image qa task, due to the following
reason, which has also been veri   ed in the following ex-
periment section. the relations between image and question
are complicated. the image may interact with the high-level
semantic representations composed from of a number of
words, such as    the red bicycle    in figure 2. however,
lstm cannot effectively capture such interactions. treating
the image representation as an individual word, the effect
of image will vanish at each time step of lstm in (ren,
kiros, and zemel 2015). as a result, the relations between
the image and the high-level semantic representations of
words may not be well exploited. in contrast, our id98
model can effectively deal with the problem. the sentence
id98    rst compose the question into a high-level semantic
representations. the multimodal convolution process further
fuse the semantic representations of image and question
together and adequately exploit their interactions.

after the mutlimodal convolution layer, the multimodal
representation   mm jointly modeling the image and question
is obtained.   mm is then fed into a softmax layer as shown
in figure 2, which produces the answer to the given image
and question pair.

experiments

in this section, we    rstly introduce the con   gurations of our
id98 model for image qa and how we train the proposed
id98 model. afterwards, the public image qa datasets
and evaluation measurements are introduced. finally, the
experimental results are presented and analyzed.

con   gurations and training
three layers of convolution and max-pooling are employed
for the sentence id98. the numbers of the feature maps
for the three convolution layers are 300, 400, and 400,
respectively. the sentence id98 is designed on a    xed archi-
tecture, which needs to be set to accommodate the maximum
length of the questions. in this paper, the maximum length
of the question is chosen as 38. the id27s are
obtained by the skip-gram model (mikolov et al. 2013)
with the dimension as 50. we use the vgg (simonyan and
zisserman 2014) network as the image id98. the dimension
of   im is set as 400. the multimodal id98 takes the image
and sentence representations as the input and generate the
joint representation with the number of feature maps as 400.
the proposed id98 model is trained with stochastic gradi-
ent descent with mini batches of 100 for optimization, where
the negative log likelihood is chosen as the loss. during
the training process, all the parameters are tuned, including
the parameters of nonlinear image mapping, image id98,
sentence id98, multimodal convolution layer, and softmax
layer. moreover, the id27s are also    ne-tuned. in
order to prevent over   tting, dropout (with id203 0.1) is
used.

image qa datasets
we test and compare our proposed id98 model on the
public image qa databases, speci   cally the daquar (ma-
linowski and fritz 2014a) and coco-qa (ren, kiros, and
zemel 2015) datasets.
daquar-all (malinowski and fritz 2014a) this dataset
consists of 6,795 training and 5,673 testing samples, which
are generated from 795 and 654 images, respectively. the
images are from all the 894 object categories. there are
mainly three types of questions in this dataset, speci   cally
the object type, object color, and number of objects. the
answer may be a single word or multiple words.
daquar-reduced (malinowski and fritz 2014a) this
dataset is a reduced version of daquar-all, comprising
3,876 training and 297 testing samples. the images are
constrained to 37 object categories. only 25 images are used
for the testing sample generation. same as the daquar-all
dataset, the answer may be a single word or multiple words.
coco-qa (ren, kiros, and zemel 2015) this dataset
consists of 79,100 training and 39,171 testing samples,
which are generated from about 8,000 and 4,000 images,
respectively. there are four types of questions, speci   cally
the object, number, color, and location. the answers are all
single-word.

evaluation measurements
one straightforward way for evaluating image qa is to uti-
lize accuracy, which measures the proportion of the correctly
answered testing questions to the total testing questions.
besides accuracy, wu-palmer similarity (wups) (wu and
palmer 1994; malinowski and fritz 2014a) is also used to
measure the performances of different models on the image
qa task. wups calculates the similarity between two words
based on their common subsequence in a taxonomy tree.

multimodal convolution   table 1: image qa performances on daquar-all.

table 2: image qa performances on daquar-reduced.

multi-world approach
(malinowski and fritz 2014a)
human answers
(malinowski, rohrbach, and fritz 2015)
human answers without image
(malinowski, rohrbach, and fritz 2015)
neural-image-qa
(malinowski, rohrbach, and fritz 2015)
-multiple words
-single word
language approach
-multiple words
-single word
proposed id98
-multiple words
-single word

accuracy

wups wups
@0.0
@0.9

7.86

11.86

38.79

50.20

50.82

67.27

11.99

16.82

33.57

17.49
19.43

17.06
17.15

21.47
24.49

23.28
25.28

22.30
22.80

27.15
30.47

57.76
62.00

56.53
58.42

59.44
66.08

a threshold parameter is required for the calculation of
wups. same as the previous work (ren, kiros, and zemel
2015; malinowski and fritz 2014a; malinowski, rohrbach,
and fritz 2015), the threshold parameters 0.0 and 0.9 are
used for the measurements wups@0.0 and wups@0.9,
respectively.
experimental results and analysis
competitor models we compare our models with re-
cently developed models for the image qa task, speci   cally
the multi-world approach (malinowski and fritz 2014a),
the vse model (ren, kiros, and zemel 2015), and the
neural-image-qa approach (malinowski, rohrbach, and
fritz 2015).
performances on image qa the performances of our
proposed id98 model on the daquar-all, daquar-
reduced, and coco-qa datasets are illustrated in table
1, 2, and 3, respectively. for daquar-all and daquar-
reduced datasets with multiple words as the answer to the
question, we treat the answer comprising multiple words as
an individual class for training and testing.

for the daquar-all dataset, we evaluate the perfor-
mances of different image qa models on the full set (   mul-
tiple words   ). the answer to the image and question pair
may be a single word or multiple words. same as the work
(malinowski, rohrbach, and fritz 2015), a subset containing
the samples with only a single word as the answer is
created and employed for comparison (   single word   ). our
proposed id98 model signi   cantly outperforms the multi-
world approach and neural-image-qa in terms of accuracy,
wups@0.0, and wups@0.9. speci   cally, our proposed
id98 model achieves over 20% improvement compared to
neural-image-qa in terms of accuracy on both    multiple
words    and    single word   . the results, shown in table. 1,
demonstrate that our id98 model can more accurately model
the image and question as well as their interactions, thus
yields better performances for the image qa task. moreover,
the language approach (malinowski, rohrbach, and fritz
2015), which only resorts to the question performs inferiorly

multi-world approach
(malinowski and fritz 2014a)
neural-image-qa
(malinowski, rohrbach, and fritz 2015)
-multiple words
-single word
language approach
-multiple words
-single word
vse (ren, kiros, and zemel 2015)
-single word
guess
bow
lstm
img+bow
vis+lstm
2-vis+blstm
proposed id98
-multiple words
-single word

accuracy

wups wups
@0.0
@0.9

12.73

18.10

51.47

29.27
34.68

32.32
31.65

18.24
32.67
32.73
34.17
34.41
35.78

38.38
42.76

36.50
40.76

38.39
38.35

29.65
43.19
43.50
44.99
46.05
46.83

43.43
47.58

79.47
79.54

80.05
80.08

77.59
81.30
81.62
81.48
82.23
82.14

80.63
82.60

to the approaches that jointly model the image and question.
the image component is thus of great help to the image qa
task. one can also see that the performances on    multiple
words    are generally inferior to those on    single word   .

for the daquar-reduced dataset, besides the neural-
image-qa approach, the vse model is also compared on
   single word   . moreover, some of the methods introduced
in (ren, kiros, and zemel 2015) are also reported and
compared. guess is the model which randomly outputs
the answer according to the question type. bow treats
each word of the question equally and sums all the word
vectors to predict the answer by id28. lstm
is performed only on the question without considering the
image, which is similar to the language approach (mali-
nowski, rohrbach, and fritz 2015). img+bow performs
the multinomial id28 based on the image
feature and a bow vector obtained by summing all the word
vectors of the question. vis+lstm and 2-vis+blstm
are two versions of the vse model. vis+lstm has only
a single lstm to encode the image and question in one
direction, while 2-vis+blstm uses a bidirectional lstm
to encode the image and question along with both directions
to fully exploit the interactions between image and each
word of the question. it can be observed that 2-vis+blstm
outperforms vis+lstm with a big margin. the same ob-
servation can also be found on the coco-qa dataset,
as shown in table 3, demonstrating that the bidirectional
lstm can more accurately model the interactions between
image and question than the single lstm. our proposed
id98 model signi   cantly outperforms the competitor mod-
els. more speci   cally, for the case of    single word   , our
proposed id98 achieves nearly 20% improvement in terms
of accuracy over the best competitor model 2-vis+blstm.
for the coco-qa dataset, img+bow outperforms
vis+lstm and 2-vis+blstm, demonstrating that the

table 3: image qa performances on coco-qa.

vse (ren, kiros, and zemel 2015)
guess
bow
lstm
img
img+bow
vis+lstm
2-vis+blstm
full
proposed id98 without
multimodal convolution layer
proposed id98 without
image representation
proposed id98

accuracy

wups wups
@0.0
@0.9

6.65
37.52
36.76
43.02
55.92
53.31
55.09
57.84

56.77

37.84

58.40

17.42
48.54
47.58
58.64
66.78
63.91
65.34
67.90

73.44
82.78
82.34
85.85
88.99
88.25
88.64
89.52

66.76

88.94

48.70

68.50

82.92

89.67

id28 of

img+bow
simple multinomial
can better model
the interactions between image and
question, compared with the lstms of vis+lstm and 2-
vis+blstm. by averaging vis+lstm, 2-vis+blstm,
and img+bow, the full model is developed, which
summarizes the interactions between image and question
from different perspectives thus yields a much better
performance. as shown in table 3, our proposed id98
model outperforms all the competitor models in terms of
all
the three evaluation measurements, even the full
model. the reason may be that the image representation is
of highly semantic meaning, which should interact with the
high semantic components of the question. our id98 model
   rstly uses the convolutional architectures to compose
the words to highly semantic representations. afterwards,
we let
the composed highly semantic
representations and use convolutional architectures to
exploit
their relations and interactions for the answer
prediction. as such, our id98 model can well model the
relations between image and question, and thus obtain the
best performances.

the image meet

in   uence of multimodal convolution layer the image
and question needs to be considered together for the image
qa. the multimodal convolution layer in our proposed
id98 model not only fuses the image and question rep-
resentations together but also learns the interactions and
relations between the two multimodal inputs for further
question prediction. the effect of the multimodal convolu-
tion layer is examined as follows. the image and question
representations are simply concatenated together as the input
of the softmax layer for the answer prediction. we train
the network in the same manner as the proposed id98
model. the results are provided in table 3. firstly, it can be
observed that without the multimodal convolution layer, the
performance on the image qa has dropped. comparing to
the simple concatenation process fusing the image and ques-
tion representations, our proposed multimodal convolution
layer can well exploit the complicated relationships between
image and question representations. thus a better perfor-
mance for the answer prediction is achieved. secondly, the

approach without multimodal convolution layer outperforms
the img+bow, vis+lstm and 2-vis+blstm, in terms
of accuracy. the better performance is mainly attributed to
the composition ability of the sentence id98. even with the
simple concatenation process, the image representation and
composed question representation can be fuse together for a
better image qa model.

in   uence of image id98 and effectiveness of sentence
id98 as can be observed in table 1, without the image
content, the accuracy of human answering the question
drops from 50% to 12%. therefore, the image content is crit-
ical to the image qa task. same as the work (malinowski,
rohrbach, and fritz 2015; ren, kiros, and zemel 2015),
we only use the question representation obtained from the
sentence id98 to predict the answer. the results are listed
in table 3. firstly, without the use of image representation,
the performance of our proposed id98 signi   cantly drops,
which again demonstrates the importance of image compo-
nent to the image qa. secondly, the model only consisting
of the sentence id98 performs better than lstm and bow
for the image qa. it indicates that the sentence id98 is
more effective to generate the question representation for
image qa, compared with lstm and bow. recall that the
model without multimodal convolution layers outperforms
img+bow, vis+lstm, and 2-vis+blstm, as explained
above. by incorporating the image representation, the better
modeling ability of our sentence id98 is demonstrated.

moreover, we examine the id38 ability
of the sentence id98 as follows. the words of the test
questions are randomly reshuf   ed. then the reformulated
questions are sent to the sentence id98 to check whether the
sentence id98 can still generate reliable question represen-
tations and make accurate answer predictions. for randomly
reshuf   ed questions, the results on coco-qa dataset are
40.74, 53.06, and 80.41 for the accuracy, wups@0.9, and
wups@0.0, respectively, which are signi   cantly inferior to
that of natural-language like questions. the result indicates
that the sentence id98 possesses the ability of modeling
natural questions. the sentence id98 uses the convolution
process to compose and summarize the neighboring words.
and the reliable ones with higher semantic meanings will
be pooled and composed further to reach the    nal sentence
representation. as such, the sentence id98 can compose
the natural-language like questions to reliable high semantic
representations.

conclusion

in this paper, we proposed one id98 model to address the
image qa problem. the proposed id98 model relies on
convolutional architectures to generate the image represen-
tation, compose consecutive words to the question repre-
sentation, and learn the interactions and relations between
the image and question for the answer prediction. experi-
mental results on public image qa datasets demonstrate the
superiority of our proposed model over the state-of-the-art
methods.

acknowledgement

the work is partially supported by china national 973
project 2014cb340301. the authors are grateful to baotian
hu and zhenguo li for their insightful discussions and
comments.

references

[antol et al. 2015] antol, s.; agrawal, a.; lu, j.; mitchell,
m.; batra, d.; zitnick, c. l.; and parikh, d. 2015. vqa:
visual id53. arxiv 1505.00468.
[chen and zitnick 2014] chen, x., and zitnick, c. l. 2014.
learning a recurrent visual representation for image caption
generation. arxiv 1411.5654.
[dahl, sainath, and hinton 2013] dahl, g. e.; sainath, t. n.;
and hinton, g. e. 2013. improving deep neural networks for
lvcsr using recti   ed linear units and dropout. in icassp.
[donahue et al. 2014] donahue,
j.; hendricks, l. a.;
guadarrama, s.; rohrbach, m.; venugopalan, s.; saenko,
k.; and darrell, t. 2014. long-term recurrent convolutional
arxiv
networks for visual recognition and description.
1411.4389.
[fang et al. 2014] fang, h.; gupta, s.;
iandola, f. n.;
srivastava, r.; deng, l.; doll  ar, p.; gao, j.; he, x.; mitchell,
m.; platt, j. c.; zitnick, c. l.; and zweig, g. 2014. from
captions to visual concepts and back. arxiv 1411.4952.
[frome et al. 2013] frome, a.; corrado, g.; shlens, j.;
bengio, s.; dean, j.; ranzato, m.; and mikolov, t. 2013.
devise: a deep visual-semantic embedding model. in nips.
[gao et al. 2015] gao, h.; mao, j.; zhou, j.; huang, z.;
wang, l.; and xu, w.
are you talking to
a machine? dataset and methods for multilingual image
id53. arxiv 1505.05612.
[geman et al. 2015] geman, d.; geman, s.; hallonquist, n.;
and younes, l. 2015. visual turing test for id161
systems. in pnas.
[hu et al. 2014] hu, b.; lu, z.; li, h.; and chen, q. 2014.
convolutional neural network architectures for matching
natural language sentences. in nips.
[kalchbrenner, grefenstette, and blunsom 2014]
kalchbrenner, n.; grefenstette, e.; and blunsom, p.
2014.
a convolutional neural network for modelling
sentences. in acl.
[karpathy and li 2014] karpathy, a., and li, f.-f. 2014.
deep visual-semantic alignments for generating image
descriptions. arxiv 1412.2306.
[karpathy, joulin, and li 2014] karpathy, a.; joulin, a.; and
li, f.-f. 2014. deep fragment embeddings for bidirectional
image sentence mapping. in nips.
[kim 2014] kim, y. 2014. convolutional neural networks
for sentence classi   cation. in emnlp.
[kiros, salakhutdinov, and zemel 2014a] kiros,
salakhutdinov, r.; and zemel, r.
neural language models. in icml.
[kiros, salakhutdinov, and zemel 2014b] kiros,
salakhutdinov, r.; and zemel, r. s.

r.;
2014a. multimodal

r.;
2014b. unifying

2015.

t.;

nagai,

embeddings with multimodal neural

visual-semantic
language models. arxiv 1411.2539.
[klein et al. 2015] klein, b.; lev, g.; sadeh, g.; and wolf,
l. 2015. fisher vectors derived from hybrid gaussian-
laplacian mixture models for image annotation. in cvpr.
[lecun and bengio 1995] lecun, y., and bengio, y. 1995.
convolutional networks for images, speech and time series.
the handbook of brain theory and neural networks.
[ma et al. 2015] ma, l.; lu, z.; shang, l.; and li, h. 2015.
multimodal convolutional neural networks for matching
image and sentence. in iccv.
[makamura et al. 2013] makamura,
t.;
funakoshi, k.; nagasaka, s.; taniguchi, t.; and iwahashi,
n. 2013. mutual learning of an object concept and language
model based on mlda and npylm. in iros.
[malinowski and fritz 2014a] malinowski, m., and fritz, m.
2014a. a multi-world approach to id53 about
real-world scenes based on uncertain input. in nips.
[malinowski and fritz 2014b] malinowski, m., and fritz, m.
2014b. towards a visual turing challenge. arxiv 1410.8027.
[malinowski and fritz 2015] malinowski, m., and fritz, m.
2015. hard to cheat: a turing test based on answering
questions about images. arxiv 1501.03302.
[malinowski, rohrbach, and fritz 2015] malinowski, m.;
rohrbach, m.; and fritz, m. 2015. ask your neurons:
a neural-based approach to answering questions about
images. arxiv 1505.01121.
[mao et al. 2014a] mao, j.; xu, w.; yang, y.; wang, j.; and
yuille, a. l. 2014a. deep captioning with multimodal
recurrent neural networks (m-id56). arxiv 1412.6632.
[mao et al. 2014b] mao, j.; xu, w.; yang, y.; wang, j.; and
yuille, a. l.
2014b. explain images with multimodal
recurrent neural networks. arxiv 1410.1090.
[mikolov et al. 2013] mikolov, t.; chen, k.; corrado, g.;
and dean, j.
ef   cient estimation of word
representations in vector space. arxiv 1301.3781.
[ordonez, kulkarni, and berg 2011] ordonez, v.; kulkarni,
g.; and berg, t. l. 2011. im2text: describing images using
1 million captioned photographs. in nips.
[ren, kiros, and zemel 2015] ren, m.; kiros, r.;
and
zemel, r. s. 2015. exploring models and data for image
id53. arxiv 1505.02074.
[simonyan and zisserman 2014] simonyan, k., and zisser-
man, a. 2014. very deep convolutional networks for large-
scale image recognition. arxiv 1409.1556.
[socher et al. 2014] socher, r.; karpathy, a.; le, q. v.;
manning, c. d.; and ng, a. y.
grounded
id152 for    nding and describing images
with sentences. in tacl.
[szegedy et al. 2015] szegedy, c.; liu, w.; jia, y.; sermanet,
p.; reed, s.; anguelov, d.; erhan, d.; vanhoucke, v.; and
rabinovich, a. 2015. going deeper with convolutions. in
cvpr.
[vinyals et al. 2014] vinyals, o.; toshev, a.; bengio, s.; and

2014.

2013.

erhan, d. 2014. show and tell: a neural image caption
generator. arxiv 1411.4555.
[wu and palmer 1994] wu, z., and palmer, m. s. 1994. verb
semantics and lexical selection. in acl.
[xu et al. 2015a] xu, k.; ba, j.; kiros, r.; cho, k.;
courville, a.; salakhutdinov, r.; zemel, r.; and bengio,
y. 2015a. show, attend and tell: neural image caption
generation with visual attention. arxiv 1502.03044.
[xu et al. 2015b] xu, r.; xiong, c.; chen, w.; and corso,
j. 2015b. jointly modeling deep video and compositional
text to bridge vision and language in a uni   ed framework.
in aaai.

