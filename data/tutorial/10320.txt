listen, attend, and walk: neural mapping

of navigational instructions to action sequences

hongyuan mei

mohit bansal

matthew r. walter

toyota technological institute at chicago

{hongyuan,mbansal,mwalter}@ttic.edu

chicago, il 60637

5
1
0
2
 
c
e
d
7
1

 

 
 
]
l
c
.
s
c
[
 
 

4
v
9
8
0
4
0

.

6
0
5
1
:
v
i
x
r
a

abstract

we propose a neural sequence-to-sequence model for direc-
tion following, a task that is essential to realizing effective
autonomous agents. our alignment-based encoder-decoder
model with long short-term memory recurrent neural net-
works (lstm-id56) translates natural language instructions
to action sequences based upon a representation of the ob-
servable world state. we introduce a multi-level aligner that
empowers our model to focus on sentence    regions    salient
to the current world state by using multiple abstractions of
the input sentence. in contrast to existing methods, our model
uses no specialized linguistic resources (e.g., parsers) or task-
speci   c annotations (e.g., seed lexicons). it is therefore gen-
eralizable, yet still achieves the best results reported to-date
on a benchmark single-sentence dataset and competitive re-
sults for the limited-training multi-sentence setting. we ana-
lyze our model through a series of ablations that elucidate the
contributions of the primary components of our model.

introduction

robots must be able to understand and successfully execute
natural language navigational instructions if they are to work
seaid113ssly alongside people. for example, someone using
a voice-commandable wheelchair might direct it to    take
me to the room across from the kitchen,    or a soldier may
command a micro aerial vehicle to    fly down the hallway
into the second room on the right.    however, interpreting
such free-form instructions (especially in unknown environ-
ments) is challenging due to their ambiguity and complexity,
such as uncertainty in their interpretation (e.g., which hall-
way does the instruction refer to), long-term dependencies
among both the instructions and the actions, differences in
the amount of detail given, and the diverse ways in which
the language can be composed. figure 1 presents an exam-
ple instruction that our method successfully follows.

previous work in this domain (chen and mooney 2011;
chen 2012; kim and mooney 2012; 2013; artzi and zettle-
moyer 2013; artzi, das, and petrov 2014) largely requires
specialized resources like semantic parsers, seed lexicons,
and re-rankers to interpret ambiguous, free-form natural lan-
guage instructions. in contrast, the goal of our work is to
learn to map instructions to actions in an end-to-end fash-
ion that assumes no prior linguistic knowledge. instead, our
model learns the meaning of all the words, spatial relations,

place your back against the wall of the    t    intersection. go
forward one segment to the intersection with the blue-tiled
hall. this interesction [sic] contains a chair. turn left. go
forward to the end of the hall. turn left. go forward one seg-
ment to the intersection with the wooden-   oored hall. this
intersection conatains [sic] an easel. turn right. go forward
two segments to the end of the hall. turn left. go forward
one segment to the intersection containing the lamp. turn
right. go forward one segment to the empty corner.

figure 1: an example of a route instruction-path pair in one of the
virtual worlds from macmahon, stankiewicz, and kuipers (2006)
with colors that indicate    oor patterns and wall paintings, and let-
ters that indicate different objects. our method successfully infers
the correct path for this instruction.

syntax, and id152 from just the raw train-
ing sequence pairs, and learns to to translate the free-form
instructions to an executable action sequence.

we propose a recurrent neural network with long short-
term memory (lstm) (hochreiter and schmidhuber 1997)
to both encode the navigational instruction sequence bidi-
rectionally and to decode the representation to an action
sequence, based on a representation of the current world
state. lstms are well-suited to this task, as they have
been shown to be effective in learning the temporal de-

1

bobjectsbarstoolcchaireeaselhhatrackllampssofawall paintingstowerbutterflyfishfloor patternsbrickblueconcreteflowergrassgravelwoodyellowlehcssecbhlpendencies that exist over such sequences, especially for
the tasks of image captioning, machine translation, and
id86
(kiros, salakhutdinov, and
zemel 2014; donahue et al. 2014; chen and zitnick 2015;
karpathy and fei-fei 2015; vinyals et al. 2015; sutskever,
vinyals, and lee 2014; rush, chopra, and weston 2015;
wen et al. 2015). additionally, we learn the correspon-
dences between words in the input navigational instruction
and actions in the output sequence using an alignment-based
lstm (bahdanau, cho, and bengio 2014; xu et al. 2015).
standard alignment methods only consider high-level ab-
stractions of the input (language), which sacri   ces infor-
mation important to identifying these correspondences. in-
stead, we introduce a multi-level aligner that empowers the
model to use both high- and low-level input representations
and, in turn, improves the accuracy of the inferred directions.
we evaluate our model on a benchmark navigation
dataset (macmahon, stankiewicz, and kuipers 2006) and
achieve the best results reported to-date on the single-
sentence task (which contains only 2000 training pairs),
without using any additional resources such as semantic
parsers, seed lexicons, or rerankers used in previous work.
on the multi-sentence task of executing a full paragraph,
where the amount of training pairs is even smaller (just a
few hundred pairs), our model performs better than several
existing methods and is competitive with the state-of-the-art,
all of which use specialized linguistic resources, extra anno-
tation, or reranking. we perform a series of ablation studies
in order to analyze the primary components of our model, in-
cluding the encoder, multi-level representations, alignment,
and bidirectionality.

related work

a great deal of attention has been paid of late to algo-
rithms that allow robots and autonomous agents to fol-
low free-form navigational route instructions (macmahon,
stankiewicz, and kuipers 2006; kollar et al. 2010; chen and
mooney 2011; chen 2012; kim and mooney 2012; 2013;
kong et al. 2014; hemachandra et al. 2015). these methods
solve what harnad (1990) refers to as the symbol grounding
problem, that of associating linguistic elements with their
corresponding manifestation in the external world.
initial
research in natural language symbol grounding focused on
manually-prescribed mappings between language and sets
of prede   ned environment features and actions (winograd
1970; macmahon, stankiewicz, and kuipers 2006). more
recent work in statistical language understanding learns
to convert free-form instructions into their referent sym-
bols by observing the use of language in a perceptual
context (mooney 2008). these methods represent natu-
ral language grounding in terms of manually de   ned lin-
guistic, spatial, and semantic features (kollar et al. 2010;
matuszek, fox, and koscher 2010; tellex et al. 2011). they
learn the model parameters from natural language corpora,
often requiring expensive annotation to pair each phrase to
its corresponding grounding.

one class of grounded id146 methods
treats the language understanding problem as one of learn-
ing a parser that maps free-form language into its formal lan-

guage equivalent. for example, matuszek, fox, and koscher
(2010) assume no prior linguistic knowledge and employ a
general-purpose supervised semantic parser learner. alter-
natively, chen and mooney (2011) parse free-form route in-
structions into formal action speci   cations that a robot con-
trol process can then execute. they learn the parser in a
weakly supervised manner from natural language instruc-
tion and action sequence pairs, together with the correspond-
ing world representation. alternatively, kim and mooney
(2012) frame grounded language learning as probabilistic
id18 (pid18) induction and use learned lex-
icons (chen and mooney 2011) to control the space of pro-
duction rules, which allows them to scale pid18s to the nav-
igation domain. kim and mooney (2013) improve upon the
accuracy by adding a subsequent re-ranking step that uses
a weakly supervised discriminative classi   er. meanwhile,
artzi and zettlemoyer (2013) learn a combinatory categor-
ical grammar (id35)-based semantic parser to convert free-
form navigational instructions to their manifestation in a
lambda-calculus representation. as with kim and mooney
(2013), they improve upon the accuracy through re-ranking.
artzi, das, and petrov (2014) extend their id35 parser learn-
ing by using statistics of the corpus to control the size of the
lexicon, resulting in improved multi-sentence accuracy.

a second class of grounded language learning techniques
function by mapping free-form utterances to their corre-
sponding object, location, and action referents in the agent   s
world model. these methods learn a probabilistic model
that expresses the association between each word in the in-
struction and its matching referent in the world model. the
problem of interpreting a new instruction then becomes one
of id136 in this learned model. kollar et al. (2010) build
a generative model over the assumed    at, sequential struc-
ture of language that includes a combination of pre-speci   ed
and learned models for spatial relations, adverbs, and verbs.
tellex et al. (2011) later propose a discriminative model that
expresses the hierarchical, compositional structure of lan-
guage. they factor the id203 distribution according to
the parse structure of the free-form command and employ
a log-linear factor graph to express the learned correspon-
dence between linguistic elements and the space of ground-
ings (objects, locations, and actions).

we adopt an alternative formulation and treat the problem
of interpreting route instructions as a sequence-to-sequence
learning problem. we learn this mapping in an end-to-end
fashion using a neural network, without using any prior lin-
guistic structure, resources, or annotation, which improves
generalizability. our method is inspired by the recent suc-
cess of such sequence-to-sequence methods for machine
translation (sutskever, vinyals, and lee 2014; bahdanau,
cho, and bengio 2014; cho et al. 2014), image and video
caption synthesis (kiros, salakhutdinov, and zemel 2014;
mao et al. 2014; donahue et al. 2014; vinyals et al. 2015;
chen and zitnick 2015; karpathy and fei-fei 2015), and
id86 (rush, chopra, and weston
2015; wen et al. 2015), which similarly adopt an encoder-
decoder approach. our model encodes the input free-form
route instruction and then decodes the embedding to identify
the corresponding output action sequence based upon the lo-

2

figure 2: our encoder-aligner-decoder model with multi-level alignment

cal, observable world state (which we treat as a third se-
quence type that we add as an extra connection to every de-
coder step). moreover, our decoder also includes alignment
to focus on the portions of the sentence relevant to the cur-
rent action, a technique that has proven effective in machine
translation (bahdanau, cho, and bengio 2014) and machine
vision (mnih et al. 2014; ba, mnih, and kavukcuoglu 2014;
xu et al. 2015). however, unlike the standard alignment
techniques, our model learns to align based not only on the
high-level input abstraction, but also the low-level represen-
tation of the input instruction, which improves performance.
recently, andreas and klein (2015) use a conditional ran-
dom    eld model to learn alignment between instructions and
actions; our lstm-based aligner performs substantially bet-
ter than this approach.

task de   nition

we consider the problem of mapping natural language nav-
igational instructions to action sequences based only on
knowledge of the local, observable environment. these in-
structions may take the form of isolated sentences (single-
sentence) or full paragraphs (multi-sentence). we are inter-
ested in learning this mapping from corpora of training data
of the form (x(i), a(i), y(i)) for i = 1, 2, . . . , n, where x(i)
is a variable length natural language instruction, a(i) is the
corresponding action sequence, and y(i) is the observable
environment representation. the model learns to produce
the correct action sequence a(i) given a previously unseen
(x(i), y(i)) pair. the challenges arise from the fact that the
instructions are free-form and complex, contain numerous
spelling and grammatical errors, and are ambiguous in their
meaning. further, the model is only aware of the local envi-
ronment in the agent   s line-of-sight.

in this paper, we consider the route instruction dataset
generated by macmahon, stankiewicz, and kuipers (2006).
the data includes free-form route instructions and their cor-
responding action sequences within three different virtual
worlds. the environments (fig. 1) consist of interconnected
hallways with a pattern (grass, brick, wood, gravel, blue,
   ower, or yellow octagons) on each hallway    oor, a paint-
ing (butter   y,    sh, or eiffel tower) on the walls, and ob-
jects (hat rack, lamp, chair, sofa, barstool, and easel) at in-
tersections. after having explored an environment, instruc-

tors were asked to give written commands that describe how
to navigate from one location to another without subsequent
access to the map. each instruction was then given to sev-
eral human followers who were tasked with navigating in the
virtual world without a map, and their paths were recorded.
many of the raw instructions include spelling and grammat-
ical errors, others are incorrect (e.g., misusing    left    and
   right   ), approximately 10% of the single sentences have no
associated action, and 20% have no feasible path.

the model

we formulate the problem of interpreting natural language
route instructions as id136 over a probabilistic model
p (a1:t|y1:t , x1:n ), where a1:t = (a1, a2, . . . , at ) is the
action sequence, yt is the world state at time t, and x1:n =
(x1, x2, . . . , xn ) is the natural language instruction,

a   
1:t = arg max

p (a1:t|y1:t , x1:n )

a1:t

= arg max

a1:t

t(cid:89)

t=1

p (at|a1:t   1, yt, x1:n )

(1a)

(1b)

this problem can be viewed as one of mapping the given
instruction sequence x1:n to the action sequence a1:t . an
effective means of learning this sequence-to-sequence map-
ping is to use a neural encoder-decoder architecture. we    rst
use a bidirectional recurrent neural network model to encode
the input sentence

is

encoder hidden state

hj = f (xj, hj   1, hj+1)
zt = c(h1, h2, . . . hn ),
the

(2a)
(2b)
where hj
for word
j     {1, . . . , n}, and f and c are nonlinear functions
that we de   ne shortly. next, the context vector zt (com-
puted by the aligner) encodes the language instruction at
time t     {1, . . . , t}. next, another id56 decodes the
context vector zt to arrive at the desired likelihood (1)
p (at|a1:t   1, yt, x1:n )

p (a1:t|y1:t , x1:n ) =

t(cid:89)

(3a)

t=1

p (at|a1:t   1, yt, x1:n ) = g(st   1, zt, yt),
(3b)
where st   1 is the decoder hidden state at time t    1, and g is
a nonlinear function. id136 then follows by maximizing
this posterior to determine the desired action sequence.

3

lstm-id56multi-level alignerdecoderencoderalignerlstm-id56lstm-id56lstm-id56go forward two segmentsto the end of the halleworld stateaction sequenceinstruction      
h (cid:62)
hj = (
j ;
annotations

      
h (cid:62)
j )(cid:62) concatenate forward
      
h j, each determined using equation 4c.

      
h j and backward

multi-level aligner the context representation of the in-
struction is computed as a weighted sum of the word vectors
xj and encoder states hj. whereas most previous work align
based only on the hidden annotations hj, we found that also
including the original input word xj in the aligner improves
performance. this multi-level representation allows the de-
coder to not just reason over the high-level, context-based
representation of the input sentence hj, but to also consider
the original low-level word representation xj. by adding xj,
the model offsets information that is lost in the high-level
abstraction of the instruction. intuitively, the model is able
to better match the salient words in the input sentence (e.g.,
   easel   ) directly to the corresponding landmarks in the cur-
rent world state yt used in the decoder. the context vector
then takes the form

(cid:88)

j

zt =

  tj

(cid:19)

(cid:18)xj
(cid:88)

hj

(5)

(6)

(8a)

the weight   tj associated with each pair (xj, hj) is

  tj = exp(  tj)/

exp(  tj),

j

where the alignment term   tj = f (st   1, xj, hj) weighs the
extent to which the word at position j and those around it
match the output at time t. the alignment is modelled as a
one-layer neural id88

  tj = v(cid:62) tanh(w st   1 + u xj + v hj),

(7)

where v, w , u, and v are learned parameters.

decoder our architecture uses an lstm decoder (fig. 3)
that takes as input the current world state yt, the context
of the instruction zt, and the lstm   s previous hidden state
st   1. the output is the id155 distribution
pa,t = p (at|a1:t   1, yt, x1:n ) over the next action (3), rep-
resented as a deep output layer (pascanu et al. 2014)

             =

             id

t
f d
t
od
t
gd
t

            

  
  

          t d

(cid:32) eyt

(cid:33)

st   1
zt
tanh
t (cid:12) cd
t (cid:12) gd
t   1 + id
t (cid:12) tanh(cd
t )

t

cd
t = f d
st = od
qt = l0(eyt + lsst + lzzt)

(8b)
(8c)
(8d)
(8e)
where e is an embedding matrix and l0, ls, and lz are
parameters to be learned.

pa,t = softmax (qt)

training we train the encoder and decoder models so as
to predict the action sequence a   
1:t according to equation 1
for a given instruction x1:n and world state y1:t from the

4

figure 3: long short-term memory (lstm) unit.

our model (fig. 2) employs lstms as the nonlinear func-
tions f and g due to their ability to learn long-term depen-
dencies that exist over the instruction and action sequences,
without suffering from exploding or vanishing gradients.
our model also integrates multi-level alignment to focus on
parts of the instruction that are more salient to the current
action at multiple levels of abstraction. we next describe
each component of our network in detail.

a

route

the natural

instruction represented as

encoder our encoder takes as input
lan-
guage
sequence
x1:n = (x1, x2, . . . , xn ), where x1 and xn are the
   rst and last words in the sentence, respectively. we treat
each word xi as a k-dimensional one-hot vector, where
k is the vocabulary size. we feed this sequence into an
lstm-id56 that summarizes the temporal relationships
between previous words and returns a sequence of hidden
annotations h1:n = (h1, h2, . . . , hn ), where the annotation
hj summarizes the words up to and including xj.

we adopt an lstm encoder architecture (fig. 3) similar

to that of graves, abdel-rahman, and hinton (2013),

             =

             ie

j
f e
j
oe
j
ge
j

            

  
  

          t e

(cid:19)

(cid:18) xj

hj   1

tanh
j (cid:12) ce
j (cid:12) ge
j   1 + ie
j (cid:12) tanh(ce
j)

j

ce
j = f e
hj = oe

(4a)

(4b)
(4c)

j, f e

j , and oe

where t e is an af   ne transformation,    is the logistic sig-
moid that restricts its input to [0, 1], ie
j are the
input, output, and forget gates of the lstm, respectively,
and ce
j is the memory cell activation vector. the memory
cell ce
j summarizes the lstm   s previous memory ce
j   1 and
the current input, which are modulated by the forget and in-
put gates, respectively. the forget and input gates enable the
lstm to regulate the extent to which it forgets its previous
memory and the input, while the output gate regulates the
degree to which the memory affects the hidden state.

our encoder employs bidirectionality, encoding the sen-
tences in both the forward and backward directions, an ap-
proach that has been found to be successful in speech recog-
nition and machine translation (graves, abdel-rahman,
and hinton 2013; bahdanau, cho, and bengio 2014;
cho et al. 2014).
the hidden annotations

in this way,

cmemory cellinput gateoutput gateforget gatetraining corpora. we use the negative log-likelihood of the
demonstrated action at each time step t as our id168,
(9)
as the entire model is a differentiable function, the parame-
ters can be learned by back-propagation.

l =     log p (a   

t|yt, x1:n ).

id136 having trained the model, we generate action
sequences by    nding the maximum a posteriori actions un-
der the learned model (1). one action sequence is completed
when the    stop    action is emitted. for the single-sentence
task, we perform this search using standard id125 to
maintain a list of the current k best hypotheses.1 we itera-
tively consider the k-best sequences up to time t as candi-
dates to generate sequences of size t + 1 and keep only the
resulting best k of them. for multi-sentence, we perform the
search sentence-by-sentence, and initialize the beam of the
next sentence with the list of previous k best hypotheses. as
a common denoising method in deep learning (sutskever,
vinyals, and lee 2014; zaremba, sutskever, and vinyals
2014; vinyals et al. 2015), we perform id136 over an
ensemble of randomly initialized models.2
experimental setup

dataset we train and evaluate our model using the pub-
licly available sail route instruction dataset collected
by macmahon, stankiewicz, and kuipers (2006). we use
the raw data in its original form (e.g., we do not correct any
spelling errors). the dataset contains 706 non-trivial naviga-
tional instruction paragraphs, produced by six instructors for
126 unique start and end position pairs spread evenly across
three virtual worlds. these instructions are segmented into
individual sentences and paired with an action sequence.
see corpus statistics in chen and mooney (2011).

world state the world state yt encodes the local, observ-
able world at time t. we make the standard assumption that
the agent is able to observe all elements of the environment
that are within line-of-sight. in the speci   c domain that we
consider for evaluation, these elements include the    oor pat-
terns, wall paintings, and objects that are not occluded by
walls. we represent the world state as a concatenation of
a simple bag-of-words vector for each direction (forward,
left, and right). the choice of bag-of-words representation
avoids manual domain-speci   c feature-engineering and the
combinatoriality of modeling exact world con   gurations.

id74 we evaluate our end-to-end model on
both the single-sentence and multi-sentence versions of the
corpus. for the single-sentence task, following previous
work, the strict evaluation metric deems a trial to be success-
ful iff the    nal position and orientation exactly match those
of the original demonstration. for multi-sentence, we disre-
gard the    nal orientation, as previous work does. however,

this setting is still more challenging than single-sentence due
to cascading errors over individual sentences.

training details we follow the same procedure as chen
and mooney (2011), training with the segmented data and
testing on both single- and multi-sentence versions. we train
our models using three-fold cross-validation based on the
three maps.
in each fold, we retain one map as test and
partition the two-map training data into training (90%) and
validation (10%) sets, the latter of which is used to tune
hyperparameters.3 we repeat this process for each of the
three folds and report (size-weighted) average test results
over these folds. we later refer to this training procedure as
   vdev.    additionally, some previous methods (p.c.) adopted
a slightly different training strategy whereby each fold trains
on two maps and uses the test map to decide on the stopping
iteration. in order to compare against these methods, we also
train a separate version of our model in this way, which we
refer to as    vtest.   

for optimization, we found adam (kingma and ba 2015)
to be very effective for training with this dataset. the train-
ing usually converges within 50 epochs. we performed early
stopping based on the validation task metric. similar to pre-
vious work (xu et al. 2015), we found that the validation
log-likelihood is not well correlated with the task metric.

results and analysis

in this section, we compare the overall performance of our
model on the single- and multi-sentence benchmarks against
previous work. we then present an analysis of our model
through a series of ablation studies.

table 1: overall accuracy (state-of-the-art in bold)

method
chen and mooney (2011)
chen (2012)
kim and mooney (2012)
kim and mooney (2013)
artzi and zettlemoyer (2013)
artzi, das, and petrov (2014)
andreas and klein (2015)
our model (vdev)
our model (vtest)

single-sent multi-sent

54.40
57.28
57.22
62.81
65.28
64.36
59.60
69.98
71.05

16.18
19.18
20.17
26.57
31.93
35.44

   

26.07
30.34

primary result we    rst investigate the ability to navigate
to the intended destination for a given natural language in-
struction. figure 1 illustrates an output example for which
our model successfully executes the input natural language
instruction. table 1 reports the overall accuracy of our
model for both the single- and multi-sentence settings. we
report two statistics with our model (vdev and vtest) in or-
der to directly compare with existing work.4

1we use a beam width of 10 to be consistent with existing work.
2at each time step t, we generate actions using the avg. of the
posterior likelihoods of 10 ensemble models, as in previous work.

3we only tuned the no. of hidden units and the drop-out
rate (srivastava et al. 2014; zaremba, sutskever, and vinyals 2014).

4subsequent evaluations are on vdev unless otherwise noted.

5

table 2: model components ablations

full model high-level aligner no aligner unidirectional no encoder

single-sentence
multi-sentence

69.98
26.07

68.09
24.79

68.05
25.04

67.44
24.50

61.63
16.67

as we can see from table 1, we surpass state-of-the-art re-
sults on the single-sentence route instruction task (for both
vdev and vtest settings), despite using no linguistic knowl-
edge or resources. our multi-sentence accuracy, which is
working with a really small amount of training data (a few
hundred paragraph pairs), is competitive with state-of-the-
art and outperforms several previous methods that use addi-
tional, specialized resources in the form of semantic parsers,
logical-form lexicons, and re-rankers.5 we note that our
model yields good results using only greedy search (beam
width of one). for vdev, we achieve 68.05 on single-
sentence and 23.93 on multi-sentence, while for vtest, we
get 70.56 on single-sentence and 27.91 on multi-sentence.

distance evaluation our evaluation required that the ac-
tion sequence reach the exact desired destination. it is of
interest to consider how close the model gets to the destina-
tion when it is not reached. table 3 displays the fraction of
test results that reach within d nodes of the destination. of-
ten, the method produces action sequences that reach points
close to the desired destination.

table 3: accuracy as a function of distance from destination

distance (d)
single-sentence
multi-sentence

0

1

2

3

71.73
26.07

86.62
42.88

92.86
59.54

95.74
72.08

multi-level aligner ablation unlike most existing meth-
ods that align based only on the hidden annotations hj, we
adopt a different approach by also including the original in-
put word xj (eqn. 5). as shown in table 2, the multi-level
representation (   full model   ) signi   cantly improves perfor-
mance over a standard aligner (   high-level aligner   ). fig-
ure 4 visualizes the alignment of words to actions in the map
environment for several sentences from the instruction para-
graph depicted in figure 1.

aligner ablation our model utilizes alignment in the de-
coder as a means of focusing on word    regions    that are
more salient to the current world state. we analyze the effect
of the learned alignment by training an alternative model

5note that with no ensemble, we are still state-of-the-art on
single-sentence and better than all comparable approaches on
multi-sentence: artzi et al. (2013, 2014) use extra annotations with
a logical-form lexicon and kim and mooney (2013) use discrim-
inative reranking, techniques that are orthogonal to our approach
and should likely improve our results as well.

in which the context vector zt is an unweighted average
(eqn. 5). as shown in the table 2, learning the alignment
does improve the accuracy of the resulting action sequence.
note that the    no aligner    model still maintains all connec-
tions between the instruction and actions, but is just using
non-learned, uniform weights.

bidirectionality ablation we train an alternative model
that uses only a unidirectional (forward) encoder. as shown
in table 2, the bidirectional encoder (   full model   ) signi   -
cantly improves accuracy.

encoder ablation we further evaluate the bene   t of en-
coding the input sentence and consider an alternative model
that directly feeds word vectors as randomly initialized em-
beddings into the decoder and relies on the alignment model
to choose the salient words. table 2 presents the results
with and without the encoder and demonstrates that there
is a substantial gain in encoding the input sentence into its
context representation. we believe the difference is due to
the id56   s ability to incorporate sentence-level information
into the word   s representation as it processes the sentence
sequentially (in both directions). this advantage helps re-
solve ambiguities, such as    turn right before . . .     versus
   turn right after . . .    .

conclusion

we presented an end-to-end, sequence-to-sequence ap-
proach to mapping natural language navigational instruc-
tions to action plans given the local, observable world
state, using a bidirectional lstm-id56 model with a multi-
level aligner. we evaluated our model on a benchmark
route instruction dataset and demonstrates that it achieves a
new state-of-the-art on single-sentence execution and yields
competitive results on the more challenging multi-sentence
domain, despite working with very small training datasets
and using no specialized linguistic knowledge or resources.
we further performed a number of ablation studies to eluci-
date the contributions of our primary model components.

acknowledgments

we thank yoav artzi, david chen, oriol vinyals, and
kelvin xu for their helpful comments. this work was sup-
ported in part by the robotics consortium of the u.s. army
research laboratory under the collaborative technology
alliance program, cooperative agreement w911nf-10-2-
0016, and by an ibm faculty award.

6

figure 4: visualization of the alignment between words to actions in a map for a multi-sentence instruction.

references

andreas, j., and klein, d. 2015. alignment-based composi-
tional semantics for instruction following. in proceedings of
the conference on empirical methods in natural language
processing (emnlp).
artzi, y., and zettlemoyer, l. 2013. weakly supervised
learning of semantic parsers for mapping instructions to ac-
tions. transactions of the association for computational
linguistics 1:49   62.
artzi, y.; das, d.; and petrov, s. 2014. learning com-
in proceedings of
pact lexicons for id35 id29.
the conference on empirical methods in natural language
processing (emnlp).
ba, j.; mnih, v.; and kavukcuoglu, k. 2014. multiple object
recognition with visual attention. arxiv:1412.7755.
bahdanau, d.; cho, k.; and bengio, y. 2014. neural ma-
chine translation by jointly learning to align and translate.
arxiv:1409.0473.
chen, d. l., and mooney, r. j. 2011. learning to interpret
natural language navigation instructions from observations.
in proceedings of the national conference on arti   cial in-
telligence (aaai).
chen, x., and zitnick, c. l. 2015. mind   s eye: a recurrent
visual representation for image id134. in pro-
ceedings of the ieee conference on id161 and
pattern recognition (cvpr).
chen, d. l. 2012. fast online lexicon learning for grounded
id146. in proceedings of the annual meeting
of the association for computational linguistics (acl).

cho, k.; van merrienboer, b.; gulcehre, c.; bahdanau, d.;
bougares, f.; schwenk, h.; and bengio, y. 2014. learn-
ing phrase representations using id56 encoder-decoder for
id151. in proceedings of the confer-
ence on empirical methods in natural language processing
(emnlp).
donahue, j.; hendricks, l. a.; guadarrama, s.; rohrbach,
m.; venugopalan, s.; saenko, k.; and darrell, t. 2014.
long-term recurrent convolutional networks for visual
recognition and description. arxiv:1411.4389.
2013.
graves, a.; abdel-rahman, m.; and hinton, g.
id103 with deep recurrent neural networks. in
icassp.
harnad, s. 1990. the symbol grounding problem. physica
d 42:335   346.
hemachandra, s.; duvallet, f.; howard, t. m.; roy, n.;
stentz, a.; and walter, m. r. 2015. learning models for
following natural language directions in unknown environ-
ments. in proceedings of the ieee international conference
on robotics and automation (icra).
hochreiter, s., and schmidhuber, j. 1997. long short-term
memory. neural computation 9(8).
karpathy, a., and fei-fei, l. 2015. deep visual-semantic
alignments for generating image descriptions. in proceed-
ings of the ieee conference on id161 and pat-
tern recognition (cvpr).
kim, j., and mooney, r. j. 2012. unsupervised pid18 in-
duction for grounded language learning with highly ambigu-
ous supervision. in proceedings of the conference on em-

7

bobjectsbarstoolcchaireeaselhhatrackllampssofawall paintingstowerbutterflyfishfloor patternsbrickblueconcreteflowergrassgravelwoodyellowlehcssecbhlgostopgogoforwardtwosegmentstothetheendofhallstopgogointersectionforwardonesegmenttothecontainingthelampgointersectionforwardonesegmenttothewiththewooden-flooredhallgostopgoforwardtothetheendofhallgostopgogogostopgogogogogostopstopgostoppirical methods in natural language processing (emnlp),
433   444.
kim, j., and mooney, r. j. 2013. adapting discriminative
in proceedings
reranking to grounded language learning.
of the annual meeting of the association for computational
linguistics (acl).
kingma, d., and ba, j. 2015. adam: a method for stochas-
tic optimization. in proceedings of the international con-
ference on learning representations.
kiros, r.; salakhutdinov, r.; and zemel, r. s. 2014. uni-
fying visual-semantic embeddings with multimodal neural
language models. arxiv:1411.2539.
kollar, t.; tellex, s.; roy, d.; and roy, n. 2010. toward un-
derstanding natural language directions. in proceedings of
the acm/ieee international conference on human-robot
interaction (hri).
kong, c.; lin, d.; bansal, m.; urtasun, r.; and fidler, s.
2014. what are you talking about? text-to-image corefer-
ence. in proceedings of cvpr.
macmahon, m.; stankiewicz, b.; and kuipers, b. 2006.
walk the talk: connecting language, knowledge, and action
in route instructions. in proceedings of the national con-
ference on arti   cial intelligence (aaai).
mao, j.; xu, w.; yang, y.; wang, j.; and yuille, a. 2014.
deep captioning with multimodal recurrent neural networks
(m-id56). arxiv:1412.6632.
matuszek, c.; fox, d.; and koscher, k. 2010. following
directions using id151. in proceed-
ings of the acm/ieee international conference on human-
robot interaction (hri).
mnih, v.; hees, n.; graves, a.; and kavukcuoglu, k. 2014.
recurrent models of visual attention. in advances in neural
information processing systems (nips).
mooney, r. j. 2008. learning to connect language and
perception. in proceedings of the national conference on
arti   cial intelligence (aaai).
pascanu, r.; gulcehre, c.; cho, k.; and bengio, y.

2014. how to construct deep recurrent neural networks.
arxiv:1312.6026.
rush, a. m.; chopra, s.; and weston, j. 2015. a neu-
ral attention model for abstractive sentence summarization.
in proceedings of the conference on empirical methods in
natural language processing (emnlp).
srivastava, n.; hinton, g.; krizhevsky, a.; sutskever, i.;
and salakhutdinov, r. 2014. dropout: a simple way to pre-
vent neural networks from over   tting. j. machine learning
research 15(1):1929   1958.
sutskever, i.; vinyals, o.; and lee, q. v. 2014. sequence
to sequence learning with neural networks. in advances in
neural information processing systems (nips).
tellex, s.; kollar, t.; dickerson, s.; walter, m. r.; banerjee,
a. g.; teller, s.; and roy, n. 2011. understanding natural
language commands for robotic navigation and mobile ma-
in proceedings of the national conference on
nipulation.
arti   cial intelligence (aaai).
vinyals, o.; toshev, a.; bengio, s.; and erhan, d. 2015.
show and tell: a neural image caption generator. in pro-
ceedings of the ieee conference on id161 and
pattern recognition (cvpr).
wen, t.-h.; ga  si  c, m.; mrk  si  c, n.; su, p.-h.; vandyke, d.;
and young, s. 2015. semantically conditioned lstm-based
id86 for spoken dialogue systems.
in proceedings of the conference on empirical methods in
natural language processing (emnlp).
winograd, t. 1970. proceedures as a representation for
data in a computer program for understanding natural
language. ph.d. dissertation, mit.
xu, k.; ba, j.; kiros, r.; cho, k.; courville, a.; salakhutdi-
nov, r.; zemel, r.; and bengio, y. 2015. show, attend and
tell: neural image id134 with visual attention.
in proceedings of the international conference on machine
learning (icml).
zaremba, w.; sutskever, i.; and vinyals, o. 2014. recurrent
neural network id173. arxiv:1409.2329.

8

