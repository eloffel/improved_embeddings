overcoming the curse of sentence length for neural machine

translation using automatic segmentation

jean pouget-abadie   

ecole polytechnique, france

dzmitry bahdanau    

jacobs university, germany

bart van merri  enboer

kyunghyun cho

universit  e de montr  eal, canada

yoshua bengio

universit  e de montr  eal, canada

cifar senior fellow

4
1
0
2

 
t
c
o
7

 

 
 
]
l
c
.
s
c
[
 
 

2
v
7
5
2
1

.

9
0
4
1
:
v
i
x
r
a

abstract

the authors of (cho et al., 2014a) have
shown that the recently introduced neural
network translation systems suffer from
a signi   cant drop in translation quality
when translating long sentences, unlike
existing phrase-based translation systems.
in this paper, we propose a way to ad-
dress this issue by automatically segment-
ing an input sentence into phrases that can
be easily translated by the neural network
translation model. once each segment has
been independently translated by the neu-
ral machine translation model, the trans-
lated clauses are concatenated to form a
   nal translation. empirical results show
a signi   cant improvement in translation
quality for long sentences.

1

introduction

up to now, most research efforts in statistical ma-
chine translation (smt) research have relied on
the use of a phrase-based system as suggested
in (koehn et al., 2003). recently, however, an
entirely new, neural network based approach has
been proposed by several research groups (kalch-
brenner and blunsom, 2013; sutskever et al.,
2014; cho et al., 2014b), showing promising re-
sults, both as a standalone system or as an addi-
tional component in the existing phrase-based sys-
tem. in this neural network based approach, an en-
coder    encodes    a variable-length input sentence
into a    xed-length vector and a decoder    decodes   
a variable-length target sentence from the    xed-
length encoded vector.

it has been observed in (sutskever et al., 2014),
(kalchbrenner and blunsom, 2013) and (cho et
al., 2014a) that
this neural network approach
    research done while these authors were visiting uni-

versit  e de montr  eal

works well with short sentences (e.g., (cid:47) 20
words), but has dif   culty with long sentences (e.g.,
(cid:39) 20 words), and particularly with sentences that
are longer than those used for training. training
on long sentences is dif   cult because few available
training corpora include suf   ciently many long
sentences, and because the computational over-
head of each update iteration in training is linearly
correlated with the length of training sentences.
additionally, by the nature of encoding a variable-
length sentence into a    xed-size vector representa-
tion, the neural network may fail to encode all the
important details.

in this paper, hence, we propose to translate sen-
tences piece-wise. we segment an input sentence
into a number of short clauses that can be con   -
dently translated by the model. we show empiri-
cally that this approach improves translation qual-
ity of long sentences, compared to using a neural
network to translate a whole sentence without seg-
mentation.

2 id56 encoder   decoder for translation

the id56 encoder   decoder (id56enc) model is
a recent implementation of the encoder   decoder
approach, proposed independently in (cho et al.,
2014b) and in (sutskever et al., 2014).
it con-
sists of two id56s, acting respectively as encoder
and decoder. each id56 maintains a set of hid-
den units that makes an    update    decision for each
symbol in an input sequence. this decision de-
pends on the input symbol and the previous hid-
den state. the id56enc in (cho et al., 2014b) uses
a special hidden unit that adaptively forgets or re-
members the previous hidden state such that the
(cid:104)t(cid:105)
activation of a hidden unit h
at time t is com-
j
puted by

(cid:104)t   1(cid:105)
(cid:104)t(cid:105)
h
j = zjh
j

(cid:104)t(cid:105)
+ (1     zj)  h
j ,

where

(cid:104)t(cid:105)
j =f

  h

zj =  

rj =  

(cid:16)
(cid:16)
(cid:16)

[wx]j + rj

(cid:2)uh(cid:104)t   1(cid:105)(cid:3)(cid:17)
(cid:17)
[wzx]j +(cid:2)uzh(cid:104)t   1(cid:105)(cid:3)
(cid:17)
[wrx]j +(cid:2)urh(cid:104)t   1(cid:105)(cid:3)

j

j

,

,

.

zj and rj are respectively the update and reset
gates.

additionally, the id56 in the decoder computes
at each step the id155 of a target
word:

p(ft,j = 1 | ft   1, . . . , f1, c) =

exp(cid:0)wjh(cid:104)t(cid:105)(cid:1)
j(cid:48)=1 exp(cid:0)wj(cid:48)h(cid:104)t(cid:105)(cid:1) ,
(cid:80)k

(1)

where ft,j is the indicator variable for the j-th
word in the target vocabulary at time t and only
a single indicator variable is on (= 1) each time.
c is the context vector, the representation of the
input sentence as encoded by the encoder.

although the model in (cho et al., 2014b) was
originally trained on phrase pairs, it is straight-
forward to train the same model with a bilin-
gual, parallel corpus consisting of sentence pairs
as has been done in (sutskever et al., 2014).
in
the remainder of this paper, we use the id56enc
trained on english   french sentence pairs (cho et
al., 2014a).

3 automatic segmentation and

translation

one hypothesis explaining the dif   culty encoun-
tered by the id56enc model when translating long
sentences is that a plain,    xed-length vector lacks
the capacity to encode a long sentence. when en-
coding a long input sentence, the encoder may lose
the track of all the subtleties in the sentence. con-
sequently, the decoder has dif   cult time recover-
ing the correct translation from the encoded repre-
sentation. one solution would be to build a larger
model with a larger representation vector to in-
crease the capacity of the model at the price of
higher computational cost.

in this section, however, we propose to segment
an input sentence such that each segmented clause
can be easily translated by the id56 encoder   
decoder.
in other words, we wish to    nd a
segmentation that maximizes the total con   dence
score which is a sum of the con   dence scores of

the phrases in the segmentation. once the con   -
dence score is de   ned, the problem of    nding the
best segmentation can be formulated as an integer
programming problem.

let e = (e1,       , en) be a source sentence com-
posed of words ek. we denote a phrase, which is a
subsequence of e, with eij = (ei,       , ej).
we use the id56 encoder   decoder to measure
how con   dently we can translate a subsequence
eij by considering the log-id203 log p(f k |
eij) of a candidate translation f k generated by the
model. in addition to the log-id203, we also
use the log-id203 log p(eij | f k) from a re-
verse id56 encoder   decoder (translating from a
target language to source language). with these
two probabilities, we de   ne the con   dence score
of a phrase pair (eij, f k) as:

c(eij, f k) =

log p(f k | eij) + log q(eij | f k)

2|log(j     i + 1)|

,

(2)

where the denominator penalizes a short segment
whose id203 is known to be overestimated by
an id56 (graves, 2013).

the con   dence score of a source phrase only is

then de   ned as

cij = max

k

c(eij, fk).

(3)

we use an approximate id125 to search for
the candidate translations f k of eij, that maximize
log-likelihood log p(f k|eij) (graves et al., 2013;
boulanger-lewandowski et al., 2013).
let xij be an indicator variable equal to 1 if we
include a phrase eij in the segmentation, and oth-
erwise, 0. we can rewrite the segmentation prob-
lem as the optimization of the following objective
function:

max

x

subject to

nk = (cid:80)

xij1i   k   j

(4)

cijxij = x    c

i   j
   k, nk = 1
is the number of source

i,j

phrases chosen in the segmentation containing
word ek.

the constraint in eq. (4) states that for each
word ek in the sentence one and only one of the
source phrases contains this word, (eij)i   k   j, is
included in the segmentation. the constraint ma-
trix is totally unimodular making this integer pro-
gramming problem solvable in polynomial time.

(cid:88)

let sk

j be the    rst index of the k-th segment
counting from the last phrase of the optimal seg-
mentation of subsequence e1j (sj := s1
j ), and sj
be the corresponding score of this segmentation
(s0 := 0). then, the following relations hold:

sj = max
1   i   j

(cij + si   1),

sj = arg max

1   i   j

(cij + si   1),

   j     1
   j     1

(5)

(6)

with eq. (5) we can evaluate sj incrementally.
with the evaluated sj   s, we can compute sj as
well (eq. (6)). by the de   nition of sk
j we    nd the
optimal segmentation by decomposing e1n into
n,n, where k is the
e
n,sk   1
sk
index of the    rst one in the sequence sk
n. this
approach described above requires quadratic time
with respect to sentence length.

,       , es2

n   1, es1

n    1

n,s1

issues and discussion

3.1
the proposed segmentation approach does not
avoid the problem of reordering clauses. unless
the source and target languages follow roughly the
same order, such as in english to french transla-
tions, a simple concatenation of translated clauses
will not necessarily be grammatically correct.

despite the lack of long-distance reordering1 in
the current approach, we    nd nonetheless signi   -
cant gains in the translation performance of neural
machine translation. a mechanism to reorder the
obtained clause translations is, however, an impor-
tant future research question.

another issue at the heart of any purely neu-
ral machine translation is the limited model vo-
cabulary size for both source and target languages.
as shown in (cho et al., 2014a), translation qual-
ity drops considerably with just a few unknown
words present in the input sentence. interestingly
enough, the proposed segmentation approach ap-
pears to be more robust to the presence of un-
known words (see sec. 5). one intuition is that the
segmentation leads to multiple short clauses with
less unknown words, which leads to more stable
translation of each clause by the neural translation
model.

finally, the proposed approach is computation-
ally expensive as it requires scoring all the sub-
phrases of an input sentence. however, the scoring
process can be easily sped up by scoring phrases

1 note that, inside each clause, the words are reordered
automatically when the clause is translated by the id56
encoder   decoder.

in parallel, since each phrase can be scored inde-
pendently.

4 experiment settings
4.1 dataset
we evaluate the proposed approach on the task
of english-to-french translation. we use a bilin-
gual, parallel corpus of 348m words selected
by the method of (axelrod et al., 2011) from
a combination of europarl (61m), news com-
mentary (5.5m), un (421m) and two crawled
corpora of 90m and 780m words respectively.2
the performance of our models was tested
on news-test2012, news-test2013, and
news-test2014. when comparing with the
phrase-based smt system moses (koehn et al.,
2007), the    rst two were used as a development set
for tuning moses while news-test2014 was
used as our test set.

to train the neural network models, we use only
the sentence pairs in the parallel corpus, where
both english and french sentences are at most 30
words long. furthermore, we limit our vocabu-
lary size to the 30,000 most frequent words for
both english and french. all other words are con-
sidered unknown and mapped to a special token
([unk]).

in both neural network training and automatic
segmentation, we do not incorporate any domain-
speci   c knowledge, except when tokenizing the
original text data.

4.2 models and approaches
we compare the proposed segmentation-based
translation scheme against the same neural net-
work model
translations without segmentation.
the id4 is done by an id56
encoder   decoder (id56enc) (cho et al., 2014b)
trained to maximize the id155
of a french translation given an english sen-
tence. once the id56enc is trained, an approxi-
mate beam-search is used to    nd possible transla-
tions with high likelihood.3

this id56enc is used for

the proposed
segmentation-based approach together with an-
other id56enc trained to translate from french to
2 the datasets and trained moses models can be down-
loaded from http://www-lium.univ-lemans.fr/
  schwenk/cslm_joint_paper/ and the website of
acl 2014 ninth workshop on statistical machine transla-
tion (wmt 14).

3 in all experiments, the beam width is 10.

english. the two id56enc   s are used in the pro-
posed segmentation algorithm to compute the con-
   dence score of each phrase (see eqs. (2)   (3)).

we also compare with the translations of a con-
ventional phrase-based machine translation sys-
tem, which we expect to be more robust when
translating long sentences.

5 results and analysis
5.1 validity of the automatic segmentation
we validate the proposed segmentation algorithm
described in sec. 3 by comparing against two
baseline segmentation approaches. the    rst one
randomly segments an input sentence such that the
distribution of the lengths of random segments has
its mean and variance identical to those of the seg-
ments produced by our algorithm. the second ap-
proach follows the proposed algorithm, however,
using a uniform random con   dence score.

model
no segmentation
random segmentation
random con   dence score
proposed segmentation

test set
13.15
16.60
16.76
20.86

1:

computed

two control

id7 score

on
table
news-test2014 for
experi-
ments. random segmentation refers to randomly
segmenting a sentence so that
the mean and
variance of the segment lengths corresponded to
the ones our best segmentation method. random
con   dence score refers to segmenting a sentence
with randomly generated con   dence score for
each segment.

from table 1 we can clearly see that the pro-
posed segmentation algorithm results in signi   -
cantly better performance. one interesting phe-
nomenon is that any random segmentation was
better than the direct translation without any seg-
mentation. this indirectly agrees well with the
previous    nding in (cho et al., 2014a) that the
id4 suffers from long sen-
tences.

importance of using an inverse model

5.2
the proposed con   dence score averages the scores
of a translation model p(f | e) and an inverse
translation model p(e | f ) and penalizes for short
phrases. however, it is possible to use alternate

figure 2: id7 score loss vs. maximum number
of unknown words in source and target sentence
when translating with the id56enc model with and
without segmentation.

de   nitions of con   dence score. for instance, one
may use only the    direct    translation model or
varying penalties for phrase lengths.

in this section, we test three different con   dence

score:
p(f | e) using a single translation model
p(f | e) + p(e | f ) using both direct and reverse
translation models without the short phrase
penalty

p(f | e) + p(e | f ) (p) using both direct and re-
verse translation models together with the
short phrase penalty

the results in table 2 clearly show the impor-
tance of using both translation and inverse trans-
lation models. furthermore, we were able to get
the best performance by incorporating the short
phrase penalty (the denominator in eq. (2)). from
here on, thus, we only use the original formula-
tion of the con   dence score which uses the both
models and the penalty.

5.3 quantitative and qualitative analysis
as expected, translation with the proposed ap-
proach helps signi   cantly with translating long
sentences (see fig. 1). we observe that trans-
lation performance does not drop for sentences
of lengths greater than those used to train the
id56enc (    30 words).
similarly, in fig. 2 we observe that translation
quality of the proposed approach is more robust

0246810max.numberofunknownwords   9   8   7   6   5   4   3   2   10id7scoredecreasewithsegm.withoutsegm.(a) id56enc without

segmentation

(b) id56enc with segmentation

(c) moses

figure 1: the id7 scores achieved by (a) the id56enc without segmentation, (b) the id56enc
with the penalized reverse con   dence score, and (c) the phrase-based translation system moses on a
newstest12-14.

model
id56enc
p(f | e)

moses
id56enc
p(f | e)

p(f | e) + p(e | f )
p(f | e) + p(e | f ) (p)

p(f | e) + p(e | f )
p(f | e) + p(e | f ) (p)

moses

dev
13.15
12.49
18.82
19.39
30.64
21.01
20.94
23.05
23.93
32.77

test
13.92
13.57
20.10
20.86
33.30
23.45
22.62
24.63
26.46
35.63

l
l

a

k
n
u
o
n

table 2: id7 scores computed on the develop-
ment and test sets. see the text for the description
of each approach. moses refers to the scores by
the conventional phrase-based translation system.
the top    ve rows consider all sentences of each
data set, whilst the bottom    ve rows includes only
sentences with no unknown words

to the presence of unknown words. we suspect
that the existence of many unknown words make
it harder for the id56enc to extract the meaning of
the sentence clearly, while this is avoided with the
proposed segmentation approach as it effectively
allows the id56enc to deal with a less number of
unknown words.

in table 3, we show the translations of ran-
domly selected long sentences (40 or more words).
segmentation improves overall translation quality,
agreeing well with our quantitative result. how-
ever, we can also observe a decrease in transla-
tion quality when an input sentence is not seg-
mented into well-formed sentential clauses. addi-
tionally, the concatenation of independently trans-
lated segments sometimes negatively impacts    u-

ency, punctuation, and capitalization by the rn-
nenc model. table 4 shows one such example.

6 discussion and conclusion

in this paper we propose an automatic segmen-
tation solution to the    curse of sentence length   
in id4. by choosing an
appropriate con   dence score based on bidirec-
tional translation models, we observed signi   cant
improvement in translation quality for long sen-
tences.
our

the proposed
segmentation-based translation is more robust to
the presence of unknown words. however, since
each segment is translated in isolation, a segmen-
tation of an input sentence may negatively impact
translation quality, especially the    uency of the
translated sentence, the placement of punctuation
marks and the capitalization of words.

investigation shows that

an important research direction in the future is
to investigate how to improve the quality of the
translation obtained by concatenating translated
segments.

acknowledgments

the authors would like to acknowledge the sup-
port of the following agencies for research funding
and computing support: nserc, calcul qu  ebec,
compute canada,
the canada research chairs
and cifar.

references
[axelrod et al.2011] amittai axelrod, xiaodong he,
and jianfeng gao. 2011. id20 via
pseudo in-domain data selection. in proceedings of

01020304050607080sentencelength05101520id7scoresourcetextreferencetextboth01020304050607080sentencelength0510152025id7scoresourcetextreferencetextboth01020304050607080sentencelength0510152025303540id7scoresourcetextreferencetextbothwithout
segmenta-
tion

source

reference

with
segmentation
without
segmentation

source

source

between the early 1970s , when the boeing 747 jumbo de   ned modern long-haul travel , and
the turn of the century , the weight of the average american 40- to 49-year-old male increased
by 10 per cent , according to u.s. health department data .

reference

segmentation [[ between the early 1970s , when the boeing 747 jumbo de   ned modern long-haul travel ,]
[ and the turn of the century , the weight of the average american 40- to 49-year-old male] [
increased by 10 per cent , according to u.s. health department data .]]
entre le d  ebut des ann  ees 1970 , lorsque le jumbo 747 de boeing a d  e   ni le voyage long-courrier
moderne , et le tournant du si`ecle , le poids de l    am  ericain moyen de 40 `a 49 ans a augment  e
de 10 % , selon les donn  ees du d  epartement am  ericain de la sant  e .
entre les ann  ees 70 , lorsque le boeing boeing a d  e   ni le transport de voyageurs modernes ; et
la    n du si`ecle , le poids de la moyenne am  ericaine moyenne `a l      egard des hommes a augment  e
de 10 % , conform  ement aux donn  ees fournies par le u.s. department of health affairs .
entre les ann  ees 1970 , lorsque les avions de service boeing ont d  epass  e le prix du travail , le
taux moyen   etait de 40 % .

with
segmentation

during his arrest ditta picked up his wallet and tried to remove several credit cards but they
were all seized and a hair sample was taken fom him.

segmentation [[during his arrest ditta] [picked up his wallet and tried to remove several credit cards but they

were all seized and] [a hair sample was taken from him.]]
au cours de son arrestation , ditta a ramass  e son portefeuille et a tent  e de retirer plusieurs cartes
de cr  edit , mais elles ont toutes   et  e saisies et on lui a pr  elev  e un   echantillon de cheveux .
pendant son arrestation j    ai utilis  e son portefeuille et a essay  e de retirer plusieurs cartes de
cr  edit mais toutes les pi`eces ont   et  e saisies et un   echantillon de cheveux a   et  e enlev  e.
lors de son arrestation il a tent  e de r  ecup  erer plusieurs cartes de cr  edit mais il a   et  e saisi de tous
les coups et des blessures.

   we can now move forwards and focus on the future and on the 90 % of assets that make up a
really good bank, and on building a great bank for our clients and the united kingdom,    new
director general, ross mcewan, said to the press .

reference

segmentation [[   we can now move forwards and focus on the future] [and] [on the 90 % of assets that make
up a really good bank, and on building] [a great bank for our clients and the united kingdom,   ]
[new director general, ross mcewan, said to the press.]]
   nous pouvons maintenant aller de l   avant , nous pr  eoccuper de l   avenir et des 90 % des actifs
qui constituent une banque vraiment bonne et construire une grande banque pour la client`ele et
pour le royaume uni   , a dit le nouveau directeur g  en  eral ross mcewan `a la presse .
   nous pouvons maintenant passer `a l   avenir et se concentrer sur l avenir ou sur les 90 % d actifs
qui constituent une bonne banque et sur la construction une grande banque de nos clients et du
royaume-uni    le nouveau directeur g  en  eral ross ross a dit que la presse.
   nous pouvons maintenant passer et   etudier les 90 % et mettre en place une banque importante
pour la nouvelle banque et le directeur g  en  eral    a soulign  e le journaliste .

without
segmentation

with
segmentation

source

there are several beautiful    ashes - the creation of images has always been one of chouinard   s
strong points - like the hair that is ruf   ed or the black fabric that extends the lines.

segmentation [[there are several beautiful    ashes - the creation of images has always been one of chouinard   s

strong points -] [like the hair that is ruf   ed or the black fabric that extends the lines.]]
il y a quelques beaux    ashs - la cr  eation d   images a toujours   et  e une force chez chouinard -
comme ces ch eveux qui s     ebouriffent ou ces tissus noirs qui allongent les lignes .
il existe plusieurs belles images - la cr  eation d images a toujours   et  e l un de ses points forts .
comme les cheveux comme le vernis ou le tissu noir qui   etend les lignes.
il existe plusieurs points forts : la cr  eation d images est toujours l un des points forts .

without specifying the illness she was suffering from, the star performer of    respect    con   rmed
to the media on 16 october that the side effects of a treatment she was receiving were    dif   cult   
to deal with.

reference

segmentation [[without specifying the illness she was suffering from, the star performer of    respect   ] [con-
   rmed to the media on 16 october that the side effects of a treatment she was receiving were]
[   dif   cult    to deal with.]]
sans pr  eciser la maladie dont elle souffrait , la c  el`ebre interpr`ete de respect avait af   rm  e aux
m  edias le 16 octobre que les effets secondaires d   un traitement qu   elle recevait   etaient    dif   -
ciles   .
sans pr  eciser la maladie qu   elle souffrait la star de l         uvre    de    respect   . il a   et  e con   rm  e
aux m  edias le 16 octobre que les effets secondaires d   un traitement ont   et  e rec  us.    dif   cile    de
traiter .
sans la pr  ecision de la maladie elle a eu l   impression de    marquer le 16 avril    les effets d   un tel
   traitement   .

with
segmentation

without
segmentation

reference

with
segmentation
without
segmentation

source

table 3: sample translations with the id56enc model taken from the test set along with the source
sentences and the reference translations.

source

segmentation

reference

with
segmentation
without
segmentation

he nevertheless praised the government for responding to his request for urgent assis-
tance which he    rst raised with the prime minister at the beginning of may .
[he nevertheless praised the government for responding to his request for urgent assis-
tance which he    rst raised ] [with the prime minister at the beginning of may . ]
il a n  eanmoins f  elicit  e le gouvernement pour avoir r  epondu `a la demande d    aide urgente
qu   il a pr  esent  ee au premier ministre d  ebut mai .
il a n  eanmoins f  elicit  e le gouvernement de r  epondre `a sa demande d    aide urgente qu   il
a soulev  ee . avec le premier ministre d  ebut mai .
il a n  eanmoins f  elicit  e le gouvernement de r  epondre `a sa demande d    aide urgente qu   il
a adress  ee au premier ministre d  ebut mai .

table 4: an example where an incorrect segmentation negatively impacts    uency and punctuation.

ondrej bojar, alexandra constantin, and evan
herbst. 2007. annual meeting of the association
for computational linguistics (acl). prague, czech
republic. demonstration session.

[sutskever et al.2014] ilya sutskever, oriol vinyals,
and quoc le. 2014. sequence to sequence learning
with neural networks. in advances in neural infor-
mation processing systems (nips 2014), december.

the acl conference on empirical methods in natu-
ral language processing (emnlp), pages 355   362.
association for computational linguistics.

[boulanger-lewandowski et al.2013] nicolas

boulanger-lewandowski, yoshua bengio,
and
pascal vincent. 2013. audio chord recognition
with recurrent neural networks. in ismir.

[cho et al.2014a] kyunghyun

bart
van
cho,
merri  enboer, dzmitry bahdanau,
and yoshua
bengio. 2014a. on the properties of neural ma-
chine translation: encoder   decoder approaches.
in eighth workshop on syntax, semantics and
structure in statistical translation, october.

[cho et al.2014b] kyunghyun cho, bart van merrien-
boer, caglar gulcehre, fethi bougares, holger
schwenk, and yoshua bengio. 2014b. learning
phrase representations using id56 encoder-decoder
in proceedings
for id151.
of the empiricial methods in natural language pro-
cessing (emnlp 2014), october. to appear.

[graves et al.2013] a. graves, a. mohamed,

and
g. hinton. 2013. id103 with deep re-
current neural networks. icassp.

[graves2013] a. graves. 2013. generating sequences
with recurrent neural networks. arxiv:1308.0850
[cs.ne], august.

[kalchbrenner and blunsom2013] nal kalchbrenner
and phil blunsom. 2013. two recurrent continuous
in proceedings of the acl
translation models.
conference on empirical methods
in natural
language processing (emnlp), pages 1700   1709.
association for computational linguistics.

[koehn et al.2003] philipp koehn, franz josef och,
and daniel marcu. 2003. statistical phrase-based
translation. in proceedings of the 2003 conference
of the north american chapter of the association
for computational linguistics on human language
technology - volume 1, naacl    03, pages 48   54,
stroudsburg, pa, usa. association for computa-
tional linguistics.

[koehn et al.2007] philipp koehn, hieu hoang,
alexandra birch, chris callison-burch, marcello
federico, nicola bertoldi, brooke cowan, wade
shen, christine moran, richard zens, chris dyer,

