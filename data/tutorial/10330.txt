structured training for neural network transition-based parsing

david weiss chris alberti michael collins

slav petrov

{djweiss,chrisalberti,mjcollins,slav}@google.com

google inc

new york, ny

5
1
0
2

 

n
u
j
 

9
1

 
 
]
l
c
.
s
c
[
 
 

1
v
8
5
1
6
0

.

6
0
5
1
:
v
i
x
r
a

abstract

we present structured id88 training for neural
network transition-based id33. we
learn the neural network representation using a gold
corpus augmented by a large number of automat-
ically parsed sentences. given this    xed network
representation, we learn a    nal layer using the struc-
tured id88 with beam-search decoding. on
the id32, our parser reaches 94.26% un-
labeled and 92.41% labeled attachment accuracy,
which to our knowledge is the best accuracy on
stanford dependencies to date. we also provide in-
depth ablative analysis to determine which aspects
of our model provide the largest gains in accuracy.

introduction

1
syntactic analysis is a central problem in lan-
guage understanding that has received a tremen-
dous amount of attention. lately, dependency
parsing has emerged as a popular approach to this
problem due to the availability of dependency tree-
banks in many languages (buchholz and marsi,
2006; nivre et al., 2007; mcdonald et al., 2013)
and the e   ciency of dependency parsers.

transition-based parsers (nivre, 2008) have
been shown to provide a good balance between
e   ciency and accuracy. in transition-based pars-
ing, sentences are processed in a linear left to
right pass; at each position, the parser needs to
choose from a set of possible actions de   ned by
the transition strategy. in greedy models, a classi-
   er is used to independently decide which transi-
tion to take based on local features of the current
parse con   guration. this classi   er typically uses
hand-engineered features and is trained on indi-
vidual transitions extracted from the gold transi-
tion sequence. while extremely fast, these greedy
models typically su   er from search errors due to
the inability to recover from incorrect decisions.
zhang and clark (2008) showed that a beam-
search decoding algorithm utilizing the structured

id88 training algorithm can greatly improve
accuracy. nonetheless, signi   cant manual fea-
ture engineering was required before transition-
based systems provided competitive accuracy with
graph-based parsers (zhang and nivre, 2011), and
only by incorporating graph-based scoring func-
tions were bohnet and kuhn (2012) able to exceed
the accuracy of graph-based approaches.

in contrast to these carefully hand-tuned ap-
proaches, chen and manning (2014) recently
presented a neural network version of a greedy
transition-based parser.
in their model, a feed-
forward neural network with a hidden layer is used
to make the transition decisions. the hidden layer
has the power to learn arbitrary combinations of
the atomic inputs, thereby eliminating the need for
hand-engineered features. furthermore, because
the neural network uses a distributed representa-
tion, it is able to model lexical, part-of-speech
(pos) tag, and arc label similarities in a contin-
uous space. however, although their model out-
performs its greedy hand-engineered counterparts,
it is not competitive with state-of-the-art depen-
dency parsers that are trained for structured search.
in this work, we combine the representational
power of neural networks with the superior search
enabled by structured training and id136, mak-
ing our parser one of the most accurate depen-
dency parsers to date. training and testing on
the id32 (marcus et al., 1993), our
transition-based parser achieves 93.99% unlabeled
(uas) / 92.05% labeled (las) attachment accu-
racy, outperforming the 93.22% uas / 91.02%
las of zhang and mcdonald (2014) and 93.27
uas / 91.19 las of bohnet and kuhn (2012).
in addition, by incorporating unlabeled data into
training, we further improve the accuracy of our
model to 94.26% uas / 92.41% las (93.46%

uas / 91.49% las for our greedy model).

in our approach we start with the basic structure
of chen and manning (2014), but with a deeper ar-
chitecture and improvements to the optimization
procedure. these modi   cations (section 2) in-
crease the performance of the greedy model by as
much as 1%. as in prior work, we train the neu-
ral network to model the id203 of individual
parse actions. however, we do not use these prob-
abilities directly for prediction.
instead, we use
the activations from all layers of the neural net-
work as the representation in a structured percep-
tron model that is trained with id125 and
early updates (section 3). on the id32,
this structured learning approach signi   cantly im-
proves parsing accuracy by 0.8%.

an additional contribution of this work is an
e   ective way to leverage unlabeled data. neu-
ral networks are known to perform very well in
the presence of large amounts of training data;
however, obtaining more expert-annotated parse
trees is very expensive. to this end, we generate
large quantities of high-con   dence parse trees by
parsing unlabeled data with two di   erent parsers
and selecting only the sentences for which the
two parsers produced the same trees (section 3.3).
this approach is known as    tri-training    (li et
al., 2014) and we show that it bene   ts our neu-
ral network parser signi   cantly more than other
approaches. by adding 10 million automatically
parsed tokens to the training data, we improve the
accuracy of our parsers by almost    1.0% on web
domain data.

we provide an extensive exploration of our
model in section 5 through ablative analysis and
other retrospective experiments. one of the goals
of this work is to provide guidance for future re-
   nements and improvements on the architecture
and modeling choices we introduce in this paper.
finally, we also note that neural network repre-
sentations have a long history in syntactic parsing
(henderson, 2004; titov and henderson, 2007;
titov and henderson, 2010); however, like chen
and manning (2014), our network avoids any re-
current structure so as to keep id136 fast and
e   cient and to allow the use of simple backprop-
agation to compute gradients. our work is also
not the    rst to apply structured training to neu-
ral networks (see e.g. peng et al. (2009) and do
and artires (2010) for conditional random field
(crf) training of neural networks). our paper ex-

figure 1: schematic overview of our neural network model.
atomic features are extracted from the i   th elements on the
stack (si) and the bu   er (bi); lci indicates the i   th leftmost
child and rci the i   th rightmost child. we use the top two
elements on the stack for the arc features and the top four
tokens on stack and bu   er for words, tags and arc labels.

tends this line of work to the setting of inexact
search with beam decoding for dependency pars-
ing; zhou et al. (2015) concurrently explored a
similar approach using a structured probabilistic
ranking objective. dyer et al. (2015) concurrently
developed the stack long short-term memory
(s-lstm) architecture, which does incorporate
recurrent architecture and look-ahead, and which
yields comparable accuracy on the id32
to our greedy model.

2 neural network model
in this section, we describe the architecture of our
model, which is summarized in figure 1. note that
we separate the embedding processing to a distinct
   embedding layer    for clarity of presentation. our
model is based upon that of chen and manning
(2014) and we discuss the di   erences between our
model and theirs in detail at the end of this section.
we use the arc-standard (nivre, 2004) transition
system.

input layer

2.1
given a parse con   guration c (consisting of a stack
s and a bu   er b), we extract a rich set of dis-
crete features which we feed into the neural net-
work. following chen and manning (2014), we
group these features by their input source: words,
pos tags, and arc labels. the features extracted

h0=[xgeg]embedding layerinputhidden layersargmaxy2gen(x)mxj=1v(yj)   (x,cj)h2=max{0,w2h1+b2}h1=max{0,w1h0+b1}p(y)/exp{ >yh2+by}8g2{word,tag,label}buffeid56dtnewsthedetnnjjvbdnsubjhadlittleeffect.rootrootstack               softmax layerid88 layerfeatures extractedearlyupdates(section3).structuredlearningre-ducesbiasandsigni   cantlyimprovesparsingac-curacyby0.6%.wedemonstrateempiricallythatbeamsearchbasedonthescoresfromtheneuralnetworkdoesnotworkaswell,perhapsbecauseofthelabelbiasproblem.asecondcontributionofthisworkisanef-fectivewaytoleverageunlabeleddataandotherparsers.neuralnetworksareknowntoperformverywellinthepresenceoflargeamountsoftrainingdata.itishoweverunlikelythattheamountofhandparseddatawillincreasesignif-icantlybecauseofthehighcostforsyntactican-notations.tothisendwegeneratelargequanti-tiesofhigh-con   denceparsetreesbyparsinganunlabeledcorpusandselectingonlythesentencesonwhichtwodifferentparsersproducedthesameparsetrees.thisideacomesfromtri-training(lietal.,2014)andwhileapplicabletootherparsersaswell,weshowthatitbene   tsneuralnetworkparsersmorethanmodelswithdiscretefeatures.adding10millionautomaticallyparsedtokenstothetrainingdataimprovestheaccuracyofourparsersfurtherby0.7%.our   nalgreedyparserachievesanunlabeledattachmentscore(uas)of93.46%onthepenntreebanktestset,whileamodelwithabeamofsize8producesanuasof94.08%(section4.tothebestofourknowledge,thesearesomeoftheverybestdependencyaccu-raciesonthiscorpus.weprovideanextensiveexplorationofourmodelinsection5.inablationexperimentsweteaseapartourvariouscontributionsandmodelingchoicesinordertoshedsomelightonwhatmat-tersinpractice.neuralnetworkrepresentationshavebeenusedinstructuredmodelsbefore(pengetal.,2009;doandartires,2010),andhavealsobeenusedforsyntacticparsing(titovandhen-derson,2007;titovandhenderson,2010),alaswithfairlycomplexarchitecturesandconstraints.ourworkontheotherhandintroducesageneralapproachforstructuredid88trainingwithaneuralnetworkrepresentationandachievesstate-of-the-artparsingresultsforenglish.2neuralnetworkmodelinthissection,wedescribethearchitectureofourmodel,whichissummarizedin   gure2.notethatweseparatetheembeddingprocessingtoadistinct   embeddinglayer   forclarityofpresentation.ourmodelisbaseduponthatofchenandmanningroothadthenewshadlittleeffect.dtnnvbdjjnnpstackbufferpartial annotationslittleeffect.feature extractionh0=[xgeg|g2{word,tag,label}]embedding layercon   gurationhidden layersp(y)/exp{  yhi+by},softmax layerid88 layerargmaxd gen(x)mxj=1v(yj)   (x,cj)h2=max{0,w2h1+b2},h1=max{0,w1h0+b1},figure1:schematicoverviewofourneuralnetworkmodel.featuregroupssi,bii2{1,2,3,4}alllc1(si),lc2(si)i2{1,2}allrc1(si),rc2(si)i2{1,2}allrc1(rc1(si))i2{1,2}alllc1(lc1(si))i2{1,2}alltable1:featuresusedinthemodel.siandbiareelementsonthestackandbuffer,respectively.lciindicatesi   thleft-mostchildandrcithei   thrightmostchild.featuresthatareincludedinadditiontothosefromchenandmanning(2014)aremarkedwith?.groupsindicateswhichvalueswereex-tractedfromeachfeaturelocation(e.g.words,tags,labels).(2014)andwediscussthedifferencesbetweenourmodelandtheirsindetailattheendofthissection.2.1inputlayergivenaparsecon   gurationc,weextractarichsetofdiscretefeatureswhichwefeedintotheneuralnetwork.followingchenandmanning(2014),wegroupthesefeaturesbytheirinputsource:words,postags,andarclabels.thefullsetoffeaturesisgivenintable2.thefeaturesextractedforeachgrouparerepresentedasasparsef   vmatrixx,wherevisthesizeofthevocabularyofthefeaturegroupandfisthenumberoffeatures:thevalueofelementxfvis1ifthef   thfeaturetakesonvaluev.weproducethreeinputmatri-ces:xwordforwordsfeatures,xtagforpostagfeatures,andxlabelforarclabels.forallfeaturegroups,weaddadditionalspecial   for each group are represented as a sparse f    v
matrix x, where v is the size of the vocabulary
of the feature group and f is the number of fea-
tures. the value of element x f v is 1 if the f    th
feature takes on value v. we produce three in-
put matrices: xword for words features, xtag for
pos tag features, and xlabel for arc labels, with
fword = ftag = 20 and flabel = 12 (figure 1).

for all feature groups, we add additional special
values for    root    (indicating the pos or word of
the root token),    null    (indicating no valid fea-
ture value could be computed) or    unk    (indicat-
ing an out-of-vocabulary item).

2.2 embedding layer
the    rst learned layer h0 in the network trans-
forms the sparse, discrete features x into a dense,
continuous embedded representation. for each
feature group xg, we learn a vg    dg embedding
matrix eg that applies the conversion:

2.4 relationship to chen and manning (2014)
our model is clearly inspired by and based on the
work of chen and manning (2014). there are a
few structural di   erences: (1) we allow for much
smaller embeddings of pos tags and labels, (2) we
use relu units in our hidden layers, and (3) we use
a deeper model with two hidden layers. somewhat
to our surprise, we found these changes combined
with an sgd training scheme (section 3.1) during
the    pre-training    phase of the model to lead to an
almost 1% accuracy gain over chen and manning
(2014). this trend held despite carefully tuning
hyperparameters for each method of training and
structure combination.

our main contribution from an algorithmic per-
spective is our training procedure: as described in
the next section, we use the structured id88
for learning the    nal layer of our model. we thus
present a novel way to leverage a neural network
representation in a id170 setting.

h0 = [xgeg | g     {word, tag, label}],

(1)

3 semi-supervised structured learning

the embedding layer has e = (cid:80)

where we apply the computation separately for
each group g and concatenate the results. thus,
g fgdg outputs,
which we reshape to a vector h0. we can choose
the embedding dimensionality d for each group
freely. since pos tags and arc labels have much
smaller vocabularies, we show in our experiments
(section 5.1) that we can use smaller dtag and
dlabel, without a loss in accuracy.

2.3 hidden layers
we experimented with one and two hidden layers
composed of m recti   ed linear (relu) units (nair
and hinton, 2010). each unit in the hidden layers
is fully connected to the previous layer:
hi = max{0, wihi   1 + bi},

(2)
where w1 is a m1    e weight matrix for the    rst
hidden layer and wi are mi    mi   1 matrices for all
subsequent layers. the weights bi are bias terms.
relu layers have been well studied in the neural
network literature and have been shown to work
well for a wide domain of problems (krizhevsky
et al., 2012; zeiler et al., 2013). through most of
development, we kept mi = 200, but we found that
signi   cantly increasing the number of hidden units
improved our results for the    nal comparison.

in this work, we investigate a semi-supervised
structured learning scheme that yields substantial
improvements in accuracy over the baseline neu-
ral network model. there are two complementary
contributions of our approach: (1) incorporating
structured learning of the model and (2) utilizing
unlabeled data. in both cases, we use the neural
network to model the id203 of each parsing
action y as a soft-max function taking the    nal hid-
den layer as its input:

p(y)     exp{  

y hi + by},
(cid:62)

(3)

where   y is a mi dimensional vector of weights for
class y and i is the index of the    nal hidden layer
of the network. at a high level our approach can
be summarized as follows:

    first, we pre-train the network   s hidden rep-
resentations by learning probabilities of pars-
ing actions. fixing the hidden representa-
tions, we learn an additional    nal output layer
using the structured id88 that uses the
output of the network   s hidden layers.
in
practice this improves accuracy by    0.6% ab-
solute.
    next, we show that we can supplement the
gold data with a large corpus of high quality

automatic parses. we show that incorporat-
ing unlabeled data in this way improves ac-
curacy by as much as 1% absolute.

3.1 id26 pretraining
to learn the hidden representations, we use
mini-batched averaged stochastic id119
(asgd) (bottou, 2010) with momentum (hinton,
2012) to learn the parameters    of the network,
where    = {eg, wi, bi,   y |    g, i, y}. we use back-
propagation to minimize the multinomial logistic
loss:

l(  ) =    (cid:88)

(cid:88)

y1 . . . y j   1 for any integer j     1: we will use c and
y1 . . . y j   1 interchangeably.
for a sentence x, de   ne gen(x) to be the set
of parse trees for x. each y     gen(x) is a se-
quence of decisions y1 . . . ym for some integer m.
we use y to denote the set of possible decisions
in the parsing model. for each decision y     y
we assume a parameter vector v(y)     rd. these
parameters will be trained using the id88.

in decoding with the id88-trained model,

we will use id125 to attempt to    nd:

m(cid:88)

j=1

log p(y j | c j,   ) +   

j

i

||wi||2

2, (4)

argmax
y   gen(x)

v(y j)      (x, y1 . . . y j   1).

where    is a id173 hyper-parameter over
the hidden layer parameters (we use    = 10   4 in
all experiments) and j sums over all decisions and
con   gurations {y j, c j} extracted from gold parse
trees in the dataset.

the speci   c update rule we apply at iteration t

is as follows:

gt =   gt   1        l(  t),

  t+1 =   t +   tgt,

(5)
(6)

where the descent direction gt is computed by a
weighted combination of the previous direction
gt   1and the current gradient    l(  t). the parame-
ter        [0, 1) is the momentum parameter while   t
is the traditional learning rate. in addition, since
we did not tune the id173 parameter   ,
we apply a simple exponential step-wise decay to
  t; for every    rounds of updates, we multiply
  t = 0.96  t   1.

the    nal component of the update is parame-
ter averaging: we maintain averaged parameters
    t =   t     t   1 + (1       t)  t, where   t is an averag-
ing weight that increases from 0.1 to 0.9999 with
1/t. combined with averaging, careful tuning of
the three hyperparameters   ,   0, and    using held-
out data was crucial in our experiments.

3.2 structured id88 training
given the hidden representations, we now describe
how the id88 can be trained to utilize these
representations. the id88 algorithm with
early updates (collins and roark, 2004) requires
a feature-vector de   nition    that maps a sentence
x together with a con   guration c to a feature vec-
tor   (x, c)     rd. there is a one-to-one mapping
between con   gurations c and decision sequences

thus each decision y j receives a score:

v(y j)      (x, y1 . . . y j   1).

in the id88 with early updates, the param-
eters v(y) are trained as follows. on each train-
ing example, we run id125 until the gold-
standard parse tree falls out of the beam.1 de-
   ne j to be the length of the beam at this point.
a structured id88 update is performed using
the gold-standard decisions y1 . . . y j as the target,
and the highest scoring (incorrect) member of the
beam as the negative example.

a key idea in this paper is to use the neural net-
work to de   ne the representation   (x, c). given
the sentence x and the con   guration c, assuming
two hidden layers, the neural network de   nes val-
ues for h1, h2, and p(y) for each decision y. we
experimented with various de   nitions of    (sec-
tion 5.2) and found that   (x, c) = [h1 h2 p(y)] (the
concatenation of the outputs from both hidden lay-
ers, as well as the probabilities for all decisions y
possible in the current con   guration) had the best
accuracy on development data.

note that it is possible to continue to use back-
propagation to learn the representation   (x, c) dur-
ing id88 training; however, we found using
asgd to pre-train the representation always led to
faster, more accurate results in preliminary exper-
iments, and we left further investigation for future
work.

incorporating unlabeled data

3.3
given the high capacity, non-linear nature of the
deep network we hypothesize that our model can

1if the gold parse tree stays within the beam until the end

of the sentence, conventional id88 updates are used.

be signi   cantly improved by incorporating more
data. one way to use unlabeled data is through
unsupervised methods such as word clusters (koo
et al., 2008); we follow chen and manning (2014)
and use pretrained id27s to initial-
ize our model. the id27s capture
similar distributional information as word clusters
and give consistent improvements by providing a
good initialization and information about words
not seen in the treebank data.

however, obtaining more training data is even
more important than a good initialization. one
potential way to obtain additional training data is
by parsing unlabeled data with previously trained
models. mcclosky et al. (2006) and huang and
harper (2009) showed that iteratively re-training
a single model (   self-training   ) can be used to
improve parsers in certain settings; petrov et al.
(2010) built on this work and showed that a slow
and accurate parser can be used to    up-train    a
faster but less accurate parser.

in this work, we adopt the    tri-training    ap-
proach of li et al. (2014): two parsers are used to
process the unlabeled corpus and only sentences
for which both parsers produced the same parse
tree are added to the training data. the intu-
ition behind this idea is that the chance of the
parse being correct is much higher when the two
parsers agree: there is only one way to be correct,
while there are many possible incorrect parses. of
course, this reasoning holds only as long as the
parsers su   er from di   erent biases.

we show that tri-training is far more e   ective
than vanilla up-training for our neural network
model. we use same setup as li et al. (2014), in-
tersecting the output of the berkeleyparser (petrov
et al., 2006), and a reimplementation of zpar
(zhang and nivre, 2011) as our baseline parsers.
the two parsers agree only 36% of the time on
the tune set, but their accuracy on those sentences
is 97.26% uas, approaching the inter annotator
agreement rate. these sentences are of course eas-
ier to parse, having an average length of 15 words,
compared to 24 words for the tune set overall.
however, because we only use these sentences to
extract individual transition decisions, the shorter
length does not seem to hurt their utility. we gen-
erate 107 tokens worth of new parses and use this
data in the id26 stage of training.

4 experiments

in this section we present our experimental setup
and the main results of our work.

4.1 experimental setup
we conduct our experiments on two english lan-
guage benchmarks: (1) the standard wall street
journal (wsj) part of the id32 (marcus
et al., 1993) and (2) a more comprehensive union
of publicly available treebanks spanning multiple
domains. for the wsj experiments, we follow
standard practice and use sections 2-21 for train-
ing, section 22 for development and section 23 as
the    nal test set. since there are many hyperpa-
rameters in our models, we additionally use sec-
tion 24 for tuning. we convert the constituency
trees to stanford style dependencies (de marne   e
et al., 2006) using version 3.3.0 of the converter.
we use a crf-based pos tagger to generate 5-
fold jack-knifed pos tags on the training set and
predicted tags on the dev, test and tune sets; our
tagger gets comparable accuracy to the stanford
pos tagger (toutanova et al., 2003) with 97.44%
on the test set. we report unlabeled attachment
score (uas) and labeled attachment score (las)
excluding punctuation on predicted pos tags, as
is standard for english.

for the second set of experiments, we follow
the same procedure as above, but with a more di-
verse dataset for training and evaluation. follow-
ing vinyals et al. (2015), we use (in addition to the
wsj), the ontonotes corpus version 5 (hovy et
al., 2006), the english web treebank (petrov and
mcdonald, 2012), and the updated and corrected
question treebank (judge et al., 2006). we train
on the union of each corpora   s training set and test
on each domain separately. we refer to this setup
as the    treebank union    setup.

in our semi-supervised experiments, we use the
corpus from chelba et al. (2013) as our source of
unlabeled data. we process it with the berkeley-
parser (petrov et al., 2006), a latent variable con-
stituency parser, and a reimplementation of zpar
(zhang and nivre, 2011), a transition-based parser
with id125. both parsers are included as
baselines in our evaluation. we select the    rst
107 tokens for which the two parsers agree as
additional training data. for our tri-training ex-
periments, we re-train the pos tagger using the
pos tags assigned on the unlabeled data from the
berkeley constituency parser. this increases pos

method
graph-based

uas las beam

method
graph-based

news web qtb

92.88 90.71
bohnet (2010)
martins et al. (2013)
92.89 90.55
zhang and mcdonald (2014) 93.22 91.02

n/a
n/a
n/a

91.38 85.22 91.49
bohnet (2010)
martins et al. (2013)
91.13 85.04 91.54
zhang and mcdonald (2014) 91.48 85.59 90.69

transition-based

(cid:63)zhang and nivre (2011)
bohnet and kuhn (2012)
chen and manning (2014)
s-lstm (dyer et al., 2015)
our greedy
our id88

93.00 90.95
93.27 91.19
91.80 89.60
93.20 90.90
93.19 91.18
93.99 92.05

tri-training

(cid:63)zhang and nivre (2011)
our greedy
our id88

92.92 90.88
93.46 91.49
94.26 92.41

32
40
1
1
1
8

32
1
8

table 1: final wsj test set results. we compare our system to
state-of-the-art graph-based and transition-based dependency
parsers. (cid:63) denotes our own re-implementation of the system
so we could compare tri-training on a competitive baseline.
all methods except chen and manning (2014) and dyer et
al. (2015) were run using predicted tags from our pos tag-
ger. for reference, the accuracy of the berkeley constituency
parser (after conversion) is 93.61% uas / 91.51% las.

accuracy slightly to 97.57% on the wsj.

4.2 model initialization & hyperparameters
in all cases, we initialized wi and    randomly us-
ing a gaussian distribution with variance 10   4. we
used    xed initialization with bi = 0.2, to ensure
that most relu units are activated during the initial
rounds of training. we did not systematically com-
pare this random scheme to others, but we found
that it was su   cient for our purposes.

for the id27 matrix eword, we
initialized the parameters using pretrained word
embeddings. we used the publicly available
id972 tool (mikolov et al., 2013) to learn
cbow embeddings following the sample con   g-
uration provided with the tool. for words not ap-
pearing in the unsupervised data and the special
   null    etc.
tokens, we used random initializa-
tion. in preliminary experiments we found no dif-
ference between training the id27s on
1 billion or 10 billion tokens. we therefore trained
the id27s on the same corpus we used
for tri-training (chelba et al., 2013).

we set dword = 64 and dtag = dlabel = 32 for
embedding dimensions and m1 = m2 = 2048 hid-
den units in our    nal experiments. for the percep-

2http://code.google.com/p/id97/

transition-based

(cid:63)zhang and nivre (2011)
bohnet and kuhn (2012)
our greedy
our id88 (b=16)

tri-training

91.15 85.24 92.46
91.69 85.33 92.21
91.21 85.41 90.61
92.25 86.44 92.06

(cid:63)zhang and nivre (2011)
our greedy
our id88 (b=16)

91.46 85.51 91.36
91.82 86.37 90.58
92.62 87.00 93.05

table 2: final treebank union test set results. we report
las only for brevity; see appendix for full results. for these
tri-training results, we sampled sentences to ensure the dis-
tribution of sentence lengths matched the distribution in the
training set, which we found marginally improved the zpar
tri-training performance. for reference, the accuracy of the
berkeley constituency parser (after conversion) is 91.66%
wsj, 85.93% web, and 93.45% qtb.

tron layer, we used   (x, c) = [h1 h2 p(y)] (con-
catenation of all intermediate layers). all hyper-
parameters (including structure) were tuned using
section 24 of the wsj only. when not tri-training,
we used hyperparameters of    = 0.2,   0 = 0.05,
   = 0.9, early stopping after roughly 16 hours of
training time. with the tri-training data, we de-
creased   0 = 0.05, increased    = 0.5, and de-
creased the size of the network to m1 = 1024,
m2 = 256 for run-time e   ciency, and trained the
network for approximately 4 days. for the tree-
bank union setup, we set m1 = m2 = 1024 for the
standard training set and for the tri-training setup.

4.3 results
table 1 shows our    nal results on the wsj test
set, and table 2 shows the cross-domain results
from the treebank union. we compare to the best
dependency parsers in the literature. for (chen
and manning, 2014) and (dyer et al., 2015), we
use reported results; the other baselines were run
by bernd bohnet using version 3.3.0 of the stan-
ford dependencies and our predicted pos tags for
all datasets to make comparisons as fair as possi-
ble. on the wsj and web tasks, our parser out-
performs all dependency parsers in our compari-
son by a substantial margin. the question (qtb)
dataset is more sensitive to the smaller beam size
we use in order to train the models in a reason-
able time; if we increase to b = 32 at id136

time only, our id88 performance goes up to
92.29% las.

since many of the baselines could not be di-
rectly compared to our semi-supervised approach,
we re-implemented zhang and nivre (2011) and
trained on the tri-training corpus. although tri-
training did help the baseline on the dev set (fig-
ure 4), test set performance did not improve sig-
ni   cantly.
in contrast, it is quite exciting to see
that after tri-training, even our greedy parser is
more accurate than any of the baseline depen-
dency parsers and competitive with the berkeley-
parser used to generate the tri-training data. as ex-
pected, tri-training helps most dramatically to in-
crease accuracy on the treebank union setup with
diverse domains, yielding 0.4-1.0% absolute las
improvement gains for our most accurate model.
unfortunately we are not able to compare to
several semi-supervised dependency parsers that
achieve some of the highest reported accuracies
on the wsj, in particular suzuki et al. (2009),
suzuki et al. (2011) and chen et al. (2013). these
parsers use the yamada and matsumoto (2003) de-
pendency conversion and the accuracies are there-
fore not directly comparable. the highest of these
is suzuki et al. (2011), with a reported accuracy
of 94.22% uas. even though the uas is not di-
rectly comparable, it is typically similar, and this
suggests that our model is competitive with some
of the highest reported accuries for dependencies
on wsj.

5 discussion

in this section, we investigate the contribution of
the various components of our approach through
ablation studies and other systematic experiments.
we tune on section 24, and use section 22 for
comparisons in order to not pollute the o   cial test
set (section 23). we focus on uas as we found
the las scores to be strongly correlated. unless
otherwise speci   ed, we use 200 hidden units in
each layer to be able to run more ablative exper-
iments in a reasonable amount of time.

impact of network structure

5.1
in addition to initialization and hyperparameter
tuning, there are several additional choices about
model structure and size a practitioner faces when
implementing a neural network model. we ex-
plore these questions and justify the particular
choices we use in the following. note that we do

figure 2: e   ect of hidden layers and pre-training on vari-
ance of random restarts. initialization was either completely
random or initialized with id97 embeddings (   pre-
trained   ), and either one or two hidden layers of size 200
were used (   200    vs    200x200   ). each point represents
maximization over a small hyperparameter grid with early
stopping based on wsj tune set uas score. dword = 64,
dtag, dlabel = 16.

not use a beam for this analysis and therefore do
not train the    nal id88 layer. this is done
in order to reduce training times and because the
trends persist across settings.

variance reduction with pre-trained embed-
dings. since the learning problem is non-
convex, di   erent initializations of the parameters
yield di   erent solutions to the learning problem.
thus, for any given experiment, we ran multiple
random restarts for every setting of our hyperpa-
rameters and picked the model that performed best
using the held-out tune set. we found it important
to allow the model to stop training early if tune set
accuracy decreased.

we visualize the performance of 32 random
restarts with one or two hidden layers and with
and without pretrained id27s in fig-
ure 2, and a summary of the    gure in table 3.
while adding a second hidden layer results in a
large gain on the tune set, there is no gain on the
dev set if pre-trained embeddings are not used.
in fact, while the overall uas scores of the tune
set and dev set are strongly correlated (   = 0.64,
p < 10   10), they are not signi   cantly correlated
if pre-trained embeddings are not used (   = 0.12,
p > 0.3). this suggests that an additional bene-
   t of pre-trained embeddings, aside from allowing
learning to reach a more accurate solution, is to
push learning towards a solution that generalizes
to more data.

91.291.491.691.8929292.192.292.392.492.592.692.7uas (%) on wsj tune setuas (%) on wsj dev setvariance of networks on tuning/dev set  pretrained 200x200pretrained 200200x200200pre
y 200    200
y
n 200    200
n

hidden wsj   24 (max) wsj   22
92.58   0.12
92.30    0.10
92.19    0.13
92.20    0.12

92.10    0.11
91.76    0.09
91.84    0.11
91.55    0.10

200

200

table 3: impact of network architecture on uas for greedy
id136. we select the best model from 32 random restarts
based on the tune set and show the resulting dev set accuracy.
we also show the standard deviation across the 32 restarts.

128

64
512 1024 2048
# hidden
91.73 92.27 92.48 92.73 92.74 92.83
1 layer
2 layers 91.89 92.40 92.71 92.70 92.96 93.13

256

table 4:
increasing hidden layer size increases wsj dev
uas. shown is the average wsj dev uas across hyperpa-
rameter tuning and early stopping with 3 random restarts with
a greedy model.

diminishing returns with increasing embed-
ding dimensions. for these experiments, we
   xed one embedding type to a high value and
reduced the dimensionality of all others to very
small values. the results are plotted in figure
3, suggesting larger embeddings do not signi   -
cantly improve results. we also ran tri-training
on a very compact model with dword = 8 and
dtag = dlabel = 2 (8   fewer parameters than our
full model) which resulted in 92.33% uas accu-
racy on the dev set. this is comparable to the full
model without tri-training, suggesting that more
training data can compensate for fewer parame-
ters.

increasing hidden units yields large gains. for
these experiments, we    xed the embedding sizes
dword = 64, dtag = dlabel = 32 and tried increas-
ing and decreasing the dimensionality of the hid-
den layers on a logarthmic scale. improvements in
accuracy did not appear to saturate even with in-
creasing the number of hidden units by an order of
magnitude, though the network became too slow
to train e   ectively past m = 2048. these results
suggest that there are still gains to be made by in-
creasing the e   ciency of larger networks, even for
greedy shift-reduce parsers.

impact of structured id88

5.2
we now turn our attention to the importance of
structured id88 training as well as the im-
pact of di   erent latent representations.

bias reduction through structured training.
to evaluate the impact of structured training, we

8

4

2

1

beam
wsj only
90.55 91.36 92.54 92.62 92.88 93.09
zn   11
92.74 93.07 93.16 93.25 93.24 93.24
softmax
id88 92.73 93.06 93.40 93.47 93.50 93.58

16

32

tri-training

91.65 92.37 93.37 93.24 93.21 93.18
zn   11
93.71 93.82 93.86 93.87 93.87 93.87
softmax
id88 93.69 94.00 94.23 94.33 94.31 94.32

table 5: id125 always yields signi   cant gains but us-
ing id88 training provides even larger bene   ts, espe-
cially for the tri-trained neural network model. the best re-
sult for each model is highlighted in bold.

  (x, c) wsj only tri-training
[h2]
[p(y)]
[h1 h2]

93.16
93.26
93.33
93.47

93.93
93.80
93.95
94.33

[h1 h2 p(y)]

table 6: utilizing all intermediate representations improves
performance on the wsj dev set. all results are with b = 8.

compare using the estimates p(y) from the neural
network directly for id125 to using the acti-
vations from all layers as features in the structured
id88. using the id203 estimates di-
rectly is very similar to ratnaparkhi (1997), where
a maximum-id178 model was used to model the
distribution over possible actions at each parser
state, and id125 was used to search for the
highest id203 parse. a known problem with
id125 in this setting is the label-bias prob-
lem. table 5 shows the impact of using structured
id88 training over using the softmax func-
tion during id125 as a function of the beam
size used. for reference, our reimplementation of
zhang and nivre (2011) is trained equivalently for
each setting. we also show the impact on beam
size when tri-training is used. although the beam
does marginally improve accuracy for the softmax
model, much greater gains are achieved when per-
ceptron training is used.

using all hidden layers crucial for structured
id88. we also investigated the impact of
connecting the    nal id88 layer to all prior
hidden layers (table 6). our results suggest that
all intermediate layers of the network are indeed
discriminative. nonetheless, aggregating all of
their activations proved to be the most e   ective
representation for the structured id88. this
suggests that the representations learned by the
network collectively contain the information re-

figure 3: e   ect of embedding dimensions on the wsj tune set.

quired to reduce the bias of the model, but not
when    ltered through the softmax layer. finally,
we also experimented with connecting both hid-
den layers to the softmax layer during backpropa-
gation training, but we found this did not signi   -
cantly a   ect the performance of the greedy model.

impact of tri-training

5.3
to evaluate the impact of the tri-training approach,
we compared to up-training with the berkely-
parser (petrov et al., 2006) alone. the results are
summarized in figure 4 for the greedy and percep-
tron neural net models as well as our reimplemen-
tated zhang and nivre (2011) baseline.

for our neural network model, training on the
output of the berkeleyparser yields only modest
gains, while training on the data where the two
parsers agree produces signi   cantly better results.
this was especially pronounced for the greedy
models: after tri-training, the greedy neural net-
work model surpasses the berkeleyparser in accu-
racy. it is also interesting to note that up-training
improved results far more than tri-training for the
baseline. we speculate that this is due to the a
lack of diversity in the tri-training data for this
model, since the same baseline model was inter-
sected with the berkeleyparser to generate the tri-
training data.

5.4 error analysis
regardless of tri-training, using the structured per-
ceptron improved error rates on some of the com-
mon and di   cult labels: root, ccomp, cc, conj,
and nsubj all improved by >1%. we inspected
the learned id88 weights v for the softmax
probabilities p(y) (see appendix) and found that
the id88 reweights the softmax probabilities
based on common confusions; e.g. a strong neg-
ative weight for the action right(ccomp) given
the softmax model outputs right(conj). note

figure 4: semi-supervised training with 107 additional to-
kens, showing that tri-training gives signi   cant improve-
ments over up-training for our neural net model.

that this trend did not hold when   (x, c) = [p(y)];
without the hidden layer, the id88 was not
able to reweight the softmax probabilities to ac-
count for the greedy model   s biases.

6 conclusion
we presented a new state of the art in dependency
parsing: a transition-based neural network parser
trained with the structured id88 and asgd.
we then combined this approach with unlabeled
data and tri-training to further push state-of-the-art
in semi-supervised id33. nonethe-
less, our ablative analysis suggests that further
gains are possible simply by scaling up our system
to even larger representations. in future work, we
will apply our method to other languages, explore
end-to-end training of the system using structured
learning, and scale up the method to larger datasets
and network structures.

acknowledgements
we would like to thank bernd bohnet for training
his parsers and turboparser on our setup. this pa-
per bene   tted tremendously from discussions with
ryan mcdonald, greg coppola, emily pitler and
fernando pereira. finally, we are grateful to all
members of the google parsing team.

124816326412889.59090.59191.592id27 dimension (dwords)uas (%)word tuning on wsj (tune set, dpos,dlabels=32)  pretrained 200x200pretrained 200200x2002001248163290.59191.592pos/label embedding dimension (dpos,dlabels)uas (%)pos/label tuning on wsj (tune set, dwords=64)  pretrained 200x200pretrained 200200x200200zn   11 (b=1)zn   11 (b=32)ours (b=1)ours (b=8)909192939495semi   supervised training (wsj dev set)  baseuptriberkeleyreferences
bernd bohnet and jonas kuhn. 2012. the best of
both worlds: a graph-based completion model for
transition-based parsers. in proc. eacl, pages 77   
87.

bernd bohnet. 2010. top accuracy and fast depen-
dency parsing is not a contradiction. in proc. col-
ing, pages 89   97.

l  eon bottou. 2010. large-scale machine learning with
in proc. compstat,

stochastic id119.
pages 177   186.

sabine buchholz and erwin marsi. 2006. conll-x
shared task on multilingual id33. in
proc. conll, pages 149   164.

ciprian chelba, tomas mikolov, mike schuster, qi ge,
thorsten brants, and phillipp koehn. 2013. one
billion word benchmark for measuring progress in
statistical id38. corr.

danqi chen and christopher d. manning. 2014. a fast
and accurate dependency parser using neural net-
works. in proc. emnlp, pages 740   750.

wenliang chen, min zhang, and yue zhang. 2013.
semi-supervised feature transformation for depen-
dency parsing. in proc. 2013 emnlp, pages 1303   
1313.

michael collins and brian roark. 2004.

incremen-
tal parsing with the id88 algorithm. in proc.
acl, main volume, pages 111   118, barcelona,
spain.

marie-catherine de marne   e, bill maccartney, and
christopher d. manning. 2006. generating typed
dependency parses from phrase structure parses. in
proc. lrec, pages 449   454.

trinh minh tri do and thierry artires. 2010. neural
in aistats, volume 9,

conditional random    elds.
pages 177   184.

chris dyer, miguel ballesteros, wang ling, austin
matthews, and noah a. smith. 2015. transition-
based id33 with stack long short-
term memory. in proc. acl.

james henderson. 2004. discriminative training of
in proc. acl,

a neural network statistical parser.
main volume, pages 95   102.

geo   rey e. hinton. 2012. a practical guide to train-
ing restricted id82s. in neural net-
works: tricks of the trade (2nd ed.), lecture notes
in computer science, pages 599   619. springer.

eduard hovy, mitchell marcus, martha palmer, lance
ramshaw, and ralph weischedel. 2006. ontonotes:
the 90% solution. in proc. hlt-naacl, pages 57   
60.

zhongqiang huang and mary harper. 2009. self-
training pid18 grammars with latent annotations
in proc. 2009 emnlp, pages
across languages.
832   841, singapore.

john judge, aoife cahill, and josef van genabith.
2006. questionbank: creating a corpus of parse-
annotated questions. in proc. acl, pages 497   504.

terry koo, xavier carreras, and michael collins.
2008. simple semi-supervised id33.
in proc. acl-hlt, pages 595   603.

alex krizhevsky, ilya sutskever, and geo   rey e. hin-
ton. 2012. id163 classi   cation with deep con-
in proc. nips, pages
volutional neural networks.
1097   1105.

zhenghua li, min zhang, and wenliang chen.
2014. ambiguity-aware ensemble training for semi-
in proc. acl,
supervised id33.
pages 457   467.

mitchell p. marcus, beatrice santorini, and mary ann
marcinkiewicz. 1993. building a large annotated
corpus of english: the id32. computa-
tional linguistics, 19(2):313   330.

andre martins, miguel almeida, and noah a. smith.
2013. turning on the turbo: fast third-order non-
projective turbo parsers. in proc. acl, pages 617   
622.

david mcclosky, eugene charniak, and mark john-
in

son. 2006. e   ective self-training for parsing.
proc. hlt-naacl, pages 152   159.

ryan mcdonald, joakim nivre, yvonne quirmbach-
brundage, yoav goldberg, dipanjan das, kuz-
man ganchev, keith hall, slav petrov, hao
zhang, oscar t  ackstr  om, claudia bedini, n  uria
bertomeu castell  o, and jungmee lee. 2013. uni-
versal dependency annotation for multilingual pars-
ing. in proc. acl, pages 92   97.

tomas mikolov, kai chen, greg corrado, and je   rey
dean. 2013. e   cient estimation of word represen-
tations in vector space. corr, abs/1301.3781.

vinod nair and geo   rey e. hinton. 2010. recti   ed
linear units improve restricted id82s.
in proc. 27th icml, pages 807   814.

joakim nivre, johan hall, sandra k  ubler, ryan mc-
donald, jens nilsson, sebastian riedel, and deniz
yuret. 2007. the conll 2007 shared task on de-
pendency parsing. in proc. conll, pages 915   932.

joakim nivre. 2004.

incrementality in deterministic
id33. in proc. acl workshop on in-
cremental parsing, pages 50   57.

joakim nivre. 2008. algorithms for deterministic in-
cremental id33. computational lin-
guistics, 34(4):513   553.

jian peng, liefeng bo, and jinbo xu. 2009. con-
ditional neural    elds. in proc. nips, pages 1419   
1427.

slav petrov and ryan mcdonald. 2012. overview of
the 2012 shared task on parsing the web. notes of
the first workshop on syntactic analysis of non-
canonical language (sancl).

slav petrov, leon barrett, romain thibaux, and dan
klein. 2006. learning accurate, compact, and inter-
pretable tree annotation. in proc. acl, pages 433   
440.

slav petrov, pi-chuan chang, michael ringgaard, and
2010. uptraining for accurate
in proc. emnlp,

hiyan alshawi.
deterministic question parsing.
pages 705   713.

adwait ratnaparkhi. 1997. a linear observed time sta-
tistical parser based on maximum id178 models.
in proc. emnlp, pages 1   10.

jun suzuki, hideki isozaki, xavier carreras, and
michael collins. 2009. an empirical study of semi-
supervised structured conditional models for depen-
dency parsing. in proc. 2009 emnlp, pages 551   
560.

jun suzuki, hideki isozaki, and masaaki nagata.
2011. learning condensed feature representations
from large unsupervised data sets for supervised
learning. in proc. acl-hlt, pages 636   641.

ivan titov and james henderson. 2007. fast and ro-
bust multilingual id33 with a gener-
ative latent variable model. in proc. emnlp, pages
947   951.

ivan titov and james henderson.

2010. a la-
tent variable model for generative dependency pars-

ing. in trends in parsing technology, pages 35   55.
springer.

kristina toutanova, dan klein, christopher d. man-
ning, and yoram singer. 2003. feature-rich part-of-
speech tagging with a cyclic dependency network.
in naacl.

oriol vinyals, lukasz kaiser, terry koo, slav petrov,
ilya sutskever, and geo   rey e. hinton.
2015.
grammar as a foreign language. arxiv:1412.7449.

hiroyasu yamada and yuji matsumoto. 2003. statis-
tical dependency analysis with support vector ma-
chines. in proc. iwpt, pages 195   206.

matthew d. zeiler, marc   aurelio ranzato, rajat
monga, mark z. mao, k. yang, quoc viet le,
patrick nguyen, andrew w. senior, vincent van-
houcke, je   rey dean, and geo   rey e. hinton.
2013. on recti   ed linear units for speech process-
ing. in proc. icassp, pages 3517   3521.

yue zhang and stephen clark.

2008. a tale of
two parsers:
investigating and combining graph-
based and transition-based id33 us-
ing beam-search. in proc. emnlp, pages 562   571.

hao zhang and ryan mcdonald. 2014. enforcing
structural diversity in cube-pruned dependency pars-
ing. in proc. acl, pages 656   661.

yue zhang and joakim nivre. 2011. transition-based
id33 with rich non-local features. in
proc. acl-hlt, pages 188   193.

hao zhou, yue zhang, and jiajun chen. 2015. a
neural probabilistic structured-prediction model for
transition-based id33. in proc. acl.

