proceedings of the 53rd annual meeting of the association for computational linguistics

and the 7th international joint conference on natural language processing, pages 1491   1500,

beijing, china, july 26-31, 2015. c(cid:13)2015 association for computational linguistics

1491

sparseovercompletewordvectorrepresentationsmanaalfaruquiyuliatsvetkovdaniyogatamachrisdyernoaha.smithlanguagetechnologiesinstitutecarnegiemellonuniversitypittsburgh,pa,15213,usa{mfaruqui,ytsvetko,dyogatama,cdyer,nasmith}@cs.cmu.eduabstractcurrentdistributedrepresentationsofwordsshowlittleresemblancetotheo-riesoflexicalsemantics.theformeraredenseanduninterpretable,thelat-terlargelybasedonfamiliar,discreteclasses(e.g.,supersenses)andrelations(e.g.,synonymyandhypernymy).wepro-posemethodsthattransformwordvec-torsintosparse(andoptionallybinary)vectors.theresultingrepresentationsaremoresimilartotheinterpretablefeaturestypicallyusedinnlp,thoughtheyaredis-coveredautomaticallyfromrawcorpora.becausethevectorsarehighlysparse,theyarecomputationallyeasytoworkwith.mostimportantly,we   ndthattheyout-performtheoriginalvectorsonbenchmarktasks.1introductiondistributedrepresentationsofwordshavebeenshowntobene   tnlptaskslikeparsing(lazari-douetal.,2013;bansaletal.,2014),nameden-tityrecognition(guoetal.,2014),andsentimentanalysis(socheretal.,2013).theattractionofwordvectorsisthattheycanbederiveddirectlyfromraw,unannotatedcorpora.intrinsicevalua-tionsonvarioustasksareguidingmethodstowarddiscoveryofarepresentationthatcapturesmanyfactsaboutlexicalsemantics(turney,2001;tur-neyandpantel,2010).yetwordvectorsdonotlookanythingliketherepresentationsdescribedinmostlexicalseman-tictheories,whichfocusonidentifyingclassesofwords(levin,1993;bakeretal.,1998;schuler,2005)andrelationshipsamongwordmeanings(miller,1995).thoughexpensivetoconstruct,conceptualizingwordmeaningssymbolicallyisimportantfortheoreticalunderstandingandalsowhenweincorporatelexicalsemanticsintocom-putationalmodelswhereinterpretabilityisde-sired.onthesurface,discretetheoriesseemin-commensuratewiththedistributedapproach,aproblemnowreceivingmuchattentionincompu-tationallinguistics(lewisandsteedman,2013;kielaandclark,2013;vecchietal.,2013;grefen-stette,2013;lewisandsteedman,2014;papernoetal.,2014).ourcontributiontothisdiscussionisanew,principledsparsecodingmethodthattransformsanydistributedrepresentationofwordsintosparsevectors,whichcanthenbetransformedintobinaryvectors(  2).unlikerecentapproachesofincorpo-ratingsemanticsindistributionalwordvectors(yuanddredze,2014;xuetal.,2014;faruquietal.,2015),themethoddoesnotrelyonanyexternalinformationsource.thetransformationresultsinlonger,sparservectors,sometimescalledan   over-complete   representation(olshausenandfield,1997).sparse,overcompleterepresentationshavebeenmotivatedinotherdomainsasawaytoin-creaseseparabilityandinterpretability,witheachinstance(here,aword)havingasmallnumberofactivedimensions(olshausenandfield,1997;lewickiandsejnowski,2000),andtoincreasestabilityinthepresenceofnoise(donohoetal.,2006).ourworkbuildsonrecentexplorationsofspar-sityasausefulformofinductivebiasinnlpandmachinelearningmorebroadly(kazamaandtsu-jii,2003;goodman,2004;friedmanetal.,2008;glorotetal.,2011;yogatamaandsmith,2014,interalia).introducingsparsityinwordvectordi-mensionshasbeenshowntoimprovedimensioninterpretability(murphyetal.,2012;fysheetal.,2014)andusabilityofwordvectorsasfeaturesindownstreamtasks(guoetal.,2014).thewordvectorsweproducearemorethan90%sparse;wealsoconsiderbinarizingtransformationsthatbringthemclosertothecategoriesandrelationsoflex-1492

icalsemantictheories.usinganumberofstate-of-the-artwordvectorsasinput,we   ndconsis-tentbene   tsofourmethodonasuiteofstandardbenchmarkevaluationtasks(  3).wealsoevalu-ateourwordvectorsinawordintrusionexperi-mentwithhumans(changetal.,2009)and   ndthatoursparsevectorsaremoreinterpretablethantheoriginalvectors(  4).weanticipatethatsparse,binaryvectorscanplayanimportantroleasfeaturesinstatisticalnlpmodels,whichstillrelypredominantlyondiscrete,sparsefeatureswhoseinterpretabilityen-ableserroranalysisandcontinueddevelopment.wehavemadeanimplementationofourmethodpubliclyavailable.12sparseovercompletewordvectorsweconsidermethodsfortransformingdensewordvectorstosparse,binaryovercompletewordvec-tors.fig.1showstwoapproaches.theoneonthetop,methoda,convertsdensevectorstosparseovercompletevectors(  2.1).theonebeneath,methodb,convertsdensevectorstosparseandbi-naryovercompletevectors(  2.2and  2.4).letvbethevocabularysize.inthefollowing,x   rl  visthematrixconstructedbystack-ingvnon-sparse   input   wordvectorsoflengthl(producedbyanarbitrarywordvectorestima-tor).wewillrefertotheseasinitializingvectors.a   rk  vcontainsvsparseovercompletewordvectorsoflengthk.   overcomplete   representa-tionlearningimpliesthatk>l.2.1sparsecodinginsparsecoding(leeetal.,2006),thegoalistorepresenteachinputvectorxiasasparselinearcombinationofbasisvectors,ai.ourexperimentsconsiderfourinitializingmethodsforthesevec-tors,discussedinappendixa.givenx,weseektosolveargmind,akx   dak22+     (a)+  kdk22,(1)whered   rl  kisthedictionaryofbasisvec-tors.  isaid173hyperparameter,and   istheregularizer.here,weusethesquaredlossforthereconstructionerror,butotherlossfunctionscouldalsobeused(leeetal.,2009).toobtainsparsewordrepresentationswewillimposean   11https://github.com/mfaruqui/sparse-codingpenaltyona.eq.1canbebrokendownintolossforeachwordvectorwhichcanbeoptimizedsep-aratelyinparallel(  2.3):argmind,avxi=1kxi   daik22+  kaik1+  kdk22(2)wheremidenotestheithcolumnvectorofmatrixm.notethatthisproblemisnotconvex.werefertothisapproachasmethoda.2.2sparsenonnegativevectorsnonnegativityinthefeaturespacehasoftenbeenshowntocorrespondtointerpretability(leeandseung,1999;cichockietal.,2009;murphyetal.,2012;fysheetal.,2014;fysheetal.,2015).toobtainnonnegativesparsewordvectors,weuseavariationofthenonnegativesparsecodingmethod(hoyer,2002).nonnegativesparsecodingfurtherconstrainstheproblemineq.2sothatdandaiarenonnegative.here,weapplythisconstraintonlytotherepresentationvectors{ai}.thus,thenewobjectivefornonnegativesparsevectorsbe-comes:argmind   rl  k   0,a   rk  v   0vxi=1kxi   daik22+  kaik1+  kdk22(3)thisproblemwillplayaroleinoursecondap-proach,methodb,towhichwewillreturnshortly.thisnonnegativityconstraintcanbeeasilyincor-poratedduringoptimization,asexplainednext.2.3optimizationweuseonlineadaptivegradientdescent(ada-grad;duchietal.,2010)forsolvingtheoptimiza-tionproblemsineqs.2   3byupdatingaandd.inordertospeeduptrainingweuseasynchronousupdatestotheparametersofthemodelinparallelforeverywordvector(duchietal.,2012;heigoldetal.,2014).however,directlyapplyingstochasticsubgradi-entdescenttoan   1-regularizedobjectivefailstoproducesparsesolutionsinboundedtime,whichhasmotivatedseveralspecializedalgorithmsthattargetsuchobjectives.weusetheadagradvari-antofonesuchlearningalgorithm,theregular-izeddualaveragingalgorithm(xiao,2009),whichkeepstrackoftheonlineaveragegradientattimet:  gt=1tptt0=1gt0here,thesubgradientsdonotincludetermsfortheregularizer;theyarederiva-tivesoftheunregularizedobjective(  =0,  =0)1493

xlvkxvdakkxvdvkbsparse overcomplete vectorssparse, binary overcomplete vectorsprojectionsparse codingnon-negative sparse codinginitial dense vectorsfigure1:methodsforobtainingsparseovercompletevectors(top,methoda,  2.1)andsparse,binaryovercompletewordvectors(bottom,methodb,  2.2and  2.4).observeddensevectorsoflengthl(left)areconvertedtosparsenon-negativevectors(center)oflengthkwhicharethenprojectedintothebinaryvectorspace(right),wherel(cid:28)k.xisdense,aissparse,andbisthebinarywordvectormatrix.strengthofcolorssignifythemagnitudeofvalues;negativeisred,positiveisblue,andzeroiswhite.withrespecttoai.wede   ne  =   sign(  gt,i,j)  tpgt,i,j(|  gt,i,j|     ),wheregt,i,j=ptt0=1g2t0,i,j.now,usingtheav-eragegradient,the   1-regularizedobjectiveisop-timizedasfollows:at+1,i,j=(0,if|  gt,i,j|       ,otherwise(4)where,at+1,i,jisthejthelementofsparsevectoraiatthetthupdateand  gt,i,jisthecorrespond-ingaveragegradient.forobtainingnonnegativesparsevectorswetakeprojectionoftheupdatedaiontork   0bychoosingtheclosestpointinrk   0ac-cordingtoeuclideandistance(whichcorrespondstozeroingoutthenegativeelements):at+1,i,j=               0,if|  gt,i,j|     0,if  <0  ,otherwise(5)2.4binarizingtransformationouraimwithmethodbistoobtainwordrep-resentationsthatcanemulatethebinary-featurexl    k%sparseglove3001.010   5300091sg3000.510   5300092gc501.010   550098multi480.110   596093table1:hyperparametersforlearningsparseovercompletevectorstunedonthews-353task.tasksareexplainedin  b.thefourinitialvectorrepresentationsxareexplainedin  a.hot,fresh,   sh,1/2,wine,saltseries,tv,appearances,episodes1975,1976,1968,1970,1977,1969dress,shirt,ivory,shirts,pantsupscale,af   uent,catering,clienteletable2:highestfrequencywordsinrandomlypickedwordclustersofbinarysparseovercom-pleteglovevectors.spacedesignedforvariousnlptasks.wecould1494

statethisasanoptimizationproblem:argmind   rl  kb   {0,1}k  vvxi=1kxi   dbik22+  kbik11+  kdk22(6)wherebdenotesthebinary(andalsosparse)rep-resentation.thisisanmixedintegerbilinearpro-gram,whichisnp-hard(al-khayyalandfalk,1983).unfortunately,thenumberofvariablesintheproblemis   kvwhichreaches100millionwhenv=100,000andk=1,000,whichisintractabletosolveusingstandardtechniques.amoretractablerelaxationtothishardprob-lemisto   rstconstrainthecontinuousrepresen-tationatobenonnegative(i.e,ai   rk   0;  2.2).then,inordertoavoidanexpensivecomputation,wetakethenonnegativewordvectorsobtainedus-ingeq.3andprojectnonzerovaluesto1,preserv-ingthe0values.table2showsarandomsetofwordclustersobtainedby(i)applyingourmethodtogloveinitialvectorsand(ii)applyingid116id91(k=100).in  3wewill   ndthatthesevectorsperformwellquantitatively.2.5hyperparametertuningmethodsaandbhavethreehyperparameters:the   1-id173penalty  ,the   2-id173penalty  ,andthelengthoftheovercompletewordvectorrepresentationk.weperformagridsearchon     {0.1,0.5,1.0}andk   {10l,20l},se-lectingvaluesthatmaximizesperformanceonone   development   wordsimilaritytask(ws-353,dis-cussedin  b)whileachievingatleast90%sparsityinovercompletevectors.  wastunedononecol-lectionofinitializingvectors(glove,discussedin  a)sothatthevectorsindarenearunitnorm.thefourvectorrepresentationsandtheircorre-spondinghyperparametersselectedbythisproce-durearesummarizedintable1.therehyperpa-rameterswerechosenformethodaandretainedformethodb.3experimentsusingmethodsaandb,weconstructedsparseovercompletevectorrepresentationsa,startingfromfourinitialvectorrepresentationsx;theseareexplainedinappendixa.weusedonebench-markevaluation(ws-353)totunehyperparame-ters,resultinginthesettingsshownintable1;sevenothertaskswereusedtoevaluatethequalityofthesparseovercompleterepresentations.the   rstoftheseisawordsimilaritytask,wherethescoreiscorrelationwithhumanjudgments,andtheothersareclassi   cationaccuraciesofan   2-regularizedlogisticregressionmodeltrainedusingthewordvectors.thesetasksaredescribedinde-tailinappendixb.3.1effectsoftransformingvectorsfirst,wequantifytheeffectsofourtransforma-tionsbycomparingtheiroutputtotheinitial(x)vectors.table3showsconsistentimprovementsofsparsifyingvectors(methoda).theexceptionsareonthesiid113xtask,whereoursparsevectorsareworsethantheskip-graminitializerandonparwiththemultilingualinitializer.sparsi   cationisbene   cialacrossallofthetextclassi   cationtasks,forallinitialvectorrepresentations.onaverageacrossallvectortypesandalltasks,sparseover-completevectorsoutperformtheircorrespondinginitializersby4.2points.2binarizedvectors(frommethodb)arealsousu-allybetterthantheinitialvectors(alsoshownintable3),andtendtooutperformthesparsi   edvariants,exceptwheninitializingwithglove.onaverageacrossallvectortypesandalltasks,bina-rizedovercompletevectorsoutperformtheircor-respondinginitializersby4.8pointsandthecon-tinuous,sparseintermediatevectorsby0.6points.fromhereon,weexploremoredeeplythesparseovercompletevectorsfrommethoda(de-notedbya),leavingbinarizationandmethodbaside.3.2effectofvectorlengthhowdoesthelengthoftheovercompletevector(k)affectperformance?wefocushereontheglovevectors,wherel=300,andreportav-erageperformanceacrossalltasks.weconsiderk=  lwhere     {2,3,5,10,15,20}.figure2plotstheaverageperformanceacrosstasksagainst  .theearlierselectionofk=3,000(  =10)givesthebestresult;gainsaremonotonicin  tothatpointandthenbegintodiminish.3.3alternativetransformationsweconsidertwoalternativetransformations.the   rstpreservestheoriginalvectorlengthbut2wereportcorrelationona100pointscale,sothattheaveragewhichincludesaccuracuiesandcorrelationisequallyrepresentatitveofboth.1495

vectorssiid113xsenti.trecsportscomp.relig.npaveragecorr.acc.acc.acc.acc.acc.acc.glovex36.977.776.295.979.786.777.976.2a38.981.481.596.387.088.882.379.4b39.781.081.295.784.687.481.678.7sgx43.681.577.897.180.285.980.178.0a41.782.781.298.284.586.581.679.4b42.881.681.695.286.588.082.979.8gcx9.768.364.675.160.576.079.461.9a12.073.377.677.068.381.081.267.2b18.773.679.279.770.579.679.468.6multix28.775.563.883.664.381.879.268.1a28.178.679.293.978.284.581.174.8b28.777.682.094.781.485.681.975.9table3:performancecomparisonoftransformedvectorstoinitialvectorsx.weshowsparseover-completerepresentationsaandalsobinarizedrepresentationsb.initialvectorsarediscussedin  aandtasksin  b.figure2:averageperformaceacrossalltasksforsparseovercompletevectors(a)producedbygloveinitialvectors,asafunctionoftheratioofktol.achievesabinary,sparsevector(b)byapplying:bi,j=(cid:26)1ifxi,j>00otherwise(7)thesecondtransformationwasproposedbyguoetal.(2014).here,theoriginalvectorlengthisalsopreserved,butsparsityisachievedthrough:ai,j=         1ifxi,j   m+   1ifxi,j   m   0otherwise(8)wherem+(m   )isthemeanofpositive-valued(negative-valued)elementsofx.thesevectorsare,obviously,notbinary.we   ndthatonaverage,acrossinitializingvec-torsandacrossalltasksthatoursparseovercom-plete(a)vectorsleadtobetterperformancethaneitherofthealternativetransformations.4interpretabilityourhypothesisisthatthedimensionsofsparseovercompletevectorsaremoreinterpretablethanthoseofdensewordvectors.followingmurphyetal.(2012),weuseawordintrusionexperiment(changetal.,2009)tocorroboratethishypothesis.inaddition,weconductqualitativeanalysisofin-terpretability,focusingonindividualdimensions.4.1wordintrusionwordintrusionexperimentsseektoquantifytheextenttowhichdimensionsofalearnedwordrep-resentationarecoherenttohumans.inonein-stanceoftheexperiment,ahumanjudgeispre-sentedwith   vewordsinrandomorderandaskedtoselectthe   intruder.   thewordsareselectedbytheexperimenterbychoosingonedimensionjofthelearnedrepresentation,thenrankingthewordsonthatdimensionalone.thedimensionsarecho-senindecreasingorderofthevarianceoftheirvaluesacrossthevocabulary.fourofthewordsarethetop-rankedwordsaccordingtoj,andthe   true   intruderisawordfromthebottomhalfofthelist,chosentobeawordthatappearsinthetop10%ofsomeotherdimension.anexampleofaninstanceis:naval,industrial,technological,marine,identity1496

x:glovesggcmultiaveragex76.278.061.968.171.0eq.775.775.860.564.169.0eq.8(guoetal.,2014)75.876.960.566.269.8a79.479.467.274.875.2table4:averageperformanceacrossalltasksandvectormodelsusingdifferenttransformations.vectorsa1a2a3avg.iaa  x61535657700.40a71707271770.45table5:accuracyofthreehumanannotatorsonthewordintrusiontask,alongwiththeaverageinter-annotatoragreement(artsteinandpoesio,2008)andfleiss     (daviesandfleiss,1982).(thelastwordistheintruder.)weformedinstancesfrominitializingvectorsandfromoursparseovercompletevectors(a).eachofthesetwocombinesthefourdifferentini-tializersx.weselectedthe25dimensionsdineachcase.eachofthe100instancespercondition(initialvs.sparseovercomplete)wasgiventothreejudges.resultsintable5con   rmthatthesparseover-completevectorsaremoreinterpretablethanthedensevectors.theinter-annotatoragreementonthesparsevectorsincreasessubstantially,from57%to71%,andthefleiss     increasesfrom   fair   to   moderate   agreement(landisandkoch,1977).4.2qualitativeevaluationofinterpretabilityifavectordimensionisinterpretable,thetop-rankingwordsforthatdimensionshoulddisplaysemanticorsyntacticgroupings.toverifythisqualitatively,weselect   vedimensionswiththehighestvarianceofvaluesininitialandsparsi-   edgcvectors.wecomparetop-rankedwordsinthedimensionsextractedfromthetworepresenta-tions.thewordsarelistedintable6,adimensionperrow.subjectively,we   ndthesemanticgroup-ingsbetterinthesparsevectorsthanintheinitialvectors.figure3visualizesthesparsi   edgcvectorsforsixwords.thedimensionsaresortedbytheaver-agevalueacrossthethree   animal   vectors.theanimal-relatedwordsusemanyofthesamedi-mensions(102commonactivedimensionsoutof500total);inconstrast,thethreecitynamesusexcombat,guard,honor,bow,trim,naval   ll,could,faced,lacking,seriously,scoredsee,n   t,recommended,depending,partdue,positive,equal,focus,respect,bettersergeant,comments,critics,she,videosafracture,breathing,wound,tissue,reliefrelationships,connections,identity,relations   les,bills,titles,collections,poems,songsnaval,industrial,technological,marinestadium,belt,championship,toll,ride,coachtable6:top-rankedwordsperdimensionforini-tialandsparsi   edgcrepresentations.eachlineshowswordsfromadifferentdimension.mostlydistinctvectors.5relatedworktothebestofourknowledge,therehasbeennopriorworkonobtainingovercompletewordvec-torrepresentationsthataresparseandcategorical.however,overcompletefeatureshavebeenwidelyusedinimageprocessing,computervision(ol-shausenandfield,1997;lewickiandsejnowski,2000)andsignalprocessing(donohoetal.,2006).nonnegativematrixfactorizationisoftenusedforinterpretablecodingofinformation(leeandse-ung,1999;liuetal.,2003;cichockietal.,2009).sparsityconstraintsareingeneralusefulinnlpproblems(kazamaandtsujii,2003;friedmanetal.,2008;goodman,2004),likepostagging(ganchevetal.,2009),dependencyparsing(mar-tinsetal.,2011),textclassi   cation(yogatamaandsmith,2014),andrepresentationlearning(ben-gioetal.,2013).includingsparsityconstraintsinbayesianmodelsoflexicalsemanticslikeldaintheformofsparsedirichletpriorshasbeenshowntobeusefulfordownstreamtaskslikepos-tagging(toutanovaandjohnson,2007),andim-provinginterpretation(paulanddredze,2012;zhuandxing,2012).1497

v379v353v76v186v339v177v114v342v332v270v222v91v303v473v355v358v164v348v324v192v24v281v82v46v277v466v465v128v11v413v98v131v445v199v475v208v431v299v357v149v80v247v231v42v44v376v152v74v254v141v341v349v234v55v477v272v217v457v57v159v223v310v436v325v211v117v360v483v363v439v403v119v329v83v371v424v179v214v268v38v102v93v89v12v172v173v285v344v78v227v426v430v241v384v460v347v171v289v380v8v2v3v5v6v7v10v14v15v16v17v18v19v20v21v22v25v26v28v29v30v31v32v33v35v36v37v39v40v41v43v45v47v49v50v51v52v54v56v58v59v60v63v64v65v67v68v69v70v72v75v77v81v87v90v92v94v99v101v103v105v106v108v110v111v116v118v122v123v125v130v132v133v136v137v138v139v140v143v144v147v148v150v155v158v160v162v165v166v167v168v169v170v174v175v178v180v181v182v183v185v188v189v190v191v193v194v195v196v202v203v204v205v212v213v215v218v220v224v226v228v232v233v235v236v238v239v240v242v243v244v248v249v250v251v252v253v255v258v259v260v261v262v263v264v265v266v271v273v274v278v282v284v287v288v290v292v293v294v296v300v302v304v307v308v311v312v313v314v316v317v318v319v320v321v322v323v327v330v331v333v334v336v338v340v343v345v346v352v356v361v362v366v368v369v370v372v373v375v377v378v381v382v383v385v386v387v388v389v390v391v392v394v395v396v398v399v400v401v402v404v405v406v407v408v409v410v412v414v415v416v417v418v419v420v422v423v425v427v428v429v433v434v435v437v441v442v444v446v449v450v451v452v453v455v456v458v459v461v462v463v464v467v468v469v471v472v478v479v480v481v482v484v485v486v488v489v490v491v492v493v494v495v497v499v500v501v487v200v326v4v121v267v230v438v134v97v104v351v219v13v88v129v286v229v350v96v107v153v145v154v34v301v374v109v397v156v161v297v115v151v245v447v53v337v79v448v283v443v201v393v365v48v126v257v246v295v120v367v27v184v209v306v269v124v470v112v187v62v474v354v454v279v146v275v221v207v71v335v73v85v440v95v23v225v411v328v305v198v163v9v135v315v142v498v291v86v476v210v359v84v100v309v176v216v432v206v421v276v237v61v157v364v127v66v256v280v113v298v197v496bostonseattlechicagodoghorsefishfigure3:visualizationofsparsi   edgcvectors.negativevaluesarered,positivevaluesareblue,zeroesarewhite.6conclusionwehavepresentedamethodthatconvertswordvectorsobtainedusinganystate-of-the-artwordvectormodelintosparseandoptionallybinarywordvectors.thesetransformedvectorsappeartocomeclosertofeaturesusedinnlptasksandout-performtheoriginalvectorsfromwhichtheyarederivedonasuiteofsemanticsandsyntacticeval-uationbenchmarks.wealso   ndthatthesparsevectorsaremoreinterpretablethanthedensevec-torsbyhumansaccordingtoawordintrusionde-tectiontest.acknowledgmentswethankalonafyshefordiscussionsonvec-torinterpretabilityandthreeanonymousreview-ersfortheirfeedback.thisresearchwassup-portedinpartbythenationalsciencefoundationthroughgrantiis-1251131andthedefensead-vancedresearchprojectsagencythroughgrantfa87501420244.thisworkwassupportedinpartbytheu.s.armyresearchlaboratoryandtheu.s.armyresearchof   ceundercontract/grantnumberw911nf-10-1-0533.ainitialvectorrepresentations(x)ourexperimentsconsiderfourpubliclyavailablecollectionsofpre-trainedwordvectors.theyvaryintheamountofdatausedandtheestimationmethod.glove.globalvectorsforwordrepresentations(penningtonetal.,2014)aretrainedonaggregatedglobalword-wordco-occurrencestatisticsfromacorpus.thesevectorsweretrainedon6billionwordsfromwikipediaandenglishgigawordandareoflength300.33http://www-nlp.stanford.edu/projects/glove/skip-gram(sg).theid97tool(mikolovetal.,2013)isfastandwidely-used.inthismodel,eachword   shuffmancodeisusedasaninputtoalog-linearclassi   erwithacontinuousprojectionlayerandwordswithinagivencontextwindowarepredicted.thesevectorsweretrainedon100bil-lionwordsofgooglenewsdataandareoflength300.4globalcontext(gc).thesevectorsarelearnedusingarecursiveneuralnetworkthatincorporatesbothlocalandglobal(document-level)contextfeatures(huangetal.,2012).thesevectorsweretrainedonthe   rst1billionwordsofenglishwikipediaandareoflength50.5multilingual(multi).faruquianddyer(2014)learnedvectorsby   rstperformingsvdontextindifferentlanguages,thenapplyingcanonicalcorrelationanalysisonpairsofvectorsforwordsthataligninparallelcorpora.thesevectorsweretrainedonwmt-2011newscorpuscontaining360millionwordsandareoflength48.6bevaluationbenchmarksourcomparisonsofwordvectorqualityconsider   vebenchmarktasks.wenowdescribethediffer-entevaluationbenchmarksforwordvectors.wordsimilarity.weevaluateourwordrepre-sentationsontwowordsimilaritytasks.the   rstisthews-353dataset(finkelsteinetal.,2001),whichcontains353pairsofenglishwordsthathavebeenassignedsimilarityratingsbyhumans.thisdatasetisusedtotunesparsevectorlearninghyperparameters(  2.5),whiletheremainingofthetasksdiscussedinthissectionarecompletelyheldout.4https://code.google.com/p/id975http://nlp.stanford.edu/  socherr/acl2012_wordvectorstextfile.zip6http://cs.cmu.edu/  mfaruqui/soft.html1498

amorerecentdataset,siid113x-999(hilletal.,2014),hasbeenconstructedtospeci   callyfocusonsimilarity(ratherthanrelatedness).itcon-tainsabalancedsetofnoun,verb,andadjectivepairs.wecalculatecosinesimilaritybetweenthevectorsoftwowordsformingatestitemandre-portspearman   srankcorrelationcoef   cient(my-ersandwell,1995)betweentherankingspro-ducedbyourmodelagainstthehumanrankings.sentimentanalysis(senti).socheretal.(2013)createdatreebankofsentencesanno-tatedwith   ne-grainedsentimentlabelsonphrasesandsentencesfrommoviereviewexcerpts.thecoarse-grainedtreebankofpositiveandnegativeclasseshasbeensplitintotraining,development,andtestdatasetscontaining6,920,872,and1,821sentences,respectively.weuseaverageofthewordvectorsofagivensentenceasfeatureforclassi   cation.theclassi   eristunedonthedev.setandaccuracyisreportedonthetestset.questionclassi   cation(trec).asanaidtoquestionanswering,aquestionmaybeclassi-   edasbelongingtooneofmanyquestiontypes.thetrecquestionsdatasetinvolvessixdiffer-entquestiontypes,e.g.,whetherthequestionisaboutalocation,aboutaperson,oraboutsomenu-mericinformation(liandroth,2002).thetrain-ingdatasetconsistsof5,452labeledquestions,andthetestdatasetconsistsof500questions.anav-erageofthewordvectorsoftheinputquestionisusedasfeaturesandaccuracyisreportedonthetestset.20newsgroupdataset.weconsiderthreebi-narycategorizationtasksfromthe20news-groupsdataset.7eachtaskinvolvescategoriz-ingadocumentaccordingtotworelatedcate-gorieswithtraining/dev./testsplitinaccordancewithyogatamaandsmith(2014):(1)sports:baseballvs.hockey(958/239/796)(2)comp.:ibmvs.mac(929/239/777)(3)religion:atheismvs.christian(870/209/717).weuseaverageofthewordvectorsofagivensentenceasfeatures.theclassi   eristunedonthedev.setandaccuracyisreportedonthetestset.npbracketing(np).lazaridouetal.(2013)constructedadatasetfromthepenntreebank(marcusetal.,1993)ofnounphrases(np)of7http://qwone.com/  jason/20newsgroupslengththreewords,wherethe   rstcanbeanad-jectiveoranounandtheothertwoarenouns.thetaskistopredictthecorrectbracketingintheparsetreeforagivennounphrase.forexample,local(phonecompany)and(bloodpressure)medicineexhibitrightandleftbracketing,respectively.weappendthewordvectorsofthethreewordsinthenpinorderandusethemasfeaturesforbinaryclassi   cation.thedatasetcontains2,227nounphrasessplitinto10folds.theclassi   eristunedonthe   rstfoldandcross-validationaccuracyisreportedontheremainingninefolds.referencesfaiza.al-khayyalandjamese.falk.1983.jointlyconstrainedbiconvexprogramming.mathematicsofoperationsresearch,pages273   286.ronartsteinandmassimopoesio.2008.inter-coderagreementforcomputationallinguistics.computa-tionallinguistics,34(4):555   596.collinf.baker,charlesj.fillmore,andjohnb.lowe.1998.theberkeleyframenetproject.inproc.ofacl.mohitbansal,kevingimpel,andkarenlivescu.2014.tailoringcontinuouswordrepresentationsfordependencyparsing.inproc.ofacl.yoshuabengio,aaroncourville,andpascalvincent.2013.representationlearning:areviewandnewperspectives.ieeetransactionsonpatternanaly-sisandmachineintelligence,35(8):1798   1828.jonathanchang,seangerrish,chongwang,jordanl.boyd-graber,anddavidm.blei.2009.readingtealeaves:howhumansinterprettopicmodels.innips.andrzejcichocki,rafalzdunek,anhhuyphan,andshun-ichiamari.2009.nonnegativematrixandtensorfactorizations:applicationstoexploratorymulti-waydataanalysisandblindsourcesepara-tion.johnwiley&sons.markdaviesandjosephlfleiss.1982.measuringagreementformultinomialdata.biometrics,pages1047   1051.davidl.donoho,michaelelad,andvladimirn.temlyakov.2006.stablerecoveryofsparseover-completerepresentationsinthepresenceofnoise.ieeetransactionsoninformationtheory,52(1).johnduchi,eladhazan,andyoramsinger.2010.adaptivesubgradientmethodsforonlinelearn-ingandstochasticoptimization.technicalreporteecs-2010-24,universityofcaliforniaberkeley.1499

johnc.duchi,alekhagarwal,andmartinj.wain-wright.2012.dualaveragingfordistributedopti-mization:convergenceanalysisandnetworkscal-ing.ieeetransactionsonautomaticcontrol,57(3):592   606.manaalfaruquiandchrisdyer.2014.improvingvectorspacewordrepresentationsusingmultilingualcorrelation.inproc.ofeacl.manaalfaruqui,jessedodge,sujayk.jauhar,chrisdyer,eduardhovy,andnoaha.smith.2015.retro   ttingwordvectorstosemanticlexicons.inproc.ofnaacl.levfinkelstein,evgeniygabrilovich,yossimatias,ehudrivlin,zachsolan,gadiwolfman,andey-tanruppin.2001.placingsearchincontext:theconceptrevisited.inproc.ofwww.jeromefriedman,trevorhastie,androberttibshi-rani.2008.sparseinversecovarianceestimationwiththegraphicallasso.biostatistics,9(3):432   441.alonafyshe,parthap.talukdar,brianmurphy,andtomm.mitchell.2014.interpretablesemanticvec-torsfromajointmodelofbrain-andtext-basedmeaning.inproc.ofacl.alonafyshe,leilawehbe,parthap.talukdar,brianmurphy,andtomm.mitchell.2015.acomposi-tionalandinterpretablesemanticspace.inproc.ofnaacl.kuzmanganchev,bentaskar,fernandopereira,andjo  aogama.2009.posteriorvs.parametersparsityinlatentvariablemodels.innips.xavierglorot,antoinebordes,andyoshuabengio.2011.domainadaptationforlarge-scalesentimentclassi   cation:adeeplearningapproach.inproc.oficml.joshuagoodman.2004.exponentialpriorsformaxi-mumid178models.inproc.ofnaacl.e.grefenstette.2013.towardsaformaldistributionalsemantics:simulatinglogicalcalculiwithtensors.arxiv:1304.5823.jiangguo,wanxiangche,haifengwang,andtingliu.2014.revisitingembeddingfeaturesforsim-plesemi-supervisedlearning.inproc.ofemnlp.georgheigold,erikmcdermott,vincentvanhoucke,andrewsenior,andmichielbacchiani.2014.asynchronousstochasticoptimizationforsequencetrainingofdeepneuralnetworks.inproc.oficassp.felixhill,roireichart,andannakorhonen.2014.siid113x-999:evaluatingsemanticmodelswith(gen-uine)similarityestimation.corr,abs/1408.3456.patriko.hoyer.2002.non-negativesparsecoding.inneuralnetworksforsignalprocessing,2002.proc.ofieeeworkshopon.erich.huang,richardsocher,christopherd.man-ning,andandrewy.ng.2012.improvingwordrepresentationsviaglobalcontextandmultiplewordprototypes.inproc.ofacl.jun   ichikazamaandjun   ichitsujii.2003.evaluationandextensionofmaximumid178modelswithin-equalityconstraints.inproc.ofemnlp.douwekielaandstephenclark.2013.detectingcompositionalityofmulti-wordexpressionsusingnearestneighboursinvectorspacemodels.inproc.ofemnlp.j.richardlandisandgaryg.koch.1977.themea-surementofobserveragreementforcategoricaldata.biometrics,33(1):159   174.angelikilazaridou,evamariavecchi,andmarcobaroni.2013.fishtransportersandmiraclehomes:howcompositionaldistributionalsemanticscanhelpnpparsing.inproc.ofemnlp.danield.leeandh.sebastianseung.1999.learningthepartsofobjectsbynon-negativematrixfactoriza-tion.nature,401(6755):788   791.honglaklee,alexisbattle,rajatraina,andan-drewy.ng.2006.ef   cientsparsecodingalgo-rithms.innips.honglaklee,rajatraina,alexteichman,andan-drewy.ng.2009.exponentialfamilysparsecod-ingwithapplicationtoself-taughtlearning.inproc.ofijcai.bethlevin.1993.englishverbclassesandalter-nations:apreliminaryinvestigation.universityofchicagopress.michaellewickiandterrencesejnowski.2000.learningovercompleterepresentations.neuralcomputation,12(2):337   365.mikelewisandmarksteedman.2013.combineddistributionalandlogicalsemantics.transactionsoftheacl,1:179   192.mikelewisandmarksteedman.2014.combiningformalanddistributionalmodelsoftemporalandin-tensionalsemantics.inproc.ofacl.xinlianddanroth.2002.learningquestionclassi-   ers.inproc.ofcoling.weixiangliu,nanningzheng,andxiaofenglu.2003.non-negativematrixfactorizationforvisualcoding.inproc.oficassp.mitchellp.marcus,maryannmarcinkiewicz,andbeatricesantorini.1993.buildingalargeanno-tatedcorpusofenglish:thepenntreebank.compu-tationallinguistics,19(2):313   330.1500

andr  ef.t.martins,noaha.smith,pedrom.q.aguiar,andm  arioa.t.figueiredo.2011.struc-turedsparsityinstructuredprediction.inproc.ofemnlp.tomasmikolov,kaichen,gregcorrado,andjeffreydean.2013.ef   cientestimationofwordrepresen-tationsinvectorspace.georgea.miller.1995.id138:alexicaldatabaseforenglish.communicationsoftheacm,38(11):39   41.brianmurphy,parthatalukdar,andtommitchell.2012.learningeffectiveandinterpretableseman-ticmodelsusingnon-negativesparseembedding.inproc.ofcoling.jeromel.myersandarnoldd.well.1995.researchdesign&statisticalanalysis.routledge.brunoa.olshausenanddavidj.field.1997.sparsecodingwithanovercompletebasisset:astrategyemployedbyv1?visionresearch,37(23):3311   3325.denispaperno,nghiathepham,andmarcobaroni.2014.apracticalandlinguistically-motivatedap-proachtocompositionaldistributionalsemantics.inproc.ofacl.michaelpaulandmarkdredze.2012.factoriallda:sparsemulti-dimensionaltextmodels.innips.jeffreypennington,richardsocher,andchristo-pherd.manning.2014.glove:globalvectorsforwordrepresentation.inproc.ofemnlp.karinkipperschuler.2005.verbnet:abroad-coverage,comprehensiveverblexicon.ph.d.the-sis,universityofpennsylvania.richardsocher,alexperelygin,jeanwu,jasonchuang,christopherd.manning,andrewy.ng,andchristopherpotts.2013.recursivedeepmod-elsforsemanticcompositionalityoverasentimenttreebank.inproc.ofemnlp.kristinatoutanovaandmarkjohnson.2007.abayesianlda-basedmodelforsemi-supervisedpart-of-speechtagging.innips.peterd.turneyandpatrickpantel.2010.fromfre-quencytomeaning:vectorspacemodelsofseman-tics.jair,37(1):141   188.peterd.turney.2001.miningthewebforsynonyms:pmi-irversuslsaontoefl.inproc.ofecml.evamariavecchi,robertozamparelli,andmarcoba-roni.2013.studyingtherecursivebehaviourofadjectivalmodi   cationwithcompositionaldistribu-tionalsemantics.inproc.ofemnlp.linxiao.2009.dualaveragingmethodsforregular-izedstochasticlearningandonlineoptimization.innips.changxu,yalongbai,jiangbian,bingao,gangwang,xiaoguangliu,andtie-yanliu.2014.rc-net:ageneralframeworkforincorporatingknowl-edgeintowordrepresentations.inproc.ofcikm.daniyogatamaandnoahasmith.2014.linguisticstructuredsparsityintextcategorization.inproc.ofacl.moyuandmarkdredze.2014.improvinglexicalembeddingswithsemanticknowledge.inproc.ofacl.junzhuandericpxing.2012.sparsetopicalcoding.arxiv:1202.3778.