deep compositional id53 with neural module networks

jacob andreas marcus rohrbach

trevor darrell

dan klein

department of electrical engineering and computer sciences

university of california, berkeley

{jda,rohrbach,trevor,klein}@{cs,eecs,eecs,cs}.berkeley.edu

7
1
0
2

 
l
u
j
 

4
2

 
 
]

v
c
.
s
c
[
 
 

4
v
9
9
7
2
0

.

1
1
5
1
:
v
i
x
r
a

abstract

visual id53 is fundamentally composi-
tional in nature   a question like where is the dog? shares
substructure with questions like what color is the dog? and
where is the cat? this paper seeks to simultaneously exploit
the representational capacity of deep networks and the com-
positional linguistic structure of questions. we describe a
procedure for constructing and learning neural module net-
works, which compose collections of jointly-trained neural
   modules    into deep networks for id53. our
approach decomposes questions into their linguistic sub-
structures, and uses these structures to dynamically instan-
tiate modular networks (with reusable components for rec-
ognizing dogs, classifying colors, etc.). the resulting com-
pound networks are jointly trained. we evaluate our ap-
proach on two challenging datasets for visual question an-
swering, achieving state-of-the-art results on both the vqa
natural image dataset and a new dataset of complex ques-
tions about abstract shapes.

1. introduction

this paper describes an approach to visual question an-
swering based on neural module networks (nmns). we an-
swer natural language questions about images using collec-
tions of jointly-trained neural    modules   , dynamically com-
posed into deep networks based on linguistic structure.

concretely, given an image and an associated question
(e.g. where is the dog?), we wish to predict a corresponding
answer (e.g. on the couch, or perhaps just couch) (figure 1).
the visual qa task has signi   cant signi   cant applications
to id176, search, and accessibility, and
has been the subject of a great deal of recent research at-
tention [2, 7, 20, 22, 25, 32]. the task requires sophisti-
cated understanding of both visual scenes and natural lan-
guage. recent successful approaches represent questions
as bags of words, or encode the question using a recurrent
neural network [22] and train a simple classi   er on the en-
coded question and image. in contrast to these monolithic

figure 1: a schematic representation of our proposed
model   the shaded gray area is a neural module network of
the kind introduced in this paper. our approach uses a nat-
ural language parser to dynamically lay out a deep network
composed of reusable modules. for visual question answer-
ing tasks, an additional sequence model provides sentence
context and learns common-sense knowledge.

approaches, another line of work for textual qa [18] and
image qa [21] uses semantic parsers to decompose ques-
tions into logical expressions. these logical expressions
are evaluated against a purely logical representation of the
world, which may be provided directly or extracted from an
image [16].

in this paper we draw from both lines of research,
presenting a technique for integrating the representational
power of neural networks with the    exible compositional
structure afforded by symbolic approaches to semantics.
rather than relying on a monolithic network structure to
answer all questions, our approach assembles a network on
the    y from a collection of specialized, jointly-learned mod-
ules (figure 1). rather than using logic to reason over truth
values, we remain entirely in the domain of visual features
and attentions.

our approach    rst analyzes each question with a seman-
tic parser, and uses this analysis to determine the basic com-

wherecountcolor...dogstanding...lstmcouchcatid98where is the dog?layoutparserputational units (attention, classi   cation, etc.) needed to
answer the question, as well as the relationships between
the modules. in figure 1, we    rst produce an attention fo-
cused on the dog, which passes its output to a location clas-
si   er. depending on the underlying structure, these mes-
sages passed between modules may be raw image features,
attentions, or classi   cation decisions; each module is de-
termined by its input and output types. different kinds of
modules are shown in different colors; attention modules
(like dog) are shown in green, while labeling modules (like
where) are shown in blue. importantly, all modules in an
nmn are independent and composable, which allows the
computation to be different for each problem instance, and
possibly unobserved during training. outside the nmn, our
   nal answer uses a recurrent network (lstm) to read the
question, which has been shown to be important to model
common sense knowledge and dataset biases [22].

we evaluate our approach on two visual question an-
swering tasks. on the recently-released vqa [2] datasets
we achieve results comparable to or better than existing ap-
proaches, and show that our approach speci   cally outper-
forms previous work on questions with compositional struc-
ture (e.g. requiring that an object be located and one of its
attributes described). it turns out, however, that many of the
questions in both datasets are quite simple, with little com-
position or reasoning required. to test our approach   s abil-
ity to handle harder questions, we introduce a new dataset
of synthetic images paired with complex questions involv-
ing spatial relations, set-theoretic reasoning, and shape and
attribute recognition. on this dataset we outperform com-
peting approaches by as much as 25% absolute accuracy.

while all the applications considered in this paper in-
volve visual id53, the general architecture is
potentially of broader usefulness, and might be more gener-
ally applied to visual referring expression resolution [6] or
id53 about natural language texts [11].

to summarize our contributions: we    rst describe neural
module networks, a general architecture for discretely com-
posing heterogeneous, jointly-trained neural modules into
deep networks. next, for the visual qa task speci   cally,
we show how to construct nmns based on the output of
a semantic parser, and use these to successfully complete
established visual id53 tasks. finally, we in-
troduce a new dataset of challenging, highly compositional
questions about abstract shapes, and show that our model
again outperforms previous approaches. we will release
this dataset, as well as code for all systems described in this
paper, upon publication.

2. motivations

we begin with two simple observations. first, state-of-
the-art performance on the full range of id161
tasks that are studied requires a variety of different deep

network topologies   there is no single    best network    for
all tasks. second, though different networks are used for
different purposes, it is commonplace to initialize systems
for many of vision tasks with a pre   x of a network trained
for classi   cation [9]. this has been shown to substantially
reduce training time and improve accuracy.

so while network structures are not universal (in the
sense that the same network is appropriate for all problems),
they are at least empirically modular (in the sense that in-
termediate representations for one task are useful for many
others).

can we generalize this idea in a way that is useful for
id53? rather than thinking of question an-
swering as a problem of learning a single function to map
from questions and contexts to answers, it   s perhaps useful
to think of it as a highly-multitask learning setting, where
each problem instance is associated with a novel task, and
the identity of that task is expressed only noisily in lan-
guage. in particular, where a simple question like is this a
truck? requires us to retrieve only one piece of information
from an image, more complicated questions, like how many
objects are to the left of the toaster? might require multi-
ple processing steps. the compositional nature of language
means that the number of such processing such steps is po-
tentially unbounded. moreover, multiple kinds of process-
ing might be required   repeated convolutions might iden-
tify a truck, but some kind of recurrent architecture is likely
necessary to count up to arbitrary numbers.

thus our goal in this paper is to specify a framework
for modular, composable, jointly-trained neural networks.
in this framework, we    rst predict the structure of the
computation needed to answer each question individually,
then realize this structure by constructing an appropriately-
shaped neural network from an inventory of reusable mod-
ules. these modules are learned jointly, rather than trained
in isolation, and specialization to individual tasks (identify-
ing properties, spatial relations, etc.) arises naturally from
the training objective.

3. related work

we consider three lines of related work: previous efforts
toward visual id53, discrete models for com-
positional semantics, and models that are structurally simi-
lar to neural module networks.

visual id53 answering questions about
images is sometimes referred to as a    visual turing test   
[21, 8].
it has only recently gained popularity, following
the emergence of appropriate datasets consisting of paired
images, questions, and answers. while the daquar
dataset [21] is restricted to indoor scenes and contains rel-
atively few examples, the cocoqa dataset [32] and the

vqa dataset [2] are signi   cantly larger and have more vi-
sual variety. both are based on images from the coco
dataset [19]. while cocoqa contains question-answer
pairs automatically generated from the descriptions asso-
ciated with the coco dataset, [2] has crowed sourced
questions-answer pairs. we evaluate our approach on vqa,
the larger and more natural of the two datasets.

notable    classical    approaches to this task include [21,
16]. both of these approaches are similar to ours in their use
of a semantic parser, but both rely on    xed logical id136
rather than learned compositional operations.

several neural models for visual questioning have al-
ready been proposed in the literature [25, 20, 7], all of which
use standard deep sequence modeling machinery to con-
struct a joint embedding of image and text, which is im-
mediately mapped to a distribution over answers. here we
attempt to more explicitly model the computational process
needed to produce each answer, but bene   t from techniques
for producing sequence and image embeddings that have
been important in previous work.

one important component of visual questioning is
grounding the question in the image. this grounding task
has previously been approached in [13, 24, 12, 15], where
the authors tried to localize phrases in an image. [31] use an
attention mechanism, to predict a heatmap for each word,
as an auxiliary task, during sentence generation. the at-
tentional component of our model is inspired by these ap-
proaches.

general id152 there is a large lit-
erature on learning to answer questions about structured
id99s from question   answer pairs,
both with and without joint learning of meanings for sim-
ple predicates [18, 16]. outside of id53, sev-
eral models have been proposed for instruction following
that impose a discrete    planning structure    over an under-
lying continuous control signal [1, 23]. we are unaware of
past use of a semantic parser to predict network structures,
or more generally to exploit the natural similarity between
set-theoretic approaches to classical id29 and
attentional approaches to id161.

neural network architectures the idea of selecting a
different network graph for each input datum is fundamen-
tal to both recurrent networks (where the network grows in
the length of the input) [5] and id56s
(where the network is built, e.g., according to the syntactic
structure of the input) [28]. but both of these approaches
ultimately involve repeated application of a single com-
putational module (e.g. an lstm [10] or gru [3] cell).
from another direction, some kinds of memory networks
[30] may be viewed as a special case of our model with
a    xed computational graph, consisting of a sequence of

attend modules followed by a classify module (see sec-
tion 4 below).

our basic contribution is in both assembling this graph
on the    y, and simultaneously in allowing the nodes to per-
form heterogeneous computations, with for    messages    of
different kinds   raw image features, attentions, classi   ca-
tion predictions   passed from one module to the next. we
are unaware of any previous work allowing such mixed col-
lections of modules to be trained jointly.

4. neural module networks for visual qa

each training datum for this task can be thought of as a

3-tuple (w, x, y), where

    w is a natural-language question
    x is an image
    y is an answer

a model is fully speci   ed by a collection of modules {m},
each with associated parameters   m, and a network layout
predictor p which maps from strings to networks. given
(w, x) as above, the model instantiates a network based on
p (w), passes x (and possibly w again) as inputs, and ob-
tains a distribution over labels (for the vqa task, we re-
quire the output module to be a classi   er). thus a model
ultimately encodes a predictive distribution p(y | w, x;   ).
in the remainder of this section, we describe the set of
modules used for the vqa task, then explain the process
by which questions are converted to network layouts.
4.1. modules

our goal here is to identify a small set of modules that
can be assembled into all the con   gurations necessary for
our tasks. this corresponds to identifying a minimal set
of composable vision primitives. the modules operate on
three basic data types:
images, unnormalized attentions,
and labels. for the particular task and modules described
in this paper, almost all interesting compositional phenom-
ena occur in the space of attentions, and it is not unreason-
able to characterize our contribution more narrowly as an
   attention-composition    network. nevertheless, other types
may be easily added in the future (for new applications or
for greater coverage in the vqa domain).

first,

width

font,

and are of

some notation: module names are typeset
in a fixed
the form
type[instance](arg1, . . . ). type is a high-level module
type (attention, classi   cation, etc.) of the kind described in
this section. instance is the particular instance of the model
under consideration   for example, attend[red] locates red
things, while attend[dog] locates dogs. weights may be
shared at both the type and instance level. modules with no
arguments implicitly take the image as input; higher-level
arguments may also inspect the image.

attention

classi   cation

attend : image     attention

classify : image    attention     label

an attention module attend[c] convolves every position
in the input image with a weight vector (distinct for each
c) to produce a heatmap or unnormalized attention. so,
for example, the output of the module attend[dog] is a
matrix whose entries should be in regions of the image
containing cats, and small everywhere else, as shown above.

re-attention

re-attend : attention     attention

a classi   cation module classify[c] takes an attention and
the input image and maps them to a distribution over labels.
for example, classify[color] should return a distribution
over colors in the region attended to.

measurement

measure : attention     label

a re-attention module re-attend[c] is essentially just a
multilayer id88 with recti   ed nonlinearities (relus),
performing a fully-connected mapping from one attention
to another. again, the weights for this mapping are distinct
for each c. so re-attend[above] should take an attention
and shift the regions of greatest activation upward (as
above), while re-attend[not] should move attention away
from the active regions. for the experiments in this paper,
the    rst fully-connected (fc) layer produces a vector of
size 32, and the second is the same size as the input.

combination

combine : attention    attention     attention

a combination module combine[c] merges two attentions
into a single attention. for example, combine[and] should
be active only in the regions that are active in both inputs,
while combine[except] should be active where the    rst in-
put is active and the second is inactive.

a measurement module measure[c] takes an attention alone
and maps it to a distribution over labels. because atten-
tions passed between modules are unnormalized, measure is
suitable for evaluating the existence of a detected object, or
counting sets of objects.

4.2. from strings to networks

having built up an inventory of modules, we now need
to assemble them into the layout speci   ed by the question.
the transformation from a natural language question to an
instantiated neural network takes place in two steps. first
we map from natural language questions to layouts, which
specify both the set of modules used to answer a given ques-
tion, and the connections between them. next we use these
layouts are used to assemble the    nal prediction networks.
we use standard tools pre-trained on existing linguis-
tic resources to obtained structured representations of ques-
tions. future work might focus on learning (or at least    ne-
tuning) this prediction process jointly with the rest of the
system.

parsing we begin by parsing each question with the stan-
ford parser [14]. to obtain a universal dependency represen-
tation [4]. dependency parses express grammatical rela-
tions between parts of a sentence (e.g. between objects and
their attributes, or events and their participants), and pro-
vide a lightweight abstraction away from the surface form

convolutionattend[dog]fcrelure-attend[above]  2stackconv.relucombine[except]attendfcsoftmaxcouchfcrelusoftmaxyesfc(a) nmn for answering the question what color is his
tie? the attend[tie] module    rst predicts a heatmap
corresponding to the location of the tie. next, the
classify[color] module uses this heatmap to pro-
duce a weighted average of image features, which are
   nally used to predict an output label.

(b) nmn for answering the question is there a red shape above a cir-
cle? the two attend modules locate the red shapes and circles,
the
re-attend[above] shifts the attention above the circles, the combine mod-
ule computes their intersection, and the measure[is] module inspects the
   nal attention and determines that it is non-empty.

figure 2: sample nmns for id53 about natural images and shapes. for both examples, layouts, attentions,
and answers are real predictions made by our model.

of the sentence. the parser also performs basic lemmati-
zation, for example turning kites into kite and were into be.
this reduces sparsity of module instances.

next, we    lter the set of dependencies to those con-
nected the wh-word in the question (the exact distance we
traverse varies depending on the task). this gives a sim-
ple symbolic form expressing (the primary) part of the sen-
tence   s meaning. for example, what is standing in the
   eld becomes what(stand); what color is the truck becomes
color(truck), and is there a circle next to a square becomes
is(circle, next-to(square)). in the process we also strip
away function words like determiners and modals, so what
type of cakes were they? and what type of cake is it? both
get converted to type(cake). the code for transforming
parse trees to structured queries will be provided in the ac-
companying software package.

these representations bear a certain resemblance to
pieces of a combinatory logic [18]: every leaf is implicitly
a function taking the image as input, and the root represents
the    nal value of the computation. but our approach, while
compositional and combinatorial, is crucially not logical:
the inferential computations operate on continuous repre-
sentations produced by neural networks, becoming discrete
only in the prediction of the    nal answer.

layout these symbolic representations already deter-
mine the structure of the predicted networks, but not the
identities of the modules that compose them. this    nal as-
signment of modules is fully determined by the structure
of the parse. all leaves become attend modules, all inter-
nal nodes become re-attend or combine modules dependent
on their arity, and root nodes become measure modules for
yes/no questions and classify modules for all other ques-
tion types.

given the mapping from queries to network layouts de-
scribed above, we have for each training example a net-
work structure, an input image, and an output label.
in
many cases,
these network structures are different, but
have tied parameters. networks which have the same
high-level structure but different
instantiations of indi-
vidual modules (for example what color is the cat?   
classify[color](attend[cat]) and where is the truck?   
classify[where](attend[truck])) can be processed in the
same batch, resulting in ef   cient computation.

as noted above, parts of this conversion process are task-
speci   c   we found that relatively simple expressions were
best for the natural image questions, while the shapes ques-
tion (by design) required deeper structures. some summary
statistics are provided in table 1.

generalizations
it is easy to imagine applications where
the input to the layout stage comes from something other
than a natural language parser. users of an image database,
for example, might write sql-like queries directly in order
to specify their requirements precisely, e.g.

is(cat) and not(is(dog))

or even mix visual and non-visual speci   cations in their
queries:

is(cat) and date > 2014-11-5

indeed, it is possible to construct this kind of    visual
sql    using precisely the approach described in this paper   
once our system is trained, the learned modules for atten-
tion, classi   cation, etc. can be assembled by any kind of
outside user, without relying on natural language speci   -
cally.

attend[tie]classify[color]yellowre-attend[above]yesattend[circle]combine[and]attend[red]measure[is]types

# instances

vqa
shapes

attend, combine, classify, measure
attend, re-attend, combine, measure

1995

8

# layouts max depth max size
66549
164

4
6

3
5

table 1: structure summary statistics for neural module networks used in this paper.    types    is the set of high-level module
types available (e.g. attend),    # instances    is the number of speci   c module instances (e.g. attend[llama]), and    # layouts   
is the number of distinct composed structures (e.g. classify[color](attend[llama])).    max depth    is the greatest depth
across all layouts, while    max size    is the greatest number of modules   for example, the network in figure 2b has depth 4
and size 5. (all numbers from training sets.)

4.3. answering natural language questions

so far our discussion has focused on the neural module
net architecture, without reference to the remainder of fig-
ure 1. our    nal model combines the output from the neu-
ral module network with predictions from a simple lstm
question encoder. this is important for two reasons. first,
because of the relatively aggressive simpli   cation of the
question that takes place in the parser, grammatical cues that
do not substantively change the semantics of the question,
but which might affect the answer, are discarded. for exam-
ple, what is    ying? and what are    ying? both get converted
to what(fly), but their answers should be kite and kites re-
spectively, even given the same underlying image features.
the question encoder thus allows us to model underlying
syntactic regularities in the data. second, it allows us to
capture semantic regularities: with missing or low-quality
image data, it is reasonable to guess that what color is the
bear? is answered by brown, and unreasonable to guess
green. the question encoder also allows us to model effects
of this kind.

all experiments in this paper use a standard single-layer
lstm with 1024 hidden units. the question modeling
component predicts a distribution over the set of answers,
like the root module of the nmn. the    nal prediction from
the model is a geometric average of these two id203
distributions, dynamically reweighted using both text and
image features. the complete model, including both the
nmn and sequence modeling component, is trained jointly.

5. training neural module networks

our training objective is simply to    nd module parame-
ters maximizing the likelihood of the data. by design, the
last module in every network outputs a distribution over la-
bels, and so each assembled network also represents a prob-
ability distribution.

because of the dynamic network structures used to an-
swer questions, some weights are updated much more fre-
quently than others. for this reason we found that learn-
ing algorithms with adaptive per-weight learning rates per-
formed substantially better than simple id119.
all the experiments described below use adadelta [33]

(thus there was no hyperparameter search over step sizes).
it is important to emphasize that the labels we have as-
signed to distinguish instances of the same module type   
cat, and, etc.   are a notational convenience, and do not re-
   ect any manual speci   cation of the behavior of the cor-
responding modules. detect[cat] is not    xed or even ini-
tialized as cat recognizer (rather than a couch recognizer
or a dog recognizer), and combine[and] isn   t    xed to com-
pute intersections of attentions (rather than unions or differ-
ences). instead, they acquire these behaviors as a byprod-
uct of the end-to-end training procedure. as can be seen
in figure 2, the image   answer pairs and parameter tying
together encourage each module to specialize in the appro-
priate way.

6. experiments: compositionality

we begin with a set of motivating experiments on syn-
thetic data. compositionality, and the corresponding abil-
ity to answer questions with arbitrarily complex structure,
is an essential part of the kind of deep image understand-
ing visual qa datasets are intended to test. at the same
time, questions in most existing natural image datasets are
quite simple, for the most part requiring that only one or two
pieces of information be extracted from an image in order
to answer it successfully, and with little evaluation of ro-
bustness in the presence of distractors (e.g. asking is there
a blue house in an image of a red house and a blue car).

as one of the primary goals of this work is to learn
models for deep semantic compositionality, we have cre-
ated shapes, a synthetic dataset that places such compo-
sitional phenomena at the forefront. this dataset consists
of complex questions about simple arrangements of col-
ored shapes (figure 3). questions contain between two and
four attributes, object types, or relationships. there are 244
questions and 15616 images in total. to eliminate mode-
guessing as a viable strategy, all questions have a yes-or-no
answer, but good performance requires that the system learn
to recognize shapes and colors, and understand both spatial
and logical relations among sets of objects.

while success on this dataset is by no means a suf   cient
condition for robust visual qa, we believe it is a necessary
one. in this respect it is similar in spirit to the babi [29]

size 4

size 5

size 6

majority
vis+lstm
nmn
nmn (easy)

64.4
71.9
89.7
97.7

62.5
62.5
92.4
91.1

61.7
61.7
85.2
89.7

all

63.0
65.3
90.6
90.8

table 2: results on the shapes dataset. here    size    is
the number of modules needed to instantiate an appropriate
nmn. our model achieves high accuracy and outperforms
a baseline from previous work, especially on highly compo-
sitional questions.    nmn (easy)    is a modi   ed training set
with no size-6 questions; these results demonstrate that our
model is able to generalize to questions more complicated
than it has ever seen at training time.

dataset, and we hope that shapes will continue to be used
in conjunction with natural image datasets.

to produce an initial set of image features, we pass the
input image through the convolutional portion of a lenet
[17] which is jointly trained with the question-answering
part of the model. we compare our approach to a reim-
plementation of the vis+lstm baseline similar to the one
described by [26], again swapping out the pre-trained image
embedding with a lenet.

as can be seen in table 2, our model achieves excellent
performance on this dataset, while the vis+lstm base-
line fares little better than a majority guesser. moreover,
the color detectors and attention transformations behave as
expected (figure 2b), indicating that our joint training pro-
cedure correctly allocates responsibilities among modules.
this con   rms that our approach is able to model complex
compositional phenomena outside the capacity of previous
approaches to visual id53.

we perform an additional experiment on a modi   ed ver-
sion of the training set, which contains no size-6 questions
(i.e. questions whose corresponding nmn has 6 modules).
here our performance does not suffer at all, and perhaps in-
creases slightly; this demonstrates that our model is able to
generalize to questions even more complicated than those it
has seen during training. using only linguistic information,
the model extrapolates simple visual patterns it has learned
to even harder questions.

7. experiments: natural images

next we consider the model   s ability to handle hard per-
ceptual problems involving natural images. here we evalu-
ate on the recently-released vqa dataset. this is the largest
resource of its kind, consisting of more than 200,000 im-
ages, each paired with three questions and ten answers per
question. data was generated by human annotators, in con-
trast to previous work, which has generated questions au-
tomatically from captions [26]. we learn our model using
the standard train/test split, training only with those answers

test-dev

yes/no

number

other

lstm [2]
vis+lstm [2]
nmn
nmn+lstm

78.20
78.9
69.38
77.7

35.7
35.2
30.7
37.2

26.6
36.4
22.7
39.3

all

48.8
53.7
42.7
54.8

test
all

   

54.1

   
55.1

table 3: results on the vqa test server. nmn+lstm is
the full model shown in figure 1, while nmn is an ablation
experiment with no whole-question lstm. the full model
outperforms previous approaches, scoring particularly well
on questions not involving a binary decision. baseline num-
bers are as reported in previous work.

marked as high con   dence. the visual input to the nmn
is the conv5 layer of a 16-layer vggnet [27] after max-
pooling. we do not    ne-tune the vggnet.

results are shown in table 3. as can be seen, we outper-
form the best published results on this task. a breakdown
of our questions by answer type reveals that our model per-
forms especially well on questions answered by an object,
attribute, or number, but worse than a sequence baseline
in the yes/no category.
inspection of training-set accura-
cies suggests that performance on yes/no questions is due
to over   tting. an ensemble with a sequence-only system
might achieve even better results; future work within the
nmn framework should focus on redesigning the measure
module to reduce effects from over   tting.

inspection of parser outputs also suggests that there is
substantial room to improve the system using a better parser.
a hand inspection of the    rst 50 parses in the training set
suggests that most (80   90%) of questions asking for simple
properties of objects are correctly analyzed, but more com-
plicated questions are more prone to picking up irrelevant
predicates. for example are these people most likely experi-
encing a work day? is parsed as be(people, likely), when
the desired analysis is is(people, work). parser errors of
this kind could be    xed with joint learning.

figure 3 is broadly suggestive of the kinds of predic-
tion errors made by the system, including plausible seman-
tic confusions (cardboard interpreted as leather, round win-
dows interpreted as clocks), normal lexical variation (con-
tainer for cup), and use of answers that are a priori plausible
but unrelated to the image (describing a horse as located in
a pen rather than a barn).

8. conclusions and future work

in this paper, we have introduced neural module net-
works, which provide a general-purpose framework for
learning collections of neural modules which can be dy-
namically assembled into arbitrary deep networks. we have
demonstrated that this approach achieves state-of-the-art
performance on existing datasets for visual question an-

how many different lights
in various different shapes
and sizes?

what is the color of the
horse?

what color is the vase?

is the bus full of passen-
gers?

is there a red shape above
a circle?

measure[count](
attend[light])

classify[color](
attend[horse])

classify[color](
attend[vase])

measure[is](

measure[is](

combine[and](
attend[bus],
attend[full])

combine[and](
attend[red],
re-attend[above](
attend[circle])))

four (four)

brown (brown)

green (green)

yes (yes)

no (no)

what is stuffed with
toothbrushes wrapped in
plastic?

where does the tabby cat
watch a horse eating hay?

what material are
boxes made of?

the

is this a clock?

is a red shape blue?

classify[what](
attend[stuff])

classify[where](
attend[watch])

classify[material](

attend[box])

measure[is](
attend[clock])

measure[is](

combine[and](
attend[red],
attend[blue]))

container (cup)

pen (barn)

leather (cardboard)

yes (no)

yes (no)

figure 3: example output from our approach on different visual qa tasks. the top row shows correct answers, while the
bottom row shows mistakes (correct answers are given in parentheses).

swering, performing especially well on questions answered
by an object or an attribute. additionally, we have in-
troduced a new dataset of highly compositional questions
about simple arrangements of shapes, and shown that our
approach substantially outperforms previous work.

so far we have maintained a strict separation between
predicting network structures and learning network param-
eters. it is easy to imagine that these two problems might
be solved jointly, with uncertainty maintained over network
structures throughout training and decoding. this might be
accomplished either with a monolithic network, by using
some higher-level mechanism to    attend    to relevant por-
tions of the computation, or else by integrating with existing

tools for learning semantic parsers [16].

the fact

that our neural module networks can be
trained to produce predictable outputs   even when freely
composed   points toward a more general paradigm of
   programs    built from neural networks. in this paradigm,
network designers (human or automated) have access to a
standard kit of neural parts from which to construct mod-
els for performing complex reasoning tasks. while visual
id53 provides a natural testbed for this ap-
proach, its usefulness is potentially much broader, extend-
ing to queries about documents and structured knowledge
bases or more general signal processing and function ap-
proximation.

acknowledgments

the authors are grateful to lisa anne hendricks, eric
tzeng, and russell stewart for useful conversations, and to
nvidia for a hardware grant. ja is supported by a national
science foundation graduate research fellowship. mr is
supported by a fellowship within the fit weltweit-program
of the german academic exchange service (daad). this
work was additionally supported by darpa, afrl, dod
muri award n000141110688, nsf awards iis-1427425
and iis-1212798, and the berkeley vision and learning
center.
references
[1] j. andreas and d. klein. grounding language with points

and paths in continuous spaces. conll, 2014. 3

[2] s. antol, a. agrawal, j. lu, m. mitchell, d. batra, c. l.
zitnick, and d. parikh. vqa: visual id53. in
iccv, 2015. 1, 2, 3, 7

[3] k. cho, b. van merri  enboer, d. bahdanau, and y. bengio.
on the properties of id4: encoder-
decoder approaches. arxiv preprint arxiv:1409.1259, 2014.
3

[4] m.-c. de marneffe and c. d. manning. the stanford typed
dependencies representation. in proceedings of the interna-
tional conference on computational linguistics, pages 1   8.
association for computational linguistics, 2008. 4

[5] j. l. elman. finding structure in time. cognitive science,

14(2):179   211, 1990. 3

[6] n. fitzgerald, y. artzi, and l. zettlemoyer. learning dis-
tributions over logical forms for referring expression gener-
ation. in proceedings of the conference on empirical meth-
ods in natural language processing, 2013. 2

[7] h. gao, j. mao, j. zhou, z. huang, l. wang, and w. xu.
are you talking to a machine? dataset and methods for mul-
tilingual image id53. in nips, 2015. 1, 3

[8] d. geman, s. geman, n. hallonquist, and l. younes. visual
turing test for id161 systems. proceedings of the
national academy of sciences, 2015. 2

[9] r. girshick, j. donahue, t. darrell, and j. malik. rich fea-
ture hierarchies for accurate id164 and semantic
segmentation. in id161 and pattern recognition
(cvpr), 2014 ieee conference on, pages 580   587. ieee,
2014. 2

[10] s. hochreiter and j. schmidhuber. long short-term memory.

neural computation, 9(8):1735   1780, 1997. 3

[11] m. iyyer, j. boyd-graber, l. claudino, r. socher, and
h. daum  e iii. a neural network for factoid question answer-
in proceedings of the conference on
ing over paragraphs.
empirical methods in natural language processing, 2014.
2

[12] a. karpathy and l. fei-fei. deep visual-semantic align-
in cvpr, 2015.

ments for generating image descriptions.
3

[13] a. karpathy, a. joulin, and l. fei-fei. deep fragment em-
beddings for bidirectional image sentence mapping. in nips,
2014. 3

[14] d. klein and c. d. manning. accurate unlexicalized parsing.
in proceedings of the annual meeting of the association for
computational linguistics, pages 423   430. association for
computational linguistics, 2003. 4

[15] c. kong, d. lin, m. bansal, r. urtasun, and s. fidler. what
are you talking about? text-to-image coreference. in cvpr,
pages 3558   3565. ieee, 2014. 3
[16] j. krishnamurthy and t. kollar.

jointly learning to parse
and perceive: connecting natural language to the physical
world. transactions of the association for computational
linguistics, 2013. 1, 3, 8

[17] y. lecun, l. bottou, y. bengio, and p. haffner. gradient-
based learning applied to document recognition. proceed-
ings of the ieee, 86(11):2278   2324, 1998. 7

[18] p. liang, m. i. jordan, and d. klein. learning dependency-
based id152. computational linguistics,
39(2):389   446, 2013. 1, 3, 5

[19] t.-y. lin, m. maire, s. belongie, j. hays, p. perona, d. ra-
manan, p. doll  ar, and c. l. zitnick. microsoft coco: com-
in id161   eccv 2014,
mon objects in context.
pages 740   755. springer, 2014. 2

[20] l. ma and z. l. andiyyer hang li. learning to answer
questions from image using convolutional neural network.
arxiv:1506.00333, 2015. 1, 3

[21] m. malinowski and m. fritz. a multi-world approach to
id53 about real-world scenes based on uncer-
tain input. in nips, 2014. 1, 2, 3

[22] m. malinowski, m. rohrbach, and m. fritz. ask your neu-
rons: a neural-based approach to answering questions about
images. in iccv, 12/2015 2015. 1, 2

[23] c. matuszek, n. fitzgerald, l. zettlemoyer, l. bo, and
d. fox. a joint model of language and perception for
grounded attribute learning. in icml, 2012. 3

[24] b. plummer, l. wang, c. cervantes, j. caicedo, j. hock-
enmaier, and s. lazebnik.
flickr30k entities: collect-
ing region-to-phrase correspondences for richer image-to-
sentence models. in iccv, 2015. 3

[25] m. ren, r. kiros, and r. zemel. image id53:
a visual semantic embedding model and a new dataset. in
nips, 2015. 1, 3

[26] m. ren, r. kiros, and r. s. zemel. image question answer-
ing: a visual semantic embedding model and a new dataset.
corr, abs/1505.02074, 2015. 7

[27] k. simonyan and a. zisserman.

very deep convolu-
tional networks for large-scale image recognition. corr,
abs/1409.1556, 2014. 7

[28] r. socher, j. bauer, c. d. manning, and a. y. ng. parsing
with compositional vector grammars. in proceedings of the
annual meeting of the association for computational lin-
guistics, 2013. 3

[29] j. weston, a. bordes, s. chopra, and t. mikolov. towards
ai-complete id53: a set of prerequisite toy
tasks. arxiv preprint arxiv:1502.05698, 2015. 6

[30] j. weston, s. chopra, and a. bordes. memory networks.

arxiv preprint arxiv:1410.3916, 2014. 3

[31] k. xu, j. ba, r. kiros, a. courville, r. salakhutdinov,
r. zemel, and y. bengio. show, attend and tell: neural im-

age id134 with visual attention. arxiv preprint
arxiv:1502.03044, 2015. 3

[32] l. yu, e. park, a. c. berg, and t. l. berg. visual madlibs:
fill in the blank image generation and id53.
arxiv:1506.00278, 2015. 1, 2

[33] m. d. zeiler. adadelta: an adaptive learning rate

method. arxiv preprint arxiv:1212.5701, 2012. 6

