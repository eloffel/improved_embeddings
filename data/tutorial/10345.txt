improved transition-based parsing

by modeling characters instead of words with lstms

miguel ballesteros       chris dyer       noah a. smith   
   nlp group, pompeu fabra university, barcelona, spain

   school of computer science, carnegie mellon university, pittsburgh, pa, usa
   computer science & engineering, university of washington, seattle, wa, usa

   marianas labs, pittsburgh, pa, usa

miguel.ballesteros@upf.edu, chris@marianaslabs.com, nasmith@cs.washington.edu

5
1
0
2

 

g
u
a
1
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
7
5
6
0
0

.

8
0
5
1
:
v
i
x
r
a

abstract

we present extensions to a continuous-
state id33 method that
makes it applicable to morphologically
rich languages.
starting with a high-
performance transition-based parser that
uses long short-term memory (lstm) re-
current neural networks to learn repre-
sentations of the parser state, we replace
lookup-based word representations with
representations constructed from the or-
thographic representations of the words,
also using lstms. this allows statistical
sharing across word forms that are simi-
lar on the surface. experiments for mor-
phologically rich languages show that the
parsing model bene   ts from incorporating
the character-based encodings of words.

1

introduction

at the heart of natural language parsing is the chal-
lenge of representing the    state    of an algorithm   
what parts of a parse have been built and what
parts of the input string are not yet accounted for   
as it incrementally constructs a parse. traditional
approaches rely on independence assumptions, de-
composition of scoring functions, and/or greedy
approximations to keep this space manageable.
continuous-state parsers have been proposed, in
which the state is embedded as a vector (titov
and henderson, 2007; stenetorp, 2013; chen and
manning, 2014; dyer et al., 2015; zhou et al.,
2015; weiss et al., 2015). dyer et al. reported
state-of-the-art performance on english and chi-
nese benchmarks using a transition-based parser
whose continuous-state embeddings were con-
structed using lstm recurrent neural networks
(id56s) whose parameters were estimated to max-
imize the id203 of a gold-standard sequence
of parse actions.

the primary contribution made in this work is to
take the idea of continuous-state parsing a step fur-
ther by making the id27s that are used
to construct the parse state sensitive to the mor-
phology of the words.1 since it it is well known
that a word   s form often provides strong evidence
regarding its grammatical role in morphologically
rich languages (ballesteros, 2013, inter alia), this
has promise to improve accuracy and statistical ef-
   ciency relative to traditional approaches that treat
each word type as opaque and independently mod-
eled.
in the traditional parameterization, words
with similar grammatical roles will only be em-
bedded near each other if they are observed in
similar contexts with suf   cient frequency. our
approach reparameterizes id27s using
the same id56 machinery used in the parser: a
word   s vector is calculated based on the sequence
of orthographic symbols representing it (  3).

although our model is provided no supervision
in the form of explicit morphological annotation,
we    nd that it gives a large performance increase
when parsing morphologically rich languages in
the spmrl datasets (seddah et al., 2013; seddah
and tsarfaty, 2014), especially in agglutinative
languages and the ones that present extensive case
systems (  4). in languages that show little mor-
phology, performance remains good, showing that
the id56 composition strategy is capable of cap-
turing both morphological regularities and arbi-
trariness in the sense of saussure (1916). finally,
a particularly noteworthy result is that we    nd that
character-based id27s in some cases
obviate explicit pos information, which is usually
found to be indispensable for accurate parsing.

a secondary contribution of this work is to
show that the continuous-state parser of dyer et al.
(2015) can learn to generate nonprojective trees.
we do this by augmenting its transition operations

1software for replicating the experiments is available

from https://github.com/clab/lstm-parser.

with a swap operation (nivre, 2009) (  2.4), en-
abling the parser to produce nonprojective depen-
dencies which are often found in morphologically
rich languages.

2 an lstm dependency parser

we begin by reviewing the parsing approach of
dyer et al. (2015) on which our work is based.

like most transition-based parsers, dyer et al.   s
parser can be understood as the sequential manip-
ulation of three data structures: a buffer b initial-
ized with the sequence of words to be parsed, a
stack s containing partially-built parses, and a list
a of actions previously taken by the parser.
in
particular, the parser implements the arc-standard
parsing algorithm (nivre, 2004).

at each time step t, a transition action is ap-
plied that alters these data structures by pushing
or popping words from the stack and the buffer;
the operations are listed in figure 1.

along with the discrete transitions above, the
parser calculates a vector representation of the
states of b, s, and a; at time step t these are de-
noted by bt, st, and at, respectively. the total
parser state at t is given by

pt = max{0, w[st; bt; at] + d}

(1)

where the matrix w and the vector d are learned
parameters. this continuous-state representation
pt is used to decide which operation to apply next,
updating b, s, and a (figure 1).
we elaborate on the design of bt, st, and at us-
ing id56s in   2.1, on the representation of partial
parses in s in   2.2, and on the parser   s decision
mechanism in   2.3. we discuss the inclusion of
swap in   2.4.

2.1 id200s
id56s are functions that read a sequence of vectors
incrementally; at time step t the vector xt is read in
and the hidden state ht computed using xt and the
previous hidden state ht   1. in principle, this al-
lows retaining information from time steps in the
distant past, but the nonlinear    squashing    func-
tions applied in the calcluation of each ht result
in a decay of the error signal used in training with
id26. lstms are a variant of id56s
designed to cope with this    vanishing gradient   
problem using an extra memory    cell    (hochreiter
and schmidhuber, 1997; graves, 2013).

past work explains the computation within an
lstm through the metaphors of deciding how
much of the current input to pass into memory
(it) or forget (ft). we refer interested readers to
the original papers and present only the recursive
equations updating the memory cell ct and hidden
state ht given xt, the previous hidden state ht   1,
and the memory cell ct   1:

it =   (wixxt + wihht   1 + wicct   1 + bi)
ft = 1     it
ct = ft (cid:12) ct   1+

it (cid:12) tanh(wcxxt + wchht   1 + bc)
ot =   (woxxt + wohht   1 + wocct + bo)
ht = ot (cid:12) tanh(ct),

where    is the component-wise logistic sig-
moid function and (cid:12) is the component-wise
(hadamard) product. parameters are all repre-
sented using w and b. this formulation differs
slightly from the classic lstm formulation in that
it makes use of    peephole connections    (gers et
al., 2002) and de   nes the forget gate so that it sums
with the input gate to 1 (greff et al., 2015). to im-
prove the representational capacity of lstms (and
id56s generally), they can be stacked in    layers.   
in these architectures, the input lstm at higher
layers at time t is the value of ht computed by the
lower layer (and xt is the input at the lowest layer).
the id200 augments the left-to-right se-
quential model of the conventional lstm with a
stack pointer. as in the lstm, new inputs are
added in the right-most position, but the stack
pointer indicates which lstm cell provides ct   1
and ht   1 for the computation of the next iterate.
further, the id200 provides a pop opera-
tion that moves the stack pointer to the previous
element. hence each of the parser data structures
(b, s, and a) is implemented with its own stack
lstm, each with its own parameters. the values
of bt, st, and at are the ht vectors from their re-
spective id200s.

2.2 composition functions
whenever a reduce operation is selected, two
tree fragments are popped off of s and combined
to form a new tree fragment, which is then popped
back onto s (see figure 1). this tree must be em-
bedded as an input vector xt.

to do this, dyer et al. (2015) use a recursive
neural network gr (for relation r) that composes

stackt

buffert action

stackt+1

(u, u), (v, v), s
(u, u), (v, v), s

b
b

reduce-right(r)
reduce-left(r)

(gr(u, v), u), s
(gr(v, u), v), s

s

(u, u), (v, v), s

(u, u), b shift
swap

b

(u, u), s
(u, u), s

b
b
b

(v, v), b

u r    v
u r    v
   
   

buffert+1 dependency

figure 1: parser transitions indicating the action applied to the stack and buffer and the resulting stack and
buffer states. bold symbols indicate (learned) embeddings of words and relations, script symbols indicate
the corresponding words and relations. dyer et al. (2015) used the shift and reduce operations in their
continuous-state parser; we add swap.

the representations of the two subtrees popped
from s (we denote these by u and v), resulting in
a new vector gr(u, v) or gr(v, u), depending on
the direction of attachment. the resulting vector
embeds the tree fragment in the same space as the
words and other tree fragments. this kind of com-
position was thoroughly explored in prior work
(socher et al., 2011; socher et al., 2013b; her-
mann and blunsom, 2013; socher et al., 2013a);
for details, see dyer et al. (2015).

nonprojective trees. here, the inclusion of the
swap operation requires breaking the linearity of
the stack by removing tokens that are not at the top
of the stack. this is easily handled with the stack
lstm. figure 1 shows how the parser is capable
of moving words from the stack (s) to the buffer
(b), breaking the linear order of words. since a
node that is swapped may have already been as-
signed as the head of a dependent, the buffer (b)
can now also contain tree fragments.

2.3 predicting parser decisions
the parser uses a probabilistic model of parser de-
cisions at each time step t. letting a(s, b) de-
note the set of allowed transitions given the stack
s and buffer s (i.e., those where preconditions
are met; see figure 1), the id203 of action
z     a(s, b) de   ned using a log-linear distribu-
tion:
p(z | pt) =

exp(cid:0)g(cid:62)
z(cid:48)   a(s,b) exp(cid:0)g(cid:62)
(cid:80)

(cid:1)
z(cid:48)pt + qz(cid:48)(cid:1) (2)

z pt + qz

(where gz and qz are parameters associated with
each action type z).
parsing proceeds by always choosing the most
probable action from a(s, b). the probabilistic
de   nition allows parameter estimation for all of
the parameters (w   , b    in all three id200s,
as well as w, d, g   , and q   ) by maximizing the
conditional likelihood of each correct parser deci-
sions given the state.

2.4 adding the swap operation
dyer et al. (2015)   s parser implemented the most
basic version of the arc-standard algorithm, which
is capable of producing only projective parse trees.
in order to deal with nonprojective trees, we also
add the swap operation which allows nonprojec-
tive trees to be produced.

the swap operation,    rst introduced by nivre
(2009), allows a transition-based parser to produce

3 word representations

the main contribution of this paper is to change
the word representations.
in this section, we
present the standard id27s as in dyer
et al. (2015), and the improvements we made gen-
erating id27s designed to capture mor-
phology based on orthographic strings.

3.1 baseline: standard id27s

dyer et al.   s parser generates a word representation
for each input token by concatenating two vectors:
a vector representation for each word type (w)
and a representation (t) of the pos tag of the to-
ken (if it is used), provided as auxiliary input to the
parser.2 a linear map (v) is applied to the result-
ing vector and passed through a component-wise
relu:

x = max{0, v[w; t] + b}

for out-of-vocabulary words, the parser uses an
   unk    token that is handled as a separate word
during parsing time. this mapping can be shown
schematically as in figure 2.

2dyer et al. (2015), included a third input representation
learned from a neural language model (   wlm). we do not in-
clude these pretrained representations in our experiments, fo-
cusing instead on character-based representations.

figure 2: baseline model id27s for an
in-vocabulary word that is tagged with pos tag
nn (right) and an out-of-vocabulary word with
pos tag jj (left).

3.2 character-based embeddings of words
following ling et al.
(2015), we compute
character-based continuous-space vector embed-
dings of words using bidirectional lstms (graves
and schmidhuber, 2005). when the parser initi-
ates the learning process and populates the buffer
with all the words from the sentence, it reads the
words character by character from left to right and
computes a continuous-space vector embedding
the character sequence, which is the h vector of
   
w. the same process
the lstm; we denote it by
is also applied in reverse (albeit with different pa-
rameters), computing a similar continuous-space
vector embedding starting from the last character
   
w); again each character
and    nishing at the    rst (
is represented with an lstm cell. after that, we
concatenate these vectors and a (learned) represen-
tation of their tag to produce the representation w.
as in   3.1, a linear map (v) is applied and passed
through a component-wise relu.

x = max

0, v[

   
w;

   
w; t] + b

(cid:110)

(cid:111)

this process is shown schematically in figure 3.
note that under this representation, out-of-
vocabulary words are treated as bidirectional
lstm encodings and thus they will be    close    to
other words that the parser has seen during train-
ing, ideally close to their more frequent, syntacti-
cally similar morphological relatives. we conjec-
ture that this will give a clear advantage over a sin-
gle    unk    token for all the words that the parser
does not see during training, as done by dyer et
al. (2015) and other parsers without additional re-
sources. in   4 we con   rm this hypothesis.
4 experiments
we applied our parsing model and several varia-
tions of it to several parsing tasks and report re-

figure 3: character-based id27 of the
word party. this representation is used for both
in-vocabulary and out-of-vocabulary words.

sults below.

4.1 data
in order to    nd out whether the character-based
representations are capable of learning the mor-
phology of words, we applied the parser to mor-
phologically rich languages speci   cally the tree-
banks of the spmrl shared task (seddah et
al., 2013; seddah and tsarfaty, 2014): arabic
(maamouri et al., 2004), basque (aduriz et al.,
2003), french (abeill  e et al., 2003), german
(seeker and kuhn, 2012), hebrew (sima   an et al.,
2001), hungarian (vincze et al., 2010), korean
(choi, 2013), polish (  swidzi  nski and woli  nski,
2010) and swedish (nivre et al., 2006b). for all
the corpora of the spmrl shared task we used
predicted pos tags as provided by the shared task
organizers.3 for these datasets, evaluation is cal-
culated using eval07.pl, which includes punc-
tuation.

we also experimented with the turkish de-
pendency treebank4 (o   azer et al., 2003) of the
conll-x shared task (buchholz and marsi,
2006). we used gold pos tags, as is common with
the conll-x data sets.

to put our results in context with the most re-
cent neural network transition-based parsers, we
run the parser in the same chinese and english

3the pos tags were calculated with the marmot tag-
ger (m  uller et al., 2013) by the best performing system of
the spmrl shared task (bj  orkelund et al., 2013). arabic:
97.38. basque: 97.02. french: 97.61. german: 98.10. he-
brew: 97.09. hungarian: 98.72. korean: 94.03. polish:
98.12. swedish: 97.27.

4since the turkish dependency treebank does not have a
development set, we extracted the last 150 sentences from the
4996 sentences of the training set as a development set.

tjjxunkwtnnxpartywpyartytrap</w><w></w><w>!w wtnnxsetups as chen and manning (2014) and dyer et
al. (2015). for chinese, we use the penn chi-
nese treebank 5.1 (ctb5) following zhang and
clark (2008b),5 with gold pos tags. for en-
glish, we used the stanford dependency (sd) rep-
resentation of the id326 (marcus et al.,
1993; marneffe et al., 2006).7. results for turk-
ish, chinese, and english are calculated using the
conll-x eval.pl script, which ignores punc-
tuation symbols.

4.2 experimental con   gurations
in order to isolate the improvements provided by
the lstm encodings of characters, we run the
id200 parser in the following con   gura-
tions:

    words: words only, as in   3.1 (but without

pos tags)

    chars:

character-based representations of
words with bidirectional lstms, as in   3.2
(but without pos tags)

    words + pos: words and pos tags (  3.1)
    chars + pos: character-based representa-
tions of words with bidirectional lstms plus
pos tags (  3.2)

none of the experimental con   gurations in-
clude pretrained word-embeddings or any addi-
tional data resources. all experiments include the
swap transition, meaning that nonprojective trees
can be produced in any language.
dimensionality. the full version of our parsing
model sets dimensionalities as follows. lstm
hidden states are of size 100, and we use two
layers of lstms for each stack. embeddings of
the parser actions used in the composition func-
tions have 20 dimensions, and the output embed-
ding size is 20 dimensions. the learned word
representations embeddings have 32 dimensions
when used, while the character-based representa-
tions have 100 dimensions, when used. part of
speech embeddings have 12 dimensions. these di-
mensionalities were chosen after running several
tests with different values, but a more careful se-
lection of these values would probably further im-
prove results.

5training: 001   815, 1001   1136. development: 886   

931, 1148   1151. test: 816   885, 1137   1147.

6training: 02   21. development: 22. test: 23.
7the pos tags are predicted by using the stanford tagger

(toutanova et al., 2003) with an accuracy of 97.3%.

4.3 training procedure

parameters are initialized randomly   refer
to
dyer et al. (2015) for speci   cs   and optimized
using stochastic id119 (without mini-
batches) using derivatives of the negative log like-
lihood of the sequence of parsing actions com-
puted using id26. training is stopped
when the learned model   s uas stops improving
on the development set, and this model is used to
parse the test set. no pretraining of any parameters
is done.

4.4 results and discussion

tables 1 and 2 show the results of the parsers for
the development sets and the    nal test sets, respec-
tively. most notable are improvements for agglu-
tinative languages   basque, hungarian, korean,
and turkish   both when pos tags are included
and when they are not. consistently, across all
languages, chars outperforms words, suggest-
ing that the character-level lstms are learning
representations that capture similar information to
parts of speech. on average, chars is on par with
words + pos, and the best average of labeled at-
tachment scores is achieved with chars + pos.

it is common practice to encode morphological
information in treebank pos tags; for instance, the
id32 includes english number and tense
(e.g., nns is plural noun and vbd is verb in past
tense). even if our character-based representations
are capable of encoding the same kind of informa-
tion, existing pos tags suf   ce for high accuracy.
however, the pos tags in treebanks for morpho-
logically rich languages do not seem to be enough.
swedish, english, and french use suf   xes for
the verb tenses and number,8 while hebrew uses
prepositional particles rather than grammatical
case. tsarfaty (2006) and cohen and smith (2007)
argued that, for hebrew, determining the correct
morphological segmentation is dependent on syn-
tactic context. our approach sidesteps this step,
capturing the same kind of information in the vec-
tors, and learning it from syntactic context. even
for chinese, which is not morphologically rich,
chars shows a bene   t over words, perhaps by
capturing regularities in syllable structure within
words.

8tense and number features provide little improvement in
a transition-based parser, compared with other features such
as case, when the pos tags are included (ballesteros, 2013).

uas

language words chars words
+ pos
87.44
arabic
83.49
basque
87.00
french
german
91.16
81.99
hebrew
78.47
hungarian
87.36
korean
89.32
polish
80.02
swedish
77.13
turkish
85.98
chinese
92.94
english
average
85.19

87.20
84.97
86.21
90.94
79.92
80.16
88.98
85.69
75.03
74.91
80.36
91.98
83.86

86.14
78.42
84.84
88.14
79.73
72.38
78.98
73.29
73.44
71.10
79.43
91.64
79.79

chars
+ pos
87.07
85.58
86.33
91.23
80.76
80.85
89.14
88.54
78.85
77.96
85.81
92.49
85.38

las

language words chars words
+ pos
84.81
arabic
74.31
basque
82.71
french
89.04
german
74.11
hebrew
69.50
hungarian
83.80
korean
81.84
polish
72.09
swedish
62.30
turkish
84.36
chinese
90.63
english
average
79.13

84.34
78.22
81.70
88.68
70.58
75.61
86.80
78.23
66.74
62.91
77.06
89.58
78.37

82.73
67.08
80.32
85.36
69.42
62.14
67.48
65.13
64.77
53.98
75.64
88.60
71.89

chars
+ pos
84.36
79.52
81.51
88.83
72.18
76.16
86.88
80.97
69.88
62.87
84.10
90.08
79.78

table 1: unlabeled attachment scores (left) and labeled attachment scores (right) on the development
sets (not a standard development set for turkish). in each table, the    rst two columns show the results of
the parser with word lookup (words) vs. character-based (chars) representations. the last two columns
add pos tags. boldface shows the better result comparing words vs. chars and comparing words +
pos vs. chars + pos.

uas

language words chars words
+ pos
86.05
arabic
82.92
basque
86.15
french
87.33
german
80.68
hebrew
78.64
hungarian
86.85
korean
87.06
polish
83.43
swedish
turkish
75.32
85.96
chinese
92.57
english
average
84.41

86.08
85.19
85.34
86.80
79.93
80.35
88.39
83.44
79.18
76.32
79.94
91.47
85.36

85.21
77.06
83.74
82.75
77.62
72.78
78.70
72.01
76.39
71.70
79.01
91.16
79.01

chars
+ pos
86.07
85.22
85.78
87.26
80.17
80.92
88.30
85.97
83.24
76.34
85.30
91.63
84.68

las

language words chars words
+ pos
83.46
arabic
73.56
basque
82.03
french
84.62
german
72.70
hebrew
69.31
hungarian
83.37
korean
79.83
polish
76.40
swedish
turkish
61.22
84.40
chinese
90.31
english
average
78.43

83.41
79.09
80.92
84.04
71.26
75.19
86.27
76.84
71.19
64.34
76.29
88.94
78.15

82.05
66.61
79.22
79.15
68.71
61.93
67.50
63.96
67.69
54.55
74.79
88.42
71.22

chars
+ pos
83.40
78.61
81.08
84.49
72.26
76.34
86.21
78.24
74.47
62.28
83.72
89.44
79.21

table 2: unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. in
each table, the    rst two columns show the results of the parser with word lookup (words) vs. character-
based (chars) representations. the last two columns add pos tags. boldface shows the better result
comparing words vs. chars and comparing words + pos vs. chars + pos.

4.4.1 learned word representations
figure 4 visualizes a sample of the character-
based bidirectional lstms   s learned representa-
tions (chars). clear clusters of past tense verbs,
gerunds, and other syntactic classes are visible.
the colors in the    gure represent the most com-
mon pos tag for each word.

4.4.2 out-of-vocabulary words
the character-based representation for words is
notably bene   cial for out-of-vocabulary (oov)
words. we tested this speci   cally by comparing
chars to a model in which all oovs are replaced
by the string    unk    during parsing. this always
has a negative effect on las (average    4.5 points,

   2.8 uas). figure 5 shows how this drop varies
with the development oov rate across treebanks;
most extreme is korean, which drops 15.5 las. a
similar, but less pronounced pattern, was observed
for models that include pos.

interestingly,

this arti   cially impoverished
model is still consistently better than words for
all languages (e.g., for korean, by 4 las). this
implies that not all of the improvement is due to
oov words; statistical sharing across orthograph-
ically close words is bene   cial, as well.
4.4.3 computational requirements
the character-based representations make the
parser slower, since they require composing the
character-based bidirectional lstms for each

figure 5: on the x-axis is the oov rate in development data, by treebank; on the y-axis is the difference
in development-set las between chars model as described in   3.2 and one in which all oov words are
given a single representation.

tant, needing some hours to have a competitive
model. in terms of memory, words requires on
average 300 mb of main memory for both train-
ing and parsing, while chars requires 450 mb.

4.4.4 comparison with state-of-the-art
table 3 shows a comparison with state-of-the-
art parsers. we include greedy transition-based
parsers that,
like ours, do not apply a beam
search (zhang and clark, 2008b) or a dynamic
oracle (goldberg and nivre, 2013). for all the
spmrl languages we show the results of balles-
teros (2013), who reported results after carrying
out a careful automatic morphological feature se-
lection experiment. for turkish, we show the re-
sults of nivre et al. (2006a) which also carried
out a careful manual morphological feature se-
lection. our parser outperforms these in most
cases. since those systems rely on morphological
features, we believe that this comparison shows
even more that the character-based representations
are capturing morphological information, though
without explicit morphological features. for en-
glish and chinese, we report (dyer et al., 2015)
which is words + pos but with pretrained word
embeddings.

we also show the best reported results on
these datasets. for the spmrl data sets, the
best performing system of the shared task is ei-
ther bj  orkelund et al. (2013) or bj  orkelund et al.
(2014), which are consistently better than our sys-

figure 4: character-based word representations
of 30 random words from the english develop-
ment set (chars). dots in red represent past tense
verbs; dots in orange represent gerund verbs; dots
in black represent present tense verbs; dots in blue
represent adjectives; dots in green represent ad-
verbs; dots in yellow represent singular nouns;
dots in brown represent plural nouns. the visu-
alization was produced using id167; see http:
//lvdmaaten.github.io/tsne/.

word of the input sentence; however, at test time
these results could be cached. on average, words
parses a sentence in 44 ms, whilechars needs 130
ms.9 training time is affected by the same cons-

9we are using a machine with 32 intel xeon cpu e5-

2650 at 2.00ghz; the parser runs on a single core.

0.050.100.150.200.250.30   15   10   50oov ratelas differencezhenareufrdehehukoplsvtr  overtlypossiblydeclaredadvancedoutnumberedachievedsuspendedapprovingrestatingretiringwashingleveragingplummetingmediancomputer-drivencranesdayssteadyeventgasolinegrandiosemeetperfectconstructivedropconsumptionprofessionconsultantreliefpitcheranswerlanguage
arabic
basque
french
german
hebrew
hungarian
korean
polish
swedish
turkish
chinese
english

uas
86.08
85.22
86.15
87.33
80.68
80.92
88.39
87.06
83.43
76.32
85.96
92.57

this work
system
las
83.41 chars
78.61 chars + pos
82.03 words + pos
84.62 words + pos
72.70 words + pos
76.34 chars + pos
86.27 chars
79.83 words + pos
76.40 words + pos
64.34 chars
84.40 words + pos
90.31 words + pos

best greedy result

uas
84.57
84.33
83.35
85.38
79.89
83.71
85.72
85.80
83.20
75.82
87.20
93.10

system

las
81.90 b   13
78.58 b   13
77.98 b   13
82.75 b   13
73.01 b   13
79.63 b   13
82.06 b   13
79.89 b   13
75.82 b   13
65.68 n+   06a
85.70 d+   15
90.90 d+   15

best published result
uas
system
las
86.21 b+   13
88.32
85.70 b+   14
89.96
85.66 b+   14
89.02
89.65 b+   13
91.64
87.41
81.65 b+   14
86.13 b+   13
89.81
87.27 b+   14
89.10
87.07 b+   13
91.75
82.75 b+   14
88.48
77.55
n/a k+   10
85.70 d+   15
87.20
94.08
92.19 w+   15

table 3: test-set performance of our best results (according to uas or las, whichever has the larger
difference), compared to state-of-the-art greedy transition-based parsers (   best greedy result   ) and best
results reported (   best published result   ). all of the systems we compare against use explicit mor-
phological features and/or one of the following: pretrained id27s, unlabeled data and a
combination of parsers; our models do not. b   13 is ballesteros (2013); n+   06a is nivre et al. (2006a);
d+   15 is dyer et al. (2015); b+   13 is bj  orkelund et al. (2013); b+   14 is bj  orkelund et al. (2014); k+   10
is koo et al. (2010); w+   15 is weiss et al. (2015).

tem for all languages. note that the comparison
is harsh to our system, which does not use unla-
beled data or explicit morphological features nor
any combination of different parsers. for turkish,
we report the results of koo et al. (2010), which
only reported unlabeled attachment scores. for
english, we report (weiss et al., 2015) and for chi-
nese, we report (dyer et al., 2015) which is words
+ pos but with pretrained id27s.

5 related work

character-based representations have been ex-
plored in other nlp tasks; for instance, dos san-
tos and zadrozny (2014) and dos santos and
guimar  aes (2015) learned character-level neural
representations for id52 and named entity
recognition, getting a large error reduction in both
tasks. our approach is similar to theirs. others
have used character-based models as features to
improve existing models. for instance, chrupa  a
(2014) used character-based recurrent neural net-
works to normalize tweets.

botha and blunsom (2014) show that stems,
pre   xes and suf   xes can be used to learn useful
word representations but relying on an external
morphological analyzer. that is, they learn the
morpheme-meaning relationship with an additive
model, whereas we do not need a morphological
analyzer. similarly, chen et al. (2015) proposed
joint learning of character and id27s
for chinese, claiming that characters contain rich
information.

methods for joint morphological disambigua-
tion and parsing have been widely explored tsar-
faty (2006; cohen and smith (2007; goldberg
and tsarfaty (2008; goldberg and elhadad (2011).
more recently, bohnet et al. (2013) presented an
arc-standard transition-based parser that performs
competitively for joint morphological tagging and
id33 for richly in   ected languages,
such as czech, finnish, german, hungarian, and
russian. our model seeks to achieve a simi-
lar bene   t to parsing without explicitly reasoning
about the internal structure of words.

zhang et al. (2013) presented efforts on chinese
parsing with characters showing that chinese can
be parsed at the character level, and that chinese
id40 is useful for predicting the cor-
rect pos tags (zhang and clark, 2008a).

to the best of our knowledge, previous work has
not used character-based embeddings to improve
dependency parsers, as done in this paper.

6 conclusion

we have presented several interesting    ndings.
first, we add new evidence that character-based
representations are useful for nlp tasks. in this
paper, we demonstrate that they are useful for
transition-based id33, since they
are capable of capturing morphological informa-
tion crucial for analyzing syntax.

the improvements provided by the character-
based representations using bidirectional lstms
are strong for agglutinative languages, such as

basque, hungarian, korean, and turkish, compar-
ing favorably to pos tags as encoded in those lan-
guages    currently available treebanks. this out-
come is important, since annotating morphologi-
cal information for a treebank is expensive. our
   nding suggests that the best investment of anno-
tation effort may be in dependencies, leaving mor-
phological features to be learned implicitly from
strings.

the character-based representations are also a
way of overcoming the out-of-vocabulary prob-
lem; without any additional resources, they en-
able the parser to substantially improve the per-
formance when oov rates are high. we expect
that, in conjunction with a pretraing regime, or in
conjunction with distributional id27s,
further improvements could be realized.

acknowledgments

mb was supported by the european com-
mission under the contract numbers fp7-ict-
610411 (project multisensor) and h2020-
ria-645012 (project kristina). this research
was supported by the u.s. army research labo-
ratory and the u.s. army research of   ce under
contract/grant number w911nf-10-1-0533 and
nsf iis-1054319. this work was completed
while nas was at cmu. thanks to joakim nivre,
bernd bohnet, fei liu and swabha swayamdipta
for useful comments.

references
anne abeill  e, lionel cl  ement, and franc  ois toussenel.
2003. building a treebank for french. in treebanks.
springer.

itziar aduriz, mar    a jes  us aranzabe, jose mari arriola,
aitziber atutxa, arantza d    az de ilarraza, aitzpea
garmendia, and maite oronoz. 2003. construction
of a basque dependency treebank. in proc of tlt.

miguel ballesteros. 2013. effective morphological
feature selection with maltoptimizer at the spmrl
2013 shared task. in proc. of spmrl-emnlp.

anders bj  orkelund, ozlem cetinoglu, rich  ard farkas,
thomas mueller, and wolfgang seeker.
2013.
(re)ranking meets morphosyntax: state-of-the-art
results from the spmrl 2013 shared task.
in
spmrl-emnlp.

2014 shared task: reranking and morpho-syntax
meet unlabeled data. in spmrl-sancl.

bernd bohnet,

joakim nivre,

igor boguslavsky,
richard farkas, filip ginter, and jan haji  c. 2013.
joint morphological and syntactic analysis for richly
in   ected languages. tacl, 1.

jan a. botha and phil blunsom. 2014. composi-
tional morphology for word representations and
language modelling. in icml.

sabine buchholz and erwin marsi. 2006. conll-x.

in proc of conll.

danqi chen and christopher d. manning. 2014. a fast
and accurate dependency parser using neural net-
works. in proc. emnlp.

xinxiong chen, lei xu, zhiyuan liu, maosong sun,
and huanbo luan. 2015. joint learning of character
and id27s. in proc. ijcai.

jinho d. choi. 2013. preparing korean data for the
shared task on parsing morphologically rich lan-
guages. arxiv e-prints, september.

grzegorz chrupa  a. 2014. normalizing tweets with
in

edit scripts and recurrent neural embeddings.
proc of acl.

shay b. cohen and noah a. smith. 2007. joint mor-
in proc.

phological and syntactic disambiguation.
emnlp-conll.

cicero nogueira dos santos and victor guimar  aes.
2015. boosting id39 with neu-
ral character embeddings. arxiv.

cicero dos santos and bianca zadrozny.

2014.
learning character-level representations for part-of-
speech tagging. in proc of icml-14.

chris dyer, miguel ballesteros, wang ling, austin
matthews, and noah a. smith. 2015. transition-
based id33 with stack long short-
term memory. in proc of acl.

felix a. gers, nicol n. schraudolph, and j  urgen
schmidhuber. 2002. learning precise timing with
lstm recurrent networks. jmlr.

yoav goldberg and michael elhadad. 2011. joint he-
brew segmentation and parsing using a pid18-la
lattice parser. in proc of acl.

yoav goldberg and joakim nivre. 2013. training
deterministic parsers with non-deterministic oracles.
tacl.

yoav goldberg and reut tsarfaty. 2008. a single gen-
erative model for joint morphological segmentation
and syntactic parsing. in proc of acl.

anders bj  orkelund,

  ozlem c   etino  glu, agnieszka
fale  nska, rich  ard farkas, thomas mueller, wolf-
gang seeker, and zsolt sz  ant  o. 2014. introducing
the ims-wroc  aw-szeged-cis entry at the spmrl

alex graves and j  urgen schmidhuber. 2005. frame-
wise phoneme classi   cation with bidirectional lstm
and other neural network architectures. neural net-
works, 18(5-6).

alex graves. 2013. generating sequences with recur-

rent neural networks. corr, abs/1308.0850.

klaus greff, rupesh kumar srivastava, jan koutn    k,
bas r. steunebrink, and j  urgen schmidhuber.
2015. lstm: a search space odyssey. corr,
abs/1503.04069.

karl moritz hermann and phil blunsom. 2013. the
role of syntax in vector space models of composi-
tional semantics. in proc. acl.

sepp hochreiter and j  urgen schmidhuber.

1997.
long short-term memory. neural computation,
9(8):1735   1780.

terry koo, alexander m. rush, michael collins,
tommi jaakkola, and david sontag. 2010. dual
decomposition for parsing with non-projective head
automata. in proc of emnlp.

wang ling, tiago lu    s, lu    s marujo, ram  on fernan-
dez astudillo, silvio amir, chris dyer, alan w
black, and isabel trancoso. 2015. finding function
in form: compositional character models for open
vocabulary word representation. in proc. emnlp.

mohamed maamouri, ann bies, tim buckwalter, and
wigdan mekki. 2004. the penn arabic treebank:
building a large-scale annotated arabic corpus.
in nemlar conference on arabic language re-
sources and tools.

mitchell p. marcus, beatrice santorini, and mary ann
marcinkiewicz. 1993. building a large annotated
the id32. computa-
corpus of english:
tional linguistics, 19(2):313   330.

marie-catherine de marneffe, bill maccartney, and
christopher d. manning. 2006. generating typed
dependency parses from phrase structure parses. in
proc of lrec.

thomas m  uller, helmut schmid, and hinrich sch  utze.
2013. ef   cient higher-order crfs for morphologi-
cal tagging. in proc of emnlp.

joakim nivre,

johan hall,

jens nilsson, g  ulsen
eryi  git, and svetoslav marinov. 2006a. labeled
pseudo-projective id33 with support
vector machines. in proc of conll.

joakim nivre, jens nilsson, and johan hall. 2006b.
talbanken05: a swedish treebank with phrase
in proc of
structure and dependency annotation.
lrec, genoa, italy.

joakim nivre. 2004.

incrementality in deterministic
id33. in proc of the workshop on in-
cremental parsing: bringing engineering and cog-
nition together.

joakim nivre. 2009. non-projective dependency pars-

ing in expected linear time. in proc of acl.

kemal o   azer, bilge say, dilek zeynep hakkani-t  ur,
and g  okhan t  ur. 2003. building a turkish tree-
bank. in treebanks, pages 261   277. springer.

ferdinand saussure. 1916. nature of the linguistic

sign. in course in general linguistics.

djam  e seddah and reut tsarfaty.

intro-
ducing the spmrl 2014 shared task on parsing
spmrl-sancl
morphologically-rich languages.
2014.

2014.

djam  e seddah, reut tsarfaty, sandra k  ubler, marie
candito, jinho choi, rich  ard farkas, jennifer fos-
ter, iakes goenaga, koldo gojenola, yoav goldberg,
et al. 2013. overview of the spmrl 2013 shared
task: cross-framework evaluation of parsing mor-
in spmrl-emnlp
phologically rich languages.
2013.

wolfgang seeker and jonas kuhn. 2012. making el-
lipses explicit in dependency conversion for a ger-
man treebank. in proc of lrec.

khalil sima   an, alon itai, yoad winter, alon altman,
and noa nativ. 2001. building a tree-bank for
in traitement automatique
modern hebrew text.
des langues.

richard socher, eric h. huang, jeffrey pennington,
andrew y. ng, and christopher d. manning. 2011.
dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. in proc of nips.

richard socher, andrej karpathy, quoc v. le, christo-
pher d. manning, and andrew y. ng.
2013a.
grounded id152 for    nding and
describing images with sentences. tacl.

richard socher, alex perelygin, jean y. wu, jason
chuang, christopher d. manning, andrew y. ng,
and christopher potts. 2013b. recursive deep mod-
els for semantic compositionality over a sentiment
treebank. in proc of emnlp.

pontus stenetorp. 2013. transition-based dependency
parsing using id56s. in proc of
nips deep learning workshop.

marek   swidzi  nski and marcin woli  nski. 2010. to-
wards a bank of constituent parse trees for polish.
in proc of tsd.

ivan. titov and james. henderson. 2007. a latent vari-
in

able model for generative id33.
proc of iwpt.

kristina toutanova, dan klein, christopher d. man-
ning, and yoram singer. 2003. feature-rich part-of-
speech tagging with a cyclic dependency network.
in proc of naacl.

reut tsarfaty. 2006.

syntactic disambiguation for modern hebrew.
proc of acl student research workshop.

integrated morphological and
in

veronika vincze, d  ora szauter, attila alm  asi, gy  orgy
m  ora, zolt  an alexin, and j  anos csirik. 2010. hun-
garian dependency treebank. in proc of lrec.

david weiss, christopher alberti, michael collins, and
slav petrov. 2015. structured training for neural
network transition-based parsing. in proc of acl.

yue zhang and stephen clark. 2008a. joint word seg-
mentation and id52 using a single percep-
tron. in proc of acl.

yue zhang and stephen clark. 2008b. a tale of two
parsers: investigating and combining graph-based
in proc
and transition-based id33.
of emnlp.

meishan zhang, yue zhang, wanxiang che, and ting
liu. 2013. chinese parsing exploiting characters.
in proc of acl.

hao zhou, yue zhang, shujian huang, and jiajun
chen. 2015. a neural probabilistic structured-
prediction model for transition-based dependency
parsing. in proc of acl.

