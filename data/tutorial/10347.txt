generating sentences from a continuous space

samuel r. bowman   

nlp group and dept. of linguistics

luke vilnis   

cics

stanford university

university of massachusetts amherst

sbowman@stanford.edu

luke@cs.umass.edu

oriol vinyals, andrew m. dai, rafal jozefowicz & samy bengio

{vinyals, adai, rafalj, bengio}@google.com

google brain

abstract

the standard recurrent neural network
language model (id56lm) generates sen-
tences one word at a time and does not
work from an explicit global sentence rep-
resentation.
in this work, we introduce
and study an id56-based variational au-
toencoder generative model that incorpo-
rates distributed latent representations of
entire sentences. this factorization al-
lows it to explicitly model holistic prop-
erties of sentences such as style, topic,
and high-level syntactic features. samples
from the prior over these sentence repre-
sentations remarkably produce diverse and
well-formed sentences through simple de-
terministic decoding. by examining paths
through this latent space, we are able to
generate coherent novel sentences that in-
terpolate between known sentences. we
present techniques for solving the di   cult
learning problem presented by this model,
demonstrate its e   ectiveness in imputing
missing words, explore many interesting
properties of the model   s latent sentence
space, and present negative results on the
use of the model in id38.

1 introduction

neural

network

language sentences.

recurrent
language models
(id56lms, mikolov et al., 2011) represent the state
of the art in unsupervised generative modeling
for natural
in supervised
settings, id56lm decoders conditioned on task-
speci   c features are the state of the art in tasks
like machine translation (sutskever et al., 2014;
bahdanau et al., 2015) and image captioning
(vinyals et al., 2015; mao et al., 2015; donahue
et al., 2015). the id56lm generates sentences
word-by-word based on an evolving distributed
state representation, which makes it a proba-
bilistic model with no signi   cant independence
   first two authors contributed equally. work was

done when all authors were at google, inc.

i went to the store to buy some groceries .
i store to buy some groceries .
i were to buy any groceries .
horses are to buy any groceries .
horses are to buy any animal .
horses the favorite any animal .
horses the favorite favorite animal .
horses are my favorite animal .

table 1: sentences produced by greedily decoding
from points between two sentence encodings with
a conventional autoencoder. the intermediate sen-
tences are not plausible english.

assumptions, and makes it capable of modeling
complex distributions over sequences,
including
those with long-term dependencies. however, by
breaking the model structure down into a series of
next-step predictions, the id56lm does not expose
an interpretable representation of global features
like topic or of high-level syntactic properties.

we propose an extension of the id56lm that is
designed to explicitly capture such global features
in a continuous latent variable. naively, maxi-
mum likelihood learning in such a model presents
an intractable id136 problem. drawing inspi-
ration from recent successes in modeling images
(gregor et al., 2015), handwriting, and natural
speech (chung et al., 2015), our model circum-
vents these di   culties using the architecture of a
variational autoencoder and takes advantage of re-
cent advances in variational id136 (kingma and
welling, 2015; rezende et al., 2014) that introduce
a practical training technique for powerful neural
network generative models with latent variables.

our contributions are as follows: we propose a
variational autoencoder architecture for text and
discuss some of the obstacles to training it as well
as our proposed solutions. we    nd that on a stan-
dard id38 evaluation where a global
variable is not explicitly needed, this model yields
similar performance to existing id56lms. we also
evaluate our model using a larger corpus on the
task of imputing missing words. for this task,
we introduce a novel evaluation strategy using an

6
1
0
2

 

y
a
m
2
1

 

 
 
]

g
l
.
s
c
[
 
 

4
v
9
4
3
6
0

.

1
1
5
1
:
v
i
x
r
a

adversarial classi   er, sidestepping the issue of in-
tractable likelihood computations by drawing in-
spiration from work on non-parametric two-sample
tests and adversarial training.
in this setting,
our model   s global latent variable allows it to do
well where simpler models fail. we    nally intro-
duce several qualitative techniques for analyzing
the ability of our model to learn high level fea-
tures of sentences. we    nd that they can produce
diverse, coherent sentences through purely deter-
ministic decoding and that they can interpolate
smoothly between sentences.

2 background

2.1 unsupervised sentence encoding
a standard id56 language model predicts each
word of a sentence conditioned on the previous
word and an evolving hidden state. while e   ec-
tive, it does not learn a vector representation of
the full sentence. in order to incorporate a contin-
uous latent sentence representation, we    rst need a
method to map between sentences and distributed
representations that can be trained in an unsuper-
vised setting. while no strong generative model
is available for this problem, three non-generative
techniques have shown promise: sequence autoen-
coders, skip-thought, and paragraph vector.

sequence autoencoders have seen some success
in pre-training sequence models for supervised
downstream tasks (dai and le, 2015) and in gen-
erating complete documents (li et al., 2015a).
an autoencoder consists of an encoder function
  enc and a probabilistic decoder model p(x|(cid:126)z =
  enc(x)), and maximizes the likelihood of an ex-
ample x conditioned on (cid:126)z, the learned code for
x.
in the case of a sequence autoencoder, both
encoder and decoder are id56s and examples are
token sequences.

standard autoencoders are not e   ective at ex-
tracting for global semantic features. in table 1,
we present the results of computing a path or ho-
motopy between the encodings for two sentences
and decoding each intermediate code. the in-
termediate sentences are generally ungrammatical
and do not transition smoothly from one to the
other. this suggests that these models do not
generally learn a smooth, interpretable feature sys-
tem for sentence encoding. in addition, since these
models do not incorporate a prior over (cid:126)z, they can-
not be used to assign probabilities to sentences or
to sample novel sentences.

two other models have shown promise in learn-
ing sentence encodings, but cannot be used in
a generative setting: skip-thought models (kiros
et al., 2015) are unsupervised learning models that
take the same model structure as a sequence au-
toencoder, but generate text conditioned on a
neighboring sentence from the target text, instead

of on the target sentence itself. finally, para-
graph vector models (le and mikolov, 2014) are
non-recurrent sentence representation models. in a
paragraph vector model, the encoding of a sentence
is obtained by performing gradient-based id136
on a prospective encoding vector with the goal of
using it to predict the words in the sentence.

2.2 the variational autoencoder

the variational autoencoder (vae, kingma and
welling, 2015; rezende et al., 2014) is a genera-
tive model that is based on a regularized version
of the standard autoencoder. this model imposes
a prior distribution on the hidden codes (cid:126)z which
enforces a regular geometry over codes and makes
it possible to draw proper samples from the model
using ancestral sampling.

the vae modi   es the autoencoder architecture
by replacing the deterministic function   enc with
a learned posterior recognition model, q((cid:126)z|x). this
model parametrizes an approximate posterior dis-
tribution over (cid:126)z (usually a diagonal gaussian) with
a neural network conditioned on x. intuitively, the
vae learns codes not as single points, but as soft
ellipsoidal regions in latent space, forcing the codes
to    ll the space rather than memorizing the train-
ing data as isolated codes.

if the vae were trained with a standard autoen-
coder   s reconstruction objective, it would learn to
encode its inputs deterministically by making the
variances in q((cid:126)z|x) vanishingly small (raiko et al.,
instead, the vae uses an objective which
2015).
encourages the model to keep its posterior distri-
butions close to a prior p((cid:126)z), generally a standard
gaussian (   = (cid:126)0,    = (cid:126)1). additionally, this objec-
tive is a valid lower bound on the true log likelihood
of the data, making the vae a generative model.
this objective takes the following form:

l(  ; x) =    kl(q  ((cid:126)z|x)||p((cid:126)z))

+ eq  ((cid:126)z|x)[log p  (x|(cid:126)z)]

(1)

    log p(x) .

this forces the model to be able to decode plausible
sentences from every point in the latent space that
has a reasonable id203 under the prior.

in the experiments presented below using vae
models, we use diagonal gaussians for the prior
and posterior distributions p((cid:126)z) and q((cid:126)z|x), using
the gaussian reparameterization trick of kingma
and welling (2015). we train our models with
stochastic id119, and at each gradient
step we estimate the reconstruction cost using a
single sample from q((cid:126)z|x), but compute the kl di-
vergence term of the cost function in closed form,
again following kingma and welling (2015).

figure 1: the core structure of our variational au-
toencoder language model. words are represented
using a learned dictionary of embedding vectors.

3 a vae for sentences

we adapt the variational autoencoder to text
by using single-layer lstm id56s (hochreiter and
schmidhuber, 1997) for both the encoder and the
decoder, essentially forming a sequence autoen-
coder with the gaussian prior acting as a regu-
larizer on the hidden code. the decoder serves as
a special id56 language model that is conditioned
on this hidden code, and in the degenerate setting
where the hidden code incorporates no useful in-
formation, this model is e   ectively equivalent to an
id56lm. the model is depicted in figure 1, and is
used in all of the experiments discussed below.

we explored several variations on this architec-
ture, including concatenating the sampled (cid:126)z to the
decoder input at every time step, using a soft-
plus parametrization for the variance, and using
deep feedforward networks between the encoder
and latent variable and the decoder and latent vari-
able. we noticed little di   erence in the model   s
performance when using any of these variations.
however, when including feedforward networks be-
tween the encoder and decoder we found that it
is necessary to use highway network layers (srivas-
tava et al., 2015) for the model to learn. we discuss
hyperparameter tuning in the appendix.
we also experimented with more sophisticated
recognition models q((cid:126)z|x), including a multistep
sampling model styled after draw (gregor et al.,
2015), and a posterior approximation using nor-
malizing    ows (rezende and mohamed, 2015).
however, we were unable to reap signi   cant gains
over our plain vae.

while the strongest results with vaes to date
have been on continuous domains like images, there
has been some work on discrete sequences: a tech-
nique for doing this using id56 encoders and de-
coders, which shares the same high-level architec-
ture as our model, was proposed under the name
variational recurrent autoencoder (vrae) for the
modeling of music in fabius and van amersfoort
(2014). while there has been other work on includ-
ing continuous latent variables in id56-style mod-
els for modeling speech, handwriting, and music
(bayer and osendorfer, 2015; chung et al., 2015),
these models include separate latent variables per
timestep and are unsuitable for our goal of model-
ing global features.

in a recent paper with goals similar to ours,
miao et al. (2015) introduce an e   ective vae-
based document-level language model that models
texts as bags of words, rather than as sequences.
they mention brie   y that they have to train the
encoder and decoder portions of the network in al-
ternation rather than simultaneously, possibly as a
way of addressing some of the issues that we dis-
cuss in section 3.1.

3.1 optimization challenges

our model aims to learn global latent represen-
tations of sentence content. we can quantify the
degree to which our model learns global features
by looking at the variational lower bound objec-
tive (1). the bound breaks into two terms: the
data likelihood under the posterior (expressed as
cross id178), and the kl divergence of the pos-
terior from the prior. a model that encodes useful
information in the latent variable (cid:126)z will have a non-
zero kl divergence term and a relatively small cross
id178 term. straightforward implementations of
our vae fail to learn this behavior: except in van-
ishingly rare cases, most training runs with most
hyperparameters yield models that consistently set
q((cid:126)z|x) equal to the prior p((cid:126)z), bringing the kl di-
vergence term of the cost function to zero.

when the model does this, it is essentially be-
having as an id56lm. because of this, it can ex-
press arbitrary distributions over the output sen-
tences (albeit with a potentially awkward left-to-
right factorization) and can thereby achieve like-
lihoods that are close to optimal. previous work
on vaes for image modeling (kingma and welling,
2015) used a much weaker independent pixel de-
coder model p(x|(cid:126)z), forcing the model to use the
global latent variable to achieve good likelihoods.
in a related result, recent approaches to image gen-
eration that use lstm decoders are able to do well
without vae-style global latent variables (theis
and bethge, 2015).

this problematic tendency in learning is com-
pounded by the lstm decoder   s sensitivity to sub-
tle variation in the hidden states, such as that in-
troduced by the posterior sampling process. this
causes the model to initially learn to ignore (cid:126)z and
go after low hanging fruit, explaining the data with
the more easily optimized decoder. once this has
happened, the decoder ignores the encoder and lit-
tle to no gradient signal passes between the two,
yielding an undesirable stable equilibrium with the
kl cost term at zero. we propose two techniques
to mitigate this issue.

kl cost annealing in this simple approach to
this problem, we add a variable weight to the kl
term in the cost function at training time. at the
start of training, we set that weight to zero, so
that the model learns to encode as much informa-

linearlinearz    encodinglstmcellencodinglstmcelldecodinglstmcelldecodinglstmcelldecodinglstmcellthis technique is parameterized by a keep rate
k     [0, 1]. we tune this parameter both for our
vae and for our baseline id56lm. taken to the
extreme of k = 0, the decoder sees no input, and is
thus able to condition only on the number of words
produced so far, yielding a model that is extremely
limited in the kinds of distributions it can model
without using (cid:126)z.

4 results: id38

in this section, we report on id38
experiments on the id32 in an e   ort to
discover whether the inclusion of a global latent
variable is helpful for this standard task. for this
reason, we restrict our vae hyperparameter search
to those models which encode a non-trivial amount
in the latent variable, as measured by the kl di-
vergence term of the variational lower bound.

results we used the standard train   test split
for the corpus, and report test set results in ta-
ble 2. the results shown re   ect the training and
test set performance of each model at the training
step at which the model performs best on the de-
velopment set. our reported    gures for the vae
re   ect the variational lower bound on the test like-
lihood, while for the id56lms, which can be eval-
uated exactly, we report the true test likelihood.
this discrepancy puts the vae at a potential dis-
advantage.

in the standard setting,

the vae performs
slightly worse than the id56lm baseline, though
it does succeed in using the latent space to a lim-
ited extent: it has a reconstruction cost (99) better
than that of the baseline id56lm, but makes up for
this with a kl divergence cost of 2. training a vae
in the standard setting without both word dropout
and cost annealing reliably results in models with
equivalent performance to the baseline id56lm, and
zero kl divergence.

to demonstrate the ability of the latent variable
to encode the full content of sentences in addition
to more abstract global features, we also provide
numbers for an inputless decoder that does not
condition on previous tokens, corresponding to a
word dropout keep rate of 0.
in this regime we
can see that the variational lower bound contains
a signi   cantly larger kl term and shows a substan-
tial improvement over the weakened id56lm, which
is essentially limited to using unigram statistics
in this setting. while it is weaker than a stan-
dard decoder, the inputless decoder has the inter-
esting property that its sentence generating pro-
cess is fully di   erentiable. advances in generative
models of this kind could be promising as a means
of generating text while using adversarial training
methods, which require di   erentiable generators.

even with the techniques described in the pre-

figure 2: the weight of the kl divergence term
of variational lower bound according to a typical
sigmoid annealing schedule plotted alongside the
(unweighted) value of the kl divergence term for
our vae on the id32.

tion in (cid:126)z as it can. then, as training progresses, we
gradually increase this weight, forcing the model to
smooth out its encodings and pack them into the
prior. we increase this weight until it reaches 1,
at which point the weighted cost function is equiv-
alent to the true variational lower bound. in this
setting, we do not optimize the proper lower bound
on the training data likelihood during the early
stages of training, but we nonetheless see improve-
ments on the value of that bound at convergence.
this can be thought of as annealing from a vanilla
autoencoder to a vae. the rate of this increase is
tuned as a hyperparameter.

figure 2 shows the behavior of the kl cost term
during the    rst 50k steps of training on penn tree-
bank (marcus et al., 1993) id38 with
kl cost annealing in place. this example re   ects a
pattern that we observed often: kl spikes early in
training while the model can encode information in
(cid:126)z cheaply, then drops substantially once it begins
paying the full kl divergence penalty, and    nally
slowly rises again before converging as the model
learns to condense more information into (cid:126)z.

word dropout and historyless decoding in
addition to weakening the penalty term on the en-
codings, we also experiment with weakening the
decoder. as in id56lms and sequence autoen-
coders, during learning our decoder predicts each
word conditioned on the ground-truth previous
word. a natural way to weaken the decoder is
to remove some or all of this conditioning infor-
mation during learning. we do this by randomly
replacing some fraction of the conditioned-on word
tokens with the generic unknown word token unk.
this forces the model to rely on the latent variable
(cid:126)z to make good predictions. this technique is a
variant of word dropout (iyyer et al., 2015; kumar
et al., 2015), applied not to a feature extractor but
to a decoder. we also experimented with standard
dropout (srivastava et al., 2014) applied to the in-
put id27s in the decoder, but this did
not help the model learn to use the latent variable.

stepkl term weightstepkl term value10.00043510.0156071010.0005557516.9776952010.00070815017.5028523010.00090322512.3231774010.00115230011.3733415010.00146937510.8861436010.00187445010.8507377010.00238952510.8916828010.00304760010.8202869010.00388467510.88054710010.00495175010.88747611010.00630982510.92248512010.00803690010.87452213010.01023197510.96923614010.013018105010.98642415010.016551112510.94229716010.021022120010.98941417010.026669127510.97285918010.03378135011.02459619010.042705142511.0453320010.053856150011.02557321010.067712157511.07247722010.084814165011.09272223010.105745172511.04164324010.131103180011.1288390.01.02.03.04.05.06.07.08.00%20%40%60%80%100%01000020000300004000050000kl term valuekl term weightstepkl term weightkl term valuemodel

train nll train ppl test nll test ppl

standard

inputless decoder

train nll train ppl test nll test ppl

id56lm 100    
vae
98 (2)

95 100    
100 101 (2)

116
119

135    
120 (15)

600 135    
300 125 (15)

> 600
380

table 2: id32 id38 results, reported as negative log likelihoods and as perplexi-
ties. lower is better for both metrics. for the vae, the kl term of the likelihood is shown in parentheses
alongside the total likelihood.

vious section, including the inputless decoder, we
were unable to train models for which the kl diver-
gence term of the cost function dominates the re-
construction term. this suggests that it is still sub-
stantially easier to learn to factor the data distribu-
tion using simple local statistics, as in the id56lm,
such that an encoder will only learn to encode in-
formation in (cid:126)z when that information cannot be
e   ectively described by these local statistics.

5 results: imputing missing words

we claim that the our vae   s global sentence fea-
tures make it especially well suited to the task of
imputing missing words in otherwise known sen-
tences.
in this section, we present a technique
for imputation and a novel evaluation strategy in-
spired by adversarial training. qualitatively, we
   nd that the vae yields more diverse and plausible
imputations for the same amount of computation
(see the examples given in table 3), but precise
quantitative evaluation requires intractable likeli-
hood computations. we sidestep this by introduc-
ing a novel evaluation strategy.

while the standard id56lm is a powerful genera-
tive model, the sequential nature of likelihood com-
putation and decoding makes it unsuitable for per-
forming id136 over unknown words given some
known words (the task of imputation). except in
the special case where the unknown words all ap-
pear at the end of the decoding sequence, sampling
from the posterior over the missing variables is in-
tractable for all but the smallest vocabularies. for
a vocabulary of size v , it requires o(v ) runs of full
id56 id136 per step of id150 or iter-
ated conditional modes. worse, because of the di-
rectional nature of the graphical model given by an
id56lm, many steps of sampling could be required
to propagate information between unknown vari-
ables and the known downstream variables. the
vae, while it su   ers from the same intractability
problems when sampling or computing map im-
putations, can more easily propagate information
between all variables, by virtue of having a global
latent variable and a tractable recognition model.
for this experiment and subsequent analysis, we
train our models on the books corpus introduced
in kiros et al. (2015). this is a collection of text
from 12k e-books, mostly    ction. the dataset,

after pruning, contains approximately 80m sen-
tences. we    nd that this much larger amount of
data produces more subjectively interesting gener-
ative models than smaller standard language mod-
eling datasets. we use a    xed word dropout rate of
75% when training this model and all subsequent
models unless otherwise speci   ed. our models (the
vae and id56lm) are trained as language models,
decoding right-to-left to shorten the dependencies
during learning for the vae. we use 512 hidden
units.

id136 method to generate imputations
from the two models, we use id125 with
beam size 15 for the id56lm and approximate iter-
ated conditional modes (besag, 1986) with 3 steps
of a beam size 5 search for the vae. this allows
us to compare the same amount of computation
for both models. we    nd that breaking decod-
ing for the vae into several sequential steps is nec-
essary to propagate information among the vari-
ables.
iterated conditional modes is a technique
for    nding the maximum joint assignment of a set
of variables by alternately maximizing conditional
distributions, and is a generalization of    hard-em   
algorithms like id116 (kearns et al., 1998). for
approximate iterated conditional modes, we    rst
initialize the unknown words to the unk token. we
then alternate assigning the latent variable to its
mode from the recognition model, and performing
constrained id125 to assign the unknown
words. both of our generative models are trained
to decode sentences from right-to-left, which short-
ens the dependencies involved in learning for the
vae, and we impute the    nal 20% of each sen-
tence. this lets us demonstrate the advantages of
the global latent variable in the regime where the
id56lm su   ers the most from its inductive bias.

adversarial evaluation drawing inspiration
from adversarial training methods for generative
models as well as non-parametric two-sample tests
(goodfellow et al., 2014; li et al., 2015b; denton
et al., 2015; gretton et al., 2012), we evaluate the
imputed sentence completions by examining their
distinguishability from the true sentence endings.
while the non-di   erentiability of the discrete id56
decoder prevents us from easily applying the ad-
versarial criterion at train time, we can de   ne a

but now , as they parked out front and owen stepped out of the car , he could see
true: that the transition was complete . id56lm: it ,     i said . vae: through the driver    s door .

you kill him and his
true: men .

id56lm: .    

vae: brother .

not surprising , the mothers dont exactly see eye to eye with me
true: on this matter .
id56lm: , i said .

vae: , right now .

table 3: examples of using id125 to impute missing words within sentences. since we decode from
right to left, note the stereotypical completions given by the id56lm, compared to the vae completions
that often use topic data and more varied vocabulary.

model

adv. err. (%)
unigram lstm

nll
id56lm

id56lm (15 bm.)
vae (3x5 bm.)

28.32
22.39

38.92
35.59

46.01
46.14

table 4: results for adversarial evaluation of im-
putations. unigram and lstm numbers are the
adversarial error (see text) and id56lm numbers
are the negative log-likelihood given to entire gen-
erated sentence by the id56lm, a measure of sen-
tence typicality. lower is better on both metrics.
the vae is able to generate imputations that are
signi   cantly more di   cult to distinguish from the
true sentences.

very    exible test time evaluation by training a dis-
criminant function to separate the generated and
true sentences, which de   nes an adversarial error.
we train two classi   ers: a bag-of-unigrams lo-
gistic regression classi   er and an lstm logistic re-
gression classi   er that reads the input sentence and
produces a binary prediction after seeing the    nal
eos token. we train these classi   ers using early
stopping on a 80/10/10 train/dev/test split of 320k
sentences, constructing a dataset of 50% complete
sentences from the corpus (positive examples) and
50% sentences with imputed completions (negative
examples). we de   ne the adversarial error as the
gap between the ideal accuracy of the discrimina-
tor (50%, i.e.
indistinguishable samples), and the
actual accuracy attained.

results as a consequence of this experimental
setup, the id56lm cannot choose anything outside
of the top 15 tokens given by the id56   s initial un-
conditional distribution p (x1|null) when produc-
ing the    nal token of the sentence, since it has not
yet generated anything to condition on, and has a
beam size of 15. table 4 shows that this weakness
makes the id56lm produce far less diverse samples
than the vae and su   er accordingly versus the ad-
versarial classi   er. additionally, we include the
score given to the entire sentence with the imputed
completion given a separate independently trained
language model. the likelihood results are com-

parable, though the id56lms favoring of generic
high-id203 endings such as    he said,    gives
it a slightly lower negative log-likelihood. mea-
suring the id56lm likelihood of sentences them-
selves produced by an id56lm is not a good mea-
sure of the power of the model, but demonstrates
that the id56lm can produce what it sees as high-
quality imputations by favoring typical local statis-
tics, even though their repetitive nature produces
easy failure modes for the adversarial classi   er.
accordingly, under the adversarial evaluation our
model substantially outperforms the baseline since
it is able to e   ciently propagate information bidi-
rectionally through the latent variable.

6 analyzing variational models

we now turn to more qualitative analysis of the
model. since our decoder model p(x|(cid:126)z) is a sophis-
ticated id56lm, simply sampling from the directed
graphical model (   rst p((cid:126)z) then p(x|(cid:126)z)) would not
tell us much about how much of the data is being
explained by each of the latent space and the de-
coder. instead, for this part of the evaluation, we
sample from the gaussian prior, but use a greedy
deterministic decoder for p(x|(cid:126)z), the id56lm con-
ditioned on (cid:126)z. this allows us to get a sense of how
much of the variance in the data distribution is be-
ing captured by the distributed vector (cid:126)z as opposed
to the decoder. interestingly, these results qualita-
tively demonstrate that large amounts of variation
in generated language can be achieved by following
this procedure. in the appendix, we provide some
results on small text classi   cation tasks.

6.1 analyzing the impact of word dropout

for this experiment, we train on the books cor-
pus and test on a held out 10k sentence test set
from that corpus. we    nd that train and test set
performance are very similar. in figure 3, we ex-
amine the impact of word dropout on the varia-
tional lower bound, broken down into kl diver-
gence and cross id178 components. we drop out
words with the speci   ed keep rate at training time,
but supply all words as inputs at test time except
in the 0% setting.

we do not re-tune the hyperparameters for each

100% word keep

    no ,     he said .
    thank you ,     he said .

75% word keep

    love you , too .    
she put her hand on his shoulder and followed him
to the door .

50% word keep

0% word keep

    maybe two or two .    
she laughed again , once again , once again , and
thought about it for a moment in long silence .

i i hear some of of of
i was noticed that she was holding the in in of the
the in

table 5: samples from a model trained with varying amounts of word dropout. we sample a vector from
the gaussian prior and apply greedy decoding to the result. note that diverse samples can be achieved
using a purely deterministic decoding procedure. once we use reach a purely inputless decoder in the
0% setting, however, the samples cease to be plausible english sentences.

he had been unable to conceal the fact that there was a logical explanation for his inability to
alter the fact that they were supposed to be on the other side of the house .

with a variety of pots strewn scattered across the vast expanse of the high ceiling , a vase of
colorful    owers adorned the tops of the rose petals littered the    oor and littered the    oor .

atop the circular dais perched atop the gleaming marble columns began to emerge from atop the
stone dais, perched atop the dais .

table 6: greedily decoded sentences from a model with 75% word keep id203, sampling from
lower-likelihood areas of the latent space. note the consistent topics and vocabulary usage.

taking each token xt = argmaxxtp(xt|x0,...,t   1, (cid:126)z).
this allows us to get a sense of how much of the
variance in the data distribution is being captured
by the distributed vector (cid:126)z as opposed to by local
language model dependencies.

these results, shown in table 5, qualitatively
demonstrate that large amounts of variation in
generated language can be achieved by following
this procedure. at the low end, where very lit-
tle of the variance is explained by (cid:126)z, we see that
greedy decoding applied to a gaussian sample does
not produce diverse sentences. as we increase the
amount of word dropout and force (cid:126)z to encode
more information, we see the sentences become
more varied, but past a certain point they begin
to repeat words or show other signs of ungram-
maticality. even in the case of a fully dropped-out
decoder, the model is able to capture higher-order
statistics not present in the unigram distribution.

additionally, in table 6 we examine the e   ect
of using lower-id203 samples from the latent
gaussian space for a model with a 75% word keep
rate. we    nd lower-id203 samples by ap-
plying an approximately volume-preserving trans-
formation to the gaussian samples that stretches
some eigenspaces by up to a factor of 4. this has
the e   ect of creating samples that are not too im-
probable under the prior, but still reach into the
tails of the distribution. we use a random linear
transformation, with matrix elements drawn from
a uniform distribution from [   c, c], with c chosen
to give the desired properties (0.1 in our experi-

figure 3: the values of the two terms of the cost
function as word dropout increases.

run, which results in the model with no dropout
encoding very little information in (cid:126)z (i.e., the kl
component is small). we can see that as we lower
the keep rate for word dropout, the amount of in-
formation stored in the latent variable increases,
and the overall likelihood of the model degrades
somewhat. results from the section 4 indicate
that a model with no latent variable would degrade
in performance signi   cantly more in the presence
of heavy word dropout.

we also qualitatively evaluate samples,

to
demonstrate that the increased kl allows meaning-
ful sentences to be generated purely from contin-
uous sampling. since our decoder model p(x|(cid:126)z) is
a sophisticated id56lm, simply sampling from the
directed graphical model (   rst p((cid:126)z) then p(x|(cid:126)z))
would not tell us about how much of the data is
being explained by the learned vector vs. the lan-
guage model.
instead, for this part of the qual-
itative evaluation, we sample from the gaussian
prior, but use a greedy deterministic decoder for x,

keep ratecross id178kl divergence100%45.010170.01035890%40.8979534.66579975%37.7100228.75151250%33.43363615.130520%34.82576320.906685keep probxentkl100%3.0598720.0009533.06082590%2.7065090.3887723.09528175%2.4625690.6958943.15846350%2.1745061.1098323.2843380%2.2350861.4781373.7132230102030405060100%90%75%50%0%keep ratekl divergencecross id178input
mean
samp. 1
samp. 2
samp. 3

we looked out at the setting sun .
they were laughing at the same time .
ill see you in the early morning .
i looked up at the blue sky .
it was down on the dance    oor .

i went to the kitchen .
i went to the kitchen .
i went to my apartment .
i looked around the room .
i turned back to the table .

how are you doing ?
what are you doing ?
    are you sure ?
what are you doing ?
what are you doing ?

table 7: three sentences which were used as inputs to the vae, presented with greedy decodes from the
mean of the posterior distribution, and from three samples from that distribution.

    i want to talk to you .    
   i want to be with you .    
   i do n   t want to be with you .    
i do n   t want to be with you .
she did n   t want to be with him .

he was silent for a long moment .
he was silent for a moment .
it was quiet for a moment .
it was dark and cold .
there was a pause .
it was my turn .

table 8: paths between pairs of random points in
vae space: note that intermediate sentences are
grammatical, and that topic and syntactic struc-
ture are usually locally consistent.

ments). here we see that the sentences are far less
typical, but for the most part are grammatical and
maintain a clear topic, indicating that the latent
variable is capturing a rich variety of global fea-
tures even for rare sentences.

6.2 sampling from the posterior

in addition to generating unconditional samples,
we can also examine the sentences decoded from
the posterior vectors p(z|x) for various sentences
x. because the model is regularized to produce dis-
tributions rather than deterministic codes, it does
not exactly memorize and round-trip the input. in-
stead, we can see what the model considers to be
similar sentences by examining the posterior sam-
ples in table 7. the codes appear to capture in-
formation about the number of tokens and parts
of speech for each token, as well as topic informa-
tion. as the sentences get longer, the    delity of
the round-tripped sentences decreases.

6.3 homotopies

the use of a variational autoencoder allows us to
generate sentences using greedy decoding on con-
tinuous samples from the space of codes. addi-
tionally, the volume-   lling and smooth nature of
the code space allows us to examine for the    rst
time a concept of homotopy (linear interpolation)
between sentences. in this context, a homotopy be-
tween two codes (cid:126)z1 and (cid:126)z2 is the set of points on the
line between them, inclusive, (cid:126)z(t) = (cid:126)z1   (1   t)+(cid:126)z2   t
for t     [0, 1]. similarly, the homotopy between two

sentences decoded (greedily) from codes (cid:126)z1 and (cid:126)z2
is the set of sentences decoded from the codes on
the line. examining these homotopies allows us to
get a sense of what neighborhoods in code space
look like     how the autoencoder organizes infor-
mation and what it regards as a continuous defor-
mation between two sentences.

while a standard non-variational id56lm does
not have a way to perform these homotopies, a
vanilla sequence autoencoder can do so. as men-
tioned earlier in the paper, if we examine the ho-
motopies created by the sequence autoencoder in
table 1, though, we can see that the transition be-
tween sentences is sharp, and results in ungram-
matical intermediate sentences. this gives evi-
dence for our intuition that the vae learns repre-
sentations that are smooth and       ll up    the space.
in table 8 (and in additional tables in the ap-
pendix) we can see that the codes mostly contain
syntactic information, such as the number of words
and the parts of speech of tokens, and that all in-
termediate sentences are grammatical. some topic
information also remains consistent in neighbor-
hoods along the path. additionally, sentences with
similar syntax and topic but    ipped sentiment va-
lence, e.g.    the pain was unbearable    vs.    the
thought made me smile   , can have similar embed-
dings, a phenomenon which has been observed with
single-id27s (for example the vectors
for    bad    and    good    are often very similar due to
their similar distributional characteristics).

7 conclusion

this paper introduces the use of a variational
autoencoder for natural language sentences. we
present novel techniques that allow us to train
our model successfully, and    nd that it can e   ec-
tively impute missing words. we analyze the la-
tent space learned by our model, and    nd that it
is able to generate coherent and diverse sentences
through purely continuous sampling and provides
interpretable homotopies that smoothly interpo-
late between sentences.

we hope in future work to investigate factoriza-
tion of the latent variable into separate style and
content components, to generate sentences condi-
tioned on extrinsic features, to learn sentence em-
beddings in a semi-supervised fashion for language

understanding tasks like id123, and to
go beyond adversarial evaluation to a fully adver-
sarial training objective.

sepp hochreiter and j  urgen schmidhuber. 1997.
long short-term memory. neural computation
9(8).

references

dzmitry bahdanau, kyunghyun cho, and yoshua
bengio. 2015. id4 by
jointly learning to align and translate. in proc.
iclr.

justin bayer and christian osendorfer. 2015.
learning stochastic recurrent networks. arxiv
preprint arxiv:1411.7610 .

julian besag. 1986. on the statistical analysis of
dirty pictures. journal of the royal statistical
society series b (methodological) pages 48   259.

junyoung chung, kyle kastner, laurent dinh,
kratarth goel, aaron courville, and yoshua
bengio. 2015. a recurrent latent variable model
for sequential data. in proc. nips .

andrew m. dai and quoc v. le. 2015. semi-

supervised sequence learning. in proc. nips .

emily denton, soumith chintala, arthur szlam,
and rob fergus. 2015. deep generative image
models using a laplacian pyramid of adversarial
networks. in proc. nips .

bill dolan, chris quirk, and chris brockett.
2004. unsupervised construction of large para-
phrase corpora: exploiting massively parallel
news sources. in proceedings of the 20th interna-
tional conference on computational linguistics.
association for computational linguistics, page
350.

je    donahue, lisa anne hendricks, sergio
guadarrama, marcus rohrbach, subhashini
venugopalan, kate saenko, and trevor darrell.
2015. long-term recurrent convolutional net-
works for visual recognition and description. in
proc. cvpr.

otto fabius and joost r. van amersfoort. 2014.
arxiv

variational recurrent auto-encoders.
preprint arxiv:1412.6581.

ian goodfellow, jean pouget-abadie, mehdi
mirza, bing xu, david warde-farley, sherjil
ozair, aaron courville, and yoshua bengio.
2014. generative adversarial nets.
in proc.
nips .

karol gregor, ivo danihelka, alex graves, and
daan wierstra. 2015. draw: a recurrent neu-
ral network for image generation.
in proc.
icml.

mohit iyyer, varun manjunatha, jordan boyd-
graber, and hal daum  e iii. 2015. deep un-
ordered composition rivals syntactic methods for
text classi   cation. in proc. acl.

michael kearns, yishay mansour, and andrew y
ng. 1998. an information-theoretic analysis of
hard and soft assignment methods for id91.
in learning in id114, springer, pages
495   520.

yoon kim. 2014. convolutional neural networks

for sentence classi   cation. emnlp .

diederik p. kingma and max welling. 2015. auto-

encoding id58. in proc. iclr.

ryan kiros, yukun zhu, ruslan salakhutdinov,
richard s zemel, antonio torralba, raquel ur-
tasun, and sanja fidler. 2015. skip-thought vec-
tors. arxiv preprint arxiv:1506.06726.

ankit kumar, ozan irsoy, jonathan su, james
bradbury, robert english, brian pierce, pe-
ter ondruska, ishaan gulrajani, and richard
socher. 2015. ask me anything: dynamic mem-
ory networks for natural language processing.
arxiv preprint arxiv:1506.07285.

quoc v. le and tom  a  s mikolov. 2014. distributed
representations of sentences and documents. in
proc. icml.

jiwei li, minh-thang luong, and dan jurafsky.
2015a. a hierarchical neural autoencoder for
paragraphs and documents. in proc. acl.

xin li and dan roth. 2002. learning question
classi   ers.
in proceedings of the 19th interna-
tional conference on computational linguistics-
volume 1 . association for computational lin-
guistics, pages 1   7.

yujia li, kevin swersky, and richard zemel.
2015b. generative moment matching networks.
in proc. icml.

junhua mao, wei xu, yi yang, jiang wang, zhi-
heng huang, and alan yuille. 2015. deep cap-
tioning with multimodal recurrent neural net-
works (m-id56). in proc. iclr.

mitchell p marcus, mary ann marcinkiewicz, and
beatrice santorini. 1993. building a large an-
notated corpus of english: the id32.
computational linguistics 19(2):313   330.

yishu miao, lei yu, and phil blunsom. 2015.
neural variational id136 for text processing.
arxiv preprint arxiv:1511.06038 .

arthur gretton, karsten m borgwardt, malte j
rasch, bernhard sch  olkopf, and alexander
smola. 2012. a kernel two-sample test. jmlr
13(1):723   773.

tom  a  s mikolov, stefan kombrink, luk  a  s burget,
jan honza   cernock`y, and sanjeev khudanpur.
2011. extensions of recurrent neural network
language model. in proc. icassp .

tapani raiko, mathias berglund, guillaume
alain, and laurent dinh. 2015. techniques
for learning binary stochastic feedforward neu-
ral networks. in proc. iclr.

danilo j. rezende and shakir mohamed. 2015.
variational id136 with normalizing    ows. in
proc. icml.

danilo j. rezende, shakir mohamed, and daan
wierstra. 2014. stochastic id26 and
approximate id136 in deep generative mod-
els. in proc. icml.

jasper snoek, hugo larochelle, and ryan p.
adams. 2012. practical bayesian optimization
of machine learning algorithms. in proc. nips .

richard socher, eric h huang, je   rey pennin,
christopher d manning, and andrew y ng.
2011. dynamic pooling and unfolding recur-
sive autoencoders for paraphrase detection. in
advances in neural information processing sys-
tems. pages 801   809.

nitish

ilya

srivastava, geo   rey hinton,

alex
and ruslan
krizhevsky,
salakhutdinov. 2014.
dropout: a simple
way to prevent neural networks from over   tting.
jmlr 15(1):1929   1958.

sutskever,

rupesh kumar srivastava, klaus gre   , and
j  urgen schmidhuber. 2015. training very deep
networks. in proc. nips .

ilya sutskever, oriol vinyals, and quoc v. le.
2014. sequence to sequence learning with neural
networks. in proc. nips .

lucas theis and matthias bethge. 2015. gener-
ative image modeling using spatial lstms. in
proc. nips .

oriol vinyals, alexander toshev, samy bengio,
and dumitru erhan. 2015. show and tell: a
neural image caption generator. in proc. cvpr.

han zhao, zhengdong lu, and pascal poupart.
2015. self-adaptive hierarchical sentence model.
ijcai .

text classi   cation

in order to further examine the the structure of the
representations discovered by the vae, we conduct
classi   cation experiments on paraphrase detection
and question type classi   cation. we train a vae
with a hidden state size of 1200 hidden units on
the books corpus, and use the posterior mean of
the model as the extracted sentence vector. we
train classi   ers on these means using the same ex-
perimental protocol as kiros et al. (2015).

method

accuracy

f1

feats
rae+dp
rae+feats
rae+dp+feats
st
bi-st
combine-st
vae
vae+feats
vae+combine-st
feats+combine-st
vae+combine-st+feats

73.2
72.6
74.2
76.8

73.0
71.2
73.0

72.9
75.0
74.8
75.8
76.9

   
   
   

83.6

81.9
81.2
82.0

81.4
82.4
82.3
83.0
83.8

table 9: results for the msr paraphrase corpus.

paraphrase detection for the task of para-
phrase detection, we use the microsoft research
paraphrase corpus (dolan et al., 2004). we com-
pute features from the sentence vectors of sentence
pairs in the same way as kiros et al. (2015), con-
catenating the elementwise products and the abso-
lute value of the elementwise di   erences of the two
vectors. we train an (cid:96)2-regularized logistic regres-
sion classi   er and tune the id173 strength
using cross-validation.

we present results in table 9 and compare to
several previous models for this task. feats is the
lexicalized baseline from socher et al. (2011). rae
uses the recursive autoencoder from that work, and
dp adds their dynamic pooling step to calculate
pairwise features. st uses features from the uni-
directional skip-thought model, bi-st uses bidirec-
tional skip-thought, and combine-st uses the con-
catenation of those features. we also experimented
with concatenating lexical features and the two
types of distributed features.

we found that our features performed slightly
worse than skip-thought features by themselves
and slightly better than recursive autoencoder fea-
tures, and were complementary and yielded strong
performance when simply concatenated with the
skip-thought features.

question classi   cation we also conduct ex-
periments on the trec question classi   cation
dataset of li and roth (2002). following kiros
et al. (2015), we train an (cid:96)2-regularized softmax
classi   er with 10-fold cross-validation to set the
id173. note that using a linear classi   er
like this one may disadvantage our representations
here, since the gaussian distribution over hidden
codes in a vae is likely to discourage linear sepa-
rability.

we present results in table 10. here, ae is
a plain sequence autoencoder. we compare with
results from a bag of word vectors (cbow, zhao
et al., 2015) and skip-thought (st). we also com-

method
st
bi-st
combine-st
ae
vae
cbow
vae, combine-st
id56
id98

accuracy

91.4
89.4
92.2
84.2
87.0
87.3
92.0

90.2
93.6

table 10: results for trec question classi   ca-
tion.

pare with an id56 classi   er (zhao et al., 2015) and
a id98 classi   er (kim, 2014) both of which, un-
like our model, are optimized end-to-end. we were
not able to make the vae codes perform better
than cbow in this case, but they did outperform
features from the sequence autoencoder.
skip-
thought performed quite well, possibly because the
skip-thought training objective of next sentence
prediction is well aligned to this task:
it essen-
tially trains the model to generate sentences that
address implicit open questions from the narrative
of the book. combining the two representations
did not give any additional performance gain over
the base skip-thought model.

hyperparameter tuning

we extensively tune the hyperparameters of each
model using an automatic bayesian hyperparame-
ter tuning algorithm (based on snoek et al., 2012)
over development set data. we run the model with
each set of hyperpameters for 10 hours, operating
12 experiments in parallel, and choose the best set
of hyperparameters after 200 runs. results for our
id38 experiments are reported in ta-
ble 11 on the next page.

additional homotopies

table 12, on the next page, shows additional homo-
topies from our model. we observe that intermedi-
ate sentences are almost always grammatical, and
often contain consistent topic, vocabulary and syn-
tactic information in local neighborhoods as they
interpolate between the endpoint sentences. be-
cause the model is trained on    ction, including ro-
mance novels, the topics are often rather dramatic.

standard

id56lm vae

inputless decoder
id56lm
vae

embedding dim.
lstm state dim.
z dim.
word dropout keep rate

464
337
   
0.66

353
191
13
0.62

305
68
   
   

499
350
111
   

table 11: automatically selected hyperparameter values used for the models used in the id32
id38 experiments.

amazing , is n   t it ?
so , what is it ?
it hurts , isnt it ?
why would you do that ?
    you can do it .
    i can do it .
i ca n   t do it .
    i can do it .
    do n   t do it .
    i can do it .
i could n   t do it .

no .
he said .
    no ,     he said .
    no ,     i said .
    i know ,     she said .
    thank you ,     she said .
    come with me ,     she said .
    talk to me ,     she said .
    do n   t worry about it ,     she said .

i dont like it , he said .
i waited for what had happened .
it was almost thirty years ago .
it was over thirty years ago .
that was six years ago .
he had died two years ago .
ten , thirty years ago .
    it    s all right here .
    everything is all right here .
    it    s all right here .
it    s all right here .
we are all right here .
come here in    ve minutes .

this was the only way .
it was the only way .
it was her turn to blink .
it was hard to tell .
it was time to move on .
he had to do it again .
they all looked at each other .
they all turned to look back .
they both turned to face him .
they both turned and walked away .

there is no one else in the world .
there is no one else in sight .
they were the only ones who mattered .
they were the only ones left .
he had to be with me .
she had to be with him .
i had to do this .
i wanted to kill him .
i started to cry .
i turned to him .

im    ne .
youre right .
    all right .
you    re right .
okay ,    ne .
    okay ,    ne .
yes , right here .
no , not right now .
    no , not right now .
    talk to me right now .
please talk to me right now .
i    ll talk to you right now .
    i    ll talk to you right now .
    you need to talk to me now .
    but you need to talk to me now .

table 12: selected homotopies between pairs of random points in the latent vae space.

