5
1
0
2

 

v
o
n
9
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
3
2
6
4
0

.

1
1
5
1
:
v
i
x
r
a

under review as a conference paper at iclr 2016

learning to represent words in context
with multilingual supervision

kazuya kawakami, chris dyer
department of computer science
carnegie mellon university
pittsburgh, pa 15213, usa
{kkawakam, cdyer}@cs.cmu.edu

abstract

we present a neural network architecture based on bidirectional lstms to com-
pute representations of words in the sentential contexts. these context-sensitive
word representations are suitable for, e.g., distinguishing different word senses
and other context-modulated variations in meaning. to learn the parameters of
our model, we use cross-lingual supervision, hypothesizing that a good represen-
tation of a word in context will be one that is suf   cient for selecting the correct
translation into a second language. we evaluate the quality of our representations
as features in three downstream tasks: prediction of semantic supersenses (which
assign nouns and verbs into a few dozen semantic classes), low resource machine
translation, and a lexical substitution task, and obtain state-of-the-art results on all
of these.

1

introduction

distributed representations of words, which represent each word as a vector in a low-dimensional
space, can be learned from unannotated text corpora using a variety of techniques (mikolov et al.,
2013; pennington et al., 2014; landauer & dumais, 1997). the value of such representations owes
to their ability to capture intuitive notions of syntactic and semantic similarity as geometric local-
ity. despite their empirically proven value as a source of features in many downstream applica-
tions (turian et al., 2010), the    one word type, one vector    assumption made by most word repre-
sentation models is problematic because words may have multiple meanings.

two standard solutions to this problem exist. the    rst to treat each word as a collection of dis-
crete, mutually exclusive senses which are individually represented as vectors (tian et al., 2014;
neelakantan et al., 2014; wu & giles, 2015; huang et al., 2012; jauhar et al., 2015). however,
identifying the appropriate sense granularity in such models is dif   cult in practice and in the-
ory (kilgarriff, 1997; erk et al., 2013). the second solution, which is the basis of this work, eschews
sense inventories (whether latent or explicit) and says that lexical meaning is a function of word and
its context (erk & pad  o, 2008; kintsch, 2001; mitchell & lapata, 2008). while previous work has
hinted at the promise of this solution, only a small number of hand-crafted word   context compo-
sition functions have been considered thus far in the literature on semantic representation learning.
this is surprising given the success of learning composition functions for computing phrase and
sentence representations (socher et al., 2011; kalchbrenner et al., 2014).

there are two central challenges faced by learning to represent words in context. the    rst is to
identifying a suitable function class for the composition function. such a function must be able to
account for the fact that a single word type may have both several completely unreleated meanings
as well as a several more or less distinct but still related meanings (cruse, 2000). for an example of
the former, the word plant may refer, depending on context, to a factory or to a living organism that
photosynthesizes. for an example of the latter, the word bank may refer to a    nancial institution
or the building housing a    nancial institution. since bidirectional id56-lstms have been shown
to be able to learn both compositional (bahdanau et al., 2014) as well as more arbitrary relation-
ships (ling et al., 2015), we use these as our composition function class (  2).

1

under review as a conference paper at iclr 2016

the second challenge is to identify an appropriate supervisory signal that will be used to    t the
parameters of the function. our motivating hypothesis   which follows a long line of work in
using parallel data as a source of information about semantics (bannard & callison-burch, 2005;
resnik & yarowsky, 1999; diab, 2003; faruqui & dyer, 2014; hermann & blunsom, 2014)   is
that a good representation of a word in context will be one that predicts how that word (in its senten-
tial context) translates into a second language (  3). we show that word-in-context representations
can be learned ef   ciently from pairs of words-in-context and single word translations into a second
language which are extracted from parallel corpora using a word alignment model.

to evaluate our proposed model and training criterion, we evaluate our learned representations as
features in three tasks: supersense tagging, low-resource machine translation (i.e., translation where
limited parallel data is available), and a lexical substitution task. success in each of these requires
models that can effectively capturing the meaning of a word in context, and in each, we show our
model obtains state-of-the-art performance (  4). additionally, the feedforward neural net model
we use as a baseline for supersense tagging outperforms existing baselines even without our new
word-in-context model.

2 model

our model for contextual words is a bidirectional sequence model based on recurrent neural net-
works (chan et al., 2015; bahdanau et al., 2014; 2015, inter alia). intuitively, this model allows
us to condition on arbitrarily long dependencies while having an implicit bias toward more local
contexts.
let w = (w1, w2, . . . , wn) be the words in a sentence with length n. we also project all words
into a    xed d-dimensional vectors x = (x1, x2, . . . , xn), using a (one-word-per-type) word lookup
table.

the model encodes each token of the sentence from left to right according to the standard long-short
term memory recurrences:

it =   (wxixt + whiht   1 + wcict   1 + bi)
ft =   (wxf xt + whf ht   1 + wcf ct   1 + bf )
ct = ft     ct   1 + it     tanh(wxcxt + whcht   1 + bc)
ot =   (wxoxt + whoht   1 + wcoct + bo)
ht = ot     tanh(ct)

      
this yields a representation
ht for each position in the sentence t which can be interpreted as the
representation of word with its left context w1, w2, . . . , wt. the same process is repeated from right
to left, yielding a vector

      
ht. the concatenation of these two vectors

is our word-in-context representation.

2.1 model intuition

ht = [

      
ht;

      
ht],

type-level id27s must necessarily represent information about multiple senses in a sin-
gle vector, and our task is to obtain. to obtain a representation of a word in its context, we want to
apply functions which mask or scale some dimensions of the vector according to its context. thus,
functions which apply same scaling function even the word and context are different, average and
multi-layer id88 for example, may not be suitable. the input gate in long-short term memory
is considered to be a suitable scaling function which take target word and its context (xt, ht, ct   1).
figure 1 show a simpli   ed version of operation to modulate one sense (vegetable plant) from am-
biguous type level vector with semantic mask which is conditioned on word and context.

3 meaning and translation

we now require a training objective that provides supervision for learning the parameters of this
model. the question we want to answer is: what is a suitable proxy (or    grounding   ) for the mean-

2

under review as a conference paper at iclr 2016

semantic mask

word in context

  

type level word representation

green

plant

figure 1: description of the operation to modulate sense from ambiguous type level vector.

ings of words in context that we can use to construct token-level (rather than type-level) word repre-
sentations?

to illustrate the problem we wish to solve, consider the meaning of the token bank in the following
sentences:

    i went to the bank to deposit my paycheck.
    i went to the river bank to eat some lunch.

one very productive strategy for learning semantic id27s is to rely on the distributional
hypothesis (harris, 1954), according to which semantically similar items occur in similar contexts.
the distributional hypothesis is, furthermore, practically appealing since it enables semantics to be
learned from large, unannotated text corpora.

despite the empirical success of the distributional hypothesis at obtaining representations of word
types, creating a representation of word tokens in terms of context is conceptually unappealing since
both the item being embedding and its context potentially share material. one possible solution
would be an autoencoding objective, or one might also distinguish between    narrow    and    wide   
context (i.e., one that determines the item being embedding and one that provides supervision).1
however, we instead advocate using an alternative proxy for meaning: how words translate. con-
sider the two examples from above as they might be translated into french.

    je suis all  e `a la banque pour d  eposer mon ch`eque de paie.
    je suis all  e sur la rive pour le d  ejeuner.

the homonymous (i.e., having two completely unrelated senses) word bank has been translated into
two different words banque and rive in french.

finally, while not quite so copious as monolingual corpora, parallel data exist in convenient elec-
tronic form in abundance, and this provides a rich resource for learning about the semantics of
natural language.

3.1 objective & parameter learning

to operationalize our hypothesis that translation provides a good supervisory signal for learning
semantic representations, we learn the parameters of source language word type embeddings and
the composition function (i.e., the parameters of the bidirectional lstms) by using the computed
representation to compute the lexical translation id203 of a word in context. that is, we use
the computed token embedding to de   ne a id203 estimate that a source language word et in
context c = (e1, . . . , et   1, et+1 . . . , en) translates into a second language as f in vocabulary f. i.e.,
p(f | et, c).

1for example, mikolv et al. (2013) showed that short multiword expressions could be embedding by using

the    wider    context that they occur in.

3

under review as a conference paper at iclr 2016

this is done by performing a softmax over the target vocabulary with the representation of the word
ht, as de   ned in the previous section. that is, we compute
u = rht + b   

p(f | et, c) =

exp(uf )

pf       f exp(uf     )

,

where parameters r and b    de   ne the projection of the source word with context representation ht
onto the target vocabulary f.
to obtain pairs of words in context and their lexical translations into a second language, we use un-
supervised word alignment techniques (dyer et al., 2013), to obtain high precision word alignments
from a parallel corpus. while modeling alignments as latent variables, or using a soft attention
mechanism would be a reasonable alternative, word alignment is fast and the proposed training
objective to be easily scaled to large corpora.

figure 2 illustrates the pre-training architecture.

lexical translation ( fr )

r e p r i s e

p l a n t e

e n t

word in context  (        ) 

ht

~~

~~

   ..

<s>

the

plant

grows

</s>

figure 2: description of cross lingual pre-training model.

3.2 parameter learning

the model parameters w and b as well as the word projection parameters ve are    rst pre-trained
with the objective function:

l =     x
(f ,e)

log p(f | e, c)

that is, we wish to    nd the parameters that maximize the lexical translation log id203 over the
whole parallel corpus of lexical translations (f ) of a source word (e) in context (c).

when we want to transfer the model to another supervised task to predict label s     s for a word e in
context c, the    nal values of the w and b parameters are transferred and formulate a similar model
to predict label s. using the transformation matrix s     r|s|  dh and the biases b           r|s|, we may
de   ne the label id203 as

u    = sht + b      
exp(u   

s)

ps       s exp(u   

s    )

.

p(s | et, c) =

the model is training by maximizing the log likelihood of the observed label in the task.

l    =     x
(s,e)

log p(s | e, c)

(1)

4

under review as a conference paper at iclr 2016

table 1: summary of parallel data.

dataset data source

en-fr
en-de
en-cs
en-fi
en-mg
en-ur

europarl-v7
europarl-v7
europarl-v7
europarl-v7

cmu

nist mt08

vocabulary

source
93,393
93,033
51,833
91568
57,668
43,524

target
139,934
323,367
164,770
630184
76,469
44,566

token

source

target

50,586,497
48,625,466
16,150,983
48,584,379
1,592,66
1,055,030

52,900,470
46,484,368
13,785,699
34,819,783
2,023,336
1,180,031

sentence

1,835,733
1,763,744
594,158
1,779,397
80,306
161,173

4 experiments

we now turn to a series of experiments to show the value of learning representations of words in
context according to the objective above. our paradigm will be to pre-train using the objective
above the parameters of a word-in-context model, and then use these (without further    ne tuning)
in downstream tasks: prediction of semantic supersenses (which assign nouns and verbs into a few
dozen semantic classes), low resource machine translation, and a lexical substitution task.

4.1 model configuration and pre-training

to pre-train our model, we extracted words (e) in contexts and translations (f ) from the europ-
erl parallel corpus (koehn, 2005). we conducted experiments with the following four languages:
french (fr), german (de), czech (cs), finnish (fi) which are quite typologically diverse. table1
shows the numbers of parallel sentences and the numbers of words. for each language pairs, we
used 2000 sentences for development and the rest were used for training.

after normal id121, we obtained alignments with fast-align tool (dyer et al., 2013). since
we are modeling single word translations and want high-quality training instances, we run the align-
ment model in both directions and obtained symmetric alignments by taking intersection between
forwards and backward alignments. to control the size of vocabulary, we took 30,000 most com-
mon words. for target languages, we removed 10 most common words. the words not in the
vocabularies are replaced with hunki token. we used sentences which have more than 10 words in a
sentence.

we used 300 dimension embeddings for source language, and bi-directional lstms have 300 hidden
units. the trained parameters are source embedding, weights and bias in the model.

we randomly initialized source id27s sampled from uniform distribution from    0.08
to 0.08. all recurrent materices with orthogonal initialization (saxe et al., 2013), and non-recurrent
weights are initialized from scaled uniform distribution (glorot & bengio, 2010). mini-batches of
size 128 are used. we used adam algorithm for optimization (kingma & ba, 2014). we trained
models with early-stopping. the perplexities on development data for english to french, german,
czech and finnish are 3.80, 6.49, 6.30, 19.25 respectively.

4.2 supersense tagging

supersenses can be thought of a generalization of words senses into a universal inventory of seman-
tic types. that is, as the number of word senses tend to be too numerous for existing models to
generalize properly with the small amounts of data available, supersenses address this problem by
id91 all senses into a tractable set of tags. table 4.2 show examples of supersense tags and
its de   nition. as such, these are generally used in semantically oriented downstream tasks such as
co-reference resolution (o   connor & heilman, 2013) and id53 (pasca & harabagiu,
2001).

following previous work, we trained our supersense tagger for nouns and verbs on the semcor
dataset. the semcor datasets consists of three parts, brown1, brown2, and brownv. we mixed these
three parts and trained supersense tagger on randomly split 4/5 of data and the rest were used as a de-
velopment set. we evaluated our model on the held-out senseval-3 all-words task (mihalcea et al.,

5

under review as a conference paper at iclr 2016

2004), as done in previous work on supersense tagging (ciaramita & altun, 2006; yuret & yatbaz,
2010). since some tokens are annotated with two labels in ambiguous cases, we followed the heuris-
tics of only using the    rst sense in the data as the correct synset/supersense (ciaramita & altun,
2006). to extract supersenses from the semcor data, we used id138 version 2.0 synsets.

to avoid the computational overhead of reading extremely wide contexts, we used sliding window
to delimit the range of contexts as in (collobert et al., 2011), that is, each token wt is embedded
using a context window of words wt   n/2, . . . , wt, . . . , wt+n/2. the window size n was    xed to 20.
we use the pre-trained parameters and we put a new task speci   c softmax leyer on top of the hidden
units (fig.2). we updated all parameters including pre-trained parameters. the weights in the
softmax layer were initialized from the scaled uniform distribution (glorot & bengio, 2010). mini-
batches of size 128 were used with the adam update rule (kingma & ba, 2014).

since this task has not previously been studied using neural networks, we also report several novel
baselines: (1) multi-layer id88 model which uses a concatenation of a source word type vector
and the average of all word type vectors in its context; (2) a forward-only lstm model; and (3)
a bi-directional lstm with random initialization (rather than cross-lingual pretraining). for fair
comparison in terms of the size of word in context representation, we double the hidden unit size of
the forward lstm model.

supersense nouns denoting
act
artifact
feeling
group
location

acts or actions
man-made objects
feelings and emotions
groupings of people or objects
spatial position

supersense
change
communication
possession
plant
social

verbs denoting
size, temperature change
telling, asking, ordering, singing
buying, selling, owning
plants
political and social activities

table 2: examples of noun and verb supersenses

4.3 lexical translation in low resource language

we investigate the bene   t to transfer cross lingually pre-trained word-in-context representation to
translation in low-resource language. since low-resource languages do not have enough data to
adequate estimate translation probabilities, we hope that we can learn more effective mappings with
pre-trained word-in-context embeddings (chahuneau et al., 2013).

we trained lexical translation model, which predict translation of aligned english sentence, for low
resource languages, malagasy and urdu on top of the pre-trained word in context model. table
1 shows the numbers of parallel sentences and the number of words. we used a dataset used
in (dou et al., 2014) for malagasy and the urdu data we used is a part of nist mt evaluation
in 2008-20122. we used 2000 sentences for development and hold-out test set. we    ltered out sen-
tences which have less than 3 words for pre-training and words occur less then 1 time are replaced
with hunki token.
we trained our baseline system with cdec (dyer et al., 2010) and obtained synchronous context-free
grammars rules to translate sentences. we added features, translation id203 and log translation
id203 from our translation model and optimized the parameters of a machine translation system
with mira, margin-infused relaxed algorithm (crammer & singer, 2003).

4.4 lexical substitution

lexical substitution is the problem of identifying meaning-preserving substitutes for a target word
given a sentential context. the task was introduced in semeval-2007 (mccarthy & navigli, 2007)
involves both    nding the synonyms and disambiguating the context. as such, it is an ideal test case
for our representations.

models are evaluated on their ability to predict the substitutes in the gold standard of the ls-se
test-set. we evaluated our model on best and best-mode task which evaluate the quality of the best

2https://catalog.ldc.upenn.edu/ldc2010t21

6

under review as a conference paper at iclr 2016

table 3: summary of results for supersense tagging.

method
random
baseline
id48
crf
mlp
lstm
bi-lstm
bi-lstm (fr)
bi-lstm (de)
bi-lstm (cs)
bi-lstm (fi)
bi-lstm (average)

semcor

precision recall
43.0
69.3
70.5
80.2
81.9
82.6
84.2
85.0
85.2
85.0
85.1
85.1

38.2
63.9
76.7
80.3
82.0
82.1
83.5
84.8
85.2
84.9
85.0
85.0

f1
40.4
66.5
77.7
80.2
81.6
82.1
83.6
84.7
85.0
84.7
84.8
84.8

senseval3
precision recall
42.1
68.7
73.7

35.8
60.1
67.6

-

-

83.6
83.8
84.6
85.8
86.2
85.9
85.9
86.0

79.7
81.9
82.9
82.8
82.7
82.8
82.4
82.7

f1
38.7
64.1
70.5

-

81.2
82.5
83.3
84.0
84.1
84.1
83.9
84.0

table 4: summary of results for translation in low resource languages.

mg

ur

perplexity     id7     perplexity     id7    

method
bi-lstm (random init)
bi-lstm (fr)
bi-lstm (de)
bi-lstm (cs)
bi-lstm (fi)
bi-lstm (average)

16.17
12.80
12.97
13.05
13.07
12.97

21.7
21.9
21.9
22.0
22.0
22.0

30.87
26.84
26.38
26.21
25.96
26.34

21.2
21.7
21.4
21.4
21.5
21.5

predictions. the original task allow to make multiple predictions but we only predict only one
substitution following (melamud et al., 2015). this task is challenging, since it requires to    nd the
best substitutes from entire word vocabulary.

the way to make prediction is the following. given a target word and it   s context, we infer word in
context representation of all possible substitutions. then take one of the most similar words which
have highest cosine similarity with target word in context vector as prediction.

for our experiments, we used a simple word alignment base candidate generation to reduce id136
time. for a target word in english, we collect all possible french translations from word alignment
and took english words 90% most frequently aligned to the french words as candidates. we used
same candidates for all our experiments including baseline for fair comparison.

5 result

5.1 supersense tagging

table 3 shows frequency weighted precision, recall f1 score3 on semcor test set and senseval3
all-words task. our bidirectional lstm model (bi-lstm) outperformed the    rst sense heuristic
baseline, the id88 trained hidden markov model proposed in (ciaramita & altun, 2006). and
our new word-in-context pre-training model result in further improvements with all language pairs.
the averaged score of 4 cross lingually pre-trained models, as in bi-lstm (average), shows signi   -
cant improvements over bi-lstm. the model pre-trained with german achieved best result f1 84.1
on senseval3. additionally, the baseline neural network models outperforms existing baselines even
without cross lingual supervision.

3it can result in an f-score that is not between precision and recall.

7

under review as a conference paper at iclr 2016

table 5: summary of results for lexical substitution.

method
base
mult
balmult
add
baladd
skipgram (baseline)
bi-lstm (fr)
bi-lstm (de)
bi-lstm (cs)
bi-lstm (fi)
bi-lstm (average)

best    
7.81
6.64
8.09
7.37
8.14
7.77
9.54
10.63
9.74
8.51
9.60

best mode    

13.41
10.89
13.41
12.11
13.41
13.16
15.79
18.09
16.04
12.99
15.73

table 6: disambiguation with multilingual supervision.

sentence

translation candidate for plant

they built a large plant to manufacture automobiles.
let   s plant    owers in the garden.

usine, installation, plante, centrale
plantes, planter, v  eg  etal, v  eg  etale, cultiver

5.2 lexical translation in low resource language

table 4 shows results on machine translation in low resource language. we report the averaged
id7 score of 5 runs to avoid optimizer randomness clark et al. (2011). the result show large
improvement on perplexity and consistent improvement on id7 in all language pairs. the average
score of 4 cross lingually trained model improved perplexity by around 3 points and id7 score by
0.3.

5.3 lexical substitution

table 5 shows results on lexical substitution task. since our word-in-context representations are
build only on europerl parallel corpora, the baseline system is skipgram id27 trained
on english side of en-fr parallel corpora, which is the largest in the corpus. the skipgram model
which take most similar word as prediction is context in-sensitive baseline. also we compared our
results with various context sensitive models, which take arithmetic mean (as in add and baladd)
and a geometrical mean (as in mult and balmult) of embeddings, proposed by (melamud et al.,
2015). they trained their baseline embeddings (as in base) on a two billion word web corpus,
ukwac (ferraresi et al., 2008).
the model achieved best measures4 10.63, best mode measure 18.90 with german supervision. and
the second best result was obtained with czech. as for comparison with melamud et al. (2015), we
cannot compare score directly since we used different corpus and candidate generation. we should
compare performance gain by taking into account context. their best model (baladd) achieved 0.33
performance gain with context where our model achieved 2.9 performance gain on best evaluation.

6 discussion

we proposed the model to predict lexical translation to build word-in-context representation. ta-
ble. 6 shows example of disambiguation with translation model in order of translation pribability.
the model correctly disambiguate industrial plant (usine in french), and vegetable plant (plantes
in french). figure 3 shows the effect of pre-trained word-in-context representation for downstream
tasks. pre-trained model start from low perplexities at the    rst update and converged earlier, in two
epochs, for low resource machine translation.

4evaluation was done by a script provided by the task organizer.

8

under review as a conference paper at iclr 2016

figure 3: effect of cross-lingual pre-training for supersense tagging (left) and for low resource
machine translation (right). orange lines are the result from the model pre-trained with french.

we investigated the effect of 4 linguistically diverse language. the results shows the bene   t of
cross-lingual pre-training in all languages, but overall the model trained with german have stable
results and the model trained with finnish tend to underperform others, especially on lexical substi-
tution task where we do not have supervised    ne-tuning process. this is probably because the large
vocabulary of finnish which is two times bigger than german.

7 related work

word representation. distributed word representations were successfully applied to several
downstream tasks such as chunking, parsing, id31 and paraphrase detection. most
of the tasks requires to use not only word representation but representation of phrases or documents.
in the previous works, many architectures were proposed to learn and use word representation. in
the sequence modeling problems such as bio chunking, conditional random    elds and recurrent
neural networks are applied to represent a sequence of word representations (turian et al., 2010;
mesnil et al., 2013). for classi   cation tasks such as document classi   cation, id31,
paraphrase detection, summation of id27s (lauly et al., 2014), convolutional neural net-
works (kalchbrenner et al., 2014) and recursive networks (socher et al., 2013; cheng & kartsaklis,
2015) were proposed to represent compositionality function of words.

learning semantics from parallel data. previous works show methods to improve word or doc-
ument level representation by incorporating multilingual context. faruqui & dyer (2014) proposed
canonical correlation analysis (cca) based method to improve the quality of type level representa-
tion by projecting word representations of translation pairs (obtained by automatic word alignments)
to be maximally correlated in common vector space. hermann & blunsom (2013) propose compo-
sitional vector space model (cvm) to build sentence representation. they represent a sentence as
the sum of its word representations and they train word representation by constraining the represen-
tations of parallel sentences to be close. coulmance et al. (2015) shows that predicting context in
target language is an effective way to train word representation shared across languages. hill et al.
(2014) investigated the quality of id27 learned by id4 model and
show its bene   t on tasks that require modeling word similarity.

compositional vector models. most prior work on compositional vector models has looked pri-
marily at the problem of computing representations of complete phrases rather than speci   cally
words in context. furthermore, one can learn reasonable generalizations from models that condition
on and the generate text using an autoencoding objective (socher et al., 2011). dhillon et al. (2012)
make the intriguing proposition that left- and right- contexts can be used to supervise each other.

9

under review as a conference paper at iclr 2016

references
bahdanau, dzmitry, cho, kyunghyun, and bengio, yoshua. id4 by jointly

learning to align and translate. arxiv preprint arxiv:1409.0473, 2014.

bahdanau, dzmitry, chorowski, jan, serdyuk, dmitriy, brakel, phil  emon, and bengio, yoshua.

end-to-end attention-based large vocabulary id103. corr, abs/1508.04395, 2015.

bannard, colin and callison-burch, chris. proc. acl. in id141 with bilingual parallel cor-

pora, 2005.

chahuneau, victor, schlinger, eva, smith, noah a., and dyer, chris.

translating into
morphologically rich languages with synthetic phrases.
in proceedings of the 2013 con-
ference on empirical methods in natural language processing, pp. 1677   1687, seat-
tle, washington, usa, october 2013. association for computational linguistics. url
http://www.aclweb.org/anthology/d13-1174.

chan, william, jaitly, navdeep, le, quoc v., and vinyals, oriol. listen, attend, and spell. corr,

abs/1508.01211, 2015.

cheng, jianpeng and kartsaklis, dimitri. syntax-aware multi-sense id27s for deep

compositional models of meaning. arxiv preprint arxiv:1508.02354, 2015.

ciaramita, massimiliano and altun, yasemin. broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. in proceedings of the 2006 conference on em-
pirical methods in natural language processing, pp. 594   602. association for computational
linguistics, 2006.

clark, jonathan h, dyer, chris, lavie, alon, and smith, noah a. better hypothesis testing for statis-
tical machine translation: controlling for optimizer instability. in proceedings of the 49th annual
meeting of the association for computational linguistics: human language technologies: short
papers-volume 2, pp. 176   181. association for computational linguistics, 2011.

collobert, ronan, weston, jason, bottou, l  eon, karlen, michael, kavukcuoglu, koray, and kuksa,
pavel. natural language processing (almost) from scratch. the journal of machine learning
research, 12:2493   2537, 2011.

coulmance, jocelyn, marty, jean-marc, wenzek, guillaume, and benhalloum, amine. trans-gram,
fast cross-lingual word-embeddings. in proceedings of the 2015 conference on empirical meth-
ods in natural language processing, pp. 1109   1113, lisbon, portugal, september 2015. associ-
ation for computational linguistics. url http://aclweb.org/anthology/d15-1131.

crammer, koby and singer, yoram. ultraconservative online algorithms for multiclass problems.

the journal of machine learning research, 3:951   991, 2003.

cruse, alan. meaning in language. an introduction to semantics and pragmatics, 2000.

dhillon, paramveer s., rodu, jordan, foster, dean p., and ungar, lyle h. two step cca: a new

spectral method for estimating vector models of words. in proc. icml, 2012.

diab, mona talat. id51 within a multilingual framework. phd thesis, uni-

versity of maryland, 2003.

dou, qing, vaswani, ashish, and knight, kevin. beyond parallel data: joint word alignment and

decipherment improves machine translation. in proceedings of emnlp, volume 2014, 2014.

dyer, chris, weese, jonathan, setiawan, hendra, lopez, adam, ture, ferhan, eidelman, vladimir,
ganitkevitch, juri, blunsom, phil, and resnik, philip. cdec: a decoder, alignment, and learning
framework for    nite-state and context-free translation models. in proceedings of the acl 2010
system demonstrations, pp. 7   12. association for computational linguistics, 2010.

dyer, chris, chahuneau, victor, and smith, noah a. a simple, fast, and effective reparameterization

of ibm model 2. in proc. naacl, 2013.

10

under review as a conference paper at iclr 2016

erk, katrin and pad  o, sebastian. a structured vector space model for word meaning in context. in

proc. emnlp, 2008.

erk, katrin, mccarthy, diana, and gaylord, nicholas. measuring word meaning in context. com-

putational linguistics, 39(3):511   554, 2013.

faruqui, manaal and dyer, chris. improving vector space word representations using multilingual

correlation. in proceedings of eacl, volume 2014, 2014.

ferraresi, adriano, zanchetta, eros, baroni, marco, and bernardini, silvia. introducing and evalu-
ating ukwac, a very large web-derived corpus of english. in proceedings of the 4th web as corpus
workshop (wac-4) can we beat google, pp. 47   54, 2008.

glorot, xavier and bengio, yoshua. understanding the dif   culty of training deep feedforward neural
networks. in international conference on arti   cial intelligence and statistics, pp. 249   256, 2010.

harris, zellig. distributional structure. word, 10(23):146   162, 1954.

hermann, karl moritz and blunsom, phil. multilingual distributed representations without word

alignment. arxiv preprint arxiv:1312.6173, 2013.

hermann, karl moritz and blunsom, phil. multilingual models for compositional distributed se-

mantics. in proc. acl, 2014.

hill, felix, cho, kyunghyun, jean, s  ebastien, devin, coline, and bengio, yoshua. embed-
ding word similarity with id4. corr, abs/1412.6448, 2014. url
http://arxiv.org/abs/1412.6448.

huang, eric h., socher, richard, manning, christopher d., and ng, andrew y. improving word

representations via global context and multiple word prototypes. in proc. acl, 2012.

jauhar, sujay k., dyer, chris, and hovy, eduard. ontologically grounded multi-sense representation

learning for semantic vector space models. in proc. naacl, 2015.

kalchbrenner, nal, grefenstette, edward, and blunsom, phil. a convolutional neural network for

modelling sentences. in proc. acl, 2014.

kilgarriff, adam. i don   t believe in word senses. computers and the humanities, 31(2):91   113,

1997.

kingma, diederik and ba, jimmy. adam: a method for stochastic optimization. arxiv preprint

arxiv:1412.6980, 2014.

kintsch, walter. predication. cognitive science, 25:173   202, 2001.

koehn, philipp. europarl: a parallel corpus for id151.

volume 5, pp. 79   86, 2005.

in mt summit,

landauer, thomas k. and dumais, susan. a solution to plato   s problem: the latent semantic analysis
theory of acquisition, induction and representation of knowledge. psychological review, 104(2):
211   240, 1997.

lauly, stanislas, boulanger, alex, and larochelle, hugo. learning id73 representa-

tions using a bag-of-words autoencoder. arxiv preprint arxiv:1401.1803, 2014.

ling, wang, lu    s, tiago, marujo, lu    s, astudillo, ram  on fernandez, amir, silvio, dyer, chris,
black, alan w, and trancoso, isabel. finding function in form: compositional character models
for open vocabulary word representation. arxiv preprint arxiv:1508.02096, 2015.

mccarthy, diana and navigli, roberto. semeval-2007 task 10: english lexical substitution task. in
proceedings of the 4th international workshop on semantic evaluations, pp. 48   53. association
for computational linguistics, 2007.

11

under review as a conference paper at iclr 2016

melamud, oren, levy, omer, and dagan, ido. a simple id27 model for lexical sub-
stitution. in proceedings of the 1st workshop on vector space modeling for natural language
processing, 2015.

mesnil, gr  egoire, he, xiaodong, deng, li, and bengio, yoshua. investigation of recurrent-neural-
in inter-

network architectures and learning methods for spoken language understanding.
speech, pp. 3771   3775, 2013.

mihalcea, r., chklovski, t., and kilgarriff, a. the senseval-3 english lexical sample task.

in
proceedings of senseval-3: third international workshop on the evaluation of systems for the
semantic analysis of text, pp. 25   28, 2004.

mikolov, tomas, chen, kai, corrado, greg, and dean, jeffrey. ef   cient estimation of word repre-

sentations in vector space. corr, abs/1301.3781, 2013.

mikolv, tomas, sutskever, ilya, chen, kai, corrado, greg, and dean, jeffrey. distributed represen-

tations of words and phrases and their compositionality. in proc. nips, 2013.

mitchell, jeff and lapata, mirella. vector-based models of semantic composition. in proc. acl,

2008.

neelakantan, arvind, shankar, jeevan, passos, alexandre, and mccallum, andrew. ef   cient non-
parametric estimation of multiple embeddings per word in vector space. in proc. emnlp, 2014.

o   connor, brendan and heilman, michael. arkref: a rule-based coreference resolution system.

corr, abs/1310.1975, 2013. url http://arxiv.org/abs/1310.1975.

pasca, marius and harabagiu, sanda. the informative role of id138 in open-domain question
answering. in proceedings of naacl-01 workshop on id138 and other lexical resources,
pp. 138   143, 2001.

pennington, jeffrey, socher, richard, and manning, christopher d. glove: global vectors for word

representation. in proc. emnlp, 2014.

reichartz, frank and paa  , gerhard. estimating supersenses with conditional random    elds. pro-

ceedings of ecmlpkdd, 2008.

resnik, philip and yarowsky, david. distinguishing systems and distinguishing senses: new eval-
uation methods for id51. natural language engineering, 5(2):113   133,
1999.

saxe, andrew m., mcclelland, james l., and ganguli, surya. exact solutions to the nonlin-
ear dynamics of learning in deep linear neural networks. corr, abs/1312.6120, 2013. url
http://arxiv.org/abs/1312.6120.

socher, richard, huang, eric h., pennington, jeffrey, ng, andrew y., and manning, chirstopher d.
dynamic pooling and unfolding recursive autoencoders for paraphrase detection. in proc. nips,
2011.

socher, richard, perelygin, alex, wu, jean y, chuang, jason, manning, christopher d, ng, an-
drew y, and potts, christopher. recursive deep models for semantic compositionality over a
sentiment treebank. in proceedings of the conference on empirical methods in natural language
processing (emnlp), volume 1631, pp. 1642. citeseer, 2013.

tian, fei, dai, hanjun, bian, jiang, gao, bin, zhang, rui, chen, enhong, and liu, tie-yan. a

probabilistic model for learning multi-prototype id27s. in proc. coling, 2014.

turian, joseph, ratinov, lev, and bengio, yoshua. word representations: a simple and general

method for semi-supervised learning. in proc. acl, 2010.

wu, zhaohui and giles, c. lee. sense-aware semantic analysis: a multi-prototype word represen-

tation model using wikipedia. in proc. aaai, 2015.

yuret, deniz and yatbaz, mehmet ali. the id87 for unsupervised word sense

disambiguation. computational linguistics, 36(1):111   127, 2010.

12

