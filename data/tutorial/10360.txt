5
1
0
2

 

v
o
n
6

 

 
 
]

g
l
.
s
c
[
 
 

1
v
4
2
0
2
0

.

1
1
5
1
:
v
i
x
r
a

towards a better understanding of predict and

s. sathiya keerthi   

count models
tobias schnabel    

november 9, 2015

rajiv khanna    

abstract

in a recent paper, levy and goldberg [2] pointed out an interesting
connection between prediction-based id27 models and count
models based on pointwise mutual information. under certain conditions,
they showed that both models end up optimizing equivalent objective
functions. this paper explores this connection in more detail and lays out
the factors leading to di   erences between these models. we    nd that the
most relevant di   erences from an optimization perspective are (i) predict
models work in a low dimensional space where embedding vectors can
interact heavily; (ii) since predict models have fewer parameters, they are
less prone to over   tting.

motivated by the insight of our analysis, we show how count models
can be regularized in a principled manner and provide closed-form solu-
tions for l1 and l2 id173. finally, we propose a new embedding
model with a convex objective and the additional bene   t of being intelli-
gible.

1

introduction

distributional semantic models [12], also called count models [1] have been pop-
ular in the computational linguistics literature for several decades. in the last
few years, however, predict models such as the skip-gram model and continu-
ous bag-of-words model have become the de facto standard for word model-
ing [5, 6]. these methods have origins in neural id38 [7], where
the goal was to improve classic language models. a recent evaluation study [1]
suggested that the predict models are superior to the count models on a range
of word similarity tasks. newer work, however, attributes a large amount of the
di   erences in performance between count and predict models to a lack of proper

   cloud & information services lab, microsoft, mountain view, ca 94043
   department of computer science, cornell university, ithaca, ny 14853
   department of electrical and computer engineering, the university of texas at austin,

austin, tx 78712

1

hyperparameter optimization [4]. this has prompted the need for understanding
the di   erences between the two types of models.

to this end, levy and goldberg [2] made an interesting connection between
two key models: a traditional count model based on pointwise mutual infor-
mation (pmi) and a predict model, namely the skip-gram model with negative
sampling (sgns). the key result is that the sgns model is equivalent to a
shifted version of the pmi method, where all values get shifted by a factor of
log k. however, the proof assumes that the input and output dimensions are
the same; hence the word vectors will still be of the size of the vocabulary.
as we show in our analysis, this assumption has important implications for
subsequent steps, such as id84 methods like singular value
decomposition (svd).

the aim of this paper is to analyze connections between count and predict
models in a more detailed fashion. in particular, we investigate the di   erences
between sgns and pmi methods more deeply. we make several observations
that help this understanding. the    rst useful result is that the shifted pmi
model comes out as an explicit version of sgns in which the context word
vectors are    xed at their one-hot representations. we point out two di   erences
between the shifted pmi model and the sgns model from an optimization
perspective. first, sgns usually works in lower dimensions, making it possible
for all word vectors two interact. second, as count models have usually more
parameters, this might cause them to over   t on the data.

we use the insight from our analysis to propose several interesting extensions
to classic id27 models. for example, our analysis allows regulariza-
tion to be added into pmi methods in a systematic way. we also propose a new
convex word vector model based on cbow which o   ers interpretability and a
well-de   ned training objective.

in short, we draw connections between existing count and prediction models
and augment them in several ways. however, in order to fully understand the
factors that di   erentiate various methods, a comprehensive set of experiments
is needed. we plan to carry out these experiments as part of future work.

2 notations
consider a corpus d = {wt}|d|
t=1 that is a sequence of words over a vocabulary
v. we will use w when referring to the index of a target word and c when
referring to the index of a context word. we de   ne the exact notion of context
in section 2.1. note that w and c are integers taking values from 1 to |v|.
each word and context word has a vectorial representation. we will use
w     rm and c     rn to denote the word vector and context vector corresponding
to word w and context word c. for vector x, xi will denote the i-th component
of x. let {w} and {c} denote the set of all word and context word vectors. one
can also think of {w} and {c} as matrices w and c whose w-th and c-th rows
are given by w and c respectively. we will use w and c to synonymously refer
to {w} and {c}.

2

let us use # to denote a count and weighting function.

context word) pair (w, c) in corpus d. let us de   ne #(w,  ) = (cid:80)
and #(  , c) =(cid:80)

in the simplest
case, for example, #(w, c) could denote the number of occurrences of the (word,
c   v #(w, c)
w   v #(w, c), the total counts for word w and context vector c.

2.1 de   ning context

an important design choice in count models is the de   nition of context. the
de   nition of context determines which co-occurrences get considered when cre-
ating a model, along with the importance of single co-occurrences. a common
way of de   ning context is based on choosing a window around each occurrence
of a word w in d. there are many options to customize this window; for ex-
ample, it can either be symmetric or asymmetric around w. there could also
exist weights (e.g., dependent on the relative positions of w and c) associated
with each (w, c) occurrence in d. one may also include down-weighting of com-
mon words, possibly for both for the current word w as well as for the context
words c1. all these choices determine the value of #(w, c). note that since the
weightings can be real, #(w, c), #(w,  ) and #(  , c) can take non-negative real
values in general.

if the window is symmetric around w and as well as the weighting functions

are symmetric, then

1. #(w, c) is symmetric, i.e., #(w, c) = #(c, w) and
2. #(w,  ) = #(  , c) if w and c denote the same word

we will refer to this as the symmetry condition.

in section 3.2.1 we will
revisit this condition and see that it helps our understanding of non-uniqueness
and interchangeability of word vectors and context word vectors.

example. consider an asymmetric window k that includes l words to the

left and r words to the right of a word w. in other words,

k = (   l,       ,   1, 1, . . . , r).

take one (word, context word) pair (wt, wt+i), where i     k. let p (wt) be the
id203 with which word wt is chosen for inclusion into the training set. for
example, mikolov et al [6] suggest the following down-sampling id203:

(cid:114)   

f (wt)

p1(wt) = 1    

(1)

where f (wt) is the frequency of word wt in the corpus d, and    is a threshold.
let p2(wt+i) be the weight associated with wt+i as context word. here again
we may want to down-sample frequent words similar to (1). let p3(i) be the
weight associated with the position of the context word wt+i in the context

1mikolov et al [6] used down-weighting of common words. it is unclear if levy and gold-

berg [2] used it.

3

(cid:88)

window. using all these individual weights we can de   ne the overall weight for
the (wt, wt+i) pair:

p (wt, wt+i) = p1(wt)p2(wt+i)p3(i)

(2)

accumulating all this info over the entire corpus gives us the total value for
(word, context word) pairs:

#(w, c) =

p (wt, wt+i)

(3)

{(t,i):wt=w,wt+i=c}

now #(w,  ) and #(  , c) can be formed using #(w, c) as described in the begin-
ning of this section.

3 background

remember that the goal of this paper is to investigate the di   erences between
count and prediction models. to do this, we start by summarizing two special
instances of count and prediction models. we    rst introduce pmi models as
representatives of count models in section 3.1, and then discuss the skip-gram
model with negative sampling (sgns) in section 3.2.

3.1 pmi models

pmi models have been used extensively in distributional semantic models [12]
to compute similarities between words. as the name implies, the key quantity
in these models is pointwise mutual information (pmi). its de   nition and id113
estimate from data are given by

pmi(w, c) = log

p (w, c)

p (w)p (c)

    log

#(w, c)    |d|
#(w,  )    #(  , c)

,

(4)

where #(w, c) is the simple count function. for a given word w, pmi represents
w by forming a vector whose components are pmi(w, c) for all c     v .

note that pmi is not de   ned when #(w, c) = 0. to circumvent this problem,

positive pmi (ppmi) replaces all negative values, i.e.,

ppmi(w, c) = max(pmi(w, c), 0).

more recently, levy and goldberg [2] have de   ned shifted variants of pmi
and the ppmi metrics. shifted pmi (spmi) is just de   ned as pmi(w, c)   log k,
and shifted positive pmi (sppmi) is de   ned as max(spmi(w, c), 0) where k is
some positive integer. pmi and ppmi are, respectively, special cases of shifted
pmi and shifted ppmi, with k set to 1. we will return to shifted pmi and
shifted ppmi in the next section, where we talk about the equivalence of certain
methods.

as all vectors produced by pmi have the vocabulary size as the dimensional-
ity, id84 techniques are often applied to the original matrix
to decrease the memory and computational requirements.

4

3.2 skip-gram with negative sampling (sgns)

sgns [6] is a popular predict model that aims to predict which word w occurs
with a context word c. one can derive the sgns [6] model from a binary
classi   cation setting where the target variable y speci   es whether a word w
occurs with context word c. sgns tries to solve this task with the logistic loss
applied to the input score x = w    c.

remember that our corpus d is a sequence of words, d = {wt}|d|

t=1. to make
notation easier, here we assume that the context is de   ned by all the words
within a window c t around each word wt. let the corresponding sequence of
sets of context words be {c t}t
t=1. to construct the training set for the sgns
model, one forms one training example for each t and each context word ct     c t.
for each ct     c t, a set of k negative context words, n t
ct is chosen at random and,
the logistic id168 f (wt, ct;n t
ct) is applied. this loss is the aggregation
of losses for one positive example and k negative examples. more formally, the
loss of one training example (cid:96)(wt, c t) is:

(cid:96)(wt, c t) =

f (wt, ct;n t

ct) def= l(wt    ct, 1) +

l(wt    c,   1),

(5)

(cid:88)

ct   ct

(cid:88)

c   n t
ct

where l(w  c, y) is the logistic loss corresponding to the word-context pair (w, c)
and target y     {1,   1}. the loss over the entire corpus is then the sum of all
individual losses:

(cid:88)

(cid:88)

w   v

c   v

(cid:96)(w, c) =

(cid:96)(wt, c t).

(6)

let us refer to this way of writing the training objective (sum over occur-

rences in the corpus) as the corpus format.

t=1

levy and goldberg [2] showed that, by accumulating data over co-occurrences
of words and context words, the objective function in (6) can be rewritten as2:

(cid:96)(w, c) =

(cid:96)w,c(w, c)

(cid:96)w,c(w, c) = #(w, c)l(w    c, 1) + k    #(w,  )    #(  , c)

|d|

(7)

(8)

l(w    c,   1)

let us refer to this (equivalent) way of writing the training objective as the co-
occurrence format. note that in the co-occurrence format, we do not compute
losses for each token individually, but instead compute a loss for each unique
(w, c) pair.

in (8) we will from now on also consider other id168s l(x, y) apart
from the logistic loss like in the original formulation. we consider general loss
functions, l(x, y) (here y     {1,   1} is the target variable) with the logistic loss
2levy and goldberg [2] write the problem as the maximization of likelihood; hence the (cid:96)

here and the one in [2] are negatives of each other.

5

|d|(cid:88)

loss name

logistic
squared

squared hinge

hinge
huber

l(x, y)

1

log(1 + e   yx)
2 (x     y)2
2 (max(1     yx, 0))2
max(1     yx, 0)

l = 1

1

2 (max(1     yx, 0))2 if yx        1;    2yx otherwise

table 1: a list of popular id168s, l.

being a special case. this generalization allows us to de   ne custom id168s
that can incorporate domain knowledge or other side information. table 1 lists
a set of popular id168s.

observation 1 unlike sgns, not all models can be reduced to the co-occurrence
format - an example is the cbow model [5]. in fact, even with the skip-gram
model, the reduction to the co-occurrence format is not possible if we use the
traditional softmax or hierarchical softmax [7] approach to speed up training. in
general, models reducible to the co-occurrence format have to only be based on
counts of words, context words and their co-occurrences. that property makes
them have close relations with count models [1, 2, 10]. another model that can
be expressed in co-occurrence form is glove [9].

3.2.1 remarks on symmetry

in case we assume symmetric windows as outlined in section 2.1 for counting
co-occurrences, we can swap word and context vectors at optimality.
observation 2 suppose (w   , c   ) is a minimizer of (cid:96). then (c   , w   ) is also
a minimizer of (cid:96); in other words, at optimality, if the word vectors and context
word vectors are completely swapped, optimality still holds. thus, depending on
how the numerical optimization of (7) initialized, w    and c    can end up being
completely swapped.

similar swap properties can be given for other models in the co-occurrence

format, e.g., glove [9].

observation 3 note that observation 2 does not mean that w = c.
if (cid:96)
had been a convex function, then, using the fact that any convex combination
of optimal solutions is also optimal, one can show the existence of an optimal
solution with w = c. however, the objective function (cid:96) in (7) is far from
convex. such non-convex objectives are usually associated with optima in which
the symmetric components take very di   erent values.
in general, even when
the symmetry condition does not hold, this discussion indicates that sgns can
end up at di   erent optima depending on the initialization of the optimization
process.

6

observation 4 mikolov et al. [6] derive (7) starting from the problem of pre-
dicting a context word given a word. however, when using negative sampling,
the symmetry condition implies that we get exactly the same formulation and
solution as for a model where we would predict a word given a context word. it is
useful to note that this comment does not hold if traditional softmax or hierarchi-
cal softmax [7] or noise-contrastive estimation [8] is used to model probabilities
with the associated negative likelihood loss.

4 connecting count and predict models

in this section, we revisit the idea that levy and goldberg [2] used to con-
nect pmi models with sgns and extend it. the central step is to explicitly
solve sgns in closed form; we can then see that the explicit solution of sgns
corresponds to the vectors that are constructed in a shifted pmi model.

4.1 solving sgns in closed form

here, we extend the analysis of levy and goldberg [2]. we provide closed-form
solutions for sgns and a broad class of id168s. as levy and goldberg
showed, it turns out that the closed-form solution of the sgns objective is
equivalent to a solution constructed by the shifted pmi model.
in contrast
to levy and goldberg, we apply approximate quadratic analysis to the sgns
objective to give a more detailed insight for a broad class of id168s.
the central idea of the analysis is to assume that, given a su   cient number
of dimensions, each score x = w    c can be minimized independently of all other
scores. let us de   ne

  w,c(x) = (cid:96)w,c(w, c) =   w,c(w    c).

(9)

to get x   

w,c, the minimizer of   w,c, we solve
w,c(x) = #(w, c)l(cid:48)(x, 1) + k    #(w,  )    #(  , c)
  (cid:48)

|d|

l(cid:48)(x,   1) = 0

(10)

the taylor series around x   

w,c is given by
  w,c(x     x   

  w,c(x) = const. +

1
2

  w,c = (#(w, c)l(cid:48)(cid:48)(x   

w,c, 1) +

w,c)2
k    #(w,  )    #(  , c)

|d|

l(cid:48)(cid:48)(x   

w,c,   1))

which, in terms of (cid:96)w,c, is

let

(cid:96)w,c(w, c) = const. +

  w,c(w    c     x   

w,c)2

1
2

  w,c = #(w, c) +

k    #(w,  )    #(  , c)

|d|

7

(11)

(12)

(13)

(14)

for the id168s listed in table 1,   w,c takes the form

  w,c =   w,c  w,c

(15)

table 2 gives details for the various id168s.

observation 5 when #(w, c) = 0, the    rst terms in (8), (10) and (12) in-
w,c to take the extreme value of        for the
volving l(x, 1) go away, causing x   
logistic loss and    1 for other losses. consider the expressions for x   
w,c in table 2
to verify this. this issue does not arise for sgns because it operates with re-
duced dimensional embeddings of words and context words in which information
associated with various (word, context word) pairs interact heavily. we believe
that this is a crucial di   erence between sgns and pmi methods.

table 2: expressions for x   
to the condition under which x   

w,c     0.

w,c and   w,c for various losses. poscondition refers

l

logistic
squared

squared hinge

hinge

huber

x   

w,c

pmi     log k
epmi   k
epmi+k
epmi   k
epmi+k

1 if pmi     log k
   1 otherwise

epmi   k
epmi+k

w,c)  w,c

  (x   

  w,c
w,c)  (x   
  w,c
  w,c
na

poscondition
pmi > log k
pmi > log k

pmi > log k
pmi > log k

  w,c

pmi > log k

observation 6 for building semantic representations that are used for com-
puting similarity, it is often not desirable to have negative components in the
word vectors. poscondition in the last column of table 2 checks when this case
occurs. this condition turns out to be the same for all losses, which is interest-
ing. there is no di   erence between squared loss, squared hinge loss and huber
loss. this is because, in the interval x     [   1, 1], all these losses have identical
  w,c(  ) and the minimizer of   w,c always occurs in [   1, 1]. these three losses
have an expression for x   
w,c that is quite di   erent from that for the logistic loss.
w,c at the extremes: 1 or    1.
hinge loss prefers to set x   

4.2 connecting sgns and shifted pmi

one of the key results of levy and goldberg [2] is that the vectors created
by shifted pmi are a solution to the sgns objective. we use the quadratic
analysis of section 4.1 to say this more cleanly.
let ei denote the unit vector in r|v| whose i-th component is 1 and all other
components are zero. suppose we use a one-hot representation for context
vectors, i.e., we    x c = ec for all c. thus, we are    xing c.

8

observation 7 suppose we    x each context vector to the one-hot represen-
tation given above. then, only w remains as the set of variables, and the
following hold.
(a) the minimizer w    of (8) is given by w = x   

w    w, where x   

w is a vector

with {x   

w,c}c as components.

w,c is the shifted pmi representation as de   ned in section 3.1.

(b) x   
(b) also, (w   , c) together form an optimal solution of sgns.

in other words, we have a closed-form solution for sgns (assuming m = n =
|v|). though levy and goldberg [2] do not mention the above construction,
this is a simple observation that easily follows from their analysis.
proof of observation 7. let   s take one c. by the way we de   ned c, we
have w    c = wc. in (cid:96) given by (7), the variable wc occurs only in the term
(cid:96)w,c. therefore, w   
w,c, which proves part (a) of observa-
tion 7. part (c) follows from the fact that, the pair, (w   , c) is such that (cid:96)w,c
is minimized for every (w, c) pair and it is not possible to do better than that.

c = arg min (cid:96)w,c = x   

5 count and predict models: di   erences

empirically, there seems to be evidence that predict and count models perform
di   erently (e.g., [1]). this is interesting since they all consider the same input
data     namely the co-occurrences of words in text. what are the reasons for this?
although the previous section pointed out a strong connection between sgns
and pmi methods (with observation 7 even indicating a near equivalence),
we believe there are two main di   erences between the two methods pertaining
(a) the dimension of the embeddings, and (b) c being    xed as the one-hot
representation. let us now expand on the two factors.

observation 8 dimension of embeddings. as already mentioned in ob-
servation 5, the small embedding dimension used by sgns for words can be
crucial for learning good word vectors. for example, co-occurring words can
in   uence each other. the full dimension used by pmi methods does not allow
this to happen; the full dimension also has the disadvantage of su   ering from
over   tting due to an excessive number of variables.

observation 9 one-hot representation for c. similar to the previous ob-
servation, there is a di   erence in which variables are optimized. sgns operates
with both, w and c as variables. as shown in subsection 4.2, pmi methods,
on the other hand, correspond to using a    xed one-hot representation for c. an
important consequence of such a representation is that it does not allow close
context words to in   uence each other. future work should empirically investigate
the role of this factor.

9

5.1 further di   erences in shifted ppmi

recall that the solution for sgns given by shifted pmi is unusable in practice,
since we have entries that are       . also, the shifted pmi solution has a high
number of dimensions, and this might not be useful in practice. levy and
goldberg present two heuristics remedy these problems. first, they propose
omitting negative terms in the objective function to make a solution feasible.
second, they suggest a subsequent svd step to reduce the dimensionality of
the shifted ppmi matrix. we below discuss each of these heuristics and their
consequences in more detail.

5.1.1 omitting terms
since in practice, we cannot work with vectors that have        entries, goldberg
and levy propose shifted ppmi instead of shifted pmi to remedy this problem.
shifted ppmi corresponds to leaving out all (w, c) terms from (7) that have
negative shifted pmi values. this is equivalent to a modi   ed sgns method in
which during negative sampling examples not satisfying the poscondition, i.e.,
those with pmi     log k < 0 are left out.

there is two issues with the above approach. first, we no longer can guar-
antee optimality for this solution. second, this also seems inconsistent with the
main idea behind negative sampling, which is to sample from unobserved (w, c)
pairs. levy and goldberg [2] make the following statement in the second para-
graph of section 5.1 of their paper:    we observe that shifted ppmi is indeed
a near-perfect approximation of the optimal solution, even though it discards
a lot of information when considering only positive cells.    however, this does
not explain the role of the discarded terms. in particular, when training sgns
with a low number of embedding dimensions, discarding those terms could be
of real importance.

5.1.2 applying svd

levy and goldberg also propose to apply svd to the sppmi matrix in order
to obtain low-dimensional embeddings. if we follow this step, we loose again
some of the optimality we had with the shifted pmi solution. more formally,
consider a sppmi matrix m whose (w, c)   th term is max(0, x   
w,c). to form
word vectors of dimension d lower than |v|, goldberg and levy suggest to apply
svd to the matrix m . let m = u   v t denote the svd. if one is interested
in an embedding of dimension d < |v|, the best rank d approximation of m ,
ud  dv t
d = ud  d or
the    symmetric version   , wssvd

d is used. to form word vectors, one can use either wsvd

   

= ud

  d.

d

observation 10 levy and goldberg [2] propose that svd is done on m or
one of its variants.3 however, if remaining faithful to sgns is the aim, (13)

3levy and goldberg [2] recommend using the matrix corresponding to shifted ppmi.

10

indicates that the weighting term   w,c also be included and that we solve

  w,c(w    c     x   

w,c)2

(16)

(cid:88)

w,c

min
w,c

1
2

it is non-trivial to solve this problem; an svd based approach will not work.

let us now look at the limiting full case, i.e., d = |v|, to point out some

relations and di   erences within the methods in the pmi class.

   

full = u

observation 11 we can use the full svd of m and de   ne word vectors wfull =
  . note that these word vectors are not the same as
u    and wsymm
shifted ppmi which uses m itself as word vectors. however, because m m t =
u   v t v   u t = u     u t = wfullwt
full, the dot products between any two word
vectors is identical for wsppmi (i.e., m ) and wfull. on the other hand, in gen-
full )t (cid:54)= m m t . what this means is that svd is consistent with
eral, wsymm
shifted ppmi, but symmetric svd is not consistent with shifted ppmi.

full (wsymm

levy and goldberg [2] recommend shifted ppmi and refer to the spectral
word vectors for it as svd and symmetric svd. their experiments showed the
symmetric version to yield better results than sgns.

5.2 summary

figure 1: di   erences between various methods. here    discarding    refers to the
leaving out of certain negative examples satisfying some condition, after their
creation.

the above subsections went into various ways of connecting sgns and pmi
methods and also brought out various di   erences. figure 1 gives a rough and

11

quick view of the various morphings of sgns into di   erent pmi methods. the
use of a small embedding dimension by sgns as opposed to the full dimensional
one hot representation used by pmi methods is probably the most important
di   erence. the discarding of certain negative examples, the approximation of
the original non-linear objective by a quadratic, and the leaving out of the
  w,c factors from the quadratic objective, are additional di   erences. carefully
designed experiments are needed to understand the individual e   ects of each
di   erence on the quality of the resulting word vectors. the various di   erences
also imply that, even though the methods at the two left ends of the    gure
involve low dimensional embeddings, they could be vastly di   erent due to the
various reasons given in this section.

6 extensions

in the following two subsections, we propose two extensions to the basic pmi
model. first, we show how to add id173 to pmi models and give explicit
solutions for l2 and l1 id173. we hope that id173 will help
improve issues with data sparsity. after that, we propose a new convex model
for id27s that is not only easy to learn, but also yields intuitive
word vectors since each dimension corresponds exactly to one context word.

6.1 adding id173 to pmi methods

let us continue with the formulation of section 4.1 and add id173. if
over   tting is one of the causes of the inferior performance of pmi methods com-
pared to sgns, then id173 should help improve performance. consider
the decoupled determination of x = wc. our modi   ed objective is now

argmin

x

  w,c(x) + rw,c(x)

where rw,c is the id173 term. as we argued in section 4.1, each wc
is decoupled from other variables. hence, we can solve each one-dimensional
optimization problem in isolation     even with id173 added. we now
derive closed-form solutions for l2 and l1 id173.

12

6.1.1 l2 id173

figure 2: the determination of the l2 regularized solution. note that, even
though h(x) is a nonlinear function, the straight line from a to b approximates
that part of the curve well.

we can down-weigh frequently occurring words and context words and add a
standard l2 regularizer as follows:

rw,c(x) =   

#(w,  )    #(  , c)

|d|

r(x) where r(x) =

1
2

x2

(17)

we will discuss l1 regularizer r(x) = |x| later. let us also focus only on the
logistic loss. dividing by

#(w,  )  #(  ,c)

we get

|d|

w   
c = arg min

x

epmi log(1 + e   x) + k log(1 + ex) +

  
2

x2

(18)

setting the derivative of the objective to zero and simplifying, we get w   
the solution of

c to be

  x = h(x) where h(x) =

.

(19)

epmi     kex

1 + ex

the line from a to b in figure 2 approximates h(x) nicely in the region where
the optimal x lies. the equation of this line is given by

  h(x) =

epmi     k

2

    epmi     k
2(pmi     log k)

x

(20)

13

solve for   h(x) =   x to get the optimal x as

x = harmonic mean of (pmi     log k,

epmi     k

2  

)

(21)

6.1.2 l1 id173
similarly, it is easy to work out closed form expressions for w   
c for l1 regular-
ization. in this case, many of the optimal values go to zero: the larger the value
of   , more is the number of zeros. there are three cases to consider. for l1
id173, the value, h(0) = epmi   k
case 1.           h(0)       . for this case it is easy to check that, at x = 0, the
left hand side derivative of the objective in (18) is non-positive, and the right
hand side derivative is non-negative. hence w   

c = 0 is the optimal solution.

plays a key role.

2

case 2. h(0) >   . for this case, the right hand side derivative at x = 0 is
negative and so the optimum value is positive. the optimum is found by solving
h(x) =   , which yields

w   
c = log

epmi       
k +   

(22)
case 3. h(0) <      . for this case, the left hand side derivative at x = 0 is
positive and so the optimum value is negative. the optimum is found by solving
h(x) =      , which yields

w   
c = log

epmi +   
k       

(23)

6.1.3 discussion

adding id173 to count models has two bene   ts. first, while the non-
regularized solution (see table 2; same as spmi) is unusable because x   
w,c goes to
extreme values when #(w, c) = 0, regularized solutions are always well-de   ned.
second, we expect id173 to help if over   tting is degrading performance
with pmi models. future work is needed to empirically study and validate the
usefulness of id173 in count models.

6.2 a convex formulation for word vectors

motivated by the analysis in section 4.1, we develop a new and simple con-
vex formulation for determining word vectors. similar to the skip-gram model,
we phrase our model as the task of predicting a word conditioned on context.
however, instead of learning representations for both words and their context
words, we    x the context vectors to a one-hot representation. this makes our
objective function convex. another advantage is that this results in intelligible
models     each vector entry refers to a weight given to a context word. also, to
obtain compact representations, we instead use l1 id173 on the word
vectors instead of learning embeddings in lower-dimensional spaces like tradi-
tional predict models.

14

let us describe the formulation in some more detail. let m denote the
dimension of the vector representation of context. depending on the context
modeling employed, m can be one to several times of |v|. let z     rm denote
the context representation. z is a function of the context, c, written as z(c).
the weight vectors {w} are also points in rm. the score of a positive example
is w   z(c). remember that the input corpus d is a sequence of words, {wt}t
t=1.
let the corresponding sequence of contexts be {c t}t
t=1. we solve the following
minimization problem.

|v|(cid:88)

w=1

min
w

  (cid:107)w(cid:107)1 +

1
t

t(cid:88)

t=1

f (wt; z(c t))

(24)

in the above,    > 0 is a id173 constant4 which can be tuned to balance
sparsity of w and performance; f includes the e   ect of positive as well as
negative examples. other forms of id173 are also possible, for example
a combination of l1 and l2 id173 (generalized elastic net).

context. we can de   ne context in several ways. here are a few possibilities:
1. a simple way is to have a context z     r|v|, with each dimension corre-
sponding to one word in the vocabulary v . similar to the sgns model,
we only assume individual interactions between words and context pairs,
i.e., each z will exactly contain one non-zero entry. this way, we can
rewrite the objective in the co-occurrence form and obtain all vectors in
closed form by accumulating the statistics for each (word, context word)
pair.

2. we can do better than option 1 above and pair a word with a window
of context words simultaneously, i.e., z can now have multiple non-zero
entries. a bag-of-words representation z     r|v| can be used to represent
the context input using the context words. for example, we can set zc =
  (c, w) if c occurs in the context window and zero otherwise, where   (c, w)
is some weighting that is a function of how far c is from w.

3. if we want to make use of word order, we can de   ne l  |v| context inputs
where l is the length of the context window and one block of |v| inputs is
used for each context word, and blocks concatenated in the proper order
of occurrence in the window.

4. depending on the purpose for which the word vectors are developed we
can also use dependency-tree based context features [3]. again, we can
use one-hot representations of these features.

learning. except option 1, aggregating at the individual (w, c) level (to
convert the objective into the co-occurrence form (7)) as well as forming a closed
form solution, is in general infeasible. this means we have to train our model

4one can also think of schemes for making    dependent on w, for example, make   w

dependent on the number of context words to which word w is attached.

15

using the objective in corpus format. to do the training, we can choose from
various optimization strategies. we can employ: (a) proper softmax over all
words; (b) hierarchical softmax [7]; (c) noise-contrastive estimation (nce) [8];
or, (d) negative sampling like in sgns [5].

we now present examples for two of the four optimization strategies men-

tioned above. with softmax, our objective is

f (wt; z(c t)) =    wt    z(c t) + log

(cid:33)

(cid:32)(cid:88)

w

w    z(c t)

.

(25)

let us now consider optimization via negative sampling. let {wj
a randomly sampled set of negative examples. then

neg}k

j=1 denote

f (wt; z(c t)) =     log   (wt    z(c t))     k(cid:88)

log   (   wj

neg    z(c t)),

(26)

where   (x) = 1/(1 + e   x).

j=1

observation 12 one of the advantages of the convex formulation described
above is interpretability. if the rth component of some w is large and positive, it
can be directly translated to the role played by the rth context term of z(c). this
direct interpretability also means that domain knowledge may be easy to infuse
into the formulation by speci   cation of constraints and extra id173 on
the weights.

observation 13 the model developed in this section is in between conven-
tional low-dimensional id27s (no interpretability, allows for higher-
order e   ects) and distributional word representations (full interpretability, no
higher-order e   ects). here, the term    higher-order e   ects    refers to the e   ects
(in   uences) that occur during the joint optimization of context and word repre-
sentations, e.g, two words can be made similar because they occur in contexts
that are also similar.

7 conclusion

in this report, we showed how to explicitly solve the sgns objective for a broad
range of id168s. this step allowed us to connect shifted pmi models to
sgns models under general id168s. furthermore, we pointed out two
important di   erences between the shifted pmi model and sgns model. first,
in the sgns model, far fewer embeddings are used in practice, making the
sgns model less prone to over   tting. second, in the pmi model, the context
vectors are    xed; hence, there is also no interaction between context vectors and
word vectors.

finally, we proposed two extensions to existing models. first, we showed
how we can incorporate id173 into pmi models to alleviate over   tting.

16

second, we presented a new embedding model that not only has a convex ob-
jective, but also results in intelligible embeddings. future work is needed to
empirically validate our proposed methods and extensions.

references

[1] m. baroni, g. dinu and g. kruszewski. don   t count, predict! a system-
atic comparison of context-counting vs. context-predicting semantic vectors.
acl, 2014.

[2] o. levy and y. goldberg. neural id27 as implicit factorization.

nips 2014.

[3] o. levy and y. goldberg. linguistic regularities in sparse and explicit

word representations. conll, 2014.

[4] o. levy, y. goldberg and i. dagan. improving distributional similarity with

lessons learned from id27s. tacl, 2015.

[5] t. mikolov, k. chen, g. corrado, and j. dean. e   cient estimation of word

representations in vector space. corr, abs/1301.3781, 2013.

[6] t. mikolov, i. sutskever, k. chen, g.s. corrado, and j. dean. distributed
representations of words and phrases and their compositionality. nips, 2013.

[7] a. mnih and g. hinton. a scalable hierarchical distributed language model.

nips, 2009.

[8] a. mnih and k. kavukcuoglu. learning id27s e   ciently with

noise-contrastive estimation. nips, 2013.

[9] j. pennington, r. socher, c. d. manning. glove: global vectors for word

representation. emnlp 2014.

[10] t. shi and z. liu. linking glove with id97.arxiv: 1411.5595v2, 2014.

[11] k. stratos, m. collins and d. hsu. model-based id27s from

decompositions of count matrices. acl, 2015.

[12] p. turney and p. pantel. from frequency to meaning: vector space models

of semantics. jair, 2010.

17

