6
1
0
2

 
r
a

m
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
8
1
0
6
0

.

1
1
5
1
:
v
i
x
r
a

published as a conference paper at iclr 2016

segmental recurrent neural networks

lingpeng kong, chris dyer
school of computer science
carnegie mellon university
pittsburgh, pa 15213, usa
{lingpenk, cdyer}@cs.cmu.edu
noah a. smith
computer science & engineering
university of washington
seattle, wa 98195, usa
nasmith@cs.washington.edu

abstract

we introduce segmental recurrent neural networks (sid56s) which de   ne,
given an input sequence, a joint id203 distribution over segmentations of
the input and labelings of the segments. representations of the input segments
(i.e., contiguous subsequences of the input) are computed by encoding their con-
stituent tokens using bidirectional recurrent neural nets, and these    segment em-
beddings    are used to de   ne compatibility scores with output labels. these lo-
cal compatibility scores are integrated using a global semi-markov conditional
random    eld. both fully supervised training   in which segment boundaries and
labels are observed   as well as partially supervised training   in which segment
boundaries are latent   are straightforward. experiments on handwriting recog-
nition and joint chinese id40/id52 show that, compared to
models that do not explicitly represent segments such as bio tagging schemes and
connectionist temporal classi   cation (ctc), sid56s obtain substantially higher
accuracies.

1

introduction

for sequential data like speech, handwriting, and dna, segmentation and segment-labeling are
abstractions that capture many common data analysis challenges. we consider the joint task of
breaking an input sequence into contiguous, arbitrary-length segments while labeling each segment.
our new approach to this problem is the segmental id56 (sid56). sid56s com-
bine two powerful machine learning tools: representation learning and id170. first,
id182 (id56s) embed every feasible segment of the input in a con-
tinuous space, and these embeddings are then used to calculate the compatibility of each candidate
segment with a label. unlike past id56-based approaches (e.g., connectionist temporal classi   cation
or ctc; graves et al., 2006) each candidate segment is represented explicitly, allowing application
in settings where an alignment between segments and labels is desired as part of the output (e.g.,
protein secondary structure prediction or information extraction from text).
at the same time, sid56s are a variant of semi-markov conditional random    elds (sarawagi & co-
hen, 2004), in that they de   ne a id155 distribution over the output space (segmen-
tation and labeling) given the input sequence (  2). this allows explicit modeling of statistical de-
pendencies, such as those between adjacent labels, and also of segment lengths (unlike widely used
symbolic approaches based on    bio    tagging; ramshaw & marcus, 1995). because the id203
score decomposes into chain-structured clique potentials, polynomial-time id145
algorithms exist for prediction and parameter estimation (  3).

1

published as a conference paper at iclr 2016

parameters can be learned with either a fully supervised objective   where both segment boundaries
and segment labels are provided at training time   and partially supervised training objectives   
where segment boundaries are latent (  4).
we compare sid56s to strong models that do not explicitly represent segments on handwriting
recognition and joint id40 and part-of-speech tagging for chinese text, showing sig-
ni   cant accuracy improvements in both, demonstrating the value of models that explicitly model
segmentation even when segmentation is not necessary for downstream tasks (  5).
2 model

given a sequence of input observations x = (cid:104)x1, x2, . . . , x|x|(cid:105) with length |x|, a segmental recur-
rent neural network (sid56) de   nes a joint distribution p(y, z | x) over a sequence of labeled
segments each of which is characterized by a duration (zi     z+) and label (yi     y ). the segment
durations constrained such that(cid:80)|z|
i=1 zi = |x|. the length of the output sequence |y| = |z| is
a random variable, and |y|     |x| with id203 1. we write the starting time of segment i as
si = 1 +(cid:80)j<i zj.
to motivate our model form, we state several desiderata. first, we are interested in the following
prediction problem,

y    = arg max

y

p(y | x) = arg max

p(y, z | x)     arg max

y

max

z

p(y, z | x).

(1)

y (cid:88)z

note the use of joint maximization over y and z as a computationally tractable substitute for
marginalizing out z; this is commonly done in natural language processing.
second, for problems where the explicit durations observations are unavailable at training time and
are inferred as a latent variable, we must be able to use a marginal likelihood training criterion,

l =     log p(y | x) =     log(cid:88)z

p(y, z | x).

(2)

in eqs. 1 and 2, the id155 of the labeled segment sequence is (assuming kth order
dependencies on y):

p(y, z | x) =

1

z(x)

|y|

(cid:89)i=1

exp f (yi   k:i, zi, x)

(3)

where z(x) is an appropriate id172 function. to ensure the expressiveness of f and the
computational ef   ciency of the maximization and marginalization problems in eqs. 1 and 2, we use
the following de   nition of f,

f (yi   k:i, zi, xsi:si+zi   1) = w

(cid:62)

  (v[gy(yi   k); . . . ; gy(yi); gz(zi);

         id56(csi:si+zi   1);         id56(csi:si+zi   1)] + a) + b

(4)

where          id56(csi:si+zi   1) is a recurrent neural network that computes the forward segment em-
bedding by    encoding    the zi-length subsequence of x starting at index si,1 and          id56 computes
the reverse segment embedding (i.e., traversing the sequence in reverse order), and gy and gz are
functions which map the label candidate y and segmentation duration z into a vector representation.
the notation [a; b; c] denotes vector concatenation. finally, the concatenated segment duration,
label candidates and segment embedding are passed through a af   ne transformation layer param-
eterized by v and a and a nonlinear activation function    (e.g., tanh), and a dot product with a
vector w and addition by scalar b computes the log potential for the clique. our proposed model
is equivalent to a semi-markov conditional random    eld with local features computed using neural
networks. figure 1 shows the model graphically.

1rather than directly reading the xi   s, each token is represented as the concatenation, ci, of a forward and
backward over the sequence of raw inputs. this permits tokens to be sensitive to the contexts they occur in, and
this is standardly used with neural net sequence labeling models (graves et al., 2006).

2

published as a conference paper at iclr 2016

we chose bidirectional lstms (graves & schmidhuber, 2005) as the implementation of the id56s
in eq. 4. lstms (hochreiter & schmidhuber, 1997) are a popular variant of id56s which have
been seen successful in many representation learning problems (graves & jaitly, 2014; karpathy &
fei-fei, 2015). bidirectional lstms enable effective computation for embedings in both directions
and are known to be good at preserving long distance dependencies, and hence are well-suited for
our task.

figure 1: graphical model showing a six-frame input and three output segments with durations
z = (cid:104)3, 2, 1(cid:105) (this particular setting of z is shown only to simplify the layout of this    gure; the model
assigns probabilities to all valid settings of z). circles represent random variables. shaded nodes are
observed in training; open nodes are latent random variables; diamonds are deterministic functions
of their parents; dashed lines indicate optional statistical dependencies that can be included at the
cost of increased id136 complexity. the graphical notation we use here draws on conventions
used to illustrate neural networks and id114.

3

id136 with id145

we are interested in three id136 problems: (i)    nding the most probable segmentation/labeling
for a model given a sequence x; (ii) evaluating the partition function z(x); and (iii) computing
the posterior marginal z(x, y), which sums over all segmentations compatible with a reference se-
quence y. these can all be solved using id145. for simplicity, we will assume
zeroth order markov dependencies between the yis. extensions to the kth order markov dependen-
cies should be straightforward. since each of these algorithms relies on the forward and reverse
segment embeddings, we    rst discuss how these can be computed before going on to the id136
algorithms.

3.1 computing segment embeddings
let the       h i,j designate the             id56 encoding of the input span (i, j), traversing from left to right, and
let       h i,j designate the reverse direction encoding using             id56. there are thus o(|x|2) vectors that
must be computed, each of length o(|x|). naively this can be computed in time o(|x|3), but the

3

x1x2x3x4x5x6((encoder biid56segmentation/labeling modelx1x2x1x2x1x2x1x2x1x1x2x1c1c2c3c4c5c6z1z2z3y1y2y3x1x2x3x4x5x6 !h1,3 !h4,5 !h6,6  h6,6  h4,5  h1,3published as a conference paper at iclr 2016

following dynamic program reduces this to o(|x|2):

      h i,i =             id56(      h 0, ci)
      h i,j =             id56(      h i,j   1, cj)
      h i,i =             id56(      h 0, ci)
      h i,j =             id56(      h i+1,j, ci)

the algorithm is executed by initializing in the values on the diagonal (representing segments of
length 1) and then inductively    lling out the rest of the matrix. in practice, we often can put a upper
bound for the length of a eligible segment thus reducing the complexity of runtime to o(|x|). this
savings can be substantial for very long sequences (e.g., those encountered in id103).

3.2 computing the most probable segmentation/labeling and z(x)
|x|) different labelings
for the input sequence x, there are 2|x|   1 possible segmentations and o(|y |
of these segments, making exhaustive computation entirely infeasible. fortunately, the partition
function z(x) may be computed in polynomial time with the following dynamic program:

  0 = 1

  j =(cid:88)i<j

  i  

(cid:88)y   y(cid:16)exp w

(cid:62)

  (v[gy(y); gz(zi);         id56(csi:si+zi   1);         id56(csi:si+zi   1)] + a) + b(cid:17) .

after computing these values, z(x) =   |x|. by changing the summations to a max operators (and
storing the corresponding arg max values), the maximal a posteriori segmentation/labeling can be
computed.
both the partition function evaluation and the search for the map outputs run in time o(|x|2    |y |)
with this dynamic program. adding nth order markov dependencies between the yis adds requires
additional information in each state and increases the time and space requirements by a factor of
o(|y |n). however, this may be tractable for small |y | and n.
avoiding over   ow. since this dynamic program sums over exponentially many segmentations
and labelings, the values in the   i chart can become very large. thus, to avoid issues with over   ow,
computations of the   i   s must be carried out in log space.2

3.3 computing z(x, y)

to compute the posterior marginal z(x, y), it is necessary to sum over all segmentations that are
compatible with a label sequence y given an input sequence x. to do so requires only a minor
modi   cation of the previous dynamic program to track how much of the reference label sequence y
has been consumed. we introduce the variable m as the index into y for this purpose. the modi   ed
recurrences are:

  0(0) = 1

  j(m) =(cid:88)i<j

  i(m     1)  
(cid:62)

(cid:16)exp w

the value z(x, y) is   |x|(|y|).

  (v[gy(yi); gz(zi);         id56(csi:si+zi   1);         id56(csi:si+zi   1)] + a) + b(cid:17) .

2an alternative strategy for avoiding over   ow in similar dynamic programs is to rescale the forward sum-
mations at each time step (rabiner, 1989; graves et al., 2006). unfortunately, in a semi-markov architecture
each term in   i sums over different segmentations (e.g., the summation for   2 will have contain some terms
that include   1 and some terms that include only   0), which means there are no common factors, making this
strategy inapplicable.

4

published as a conference paper at iclr 2016

4 parameter learning

we consider two different learning objectives.

4.1 supervised learning

in the supervised case, both the segment durations (z) and their labels (y) are observed.

l = (cid:88)(x,y,z)   d     log p(y, z | x)
= (cid:88)(x,y,z)   d

log z(x)     log z(x, y, z)

in this expression, the unnormalized id155 of the reference segmentation/labeling,
given the input x is written as z(x, y, z).

4.2 partially supervised learning

in the partially supervised case, only the labels are observed and the segments (the z) are unobserved
and marginalized.

l = (cid:88)(x,y)   d     log p(y | x)
= (cid:88)(x,y)   d (cid:88)z   z(x,y)
= (cid:88)(x,y)   d

log z(x)     log z(x, y)

    log p(y, z | x)

for both the fully and partially supervised scenarios, the necessary derivatives can be computed
using automatic differentiation or (equivalently) with backward variants of the above dynamic pro-
grams (sarawagi & cohen, 2004).

5 experiments

we present two sets of experiments to compare segmental recurrent neural networks against mod-
els that do not include explicit representations of segmentation. for the handwriting recognition
task, we consider connectionist temporal classi   cation (ctc) (graves et al., 2006); for chinese
id40, we consider bio tagging. in these experiments, we do not include markovian
dependencies between adjacent labels for our models or the baselines.

5.1 online handwriting recognition

dataset we use the handwriting dataset from kassel (1995). this dataset is an online collection of
hand-written words from 150 writers. it is recorded as the coordinates (x, y) at time t plus special
pen-down/pen-up notations. we break the coordinates into strokes using the pen-down and pen-up
notations. one character typically consists one or more contiguous strokes.3
the dataset is split into train, development and test set following kassel (1995). table 1 presents the
statistics for the dataset.
a well-know variant of this dataset was introduced by taskar et al. (2004). taskar et al. (2004)
selected a    clean    subset of about 6,100 words and rasterized and normalized the images of each
letter. then, the uppercased letters (since they are usually the    rst character in a word) are removed
and only the lowercase letters are used. the main difference between our dataset and theirs is that
their dataset is    of   ine        taskar et al. (2004) mapped each character into a bitmap and treated the
segmentation of characters as a preprocessing step. we use the richer representation of the sequence
of strokes as input.

3there are infrequent cases where one stroke can go across multiple characters or the strokes which form

the character can be not contiguous. we leave those cases for future work.

5

published as a conference paper at iclr 2016

#words
4,368
1,269
637
6,274

#characters

37,247
10,905
5,516
53,668

train
dev
test
total

table 1: statistics of the online handwriting recognition dataset

implementation we trained two versions of our model on this dataset, namely, the fully supervised
model (  4.1), which takes advantage of the gold segmentations on training data, and the partially
supervised model (  4.2) in which the gold segmentations are only used in the evaluation. a ctc
model reimplemented on the top of our encoder biid56s layer (figure 1) is used as a baseline so
that we can see the effect of explicitly representing the segmentation.4 for the decoding of the
ctc model, we simply use the best path decoding, where we assume that the most probable path
will correspond to the most probable labeling, although it is known that pre   x search decoding can
slightly improve the results (graves et al., 2006).
as a preprocessing step, we    rst represented each point in the dataset using a 4 dimensional vector,
p = (px, py,    px,    py), where px and py are the normalized coordinates of the point and    px
and    py are the corresponding changes in the coordinates with respect to the previous point.    px
and    py are meant to capture basic direction information. then we map the points inside one
stroke into a    xed-length vector using a bi-direction lstm. speci   cally, we concatenated the last
position   s hidden states in both directions and use it as the input vector x for the stroke.
in all the experiments, we use adam (kingma & ba, 2014) with    = 1    10   6 to optimize the
parameters in the models. we train these models until convergence and picked the best model over
the iterations based on development set performance then report performance on the test set.
we used 5 as the hidden state dimension in the bidirectional id56s, which map the points into    xed-
length stroke embeddings (hence the input vector size 5  2 = 10). we set the hidden dimensions of c
in our model and ctc model to 24 and segment embedding h in our model as 18. these dimensions
were chosen based on intuitively reasonable values, and it was con   rmed on development data
that they performed well. we tried to experiment with larger hidden dimensions and we found the
performance did not vary much. future work might more carefully optimize these parameters.
as for speed, the partially supervised sid56s run at    40 instances per second and the fully super-
vised sid56s run at    53 instances during training using a single cpu.
results the results of the online handwriting recognition task are presented in table 2. we see that
both of our models outperform the baseline ctc model, which does not carry an explicit represen-
tation for the segments being labeled, by a signi   cant margin. an interesting    nding is, although the
partially supervised model performs slightly worse in the development set, it actually outperforms
the fully supervised model in the test set. because the test set is written by different people from the
train and development set, they exhibit different styles in their handwriting; our results suggest that
the partially supervised model may generalize better across different writing styles.

5.2

joint chinese id40 and id52

in this section, we will look into two related tasks. the    rst task is joint chinese id40
and id52, where the z variables will group the chinese characters into words and the y
variables assign pos tags as labels to these words. we also tested our model on pure chinese word
segmentation task, where the assignments of z is the only thing we care about (simulated using a
single label for all segments).

4the ctc interpretation rules specify that repeated symbols, e.g. aa will be interpreted as a single token
of a. however since the segments in the handwriting recognition problem are extremely short, we use different
rules and interpret this as aa. that is, only the blank symbol may be used to represent extended durations. our
experiments indicate this has little effect, and graves (p.c.) reports that this change does not harm performance
in general.

6

published as a conference paper at iclr 2016

dev

test

fseg

rseg

error
pseg
98.7% 98.4% 98.6% 4.2% 99.2% 99.1% 99.2% 2.7%
98.9% 98.6% 98.8% 4.3% 98.8% 98.6% 98.6% 5.4%
13.8%

15.2%

error

rseg

fseg

pseg

-

-

-

-

-

-

sid56s (partial)
sid56s (full)

ctc

table 2: hand-writing recognition task

dev
rseg

test
rseg

biid56s
sid56s

biid56s
sid56s

pseg
fseg
93.2% 92.9% 93.0% 94.7% 95.2% 95.0%
93.8% 93.8% 93.8% 95.3% 95.8% 95.5%

fseg

pseg

rtag

ptag
ftag
87.1% 86.9% 87.0% 88.1% 88.5% 88.3%
89.0% 89.1% 89.0% 89.8% 90.3% 90.0%

rtag

ftag

ptag

table 3: joint chinese id40 and id52

dataset we used standard benchmark datasets for these two tasks. for the joint chinese word
segmentation and id52 task, we use the penn chinese treebank 5 (xue et al., 2005), fol-
lowing the standard train/dev/test splits. for the pure chinese id40 task, we used the
sighan 2005 dataset5. this dataset contains four portions, covering both simpli   ed and traditional
chinese. since there is no pre-assigned dev set in this dataset (only train and test set are provided),
we manually split the original train set into two, one of which (roughly the same size as the test
set) is used as the dev set. for both tasks, we use wang2vec (ling et al., 2015) to generate the
pre-trained character embeddings from the chinese gigaword (graff & chen, 2005).
implementation only supervised version sid56s (  4.1) is tested in these tasks. the baseline model
is a bi-directional lstm tagger (basically the same structure as our encoder biid56s in figure 1). it
takes the c at each time step and pushes it through an element-wise non-linear transformation (tanh)
followed by an af   ne transformation to map it to the same dimension as the number of labels. the
total loss is therefore the sum of negative log probabilities over the sequence. greedy decoding is
applied in the baseline model, making it a zeroth order model like our sid56s.
in order to perform segmentation and id52 jointly, we composed the pos tags with    b    or
   i    to represent the segmentation point. for the segmentation-only task, in the sid56s we simply
used same dummy tag for all y and only care about the z assignments. in the biid56 case, we used
   b    and    i    tags.
for both tasks, the dimension for the input character embedding is 64. for our model, the dimension
for c and the segment embedding h is set to 24. for the baseline bi-directional lstm tagger, we set
the hidden dimension (the c equivalent) size to 128. here we deliberately chose a larger size than
in our model hoping to make the number of parameters in the bi-directional lstm tagger roughly
the same as our model. we trained these models until convergence and picked the best model over
iterations based on its performance on the development set.
as for speed, the sid56s run at    3.7 sentence per second during training on the ctb dataset using
a single cpu.
results table 3 presents the results for the joint chinese id40 task. we can see that
in both segmentation and id52, the sid56s achieve higher f -scores than the biid56s.
table 4 presents the results for the pure chinese id40 task. the sid56s perform
better than the biid56s with the exception of the pku portion of the dataset. the reason for this is
probably because the training set in this portion is the smallest among the four. thus leads to high
variance in the test results.

5http://www.sighan.org/bakeoff2005/

7

published as a conference paper at iclr 2016

biid56s

sid56s
rseg

fseg

rseg

pseg
fseg
92.7% 93.1% 92.9% 93.3% 93.7% 93.5%
cu
92.8% 93.5% 93.1% 93.2% 94.2% 93.7%
as
msr 89.9% 90.1% 90.0% 90.9% 90.4% 90.7%
pku 91.5% 91.2% 91.3% 90.6% 90.6% 90.6%

pseg

table 4: chinese id40 results on sighan 2005 dataset. there are four portions of
the dataset from city university of hong kong (cu), academia sinica (as), microsoft research
(msr) and peking university (pku). the former two are in traditional chinese and the latter two
are in simpli   ed chinese.

6 related work

segmental labeling problems have been widely studied. a widely used approach to a segmental
labeling problems with neural networks is the connectionist temporal classi   cation (ctc) objective
and decoding rule of graves et al. (2006). ctc reduces the    segmental    sequence label problem to
a classical sequence labeling problem in which every position in an input sequence x is explicitly
labeled by interpreting repetitions of input labels   or input labels followed by a special    blank    out-
put symbol   as being a single label with a longer duration. during training, the marginal likelihood
of the set of labelings compatible (according to the ctc interpretation rules) with the reference label
y is maximized. ctc has demonstrated impressive success in various fully discriminative end-to-
end id103 models (graves & jaitly, 2014; maas et al., 2015; hannun et al., 2014, inter
alia).
although ctc has been used successfully and its reuse of conventional sequence labeling architec-
tures is appealing, it has several potentially serious limitations. first, it is not possible to model inter-
label dependencies explicitly   these must instead be captured indirectly by the underlying id56s.
second, ctc has no explicit segmentation model. although this is most serious in applications
where segmentation is a necessary/desired output (e.g., information extraction, protein secondary
structure prediction), we argue that explicit segmentation is potentially valuable even when the seg-
mentation is not required. to illustrate the value of explicit segments, consider the problem of phone
recognition. for this task, segmental duration is strongly correlated with label identity (e.g., while
an [o] phone token might last 300ms, it is unlikely that a [t] would) and thus modeling it explicitly
may be useful. finally, making an explicit labeling decision for every position (and introducing a
special blank symbol) in an input sequence is conceptually unappealing.
several alternatives to ctc have been approached, such as using various attention mechanisms in
place of marginalization (chan et al., 2015; bahdanau et al., 2015). these have been applied to end-
to-end discriminative id103 problem. a more direct alternative to our method   indeed
it was proposed to solve several of the same problems we identi   ed   is due to graves (2012).
however, a crucial difference is that our model explicitly constructs representations of segments
which are used to label the segment while that model relies on a marginalized frame-level labeling
with a null symbol.
the work of abdel-hamid (2013) also seeks to construct embeddings of multi-frame segments.
their approach is quite different than the one taken here. first, they compute representations of
variable-sized segments by uniformly sampling a    xed number of frames and using these to con-
struct a representation of the segment with a simple feedforward network. second, they do not
consider them problem of latent segmentation.
finally, using neural networks to provide local features in conditional random    eld models has also
been proposed for sequential models (peng et al., 2009) and tree-structured models (durrett & klein,
2015). to our knowledge, this is the    rst application to semi-markov structures.

8

published as a conference paper at iclr 2016

7 conclusion

we have proposed a new model for segment labeling problems that learns representations of seg-
ments of an input sequence and then labels these. we outperform existing alternatives both when
segmental information should be recovered and when it is only latent. we have not trained the seg-
mental representations to be of any use beyond making good labeling (or segmentation) decisions,
but an intriguing avenue for future work would be to construct representations that are useful for
other tasks.

acknowledgments

the authors thank the anonymous reviewers, yanchuan sim, and hao tang for their helpful feed-
back. this work was sponsored in part by the defense advanced research projects agency
(darpa) information innovation of   ce (i2o) under the low resource languages for emergent
incidents (lorelei) program issued by darpa/i2o under contract no. hr0011-15-c-0114.

references
abdel-hamid, huda. structural-functional analysis of plant cyclic nucleotide gated ion chan-

nels. phd thesis, university of toronto, 2013.

bahdanau, dzmitry, chorowski, jan, serdyuk, dmitriy, brakel, phil  emon, and bengio, yoshua.

end-to-end attention-based large vocabulary id103. corr, abs/1508.04395, 2015.

chan, william, jaitly, navdeep, le, quoc v., and vinyals, oriol. listen, attend, and spell. corr,

abs/1508.01211, 2015.

durrett, greg and klein, dan. neural crf parsing. in proc. acl, 2015.

graff, david and chen, ke. chinese gigaword. ldc catalog no.: ldc2003t09, 1, 2005.

graves, alex. sequence transduction with recurrent neural networks. in proc. icml, 2012.

graves, alex and jaitly, navdeep. towards end-to-end id103 with recurrent neural

networks. in proc. icml, 2014.

graves, alex and schmidhuber, j  urgen. framewise phoneme classi   cation with bidirectional lstm

and other neural network architectures. neural networks, 18(5):602   610, 2005.

graves, alex, fern  andez, santiago, gomez, faustino, and schmidhuber, j  urgen. connectionist
temporal classi   cation: labelling unsegmented sequence data with recurrent neural networks. in
proc. icml, 2006.

hannun, awni y., case, carl, casper, jared, catanzaro, bryan c., diamos, greg, elsen, erich,
prenger, ryan, satheesh, sanjeev, sengupta, shubho, coates, adam, and ng, andrew y. deep
speech: scaling up end-to-end id103. corr, abs/1412.5567, 2014.

hochreiter, sepp and schmidhuber, j  urgen. long short-term memory. neural computation, 9(8):

1735   1780, 1997.

karpathy, andrej and fei-fei, li. deep visual-semantic alignments for generating image descrip-

tions. in proc. cvpr, 2015.

kassel, robert h. a comparison of approaches to on-line handwritten character recognition. phd

thesis, massachusetts institute of technology, 1995.

kingma, diederik and ba, jimmy. adam: a method for stochastic optimization. arxiv preprint

arxiv:1412.6980, 2014.

ling, wang, dyer, chris, black, alan w, and trancoso, isabel. two/too simple adaptations of

id97 for syntax problems. in proc. naacl, 2015.

maas, andrew l., xie, ziang, jurafsky, dan, and ng, andrew y. lexicon-free conversational speech

recognition with neural networks. in proc. naacl, 2015.

9

published as a conference paper at iclr 2016

peng, jian, bo, liefeng, and xu, jinbo. conditional neural    elds. in proc. nips, 2009.

rabiner, lawrence r. a tutorion on id48 and selected applications in speech

recognition. proc. ieee, 77(2), 1989.

ramshaw, lance a. and marcus, mitchell p. text chunking using transformation-based learning. in

proceedings of the workshop on very large corpora, 1995.

sarawagi, sunita and cohen, william w. semi-markov conditional random    elds for information

extraction. in proc. nips, 2004.

taskar, ben, guestrin, carlos, and koller, daphne. max-margin markov networks. nips, 16:25,

2004.

xue, naiwen, xia, fei, chiou, fu-dong, and palmer, martha. the penn chinese treebank: phrase

structure annotation of a large corpus. natural language engineering, 11(2):207   238, 2005.

10

