6
1
0
2

 

b
e
f
6
2

 

 
 
]
l
c
.
s
c
[
 
 

4
v
6
2
4
6
0

.

1
1
5
1
:
v
i
x
r
a

published as a conference paper at iclr 2016

reasoning in vector space:
an exploratory study of id53

moontae lee   
department of computer science
cornell university university
ithaca, ny 14850, usa
moontae@cs.cornell.edu

xiaodong he, wen-tau yih, jianfeng gao & li deng
microsoft research
redmond, wa 98052, usa
{xiaohe,scottyih,jfgao,deng}@microsoft.com

paul smolensky   
department of cognitive science
johns hopkins university
baltimore, md 21218, usa
smolensky@jhu.edu

abstract

id53 tasks have shown remarkable progress with distributed vec-
tor representation. in this paper, we investigate the recently proposed facebook
babi tasks which consist of twenty different categories of questions that require
complex reasoning. because the previous work on babi are all end-to-end mod-
els, errors could come from either an imperfect understanding of semantics or in
certain steps of the reasoning. for clearer analysis, we propose two vector space
models inspired by tensor product representation (tpr) to perform knowledge
encoding and logical reasoning based on common-sense id136. they together
achieve near-perfect accuracy on all categories including positional reasoning and
path    nding that have proved dif   cult for most of the previous approaches. we
hypothesize that the dif   culties in these categories are due to the multi-relations
in contrast to uni-relational characteristic of other categories. our exploration
sheds light on designing more sophisticated dataset and moving one step toward
integrating transparent and interpretable formalism of tpr into existing learning
paradigms.

1

introduction

ideal machine learning systems should be capable not only of learning rules automatically from
training data, but also of transparently incorporating existing principles. while an end-to-end frame-
work is suitable for learning without human intervention, existing human knowledge is often valu-
able in leveraging data toward better generalization to novel input. id53 (qa) is
one of the ultimate tasks in natural language processing (nlp) on which synergy between the two
capabilities could enable better understanding and reasoning.

recently the facebook babi tasks were introduced to evaluate complex reading comprehension via
qa (weston et al. (2015)); these have received considerable attention. understanding natural ques-
tions, for example in webquestions tasks (berant et al. (2013)), requires signi   cant comprehension
of the semantics, yet reasoning out the answers is then relatively simple (e.g., bordes et al. (2014);

   this research was conducted while the    rst author held a summer internship in microsoft research, red-

mond, and the last author was a visiting researcher there.

1

published as a conference paper at iclr 2016

yih et al. (2015)). in contrast, the synthetic questions in babi require rather complex reasoning over
multiple computational steps while demanding only minimal semantic understanding. as the pre-
vious work on babi consists only of end-to-end models (weston et al. (2014); kumar et al. (2015);
sukhbaatar et al. (2015); peng et al. (2015)), it is unclear whether incorrect answers arise from an
imperfect semantic understanding, inadequate knowledge encoding, or insuf   cient model capac-
ity (dupoux (2015)). this is partly because the current paradigms based on neural networks have no
interpretable intermediate representations which modelers can use to assess the knowledge present
in the vectorial encoding of the system   s understanding of the input sentences. our approach, in
contrast, can illuminate what knowledge is caputred in each representation via the formalism of
tpr.

tensor product representation (tpr), proposed by smolensky (1990); smolensky & legendre
(2006), is a mathematical method to represent complex structures from basic vectorial building
blocks, so called    llers and roles. for example, one can encode a binary tree by binding    ller vec-
tors corresponding to the left- and right-child entities to role vectors corresponding to the    left child   
and    right child    positions, respectively. arbitrary trees can be represented by recursively applying
the same method. as an outer product (i.e., tensor product) realizes the binding operation, both
   ller and role components are decodable from the resulting representation via the inner product; this
is called unbinding. tpr is known to be capable of various applications such as tree operations,
grammar processing and lambda-calculus evaluation (smolensky (2012)).

in this paper, we endeavor to disentangle the problem cleanly into id29, knowledge
encoding, and logical reasoning. proposing two vector-space models inspired by tpr, we    rst pro-
vide an in-depth analysis of the babi dataset by id91, based solely on their logical properties,
the twenty question categories de   ned by babi. such analysis enables us to conjecture why most
existing models, in spite of their complexity, have failed to achieve good accuracy on positional
reasoning and path    nding tasks, whereas peng et al. (2015) achieved successful results. if the babi
tasks turn out to be considerably simpler than intended for its ultimate purpose of providing a major
step towards    ai-complete id53   , then more elaborated tasks will be required to test
the power of proposed qa models such as memory networks.

as a further contribution, we also develop the foundation of a theory that maps id136 for log-
ical reasoning to computation over tprs, generalizing our models under the rigorous tpr for-
malism. due to the page limit, this theoretical foundation is relegated to the supplementary ma-
terials (smolensky et al. (2016)). the experimental results show that accurate id136 based on
common-sense knowledge is transparently attainable in this formalism. we hope our exploration
can contribute to the further improvement of end-to-end models toward the transparency and inter-
pretability. to the best of our knowledge, our in-depth analysis of babi and of logical reasoning
over distributed vectorial representations are each the    rst of their kind.

2 related work

since the seminal work of bengio et al. (2003), researchers have paid increasing attention to various
distributed representations in continuous vector spaces. in the computer science literature, skip-
gram/cbow (mikolov et al. (2013)) and glove (pennington et al. (2014)) are popular models that
are trained based on the distributional similarities in word co-occurrence patterns; they have been
frequently utilized as initial embeddings for a variety of other nlp tasks. in the cognitive science
literature, on the other hand, beagle (jones & mewhort (2007)) and dvrs (ustun et al. (2014))
are trained differently, with random initializations and circular convolution. they assign two vec-
tors for each word: an environmental vector to describe physical properties and a lexical vector to
indicate meaning.

whereas such representations are known to provide a useful way to incorporate prior linguistic
knowledge, their usefulness is not clear for reasoning-oriented tasks. in other contexts, grefenstette
(2013) shows how to simulate predicate logic with matrices and tensors. similarly, rocktaschel et al.
(2014) try to    nd low-dimensional embeddings which can model    rst-order logic in a vectorial
manner. these models are only concentrated on general logic problems without considering nlp
tasks. note that vectorial encodings are necessary in many machine learning models such as neural
networks. reasoning based on linguistic cues in vector space uniquely characterizes our paper
among these relevant work.

2

published as a conference paper at iclr 2016

category 1: single supporting fact
01: mary moved to the bathroom.
02: john went to the hallway.
03: where is mary? bathroom 1
04: daniel went back to the hallway.
05: sandra moved to the garden.
06: where is daniel? hallway 4

category 2: two supporting facts
01: mary went to the kitchen.
02: sandra journeyed to the of   ce.
03: mary got the football there.
04: mary travelled to the garden.
05: where is the football? garden 3 4
06: john travelled to the of   ce.
07: sandra moved to the garden.
08: where is the football? garden 3 4
09: mary dropped the football.
10: mary journeyed to the kitchen.
11: where is the football? garden 9 4

category 3: three supporting facts
01: sandra went back to the hallway.
02: daniel took the apple.
03: john travelled to the kitchen.
04: daniel travelled to the bedroom.
05: daniel got the football there.
06: daniel went to the hallway.
07: where was the apple before the hallway? bedroom 2 6 4
08: mary went back to the bedroom.
09: daniel discarded the football.
10: daniel got the football.
11: mary went to the garden.
12: daniel travelled to the of   ce.
13: daniel went back to the bedroom.
14: where was the football before the bedroom? of   ce 10 13 12
15: daniel went back to the hallway.
16: mary went back to the bathroom.
17: daniel dropped the apple.
18: sandra journeyed to the kitchen.
19: where was the apple before the of   ce? hallway 17 12 6

figure 1: sample statements(black), questions(blue), answers(red), and clues(green) for categories
1, 2, and 3.

the tasks in babi have been studied mainly within the context of the memory network (memnn)
model, which consists of an array of representations called    memory    and four learnable modules:
the i-module encodes the input into feature representation, the g-module updates relevant memory
slots, the o-module performs id136s to compute output features given the input representation
and the current memory, and    nally the r-module decodes the output feature-based representation
to the    nal response. since the proposal of the basic memnn (weston et al. (2014)) model, the
adaptive/nonlinear memnn (weston et al. (2015)), dmn (kumar et al. (2015)), and memn2n
(sukhbaatar et al. (2015)) models have been developed by varying certain parts of these modules.
nonetheless, none of these models except peng et al. (2015) successfully accomplish either posi-
tional reasoning or path    nding tasks. our speculation about the performance by peng et al. (2015)
will be given in a later section based on our babi analysis.

3 models and analysis

the babi dataset consists of twenty different types of questions where each question category is
claimed to be atomic and independent from the others (weston et al. (2015)). in this section, we
investigate clusters of categories with sample qa problems, analyzing what kinds of logical proper-
ties are shared across various types. we also elucidate, based on our vector space models, why it is
dif   cult to achieve good accuracy on certain categories: positional reasoning and path    nding.

3.1 containee-container relationship

supporting facts (1, 2, 3) the    rst three question categories of babi ask for the current or previ-
ous locations of actors and objects based on the statements given prior to the question. category 1   3
questions respectively require precisely one, two, or three supporting facts to reason out the proper
answers. figure 1 illustrates sample statements and questions extracted from real examples in the
training set. reasoning in category 1 implicitly requires a simple common-sense reasoning rule that
   an actor cannot exist in two different locations at the same time.    in order to answer the questions
in category 2, we implicitly need another rule that    an object that belongs to an actor follows its
owner   s location.    further, if an item is dropped at one particular location, it will permanently stay
in that location until someone grabs it and moves around with it later.

while two independent relations, pick/drop and move, seem to be involved in parallel in the category
2 tasks, these questions can be all uniformly answered under the transitivity of a containee belongs
to a container. if an actor moves to a location, he/she (a containee) now belongs to that location

3

published as a conference paper at iclr 2016

(a container). similarly, if an actor acquires an object, the item (a containee) newly belongs to
that actor (a container). transitivity then logically implies that the object belongs to the location
occupied by the owner.

relational translations/answers
mary belongs to the kitchen (from nowhere).
the football belongs to mary.

# statements/questions
1 mary went to the kitchen.
3 mary got the football there.
4 mary travelled to the garden. mary belongs to the garden (from the kitchen). mgt m(g     k)t
5 where is the football?
9 mary dropped the football.
10 mary journeyed to the kitchen. mary belongs to the kitchen (from the garden). mkt m(k     g)t
11 where is the football?

3, 4
garden
the football belongs to where mary belongs to. f gt

encodings/clues
mkt m(k     n)t
f mt f mt

garden

f gt

9, 4

table 1: sample containee-belongs to-container translations and corresponding encodings about
mary from category 2. symbols in encodings are all d-dimensional vectors for actors (mary), ob-
jects (f ootball), and locations(nowhere, kitchen, garden). translations and encodings for category
3 are also speci   ed with the parentheses and circle operation, respectively.

knowing that every actor and object is unique without any ambiguity, one can encode such
containee-conatainer relationships by the following model using distributed representations. as-
sume all entities: actors, objects, and locations are represented by d-dimensional unit vectors in
rd.1 then each statement is encoded by a second-order tensor (or matrix) in which the con-
tainee vector is bound to the container vector via the fundamental binding operation of tpr, the
tensor (or outer) product2     in tensor notation, (containee)     (container), or in matrix notation,
(containee)(container)t     and then stored in a slot in a memory. when an item is dropped, we
perform an id136 to store the appropriate knowledge in memory. for the example in table 1, the
container of the football at statement 9     the garden     is determined after    guring out the most
recent owner of the football, mary; transitivity is implemented through simple matrix multiplica-
tion of the encodings of statement 3 (locating the football) and statement 4 (locating the football   s
current owner, mary):

(f mt )    (mgt ) = f (mt    m)gt = f gt

(    mt m = ||m||2

2 = 1)

finally, category 3 asks the trajectory of items considering the previous locations of actors. thus the
overall task is to understand the relocation sequence of each actor and from this to reconstruct the
trajectory of item locations. whereas memnns introduced an additional vector for each statement
for encoding a time stamp, we de   ne another binding operation     : rd    rd        rd. this binding
operation maps a pair of (next, prev) location vectors into a d-dimensional vector via a d    2d
temporal encoding matrix u like the following:

n     p = u (cid:20)n

p(cid:21)     rd.

in table 1, the second expression in the encodings column speci   es temporal encodings that identify
location transitions: statement 4, translated as mary belongs to the garden (from the kitchen), is
encoded as m(g     k)t . we can now reason to the proper answers for the questions in figure 1 by the
following id136 steps, using basic encodings (for c1 & c2) and temporal encodings (for c3):

c1. where is mary?

(a) left-multiply by mt all statements prior to time 3. (yields m    mt bt , mt    jht .)
(b) pick the most recent container where 2-norms of the multiplications in (a) are approximately

1.0. (yields bt ; mt j is small.)

(c) answer by    nding the location corresponding to the result representation.     bathroom

c2. where is the football?

1topologically speaking, the unit hypersphere can be constructed by adding one more point (   at in   nity   )

to euclidean space. thus sampling from the hypersphere does not limit the generality of representations.

2in tpr terms, the containee corresponds to a    ller, and the container corresponds to a role.

4

published as a conference paper at iclr 2016

(a) left-multiply by f t all statements prior to the current time. (yields f t    mkt , f t    sot ,

f t    f mt , f t    mgt .)

(b) pick the most recent container where 2-norms of the multiplications in (a) are approximately

1.0. (yields mt .)

(c) if the container is an actor (e.g., mary in statement 3),

    find the most recent container of the actor by left-multiplying by mt (yields gt .)
    answer by the most recent container.     garden for the questions at time 5 and 8.

(d) if the container is a location (e.g., garden in statement 9), simply answer by the container.

c3. where was the apple before the hallway?

(a) left-multiply by at all existing temporal encodings prior to time 7. (yields at    s(h     n)t ,

at    adt , ... .)

(b) pick the earliest container (the start of the trajectory).     daniel in statement 2.
(c) find the containers of daniel by left-multiplying by dt the temporal encodings between
time 2 and 7. (yields dt    adt , dt    j(k     n)t , dt    d(b     n)t , dt    f dt , dt    d(h     b), ... .)
(d) by multiplying by the pseudo-inverse u    , unbind 2d-dimensional vectors between time 4

and 7. (yields u    (b     n)     [b; n], then [h; b].)

(e) reconstruct the item trajectory in sequence.     nowhere     bedroom     hallway
(f) answer with (the most recent) location which is prior to the hallway.     bedroom

three argument relations (5)
in this category, there is a new type of statement which speci   es
ownership transfer: an actor gives an object to another actor. since now some relations involve three
arguments, (source-actor, object, target-actor), we need to encode an ownership trajectory instead
of a location trajectory.

encodings/clues

# statements/questions
1 jeff took the milk there.
2 jeff gave the milk to bill.
3 who did jeff give the milk to?
4 daniel travelled to the of   ce.
5 daniel journeyed to the hallway. daniel belongs to the hallway.
6 who received the milk?
7 bill went to the kitchen.
8 fred grabbed the apple there.
9 what did jeff give to bill?

relational translations/answers
the milk belongs to jeff (from none). m(j     n)t
the milk belongs to bill (from jeff).
m(b     j)t
2
bill
daniel belongs to the of   ce.
dot
dht
2
bkt

bill
bill belongs to the kitchen.
the apple belongs to fred (from none). a(f     n)t
milk

2

table 2: sample containee-belongs to-container translations and corresponding encodings for an
example from category 5. symbols in encodings are all d-dimensional vectors for actors (nobody,
jeff, daniel, bill, f red), objects (milk, apple), and locations (of   ce, kitchen).3

analogously to the     operation used for category 3, we realize the     operation by de   ning a map
    : rd    rd        rd. this new binding operation maps a pair of (next, prev) owner vectors into a
d-dimensional vector via a d    2d matrix v in the exactly same fashion: n     p = v [n; p]     rd. due
to the similarity in encoding, the id136 is also analogous to the id136 for category 3.

c5. three questions of table 2?

(a) find the owners of the milk by left-multiplying by mt the encodings prior to time 3.
(b) unbind the owner transitions by multiplying them by the pseudo-inverse v    .
(c) reconstruct the ownership trajectory for the milk.     nobody     jeff     bill
(d) answer accordingly each question based on the trajectory.

though no more complex examples or distinct categories exist in the dataset, it is clear that our en-
coding scheme is capable of inferring the full trajectory of item location considering both relocation
of actors and transfers of ownership. in such cases, both     and     will be used at the same time in

3to avoid notational confusion, we modify the name of an actor (from mary to daniel) and a location (from

the bathroom to the of   ce) from the real example in category 5.

5

published as a conference paper at iclr 2016

category 6: yes/no questions
01: daniel went back to the hallway.
02: john got the apple there.
03: is daniel in the hallway? yes 1
04: john dropped the apple.
05: mary got the apple there.
06: is daniel in the hallway? yes 1
07: daniel moved to the bedroom.
08: sandra travelled to the hallway.
09: is daniel in the hallway? no 7

category 8: list/sets
01: mary took the milk there.
02: mary went to the of   ce.
03: what is mary carrying? milk 1
04: mary took the apple there.
05: sandra journeyed to the bedroom.
06: what is mary carrying? milk,apple 1 4
07: mary put down the milk.
08: mary discarded the apple.
09: what is mary carrying? nothing 1 7 4 8

category 7: counting
01: mary took the apple there.
02: john travelled to the of   ce.
03: how many objects is mary carrying? one 1
04: mary travelled to the bathroom.
05: sandra went back to the bedroom.
06: how many objects is mary carrying? one 1
07: mary got the football there.
08: mary went to the of   ce.
09: how many objects is mary carrying? two 1 7
10: mary passed the apple to john.
11: mary left the football.
12: how many objects is mary carrying? none 1 7 10 11

category 9: simple negation
01: sandra travelled to the garden.
02: sandra is no longer in the garden.
03: is sandra in the garden? no 2
04: sandra is in the garden.
05: sandra journeyed to the hallway.
06: is sandra in the hallway? yes 5

figure 2: sample statements(black), questions(blue), answers(red), and clues(green) for category
6, 7, 8, and 9. answer types are different from the previous categories.

encoding. (e.g., encoding for time 5 will be then d(h     o)t . note also that there may be multiple
transfers between the same pair of actors in a history prior to the given question. while any of them
could be appropriate evidence to justify different answers, the ground-truth answers in the training
set turned out to be all based on the most recent clues.

answer variations (6, 7, 8, 9) as shown in figure 2, the responses to questions of categories
6-9 require different measures of the inferred element. for example, the statements in category
6 are structurally equivalent to the statements in category 2, while the questions concern only a
current location, similar to category 1. however, each question is formulated in a binary yes/no
format, con   rming    is daniel in the hallway?    instead of asking    where is daniel?   . category
7 is isomorphic to category 5 in the sense that actors can pick up, drop, and pass objects to other
actors. however, each question inquires the number of objects currently belonging to the given
actor. on the other hand, a response in category 8 must give the actual names of objects instead of
counting their number. the statements in this category are based not on category 5, but on category
2 which is simpler due to the lack of ownership transfer. lastly, statements in category 9 can contain
a negative quanti   er such as    no    or    no longer   . responses con   rm or not the location of actors via
yes/no dichotomy as for category 6. however, the overall story is based on the simplest category 1.

since answer measures are the only differences of these categories from category 1, 2, 3, and 5, no
additional encodings or id136s are necessary. however, there are several caveats in formulating
actual answers: 1) for yes/no questions, we should know the answers must be either yes or no in
advance based on the training examples. 2) when counting the number of belongings, the answer
must use english number words rather than arabic numerals. 3) when enumerating the names
of belongings, names must be sequenced by their order of acquisition. 4) a negative quanti   er is
realized by binding the initial default location nowhere back to the given actor. note that there is no
double negation.

statement variations (10, 11, 12, 13) statements in categories 10-13 contain more challenging
linguistic elements such as conjunctions (and/or) or pronouns (he/she/they). while statements in
category 10 is structurally similar to category 1   s, an actor can be located in either one or another
location. due to such uncertainty, some questions must be answered inde   nitely by    maybe   . on the
other hand, each statement in category 12 can contain multiple actors conjoined by    and    to indicate
that these actors all carry out the action. aside from such conjunctions, statements and questions
are isomorphic to category 1   s. statements in categories 11/13 can consist of a singular/plural
pronoun to indicate single/multiple actors mentioned earlier. since coreference resolution is itself a

6

published as a conference paper at iclr 2016

category 10: inde   nite knowledge
01: julie travelled to the kitchen.
02: bill is either in the school or the of   ce.
03: is bill in the of   ce? maybe 2
04: bill went back to the bedroom.
05: bill travelled to the kitchen.
06: is bill in the kitchen? yes 5

category 12: conjunction
01: daniel and sandra went back to the kitchen.
02: daniel and john went back to the hallway.
03: where is daniel? hallway 2
04: daniel and john moved to the bathroom.
05: sandra and mary travelled to the of   ce.
06: where is daniel? bathroom 4

category 11: basic coreference
01: mary went back to the bathroom.
02: after that she went to the bedroom.
03: where is mary? bedroom 1 2
04: daniel moved to the of   ce.
05: afterwards he moved to the hallway.
06: where is daniel? hallway 4 5

category 13: compound coreference
01: mary and daniel went to the bathroom.
02: then they journeyed to the hallway.
03: where is daniel? hallway 1 2
04: sandra and john moved to the kitchen.
05: then they moved to the hallway.
06: where is john? hallway 4 5

figure 3: sample statements(black), questions(blue), answers(red), and clues(green) for category
10, 11, 12, and 13. statement types are different from the previous categories.

dif   cult problem, all pronouns are limited to refer only to actors mentioned in the immediately prior
statement.

to encode conjunctions, we can still leverage the same method: conjoin two objects by another
bilinear binding operation     : rd    rd        rd, and unbind similarly via the pseudo-inverse of
the corresponding matrix. in our implementation, every statement is encoded using such a binding
operation. for instance, the    rst two statements of the given category 10 example are encoded into
j(k     k)t and b(s     o)t , with     encoding or. if two locations unbound from the target actor are
identical, we output a yes/no de   nite answer, whereas two different locations imply the inde   nite
answer    maybe    if one of the unbound locations matches the queried location. for the conjunction
and in category 12, exactly the same formalism is applicable for conjoining actors instead. whereas
a singular pronoun appearing at time t in category 11 is simply replaced by the actor mentioned at
time t     1, we also use    -binding to provide the multiple coreference needed for category 13. for
instance, the    rst statement in the given category 13 example is encoded as (m     d)bt and the same
encoding is substituted for    they    to represent the actors in the following statement.

deduction/induction (15, 16, 18, 20) while the statements and questions in these categories seem
different at    rst glance, their goals are all to reason using a transitivity-like rule. categories 15
creates a food chain among various animals, and category 18 yields a partial/total order of sizes
among various objects. whereas id136 in these two categories is deductive, categories 16 and
20 require inductive id136. in all four categories, every statement is easily represented by a
containee-container relation obeying transitivity. for instance, the category 15 example of figure
4 is encoded by {mct , wmt , cst , swt }. then the answer for the    rst question:    what is jessica
afraid of?    will be answered by left-multiplying these by the transpose of j = m and    nding the
one whose norm is approximately 1.0, which is mct . thus the result j t    (mct ) = mt (mct ) =
(mt m)ct = ct produces the desired answer cat. similarly, in category 18, if question encoding
(e.g.,    does the chocolate    t in the box?    = cbt ) is achievable by some inner products of statement
encodings, the answer must be    yes   , otherwise,    no   .

on the other hand, in category 16, transitivity is applied reversely as a container-containee fashion.
for instance,    lily is a    ion    is encoded by    lt , whereas    lily is green    is encoded by lgt . in
encoding    x is-a y   , we put the more general concept at the left side of the outer-product binding
y xt ; to encode    x has-property z    we use xz t . this allows us to induce a property for the general
category y based on the single observation of one of its members, via simple id127,
just as transitive id136 was implemented above: (   lt )    (lgt ) =    gt , meaning       ion is green.   
similarly in category 20, there exists precisely one statement which describes a property of an
actor (e.g.,    sumit is bored.    = bst ). then a statement describes the actor   s relocation (e.g.,    sumit
journeyed to the garden.    = sgt ), yielding an inductive conclusion by id127:    being
boring makes people go to the garden.    = (bst )    (sgt ) = bgt . the inductive reasoning also
generalizes to other actions (e.g., the reason for later activity,    sumit grabbed the football.    = sf t ,
is also being bored, because (bst )    (sf t ) = bf t ).

7

published as a conference paper at iclr 2016

category 15. basic deduction
01: mice are afraid of cats.
02: emily is a mouse.
03: wolves are afraid of mice.
04: cats are afraid of sheep.
05: winona is a cat.
06: sheep are afraid of wolves.
07: jessica is a mouse.
08: gertrude is a sheep.
09: what is jessica afraid of? cat 7 1
10: what is emily afraid of? cat 2 1
11: what is jessica afraid of? cat 7 1
12: what is winona afraid of? sheep 5 4

category 16: basic induction
01: bernhard is a lion.
02: julius is a lion.
03: lily is a lion.
04: bernhard is green.
05: lily is green.
06: brian is a lion.
07: greg is a swan.
08: greg is gray.
09: julius is yellow.
10: what color is brian? green 6 3 5

category 18: reasoning about size
01: the suitcase is bigger than the container.
02: the container    ts inside the box.
03: the chest is bigger than the chocolate.
04: the suitcase    ts inside the box.
05: the chest    ts inside the box.
06: does the chocolate    t in the box? yes 5 3
07: does the chocolate    t in the box? yes 5 3
08: does the box    t in the container? no 1 4
09: is the box bigger than the chocolate? yes 5 3
10: does the box    t in the chocolate? no 3 5

category 20: reasoning about motivations
01: sumit is bored.
02: where will sumit go? garden 1
03: yann is hungry.
04: where will yann go? kitchen 3
05: yann went back to the kitchen.
06: why did yann go to the kitchen? hungry 3
07: sumit journeyed to the garden.
08: why did sumit go to the garden? bored 1
09: yann picked up the apple there.
10: why did yann get the apple? hungry 3
11: sumit grabbed the football there.
12: why did sumit get the football? bored 1

figure 4: sample statements(black), questions(blue), answers(red), and clues(green) for category
15, 16, 18, and 20. categories 15 and 18 create chains from smaller/weaker to stronger/larger,
whereas categories 16 and 20 from general ones to speci   c ones.

cats

box

mice

sheep

suitcase

chest

wolves

container

chocolate

figure 5: the circular food chain (category 16) and the partial order (category 18) corresponding
to the examples in figure 4. the arrows imply afraid-of and    ts-inside relations, respectively.

prior knowledge (4, 14) though statements in category 4 looks quite dissimilar from those in
the other categories, they can be eventually modeled by a uni-relational reasoning chain based on the
containee-container relation, provided we know that    north    and    south    are opposite to each other.
thus the    rst two statements in the    rst category 4 example in figure 6 yield {kot , gkt }, from
which we infer (gkt )    (kot ) = got    the of   ce is north of the garden.    while the questions are all
simple knowledge con   rmation, note that a relational word (e.g.,    east   ) might never appear in the
prior statements, as illustrated in the second example of category 4 in figure 6. however the most
important point is that two non-collinear relations (e.g.,    north   ,    east   ) never appear together in the
same example.

on the other hand, statements in category 14 are no longer chronologically ordered. in order to
infer a correct locational trajectory without repeating statements multiple times, we prede   ne four
vectors for each time stamp: yesterday, this morning, this afternoon, and this evening, and bind
location with the corresponding stamp instead of the previous location. for example, the encoding
for the statement at time 2 now becomes j(b     m)t instead of j(b     p)t . knowing the correct order
of these four time stamps, which could be learned from the training examples, we can easily reorder
by unbinding time stamps.

8

published as a conference paper at iclr 2016

category 4. two argument relation
01: the of   ce is north of the kitchen.
02: the garden is south of the kitchen.
03: what is north of the kitchen? of   ce 1
                                                            
01: the kitchen is west of the garden.
02: the hallway is west of the kitchen.
03: what is the garden east of? kitchen 1

category 14: time manipulation
01: yesterday julie went back to the park.
02: julie went to the bedroom this morning.
03: bill journeyed to the cinema yesterday.
04: this morning bill went back to the park.
05: where was bill before the park? cinema 4 3
06: this evening julie went to the school.
07: this afternoon julie went back to the park.
08: where was julie before the bedroom? park 2 1

category 17: positional reasoning
01: the triangle is above the pink rectangle.
02: the blue square is to the left of the triangle.
03: is the pink rectangle to the right of the blue
square? yes 1 2
                                                            
01: the red sphere is below the yellow square.
02: the red sphere is above the blue square.
03: is the blue square below the yellow square?
yes 2 1

category 19: path finding
01: the bedroom is south of the hallway.
02: the bathroom is east of the of   ce.
03: the kitchen is west of the garden.
04: the garden is south of the of   ce.
05: the of   ce is south of the bedroom.
06: how do you go from the garden to the bed-
room? n,n 4 5

figure 6: sample statements(black), questions(blue), answers(red), and clues(green) for category 4,
14, 17, and 19. categories 4 and 17 contains two different examples separated by a horizontal line.

3.2 multiple relationships

path finding (19) our goal in this category is to    nd the path from one location to another location
in a manhattan-grid-like sense. note that if a is north of b, and b is north of c, then the right path
from a to c in grid must be    north, north    rather than simply    north   . we assume given four
d    d non-singular matrices n, e, w, s encoding four different directions satisfying n = s   1 and
e = w    1. then

# statements/questions
1 the bedroom is south of the hallway.
2 the   athroom is east of the of   ce.
3 the kitchen is west of the garden.
4 the garden is south of the of   ce.
5 the of   ce is south of the bedroom.
6 how do you go from the garden to the bedroom? n,n

encodings seq
translations/answers/clues
(1)
decides b given the initial h.
b = sh
(3)
defer until we know either o or   .    = eo
(5)
defer until we know either g or k. k = w g
(4)
defer until we know either o or g. g = so
(2)
decides o given b.
o = sb
(6)
b = xg

4, 5

table 3: sample multi-relational translations and corresponding encodings from category 19. sym-
bols in encodings are either d-dimensional object vectors (hallway, bedroom, of   ce,   athroom,
garden, kitchen) or d    d directional matrices (south, east, w est, n orth). the last column shows
the sequence of actual running order.

after initializing the    rst object in the right-hand side (e.g.,    hallway   ) by a random vector, we
decide the rest of the object vectors in sequence by multiplying the directional matrix (or its
inverse in case that the right-hand side is unknown and the left-hand side is known).
in case
that both sides are unknown, we defer such a statement by putting it into a queue.
in fact,
the solution path x can be determined either by selecting, of all combinations of two directions
{nn, ne, nw, ns, ... sn, se, sw, ss}, the one which best satis   es b = xg (in the example of table
3) or by solving this equation based on iterative substitutions. note also that we need to know that
(n, e, w, s) in the answers correspond to (north, east, west, south), respectively, which could be
learned from training data.

positional reasoning (17) while this category could be seen similar to path finding, each ques-
tion only asks a relative position between two objects. for instance, if    r is below s   , and    b is below
r   , then the position of b with respect to s must be simply    below    rather than    below, below   . even
if an object is mentioned to be left of another object, it could be also located in left-above or left-
below of another object. due to these subtleties, we here adopt redundant representations with four
d    d singular matrices (a, b, l, r) corresponding to four directions: (above, below, left, right).

9

published as a conference paper at iclr 2016

for this directional subsumption, in contrast to the non-singularity of the directional matrices for
category 19, we now strictly enforce idempotency to these matrices (i.e., an = ... = a2 = a 6= i).
then we de   ne the following four 4d    4d block matrices and encode each statement with these
matrices in the same manner as for category 19.

a =    
      

a 0
0
i
0
0
0
0

0
0
i
0

0
0
0
i

   
      

b =    
      

0

0
i
0 b 0
0
i
0
0

0
0

0
0
0
i

   
      

l =    
      

i
0
0
0

0
0

0
0
0
i
0 l 0
0
i

0

   
      

r =    
      

i
0
0
0

0
i
0
0

0
0
0
0
0
i
0 r

   
      

in this encoding, each of the four d-dimensional subspaces of r4d plays a role of indicating relative
positions with respect to (above, below, left, right), independently. carrying out the encoding of    r
is below s   , r = bs, ensures that the components of r and s differ only in the dimensions from
(d + 1) to 2d (from the b block of b); that is, rk = sk for k = 1, 3, 4 (where si indicates the i-th
d-dimensional sub-block of s). this is actually inconsistent with the encoding of    s is above r   ,
which demands that s and r differ only in their    rst sub-block. thus in order to determine whether
or not s is indeed above r, it is necessary to check whether r2 = bs2 as well as whether s1 = ar1.
if either condition is satis   ed, we can con   rm    s is above to r    . similarly, horizontal relations must
be checked on both the third and fourth d-dimensional sub-blocks.

4 experimental results

we implement our models and algorithms under the analysis given in the previous section. due to
the small vocabulary (mostly less than or equal to four elements among actors, objects, locations,
and actions) and non-ambiguous grammars, a simple dependency parser4 and basic named entity
recognition enable us to achieve 100% accurate id29. then we translate every statement
into a representation based on the appropriate containee-container or multiway relation, and then
store it in an array of memory slots. the logical reasoning after id29 and knowledge
representation no longer refers to the original text symbols.

type

accuracy

model
type

accuracy

model

c5

c4

c3

c2

c10
c1
c6
99%
100% 100% 100% 100% 99.3% 100%
dmn sid166
dmn mnn
mnn mnn mnn mnn
c20
c19
c11
c14
c15
c16
100% 100% 100% 100% 100% 100%
36%
100%
mnn mnn mnn dmn mnn mnn multitask mnn mnn mnn

c9
96.5% 100%
dmn
c18
95%

96.9%
dmn
c17
72%

c12

c13

c7

c8

table 4: best accuracies for each category and the model that achieved the best accuracy. mnn
indicates strongly-supervised memnn trained with the clue numbers, and dmn indicates dynamic
memnn, and sid166 indicates structured id166 with the coreference resolution and srl features.
multitask indicates multitask training.

in contrast to all previous models reported in table 4, in table 5 we also report test accuracy on
the training data to measure how well our models incorporate common sense. note that testing on
the training data is available because our training procedure only parses the appropriate semantic
components such as actors, objects, locations, actions, and the forms of answers without using given
answers and clues for tuning the model parameters.

note that the imperfect accuracy in category 16 is due to the ambiguity of evidence. as given in
figure 4, one can answer the color of brian as    yellow    because the latest evidence tells julius who
is a lion is yellow. similarly, in category 5, the 8th story consists of incorrect/inconsistent answers
at time 14 and 17 (for training), as they ignore the most recent ownership transfers and choose
some old history as ground-truth answers. (the 63rd and 186th stories in the test data also consist
of incorrect answers, at time 27 and 22, respectively) other than these two categories, we achieve
perfect accuracies performing common-sense operations only on representations in memory.

4we use stanford dependency parser. http://nlp.stanford.edu/software/stanford-dependencies.shtml

10

published as a conference paper at iclr 2016

type

training

test
type

training

test

c5

c4

c3

c2

c1
c10
100% 100% 100% 100% 99.8% 100% 100% 100% 100% 100%
100% 100% 100% 100% 99.8% 100% 100% 100% 100% 100%
c11
c20
100% 100% 100% 100% 100% 99.4% 100% 100% 100% 100%
100% 100% 100% 100% 100% 99.5% 100% 100% 100% 100%

c19

c18

c12

c13

c14

c15

c16

c17

c6

c7

c8

c9

table 5: accuracies on training and test data on our models. we achieve near-perfect accuracy in
almost every category including positional reasoning and path    nding.

as the experimental results show, there is a clear distinction between two sets of tasks. tasks in
most categories can be modeled by a containee-container-like relationship respecting a transitivity-
like id136 rule, whose goals are to create a linear/circular chain. on the other hand, positional
reasoning and path    nding require multiple relationships where each corresponding pair (e.g., north
vs. south) has its own transitivity structure, operating independently of other pairs (e.g. east vs.
west). we hypothesize that this difference poses a major dif   culty for most of memory network
models to perform an accurate id136 for positional reasoning and path    nding.

recently, neural reasoner (nr) by peng et al. (2015) improves the accuracy for these two dif   cult
categories by a large margin, achieving 97.9% and 87.0% when using 10k training set. 5 different
from other memory network models, nr has multiple reasoning layers. starting from the initial
statements and questions, nr constructs new statements and questions at the next layer, and repeats
this process recursively over multiple layers. as both positional reasoning and path    nding require
generating id136s from, and new versions of, relevant statements for each relationship (e.g.,
   x is north of     can become    y is south of x   ), the abilities to generate new facts and to derive    nal
answers by integrating them from multiple relationships could be a key reason why nr is successful,
like our tpr-based reasoner. while nr in experiment is simpli   ed so that all new facts maintain
the same initial representations, the question representation changes for each layer considering all
existing facts and the previously evolved question. due to the simplicity of the task, we conjecture
that evolving representations of the question could be suf   cient to comprise the key ingredient for
each multi-relationship. however, it seems that training such multiple layers requires a large amount
of training data, yielding drastically different performance of nr on two different dataset sizes.

5 conclusion

the major contributions of this paper are two-fold. first, we throughly analyze the recently ac-
claimed babi question-answering tasks by grouping the twenty categories based on their relational
properties. our analysis reveals that most categories except positional reasoning and path    nding
are governed by uni-relational characteristics. as these turn out to support id136 in a similar
manner under transitivity, it could be dangerous to evaluate the capacity of network models based
only on their performance on babi. in contrast, two more dif   cult categories require the capability
of performing multi-relational reasoning, a capability which is apparently missing in most previous
models. one could later develop a more sophisticated dataset that needs substantially harder reason-
ing by introducing multiple relationships. second, we propose two vector space models which can
perform logistic reasoning for qa with distributed representations. while tpr has been used for
various problems such as tree/grammar encoding and lambda-calculus evaluation, logical reasoning
is a new area of application that requires iterative processing of tprs. in subsequent work, we will
generalize the vector-space approach for multi-relational problems. we hope these studies shed light
on the viability of developing further reasoning models which can perform id136 with existing
knowledge in an interpretable and transparent manner.

55 all accuracy values of various models reported in the experimental section of the present paper are based

on a 1k training set. neural reasoner achieves 66.4% and 17.3% when using the 1k dataset.

11

published as a conference paper at iclr 2016

references
bengio, yoshua, ducharme, rejean, vincent, pascal, and janvin, christian. a neural probabilistic language

model. jmlr, 3:1137   1155, 2003.

berant, jonathan, chou, andrew, frostig, roy, and liang, percy. id29 on freebase from question-
answer pairs. in proceedings of the 2013 conference on empirical methods in natural language processing,
pp. 1533   1544, seattle, washington, usa, october 2013. association for computational linguistics. url
http://www.aclweb.org/anthology/d13-1160.

bordes, antoine, chopra, sumit, and weston, jason.

id53 with subgraph embed-
dings.
in proceedings of the 2014 conference on empirical methods in natural language processing
(emnlp), pp. 615   620, doha, qatar, october 2014. association for computational linguistics. url
http://www.aclweb.org/anthology/d14-1067.

dupoux, emmanuel. deconstructing ai-complete question-answering: going beyond toy tasks. 2015. url

http://bootphon.blogspot.com/2015/04/deconstructing-ai-complete-question.html.

grefenstette, edward. towards a formal id65: simulating logical calculi with tensors.

association for computational linguistics, 2013.

jones, michael n. and mewhort, douglas j. k. representing word meaning and order information in a com-

posite holographic lexicon. psychological review, 114:1   37, 2007.

kumar, ankit, irsoy, ozan, su, jonathan, bradbury, james, english, robert, pierce, brian, ondruska, peter,
gulrajani, ishaan, and socher, richard. ask me anything: dynamic memory networks for natural language
processing. corr, abs/1506.07285, 2015. url http://arxiv.org/abs/1506.07285.

mikolov, tomas, sutskever, ilya, chen, kai, corrado, greg s, and dean, jeff. distributed representations of

words and phrases and their compositionality. pp. 3111   3119. 2013.

peng, baolin, lu, zhengdong, li, hang, and wong, kam-fai. towards neural network-based reasoning. corr,

abs/1508.05508, 2015. url http://arxiv.org/abs/1508.05508.

pennington, jeffrey, socher, richard, and manning, christopher d. glove: global vectors for word represen-

tation. pp. 1532   1543, 2014. url http://www.aclweb.org/anthology/d14-1162.

rocktaschel, tim, singh, sameer, bosnjak, matko, and riedel, sebastian. low-dimensional embeddings of

logic. 2014.

smolensky, paul. tensor product variable binding and the representation of symbolic structures in connectionist

systems. arti   cial intelligence, 46(1-2), 1990.

smolensky, paul. symbolic functions from neural computation. philosophical transactions of the royal

society, 370:3543   3569, 2012.

smolensky, paul and legendre, geraldine. the harmonic mind: from neural computation to optimality-

theoretic grammarvolume i: cognitive architecture. the mit press, 2006.

smolensky, paul, lee, moontae, he, xiaodong, yih, wen-tau, gao, jianfeng, and deng, li. basic
technical report, microsoft research, 2016. url

reasoning with tensor product representations.
http://arxiv.org/abs/1601.02745.

sukhbaatar, sainbayar, szlam, arthur, weston, jason, and fergus, rob. end-to-end memory networks. corr,

abs/1503.08895, 2015. url http://arxiv.org/abs/1503.08895.

ustun, volkan, rosenbloom, paul s., sagae, kenji, and demski, abram. distributed vector representations of

words in the sigma cognitive architecture. 2014.

weston, jason, chopra, sumit, and bordes, antoine. memory networks. corr, abs/1410.3916, 2014. url

http://arxiv.org/abs/1410.3916.

weston, jason, bordes, antoine, chopra, sumit, mikolov, tomas, rush, alexander m., and van merrienboer,
bart. towards ai-complete id53: a set of prerequisite toy tasks. volume abs/1502.05698.
2015. url http://arxiv.org/abs/1502.05698.

yih, wen-tau, chang, mingwei, he, xiaodong, and gao, jianfeng. id29 via staged query graph

generation: id53 with knowledge base. 2015.

12

