5
1
0
2

 

v
o
n
4
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
6
8
5
4
0

.

1
1
5
1
:
v
i
x
r
a

under review as a conference paper at iclr 2016

character-based
id4

wang ling & isabel trancoso
l2f spoken systems lab
instituto superior t  ecnico
lisbon, portugal
{wlin,isabel.trancoso}@inesc-id.pt

chris dyer & alan black
language technologies institute
carnegie mellon university
pittsburga, pa 15213, usa
{cdyer,awb}@cs.cmu.edu

abstract

we introduce a id4 model that views the input and output
sentences as sequences of characters rather than words. since word-level informa-
tion provides a crucial source of bias, our input model composes representations
of character sequences into representations of words (as determined by whites-
pace boundaries), and then these are translated using a joint attention/translation
model. in the target language, the translation is modeled as a sequence of word
vectors, but each word is generated one character at a time, conditional on the
previous character generations in each word. as the representation and generation
of words is performed at the character level, our model is capable of interpreting
and generating unseen word forms. a secondary bene   t of this approach is that
it alleviates much of the challenges associated with preprocessing/id121 of
the source and target languages. we show that our model can achieve translation
results that are on par with conventional word-based models.

1

introduction

in the past, efforts at performing translation at the character-level (vilar et al., 2007) or subword-
level (neubig et al., 2013) have failed to produce competitive results compared to word-based coun-
terparts (brown et al., 1993; koehn et al., 2007; chiang, 2005), with the exception of closely related
languages (nakov & tiedemann, 2012). however, developing sequence to sequence models that
are capable at reading and generating and generating words at the character basis is attractive for
multiple reasons. firstly, it opens the possibility for models reason about unseen source words, such
as morphological variants of observed words. secondly, it allows the production of unseen target
words effectively recasting translation as an open vocabulary task. finally, we bene   t from a sig-
ni   cant reduction of the source and target vocabulary size as only characters need to be modelled
explicitly. as the number of word types increases rapidly with the size of the dataset heaps (1978),
while the number of letter types in the majority languages is    xed, character mt models can po-
tentially solve many scalability issues in mt, both in terms of computational speed and memory
requirements. namely, the computational cost of performing a softmax over the whole vocabulary,
and the memory needed to represent each existing word type explicitly.
in this work, we present a neural translation model that learns to encode and decode using at the
character level. we show that contrarily to previous belief (vilar et al., 2007; neubig et al., 2013),
models that work at the character level can generate results competitive with word-based models.
this is accomplished by indirectly incorporating the knowledge of words into the model using a
hierarchical architecture that generates the word representations from characters, maps the word
representation into the target language and the continuous space and then proceeds to generate the

1

under review as a conference paper at iclr 2016

target word character by character. as the composition of the words is based on characters, the
model can learn, for instance, morphological aspects in the source language, allowing it to build
effective representations for unseen words. on the other hand, the character-based generation model
allows the model to perform translation into word forms that are not present in the training corpus.

2 character-based machine translation model

this section describes our character-based machine translation model. as an automatic translation
task, it perform the translation of a source sentence s = s0, . . . , sn, where si is the source word at
index i into the target sentence t = t0, . . . , tm, where tj is the target word at index j. this can be
decomposed into a serie of predictions, where the model predicts the next target word tp, given the
source sentence s and the previously generated target words t0, . . . , tp   1.

notation we shall represent vectors with lowercase bold letters (e.g. a), matrixes with uppercase
bold letters (e.g. a), scalar values as regular lowercase letters (e.g. a) and sets as regular uppercased
letters (e.g. a). when referring to whole source and target sentences, we shall use the variables s
and t, respectively. individual source and target words, shall be referred as si and tj, where i and
j are the indexes of the words within the sentence. furthermore, we use variables n and m to refer
to the lengths of the source and target sentences. finally, we shall de   ne that s0 and t0 represent a
special start of sentence token denoted as sos and that sn and tm represent a special end of sentence
token denoted as eos. to refer to individual characters, we shall use the notation si,u and tj,v, which
denotes the u-th character in the i-th word in the source sentence and to the v-th character in the j-th
word in the target sentence, respectively. we use the variables x and y as the lengths of the source
and target words. we also de   ne that the    rst character within a word is always a start of word token
denoted as sow and the last characters si,x and sj,y are always the end of word character eow.

2.1

joint alignment and translation model

while our model can be applied to any neural translation model , we shall adapt the attention-based
translation model presented in (bahdanau et al., 2015), which is described as follows. an illustration
of the model is shown in figure 1. in this model, the translation of a new target word tp, given the
source sentence s0, . . . , sn and the current target context t0, . . . , tp   1 is performed in the following
steps.

1-source word projection source words are mapped into ds,w-dimentional vectors. the most
common approach to perform this projection is through a word lookup table, where each word type
is attributed a independent set of parameters. thus, a word lookup table with s words will require
ds,w    s parameters, where ds,w is the size of the id27s.

0 , . . . , gf

2-context via blstms a context-aware representation of each source word vector s0, . . . , sn is
obtained by using two lstms (hochreiter & schmidhuber, 1997). the forward lstm generates the
state sequence gf
n from the input sequence s0, . . . , sn, and encode the left context. then, the
backward lstm, reads the input sequence in the reverse order sn, . . . , s0 generating the backward
0, which encodes the right context. consequently, for each word si, the
state sequence gb
global context is obtained as a linear combination of the respective forward state gf
i and backward
state gb
i .

n, . . . , gb

3-target word projection the projection of target words essentially follows the same approach
as the projection of the source words. a distinct word lookup table for the target language is used
with dt,w    t parameters, where dt,w is the dimensionality of the target word vectors and t is the
size of the target vocabulary.

4-context via forward lstm unlike the source sentence s, the target sentence is not known a
priori. thus, a forward lstm is built over the translated sentence t0, . . . , tp   1, generating states
lf
0 , . . . , lf

p   1 encodes the currently translated context.

p   1. as result, state lf

2

under review as a conference paper at iclr 2016

figure 1: illustration of the joint alignment and translation model. square boxes represent vectors
of neuron activations.

5-alignment via attention for each target context lf
p   1, the attention model learns the attention
that is attributed to each of the source vectors b0, . . . , bn. essentially, the model computes a score
zi for each source word by applying the following function:

zi = s tanh(wtlf

p   1 + wsbi),

where the source vector bi and target vector lf
p   1 are combined into a ds-dimensional vector using
the parameters ws and wt, respectively, and a non-linearity is applied (in our case, we apply a
hiperbolic tangent function tanh). then, the the score is obtained using the vertical vector s. this
operation is performed for each source index obtaining the scores z0, . . . , zn. then, a softmax over
all scores as is performed as follows:

(cid:80)

exp(zi)

j   [0,n] exp(zj)

ai =

this function yields a set of attention coef   cients a0, . . . , an with the property that each coef   cient
is a value between 0 and 1, and the sum of all coef   cients is 1. in mt, this can be interpreted as
a soft alignment for the target word tp over each of the source words. these attentions are used to
obtain representation of the source sentence for predicting word wp, which is simply the weighted

average a =(cid:80)

i   [0,n] aibi.

6-target word generation in the previous steps, the model builds two vectors that are used for
the prediction of the next word in the translation tp. the    rst is the target context vector lf
p   1,
which encodes the information of all words preceding tp, and a, which contains the information of
the most likely source word/words to generate wp. a common approach to word prediction is to
apply a softmax function over the target language vocabulary. more formally, given the conditioned

3

4-context via forward lstm3-target word proejction1-source word projectionwhereisthelibrary2-context via blstm5-alignment via attention modeldondeestala*******0.050.000.050.906-target word generationbiblioteca**add wordto output and repeatsosunder review as a conference paper at iclr 2016

variables, the source attention a and the target context lf
being the next translated word tp is given by:

p   1, the id203 of a given word type tp

p (tp|a, lf

p   1) =

(cid:80)

exp(estp
a a+stp
j   [0,t ] exp(esj

lf
p   1)
aa+sj
l lf

l

,

p   1 )

where sa and sl are the parameters that map the conditioned vectors into a score for each word type
in the target language vocabulary t . the parameters for a speci   c word type j are obtained as sj
a
and sj

l , respectively. then, scores are normalized into a id203.

2.2 character-based machine translation

we now present our adaptation of the word-based neural network model to operate over character
sequences rather than word sequences. however, unlike previous approaches that attempt to discard
the notion of words completely (vilar et al., 2007; neubig et al., 2013), we propose an hierarhical
architecture, which replaces the word lookup tables (steps 1 and 3) and the word softmax (step 6)
with character-based alternatives, which compose the notion of words from individual characters.
the advantage of this approach is that we bene   t from properties of character-based approaches (e.g.
compactness and orthographic sensitivity), but can also easily be incorporated into any word-based
neural approaches.

character-based word representation the work in (ling et al., 2015; ballesteros et al., 2015)
proposes a compositional model for learning word vectors from characters. similar to word lookup
tables, a word string sj is mapped into a ds,w-dimensional vector, but rather than allocating param-
eters for each individual word type, the word vector sj is composed by a series of transformation
using its character sequence sj,0, . . . , sj,x.

figure 2: illustration of the c2w model. square boxes represent vectors of neuron activations.

the illustration of the model is shown in 2. essentially, the model builds a representation of the word
using characters, by reading characters from left to right and vice-versa. more formally, given an in-
put word sj = sj,0, . . . , sj,x, the model projects each character into a continuous ds,c-dimensional
vectors sj,0, . . . , sj,x using a character lookup table. then, it builds a forward lstm state se-
quence hf
k by reading the character vectors sj,0, . . . , sj,x. another, backward lstm reads
the character vectors in the reverse order generating the backward states hb
0. finally, the

0 , . . . , hf

k, . . . , hb

4

* c2w compositional modelblstmwhereword vector for "where"under review as a conference paper at iclr 2016

representation of the word sj is obtained by combining the    nal states as follows:

sj = ds,f hf

k + ds,bhb

0 + bs,d,

where ds,f , ds,b and bs,d are parameters that determine how the states are combined.
as the c2w model maps a word string into a vector, we can simply replace the word lookup tables
(steps 1 and 3) with two c2w models in order to obtain a character-based translation model at the
input. next, we shall describe a method to output words at the character level.

character-based word generation a word softmax requires a separate set of parameters for
each word type. thus, a word softmax cannot generate unseen words in the training set, and requires
a large amount of parameters due to the fact that each word type must be modelled independently.
furthermore, another well know problem is that the whole target vocabulary t must be traversed
for each prediction during both training and testing phases. while at training time approximations
such as noise contrastive estimation (gutmann & hyvarinen, 2010) can be applied, traversing t is
still required at test time.

figure 3: illustration of the v2c model. square boxes represent vectors of neuron activations.

we address these problems by de   ning a character-based word generation model. an illustration of
the v2c (vector to characters) is shown in figure 3. we de   ne a character vocabulary for the target
language tc and a given word tj as a sequence of characters tj,0, . . . , tj,y, the id203 of a given
word is rede   ned as:

p (wp|a, lf

p   1) =

p (wj,i|wk,0, . . . , wj,j   1, a, lf

p   1)

(cid:89)

i   [0,y]

the intuition is that rather than learning to predict single words, our model predicts the character
sequence of the output word. each prediction is dependent on the input of the model (aligned source
words a and and target word context lf
p   1) and also on the previously generated character context
tj,0, . . . , tj,q   1, where q is the index of the last predicted character.
the character context is generated by a lstm. first, we project each target character tj,0, . . . , tj,q   1
with a character lookup table in the target language (we use the same lookup table used in the c2w
model for the target language) into a dt,c-dimensional vector tj,0, . . . , tj,q   1. then, each vector is
concatenated to the vectors a and lf
p   1, and passed as the input to an lstm, generating the sequence
of states yf
q   1. then, the prediction of the character tk,q obtained as the softmax function:

0 , . . . , yf

5

** v2c generation modelforward lstmestaestaeowsowunder review as a conference paper at iclr 2016

p (tj,q|tj,0, . . . , tj,q   1, a, lf

p   1) =

(cid:80)

y

exp(swk,q
yf
q   1)
yyf
i   [0,tc] exp(si

q   1)

,

where sy are the parameters that convert the state y into a score for each output character, and si
y
denotes the parameters respective to the character type i. the word terminates once the end of word
token eow is generated.
finally, the model is also required to produce the end of sentence token eos, similarly to a word
softmax. in our model, we simply consider the eos token as a word whose only character is eos.
in another words, it must generate the sequence eos,eos.

2.3 training

during training the whole set of target words t0, . . . , tm are known, and we simply maximize the
log likelihood that the sequence of words log p(t0 | a, sos) + . . . + log p(tm | a, lf
m   1). more
formally, we wish to maximize the log likelihood of the training data de   ned as:

(cid:88)

(cid:88)

(s,t)   d

p   [0,m]

log p(tq | a, lf

m   1)

where d is the set of parallel sentences used as training data.
in the word-based model, optimizing this function can be performed by maximizing the word soft-
max objective. in the character-based model, this each word prediction is factored into a set of
character predictions. more concretely, we we maximize the following log-likelihood:

(cid:88)

(cid:88)

(cid:88)

(s,t)   d

p   [0,m]

q   [0,length(tp)]

2.4 decoding

log p(tp,q | tk,0, . . . , tk,q   1, a, lf

m   1)

in previous work (bahdanau et al., 2015), decoding is performed using id125. in the word-
based approach, we de   ne a stack of translation hypothesis per timestamp a, where each position
is a set of translation hypothesis. with a0 = sos, at each timestamp j, we condition on each
of the previous contexts t = aj   1 and add new hypothesis for all words obtained in the softmax
function. for instance, given the partial translations a1 = a, b, and the vocabulary t = a, b, c,
then a2 would be composed by aa, ab, ac, ba, bb, bc. we set a beam kw, which de   nes the
number of hypothesis to be expanded prioritizing hypothesis with the highest sentence id203.
an hypothesis is    nal once it generates the end of sentence token eos.
whereas, the word softmax simply returns a list of candidates for the next word by iterating through
each of the target word types, the v2c model can generate an in   nite number of target words.
thus, in the character-based mt model, a second decoding process is needed to return the list of
top scoring words at each timestamp. that is, we de   ne a second id125 decoder with beam
kc, and perform a stack-based coding on the character-level for each word prediction. the decoder
de   nes a stack b, where at each timestamp, a new character is generated for each hypothesis. in
this case, the id125 is run until kw    nal hypothesis are found (generation of eow), as it must
return at least kw new hypothesis to ensure that the word level search is complete.

2.5 layer-wise training

our character-based model de   nes a three layer hierarchy. firstly, characters are composed into
word vectors using the c2w model. then, the attention model searches for the next source word to
translate. finally, the generation of the target word is obtained using the v2c model. each of these
layers contain in many cases multiple non-linear projections, training is bound to be signi   cantly
more complex than word-based models. in practice, this causes more epochs to be necessary for
convergence, and the model can converge to a suboptimal local optimum. furthermore, while the

6

under review as a conference paper at iclr 2016

v2c model is generally more ef   cient than a word softmax, the introduction of the c2w model
signi   cantly slows down training, as simple word table lookup is replaced by a compositional model.
inspired by previous work on training deep multi-layered id88s (bengio et al., 2007), we start
by training the attention and v2c models, which are directly linked to the output of the model. the
c2w model, which is the bottleneck of the model, is temporarily replaced by word lookup tables,
and the model is trained to maximize the translation score in the development set.
the c2w model is introduced afterwards by    rst training the c2w model to produce the same word
vectors as the word lookup tables for all training word types. more formally, given a word w, and
the embeddings from the word lookup table   w1, . . . ,   wdw, and the embeddings produced by the
c2w model w1, . . . , wdw, we wish to optimize the parameters to minimize the square distance
(   w1     w1)2, . . . , (wdw )2. as result, c2w model will generate similar embeddings as the word
lookup table, and replacing them will not degenerate the results signi   cantly. finally, the full set of
parameters (c2w, attention model and v2c) are    ne-tuned to maximize the translation quality on
the development set.

2.6 weak supervision for attention model

a problem with id12 is the fact that    nding the latent attention coef   cients ai, requires
a large number of epochs (hermann et al., 2015). furthermore, as the attention model de   ned
in (bahdanau et al., 2015) does not de   ne any domain knowledge regarding word alignments, such
as distortion, symmetry and fertility (brown et al., 1993) this model is likely to over   t for small
amounts of data. to address this problem, we use ibm model 4 to produce the word alignments
for the training parallel sentences, impose a soft restriction to induce the attention model to produce
alignments similar to word alignments produced by the ibm model. more formally, given that
the target word is aligned to the source word at index k in ibm model 4, we wish the model to
maximize the coef   cient ak. as ak is obtained from a softmax, this is essentially means that we
wish to maximize the id203 that the target word wk is selected. as word alignments tend to be
one-to-one, for target words with multiple or no alignments, we simply set no soft restriction.

3 experiments

in this section, we present and analyse the results using our proposed character-based model.

3.1 setup

we test our model in two datasets. first, we 600k sentence pairs for training from europarl (koehn,
2005), in the english-portuguese language pair. then, we de   ne another 500 sentence pairs for
development and 500 sentence pairs for testing. we also use the english-french 20k sentence pairs
from btec as a small scale experiment. as development and test sets, we the cstar03 and
iwslt04 held out sets, respectively.
both languages were tokenized with penn tree bank tokenizer1. as for casing, word-based models
trained using the lowercased parallel data. at testing time, we uppercase using the model using the
script at (koehn et al., 2007). this is a common practice for word-based models, as the sparcity
induced by the same word in different casings (play vs play) is problematic for word based models.
for the character-based model, the model is trained on the true case on both source and target
sides. that is, the model is responsible for interpreting and generating true cased sentences. finally,
evaluation is performed using id7 (papineni et al., 2002) and we always use a single sentence as
reference.
as for the hyper parameters of the model, all lstm states and cells are set to 150 dimensions. word
projection dimensions for the source and target languages ds,w and dt,w were set to 50. similarly,
the character projection dimensions were also set to ds,c and dt,c. for the alignment model, dz was
set to 100. finally, the id125 at the work level kw was set to 5, and the beam size for the
character level search kc was set to 5. training is performed until there is no id7 (papineni et al.,
2002) improvement for 5 epochs on the development set. systems are trained using mini-batch

1https://www.cis.upenn.edu/ treebank/id121.html

7

under review as a conference paper at iclr 2016

c2w model
answer
well-founded
response well-balanced
answers
much-needed
self-employed
reply
uncontrolled

answered
replies

inherited

responder
respondeu
responderam response
answering

answer
reply

responda
responde
respondem

join

question

word lookup table

well-founded

responder

described
impressed

bizarre
santer
unclear

reagir
aderir

responda
aceder

agradecer

table 1: most similar in-vocabulary words under the c2w model (left) and the word look up table
(right); the two    rst query words are obtained from the source language projections (english) and
the last word is obtained from target language projections (portuguese).

id119 with mini-batches of 40 sentence pairs. for word-based models on europarl, rather
than normalizing over the whole word inventory, we use noise contrastive estimation (gutmann &
hyvarinen, 2010), which subsamples a set of 100 negative samples at each prediction.

3.2 results

for the btec dataset, the word-based neural model achieves a id7 score of 15.38 in the french
to english translation direction, while the character-based model achieves a score of 15.45. on the
europarl, the word-based model obtains a id7 score of 19.29, while our proposed character-based
model obtains a id7 score of 19.49. while, differences are not signi   cant, this is a strong results
if we consider that previous work (vilar et al., 2007; neubig et al., 2013) revealed signi   cantly lower
translation quality using character-based approaches.

word representation in order to analyse the word representation obtained after training the
model, we generate the vector representation of the c2w models trained on the europarl trans-
lation task with 600k sentence pairs for all words in the training set. table1 provides examples for
words in english and portuguese, and words whose representations are closest to them, measure in
terms of cosine similarity. we observe evidence that the fact that word representations are com-
posed from characters as atomic units is not a limitation of their representative capabilities, as the
model can learn that orthographically divergent words (answer vs. response) have similar meanings.
compared to the word-based model, we can observe that in many cases, the c2w model, prefers
to gather words that are orthographically similar, such as different forms of answer and responder,
which does not happen in word lookup tables. this is because, different conjugations in english
are not always translated into the same word, and as lookup tables do not regard the orthographic
similarity between words, source words that do not share translations are essentially distinct words
and placed in different vector spaces. however, the sequential nature of the c2w model, makes it
desirable to place these words in the same vector space as less memorization is required. conse-
quently, not only less parameters are needed to encode such word groups, but also, in the event a
unknown verb conjugation is observed at test time, a reasonable vector can still be found for such a
word. for instance, if we do not know the translation of answered, we can still translate the word as
answer and obtain a reasonable translation of the sentence.
evidently, the assumption that similar words have similar meanings is not always true. for instance,
the word well-founded is not similar to much-needed. in such cases, our model would behave equiv-
alently to the word-based model.

word generation a strong aspect in the v2c model is that the model can generate unseen words.
in table 2, we provide three examples of unknown words that have been generated in portuguese.
the    rst is the translation of unknown word subsidisation to subsidade. unfortunately, this is incor-
rect as subsidade does not exist in the portuguese vocabulary. however, it is encouraging to observe
that the model is trying to translate the unknown word from its parts. firstly, the english suf   x
-ation and the portuguese suf   x -dade are common endings for nouns. furthermore, it can roughly
copy the stem of the word subsi. unfortunately, the correct translation is subs    dio. as the model has
not seen any of these forms, it is unable to decide, which is the correct form. this hints that the v2c
model could be pre-trained on large amounts of parallel data in order to recognize existing word
forms, which will be left as future work. on the other hand, the word-based model simply translates
this word to autores, which is the translation for authors.

8

under review as a conference paper at iclr 2016

original

character translation

word translation

original

character translation

word translation

. . . that does not mean that we want to bring an end to subsidisation .
. . . isso n  ao signi   ca a quest  ao de que se trata de um    m `a subsidade .
... isso n  ao signi   ca que isso , para conseguir reduzir os autores .
the budget for the reconstruction of ...
o orc  amento para as reconstruc     oes ...
o orc  amento inerente `a reconstruc     ao ...

table 2: examples of unknown words that have been generated. the unknown word in the transla-
tion as well as their aligned words are marked in bold. the original, character translation and word
translations rows correspond to the original sentence, the translation by the character-based neural
model and the translation obtained by the word-based model, respectively

an example of an instance that a unseen word correctly is the plural of the noun reconstruc     ao. this
is done by learning the general portuguese rule for nouns ending in -  ao, which is converted into
-  oes, whereas in general nouns are converted into plural by simply adding an -s. the reason for
generating the plural form, while the english word is in singular, is the fact that its preceding word
as is an determiner for plural words. this hints that cross word dependencies in the generation
model are perserved.

4 related work

our work is related to the recent advances in id4 models, where a single
neural network is trained to maximize the id155 of the target sentence t, given
the source s. the different models proposed de   ne different architectures to estimate this probabil-
ity. these include the usage of convolutional archituctures (kalchbrenner & blunsom, 2013), lstm
encoder-decoders (sutskever et al., 2014) and attention-based models (bahdanau et al., 2015). how-
ever, in all these approaches, the representation and generation of words are always performed at the
word level, using a word lookup table and softmax.
in our work, we focus on the de   nition of mechanisms to perform the word representation and
generation on the character level. thus, our methods are applicable to neural mt models pro-
posed previously. on the representation level, it has been shown that words can be composed using
character-based approaches (santos & zadrozny, 2014; kim et al., 2015; ling et al., 2015; balles-
teros et al., 2015).
generating output as a sequence of characters is receiving increasing attention in other domains.
for example, id103 models (chan et al., 2015; maas et al., 2015) and language mod-
eling (sutskever et al., 2011; mikolov et al., 2012). while this work is the    rst neural architecture
we are aware of that uses id187, a variety of bayesian language and translation mod-
els have been proposed that use subword models to generate word types which are in turn used to
generate text (chahuneau et al., 2013a; goldwater et al., 2011).

5 conclusion

in this work, we presented an approach to perform automatic translation using characters as atomic
units. our approach uses compositional models to compose words from individual characters in
order to learn orthographically sentient word representations. then, we de   ne a generation model
to produce translated words at the character level. we show that our methods can improve over
equivalent word-based neural translation models, as our models can learn to interpret and generate
unseen words.
as we present an end-to-end translation system that makes the open vocabulary assumption, we
leave much room for future work, as our models make very simplistic assumptions about language.
much of the prior information regarding morphology (chahuneau et al., 2013b), cognates (beinborn
et al., 2013) and rare word translation (sennrich et al., 2015) among others, should be incorporated
for better translation.

9

under review as a conference paper at iclr 2016

references
bahdanau, dzmitry, cho, kyunghyun, and bengio, yoshua. id4 by jointly
learning to align and translate. corr, abs/1409.0473, 2015. url http://arxiv.org/abs/
1409.0473.

ballesteros, miguel, dyer, chris, and smith, noah a. improved transition-based parsing by model-

ing characters instead of words with lstms. in proc. emnlp, 2015.

beinborn, lisa, zesch, torsten, and gurevych, iryna. cognate production using character-based
machine translation. in proceedings of the sixth international joint conference on natural lan-
guage processing, pp. 883   891, 2013.

bengio, yoshua, lamblin, pascal, popovici, dan, and larochelle, hugo. greedy layer-wise training
of deep networks. in sch  olkopf, b., platt, j.c., and hoffman, t. (eds.), advances in neural infor-
mation processing systems 19, pp. 153   160. mit press, 2007. url http://papers.nips.
cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf.

brown, peter f., pietra, vincent j. della, pietra, stephen a. della, and mercer, robert l. the
mathematics of id151: parameter estimation. comput. linguist., 19:263   
311, june 1993. issn 0891-2017. url http://portal.acm.org/citation.cfm?id=
972470.972474.

chahuneau, victor, dyer, chris, and smith, noah a. knowledge-rich morphological priors for

bayesian language models. in proc. naacl, 2013a.

chahuneau, victor, schlinger, eva, smith, noah a., and dyer, chris. translating into morphologi-

cally rich languages with synthetic phrases. in proc. of emnlp, 2013b.

chan, william, jaitly, navdeep, le, quoc v., and vinyals, oriol. listen, attend, and spell. corr,

abs/1508.01211, 2015.

chiang, david. a hierarchical phrase-based model for id151. in proceedings
of the 43rd annual meeting on association for computational linguistics, acl    05, pp. 263   270,
stroudsburg, pa, usa, 2005. association for computational linguistics. doi: 10.3115/1219840.
1219873. url http://dx.doi.org/10.3115/1219840.1219873.

goldwater, sharon, grif   ths, thomas l., and johnson, mark. producing power-law distributions

and damping word frequencies with two-stage language models. jmlr, 2011.

gutmann, michael and hyvarinen, aapo. noise-contrastive estimation: a new estimation princi-
ple for unnormalized statistical models. 2010. url http://citeseerx.ist.psu.edu/
viewdoc/summary?doi=10.1.1.161.7404.

heaps, harold s. information retrieval: computational and theoretical aspects. academic press,

1978.

hermann, karl moritz, ko  cisk  y, tom  a  s, grefenstette, edward, espeholt, lasse, kay, will, suley-
in advances
man, mustafa, and blunsom, phil. teaching machines to read and comprehend.
in neural information processing systems (nips), 2015. url http://arxiv.org/abs/
1506.03340.

hochreiter, sepp and schmidhuber, j  urgen. long short-term memory. neural comput., 9(8),
november 1997. issn 0899-7667. doi: 10.1162/neco.1997.9.8.1735. url http://dx.doi.
org/10.1162/neco.1997.9.8.1735.

kalchbrenner, nal and blunsom, phil. recurrent continuous translation models. seattle, october

2013. association for computational linguistics.

kim, yoon, jernite, yacine, sontag, david, and rush, alexander m. character-aware neural
language models. corr, abs/1508.06615, 2015. url http://arxiv.org/abs/1508.
06615.

10

under review as a conference paper at iclr 2016

koehn, philipp. europarl: a parallel corpus for id151. in conference
proceedings: the tenth machine translation summit, pp. 79   86, phuket, thailand, 2005. aamt,
aamt. url http://mt-archive.info/mts-2005-koehn.pdf.

koehn, philipp, hoang, hieu, birch, alexandra, callison-burch, chris, zens, richard, aachen,
rwth, constantin, alexandra, federico, marcello, bertoldi, nicola, dyer, chris, cowan, brooke,
shen, wade, moran, christine, and bojar, ondrej. moses: open source toolkit for statistical
machine translation. in proceedings of the 45th annual meeting of the association for computa-
tional linguistics companion volume proceedings of the demo and poster sessions, pp. 177   180,
prague, czech republic, june 2007. association for computational linguistics.

ling, wang, lu    s, tiago, marujo, lu    s, astudillo, ram  on fernandez, amir, silvio, dyer, chris,
black, alan w, and trancoso, isabel. finding function in form: compositional character models
for open vocabulary word representation. in proc. emnlp, 2015.

maas, andrew l., xie, ziang, jurafsky, dan, and ng, andrew y. lexicon-free conversational speech

recognition with neural networks. in proc. naacl, 2015.

mikolov, tom  a  s, sutskever, ilya, deoras, anoop, le, hai-son, kombrink, stefan, and cer-
nocky, j. subword id38 with neural networks. preprint (http://www.    t. vutbr.
cz/imikolov/id56lm/char. pdf), 2012.

nakov, preslav and tiedemann, j  org. combining word-level and character-level models for ma-
chine translation between closely-related languages. in the 50th annual meeting of the associa-
tion for computational linguistics, proceedings of the conference, july 8-14, 2012, jeju island,
korea - volume 2: short papers, pp. 301   305, 2012. url http://www.aclweb.org/
anthology/p12-2059.

neubig, graham, watanabe, taro, mori, shinsuke, and kawahara, tatsuya. substring-based ma-

chine translation. machine translation, 27(2):139   166, june 2013.

papineni, kishore, roukos, salim, ward, todd, and zhu, wei-jing. id7: a method for automatic
in proceedings of the 40th annual meeting on association
evaluation of machine translation.
for computational linguistics, acl    02, pp. 311   318, stroudsburg, pa, usa, 2002. association
for computational linguistics. doi: 10.3115/1073083.1073135. url http://dx.doi.org/
10.3115/1073083.1073135.

santos, cicero d. and zadrozny, bianca. learning character-level representations for part-of-speech
tagging. in proceedings of the 31st international conference on machine learning (icml-14),
pp. 1818   1826. jmlr workshop and conference proceedings, 2014. url http://jmlr.
org/proceedings/papers/v32/santos14.pdf.

sennrich, rico, haddow, barry, and birch, alexandra. id4 of rare words
with subword units. corr, abs/1508.07909, 2015. url http://arxiv.org/abs/1508.
07909.

sutskever, ilya, martens, james, and hinton, geoffrey. generating text with recurrent neural net-

works. in proc. icml, 2011.

sutskever, ilya, vinyals, oriol, and le, quoc v. sequence to sequence learning with neural net-

works. corr, abs/1409.3215, 2014. url http://arxiv.org/abs/1409.3215.

vilar, david, peter, jan-t., and ney, hermann. can we translate letters? in proceedings of the sec-
ond workshop on id151, statmt    07, pp. 33   39, stroudsburg, pa, usa,
2007. association for computational linguistics. url http://dl.acm.org/citation.
cfm?id=1626355.1626360.

11

