6
1
0
2

 
r
a

m
1

 

 
 
]

g
l
.
s
c
[
 
 

4
v
4
1
1
6
0

.

1
1
5
1
:
v
i
x
r
a

published as a conference paper at iclr 2016

multi-task sequence to sequence learning

minh-thang luong   , quoc v. le, ilya sutskever, oriol vinyals, lukasz kaiser
google brain
lmthang@stanford.edu,{qvl,ilyasu,vinyals,lukaszkaiser}@google.com

abstract

sequence to sequence learning has recently emerged as a new paradigm in super-
vised learning. to date, most of its applications focused on only one task and not
much work explored this framework for multiple tasks. this paper examines three
id72 (mtl) settings for sequence to sequence models: (a) the one-
to-many setting     where the encoder is shared between several tasks such as ma-
chine translation and syntactic parsing, (b) the many-to-one setting     useful when
only the decoder can be shared, as in the case of translation and image caption
generation, and (c) the many-to-many setting     where multiple encoders and de-
coders are shared, which is the case with unsupervised objectives and translation.
our results show that training on a small amount of parsing and image caption
data can improve the translation quality between english and german by up to 1.5
id7 points over strong single-task baselines on the wmt benchmarks. further-
more, we have established a new state-of-the-art result in constituent parsing with
93.0 f1. lastly, we reveal interesting properties of the two unsupervised learning
objectives, autoencoder and skip-thought, in the mtl context: autoencoder helps
less in terms of perplexities but more on id7 scores compared to skip-thought.

1

introduction

id72 (mtl) is an important machine learning paradigm that aims at improving
the generalization performance of a task using other related tasks. such framework has been
widely studied by thrun (1996); caruana (1997); evgeniou & pontil (2004); ando & zhang (2005);
argyriou et al. (2007); kumar & iii (2012), among many others. in the context of deep neural net-
works, mtl has been applied successfully to various problems ranging from language (liu et al.,
2015), to vision (donahue et al., 2014), and speech (heigold et al., 2013; huang et al., 2013).

recently, sequence to sequence (id195) learning, proposed by kalchbrenner & blunsom (2013),
sutskever et al. (2014), and cho et al. (2014), emerges as an effective paradigm for dealing with
variable-length inputs and outputs. seq2seid24, at its core, uses recurrent neural networks
to map variable-length input sequences to variable-length output sequences. while relatively new,
the id195 approach has achieved state-of-the-art results in not only its original application     ma-
chine translation     (luong et al., 2015b; jean et al., 2015a; luong et al., 2015a; jean et al., 2015b;
luong & manning, 2015), but also image id134 (vinyals et al., 2015b), and con-
stituency parsing (vinyals et al., 2015a).

despite the popularity of id72 and sequence to sequence learning, there has been little
work in combining mtl with seq2seid24. to the best of our knowledge, there is only one
recent publication by dong et al. (2015) which applies a id195 models for machine translation,
where the goal is to translate from one language to multiple languages. in this work, we propose
three mtl approaches that complement one another: (a) the one-to-many approach     for tasks that
can have an encoder in common, such as translation and parsing; this applies to the multi-target
translation setting in (dong et al., 2015) as well, (b) the many-to-one approach     useful for multi-
source translation or tasks in which only the decoder can be easily shared, such as translation and
image captioning, and lastly, (c) the many-to-many approach     which share multiple encoders and
decoders through which we study the effect of unsupervised learning in translation. we show that
syntactic parsing and image id134 improves the translation quality between english

   minh-thang luong is also a student at stanford university.

1

published as a conference paper at iclr 2016

figure 1: sequence to sequence learning examples     (left) machine translation (sutskever et al.,
2014) and (right) constituent parsing (vinyals et al., 2015a).

and german by up to +1.5 id7 points over strong single-task baselines on the wmt benchmarks.
furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 f1.
we also explore two unsupervised learning objectives, sequence autoencoders (dai & le, 2015) and
skip-thought vectors (kiros et al., 2015), and reveal their interesting properties in the mtl setting:
autoencoder helps less in terms of perplexities but more on id7 scores compared to skip-thought.

2 sequence to sequence learning

sequence to sequence learning (id195) aims to directly model the id155 p(y|x) of
mapping an input sequence, x1, . . . , xn, into an output sequence, y1, . . . , ym. it accomplishes such
goal through the encoder-decoder framework proposed by sutskever et al. (2014) and cho et al.
(2014). as illustrated in figure 1, the encoder computes a representation s for each input sequence.
based on that input representation, the decoder generates an output sequence, one unit at a time, and
hence, decomposes the id155 as:

log p(y|x) = x

m

j=1

log p (yj|y<j , x, s)

(1)

a natural model for sequential data is the recurrent neural network (id56), which is used by most of
the recent id195 work. these work, however, differ in terms of: (a) architecture     from unidirec-
tional, to bidirectional, and deep multi-layer id56s; and (b) id56 type     which are long-short term
memory (lstm) (hochreiter & schmidhuber, 1997) and the gated recurrent unit (cho et al., 2014).

another important difference between id195 work lies in what constitutes the input represen-
tation s. the early id195 work (sutskever et al., 2014; cho et al., 2014; luong et al., 2015b;
vinyals et al., 2015b) uses only the last encoder state to initialize the decoder and sets s = [ ]
in eq. (1). recently, bahdanau et al. (2015) proposes an attention mechanism, a way to provide
id195 models with a random access memory, to handle long input sequences. this is accomplished
by setting s in eq. (1) to be the set of encoder hidden states already computed. on the decoder side,
at each time step, the attention mechanism will decide how much information to retrieve from that
memory by learning where to focus, i.e., computing the alignment weights for all input positions.
recent work such as (xu et al., 2015; jean et al., 2015a; luong et al., 2015a; vinyals et al., 2015a)
has found that it is crucial to empower id195 models with the attention mechanism.

3 multi-task sequence-to-sequence learning

we generalize the work of dong et al. (2015) to the multi-task sequence-to-sequence learning set-
ting that includes the tasks of machine translation (mt), constituency parsing, and image caption
generation. depending which tasks involved, we propose to categorize multi-task seq2seid24
into three general settings. in addition, we will discuss the unsupervised learning tasks considered
as well as the learning process.

3.1 one-to-many setting

this scheme involves one encoder and multiple decoders for tasks in which the encoder can be
shared, as illustrated in figure 2. the input to each task is a sequence of english words. a separate
decoder is used to generate each sequence of output units which can be either (a) a sequence of tags

2

published as a conference paper at iclr 2016

german (translation)

english

tags (parsing)

english (unsupervised)

figure 2: one-to-many setting     one encoder, multiple decoders. this scheme is useful for either
multi-target translation as in dong et al. (2015) or between different tasks. here, english and ger-
man imply sequences of words in the respective languages. the    values give the proportions of
parameter updates that are allocated for the different tasks.

for constituency parsing as used in (vinyals et al., 2015a), (b) a sequence of german words for ma-
chine translation (luong et al., 2015a), and (c) the same sequence of english words for autoencoders
or a related sequence of english words for the skip-thought objective (kiros et al., 2015).

3.2 many-to-one setting

this scheme is the opposite of the one-to-many setting. as illustrated in figure 3, it consists of mul-
tiple encoders and one decoder. this is useful for tasks in which only the decoder can be shared, for
example, when our tasks include machine translation and image id134 (vinyals et al.,
2015b). in addition, from a machine translation perspective, this setting can bene   t from a large
amount of monolingual data on the target side, which is a standard practice in machine translation
system and has also been explored for neural mt by gulcehre et al. (2015).

german (translation)

image (captioning)

english

english (unsupervised)

figure 3: many-to-one setting     multiple encoders, one decoder. this scheme is handy for tasks in
which only the decoders can be shared.

3.3 many-to-many setting

lastly, as the name describes, this category is the most general one, consisting of multiple encoders
and multiple decoders. we will explore this scheme in a translation setting that involves sharing
multiple encoders and multiple decoders. in addition to the machine translation task, we will include
two unsupervised objectives over the source and target languages as illustrated in figure 4.

3.4 unsupervised learning tasks

our very    rst unsupervised learning task involves learning autoencoders from monolingual corpora,
which has recently been applied to sequence to sequence learning (dai & le, 2015). however, in
dai & le (2015)   s work, the authors only experiment with pretraining and then    netuning, but not
joint training which can be viewed as a form of id72 (mtl). as such, we are very
interested in knowing whether the same trend extends to our mtl settings.

additionally, we investigate the use of the skip-thought vectors (kiros et al., 2015) in the context of
our mtl framework. skip-thought vectors are trained by training sequence to sequence models on
pairs of consecutive sentences, which makes the skip-thought objective a natural seq2seid24
candidate. a minor technical dif   culty with skip-thought objective is that the training data must

3

published as a conference paper at iclr 2016

german (translation)

english

english (unsupervised)

german (unsupervised)

figure 4: many-to-many setting     multiple encoders, multiple decoders. we consider this scheme
in a limited context of machine translation to utilize the large monolingual corpora in both the
source and the target languages. here, we consider a single translation task and two unsupervised
autoencoder tasks.

consist of ordered sentences, e.g., paragraphs. unfortunately, in many applications that include
machine translation, we only have sentence-level data where the sentences are unordered. to address
that, we split each sentence into two halves; we then use one half to predict the other half.

3.5 learning

dong et al. (2015) adopted an alternating training approach, where they optimize each task for a
   xed number of parameter updates (or mini-batches) before switching to the next task (which is a
different language pair). in our setting, our tasks are more diverse and contain different amounts of
training data. as a result, we allocate different numbers of parameter updates for each task, which
are expressed with the mixing ratio values   i (for each task i). each parameter update consists of
training data from one task only. when switching between tasks, we select randomly a new task i
with id203

  i

.

pj   j

our convention is that the    rst task is the reference task with   1 = 1.0 and the number of training
parameter updates for that task is prespeci   ed to be n . a typical task i will then be trained for   i
  n
  1
parameter updates. such convention makes it easier for us to fairly compare the same reference task
in a single-task setting which has also been trained for exactly n parameter updates.

when sharing an encoder or a decoder, we share both the recurrent connections and the correspond-
ing embeddings.

4 experiments

we evaluate the id72 setup on a wide variety of sequence-to-sequence tasks: con-
stituency parsing, image id134, machine translation, and a number of unsupervised
learning as summarized in table 1.

4.1 data

our experiments are centered around the translation task, where we aim to determine whether other
tasks can improve translation and vice versa. we use the wmt   15 data (bojar et al., 2015) for
the english   german translation problem. following luong et al. (2015a), we use the 50k most
frequent words for each language from the training corpus.1 these vocabularies are then shared
with other tasks, except for parsing in which the target    language    has a vocabulary of 104 tags. we
use newstest2013 (3000 sentences) as a validation set to select our hyperparameters, e.g., mixing
coef   cients. for testing, to be comparable with existing results in (luong et al., 2015a), we use the
   ltered newstest2014 (2737 sentences)2 for the english   german translation task and newstest2015
(2169 sentences)3 for the german   english task. see the summary in table 1.

1the corpus has already been tokenized using the default tokenizer from moses. words not in these vocab-

ularies are represented by the token <unk>.

2http://statmt.org/wmt14/test-filtered.tgz
3http://statmt.org/wmt15/test.tgz

4

published as a conference paper at iclr 2016

task
english   german translation
german   english translation
english unsupervised
german unsupervised
penn tree bank parsing
high-con   dence corpus parsing
image captioning

test
train valid
size
size
size
3003
4.5m 3000
4.5m 3000
2169
12.1m details in text
13.8m
2416
1700
40k
11.0m 1700
2416
596k
4115

-

vocab size

train
source target epoch
50k
50k
50k
50k
50k
50k

50k
50k
50k
50k
104
104
50k

12
12
6
6
40
6
10

-

finetune

start cycle

8
8
4
4
20
4
5

1
1
0.5
0.5
4
0.5
1

table 1: data & training details     information about the different datasets used in this work. for
each task, we display the following statistics: (a) the number of training examples, (b) the sizes of
the vocabulary, (c) the number of training epochs, and (d) details on when and how frequent we
halve the learning rates (   netuning).

for the unsupervised tasks, we use the english and german monolingual corpora from wmt   15.4
since in our experiments, unsupervised tasks are always coupled with translation tasks, we use the
same validation and test sets as the accompanied translation tasks.

for constituency parsing, we experiment with two types of corpora:

1. a small corpus     the widely used penn tree bank (ptb) dataset (marcus et al., 1993) and,
2. a large corpus     the high-con   dence (hc) parse trees provided by vinyals et al. (2015a).

the two parsing tasks, however, are evaluated on the same validation (section 22) and test (sec-
tion 23) sets from the ptb data. note also that the parse trees have been linearized following
vinyals et al. (2015a). lastly, for image id134, we use a dataset of image and caption
pairs provided by vinyals et al. (2015b).

4.2 training details

in all experiments, following sutskever et al. (2014) and luong et al. (2015b), we train deep lstm
models as follows: (a) we use 4 lstm layers each of which has 1000-dimensional cells and embed-
dings,5 (b) parameters are uniformly initialized in [-0.06, 0.06], (c) we use a mini-batch size of 128,
(d) dropout is applied with id203 of 0.2 over vertical connections (pham et al., 2014), (e) we
use sgd with a    xed learning rate of 0.7, (f) input sequences are reversed, and lastly, (g) we use a
simple    netuning schedule     after x epochs, we halve the learning rate every y epochs. the values x
and y are referred as    netune start and    netune cycle in table 1 together with the number of training
epochs per task.

as described in section 3, for each multi-task experiment, we need to choose one task to be the refer-
ence task (which corresponds to   1 = 1). the choice of the reference task helps specify the number
of training epochs and the    netune start/cycle values which we also when training that reference
task alone for fair comparison. to make sure our    ndings are reliable, we run each experimental
con   guration twice and report the average performance in the format mean (stddev).

4.3 results

we explore several id72 scenarios by combining a large task (machine translation)
with: (a) a small task     penn tree bank (ptb) parsing, (b) a medium-sized task     image caption
generation, (c) another large task     parsing on the high-con   dence (hc) corpus, and (d) lastly,
unsupervised tasks, such as autoencoders and skip-thought vectors. in terms of id74,
we report both validation and test perplexities for all tasks. additionally, we also compute test id7
scores (papineni et al., 2002) for the translation task.

4the training sizes reported for the unsupervised tasks are only 10% of the original wmt   15 monolingual
corpora which we randomly sample from. such reduced sizes are for faster training time and already about
three times larger than that of the parallel data. we consider using all the monolingual data in future work.

5for image id134, we use 1024 dimensions, which is also the size of the image embeddings.

5

published as a conference paper at iclr 2016

4.3.1 large tasks with small tasks

in this setting, we want to understand if a small task such as ptb parsing can help improve the
performance of a large task such as translation. since the parsing task maps from a sequence of
english words to a sequence of parsing tags (vinyals et al., 2015a), only the encoder can be shared
with an english   german translation task. as a result, this is a one-to-many mtl scenario (  3.1).
to our surprise, the results in table 2 suggest that by adding a very small number of parsing mini-
batches (with mixing ratio 0.01, i.e., one parsing mini-batch per 100 translation mini-batches), we
can improve the translation quality substantially. more concretely, our best multi-task model yields
a gain of +1.5 id7 points over the single-task baseline. it is worth pointing out that as shown in
table 2, our single-task baseline is very strong, even better than the equivalent non-attention model
reported in (luong et al., 2015a). larger mixing coef   cients, however, over   t the small ptb corpus;
hence, achieve smaller gains in translation quality.

for parsing, as vinyals et al. (2015a) have shown that attention is crucial to achieve good parsing
performance when training on the small ptb corpus, we do not set a high bar for our attention-free
systems in this setup (better performances are reported in section 4.3.3). nevertheless, the parsing
results in table 2 indicate that mtl is also bene   cial for parsing, yielding an improvement of up to
+8.9 f1 points over the baseline.6 it would be interesting to study how mtl can be useful with the
presence of the attention mechanism, which we leave for future work.

task
(luong et al., 2015a)

translation
ptb parsing

valid ppl

translation
test ppl

test id7

parsing
test f1

8.1
our single-task systems

-

14.0

8.8 (0.3)

8.3 (0.2)

14.3 (0.3)

-

-

-
our multi-task systems

-

-

43.3 (1.7)

translation + ptb parsing (1x)
translation + ptb parsing (0.1x)
translation + ptb parsing (0.01x)

8.5 (0.0)
8.3 (0.1)
8.2 (0.2)

8.2 (0.0)
7.9 (0.0)
7.7 (0.2)

14.7 (0.1)
15.1 (0.0)
15.8 (0.4)

54.5 (0.4)
55.2 (0.0)
39.8 (2.7)

table 2: english   german wmt   14 translation & penn tree bank parsing results     shown
are perplexities (ppl), id7 scores, and parsing f1 for various systems. for muli-task models,
reference tasks are in italic with the mixing ratio in parentheses. our results are averaged over two
runs in the format mean (stddev). best results are highlighted in boldface.

4.3.2 large tasks with medium tasks

we investigate whether the same pattern carries over to a medium task such as image caption gen-
eration. since the image id134 task maps images to a sequence of english words
(vinyals et al., 2015b; xu et al., 2015), only the decoder can be shared with a german   english
translation task. hence, this setting falls under the many-to-one mtl setting (  3.2).
the results in table 3 show the same trend we observed before, that is, by training on another task for
a very small fraction of time, the model improves its performance on its main task. speci   cally, with
5 parameter updates for image id134 per 100 updates for translation (so the mixing ratio
of 0.05), we obtain a gain of +0.7 id7 scores over a strong single-task baseline. our baseline is
almost a id7 point better than the equivalent non-attention model reported in luong et al. (2015a).

4.3.3 large tasks with large tasks

our    rst set of experiments is almost the same as the one-to-many setting in section 4.3.1 which
combines translation, as the reference task, with parsing. the only difference is in terms of parsing

6while perplexities correlate well with id7 scores as shown in (luong et al., 2015b), we observe empir-
ically in section 4.3.3 that parsing perplexities are only reliable if it is less than 1.3. hence, we omit parsing
perplexities in table 2 for clarity. the parsing test perplexities (averaged over two runs) for the last four rows
in table 2 are 1.95, 3.05, 2.14, and 1.66. valid perplexities are similar.

6

published as a conference paper at iclr 2016

task
(luong et al., 2015a)

translation
captioning

translation + captioning (1x)
translation + captioning (0.1x)
translation + captioning (0.05x)
translation + captioning (0.01x)

valid ppl

translation

test ppl

-

14.3
our single-task systems

test id7

16.9

11.0 (0.0)

12.5 (0.2)

17.8 (0.1)

captioning
valid ppl

-

-

-

-
our multi-task systems
14.0

11.9

10.5 (0.4)
10.3 (0.1)
10.6 (0.0)

12.1 (0.4)
11.8 (0.0)
12.3 (0.1)

-

30.8 (1.3)

16.7

18.0 (0.6)
18.5 (0.0)
18.1 (0.4)

43.3

28.4 (0.3)
30.1 (0.3)
35.2 (1.4)

table 3: german   english wmt   15 translation & captioning results     shown are perplexities
(ppl) and id7 scores for various tasks with similar format as in table 2. reference tasks are in
italic with mixing ratios in parentheses. the average results of 2 runs are in mean (stddev) format.

data. instead of using the small penn tree bank corpus, we consider a large parsing resource, the
high-con   dence (hc) corpus, which is provided by vinyals et al. (2015a). as highlighted in table 4,
the trend is consistent; mtl helps boost translation quality by up to +0.9 id7 points.

task
(luong et al., 2015a)

valid ppl

-
our systems

translation
test ppl

test id7

8.1

14.0

translation
translation + hc parsing (1x)
translation + hc parsing (0.1x)
translation + hc parsing (0.05x)

8.8 (0.3)
8.5 (0.0)
8.2 (0.3)
8.4 (0.0)

8.3 (0.2)
8.1 (0.1)
7.7 (0.2)
8.0 (0.1)

14.3 (0.3)
15.0 (0.6)
15.2 (0.6)
14.8 (0.2)

table 4: english   german wmt   14 translation     shown are perplexities (ppl) and id7 scores
of various translation models. our multi-task systems combine translation and parsing on the high-
con   dence corpus together. mixing ratios are in parentheses and the average results over 2 runs are
in mean (stddev) format. best results are bolded.

the second set of experiments shifts the attention to parsing by having it as the reference task. we
show in table 5 results that combine parsing with either (a) the english autoencoder task or (b)
the english   german translation task. our models are compared against the best attention-based
systems in (vinyals et al., 2015a), including the state-of-the-art result of 92.8 f1.

before discussing the multi-task results, we note a few interesting observations. first, very small
parsing perplexities, close to 1.1, can be achieved with large training data.7 second, our baseline
system can obtain a very competitive f1 score of 92.2, rivaling vinyals et al. (2015a)   s systems. this
is rather surprising since our models do not use any attention mechanism. a closer look into these
models reveal that there seems to be an architectural difference: vinyals et al. (2015a) use 3-layer
lstm with 256 cells and 512-dimensional embeddings; whereas our models use 4-layer lstm with
1000 cells and 1000-dimensional embeddings. this further supports    ndings in (jozefowicz et al.,
2016) that larger networks matter for sequence models.

for the multi-task results, while autoencoder does not seem to help parsing, translation does. at
the mixing ratio of 0.05, we obtain a non-negligible boost of 0.2 f1 over the baseline and with
92.4 f1, our multi-task system is on par with the best single system reported in (vinyals et al.,
2015a). furthermore, by ensembling 6 different multi-task models (trained with the translation task
at mixing ratios of 0.1, 0.05, and 0.01), we are able to establish a new state-of-the-art result in
english constituent parsing with 93.0 f1 score.

7training solely on the small penn tree bank corpus can only reduce the perplexity to at most 1.6, as
evidenced by poor parsing results in table 2. at the same time, these parsing perplexities are much smaller than
what can be achieved by a translation task. this is because parsing only has 104 tags in the target vocabulary
compared to 50k words in the translation case. note that 1.0 is the theoretical lower bound.

7

published as a conference paper at iclr 2016

task

lstm+a (vinyals et al., 2015a)
lstm+a+e (vinyals et al., 2015a)
our systems

hc parsing
hc parsing + autoencoder (1x)
hc parsing + autoencoder (0.1x)
hc parsing + autoencoder (0.01x)
hc parsing + translation (1x)
hc parsing + translation (0.1x)
hc parsing + translation (0.05x)
hc parsing + translation (0.01x)
ensemble of 6 multi-task systems

parsing

valid ppl

test f1
92.5
92.8

-
-

1.12/1.12
1.12/1.12
1.12/1.12
1.12/1.13
1.12/1.13
1.13/1.13
1.11/1.12
1.12/1.12

-

92.2 (0.1)
92.1 (0.1)
92.1 (0.1)
92.0 (0.1)
91.5 (0.2)
92.0 (0.2)
92.4 (0.1)
92.2 (0.0)

93.0

table 5: large-corpus parsing results     shown are perplexities (ppl) and f1 scores for various
parsing models. mixing ratios are in parentheses and the average results over 2 runs are in mean
(stddev) format. we show the individual perplexities for all runs due to small differences among
them. for vinyals et al. (2015a)   s parsing results, lstm+a represents a single lstm with atten-
tion, whereas lstm+a+e indicates an ensemble of 5 systems. important results are bolded.

4.3.4 multi-tasks and unsupervised learning

our main focus in this section is to determine whether unsupervised learning can help improve
translation. speci   cally, we follow the many-to-many approach described in section 3.3 to couple
the german   english translation task with two unsupervised learning tasks on monolingual corpora,
one per language. the results in tables 6 show a similar trend as before, a small amount of other
tasks, in this case the autoencoder objective with mixing coef   cient 0.05, improves the translation
quality by +0.5 id7 scores. however, as we train more on the autoencoder task, i.e. with larger
mixing ratios, the translation performance gets worse.

task
(luong et al., 2015a)

translation

valid ppl

-

translation
test ppl

14.3

test id7

16.9

our single-task systems

11.0 (0.0)

12.5 (0.2)

17.8 (0.1)

german
test ppl

english
test ppl

-

-

-

-

translation + autoencoders (1.0x)
translation + autoencoders (0.1x)
translation + autoencoders (0.05x)

our multi-task systems with autoencoders
16.0
17.7

12.3
11.4

13.9
12.7

10.9 (0.1)

12.0 (0.0)

18.3 (0.4)

our multi-task systems with skip-thought vectors

translation + skip-thought (1x)
translation + skip-thought (0.1x)
translation + skip-thought (0.01x)

10.4 (0.1)
10.7 (0.0)
11.0 (0.1)

10.8 (0.1)
11.4 (0.2)
12.2 (0.0)

17.3 (0.2)
17.8 (0.4)
17.8 (0.3)

1.01
1.13

1.40 (0.01)

2.10
1.44

2.38 (0.39)

36.9 (0.1)
52.8 (0.3)
76.3 (0.8)

31.5 (0.4)
53.7 (0.4)
142.4 (2.7)

table 6: german   english wmt   15 translation & unsupervised learning results     shown are
perplexities for translation and unsupervised learning tasks. we experiment with both autoencoders
and skip-thought vectors for the unsupervised objectives. numbers in mean (stddev) format are the
average results of 2 runs; others are for 1 run only.

skip-thought objectives, on the other hand, behave differently. if we merely look at the perplexity
metric, the results are very encouraging: with more skip-thought data, we perform better consistently
across both the translation and the unsupervised tasks. however, when computing the id7 scores,
the translation quality degrades as we increase the mixing coef   cients. we anticipate that this is
due to the fact that the skip-thought objective changes the nature of the translation task when using
one half of a sentence to predict the other half. it is not a problem for the autoencoder objectives,
however, since one can think of autoencoding a sentence as translating into the same language.

8

published as a conference paper at iclr 2016

we believe these    ndings pose interesting challenges in the quest towards better unsupervised objec-
tives, which should satisfy the following criteria: (a) a desirable objective should be compatible with
the supervised task in focus, e.g., autoencoders can be viewed as a special case of translation, and (b)
with more unsupervised data, both intrinsic and extrinsic metrics should be improved; skip-thought
objectives satisfy this criterion in terms of the intrinsic metric but not the extrinsic one.

5 conclusion

in this paper, we showed that id72 (mtl) can improve the performance of the
attention-free sequence to sequence model of (sutskever et al., 2014). we found it surprising that
training on syntactic parsing and image caption data improved our translation performance, given
that these datasets are orders of magnitude smaller than typical translation datasets. furthermore, we
have established a new state-of-the-art result in constituent parsing with an ensemble of multi-task
models. we also show that the two unsupervised learning objectives, autoencoder and skip-thought,
behave differently in the mtl context involving translation. we hope that these interesting    ndings
will motivate future work in utilizing unsupervised data for sequence to sequence learning. a crit-
icism of our work is that our sequence to sequence models do not employ the attention mechanism
(bahdanau et al., 2015). we leave the exploration of mtl with attention for future work.

acknowledgments

we thank chris manning for helpful feedback on the paper and members of the google brain team
for thoughtful discussions and insights.

references
ando, rie kubota and zhang, tong. a framework for learning predictive structures from multiple

tasks and unlabeled data. jmlr, 6:1817   1853, 2005.

argyriou, andreas, evgeniou, theodoros, and pontil, massimiliano. multi-task id171. in

nips, 2007.

bahdanau, dzmitry, cho, kyunghyun, and bengio, yoshua. id4 by jointly

learning to align and translate. in iclr, 2015.

bojar, ond  rej, chatterjee, rajen, federmann, christian, haddow, barry, huck, matthias, hokamp,
chris, koehn, philipp, logacheva, varvara, monz, christof, negri, matteo, post, matt, scarton,
carolina, specia, lucia, and turchi, marco. findings of the 2015 workshop on statistical machine
translation. in wmt, 2015.

caruana, rich. multitask learning. machine learning, 28(1):41   75, 1997.

cho, kyunghyun, van merrienboer, bart, gulcehre, caglar, bougares, fethi, schwenk, holger,
and bengio, yoshua. learning phrase representations using id56 encoder-decoder for statistical
machine translation. in emnlp, 2014.

dai, andrew m. and le, quoc v. semi-supervised sequence learning. in nips, 2015.

donahue, jeff, jia, yangqing, vinyals, oriol, hoffman, judy, zhang, ning, tzeng, eric, and darrell,

trevor. decaf: a deep convolutional activation feature for generic visual recognition, 2014.

dong, daxiang, wu, hua, he, wei, yu, dianhai, and wang, haifeng. id72 for

multiple language translation. in acl, 2015.

evgeniou, theodoros and pontil, massimiliano. regularized multi   task learning.

2004.

in sigkdd,

gulcehre, caglar, firat, orhan, xu, kelvin, cho, kyunghyun, barrault, loic, lin, huei-chi,
bougares, fethi, schwenk, holger, and bengio, yoshua. on using monolingual corpora in neural
machine translation. arxiv preprint arxiv:1503.03535, 2015.

9

published as a conference paper at iclr 2016

heigold, georg, vanhoucke, vincent, senior, alan, nguyen, patrick, ranzato, marc   aurelio, devin,
matthieu, and dean, jeffrey. multilingual acoustic models using distributed deep neural networks.
in icassp, 2013.

hochreiter, sepp and schmidhuber, j  urgen. long short-term memory. neural computation, 9(8):

1735   1780, 1997.

huang, jui-ting, li, jinyu, yu, dong, deng, li, and gong, yifan. cross-language knowledge

transfer using multilingual deep neural network with shared hidden layers. in icassp, 2013.

jean, s  ebastien, cho, kyunghyun, memisevic, roland, and bengio, yoshua. on using very large

target vocabulary for id4. in acl, 2015a.

jean, s  ebastien, firat, orhan, cho, kyunghyun, memisevic, roland, and bengio, yoshua. montreal

id4 systems for wmt   15. in wmt, 2015b.

jozefowicz, r., vinyals, o., schuster, m., shazeer, n., and wu, y. exploring the limits of language

modeling. arxiv preprint arxiv:1602.02410, 2016.

kalchbrenner, nal and blunsom, phil. recurrent continuous translation models. in emnlp, 2013.

kiros, ryan, zhu, yukun, salakhutdinov, ruslan, zemel, richard s., torralba, antonio, urtasun,

raquel, and fidler, sanja. skip-thought vectors. in nips, 2015.

kumar, abhishek and iii, hal daum  e. learning task grouping and overlap in id72.

in icml, 2012.

liu, xiaodong, gao, jianfeng, he, xiaodong, deng, li, duh, kevin, and wang, ye-yi. represen-
tation learning using multi-task deep neural networks for semantic classi   cation and information
retrieval. in naacl, 2015.

luong, minh-thang and manning, christopher d. stanford id4 systems for

spoken language domain. in iwslt, 2015.

luong, minh-thang, pham, hieu, and manning, christopher d. effective approaches to attention-

based id4. in emnlp, 2015a.

luong, minh-thang, sutskever, ilya, le, quoc v., vinyals, oriol, and zaremba, wojciech. ad-

dressing the rare word problem in id4. in acl, 2015b.

marcus, mitchell p., marcinkiewicz, mary ann, and santorini, beatrice. building a large annotated

corpus of english: the id32. computational linguistics, 19(2):313   330, 1993.

papineni, kishore, roukos, salim, ward, todd, and jing zhu, wei. id7: a method for automatic

evaluation of machine translation. in acl, 2002.

pham, vu, bluche, th  eodore, kermorvant, christopher, and louradour, j  er  ome. dropout improves
recurrent neural networks for handwriting recognition. in frontiers in handwriting recognition
(icfhr), 2014 14th international conference on, pp. 285   290. ieee, 2014.

sutskever, ilya, vinyals, oriol, and le, quoc v. sequence to sequence learning with neural net-

works. in nips, 2014.

thrun, sebastian. is learning the n-th thing any easier than learning the    rst? in nips, 1996.

vinyals, oriol, kaiser, lukasz, koo, terry, petrov, slav, sutskever, ilya, and hinton, geoffrey.

grammar as a foreign language. in nips, 2015a.

vinyals, oriol, toshev, alexander, bengio, samy, and erhan, dumitru. show and tell: a neural

image caption generator. in cvpr, 2015b.

xu, kelvin, ba, jimmy, kiros, ryan, cho, kyunghyun, courville, aaron c., salakhutdinov, ruslan,
zemel, richard s., and bengio, yoshua. show, attend and tell: neural image id134
with visual attention. in icml, 2015.

10

