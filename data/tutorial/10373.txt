effective approaches to attention-based id4

minh-thang luong

hieu pham

christopher d. manning

computer science department, stanford university, stanford, ca 94305

{lmthang,hyhieu,manning}@stanford.edu

abstract

x

y

z

<eos>

5
1
0
2

 

p
e
s
0
2

 

 
 
]
l
c
.
s
c
[
 
 

5
v
5
2
0
4
0

.

8
0
5
1
:
v
i
x
r
a

an attentional mechanism has lately been
used to improve neural machine transla-
tion (id4) by selectively focusing on
parts of the source sentence during trans-
lation. however,
there has been little
work exploring useful architectures for
attention-based id4. this paper exam-
ines two simple and effective classes of at-
tentional mechanism: a global approach
which always attends to all source words
and a local one that only looks at a subset
of source words at a time. we demonstrate
the effectiveness of both approaches on the
wmt translation tasks between english
and german in both directions. with local
attention, we achieve a signi   cant gain of
5.0 id7 points over non-attentional sys-
tems that already incorporate known tech-
niques such as dropout. our ensemble
model using different attention architec-
tures yields a new state-of-the-art result in
the wmt   15 english to german transla-
tion task with 25.9 id7 points, an im-
provement of 1.0 id7 points over the
existing best system backed by id4 and
an id165 reranker.1

1 introduction

id4 (id4) achieved
state-of-the-art performances in large-scale trans-
lation tasks such as from english to french
(luong et al., 2015) and english to german
(jean et al., 2015). id4 is appealing since it re-
quires minimal domain knowledge and is concep-
tually simple. the model by luong et al. (2015)
reads through all the source words until the end-of-
sentence symbol <eos> is reached. it then starts

1all our code and models are publicly available at

http://nlp.stanford.edu/projects/id4.

a

b

c

d <eos> x

y

z

figure 1: id4     a stack-
ing recurrent architecture for translating a source
sequence a b c d into a target sequence x y
z. here, <eos> marks the end of a sentence.

emitting one target word at a time, as illustrated in
figure 1. id4 is often a large neural network that
is trained in an end-to-end fashion and has the abil-
ity to generalize well to very long word sequences.
this means the model does not have to explicitly
store gigantic phrase tables and language models
as in the case of standard mt; hence, id4 has
a small memory footprint. lastly, implementing
id4 decoders is easy unlike the highly intricate
decoders in standard mt (koehn et al., 2003).

in parallel,

the concept of    attention    has
gained popularity recently in training neural net-
works, allowing models to learn alignments be-
tween different modalities, e.g., between image
objects and agent actions in the dynamic con-
trol problem (mnih et al., 2014), between speech
frames and text in the id103 task
(?), or between visual features of a picture and
its text description in the image caption gener-
ation task (xu et al., 2015).
in the context of
id4, bahdanau et al. (2015) has successfully ap-
plied such attentional mechanism to jointly trans-
late and align words. to the best of our knowl-
edge, there has not been any other work exploring
the use of attention-based architectures for id4.
in this work, we design, with simplicity and ef-

fectiveness in mind, two novel types of attention-
based models: a global approach in which all
source words are attended and a local one whereby
only a subset of source words are considered at a
time. the former approach resembles the model
of (bahdanau et al., 2015) but is simpler architec-
turally. the latter can be viewed as an interesting
blend between the hard and soft id12
proposed in (xu et al., 2015):
it is computation-
ally less expensive than the global model or the
soft attention; at the same time, unlike the hard at-
tention, the local attention is differentiable almost
everywhere, making it easier to implement and
train.2 besides, we also examine various align-
ment functions for our attention-based models.

experimentally, we demonstrate that both of
our approaches are effective in the wmt trans-
lation tasks between english and german in both
directions. our attentional models yield a boost
of up to 5.0 id7 over non-attentional systems
which already incorporate known techniques such
as dropout. for english to german translation,
we achieve new state-of-the-art (sota) results
for both wmt   14 and wmt   15, outperforming
previous sota systems, backed by id4 mod-
els and id165 lm rerankers, by more than 1.0
id7. we conduct extensive analysis to evaluate
our models in terms of learning, the ability to han-
dle long sentences, choices of attentional architec-
tures, alignment quality, and translation outputs.

2 id4

a id4 system is a neural
network that directly models the conditional prob-
ability p(y|x) of translating a source sentence,
x1, . . . , xn, to a target sentence, y1, . . . , ym.3 a
basic form of id4 consists of two components:
(a) an encoder which computes a representation s
for each source sentence and (b) a decoder which
generates one target word at a time and hence de-
composes the id155 as:

log p(y|x) = xm

j=1

log p (yj|y<j, s)

(1)

a natural

choice to model

composition in the decoder

is

such a de-
a

to use

2there is a recent work by gregor et al. (2015), which is
very similar to our local attention and applied to the image
generation task. however, as we detail later, our model is
much simpler and can achieve good performance for id4.

3all sentences are assumed to terminate with a special

   end-of-sentence    token <eos>.

as

neural

(id56)

network

architec-
recurrent
the recent id4 work
ture, which most of
(kalchbrenner and blunsom, 2013;
such
cho et al., 2014;
sutskever et al., 2014;
luong et al., 2015;
bahdanau et al., 2015;
jean et al., 2015) have in common. they, how-
ever, differ in terms of which id56 architectures
are used for the decoder and how the encoder
computes the source sentence representation s.

used
for

kalchbrenner and blunsom (2013)

an
id56 with the standard hidden unit
the
decoder and a convolutional neural network for
encoding the source sentence representation. on
the other hand, both sutskever et al. (2014) and
luong et al. (2015) stacked multiple layers of an
id56 with a long short-term memory (lstm)
hidden unit for both the encoder and the decoder.
cho et al. (2014), bahdanau et al. (2015),
and
jean et al. (2015) all adopted a different version of
the id56 with an lstm-inspired hidden unit, the
gated recurrent unit (gru), for both components.4
in more detail, one can parameterize the proba-

bility of decoding each word yj as:

p (yj|y<j, s) = softmax (g (hj))

(2)

with g being the transformation function that out-
puts a vocabulary-sized vector.5 here, hj is the
id56 hidden unit, abstractly computed as:

hj = f (hj   1, s),

(3)

the

where f computes the current hidden state
given the previous hidden state and can be
either a vanilla id56 unit, a gru, or an lstm
unit.
in (kalchbrenner and blunsom, 2013;
cho et al., 2014;
sutskever et al., 2014;
luong et al., 2015),
representa-
tion s is only used once to initialize the
in
decoder hidden state. on the other hand,
(bahdanau et al., 2015;
and
this work, s,
implies a set of source
in fact,
hidden states which are consulted throughout the
entire course of the translation process. such an
approach is referred to as an attention mechanism,
which we will discuss next.

jean et al., 2015)

source

in this work, following (sutskever et al., 2014;
luong et al., 2015), we use the stacking lstm
architecture for our id4 systems, as illustrated

4they all used a single id56 layer except for the latter two

works which utilized a bidirectional id56 for the encoder.

5one can provide g with other inputs such as the currently

predicted word yj as in (bahdanau et al., 2015).

in figure 1. we use the lstm unit de   ned in
(zaremba et al., 2015). our training objective is
formulated as follows:

jt = x(x,y)   d

    log p(y|x)

(4)

with d being our parallel training corpus.

3 attention-based models

our various attention-based models are classifed
into two broad categories, global and local. these
classes differ in terms of whether the    attention   
is placed on all source positions or on only a few
source positions. we illustrate these two model
types in figure 2 and 3 respectively.

common to these two types of models is the fact
that at each time step t in the decoding phase, both
approaches    rst take as input the hidden state ht
at the top layer of a stacking lstm. the goal is
then to derive a context vector ct that captures rel-
evant source-side information to help predict the
current target word yt. while these models differ
in how the context vector ct is derived, they share
the same subsequent steps.

speci   cally, given the target hidden state ht and
the source-side context vector ct, we employ a
simple concatenation layer to combine the infor-
mation from both vectors to produce an attentional
hidden state as follows:

  ht = tanh(wc[ct; ht])

(5)

the attentional vector   ht is then fed through the
softmax layer to produce the predictive distribu-
tion formulated as:

p(yt|y<t, x) = softmax(ws

  ht)

(6)

we now detail how each model type computes

the source-side context vector ct.

3.1 global attention
the idea of a global attentional model is to con-
sider all the hidden states of the encoder when de-
riving the context vector ct.
in this model type,
a variable-length alignment vector at, whose size
equals the number of time steps on the source side,
is derived by comparing the current target hidden
state ht with each source hidden state   hs:

at(s) = align(ht,   hs)

(7)

=

exp(cid:0)score(ht,   hs)(cid:1)
ps    exp(cid:0)score(ht,   hs   )(cid:1)

attention layer

context vector

ct

global align weights

at

  hs

yt

  ht

ht

figure 2: global attentional model     at each time
step t, the model infers a variable-length align-
ment weight vector at based on the current target
state ht and all source states   hs. a global context
vector ct is then computed as the weighted aver-
age, according to at, over all the source states.

here, score is referred as a content-based function
for which we consider three different alternatives:

  hs
wa

  hs

score(ht,   hs) =         
      

h   
t
h   
t
v   

a tanh(cid:0)wa[ht;   hs](cid:1)

dot
general
concat

besides, in our early attempts to build attention-
based models, we use a location-based function
in which the alignment scores are computed from
solely the target hidden state ht as follows:

at = softmax(waht)

location

(8)

given the alignment vector as weights, the context
vector ct is computed as the weighted average over
all the source hidden states.6

comparison to (bahdanau et al., 2015)     while
our global attention approach is similar in spirit
to the model proposed by bahdanau et al. (2015),
there are several key differences which re   ect how
we have both simpli   ed and generalized from
the original model. first, we simply use hid-
den states at the top lstm layers in both the
encoder and decoder as illustrated in figure 2.
bahdanau et al. (2015), on the other hand, use
the concatenation of the forward and backward
source hidden states in the bi-directional encoder

6eq. (8) implies that all alignment vectors at are of the
same length. for short sentences, we only use the top part of
at and for long sentences, we ignore words near the end.

attention layer

ct

context vector

aligned position

at

local weights

pt

  hs

yt

  ht

ht

figure 3: local attention model     the model    rst
predicts a single aligned position pt for the current
target word. a window centered around the source
position pt is then used to compute a context vec-
tor ct, a weighted average of the source hidden
states in the window. the weights at are inferred
from the current target state ht and those source
states   hs in the window.

and target hidden states in their non-stacking uni-
directional decoder. second, our computation path
is simpler; we go from ht     at     ct       ht
then make a prediction as detailed in eq. (5),
eq. (6), and figure 2. on the other hand, at
any time t, bahdanau et al. (2015) build from the
previous hidden state ht   1     at     ct    
ht, which, in turn, goes through a deep-output
and a maxout layer before making predictions.7
lastly, bahdanau et al. (2015) only experimented
with one alignment function, the concat product;
whereas we show later that the other alternatives
are better.

3.2 local attention

the global attention has a drawback that it has to
attend to all words on the source side for each tar-
get word, which is expensive and can potentially
render it impractical to translate longer sequences,
e.g., paragraphs or documents. to address this
de   ciency, we propose a local attentional mech-
anism that chooses to focus only on a small subset
of the source positions per target word.

this model takes inspiration from the tradeoff
between the soft and hard attentional models pro-
posed by xu et al. (2015) to tackle the image cap-
tion generation task. in their work, soft attention

7we will refer to this difference again in section 3.3.

refers to the global attention approach in which
weights are placed    softly    over all patches in the
source image. the hard attention, on the other
hand, selects one patch of the image to attend to at
a time. while less expensive at id136 time, the
hard attention model is non-differentiable and re-
quires more complicated techniques such as vari-
ance reduction or id23 to train.
our local attention mechanism selectively fo-
cuses on a small window of context and is differ-
entiable. this approach has an advantage of avoid-
ing the expensive computation incurred in the soft
attention and at the same time, is easier to train
than the hard attention approach. in concrete de-
tails, the model    rst generates an aligned position
pt for each target word at time t. the context vec-
tor ct is then derived as a weighted average over
the set of source hidden states within the window
[pt   d, pt+d]; d is empirically selected.8 unlike
the global approach, the local alignment vector at
is now    xed-dimensional, i.e.,     r2d+1. we con-
sider two variants of the model as below.

monotonic alignment (local-m)     we simply set
pt = t assuming that source and target sequences
are roughly monotonically aligned. the alignment
vector at is de   ned according to eq. (7).9

predictive alignment (local-p)     instead of as-
suming monotonic alignments, our model predicts
an aligned position as follows:

pt = s    sigmoid(v   

p tanh(wpht)),

(9)

wp and vp are the model parameters which will
be learned to predict positions. s is the source sen-
tence length. as a result of sigmoid, pt     [0, s].
to favor alignment points near pt, we place a
gaussian distribution centered around pt . specif-
ically, our alignment weights are now de   ned as:

at(s) = align(ht,   hs) exp(cid:18)   

(s     pt)2
2  2 (cid:19) (10)

we use the same align function as in eq. (7) and
2 .
the standard deviation is empirically set as    = d
note that pt is a real nummber; whereas s is an
integer within the window centered at pt.10

8if the window crosses the sentence boundaries, we sim-
ply ignore the outside part and consider words in the window.
9local-m is the same as the global model except that the

vector at is    xed-length and shorter.

10local-p is similar to the local-m model except that we dy-
namically compute pt and use a truncated gaussian distribu-
tion to modify the original alignment weights align(ht,   hs)
as shown in eq. (10). by utilizing pt to derive at, we can
compute backprop gradients for w
p and vp. this model is
differentiable almost everywhere.

x

y

z

<eos>

  ht

attention layer

a

b

c

d <eos> x

y

z

figure 4: input-feeding approach     attentional
vectors   ht are fed as inputs to the next time steps to
inform the model about past alignment decisions.

comparison to (gregor et al., 2015)     have pro-
posed a selective attention mechanism, very simi-
lar to our local attention, for the image generation
task. their approach allows the model to select an
image patch of varying location and zoom. we,
instead, use the same    zoom    for all target posi-
tions, which greatly simpli   es the formulation and
still achieves good performance.

input-feeding approach

3.3
in our proposed global and local approaches,
the attentional decisions are made independently,
which is suboptimal. whereas, in standard mt,
a coverage set
is often maintained during the
translation process to keep track of which source
words have been translated. likewise, in atten-
tional id4s, alignment decisions should be made
jointly taking into account past alignment infor-
mation. to address that, we propose an input-
feeding approach in which attentional vectors   ht
are concatenated with inputs at the next time steps
as illustrated in figure 4.11 the effects of hav-
ing such connections are two-fold: (a) we hope
to make the model fully aware of previous align-
ment choices and (b) we create a very deep net-
work spanning both horizontally and vertically.

use

work

other

context

comparison

   
to
vectors,
bahdanau et al. (2015)
similar to our ct, in building subsequent hidden
states, which can also achieve the    coverage   
effect. however, there has not been any analysis
of whether such connections are useful as done
11if n is the number of lstm cells, the input size of the

   rst lstm layer is 2n; those of subsequent layers are n.

in this work. also, our approach is more general;
as illustrated in figure 4, it can be applied to
general stacking recurrent architectures, including
non-attentional models.

xu et al. (2015) propose a doubly attentional
approach with an additional constraint added to
the training objective to make sure the model pays
equal attention to all parts of the image during the
id134 process. such a constraint can
also be useful to capture the coverage set effect
in id4 that we mentioned earlier. however, we
chose to use the input-feeding approach since it
provides    exibility for the model to decide on any
attentional constraints it deems suitable.

4 experiments

we evaluate the effectiveness of our models
on the wmt translation tasks between en-
glish and german in both directions.
new-
stest2013 (3000 sentences) is used as a develop-
ment set to select our hyperparameters. transla-
tion performances are reported in case-sensitive
id7 (papineni et al., 2002) on newstest2014
(2737 sentences) and newstest2015 (2169 sen-
tences). following (luong et al., 2015), we report
translation quality using two types of id7: (a)
tokenized12 id7 to be comparable with existing
id4 work and (b) nist13 id7 to be compara-
ble with wmt results.

4.1 training details

all our models are trained on the wmt   14 train-
ing data consisting of 4.5m sentences pairs (116m
english words, 110m german words). similar
to (jean et al., 2015), we limit our vocabularies to
be the top 50k most frequent words for both lan-
guages. words not in these shortlisted vocabular-
ies are converted into a universal token <unk>.
when training our id4 systems, following
(bahdanau et al., 2015; jean et al., 2015), we    l-
ter out sentence pairs whose lengths exceed
50 words and shuf   e mini-batches as we pro-
ceed. our stacking lstm models have 4 lay-
ers, each with 1000 cells, and 1000-dimensional
embeddings. we follow (sutskever et al., 2014;
luong et al., 2015) in training id4 with similar
settings: (a) our parameters are uniformly initial-
ized in [   0.1, 0.1], (b) we train for 10 epochs us-

12all texts are tokenized with tokenizer.perl and

id7 scores are computed with multi-id7.perl.

13with the mteval-v13a script as per wmt guideline.

system
winning wmt   14 system     phrase-based + large lm (buck et al., 2014)
existing id4 systems
id56search (jean et al., 2015)
id56search + unk replace (jean et al., 2015)
id56search + unk replace + large vocab + ensemble 8 models (jean et al., 2015)
our id4 systems
base
base + reverse
base + reverse + dropout
base + reverse + dropout + global attention (location)
base + reverse + dropout + global attention (location) + feed input
base + reverse + dropout + local-p attention (general) + feed input
base + reverse + dropout + local-p attention (general) + feed input + unk replace
ensemble 8 models + unk replace

ppl

id7
20.7

16.5
19.0
21.6

11.3

12.6 (+1.3)
14.0 (+1.4)
16.8 (+2.8)
18.1 (+1.3)
19.0 (+0.9)
20.9 (+1.9)
23.0 (+2.1)

10.6
9.9
8.1
7.3
6.4

5.9

table 1: wmt   14 english-german results     shown are the perplexities (ppl) and the tokenized id7
scores of various systems on newstest2014. we highlight the best system in bold and give progressive
improvements in italic between consecutive systems. local-p referes to the local attention with predictive
alignments. we indicate for each attention model the alignment score function used in pararentheses.

ing plain sgd, (c) a simple learning rate sched-
ule is employed     we start with a learning rate of
1; after 5 epochs, we begin to halve the learning
rate every epoch, (d) our mini-batch size is 128,
and (e) the normalized gradient is rescaled when-
ever its norm exceeds 5. additionally, we also
use dropout with id203 0.2 for our lstms as
suggested by (zaremba et al., 2015). for dropout
models, we train for 12 epochs and start halving
the learning rate after 8 epochs. for local atten-
tion models, we empirically set the window size
d = 10.

our code is implemented in matlab. when
running on a single gpu device tesla k40, we
achieve a speed of 1k target words per second.
it takes 7   10 days to completely train a model.

the winning
a

4.2 english-german results
we compare our id4 systems in the english-
german task with various other systems. these
include
system in wmt   14
(buck et al., 2014),
system
phrase-based
whose language models were trained on a huge
the common crawl corpus.
monolingual
for end-to-end id4 systems,
to the best of
our knowledge,
(jean et al., 2015) is the only
work experimenting with this language pair and
currently the sota system. we only present
results for some of our id12 and will
later analyze the rest in section 5.

text,

as

shown in table 1, we achieve pro-

gressive improvements when (a) reversing the
source sentence, +1.3 id7, as proposed in
(sutskever et al., 2014) and (b) using dropout,
+1.4 id7. on top of that, (c) the global atten-
tion approach gives a signi   cant boost of +2.8
id7, making our model slightly better than the
base attentional system of bahdanau et al. (2015)
(row id56search). when (d) using the input-
feeding approach, we seize another notable gain
of +1.3 id7 and outperform their system. the
local attention model with predictive alignments
(row local-p) proves to be even better, giving
us a further improvement of +0.9 id7 on top
of the global attention model.
is interest-
ing to observe the trend previously reported in
(luong et al., 2015) that perplexity strongly corre-
lates with translation quality. in total, we achieve
a signi   cant gain of 5.0 id7 points over the
non-attentional baseline, which already includes
known techniques such as source reversing and
dropout.

it

the unknown replacement technique proposed
in (luong et al., 2015; jean et al., 2015) yields an-
other nice gain of +1.9 id7, demonstrating that
our attentional models do learn useful alignments
for unknown works. finally, by ensembling 8
different models of various settings, e.g., using
different attention approaches, with and without
dropout etc., we were able to achieve a new sota
result of 23.0 id7, outperforming the existing

best system (jean et al., 2015) by +1.4 id7.

system
top     id4 + 5-gram rerank (montreal)
our ensemble 8 models + unk replace

id7
24.9
25.9

table 2: wmt   15 english-german results    
nist id7 scores of
the winning entry in
wmt   15 and our best one on newstest2015.

latest results in wmt   15     despite the fact that
our models were trained on wmt   14 with slightly
less data, we test them on newstest2015 to demon-
strate that they can generalize well to different test
sets. as shown in table 2, our best system es-
tablishes a new sota performance of 25.9 id7,
outperforming the existing best system backed by
id4 and a 5-gram lm reranker by +1.0 id7.

4.3 german-english results
we carry out a similar set of experiments for the
wmt   15 translation task from german to en-
glish. while our systems have not yet matched
the performance of the sota system, we never-
theless show the effectiveness of our approaches
with large and progressive gains in terms of id7
as illustrated in table 3. the attentional mech-
anism gives us +2.2 id7 gain and on top of
that, we obtain another boost of up to +1.0 id7
from the input-feeding approach. using a better
alignment function, the content-based dot product
one, together with dropout yields another gain of
+2.7 id7. lastly, when applying the unknown
word replacement technique, we seize an addi-
tional +2.1 id7, demonstrating the usefulness
of attention in aligning rare words.

5 analysis

we conduct extensive analysis to better understand
our models in terms of learning, the ability to han-
dle long sentences, choices of attentional architec-
tures, and alignment quality. all results reported
here are on english-german newstest2014.

system
wmt   15 systems
sota     phrase-based (edinburgh)
id4 + 5-gram rerank (mila)
our id4 systems
base (reverse)
+ global (location)
+ global (location) + feed
+ global (dot) + drop + feed
+ global (dot) + drop + feed + unk

ppl.

id7

29.2
27.6

16.9

19.1 (+2.2)
20.1 (+1.0)
22.8 (+2.7)
24.9 (+2.1)

14.3
12.7
10.9

9.7

table 3: wmt   15 german-english results    
performances of various systems (similar to ta-
ble 1). the base system already includes source
reversing on which we add global attention,
dropout, input feeding, and unk replacement.

t
s
o
c
 
t
s
e
t

6

5

4

3

2

 

0.2

0.4

0.6

 

basic
basic+reverse
basic+reverse+dropout
basic+reverse+dropout+globalattn
basic+reverse+dropout+globalattn+feedinput
basic+reverse+dropout+plocalattn+feedinput

0.8

1

mini   batches

1.2

1.4

1.6

1.8

x 105

figure 5: learning curves     test cost (ln perplex-
ity) on newstest2014 for english-german id4s
as training progresses.

+ curve) learns slower than other non-dropout
models, but as time goes by, it becomes more ro-
bust in terms of minimizing test errors.

5.2 effects of translating long sentences

we follow (bahdanau et al., 2015) to group sen-
tences of similar lengths together and compute
a id7 score per group. figure 6 shows that
our attentional models are more effective than the
non-attentional one in handling long sentences:
the quality does not degrade as sentences become
longer. our best model (the blue + curve) outper-
forms all other systems in all length buckets.

5.1 learning curves
we compare models built on top of one another as
listed in table 1. it is pleasant to observe in fig-
ure 5 a clear separation between non-attentional
and attentional models. the input-feeding ap-
proach and the local attention model also demon-
strate their abilities in driving the test costs lower.
the non-attentional model with dropout (the blue

5.3 choices of attentional architectures

we examine different id12 (global,
local-p) and different alignment func-
local-m,
tions (location, dot, general, concat) as described
in section 3. due to limited resources, we can-
not run all the possible combinations. however,
results in table 4 do give us some idea about dif-
ferent choices. the location-based function does

25

20

15

u(cid:9)
e
l
b

10

 

10

 

ours, no attn (id7 13.9)
ours, local   p attn (id7 20.9)
ours, best system (id7 23.0)
wmt   14 best (id7 20.7)
jeans et al., 2015 (id7 21.6)

method

global (location)
local-m (general)
local-p (general)

ensemble

berkeley aligner

aer
0.39
0.34
0.36
0.34
0.32

20

30

40

sent lengths

50

60

70

table 6: aer scores     results of various models
on the rwth english-german alignment data.

figure 6: length analysis     translation qualities
of different systems as sentences become longer.

system

global (location)
global (dot)
global (general)
local-m (dot)
local-m (general)
local-p (dot)
local-p (general)

ppl

6.4
6.1
6.1
>7.0
6.2
6.6
5.9

id7

before after unk
19.3 (+1.2)
18.1
18.6
20.5 (+1.9)
19.1 (+1.8)
17.3

x

18.6
18.0
19

x

20.4 (+1.8)
19.6 (+1.9)
20.9 (+1.9)

table 4: attentional architectures     perfor-
mances of different attentional models. we trained
two local-m (dot) models; both have ppl > 7.0.

not learn good alignments:
the global (location)
model can only obtain a small gain when per-
forming unknown word replacement compared to
using other alignment functions.14 for content-
based functions, our implementation concat does
not yield good performances and more analysis
should be done to understand the reason.15 it is
interesting to observe that dot works well for the
global attention and general is better for the local
attention. among the different models, the local
attention model with predictive alignments (local-
p) is best, both in terms of perplexities and id7.

5.4 alignment quality
a by-product of attentional models are word align-
ments. while (bahdanau et al., 2015) visualized

14there is a subtle difference in how we retrieve align-
ments for the different alignment functions. at time step t in
which we receive yt   1 as input and then compute ht, at, ct,
and   ht before predicting yt, the alignment vector at is used
as alignment weights for (a) the predicted word yt in the
location-based alignment functions and (b) the input word
yt   1 in the content-based functions.

15with concat, the perplexities achieved by different mod-
els are 6.7 (global), 7.1 (local-m), and 7.1 (local-p). such
high perplexities could be due to the fact that we simplify the
a to set the part that corresponds to   hs to identity.
matrix w

alignments for some sample sentences and ob-
served gains in translation quality as an indica-
tion of a working attention model, no work has as-
sessed the alignments learned as a whole. in con-
trast, we set out to evaluate the alignment quality
using the alignment error rate (aer) metric.

given the gold alignment data provided by
rwth for 508 english-german europarl sen-
tences, we    force    decode our attentional models
to produce translations that match the references.
we extract only one-to-one alignments by select-
ing the source word with the highest alignment
weight per target word. nevertheless, as shown in
table 6, we were able to achieve aer scores com-
parable to the one-to-many alignments obtained
by the berkeley aligner (liang et al., 2006).16

we also found that the alignments produced by
local id12 achieve lower aers than
those of the global one. the aer obtained by the
ensemble, while good, is not better than the local-
m aer, suggesting the well-known observation
that aer and translation scores are not well cor-
related (fraser and marcu, 2007). we show some
alignment visualizations in appendix a.

it

5.5 sample translations
we show in table 5 sample translations in both
directions.
it appealing to observe the ef-
fect of attentional models in correctly translating
names such as    miranda kerr    and    roger dow   .
non-attentional models, while producing sensi-
ble names from a language model perspective,
lack the direct connections from the source side
to make correct translations. we also observed
an interesting case in the second example, which
requires translating the doubly-negated phrase,
   not incompatible   . the attentional model cor-
rectly produces    nicht . . . unvereinbar   ; whereas
the non-attentional model generates    nicht verein-

16we concatenate the 508 sentence pairs with 1m sentence

pairs from wmt and run the berkeley aligner.

(cid:9)
(cid:9)
(cid:9)
(cid:9)
english-german translations
src orlando bloom and miranda kerr still love each other
ref orlando bloom und miranda kerr lieben sich noch immer
best orlando bloom und miranda kerr lieben einander noch immer .
base orlando bloom und lucas miranda lieben einander noch immer .
src

       we     re pleased the faa recognizes that an enjoyable passenger experience is not incompatible
with safety and security ,        said roger dow , ceo of the u.s. travel association .
    wir freuen uns , dass die faa erkennt , dass ein angenehmes passagiererlebnis nicht im wider-
spruch zur sicherheit steht     , sagte roger dow , ceo der u.s. travel association .
       wir freuen uns , dass die faa anerkennt , dass ein angenehmes ist nicht mit sicherheit und
sicherheit unvereinbar ist        , sagte roger dow , ceo der us - die .
       wir freuen uns   uber die <unk> , dass ein <unk> <unk> mit sicherheit nicht vereinbar ist mit
sicherheit und sicherheit        , sagte roger cameron , ceo der us - <unk> .

ref

best

base

in an interview , however , bloom said that he and kerr still love .

in einem interview sagte bloom jedoch , dass er und kerr sich noch immer lieben .

german-english translations
src
ref however , in an interview , bloom has said that he and kerr still love each other .
best
base however , in an interview , bloom said that he and tina were still <unk> .
src wegen der von berlin und der europ  aischen zentralbank verh  angten strengen sparpolitik in
verbindung mit der zwangsjacke , in die die jeweilige nationale wirtschaft durch das festhal-
ten an der gemeinsamen w  ahrung gen  otigt wird , sind viele menschen der ansicht , das projekt
europa sei zu weit gegangen
the austerity imposed by berlin and the european central bank , coupled with the straitjacket
imposed on national economies through adherence to the common currency , has led many people
to think project europe has gone too far .

ref

best because of the strict austerity measures imposed by berlin and the european central bank in
connection with the straitjacket in which the respective national economy is forced to adhere to
the common currency , many people believe that the european project has gone too far .

base because of the pressure imposed by the european central bank and the federal central bank
with the strict austerity imposed on the national economy in the face of the single currency ,
many people believe that the european project has gone too far .

table 5: sample translations     for each example, we show the source (src), the human translation (ref),
the translation from our best model (best), and the translation of a non-attentional model (base). we
italicize some correct translation segments and highlight a few wrong ones in bold.

bar   , meaning    not compatible   .17 the attentional
model also demonstrates its superiority in translat-
ing long sentences as in the last example.

6 conclusion

in this paper, we propose two simple and effective
attentional mechanisms for neural machine trans-
lation:
the global approach which always looks
at all source positions and the local one that only
attends to a subset of source positions at a time.
we test the effectiveness of our models in the
wmt translation tasks between english and ger-
man in both directions. our local attention yields
large gains of up to 5.0 id7 over non-attentional

models which already incorporate known tech-
niques such as dropout. for the english to ger-
man translation direction, our ensemble model has
established new state-of-the-art results for both
wmt   14 and wmt   15, outperforming existing
best systems, backed by id4 models and id165
lm rerankers, by more than 1.0 id7.

we have compared various alignment functions
and shed light on which functions are best for
which attentional models. our analysis shows that
attention-based id4 models are superior to non-
attentional ones in many cases, for example in
translating names and handling long sentences.

acknowledgment

17the reference uses a more fancy translation of    incom-
patible   , which is    im widerspruch zu etwas stehen   . both
models, however, failed to translate    passenger experience   .

we gratefully acknowledge support from a gift
from bloomberg l.p. and the support of nvidia

corporation with the donation of tesla k40 gpus.
we thank andrew ng and his group as well as
the stanford research computing for letting us
use their computing resources. we thank rus-
sell stewart for helpful discussions on the models.
lastly, we thank quoc le, ilya sutskever, oriol
vinyals, richard socher, michael kayser, jiwei
li, panupong pasupat, kelvin guu, members of
the stanford nlp group and the annonymous re-
viewers for their valuable comments and feedback.

[papineni et al.2002] kishore papineni, salim roukos,
todd ward, and wei jing zhu.
2002. id7: a
method for automatic evaluation of machine trans-
lation. in acl.

[sutskever et al.2014] i. sutskever, o. vinyals, and
q. v. le. 2014. sequence to sequence learning with
neural networks. in nips.

[xu et al.2015] kelvin xu, jimmy ba, ryan kiros,
kyunghyun cho, aaron c. courville, ruslan
salakhutdinov, richard s. zemel, and yoshua ben-
gio. 2015. show, attend and tell: neural image cap-
tion generation with visual attention. in icml.

[zaremba et al.2015] wojciech

sutskever, and oriol vinyals.
neural network id173. in iclr.

ilya
zaremba,
2015. recurrent

a alignment visualization

we visualize the alignment weights produced by
our different id12 in figure 7. the vi-
sualization of the local attention model is much
sharper than that of the global one. this contrast
matches our expectation that local attention is de-
signed to only focus on a subset of words each
time. also, since we translate from english to ger-
man and reverse the source english sentence, the
white strides at the words    reality    and    .    in the
global attention model reveals an interesting ac-
cess pattern: it tends to refer back to the beginning
of the source sequence.

compared to the alignment visualizations in
(bahdanau et al., 2015), our alignment patterns
are not as sharp as theirs. such difference could
possibly be due to the fact that translating from
english to german is harder than translating into
french as done in (bahdanau et al., 2015), which
is an interesting point to examine in future work.

references

[bahdanau et al.2015] d. bahdanau, k. cho,

and
y. bengio. 2015. id4 by
jointly learning to align and translate. in iclr.

[buck et al.2014] christian buck, kenneth hea   eld,
and bas van ooyen. 2014. id165 counts and lan-
guage models from the common crawl. in lrec.

[cho et al.2014] kyunghyun cho, bart van merrien-
boer, caglar gulcehre, fethi bougares, holger
schwenk, and yoshua bengio.
2014. learning
phrase representations using id56 encoder-decoder
for id151. in emnlp.

[fraser and marcu2007] alexander fraser and daniel
marcu. 2007. measuring word alignment quality
for id151. computational
linguistics, 33(3):293   303.

[gregor et al.2015] karol gregor, ivo danihelka, alex
graves, danilo jimenez rezende, and daan wier-
stra. 2015. draw: a recurrent neural network for
image generation. in icml.

[jean et al.2015] s  ebastien jean, kyunghyun cho,
roland memisevic, and yoshua bengio. 2015. on
using very large target vocabulary for neural ma-
chine translation. in acl.

[kalchbrenner and blunsom2013] n. kalchbrenner and
p. blunsom. 2013. recurrent continuous translation
models. in emnlp.

[koehn et al.2003] philipp koehn, franz josef och,
and daniel marcu. 2003. statistical phrase-based
translation. in naacl.

[liang et al.2006] p. liang, b. taskar, and d. klein.

2006. alignment by agreement. in naacl.

[luong et al.2015] m.-t. luong, i. sutskever, q. v. le,
o. vinyals, and w. zaremba. 2015. addressing the
rare word problem in id4. in
acl.

[mnih et al.2014] volodymyr mnih, nicolas heess,
alex graves, and koray kavukcuoglu. 2014. re-
current models of visual attention. in nips.

understand
europe
in theory
exists
but

why

they

do not

in reality

.

not

understand
europe
in theory
exists
but

why

they

do not

in reality

.

not

sie
verstehen
nicht
,
warum
europa
theoretisch
zwar
existiert
,
aber
nicht
in
wirklichkeit
.

sie
verstehen
nicht
,
warum
europa
theoretisch
zwar
existiert
,
aber
nicht
in
wirklichkeit
.

understand
europe
in theory
exists
but

why

they

do not

in reality

.

not

understand
europe
in theory
exists
but

why

they

do not

in reality

.

not

sie
verstehen
nicht
,
warum
europa
theoretisch
zwar
existiert
,
aber
nicht
in
wirklichkeit
.

sie
verstehen
nicht
,
warum
europa
theoretisch
zwar
existiert
,
aber
nicht
in
wirklichkeit
.

figure 7: alignment visualizations     shown are images of the attention weights learned by various
models: (top left) global, (top right) local-m, and (bottom left) local-p. the gold alignments are displayed
at the bottom right corner.

