mapping unseen words to task-trained embedding spaces

pranava swaroop madhyastha    mohit bansal    kevin gimpel    karen livescu   

   universitat polit`ecnica de catalunya

pranava@cs.upc.edu

   toyota technological institute at chicago

{mbansal,kgimpel,klivescu}@ttic.edu

6
1
0
2

 

n
u
j
 

3
2

 
 
]
l
c
.
s
c
[
 
 

2
v
7
8
3
2
0

.

0
1
5
1
:
v
i
x
r
a

abstract

we consider the supervised training set-
ting in which we learn task-speci   c word
embeddings. we assume that we start
with initial embeddings learned from unla-
belled data and update them to learn task-
speci   c embeddings for words in the su-
pervised training data. however, for new
words in the test set, we must use ei-
ther their initial embeddings or a single
unknown embedding, which often leads
to errors. we address this by learning a
neural network to map from initial em-
beddings to the task-speci   c embedding
space, via a multi-loss objective func-
tion. the technique is general, but here
we demonstrate its use for improved de-
pendency parsing (especially for sentences
with out-of-vocabulary words), as well as
for downstream improvements on senti-
ment analysis.

1 introduction

performance on nlp tasks drops signi   cantly
when moving from training sets to held-out
data (petrov et al., 2010). one cause of this drop
is words that do not appear in the training data but
appear in test data, whether in the same domain or
in a new domain. we refer to such out-of-training-
vocabulary (ootv) words as unseen words. nlp
systems often make errors on unseen words and, in
structured tasks like id33, this can
trigger a cascade of errors in the sentence.

id27s can counter the effects of
limited training data (necsulescu et al., 2015;
turian et al., 2010; collobert et al., 2011). while
the effectiveness of pretrained embeddings can
be heavily task-dependent (bansal et al., 2014),

task-speci   c

there is a great deal of work on updating embed-
dings during supervised training to make them
more
(kalchbrenner et al., 2014;
qu et al., 2015; chen and manning, 2014). these
task-trained embeddings have shown encouraging
results but raise some concerns: (1) the updated
embeddings of infrequent words are prone to
over   tting, and (2) many words in the test data
are not contained in the training data at all. in the
latter case, at test time, systems either use a single,
generic embedding for all unseen words or use
their initial embeddings (typically derived from
unlabelled data)
(s  gaard and johannsen, 2012;
collobert et al., 2011). neither choice is ideal:
a single unknown embedding con   ates many
words, while the initial embeddings may be in
a space that
is not comparable to the trained
embedding space.

in this paper, we address both concerns by
learning to map from the initial embedding space
to the task-trained space. we train a neural net-
work mapping function that takes initial word em-
beddings and maps them to task-speci   c embed-
dings that are trained for the given task, via a
multi-loss objective function. we tune the map-
per   s hyperparameters to optimize performance
on each domain of interest,
thereby achieving
some of the bene   ts of id20. we
demonstrate signi   cant improvements in depen-
dency parsing across several domains and for the
downstream task of dependency-based sentiment
analysis using the model of tai et al. (2015).

2 mapping unseen representations

let v = {w1, . . . , wv } be the vocabulary of
word types in a large, unannotated corpus. let
i denote the initial (original) embedding of word
eo
wi computed from this corpus. the initial em-

annotated
training
sentences

initial

embeddings

eo
i

parser
training

model

parameters

w

task-trained
embeddings

et
i

mapper function

initial

embeddings

eo
i

non-linear

layer

mapper function

loss(et

i, em
i )

non-linear

layer

mapped

embeddings

em
i

(a) mapper training

figure 1: system pipeline

unseen
mapped

embeddings

em
i

parser

seen

task-trained
embeddings

et
i

testing
sentences

model

parameters

w

(b) test-time: parsing with mapped embed-
dings

the mapper

the goal of

beddings are typically learned in an unsupervised
way, but for our purposes they can be any ini-
tial embeddings. let t     v be the subset of
words that appear in the annotated training data
for some supervised task-speci   c training. we de-
   ne unseen words as those in the set v \ t . while
our approach is general, for concreteness, we con-
sider the task of id33, so the anno-
tated data consists of sentences paired with depen-
dency trees. we assume a dependency parser that
i for word
learns task-speci   c id27s et
i .
wi     t , starting from the original embedding eo
in this work, we use the stanford neural depen-
dency parser (chen and manning, 2014).
is as

follows.
we are given a training set of n pairs
of initial and task-trained embeddings d =
n(cid:1)(cid:9), and we want to learn a
(cid:8)(cid:0)eo
function g that maps each initial embedding eo
i to
be as close as possible to its corresponding output
embedding et
i. we denote the mapped embedding
i , i.e., em
em
figure 1a describes the training procedure of
the mapper. we use a supervised parser which is
trained on an annotated dataset and initialized with
pre-trained id27s eo
i . the parser uses
back-propagation to update these embeddings dur-
ing training, producing task-trained embeddings et
i
for all wi     t . after we train the parser, the map-
ping function g is trained to map an initial word
i to its parser-trained embedding et
i.
embedding eo

1(cid:1) , . . . ,(cid:0)eo

i = g (eo

n , et

1, et

i ).

at test (or development) time, we use the trained
mapper g to transform the original embeddings of
unseen test words to the parser-trained space (see
figure 1b). when parsing held-out data, we use
the same parser model parameters (w ) as shown
in figure 1b. the only difference is that now some
of the id27s (i.e., for unseen words)
have changed to mapped ones.

2.1 mapper architecture

our proposed mapper is a multi-layer feedfor-
ward neural network that takes an initial word em-
bedding as input and outputs a mapped represen-
tation of the same dimensionality.
in particular,
we use a single hidden layer with a hardtanh non-
linearity, so the function g is de   ned as:

g(eo

i ) = w2(hard tanh(w1eo

i + b1)) + b2

(1)

where w1 and w2 are parameter matrices and b1
and b2 are bias vectors.

the    hardtanh    non-linearity is the standard

   hard    version of hyperbolic tangent:

hard tanh(z) =

   1 if z <    1
z

if    1     z     1
if z > 1

1

         
      

in preliminary experiments we compared with
other non-linear functions (sigmoid, tanh, and
relu), as well as with zero and more than one
non-linear layers. we found that fewer or more
non-linear layers did not improve performance.

2.2 id168

we use a weighted, multi-loss regression ap-
proach, optimizing a weighted sum of mean
squared error and mean absolute error:

and we want to use the parser on a new domain
(e.g., email). we also tried dropout-based regular-
ization (srivastava et al., 2014) for the non-linear
layer but did not see any signi   cant improvement.

loss(y,   y) =

  

n

x

j=1

|yj       yj| + (1       )

n

x

j=1

|yj       yj|2

(2)

i

i (the ground truth) and   y = em

(the
where y = et
prediction) are n-dimensional vectors. this multi-
loss approach seeks to make both the conditional
mean of the predicted representation close to the
task-trained representation (via the squared loss)
and the conditional median of the predicted rep-
resentation close to the task-trained one (via the
mean absolute loss). a weighted multi-criterion
objective allows us to avoid making strong as-
sumptions about
transformation to
be learned. we tune the hyperparameter    on
domain-speci   c held-out data. we try to minimize
the assumptions in our formulation of the loss, and
let the tuning determine the particular mapper con-
   guration that works best for each domain. strict
squared loss or an absolute loss are just special
forms of this id168.

the optimal

for optimization, we use batch limited memory
bfgs (l-bfgs) (liu and nocedal, 1989). in pre-
liminary experiments comparing with stochastic
optimization, we found l-bfgs to be more sta-
ble to train and easier to check for convergence
(as has recently been found in other settings as
well (ngiam et al., 2011)).

2.3 id173

net

we

use

elastic

regulariza-
tion (liu and nocedal, 1989), which linearly
combines    1 and    2 penalties on the parameters to
control the capacity of the mapper function. this
equates to minimizing:

f (  ) = l(  ) +   1k  k1 +

  2
2

k  k2

2

where    is the full set of mapper parameters and
l(  ) is the id168 (eq. 2 summed over
mapper training examples). we tune the hyper-
parameters of the regularizer and the loss func-
tion separately for each task, using a task-speci   c
development set. this gives us additional    exi-
bility to map the embeddings for the domain of
interest, especially when the parser training data
comes from a particular domain (e.g., newswire)

2.4 mapper-parser thresholds

certain words in the parser training data t are
very infrequent, which may lead to inferior task-
speci   c embeddings et
i learned by the parser. we
want our mapper function to be learned on high-
quality task-trained embeddings. after learning a
strong mapping function, we can use it to remap
the inferior task-trained embeddings.

we thus consider several frequency thresholds
that control which id27s to use to train
the mapper and which to map at test time. below
are the speci   c thresholds that we consider:

mapper-training threshold (  t) the mapper is
trained only on embedding pairs for words seen at
least   t times in the training data t .

mapping threshold (  m) for test-time infer-
ence, the mapper will map any word whose count
in t is less than   m. that is, we discard parser-
trained embeddings et
i of these infrequent words
and use our mapper to map the initial embeddings
i instead.
eo
parser threshold (  p) while training the
parser, for words that appear fewer than   p times
in t , the parser replaces them with the    unknown   
embedding. thus, no parser-trained embeddings
will be learned for these words.

in our experiments, we explore a small set of
values from this large space of possible threshold
combinations (detailed below). we consider only
relatively small values for the mapper-training (  t)
and parser thresholds (  p) because as we increase
them, the number of training examples for the
mapper decreases, making it harder to learn an ac-
curate mapping function1.

3 related work

there

are

approaches,
a
single

several

categories

re-
that
unseen
(s  gaard and johannsen, 2012;

including
embedding

of
those
for

lated
learn
words

1note that the training of the mapper tends to be very
quick because training examples are word types rather than
word tokens. when we increase   t, the number of training
examples reduces further. hence, since we do not have many
examples, we want the mapping procedure to have as much
   exibility as possible, so we use multiple losses and regular-
ization strategies, and then tune their relative strengths.

use

that

id165

character-level

chen and manning, 2014; collobert et al., 2011),
those
in-
(luong et al., 2013;
formation
ling et al., 2015;
botha and blunsom, 2014;
ballesteros et al., 2015),
those
using
morphological
informa-
and
tion (candito and crabb  e, 2009; habash, 2009;
marton et al., 2010;
seddah et al., 2010;
bansal and klein, 2011;
attia et al., 2010;
keller and lapata, 2003),
ap-
proaches
jean et al., 2015;
luong et al., 2015;
chitnis and denero, 2015).
the representation for the unknown token is either
learned speci   cally or computed from a selection
of rare words, for example by averaging their
embedding vectors.

(dyer et al., 2015;

hybrid

and

task-trained

embeddings

other work has also found improvements
by combining pre-trained,    xed embeddings
with
(kim, 2014;
paulus et al., 2014). also relevant are approaches
developed speci   cally to handle large target vo-
cabularies (including many rare words) in neural
machine translation systems (jean et al., 2015;
luong et al., 2015; chitnis and denero, 2015).

closely related to our approach is that of
tafforeau et al. (2015). they induce embeddings
for unseen words by combining the embeddings
of the k nearest neighbors. in sec. 4, we show that
our approach outperforms theirs. also related is
the approach taken by kiros et al. (2015). they
learn a linear mapping of the initial embedding
space via unregularized id75. our ap-
proach differs by considering nonlinear mapping
functions, comparing different losses and mapping
thresholds, and learning separately tuned mappers
for each domain of interest. moreover, we focus
on empirically evaluating the effect of the map-
ping of unseen words, showing statistically signif-
icant improvements on both parsing and a down-
stream task (id31).

4 experimental setup
4.1 dependency parser

we use the feed-forward neural network de-
pendency parser of chen and manning (2014). in
all our experiments (unless stated otherwise), we
use the default arc-standard parsing con   gura-
tion and hyperparameter settings.
for evalua-
tion, we compute the percentage of words that
get the correct head, reporting both unlabelled
attachment score (uas) and labelled attachment

score (las). las additionally requires the pre-
dicted dependency label to be correct. to mea-
sure statistical signi   cance, we use a bootstrap
test (efron and tibshirani, 1986) with 100k sam-
ples.

4.2 pre-trained id27s

we use the 100-dimensional glove word em-
beddings from pennington et al. (2014). these
were trained on wikipedia 2014 and the gigaword
v5 corpus and have a vocabulary size of approxi-
mately 400,000.2

4.3 datasets

we consider a number of datasets with varying
rates of ootv words. we de   ne the ootv rate
(or, equivalently, the unseen rate) of a dataset as
the percentage of the vocabulary (types) of words
occurring in the set that were not seen in training.

street

(wsj)

conduct

journal

and
wall
ontonotes-wsj we
experiments
on the wall street journal portion of the english
id32 dataset (marcus et al., 1993). we
follow the standard splits: sections 2-21 for train-
ing, section 22 for validation, and section 23 for
testing. we convert the original phrase structure
trees into dependency trees using stanford basic
dependencies (de marneffe and manning, 2008)
in the stanford dependency parser. the pos
tags are obtained using the stanford pos
tagger (toutanova et al., 2003) in a 10-fold jack-
kni   ng setup on the training data (achieving an
accuracy of 96.96%). the ootv rate in the
development and test sets is approximately 2-3%.
we also conduct experiments on the ontonotes
4.0 dataset (which we denote ontonotes-wsj).
this dataset contains the same sentences as the
wsj corpus (and we use the same data splits),
but has signi   cantly different annotations. the
ontonotes-wsj training data is used for the web
treebank test experiments. we perform the same
pre-processing steps as for the wsj dataset.

web treebank we expect our mapper to be
most effective when parsing held-out data with

2

http://www-nlp.stanford.edu/data/glove.6b.100d.txt.gz;

the

with

have

from

experimented

also
50-dimensional

down-
we
embed-
loadable
with
dings
and
that we
id97 (mikolov et al., 2013) embeddings
trained ourselves; in preliminary experiments the glove em-
beddings performed best, so we use them for all experiments
below.

collobert et al. (2011)

senna

many unseen words. this often happens when the
held-out data is drawn from a different distribution
than the training data. for example, when training
a parser on newswire and testing on web data,
many errors occur due to differing patterns of syn-
tactic usage and unseen words (foster et al., 2011;
petrov and mcdonald, 2012; kong et al., 2014;
wang et al., 2014).

we explore this setting by training our parser
on ontonotes-wsj and testing on the web tree-
bank (petrov and mcdonald, 2012), which in-
cludes    ve domains: answers, email, newsgroups,
reviews, and weblogs. each domain contains ap-
proximately 2000-4000 manually annotated syn-
tactic parse trees in the ontonotes 4.0 style.
in
this case, we are adapting the parser which is
trained on ontonotes corpora using the small de-
velopment set for each of the sub-domains (the
size of the web treebank dev corpora is only
around 1000-2000 trees so we use it for valida-
tion instead of including it in training). as be-
fore, we convert the phrase structure trees to de-
pendency trees using stanford basic dependen-
cies. the parser and the mapper hyperparame-
ters were tuned separately on the development set
for each domain. the unseen rate is typically 6-
10% in the domains of the web treebank. we
used the stanford tagger (toutanova et al., 2003),
which was trained on the ontonotes training cor-
pus, for part-of-speech tagging the web treebank
corpora. the tagger used bidirectional architec-
ture and it included word shape and distributional
similarity features. we train a separate mapper
for each domain, tuning mapper hyperparameters
separately for each domain using the development
sets. in this way, we obtain some of the bene   ts of
id20 for each target domain.

downstream task: id31 with
dependency tree lstms we also perform ex-
periments to analyze the effects of embedding
mapping on a downstream task,
in this case
id31 using the stanford sentiment
treebank (socher et al., 2013). we use the de-
pendency tree long short-term memory network
(tree-lstm) proposed by tai et al. (2015), sim-
ply replacing their default dependency parser with
our version that maps unseen words. the de-
pendency parser is trained on the wsj corpus
and mapped using the wsj development set. we
use the same mapper that was optimized for the
wsj development set, without further hyperpa-

rameter tuning for the mapper. for the tree-
lstm model, we use the same hyperparame-
ter tuning as described in tai et al. (2015). we
use the standard train/development/test splits of
6820/872/1821 sentences for the binary classi   ca-
tion task and 8544/1101/2210 for the    ne-grained
task.

4.4 mapper settings and hyperparameters

the initial embeddings given to the mapper
are the same as the initial embeddings given to
the parser. these are the 100-dimensional glove
embeddings mentioned above. the output di-
mensionality of the mapper is also    xed to 100.
all model parameters of the mappers are initial-
ized to zero. we set the dimensionality of the
non-linear layer to 400 across all experiments.
the model parameters are optimized by maximiz-
ing the weighted multiple-loss objective using l-
bfgs with elastic-net id173 (section 2).
the hyperparameters include the relative weight
of the two objective terms (  ) and the regulariza-
tion constants (  1,   2). for   , we search over val-
ues in {0, 0.1, 0.2, . . . , 1}. for each of   1 and   2,
we consider values in {10   1, 10   2, . . . , 10   9, 0}.
the hyperparameters are tuned via grid search to
maximize the uas on the development set.

5 results and analysis
5.1 results on wsj, ontonotes, and

switchboard

the upper half of table 1 shows our main test
results on wsj, ontonotes, and switchboard, the
low-ootv rate datasets. due to the small initial
ootv rates (<3%), we only see modest gains of
0.3-0.4% in uas, with statistical signi   cance at
p < 0.05 for wsj and ontonotes and p < 0.07
for switchboard. the initial ootv rates are cut
in half by our mapper, with the remaining un-
known words largely being numerical strings and
misspellings.3 when only considering test sen-
tences containing ootv words (the row labeled
   ootv subset   ), the gains are signi   cantly larger
(0.5-0.8% uas at p < 0.05).

5.2 results on web treebank

the lower half of table 1 shows our main test
results on the web treebank   s    ve domains, the

3we could potentially train the initial embeddings on a
larger corpus or use heuristics to convert unknown numbers
and misspellings to forms contained in our initial embed-
dings.

lower ootv word rate

wsj

91.85   92.21
uas
89.49   89.73
las
ootv %
2.72   1.45
ootv uas 89.88   90.51
#sents

337

ontonotes
90.17   90.49
87.68   87.92

2.72   1.4

avg.

90.38   90.70
87.92   88.14

   

89.27   89.81

89.12   89.78

329

   

answers

82.67   83.21
78.98   79.59
8.53   1.22
80.88   81.75

671

emails

81.76   82.42
77.93   78.56
10.56   3.01
79.29   81.02

644

higher ootv word rate
reviews

newsgroups
84.68   85.13
81.88   82.71
10.34   1.04
82.54   83.71

579

84.25   84.99
81.26   81.92
6.84   0.73
81.17   82.22

632

weblogs

87.73   88.43
85.68   86.29
8.45   0.38
86.43   87.31

541

avg.

84.22   84.84
81.01   81.81

   

82.06   83.20

   

table 1: results of id33 on various treebanks. entries of the form a   b give results for parsing without mapped
embeddings (a) and with mapped embeddings (b).    ootv %    entries a   b indicate that a% of the test set vocabulary was
unseen in the parser training, and b% remain unknown after mapping the embeddings.    ootv uas    refers to uas measured
on the subset of the test set sentences that contain at least one ootv word, and    #sents    gives the number of sentences in this
subset.

xcomp

cc

conj

cc

nn

nsubj

xcomp

aux

dobj

det

conj

attr

cc

wife

and

i

attempted

to

adopt

a

dog

and was

nothing

but

frustrating

nsubj

nsubj

unseen word

cc

conj

(a) we obtain correct attachments and correct tree after the mapper maps the unseen word    attempted   .

xcomp

dobj

aux

dobj

det

try google

to    nd

the

title

. . .

aux

xcomp

unseen word

(b) the mapper incorrectly maps    google   , resulting in wrong attachments and wrong tree.

figure 2: examples where the mapper helps and hurts: in the above examples the top arcs are before mapping and bottom ones
are after mapping; dotted lines refer to incorrect attachment.

high-ootv rate datasets. as expected, the map-
per has a much larger impact when parsing these
out-of-domain datasets with high ootv word
rates.4

the ootv rate reduction is much larger than
for the wsj-style datasets, and the parsing im-
provements (uas and las) are statistically sig-
ni   cant at p < 0.05. on subsets containing at
least one ootv word (that also has an initial
embedding), we see an average gain of 1.14%
uas (see row labeled    ootv subset   ).
in this
case, all improvements are statistically signi   cant
at p < 0.02. we observe that the relative reduction
in ootv% for the web treebanks is larger than

4as stated above, we train the parser on the ontonotes
dataset, but tune mapper hyperparameters to maximize pars-
ing performance on each development section of the web
treebank   s    ve domains. we then map the ootv word vec-
tors on each test set domain using the learned mapper for that
domain.

for the wsj, ontonotes, or switchboard datasets.
in particular, we are able to reduce the ootv%
by 71-95% relative. we also see the intuitive trend
that larger relative reductions in ootv rate corre-
late with larger accuracy improvements.

5.3 downstream results

we now report results using the dependency
tree-lstm of tai et al. (2015) for sentiment anal-
ysis on the stanford sentiment treebank. we con-
sider both the binary (positive/negative) and    ne-
grained classi   cation tasks ({very negative, nega-
tive, neutral, positive, and very positive}). we use
the implementation provided by tai et al. (2015),
changing only the dependency parses that are fed
to their model. the sentiment dataset contains ap-
proximately 25% ootv words in the training set
vocabulary, 5% in the development set vocabulary,
and 9% in the test set vocabulary. we map un-

fine-grained
48.4   49.5

binary

85.7    86.1

baseline

84.11

t1

t3

t5

84.89

84.97

84.81

t   
84.14

table 2: improvements on stanford sentiment treebank test
set using our parser with the dependency tree-lstm.

table 3: average web treebank development uas at differ-
ent threshold settings.

seen words using the mapper tuned on the wsj
development set. we use the same dependency
tree-lstm experimental settings as tai et al. re-
sults are shown in table 2. we improve upon the
original accuracies in both binary and    ne-grained
classi   cation. 5 we also reduce the ootv rate
from 25% in the training set vocabulary to about
6%, and from 9% in the test set vocabulary down
to 4%.

5.4 effect of thresholds

we also experimented with different values for
the thresholds described in section 2. for the map-
ping threshold   m, mapper-training threshold   t,
and parser threshold   p, we consider the following
four settings:

t1 :   m =   t =   p = 1
t3 :   m =   t =   p = 3
t5 :   m =   t =   p = 5
t    :   m =    ,   p =   t = 5

using   m =     corresponds to mapping all words
at test time, even words that we have seen many
times in the training data and learned task-speci   c
embeddings for.

we report the average development set uas
over all web treebank domains in table 3. we
see that t3 performs best, though settings t1 and
t5 also improve over the baseline. at threshold t3
we have approximately 20,000 examples for train-
ing the mapper, while at threshold t5 we have only
about 10,000 examples. we see a performance
drop at t   , so it appears better to directly use
the task-speci   c embeddings for words that appear
frequently in the training data. in other results re-
ported in this paper, we used t3 for the web tree-
bank test sets and t1 for the rest.

5.5 effect of weighted multi-loss objective

we analyzed the results when varying   , which
balances between the two components of the map-
per   s multi-loss objective function. we found that,

5note that we report accuracies and improvements on the
dependency parse based system of tai et al. (2015) because
the neural parser that we use is dependency-based.

for all domains except answers, the best results
are obtained with some    between 0 and 1. the
optimal values outperformed the cases with    = 0
and    = 1 by 0.1-0.3% uas absolute. however,
on the answers domain, the best performance was
achieved with    = 0; i.e., the mapper preferred
mean squared error. for other domains, the opti-
mal    tended to be within the range [0.3, 0.7].

5.6 comparison with related work

we compare to the approach presented by
tafforeau et al. (2015). they propose to re   ne
embeddings for unseen words based on the rela-
tive shifts of their k nearest neighbors in the orig-
inal embeddings space. speci   cally, they de   ne
   arti   cial re   nement    as:

  r(t) =   o(t) +

k

x

k=1

  k(  r(nk)       o(nk))

(3)

where   r(.) is the vector in the re   ned embedding
space and   o(.) is the vector in the original em-
bedding space. they de   ne   k to be proportional
to the cosine similarity between the target unseen
word (t) and neighbor (nk):

  k = s(t, nk) =

  o(t).  o(nk)
|  (t)||  o(nk)|

avg. uas avg. las

baseline
id92
our mapped

84.11
84.54
84.97

81.02
81.38
81.79

table 4: comparison to k-nearest neighbor matching of
tafforeau et al. (2015).

table 4 shows the average performance of the
models over the development sets of the web tree-
bank. on average, our mapper outperforms the k-
nn approach (k = 3).

5.7 id33 examples

in figure 2, we show two sentences: an instance
where the mapper helps and another where the
mapper hurts the parsing performance.6 in the    rst

6sentences in figure 2 are taken from the development

portion of the answers domain from the web treebank.

sentence (figure 2a), the parsing model has not
seen the word    attempted    during training. note
that the sentence contains 3 verbs:    attempted   ,
   adopt   , and    was   . even with the pos tags, the
parser was unable to get the correct dependency
attachment. after mapping, the parser correctly
makes    attempted    the root and gets the correct
arcs and the correct tree. the 3 nearest neighbors
of    attempted    in the mapped embedding space are
   attempting   ,    tried   , and    attempt   . we also see
here that a single unseen word can lead to multi-
ple errors in the parse.

in the second example (figure 2b), the default
model assigns the correct arcs using the pos in-
formation even though it has not seen the word
   google   . however, using the mapped represen-
tation for    google   , the parser makes errors. the
3-nearest neighbors for    google    in the mapped
space are    damned   ,    look   , and    hash   . we hy-
pothesize that the mapper has mapped this noun
instance of    google    to be closer to verbs instead
of nouns, which would explain the incorrect at-
tachment.

5.8 analyzing mapped representations

   poor   ,

   horrible   ,

to understand the mapped embedding space,
we use id167 (van der maaten and hinton, 2008)
to visualize a small subset of embeddings. in fig-
ure 3, we plot the initial embeddings, the parser-
trained embeddings, and    nally the mapped em-
beddings. we include four unseen words (shown
in caps):
   marvelous   , and
   magni   cent   . in figure 3a and figure 3b, the em-
beddings for the unseen words are identical (even
though id167 places them in different places when
producing its projection).
in figure 3c, we ob-
serve that the mapper has placed the unseen words
within appropriate areas of the space with respect
to similarity with the seen words. we contrast this
with figure 3b, in which the unseen words appear
to be within a different region of the space from
all seen words.

6 conclusion

we have described a simple method to resolve
unseen words when training supervised models
that learn task-speci   c id27s: a feed-
forward neural network that maps initial embed-
dings to the task-speci   c embedding space. we
demonstrated signi   cant improvements in depen-
dency parsing accuracy across several domains, as
well as improvements on a downstream task. our

(a) initial representational space

(b) learned representational space

(c) mapped representational space

figure 3: id167 plots on initial, parser trained, and mapped
representations.

approach is simple, effective, and applicable to
many other settings, both inside and outside nlp.

acknowledgments

we would like to thank the anonymous review-
ers for their useful comments. this research was

supported by a google faculty research award to
mohit bansal, karen livescu and kevin gimpel.

references
[attia et al.2010] mohammed attia, jennifer foster,
deirdre hogan, joseph le roux, lamia tounsi, and
josef van genabith.
2010. handling unknown
words in statistical latent-variable parsing models
for arabic, english and french.
in proceedings of
the naacl hlt 2010 first workshop on statistical
parsing of morphologically-rich languages, pages
67   75. association for computational linguistics.

[ballesteros et al.2015] miguel ballesteros, chris dyer,
and noah a. smith. 2015.
improved transition-
based parsing by modeling characters instead of
words with lstms.
in proceedings of the 2015
conference on empirical methods in natural lan-
guage processing, pages 349   359, lisbon, portugal,
september. association for computational linguis-
tics.

[bansal and klein2011] mohit bansal and dan klein.
2011. web-scale features for full-scale parsing. in
proceedings of acl.

[bansal et al.2014] mohit bansal, kevin gimpel, and
karen livescu. 2014. tailoring continuous word
representations for id33. in proceed-
ings of acl.

[botha and blunsom2014] jan a. botha and phil blun-
som. 2014. compositional morphology for word
representations and language modelling. in interna-
tional conference on machine learning (icml).

[candito and crabb  e2009] marie candito and beno    t
crabb  e. 2009. improving generative statistical pars-
ing with semi-supervised word id91. in pro-
ceedings of the 11th international conference on
parsing technologies, pages 138   141. association
for computational linguistics.

[chen and manning2014] danqi chen and christo-
pher d manning. 2014. a fast and accurate de-
pendency parser using neural networks.
in em-
pirical methods in natural language processing
(emnlp).

[chitnis and denero2015] rohan chitnis and john
denero. 2015. variable-length word encodings
for neural translation models.
in proceedings of
the 2015 conference on empirical methods in natu-
ral language processing, pages 2088   2093, lisbon,
portugal, september. association for computational
linguistics.

[collobert et al.2011] r. collobert, j. weston, l. bot-
tou, m. karlen, k. kavukcuoglu, and p. kuksa.
2011. natural language processing (almost) from
scratch.
journal of machine learning research,
12:24932537.

[de marneffe and manning2008] marie-catherine

de marneffe and christopher d manning. 2008.
stanford typed dependencies manual. technical
report, stanford university.

[dyer et al.2015] chris dyer, miguel ballesteros,
wang ling, austin matthews, and noah a. smith.
2015. transition-based id33 with
stack long short-term memory.
in proceedings of
the 53rd annual meeting of the association for
computational linguistics and the 7th international
joint conference on natural language processing
(volume 1: long papers), pages 334   343, bei-
jing, china, july. association for computational
linguistics.

[efron and tibshirani1986] b. efron and r. tibshirani.
1986. bootstrap methods for standard errors, con-
   dence intervals, and other measures of statistical
accuracy. statist. sci., 1(1):54   75, 02.

[foster et al.2011] jennifer foster,

  ozlem c   etinoglu,
joachim wagner, joseph le roux, stephen hogan,
joakim nivre, deirdre hogan, and josef van gen-
abith. 2011. # hardtoparse: id52 and parsing
the twitterverse. in aaai 2011 workshop on ana-
lyzing microtext, pages 20   25.

[habash2009] nizar habash. 2009. remoov: a tool
for online handling of out-of-vocabulary words in
machine translation.
in proceedings of the 2nd
international conference on arabic language re-
sources and tools (medar), cairo, egypt.

[jean et al.2015] s  ebastien jean, kyunghyun cho,
roland memisevic, and yoshua bengio. 2015. on
using very large target vocabulary for neural ma-
chine translation. in proceedings of the 53rd annual
meeting of the association for computational lin-
guistics and the 7th international joint conference
on natural language processing (volume 1: long
papers), pages 1   10, beijing, china, july. associa-
tion for computational linguistics.

[kalchbrenner et al.2014] nal kalchbrenner, edward
grefenstette, and phil blunsom. 2014. a convo-
lutional neural network for modelling sentences. in
proceedings of the 52nd annual meeting of the as-
sociation for computational linguistics (volume 1:
long papers), pages 655   665, baltimore, maryland,
june. association for computational linguistics.

[keller and lapata2003] frank keller and mirella la-
pata.
2003. using the web to obtain frequen-
cies for unseen bigrams. computational linguistics,
29(3):459   484.

[kim2014] yoon kim. 2014. convolutional neural net-
works for sentence classi   cation. in proceedings of
the 2014 conference on empirical methods in nat-
ural language processing (emnlp), pages 1746   
1751, doha, qatar, october. association for com-
putational linguistics.

[kiros et al.2015] ryan kiros, yukun zhu, ruslan
salakhutdinov, richard s zemel, antonio torralba,
raquel urtasun, and sanja fidler.
2015. skip-
thought vectors. arxiv preprint arxiv:1506.06726.

[kong et al.2014] lingpeng kong, nathan schneider,
swabha swayamdipta, archna bhatia, chris dyer,
and noah a. smith. 2014. a dependency parser for
tweets. in proceedings of the 2014 conference on
empirical methods in natural language processing
(emnlp), pages 1001   1012, doha, qatar, october.
association for computational linguistics.

[ling et al.2015] wang ling, tiago lu    s, lu    s marujo,
r  amon fernandez astudillo, silvio amir, chris
dyer, alan w black, and isabel trancoso. 2015.
finding function in form: compositional character
models for open vocabulary word representation. in
proc. of emnlp.

[liu and nocedal1989] d. c. liu and j. nocedal. 1989.
on the limited memory bfgs method for large
scale optimization. math. programming, 45(3, (ser.
b)):503   528.

[luong et al.2013] thang luong, richard socher, and
christopher manning. 2013. better word repre-
sentations with id56s for mor-
phology.
in proceedings of the seventeenth con-
ference on computational natural language learn-
ing, pages 104   113, so   a, bulgaria, august. asso-
ciation for computational linguistics.

[luong et al.2015] thang luong, ilya sutskever, quoc
le, oriol vinyals, and wojciech zaremba. 2015.
addressing the rare word problem in neural ma-
chine translation. in proceedings of the 53rd annual
meeting of the association for computational lin-
guistics and the 7th international joint conference
on natural language processing (volume 1: long
papers), pages 11   19, beijing, china, july. associ-
ation for computational linguistics.

[marcus et al.1993] mitchell p marcus, mary ann
marcinkiewicz, and beatrice santorini.
1993.
building a large annotated corpus of english:
the id32.
linguistics,
19(2):313   330.

computational

[marton et al.2010] yuval marton, nizar habash, and
owen rambow. 2010.
improving arabic depen-
dency parsing with lexical and in   ectional morpho-
logical features.
in proceedings of the naacl
hlt 2010 first workshop on statistical parsing of
morphologically-rich languages, pages 13   21, los
angeles, ca, usa, june. association for computa-
tional linguistics.

[mikolov et al.2013] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013. ef   cient estima-
tion of word representations in vector space. arxiv
preprint arxiv:1301.3781.

[necsulescu et al.2015] silvia

sara
mendes, david jurgens, n  uria bel, and roberto

necsulescu,

navigli. 2015. reading between the lines: over-
coming data sparsity for accurate classi   cation of
lexical relationships. in proceedings of the fourth
joint conference on lexical and computational
semantics, pages 182   192, denver, colorado, june.
association for computational linguistics.

[ngiam et al.2011] jiquan ngiam, adam coates, ah-
bik lahiri, bobby prochnow, quoc v. le, and an-
drew y. ng. 2011. on optimization methods for
deep learning. in proceedings of the 28th interna-
tional conference on machine learning (icml-11),
pages 265   272.

[paulus et al.2014] romain paulus, richard socher,
and christopher d manning. 2014. global belief
id56s.
in advances in neural
information processing systems, pages 2888   2896.

[pennington et al.2014] jeffrey pennington, richard
socher, and christopher manning. 2014. glove:
global vectors for word representation. in proceed-
ings of the 2014 conference on empirical methods
in natural language processing (emnlp), pages
1532   1543, doha, qatar, october. association for
computational linguistics.

[petrov and mcdonald2012] slav petrov and ryan mc-
donald. 2012. overview of the 2012 shared task
on parsing the web. notes of the first workshop
on syntactic analysis of non-canonical language
(sancl).

[petrov et al.2010] slav petrov,

pi-chuan chang,
michael ringgaard, and hiyan alshawi.
2010.
uptraining for accurate deterministic question
parsing. in proceedings of the 2010 conference on
empirical methods in natural language process-
ing, pages 705   713. association for computational
linguistics.

[qu et al.2015] lizhen qu, gabriela ferraro, liyuan
zhou, weiwei hou, nathan schneider, and timothy
baldwin. 2015. big data small data, in domain out-
of domain, known word unknown word: the impact
of word representation on sequence labelling tasks.
arxiv preprint arxiv:1504.05319.

[seddah et al.2010] djam  e seddah, grzegorz chrupa  a,
ozlem cetinoglu, josef van genabith, and marie
candito. 2010. lemmatization and lexicalized sta-
tistical parsing of morphologically-rich languages:
the case of french.
in proceedings of the naacl
hlt 2010 first workshop on statistical parsing of
morphologically-rich languages, pages 85   93, los
angeles, ca, usa, june. association for computa-
tional linguistics.

[socher et al.2013] richard socher, alex perelygin,
jean wu, jason chuang, christopher d. manning,
andrew ng, and christopher potts. 2013. recur-
sive deep models for semantic compositionality over
a sentiment treebank.
in proceedings of the 2013

conference on empirical methods in natural lan-
guage processing, pages 1631   1642, seattle, wash-
ington, usa, october. association for computa-
tional linguistics.

[s  gaard and johannsen2012] anders s  gaard and an-
ders johannsen. 2012. robust learning in random
subspaces: equipping nlp for oov effects. in pro-
ceedings of coling 2012: posters, mumbai, india,
december.

[srivastava et al.2014] nitish srivastava, geoffrey hin-
ton, alex krizhevsky, ilya sutskever, and ruslan
salakhutdinov. 2014. dropout: a simple way to
prevent neural networks from over   tting. journal of
machine learning research, 15:1929   1958.

[tafforeau et al.2015] jeremie

thierry
artieres, benoit favre, and frederic bechet. 2015.
adapting lexical
representation and oov han-
dling from written to spoken language with word
embedding. in interspeech.

tafforeau,

[tai et al.2015] kai sheng tai, richard socher, and
christopher d. manning. 2015. improved semantic
representations from tree-structured long short-term
memory networks. in proceedings of the 53rd an-
nual meeting of the association for computational
linguistics and the 7th international joint confer-
ence on natural language processing (volume 1:
long papers), pages 1556   1566, beijing, china,
july. association for computational linguistics.

[toutanova et al.2003] kristina toutanova, dan klein,
christopher d manning, and yoram singer. 2003.
feature-rich part-of-speech tagging with a cyclic de-
pendency network.
in proceedings of the 2003
conference of the north american chapter of the
association for computational linguistics on hu-
man language technology-volume 1, pages 173   
180. association for computational linguistics.

[turian et al.2010] joseph turian, lev ratinov, and
yoshua bengio. 2010. word representations: a sim-
ple and general method for semisupervised learn-
ing. in proceedings of the 48th annual meeting of
the association for computational linguistics, page
384394. association for computational linguistics.

[van der maaten and hinton2008] laurens van

der
maaten and geoffrey hinton. 2008. visualizing
data using id167.
journal of machine learning
research, 9(2579-2605):85.

[wang et al.2014] william yang wang, lingpeng
kong, kathryn mazaitis, and william w cohen.
2014. id33 for weibo: an ef   cient
probabilistic logic programming approach.
in
proceedings of
the 2014 conference on empir-
ical methods in natural language processing
(emnlp), pages 1152   1158, doha, qatar, october.
association for computational linguistics.

