what to talk about and how? selective generation using

lstms with coarse-to-fine alignment

hongyuan mei

mohit bansal

matthew r. walter

toyota technological institute at chicago

{hongyuan,mbansal,mwalter}@ttic.edu

chicago, il 60637

6
1
0
2

 

n
a
j
 

8

 
 
]
l
c
.
s
c
[
 
 

2
v
8
3
8
0
0

.

9
0
5
1
:
v
i
x
r
a

abstract

an

i.e.,

propose

end-to-end,

for selective generation,

we
domain-
independent neural encoder-aligner-decoder
model
the
joint task of content selection and surface
realization. our model    rst encodes a full
set of over-determined database event records
via an lstm-based recurrent neural network,
then utilizes a novel coarse-to-   ne aligner to
identify the small subset of salient records to
talk about, and    nally employs a decoder to
generate free-form descriptions of the aligned,
selected records. our model achieves the
best selection and generation results reported
to-date (with 59% relative improvement in
generation) on the benchmark weather-
gov dataset, despite using no specialized
features or linguistic resources. using an
improved k-nearest neighbor beam    lter
helps further. we also perform a series of
ablations and visualizations to elucidate the
contributions of our key model components.
lastly, we evaluate the generalizability of
our model on the robocup dataset, and get
results that are competitive with or better than
the state-of-the-art, despite being severely
data-starved.

introduction

1
we consider the important task of producing a natu-
ral language description of a rich world state rep-
resented as an over-determined database of event
records. this task, which we refer to as selective
generation, is often formulated as two subproblems:
content selection, which involves choosing a sub-
set of relevant records to talk about from the ex-

1

haustive database, and surface realization, which is
concerned with generating natural language descrip-
tions for this subset. learning to perform these tasks
jointly is challenging due to the ambiguity in decid-
ing which records are relevant, the complex depen-
dencies between selected records, and the multiple
ways in which these records can be described.

previous work has made signi   cant progress on
this task (chen and mooney, 2008; angeli et al.,
2010; kim and mooney, 2010; konstas and lap-
ata, 2012). however, most approaches solve the
two content selection and surface realization sub-
tasks separately, use manual domain-dependent re-
sources (e.g., semantic parsers) and features, or em-
ploy template-based generation. this limits do-
main adaptability and reduces coherence. we take
an alternative, neural encoder-aligner-decoder ap-
proach to free-form selective generation that jointly
performs content selection and surface realization,
without using any specialized features, resources, or
generation templates. this enables our approach to
generalize to new domains. further, our memory-
based model captures the long-range contextual de-
pendencies among records and descriptions, which
are integral to this task (angeli et al., 2010).

we formulate our model as an encoder-aligner-
decoder framework that uses recurrent neural net-
works with long short-term memory units (lstm-
id56s) (hochreiter and schmidhuber, 1997) to-
gether with a coarse-to-   ne aligner to select and
   translate    the rich world state into a natural lan-
guage description. our model    rst encodes the full
set of over-determined event records using a bidirec-
tional lstm-id56. a novel coarse-to-   ne aligner

then reasons over multiple abstractions of the input
to decide which of the records to discuss. the model
next employs an lstm decoder to generate natural
language descriptions of the selected records.

the use of lstms, which have proven effective
for similar long-range generation tasks (sutskever et
al., 2014; vinyals et al., 2015b; karpathy and fei-
fei, 2015), allows our model to capture the long-
range contextual dependencies that exist in selec-
tive generation. further, the introduction of our pro-
posed variation on alignment-based lstms (bah-
danau et al., 2014; xu et al., 2015) enables our
model to learn to perform content selection and sur-
face realization jointly, by aligning each generated
word to an event record during decoding. our novel
coarse-to-   ne aligner avoids searching over the full
set of over-determined records by employing two
stages of increasing complexity: a pre-selector and
a re   ner acting on multiple abstractions (low- and
high-level) of the record input. the end-to-end na-
ture of our framework has the advantage that it can
be trained directly on corpora of record sets paired
with natural language descriptions, without the need
for ground-truth content selection.

we evaluate our model on a benchmark weather
forecasting dataset (weathergov) and achieve
the best results reported to-date on content selection
(12% relative improvement in f-1) and language
generation (59% relative improvement in id7),
despite using no domain-speci   c resources. we
also perform a series of ablations and visualiza-
tions to elucidate the contributions of the primary
model components, and also show improvements
with a simple, k-nearest neighbor beam    lter ap-
proach. finally, we demonstrate the generalizability
of our model by directly applying it to a benchmark
sportscasting dataset (robocup), where we get re-
sults competitive with or better than state-of-the-art,
despite being extremely data-starved.

2 related work

selective generation is a relatively new research area
and more attention has been paid to the individ-
ual content selection and selective realization sub-
problems. with regards to the former, barzilay and
lee (2004) model the content structure from unan-
notated documents and apply it to the application

of text summarization. barzilay and lapata (2005)
treat content selection as a collective classi   cation
problem and simultaneously optimize the local label
assignment and their pairwise relations. liang et al.
(2009) address the related task of aligning a set of
records to given textual description clauses. they
propose a generative semi-markov alignment model
that jointly segments text sequences into utterances
and associates each to the corresponding record.

surface realization is often treated as a problem
of producing text according to a given grammar.
soricut and marcu (2006) propose a language gen-
eration system that uses the widl-representation,
a formalism used to compactly represent probabil-
ity distributions over    nite sets of strings. wong
and mooney (2007) and lu and ng (2011) use syn-
chronous context-free grammars to generate natural
language sentences from formal meaning represen-
tations. similarly, belz (2008) employs probabilis-
tic context-free grammars to perform surface real-
ization. other effective approaches include the use
of tree conditional random    elds (lu et al., 2009)
and template extraction within a log-linear frame-
work (angeli et al., 2010).

recent work seeks to solve the full selective
generation problem through a single framework.
chen and mooney (2008) and chen et al. (2010)
learn alignments between comments and their cor-
responding event records using a translation model
for parsing and generation. kim and mooney (2010)
implement a two-stage framework that decides what
to discuss using a combination of the methods of
lu et al. (2008) and liang et al. (2009), and then
produces the text based on the generation system of
wong and mooney (2007).

angeli et al. (2010) propose a uni   ed concept-
to-text model that treats joint content selection and
surface realization as a sequence of local decisions
represented by a log-linear model. similar to other
work, they train their model using external align-
ments from liang et al. (2009). generation then fol-
lows as id136 over this model, where they    rst
choose an event record, then the record   s    elds (i.e.,
attributes), and    nally a set of templates that they
then    ll in with words for the selected    elds. their
ability to model long-range dependencies relies on
their choice of features for the log-linear model,
while the template-based generation further employs

2

some domain-speci   c features for    uent output.

konstas and lapata (2012) propose an alternative
method that simultaneously optimizes the content
selection and surface realization problems. they
employ a probabilistic context-free grammar that
speci   es the structure of the event records, and then
treat generation as    nding the best derivation tree
according to this grammar. however, their method
still selects and orders records in a local fashion via
a markovized chaining of records. konstas and la-
pata (2013) improve upon this approach with global
id194s. however, this approach
also requires alignment during training, which they
estimate using the method of liang et al. (2009).

we treat the problem of selective generation as
end-to-end learning via a recurrent neural network
encoder-aligner-decoder model, which enables us
to jointly learn content selection and surface re-
alization directly from database-text pairs, without
the need for an external aligner or ground-truth se-
lection labels. the use of lstm-id56s enables
our model to capture the long-range dependencies
that exist among the records and natural language
output. additionally, the model does not rely on
any manually-selected or domain-dependent fea-
tures, templates, or parsers, and is thereby general-
izable. the alignment-id56 approach has recently
proven successful for generation-style tasks, e.g.,
machine translation (bahdanau et al., 2014) and im-
age captioning (xu et al., 2015). since selective
generation requires identifying the small number of
salient records among an over-determined database,
we avoid performing exhaustive search over the full
record set, and instead propose a novel coarse-to-
   ne aligner that divides the search complexity into
pre-selection and re   nement stages.

3 task de   nition

we consider the problem of generating a natural
language description for a rich world state speci-
   ed in terms of an over-determined set of records
(database). this problem requires deciding which
of the records to discuss (content selection) and
how to discuss them (surface realization). train-
ing data consists of scenario pairs (r(i), x(i)) for
i = 1, 2, . . . , n, where r(i) is the complete set of
records and x(i) is the natural language description

r1:n :

temperature(time=17-06, min=48, mean=53, max=61)
windspeed(time=17-06, min=3, mean=6, max=11)
winddir(time=17-06, mode=ssw)
gust(time=17-06, min=0, mean=0, max=0)
skycover(time=17-21, mode=0-25)
skycover(time=02-06, mode=75-100)
precipchance(time=17-06, min=2, mean=14, max=20)
rainchance(time=17-06, mode=somechance)

x1:n :

   a 20 percent chance of showers after midnight. increas-
ing clouds, with a low around 48 southwest wind between
5 and 10 mph   

(a) weathergov

r1:n :

x1:n :

pass(arg1=purple6, arg2=purple3)
kick(arg1=purple3)
badpass(arg1=purple3, arg2=pink9)
turnover(arg1=purple3, arg2=pink9)

   purple3 made a bad pass that was picked off by pink9   

(b) robocup

figure 1: sample database-text pairs chosen from the
(a) weathergov and (b) robocup datasets.

(fig. 1). at test time, only the records are given. we
evaluate our model in the context of two publicly-
available benchmark selective generation datasets.

weathergov the weather forecasting dataset
(see fig. 1(a)) of liang et al. (2009) consists of
29528 scenarios, each with 36 weather records (e.g.,
temperature, sky cover, etc.) paired with a natural
language forecast ( 28.7 avg. word length).

robocup we evaluate our model   s generaliz-
ability on the sportscasting dataset of chen and
mooney (2008), which consists of only 1539 pairs
of temporally ordered robot soccer events (e.g., pass,
score) and commentary drawn from the four-game
2001   2004 robocup    nals (see fig. 1(b)). each
scenario contains an average of 2.4 event records
and a 5.7 word natural language commentary.

4 the model

we formulate selective generation as id136
over a probabilistic model p (x1:t|r1:n ), where
r1:n = (r1, r2, . . . , rn ) is the input set of over-

3

as multi-level representation of the input to compute
the selection decision zt at each decoding step t. the
model then employs an id56 decoder to arrive at the
word likelihood p (xt|x0:t   1, r1:n ) as a function of
the multi-level input and the hidden state of the de-
coder st   1 at time step t     1. in order to model the
long-range dependencies among the records and de-
scriptions (which is integral to effectively perform-
ing selective generation (angeli et al., 2010; kon-
stas and lapata, 2012; konstas and lapata, 2013)),
our model employs lstm units as the nonlinear en-
coder and decoder functions.

the set of event

encoder our lstm-id56 encoder
(fig. 2)
takes as input
records rep-
resented as a sequence r1:n = (r1, r2, . . . , rn )
and returns a sequence of hidden annotations
h1:n = (h1, h2, . . . , hn ), where the annotation hj
summarizes the record rj. this results in a represen-
tation that models the dependencies that exist among
the records in the database.

we adopt an encoder architecture similar to that

of graves et al. (2013)

             ie

j
f e
j
oe
j
ge
j

             =

               

  
  

             t e

(cid:19)

(cid:18) rj

hj   1

tanh
j (cid:12) ce
j (cid:12) ge
j   1 + ie
j (cid:12) tanh(ce
j)

j

ce
j = f e
hj = oe

(2a)

(2b)
(2c)

where t e is an af   ne transformation,    is the logis-
tic sigmoid that restricts its input to [0, 1], ie
j, f e
j ,
j are the input, forget, and output gates of the
and oe
j is the memory cell acti-
lstm, respectively, and ce
vation vector. the memory cell ce
j summarizes the
j   1 and the current in-
lstm   s previous memory ce
put, which are modulated by the forget and input
gates, respectively. our encoder operates bidirec-
tionally, encoding the records in both the forward
and backward directions, which provides a better
summary of the input records. in this way, the hid-
      
      
j )(cid:62) concatenate for-
h (cid:62)
h (cid:62)
den annotations hj = (
j ;
      
      
ward
h j and backward
h j annotations, each deter-
mined using equation (2c).

coarse-to-fine aligner having encoded the in-
put records r1:n to arrive at the hidden annotations

4

figure 2: our model architecture with a bidirectional
lstm encoder, coarse-to-   ne aligner, and decoder.

determined event records,1 x1:t = (x1, x2, . . . , xt )
is the generated description with xt being the word
at time t and x0 being a special start token:

p (x1:t|r1:n )

x   
1:t = arg max

x1:t

= arg max

t(cid:89)

x1:t

t=1

p (xt|x0:t   1, r1:n )

(1a)

(1b)

the goal of id136 is to generate a natural lan-
guage description for a given set of records. an
effective means of learning to perform this gen-
eration is to use an encoder-aligner-decoder archi-
tecture with a recurrent neural network, which has
proven effective for related problems in machine
translation (bahdanau et al., 2014) and image cap-
tioning (xu et al., 2015). we propose a variation on
this general model with novel components that are
well-suited to the selective generation problem.
our model (fig. 2)    rst encodes each input record
rj into a hidden state hj with j     {1, . . . , n} us-
ing a bidirectional recurrent neural network (id56).
our novel coarse-to-   ne aligner then acts on a con-
catenation mj of each record and its hidden state
1these records may take the form of an unordered set
or have a natural ordering (e.g.,
in the case of
robocup). in order to make our model generalizable, we treat
the set as a sequence and use the order speci   ed by the dataset.
we note that it is possible that a different ordering will yield
improved performance, since ordering has been shown to be
important when operating on sets (vinyals et al., 2015a).

temporal

lstmlstmlstmstd. align.refinerlstmlstmpre-selec.in selective generation,

h1:n , the model then seeks to select the content at
each time step t that will be used for generation. our
model performs content selection using an extension
of the alignment mechanism proposed by bahdanau
et al. (2014), which allows for selection and genera-
tion that is independent of the ordering of the input.
the given set of event
records is over-determined with only a small sub-
set of salient records being relevant to the out-
put natural language description. standard align-
ment mechanisms limit the accuracy of selection
and generation by scanning the entire range of over-
determined records.
in order to better address the
selective generation task, we propose a coarse-to-
   ne aligner that prevents the model from being dis-
tracted by non-salient records. our model aligns
based on multiple abstractions of the input: both the
original input record as well as the hidden annota-
tions mj = (r(cid:62)
j )(cid:62), an approach that has previ-
ously been shown to yield better results than align-
ing based only on the hidden state (mei et al., 2015).
our coarse-to-   ne aligner avoids searching over
the full set of over-determined records by using two
stages of increasing complexity: a pre-selector and
re   ner (fig. 2). the pre-selector    rst assigns to each
record a id203 pj of being selected, while the
standard aligner computes the alignment likelihood
wtj over all the records at each time step t during
decoding. next, the re   ner produces the    nal se-
lection decision by re-weighting the aligner weights
wtj with the pre-selector probabilities pj:
q(cid:62) tanh(p mj)
pj = sigmoid
  tj = v(cid:62)tanh(w st   1 + u mj)
exp(  tj)
wtj = exp(  tj)/

(3b)
(3c)

j ; h(cid:62)

(cid:16)

(cid:17)

(3a)

  tj = pjwtj/

pjwtj

(cid:88)

j

  tjmj

zt =

(3d)

(3e)

j

where p , q, u, w , v are learned parameters. ideally,
the selection decision would be based on the highest-
value alignment zt = mk where k = arg maxj   tj.
however, we use the weighted average (eqn. 3e) as
its soft approximation to maintain differentiability of
the entire architecture.

(cid:88)
(cid:88)

j

the pre-selector assigns large values (pj > 0.5)
to a small subset of salient records and small val-
ues (pj < 0.5) to the rest. this modulates the stan-
dard aligner, which then has to assign a large weight
wtj in order to select the j-th record at time t. in
this way, the learned prior pj makes it dif   cult for
the alignment (attention) to be distracted by non-
salient records. further, we can relate the output
of the pre-selector to the number of records that are
selected. speci   cally, the output pj expresses the
extent to which the j-th record should be selected.
j=1 pj can then be regarded as
a real-valued approximation to the total number of
pre-selected records (denoted as   ), which we regu-
larize towards, based on validation (see eqn. 5).

the summation (cid:80)n

decoder our architecture uses an lstm decoder
that takes as input the current context vector zt,
the last word xt   1, and the lstm   s previous hid-
den state st   1. the decoder outputs the conditional
id203 distribution px,t = p (xt|x0:t   1, r1:n )
over the next word, represented as a deep output
layer (pascanu et al., 2014),

             id

t
f d
t
od
t
gd
t

             =

               

  
  

             t d

      ext   1

      

st   1
zt
tanh
t (cid:12) cd
t (cid:12) gd
t   1 + id
t (cid:12) tanh(cd
t )

cd
t = f d
st = od
lt = l0(ext   1 + lsst + lzzt)

t

px,t = softmax (lt)

(4a)

(4b)
(4c)
(4d)
(4e)

where e (an embedding matrix), l0, ls, and lz are
parameters to be learned.

((cid:80)n

training and id136 we train the model us-
ing the database-record pairs (r1:n , x1:t ) from the
training corpora so as to maximize the likelihood of
the ground-truth language description x   
1:t (eqn. 1).
additionally, we introduce a id173 term
j=1 pj       )2 that enables the model to in   uence
the pre-selector weights based on the aforemen-
tioned relationship between the output of the pre-
selector and the number of selected records. more-
over, we also introduce the term (1.0     max(pj)),
which accounts for the fact that at least one record
should be pre-selected. note that when    is equal to

5

n, the pre-selector is forced to select all the records
(pj = 1.0 for all j), and the coarse-to-   ne alignment
reverts to the standard alignment introduced by bah-
danau et al. (2014). together with the negative log-
likelihood of the ground-truth description x   
1:t , our
id168 becomes
l =     log p (x   

(5a)

=     t(cid:88)
       n(cid:88)

t=1

log p (x   

1:t|r1:n ) + g
(cid:16)

t|x0:t   1, r1:n ) + g
(cid:17)

1     max(pj)

      2

+

g =

pj       

(5b)

(5c)

j=1

having trained the model, we generate the natural
language description by    nding the maximum a pos-
teriori words under the learned model (eqn. 1). for
id136, we perform greedy search starting with
the    rst word x1. id125 offers a way to per-
form approximate joint id136     however, we
empirically found that id125 does not perform
any better than greedy search on the datasets that we
consider, an observation that is shared with previous
work (angeli et al., 2010). we later discuss an al-
ternative k-nearest neighbor-based beam    lter (see
sec 6.2).

5 experimental setup
datasets we analyze our model on the benchmark
weathergov dataset, and use the data-starved
robocup dataset to demonstrate the model   s gen-
eralizability. following angeli et al. (2010), we
use weathergov training, development, and test
splits of size 25000, 1000, and 3528, respectively.
for robocup, we follow the evaluation method-
ology of previous work (chen and mooney, 2008),
performing three-fold cross-validation whereby we
train on three games (approximately 1000 scenarios)
and test on the fourth. within each split, we hold
out 10% of the training data as the development set
to tune the early-stopping criterion and   . we then
report the standard average performance (weighted
by the number of scenarios) over these four splits.

(id7), and choose 500 units from {250, 500, 750}
and    = 8.5 from {6.5, 7.5, 8.5, 10.5, 12.5}. for
robocup, we only tune    on the development set
and choose    = 5.0 from the set {1.0, 2.0, . . . , 6.0}.
however, we do not retune the number of hidden
units on robocup. for each iteration, we ran-
domly sample a mini-batch of 100 scenarios during
back-propagation and use adam (kingma and ba,
2015) for optimization. training typically converges
within 30 epochs. we select the model according to
the id7 score on the development set.2

id74 we consider two metrics as a
means of evaluating the effectiveness of our model
on the two selective generation subproblems. for
content selection, we use the f-1 score of the set of
selected records as de   ned by the harmonic mean of
precision and recall with respect to the ground-truth
selection record set. we de   ne the set of selected
records as consisting of the record with the largest
selection weight   ti computed by our aligner at each
decoding step t.

we evaluate the quality of surface realization us-
ing the id7 score3 (a 4-gram matching-based pre-
cision) (papineni et al., 2001) of the generated de-
scription with respect to the human-created refer-
ence. to be comparable to previous results on
weathergov, we also consider a modi   ed id7
score (cid7) that does not penalize numerical de-
viations of at most    ve (angeli et al., 2010) (i.e.,
to not penalize    low around 58    compared to a ref-
erence    low around 60   ). on robocup, we also
evaluate the id7 score in the case that ground-
truth content selection is known (sid7g), to be
comparable to previous work.

6 results and analysis

we analyze the effectiveness of our model on
the benchmark weathergov (as primary) and
robocup (as generalization) datasets. we also
present several ablations to illustrate the contribu-
tions of the primary model components.

training details on weathergov, we lightly
tune the number of hidden units and    on the de-
velopment set according to the generation metric

2we implement our model in theano (bergstra et al., 2010;
bastien et al., 2012) and will make the code publicly available.
3we compute id7 using the publicly available evaluation

provided by angeli et al. (2010).

6

table 1: primary weathergov results

method
kl12
kl13
alk10
our model

f-1
   
   

65.40
73.21

sid7 cid7

33.70
36.54
38.40
61.01

   
   

51.50
70.39

6.1 primary results (weathergov)
we report the performance of content selection and
surface realization using f-1 and two id7 scores
(standard sid7 and the customized cid7 of an-
geli et al. (2010)), respectively (sec. 5). table 1
compares our test results against previous meth-
ods that include kl12 (konstas and lapata, 2012),
kl13 (konstas and lapata, 2013), and alk10 (an-
geli et al., 2010). our method achieves the best
results reported to-date on all three metrics, with
relative improvements of 11.94% (f-1), 58.88%
(sid7), and 36.68% (cid7) over the previous
state-of-the-art.

6.2 beam filter with k-nearest neighbors
we considered id125 as an alternative to
greedy search in our primary setup (eqn. 1), but
this performs worse, similar to what previous work
found on this dataset (angeli et al., 2010). as an
alternative, we consider a beam    lter based on a k-
nearest neighborhood. see supplementary material
for details. table 9 shows that this id92 beam    lter
improves results over the primary greedy results.

table 2: id92 beam    lter (test set)

primary

id92 beam filter

sid7
cid7

61.01
70.39

61.76
71.23

6.3 ablation analysis (weathergov)
next, we present several ablations to analyze the
contribution of our model components.4
aligner ablation first, we evaluate the contribu-
tion of our proposed coarse-to-   ne aligner by com-
paring our model with the basic encoder-aligner-
introduced by bahdanau et al.
decoder model

4these results are based on our primary model of sec. 6.1

and on the development set.

(2014). table 3 reports the results demonstrating
that our aligner yields superior f-1 and id7 scores
relative to a standard aligner.

table 3: coarse-to-   ne aligner ablation (dev set)

method
basic
coarse-to-   ne

f-1

sid7 cid7

60.35
76.28

63.54
65.58

74.90
75.78

encoder ablation next, we consider the effec-
tiveness of the encoder. table 4 compares the results
with and without the encoder on the development
set, and demonstrates that there is a signi   cant gain
from encoding the event records using the lstm-
id56. we attribute this improvement to the lstm-
id56   s ability to capture the relationships that exist
among the records, which is known to be essential
to selective generation (barzilay and lapata, 2005;
angeli et al., 2010).

table 4: encoder ablation (dev set)

encoder
with
without

f-1

sid7 cid7

76.28
57.45

65.58
56.47

75.78
68.63

6.4 qualitative analysis (weathergov)
output examples fig. 3 shows an example
record set with its output description and record-
word alignment heat map. as shown, our model
learns to align records with their corresponding
words (e.g., winddir and    southeast,    temperature
and    71,    windspeed and    wind 10,    and gust and
   winds could gust as high as 30 mph   ). it also learns
the subset of salient records to talk about (matching
the ground-truth description perfectly for this ex-
ample, i.e., a standard id7 of 100.00). we also
see some word-level mismatch, e.g.,    cloudy    mis-
aligns to id-0 temp and id-10 precipchance, which
we attribute to the high correlation between these
types of records (   garbage collection    in liang et
al. (2009)).
id27s training our decoder has the
effect of learning embeddings for the words in the
training set (via the embedding matrix e in eqn. 4).
here, we explore the extent to which these learned

7

record details:
id-0: temperature(time=06-21, min=52, mean=63, max=71);
id-3: winddir(time=06-21, mode=sse);
id-5: skycover(time=6-21, mode=50-75);
id-15: thunderchance(time=13-21, mode=schc)

id-2: windspeed(time=06-21, min=8, mean=17, max=23);

id-4: gust(time=06-21, min=0, mean=10, max=30);

id-10: precipchance(time=06-21, min=19, mean=32, max=73);

figure 3: an example generation for a set of records from weathergov.

embeddings capture semantic relationships among
the training words. table 10 presents nearest neigh-
bor words for some of the common words from the
weathergov dataset (according to cosine similar-
ity in the embedding space). more details of other
embedding approaches that we tried are discussed
in the supplementary material section.

table 5: nearest neighbor word for example words

word
gusts
clear
isolated
southeast
storms

decreasing

nearest neighbor

gust
sunny
scattered
northeast

winds
falling

6.5 out-of-domain results (robocup)
we use the robocup dataset
to evaluate the
domain-independence of our model. the dataset
is severely data-starved with only 1000 (approx.)
training pairs, which is much smaller than is typi-
cally necessary to train id56s. this results in higher
variance in the trained model distributions, and we
thus adopt the standard denoising method of ensem-
bles (sutskever et al., 2014; vinyals et al., 2015b;
zaremba et al., 2014).5

5we use an ensemble of    ve randomly initialized models.

table 6: robocup results

method
cm08
ljk09
ckm10
alk10
kl12
our model

f-1

72.00
75.70
79.30
79.90

   

81.58

sid7 sid7g
28.70

   
   
   
   

24.88
25.28

   
   

28.80
30.90
29.40

following previous work, we perform two exper-
iments on the robocup dataset (table 6), the    rst
considering full selective generation and the second
assuming ground-truth content selection at test time.
on the former, we obtain a standard id7 score
(sid7) of 25.28, which exceeds the best score of
24.88 (konstas and lapata, 2012). additionally,
we achieve an selection f-1 score of 81.58, which
is also the best result reported to-date. in the case
of assumed (known) ground-truth content selection,
our model attains an sid7g score of 29.40, which
is competitive with the state-of-the-art.6

7 conclusion

we presented an encoder-aligner-decoder model for
selective generation that does not use any spe-
cialized features,
linguistic resources, or genera-

6the chen and mooney (2008) sid7g result is from an-

geli et al. (2010).

8

tion templates. our model employs a bidirec-
tional lstm-id56 model with a novel coarse-to-
   ne aligner that jointly learns content selection and
surface realization. we evaluate our model on
the benchmark weathergov dataset and achieve
state-of-the-art selection and generation results. we
achieve further improvements via a k-nearest neigh-
bor beam    lter. we also present several model ab-
lations and visualizations to elucidate the effects of
the primary components of our model. moreover,
our model generalizes to a different, data-starved do-
main (robocup), where it achieves results compet-
itive with or better than the state-of-the-art.

acknowledgments

we thank gabor angeli, david chen, and ioannis
konstas for their helpful comments.

a supplementary material

the following provides further evaluations of our
model as a supplement to our original manuscript.

a.1 beam filter with k-nearest neighbors
we perform greedy search as an approximation to
full id136 over the set of decision variables
(eqn. 1). we considered id125 as an alterna-
tive, but as with previous work on this dataset (an-
geli et al., 2010), we found that greedy search still
yields better id7 performance (table 7).

table 7: effect of beam width

beam width m
dev sid7
dev cid7
test sid7
test cid7

1

2

5

10

65.58
75.78
61.01
70.39

64.70
74.91
60.15
69.42

57.02
65.83
53.70
61.95

47.07
54.19
44.32
51.01

as an alternative, we consider a beam    lter based
on a k-nearest neighborhood. first, we generate the
m-best description candidates (i.e., a beam width
of m) for a given input record set (database) us-
ing standard id125. next, we    nd the k
nearest neighbor database-description pairs from the
training data, based on the cosine similarity of each
neighbor database with the given input record. we
then compute the id7 score for each of the m de-
scription candidates relative to the k nearest neigh-

bor descriptions (as references) and select the candi-
date with the highest id7 score. we tune k and
m on the development set and report the results in
table 8. table 9 presents the test results with this
tuned setting (m = 2, k = 1), where we achieve
id7 scores better than our primary greedy results.

table 8: id92 beam    lter (dev set)

sid7 m = 2 m = 5 m = 10

k = 1
k = 2
k = 5
k = 10

65.99
65.89
65.64
65.91

65.88
65.98
65.45
65.89

65.65
65.83
65.41
65.12

cid7 m = 2 m = 5 m = 10

k = 1
k = 2
k = 5
k = 10

76.21
75.99
75.90
75.95

76.13
76.03
75.63
75.87

75.98
75.82
75.41
75.23

table 9: id92 beam    lter (test set)

primary

id92 (m = 2, k = 1)

sid7
cid7

61.01
70.39

61.76
71.23

a.2 id27s (trained & pretrained)
training our decoder has the effect of learning em-
beddings for the words in the training set (via the
embedding matrix e in eqn. 4). here, we ex-
plore the extent to which these learned embeddings
capture semantic relationships among the training
words. table 10 presents nearest neighbor words for
some of the common words from the weather-
gov dataset (according to cosine similarity in the
embedding space).

table 10: nearest neighbor word for example words

word
gusts
clear
isolated
southeast
storms

decreasing

nearest neighbor

gust
sunny
scattered
northeast

winds
falling

9

we also consider different ways of using pre-
trained id27s (mikolov et al., 2013) to
bootstrap the quality of our learned embeddings.
one approach initializes our embedding matrix with
the pre-trained vectors and then re   nes the embed-
ding based on our training corpus. the second con-
catenates our learned embedding matrix with the
pre-trained vectors in an effort to simultaneously ex-
ploit general similarities as well as those learned
for the domain. as shown previously for other
tasks (vinyals et al., 2014; vinyals et al., 2015b), we
   nd that the use of pre-trained embeddings results in
negligible improvements (on the development set).

references
gabor angeli, percy liang, and dan klein. 2010. a
simple domain-independent probabilistic approach to
in proceedings of the conference on
generation.
empirical methods in natural language processing
(emnlp), pages 502   512.

dzmitry bahdanau, kyunghyun cho, and yoshua ben-
2014. id4 by jointly
arxiv preprint

gio.
learning to align and translate.
arxiv:1409.0473.

regina barzilay and mirella lapata. 2005. collective
content selection for concept-to-text generation.
in
proceedings of the human language technology con-
ference and the conference on empirical methods in
natural language processing (hlt/emnlp), pages
331   338.

regina barzilay and lillian lee. 2004. catching the
drift: probabilistic content models, with applications
in proceedings of
to generation and summarization.
the conference of the north american chapter of the
association for computational linguistics human
language technologies (naacl hlt), pages 113   
120.

fr  ed  eric bastien, pascal lamblin, razvan pascanu,
james bergstra, ian j. goodfellow, arnaud berg-
eron, nicolas bouchard, and yoshua bengio. 2012.
theano: new features and speed improvements. nips
workshop on deep learning and unsupervised fea-
ture learning.

anja belz.

2008.

automatic generation of
weather forecast texts using comprehensive probabilis-
tic generation-space models. natural language engi-
neering, 14(04):431   455.

james bergstra, olivier breuleux, fr  ed  eric bastien, pas-
cal lamblin, razvan pascanu, guillaume desjardins,
joseph turian, david warde-farley, and yoshua ben-
gio. 2010. theano: a cpu and gpu math expression

compiler. in proceedings of the scienti   c computing
with python conference (scipy).

david l. chen and raymond j. mooney. 2008. learning
to sportscast: a test of grounded id146.
in proceedings of the international conference on ma-
chine learning (icml), pages 128   135.

david l. chen, joohyun kim, and raymond j. mooney.
2010. training a multilingual sportscaster: using per-
ceptual context to learn language. journal of arti   cial
intelligence research, 37:397   435.

alex graves, mohamed abdel-rahman, and geoffrey
hinton. 2013. id103 with deep recurrent
in proceedings of the ieee inter-
neural networks.
national conference on acoustics, speech and signal
processing (icassp), pages 6645   6649.

sepp hochreiter and j  urgen schmidhuber. 1997. long
short-term memory. neural computation, 9(8):1735   
1780.

andrej karpathy and li fei-fei. 2015. deep visual-
semantic alignments for generating image descrip-
tions. in proceedings of the ieee conference on com-
puter vision and pattern recognition (cvpr), pages
3128   3137.

joohyun kim and raymond j mooney. 2010. gen-
erative alignment and id29 for learning
in proceedings of the
from ambiguous supervision.
international conference on computational linguis-
tics (coling), pages 543   551.
diederik kingma and jimmy ba.

2015. adam: a
method for stochastic optimization. in proceedings of
the international conference on learning representa-
tions (iclr).

ioannis konstas and mirella lapata. 2012. unsuper-
vised concept-to-text generation with hypergraphs. in
proceedings of the conference of the north american
chapter of the association for computational linguis-
tics human language technologies (naacl hlt),
pages 752   761.

ioannis konstas and mirella lapata. 2013. inducing doc-
in pro-
ument plans for concept-to-text generation.
ceedings of the conference on empirical methods in
natural language processing (emnlp), pages 1503   
1514.

percy liang, michael i. jordan, and dan klein. 2009.
learning semantic correspondences with less supervi-
in proceedings of the joint conference of the
sion.
annual meeting of the association for computational
linguistics and the international joint conference on
natural language processing (acl/ijcnlp), pages
91   99.

wei lu and hwee tou ng. 2011. a probabilistic forest-
to-string model for language generation from typed
in proceedings of the
id198 expressions.

10

conference on empirical methods in natural lan-
guage processing (emnlp), pages 1611   1622.

wei lu, hwee tou ng, wee sun lee, and luke s zettle-
moyer. 2008. a generative model for parsing natural
language to meaning representations. in proceedings
of the conference on empirical methods in natural
language processing (emnlp), pages 783   792.

wei lu, hwee tou ng, and wee sun lee. 2009. natu-
ral language generation with tree conditional random
   elds. in proceedings of the conference on empirical
methods in natural language processing (emnlp),
pages 400   409.

hongyuan mei, mohit bansal, and matthew r. walter.
2015. listen, attend, and walk: neural mapping of
navigational instructions to action sequences. arxiv
preprint arxiv:1506.04089.

tomas mikolov, kai chen, greg corrado, and jeffrey
dean. 2013. ef   cient estimation of word represen-
in proceedings of the in-
tations in vector space.
ternational conference on learning representations
(iclr).

kishore papineni, salim roukos, todd ward, and wei-
jing zhu. 2001. id7: a method for automatic eval-
uation of machine translation. in proceedings of the
annual meeting of the association for computational
linguistics (acl), pages 311   318.

razvan pascanu, caglar gulcehre, kyunghyun cho, and
yoshua bengio. 2014. how to construct deep re-
in proceedings of the in-
current neural networks.
ternational conference on learning representations
(iclr).

radu soricut and daniel marcu. 2006. stochastic lan-
guage generation using widl-expressions and its ap-
plication in machine translation and summarization.
in proceedings of the international conference on

computational linguistics and the annual meeting of
the association for computational linguistics (col-
ing/acl), pages 1105   1112.

ilya sutskever, oriol vinyals, and quoc v. lee. 2014.
sequence to sequence learning with neural networks.
in advances in neural information processing sys-
tems (nips).

oriol vinyals, lukasz kaiser, terry koo, slav petrov,
2014.
arxiv preprint

ilya sutskever,
grammar as a foreign language.
arxiv:1412.7449.

and geoffrey hinton.

oriol vinyals, samy bengio, and manjunath kudlur.
2015a. order matters: sequence to sequence for sets.
arxiv preprint arxiv:1511.06391.

oriol vinyals, alexander toshev, samy bengio, and du-
mitru erhan. 2015b. show and tell: a neural image
in proceedings of the ieee con-
caption generator.
ference on id161 and pattern recognition
(cvpr), pages 3156   3164.

yuk wah wong and raymond j mooney. 2007. gen-
eration by inverting a semantic parser that uses statis-
tical machine translation. in proceedings of the con-
ference of the north american chapter of the associa-
tion for computational linguistics human language
technologies (naacl hlt), pages 172   179.

kelvin xu, jimmy ba, ryan kiros, kyunghyun cho,
aaron courville, ruslan salakhutdinov, richard
zemel, and yoshua bengio. 2015. show, attend and
tell: neural image id134 with visual at-
in proceedings of the international confer-
tention.
ence on machine learning (icml).

wojciech zaremba, ilya sutskever, and oriol vinyals.
2014. recurrent neural network id173. arxiv
preprint arxiv:1409.2329.

11

