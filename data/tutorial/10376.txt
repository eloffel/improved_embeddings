3
1
0
2

 

p
e
s
7

 

 
 
]
l
c
.
s
c
[
 
 

3
v
1
8
7
3

.

1
0
3
1
:
v
i
x
r
a

ef   cient estimation of word representations in

vector space

tomas mikolov

kai chen

google inc., mountain view, ca

google inc., mountain view, ca

tmikolov@google.com

kaichen@google.com

greg corrado

jeffrey dean

google inc., mountain view, ca

google inc., mountain view, ca

gcorrado@google.com

jeff@google.com

abstract

we propose two novel model architectures for computing continuous vector repre-
sentations of words from very large data sets. the quality of these representations
is measured in a word similarity task, and the results are compared to the previ-
ously best performing techniques based on different types of neural networks. we
observe large improvements in accuracy at much lower computational cost, i.e. it
takes less than a day to learn high quality word vectors from a 1.6 billion words
data set. furthermore, we show that these vectors provide state-of-the-art perfor-
mance on our test set for measuring syntactic and semantic word similarities.

1

introduction

many current nlp systems and techniques treat words as atomic units - there is no notion of similar-
ity between words, as these are represented as indices in a vocabulary. this choice has several good
reasons - simplicity, robustness and the observation that simple models trained on huge amounts of
data outperform complex systems trained on less data. an example is the popular id165 model
used for statistical id38 - today, it is possible to train id165s on virtually all available
data (trillions of words [3]).
however, the simple techniques are at their limits in many tasks. for example, the amount of
relevant in-domain data for automatic id103 is limited - the performance is usually
dominated by the size of high quality transcribed speech data (often just millions of words). in
machine translation, the existing corpora for many languages contain only a few billions of words
or less. thus, there are situations where simple scaling up of the basic techniques will not result in
any signi   cant progress, and we have to focus on more advanced techniques.
with progress of machine learning techniques in recent years, it has become possible to train more
complex models on much larger data set, and they typically outperform the simple models. probably
the most successful concept is to use distributed representations of words [10]. for example, neural
network based language models signi   cantly outperform id165 models [1, 27, 17].

1.1 goals of the paper

the main goal of this paper is to introduce techniques that can be used for learning high-quality word
vectors from huge data sets with billions of words, and with millions of words in the vocabulary. as
far as we know, none of the previously proposed architectures has been successfully trained on more

1

than a few hundred of millions of words, with a modest dimensionality of the word vectors between
50 - 100.
we use recently proposed techniques for measuring the quality of the resulting vector representa-
tions, with the expectation that not only will similar words tend to be close to each other, but that
words can have multiple degrees of similarity [20]. this has been observed earlier in the context
of in   ectional languages - for example, nouns can have multiple word endings, and if we search for
similar words in a subspace of the original vector space, it is possible to    nd words that have similar
endings [13, 14].
somewhat surprisingly, it was found that similarity of word representations goes beyond simple
syntactic regularities. using a word offset technique where simple algebraic operations are per-
formed on the word vectors, it was shown for example that vector(   king   ) - vector(   man   ) + vec-
tor(   woman   ) results in a vector that is closest to the vector representation of the word queen [20].
in this paper, we try to maximize accuracy of these vector operations by developing new model
architectures that preserve the linear regularities among words. we design a new comprehensive test
set for measuring both syntactic and semantic regularities1, and show that many such regularities
can be learned with high accuracy. moreover, we discuss how training time and accuracy depends
on the dimensionality of the word vectors and on the amount of the training data.

1.2 previous work

representation of words as continuous vectors has a long history [10, 26, 8]. a very popular model
architecture for estimating neural network language model (nnlm) was proposed in [1], where a
feedforward neural network with a linear projection layer and a non-linear hidden layer was used to
learn jointly the word vector representation and a statistical language model. this work has been
followed by many others.
another interesting architecture of nnlm was presented in [13, 14], where the word vectors are
   rst learned using neural network with a single hidden layer. the word vectors are then used to train
the nnlm. thus, the word vectors are learned even without constructing the full nnlm. in this
work, we directly extend this architecture, and focus just on the    rst step where the word vectors are
learned using a simple model.
it was later shown that the word vectors can be used to signi   cantly improve and simplify many
nlp applications [4, 5, 29]. estimation of the word vectors itself was performed using different
model architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word
vectors were made available for future research and comparison2. however, as far as we know, these
architectures were signi   cantly more computationally expensive for training than the one proposed
in [13], with the exception of certain version of log-bilinear model where diagonal weight matrices
are used [23].

2 model architectures

many different types of models were proposed for estimating continuous representations of words,
including the well-known latent semantic analysis (lsa) and id44 (lda).
in this paper, we focus on distributed representations of words learned by neural networks, as it was
previously shown that they perform signi   cantly better than lsa for preserving linear regularities
among words [20, 31]; lda moreover becomes computationally very expensive on large data sets.
similar to [18], to compare different model architectures we de   ne    rst the computational complex-
ity of a model as the number of parameters that need to be accessed to fully train the model. next,
we will try to maximize the accuracy, while minimizing the computational complexity.

1the test set is available at www.fit.vutbr.cz/  imikolov/id56lm/word-test.v1.txt
2http://ronan.collobert.com/senna/

http://metaoptimize.com/projects/wordreprs/
http://www.fit.vutbr.cz/  imikolov/id56lm/
http://ai.stanford.edu/  ehhuang/

2

for all the following models, the training complexity is proportional to

o = e    t    q,

(1)

where e is number of the training epochs, t is the number of the words in the training set and q is
de   ned further for each model architecture. common choice is e = 3    50 and t up to one billion.
all models are trained using stochastic id119 and id26 [26].

2.1 feedforward neural net language model (nnlm)

the probabilistic feedforward neural network language model has been proposed in [1]. it consists
of input, projection, hidden and output layers. at the input layer, n previous words are encoded
using 1-of-v coding, where v is size of the vocabulary. the input layer is then projected to a
projection layer p that has dimensionality n    d, using a shared projection matrix. as only n
inputs are active at any given time, composition of the projection layer is a relatively cheap operation.
the nnlm architecture becomes complex for computation between the projection and the hidden
layer, as values in the projection layer are dense. for a common choice of n = 10, the size of the
projection layer (p ) might be 500 to 2000, while the hidden layer size h is typically 500 to 1000
units. moreover, the hidden layer is used to compute id203 distribution over all the words in the
vocabulary, resulting in an output layer with dimensionality v . thus, the computational complexity
per each training example is

q = n    d + n    d    h + h    v,

(2)
where the dominating term is h    v . however, several practical solutions were proposed for
avoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized
models completely by using models that are not normalized during training [4, 9]. with binary tree
representations of the vocabulary, the number of output units that need to be evaluated can go down
to around log2(v ). thus, most of the complexity is caused by the term n    d    h.
in our models, we use hierarchical softmax where the vocabulary is represented as a huffman binary
tree. this follows previous observations that the frequency of words works well for obtaining classes
in neural net language models [16]. huffman trees assign short binary codes to frequent words, and
this further reduces the number of output units that need to be evaluated: while balanced binary tree
would require log2(v ) outputs to be evaluated, the huffman tree based hierarchical softmax requires
only about log2(u nigram perplexity(v )). for example when the vocabulary size is one million
words, this results in about two times speedup in evaluation. while this is not crucial speedup for
neural network lms as the computational bottleneck is in the n   d  h term, we will later propose
architectures that do not have hidden layers and thus depend heavily on the ef   ciency of the softmax
id172.

2.2 recurrent neural net language model (id56lm)

recurrent neural network based language model has been proposed to overcome certain limitations
of the feedforward nnlm, such as the need to specify the context length (the order of the model n),
and because theoretically id56s can ef   ciently represent more complex patterns than the shallow
neural networks [15, 2]. the id56 model does not have a projection layer; only input, hidden and
output layer. what is special for this type of model is the recurrent matrix that connects hidden
layer to itself, using time-delayed connections. this allows the recurrent model to form some kind
of short term memory, as information from the past can be represented by the hidden layer state that
gets updated based on the current input and the state of the hidden layer in the previous time step.
the complexity per training example of the id56 model is

q = h    h + h    v,

(3)

where the word representations d have the same dimensionality as the hidden layer h. again, the
term h    v can be ef   ciently reduced to h    log2(v ) by using hierarchical softmax. most of the
complexity then comes from h    h.

3

2.3 parallel training of neural networks

to train models on huge data sets, we have implemented several models on top of a large-scale
distributed framework called distbelief [6], including the feedforward nnlm and the new models
proposed in this paper. the framework allows us to run multiple replicas of the same model in
parallel, and each replica synchronizes its gradient updates through a centralized server that keeps
all the parameters. for this parallel training, we use mini-batch asynchronous id119 with
an adaptive learning rate procedure called adagrad [7]. under this framework, it is common to use
one hundred or more model replicas, each using many cpu cores at different machines in a data
center.

3 new id148

in this section, we propose two new model architectures for learning distributed representations
of words that try to minimize computational complexity. the main observation from the previous
section was that most of the complexity is caused by the non-linear hidden layer in the model. while
this is what makes neural networks so attractive, we decided to explore simpler models that might
not be able to represent the data as precisely as neural networks, but can possibly be trained on much
more data ef   ciently.
the new architectures directly follow those proposed in our earlier work [13, 14], where it was
found that neural network language model can be successfully trained in two steps:    rst, continuous
word vectors are learned using simple model, and then the id165 nnlm is trained on top of these
distributed representations of words. while there has been later substantial amount of work that
focuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one.
note that related models have been proposed also much earlier [26, 8].

3.1 continuous bag-of-words model

the    rst proposed architecture is similar to the feedforward nnlm, where the non-linear hidden
layer is removed and the projection layer is shared for all words (not just the projection matrix);
thus, all words get projected into the same position (their vectors are averaged). we call this archi-
tecture a bag-of-words model as the order of words in the history does not in   uence the projection.
furthermore, we also use words from the future; we have obtained the best performance on the task
introduced in the next section by building a log-linear classi   er with four future and four history
words at the input, where the training criterion is to correctly classify the current (middle) word.
training complexity is then

(4)
we denote this model further as cbow, as unlike standard bag-of-words model, it uses continuous
distributed representation of the context. the model architecture is shown at figure 1. note that the
weight matrix between the input and the projection layer is shared for all word positions in the same
way as in the nnlm.

q = n    d + d    log2(v ).

3.2 continuous skip-gram model

the second architecture is similar to cbow, but instead of predicting the current word based on the
context, it tries to maximize classi   cation of a word based on another word in the same sentence.
more precisely, we use each current word as an input to a log-linear classi   er with continuous
projection layer, and predict words within a certain range before and after the current word. we
found that increasing the range improves quality of the resulting word vectors, but it also increases
the computational complexity. since the more distant words are usually less related to the current
word than those close to it, we give less weight to the distant words by sampling less from those
words in our training examples.
the training complexity of this architecture is proportional to

(5)
where c is the maximum distance of the words. thus, if we choose c = 5, for each training word
we will select randomly a number r in range < 1; c >, and then use r words from history and

q = c    (d + d    log2(v )),

4

figure 1: new model architectures. the cbow architecture predicts the current word based on the
context, and the skip-gram predicts surrounding words given the current word.

r words from the future of the current word as correct labels. this will require us to do r    2
word classi   cations, with the current word as input, and each of the r + r words as output. in the
following experiments, we use c = 10.

4 results

to compare the quality of different versions of word vectors, previous papers typically use a table
showing example words and their most similar words, and understand them intuitively. although
it is easy to show that word france is similar to italy and perhaps some other countries, it is much
more challenging when subjecting those vectors in a more complex similarity task, as follows. we
follow previous observation that there can be many different types of similarities between words, for
example, word big is similar to bigger in the same sense that small is similar to smaller. example
of another type of relationship can be word pairs big - biggest and small - smallest [20]. we further
denote two pairs of words with the same relationship as a question, as we can ask:    what is the
word that is similar to small in the same sense as biggest is similar to big?   
somewhat surprisingly, these questions can be answered by performing simple algebraic operations
with the vector representation of words. to    nd a word that is similar to small in the same sense as
biggest is similar to big, we can simply compute vector x = vector(   biggest   )    vector(   big   ) +
vector(   small   ). then, we search in the vector space for the word closest to x measured by cosine
distance, and use it as the answer to the question (we discard the input question words during this
search). when the word vectors are well trained, it is possible to    nd the correct answer (word
smallest) using this method.
finally, we found that when we train high dimensional word vectors on a large amount of data, the
resulting vectors can be used to answer very subtle semantic relationships between words, such as
a city and the country it belongs to, e.g. france is to paris as germany is to berlin. word vectors
with such semantic relationships could be used to improve many existing nlp applications, such
as machine translation, information retrieval and id53 systems, and may enable other
future applications yet to be invented.

5

w(t-2)w(t+1)w(t-1)w(t+2)w(t)sum       input         projection         outputw(t)          input         projection      outputw(t-2)w(t-1)w(t+1)w(t+2)                   cbow                                                   skip-gramtable 1: examples of    ve types of semantic and nine types of syntactic questions in the semantic-
syntactic word relationship test set.

type of relationship
common capital city
all capital cities
currency
city-in-state
man-woman
adjective to adverb
opposite
comparative
superlative
present participle
nationality adjective
past tense
plural nouns
plural verbs

word pair 1

word pair 2

athens
astana
angola
chicago
brother
apparent
possibly

great
easy
think

switzerland

walking
mouse
work

greece

kazakhstan

kwanza
illinois
sister

apparently
impossibly

greater
easiest
thinking
swiss
walked
mice
works

oslo
harare
iran

norway
zimbabwe

rial

stockton
grandson

california

granddaughter

rapid
ethical
tough
lucky
read

cambodia
swimming

dollar
speak

rapidly
unethical
tougher
luckiest
reading

cambodian

swam
dollars
speaks

4.1 task description

to measure quality of the word vectors, we de   ne a comprehensive test set that contains    ve types
of semantic questions, and nine types of syntactic questions. two examples from each category are
shown in table 1. overall, there are 8869 semantic and 10675 syntactic questions. the questions
in each category were created in two steps:    rst, a list of similar word pairs was created manually.
then, a large list of questions is formed by connecting two word pairs. for example, we made a
list of 68 large american cities and the states they belong to, and formed about 2.5k questions by
picking two word pairs at random. we have included in our test set only single token words, thus
multi-word entities are not present (such as new york).
we evaluate the overall accuracy for all question types, and for each question type separately (se-
mantic, syntactic). question is assumed to be correctly answered only if the closest word to the
vector computed using the above method is exactly the same as the correct word in the question;
synonyms are thus counted as mistakes. this also means that reaching 100% accuracy is likely
to be impossible, as the current models do not have any input information about word morphology.
however, we believe that usefulness of the word vectors for certain applications should be positively
correlated with this accuracy metric. further progress can be achieved by incorporating information
about structure of words, especially for the syntactic questions.

4.2 maximization of accuracy

we have used a google news corpus for training the word vectors. this corpus contains about
6b tokens. we have restricted the vocabulary size to 1 million most frequent words. clearly, we
are facing time constrained optimization problem, as it can be expected that both using more data
and higher dimensional word vectors will improve the accuracy. to estimate the best choice of
model architecture for obtaining as good as possible results quickly, we have    rst evaluated models
trained on subsets of the training data, with vocabulary restricted to the most frequent 30k words.
the results using the cbow architecture with different choice of word vector dimensionality and
increasing amount of the training data are shown in table 2.
it can be seen that after some point, adding more dimensions or adding more training data provides
diminishing improvements. so, we have to increase both vector dimensionality and the amount
of the training data together. while this observation might seem trivial, it must be noted that it is
currently popular to train word vectors on relatively large amounts of data, but with insuf   cient size

6

table 2: accuracy on subset of the semantic-syntactic word relationship test set, using word
vectors from the cbow architecture with limited vocabulary. only questions containing words from
the most frequent 30k words are used.

dimensionality / training words

50
100
300
600

24m 49m 98m 196m 391m 783m
23.2
13.4
32.2
19.4
23.2
45.9
50.4
24.0

15.7
23.1
29.2
30.1

18.6
27.8
35.3
36.5

19.1
28.7
38.6
40.8

22.5
33.4
43.7
46.6

table 3: comparison of architectures using models trained on the same data, with 640-dimensional
word vectors. the accuracies are reported on our semantic-syntactic word relationship test set,
and on the syntactic relationship test set of [20]

model

semantic-syntactic word relationship test set

msr word relatedness

architecture

semantic accuracy [%]

syntactic accuracy [%]

test set [20]

id56lm
nnlm
cbow

skip-gram

9
23
24
55

36
53
64
59

35
47
61
56

(such as 50 - 100). given equation 4, increasing amount of training data twice results in about the
same increase of computational complexity as increasing vector size twice.
for the experiments reported in tables 2 and 4, we used three training epochs with stochastic gradi-
ent descent and id26. we chose starting learning rate 0.025 and decreased it linearly, so
that it approaches zero at the end of the last training epoch.

4.3 comparison of model architectures

first we compare different model architectures for deriving the word vectors using the same training
data and using the same dimensionality of 640 of the word vectors. in the further experiments, we
use full set of questions in the new semantic-syntactic word relationship test set, i.e. unrestricted to
the 30k vocabulary. we also include results on a test set introduced in [20] that focuses on syntactic
similarity between words3.
the training data consists of several ldc corpora and is described in detail in [18] (320m words,
82k vocabulary). we used these data to provide a comparison to a previously trained recurrent
neural network language model that took about 8 weeks to train on a single cpu. we trained a feed-
forward nnlm with the same number of 640 hidden units using the distbelief parallel training [6],
using a history of 8 previous words (thus, the nnlm has more parameters than the id56lm, as the
projection layer has size 640    8).
in table 3, it can be seen that the word vectors from the id56 (as used in [20]) perform well mostly
on the syntactic questions. the nnlm vectors perform signi   cantly better than the id56 - this is
not surprising, as the word vectors in the id56lm are directly connected to a non-linear hidden
layer. the cbow architecture works better than the nnlm on the syntactic tasks, and about the
same on the semantic one. finally, the skip-gram architecture works slightly worse on the syntactic
task than the cbow model (but still better than the nnlm), and much better on the semantic part
of the test than all the other models.
next, we evaluated our models trained using one cpu only and compared the results against publicly
available word vectors. the comparison is given in table 4. the cbow model was trained on subset

3we thank geoff zweig for providing us the test set.

7

table 4: comparison of publicly available word vectors on the semantic-syntactic word relation-
ship test set, and word vectors from our models. full vocabularies are used.

model

vector

dimensionality

training
words

accuracy [%]

collobert-weston nnlm
turian nnlm
turian nnlm
mnih nnlm
mnih nnlm
mikolov id56lm
mikolov id56lm
huang nnlm
our nnlm
our nnlm
our nnlm
cbow
skip-gram

50
50
200
50
100
80
640
50
20
50
100
300
300

semantic

9.3
1.4
1.4
1.8
3.3
4.9
8.6
13.3
12.9
27.9
34.2
15.5
50.0

syntactic total
11.0
2.1
1.8
5.8
8.8
12.7
24.6
12.3
20.3
43.2
50.8
36.1
53.3

12.3
2.6
2.2
9.1
13.2
18.4
36.5
11.6
26.4
55.8
64.5
53.1
55.9

660m
37m
37m
37m
37m
320m
320m
990m

6b
6b
6b

783m
783m

table 5: comparison of models trained for three epochs on the same data and models trained for
one epoch. accuracy is reported on the full semantic-syntactic data set.

model

vector

dimensionality

training
words

accuracy [%]

training time

[days]

3 epoch cbow
3 epoch skip-gram
1 epoch cbow
1 epoch cbow
1 epoch cbow
1 epoch skip-gram
1 epoch skip-gram
1 epoch skip-gram

300
300
300
300
600
300
300
600

semantic

15.5
50.0
13.8
16.1
15.4
45.6
52.2
56.7

syntactic total
36.1
53.3
33.6
36.1
36.2
49.2
53.8
55.5

53.1
55.9
49.9
52.6
53.3
52.2
55.1
54.5

783m
783m
783m
1.6b
783m
783m
1.6b
783m

1
3
0.3
0.6
0.7
1
2
2.5

of the google news data in about a day, while training time for the skip-gram model was about three
days.
for experiments reported further, we used just one training epoch (again, we decrease the learning
rate linearly so that it approaches zero at the end of training). training a model on twice as much
data using one epoch gives comparable or better results than iterating over the same data for three
epochs, as is shown in table 5, and provides additional small speedup.

4.4 large scale parallel training of models

as mentioned earlier, we have implemented various models in a distributed framework called dis-
tbelief. below we report the results of several models trained on the google news 6b data set,
with mini-batch asynchronous id119 and the adaptive learning rate procedure called ada-
grad [7]. we used 50 to 100 model replicas during the training. the number of cpu cores is an

8

table 6: comparison of models trained using the distbelief distributed framework. note that
training of nnlm with 1000-dimensional vectors would take too long to complete.

model

vector

dimensionality

training
words

accuracy [%]

training time

[days x cpu cores]

nnlm
cbow
skip-gram

100
1000
1000

semantic

34.2
57.3
66.1

syntactic total
50.8
63.7
65.6

64.5
68.9
65.1

6b
6b
6b

14 x 180
2 x 140
2.5 x 125

table 7: comparison and combination of models on the microsoft sentence completion challenge.

architecture
4-gram [32]
average lsa similarity [32]
log-bilinear model [24]
id56lms [19]
skip-gram
skip-gram + id56lms

accuracy [%]

39
49
54.8
55.4
48.0
58.9

estimate since the data center machines are shared with other production tasks, and the usage can
   uctuate quite a bit. note that due to the overhead of the distributed framework, the cpu usage of
the cbow model and the skip-gram model are much closer to each other than their single-machine
implementations. the result are reported in table 6.

4.5 microsoft research sentence completion challenge

the microsoft sentence completion challenge has been recently introduced as a task for advancing
id38 and other nlp techniques [32]. this task consists of 1040 sentences, where one
word is missing in each sentence and the goal is to select word that is the most coherent with the
rest of the sentence, given a list of    ve reasonable choices. performance of several techniques has
been already reported on this set, including id165 models, lsa-based model [32], log-bilinear
model [24] and a combination of recurrent neural networks that currently holds the state of the art
performance of 55.4% accuracy on this benchmark [19].
we have explored the performance of skip-gram architecture on this task. first, we train the 640-
dimensional model on 50m words provided in [32]. then, we compute score of each sentence in
the test set by using the unknown word at the input, and predict all surrounding words in a sentence.
the    nal sentence score is then the sum of these individual predictions. using the sentence scores,
we choose the most likely sentence.
a short summary of some previous results together with the new results is presented in table 7.
while the skip-gram model itself does not perform on this task better than lsa similarity, the scores
from this model are complementary to scores obtained with id56lms, and a weighted combination
leads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and
58.7% on the test part of the set).

5 examples of the learned relationships

table 8 shows words that follow various relationships. we follow the approach described above: the
relationship is de   ned by subtracting two word vectors, and the result is added to another word. thus
for example, paris - france + italy = rome. as it can be seen, accuracy is quite good, although
there is clearly a lot of room for further improvements (note that using our accuracy metric that

9

table 8: examples of the word pair relationships, using the best word vectors from table 4 (skip-
gram model trained on 783m words with 300 dimensionality).

relationship
france - paris
big - bigger

miami - florida
einstein - scientist
sarkozy - france

copper - cu

berlusconi - silvio
microsoft - windows
microsoft - ballmer

example 1
italy: rome
small: larger

baltimore: maryland

example 2
japan: tokyo
cold: colder
dallas: texas

messi: mid   elder
berlusconi: italy

mozart: violinist
merkel: germany

example 3

florida: tallahassee

quick: quicker
kona: hawaii
picasso: painter
koizumi: japan

zinc: zn

sarkozy: nicolas
google: android
google: yahoo

gold: au

uranium: plutonium

putin: medvedev

ibm: linux

ibm: mcnealy
france: tapas

obama: barack
apple: iphone
apple: jobs
usa: pizza

japan - sushi

germany: bratwurst

assumes exact match, the results in table 8 would score only about 60%). we believe that word
vectors trained on even larger data sets with larger dimensionality will perform signi   cantly better,
and will enable the development of new innovative applications. another way to improve accuracy is
to provide more than one example of the relationship. by using ten examples instead of one to form
the relationship vector (we average the individual vectors together), we have observed improvement
of accuracy of our best models by about 10% absolutely on the semantic-syntactic test.
it is also possible to apply the vector operations to solve different tasks. for example, we have
observed good accuracy for selecting out-of-the-list words, by computing average vector for a list of
words, and    nding the most distant word vector. this is a popular type of problems in certain human
intelligence tests. clearly, there is still a lot of discoveries to be made using these techniques.

6 conclusion

in this paper we studied the quality of vector representations of words derived by various models on
a collection of syntactic and semantic language tasks. we observed that it is possible to train high
quality word vectors using very simple model architectures, compared to the popular neural network
models (both feedforward and recurrent). because of the much lower computational complexity, it
is possible to compute very accurate high dimensional word vectors from a much larger data set.
using the distbelief distributed framework, it should be possible to train the cbow and skip-gram
models even on corpora with one trillion words, for basically unlimited size of the vocabulary. that
is several orders of magnitude larger than the best previously published results for similar models.
an interesting task where the word vectors have recently been shown to signi   cantly outperform the
previous state of the art is the semeval-2012 task 2 [11]. the publicly available id56 vectors were
used together with other techniques to achieve over 50% increase in spearman   s rank correlation
over the previous best result [31]. the neural network based word vectors were previously applied
to many other nlp tasks, for example id31 [12] and paraphrase detection [28]. it can
be expected that these applications can bene   t from the model architectures described in this paper.
our ongoing work shows that the word vectors can be successfully applied to automatic extension
of facts in knowledge bases, and also for veri   cation of correctness of existing facts. results
from machine translation experiments also look very promising.
in the future, it would be also
interesting to compare our techniques to latent relational analysis [30] and others. we believe that
our comprehensive test set will help the research community to improve the existing techniques for
estimating the word vectors. we also expect that high quality word vectors will become an important
building block for future nlp applications.

10

7 follow-up work

after the initial version of this paper was written, we published single-machine multi-threaded c++
code for computing the word vectors, using both the continuous bag-of-words and skip-gram archi-
tectures4. the training speed is signi   cantly higher than reported earlier in this paper, i.e. it is in the
order of billions of words per hour for typical hyperparameter choices. we also published more than
1.4 million vectors that represent named entities, trained on more than 100 billion words. some of
our follow-up work will be published in an upcoming nips 2013 paper [21].

references
[1] y. bengio, r. ducharme, p. vincent. a neural probabilistic language model. journal of ma-

chine learning research, 3:1137-1155, 2003.

[2] y. bengio, y. lecun. scaling learning algorithms towards ai. in: large-scale kernel ma-

chines, mit press, 2007.

[3] t. brants, a. c. popat, p. xu, f. j. och, and j. dean. large language models in machine
translation. in proceedings of the joint conference on empirical methods in natural language
processing and computational language learning, 2007.

[4] r. collobert and j. weston. a uni   ed architecture for natural language processing: deep
neural networks with multitask learning. in international conference on machine learning,
icml, 2008.

[5] r. collobert, j. weston, l. bottou, m. karlen, k. kavukcuoglu and p. kuksa. natural lan-
guage processing (almost) from scratch. journal of machine learning research, 12:2493-
2537, 2011.

[6] j. dean, g.s. corrado, r. monga, k. chen, m. devin, q.v. le, m.z. mao, m.a. ranzato, a.

senior, p. tucker, k. yang, a. y. ng., large scale distributed deep networks, nips, 2012.

[7] j.c. duchi, e. hazan, and y. singer. adaptive subgradient methods for online learning and

stochastic optimization. journal of machine learning research, 2011.

[8] j. elman. finding structure in time. cognitive science, 14, 179-211, 1990.
[9] eric h. huang, r. socher, c. d. manning and andrew y. ng. improving word representations
via global context and multiple word prototypes. in: proc. association for computational
linguistics, 2012.

[10] g.e. hinton, j.l. mcclelland, d.e. rumelhart. distributed representations. in: parallel dis-
tributed processing: explorations in the microstructure of cognition. volume 1: foundations,
mit press, 1986.

[11] d.a. jurgens, s.m. mohammad, p.d. turney, k.j. holyoak. semeval-2012 task 2: measuring
degrees of relational similarity. in: proceedings of the 6th international workshop on semantic
evaluation (semeval 2012), 2012.

[12] a.l. maas, r.e. daly, p.t. pham, d. huang, a.y. ng, and c. potts. learning word vectors for

id31. in proceedings of acl, 2011.

[13] t. mikolov. id38 for id103 in czech, masters thesis, brno uni-

versity of technology, 2007.

[14] t. mikolov, j. kopeck  y, l. burget, o. glembek and j.   cernock  y. neural network based lan-

guage models for higly in   ective languages, in: proc. icassp 2009.

[15] t. mikolov, m. kara     at, l. burget, j.   cernock  y, s. khudanpur. recurrent neural network

based language model, in: proceedings of interspeech, 2010.

[16] t. mikolov, s. kombrink, l. burget, j.   cernock  y, s. khudanpur. extensions of recurrent neural

network language model, in: proceedings of icassp 2011.

[17] t. mikolov, a. deoras, s. kombrink, l. burget, j.   cernock  y. empirical evaluation and com-
bination of advanced id38 techniques, in: proceedings of interspeech, 2011.

4the code is available at https://code.google.com/p/id97/

11

[18] t. mikolov, a. deoras, d. povey, l. burget, j.   cernock  y. strategies for training large scale
neural network language models, in: proc. automatic id103 and understand-
ing, 2011.

[19] t. mikolov. statistical language models based on neural networks. phd thesis, brno univer-

sity of technology, 2012.

[20] t. mikolov, w.t. yih, g. zweig. linguistic regularities in continuous space word represen-

tations. naacl hlt 2013.

[21] t. mikolov, i. sutskever, k. chen, g. corrado, and j. dean. distributed representations of

words and phrases and their compositionality. accepted to nips 2013.

[22] a. mnih, g. hinton. three new id114 for statistical language modelling. icml,

2007.

[23] a. mnih, g. hinton. a scalable hierarchical distributed language model. advances in neural

information processing systems 21, mit press, 2009.

[24] a. mnih, y.w. teh. a fast and simple algorithm for training neural probabilistic language

models. icml, 2012.

[25] f. morin, y. bengio. hierarchical probabilistic neural network language model. aistats,

2005.

[26] d. e. rumelhart, g. e. hinton, r. j. williams. learning internal representations by back-

propagating errors. nature, 323:533.536, 1986.

[27] h. schwenk. continuous space language models. computer speech and language, vol. 21,

2007.

[28] r. socher, e.h. huang, j. pennington, a.y. ng, and c.d. manning. dynamic pooling and

unfolding recursive autoencoders for paraphrase detection. in nips, 2011.

[29] j. turian, l. ratinov, y. bengio. word representations: a simple and general method for

semi-supervised learning. in: proc. association for computational linguistics, 2010.

[30] p. d. turney. measuring semantic similarity by latent relational analysis. in: proc. interna-

tional joint conference on arti   cial intelligence, 2005.

[31] a. zhila, w.t. yih, c. meek, g. zweig, t. mikolov. combining heterogeneous models for

measuring relational similarity. naacl hlt 2013.

[32] g. zweig, c.j.c. burges. the microsoft research sentence completion challenge, microsoft

research technical report msr-tr-2011-129, 2011.

12

