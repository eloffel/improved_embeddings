5
1
0
2

 

v
o
n
8
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
6
2
9
5
0

.

1
1
5
1
:
v
i
x
r
a

combining neural networks and id148

to improve id36

thien huu nguyen

computer science department

new york university

new york, ny 10003 usa

thien@cs.nyu.edu

ralph grishman

computer science department

new york university

new york, ny 10003 usa
grishman@cs.nyu.edu

abstract

the last decade has witnessed the success of
the traditional feature-based method on ex-
ploiting the discrete structures such as words
or lexical patterns to extract relations from
text. recently, convolutional and recurrent
neural networks has provided very effective
mechanisms to capture the hidden structures
within sentences via continuous representa-
tions, thereby signi   cantly advancing the per-
formance of id36.
the ad-
vantage of convolutional neural networks is
their capacity to generalize the consecutive k-
grams in the sentences while recurrent neural
networks are effective to encode long ranges
of sentence context. this paper proposes to
combine the traditional feature-based method,
the convolutional and recurrent neural net-
works to simultaneously bene   t from their ad-
vantages. our systematic evaluation of dif-
ferent network architectures and combination
methods demonstrates the effectiveness of this
approach and results in the state-of-the-art
performance on the ace 2005 and semeval
dataset.

1 introduction

we studies the id36 (re) problem, one
of the important problem of information extraction
and natural language processing (nlp). given two
entity mentions in a sentence (relation mentions), we
need to identify the semantic relationship (if any) be-
tween the two entity mentions. one example is the
recognition of the located relation between    he   
and    texas    in the sentence    he lives in texas   .

the

are

the

last

decade

the two methods dominating re research
feature-based
in
method (kambhatla, 2004; boschee et al., 2005;
grishman et al., 2005;
zhou et al., 2005;
jiang and zhai, 2007;
chan and roth, 2010;
sun et al., 2011) and the kernel-based method
(zelenko et al., 2003; culotta and sorensen, 2004;
bunescu and mooney, 2005a;
zhang et al., 2006;
bunescu and mooney, 2005b;
qian et al., 2008;
zhou et al., 2007;
nguyen et al., 2009;
plank and moschitti, 2013).
these research extensively studies the leverage
of linguistic analysis and knowledge resources to
construct the feature representations, involving the
combination of discrete properties such as lexicon,
syntax, gazetteers. although these approaches are
able to exploit the symbolic (discrete) structures
within relation mentions,
they also suffer from
the dif   culty to generalize over the unseen words
(gorid113y et al., 2015), motivating some very recent
work on employing the continuous representations
of words (id27s) to do re. the most
popular method involves neural networks (nns)
that effectively learn hidden structures of relation
mentions from such id27s, thus achiev-
ing the top performance for re (zeng et al., 2014;
dos santos et al., 2015; xu et al., 2015).

the nn research for id36 and
centered around two main
classi   cation has
neu-
network
(dos santos et al., 2015;
ral
zeng et al., 2015) and recursive/recurrent neural
networks (socher et al., 2012; xu et al., 2015). the
distinction between convolutional neural networks

architectures:
(id98s)

convolutional

networks

and recurrent neural networks (id56s) for re is
that
the former aim to generalize the local and
consecutive context (i.e, the k-grams) of the relation
mentions (nguyen and grishman, 2015a) while the
latter adaptively accumulate the context information
in the whole sentence via the memory units, thereby
encoding the global and possibly unconsecutive pat-
terns for re (hochreiter and schmidhuber, 1997;
cho et al., 2014).
the traditional
feature-based method (i.e, the log-linear or maxent
model with hand-crafted features), the id98s and
the id56s tend to focus on different angles for re.
guided from this intuition, in this work, we propose
to combine the three models to further improve the
performance of re.

consequently,

while the architecture design of id98s for re is
quite established due to the extensive studies in the
last couple of years, the application of id56s to re
is only very recent and the optimal designs of id56s
for re are still an ongoing research. in this work,
we    rst perform a systematic exploration of various
network architectures to seek the best id56 model
for re. in the next step, we extensively study differ-
ent methods to assemble the log-linear model, id98s
and id56s for re, leading to the combined mod-
els that yield the state-of-the-art performance on the
ace 2005 and semeval dataset. to the best of our
knowledge, this is the    rst work to systematically
examine the id56 architectures as well as combine
them with id98s and the traditional feature-based
approach for re.

2 models

relation mentions consist of sentences marked with
two entity mentions of interest. in this paper, we ex-
amine two different representations for the sentences
in re: (i) the standard representation, called seq
that takes all the words in the sentences into account
and (ii) the dependency representation, called dep
that only considers the words along the dependency
paths between the two entity mention heads of the
sentences. in the following, unless indicated speci   -
cally, all the statements about the sentences hold for
both representations seq and dep.

throughout this paper, for convenience, we as-
sume that the input sentences of the relation men-
tions have the same    xed length n. this can be

achieved by setting n to the length of the longest
input sentences and padding the shorter sentences
with a special token. let w = w1w2 . . . wn be
the input sentence of some relation mention, where
wi is the i-th word in the sentence. also, let wi1
and wi2 be the two heads of the two entity mentions
of interest. in order to prepare the relation mention
for neural networks, we    rst transform each word
wi into a real-valued vector xi using the concate-
nation of the following seven vectors, motivated by
the previous research on neural networks and feature
analysis for re (zhou et al., 2005; sun et al., 2011;
gorid113y et al., 2015).

- the real-valued id27 vector ei of wi,
obtained by looking up the id27 table e.
- the real-valued distance embedding vectors di1,
di2 to encode the relative distances i     i1 and i    
i2 of wi to the two entity heads of interest wi1 and
wi2: di1 = d[i     i1], di2 = d[i     i2] where d is
the distance embedding table (initialized randomly).
the objective is to inform the networks the positions
of the two entity mentions for relation prediction.

- the real-valued embedding vectors for entity
types ti and chunks qi to embed the entity type and
chunking information for wi. these vectors are gen-
erated by looking up the entity type and chunk em-
bedding tables (also initialized randomly) (i.e, t and
q respectively) for the entity type enti and chunking
label chunki of wi: ti = t [enti], qi = q[chunki].
- the binary vector pi with one dimension to indi-
cate whether the word wi is on the dependency path
between wi1 and wi2 or not.

- the binary vector gi whose dimensions corre-
spond to the possible relations between words in the
dependency trees. the value at a dimension of gi
is only set to 1 if there exists one edge of the corre-
sponding relation connected to wi in the dependency
tree.

the transformation from the word wi to the vec-
tor xi = [ei, di1 , di2 , ti, qi, pi, gi] essentially con-
verts the relation mention with the input sentence w
into a real-valued matrix x = [x1, x2, . . . , xn], to be
used by the neural networks presented below.

2.1 the separate models

we describe two typical nn architectures for re un-
derlying the combined models in this work.

2.1.1 the convolutional neural networks

in id98s (kalchbrenner et al., 2014; kim, 2014),
given a window size of k, we have a set of ck feature
maps (   lters). each feature map f is a weight matrix
f = [f1, f2, . . . , fk] where fi is a vector to be learnt
during training as the model parameters. the core of
id98s is the application of the convolutional opera-
tor on the input matrix x and the    lter matrix f to
n   k+1],
produce a score sequence sf = [sf
interpreted as a more abstract representation of the
input matrix x:

2, . . . , sf

1, sf

k   1

sf
i = g(

x

fj+1xj+i + b)

j=0

where b is a bias term and g is the tanh function.
in the next step, we further abstract the scores
in sf by aggregating it via the max function to ob-
max. we then repeat
tain the max-pooling score sf
this process for all the ck feature maps with differ-
ent window sizes k to generate a vector of the max-
pooling scores. in the    nal step, we pass this vector
into some standard multilayer neural network, fol-
lowed by a softmax layer to produce the probabilis-
tic distribution pc(y|x) over the possible relation
classes y in the prediction task.

2.1.2 the recurrent neural networks

in id56s, we consider the input matrix x =
[x1, x2, . . . , xn] as a sequence of column vectors in-
dexed from 1 to n. at each step i, we compute the
hidden vector hi from the current input vector xi and
the previous hidden vector hi   1 using the non-linear
transformation function   : hi =   (xi, hi   1).

this recurrent computation can be done via
(i) the
three different directional mechanisms:
forward mechanism that
recurs from 1 to n
and generate the forward hidden vector sequence:
r(x1, x2, . . . , xn) = h1, h2, . . . , hn, (ii) the back-
ward mechanism that runs id56s from n to 1 and
results in the backward hidden vector sequence
1, and (iii)
r(xn, xn   1, . . . , x1) = h   
the bidirectional mechanism that performs id56s in
both directions to produce the forward and backward
hidden vector sequences, and then concatenate them
at each position to generate the new hidden vector
sequence hb

n   1, . . . , h   

n, h   

i].
i = [hi, h   

2, . . . , hb

n: hb

1, hb

1

1the initial hidden vectors are set to the zero vector.

given the hidden vector sequence h1, h2, . . . , hn
obtained from one of the three mechanisms above,
we study two following strategies to generate the
representation vector vr for the initial relation men-
tion. note that this representation vector can be
again fed into some standard multilayer neural net-
work with a softmax layer in the end, resulting in the
distribution pr(y|x) for the id56 models:

- the head strategy:

in this strategy, vr is
the concatenation of the hidden vectors at the po-
sitions of the two entity mention heads of interest:
vr = [hi1 , hi2]. this is motivated by the importance
of the two mention heads in re (sun et al., 2011;
nguyen and grishman, 2014).

- the max strategy: this strategy is similar to our
max-pooling mechanism in id98s. in particular, vr
is obtained by taking the maximum along each di-
mension of the hidden vectors h1, h2, . . . , hn. the
idea is to further abstract the hidden vectors by re-
taining only the most important feature in each di-
mension.

function,

regarding the non-linear

the sim-
plest form of    in the literature considers it as
a one-layer feed-forward neural network, called
f f : hi = f f (xi, hi   1) =   (u xi + v hi   1)
where    is the sigmoid function.
unfortu-
nately,
the application of f f causes the so-
called    vanishing/exploding gradient    problems
(bengio et al., 1994), making it challenging to train
id56s properly (pascanu et al., 2012). these prob-
lems are overcome by the long-short term memory
units (lstm) (hochreiter and schmidhuber, 1997;
graves et al., 2009). in this work, we apply a vari-
ant of the memory units: the id149
from cho et al. (2014), called gru. gru is shown
to be much simpler than lstm in terms of compu-
tation but still achieves the comparable performance
(cho et al., 2014).

2.2 the combined models

we    rst present three different methods to assemble
id98s and id56s: ensembling, stacking and voting,
to be investigated in this work. the combination of
the neural networks with the log-linear model would
be discussed in the next section.

2.2.1 ensembling

2.3 the hybrid models

in this method, we    rst run some id98 and id56
in section 2.1 over the input matrix x to gather the
corresponding distributions pc(y|x) and pr(y|x).
we then combine the id98 and id56 by multiplying
their distributions (element-wise): pensemble(y|x) =
z pc(y|x)pr(y|x) (z is a id172 constant).
2.2.2 stacking

1

the overall architecture of the stacking method
is to use one of the two network architectures (i.e,
id98s and id56s) to generalize the hidden vectors
of the other architecture. the expectation is that we
can learn more effective features for re via such a
deeper architecture by alternating between the local
and global representations provided by id98s and
id56s.

we examine two variants for this method. the
   rst variant, called id56-id98, applies the id98
model in section 2.1.1 on the hidden vector se-
quence generated by some id56 in section 2.1.2 to
perform re. the second variant, called id98-id56,
on the other hand, utilize the id98 model to ac-
quire the hidden vector sequence, that is, in turn,
fed as the input into some id56 for re. for the
second variant, as the length of the hidden vector
n   k+1] in the id98 model de-
sf = [sf
pends on the speci   ed window size k for the fea-
ture map f, we need to pad the input matrix x
with     k
2     zero column vectors on both sides to en-
sure the same    xed length n for all the hidden vec-
n]. besides, we need to re-
tors: sf = [sf
arrange the scores in the hidden vectors from dif-
ferent feature maps of the id98 so they are grouped
according to the positions in the sentence, thus being
compatible with the input requirement of id56s.

2, . . . , sf

2, . . . , sf

1, sf

1, sf

2.2.3 voting

instead of integrating id98s and id56s at the
model level as the two previous methods, the vot-
ing method makes decision for a relation mention
x by voting the individual decisions of the differ-
ent models. while there are several voting schemes
in the literature, for this work, we employ the sim-
plest scheme of majority voting. if there are more
than one relation classes receiving the highest num-
ber of votes, the relation class returned by a model
and having the highest id203 would be chosen.

in order to further improve the re performance
of models above, we investigate the integration of
these neural network models with the traditional log-
linear model that relies on various linguistic features
from the past research on re (zhou et al., 2005;
sun et al., 2011; gorid113y et al., 2015). speci   cally,
in such integration models (called the hybrid mod-
els), the relation class distribution is obtained from
the element-wise multiplication between the dis-
tributions of the neural network models and the
log-linear model.
let us take the ensembling
model in section 2.2.1 as an example. the cor-
responding hybrid model in this case would be:
z pc(y|x)pr(y|x)plogin(y|x), as-
phybrid(y|x) = 1
suming plogin(y|x) be the distribution of the log-
linear model and z be the id172 constant.
the parameters of the log-linear model are learnt
jointly with the parameters of the neural networks.

hypothesis: let s be the set of relation mentions
correctly predicted by some neural network model in
some dataset (the coverage set). the introduction of
the log-linear model into this neural network model
essentially changes the coverage set of the network,
resulting in the new coverage set s     that might or
might not subsume the original set s. in this work,
we hypothesize that although s and s     overlap, there
are still some relation mentions that only belong to
either set. consequently, we propose to implement
a majority voting system (called the hybrid-voting
system) on the outputs of the network and its corre-
sponding hybrid model to enhance both models.

note that the voting models in section 2.2.3 in-
volve the voting on two models (i.e, id98 and
id56). in order to integrate the log-linear model into
such voting models, we    rst augment the separate
id98 and id56 models with the log-linear model
before we perform the voting procedure on the re-
sulting models. finally, the corresponding hybrid-
voting systems would involve the voting on four
models (id98, hybrid id98, id56 and hybrid id56).

2.4 training

we train the models by minimizing the negative
log-likelihood function using the stochastic gradient
descent algorithm with shuf   ed mini-batches and
the adadelta update rule (zeiler, 2012; kim, 2014).

the gradients are computed via back-propagation
while id173 is executed by a dropout on
the hidden vectors before the the multilayer neu-
ral networks (hinton et al., 2012). during training,
besides the weight matrices, we also optimized the
embedding tables e, d, t, q to achieve the opti-
mal state. finally, we rescale the weights whose
l2-norms exceed a hyperparameter
(kim, 2014;
nguyen and grishman, 2015a).

3 experiments

3.1 resources and parameters

for all the experiments below, we utilize the pre-
trained id27s id97 with 300 di-
mensions from mikolov et al.
(2013) to initialize
the id27 table e. the parameters for
id98s and traning the networks are inherited from
the previous studies, i.e, the window size set for fea-
ture maps = {2, 3, 4, 5}, 150 feature maps for each
window size, 50 dimensions for all the embedding
tables (except the id27 table e), the
dropout rate = 0.5, the mini-batch size = 50, the
hyperparameter for the l2 norms = 3 (kim, 2014;
nguyen and grishman, 2015a). regarding id56s,
we employ 300 units in the hidden layers.

3.2 dataset

on

two

evaluate

our models

we
datasets:
the ace 2005 dataset
relation extrac-
tion and the semeval-2010 task 8 dataset
(hendrickx et al., 2010)
classi   ca-
tion.

relation

for

for

the ace 2005 corpus comes with 6 different
domains: broadcast conversation (bc), broadcast
news (bn), telephone conversation (cts), newswire
(nw), usenet (un) and webblogs (wl). follow-
ing the common practice of id20 re-
search on this dataset (plank and moschitti, 2013;
nguyen and grishman, 2014; nguyen et al., 2015c;
gorid113y et al., 2015), we use news (the union of
bn and nw) as the training data, a half of bc as the
development set and the remainder (cts, wl and the
other half of bc) as the test data. note that we are
using the data prepared by gorid113y et. al (2015),
thus utilizing the same data split on bc as well as
the same data processing and nlp toolkits. the to-

tal number of relations in the training set is 43,4972.
we employ the bio annotation scheme to capture
the chunking information for words in the sentences
and only mark the entity types of the two entity men-
tion heads (obtained from human annotation) for this
dataset.

the semeval dataset concerns the relation classi-
   cation task that aims to determine the relation type
(or no relation) between two entities in sentences.
in order to make it compatible with the previous
research (socher et al., 2012; gorid113y et al., 2015),
for this dataset, besides the id27s and
the distance embeddings, we apply the name tag-
ging, id52 and id138 features
(inherited from socher et al.
(2012) and encoded
by the real-valued vectors for each word). the
other settings are also adopted from the past studies
(socher et al., 2012; xu et al., 2015).

3.3 id56 architectures

this section evaluates the performance of various
id56 architectures for re on the development set.
in particular, we compare different design combi-
nations of the four following factors: (i) sentence
representations (i.e, seq or dep), (ii) transforma-
tion functions    (i.e, ff or gru), (iii) the strate-
gies to employ the hidden vector sequence for re
(i.e, head or max), and (iv) the directions to run
id56s (i.e, forward (   ), backward (   ) or bidirec-
tional (      )). table 1 presents the results.

ff

gru

systems

dep
       60.78
head     55.55
    57.69
       50.00
max     52.08
    45.07
       63.32
head     63.69
    61.57
       60.96
max     61.97
    61.56

seq
63.22
60.05
58.54
51.22
53.96
33.50
63.23
62.77
62.55
64.24
64.59
64.30

table 1: performance (f1 scores) of id56s on the dev set

the main conclusions include:

2it was an error in gorid113y et al. (2015) that reported 43,518
total relations in the training set. the authors acknowledged this
error.

(i) assuming the same choices for the other three
corresponding factors, gru is more effective than
ff, seq is better than dep most of the time and
head outperforms max (except the case where
seq and gru are applied) for re with id56s.

(ii) regarding the direction mechanisms, the bidi-
rectional mechanism achieves the best performance
for the head strategy while the forward direction is
the best mechanism for the max strategy. this can
be partly explained by the lack of past or future con-
text information in the head strategy when we fol-
low the backward or forward direction respectively.
the best performance corresponds to the applica-
tion of the seq representation, the gru function
and the max strategy that would be used in all the
id56 models below. we call such id56 models with
the forward, backward and bidirectional mechanism
forward, backward and bidirect respec-
tively. we also apply the seq representation for the
id98 model (called id98) in the following experi-
ments for consistency.

3.4 evaluating the combined models

model

p

69.16
69.33
65.60
68.35

bidirect
forward
backward
id98
ensembling
71.22
id98-bidirect
id98-forward
66.19
id98-backward 65.09
stacking
66.55
id98-bidirect
id98-forward
69.46
id98-backward 72.58
65.63
bidirect-id98
forward-id98
73.13
backward-id98 67.60
voting
71.08
id98-bidirect
id98-forward
70.38
id98-backward 69.78

r

59.97
60.45
63.05
59.16

54.13
59.64
60.13

59.97
63.05
58.35
61.59
58.67
58.51

60.94
59.32
61.75

f1

64.24
64.59
64.30
63.42

61.51
62.75
62.51

63.09
66.10
64.69
63.55
65.11
62.73

65.62
64.38
65.52

table 2: performance of the combination methods

we evaluate the combination methods for id98s
and id56s presented in section 2.2. in particular,
for each method, we examine three models that are
combined from one of the three id56 models for-
ward, backward, bidirect and the id98
model. for instance, in the stacking method, the
three combined models corresponding to the id56-

id98 variant are forward-id98, backward-
id98, bidirect-id98 while the three combined
models corresponding to the id98-id56 variant
are id98-forward, id98-backward, id98-
bidirect. the notations for the other methods are
self-explained. the model performance on the de-
velopment set is given in table 3.4 that also includes
the performance of the separate models (i.e, id98,
forward, backward, bidirect) for conve-
nient comparison.

the    rst observation is that

the ensembling
method is not an effective way to combine id98s
and id56s as its performance is worse than the
separate models. second, regarding the stacking
method, the best way to combine id98s and id56s
in this framework is to assemble the id98 model
and the forward model.
in fact, the combina-
tion of the id98 and forward models helps to
improve the performance of the separate models in
both variants of this method (referring to the models
id98-forward and forward-id98). finally,
the voting method is also helpful as it outperforms
the separate models with the id98-bidirect and
id98-backward combinations.

for the following experiments, we would only fo-
cus on the three best combined models in this sec-
tion, i.e, the id98-forward model in the stacking
method (called stack-forward) and the id98-
bidirect, id98-backward models in the vot-
ing methods (called vote-bidirect and vote-
backward respectively).

3.5 evaluating the hybrid models
this section investigates the hybrid and hybrid-
voting models (section 2.3) to see if they can fur-
ther improve the performance of the neural network
models. in particular, we evaluate the separate mod-
els: id98, bidirect, forward, backward,
and the combined models: stack-forward,
vote-bidirect and vote-backward when
they are augmented with the traditional log-linear
model (the hybrid models). besides, in order to ver-
ify the hypothesis in section 2.3, we also test the cor-
responding hybrid-voting models. the experimental
results are shown in table 3. there are three main
conclusions:

(i) for all the models in columns    neural net-
works   ,    hybrid models    and    hybrid-voting mod-

model

neural networks
p
f1

r

hybrid models

hybrid-voting models

68.35
69.16
69.33
65.60

id98
bidirect
forward
backward
combined models
vote-bidirect
71.08
stack-forward 69.46
vote-backward 69.78

59.16
59.97
60.45
63.05

63.42
64.24
64.59
64.30

p

66.44
68.04
66.11
66.03

r

64.51
59.00
63.86
62.07

f1

65.46
63.19
64.96
63.99

60.94
63.05
61.75

65.64
66.99
65.10
table 3: performance of the hybrid models

65.62
66.10
65.52

69.24
65.93
67.30

62.40
68.07
63.05

p

69.07
71.13
72.69
71.56

71.30
69.32
70.79

r

63.70
60.29
61.26
63.21

62.40
66.29
64.02

f1

66.27
65.26
66.49
67.13

66.55
67.77
67.23

system

p

66.56
74.39

fcm
hybrid fcm
separate systems
log-linear
id98
bidirect
forward
backward
hybrid-voting systems
70.40
vote-bidirect
65.75
stack-forward
vote-backward 69.57

68.44
65.62
65.23
63.64
60.44

bc
r

57.86
55.35

50.07
61.06
61.06
59.39
61.2

63.84
66.48
63.28

f1
61.9
63.48

57.83
63.26
63.07
61.44
60.82

66.96   
66.11   
66.28   

p

65.62
74.53

73.62
65.92
66.15
60.12
58.20

66.74
63.58
65.91

cts
r

44.35
45.01

41.57
48.12
49.26
50.57
54.01

49.92
51.72
52.21

f1

52.93
56.12

53.14
55.63
56.47
54.93
56.03

57.12   
57.04   
58.26   

p

57.80
65.63

60.40
54.14
55.91
55.54
51.03

59.24
56.35
58.81

wl
r

44.62
47.59

47.31
53.68
51.56
54.67
52.55

54.96
57.22
55.81

f1

50.36
55.17

53.06
53.91
53.65
55.10
51.78

57.02   
56.78   
57.27   

ave
55.06
58.26

54.68
57.60
57.73
57.16
56.21

60.37
59.98
60.60

table 4: comparison to the state-of-the-art. the cells marked with    designates the models that are signi   cantly better than the
other neural network models (   < 0.05) on the corresponding domains.

els   , we see that the combined models outperform
their corresponding separate models (only except the
hybrid model of vote-backward), thereby fur-
ther con   rming the bene   ts of the combined models.
(ii) comparing columns    neural networks    and
   hybrid models   , we    nd that the traditional log-
linear model signi   cantly helps the id98 model.
the effects on the other models are not clear.

(iii) more interestingly, for all the neural networks
being examined (either separate or combined), the
corresponding hybrid-voting systems substantially
improve both the neural network models as well as
the corresponding hybrid models, testifying to the
hypothesis about the hybrid-voting approach in sec-
tion 2.3. note that the simpler voting systems on
three models: the log-linear model, the id98 model
and some id56 model (i.e, either bidirect, for-
ward or backward) produce the worse perfor-
mance than the hybrid-voting methods (the respec-
tive performance is 66.13%, 65.27%, and 65.96%).

(2015).

tional embedding model (fcm) and the hybrid fcm
model from gorid113y et al.
in this sec-
tion, we compare the proposed hybrid-voting sys-
tems with these state-of-the-art systems on the test
domains bc, cts, wl. table 4 reports the re-
sults. for completeness, we also include the perfor-
mance of the log-linear model and the separate mod-
els id98, bidirect, forward, backward,
serving as the other baselines for this work.

from the table, we see that although the sepa-
rate neural networks outperform the fcm model
across domains, they are still worse than the hybrid
fcm model due to the introduction of the log-linear
model into fcm. however, when the networks are
combined and integrated with the log-linear model,
they (the hybrid-voting systems) become signi   -
cantly better than the fcm models across all do-
mains (up to 2% improvement on the average ab-
solute f score), yielding the state-of-the-art perfor-
mance for the unseen domains in this dataset.

3.6 comparing to the state-of-the-art
the state-of-the-art system on the ace 2005 for the
unseen domains has been the feature-rich composi-

3.7 relation classi   cation experiments
we further evaluate the proposed systems for the re-
lation classi   cation task on the semeval dataset. ta-

ble 5 presents the performance of the seprate mod-
els, the proposed systems as well as the other repre-
sentative systems on this task. the most important
observation is that the hybrid-voting systems vote-
bidirect and vote-backward achieve the
state-of-the-art performance for this dataset, further
highlighting their bene   t for relation classi   cation.
the hybrid-voting stack-forward system per-
forms less effectively in this case, possibly due to
the small size of the semeval dataset that is not suf-
   cient to training such a deep model.

classi   er
id166 (hendrickx et al., 2010)
id56 (socher et al., 2012)
mvid56 (socher et al., 2012)
id98 (zeng et al., 2014)
cr-id98 (dos santos et al., 2015)
fcm (gorid113y et al., 2015)
hybrid fcm (gorid113y et al., 2015)
depnn (liu et al., 2015)
sdp-lstm (xu et al., 2015)
id98
bidirect
forward
backward
vote-bidirect
stack-forward
vote-backward

f
82.2
77.6
82.4
82.7
84.1   
83.0
83.4
83.6
83.7
83.5
81.8
81.9
82.4
84.1
83.4
84.1

table 5: performance of relation classi   cation systems. the
   refers to special treatment to the other class.

3.8 analysis
in order to better understand the reason helping the
combination of id98s and id56s outperform the
individual networks, we evaluate the performance
breakdown per relation for the id98 and bidirect
models. the results on the development set of the
ace 2005 dataset are provided in tabel 6.

relation class

p

phys
66.7
part-whole 68.6
64.2
art
70.2
org-aff
per-soc
71.1
65.9
gen-aff
all
68.4

id98

r
34.7
67.8
51.2
83.0
59.3
55.1
59.2

bidirect

f1
45.7
68.2
57.0
76.0
64.6
60.0
63.4

p

57.4
74.4
68.6
79.3
69.6
59.0
69.2

f

50.9
70.1
41.7
76.1
59.3
46.9
60.0

f1
54.0
72.2
51.9
77.7
64.0
52.3
64.2

table 6: the performance breakdown per relation for id98
and bidirect on the development set.

one of the main insights is although id98 and
bidirect have the comparable overall perfor-

mance, their recalls on individual relations are very
diverged.
in particular, the bidirect has much
better recall for the phys relation while the re-
calls of id98 are signi   cantly better for the art,
org-aff and gen-aff relations. a closer in-
vestigation reveals two facts: (i) the phys relation
mentions that are only correctly predicted by bidi-
rect involve the long distances between two en-
tity mentions, such as the phys relation between
   some    (a person entity) and    desert    (a location en-
tity) in the following sentence:    some of the 40,000
british troops are kicking up a lot of dust in the
iraqi desert making sure that nothing is left behind
them that could hurt them.   , and (ii) the art, org-
aff, gen-aff relation mentions only correctly
predicted by id98 contains the patterns between the
two entity mentions that are short but meaningful
enough to decide the relation classes, such as    the
iraqi unit in possession of those guns    (the art re-
lation between    unit    and    guns   ), or    the al qaeda
chief operations of   cer    (the org-aff relation be-
tween    al qaeda    and    of   cer   ). the failure of
id98 on the phys relation mentions with long dis-
tances originates from its mechanism to model short
and consecutive k-grams (up to length 5 in our case),
causing the dif   culty to capture the long and/or un-
consecutive patterns. bidirect, on the other hand,
fails to predict the short (but expressive enough) pat-
terns for art, org-aff, gen-aff because it in-
volves the hidden vectors that only model the con-
text words outside the short patterns, potentially in-
troducing unnecessary and noisy information into
the max-pooling scores for prediction. eventually,
the combination of id56s and id98s helps to com-
pensate the drawbacks of each model.

4 related work

of

for

the

from

invention

representations

the
starting
words
distributed
mnih and hinton, 2008;
(bengio et al., 2003;
turian et al., 2010;
collobert and weston, 2008;
id56s
mikolov et al., 2013),
vari-
have
labeling
ous nlp tasks,
(collobert et al., 2011), sentence modeling and clas-
si   cation (kalchbrenner et al., 2014; kim, 2014),
paraphrase identi   cation (yin and sch  utze, 2015),

including sequential

signi   cant

successes

gained

id98s

and

on

extraction

(nguyen and grishman, 2015b;
event
chen et al., 2015) for id98s and machine transla-
tion (cho et al., 2014; bahdanau et al., 2015) for
id56s, to name a few.

in particular, socher et al.

for id36/classi   cation, most work
on neural networks has focused on the relation clas-
si   cation task.
(2012)
and ebrahimi and dou (2015) study the recursive
nns that recur over the tree structures while xu et
al. (2015) and zhang and wang (2015) investigate
recurrent nns. regarding id98s, zeng et al. (2014)
examine id98s via the sequential representation of
sentences, dos santos et al. (2015) explore a rank-
ing id168 with data cleaning while zeng et al.
(2015) propose dynamic pooling and multi-instance
learning. for re, yu et al.
(2015) and gorid113y
et al. (2015) work on the feature-rich compositional
embedding models. finally, the only work that com-
bines nn architectures is due to liu et al. (2015) but
it only focuses on the stacking of the recursive nns
and id98s for relation classi   cation.

5 conclusion

we investigate different methods to combine id98s,
id56s as well as the hybrid models to integrate the
log-linear model into the nns. the experimental re-
sults demonstrate that the simple majority voting be-
tween id98s, id56s and their corresponding hybrid
models is the best combination method. we achieve
the state-of-the-art performance for both relation ex-
traction and relation classi   cation. in the future, we
plan to further evaluate the proposed methods on the
other tasks such as event extraction and slot    lling
in the kbp evaluation.

acknowledgment

we would like to thank matthew gorid113y and
mo yu for providing the dataset. thank you to
kyunghyun cho and yifan he for valuable sugges-
tions.

references

[bengio et al.1994] yoshua bengio, patrice simard, and
paolo frasconi. 1994. learning long-term dependen-
cies with id119 is dif   cult.
in journal of
machine learning research 3.

[bengio et al.2003] yoshua bengio, r  ejean ducharme,
pascal vincent, and christian jauvin. 2003. a neural
probabilistic language model. in journal of machine
learning research 3.

[boschee et al.2005] elizabeth

, and alex zamanian.

weischedel,
matic information extraction.
international conference on intelligence analysis.

boschee,

ralph
2005. auto-
in proceedings of the

[bunescu and mooney2005a] razvan bunescu and ray-
mond mooney. 2005a. a shortest path dependency
kernel for id36. in hlt-emnlp.

[bunescu and mooney2005b] razvan bunescu and ray-
mond j. mooney. 2005b. subsequence kernels for
id36. in nips.

[chan and roth2010] yee s. chan and dan roth. 2010.
exploiting background knowledge for relation extrac-
tion. in coling.

[chen et al.2015] yubo chen, liheng xu, kang liu, dao-
jian zeng, and jun zhao. 2015. event extraction via
dynamic multi-pooling convolutional neural networks.
in acl-ijcnlp.

[cho et al.2014] kyunghyun cho, bart van merrienboer,
caglar gulcehre, dzmitry bahdanau, fethi bougares,
holger schwenk, and yoshua bengio. 2014. learning
phrase representations using id56 encoder   decoder for
id151. in emnlp.

[collobert and weston2008] ronan collobert and jason
weston. 2008. a uni   ed architecture for natural lan-
guage processing: deep neural networks with multi-
task learning. in icml.

[collobert et al.2011] ronan collobert, jason weston,
lon bottou, michael karlen, koray kavukcuoglu, and
pavel p. kuksa. 2011. natural language processing
(almost) from scratch. in corr.

[culotta and sorensen2004] aron culotta and jeffrey
sorensen. 2004. dependency tree kernels for relation
extraction. in acl.

[dos santos et al.2015] cicero dos santos, bing xiang,
and bowen zhou. 2015. classifying relations by
ranking with convolutional neural networks. in acl-
ijcnlp.

[ebrahimi and dou2015] javid ebrahimi and dejing dou.
2015. chain based id56 for relation classi   cation. in
naacl.

[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2015. neural machine
translation by jointly learning to align and translate.
in iclr.

[gorid113y et al.2015] matthew r. gorid113y, mo yu, and
mark dredze. 2015.
improved id36
with feature-rich compositional embedding models. in
emnlp.

[graves et al.2009] a. graves, marcus eichenberger-
liwicki, s. fernandez, r. bertolami, h. bunke, and
j. schmidhuber. 2009. a novel connectionist system
for unconstrained handwriting recognition.
in ieee
transactions on pattern analysis and machine intelli-
gence.

[grishman et al.2005] ralph grishman, david west-
brook, and adam meyers. 2005. nyus english ace
2005 system description.
in ace 2005 evaluation
workshop.

[hendrickx et al.2010] iris hendrickx, su nam kim, zor-
nitsa kozareva, preslav nakov, diarmuid   o s  eaghdha,
sebastian pad  o, marco pennacchiotti, lorenza ro-
mano, and stan szpakowicz. 2010. semeval-2010
task 8: multi-way classi   cation of semantic relations
between pairs of nominals. in semeval.

[hinton et al.2012] geoffrey e. hinton, nitish srivastava,
alex krizhevsky, ilya sutskever, and ruslan salakhut-
dinov.
improving neural networks by pre-
venting co-adaptation of feature detectors. in corr,
abs/1207.0580.

2012.

[hochreiter and schmidhuber1997] sepp hochreiter and
jurgen schmidhuber. 1997. long short-term memory.
in neural computation.

[jiang and zhai2007] jing jiang and chengxiang zhai.
2007. a systematic exploration of the feature space
for id36. in naacl-hlt.

[kalchbrenner et al.2014] nal kalchbrenner, edward
grefenstette, and phil blunsom. 2014. a convolu-
tional neural network for modelling sentences.
in
acl.

[kambhatla2004] nanda kambhatla. 2004. combining
lexical, syntactic, and semantic features with maxi-
mum id178 models for information extraction.
in
acl.

[kim2014] yoon kim. 2014. convolutional neural net-

works for sentence classi   cation. in emnlp.

[liu et al.2015] yang liu, furu wei, sujian li, heng
ji, ming zhou, and houfeng wang.
2015. a
dependency-based neural network for relation classi-
   cation. in acl-ijcnlp.

[mikolov et al.2013] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013. ef   cient estimation
of word representations in vector space. in iclr.

[mnih and hinton2008] andriy mnih and geoffrey hin-
ton. 2008. a scalable hierarchical distributed lan-
guage model. in nips.

[nguyen and grishman2014] thien huu nguyen and
ralph grishman. 2014. employing word represen-
tations and id173 for id20 of re-
lation extraction. in acl.

[nguyen and grishman2015a] thien huu nguyen and
ralph grishman. 2015a. id36: per-
spective from convolutional neural networks. in the

naacl workshop on vector space modeling for nlp
(vsm).

[nguyen and grishman2015b] thien huu nguyen and
ralph grishman. 2015b. id37 and do-
main adaptation with convolutional neural networks.
in acl-ijcnlp.

[nguyen et al.2009] truc-vien t. nguyen, alessandro
moschitti, and giuseppe riccardi. 2009. convolu-
tion kernels on constituent, dependency and sequential
structures for id36. in emnlp.

[nguyen et al.2015c] thien huu nguyen, barbara plank,
and ralph grishman. 2015c. semantic representa-
tions for id20: a case study on the tree
kernel-based method for id36. in acl-
ijcnlp.

[pascanu et al.2012] razvan pascanu, tomas mikolov,
2012. on the dif   culty of
and yoshua bengio.
training recurrent neural networks. in arxiv preprint
arxiv:1211.5063.

[plank and moschitti2013] barbara plank and alessandro
moschitti. 2013. embedding semantic similarity in
tree kernels for id20 of relation extrac-
tion. in acl.

[qian et al.2008] longhua qian, guodong zhou, fang
kong, qiaoming zhu, and peide qian. 2008. ex-
ploiting constituent dependencies for tree kernel-based
semantic id36. in coling.

[socher et al.2012] richard

brody huval,
christopher d. manning, and andrew y. ng. 2012.
semantic compositionality through recursive matrix-
vector spaces. in emnlp.

socher,

[sun et al.2011] ang sun, ralph grishman, and satoshi
sekine. 2011. semi-supervised id36
with large-scale word id91. in acl.

[turian et al.2010] joseph turian, lev-arie ratinov, and
yoshua bengio. 2010. word representations: a sim-
ple and general method for semi-supervised learning.
in acl.

[xu et al.2015] yan xu, lili mou, ge li, yunchuan chen,
hao peng, and zhi jin. 2015. classifying relations
via long short term memory networks along shortest
dependency paths. in emnlp.

[yin and sch  utze2015] wenpeng yin

and hinrich
2015. convolutional neural network for

sch  utze.
paraphrase identi   cation. in naacl.

[yu et al.2015] mo yu, matthew r. gorid113y, and mark
dredze. 2015. combining id27s and fea-
ture embeddings for    ne-grained id36.
in naacl.

[zeiler2012] matthew d. zeiler.

2012.

an adaptive learning rate method.
abs/1212.5701.

adadelta:
in corr,

[zelenko et al.2003] dmitry zelenko, chinatsu aone, and
anthony richardella. 2003. exploring various knowl-
edge in id36.
in journal of machine
learning research.

[zeng et al.2014] daojian zeng, kang liu, siwei lai,
guangyou zhou, and jun zhao. 2014. relation clas-
si   cation via convolutional deep neural network.
in
coling.

[zeng et al.2015] daojian zeng, kang liu, yubo chen,
and jun zhao. 2015. distant supervision for rela-
tion extraction via piecewise convolutional neural net-
works. in emnlp.

[zhang and wang2015] dongxu zhang and dong wang.
2015. relation classi   cation via recurrent neural net-
work. in arxiv:1503.00185.

[zhang et al.2006] min zhang, jie zhang, jian su, and
guodong zhou. 2006. a composite kernel to extract
relations between entities with both    at and structured
features. in coling-acl.

[zhou et al.2005] guodong zhou, jian su, jie zhang, and
min zhang. 2005. exploring various knowledge in
id36. in acl.

[zhou et al.2007] guodong

zhang,
donghong ji, and qiaoming zhu.
tree
kernel-based id36 with context-sensitive
structured parse tree information. in emnlp-conll.

zhou,

2007.

min

