5
1
0
2

 
t
c
o
6

 

 
 
]

r

i
.
s
c
[
 
 

1
v
2
6
5
1
0

.

0
1
5
1
:
v
i
x
r
a

parameterized neural network language models for information

n. despres   

retrieval
s. lamprier   

nicolas.despres@gmail.com

sylvain.lamprier@lip6.fr

b. piwowarski   
benjamin@bpiwowar.net

abstract

information retrieval (ir) models need to deal with two di   cult issues, vocabulary mismatch and
term dependencies. vocabulary mismatch corresponds to the di   culty of retrieving relevant documents
that do not contain exact query terms but semantically related terms. term dependencies refers to the
need of considering the relationship between the words of the query when estimating the relevance of
a document. a multitude of solutions has been proposed to solve each of these two problems, but no
principled model solve both. in parallel, in the last few years, language models based on neural networks
have been used to cope with complex natural language processing tasks like emotion and paraphrase
detection. although they present good abilities to cope with both term dependencies and vocabulary
mismatch problems, thanks to the distributed representation of words they are based upon, such models
could not be used readily in ir, where the estimation of one language model per document (or query)
is required. this is both computationally unfeasible and prone to over-   tting. based on a recent work
that proposed to learn a generic language model that can be modi   ed through a set of document-speci   c
parameters, we explore use of new neural network models that are adapted to ad-hoc ir tasks. within
the language model ir framework, we propose and study the use of a generic language model as well
as a document-speci   c language model. both can be used as a smoothing component, but the latter is
more adapted to the document at hand and has the potential of being used as a full document language
model. we experiment with such models and analyze their results on trec-1 to 8 datasets.

introduction

1
to improve search e   ectiveness, information retrieval (ir) have sought for a long time to properly take into
account term dependencies and tackle term mismatch issues. both problems have been tackled by various
models, ranging from empirical to more principled approaches, but no principled approach for both problems
have been proposed so far. this paper proposes an approach based on recent developments of neural network
language models.

taking into account dependent query terms (as compound words for instance) in document relevance
estimations usually increases the precision of the search process. this corresponds to developing approaches
for identifying term dependencies and considering spatial proximity of such identi   ed linked terms in the
documents. among the    rst proposals to cope with term dependency issues, fagan et al.
[7] proposed to
consider pairs of successive terms (bi-grams) in vector space models. the same principle can be found in
language models such as in [22] that performs mixtures of uni- and bi-gram models. other works have sought
to combine the scores of models by taking into account di   erent co-occurrence patterns, such as [14] which
proposes a markov random    eld model to capture the term dependencies of queries and documents.
in
each case, the problem comes down to computing accurate estimates of id165 language models (or variants
thereof), i.e. language models where the id203 distribution of a term depends on a    nite sequence of
previous terms.

   sorbonne universit  s, upmc univ paris 06, cnrs, lip6 umr 7606, 4 place jussieu 75005 paris

1

on the other hand, taking into account semantic relationships (such as synonymy) increases recall by
enabling the retrieval of relevant documents that do not contain the exact query terms but use some se-
mantically related terms. this is particularly important because of the asymmetry between documents and
queries. two main di   erent approaches are used to cope with this problem. the    rst is to use (pseudo)
relevance feedback to add query terms that were not present in the original query. the second is to use
distributed models, like id45, where terms and documents are represented in a latent
space, which might be probabilistic or vectorial. none of these approaches have taken term dependencies
into account.

in this paper, we show that neural network language models, that leverage a distributed representation of
words, handle naturally terms dependence, and have hence an interesting potential in ir. neural language
models [2] have been successfully used in many natural language processing tasks, like part-of-speech tagging
[2], semantic labeling and more recently, translation. the main interest of such works is that they allow
to take into account both long-range term dependencies and semantic relatedness of words, thanks to the
distributed representation of words in a vector space. however, most of the works focused on building generic
language model, i.e. what would be called in ir a    background language model   . such language models
cannot be used directly for ad-hoc ir tasks, since, due to their huge number of parameters, learning accurate
individual models for each document of the collection using maximum likelihood techniques like for classical
unigram multinomial language models appears completely intractable. equally importantly, the learned
document model would be over-speci   c     the hypothesis that the document language model generate the
query would not be held anymore.

an interesting alternative was proposed by le and mikolov [12] who recently published a neural network
language model in which they propose to represent a context (a document, or a paragraph) as a vector that
modi   es the language model, which avoids building costly individual models for each document to consider.
our work is based on the    ndings of le and mikolov [12]. in this work, we follow a distributed approach
where the document is represented by a vector in rn. our contributions are the following:

1. we generalize the model proposed in [12], de   ning a more powerful architecture and new ways to

consider individual speci   cities of the documents;

2. we apply the model for ad-hoc information retrieval;

3. we perform intensive experiments on standard ir collections (trec-1 to 8) and analyze the results.

the outline of the paper is as follows. we    rst brie   y overview the many related works (section 2),

before exposing the language models (section 3). finally, experiments are reported (section 4).

2 related works
this section discuss related works by    rst presenting those dealing with the term dependencies and mismatch
problems. we then introduce related works about neural network language models (nnlm).

2.1 handling term dependencies
early approaches for handling term dependencies in ir considered extensions of the bag of word representa-
tion of texts, by including bi-grams to the vocabulary. such an approach was taken by fagan [7] for vector
space models, while the language model counterpart were proposed in the late of 90s [22, 24, 8] where the
authors proposed to use a mixture of the bigram and unigram language models, the di   erence being in how to
estimate the bigram language model or on how bigram are selected ([8] used a dependency grammar parsing
to generate candidate bigrams). this approach has proved to be not so successful, most probably because
more complex units imply sparser data [27], which in turn implies inaccurate bigram id203 estimations.
an alternative, where the mixture is de   ned within the quantum probabilistic framework, was proposed by
sordoni et al.[23]. this work proposed an elegant way to combine unigram and bigram distributions by

2

leveraging this new probabilistic framework. in this paper, we investigate another principled way to model
distributions over id165s, with n not being restricted to 1 or 2.

more sophisticated probabilistic models have been proposed to deal with more terms, as well as di   erent
dependency constraints between terms. region-based proximity models combine the score of several models,
each dealing with a speci   c dependency between query terms. for example, one of the submodels could be
computing a score for three query terms co-occurring close together in the document. metzler and croft
[14] proposed a markov random field approach where each clique corresponds to a set or sequence of query
terms. this work was extended by bendersky et al.
[1] who considered sets of concepts (phrases or set of
words) instead of set of words. in a di   erent probabilistic framework, blanco [3] proposed to extend bm25f,
a model able to take into account di   erent source of evidence to compute the importance of a term for a
document, to take into account term proximity by de   ning operators on so-called virtual regions. in this
work, these works are somehow orthogonal to ours since we are not interested by combining various sources
of evidences, but rather by investigating whether a parametric language model can capture typical sequence
of terms.

2.2 vocabulary mismatch
one of the most used techniques to deal with the problem of vocabulary mismatch is id183, based
on pseudo-relevance feedback, whereby terms are added to the query based on a set of (pseudo) relevant
documents [13]. it has been shown to improve search results in some cases, but is prone to the problem of
query drift, which can be controlled using statistical theories like the portfolio theory [4]. using pseudo-
relevance feedback is orthogonal to our work, and could be used as an extension to estimate a query relevance
language model [11].

global analysis can be used to enrich the id194 by using co-occurrence information at
the dataset level. for example, goyal et al. [9] use a term association matrix to modify the term-document
matrix and account for term relatedness. we believe that such information to be encoded in the neural
language model we propose to use in this paper.

id84 techniques such as latent semantic models have been proposed for dealing with
vocabulary mismatch issues [5]. the idea is to represent both the document and the query in a latent
space, and to compute a retrieval score based on these representations. however, such models do not work
well in practice because many document speci   c terms are discarded during the id84
process [25]. it is thus necessary to combine scores from such latent models with scores from standard ir
approaches such as bm25 to observe e   ectiveness improvements. this approach has been followed by [25]
in vector spaces and deveaud et al.
[6] for probabilistic (lda) models. in the latter work, a latent-based
language model is used as a background language model. in this paper, we consider a combination of a
document-speci   c neural network language model with a standard unigram multinomial one.

2.3 neural network language models
the idea of using neural networks to build language models emerged in the last ten years. this area of
research is included in the recent very active    eld of    representation learning   . bengio et al. [2] is one of the
   rst works that model text generation (at a word level) using neural networks. the model does so by using
a state (a vector in rn) to represent the history. this state de   nes a id203 distribution over words,
and can be updated with a new observation, hence allowing to de   ne a language model over a text.

such an idea of representation of texts in some latent space has been widely explored since then. a recent
successful work is the well-known id97 model [15] that proposed a simple neural network architecture to
predict a term within a prede   ned window. this model is fast to learn, thus allowing to compute distributed
representations of words over large collections, and has also been shown to implicitly encode relationships
(syntactic and semantic) between words: for example, there exists a translation that transforms the repre-
sentation of a singular word (e.g.    computer   ) into the representation of its plural form (   computers   ). other
works based on similar ideas were applied to sentiment detection [21] or automated translation [20].

3

closer to ir, the idea of representing the history as a state, as in [2], in a vectorial space has been
exploited by palangi et al. [17] who proposed to use the state obtained at the end of the document (resp.
the query) as a vectorial representation of the whole document (resp. the query). the relevance score is
then equal to the cosine between the query and document vectors. they trained the model over clickthrough
data and observed that their model was able to better rank clicked documents above the others.

compared to this work, our approach does not rely on external click data, and has the advantage of
conditioning the language model on the document vector, thus being less in   uenced by the end of the
document. it is based on the idea of using parameters to modify a generative probabilistic model. this is
what we call hereafter a parametrized model.

parameterized probabilistic models have been    rst applied to speaker recognition. the need arise because
those systems have to adapt fast enough to a new user. few researchers have tackled this problem by designing
a id48 whose id203 distribution depends on contextual variables. wilson and bobick [26] proposed
probabilistic models where the means of gaussian distribution vary linearly as a function of the context. as
the output distribution depends not only on the state but also on the context, a model may express many
distributions with a limited number of additional parameters.

this idea has been exploited by le and mikolov [12], who proposed a parameterized language model
which they experimented for id31 and an information related task where relationships between
query snippets are encoded by distances of their representations in the considered projection space. we
propose in this paper to extend this approach, by designing more sophisticated models dedicated for ad-hoc
ir tasks and experimenting them on various ir corpora (trec-1 to 8).

3 neural network ir models
in this section, we    rst present some background on classical language models for ir. then, we present
our contribution that allows the resulting ir model to deal with term dependencies and cope with term
mismatch issues using representation learning techniques. at last, we present a parametric extension that
performs document-dependent transformations of the model.

as most of the models deal with sequences, to clarify and shorten notations, we de   ne xi...j as the

sequence xi, xi+1, . . . , xj   1, xj and suppose that if i > j, the sequence is empty.

3.1 background: language models for ir
language models are probabilistic generative models of text     viewed as a sequence of terms. if a text is
composed of a sequence of terms t1...n, where each ti corresponds to a word in a pre-de   ned vocabulary, we
can compute the id203 of observing this sequence given the language model m as:

n(cid:89)

p (t1...n|m ) =

p (ti|t1...i   1, m )

language models are used in ir as a simple yet e   ective way to compute the relevance of a document to a
query [27]. there exists di   erent types of language models for ir, but one of the most standard is to equate
the relevance of the document d with the likelihood of the language model generating the query, using the
evaluated document language model md.

i=1

p (d relevant to q) = p (q| md)

where md is the so-called document language model, which is the model within the family of models m that
maximizes the id203 of observing the document d composed of terms d1...n, that is:

4

p (d|m )

md = argmax
m   m

= argmax
m   m

n(cid:88)

i=1

log p (di|d1...i   1, m )

(1)

among the di   erent families of generative models, the id165 multinomial family is the most used in
ir, with n usually equal to 1. the multinomial model assumes the independence of terms given the n     1
previous ones in the sequence. formally, if m is within this family, then

p (ti|t1...i   1, m ) = p (ti|ti   n+1...i   1, m )

    =   (ti|ti   n+1...i   1)

where    is a id155 table giving the id203 of observing the term ti after having observed
the sequence ti   n+1...i   1.

for a given document, the parameters    that maximize equation (1) can be computed in a closed form

formula:

  (ti|ti   n+1...i   1) =

count of (ti   n+1...i) in d
count of (ti   n+1...i   1   ) in d

(2)
where     correspond to any term of the vocabulary (i.e.
ti   n+1...i   1    corresponds to the sequence
ti   n+1, . . . , ti   1, u where u     w). with n = 1, we get a simple unigram model that does not consider
the context of the term (note that in that case the denominator is equal to the length of the document).
for instance, in a document about boston,    trail    is more likely to occur after    freedom    than in other doc-
uments. this information is then important to take into account to build more accurate ir models, since
a good document model about boston would give a higher id203 to    trail    occurring after    freedom   ,
and thus the corresponding documents would get a higher score for queries containing the sequence    freedom
trail   . works like [22] have explored the use of language models with n > 1. in such cases, the models are
able to capture term dependencies but usually at the cost of higher complexity and loss of generalization due
to the sparsity of the data     longer sequences are more unlikely to occur in a document d, even for sequences
that are strongly related with d in terms of content from a user perspective. with n     2, the estimated
probabilities would be in most cases equal to 0.

even with n = 1, the estimation given by the maximum likelihood might be wrong, and discard documents
just because they do not contain one query term, even if they contain several occurrences of all others. to
avoid such problems, smoothing techniques are used to avoid a zero id203 (and thus a score of 0) by
mixing the document language model with a collection language model1 denoted mc. a collection language
model mc correspond to the language model that maximizes the id203 of observing the sequences of
terms contained in the document of the collection c.
a standard smoothing method is the jelinek-mercer one that consists in a mixture of the document
language model and the collection language model. formally, given a smoothing coe   cient        [0, 1], the
language model of the document becomes:

p (ti|ti   n+1...i   1,   , d, c) = (1       )p (ti|ti   n+1...i   1, md) +   p (ti|ti   n+1...i   1, mc)

(3)

even for low values of n, the collection-based smoothing might not be e   ective. in the following, we
propose to develop new language models, that consider more sophisticated methodologies for smoothing,
able to deal with long term dependencies and vocabulary mismatch issues.

3.2 neural language models for ir
distributed representations of words and documents have been known for long in ir as latent semantic
indexing techniques were introduced in 1999 [5]. they are useful since they overcome the sparsity problem

1it can be the collection the document belongs to, or any external collection of documents

5

we just evoked by relying on the spatial relatedness of the embedded objects. for example, the words    cat   
and    dogs    should be closer together than    cat    and    pencil    in the vector space. this has been exploited
in ir to deal with vocabulary mismatch problem, but the idea of leveraging this kind of representation for
language models is more recent [2]. such language models are built using neural networks (hence their name
neural network language models) and o   er several advantages:

    compression abilities o   ered by representation learning techniques allow us to consider longer term

dependencies (i.e., longer id165s) than with classical approaches;

    geometric constraints implied by the continuous space used to represent words induce some natural
smoothing on the extracted relationships, which enables better generalization abilities and avoids well-
known di   culties related to zero counts of query words (or id165s) in the considered documents.

in this work, we propose to include such a distributed language model in the classical id203 com-
putations of ir query terms. thus, rather than equation 3, we propose to consider the following id203
computation:

p (ti|ti   n+1...i   1,   ,   , d, c) = (1       )((1       )p (ti|md) +   p (ti|mc)) +   pn n (ti|ti   n+1...i   1, d, c)

(4)
where pn n (ti|ti   n+1...i   1, d, c) is the id203 of observing the term ti after the sequence ti   n+1...i   1 in
the document d of the collection c according to our neural network model. it corresponds to introduce term
dependencies and vocabulary proximity in a classical unigram language model that is not able to capture
such relationships.

two version of our model are detailed hereafter:
    a generic neural network language model de   ned by a background collection (section 3.2.1) ;
    a document-speci   c neural network language model estimated by a background collection and the
document at hand (section 3.2.2). note that in that case, we are interested by the performance of
the model when    is close to 1 (ideally, 1) since this would mean that the document-speci   c model is
speci   c enough to fully represent the document.

both generic and document-dependent models are represented in    gure 1, where the black part corre-
sponds to the common framework for both models and the green and blue parts respectively stand for speci   c
layers for the generic and the document-dependent models.

3.2.1 generic neural model
there are two types of neural network language model, those that take into account an in   nite context, and
those that only take into account a limited number of previous terms. the former are based on recursive
neural networks, while the latter are standard feedforward ones. in this work, we investigate the use of the
feedforward networks since they are easier to train. moreover, we expect that the document-speci   c model
that we describe in the next section captures longer term dependencies.
the input of the neural network corresponds to the n     1 previous terms. for the    rst n     1 words of
a document, we use a special    padding    term. to each term ti corresponds a vector zt
i in the vector space
rm0. the n    1 vectors are then transformed through a function    (descr below) into a state vector s in rmf
where f is the index of the last layer. this state vector purpose is to summarize the contextual information,
which is then taken as an input by the last layer (hsm in the    gure) that computes a id203 for each
term of the vocabulary.

in our experiments, we considered three di   erent functions for   , each of which being a composition of
di   erent functions. following the neural network literature, we term each of the component functions a
layer.

6

model 1 (m1): linear(m1)     tanh the    rst layer transforms linearly the n     1 vectors zj     rm0 in a
vector in rm1, that is:

n   1(cid:88)

l1 =

ajzj + b

where each aj is a matrix of dimension m1    k and b is a bias vector in rm1.the second layer introduces a
non-linearity by computing the hyperbolic tangent (tanh) of each of its inputs

j=1

   j l2j = tanh(l1j)

in this model, the function    has (n     1)    m0    m1 parameters.
model 2 (m2): linear(m1)   tanh   linear(m2)   tanh the second function    we consider is an extension
of the    rst one where we add a linear (matrix b of dimension m2    m1) and a non-linear layer (tanh). in
this model, the function    has (n     1)    m0    m1 + m1    m2 parameters.
model 3 (m2max): linear(  m1)     max(  )     linear(m2)     tanh in the third model, we substitute to
the second layer (hyperbolic tangent) of the previous model another non-linear function, a maximum pooling
layer. maximum pooling layers are useful in deep neural networks because they introduce an invariant [10],
i.e. they allow to learn more easily that    big    in a    a big blue cone    and    a big cone    has the same in   uence
on the next term to appear. it is de   ned by a parameter    (set to 4 in our experiments)

max-pooling  (x)j = max{x    (j   1), . . . , x    j   1}

in this model, the function    has (n     1)          m0    m1 + m1    m2 parameters.

then, from a sequence of n     1 terms (t1...n   1), we get a summary vector   (z1...n   1) that is used to
compute id203 distributions over terms     the id203 that each term in the vocabulary occurs after
the sequence of n     1 terms.
in our model, we use a hierarchical softmax (hsm), which corresponds to an e   cient multivariate
di   erentiable function that maps an input vector (given by    ) to a vector in rv whose values sum to 1 [16].
it allows us to compute the id203 hsmt(v) of a term t given an input vector v, with a complexity
logarithmic with respect to the number of words in the vocabulary. the hsm is associated to a (binary)
tree where leaves correspond to words. formally, the function is de   ned as:

hsmt(v) =

s   path(t)

1

1 + exp(bs(t)    xs    v)

(5)

where hsmt denotes the component corresponding to the word t in the distribution encoded by the hsm
layer, v corresponds to the input vector given to the function, path(t) corresponds to the set of nodes leading
to the leaf corresponding to word t, xs is the vector associated to the inner node s of the tree, and bs(t) is
-1 (resp. 1) if the word t can be accessed through the left (resp. right) branch from node s. in our case, the
tree of the hsm function is a hu   man tree starting with trees composed of one node (the terms) associated
with a weight (the number of occurrences in the collection), the algorithm combines iteratively the two trees
with the lowest weights into a new tree formed by a new node with two branches leading to the two selected
trees. the set of vectors xs associated to each node s correspond are parameters of the model.
this allows us to easily compute a distribution of conditional probabilities pn n (t|t1...n   1) for the next
word w         knowing the past sequence of n     1 observed words:

pn n (t|t1...n   1) = hsmt(  (t1...n   1))

(6)

at last, to get an operational model that enables the computation of the id203 of a given query
with regards to the generic model, one has to learn the representation of words, the parameters of the hsm

7

(cid:89)

t1

      

tn   2

tn   1

d

  

  

hsm

hsm

{p (t|t1, . . . , tn   1)}

figure 1: general architecture of the generic (black and green/dashed) and document-dependent (black and
blue/dotted) language models

and the parameters of the functions. we denote this set of parameters by   . the learning problem can then
be formulated as the maximization of the id203 of observing the documents d in the collection d :

(cid:88)

|d|(cid:88)

d   d

i=0

      = argmax

  

w(di) log hsmdi(  (zd

i   n+1...i   1))

(7)

where w(di) is a weight used to lower the importance of frequent terms. this was used by mikolov [15] to
ensure a better learning, and we used the same settings.

3.2.2 document-dependent model
the generic neural network language model presented above handles long term dependencies and semantic
relatedness between words for all documents in the collection. this language model can be a good alternative
to the multinomial unigram collection language model used for smoothing. however, we believe that taking
into account speci   cities of the document at hand leads to a better language model, and hence to better
retrieval results. indeed, as explained above on an example about boston, term relationships can be di   erent
from documents to others.

learning speci   c neural language models for all individual documents is unfeasible for the same reasons
as for id165 language models for n > 1: learning term dependencies and semantic relatedness on the
sequences contained in a single considered document is likely to lead to over-training issues due to the lack of
vocabulary diversity and sequence samples. to overcome this problem, we follow the approach of [12] where
the id203 distribution of a generic neural network language model is modi   ed by a relatively small set of
parameters that can be reliably learned from the observation of a single document: the dimension of such a
vector (100-200) is typically much smaller than the number of parameters of a multinomial distribution (size
of the vocabulary). following this approach is interesting since it allows to    rst learn a language model from
the whole collection, and then only learn how to modify it for a speci   c piece of content. using parameters
to modify the behavior of a generative probabilistic model has been used in many works in signal processing,
like gesture recognition [26], where the model has to be quickly adapted to a speci   c user: such models
bene   t from a large source of information (the collection of all gestures or documents in our case) and at
the same time can be made speci   c enough to describe an individual user or document.
the modi   cation of the neural network language model is shown in    gure 1 (blue/dotted part). a
document-speci   c vector zd     rmf is used to modify the generic language model learned on the whole

8

name
m1
m2

m2max

model

linear(100) - tanh

linear(100) - tanh - linear(100) - tanh
linear(400) - max(4) - linear(100) - tanh

  

40,000
50,000
170,000

word/hsm
75,043,700
75,043,700
75,043,700

table 1: models and number of parameters (resp.
classi   er, and in total)

for the function   , the term representation and hsm

collection, by modifying the vector representing the context of the word to appear. the modi   ed vector is
then used to generate a id203 distribution by using, as for the generic language model, a hsm.
in this paper, we consider two    merging    operations    : rmf    rmf     rmf that associate the state

vector s given by    and the document speci   c vector zd to

    their sum, i.e.   (s, zd) = s + zd
    their component-wise product, i.e.   (s, zd) = s (cid:12) zd

these two functions are simple yet they can substantially modify the distribution of the language model.
taking again the example of boston, the components of zd would bias the model to words likely to occur
in such documents, thereby e.g.
increasing the id203 to    nd    trail    after    freedom   . this is done by
making the state vector more orthogonal to vectors in the hsm that lead to    trail    than to other words
associated to    freedom   .

note that, because the solution of our optimization problem is not unique, equivalent solutions (in term
of the cost function) could be obtained by rotation     this would in turn have an impact on the bene   t
of using such modi   cations. our experiments show however that there is a gain associated to using even
simple functions like term-wise multiplication or addition. future works will explore more sophisticated and
appropriate transformations of the state vector.

in theory, this document-speci   c language model could be used alone (i.e. without any smoothing, since
it is based on a background lm) to estimate the likelihood of generating a query. in practice, as shown in the
experiments below, the model is not yet powerful enough to be used so. however, combined with a classical
multinomial unigram model as proposed by equation 4, it allows to observe interesting improvements for
ir ad-hoc tasks. this corresponds to a    rst step towards a principled solution for handling vocabulary
mismatch and term dependencies.

4 experiments
we used the trec-1 to 8 collections for experimenting our models. we used as a baseline bm25 [19] with
standard parameter settings (k1 = 1.2 and b = 0.5) since it has a reasonable performance on these collections.
we left out the comparison with stronger baselines since we were interested    rst in comparing the di   erent
models between themselves. the collection was pre-processed using the porter stemmer, with no stop words.
to learn our language models, we pre-processed the collection using the same pre-processing (no stop
words, porter stemmer), but removed words occurring less than 5 times in the dataset since their learned
representation would have been wrongly estimated. the vocabulary size we obtained was of 375,219 words.
we used the id97 [15] implementation2 to pre-compute the initial word representations and hierarchical
softmax parameters. we used standard id97 parameters (window of size 5, and used an initial learning
rate of 0.5) and iterated 5 times over the corpus, thus ensuring that the initial representation of words is
meaningful and easing the following learning process.

then, we    rst trained the generic language models (section 3.2.1) that are listed in table 1 along with the
number of parameters, where we distinguish the computation of the context vector (  ) and the parameters

2code available https://code.google.com/p/id97/

9

representing words (initial representation and hsm parameters). we can see that most of the parameters
are related to words.

following common practice with stochastic id119, we used a steepest id119 to learn

our terms representation, using the following decreasing learning rate:

 k =

 0

1 + k      

where  0 = 0.1 and    = 2e     4. following the literature in neural language model learning, we used mini-
batch stochastic optimization with batches of 100 documents. after 50000 iterations, the average perplexity
was around 2. the number of iterations has thus been set empirically to 50000, as it corresponded to (1)
having seen (in expectation) at least 5 times each word of the collection and (2) the likelihood was not
evolving anymore.

for each topic, we selected the top-100 documents as ranked by bm25, and learned the document
parameters zd. we also used a id1193 with resilient id26 (rprop) [18] using an initial
learning rate of 0.1, and iterated until the di   erence between two successive settings be under 1e     4.

results are reported in figure 2 and table 2 where we compare the di   erent ir models for map and
gmap metrics on all trec datasets.
in this    gure lm stands for the classical unigram multinomial
language model with jelinek-mercer smoothing (equation 3). our 9 models are noted by the name of the
model (m1, m2 or m2max following which    function is used) followed by the operator used to transform
the representation with the document vector (# for none which corresponds to the generic model, + for the
addition operator and     for the component-wise multiplication). the x-axis is the value of    (in equation 3
for lm and in equation 4 for our neural models). we set    to 0.5 in equation 4.

we    rst observe that compared to classical jelinek-mercer lm, there is a slight overall improvements
with our document-speci   c models for both metrics. furthermore, there is greater stability with respect to
the smoothing coe   cient   , where the best values are for small values of   . we also note that the best
improvements are for badly performing topics (since gmap is more improved than map).

comparing the di   erent models, we see that the generic language models perform worse than their speci   c
counterparts. in table 3, we compared the performance of the model when using a generic and a document-
speci   c language model (we set    to 0.01), and observed that in the vast majority of cases we improved the
performance by using a document speci   c model.

while there is a di   erence on each trec dataset, there is no clear pattern between the di   erent models
m1, m2 and m2max. it seems however that more complex models do not perform better. we hypothesize
that there is more to gain when trying di   erent merging functions   .

we also observe that taking    = 1 for our document-speci   c language models, which corresponds to only
considering probabilities from our neural models without any other form of smoothing, does not degrade
as much as extreme values of    for the classical multinomial unigram model lm (   = 0 for the document
model only or    = 1 for the collection one). this is an encouraging result since it shows that using document
speci   c parameters enables to well capture some speci   cities of the documents. it actually modi   es the term
representation, leading to better results than a generic language model.

document length has also an in   uence on the latter observation: we observe that for trec-1 to trec-4
the drop is lesser than for trec-5 to trec-8. this is to be related to the average length of documents
which is higher for latter trec datasets. it shows that our document speci   c models are less e   ective for
longer documents, and that more sophisticated models for modifying the context state with the document
speci   c parameters are needed.

5 conclusion
in this paper, we have proposed new parametric neural network language models speci   cally designed for
adhoc ir tasks. we proposed various variants for both a generic and a document speci   c version of the

3we used a di   erent technique here since we wanted a fast convergence over a small set of parameters and a single document

10

figure 2: map and gmap for various trec datasets for bm25, the jelinek-mercer model (lm) and our
neural network models (the generic language model is speci   ed by a #). the x-axis is the value of    (in
equation 3 for lm and in equation 4 for our models). we set    to 0.5 in equation 4

11

0.0750.0800.0850.0900.0950.1000.105trec1map0.0210.0220.0230.0240.0250.0260.0270.0280.0290.030trec1gmap0.0750.0800.0850.0900.0950.100trec2map0.0240.0260.0280.0300.0320.0340.036trec2gmap0.120.130.140.150.160.17trec3map0.0500.0550.0600.0650.0700.0750.0800.0850.090trec3gmap0.060.070.080.090.100.110.120.13trec4map0.0220.0240.0260.0280.0300.0320.0340.0360.038trec4gmap0.050.060.070.080.090.100.110.120.13trec5map0.0080.0100.0120.0140.0160.0180.0200.0220.024trec5gmap0.120.130.140.150.160.170.180.190.20trec6map0.0300.0350.0400.0450.0500.0550.060trec6gmap0.00.20.40.60.81.0lambda0.090.100.110.120.130.140.150.16trec7map0.00.20.40.60.81.0lambda0.0350.0400.0450.0500.0550.0600.0650.0700.075trec7gmap0.00.20.40.60.81.0lambda0.100.120.140.160.180.200.22trec8map0.00.20.40.60.81.0lambda0.040.050.060.070.080.090.10trec8gmapm1#m1*m1+m2#m2*m2+m2max#m2max*m2max+bm25lm1
0.1
bm25
0.1
lm
0.1
m1#
0.1
m1*
0.1
m1+
0.1
m2#
0.1
m2*
0.1
m2+
m2max# 0.1
0.1
m2max*
m2max+ 0.1

2
0.1
0.1
0.09
0.1
0.1
0.09
0.09
0.09
0.09
0.09
0.09

3

0.16
0.17
0.15
0.16
0.17
0.15
0.17
0.16
0.15
0.16
0.16

map

4

0.12
0.11
0.1
0.11
0.11
0.1
0.11
0.11
0.1
0.1
0.11

5

0.12
0.12
0.11
0.12
0.11
0.11
0.12
0.12
0.11
0.11
0.11

6

0.19
0.19
0.18
0.19
0.2
0.18
0.19
0.19
0.18
0.19
0.19

7

0.15
0.15
0.14
0.16
0.16
0.14
0.15
0.15
0.14
0.14
nan

8

0.19
0.2
0.19
0.2
0.2
0.18
0.2
0.19
0.18
0.18
0.19

1

0.03
0.03
0.03
0.03
0.03
0.03
0.03
0.03
0.03
0.03
0.03

2

0.03
0.03
0.03
0.03
0.03
0.03
0.03
0.03
0.03
0.03
0.03

3

0.08
0.09
0.07
0.08
0.09
0.07
0.09
0.09
0.08
0.08
0.08

gmap
5
4

0.04
0.03
0.03
0.03
0.03
0.03
0.03
0.03
0.03
0.03
0.03

0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02

6

0.05
0.06
0.05
0.06
0.06
0.05
0.06
0.05
0.05
0.05
0.05

7

0.06
0.07
0.06
0.07
0.07
0.06
0.06
0.07
0.06
0.06
nan

8

0.08
0.09
0.08
0.09
0.09
0.07
0.09
0.09
0.07
0.08
0.08

table 2: map and gmap for models trained for 50000 iterations with    = 0.01 and a modifying vector zd
for various trec datasets

1
0.0056
m1*
0.0044
m1+
0.0054
m2*
0.0026
m2+
m2max*
0.0012
m2max+ 0.0028

2
0.0024
0.0029
0.0028
0.0020
0.0007
0.0013

3
0.0105
0.0127
0.0147
0.0125
0.0049
0.0068

4
0.0061
0.0043
0.0035
0.0050
-0.0003
0.0024

5
0.0055
0.0046
0.0115
0.0096
0.0005
0.0018

6
0.0119
0.0133
0.0149
0.0154
0.0069
0.0071

7
0.0148
0.0120
0.0132
0.0156
0.0010
0.0026

8
0.0093
0.0108
0.0167
0.0125
0.0011
0.0038

table 3: average di   erence between the ap of the document-speci   c and generic models with    = 0.01.

model. while the generic version of the model learns term dependencies and accurate representations of
words at a collection level, our document-speci   c language model is modi   ed for each document, through
the use of a vector of small dimension, which only contains a few hundred parameters, and thus can be
learned e   ciently and estimated correctly from a single document. this vector is used to modify the state
vector representing the context of the word for which we want to compute the id203 of occurring, by
using a component-wise multiplication or an addition operator.

we experimented with trec-1 to trec-8 ir test collections, where the top 100 results retrieved by
bm25 were reranked using our generic and document speci   c language models. the results show that using a
document-speci   c language model improves results over a baseline classical jelinek-mercer language model.
we have also shown that the document-speci   c model obtained better results than the generic one, thus
validating the approach document-dependent parameterization of the term representations.

while the results do not show a substantial improvement, we believe such models are interesting because,
by improving the way a generic language model can be modi   ed using a small set of parameters that represent
a textual object (document, paragraph, sentence, etc.), we could    nally reach a point where the document-
speci   c model can make a big di   erence. future work will thus study di   erent architectures of the neural
language model (including recurrent to consider longer dependencies), as well as use a relevance language
model [11] based on pseudo-relevance feedback     this might be more reliable since language models will be
learned from documents and applied on documents (rather than queries).

references
[1] m. bendersky and w. b. croft. modeling higher-order term dependencies in information retrieval using

query hypergraphs. in sigir    12, 2012.

12

[2] y. bengio, r. ducharme, p. vincent, and c. jauvin. a neural probabilistic language model. journal

of machine learning research, 2003.

[3] r. blanco and p. boldi. extending bm25 with multiple query operators. in sigir    12, 2012.

[4] k. collins-thompson. reducing the risk of id183 via robust constrained optimization. in

cikm    09, 2009.

[5] deerwester, scott, dumais, susan t, furnas, george w, landauer, thomas k, and harshman, richard.

indexing by latent semantic analysis. 1990.

[6] r. deveaud, e. sanjuan, and p. bellot. unsupervised latent concept modeling to identify query

facets. in oair   13, 2013.

[7] j. l. fagan. automatic phrase indexing for document retrieval: an examination of syntactic and

non-syntactic methods. in sigir   87, 1987.

[8] j. gao, j.-y. nie, g. wu, and g. cao. dependence language model for information retrieval.

sigir   04, 2004.

in

[9] p. goyal, l. behera, and t. m. mcginnity. a novel neighborhood based document smoothing model

for information retrieval. ir, 2013.

[10] n. kalchbrenner, e. grefenstette, and p. blunsom. a convolutional neural network for modelling

sentences. arxiv.org, 2014.

[11] v. lavrenko. a generative theory of relevance. springer, 2010.

[12] q. v. le and t. mikolov. distributed representations of sentences and documents. in icml   14, 2014.

[13] c. d. manning, p. raghavan, and h. sch  tze.

university press, 2008.

introduction to information retrieval. cambridge

[14] d. metzler and w. b. croft. a markov random    eld model for term dependencies. in sigir, 2005.

[15] t. mikolov, i. sutskever, k. chen, g. s. corrado, and j. dean. distributed representations of words

and phrases and their compositionality. nips   14, 2013.

[16] f. morin and y. bengio. hierarchical probabilistic neural network language model. aistats 2005,

2005.

[17] h. palangi, l. deng, y. shen, j. gao, x. he, j. chen, x. song, and r. ward. semantic modelling with

long-short-term memory for information retrieval. arxiv.org, 2014.

[18] m. riedmiller and h. braun. a direct adaptive method for faster id26 learning: the rprop

algorithm. in ieee international conference on neural networks, 1993.

[19] s. e. robertson and h. zaragoza. the probabilistic relevance framework: bm25 and beyond. foun-

dations and trends in information retrieval. 2009.

[20] h. schwenk. continuous space translation models for phrase-based id151 .

in proceedings of coling 2012: posters, 2013.

[21] r. socher, c. d. manning, b. huval, and a. y. ng. semantic compositionality through recursive
matrix-vector spaces. in emnlp-conll    12: proceedings of the 2012 joint conference on empirical
methods in natural language processing and computational natural language learning, 2012.

[22] f. song and w. b. croft. a general language model for information retrieval. in cikm   99, 1999.

13

[23] a. sordoni, j.-y. nie, and y. bengio. modeling term dependencies with quantum language models
for ir. in sigir    13: proceedings of the 36th international acm sigir conference on research and
development in information retrieval, 2013.

[24] m. srikanth and r. k. srihari. biterm language models for document retrieval. in sigir   02, 2002.

[25] q. wang, j. xu, h. li, and n. craswell. regularized id45: a new approach to

large-scale id96. acm tois, 2013.

[26] a. d. wilson and a. f. bobick. parametric id48 for gesture recognition.

transactions on pattern analysis and machine intelligence, 1999.

ieee

[27] c. zhai. statistical language models for information retrieval: a critical review. foundations and

trends in information retrieval, 2008.

14

