6
1
0
2

 
r
a

m
1

 

 
 
]
l
c
.
s
c
[
 
 

4
v
4
6
6
6
0

.

9
0
5
1
:
v
i
x
r
a

published as a conference paper at iclr 2016

reasoning about entailment with
neural attention

tim rockt  aschel
university college london
t.rocktaschel@cs.ucl.ac.uk

edward grefenstette & karl moritz hermann
google deepmind
{etg,kmh}@google.com

tom  a  s ko  cisk  y & phil blunsom
google deepmind & university of oxford
{tkocisky,pblunsom}@google.com

abstract

while most approaches to automatically recognizing entailment relations have
used classi   ers employing hand engineered features derived from complex nat-
ural language processing pipelines, in practice their performance has been only
slightly better than bag-of-word pair classi   ers using only lexical similarity. the
only attempt so far to build an end-to-end differentiable neural network for en-
tailment failed to outperform such a simple similarity classi   er. in this paper, we
propose a neural model that reads two sentences to determine entailment using
long short-term memory units. we extend this model with a word-by-word neural
attention mechanism that encourages reasoning over entailments of pairs of words
and phrases. furthermore, we present a qualitative analysis of attention weights
produced by this model, demonstrating such reasoning capabilities. on a large
entailment dataset this model outperforms the previous best neural model and a
classi   er with engineered features by a substantial margin. it is the    rst generic
end-to-end differentiable system that achieves state-of-the-art accuracy on a tex-
tual entailment dataset.

1

introduction

the ability to determine the semantic relationship between two sentences is an integral part of ma-
chines that understand and reason with natural language. recognizing id123 (rte) is
the task of determining whether two natural language sentences are (i) contradicting each other, (ii)
not related, or whether (iii) the    rst sentence (called premise) entails the second sentence (called
hypothesis). this task is important since many natural language processing (nlp) problems, such
as information extraction, id36, text summarization or machine translation, rely on it
explicitly or implicitly and could bene   t from more accurate rte systems (dagan et al., 2006).
state-of-the-art systems for rte so far relied heavily on engineered nlp pipelines, extensive manual
creation of features, as well as various external resources and specialized subcomponents such as
negation detection (e.g. lai and hockenmaier, 2014; jimenez et al., 2014; zhao et al., 2014; beltagy
et al., 2015). despite the success of neural networks for paraphrase detection (e.g. socher et al.,
2011; hu et al., 2014; yin and sch  utze, 2015), end-to-end differentiable neural architectures failed
to get close to acceptable performance for rte due to the lack of large high-quality datasets. an
end-to-end differentiable solution to rte is desirable, since it avoids speci   c assumptions about the
underlying language. in particular, there is no need for language features like part-of-speech tags
or dependency parses. furthermore, a generic sequence-to-sequence solution allows to extend the
concept of capturing entailment across any sequential data, not only natural language.
recently, bowman et al. (2015) published the stanford natural language id136 (snli) cor-
pus accompanied by a neural network with long short-term memory units (lstm, hochreiter and
schmidhuber, 1997), which achieves an accuracy of 77.6% for rte on this dataset. it is the    rst
time a generic neural model without hand-crafted features got close to the accuracy of a simple
lexicalized classi   er with engineered features for rte. this can be explained by the high quality

1

published as a conference paper at iclr 2016

and size of snli compared to the two orders of magnitude smaller and partly synthetic datasets
so far used to evaluate rte systems. bowman et al.   s lstm encodes the premise and hypothesis
as dense    xed-length vectors whose concatenation is subsequently used in a multi-layer id88
(mlp) for classi   cation. in contrast, we are proposing an attentive neural network that is capable of
reasoning over entailments of pairs of words and phrases by processing the hypothesis conditioned
on the premise.
our contributions are threefold: (i) we present a neural model based on lstms that reads two sen-
tences in one go to determine entailment, as opposed to mapping each sentence independently into
a semantic space (  2.2), (ii) we extend this model with a neural word-by-word attention mechanism
to encourage reasoning over entailments of pairs of words and phrases (  2.4), and (iii) we provide
a detailed qualitative analysis of neural attention for rte (  4.1). our benchmark lstm achieves
an accuracy of 80.9% on snli, outperforming a simple lexicalized classi   er tailored to rte by 2.7
percentage points. an extension with word-by-word neural attention surpasses this strong bench-
mark lstm result by 2.6 percentage points, setting a new state-of-the-art accuracy of 83.5% for
recognizing entailment on snli.

2 methods
in this section we discuss lstms (  2.1) and describe how they can be applied to rte (  2.2). we
introduce an extension of an lstm for rte with neural attention (  2.3) and word-by-word attention
(  2.4). finally, we show how such attentive models can easily be used for attending both ways: over
the premise conditioned on the hypothesis and over the hypothesis conditioned on the premise (  2.5).

2.1 lstms

recurrent neural networks (id56s) with long short-term memory (lstm) units (hochreiter and
schmidhuber, 1997) have been successfully applied to a wide range of nlp tasks, such as machine
translation (sutskever et al., 2014), constituency parsing (vinyals et al., 2014), id38
(zaremba et al., 2014) and recently rte (bowman et al., 2015). lstms encompass memory cells
that can store information for a long period of time, as well as three types of gates that control the
   ow of information into and out of these cells: input gates (eq. 2), forget gates (eq. 3) and output
gates (eq. 4). given an input vector xt at time step t, the previous output ht   1 and cell state ct   1,
an lstm with hidden size k computes the next output ht and cell state ct as

(1)

(2)
(3)

ot =   (woh + bo)
ct = ft (cid:12) ct   1 + it (cid:12) tanh(wch + bc)
ht = ot (cid:12) tanh(ct)

(4)
(5)
(6)

where wi, wf , wo, wc     r2k  k are trained matrices and bi, bf , bo, bc     rk trained biases that
parameterize the gates and transformations of the input,    denotes the element-wise application of
the sigmoid function and (cid:12) the element-wise multiplication of two vectors.

2.2 recognizing id123 with lstms

lstms can readily be used for rte by independently encoding the premise and hypothesis as
dense vectors and taking their concatenation as input to an mlp classi   er (bowman et al., 2015).
this demonstrates that lstms can learn semantically rich sentence representations that are suitable
for determining id123.

2.2.1 conditional encoding

in contrast to learning sentence representations, we are interested in neural models that read both
sentences to determine entailment, thereby reasoning over entailments of pairs of words and phrases.
figure 1 shows the high-level structure of this model. the premise (left) is read by an lstm. a sec-
ond lstm with different parameters is reading a delimiter and the hypothesis (right), but its memory

2

(cid:21)

(cid:20) xt

ht   1

h =

it =   (wih + bi)
ft =   (wf h + bf )

published as a conference paper at iclr 2016

figure 1: recognizing id123 using (a) conditional encoding via two lstms, one over
the premise and one over the hypothesis conditioned on the representation of the premise (c5), (b)
attention only based on the last output vector (h9) or (c) word-by-word attention based on all output
vectors of the hypothesis (h7, h8 and h9).

state is initialized with the last cell state of the previous lstm (c5 in the example), i.e. it is condi-
tioned on the representation that the    rst lstm built for the premise (a). we use id97 vectors
(mikolov et al., 2013) as word representations, which we do not optimize during training. out-of-
vocabulary words in the training set are randomly initialized by sampling values uniformly from
(   0.05, 0.05) and optimized during training.1 out-of-vocabulary words encountered at id136
time on the validation and test corpus are set to    xed random vectors. by not tuning representations
of words for which we have id97 vectors, we ensure that at id136 time their representation
stays close to unseen similar words for which we have id97 embeddings. we use a linear layer
to project word vectors to the dimensionality of the hidden size of the lstm, yielding input vectors
xi. finally, for classi   cation we use a softmax layer over the output of a non-linear projection of
the last output vector (h9 in the example) into the target space of the three classes (entailment,
neutral or contradiction), and train using the cross-id178 loss.

2.3 attention

attentive neural networks have recently demonstrated success in a wide range of tasks ranging from
handwriting synthesis (graves, 2013), digit classi   cation (mnih et al., 2014), machine translation
(bahdanau et al., 2015), image captioning (xu et al., 2015), id103 (chorowski et al.,
2015) and sentence summarization (rush et al., 2015), to geometric reasoning (vinyals et al., 2015).
the idea is to allow the model to attend over past output vectors (see figure 1 b), thereby mitigating
the lstm   s cell state bottleneck. more precisely, an lstm with attention for rte does not need to
capture the whole semantics of the premise in its cell state. instead, it is suf   cient to output vectors
while reading the premise and accumulating a representation in the cell state that informs the second
lstm which of the output vectors of the premise it needs to attend over to determine the rte class.
let y     rk  l be a matrix consisting of output vectors [h1        hl] that the    rst lstm produced
when reading the l words of the premise, where k is a hyperparameter denoting the size of em-
beddings and hidden layers. furthermore, let el     rl be a vector of 1s and hn be the last output
vector after the premise and hypothesis were processed by the two lstms respectively. the atten-
tion mechanism will produce a vector    of attention weights and a weighted representation r of the
premise via

m = tanh(wyy + whhn     el)
   = softmax(wt m)
r = y  t

m     rk  l
       rl
r     rk

(7)
(8)
(9)

1we found 12.1k words in snli for which we could not obtain id97 embeddings, resulting in 3.65m

tunable parameters.

3

x1c1h1x2c2h2x3c3h3x4c4h4x5c5h5x6c6h6x7c7h7x8c8h8x9c9h9aweddingpartytakingpictures::someonegotmarriedpremisehypothesis(a)conditionalencoding(c)word-by-wordattention(b)attentionpublished as a conference paper at iclr 2016

where wy, wh     rk  k are trained projection matrices, w     rk is a trained parameter vector and
wt denotes its transpose. note that the outer product whhn     el is repeating the linearly trans-
formed hn as many times as there are words in the premise (i.e. l times). hence, the intermediate
attention representation mi (ith column vector in m) of the ith word in the premise is obtained
from a non-linear combination of the premise   s output vector hi (ith column vector in y) and the
transformed hn . the attention weight for the ith word in the premise is the result of a weighted
combination (parameterized by w) of values in mi.
the    nal sentence-pair representation is obtained from a non-linear combination of the attention-
weighted representation r of the premise and the last output vector hn using
        rk
h

= tanh(wpr + wxhn )

(10)

   
h

where wp, wx     rk  k are trained projection matrices.

2.4 word-by-word attention

for determining whether one sentence entails another it can be a good strategy to check for en-
tailment or contradiction of individual word- and phrase-pairs. to encourage such behavior we
employ neural word-by-word attention similar to bahdanau et al. (2015), hermann et al. (2015) and
rush et al. (2015). the difference is that we do not use attention to generate words, but to obtain
a sentence-pair encoding from    ne-grained reasoning via soft-alignment of words and phrases in
the premise and hypothesis. in our case, this amounts to attending over the    rst lstm   s output
vectors of the premise while the second lstm processes the hypothesis one word at a time, thus
generating attention weight-vectors   t over all output vectors of the premise for every word xt with
t     (l + 1, n ) in the hypothesis (figure 1 c). this can be modeled as follows:

t + tanh(wtrt   1)

mt = tanh(wyy + (whht + wrrt   1)     el)
  t = softmax(wt mt)
rt = y  t

(11)
(12)
(13)
note that rt is dependent on the previous attention representation rt   1 to inform the model about
what was attended over in the previous step (see eq. 11 and 13).
as in the previous section, the    nal sentence-pair representation is obtained from a non-linear com-
bination of the last attention-weighted representation of the premise (here based on the last word of
the hypothesis) rn and the last output vector using
= tanh(wprn + wxhn )

mt     rk  l
  t     rl
rt     rk

        rk
h

(14)

   
h

2.5 two-way attention

inspired by bidirectional lstms that read a sequence and its reverse for improved encoding (graves
and schmidhuber, 2005), we introduce two-way attention for rte. the idea is to use the same model
(i.e. same structure and weights) to attend over the premise conditioned on the hypothesis, as well
as to attend over the hypothesis conditioned on the premise, by simply swapping the two sequences.
this produces two sentence-pair representations that we concatenate for classi   cation.

3 experiments

we conduct experiments on the stanford natural language id136 corpus (snli, bowman et al.,
2015). this corpus is two orders of magnitude larger than other existing rte corpora such as
sentences involving compositional knowledge (sick, marelli et al., 2014). furthermore, a large
part of training examples in sick were generated heuristically from other examples. in contrast,
all sentence-pairs in snli stem from human annotators. the size and quality of snli make it a
suitable resource for training neural architectures such as the ones proposed in this paper.
we use adam (kingma and ba, 2015) for optimization with a    rst momentum coef   cient of 0.9
and a second momentum coef   cient of 0.999.2 for every model we perform a small grid search

2standard con   guration recommended by kingma and ba.

4

published as a conference paper at iclr 2016

table 1: results on the snli corpus.

model
lexicalized classi   er (bowman et al., 2015)
lstm (bowman et al., 2015)
conditional encoding, shared
conditional encoding, shared
conditional encoding
attention
attention, two-way
word-by-word attention
word-by-word attention, two-way

|  |w+m
-

|  |m
k
-
-
100     10m 221k
111k
100
252k
159
116
252k
242k
100
242k
100
252k
100
100
252k

3.8m
3.9m
3.9m
3.9m
3.9m
3.9m
3.9m

train dev
99.7
84.4
83.7
84.4
83.5
85.4
86.5
85.3
86.6

-
-
81.9
83.0
82.1
83.2
83.0
83.7
83.6

test
78.2
77.6
80.9
81.4
80.9
82.3
82.4
83.5
83.2

over combinations of the initial learning rate [1e-4, 3e-4, 1e-3], dropout3 [0.0, 0.1, 0.2] and (cid:96)2-
id173 strength [0.0, 1e-4, 3e-4, 1e-3]. subsequently, we take the best con   guration based
on performance on the validation set, and evaluate only that con   guration on the test set.

4 results and discussion

results on the snli corpus are summarized in table 1. the total number of model parameters,
including tunable word representations, is denoted by |  |w+m (without word representations |  |m).
to ensure a comparable number of parameters to bowman et al.   s model that encodes the premise
and hypothesis independently using one lstm, we also run experiments for conditional encoding
where the parameters of both lstms are shared (   conditional encoding, shared    with k = 100)
as opposed to using two independent lstms. in addition, we compare our attentive models to two
benchmark lstms whose hidden sizes were chosen so that they have at least as many parameters as
the attentive models. since we are not tuning word vectors for which we have id97 embeddings,
the total number of parameters |  |w+m of our models is considerably smaller. we also compare
our models against the benchmark lexicalized classi   er used by bowman et al., which constructs
features from the id7 score between the premise and hypothesis, length difference, word overlap,
uni- and bigrams, part-of-speech tags, as well as cross uni- and bigrams.

conditional encoding we found that processing the hypothesis conditioned on the premise in-
stead of encoding each sentence independently gives an improvement of 3.3 percentage points in
accuracy over bowman et al.   s lstm. we argue this is due to information being able to    ow from
the part of the model that processes the premise to the part that processes the hypothesis. specif-
ically, the model does not waste capacity on encoding the hypothesis (in fact it does not need to
encode the hypothesis at all), but can read the hypothesis in a more focused way by checking words
and phrases for contradictions and entailments based on the semantic representation of the premise.
one interpretation is that the lstm is approximating a    nite-state automaton for rte (cf. angeli
and manning, 2014). another difference to bowman et al.   s model is that we are using id97
instead of glove for word representations and, more importantly, do not    ne-tune these word em-
beddings. the drop in accuracy from train to test set is less severe for our models, which suggest
that    ne-tuning id27s could be a cause of over   tting.
our lstm outperforms a simple lexicalized classi   er by 2.7 percentage points. to the best of our
knowledge, this is the    rst instance of a neural end-to-end differentiable model to achieve state-of-
the-art performance on a id123 dataset.

attention by incorporating an attention mechanism we found a 0.9 percentage point improvement
over a single lstm with a hidden size of 159, and a 1.4 percentage point increase over a benchmark
model that uses two lstms for conditional encoding (one for the premise and one for the hypothesis
conditioned on the representation of the premise). the attention model produces output vectors

3as in zaremba et al. (2014), we apply dropout only on the inputs and outputs of the network.

5

published as a conference paper at iclr 2016

(a)

(c)

(b)

(d)

figure 2: attention visualizations.

summarizing contextual information of the premise that is useful to attend over later when reading
the hypothesis. therefore, when reading the premise, the model does not have to build up a semantic
representation of the whole premise, but instead a representation that helps attending over the right
output vectors when processing the hypothesis. in contrast, the output vectors of the premise are
not used by the benchmark lstms. thus, these models have to build up a representation of the
whole premise and carry it over through the cell state to the part that processes the hypothesis   a
bottleneck that can be overcome to some degree by using attention.

word-by-word attention enabling the model to attend over output vectors of the premise for ev-
ery word in the hypothesis yields another 1.2 percentage point improvement compared to attending
based only on the last output vector of the premise. we argue that this is due to the model being
able to check for entailment or contradiction of individual words and phrases in the hypothesis, and
demonstrate this effect in the qualitative analysis below.

two-way attention allowing the model to also attend over the hypothesis based on the premise
does not seem to improve performance for rte. we suspect that this is due to entailment being
an asymmetric relation. hence, using the same lstm to encode the hypothesis (in one direction)
and the premise (in the other direction) might lead to noise in the training signal. this could be
addressed by training different lstms at the cost of doubling the number of model parameters.

4.1 qualitative analysis

it is instructive to analyze which output representations the model is attending over when deciding
the class of an rte example. note that interpretations based on attention weights have to be taken
with care since the model is not forced to solely rely on representations obtained from attention
(see hn in eq. 10 and 14). in the following we visualize and discuss the attention patterns of the
presented attentive models. for each attentive model we hand-picked examples from ten randomly
drawn samples of the validation set.

attention figure 2 shows to what extent the attentive model focuses on contextual representations
of the premise after both lstms processed the premise and hypothesis respectively. note how the
model pays attention to output vectors of words that are semantically coherent with the premise
(   riding    and    rides   ,    animal    and    camel   , 2a) or in contradiction, as caused by a single word
(   blue    vs.    pink   , 2b) or multiple words (   swim    and    lake    vs.    frolicking    and    grass   , 2c).
interestingly, the model shows contextual understanding by not attending over    yellow   , the color

6

published as a conference paper at iclr 2016

(a)

(b)

(c)

(d)

(e)

(f)

(g)

figure 3: word-by-word attention visualizations.

7

published as a conference paper at iclr 2016

of the toy, but    pink   , the color of the coat. however, for more involved examples with longer
premises we found that attention is more uniformly distributed (2d). this suggests that conditioning
attention only on the last output has limitations when multiple words need to be considered for
deciding the rte class.

word-by-word attention visualizations of word-by-word attention are depicted in figure 3. we
found that word-by-word attention can easily detect if the hypothesis is simply a reordering of words
in the premise (3a). furthermore, it is able to resolve synonyms (   airplane    and    aircraft   , 3c) and
capable of matching multi-word expressions to single words (   garbage can    to    trashcan   , 3b). it is
also noteworthy that irrelevant parts of the premise, such as words capturing little meaning or whole
uninformative relative clauses, are correctly neglected for determining entailment (   which also has
a rope leading out of it   , 3b).
word-by-word attention seems to also work well when words in the premise and hypothesis are
connected via deeper semantics or common-sense knowledge (   snow    can be found    outside    and a
   mother    is an    adult   , 3e and 3g). furthermore, the model is able to resolve one-to-many relation-
ships (   kids    to    boy    and    girl   , 3d)
attention can fail, for example when the two sentences and their words are entirely unrelated (3f).
in such cases, the model seems to back up to attending over function words, and the sentence-pair
representation is likely dominated by the last output vector (hn ) instead of the attention-weighted
representation (see eq. 14).

5 conclusion

in this paper, we show how the state-of-the-art in recognizing id123 on a large, human-
curated and annotated corpus, can be improved with general end-to-end differentiable models. our
results demonstrate that lstm recurrent neural networks that read pairs of sequences to produce
a    nal representation from which a simple classi   er predicts entailment, outperform both a neural
baseline as well as a classi   er with hand-engineered features. extending these models with attention
over the premise provides further improvements to the predictive abilities of the system, resulting
in a new state-of-the-art accuracy for recognizing entailment on the stanford natural language
id136 corpus.
the models presented here are general sequence models, requiring no appeal to natural language-
speci   c processing beyond id121, and are therefore a suitable target for id21
through pre-training the recurrent systems on other corpora, and conversely, applying the models
trained on this corpus to other entailment tasks. future work will focus on such id21
tasks, as well as scaling the methods presented here to larger units of text (e.g. paragraphs and
entire documents) using hierarchical attention mechanisms. additionally, it would be worthwhile
exploring how other, more structured forms of attention (e.g. graves et al., 2014; sukhbaatar et al.,
2015), or other forms of differentiable memory (e.g. grefenstette et al., 2015; joulin and mikolov,
2015) could help improve performance on rte over the neural models presented in this paper.
furthermore, we aim to investigate the application of these generic models to non-natural language
sequential entailment problems.

acknowledgements

we thank nando de freitas, samuel bowman, jonathan berant, danqi chen, christopher manning,
and the anonymous iclr reviewers for their helpful comments on drafts of this paper.

references
gabor angeli and christopher d manning. naturalli: natural logic id136 for common sense reasoning. in

emnlp, 2014.

dzmitry bahdanau, kyunghyun cho, and yoshua bengio. id4 by jointly learning to

align and translate. in iclr, 2015.

islam beltagy, stephen roller, pengxiang cheng, katrin erk, and raymond j. mooney. representing meaning

with a combination of logical form and vectors. arxiv preprint arxiv:1505.06816, 2015.

8

published as a conference paper at iclr 2016

samuel r bowman, gabor angeli, christopher potts, and christopher d manning. a large annotated corpus

for learning natural language id136. in emnlp, 2015.

jan chorowski, dzmitry bahdanau, dmitriy serdyuk, kyunghyun cho, and yoshua bengio. attention-based

models for id103. in nips, 2015.

ido dagan, oren glickman, and bernardo magnini. the pascal recognising id123 challenge. in
machine learning challenges. lecture notes in computer science, volume 3944, pages 177   190. springer,
2006.

alex graves. generating sequences with recurrent neural networks. arxiv preprint arxiv:1308.0850, 2013.
alex graves and j  urgen schmidhuber. framewise phoneme classi   cation with bidirectional lstm and other

neural network architectures. neural networks, 18(5):602   610, 2005.

alex graves, greg wayne, and ivo danihelka. id63s. arxiv preprint arxiv:1410.5401, 2014.
edward grefenstette, karl moritz hermann, mustafa suleyman, and phil blunsom. learning to transduce with

unbounded memory. in nips, 2015.

karl moritz hermann, tom  a  s ko  cisk  y, edward grefenstette, lasse espeholt, will kay, mustafa suleyman,

and phil blunsom. teaching machines to read and comprehend. in nips, 2015.

sepp hochreiter and j  urgen schmidhuber. long short-term memory. neural computation, 9(8):1735   1780,

1997.

baotian hu, zhengdong lu, hang li, and qingcai chen. convolutional neural network architectures for

matching natural language sentences. in nips, pages 2042   2050, 2014.

sergio jimenez, george duenas, julia baquero, alexander gelbukh, av juan dios b  atiz, and av mendiz  abal.
unal-nlp: combining soft cardinality features for semantic textual similarity, relatedness and entailment. in
semeval 2014, page 732, 2014.

armand joulin and tomas mikolov. inferring algorithmic patterns with stack-augmented recurrent nets. arxiv

preprint arxiv:1503.01007, 2015.

diederik kingma and jimmy ba. adam: a method for stochastic optimization. in iclr, 2015.
alice lai and julia hockenmaier.
semeval 2014, page 329, 2014.

illinois-lh: a denotational and distributional approach to semantics.

in

marco marelli, luisa bentivogli, marco baroni, raffaella bernardi, stefano menini, and roberto zamparelli.
semeval-2014 task 1: evaluation of compositional distributional semantic models on full sentences through
semantic relatedness and id123. in semeval 2014, 2014.

tomas mikolov, ilya sutskever, kai chen, greg s corrado, and jeff dean. distributed representations of words

and phrases and their compositionality. in nips, pages 3111   3119, 2013.

volodymyr mnih, nicolas heess, alex graves, et al. recurrent models of visual attention. in nips, pages

2204   2212, 2014.

alexander m rush, sumit chopra, and jason weston. a neural attention model for abstractive sentence sum-

marization. in emnlp, 2015.

richard socher, eric h huang, jeffrey pennin, christopher d manning, and andrew y ng. dynamic pooling

and unfolding recursive autoencoders for paraphrase detection. in nips, pages 801   809, 2011.

sainbayar sukhbaatar, arthur szlam, jason weston, and rob fergus. end-to-end memory networks. arxiv

preprint arxiv:1503.08895, 2015.

ilya sutskever, oriol vinyals, and quoc vv le. sequence to sequence learning with neural networks. in nips,

pages 3104   3112, 2014.

oriol vinyals, lukasz kaiser, terry koo, slav petrov, ilya sutskever, and geoffrey hinton. grammar as a

foreign language. in nips, 2014.

oriol vinyals, meire fortunato, and navdeep jaitly. id193. in nips, 2015.
kelvin xu, jimmy ba, ryan kiros, kyunghyun cho, aaron courville, ruslan salakhutdinov, richard zemel,
and yoshua bengio. show, attend and tell: neural image id134 with visual attention. in icml,
2015.

wenpeng yin and hinrich sch  utze. convolutional neural network for paraphrase identi   cation. in naacl-

hlt, pages 901   911, 2015.

wojciech zaremba, ilya sutskever, and oriol vinyals. recurrent neural network id173. arxiv preprint

arxiv:1409.2329, 2014.

jiang zhao, tian tian zhu, and man lan. ecnu: one stone two birds: ensemble of heterogenous measures for

semantic relatedness and id123. in semeval 2014, page 271, 2014.

9

