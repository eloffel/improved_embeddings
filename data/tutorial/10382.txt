6
1
0
2

 

n
u
j
 

5

 
 
]
l
c
.
s
c
[
 
 

4
v
8
3
7
2

.

1
1
4
1
:
v
i
x
r
a

id97 parameter learning explained

xin rong

ronxin@umich.edu

abstract

the id97 model and application by mikolov et al. have attracted a great
amount of attention in recent two years. the vector representations of words learned
by id97 models have been shown to carry semantic meanings and are useful in
various nlp tasks. as an increasing number of researchers would like to experiment
with id97 or similar techniques, i notice that there lacks a material that compre-
hensively explains the parameter learning process of id27 models in details,
thus preventing researchers that are non-experts in neural networks from understanding
the working mechanism of such models.

this note provides detailed derivations and explanations of the parameter up-
date equations of the id97 models, including the original continuous bag-of-word
(cbow) and skip-gram (sg) models, as well as advanced optimization techniques,
including hierarchical softmax and negative sampling. intuitive interpretations of the
gradient equations are also provided alongside mathematical derivations.

in the appendix, a review on the basics of neuron networks and id26
is provided. i also created an interactive demo, wevi, to facilitate the intuitive under-
standing of the model.1

1 continuous bag-of-word model

1.1 one-word context

we start from the simplest version of the continuous bag-of-word model (cbow) intro-
duced in mikolov et al. (2013a). we assume that there is only one word considered per
context, which means the model will predict one target word given one context word, which
is like a bigram model. for readers who are new to neural networks, it is recommended that
one go through appendix a for a quick review of the important concepts and terminologies
before proceeding further.

figure 1 shows the network model under the simpli   ed context de   nition2.

in our
setting, the vocabulary size is v , and the hidden layer size is n . the units on adjacent

1an online interactive demo is available at: http://bit.ly/wevi-online.
2in figures 1, 2, 3, and the rest of this note, w(cid:48) is not the transpose of w, but a di   erent matrix

instead.

1

figure 1: a simple cbow model with only one word in the context

layers are fully connected. the input is a one-hot encoded vector, which means for a given
input context word, only one out of v units, {x1,       , xv }, will be 1, and all other units
are 0.
the weights between the input layer and the output layer can be represented by a
v    n matrix w. each row of w is the n -dimension vector representation vw of the
associated word of the input layer. formally, row i of w is vt
w. given a context (a word),
assuming xk = 1 and xk(cid:48) = 0 for k(cid:48) (cid:54)= k, we have

h = wt x = wt

(k,  ) := vt
wi

,

(1)

which is essentially copying the k-th row of w to h. vwi is the vector representation of the
input word wi . this implies that the link (activation) function of the hidden layer units is
simply linear (i.e., directly passing its weighted sum of inputs to the next layer).
ij},
which is an n    v matrix. using these weights, we can compute a score uj for each word
in the vocabulary,

from the hidden layer to the output layer, there is a di   erent weight matrix w(cid:48) = {w(cid:48)

(2)
wj is the j-th column of the matrix w(cid:48). then we can use softmax, a log-linear
where v(cid:48)
classi   cation model, to obtain the posterior distribution of words, which is a multinomial
distribution.

wj

uj = v(cid:48)

t h,

p(wj|wi ) = yj =

(cid:80)v

exp(uj)
j(cid:48)=1 exp(uj(cid:48))

,

(3)

where yj is the output of the j-the unit in the output layer. substituting (1) and (2) into

2

input layerhidden layeroutput layerx1x2x3xkxvy1y2y3yjyvh1h2hihnwv  n={wki}w'n  v={w'ij}(3), we obtain

p(wj|wi ) =

(cid:17)

(cid:16)

exp

(cid:80)v

v(cid:48)

(cid:16)

t vwi
wj
v(cid:48)
wj(cid:48)

j(cid:48)=1 exp

t vwi

(cid:17)

(4)

note that vw and v(cid:48)

w are two representations of the word w. vw comes from rows of
w, which is the input   hidden weight matrix, and v(cid:48)
w comes from columns of w(cid:48), which
is the hidden   output matrix. in subsequent analysis, we call vw as the    input vector   ,
and v(cid:48)

w as the    output vector    of the word w.

update equation for hidden   output weights

let us now derive the weight update equation for this model. although the actual com-
putation is impractical (explained below), we are doing the derivation to gain insights on
this original model with no tricks applied. for a review of basics of id26, see
appendix a.

the training objective (for one training sample) is to maximize (4), the conditional
id203 of observing the actual output word wo (denote its index in the output layer
as j   ) given the input context word wi with regard to the weights.

max p(wo|wi ) = max yj   

= max log yj   
= uj        log

v(cid:88)

j(cid:48)=1

exp(uj(cid:48)) :=    e,

(5)

(6)

(7)

where e =     log p(wo|wi ) is our id168 (we want to minimize e), and j    is the
index of the actual output word in the output layer. note that this id168 can be
understood as a special case of the cross-id178 measurement between two probabilistic
distributions.

let us now derive the update equation of the weights between hidden and output layers.

take the derivative of e with regard to j-th unit   s net input uj, we obtain

= yj     tj := ej

   e
   uj

(8)

where tj = 1(j = j   ), i.e., tj will only be 1 when the j-th unit is the actual output word,
otherwise tj = 0. note that this derivative is simply the prediction error ej of the output
layer.
ij to obtain the gradient on the hidden   output

next we take the derivative on w(cid:48)

weights.

   e
   w(cid:48)

ij

=

   e
   uj

      uj
   w(cid:48)

ij

= ej    hi

3

(9)

therefore, using stochastic id119, we obtain the weight updating equation for
hidden   output weights:

w(cid:48)

ij

(new) = w(cid:48)

ij

(old)           ej    hi.

(10)

or

for j = 1, 2,       , v.

v(cid:48)

(new) = v(cid:48)

(old)           ej    h

wj

wj

(11)
where    > 0 is the learning rate, ej = yj     tj, and hi is the i-th unit in the hidden layer;
v(cid:48)
wj is the output vector of wj. note that this update equation implies that we have to
go through every possible word in the vocabulary, check its output id203 yj, and
compare yj with its expected output tj (either 0 or 1).
if yj > tj (   overestimating   ),
then we subtract a proportion of the hidden vector h (i.e., vwi ) from v(cid:48)
wj , thus making
v(cid:48)
wj farther away from vwi ; if yj < tj (   underestimating   , which is true only if tj = 1,
i.e., wj = wo), we add some h to v(cid:48)
if yj is very
close to tj, then according to the update equation, very little change will be made to the
weights. note, again, that vw (input vector) and v(cid:48)
w (output vector) are two di   erent
vector representations of the word w.

, thus making v(cid:48)

closer3 to vwi .

wo

wo

update equation for input   hidden weights
having obtained the update equations for w(cid:48), we can now move on to w. we take the
derivative of e on the output of the hidden layer, obtaining

v(cid:88)

j=1

v(cid:88)

j=1

   e
   hi

=

   e
   uj

      uj
   hi

=

ej    w(cid:48)

ij := ehi

(12)

where hi is the output of the i-th unit of the hidden layer; uj is de   ned in (2), the net
input of the j-th unit in the output layer; and ej = yj     tj is the prediction error of the
j-th word in the output layer. eh, an n -dim vector, is the sum of the output vectors of
all words in the vocabulary, weighted by their prediction error.

next we should take the derivative of e on w. first, recall that the hidden layer
performs a linear computation on the values from the input layer. expanding the vector
notation in (1) we get

now we can take the derivative of e with regard to each element of w, obtaining

k=1

   e
   wki

=

   e
   hi

      hi
   wki

= ehi    xk

3here when i say    closer    or    farther   , i meant using the inner product instead of euclidean as the

distance measurement.

4

(13)

(14)

v(cid:88)

hi =

xk    wki

this is equivalent to the tensor product of x and eh, i.e.,

= x     eh = xeht

   e
   w

(15)
from which we obtain a v    n matrix. since only one component of x is non-zero, only
   w is non-zero, and the value of that row is eht , an n -dim vector. we obtain
one row of    e
the update equation of w as

v(new)

wi

= v(old)

wi

      eht

(16)

where vwi is a row of w, the    input vector    of the only context word, and is the only row
of w whose derivative is non-zero. all the other rows of w will remain unchanged after
this iteration, because their derivatives are zero.
intuitively, since vector eh is the sum of output vectors of all words in vocabulary
weighted by their prediction error ej = yj     tj, we can understand (16) as adding a portion
of every output vector in vocabulary to the input vector of the context word. if, in the
output layer, the id203 of a word wj being the output word is overestimated (yj > tj),
then the input vector of the context word wi will tend to move farther away from the output
vector of wj; conversely if the id203 of wj being the output word is underestimated
(yj < tj), then the input vector wi will tend to move closer to the output vector of wj;
if the id203 of wj is fairly accurately predicted, then it will have little e   ect on the
movement of the input vector of wi . the movement of the input vector of wi is determined
by the prediction error of all vectors in the vocabulary; the larger the prediction error, the
more signi   cant e   ects a word will exert on the movement on the input vector of the
context word.

as we iteratively update the model parameters by going through context-target word
pairs generated from a training corpus, the e   ects on the vectors will accumulate. we
can imagine that the output vector of a word w is    dragged    back-and-forth by the input
vectors of w   s co-occurring neighbors, as if there are physical strings between the vector
of w and the vectors of its neighbors. similarly, an input vector can also be considered as
being dragged by many output vectors. this interpretation can remind us of gravity, or
force-directed graph layout. the equilibrium length of each imaginary string is related to
the strength of cooccurrence between the associated pair of words, as well as the learning
rate. after many iterations, the relative positions of the input and output vectors will
eventually stabilize.

1.2 multi-word context

figure 2 shows the cbow model with a multi-word context setting. when computing
the hidden layer output, instead of directly copying the input vector of the input context
word, the cbow model takes the average of the vectors of the input context words, and

5

use the product of the input   hidden weight matrix and the average vector as the output.

h =

wt (x1 + x2 +        + xc)
(vw1 + vw2 +        + vwc )t

1
c
1
c

=

(18)
where c is the number of words in the context, w1,       , wc are the words the in the context,
and vw is the input vector of a word w. the id168 is
e = =     log p(wo|wi,1,       , wi,c)

(19)

=    uj    + log

exp(uj(cid:48))

v(cid:88)

j(cid:48)=1
t    h + log

v(cid:88)

j(cid:48)=1

=    v(cid:48)

wo

exp(v(cid:48)

wj

t    h)

(17)

(20)

(21)

which is the same as (7), the objective of the one-word-context model, except that h is
di   erent, as de   ned in (18) instead of (1).

figure 2: continuous bag-of-word model

6

input layerhidden layeroutput layerwv  nwv  nwv  nw'n  vyjhix2kx1kxckc  v-dimn-dimv-dimthe update equation for the hidden   output weights stay the same as that for the

one-word-context model (11). we copy it here:
(old)           ej    h

(new) = v(cid:48)

v(cid:48)

for j = 1, 2,       , v.

wj

(22)
note that we need to apply this to every element of the hidden   output weight matrix for
each training instance.
the update equation for input   hidden weights is similar to (16), except that now we

wj

need to apply the following equation for every word wi,c in the context:
for c = 1, 2,       , c.

         eht

v(new)
wi,c

= v(old)
wi,c

    1
c

(23)

where vwi,c is the input vector of the c-th word in the input context;    is a positive learning
rate; and eh =    e
is given by (12). the intuitive understanding of this update equation
   hi
is the same as that for (16).

2 skip-gram model

the skip-gram model is introduced in mikolov et al. (2013a,b). figure 3 shows the skip-
gram model. it is the opposite of the cbow model. the target word is now at the input
layer, and the context words are on the output layer.

we still use vwi to denote the input vector of the only word on the input layer, and
thus we have the same de   nition of the hidden-layer outputs h as in (1), which means h is
simply copying (and transposing) a row of the input   hidden weight matrix, w, associated
with the input word wi . we copy the de   nition of h below:

h = wt

(k,  ) := vt
wi

,

(24)

on the output layer, instead of outputing one multinomial distribution, we are output-
ing c multinomial distributions. each output is computed using the same hidden   output
matrix:

p(wc,j = wo,c|wi ) = yc,j =

(25)

(cid:80)v

exp(uc,j)
j(cid:48)=1 exp(uj(cid:48))

where wc,j is the j-th word on the c-th panel of the output layer; wo,c is the actual c-th
word in the output context words; wi is the only input word; yc,j is the output of the j-th
unit on the c-th panel of the output layer; uc,j is the net input of the j-th unit on the c-th
panel of the output layer. because the output layer panels share the same weights, thus

uc,j = uj = v(cid:48)

wj

t    h, for c = 1, 2,       , c

(26)

where v(cid:48)
taken from a column of the hidden   output weight matrix, w(cid:48).

wj is the output vector of the j-th word in the vocabulary, wj, and also v(cid:48)

wj is

7

figure 3: the skip-gram model.

the derivation of parameter update equations is not so di   erent from the one-word-

context model. the id168 is changed to

e =     log p(wo,1, wo,2,       , wo,c|wi )

c(cid:89)

(cid:80)v
c + c    log
uj   

c=1

exp(uc,j   
c )
j(cid:48)=1 exp(uj(cid:48))

v(cid:88)

j(cid:48)=1

=     log

=     c(cid:88)

c=1

exp(uj(cid:48))

(27)

(28)

(29)

where j   

c is the index of the actual c-th output context word in the vocabulary.

we take the derivative of e with regard to the net input of every unit on every panel

of the output layer, uc,j and obtain

   e
   uc,j

= yc,j     tc,j := ec,j

(30)

which is the prediction error on the unit, the same as in (8). for notation simplicity, we
de   ne a v -dimensional vector ei = {ei1,       , eiv } as the sum of prediction errors over all

8

input layer hidden layer output layer wv  n	
   w'n  v	
   c  v-dim	
   n-dim	
   v-dim	
   xk	
   hi	
   w'n  v	
   w'n  v	
   y2,j	
   y1,j	
   yc,j	
   context words:

c(cid:88)

ec,j

eij =

(31)

next, we take the derivative of e with regard to the hidden   output matrix w(cid:48), and
obtain

c=1

c(cid:88)

c=1

   e
   w(cid:48)

ij

=

   e
   uc,j

      uc,j
   w(cid:48)

ij

= eij    hi

thus we obtain the update equation for the hidden   output matrix w(cid:48),

w(cid:48)

ij

(new) = w(cid:48)

ij

(old)           eij    hi

or

v(cid:48)

wj

(new) = v(cid:48)

wj

(old)           eij    h

for j = 1, 2,       , v.

(32)

(33)

(34)

the intuitive understanding of this update equation is the same as that for (11), except
that the prediction error is summed across all context words in the output layer. note that
we need to apply this update equation for every element of the hidden   output matrix for
each training instance.
the derivation of the update equation for the input   hidden matrix is identical to (12)
to (16), except taking into account that the prediction error ej is replaced with eij. we
directly give the update equation:

v(new)

wi

= v(old)

wi

          eht

where eh is an n -dim vector, each component of which is de   ned as

v(cid:88)

ehi =

eij    w(cid:48)
ij.

(35)

(36)

the intuitive understanding of (35) is the same as that for (16).

j=1

3 optimizing computational e   ciency

so far the models we have discussed (   bigram    model, cbow and skip-gram) are both in
their original forms, without any e   ciency optimization tricks being applied.
for all these models, there exist two vector representations for each word in the vo-
cabulary: the input vector vw, and the output vector v(cid:48)
w. learning the input vectors is
cheap; but learning the output vectors is very expensive. from the update equations (22)
and (33), we can    nd that, in order to update v(cid:48)
w, for each training instance, we have to
iterate through every word wj in the vocabulary, compute their net input uj, id203

9

prediction yj (or yc,j for skip-gram), their prediction error ej (or eij for skip-gram), and
   nally use their prediction error to update their output vector v(cid:48)
j.

doing such computations for all words for every training instance is very expensive,
making it impractical to scale up to large vocabularies or large training corpora. to solve
this problem, an intuition is to limit the number of output vectors that must be updated per
training instance. one elegant approach to achieving this is hierarchical softmax; another
approach is through sampling, which will be discussed in the next section.

both tricks optimize only the computation of the updates for output vectors. in our
, the
   h , the weighted sum of predictions

derivations, we care about three values: (1) e, the new objective function; (2)    e
   v(cid:48)
new update equation for the output vectors; and (3)    e
errors to be backpropagated for updating input vectors.

w

3.1 hierarchical softmax

hierarchical softmax is an e   cient way of computing softmax (morin and bengio, 2005;
mnih and hinton, 2009). the model uses a binary tree to represent all words in the
vocabulary. the v words must be leaf units of the tree. it can be proved that there are
v     1 inner units. for each leaf unit, there exists a unique path from the root to the unit;
and this path is used to estimate the id203 of the word represented by the leaf unit.
see figure 4 for an example tree.

figure 4: an example binary tree for the hierarchical softmax model. the white units are
words in the vocabulary, and the dark units are inner units. an example path from root to
w2 is highlighted. in the example shown, the length of the path l(w2) = 4. n(w, j) means
the j-th unit on the path from root to the word w.

in the hierarchical softmax model, there is no output vector representation for words.
n(w,j). and the id203 of

instead, each of the v     1 inner units has an output vector v(cid:48)
a word being the output word is de   ned as

l(w)   1(cid:89)

j=1

(cid:16)(cid:74)n(w, j + 1) = ch(n(w, j))(cid:75)    v(cid:48)

(cid:17)

t h

n(w,j)

(37)

p(w = wo) =

  

10

w1w2w3w4wv-1wvn(w2,1)n(w2,2)n(w2,3)where ch(n) is the left child of unit n; v(cid:48)
n(w,j) is the vector representation (   output vector   )
of the inner unit n(w, j); h is the output value of the hidden layer (in the skip-gram model
h = vwi ; and in cbow, h = 1
c

(cid:80)c
c=1 vwc);(cid:74)x(cid:75) is a special function de   ned as
(cid:40)
(cid:74)x(cid:75) =

if x is true;
otherwise.

1
   1

(38)

let us intuitively understand the equation by going through an example. looking at
figure 4, suppose we want to compute the id203 that w2 being the output word. we
de   ne this id203 as the id203 of a random walk starting from the root ending
at the leaf unit in question. at each inner unit (including the root unit), we need to assign
the probabilities of going left and going right.4 we de   ne the id203 of going left at
an inner unit n to be

(cid:16)

(cid:17)

p(n, left) =   

v(cid:48)

n

t    h

which is determined by both the vector representation of the inner unit, and the hidden
layer   s output value (which is then determined by the vector representation of the input
word(s)). apparently the id203 of going right at unit n is

(cid:16)

(cid:17)

(cid:16)   v(cid:48)

(cid:17)

p(n, right) = 1       

v(cid:48)

n

t    h

=   

t    h

n

following the path from the root to w2 in figure 4, we can compute the id203 of w2
being the output word as

p(w2 = wo) = p (n(w2, 1), left)    p (n(w2, 2), left)    p (n(w2, 3), right)

=   

v(cid:48)

t h

n(w2,1)

t h

n(w2,2)

t h

n(w2,3)

(cid:17)      

(cid:16)   v(cid:48)

(cid:16)

(cid:17)      

v(cid:48)

(cid:17)

(cid:16)

which is exactly what is given by (37). it should not be hard to verify that

v(cid:88)

p(wi = wo) = 1

i=1

making the hierarchical softmax a well de   ned multinomial distribution among all words.
now let us derive the parameter update equation for the vector representations of the
inner units. for simplicity, we look at the one-word context model    rst. extending the
update equations to cbow and skip-gram models is easy.

for the simplicity of notation, we de   ne the following shortenizations without intro-

ducing ambiguity:

(cid:74)  (cid:75) :=(cid:74)n(w, j + 1) = ch(n(w, j))(cid:75)

4while an inner unit of a binary tree may not always have both children, a binary hu   man tree   s inner
units always do. although theoretically one can use many di   erent types of trees for hierarchical softmax,
id97 uses a binary hu   man tree for fast training.

(39)

(40)

(41)

(42)

(43)

(44)

11

for a training instance, the error function is de   ned as

e =     log p(w = wo|wi ) =    

we take the derivative of e with regard to v(cid:48)

v(cid:48)
j := v(cid:48)

nw,j

j

j=1

jh, obtaining

l(w)   1(cid:88)
log   ((cid:74)  (cid:75)v(cid:48)
(cid:17)(cid:74)  (cid:75)
((cid:74)  (cid:75) = 1)
((cid:74)  (cid:75) =    1)

t h)     1

(cid:16)
  ((cid:74)  (cid:75)v(cid:48)
(cid:40)

j

j

=

  (v(cid:48)
  (v(cid:48)
=   (v(cid:48)

t h)     1
t h)
t h)     tj

j

t h)

(45)

(46)

(47)

(48)

(49)

   e
   v(cid:48)
jh

=

next we take the derivative of e with regard to the vector representation of the inner

where tj = 1 if(cid:74)  (cid:75) = 1 and tj = 0 otherwise.

j

unit n(w, j) and obtain

   e
   v(cid:48)

j

=

   e
   v(cid:48)
jh

      v(cid:48)
jh
   v(cid:48)

j

which results in the following update equation:
(old)       

(new) = v(cid:48)

v(cid:48)

j

j

  (v(cid:48)

j

t h)     tj

which should be applied to j = 1, 2,       , l(w)     1. we can understand   (v(cid:48)
t h)     tj as
the prediction error for the inner unit n(w, j). the    task    for each inner unit is to predict
whether it should follow the left child or the right child in the random walk. tj = 1 means
the ground truth is to follow the left child; tj = 0 means it should follow the right child.
  (v(cid:48)
t h) is the prediction result. for a training instance, if the prediction of the inner unit
is very close to the ground truth, then its vector representation v(cid:48)
j will move very little;
otherwise v(cid:48)
j will move in an appropriate direction by moving (either closer or farther away5
from h) so as to reduce the prediction error for this instance. this update equation can
be used for both cbow and the skip-gram model. when used for the skip-gram model,
we need to repeat this update procedure for each of the c words in the output context.

j

j

5again, the distance measurement is inner product.

12

=

  (v(cid:48)

j

t h)     tj

(cid:16)
(cid:16)

(cid:17)    h
(cid:17)    h

(50)

(51)

in order to backpropagate the error to learn input   hidden weights, we take the deriva-

tive of e with regard to the output of the hidden layer and obtain

l(w)   1(cid:88)
l(w)   1(cid:88)

j=1

   e
   v(cid:48)
jh

(cid:16)

      v(cid:48)

jh
   h

  (v(cid:48)

j

t h)     tj

   e
   h

=

=

j=1

:= eh

(cid:17)    v(cid:48)

j

(52)

(53)

(54)

which can be directly substituted into (23) to obtain the update equation for the input
vectors of cbow. for the skip-gram model, we need to calculate a eh value for each word
in the skip-gram context, and plug the sum of the eh values into (35) to obtain the update
equation for the input vector.

from the update equations, we can see that the computational complexity per training
instance per context word is reduced from o(v ) to o(log(v )), which is a big improvement
in speed. we still have roughly the same number of parameters (v    1 vectors for inner-units
compared to originally v output vectors for words).

3.2 negative sampling

the idea of negative sampling is more straightforward than hierarchical softmax: in order
to deal with the di   culty of having too many output vectors that need to be updated per
iteration, we only update a sample of them.

apparently the output word (i.e., the ground truth, or positive sample) should be kept
in our sample and gets updated, and we need to sample a few words as negative samples
(hence    negative sampling   ). a probabilistic distribution is needed for the sampling pro-
cess, and it can be arbitrarily chosen. we call this distribution the noise distribution, and
denote it as pn(w). one can determine a good distribution empirically.6

in id97, instead of using a form of negative sampling that produces a well-de   ned
posterior multinomial distribution, the authors argue that the following simpli   ed training
objective is capable of producing high-quality id27s:7

e =     log   (v(cid:48)

wo

log   (   v(cid:48)

wj

t h)

(55)

where wo is the output word (i.e., the positive sample), and v(cid:48)
the output value of the hidden layer: h = 1
c

is its output vector; h is
c=1 vwc in the cbow model and h = vwi

wo

6as described in (mikolov et al., 2013b), id97 uses a unigram distribution raised to the 3

4 th power

for the best quality of results.

7goldberg and levy (2014) provide a theoretical analysis on the reason of using this objective function.

13

t h)     (cid:88)
(cid:80)c

wj   wneg

in the skip-gram model; wneg = {wj|j = 1,       , k} is the set of words that are sampled
based on pn(w), i.e., negative samples.

to obtain the update equations of the word vectors under negative sampling, we    rst

take the derivative of e with regard to the net input of the output unit wj:

(cid:40)

   e

   v(cid:48)
wj

t h

=

  (v(cid:48)
  (v(cid:48)
=   (v(cid:48)

wj

t h)     1
t h)
t h)     tj

wj

wj

if wj = wo
if wj     wneg

(56)

(57)

(cid:16)

(cid:16)

(cid:17)

(cid:17)

where tj is the    label    of word wj. t = 1 when wj is a positive sample; t = 0 otherwise.
next we take the derivative of e with regard to the output vector of the word wj,

   e
   v(cid:48)
wj

=

   e

   v(cid:48)
wj

t h

t h

      v(cid:48)
wj
   v(cid:48)
wj

=

  (v(cid:48)

wj

t h)     tj

h

(58)

which results in the following update equation for its output vector:

v(cid:48)

(new) = v(cid:48)

(old)       

  (v(cid:48)

t h)     tj

(59)
which only needs to be applied to wj     {wo}   wneg instead of every word in the vocabulary.
this shows why we may save a signi   cant amount of computational e   ort per iteration.

wj

wj

wj

h

the intuitive understanding of the above update equation should be the same as that
of (11). this equation can be used for both cbow and the skip-gram model. for the
skip-gram model, we apply this equation for one context word at a time.

to backpropagate the error to the hidden layer and thus update the input vectors
of words, we need to take the derivative of e with regard to the hidden layer   s output,
obtaining

   e
   h

=

=

(cid:88)
(cid:88)

wj   {wo}   wneg

wj   {wo}   wneg

t h

   e

(cid:16)

   v(cid:48)
wj
  (v(cid:48)

      v(cid:48)

wj
t h
   h
t h)     tj

wj

(cid:17)

v(cid:48)
wj := eh

(60)

(61)

by plugging eh into (23) we obtain the update equation for the input vectors of the
cbow model. for the skip-gram model, we need to calculate a eh value for each word in
the skip-gram context, and plug the sum of the eh values into (35) to obtain the update
equation for the input vector.

14

acknowledgement

the author would like to thank eytan adar, qiaozhu mei, jian tang, dragomir radev,
daniel pressel, thomas dean, sudeep gandhe, peter lau, luheng he, tomas mikolov,
hao jiang, and oded shmueli for discussions on the topic and/or improving the writing of
the note.

references

goldberg, y. and levy, o. (2014). id97 explained: deriving mikolov et al.   s negative-

sampling word-embedding method. arxiv:1402.3722 [cs, stat]. arxiv: 1402.3722.

mikolov, t., chen, k., corrado, g., and dean, j. (2013a). e   cient estimation of word

representations in vector space. arxiv preprint arxiv:1301.3781.

mikolov, t., sutskever, i., chen, k., corrado, g. s., and dean, j. (2013b). distributed
representations of words and phrases and their compositionality. in advances in neural
information processing systems, pages 3111   3119.

mnih, a. and hinton, g. e. (2009). a scalable hierarchical distributed language model.
in koller, d., schuurmans, d., bengio, y., and bottou, l., editors, advances in neural
information processing systems 21, pages 1081   1088. curran associates, inc.

morin, f. and bengio, y. (2005). hierarchical probabilistic neural network language model.

in aistats, volume 5, pages 246   252. citeseer.

15

a back propagation basics

a.1 learning algorithms for a single unit
figure 5 shows an arti   cial neuron (unit). {x1,       , xk} are input values; {w1,       , wk} are
weights; y is a scalar output; and f is the link function (also called activation/decision/transfer
function).

figure 5: an arti   cial neuron

the unit works in the following way:

y = f (u),

(62)

where u is a scalar number, which is the net input (or    new input   ) of the neuron. u is
de   ned as

k(cid:88)

u =

wixi.

using vector notation, we can write

i=0

u = wt x

(63)

(64)

note that here we ignore the bias term in u. to include a bias term, one can simply

add an input dimension (e.g., x0) that is constant 1.

apparently, di   erent link functions result in distinct behaviors of the neuron. we

discuss two example choices of link functions here.

the    rst example choice of f (u) is the unit step function (aka heaviside step

function):

f (u) =

(cid:40)

1
0

if u > 0
otherwise

(65)

a neuron with this link function is called a id88. the learning algorithm for a

id88 is the id88 algorithm. its update equation is de   ned as:

w(new) = w(old)           (y     t)    x

(66)

16

x1x2x3xkfyw1w2w3wkwhere t is the label (gold standard) and    is the learning rate (   > 0). note that a
id88 is a linear classi   er, which means its description capacity can be very limited.
if we want to    t more complex functions, we need to use a non-linear model.

the second example choice of f (u) is the logistic function (a most common kind of

sigmoid function), de   ned as

  (u) =

1

1 + e   u

(67)

the logistic function has two primary good properties: (1) the output y is always between
0 and 1, and (2) unlike a unit step function,   (u) is smooth and di   erentiable, making the
derivation of update equation very easy.

note that   (u) also has the following two properties that can be very convenient and

will be used in our subsequent derivations:

  (   u) = 1       (u)

d  (u)

du

=   (u)  (   u)

(68)

(69)

we use stochastic id119 as the learning algorithm of this model. in order to
derive the update equation, we need to de   ne the error function, i.e., the training objective.
the following objective function seems to be convenient:

e =

(t     y)2

1
2

we take the derivative of e with regard to wi,

   e
   wi

   e
   y

      y
   u

      u
   wi

=
= (y     t)    y(1     y)    xi

(70)

(71)

(72)

   u = y(1     y) because y = f (u) =   (u), and (68) and (69). once we have the

where    y
derivative, we can apply stochastic id119:

w(new) = w(old)           (y     t)    y(1     y)    x.

(73)

a.2 back-propagation with multi-layer network
figure 6 shows a multi-layer neural network with an input layer {xk} = {x1,       , xk}, a
hidden layer {hi} = {h1,       , hn}, and an output layer {yj} = {y1,       , ym}. for clarity
we use k, i, j as the subscript for input, hidden, and output layer units respectively. we use
ui and u(cid:48)
j to denote the net input of hidden layer units and output layer units respectively.
we want to derive the update equation for learning the weights wki between the input
and hidden layers, and w(cid:48)
ij between the hidden and output layers. we assume that all the

17

figure 6: a multi-layer neural network with one hidden layer

computation units (i.e., units in the hidden layer and the output layer) use the logistic
function   (u) as the link function. therefore, for a unit hi in the hidden layer, its output
is de   ned as

similarly, for a unit yj in the output layer, its output is de   ned as

hi =   (ui) =   

wkixk

.

yj =   (u(cid:48)

j) =   

w(cid:48)
ijhi

.

(cid:33)

(cid:33)

(cid:32) k(cid:88)
(cid:32) n(cid:88)

k=1

i=1

m(cid:88)

j=1

e(x, t, w, w(cid:48)) =

1
2

(yj     tj)2,

(74)

(75)

(76)

we use the squared sum error function given by

where w = {wki}, a k    n weight matrix (input-hidden), and w(cid:48) = {w(cid:48)
ij}, a n    m
weight matrix (hidden-output). t = {t1,       , tm}, a m -dimension vector, which is the
gold-standard labels of output.

to obtain the update equations for wki and w(cid:48)

ij, we simply need to take the derivative
of the error function e with regard to the weights respectively. to make the derivation
straightforward, we do start computing the derivative for the right-most layer (i.e., the
output layer), and then move left. for each layer, we split the computation into three
steps, computing the derivative of the error with regard to the output, net input, and
weight respectively. this process is shown below.

18

y1y2y3ymh1h2hnx1x2x3xkinput layerhidden layeroutput layer{wki}{w'ij}we start with the output layer. the    rst step is to compute the derivative of the error

w.r.t. the output:

= yj     tj.

   e
   yj

(77)

the second step is to compute the derivative of the error with regard to the net input of
the output layer. note that when taking derivatives with regard to something, we need to
keep everything else    xed. also note that this value is very important because it will be
reused multiple times in subsequent computations. we denote it as ei(cid:48)

j for simplicity.

   e
   u(cid:48)

j

=

   e
   yj

      yj
   u(cid:48)

j

= (yj     tj)    yj(1     yj) := ei(cid:48)

j

(78)

the third step is to compute the derivative of the error with regard to the weight between
the hidden layer and the output layer.

   e
   w(cid:48)

ij

=

   e
   u(cid:48)

j

      u(cid:48)
   w(cid:48)

j

ij

= ei(cid:48)

j    hi

(79)

so far, we have obtained the update equation for weights between the hidden layer and the
output layer.

w(cid:48)

ij

(new) = w(cid:48)
= w(cid:48)

ij

ij

(old)              e
   w(cid:48)
j    hi.
(old)           ei(cid:48)

ij

(80)

(81)

where    > 0 is the learning rate.

we can repeat the same three steps to obtain the update equation for weights of the

previous layer, which is essentially the idea of back propagation.

we repeat the    rst step and compute the derivative of the error with regard to the
output of the hidden layer. note that the output of the hidden layer is related to all units
in the output layer.

m(cid:88)

j=1

   e
   hi

=

   u(cid:48)
j
   hi

   e
   u(cid:48)

j

=

m(cid:88)

j=1

ei(cid:48)

j    w(cid:48)
ij.

(82)

then we repeat the second step above to compute the derivative of the error with regard
to the net input of the hidden layer. this value is again very important, and we denote it
as eii.

   e
   ui

=

   e
   hi

      hi
   ui

=

ei(cid:48)

j    w(cid:48)

ij    hi(1     hi) := eii

(83)

m(cid:88)

j=1

next we repeat the third step above to compute the derivative of the error with regard to
the weights between the input layer and the hidden layer.
= eii    xk,

(84)

=

   e
   wki

   e
   ui

      ui
   wki

19

finally, we can obtain the update equation for weights between the input layer and the
hidden layer.

wki

(new) = wki

(old)           eii    xk.

(85)

from the above example, we can see that the intermediate results (ei(cid:48)

j) when computing
the derivatives for one layer can be reused for the previous layer. imagine there were another
layer prior to the input layer, then eii can also be reused to continue computing the chain
of derivatives e   ciently. compare equations (78) and (83), we may    nd that in (83), the
ij is just like the    error    of the hidden layer unit hi. we may interpret
this term as the error    back-propagated    from the next layer, and this propagation may
go back further if the network has more hidden layers.

factor(cid:80)m

j=1 ei(cid:48)

jw(cid:48)

b wevi: id27 visual inspector

an interactive visual interface, wevi (id27 visual inspector), is available online
to demonstrate the working mechanism of the models described in this paper. see figure 7
for a screenshot of wevi.

the demo allows the user to visually examine the movement of input vectors and output
vectors as each training instance is consumed. the training process can be also run in batch
mode (e.g., consuming 500 training instances in a row), which can reveal the emergence of
patterns in the weight matrices and the corresponding word vectors. principal component
analysis (pca) is employed to visualize the    high   -dimensional vectors in a 2d scatter
plot. the demo supports both cbow and skip-gram models.

after training the model, the user can manually activate one or multiple input-layer
units, and inspect which hidden-layer units and output-layer units become active. the
user can also customize training data, hidden layer size, and learning rate. several preset
training datasets are provided, which can generate di   erent results that seem interesting,
such as using a toy vocabulary to reproduce the famous word analogy: king - queen = man
- woman.

it is hoped that by interacting with this demo one can quickly gain insights of the
working mechanism of the model. the system is available at http://bit.ly/wevi-online.
the source code is available at http://github.com/ronxin/wevi.

20

figure 7: wevi screenshot (http://bit.ly/wevi-online)

21

