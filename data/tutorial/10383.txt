improving id4 models with monolingual data

rico sennrich and barry haddow and alexandra birch

school of informatics, university of edinburgh

{rico.sennrich,a.birch}@ed.ac.uk, bhaddow@inf.ed.ac.uk

6
1
0
2

 

n
u
j
 

3

 
 
]
l
c
.
s
c
[
 
 

4
v
9
0
7
6
0

.

1
1
5
1
:
v
i
x
r
a

abstract

id4 (id4) has
obtained state-of-the art performance for
language pairs, while only us-
several
ing parallel data for training.
target-
side monolingual data plays an impor-
tant role in boosting    uency for phrase-
based id151, and
we investigate the use of monolingual data
for id4. in contrast to previous work,
which combines id4 models with sep-
arately trained language models, we note
that encoder-decoder id4 architectures
already have the capacity to learn the same
information as a language model, and we
explore strategies to train with monolin-
gual data without changing the neural net-
work architecture. by pairing monolin-
gual training data with an automatic back-
translation, we can treat it as additional
parallel training data, and we obtain sub-
stantial improvements on the wmt 15
task english   german (+2.8   3.7 id7),
and for the low-resourced iwslt 14 task
turkish   english (+2.1   3.4 id7), ob-
taining new state-of-the-art results. we
also show that    ne-tuning on in-domain
monolingual and parallel data gives sub-
stantial improvements for the iwslt 15
task english   german.

cal machine translation, and we investigate the use
of monolingual data for id4.

language models trained on monolingual data
role in statistical ma-
have played a central
chine translation since the    rst
ibm models
(brown et al., 1990). there are two major rea-
sons for their importance. firstly, word-based
and phrase-based translation models make strong
independence assumptions, with the id203
of translation units estimated independently from
context, and language models, by making different
independence assumptions, can model how well
these translation units    t together. secondly, the
amount of available monolingual data in the tar-
get language typically far exceeds the amount of
parallel data, and models typically improve when
trained on more data, or data more similar to the
translation task.

in

for

(attentional)

encoder-decoder

neural machine

archi-
tectures
translation
(sutskever et al., 2014;
bahdanau et al., 2015),
the decoder is essentially an id56 language model
that is also conditioned on source context, so
the    rst rationale, adding a language model to
compensate for the independence assumptions of
the translation model, does not apply. however,
the data argument is still valid in id4, and we
expect monolingual data to be especially helpful
if parallel data is sparse, or a poor    t for the
translation task, for instance because of a domain
mismatch.

1 introduction

id4 (id4) has obtained
state-of-the art performance for several language
pairs, while only using parallel data for training.
target-side monolingual data plays an important
role in boosting    uency for phrase-based statisti-

the research presented in this publication was conducted
in cooperation with samsung electronics polska sp. z o.o. -
samsung r&d institute poland.

in contrast to previous work, which integrates a
separately trained id56 language model into the
id4 model (g  l  ehre et al., 2015), we explore
strategies to include monolingual training data in
the training process without changing the neural
network architecture. this makes our approach
applicable to different id4 architectures.

the main contributions of this paper are as fol-

lows:

    we show that we can improve the machine

translation quality of id4 systems by mix-
ing monolingual
target sentences into the
training set.

    we investigate two different methods to    ll
the source side of monolingual training in-
stances: using a dummy source sentence, and
using a source sentence obtained via back-
translation, which we call synthetic. we    nd
that the latter is more effective.

    we successfully adapt id4 models to a new
domain by    ne-tuning with either monolin-
gual or parallel in-domain data.

2 id4

we follow the id4 archi-
tecture by bahdanau et al. (2015), which we will
brie   y summarize here. however, we note that our
approach is not speci   c to this architecture.

the id4 system is imple-
mented as an encoder-decoder network with recur-
rent neural networks.

the encoder

is a bidirectional neural net-
work with id149 (cho et al., 2014)
that reads an input sequence x = (x1, ..., xm)
and calculates a forward sequence of hidden
states (
h m), and a backward sequence
      
h j are
(
h 1, ...,
concatenated to obtain the annotation vector hj.

      
h 1, ...,
      
h m). the hidden states

h j and

      

      

      

the decoder is a recurrent neural network that
predicts a target sequence y = (y1, ..., yn). each
word yi is predicted based on a recurrent hidden
state si, the previously predicted word yi   1, and
a context vector ci. ci is computed as a weighted
sum of the annotations hj. the weight of each
annotation hj is computed through an alignment
model   ij, which models the id203 that yi is
aligned to xj. the alignment model is a single-
layer feedforward neural network that is learned
jointly with the rest of the network through back-
propagation.

a detailed description can be found in
(bahdanau et al., 2015). training is performed on
a parallel corpus with stochastic id119.
for translation, a id125 with small beam
size is employed.

3 id4 training with monolingual

training data

in machine translation, more monolingual data
(or monolingual data more similar to the test set)

serves to improve the estimate of the prior prob-
ability p(t ) of the target sentence t , before tak-
ing the source sentence s into account.
in con-
trast to (g  l  ehre et al., 2015), who train separate
language models on monolingual training data and
incorporate them into the neural network through
shallow or deep fusion, we propose techniques
to train the main id4 model with monolingual
data, exploiting the fact that encoder-decoder neu-
ral networks already condition the id203 dis-
tribution of the next target word on the previous
target words. we describe two strategies to do this:
providing monolingual training examples with an
empty (or dummy) source sentence, or providing
monolingual training data with a synthetic source
sentence that is obtained from automatically trans-
lating the target sentence into the source language,
which we will refer to as back-translation.

3.1 dummy source sentences

the    rst technique we employ is to treat mono-
lingual
training examples as parallel examples
with empty source side, essentially adding train-
ing examples whose context vector ci is uninfor-
mative, and for which the network has to fully
rely on the previous target words for its predic-
tion. this could be conceived as a form of dropout
(hinton et al., 2012), with the difference that the
training instances that have the context vector
dropped out constitute novel training data. we
can also conceive of this setup as multi-task learn-
ing, with the two tasks being translation when the
source is known, and language modelling when it
is unknown.

during training, we use both parallel and mono-
lingual training examples in the ratio 1-to-1, and
randomly shuf   e them. we de   ne an epoch as one
iteration through the parallel data set, and resam-
ple from the monolingual data set for every epoch.
we pair monolingual sentences with a single-word
dummy source side <null> to allow processing of
both parallel and monolingual training examples
with the same network graph.1 for monolingual
minibatches2, we freeze the network parameters
of the encoder and the attention model.

one problem with this integration of monolin-

1one could force the context vector ci to be 0 for monolin-
gual training instances, but we found that this does not solve
the main problem with this approach, discussed below.

2for ef   ciency, bahdanau et al. (2015) sort sets of 20
minibatches according to length. this also groups monolin-
gual training instances together.

gual data is that we cannot arbitrarily increase the
ratio of monolingual training instances, or    ne-
tune a model with only monolingual training data,
because different output layer parameters are opti-
mal for the two tasks, and the network    unlearns   
its conditioning on the source context if the ratio
of monolingual training instances is too high.

3.2 synthetic source sentences

to ensure that the output layer remains sensitive to
the source context, and that good parameters are
not unlearned from monolingual data, we propose
to pair monolingual training instances with a syn-
thetic source sentence from which a context vec-
tor can be approximated. we obtain these through
back-translation, i.e. an automatic translation of
the monolingual target text into the source lan-
guage.

during training, we mix synthetic parallel text
into the original (human-translated) parallel text
and do not distinguish between the two: no net-
work parameters are frozen. importantly, only the
source side of these additional training examples
is synthetic, and the target side comes from the
monolingual corpus.

4 evaluation

we evaluate id4 training on parallel
and with additional monolingual data,
english   german
using training and test data
15
english   german,
turkish   english.

text,
on
turkish   english,
from wmt
for
for

iwslt 15
14

for english   german,

iwslt

and

and

the id4 system for

4.1 data and methods
we use groundhog3
the implementation
experiments
of
(bahdanau et al., 2015;
jean et al., 2015a).
we generally follow the settings and training
procedure described by sennrich et al. (2016).

all

as

for english   german, we report case-sensitive
id7 on detokenized text with mteval-v13a.pl for
comparison to of   cial wmt and iwslt results.
for turkish   english, we report case-sensitive
id7 on tokenized text with multi-id7.perl for
comparison to results by g  l  ehre et al. (2015).

g  l  ehre et al. (2015) determine the network
vocabulary based on the parallel training data,

dataset
wmtparallel
witparallel
wmtmono_de
wmtsynth_de
wmtmono_en
wmtsynth_en

sentences
4 200 000
200 000
160 000 000
3 600 000
118 000 000
4 200 000

table 1: english   german training data.

and replace out-of-vocabulary words with a spe-
cial unk symbol. they remove monolingual sen-
tences with more than 10% unk symbols. in con-
trast, we represent unseen words as sequences of
subword units (sennrich et al., 2016), and can rep-
resent any additional training data with the exist-
ing network vocabulary that was learned on the
parallel data. in all experiments, the network vo-
cabulary remains    xed.

4.1.1 english   german
we use all parallel training data provided by wmt
2015 (bojar et al., 2015)4. we use the news
crawl corpora as additional training data for the
experiments with monolingual data. the amount
of training data is shown in table 1.

baseline models are trained for a week. ensem-
bles are sampled from the last 4 saved models of
training (saved at 12h-intervals). each model is
   ne-tuned with    xed embeddings for 12 hours.

for the experiments with synthetic parallel
data, we back-translate a random sample of
3 600 000 sentences from the german monolin-
gual data set into english. the german   english
system used for
this is the baseline system
(parallel). translation took about a week on
an nvidia titan black gpu. for experiments
in german   english, we back-translate 4 200 000
monolingual english sentences into german, us-
ing the english   german system +synthetic.
note that we always use single models for back-
translation, not ensembles. we leave it to fu-
ture work to explore how sensitive id4 training
with synthetic data is to the quality of the back-
translation.

the

truecase
training
tokenize
and
we
rare words via bpe
data,
and represent
(sennrich et al., 2016).
speci   cally, we fol-
low sennrich et al. (2016) in performing bpe on
the joint vocabulary with 89 500 merge operations.

3github.com/sebastien-j/lv_groundhog

4http://www.statmt.org/wmt15/

dataset
wit
setimes
gigawordmono
gigawordsynth

sentences
160 000
160 000
177 000 000
3 200 000

table 2: turkish   english training data.

the network vocabulary size is 90 000.

we also perform experiments on the iwslt
15 test sets to investigate a cross-domain setting.5
the test sets consist of ted talk transcripts. as in-
domain training data, iwslt provides the wit3
parallel corpus (cettolo et al., 2012), which also
consists of ted talks.

4.1.2 turkish   english
we use data provided for the iwslt 14 machine
translation track (cettolo et al., 2014), namely the
wit3 parallel corpus (cettolo et al., 2012), which
consists of ted talks, and the setimes corpus
(tyers and alperen, 2010).6 after removal of sen-
tence pairs which contain empty lines or lines with
a length ratio above 9, we retain 320 000 sen-
tence pairs of training data. for the experiments
with monolingual training data, we use the en-
glish ldc gigaword corpus (fifth edition). the
amount of training data is shown in table 2. with
only 320 000 sentences of parallel data available
for training, this is a much lower-resourced trans-
lation setting than english   german.

g  l  ehre et al. (2015) segment the turkish text
with the morphology tool zemberek, followed by
a disambiguation of the morphological analysis
(sak et al., 2007), and removal of non-surface to-
kens produced by the analysis. we use the same
preprocessing7. for both turkish and english,
we represent rare words (or morphemes in the
case of turkish) as character bigram sequences
(sennrich et al., 2016). the 20 000 most frequent
words (morphemes) are left unsegmented. the
networks have a vocabulary size of 23 000 sym-
bols.
to

training
set, we back-translate a random sample of
3 200 000 sentences from gigaword. we use an
english   turkish id4 system trained with the
same settings as the turkish   english baseline
system.

synthetic

parallel

obtain

a

5http://workshop2015.iwslt.org/
6http://workshop2014.iwslt.org/
7github.com/orhanf/zemberekmorphtr

we found over   tting to be a bigger problem
than with the larger english   german data
set, and follow g  l  ehre et al. (2015) in using
(graves, 2011),
gaussian noise (stddev 0.01)
and dropout on the output
layer
(p=0.5)
(hinton et al., 2012). we also use early stop-
ping, based on id7 measured every three hours
on tst2010, which we treat as development set. for
turkish   english, we use gradient clipping with
threshold 5,
following g  l  ehre et al. (2015),
in contrast to the threshold 1 that we use for
english   german, following jean et al. (2015a).

4.2 results
4.2.1 english   german wmt 15
table 3 shows english   german results with
wmt training and test data. we    nd that mixing
parallel training data with monolingual data with a
dummy source side in a ratio of 1-1 improves qual-
ity by 0.4   0.5 id7 for the single system, 1 id7
for the ensemble. we train the system for twice
as long as the baseline to provide the training al-
gorithm with a similar amount of parallel training
instances. to ensure that the quality improvement
is due to the monolingual training instances, and
not just increased training time, we also continued
training our baseline system for another week, but
saw no improvements in id7.

including synthetic data during training is
very effective, and yields an improvement over
our baseline by 2.8   3.4 id7.
our best
ensemble system also outperforms a syntax-
based baseline (sennrich and haddow, 2015) by
1.2   2.1 id7. we also substantially outper-
form id4 results reported by jean et al. (2015a)
and luong et al. (2015), who previously reported
sota result.8 we note that the difference is par-
ticularly large for single systems, since our ensem-
ble is not as diverse as that of luong et al. (2015),
who used 8 independently trained ensemble com-
ponents, whereas we sampled 4 ensemble compo-
nents from the same training run.

4.2.2 english   german iwslt 15
table 4 shows english   german results on
iwslt test sets. iwslt test sets consist of ted
talks, and are thus very dissimilar from the wmt

8luong et al. (2015) report 20.9 id7 (tokenized) on
newstest2014 with a single model, and 23.0 id7 with an
ensemble of 8 models. our best single system achieves a to-
kenized id7 (as opposed to untokenized scores reported in
table 3) of 23.8, and our ensemble reaches 25.0 id7.

id7

name

training instances

syntax-based (sennrich and haddow, 2015)
neural mt (jean et al., 2015b)
parallel
+monolingual
+synthetic

37m (parallel)

44m (parallel) / 36m (synthetic)

49m (parallel) / 49m (monolingual)

newstest2014
single
ens-4
22.6

-
-

20.4
21.4
23.8

-

19.9
20.4
22.7

newstest2015
single
ens-4
24.4
22.4
22.8
23.2
25.7

23.6
24.6
26.5

-
-

table 3: english   german translation performance (id7) on wmt training/test sets. ens-4: ensemble
of 4 models. number of training instances varies due to differences in training time and speed.

name

   ne-tuning

data

instances

tst2013

id7
tst2014

tst2015

id4 (luong and manning, 2015) (single model)
id4 (luong and manning, 2015) (ensemble of 8)
parallel

-
-

-
-

2+witmono_de wmtparallel / witmono
2+witsynth_de witsynth
2+witparallel wit

200k/200k

200k
200k

1
2 +synthetic
3
4
5

29.4
31.4
25.2
26.5
26.6
28.2
30.4

-

27.6
22.6
23.5
23.6
24.4
25.9

-

30.1
24.0
25.5
25.4
26.7
28.4

table 4: english   german translation performance (id7) on iwslt test sets (ted talks). single
models.

test sets, which are news texts. we investigate if
monolingual training data is especially valuable if
it can be used to adapt a model to a new genre or
domain, speci   cally adapting a system trained on
wmt data to translating ted talks.

systems 1 and 2 correspond to systems in table
3, trained only on wmt data. system 2, trained on
parallel and synthetic wmt data, obtains a id7
score of 25.5 on tst2015. we observe that even a
small amount of    ne-tuning9, i.e. continued train-
ing of an existing model, on wit data can adapt
a system trained on wmt data to the ted do-
main. by back-translating the monolingual wit
corpus (using a german   english system trained
on wmt data, i.e. without in-domain knowledge),
we obtain the synthetic data set witsynth. a sin-
gle epoch of    ne-tuning on witsynth (system 4) re-
sults in a id7 score of 26.7 on tst2015, or an im-
provement of 1.2 id7. we observed no improve-
ment from    ne-tuning on witmono, the monolin-
gual ted corpus with dummy input (system 3).

these adaptation experiments with monolin-
gual data are slightly arti   cial in that parallel train-
ing data is available. system 5, which is    ne-
tuned with the original wit training data, obtains
a id7 of 28.4 on tst2015, which is an improve-

9we leave the id27s    xed for    ne-tuning.

name
pbsmt (haddow et al., 2015)
id4 (g  l  ehre et al., 2015)
+shallow fusion
+deep fusion
parallel
+synthetic
+synthetic (ensemble of 4)

id7

2014
28.8
23.6
23.7
24.0
25.9
29.5
30.8

2015
29.3

-
-
-

26.7
30.4
31.6

table 5: german   english translation perfor-
mance (id7) on wmt training/test sets (new-
stest2014; newstest2015).

ment of 2.9 id7. while it is unsurprising that
in-domain parallel data is most valuable, we    nd
it encouraging that id4 id20 with
monolingual data is also possible, and effective,
since there are settings where only monolingual
in-domain data is available.

the best results published on this dataset are
by luong and manning (2015), obtained with an
ensemble of 8 independently trained models.
in
a comparison of single-model results, we outper-
form their model on tst2013 by 1 id7.

4.2.3 german   english wmt 15
results for german   english on the wmt 15
data sets are shown in table 5. like for the
reverse translation direction, we see substan-
tial improvements (3.6   3.7 id7) from adding
monolingual training data with synthetic source
sentences, which is substantially bigger
than
the improvement observed with deep fusion
(g  l  ehre et al., 2015); our ensemble outperforms
the previous state of the art on newstest2015 by
2.3 id7.

4.2.4 turkish   english iwslt 14
table 6 shows results for turkish   english. on
average, we see an improvement of 0.6 id7 on
the test sets from adding monolingual data with a
dummy source side in a 1-1 ratio10, although we
note a high variance between different test sets.

with synthetic training data (gigawordsynth), we
outperform the baseline by 2.7 id7 on average,
and also outperform results obtained via shallow
or deep fusion by g  l  ehre et al. (2015) by 0.5
id7 on average. to compare to what extent syn-
thetic data has a id173 effect, even without
novel training data, we also back-translate the tar-
get side of the parallel training text to obtain the
training corpus parallelsynth. mixing the original
parallel corpus with parallelsynth (ratio 1-1) gives
some improvement over the baseline (1.7 id7
on average), but the novel monolingual training
data (gigawordmono) gives higher improvements,
despite being out-of-domain in relation to the test
sets. we speculate that novel in-domain monolin-
gual data would lead to even higher improvements.

4.2.5 back-translation quality for synthetic

data

one question that our previous experiments leave
open is how the quality of the automatic back-
translation affects training with synthetic data. to
investigate this question, we back-translate the
same german monolingual corpus with three dif-
ferent german   english systems:

    with our baseline system and greedy decod-

ing

    with our baseline system and id125
(beam size 12). this is the same system used
for the experiments in table 3.

10we also experimented with higher ratios of monolingual

data, but this led to decreased id7 scores.

id7

de   en

en   de

back-translation
none
parallel (greedy)
parallel (beam 12)
synthetic (beam 12)
ensemble of 3
ensemble of 12

2015

-

22.3
25.0
28.3

-
-

2014
20.4
23.2
23.8
23.9
24.2
24.7

2015
23.6
26.0
26.5
26.6
27.0
27.6

table 7: english   german translation perfor-
mance (id7) on wmt training/test sets (new-
stest2014; newstest2015). systems differ in how
the synthetic training data is obtained. ensembles
of 4 models (unless speci   ed otherwise).

    with the german   english system that was
itself trained with synthetic data (beam size
12).

id7 scores of the german   english sys-
tems, and of the resulting english   german sys-
tems that are trained on the different back-
translations, are shown in table 7. the quality
of the german   english back-translation differs
substantially, with a difference of 6 id7 on new-
stest2015. regarding the english   german sys-
tems trained on the different synthetic corpora, we
   nd that the 6 id7 difference in back-translation
quality leads to a 0.6   0.7 id7 difference in
translation quality. this is balanced by the fact
that we can increase the speed of back-translation
by trading off some quality, for instance by reduc-
ing beam size, and we leave it to future research
to explore how much the amount of synthetic data
affects translation quality.

we also show results for an ensemble of 3 mod-
els (the best single model of each training run),
and 12 models (all 4 models of each training run).
thanks to the increased diversity of the ensemble
components, these ensembles outperform the en-
sembles of 4 models that were all sampled from
the same training run, and we obtain another im-
provement of 0.8   1.0 id7.

4.3 contrast to phrase-based smt
the back-translation of monolingual target data
into the source language to produce synthetic
parallel
text has been previously explored for
phrase-based smt (bertoldi and federico, 2009;
lambert et al., 2011). while our approach is tech-
nically similar, synthetic parallel data ful   lls novel

name

training

id7

data
baseline (g  l  ehre et al., 2015)
deep fusion (g  l  ehre et al., 2015)
baseline
parallelsynth
gigawordmono
gigawordsynth

parallel/parallelsynth
parallel/gigawordmono
parallel/gigawordsynth

parallel

instances

tst2011

tst2012

tst2013

tst2014

18.4
20.2
18.6
19.9
18.8
21.2

18.8
20.2
18.2
20.4
19.6
21.1

19.9
21.3
18.4
20.1
19.4
21.8

18.7
20.6
18.3
20.0
18.2
20.4

7.2m
6m/6m

7.6m/7.6m
8.4m/8.4m

table 6: turkish   english translation performance (tokenized id7) on iwslt test sets (ted talks).
single models. number of training instances varies due to early stopping.

system

parallel
+synthetic
pbsmt gain
id4 gain

id7

wmt iwslt
20.1
20.8
+0.7
+2.9

21.5
21.6
+0.1
+1.2

8:

smt

phrase-based

results
table
(english   german) on wmt test sets (aver-
age of newstest201{4,5}), and iwslt test sets
(average of tst201{3,4,5}), and average id7
gain from adding synthetic data for both pbsmt
and id4.

y
p
o
r
t
n
e
-
s
s
o
r
c

8

6

4

2

0

parallel (dev)
parallel (train)
parallelsynth (dev)
parallelsynth (train)
gigawordmono (dev)
gigawordmono (train)
gigawordsynth (dev)
gigawordsynth (train)

5

10

15

20

25

30

training time (training instances   106)

roles in id4.

to explore the relative effectiveness of back-
translated data
for phrase-based smt and
id4, we train two phrase-based smt systems
with moses
(koehn et al., 2007), using only
wmtparallel, or both wmtparallel and wmtsynth_de
for training the translation and reordering model.
both systems contain the same language model,
a 5-gram kneser-ney model trained on all avail-
able wmt data. we use the baseline features
described by haddow et al. (2015).
results are shown in table 8.

in phrase-
based smt, we    nd that the use of back-translated
training data has a moderate positive effect on
the wmt test sets (+0.7 id7), but not on the
iwslt test sets. this is in line with the ex-
pectation that the main effect of back-translated
data for phrase-based smt is id20
(bertoldi and federico, 2009). both the wmt test
sets and the news crawl corpora which we used
as monolingual data come from the same source,
a web crawl of newspaper articles.11 in contrast,
news crawl is out-of-domain for the iwslt test
sets.

11the wmt test sets are held-out from news crawl.

figure 1: turkish   english training and develop-
ment set (tst2010) cross-id178 as a function of
training time (number of training instances) for
different systems.

in contrast to phrase-based smt, which can
make use of monolingual data via the language
model, id4 has so far not been able to use mono-
lingual data to great effect, and without requir-
ing architectural changes. we    nd that the effect
of synthetic parallel data is not limited to domain
adaptation, and that even out-of-domain synthetic
data improves id4 quality, as in our evaluation
on iwslt. the fact that the synthetic data is more
effective on the wmt test sets (+2.9 id7) than
on the iwslt test sets (+1.2 id7) supports the
hypothesis that id20 contributes to
the effectiveness of adding synthetic data to id4
training.

it is an important    nding that back-translated
data, which is mainly effective for domain adapta-
tion in phrase-based smt, is more generally use-
ful in id4, and has positive effects that go beyond
id20. in the next section, we will in-
vestigate further reasons for its effectiveness.

wmtparallel (dev)
wmtparallel (train)
wmtsynth (dev)
wmtsynth (train)

system
parallel
+mono
+synthetic

produced attested natural
53.4% 74.9%
61.6% 84.6%
56.4% 82.5%

1078
994
1217

y
p
o
r
t
n
e
-
s
s
o
r
c

8

6

4

2

0

20

40

60

80

training time (training instances   106)

figure 2: english   german training and develop-
ment set (newstest2013) cross-id178 as a func-
tion of training time (number of training instances)
for different systems.

4.4 analysis

the model

trained on only parallel

we previously indicated that over   tting is a con-
cern with our baseline system, especially on small
data sets of several hundred thousand training
sentences, despite the id173 employed.
this over   tting is illustrated in figure 1, which
plots training and development set cross-id178
by training time for turkish   english models.
for comparability, we measure training set cross-
id178 for all models on the same random sam-
ple of the parallel
training set. we can see
that
train-
ing data quickly over   ts, while all three mono-
lingual data sets (parallelsynth, gigawordmono, or
gigawordsynth) delay over   tting, and give bet-
ter perplexity on the development set.
the
best development set cross-id178 is reached by
gigawordsynth.
2

for
english   german, comparing the system trained
on only parallel data and the system that includes
synthetic training data. since more training data is
available for english   german, there is no indi-
cation that over   tting happens during the    rst 40
million training instances (or 7 days of training);
while both systems obtain comparable training
set cross-entropies, the system with synthetic data
reaches a lower cross-id178 on the development
set. one explanation for this is the domain effect
discussed in the previous section.

cross-id178

figure

shows

a central theoretical expectation is that mono-
lingual target-side data improves the model   s    u-

table 9: number of words in system out-
put that do not occur in parallel training data
(countref = 1168), and proportion that is attested
in data, or natural according to native speaker.
english   german; newstest2015; ensemble sys-
tems.

ency, its ability to produce natural target-language
sentences. as a proxy to sentence-level    u-
ency, we investigate word-level    uency, specif-
ically words produced as sequences of subword
units, and whether id4 systems trained with ad-
ditional monolingual data produce more natural
words. for instance, the english   german sys-
tems translate the english phrase civil rights pro-
tections as a single compound, composed of three
subword units: b  rger|rechts|schutzes12, and we
analyze how many of these multi-unit words that
the translation systems produce are well-formed
german words.

we compare the number of words in the system
output for the newstest2015 test set which are pro-
duced via subword units, and that do not occur in
the parallel training corpus. we also count how
many of them are attested in the full monolingual
corpus or the reference translation, which we all
consider    natural   . additionally, the main authors,
a native speaker of german, annotated a random
subset (n = 100) of unattested words of each sys-
tem according to their naturalness13, distinguish-
ing between natural german words (or names)
such as literatur|klassen    literature classes   , and
nonsensical ones such as *as|best|atten (a miss-
spelling of astbestmatten    asbestos mats   ).

in the results (table 9), we see that the sys-
tems trained with additional monolingual or syn-
thetic data have a higher proportion of novel words
attested in the non-parallel data, and a higher
proportion that is deemed natural by our annota-
tor. this supports our expectation that additional
monolingual data improves the (word-level)    u-
ency of the id4 system.

12subword boundaries are marked with    |   .
13for the annotation, the words were blinded regarding the

system that produced them.

5 related work

to our knowledge,
the integration of mono-
lingual data for pure neural machine trans-
lation architectures was    rst
investigated by
(g  l  ehre et al., 2015), who train monolingual
language models independently, and then integrate
them during decoding through rescoring of the
beam (shallow fusion), or by adding the recur-
rent hidden state of the language model to the de-
coder state of the encoder-decoder network, with
an additional controller mechanism that controls
the magnitude of the lm signal (deep fusion). in
deep fusion, the controller parameters and output
parameters are tuned on further parallel training
data, but the language model parameters are    xed
during the    netuning stage.
jean et al. (2015b)
also report on experiments with reranking of id4
output with a 5-gram language model, but im-
provements are small (between 0.1   0.5 id7).

the production of synthetic parallel texts bears
resemblance to data augmentation techniques used
in id161, where datasets are often
augmented with rotated, scaled, or otherwise
distorted variants of the (limited) training set
(rowley et al., 1996).

another similar avenue of research is self-
training (mcclosky et al., 2006; schwenk, 2008).
the main difference is that self-training typically
refers to scenario where the training set is en-
hanced with training instances with arti   cially
produced output labels, whereas we start with
human-produced output (i.e. the translation), and
arti   cially produce an input. we expect that this
is more robust towards noise in the automatic
translation.
improving id4 with monolingual
source data, following similar work on phrase-
based smt (schwenk, 2008), remains possible fu-
ture work.
domain

networks
via continued training has been shown to
language models by
be effective for neural
(ter-sarkisov et al., 2015),
and in work par-
allel
translation models
(luong and manning, 2015). we are the    rst
to show that we can effectively adapt neural
translation models with monolingual data.

for neural

adaptation

to ours,

neural

of

6 conclusion

architecture. providing training examples with
dummy source context was successful to some ex-
tent, but we achieve substantial gains in all tasks,
and new sota results, via back-translation of
monolingual target data into the source language,
and treating this synthetic data as additional train-
ing data. we also show that small amounts of in-
domain monolingual data, back-translated into the
source language, can be effectively used for do-
main adaptation. in our analysis, we identi   ed do-
main adaptation effects, a reduction of over   tting,
and improved    uency as reasons for the effective-
ness of using monolingual data for training.

while our experiments did make use of mono-
lingual training data, we only used a small ran-
dom sample of the available data, especially for
the experiments with synthetic parallel data. it is
conceivable that larger synthetic data sets, or data
sets obtained via data selection, will provide big-
ger performance bene   ts.

because we do not change the neural net-
work architecture to integrate monolingual train-
ing data, our approach can be easily applied to
other id4 systems. we expect that the effective-
ness of our approach not only varies with the qual-
ity of the mt system used for back-translation, but
also depends on the amount (and similarity to the
test set) of available parallel and monolingual data,
and the extent of over   tting of the baseline model.
future work will explore the effectiveness of our
approach in more settings.

acknowledgments

the research presented in this publication was
conducted in cooperation with samsung elec-
tronics polska sp. z o.o. - samsung r&d in-
stitute poland.
this project received funding
from the european union   s horizon 2020 research
and innovation programme under grant agreement
645452 (qt21).

references

[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2015. neural machine
translation by jointly learning to align and trans-
late. in proceedings of the international conference
on learning representations (iclr).

in this paper, we propose two simple methods to
use monolingual training data during training of
id4 systems, with no changes to the network

[bertoldi and federico2009] nicola bertoldi and mar-
cello federico. 2009. id20 for sta-
tistical machine translation with monolingual re-
sources. in proceedings of the fourth workshop on

id151 statmt 09. associ-
ation for computational linguistics.

[bojar et al.2015] ond  rej bojar, rajen chatterjee,
christian federmann, barry haddow, matthias
huck, chris hokamp, philipp koehn, varvara
logacheva, christof monz, matteo negri, matt
post, carolina scarton, lucia specia, and marco
turchi. 2015. findings of the 2015 workshop on
id151. in proceedings of
the tenth workshop on statistical machine transla-
tion, pages 1   46, lisbon, portugal. association for
computational linguistics.

[brown et al.1990] p.f. brown, s.a. della pietra, v.j.
della pietra, f. jelinek, j.d. lafferty, r.l. mercer,
and p.s. roossin. 1990. a statistical approach
to machine translation. computational linguistics,
16(2):79   85.

[cettolo et al.2012] mauro cettolo, christian girardi,
and marcello federico. 2012. wit3: web inven-
tory of transcribed and translated talks.
in pro-
ceedings of the 16th conference of the european
association for machine translation (eamt), pages
261   268, trento, italy.

[cettolo et al.2014] mauro cettolo, jan niehues, se-
bastian st  ker, luisa bentivogli, and marcello fed-
erico. 2014. report on the 11th iwslt evaluation
campaign, iwslt 2014. in proceedings of the 11th
workshop on spoken language translation, pages
2   16, lake tahoe, ca, usa.

[cho et al.2014] kyunghyun cho, bart van merrien-
boer, caglar gulcehre, dzmitry bahdanau, fethi
bougares, holger schwenk, and yoshua bengio.
2014. learning phrase representations using id56
encoder   decoder for statistical machine transla-
tion.
in proceedings of the 2014 conference on
empirical methods in natural language processing
(emnlp), pages 1724   1734, doha, qatar. associa-
tion for computational linguistics.

[graves2011] alex graves.

2011. practical varia-
tional id136 for neural networks. in j. shawe-
taylor, r.s. zemel, p.l. bartlett, f. pereira, and
k.q. weinberger, editors, advances in neural in-
formation processing systems 24, pages 2348   2356.
curran associates, inc.

[g  l  ehre et al.2015]   aglar g  l  ehre, orhan firat,
kelvin xu, kyunghyun cho, lo  c barrault, huei-
chi lin, fethi bougares, holger schwenk, and
yoshua bengio.
2015. on using monolingual
corpora in id4. corr,
abs/1503.03535.

[haddow et al.2015] barry haddow, matthias huck,
alexandra birch, nikolay bogoychev, and philipp
koehn. 2015. the edinburgh/jhu phrase-based
machine translation systems for wmt 2015.
in
proceedings of the tenth workshop on statistical
machine translation, pages 126   133, lisbon, por-
tugal. association for computational linguistics.

[hinton et al.2012] geoffrey e. hinton, nitish srivas-
tava, alex krizhevsky, ilya sutskever, and rus-
lan salakhutdinov. 2012.
improving neural net-
works by preventing co-adaptation of feature detec-
tors. corr, abs/1207.0580.

[jean et al.2015a] s  bastien jean, kyunghyun cho,
roland memisevic, and yoshua bengio. 2015a. on
using very large target vocabulary for neural ma-
chine translation. in proceedings of the 53rd an-
nual meeting of the association for computational
linguistics and the 7th international joint confer-
ence on natural language processing (volume 1:
long papers), pages 1   10, beijing, china. associa-
tion for computational linguistics.

[jean et al.2015b] s  bastien

jean,

orhan

firat,
kyunghyun cho, roland memisevic, and yoshua
bengio. 2015b. montreal neural machine transla-
tion systems for wmt   15 . in proceedings of the
tenth workshop on id151,
pages 134   140, lisbon, portugal. association for
computational linguistics.

[koehn et al.2007] philipp koehn, hieu hoang,
alexandra birch, chris callison-burch, marcello
federico, nicola bertoldi, brooke cowan, wade
shen, christine moran, richard zens, chris dyer,
ond  rej bojar, alexandra constantin, and evan
herbst. 2007. moses: open source toolkit for
id151. in proceedings of
the acl-2007 demo and poster sessions, pages
177   180, prague, czech republic. association for
computational linguistics.

[lambert et al.2011] patrik lambert, holger schwenk,
christophe servan, and sadaf abdul-rauf. 2011.
investigations on translation model adaptation us-
ing monolingual data.
the
sixth workshop on id151,
pages 284   293, edinburgh, scotland. association
for computational linguistics.

in proceedings of

[luong and manning2015] minh-thang luong and
christopher d. manning. 2015. stanford neural
machine translation systems for spoken language
domains.
the international
workshop on spoken language translation 2015,
da nang, vietnam.

in proceedings of

[luong et al.2015] thang luong, hieu pham, and
christopher d. manning.
2015. effective ap-
proaches to attention-based neural machine trans-
lation.
in proceedings of the 2015 conference on
empirical methods in natural language process-
ing, pages 1412   1421, lisbon, portugal. associa-
tion for computational linguistics.

[mcclosky et al.2006] david mcclosky, eugene char-
niak, and mark johnson. 2006. effective self-
training for parsing.
in proceedings of the main
conference on human language technology con-
ference of the north american chapter of the asso-
ciation of computational linguistics, hlt-naacl
   06, pages 152   159, new york. association for
computational linguistics.

[rowley et al.1996] henry rowley, shumeet baluja,
and takeo kanade. 1996. neural network-based
face detection.
in id161 and pattern
recognition    96.

[sak et al.2007] ha  sim sak, tunga g  ng  r, and mu-
rat sara  lar. 2007. id60
of turkish text with id88 algorithm. in ci-
cling 2007, pages 107   118.

[schwenk2008] holger schwenk. 2008. investigations
on large-scale lightly-supervised training for sta-
tistical machine translation. in international work-
shop on spoken language translation, pages 182   
189.

[sennrich and haddow2015] rico sennrich and barry
haddow.
2015. a joint dependency model of
morphological and syntactic structure for statisti-
cal machine translation. in proceedings of the 2015
conference on empirical methods in natural lan-
guage processing, pages 2081   2087, lisbon, portu-
gal. association for computational linguistics.

[sennrich et al.2016] rico sennrich, barry haddow,
and alexandra birch. 2016. neural machine trans-
lation of rare words with subword units. in pro-
ceedings of the 54th annual meeting of the asso-
ciation for computational linguistics (acl 2016),
berlin, germany.

[sutskever et al.2014] ilya sutskever, oriol vinyals,
2014. sequence to sequence
and quoc v. le.
learning with neural networks.
in advances in
neural information processing systems 27: annual
conference on neural information processing sys-
tems 2014, pages 3104   3112, montreal, quebec,
canada.

[ter-sarkisov et al.2015] alex ter-sarkisov, holger
schwenk, fethi bougares, and lo  c barrault. 2015.
incremental adaptation strategies for neural net-
work language models.
in proceedings of the
3rd workshop on continuous vector space models
and their compositionality, pages 48   56, beijing,
china. association for computational linguistics.

[tyers and alperen2010] francis m. tyers and mu-
rat s. alperen. 2010. setimes: a parallel corpus
of balkan languages. in workshop on exploitation
of multilingual resources and tools for central and
(south) eastern european languages at the lan-
guage resources and evaluation conference, pages
1   5.

