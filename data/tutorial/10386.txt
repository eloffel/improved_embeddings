6
1
0
2

 
r
a

 

m
8
2

 
 
]
l
c
.
s
c
[
 
 

4
v
8
0
1
4
0

.

1
1
5
1
:
v
i
x
r
a

under review as a conference paper at iclr 2016

lstm-based deep learning models for non-
factoid answer selection

ming tan, cicero dos santos, bing xiang & bowen zhou
ibm watson core technologies
yorktown heights, ny, usa
{mingtan,cicerons,bingxia,zhou}@us.ibm.com

abstract

in this paper, we apply a general deep learning (dl) framework for the answer
selection task, which does not depend on manually de   ned features or linguistic
tools. the basic framework is to build the embeddings of questions and answers
based on bidirectional long short-term memory (bilstm) models, and measure
their closeness by cosine similarity. we further extend this basic model in two di-
rections. one direction is to de   ne a more composite representation for questions
and answers by combining convolutional neural network with the basic frame-
work. the other direction is to utilize a simple but ef   cient attention mechanism in
order to generate the answer representation according to the question context. sev-
eral variations of models are provided. the models are examined by two datasets,
including trec-qa and insuranceqa. experimental results demonstrate that the
proposed models substantially outperform several strong baselines.

1

introduction

the answer selection problem can be formulated as follows: given a question q and an answer
candidate pool {a1, a2,       , as} for this question, we aim to search for the best answer candidate
ak, where 1     k     s. an answer is a token sequence with an arbitrary length, and a question can
correspond to multiple ground-truth answers. in testing, the candidate answers for a question may
not be observed in the training phase. answer selection is one of the essential components in typical
id53 (qa) systems. it is also a stand-alone task with applications in knowledge base
construction and information extraction.
the major challenge of this task is that the correct answer might not directly share lexical units with
the question. instead, they may only be semantically related. moreover, the answers are sometimes
noisy and contain a large amount of unrelated information.
recently, deep learning models have obtained a signi   cant success on various natural language
processing tasks, such as semantic analysis (tang et al., 2015), machine translation (bahdanau et al.,
2015) and text summarization (rush et al., 2015).
in this paper, we propose a deep learning framework for answer selection which does not require any
feature engineering, linguistic tools, or external resources. this framework is based on building bi-
directional long short term memory (bilstm) models on both questions and answers respectively,
connecting with a pooling layer and utilizing a similarity metric to measure the matching degree.
we improve this basic model from two perspectives. firstly, a simple pooling layer may suffer from
the incapability of keeping the local linguistic information. in order to obtain better embeddings
for the questions and answers, we build a convolutional neural network (id98) structure on top
of bilstm. secondly, in order to better distinguish candidate answers according to the question,
we introduce a simple but ef   cient attention model to this framework for the answer embedding
generation according to the question context.
we report experimental results for two answer selection datasets: (1) insuranceqa (feng et al.,
2015) 1, a recently released large-scale non-factoid qa dataset from the insurance domain. the

1git clone https://github.com/shuzi/insuranceqa.git

1

under review as a conference paper at iclr 2016

proposed models demonstrate a signi   cant out-performance compared to two non-dl baselines and
a strong dl baseline based on id98. (2) trec-qa 2, which was created by wang et al. (2007)
based on text retrieval conference (trec) qa track data. the proposed models outperform
various strong baselines.
the rest of the paper is organized as follows: section 2 describes the related work for answer
selection; section 3 provides the details of the proposed models; experimental settings and results
of insuranceqa and trec-qa datasets are discussed in section 4 and 5 respectively; finally, we
draw conclusions in section 6.

2 related work

previous work on answer selection normally used feature engineering, linguistic tools, or external
resources. for example, semantic features were constructed based on id138 in (yih et al., 2013).
this model pairs semantically related words based on word semantic relations. in (wang & man-
ning, 2010; wang et al., 2007), the answer selection problem is transformed to a syntactical matching
between the question/answer parse trees. some work tried to ful   ll the matching using minimal edit
sequences between dependency parse trees (heilman & smith, 2010; yao et al., 2013). recently,
discriminative tree-edit features extraction and engineering over parsing trees were automated in
(severyn & moschitti, 2013).
while these methods show effectiveness, they might suffer from the availability of additional re-
sources, the effort of feature engineering and the systematic complexity by introducing linguistic
tools, such as parse trees and dependency trees.
there were prior methods using deep learning technologies for the answer selection task. the ap-
proaches for non-factoid id53 generally pursue the solution on the following direc-
tions: firstly, the question and answer representations are learned and matched by certain similarity
metrics (feng et al., 2015; yu et al., 2014; dos santos et al., 2015). secondly, a joint feature vector
is constructed based on both the question and the answer, and then the task can be converted into a
classi   cation or learning-to-rank problem (wang & nyberg, 2015). finally, recently proposed mod-
els for textual generation can intrinsically be used for answer selection and generation (bahdanau
et al., 2015; vinyals & le, 2015).
the framework proposed in this work belongs to the    rst category. there are two major differences
between our approaches and the work in (feng et al., 2015): (1) the architectures developed in
(feng et al., 2015) are only based on id98, whereas our models are based on bidirectional lstms,
which are more capable of exploiting long-range sequential context information. moreover, we also
integrate the id98 structures on the top of bilstm for better performance. (2) feng et al. (2015)
tackle the question and answer independently, while the proposed structures develop an ef   cient
attentive models to generate answer embeddings according to the question.

3 approach

in this section, we describe the proposed framework and its variations. we    rst introduce the general
framework, which is to build bi-directional lstm on both questions and their answer candidates,
and then use the similarity metric to measure the distance of question answer pairs. in the following
two subsections, we extend the basic model in two independent directions.

3.1 basic model: qa-lstm

long short-term memory (lstm): recurrent neural networks (id56) have been widely ex-
ploited to deal with variable-length sequence input. the long-distance history is stored in a recurrent
hidden vector which is dependent on the immediate previous hidden vector. lstm (hochreiter &
schmidhuber, 1997) is one of the popular variations of id56 to mitigate the gradient vanish prob-
lem of id56. our lstm implementation is similar to the one in (graves et al., 2013) with minor

2the data is obtained from (yao et al., 2013) http://cs.jhu.edu/  xuchen/packages/

jacana-qa-naacl2013-data-results.tar.bz2

2

under review as a conference paper at iclr 2016

modi   cation. given an input sequence x = {x(1), x(2),       , x(n)}, where x(t) is an e-dimension
word vector in this paper. the hidden vector h(t) ( the size is h ) at the time step t is updated as
follows.

it =   (wix(t) + uih(t     1) + bi)
ft =   (wf x(t) + uf h(t     1) + bf )
ot =   (wox(t) + uoh(t     1) + bo)
  ct = tanh(wcx(t) + uch(t     1) + bc)
ct = it       ct + ft     ct   1
ht = ot     tanh(ct)

(1)
(2)
(3)
(4)
(5)
(6)

in the lstm architecture, there are three gates (input i, forget f and output o), and a cell memory
vector c.    is the sigmoid function. the input gate can determine how incoming vectors xt alter
the state of the memory cell. the output gate can allow the memory cell to have an effect on the
outputs. finally, the forget gate allows the cell to remember or forget its previous state. w     rh  e,
u     rh  h and b     rh  1 are the network parameters.
bidirectional long short-term memory (bilstm): single direction lstms suffer a weakness
of not utilizing the contextual information from the future tokens. bidirectional lstm utilizes both
the previous and future context by processing the sequence on two directions, and generate two
independent sequences of lstm output vectors. one processes the input sequence in the forward
direction, while the other processes the input in the reverse direction. the output at each time step
is the concatenation of the two output vectors from both directions, ie. ht =
qa-lstm: the basic model in this work is shown in figure 1. bilstm generates distributed
representations for both the question and answer independently, and then utilize cosine similarity to
measure their distance. following the same ranking loss in (feng et al., 2015; weston et al., 2014;
hu et al., 2014), we de   ne the training objective as a hinge loss.

      
ht (cid:107)       
ht.

l = max{0, m     cosine(q, a+) + cosine(q, a   )}

(7)
where a+ is a ground truth answer, a    is an incorrect answer randomly chosen from the entire
answer space, and m is constant margin. we treat any question with more than one ground truth as
multiple training examples, each for one ground truth.
there are three simple ways to generate representations for questions and answers based on the
word-level bilstm outputs: (1) average pooling; (2) max pooling; (3) the concatenation of the last
vectors on both directions. the three strategies are compared with the experimental performance
in section 5. dropout operation is performed on the qa representations before cosine similarity
matching.
finally, from preliminary experiments, we observe that the architectures, in which both question and
answer sides share the same network parameters, is signi   cantly better than the one that the question
and answer sides own their own parameters separately, and converges much faster. as discussed
in (feng et al., 2015), this is reasonable, because for a shared layer network, the corresponding
elements in question and answer vectors represent the same bilstm outputs. while for the network
with separate question and answer parameters, there is no such constraint and the model has double-
sized parameters, making it dif   cult to learn for the optimizer.

figure 1: basic model: qa-lstm

3

cosinequestionansweroqoamean/maxpoolingmean/maxpoolingbilstmbilstmunder review as a conference paper at iclr 2016

figure 2: qa-lstm/id98

3.2 qa-lstm/id98

in the previous subsection, we generate the question and answer representations only by simple
operations, such as max or mean pooling. in this subsection, we resort to a id98 structure built on
the outputs of bilstm, in order to give a more composite representation of questions and answers.
the structure of id98 in this work is similar to the one in (feng et al., 2015), as shown in figure 2.
unlike the traditional forward neural network, where each output is interactive with each input, the
convolutional structure only imposes local interactions between the inputs within a    lter size m.
in this work, for every window with the size of m in bilstm output vectors, ie. hm(t) =
[h(t), h(t + 1),       , h(t + m     1)], where t is a certain time step, the convolutional    lter f =
[f(0)       f(m     1)] will generate one value as follows.

of (t) = tanh(cid:34)(cid:32)m   1(cid:88)i=0

h(t + i)t f(i)(cid:33) +b(cid:35)

(8)

where b is a bias, and f and b are the parameters of this single    lter.
same as typical id98s, a max-k pooling layer is built on the top of the convolutional layer. intu-
itively, we want to emphasize the top-k values from each convolutional    lter. by k-maxpooling, the
maximum k values will be kept for one    lter, which indicate the highest degree that a    lter matches
the input sequence.
finally, there are n parallel    lters, with different parameter initialization, and the convolutional
layer gets n-dimension output vectors. we get two output vectors with dimension of kn for the
questions and answers respectively. in this work, k = 1. k > 1 did not show any obvious improve-
ment in our early experiments. the intuition of this structure is, instead of evenly considering the
lexical information of each token as the previous subsection, we emphasize on certain parts of the
answer, such that qa-lstm/id98 can more effectively differentiate the ground truths and incorrect
answers.

3.3 attention-based qa-lstm

in the previous subsection, we described one extension from the basic model, which targets at pro-
viding more composite embeddings for questions and answers respectively. in this subsection, we
investigate an extension from another perspective. instead of generating qa representation indepen-
dently, we leverage a simple attention model for the answer vector generation based on questions.
the    xed width of hidden vectors becomes a bottleneck, when the bidirectional lstm models must
propagate dependencies over long distances over the questions and answers. an attention mech-
anism are used to alleviate this weakness by dynamically aligning the more informative parts of
answers to the questions. this strategy has been used in many other natural language processing
tasks, such as machine translation (bahdanau et al., 2015; sutskever et al., 2014), sentence summa-
rization (rush et al., 2015) and factoid id53 (hermann et al., 2015; sukhbaatar et al.,
2015).

4

cosinequestionansweroqoaoutputlayeroutputlayerconvolutionalfiltersconvolutionalfiltersbilstmbilstmmax-1poolingmax-1poolingunder review as a conference paper at iclr 2016

# of qs
# of as

train validation test1 test2
1800
12887
18540
2593

1800
2616

1000
1454

table 1: numbers of questions and answers of insuranceqa.

inspired by the work in (hermann et al., 2015), we develop a very simple but ef   cient word-level
attention on the basic model. figure 3 shows the structure. prior to the average or mean pooling,
each bilstm output vector will be multiplied by a softmax weight, which is determined by the
question embedding from bilstm.
speci   cally, given the output vector of bilstm on the answer side at time step t, ha(t), and the

question embedding, oq, the updated vector(cid:101)ha(t) for each answer token are formulated below.

msma,q(t))

(cid:101)ha(t) = ha(t)sa,q(t)

ma,q(t) = tanh(wamha(t) + wqmoq)
sa,q(t)     exp(wt

(9)
(10)
(11)
where wam, wqm and wms are attention parameters. conceptually, the attention mechanism give
more weights on certain words, just like tf-idf for each word. however, the former computes the
weights according to question information.
the major difference between this approach and the one in (hermann et al., 2015) is that hermann
et al. (2015)   s attentive reader emphasizes the informative part of supporting facts, and then uses a
combined embedding of the query and the supporting facts to predict the factoid answers. in this
work, we directly use the attention-based representations to measure the question/answer distances.
experiments indicate the attention mechanism can more ef   ciently distinguish correct answers from
incorrect ones according to the question text.

3.4 qa-lstm/id98 with attention

the two extensions introduced previously are combined in a simple manner. first, the bilstm hid-
den vectors of answers ha(t) are multiplied by sa,q(t), which is computed from the question average

pooling vectors oq, and updated to(cid:101)ha(t), illustrated in eq. 9-11. then, the original question and

updated answer hidden vectors serve as inputs of id98 structure respectively, such that the question
context can be used to evaluate the softmax weights of the input of id98. from the experiments, we
observe that the two extensions vary on their contributions on the performance improvement accord-
ing to different datasets. however, qa-lstm/id98 with attention can outperform the baselines on
both datasets.

insuranceqa experiments

4
having described a number of models in the previous section, we evaluate the proposed approaches
on the insurance domain dataset, insuranceqa, provided by feng et al. (2015). the insuranceqa
dataset provides a training set, a validation set, and two test sets. we do not see obvious categorical
differentiation between two tests    questions. one can see the details of insuranceqa data in (feng

figure 3: qa-lstm with attention

5

cosinequestionansweroqoaeha(t)=ha(t)sa,q(t)mean/maxpoolingmean/maxpoolingwithattentionbilstmbilstmunder review as a conference paper at iclr 2016

a. bag-of-word
b. metzler-bendersky ir model
c. architecture-ii in (feng et al., 2015)
d. architecture-ii with gesd

validation test1 test2
32.2
31.9
50.8
52.7
59.2
61.8
65.4
61.0

32.1
55.1
62.8
65.3

table 2: baseline results of insuranceqa

et al., 2015). we list the numbers of questions and answers of the dataset in table 1. a question
may correspond to multiple answers. the questions are much shorter than answers. the average
length of questions is 7, and the average length of answers is 94. the long answers comparing to the
questions post challenges for answer selection task. this corpus contains 24981 unique answers in
total. for the development and test sets, the dataset also includes an answer pool of 500 candidate
answers for each question. these answer pools were constructed by including the correct answer(s)
and randomly selecting candidate from the complete set of unique answers. the top-1 accuracy of
the answer pool is reported.

4.1 setup

the models in this work are implemented with theano (bastien et al., 2012) from scratch, and all
experiments are processed in a gpu cluster. we use the accuracy on validation set to locate the best
epoch and best hyper-parameter settings for testing.
the id27 is trained by id97 (mikolov et al., 2013), and the word vector size is 100.
id27s are also parameters and are optimized as well during the training. stochastic
id119 (sgd) is the optimization strategy. we tried different margin values, such as
0.05, 0.1 and 0.2, and    nally    xed the margin as 0.2. we also tried to include l2 norm in the
training objective. however, preliminary experiments show that id173 factors do not show
any improvements. also, the dimension of lstm output vectors is 141 for one direction, such that
bilstm has a comparable number of parameters with a single-direction lstm with 200 dimension.
we train our models in mini-batches (the batch size b is 20), and the maximum length l of questions
and answers is 200. any tokens out of this range will be discarded. because the questions or answers
within a mini-batch may have different lengths, we resort to a mask matrix m     rb  l to indicate
the real length of each token sequence.

4.2 baselines

for comparison, we report the performances of four baselines in table 2: two state-of-the-art non-
dl approaches and two variations of a strong dl approach based on id98 as follows.
bag-of-word: the idf-weighted sum of word vectors for the question and for all of its answer
candidates is used as a feature vector. similar to this work, the candidates are re-ranked according
the cosine similarity to a question.
metzler-bendersky ir model: a state-of-the-art weighted dependency (wd) model, which em-
ploys a weighted combination of term-based and term proximity-based ranking features to score
each candidate answer.
architecture-ii in (feng et al., 2015): instead of using lstm, a id98 model is employed to learn
a distributed vector representation of a given question and its answer candidates, and the answers
are scored by cosine similarity with the question. no attention model is used in this baseline.
architecture-ii with geometricmean of euclidean and sigmoid dot product (gesd): gesd
is used to measure the distance between the question and answers. this is the model which achieved
the best performance in (feng et al., 2015).

4.3 results and discussions

in this section, detailed analysis on experimental results are given. table 3 summarizes the results of
our models on insuranceqa. from row (a) to (c), we list qa-lstm without either id98 structure

6

under review as a conference paper at iclr 2016

model

a qa-lstm basic-model(head/tail)
b qa-lstm basic-model(avg pooling)
c qa-lstm basic-model(max pooling)
d qa-lstm/id98(fcount=1000)
e qa-lstm/id98(fcount=2000)
f qa-lstm/id98(fcount=4000)
g qa-lstm with attention (max pooling)
h qa-lstm with attention (avg pooling)
i

qa-lstm/id98 (fcount=4000) with attention

validation test1 test2
51.2
54.0
54.0
58.5
58.0
64.3
65.5
62.3
62.6
64.8
62.2
66.2
60.3
66.5
68.4
62.2
63.3
67.2

53.1
58.2
63.1
65.9
66.8
64.6
63.7
68.1
65.7

table 3: the experimental results of insuranceqa for qa-lstm, qa-lstm/id98 and qa-lstm
with attentions

or attention model. they vary on how to utilize the bilstm output vectors to form sentential em-
beddings for questions and answers in shown in section 3.1. we can observe that just concatenating
of the last vectors from both direction (a) performs the worst. it is surprised to see using max-
pooling (c) is much better than average pooling (b). the potential reason is that the max-pooling
extracts more local values for each dimension, so that more local information can be re   ected on the
output embeddings.
from row (d) to (f), id98 layers are built on the top of the bilstm with different    lter numbers.
we set the    lter width m = 2, and we did not see better performance if we increase m to 3 or
4. row (f) with 4000    lters gets the best validation accuracy, obtained a comparable performance
with the best baseline (row (d) in table 2 ). row f shared a highly analogous id98 structure with
architecture ii in (feng et al., 2015), except that the later used a shallow hidden layer to transform
the id27s into the input of id98 structure, while row f take the output of bilstm as
id98 input.
row (g) and (h) corresponds to qa-lstm with the attention model.
(g) connects the output
vectors of answers after attention with a max pooling layer, and (h) with an average pooling. in
comparison to model (c), model (g) shows over 2% improvement on both validation and test2 sets.
with respect to the model with mean pooling layers (b), the improvement from attention is more
remarkable. model (h) is over 8% higher on all datasets compared to (b), and gets improvements
from the best baseline by 3%, 2.8% and 1.2% on the validation, test1 and test2 sets, respectively.
compared to architecture ii in (feng et al., 2015), which involved a large number of id98    lters,
(h) model also has fewer parameters.
row (i) corresponds to section 3.4, where id98 and attention mechanism are combined. although
compared to (f), it shows 1% improvement on all sets, we fail to see obvious improvements com-
pared to model (h). although model (i) achieves better number on test2, but does not on validation
and test1. we assume that the effective attention might have vanished during the id98 operations.
however, both (h) and (i) outperform all baselines.
we also investigate the proposed models on how they perform with respect to long answers. we
divide the questions of test1 and test2 sets into eleven buckets, according to the average length of
their ground truths. in the table of figure 4, we list the bucket levels and the number of questions
which belong to each bucket, for example, test1 has 165 questions, whose average ground truth
lengths are 55 < l     60. we select models of (c), (f), (h) and (i) in table 3 for comparison.
model (c) is without attention and sentential embeddings are formed only by max pooling. model
(f) utilizes id98, while model (h) and (i) integrate attention. as shown in the left    gure in figure
4, (c) gets better or close performance compared to other models on buckets with shorter answers
(    50,    55,    60). however, as the ground lengths increase, the gap between (c) and other models
becomes more obvious. the similar phenomenon is also observed in the right    gure for test2. this
suggests the effectiveness of the two extensions from the basic model of qa-lstm, especially for
long-answer questions.
feng et al. (2015) report that gesd outperforms cosine similarity in their models. however, the
proposed models with gesd as similarity scores do not provide any improvement on accuracy.

7

under review as a conference paper at iclr 2016

buckets    50    55    60    65    70    80    90    100    120    160 >160
156
test1
test2
142

147
115

191
170

121
105

165
192

152
168

167
191

137
169

161
153

223
230

180
165

figure 4: the accuracy of test1 and test2 of insuranceqa sets for the four models (c, h, f and i
in table 3 ), on different levels of ground truth answer lengths. the table divided each test set into
11 buckets. the    gures above show the accuracy of each bucket.

models
wang et al. (2007)
heilman & smith (2010)
wang & manning (2010)
yao et al. (2013)
severyn & moschitti (2013)
yih et al. (2013)-bdt
yih et al. (2013)-lclr
wang & nyberg (2015)
architecture-ii (feng et al., 2015)

map mrr
0.6852
0.6029
0.6091
0.6917
0.6852
0.6029
0.7477
0.6307
0.7358
0.6781
0.7894
0.6940
0.7092
0.7700
0.7134
0.7913
0.7998
0.7106

table 4: test results of baselines on trec-qa

finally, we replace the cosine similarity with a mlp structure, whose input (282x2-dimension) is
the concatenation of question and answer embeddings, and the output is a single similarity score
and test the modi   ed models by a variety of hidden layer size (100,500,1000). we observe that the
modi   ed models not only get >10% accuracy decrease, but also converge much slower. one possi-
ble explanation is the involvement of more network parameters by mlp makes it more dif   cult for
training, although we believed that mlp might partially avoid the conceptual challenge of projecting
questions and answers in the same high-dimensional space, introduced by cosine similarity.

5 trec-qa experiments
in this section we detail our experimental setup and results using the trec-qa dataset.

5.1 data, metrics and baselines

in this paper, we adopt trec-qa, created by wang et al. (2007) based on text retrieval conference
(trec) qa track (8-13) data. we follow the exact approach of train/dev/test questions selection in

models

a qa-lstm (avg-pool)
b qa-lstm with attention
c qa-lstm/id98
d qa-lstm/id98 with attention
qa-lstm/id98 with attention
e
(lstm hiddenvector=500)

map mrr
76.52
68.19
78.49
68.96
70.61
81.04
83.22
71.11
72.79
82.40

table 5: test results of the proposed models on trec-qa

8

under review as a conference paper at iclr 2016

(wang & nyberg, 2015), in which all questions with only positive or negative answers are removed.
finally, we have 1162 training questions, 65 development questions and 68 test questions.
following previous work on this task, we use mean average precision (map) and mean reciprocal
rank (mrr) as id74, which are calculated using the of   cial evaluation scripts.
in table 4, we list the performance of some prior work on this dataset, which can be referred to
(wang & nyberg, 2015). we implemented the architecture ii in (feng et al., 2015) from scratch.
wang & nyberg (2015) and feng et al. (2015) are the best baselines on map and mrr respectively.

5.2 setup

we keep the con   gurations same as those in insuranceqa in section 4.1, except the following differ-
ences: first, we set the minibatch size as 10; second, we set the maximum length of questions and
answers as 40 instead of 200. third, following (wang & nyberg, 2015), we use 300-dimensional
vectors that were trained and provided by id97 3. finally, we use the models from the epoch
with the best map on the validation set for training. moreover, although trec-qa dataset provided
negative answer candidates for each training question, we randomly select the negative answers from
all the candidate answers in the training set.

5.3 results

table 5 shows the performance of the proposed models. compared to model (a), which is with
average pooling on top of bilstm but without attention, model (b) with attention improves map
by 0.7% and mrr by approximately 2%. the combination of id98 with qa-lstm (model-c)
gives greater improvement on both map and mrr from model (a). model (d), which combines
the ideas of model (b) and (c), achieves the performance, competitive to the best baselines on map,
and 2   4% improvement on mrr compared to (wang & nyberg, 2015) and (feng et al., 2015).
finally, model (e), which corresponds to the same model (d) but uses a lstm hidden vector size
of 500, achieves the best results for both metrics and outperforms the baselines.

6 conclusion

in this paper, we study the answer selection task by employing a bidirectional-lstm based deep
learning framework. the proposed framework does not rely on feature engineering, linguistic tools
or external resources, and can be applied to any domain. we further extended the basic framework
on two directions. firstly, we combine a convolutional neural network into this framework, in
order to give more composite representations for questions and answers. secondly, we integrate a
simple but ef   cient attention mechanism in the generation of answer embeddings according to the
question. finally, two extensions combined together. we conduct experiments using the trec-
qa dataset and the recently published insuranceqa dataset. our experimental results demonstrate
that the proposed models outperform a variety of strong baselines. in the future, we would like to
further evaluate the proposed approaches for different tasks, such as answer quality prediction in
community qa and recognizing id123. with respect to the structural perspective, we
plan to generate the attention mechanism to phrasal or sentential levels.

references
bahdanau, dzmitry, cho, kyunghyun, and bengio, yoshua. id4 by jointly
learning to align and translate. proceedings of international conference of learning representa-
tions, 2015.

bastien, frederic, lamblin, pascal, pascanu, razvan, bergstra, james, goodfellow, ian j., bergeron,
arnaud, bouchard, nicolas, and bengio, yoshua. theano: new features and speed improvements.
deep learning and unsupervised id171 nips 2012 workshop, 2012.

3https://code.google.com/p/id97/

9

under review as a conference paper at iclr 2016

dos santos, cicero, barbosa, luciano, bogdanova, dasha, and zadrozny, bianca. learning hybrid
representations to retrieve semantically equivalent questions. in proceedings of acl, pp. 694   
699, beijing, china, july 2015.

feng, minwei, xiang, bing, glass, michael, wang, lidan, and zhou, bowen. applying deep
ieee automatic id103

learning to answer selection: a study and an open task.
and understanding workshop (asru), 2015.

graves, alex, mohamed, abdel-rahman, and hinton, geoffrey. id103 with deep re-
in ieee international conference on acoustics, speech and signal

current neural networks.
processing (icassp), 2013.

heilman, michael and smith, noah a. tree edit models for recognizing id123s, para-
phrases, and answers to questions. annual conference of the north american chapter of the
association for computational linguistics. association for computational linguistics (naacl),
2010.

hermann, karl moritz, kocisky, tomas, grefenstette, edward, espeholt, lasse, kay, will, suley-
man, mustafa, and blunsom, phil. teaching machines to read and comprehend. in advances in
neural information processing systems (nips), 2015.

hochreiter, sepp and schmidhuber, jurgen. long short-term memory. neural computation, 1997.

hu, baotian, lu, zhengdong, li, hang, and chen, qingcai. convolutional neural network archi-
tectures for matching natural language sentences. advances in neural information processing
systems (nips), 2014.

mikolov, tomas, sutskever, ilya, chen, kai, corrado, greg s., and dean, jeff. distributed rep-
resentations of words and phrases and their compositionality. advances in neural information
processing systems (nips), 2013.

rush, alexander, chopra, sumit, and weston, jason. a neural attention model for sentence sum-
marization. proceedings of the 2015 conference on empirical methods in natural language
processing (emnlp), 2015.

severyn, aliaksei and moschitti, alessandro. automatic feature engineering for answer selection
and extraction. in proceedings of conference on empirical methods in natural language pro-
cessing (emnlp), 2013.

sukhbaatar, sainbayar, szlam, arthur, weston, jason, and fergus, rob. end-to-end memory net-

works. arxiv preprint arxiv:1503.08895, 2015.

sutskever, ilya, vinyals, oriol, and le, quoc v. sequence to sequence learning with neural net-

works. advances in neural information processing systems, 2014.

tang, duyu, qin, bing, and liu, ting. document modeling with gated recurrent neural network
in proceedings of conference on empirical methods in natural

for sentiment classi   cation.
language processing (emnlp), 2015.

vinyals, oriol and le, quoc v. a neural conversational model. proceedings of the 31st international

conference on machine learning, 2015.

wang, di and nyberg, eric. a long short-term memory model for answer sentence selection in
id53. proceedings of the 53rd annual meeting of the association for computa-
tional linguistics and the 7th international joint conference on natural language processing,
2015.

wang, mengqiu and manning, christopher. probabilistic tree-edit models with structured latent vari-
ables for id123 and id53. the proceedings of the 23rd international
conference on computational linguistics (coling), 2010.

wang, mengqiu, smith, noah, and teruko, mitamura. what is the jeopardy model? a quasi-

synchronous grammar for qa. the proceedings of emnlp-conll, 2007.

10

under review as a conference paper at iclr 2016

weston, jason, chopra, sumit, and adams, keith. #tagspace: semantic embeddings from hash-
tags. proceedings of the 2014 conference on empirical methods in natural language processing
(emnlp), 2014.

yao, xuchen, durme, benjamin, and clark, peter. answer extraction as sequence tagging with tree

id153. proceedings of naacl-hlt, 2013.

yih, wen-tau, chang, ming-wei, meek, christopher, and pastusiak, andrzej. id53
using enhanced lexical semantic models. proceedings of the 51st annual meeting of the associa-
tion for computational linguist (acl), 2013.

yu, lei, hermann, karl m., blunsom, phil, and pulman, stephen. deep learning for answer sentence

selection. nips deep learning workshop, 2014.

11

