pte: predictive text embedding through large-scale

heterogeneous text networks

jian tang

microsoft research asia
jiatang@microsoft.com

meng qu   

peking university

mnqu@pku.edu.cn

qiaozhu mei

university of michigan
qmei@umich.edu

5
1
0
2

 

g
u
a
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
0
0
2
0
0

.

8
0
5
1
:
v
i
x
r
a

abstract
unsupervised text embedding methods, such as skip-gram
and paragraph vector, have been attracting increasing at-
tention due to their simplicity, scalability, and e   ectiveness.
however, comparing to sophisticated deep learning architec-
tures such as convolutional neural networks, these methods
usually yield inferior results when applied to particular ma-
chine learning tasks. one possible reason is that these text
embedding methods learn the representation of text in a
fully unsupervised way, without leveraging the labeled in-
formation available for the task. although the low dimen-
sional representations learned are applicable to many di   er-
ent tasks, they are not particularly tuned for any task. in
this paper, we    ll this gap by proposing a semi-supervised
representation learning method for text data, which we call
the predictive text embedding (pte). predictive text embed-
ding utilizes both labeled and unlabeled data to learn the
embedding of text. the labeled information and di   erent
levels of word co-occurrence information are    rst represented
as a large-scale heterogeneous text network, which is then
embedded into a low dimensional space through a princi-
pled and e   cient algorithm. this low dimensional embed-
ding not only preserves the semantic closeness of words and
documents, but also has a strong predictive power for the
particular task. compared to recent supervised approaches
based on convolutional neural networks, predictive text em-
bedding is comparable or more e   ective, much more e   cient,
and has fewer parameters to tune.

categories and subject descriptors
i.2.6 [arti   cial intelligence]: learning

general terms
algorithms, experimentation
   this work was done when the second author was an intern
at microsoft research asia.

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro   t or commercial advantage and that copies bear this notice and the full cita-
tion on the    rst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speci   c permission
and/or a fee. request permissions from permissions@acm.org.
kdd   15, august 10-13, 2015, sydney, nsw, australia.
c(cid:13) 2015 acm. isbn 978-1-4503-3664-2/15/08 ...$15.00.
doi: http://dx.doi.org/10.1145/2783258.2783307.

keywords
predictive text embedding, representation learning

1.

introduction

learning a meaningful and e   ective representation of text,
e.g., for words and documents, is a critical prerequisite for
many machine learning tasks such as text classi   cation, clus-
tering and retrieval. traditionally, every word is represented
independently to each other, and each document is rep-
resented as a    bag-of-words   . however, both representa-
tions su   er from problems such as data sparsity, polysemy,
and synonymy, as the semantic relatedness between di   erent
words are commonly ignored.

distributed representations of words and documents [18,
10] e   ectively address this problem through representing
words and documents in low-dimensional spaces, in which
similar words and documents are embedded closely to each
other. the essential idea of these approaches comes from
the distributional hypothesis that    you shall know a word
by the company it keeps    (firth, j.r. 1957:11) [7]. mik-
ilov et al. proposed a simple and elegant id27
model called the skip-gram [18], which uses the embedding
of the target word to predict the embedding of each individ-
ual context word in a local window. le and mikolov further
extended this idea and proposed the paragraph vectors [10]
in order to embed arbitrary pieces of text, e.g., sentences
and documents. the basic idea is to use the embeddings
of sentences/documents to predict the embeddings of words
in the sentences/documents. comparing to other classical
approaches that also utilize the distributional similarity of
word context, such as the brown id91 or nearest neigh-
bors, these text embedding approaches have been proved to
be quite e   cient, scaling up to millions of documents on a
single machine [18].

because of the unsupervised learning process, the repre-
sentations learned through these text embedding models are
general enough and can be applied to a variety of tasks such
as classi   cation, id91 and ranking. however, when
compared end-to-end with sophisticated deep learning ap-
proaches such as the convolutional neural networks (id98s)
[5, 8], the performance of text embeddings usually falls short
on speci   c tasks [30]. this is perhaps not surprising as the
deep neural networks fully leverage labeled information that
is available for a task when they learn the representations of
the data. most text embedding methods are not able to con-
sider labeled information when learning the representations;
the labels only kick in later when a classi   er is trained using
the representations as features.
in other words, unsuper-

figure 1: illustration of converting a partially labeled text corpora to a heterogeneous text network. the word-word co-
occurrence network and word-document network encode the unsupervised information, capturing the local context-level and
document-level word co-occurrences respectively; the word-label network encodes the supervised information, capturing the
class-level word co-occurrences.

vised text embeddings are generalizable for di   erent tasks
but have a weaker predictive power for a particular task.

despite this de   ciency, there are still considerable advan-
tages of text embedding approaches comparing to deep neu-
ral networks. first, the training of deep neural networks,
especially convolutional neural networks is computational
intensive, which usually requires multiple gpus or clusters
of cpus when processing a large amount of data; second,
convolutional neural networks usually assume the availabil-
ity of a large amount of labeled examples which is unrealis-
tic in many tasks. the easily obtainable unlabeled data are
usually used through an indirect way of pre-training; third,
the training of id98s requires exhaustive tuning of many
parameters, which is very time consuming even for experts
and infeasible for non-experts. on the other hand, text em-
bedding methods like skip-gram are much more e   cient, are
much easier to tune, and naturally accommodate unlabeled
data.

in this paper, we    ll this gap by proposing the predic-
tive text embedding (pte), which adapts the advantages of
unsupervised text embeddings but naturally utilizes labeled
information in representation learning. with predictive text
embedding, an e   ective low dimensional representation is
learned jointly from limited labeled examples and a large
amount of unlabeled examples. comparing to unsupervised
embeddings, this representation is optimized for particular
tasks like what convolutional neural networks do (i.e., the
representation has strong predictive power for the particular
classi   cation task).

the proposed method naturally extends our previous work
of unsupervised information network embedding [27] and
   rst learns a low dimensional embedding for words through
a heterogeneous text network. the network encodes di   er-
ent levels of co-occurrence information between words and
words, words and documents, and words and labels. the
network is embedded into a low dimensional vector space
that preserves the second-order proximity [27] between the
vertices in the network. the representation of an arbitrary
piece of text (e.g., a sentence or a document) can be simply
inferred as the average of the word representations, which
turns out to be quite e   ective. the whole optimization pro-
cess remains very e   cient, which scales up to millions of
documents and billions of tokens on a single machine.

we conduct extensive experiments with real-world text
corpora, including both long and short documents. experi-
mental results show that the predictive text embeddings sig-

ni   cantly outperform the state-of-the-art unsupervised em-
beddings in various text classi   cation tasks. compared end-
to-end with convolutional neural networks for text classi   -
cation [8], predictive text embedding outperforms on long
documents and generates comparable results on short doc-
uments. pte enjoys various advantages over convolutional
neural networks as it is much more e   cient, accommodates
large-scale unlabeled data e   ectively, and is less sensitive to
model parameters. we believe our exploration points to a
direction of learning text embeddings that could compete
head-to-head with deep neural networks in particular tasks.

to summarize, we make the following contributions:
    we propose to learn predictive text embeddings in
a semi-supervised manner. unlabeled data and la-
beled information are integrated into a heterogeneous
text network which incorporates di   erent levels of co-
occurrence information in text.

    we propose an e   cient algorithm    pte   , which learns
a distributed representation of text through embed-
ding the heterogeneous text network into a low dimen-
sional space. the algorithm is very e   cient and has
few parameters to tune.

    we conduct extensive experiments using various real-
world data sets and compare predictive text embed-
ding end-to-end with both unsupervised text embed-
dings and convolutional neural networks.

the rest of this paper is organized as follows. we    rst
introduce the related work in section 2. section 3 formally
de   nes the problem of predictive text embedding through
heterogeneous text networks. section 4 introduces the pro-
posed algorithm in details. section 5 presents the results of
empirical experiments. we conclude in section 6.

2. related work

our work is mainly related to distributed text represen-

tation learning and information network embedding.
2.1 distributed text embedding

distributed representation of text has proved to be quite
e   ective in many natural language processing tasks such as
word analogy [18], id52 [6], parsing [6], language
modeling [17], and id31 [16, 10, 5, 8]. ex-
isting approaches can be generally classi   ed into two cat-
egories: unsupervised and supervised. recent developed

unsupervised approaches normally learn the embeddings of
words and/or documents by utilizing word co-occurrences
in the local context (e.g., skip-gram [18]) or at document
level (e.g., paragraph vectors [10]). these approaches are
quite e   cient, scaling up to millions of documents. the su-
pervised approaches [5, 8, 23, 6] are usually based on deep
neural network architectures, such as recursive neural ten-
sor networks (rntns) [24] or convolutional neural networks
(id98s) [11]. in rntns, each word is embedded into a low
dimensional vector, and the embeddings of the phrases are
recursively learned by applying the same tensor-based com-
position function over the sub-phrases or words in a parse
tree. in id98s [5], each word is also represented with a vec-
tor, and the same convolutional kernel is applied over the
context windows in di   erent positions of the sentences, fol-
lowed by a max-pooling and fully connected layer.

the major di   erence between these two categories of ap-
proaches is how they utilize labeled and unlabeled informa-
tion in the representation learning phase. the unsupervised
methods do not include labeled information when learning
the representations and only use the labels to train the classi-
   er after the data is transformed into the learned representa-
tion. rntns and id98s incorporate the labels directly into
representation learning, so the learned representations are
particularly tuned for the classi   cation task. to incorporate
unlabeled examples, however, these neural nets usually have
to use an indirect approach such as to pretrain the word
embeddings with unsupervised approaches. comparing to
these two lines of work, pte learns the text vectors in a
semi-supervised way - the representation learning algorithm
directly utilizes both labeled information and large-scale un-
labeled data.

another piece of work similar to predictive word embed-
ding is [15], which learns word vectors that are particularly
tuned for id31. however, their approach does
not scale to millions of documents and does not generalize
to other classi   cation tasks.

2.2

information network embedding

our work is also related to the problem of network/graph
embedding as the word representations of pte are learned
through a heterogeneous text network. embedding net-
works/graphs into low dimensional spaces is very useful in
a variety of applications, e.g., node classi   cation [3] and
link prediction [13]. classical graph embedding algorithms
such as mds [9], isomap [28] and laplacian eigenmap [1]
are not applicable for embedding large-scale networks that
contain millions of vertices and billions of edges. there
are some recent work attempting to embed very large real-
world networks. perozzi et al.
[20] proposed a network
embedding model called the    deepwalk,    which uses trun-
cated id93 on the networks and is only applicable
for networks with binary edges. our previous work pro-
posed a novel large-scale network embedding model called
the    line,    which is suitable for arbitrary types of informa-
tion networks: undirected or directed, binary or weighted [27].
the line model optimizes an objective function which aims
to preserve both the local and global network structures.
both deepwalk and line are unsupervised and only handle
homogeneous networks. the network embedding algorithm
used by pte extends the line to deal with heterogeneous
networks, in which multiple types of vertices (including the
class labels) and edges exist.

3. problem definition

let us begin with formally de   ning the problem of predic-
tive text embedding through heterogeneous text networks.
comparing to unsupervised text embedding approaches in-
cluding skip-gram and paragraph vectors that learn gen-
eral semantic representations of text, our goal is to learn
a representation of text that is optimized for a given text
classi   cation task.
in other words, we anticipate the text
embedding to have a strong predictive power of the perfor-
mance of the given task. the basic idea is to incorporate
both the labeled and unlabeled information when learning
the text embeddings. to achieve this, it is desirable to    rst
have an uni   ed representation to encode both types of infor-
mation. in this paper, we propose di   erent types of networks
to achieve this, including word-word co-occurrence networks,
word-document networks, and word-label networks.

definition 1. (word-word network) word-word co-
occurrence network, denoted as gww = (v, eww), captures
the word co-occurrence information in local contexts of the
unlabeled data. v is a vocabulary of words and eww is the
set of edges between words. the weight wij of the edge be-
tween word vi and vj is de   ned as the number of times that
the two words co-occur in the context windows of a given
window size.

the word-word network captures the word co-occurrences
in local contexts, which is the essential information used
by existing id27 approaches such as skip-gram.
beyond the local contexts, word co-occurrence at the docu-
ment level is also widely explored in classical text representa-
tions such as statistical topic models, e.g., the latent dirich-
let allocation [4]. to capture the document-level word co-
occurrences, we introduce another network, word-document
network, de   ned as below:

definition 2. (word-document network) word-

document network, denoted as gwd = (v     d, ewd), is a
bipartite network where d is a set of documents and v is
a set of words. ewd is the set of edges between words and
documents. the weight wij between word vi and document
dj is simply de   ned as the number of times vi appears in
document dj.

the word-word and word-document networks encode the
unlabeled information in large-scale corpora, capturing word
co-occurrences at both the local context level and the docu-
ment level. to encode the labeled information, we introduce
the word-label network, which captures word co-occurrences
at category-level .

definition 3. (word-label network) word-label net-
work, denoted as gwl = (v     l, ewl), is a bipartite network
that captures category-level word co-occurrences. l is a set
of class labels and v a set of words. ewl is a set of edges
between words and classes. the weight wij of the edge be-
(d:ld=j) ndi,
where ndi is the term frequency of word vi in document d,
and ld is the class label of document d.

tween word vi and class cj is de   ned as: wij =(cid:80)

the three types of networks above can be further inte-

grated into one heterogeneous text network.

definition 4. (heterogeneous text network) the het-

erogeneous text network is the combination of word-word,

word-document, and word-label networks constructed from
both unlabeled and labeled text data. it captures di   erent
levels of word co-occurrences and contains both labeled and
unlabeled information.

note that the de   nition of a heterogeneous text network
can be generalized to integrate other types of networks such
as word-sentence, word-paragraph, and document-label net-
works. in this work we are using the three types of networks
(word-word, word-document, and word-label) as an illustra-
tive example. we particularly focus on word networks in
order to    rst represent words into low dimensional spaces.
the representation of other text units (e.g., sentences or
paragraphs) can be then computed through aggregating the
word representations.

finally, we formally de   ne the problem of predictive text

embedding as follows:

definition 5. (predictive text embedding) given a
large collection of text data with unlabeled and labeled infor-
mation, the problem of predictive text embedding aims
to learn low dimensional representations of words by embed-
ding the heterogeneous text network constructed from the
collection into a low dimensional vector space.

4. predictive text embedding

in this section, we introduce the proposed method that
learns predictive text embedding through heterogeneous text
networks. our method    rst learns vector representations of
words by embedding the heterogeneous text networks con-
structed from free text into a low dimensional space, and
then infer text embeddings based on the learned word vec-
tors. as the heterogeneous text network is composed of
three bipartite networks, we    rst introduce an approach for
embedding individual bipartite networks.
4.1 bipartite network embedding

in our previous work, we introduced the line model to
learn the embedding of large-scale information networks [27].
line is mainly designed for homogeneous networks, i.e.,
networks with the same types of nodes. line cannot be
directly applied to heterogeneous networks as the weights
on di   erent types of edges are not comparable. here, we
   rst adapt the line model for embedding bipartite net-
works. the essential idea is to make use of the second-order
proximity [27] between vertices, which assumes vertices with
similar neighbors are similar to each other and thus should
be represented closely in a low dimensional space.
given a bipartite network g = (va     vb, e), where va
and vb are two disjoint sets of vertices of di   erent types,
and e is the set of edges between them. we    rst de   ne the
id155 of vertex vi in set va generated by
vertex vj in set vb as:

(cid:80)

i    (cid:126)uj)
exp((cid:126)ut
i(cid:48)   a exp((cid:126)ut

p(vi|vj) =

,

i(cid:48)    (cid:126)uj)

(1)
where (cid:126)ui is the embedding vector of vertex vi in va, and (cid:126)uj is
the embedding vector of vertex vj in vb. for each vertex vj
in vb, eq (1) de   nes a conditional distribution p(  |vj) over
all the vertices in the set va; for each pair of vertices vj, vj(cid:48) ,
the second-order proximity can actually be determined by
their conditional distributions p(  |vj), p(  |vj(cid:48) ). to preserve

(cid:88)

j   b

the second-order proximity, we can make the conditional dis-
tribution p(  |vj) be close to its empirical distribution   p(  |vj),
which can be achieved by minimizing the following objective
function:

o =

  jd(  p(  |vj), p(  |vj)),

(2)

where d(  ,  ) is the kl-divergence between two distributions,
  j is the importance of vertex vj in the network, which can
i wij, and the empirical dis-
tribution can be de   ned as   p(vi|vj) = wij
. omitting some
constants, the objective function (2) can be calculated as:

be set as the degree degj =(cid:80)
o =     (cid:88)

wij log p(vj|vi).

(3)

degj

(i,j)   e

the objective (3) can be optimized with stochastic gradi-
ent descent using the techniques of edge sampling [27] and
negative sampling [18]. in each step, a binary edge e = (i, j)
is sampled with the id203 proportional to its weight
wij, and meanwhile multiple negative edges (i, j) are sam-
pled from a noise distribution pn(j). the sampling pro-
cedures address signi   cant de   ciency of stochastic gradient
descent in learning network embeddings. for the detailed
optimization process, readers can refer to [27].

the embeddings of the word-word, word-document, and
word-label network can all be learned by the above model.
note that the word-word network is essentially a bipartite-
network by treating each undirected edge as two directed
edges, and then va is de   ned as the set of the source nodes,
vb as the set of target nodes. therefore, we can de   ne
the conditional probabilities p(vi|vj), p(vi|dj) and p(vi|lj)
according to equation (1), and then learn the embeddings
by optimizing objective function (3). next, we introduce
our approach of embedding the heterogeneous text network.
4.2 heterogeneous text network embedding
the heterogeneous text network is composed of three bi-
partite networks: word-word, word-document and word-label
networks, where the word vertices are shared across the three
networks. to learn the embeddings of the heterogeneous
text network, an intuitive approach is to collectively em-
bed the three bipartite networks, which can be achieved by
minimizing the following objective function:

where

opte = oww + owd + owl,

(i,j)   eww

oww =     (cid:88)
owd =     (cid:88)
owl =     (cid:88)

(i,j)   ewd

(i,j)   ewl

wij log p(vi|vj)

wij log p(vi|dj)

wij log p(vi|lj)

(4)

(5)

(6)

(7)

the objective function (4) can be optimized in di   erent
ways, depending on how the labeled information, i.e., the
word-label network, is used. one solution is to train the
model with the unlabeled data (the word-word and word-
document networks) and the labeled data simultaneously.

we call this approach joint training. an alternative solution
is to learn the embeddings with unlabeled data    rst, and
then    ne-tune the embeddings with the word-label network.
this is inspired by the idea of pre-training and    ne-tuning
in the literature of deep learning [2].

in joint training, all three types of networks are used
together. a straightforward solution to optimize the ob-
jective (4) is to merge the all the edges in the three sets
eww, ewd, ewl and then deploy edge sampling [27], which
samples an edge for model updating in each step, with the
sampling id203 proportional to its weight. however,
when the network is heterogeneous, the weights of the edges
between di   erent types of vertices are not comparable to
each other. a more reasonable solution is to alternatively
sample from the three sets of edges. we summarize the de-
tailed training algorithm in alg. 1.

algorithm 1: joint training.

data: gww, gwd, gwl, number of samples t , number of

negative samples k.

result: id27s (cid:126)w.
while iter     t do

    sample an edge from eww and draw k negative edges,

and update the id27s;

    sample an edge from ewd and draw k negative edges,

and update the word and document embeddings;

    sample an edge from ewl and draw k negative edges,

and update the word and label embeddings;

end

algorithm 2: pre-training + fine-tuning.

data: gww, gwd, gwl, number of samples t , number of

negative samples k.

result: id27s (cid:126)w.
while iter     t do

    sample an edge from eww and draw k negative edges,

and update the id27s;

    sample an edge from ewd and draw k negative edges,

and update the word and document embeddings;

end
while iter     t do

    sample an edge from ewl and draw k negative edges,

and update the word and label embeddings;

end

similarly, we summarize the training process of pre-training

and    ne-tuning in alg. 2.
4.3 text embedding

the heterogeneous text network encodes word co-occurrences

at di   erent levels, extracted from both unlabeled data and
labeled information for a speci   c classi   cation task. there-
fore, the word representations learned by embedding the het-
erogeneous text network are not only more robust but also
optimized for that task. once the word vectors are learned,
the representation of an arbitrary piece of text can be ob-
tained by simply averaging the vectors of the words in that
piece of text. that is, the vector representation of a piece
of text d = w1w2        , wn can be computed as

n(cid:88)

where (cid:126)ui is the embedding of word wi.

in fact, the average of the id27s is the solution

to minimizing the following objective function:

l((cid:126)ui, (cid:126)d),

i=1

o =

(9)
where the id168 l(  ,  ) between the id27 (cid:126)ui
and text embedding (cid:126)d is speci   ed as the euclidean distance.
related is the id136 process of paragraph vectors [10],
which minimizes the same objective but with a di   erent loss
function l((cid:126)ui, (cid:126)d) =    
. it however does not lead
to a close form solution and has to be optimized by gradient
descent algorithm.
5. experiments

1+exp(   (cid:126)ut

(cid:126)d)

1

i

in this section, we move forward to evaluate the e   ec-
tiveness of the proposed pte algorithm for predictive text
embedding. a variety of text classi   cation tasks and data
sets are selected for this purpose. the experiments are set
up as the following.
5.1 experiment setup
data sets
we select two types of text corpora, which consist of either
long or short documents.
long document corpora: (1) 20ng, the widely used
text classi   cation data set 20newsgroup1, containing 20 cat-
egories; (2)wiki, a snapshot of wikipedia corpus in april
2010 containing around two million english articles. only
common words appeared in the vocabulary of wiki2010 [22]
are kept. we choose seven diverse categories for the classi   -
cation task, including    arts,       history,       human,       mathe-
matics,      nature,      technology,    and    sports    from dbpedia
ontology2. for each category, we randomly select 9,000 ar-
ticles as labeled documents for training; (3) imdb, a data
set for sentiment classi   cation from [15]3. to avoid the dis-
tribution bias between the training and test data sets, we
randomly shu   e the training and test data sets; (4) rcv1,
a large benchmark corpus for text classi   cation [12]4. four
subsets including corporate, economics, government
and market are extracted from the corpus. in rcv1 data
sets, all the documents have already been represented as
   bag-of-words,    and orders between words are lost.
short document corpora: (1) dblp, which contains ti-
tles of papers from the computer science bibliography5. we
choose six diverse research    elds for classi   cation includ-
ing    database,      arti   cial intelligence,      hardware,      system,   
   programming languages,    and    theory.    for each    eld, we
select representative conferences and collect the papers pub-
lished in the selected conferences as the labeled documents;
(2) mr, a movie review data set,
in which each review
only contains one sentence [19]6; (3) twitter, a corpus
1

available at http://qwone.com/~jason/20newsgroups/
available

http://downloads.dbpedia.org/3.9/en/article_

at

2

categories_en.nq.bz2.
3

available at http://ai.stanford.edu/~amaas/data/sentiment/
available at http://www.ai.mit.edu/projects/jmlr/papers/volume5/

4

lewis04a/lyrl2004_rcv1v2_readme.htm
5

available at http://arnetminer.org/billboard/citation
available

http://www.cs.cornell.edu/people/pabo/

at

n(cid:88)

(cid:126)d =

1
n

(cid:126)ui,

(8)

6

i=1

movie-review-data/

of tweets for sentiment classi   cation7, from which we ran-
domly sampled 1,200,000 tweets and split them into train-
ing and testing sets.

no further text id172 such as removing stop words
or id30 is done on top of the original data. we sum-
marize the detailed statistics of these data sets in table 1.
compared algorithms
we compare the pte algorithm with other representation
learning algorithms for text data,
including the classical
   bag-of-words    representation and the state-of-the-art ap-
proaches to unsupervised and supervised text embedding.

    bow: the classical    bag-of-words    representation. each
document is represented with a |v |-dimensional vector,
in which the weight of each dimension is calculated
with the tfidf weighting [21].

    skip-gram: the state-of-the-art id27 model
proposed by mikolov et al. [18]. for the document
embedding, we simply take the average of the word
embeddings as explained in section 4.3.

    pvdbow: the distributed bag-of-words version of para-
graph vector model proposed by le and mikolv [10], in
which the orders of words in a document are ignored.
    pvdm: the distributed memory version of paragraph

vector which considers the order of the words [10].

    line: the large-scale information network embedding
model proposed by tang et al. [27]. we use the line
model to learn unsupervised embeddings with the word-
word network, word-document network or the combi-
nation of the two networks.

    id98: the supervised text embedding approach based
on a convolutional neural network [8]. though id98
is proposed for modeling sentences, we adapt it for
general word sequences including long documents. al-
though id98 typically works with fully labeled docu-
ments, it can also utilize unlabeled data by pre-training
the model with unsupervised id27s, which
is marked as id98(pretrain).

    pte: our proposed approach for learning predictive
text embedding. there are di   erent variants of pte
that use di   erent combinations of the word-word, word-
document and word-label networks. we denote pte(gwl)
for the version that uses the word-label network only;
pte(pretrain) learns an unsupervised embedding with
the word-word and word-document networks, and then
   ne-tune the id27s with the word-label
network; pte(joint) jointly trains the heterogeneous
text network composed of all the three networks.

classi   cation and parameter settings
once the vector representations of documents are constructed
or learned, we apply the same classi   cation process using the
same training data set. in particular, all the documents in
the training set are used in both the representation learning
phase and the classi   er learning phase. the class labels of
these documents are not used in the representation learning

7

available

at

http://thinknook.com/

twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/

phase if an unsupervised embedding method is used; they
only kick in at the classi   er learning phase. the class label
are used in both the representation learning phase and the
classi   er learning phase if a predictive embedding method
is used. the test data is held-out from both phases. in the
classi   cation phase, we use the one-vs-rest id28
model in the liblinear package8. the classi   cation perfor-
mance is measured with the micro-f1 and macro-f1 metrics.
for skip-gram, pvdbow, pvdm and pte, the mini-batch
size of the stochastic id119 is set as 1; the learning
rate is set as   t =   0(1   t/t ), in which t is the total number
of mini-batches or edge samples and   0 = 0.025; the number
of negative samples is set as 5; the window size is set as 5 in
skip-gram, pvdbow, pvdm and when constructing the
word-word co-occurrence network. we use the structure of
the id98 in [8], which uses one convolution layer, followed
by a max-pooling layer and a fully-connected layer. follow-
ing [8], we set the window size in the convolution layer as
3 and the number of feature maps as 100. for id98, 1% of
the training data set is randomly selected as the validation
data for early stopping. the dimensionality of word vectors
is set as 100 by default for all the embedding models.

note that for the pte models, the parameters are all
set as above by default on di   erent data sets. the only
parameter that needs to be tuned is the number of samples
t in edge sampling, which can be safely set to be large.
5.2 quantitative results
5.2.1 performance on long documents
table 2 and 3 compare the performance of text classi   ca-
tion on long documents. let us start with table 2 on 20ng,
wiki and imdb data sets. we    rst compare the perfor-
mance of unsupervised embedding methods, which use either
local word co-occurrences (skip-gram, line(gww)), docu-
ment level word co-occurrences (pv-dbow, line(gwd)),
or the combination (line(gww+gwd)). we can see that the
line(gwd) with document-level word co-occurrences per-
forms the best among the unsupervised embeddings. the
performance of pvdm is inferior to that of pvdbow, which
is di   erent from what is reported in [10]. unfortunately we
are not able to replicate their results. similar results to ours
are also reported in [14]. for the results of pvdbow on the
imdb data set, our results are di   erent from those reported
in [10, 16]. this is because their embeddings are trained
on the mixture of training and test data sets while our em-
beddings are only trained with the training data, which we
believe is a more reasonable experiment setup.

next we compare the performance of predictive embed-
dings, including di   erent variants of id98 and pte. when
pte is jointly trained using the heterogeneous text network
or with the combination of word-document and word-label
networks, it performs the best among all the approaches. all
pte approaches jointly trained with the word-label network
(e.g., gww + gwl) outperform their corresponding unsuper-
vised embedding approaches (e.g., gww), which shows the
power of learning predictive text embeddings with the su-
pervision. pte(joint) consistently outperforms pte(gwl),
demonstrating that incorporating unlabeled information, i.e.,
word-word and word-document networks, also improves the
quality of the embeddings. pte(joint) also signi   cantly out-
performs pte(pretrain). this shows that jointly training
8http://www.csie.ntu.edu.tw/~cjlin/liblinear/

table 1: statistics of the data sets

long documents

short documents

name
train
test
|v|

doc. length

#classes

20ng
11,314
7,532
89,039
305.77

20

wiki

1,911,617*

21,000
913,881
672.56

7

imdb corporate economics government market dblp
61,479
25,000
20,000
25,000
71,381
22,270
231.65

245,650
122,827
141,740
102.23

138,990
69,496
139,960
169.07

132,040
66,020
64,049
119.83

77,242
38,623
65,254
145.10

9.51

2

4
*in the wiki data set, only 42,000 documents are labeled.

18

10

23

6

mr
7,108
3,554
17,376
22.02

2

twitter
800,000
400,000
405,994

14.36

2

table 2: results of text classi   cation on long documents.

type

word

unsupervised
embedding

predictive
embedding

algorithm

micro-f1 macro-f1 micro-f1 macro-f1 micro-f1 macro-f1

20ng

wikipedia

imdb

bow

skip-gram
pvdbow

pvdm

line(gww)
line(gwd)

line(gww + gwd)

id98

id98(pretrain)

pte(gwl)

pte(gww + gwl)
pte(gwd + gwl)

pte(pretrain)

pte(joint)

80.88
70.62
75.13
61.03
72.78
79.73
78.74
78.85
80.15
82.70
83.90
84.39
82.86
84.20

79.30
68.99
73.48
56.46
70.95
78.40
77.39
78.29
79.43
81.97
83.11
83.64
82.12
83.39

79.95
75.80
76.68
72.96
77.72
80.14
79.91
79.72
79.25
79.00
81.65
82.29
79.18
82.51

80.03
75.77
76.75
72.76
77.72
80.13
79.94
79.77
79.32
79.02
81.62
82.27
79.21
82.49

86.54
85.34
86.76
82.33
86.16
89.14
89.07
86.15
89.00
85.98
89.14
89.76
86.28
89.80

86.54
85.34
86.76
82.33
86.16
89.14
89.07
86.15
89.00
85.98
89.14
89.76
86.28
89.80

table 3: results of text classi   cation on long documents (rcv1 data sets).

algorithm

micro-f1 macro-f1 micro-f1 macro-f1 micro-f1 macro-f1 micro-f1 macro-f1

corporate

economics

government

market

bow

pvdbow
line(gwd)
pte(gwl)

pte(pretrain)

pte(joint)

78.45
65.87
76.76
76.69
77.03
79.20

63.80
45.78
60.30
60.48
61.03
64.29

86.18
79.63
85.55
84.88
84.95
87.05

81.67
74.82
81.46
80.02
80.63
83.01

77.43
70.74
77.82
78.26
78.48
79.63

62.38
54.08
63.34
63.69
64.50
66.15

95.55
91.81
95.66
95.58
95.54
96.19

94.09
88.88
93.90
93.84
93.79
94.58

table 4: results of text classi   cation on short documents.

type

word

unsupervised
embedding

predictive
embedding

algorithm

micro-f1 macro-f1 micro-f1 macro-f1 micro-f1 macro-f1

dblp

mr

twitter

bow

skip-gram
pvdbow

pvdm

line(gww)
line(gwd)

line(gww + gwd)

id98

id98(pretrain)

pte(gwl)

pte(gww + gwl)
pte(gwd + gwl)

pte(pretrain)

pte(joint)

75.28
73.08
67.19
37.11
73.98
71.50
74.22
76.16
75.39
76.45
76.80
77.46
76.53
77.15

71.59
68.92
62.46
34.38
69.92
67.23
70.12
73.08
72.28
72.74
73.28
74.03
72.94
73.61

71.90
67.05
67.78
58.22
71.07
69.25
71.13
72.71
68.96
73.44
72.93
73.13
73.27
73.58

71.90
67.05
67.78
58.17
71.06
69.24
71.12
72.69
68.87
73.42
72.92
73.11
73.24
73.57

75.27
73.02
71.29
70.75
73.19
73.19
73.84
75.97
75.92
73.92
74.93
75.61
73.79
75.21

75.27
73.00
71.18
70.73
73.18
73.19
73.84
75.96
75.92
73.91
74.92
75.61
73.79
75.21

with unlabeled and labeled data is much more e   ective com-
pared to separating them into two phases of pre-training and
   ne-tuning.

it is interesting to observe that pte(joint) consistently
outperforms id98. this is promising as pte does not use a
sophisticated neural network architecture. we also attempt
to pre-train the id98 with the id27s learned by

(a) 20ng

(b) imdb

(c) dblp

(d) mr

figure 2: performance w.r.t. # labeled data.

the line(gwl + gwd) and then    ne tune it with the labels.
surprisingly, the performance of id98 with pre-training sig-
ni   cantly improves on the 20ng and imdb data sets and
remains almost the same on the wiki data set. this im-
plies that pre-training id98 with a well learned unsuper-
vised embeddings can be very useful. however, even with
pre-training the performance of id98 is still inferior to that
of pte(joint). this is probably because the pte model can
jointly train with both the unlabeled and labeled data while
id98 can only utilize them separately through pre-training
and    ne-tuning. pte(joint) also outperforms the classical
   bag-of-words    representation even though the dimensional-
ity of the embeddings learned by the pte is way smaller
than that of    bag-of-words.   

table 3 reports the results on the rcv1 data sets. as
the order between the words is lost, the embedding methods
that require the word order information are not applicable.
similar results are observed. predictive text embeddings
outperform unsupervised embeddings. pte(joint) is also
much more e   ective than pte(pretrain).

all the embedding approaches (except    bag-of-words   ) are
trained with asynchronous stochastic id119 algo-
rithm using 20 threads on a single machine with 1t memory,
40 cpu cores at 2.0ghz. we compare the running time of
id98 and pte(joint) on the imdb data set. the pte(joint)
method is typically more than 10 times faster than the id98
models. when pre-trained with preexisting word embed-
dings, id98 converges much faster, but still more than 5
times slower than pte(joint).

5.2.2 performance on short documents
table 4 compares the performance on short documents.
among unsupervised embeddings, the line(gww + gwd),
which combines the document-level and local context-level
word co-occurrences, performs the best. the line(gww)
utilizing the local context-level word co-occurrences outper-
forms the line(gwd) using document-level word co-occurrences,
which is opposite to the observations on long documents.
this is because document-level word co-occurrences su   er
from the sparsity in short documents, with similar results
observed in statistical topic models [26]. the performance
of pvdm is still inferior to that of pvdbow, which is con-
sistent with the results on long documents.

for predictive embeddings, the best performance is ob-
tained by the pte (on dblp, mr ) or id98 (on twit-
ter). among the pte approaches, the predictive embed-
dings learned by incorporating the word-label network out-
perform the corresponding unsupervised embeddings, which
is consistent with the results on long documents. pte(joint)

outperforms line(gwl), showing the usefulness of incorpo-
rating unlabeled information. pte(joint) also signi   cantly
outperforms the pte(pretrain), showing the advantage of
jointly training with the labeled and unlabeled data.

on the short documents, we observe that pte(joint) does
not consistently outperform the id98. the reason is proba-
bly due to the problem of word sense ambiguity, which be-
comes more serious on the short documents. id98 reduces
the problem of word ambiguity through using the word or-
ders in local context in the convolutional kernels while pte
does not leverage the orders. we believe there is consider-
able room to improve predictive text embedding by utilizing
word orders, which we leave as future work.
5.3 effects of labeled data

we compare id98 and pte head-to-head by varying the
percentages of labeled data. we consider the cases without
or with unlabeled data, mimicking the scenarios of super-
vised and semi-supervised learning. in the setting of semi-
supervised learning, we also compare with classical semi-
supervised approaches, naive bayes with em (nb+em) [25]
and label propagation (lp) [31]. fig. 2 reports the perfor-
mance on both long and short documents. overall, both
id98s and ptes improve when the size of labeled data
increases.
in the supervised settings, i.e., between id98
and pte(gwl), pte(gwl) outperforms or is comparable
to id98 on both the long and short documents.
in the
semi-supervised settings, i.e., between id98(pretrain) and
pte(joint), pte(joint) consistently outperforms id98(pretrain),
which is pre-trained with the best performing unsuper-
vised id27s. the pte(joint) also outperforms
the state-of-the-art semi-supervised approaches naive bayes
with em and label propagation.

we also notice that when the size of labeled data is scarce,
pre-training id98 with unsupervised embeddings is quite
helpful, especially on the short documents.
it even out-
performs all ptes when the training examples are too few.
however, when the size of labeled data increases, pre-training
id98 does not always improve its performance (e.g., on the
dblp and mr data sets).

note that for skip-gram, increasing the number of labeled
data in training does not further increase the performance.
we also notice that when the labeled documents are too
few, the performance of pte is inferior to the skip-gram on
the dblp data set. the reason is that when the number of
labeled examples is scarce, the word-label network is very
noisy and pte treats the word-label network equally to the
robust word-word/word-document networks. a way to    x is
to adjust the sampling id203 from the word-label and

llllllllll0.20.40.60.81.00.500.600.700.80# percentage of labeled datamicro   f1lskip   grampte(gwl)id98pte(pretrain)id98(pretrain)pte(joint)nb+emlpllllllllll0.20.40.60.81.00.700.750.800.850.90# percentage of labeled datamicro   f1lskip   grampte(gwl)id98pte(pretrain)id98(pretrain)pte(joint)nb+emlpllllllllll0.20.40.60.81.00.640.680.720.76# percentage of labeled datamicro   f1lskip   grampte(gwl)id98pte(pretrain)id98(pretrain)pte(joint)nb+emlpllllllllll0.20.40.60.81.00.600.640.680.72# percentage of labeled datamicro   f1lskip   grampte(gwl)id98pte(pretrain)id98(pretrain)pte(joint)nb+emlpword-word/word-document when the labeled data is scarce.
we leave it as future work.
5.4 effects of unlabeled data

(a) 20ng

(b) dblp

figure 3: performance w.r.t. # unlabeled data.

we also analyze the performance of the id98s and ptes
w.r.t. the size of unlabeled data. for id98, the unlabeled
data is used for pre-training while for the pte, the unlabeled
data can be used for either pre-training or jointly training.
fig. 3 reports the results on 20ng and dblp data sets. due
to space limitation, we omit the results on other data sets,
which are similar. on 20ng, we use 10% documents as la-
beled while the rest is used as the unlabeled; on dblp, we
randomly sample 200,000 titles of the papers published in
the other conferences as the unlabeled data. we use the un-
supervised embeddings (learned by line(gww + gwd)) as
the pre-training of id98. we can see that the performance
of both id98 and pte improves when the size of unlabeled
data increases. for pte, the way of jointly training with
the unlabeled and labeled data is much more e   ective than
separating them into pre-training and    ne-tuning.
5.5 parameter sensitivity

(a) 20ng

(b) dblp

figure 4: sensitivity of pte w.r.t. number of samples t .

for the proposed pte models, as mentioned previously,
most of the parameters except for the number of edge sam-
ples t are not sensitive to di   erent data sets and can be
set by default. here we analyze the performance sensitivity
of pte(joint) w.r.t the number of samples t . fig. 4 re-
ports the results on the 20ng and dblp data sets. on both
data sets, we can see that when the number of samples t
becomes large enough, the performance of pte(joint) con-
verges. therefore, in practice, one can just set the number
of samples t to be su   ciently large. a reasonable estima-
tion of t we    nd in practice is several times of the number
of edges in the heterogeneous text networks.
5.6 document visualization

finally, we give an illustrative visualization of the docu-
ments in 20ng to compare the unsupervised and predictive

(a) train(line(gwd))

(b) train(pte(gwl))

(c) test(line(gwd))

(d) test(pte(gwl))

figure 5: document visualization using unsupervised and
predictive embeddings on 20ng data set, visualized with the
id167 tool [29].

embeddings. we used line(gwd) for the unsupervised em-
bedding and pte(gwl) for the predictive embedding. both
the training and test documents are visualized. fig. 5 shows
the visualization results. we can see that on both training
and test data sets, the predictive embedding much better
distinguishes di   erent classes than those learned by the un-
supervised embedding, which intuitively shows the power of
predictive embeddings on the task of text classi   cation.

6. discussion and conclusion
unsupervised embeddings. the essential information
used by the unsupervised approaches is the local context-
level or document-level word co-occurrences. on long doc-
uments, we can see that document-level word co-occurrences
are more useful than the local context-level word co-occurrences,
and the combination of two does not further improve the
result; on short documents, the local context-level word co-
occurrences are more useful than the document-level word
co-occurrences, and their combination will further improve
the embedding. document-level word co-occurrences su   ers
from short lengths of documents.
predictive embeddings. comparing between id98 and
pte, the id98 model seems to handle labeled information
more e   ectively, especially on short documents. this is be-
cause id98 uses a much more complicated structure than
the pte, which in particular utilizes word orders in the lo-
cal context and addresses word sense ambiguity.

therefore, in the case when labeled data is very sparse,
id98 can outperform the pte, especially on short docu-
ments. however, this advantage is at the expense of inten-
sive computation and exhaustive parameter tuning. on the
other hand pte is much faster and much easier to con   g-
ure (with few parameters to tune). when the labeled data
becomes abundant, the performance of pte will be at least
comparable and usually superior to id98.

0.00.20.40.60.81.00.500.600.70# percentage of unlabeled datamicro   f1pte(pretrain)id98(pretrain)pte(joint)0.00.20.40.60.81.00.720.730.740.750.76# percentage of unlabeled datamicro   f1pte(pretrain)id98(pretrain)pte(joint)llllllllllllllllllll02004006008000.800.810.820.830.84#samples(*million) micro   f1lllllllllllllllll0204060801000.740.750.760.77#samples(*million) micro   f1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  40   2002040   40   2002040                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  40   2002040   40   2002040                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               40   2002040   40   2002040                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               2002040   40   2002040compared to the id98 model, an obvious advantage of
the pte model is its capability of being able to jointly train
with both the unlabeled and labeled data. id98 can only
make use of unlabeled data through an indirect way, i.e.,
pre-training with the unsupervised id27s learned
from other methods. pre-training may not always help when
the size of labeled data becomes abundant.

practical guidelines.

based on the above discussions, we provide the following
practical guidelines, which suggest to choose between id98
or pte in di   erent scenarios.

(1) when no labeled data is available, we suggest using
line(gwd) for learning an unsupervised word embed-
ding from long documents and using line(gwd+gww)
from short documents.

(2) when few labeled data is available, on short documents,
we suggest to learn an unsupervised embedding    rst
(according to the    rst guideline) and then pre-train
id98 with the unsupervised id27; on long
documents, we suggest using pte.

(3) when the labeled data are abundant, on long docu-
ments, we strongly suggest using the pte model to
jointly train the embedding with both the unlabeled
and labeled data; on short documents, the selection
between pte(joint) and id98 or id98(pretrain) basi-
cally trades performance with e   ciency.

we believe this study provides an interesting direction to
e   ciently learn predictive, distributed text embeddings. it
is feasible and desirable to develop similar methods that
compete with deep neural networks end-to-end on speci   c
tasks but avoid complex model architectures or intensive
computation. there is considerable room to improve pte,
for example by considering the orders of words.

acknowledgments
qiaozhu mei is supported by the national science founda-
tion under grant numbers iis-1054199 and ccf-1048168.

7. references
[1] m. belkin and p. niyogi. laplacian eigenmaps and spectral

techniques for embedding and id91. in nips,
volume 14, pages 585   591, 2001.

[2] y. bengio, a. courville, and p. vincent. representation

learning: a review and new perspectives. pattern analysis
and machine intelligence, ieee transactions on,
35(8):1798   1828, 2013.

[3] s. bhagat, g. cormode, and s. muthukrishnan. node

classi   cation in social networks. in social network data
analytics, pages 115   148. springer, 2011.

[4] d. m. blei, a. y. ng, and m. i. jordan. latent dirichlet

allocation. jmlr, 3:993   1022, 2003.

[5] p. blunsom, e. grefenstette, n. kalchbrenner, et al. a

convolutional neural network for modelling sentences. in
acl, 2014.

[6] r. collobert, j. weston, l. bottou, m. karlen,

k. kavukcuoglu, and p. kuksa. natural language
processing (almost) from scratch. jmlr, 12:2493   2537,
2011.

[7] j. r. firth. a synopsis of linguistic theory, 1930   1955. in j.

r. firth (ed.), studies in linguistic analysis, pages 1   32.

[8] y. kim. convolutional neural networks for sentence
classi   cation. arxiv preprint arxiv:1408.5882, 2014.

[9] j. b. kruskal and m. wish. multidimensional scaling,

volume 11. sage, 1978.

[10] q. v. le and t. mikolov. distributed representations of

sentences and documents. arxiv preprint arxiv:1405.4053,
2014.

[11] y. lecun and y. bengio. convolutional networks for

images, speech, and time series. the handbook of brain
theory and neural networks, 3361:310, 1995.

[12] d. d. lewis, y. yang, t. g. rose, and f. li. rcv1: a new

benchmark collection for text categorization research.
jmlr, 5:361   397, 2004.

[13] d. liben-nowell and j. kleinberg. the link-prediction
problem for social networks. journal of the american
society for information science and technology,
58(7):1019   1031, 2007.

[14] y. liu, z. liu, t.-s. chua, and m. sun. topical word

embeddings. 2015.

[15] a. l. maas, r. e. daly, p. t. pham, d. huang, a. y. ng,

and c. potts. learning word vectors for id31.
in acl-hlt, pages 142   150, 2011.

[16] g. mesnil, m. ranzato, t. mikolov, and y. bengio.

ensemble of generative and discriminative techniques for
id31 of movie reviews. arxiv preprint
arxiv:1412.5335, 2014.

[17] t. mikolov, m. kara     at, l. burget, j. cernock`y, and

s. khudanpur. recurrent neural network based language
model. in interspeech, pages 1045   1048, 2010.

[18] t. mikolov, i. sutskever, k. chen, g. s. corrado, and

j. dean. distributed representations of words and phrases
and their compositionality. in nips, pages 3111   3119, 2013.

[19] b. pang and l. lee. seeing stars: exploiting class

relationships for sentiment categorization with respect to
rating scales. in acl, pages 115   124, 2005.

[20] b. perozzi, r. al-rfou, and s. skiena. deepwalk: online

learning of social representations. in kdd, pages 701   710.
acm, 2014.

[21] g. salton and c. buckley. term-weighting approaches in

automatic text retrieval. information processing &
management, 24(5):513   523, 1988.

[22] c. shaoul. the westbury lab wikipedia corpus. edmonton,

ab: university of alberta, 2010.

[23] y. shen, x. he, j. gao, l. deng, and g. mesnil. a latent

semantic model with convolutional-pooling structure for
information retrieval. in cikm, pages 101   110. acm, 2014.

[24] r. socher, a. perelygin, j. y. wu, j. chuang, c. d.

manning, a. y. ng, and c. potts. recursive deep models
for semantic compositionality over a sentiment treebank. in
emnlp, volume 1631, page 1642, 2013.

[25] j. su, j. s. shirab, and s. matwin. large scale text

classi   cation using semi-supervised multinomial naive
bayes. in icml, pages 97   104, 2011.

[26] j. tang, z. meng, x. nguyen, q. mei, and m. zhang.

understanding the limiting factors of id96 via
posterior contraction analysis. in icml, pages 190   198,
2014.

[27] j. tang, m. qu, m. wang, m. zhang, j. yan, and q. mei.

line: large-scale information network embedding. in
www, pages 1067   1077, 2015.

[28] j. b. tenenbaum, v. de silva, and j. c. langford. a global

geometric framework for nonlinear dimensionality
reduction. science, 290(5500):2319   2323, 2000.

[29] l. van der maaten and g. hinton. visualizing data using

id167. jmlr, 9(2579-2605):85, 2008.

[30] j. weston, s. chopra, and k. adams. tagspace: semantic
embeddings from hashtags. in emnlp, pages 1822   1827,
2014.

[31] d. zhou, o. bousquet, t. n. lal, j. weston, and

b. sch  olkopf. learning with local and global consistency.
nips, 16(16):321   328, 2004.

