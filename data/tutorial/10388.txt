5
1
0
2

 

v
o
n
9
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
8
8
3
6
0

.

1
1
5
1
:
v
i
x
r
a

under review as a conference paper at iclr 2016

sense2vec - a fast and accurate method
for id51 in
neural id27s.

andrew trask & phil michalak & john liu
digital reasoning systems, inc.
nashville, tn 37212, usa
{andrew.trask,phil.michalak,john.liu}@digitalreasoning.com

abstract

neural word representations have proven useful in natural language processing
(nlp) tasks due to their ability to ef   ciently model complex semantic and syn-
tactic word relationships. however, most techniques model only one representa-
tion per word, despite the fact that a single word can have multiple meanings or
   senses   . some techniques model words by using multiple vectors that are clus-
tered based on context. however, recent neural approaches rarely focus on the
application to a consuming nlp algorithm. furthermore, the training process of
recent word-sense models is expensive relative to single-sense embedding pro-
cesses. this paper presents a novel approach which addresses these concerns by
modeling multiple embeddings for each word based on supervised disambigua-
tion, which provides a fast and accurate way for a consuming nlp model to select
a sense-disambiguated embedding. we demonstrate that these embeddings can
disambiguate both contrastive senses such as nominal and verbal senses as well
as nuanced senses such as sarcasm. we further evaluate part-of-speech disam-
biguated embeddings on neural id33, yielding a greater than 8%
average error reduction in unlabeled attachment scores across 6 languages.

1

introduction

nlp systems seek to automate the extraction of information from human language. a key challenge
in this task is the complexity and sparsity in natural language, which leads to a phenomenon known
as the curse of dimensionality. to overcome this, recent work has learned real valued, distributed
representations for words using neural networks (g.e. hinton, 1986; bengio et al., 2003; morin &
bengio, 2005; mnih & hinton, 2009). these    neural language models    embed a vocabulary into
a smaller dimensional linear space that models    the id203 function for word sequences, ex-
pressed in terms of these representations    (bengio et al., 2003). the result is a vector-space model
(vsm) that represents word meanings with vectors that capture the semantic and syntactic informa-
tion of words (maas & ng, 2010). these distributed representations model shades of meaning across
their dimensions, allowing for multiple words to have multiple real-valued relationships encoded in
a single vector (liang & potts, 2015).
various forms of distributed representations have shown to be useful for a wide variety of nlp
tasks including part-of-speech tagging, id39, analogy/similarity querying,
id68, and id33 (al-rfou et al., 2013; al-rfou et al., 2015; mikolov et al.,
2013a;b; chen & manning, 2014). extensive research has been done to tune these embeddings to
various tasks by incorporating features such as character (compositional) information, word order
information, and multi-word (phrase) information (ling et al., 2015; mikolov et al., 2013c; zhang
et al., 2015; trask et al., 2015).
despite these advancements, most id27 techniques share a common problem in that each
word must encode all of its potential meanings into a single vector (huang et al., 2012). for words
with multiple meanings (or    senses   ), this creates a superposition in vector space where a vector
takes on a mixture of its individual meanings. in this work, we will show that this superposition

1

under review as a conference paper at iclr 2016

obfuscates the context speci   c meaning of a word and can have a negative effect on nlp classi   ers
leveraging the superposition as input data. furthermore, we will show that disambiguating multiple
word senses into separate embeddings alleviates this problem and the corresponding confusion to an
nlp model.

2 related work

2.1 id97

mikolov et al. (2013a) proposed two simple methods for learning continuous id27s
using neural networks based on skip-gram or continuous-bag-of-word (cbow) models and named
it id97. word vectors built from these methods map words to points in space that effectively
encode semantic and syntactic meaning despite ignoring word order information. furthermore, the
word vectors exhibited certain algebraic relations, as exempli   ed by example:    v[man] - v[king] +
v[queen]     v[woman]   . subsequent work leveraging such neural id27s has proven to
be effective on a variety of natural id38 tasks (al-rfou et al., 2013; al-rfou et al.,
2015; chen & manning, 2014).

2.2 wang2vec

because id27s in id97 are insensitive to word order, they are suboptimal when used
for syntactic tasks like id52 or id33. ling et al. (2015) proposed modi   ca-
tions to id97 that incorporated word order. consisting of structured skip-gram and continuous
window methods that are together termed wang2vec, these models demonstrate signi   cant ability
to model syntactic representations. they come, however, at the cost of computation speed. fur-
thermore, because words have a single vector representation in wang2vec, the method is unable to
model polysemic words with multiple meanings. for instance, the word    work    in the sentence    we
saw her work    can be either a verb or noun depending on the broader context in surrounding this
sentence. this technique encodes the co-occurrence statistics for each sense of a word into one or
more    xed dimensional embeddings, generating embeddings that model multiple uses of a word.

2.3 statistical multi-prototype vector-space models of word meaning

perhaps a seminal work to vector-space word-sense disambiguation, the approach by reisinger &
mooney (2010) creates a vector-space model that encodes multiple meanings for words by    rst
id91 the contexts in which a word appears. once the contexts are clustered, several prototype
vectors can be initialized by averaging the statistically generated vectors for each word in the cluster.
this process of computing clusters and creating embeddings based on a vector for each cluster
has become the canonical strategy for word-sense disambiguation in vector spaces. however, this
approach presents no strategy for the context speci   c selection of potentially many vectors for use
in an nlp classi   er.

2.4 id91 weighted average context embeddings

our technique is inspired by the work of huang et al. (2012), which uses a multi-prototype neu-
ral vector-space model that clusters contexts to generate prototypes. unlike reisinger & mooney
(2010), the context embeddings are generated by a neural network in the following way: given a
pre-trained id27 model, each context embedding is generated by computing a weighted
sum of the words in the context (weighted by tf-idf). then, for each term, the associated context
embeddings are clustered. the clusters are used to re-label each occurrence of each word in the cor-
pus. once these terms have been re-labeled with the cluster   s number, a new word model is trained
on the labeled embeddings (with a different vector for each) generating the word-sense embeddings.
in addition to the selection problem and id91 overhead described in the previous subsection,
this model also suffers from the need to train neural id27s twice, which is a very expen-
sive endeavor.

2

under review as a conference paper at iclr 2016

2.5 id91 convolutional context embeddings

recent work has explored leveraging convolutional approaches to modeling the context embeddings
that are clustered into word prototypes. unlike previous approaches, chen et al. (2015) selects the
number of word clusters for each word based on the number of de   nitions for a word in the id138
gloss (as opposed to other approaches that commonly pick a    xed number of clusters). a variant
on the mssg model of neelakantan et al. (2015), this work uses the id138 glosses dataset and
convolutional embeddings to initialize the word prototypes.
in addition to the selection problem, id91 overhead, and the need to train neural embeddings
multiple times, this higher-quality model is somewhat limited by the vocabulary present in the en-
glish id138 resource. furthermore, the majority of the id138s relations connect words from
the same part-of-speech (pos).    thus, id138 really consists of four sub-nets, one each for nouns,
verbs, adjectives and adverbs, with few cross-pos pointers.   1

3 the sense2vec model

we expand on the work of huang et al. (2012) by leveraging supervised nlp labels instead of
unsupervised clusters to determine a particular word instance   s sense. this eliminates the need to
train embeddings multiple times, eliminates the need for a id91 step, and creates an ef   cient
method by which a supervised classi   er may consume the appropriate word-sense embedding.

figure 1: a graphical representation of wang2vec.

figure 2: a graphical representation of sense2vec.

given a labeled corpus (either by hand or by a model) with one or more labels per word, the
sense2vec model    rst counts the number of uses (where a unique word maps set of one or more

1https://id138.princeton.edu/

3

under review as a conference paper at iclr 2016

labels/uses) of each word and generates a random    sense embedding    for each use. a model is then
trained using either the cbow, skip-gram, or structured skip-gram model con   gurations. instead
of predicting a token given surrounding tokens, this model predicts a word sense given surrounding
senses.

3.1 subjective evaluation - subjective baseline

for subjective evaluation of these id27s, we trained models using several datasets for
comparison. first, we trained using id97   s continuous bag of words 2 approach on the large
unlabeled corpus used for the google word analogy task 3. several id27s and their
closest terms measured by cosine similarity are displayed in table 1 below.

table 1: single-sense baseline cosine similarities
apple
iphone
ipad

bank
banks
banking

hsbc

citibank
lender
lending

1.0
.718
.672
.599 microsoft
.586
.566
.559

ipod
imac
iphones

1.0
.687
.649
.603
.595
.594
.578

so
but
it
if
even
do
just

1.0
bad
.879
good
.858 worse
lousy
.842
stupid
.833
horrible
.831
.808
awful

1.0
.727
.718
.717
.710
.703
.697

perfect
perfection
perfectly
ideal
   awless
good
always

1.0
.681
.670
.644
.637
.622
.572

in this table, observe that the    bank    column is similar to proper nouns (   hsbc   ,    citibank   ), verbs
(   lending   ,   banking   ), and nouns (   banks   ,   lender   ). this is because the term    bank    is used in 3
different ways, as a proper noun, verb, and noun. this embedding for    bank    has modeled a mixture
of these three meanings.    apple   ,    so   ,    bad   , and    perfect    can also have a mixture of meanings. in
some cases, such as    apple   , one interpretation of the word is completely ignored (apple the fruit).
in the case of    so   , there is also an interjection sense of    so    that is not well represented in the vector
space.

3.2 subjective evaluation - part-of-speech disambiguation

for part-of-speech disambiguation, we labeled the dataset from section 3.1 with part-of-speech
tags using the polyglot universal dependency part-of-speech tagger of al-rfou et al. (2013) and
trained sense2vec with identical parameters as section 3.1. in table 2, we see that this method has
successfully disambiguated the difference between the noun    apple    referring to the fruit and the
proper noun    apple    referring to the company. in table 3, we see that all three uses of the word
   bank    have been disambiguated by their respective parts of speech, and in table 4, nuanced senses
of the word    so    have also been disambiguated.

table 2: part-of-speech cosine similarities for the word: apple

1.0

apple

apple
apples
pear
peach

noun
noun .639 microsoft
noun .581
iphone
ipad
noun .579
blueberry noun .570
almond
noun .541

samsung
blackberry

propn
1.0
propn .603
noun
.591
noun
.586
propn .572
propn .564

2command line params: -size 500 -window 10 -negative 10 -hs 0 -sample 1e-5 -iter 3 -min-count 10
3the data.txt    le generated from http://id97.googlecode.com/svn/trunk/demo-train-big-model-v1.sh

4

under review as a conference paper at iclr 2016

table 3: part-of-speech cosine similarities for the word: bank
noun
1.0
bank
noun .786
banks
banking noun .629
noun .619
lender
citibank
propn .570 wachovia
bank
ubs
propn .535
grindlays

1.0
propn
.570
noun
propn .536
propn .523
propn .503
propn .492

bank
1.0
gamble verb .533
verb .485
earn
invest
verb .470
reinvest verb .466
donate
verb .466

bank
bank
hsbc

verb

table 4: part-of-speech cosine similarities for the word: so
intj
intj
intj
intj
intj
intj

adj
adj
condemnable adj
adj
adj
adj

adv
adv
conj
sconj
adv
adv

disputable
disapprove
contestable

1.0
.753
.752
.720
.694
.671

1.0
.527
.520
.513
.505
.503

but
really

so
too
but

because

poved

so

1.0
.588
.584
.578
.559
.558

so
now

obviously
basically

okay

actually

3.3 subjective evaluation - sentiment disambiguation

for sentiment disambiguation, the imdb labeled training corpus was labeled with part-of-speech
tags using the polyglot part-of-speech tagger from al-rfou et al. (2013). adjectives were then
labeled with the positive or negative sentiment associated with each comment. a cbow sense2vec
model was then trained on the resulting dataset, disambiguating between both part-of-speech and
sentiment (for adjectives).
table 5 shows the difference between the positive and negative vectors for the word    bad   . the neg-
ative vector is most similar to word indicating the classical meaning of bad (including the negative
version of    good   , e.g.    good grief!   ). the positive    bad    vector denotes a tone of sarcasm, most
closely relating to the positive sense of    good    (e.g.    good job!   ).

table 5: sentiment cosine similarities for the word: bad

bad
neg 1.0
bad
terrible neg .905
good
horrible neg .872 wrong
neg .870
funny
awful
neg .863
good
great
neg .845 weird
stupid

pos
pos
pos
pos
pos
pos

1.0
.753
.752
.720
.694
.671

table 6 shows the positive and negative senses of the word    perfect   . the positive version of the
word clusters most closely with words indicating excellence. the positive version clusters with the
more sarcastic interpretation.

5

under review as a conference paper at iclr 2016

table 6: sentiment cosine similarities for the word: perfect

perfect

real

1.0

neg
perfect
neg 0.682 wonderful
brilliant
incredible
fantastic

unfortunate neg 0.680
neg 0.673
neg 0.673
neg 0.673
neg 0.661
neg 0.650

serious
complete
ordinary
typical

misguided

great

excellent
amazing

pos
pos
pos
pos
pos
pos
pos
pos

1.0
0.843
0.842
0.840
0.839
0.823
0.822
0.814

4 named entity resolution

to evaluate the embeddings when disambiguating on named entity resolution (ner), we labeled
the standard id97 dataset from section 3.2 with named entity labels. this demonstrated how
sense2vec can also disambiguate between multi-word sequences of text as well as single word se-
quences of text. below, we see that the word    washington    is disambiguated with both a person
and a gpe sense of the word. furthermore, we see that hillary clinton is very similar to titles that
she has held within the time span of the dataset.

table 7: disambiguation for the word: washington

george washington

henry knox

philip schuyler
nathanael greene
benjamin lincoln

william howe

gpe .665
person name .656 washington d
gpe .591
person name .624 washington dc
person name .618
gpe .559
person name .613 warsaw embassy gpe .524
gpe .516
person name .602
person name .591
gpe .507

maryland

seattle

wash

table 8: entity resolution for the term: hillary clinton

secretary of state

senator
senate
chief

white house

congress

title
title

0.661
0.613
org name 0.564
0.555
org name 0.564
org name 0.547

title

5 neural id33

to quantitatively evaluate disambiguated sense embeddings relative to the current standard, we com-
pared sense2vec embeddings and wang2vec embeddings on neural syntactic id33
tasks in six languages. first, we trained two sets of embeddings on the bulgarian, german, english,
french, italian, and swedish wikipedia datasets from the polyglot website4. the baseline em-
beddings were trained without any part-of-speech disambiguation using the structured skip-gram
approach of ling et al. (2015). for each language, the sense2vec embeddings were trained by
disambiguating terms using the language speci   c polyglot part-of-speech tagger of al-rfou et al.
(2013), and embedded in the same structured skip-gram approach. both were trained using identical
parametrization 5.

4https://sites.google.com/site/rmyeid/projects/polyglot
5command line params: -size 50 -window 5 -negative 10 -hs 0 -sample 1e-4 -iter 5 -cap 0

6

under review as a conference paper at iclr 2016

each of these embeddings was used to train a dependency parse model using the parser outlined in
(chen & manning, 2014). all were trained on the the respective language   s universal dependencies
treebank. the standard splits were used.6 for the parser trained on the sense2vec emeddings, the
pos speci   c embedding was used as the input. the part-of-speech label was determined using the
gold-standard pos tags from the treebank. it should be noted that the parser of (chen & manning,
2014) uses trained part-of-speech embeddings as input which are indexed based on gold-standard
pos tags. thus, differences in quality between parsers trained on the two embedding styles are
due to clarity in the id27s as opposed to the addition of part-of-speech information
because both model styles train on gold standard pos information. for each language, the unlabeled
attachment scores are outlined in table 7.

wang

table 9: unlabeled attachment scores and percent error reductions
set
dev
test*
test
dev
test*
test
dev
test
error
margin abs.
avg.

swedish mean
bulgarian german english french
80.28
78.94
73.82
68.86
78.60
82.47
70.10
60.25
78.88
82.51
70.53
60.54
81.21
81.94
75.43
72.61
80.38
84.44
71.66
64.17
72.16
84.60
80.69
64.43
6.56% 3.98% 12.06% 8.52%
13.69%
10.95% 12.82% 5.50% 8.21% 12.71% 8.78%
10.93% 14.54% 5.86% 5.32% 13.58% 9.23%
12.32% 10.29% 6.03% 6.09% 12.39%

90.03
90.17
90.39
90.69
90.41
90.86
7.05%
2.47%
5.17%
4.76%

italian
84.99
84.99
85.45
85.57
86.13
86.18

85.02
83.61
83.88
86.10
85.48
85.93
7.76%

sense

the    error margin    section of table 7 describes the percentage reduction in error for each language.
disambiguating based on part-of-speech using sense2vec reduced the error in all six languages with
an average reduction greater than 8%.

6 conclusion and future work

in this work, we have proposed a new model for id51 that uses supervised
nlp labeling to disambiguate between word senses. much like previous models, it leverages a form
of context id91 to disambiguate the use of a term. however, instead of using unsupervised clus-
tering methods, our approach clusters using supervised labels which can analyze a speci   c word   s
context and assign a label. this signi   cantly reduces the computational overhead of word-sense
modeling and provides a natural mechanism for other nlp tasks to select the appropriate sense em-
bedding. furthermore, we show that disambiguated embeddings can increase the accuracy of syn-
tactic id33 in a variety of languages. future work will explore how disambiguated
embeddings perform using other varieties of supervised labels and consuming nlp tasks.

references
al-rfou, rami, perozzi, bryan, and skiena, steven. polyglot: distributed word representations
for multilingual nlp. corr, abs/1307.1662, 2013. url http://arxiv.org/abs/1307.
1662.

al-rfou, rami, kulkarni, vivek, perozzi, bryan, and skiena, steven. polyglot-ner: massive mul-
tilingual id39. proceedings of the 2015 siam international conference on
data mining, vancouver, british columbia, canada, april 30 - may 2, 2015, april 2015.

bengio, yoshua, ducharme, r  ejean, vincent, pascal, and janvin, christian. a neural probabilistic

language model. j. mach. learn. res., 3:1137   1155, march 2003. issn 1532-4435.

6the german, french, and italian treebanks had occasional tokens that both spanned multiple indices and
overlapped with the index of the previous and following token (ex. 0, 0-1, 1,...), a property which is incompati-
ble with the (chen & manning, 2014) parser. these tokens were removed. if their removal created a malformed
tree, the sentence was removed automatically by the parser and logged accordingly.

7

under review as a conference paper at iclr 2016

chen, danqi and manning, christopher. a fast and accurate dependency parser using neural net-
works. in proceedings of the 2014 conference on empirical methods in natural language pro-
cessing (emnlp), pp. 740   750, doha, qatar, october 2014. association for computational lin-
guistics. url http://www.aclweb.org/anthology/d14-1082.

chen, tao, xu, ruifeng, he, yulan, and wang, xuan. improving distributed representation of word
sense via id138 gloss composition and context id91. in proceedings of the 53rd annual
meeting of the association for computational linguistics and the 7th international joint con-
ference on natural language processing (volume 2: short papers), pp. 15   20, beijing, china,
july 2015. association for computational linguistics. url http://www.aclweb.org/
anthology/p15-2003.

g.e. hinton, j.l. mcclelland, d.e. rumelhart. distributed representations. parallel dis-tributed

processing: explorations in the microstructure of cognition, 1(3):77   109, 1986.

huang, eric h., socher, richard, manning, christopher d., and ng, andrew y. improving word
in proceedings of the 50th
representations via global context and multiple word prototypes.
annual meeting of the association for computational linguistics: long papers - volume 1, acl
   12, pp. 873   882, stroudsburg, pa, usa, 2012. association for computational linguistics. url
http://dl.acm.org/citation.cfm?id=2390524.2390645.

liang, p. and potts, c. bringing machine learning and id152 together. annual

reviews of linguistics, 1(1):355   376, 2015.

ling, wang, dyer, chris, black, alan w, and trancoso, isabel. two/too simple adaptations of
id97 for syntax problems. in proceedings of the 2015 conference of the north american
chapter of the association for computational linguistics: human language technologies, pp.
1299   1304, denver, colorado, may   june 2015. association for computational linguistics. url
http://www.aclweb.org/anthology/n15-1142.

maas, andrew l and ng, andrew y. a probabilistic model for semantic word vectors. in nips

workshop on deep learning and unsupervised id171, 2010.

mikolov, tomas, chen, kai, corrado, greg, and dean, jeffrey. ef   cient estimation of word repre-
sentations in vector space. corr, abs/1301.3781, 2013a. url http://arxiv.org/abs/
1301.3781.

mikolov, tomas, le, quoc v., and sutskever, ilya. exploiting similarities among languages for
machine translation. corr, abs/1309.4168, 2013b. url http://arxiv.org/abs/1309.
4168.

mikolov, tomas, sutskever, ilya, chen, kai, corrado, greg, and dean, jeffrey. distributed repre-
sentations of words and phrases and their compositionality. corr, abs/1310.4546, 2013c. url
http://arxiv.org/abs/1310.4546.

mnih, andriy and hinton, geoffrey e. a scalable hierarchical distributed language model.

in
koller, d., schuurmans, d., bengio, y., and bottou, l. (eds.), advances in neural information
processing systems 21, pp. 1081   1088. curran associates, inc., 2009.

morin, frederic and bengio, yoshua. hierarchical probabilistic neural network language model. in
proceedings of the international workshop on arti   cial intelligence and statistics, pp. 246   252.
citeseer, 2005.

neelakantan, arvind, shankar, jeevan, passos, alexandre, and mccallum, andrew. ef   cient non-
parametric estimation of multiple embeddings per word in vector space. corr, abs/1504.06654,
2015. url http://arxiv.org/abs/1504.06654.

reisinger, joseph and mooney, raymond j. multi-prototype vector-space models of word meaning.
in human language technologies: the 2010 annual conference of the north american chapter
of the association for computational linguistics, hlt    10, pp. 109   117, stroudsburg, pa, usa,
2010. association for computational linguistics.
isbn 1-932432-65-5. url http://dl.
acm.org/citation.cfm?id=1857999.1858012.

8

under review as a conference paper at iclr 2016

trask, andrew, gilmore, david, and russell, matthew. modeling order in neural id27s

at scale. corr, abs/1506.02338, 2015. url http://arxiv.org/abs/1506.02338.

zhang, xiang, zhao, junbo, and lecun, yann. character-level convolutional networks for text clas-

si   cation. corr, abs/1509.01626, 2015. url http://arxiv.org/abs/1509.01626.

9

