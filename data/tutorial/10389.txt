5
1
0
2

 

n
u
j
 

1

 
 
]

g
l
.
s
c
[
 
 

1
v
9
1
6
0
0

.

6
0
5
1
:
v
i
x
r
a

blocks and fuel

blocks and fuel: frameworks for deep learning

bart van merri  enboer
montreal institute for learning algorithms, university of montreal, montreal, canada

bart.van.merrienboer@umontreal.ca

dzmitry bahdanau
jacobs university, bremen, germany

d.bahdanau@jacobs-university.de

vincent dumoulin
dmitriy serdyuk
david warde-farley
montreal institute for learning algorithms, university of montreal, montreal, canada

dumouliv@iro.umontreal.ca

serdyuk@iro.umontreal.ca

wardefar@iro.umontreal.ca

jan chorowski
university of wroc law, wroc law, poland

jan.chorowski@ii.uni.wroc.pl

yoshua bengio
montreal institute for learning algorithms, university of montreal, montreal, canada
cifar senior fellow

yoshua.bengio@umontreal.ca

abstract

we introduce two python frameworks to train neural networks on large datasets: blocks and
fuel. blocks is based on theano, a id202 compiler with cuda-support (bastien et al.,
2012; bergstra et al., 2010). it facilitates the training of complex neural network models by
providing parametrized theano operations, attaching metadata to theano   s symbolic com-
putational graph, and providing an extensive set of utilities to assist training the networks,
e.g. training algorithms, logging, monitoring, visualization, and serialization. fuel provides
a standard format for machine learning datasets. it allows the user to easily iterate over
large datasets, performing many types of pre-processing on the    y.

keywords: neural networks, gpgpu, large-scale machine learning

1. introduction

blocks and fuel are being developed by the montreal institute of learning algorithms
(mila) at the university of montreal. their focus lies on quick prototyping of complex
neural network models. the intended target audience is researchers who design and exper-
iment machine learning algorithms, especially deep learning algorithms.

several other libraries built on top of theano exist, including pylearn2 and groundhog
(also developed by mila), lasagne, and keras. like its mila-developed predecessors,
blocks maintains a focus on research and rapid prototyping. blocks di   erentiates itself most
notably from the above mentioned toolkits in its unique relationship with theano. instead
of introducing new abstract objects representing    models    or    layers   , blocks annotates the
theano computational graph, maintaining the    exibility of theano while making large
models manageable.

data processing is an integral part of training neural networks, which is not addressed
by many of the aforementioned frameworks. fuel aims to    ll this gap. it provides tools to
download datasets and iterate/preprocess them e   ciently.

1

van merri  enboer et al.

both blocks and fuel were developed from the very beginning with a strong focus on
software engineering best practices. the development teams strive for high test coverage,
thorough documentation and carefully considered apis.

2. blocks

blocks comprises several components, which can be used independently from each other.

2.1 bricks

theano is a popular choice for the implementation of neural networks (see e.g. goodfellow et al.
(2013b); pascanu et al. (2013)). blocks and many other libraries, such as pylearn2 (goodfellow et al.,
2013a), build on theano by providing reusable components that are common in neural
networks, such as linear transformations followed by non-linear activations, or more com-
plicated components such as lstm units. in blocks these components are referred to as
bricks or    parametrized theano operations   .

bricks consist of a set of theano shared variables, for example the weight matrix of a
linear transformation or the    lters of a convolutional layer. bricks use these parameters to
transform symbolic theano variables.

bricks can contain other bricks within them. this introduces a hierarchy on top of the
   at computational graph de   ned by theano, which makes it easier to address and con   gure
complex models programmatically.

the parameters of bricks can be initialized using a variety of schemes that are popular
in the neural network literature, such as sparse initialization, orthogonal initialization for
recurrent weights, etc.

blocks comes with a large number of    bricks   . besides standard activations and transfor-
mations used in feedforward networks (maxout, convolutional layers, table lookups) these
also include a variety of more advanced recurrent neural network components like lstm,
gru, and support for attention mechanisms (for an overview of di   erent kinds of net-
work architectures, id173 methods, and optimization algorithms see bengio et al.
(2015)).

2.2 graph management

large neural networks can often result in theano computational graphs containing hundreds
of variables and operations. blocks does not attempt to abstract away this complex graph,
but to make it manageable by annotating variables in the graph. each input, output, and
parameter of a brick is annotated as such. variables can also be annotated with the role
they play in a model, such as weights, biases,    lters, etc.

a series of convenience tools were written that allow users to    lter the symbolic compu-
tational graph based on these annotations, and apply transformations to the graph. many
id173 methods such as weight decay, weight noise, or dropout can be implemented
in a generic, model-agnostic way. furthermore a complex query mechanism allows for their
   ne-grained application such as    apply weight noise to all weights that belong to an lstm
unit whose parent is a brick with the name foo   .

2

blocks and fuel

2.3 training algorithms

the id119 training algorithm in blocks is composed of di   erent    step rules   
that modify the descent direction (learning rate scaling, momentum, gradient clipping,
weight norm clipping, etc.). a variety of algorithms such as adagrad, adadelta, adam,
rmsprop are available as step rules.

2.4 training

experiment management is performed using a    main loop   , which combines a theano graph
with a training algorithm and a fuel data stream. the main loop has a    exible extension
interface, which is used to perform tasks such as monitoring on a validation set, serialization,
learning rate scheduling, plotting, printing and saving logs, etc.

3. fuel

fuel   s goal is to provide a common interface to a variety of data formats and published
datasets such as mnist, cifar-10, id163, etc. while making it easy for users to write
an interface to new datasets.

blocks relies on fuel for its data interface, but fuel can easily be used by other machine

learning frameworks that interface with datasets.

3.1 iteration and preprocessing pipeline

fuel allows for di   erent ways of iterating over these datasets, such as sequential or shuf-
   ed minibatches, support for in-memory and out-of-core datasets, and resampling (cross
validation, id64).

it also provides a variety of on-the-   y preprocessing methods such as random crop-
ping of images, creating id165s from text    les, and the ability to implement many other
methods easily. these preprocessing steps can be chained together to form more complex
transformations of the input data.

to sidestep python   s global interpreter lock (gil) and ensure optimal performance,
fuel can perform all operations in a separate process, transferring the processed data to the
training process using tcp sockets.

3.2 standardized data format

datasets are distributed in a wide range of formats. fuel simpli   es dataset storage by
converting all built-in datasets to annotated hdf5    les (the hdf group, 1997-2015). in
addition to being an e   cient format for large datasets that don   t    t into memory, hdf5
is easy to organize and document. all of the data is stored in a single hdf5    le, with the
following metadata attached:

    what are the data sources available (e.g. features, targets, etc.)?

    how are these data sources o   cially split (e.g. training, validation, and test sets)?

    are some data sources unavailable for some splits (e.g. test set only o   ers unlabeled

examples)?

3

van merri  enboer et al.

    what are the axes semantics for a given data source (e.g. batch, feature, width, height,

channel, time, etc.)?

integrating user data into fuel via hdf5 is straightforward, and simply requires the
data to be written to an hdf5    le with metadata according to the speci   cations. finally,
while standardizing by convention on hdf5, the fuel dataset api is independent of it;
users are free to implement dataset objects employing other backends and use them with
the rest of fuel   s components.

3.3 automated data management

fuel o   ers built-in scripts that automate the task of downloading datasets, (similar to e.g.
skdata1) and converting them to fuel   s hdf5 speci   cation.

the fuel    download script is used to download raw data    les. downloading the raw
mnist data    les is as easy as typing fuel    download mnist. the fuel    convert script is
used to convert raw data    les into hdf5-format.

reproducibility being an important feature of both fuel and blocks, the fuel    convert
script automatically tags all    les it creates with relevant module and interface versions and
the exact command that was used to generate these    les. inspection of this metadata is
done with the fuel    info script.

4. serialization and checkpointing

the training of large, deep neural networks can often take days or even weeks. hence, regular
checkpointing of training progress is important. blocks aims to make the resumption of
experiments entirely transparent, even across platforms, while ensuring the reproducibility
of these experiments.

this goal is complicated by shortcomings in python   s pickle serialization module,
which is unable to serialize many iterators, which fuel heavily depends on in order to
iterate over large datasets e   ciently. to circumvent this we reimplemented the itertools
module from the python standard library to be serializable2.

as a result, blocks experiments are able to be interrupted in the middle of a pass over

the dataset, serialized, and resumed later, without a   ecting the    nal training results.

5. documentation and community

blocks and fuel are well documented, with both api documentation and tutorials available
online. two active mailing lists3 support users of the libraries. a separate repository4 is
maintained for users to contribute non-trivial examples of the use of blocks. implementa-
tions of id4 models (id4, bahdanau et al. (2015)) and the deep
recurrent attentive writer (draw, gregor et al. (2015)) model are publicly available ex-
amples of state-of-the-art models succesfully implemented using blocks.

1. https://jaberg.github.io/skdata/
2. https://github.com/mila-udem/picklable-itertools
3. https://groups.google.com/d/forum/blocks-users and https://groups.google.com/d/forum/fuel-users
4. https://github.com/mila-udem/blocks-examples

4

blocks and fuel

acknowledgments

the authors would like to acknowledge the support of the following agencies for research
funding and computing support: nserc, calcul qu  ebec, compute canada, the canada
research chairs and cifar. bahdanau thanks planet intelligent systems gmbh for their
   nancial support. we would also like to thank the developers of theano.

references

dzmitry bahdanau, kyunghyun cho, and yoshua bengio. id4 by
jointly learning to align and translate. in proceedings of the international conference on
learning representations (iclr), 2015.

fr  ed  eric bastien, pascal lamblin, razvan pascanu, james bergstra, ian j. goodfellow,
arnaud bergeron, nicolas bouchard, and yoshua bengio. theano: new features and speed
improvements. nips workshop: deep learning and unsupervised id171,
2012.

yoshua bengio, ian j. goodfellow, and aaron courville. deep learning. book in preparation
for mit press, 2015. url http://www.iro.umontreal.ca/  bengioy/dlbook.

james bergstra, olivier breuleux, fr  ed  eric bastien, pascal lamblin, razvan pascanu, guil-
laume desjardins, joseph turian, david warde-farley, and yoshua bengio. theano: a
cpu and gpu math expression compiler.
in proceedings of the python for scienti   c
computing conference (scipy), june 2010.

ian j. goodfellow, david warde-farley, pascal lamblin, vincent dumoulin, mehdi mirza,
razvan pascanu, james bergstra, fr  ed  eric bastien, and yoshua bengio. pylearn2:
a machine learning research library.
arxiv preprint arxiv:1308.4214, 2013a. url
http://arxiv.org/abs/1308.4214.

ian j. goodfellow, david warde-farley, mehdi mirza, aaron courville, and yoshua bengio.

maxout networks. pages 1319   1327, 2013b.

karol gregor, ivo danihelka, alex graves, and daan wierstra. draw: a recurrent neural

network for image generation. arxiv preprint arxiv:1502.04623, 2015.

razvan pascanu, tomas mikolov, and yoshua bengio. on the di   culty of training recurrent

neural networks. 2013.

the hdf group.

hierarchical data

format,

version

5,

1997-2015.

http://www.hdfgroup.org/hdf5/.

5

