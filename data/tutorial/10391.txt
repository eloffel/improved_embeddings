take and took, gaggle and goose, book and read: evaluating the utility

of vector differences for lexical relation learning

ekaterina vylomova,1 laura rimell,2 trevor cohn,1 and timothy baldwin1
1department of computing and information systems, university of melbourne

evylomova@gmail.com laura.rimell@cl.cam.ac.uk {tcohn,tbaldwin}@unimelb.edu.au

2computer laboratory, university of cambridge

6
1
0
2

 

g
u
a
3
1

 

 
 
]
l
c
.
s
c
[
 
 

4
v
2
9
6
1
0

.

9
0
5
1
:
v
i
x
r
a

abstract

recent work has shown that simple vector
subtraction over id27s is surpris-
ingly effective at capturing different lexical
relations, despite lacking explicit supervision.
prior work has evaluated this intriguing result
using a word analogy prediction formulation
and hand-selected relations, but the generality
of the    nding over a broader range of lexical
relation types and different learning settings
has not been evaluated. in this paper, we carry
out such an evaluation in two learning settings:
(1) spectral id91 to induce word rela-
tions, and (2) supervised learning to classify
vector differences into relation types. we    nd
that id27s capture a surprising
amount of information, and that, under suit-
able supervised training, vector subtraction
generalises well to a broad range of relations,
including over unseen lexical items.

1

introduction

learning to identify lexical relations is a fundamen-
tal task in natural language processing (   nlp   ), and
can contribute to many nlp applications including
id141 and generation, machine translation,
and ontology building (banko et al., 2007; hen-
drickx et al., 2010).

recently, attention has been focused on iden-
tifying lexical relations using id27s,
which are dense, low-dimensional vectors ob-
tained either from a    predict-based    neural net-
work trained to predict word contexts, or a    count-
based    traditional distributional similarity method
combined with id84. the skip-
gram model of mikolov et al. (2013a) and other
similar language models have been shown to per-
form well on an analogy completion task (mikolov
et al., 2013b; mikolov et al., 2013c; levy and
goldberg, 2014a), in the space of relational sim-

ilarity prediction (turney, 2006), where the task
is to predict the missing word in analogies such
as a:b :: c:    ?   . a well-known example involves
predicting the vector queen from the vector com-
bination king     man + woman, where linear
operations on word vectors appear to capture the
lexical relation governing the analogy, in this
case opposite-gender. the results extend to
several semantic relations such as capital-of
(paris   france+poland     warsaw) and mor-
phosyntactic relations such as pluralisation
(cars     car + apple     apples). remarkably,
since the model is not trained for this task, the re-
lational structure of the vector space appears to be
an emergent property.

the key operation in these models is vector dif-
ference, or vector offset. for example, the paris    
france vector appears to encode capital-of, pre-
sumably by cancelling out the features of paris
that are france-speci   c, and retaining the features
that distinguish a capital city (levy and goldberg,
2014a). the success of the simple offset method
on analogy completion suggests that the difference
vectors (   diffvec    hereafter) must themselves
be meaningful: their direction and/or magnitude
encodes a lexical relation.

previous analogy completion tasks used with
id27s have limited coverage of lexical
relation types. moreover, the task does not explore
the full implications of diffvecs as meaningful
vector space objects in their own right, because
it only looks for a one-best answer to the particu-
lar lexical analogies in the test set. in this paper,
we introduce a new, larger dataset covering many
well-known lexical relation types from the linguis-
tics and cognitive science literature. we then apply
diffvecs to two new tasks: unsupervised and su-
pervised id36. first, we cluster the
diffvecs to test whether the clusters map onto
true lexical relations. we    nd that the id91

works remarkably well, although syntactic relations
are captured better than semantic ones.

second, we perform classi   cation over the diff-
vecs and obtain remarkably high accuracy in a
closed-world setting (over a prede   ned set of word
pairs, each of which corresponds to a lexical re-
lation in the training data). when we move to an
open-world setting including random word pairs
    many of which do not correspond to any lexical
relation in the training data     the results are poor.
we then investigate methods for better attuning the
learned class representation to the lexical relations,
focusing on methods for automatically synthesis-
ing negative instances. we    nd that this improves
the model performance substantially.

we also    nd that hyper-parameter optimised
count-based methods are competitive with predict-
based methods under both id91 and super-
vised relation classi   cation, in line with the    nd-
ings of levy et al. (2015a).

2 background and related work

a lexical relation is a binary relation r holding be-
tween a word pair (wi, wj); for example, the pair
(cart, wheel) stands in the whole-part relation.
relation learning in nlp includes relation extrac-
tion, relation classi   cation, and relational similarity
prediction. in id36, related word pairs
in a corpus and the relevant relation are identi   ed.
given a word pair, the relation classi   cation task in-
volves assigning a word pair to the correct relation
from a pre-de   ned set. in the open information ex-
traction paradigm (banko et al., 2007; weikum and
theobald, 2010), also known as unsupervised re-
lation extraction, the relations themselves are also
learned from the text (e.g. in the form of text labels).
on the other hand, relational similarity prediction
involves assessing the degree to which a word pair
(a, b) stands in the same relation as another pair
(c, d), or to complete an analogy a:b :: c:    ?   . re-
lation learning is an important and long-standing
task in nlp and has been the focus of a number of
shared tasks (girju et al., 2007; hendrickx et al.,
2010; jurgens et al., 2012).

recently, attention has turned to using vector
space models of words for relation classi   cation
and relational similarity prediction. distributional
word vectors have been used for detection of rela-
tions such as hypernymy (geffet and dagan, 2005;
kotlerman et al., 2010; lenci and benotto, 2012;
weeds et al., 2014; rimell, 2014; santus et al.,
2014) and qualia structure (yamada et al., 2009).

an exciting development, and the inspiration for
this paper, has been the demonstration that vec-
tor difference over id27s (mikolov
et al., 2013c) can be used to model word anal-
ogy tasks. this has given rise to a series of pa-
pers exploring the diffvec idea in different con-
texts. the original analogy dataset has been used to
evaluate predict-based language models by mnih
and kavukcuoglu (2013) and also zhila et al.
(2013), who combine a neural language model with
a pattern-based classi   er. kim and de marneffe
(2013) use id27s to derive representa-
tions of adjective scales, e.g. hot   warm   cool   
cold. fu et al. (2014) similarly use embeddings to
predict hypernym relations, in this case id91
words by topic to show that hypernym diffvecs
can be broken down into more    ne-grained rela-
tions. neural networks have also been developed
for joint learning of lexical and relational similar-
ity, making use of the id138 relation hierarchy
(bordes et al., 2013; socher et al., 2013; xu et al.,
2014; yu and dredze, 2014; faruqui et al., 2015;
fried and duh, 2015).

another strand of work responding to the vector
difference approach has analysed the structure of
predict-based embedding models in order to help
explain their success on the analogy and other tasks
(levy and goldberg, 2014a; levy and goldberg,
2014b; arora et al., 2015). however, there has been
no systematic investigation of the range of relations
for which the vector difference method is most
effective, although there have been some smaller-
scale investigations in this direction. makrai et al.
(2013) divide antonym pairs into semantic classes
such as quality, time, gender, and distance,    nd-
ing that for about two-thirds of antonym classes,
diffvecs are signi   cantly more correlated than
random. necs  ulescu et al. (2015) train a classi   er
on word pairs, using id27s to predict
coordinates, hypernyms, and meronyms. roller and
erk (2016) analyse the performance of vector con-
catenation and difference on the task of predicting
lexical entailment and show that vector concatena-
tion overwhelmingly learns to detect hearst pat-
terns (e.g., including, such as). k  oper et al. (2015)
undertake a systematic study of morphosyntac-
tic and semantic relations on id27s
produced with id97 (   w2v    hereafter; see
  3.1) for english and german. they test a variety
of relations including word similarity, antonyms,
synonyms, hypernyms, and meronyms, in a novel
analogy task. although the set of relations tested by

k  oper et al. (2015) is somewhat more constrained
than the set we use, there is a good deal of overlap.
however, their evaluation is performed in the con-
text of relational similarity, and they do not perform
id91 or classi   cation on the diffvecs.

3 general approach and resources

we de   ne the task of lexical relation learning
to take a set of (ordered) word pairs {(wi, wj)}
and a set of binary lexical relations r = {rk},
and map each word pair (wi, wj) as follows: (a)
(wi, wj) (cid:55)    rk     r, i.e. the    closed-world    set-
ting, where we assume that all word pairs can be
uniquely classi   ed according to a relation in r; or
(b) (wi, wj) (cid:55)    rk     r     {  } where    signi   es
the fact that none of the relations in r apply to the
word pair in question, i.e. the    open-world    setting.
our starting point for lexical relation learning is
the assumption that important information about
various types of relations is implicitly embedded in
the offset vectors. while a range of methods have
been proposed for composing word vectors (baroni
et al., 2012; weeds et al., 2014; roller et al., 2014),
in this research we focus exclusively on diffvec
(i.e. w2     w1). a second assumption is that there
exist dimensions, or directions, in the embedding
vector spaces responsible for a particular lexical
relation. such dimensions could be identi   ed and
exploited as part of a id91 or classi   cation
method, in the context of identifying relations be-
tween word pairs or classes of diffvecs.

in order to test the generalisability of the diff-
vec method, we require: (1) id27s,
and (2) a set of lexical relations to evaluate against.
as the focus of this paper is not the word embed-
ding pre-training approaches so much as the utility
of the diffvecs for lexical relation learning, we
take a selection of four pre-trained word embed-
dings with strong currency in the literature, as de-
tailed in   3.1. we also include the state-of-the-art
count-based approach of levy et al. (2015a), to test
the generalisability of diffvecs to count-based
id27s.

for the lexical relations, we want a range of rela-
tions that is representative of the types of relational
learning tasks targeted in the literature, and where
there is availability of annotated data. to this end,
we construct a dataset from a variety of sources, fo-
cusing on lexical semantic relations (which are less
well represented in the analogy dataset of mikolov
et al. (2013c)), but also including morphosyntactic
and morphosemantic relations (see   3.2).

dimensions training data

name
w2v

glove
senna
hlbl
w2vwiki
glovewiki
svdwiki

300
200
100
200
300
300
300

100    109
6    109
37    106
37    106
50    106
50    106
50    106

table 1: the pre-trained id27s used in
our experiments, with the number of dimensions
and size of the training data (in word tokens). the
models trained on english wikipedia (   wiki   ) are
in the lower half of the table.

3.1 id27s
we consider four highly successful word embed-
ding models in our experiments: w2v (mikolov et
al., 2013a; mikolov et al., 2013b), glove (pen-
nington et al., 2014), senna (collobert and we-
ston, 2008), and hlbl (mnih and hinton, 2009),
as detailed below. we also include svd (levy et
al., 2015a), a count-based model which factorises
a positive pmi (ppmi) matrix. for consistency of
comparison, we train svd as well as a version
of w2v and glove (which we call w2vwiki and
glovewiki, respectively) on the english wikipedia
corpus (comparable in size to the training data of
senna and hlbl), and apply the preprocessing
of levy et al. (2015a). we additionally normalise
the w2vwiki and svdwiki vectors to unit length;
glovewiki is natively normalised by column.1

w2v cbow (continuous bag-of-words;
mikolov et al. (2013a)) predicts a word from its
context using a model with the objective:

t(cid:88)

i=1

log

j =

1
t

(cid:32)

(cid:80)

(cid:33)

i

w(cid:62)

(cid:32)

exp

(cid:80)v

k=1 exp

  wi+j

j   [   c,+c],j(cid:54)=0
w(cid:62)

(cid:80)

k

j   [   c,+c],j(cid:54)=0

  wi+j

(cid:33)

where wi and   wi are the vector representations
for the ith word (as a focus or context word, re-
spectively), v is the vocabulary size, t is the
number of tokens in the corpus, and c is the con-
text window size.2 google news data was used

1we ran a series of experiments on normalised and unnor-
malised w2v models, and found that normalisation tends to
boost results over most of our relations (with the exception
of lexsemevent and nouncoll). we leave a more detailed
investigation of normalisation to future work.

2in a slight abuse of notation, the subscripts of w do double

relation
lexsemhyper
lexsemmero
lexsemattr
lexsemcause
lexsemspace
lexsemref
lexsemevent
nounsp
verb3
verbpast
verb3past
lvc
verbnoun
prefix
nouncoll

description
hypernym
meronym
characteristic quality, action
cause, purpose, or goal
location or time association
expression or representation
object   s action
plural form of a noun
   rst to third person verb present-tense form
present-tense to past-tense verb form
third person present-tense to past-tense verb form
light verb construction
nominalisation of a verb
pre   xing with re morpheme
collective noun

source
semeval   12 + bless
semeval   12 + bless
semeval   12
semeval   12
semeval   12
semeval   12
bless

pairs
1173
2825
71
249
235
187
3583
100 msr
99 msr
100 msr
100 msr
58

tan et al. (2006b)

example
(animal, dog)
(airplane, cockpit)
(cloud, rain)
(cook, eat)
(aquarium,    sh)
(song, emotion)
(zip, coat)
(year, years)
(accept, accepts)
(know, knew)
(creates, created)
(give, approval)
(approve, approval)
(vote, revote)
(army, ants)

3303 id138
118 wiktionary
257 web source
table 2: description of the 15 lexical relations.

to train the model. we use the focus word vec-
tors, w = {wk}v
k=1, normalised such that each
(cid:107)wk(cid:107) = 1.

the glove model (pennington et al., 2014) is
based on a similar bilinear formulation, framed as
a low-rank decomposition of the matrix of corpus
co-occurrence frequencies:

v(cid:88)

i,j=1

j =

1
2

f (pij)(w(cid:62)

i   wj     log pij)2 ,

where wi is a vector for the left context, wj is a
vector for the right context, pij is the relative fre-
quency of word j in the context of word i, and f
is a heuristic weighting function to balance the in-
   uence of high versus low term frequencies. the
model was trained on english wikipedia and the
english gigaword corpus version 5.

the svd model (levy et al., 2015a) uses pos-
itive pointwise mutual information (pmi) matrix
de   ned as:

ppmi(w, c) = max(log

  p (w, c)
  p (w)   p (c)

, 0) ,

where   p (w, c) is the joint id203 of word
w and context c, and   p (w) and   p (c) are their
marginal probabilities. the matrix is factorised by
singular value decomposition.

hlbl (mnih and hinton, 2009) is a log-bilinear
formulation of an id165 language model, which
predicts the ith word based on context words (i    
n, . . . , i     2, i     1). this leads to the following

duty, denoting either the embedding for the ith token, wi, or
kth word type, wk.

training objective:

t(cid:88)
(cid:80)v
exp(   w(cid:62)
k=1 exp(   w(cid:62)
where   wi =(cid:80)n   1

j =

1
t

i=1

i wi + bi)

,

i wk + bk)

j=1 cjwi   j is the context embed-
ding, cj is a scaling matrix, and b    is a bias term.
the    nal model, senna (collobert and weston,
2008), was initially proposed for multi-task train-
ing of several language processing tasks, from lan-
guage modelling through to semantic role labelling.
here we focus on the statistical language modelling
component, which has a pairwise ranking objective
to maximise the relative score of each word in its
local context:

t(cid:88)

v(cid:88)

i=1

k=1

max(cid:2)0, 1     f (wi   c, . . . , wi   1, wi)
+ f (wi   c, . . . , wi   1, wk)(cid:3) ,

j =

1
t

where the last c     1 words are used as context, and
f (x) is a non-linear function of the input, de   ned
as a multi-layer id88.

for hlbl and senna, we use the pre-trained
embeddings from turian et al. (2010), trained on
the reuters english newswire corpus. in both cases,
the embeddings were scaled by the global stan-
dard deviation over the word-embedding matrix,
wscaled = 0.1    w

for w2vwiki, glovewiki and svdwiki we used
english wikipedia. we followed the same prepro-
cessing procedure described in levy et al. (2015a),3
i.e., lower-cased all words and removed non-textual
elements. during the training phase, for each model

  (w ).

3although the w2v model trained without preprocessing
performed marginally better, we used preprocessing through-
out for consistency.

we set a word frequency threshold of 5. for the
svd model, we followed the recommendations of
levy et al. (2015a) in setting the context window
size to 2, negative sampling parameter to 1, eigen-
value weighting to 0.5, and context distribution
smoothing to 0.75; other parameters were assigned
their default values. for the other models we used
the following parameter values: for w2v, context
window = 8, negative samples = 25, hs = 0, sample
= 1e-4, and iterations = 15; and for glove, context
window = 15, x max = 10, and iterations = 15.

3.2 lexical relations
in order to evaluate the applicability of the diff-
vec approach to relations of different types, we
assembled a set of lexical relations in three broad
categories: lexical semantic relations, morphosyn-
tactic paradigm relations, and morphosemantic re-
lations. we constrained the relations to be binary
and to have    xed directionality.4 consequently we
excluded symmetric lexical relations such as syn-
onymy. we additionally constrained the dataset to
the words occurring in all embedding sets. there
is some overlap between our relations and those in-
cluded in the analogy task of mikolov et al. (2013c),
but we include a much wider range of lexical se-
mantic relations, especially those standardly evalu-
ated in the relation classi   cation literature. we man-
ually    ltered the data to remove duplicates (e.g., as
part of merging the two sources of lexsemhyper
intances), and normalise directionality.
the    nal dataset consists of 12,458 triples
(cid:104)relation, word1, word2(cid:105), comprising 15 relation
types, extracted from semeval   12 (jurgens et al.,
2012), bless (baroni and lenci, 2011), the msr
analogy dataset (mikolov et al., 2013c), the light
verb dataset of tan et al. (2006a), princeton word-
net (fellbaum, 1998), wiktionary,5 and a web lex-
icon of collective nouns,6 as listed in table 2.7

4 id91

assuming diffvecs are capable of capturing all
lexical relations equally, we would expect cluster-
ing to be able to identify sets of word pairs with

4word similarity is not included; it is not easily captured
by diffvec since there is no homogeneous    content    to the
lexical relation which could be captured by the direction and
magnitude of a difference vector (other than that it should be
small).

5http://en.wiktionary.org
6http://www.rinkworks.com/words/collective.

shtml

7the dataset is available at http://github.com/ivri/

diffvec

lexsemattr
lexsemcause
nouncoll
lexsemevent

lexsemhyper
lvc
lexsemmero
nounsp

prefix
lexsemref
lexsemspace
verb3

verb3past
verbpast
verbnoun

figure 1: id167 projection (van der maaten and
hinton, 2008) of diffvecs for 10 sample word
pairs of each relation type, based on w2v. the
intersection of the two axes identify the projection
of the zero vector. best viewed in colour.

high relational similarity, or equivalently clusters
of similar offset vectors. under the additional as-
sumption that a given word pair corresponds to
a unique lexical relation (in line with our de   ni-
tion of the lexical relation learning task in   3), a
hard id91 approach is appropriate. in order to
test these assumptions, we cluster our 15-relation
closed-world dataset in the    rst instance, and eval-
uate against the lexical resources in   3.2.

as further motivation, we projected the diff-
vec space for a small number of samples of each
class using id167 (van der maaten and hinton,
2008), and found that many of the morphosyntactic
relations (verb3, verbpast, verb3past, nounsp)
form tight clusters (figure 1).

we cluster the diffvecs between all word
pairs in our dataset using spectral id91
(von luxburg, 2007). spectral id91 has two
hyperparameters: the number of clusters, and the
pairwise similarity measure for comparing diff-
vecs. we tune the hyperparameters over devel-
opment data, in the form of 15% of the data ob-
tained by random sampling, selecting the con   gura-
tion that maximises the v-measure (rosenberg and
hirschberg, 2007). figure 2 presents v-measure
values over the test data for each of the four word
embedding models. we show results for different
numbers of clusters, from n = 10 in steps of 10,
up to n = 80 (beyond which the id91 quality
diminishes).8 observe that w2v achieves the best
8although 80 clusters (cid:29) our 15 relation types, the se-
meval   12 classes each contain numerous subclasses, so the
larger number may be more realistic.

w2v
w2v wiki
glove

glovewiki
svd wiki
hlbl

senna

e
r
u
s
a
e

m
v

-

0
4
0

.

5
3
0

.

0
3
0

.

5
2
0

.

0
2
0

.

5
1
0

.

10

20

30

40

50

60

70

80

number of clusters

figure 2: spectral id91 results, comparing
cluster quality (v-measure) and the number of clus-
ters. diffvecs are clustered and compared to the
known relation types. each line shows a different
source of id27s.

results, with a v-measure value of around 0.36,9
which is relatively constant over varying numbers
of clusters. glove and svd mirror this result,
but are consistently below w2v at a v-measure
of around 0.31. hlbl and senna performed very
similarly, at a substantially lower v-measure than
w2v or glove, closer to 0.21. as a crude calibra-
tion for these results, over the related id91
task of word sense induction, the best-performing
systems in semeval-2010 task 4 (manandhar et
al., 2010) achieved a v-measure of under 0.2.

the

for w2vwiki

lower v-measure

and
glovewiki (as compared to w2v and glove,
respectively) indicates that the volume of training
data plays a role in the id91 results. however,
both methods still perform well above senna and
hlbl, and w2v has a clear empirical advantage
over glove. we note that svdwiki performs
almost as well as w2vwiki, consistent with the
results of levy et al. (2015a).

we additionally calculated the id178 for each
lexical relation, based on the distribution of in-
stances belonging to a given relation across the
different clusters (and simple id113). for each em-
bedding method, we present the id178 for the
cluster size where v-measure was maximised over
the development data. since the samples are dis-
tributed nonuniformly, we normalise id178 re-
sults for each method by log(n) where n is the
number of samples in a particular relation. the re-

9v-measure returns a value in the range [0, 1], with 1 indi-

cating perfect homogeneity and completeness.

lexsemattr
lexsemcause
lexsemspace
lexsemref
lexsemhyper
lexsemevent
lexsemmero
nounsp
verb3
verbpast
verb3past
lvc
verbnoun
prefix
nouncoll

w2v
0.49
0.47
0.49
0.44
0.44
0.46
0.40
0.07
0.05
0.09
0.07
0.28
0.31
0.32
0.21

glove
0.54
0.53
0.55
0.50
0.50
0.47
0.42
0.14
0.06
0.14
0.05
0.55
0.33
0.30
0.27

hlbl
0.62
0.56
0.54
0.54
0.43
0.47
0.42
0.22
0.49
0.38
0.49
0.32
0.35
0.55
0.46

senna
0.63
0.57
0.58
0.56
0.45
0.48
0.43
0.29
0.44
0.35
0.52
0.30
0.36
0.58
0.44

table 3: the id178 for each lexical relation over
the id91 output for each set of pre-trained
id27s.

sults are in table 3, with the lowest id178 (purest
id91) for each relation indicated in bold.

looking across the different lexical relation
types, the morphosyntactic paradigm relations
(nounsp and the three verb relations) are by
far the easiest to capture. the lexical semantic rela-
tions, on the other hand, are the hardest to capture
for all embeddings.

considering w2v embeddings, for verb3 there
was a single cluster consisting of around 90%
of verb3 word pairs. most errors resulted from
pos ambiguity, leading to confusion with verb-
noun in particular. example verb3 pairs incor-
rectly clustered are: (study, studies), (run, runs),
and (like, likes). this polysemy results in the dis-
tance represented in the diffvec for such pairs
being above average for verb3, and consequently
clustered with other cross-pos relations.

for verbpast, a single relatively pure cluster
was generated, with minor contamination due
to pairs such as (hurt, saw), (utensil, saw), and
(wipe, saw). here, the noun saw is ambiguous with
a high-frequency past-tense verb; hurt and wipe
also have ambigous pos.

a related phenomenon was observed for
nouncoll, where the instances were assigned to
a large mixed cluster containing word pairs where
the second word referred to an animal, re   ect-
ing the fact that most of the collective nouns in
our dataset relate to animals, e.g. (stand, horse),
(ambush, tigers), (antibiotics, bacteria). this is in-
teresting from a diffvec point of view, since it
shows that the lexical semantics of one word in
the pair can overwhelm the semantic content of the
diffvec (something that we return to investigate
in   5.4). lexsemmero was also split into multiple

clusters along topical lines, with separate clusters
for weapons, dwellings, vehicles, etc.

given the encouraging results from our cluster-
ing experiment, we next evaluate diffvecs in a
supervised relation classi   cation setting.

5 classi   cation

a natural question is whether we can accurately
characterise lexical relations through supervised
learning over the diffvecs. for these experi-
ments we use the w2v, w2vwiki, and svdwiki em-
beddings exclusively (based on their superior per-
formance in the id91 experiment), and a sub-
set of the relations which is both representative of
the breadth of the full relation set, and for which
we have suf   cient data for supervised training
and evaluation, namely: nouncoll, lexsemevent,
lexsemhyper, lexsemmero, nounsp, prefix,
verb3, verb3past, and verbpast (see table 2).

we consider two applications: (1) a closed-
world setting similar to the unsupervised evalua-
tion, in which the classi   er only encounters word
pairs which correspond to one of the nine relations;
and (2) a more challenging open-world setting
where random word pairs     which may or may not
correspond to one of our relations     are included
in the evaluation. for both settings, we further in-
vestigate whether there is a lexical memorisation
effect for a broad range of relation types of the
sort identi   ed by weeds et al. (2014) and levy et
al. (2015b) for hypernyms, by experimenting with
disjoint training and test vocabulary.

5.1 closed-world classi   cation
for the closed-world setting, we train and
test a multiclass classi   er on datasets comprising
(cid:104)diffvec, r(cid:105) pairs, where r is one of our nine
relation types, and diffvec is based on one of
w2v, w2vwiki and svd. as a baseline, we cluster
the data as described in   4, running the clusterer
several times over the 9-relation data to select the
optimal v-measure value based on the develop-
ment data, resulting in 50 clusters. we label each
cluster with the majority class based on the training
instances, and evaluate the resultant labelling for
the test instances.

we use an id166 with a linear kernel, and report

results from 10-fold cross-validation in table 4.

the id166 achieves a higher f-score than the
baseline on almost every relation, particularly on
lexsemhyper, and the lower-frequency nounsp,
nouncoll, and prefix. most of the relations    

baseline

0.60
0.90
0.87
0.00
0.99
0.78
0.99
0.00
0.19
0.84

w2v
0.93
0.97
0.98
0.83
0.98
0.98
0.98
0.82
0.95
0.97

w2vwiki
0.91
0.96
0.97
0.78
0.96
0.98
0.98
0.34
0.91
0.95

svdwiki
0.91
0.96
0.97
0.74
0.97
0.95
0.96
0.60
0.92
0.95

relation
lexsemhyper
lexsemmero
lexsemevent
nounsp
verb3
verbpast
verb3past
prefix
nouncoll
micro-average
table 4: f-scores (f) for closed-world classi-
   cation, for a baseline method based on id91
+ majority-class labelling, a multiclass linear id166
trained on w2v, w2vwiki and svdwiki diffvec
inputs.

even the most dif   cult ones from our id91
experiment     are classi   ed with very high f-
score. that is, with a simple linear transforma-
tion of the embedding dimensions, we are able to
achieve near-perfect results. the prefix relation
achieved markedly lower recall, resulting in a lower
f-score, due to large differences in the predomi-
nant usages associated with the respective words
(e.g., (union, reunion), where the vector for union
is heavily biased by contexts associated with trade
unions, but reunion is heavily biased by contexts re-
lating to social get-togethers; and (entry, reentry),
where entry is associated with competitions and en-
trance to schools, while reentry is associated with
space travel). somewhat surprisingly, given the
small dimensionality of the input (vectors of size
300 for all three methods), we found that the lin-
ear id166 slightly outperformed a non-linear id166
using an rbf kernel. we observe no real differ-
ence between w2vwiki and svdwiki, supporting the
hypothesis of levy et al. (2015a) that under ap-
propriate parameter settings, count-based methods
achieve high results. the impact of the training data
volume for pre-training of the embeddings is also
less pronounced than in the case of our id91
experiment.

5.2 open-world classi   cation
we now turn to a more challenging evaluation set-
ting: a test set including word pairs drawn at ran-
dom. this setting aims to illustrate whether a diff-
vec-based classi   er is capable of differentiating
related word pairs from noise, and can be applied
to open data to learn new related word pairs.10

   er for each relation type, using 2

for these experiments, we train a binary classi-
3 of our relation
10hereafter we provide results for w2v only, as we found

that svd achieved similar results.

relation
lexsemhyper
lexsemmero
lexsemevent
nounsp
verb3
verbpast
verb3past
prefix
nouncoll

orig

p r f
0.95 0.92 0.93
0.13 0.96 0.24
0.44 0.98 0.61
0.95 0.68 0.8
0.75 1.00 0.86
0.94 0.86 0.90
0.76 0.95 0.84
1.00 0.29 0.44
0.43 0.74 0.55

+neg

p r f
0.99 0.84 0.91
0.95 0.84 0.89
0.93 0.90 0.91
1.00 0.68 0.81
0.93 0.93 0.93
0.97 0.84 0.90
0.87 0.93 0.90
1.00 0.13 0.23
0.97 0.41 0.57

table 5: precision (p) and recall (r) for open-
world classi   cation, using the binary classi   er
without (   orig   ) and with (   +neg   ) negative sam-
ples .

data for training and 1
3 for testing. the test data is
augmented with an equal quantity of random pairs,
generated as follows:
(1) sample a seed lexicon by drawing words pro-

portional to their frequency in wikipedia;11

(2) take the cartesian product over pairs of words

from the seed lexicon;

(3) sample word pairs uniformly from this set.
this procedure generates word pairs that are repre-
sentative of the frequency pro   le of our corpus.

we train 9 binary rbf-kernel id166 classi   ers
on the training partition, and evaluate on our ran-
domly augmented test set. fully annotating our
random word pairs is prohibitively expensive, so
instead, we manually annotated only the word pairs
which were positively classi   ed by one of our mod-
els. the results of our experiments are presented
in the left half of table 5, in which we report
on results over the combination of the original
test data from   5.1 and the random word pairs,
noting that recall (r) for open-world takes
the form of relative recall (pantel et al., 2004)
over the positively-classi   ed word pairs. the re-
sults are much lower than for the closed-word set-
ting (table 4), most notably in terms of precision
(p). for instance, the random pairs (have, works),
(turn, took), and (works, started) were incorrectly
classi   ed as verb3, verbpast and verb3past, re-
spectively. that is, the model captures syntax, but
lacks the ability to capture lexical paradigms, and
tends to overgenerate.

5.3 open-world training with negative

sampling

to address the problem of incorrectly classifying
random word pairs as valid relations, we retrain the
classi   er on a dataset comprising both valid and

automatically-generated negative distractor sam-
ples. the basic intuition behind this approach is
to construct samples which will force the model
to learn decision boundaries that more tightly cap-
ture the true scope of a given relation. to this end,
we automatically generated two types of negative
distractors:
opposite pairs: generated by switching the or-
der of word pairs, opposw1 ,w2 = word1    
word2. this ensures the classi   er adequately
captures the asymmetry in the relations.
shuf   ed pairs: generated by replacing w2 with
a random word w(cid:48)
2 from the same relation,
2     word1. this is tar-
shuffw1 ,w2 = word(cid:48)
geted at relations that take speci   c word
classes in particular positions, e.g., (vb, vbd)
word pairs, so that the model learns to encode
the relation rather than simply learning the
properties of the word classes.

both types of distractors are added to the train-
ing set, such that there are equal numbers of valid
relations, opposite pairs and shuf   ed pairs.
after training our classi   er, we evaluate its pre-
dictions in the same way as in   5.2, using the same
test set combining related and random word pairs.12
the results are shown in the right half of table 5 (as
   +neg   ). observe that the precision is much higher
and recall somewhat lower compared to the classi-
   er trained with only positive samples. this follows
from the adversarial training scenario: using nega-
tive distractors results in a more conservative classi-
   er, that correctly classi   es the vast majority of the
random word pairs as not corresponding to a given
relation, resulting in higher precision at the expense
of a small drop in recall. overall this leads to higher
f-scores, as shown in figure 3, other than for hy-
pernyms (lexsemhyper) and pre   xes (prefix).
for example, the standard classi   er for nouncoll
learned to match word pairs including an animal
name (e.g., (plague, rats)), while training with neg-
ative samples resulted in much more conservative
predictions and consequently much lower recall.
the classi   er was able to capture (herd, horses) but
not (run, salmon), (party, jays) or (singular, boar)
as instances of nouncoll, possibly because of poly-
semy. the most striking difference in performance
was for lexsemmero, where the standard classi-
   er generated many false positive noun pairs (e.g.
(series, radio)), but the false positive rate was con-
siderably reduced with negative sampling.

11filtered to consist of words for which we have embed-

dings.

12but noting that relative recall for the random word pairs
is based on the pool of positive predictions from both models.

0
1

.

8
0

.

6

.

0

4
0

.

2

.

0

0

.

0

f

/

r

/

p

p
p+neg

r
r+neg

f
f+neg

0

1

2

3

4

5

volume of random word pairs

figure 4: evaluation of the open-world model
when trained on split vocabulary, for varying num-
bers of random word pairs in the test dataset (ex-
pressed as a multiplier relative to the number of
closed-world test instances).

classi   er.

6 conclusions

this paper is the    rst to test the generalisability
of the vector difference approach across a broad
range of lexical relations (in raw number and also
variety). using id91 we showed that many
types of morphosyntactic and morphosemantic dif-
ferences are captured by diffvecs, but that lexical
semantic relations are captured less well, a    nd-
ing which is consistent with previous work (k  oper
et al., 2015). in contrast, classi   cation over the
diffvecs works extremely well in a closed-world
setting, showing that dimensions of diffvecs en-
code lexical relations. classi   cation performs less
well over open data, although with the introduction
of automatically-generated negative samples, the
results improve substantially. negative sampling
also improves classi   cation when the training and
test vocabulary are split to minimise lexical mem-
orisation. overall, we conclude that the diffvec
approach has impressive utility over a broad range
of lexical relations, especially under supervised
classi   cation.

acknowledgments

lr was supported by epsrc grant ep/i037512/1
and erc starting grant discotex (306920). tc
and tb were supported by the australian research
council.

figure 3: f-score for open-world classi   cation,
comparing models trained with and without nega-
tive samples.

5.4 lexical memorisation
weeds et al. (2014) and levy et al. (2015b) re-
cently showed that supervised methods using diff-
vecs achieve arti   cially high results as a result of
   lexical memorisation    over frequent words asso-
ciated with the hypernym relation. for example,
(animal, cat), (animal, dog), and (animal, pig) all
share the superclass animal, and the model thus
learns to classify as positive any word pair with
animal as the    rst word.

to address this effect, we follow levy et al.
(2015b) in splitting our vocabulary into training and
test partitions, to ensure there is no overlap between
training and test vocabulary. we then train classi-
   ers with and without negative sampling (  5.3),
incrementally adding the random word pairs from
  5.2 to the test data (from no random word pairs
to    ve times the original size of the test data) to in-
vestigate the interaction of negative sampling with
greater diversity in the test set when there is a split
vocabulary. the results are shown in figure 4.

observe that the precision for the standard clas-
si   er decreases rapidly as more random word pairs
are added to the test data. in comparison, the pre-
cision when negative sampling is used shows only
a small drop-off, indicating that negative sampling
is effective at maintaining precision in an open-
world setting even when the training and test
vocabulary are disjoint. this bene   t comes at the
expense of recall, which is much lower when neg-
ative sampling is used (note that recall stays rela-
tively constant as random word pairs are added, as
the vast majority of them do not correspond to any
relation). at the maximum level of random word
pairs in the test data, the f-score for the negative
sampling classi   er is higher than for the standard

usage of negative samples0.00.20.40.60.81.0lexsemhyperlexsemmerolexsemeventnounspverb3verbpastverb3pastprefixnouncollno negative samples   with negative samplesreferences
sanjeev arora, yuanzhi li, yingyu liang, tengyu ma,
and andrej risteski. 2015. id93 on con-
text spaces: towards an explanation of the mysteries
of semantic id27s. arxiv:1502.03520
[cs.lg].

michele banko, michael j. cafarella, stephen soder-
land, matthew broadhead, and oren etzioni. 2007.
in pro-
id10 for the web.
ceedings of the 20th international joint conference
on arti   cial intelligence (ijcai-2007), pages 2670   
2676, hyderabad, india.

marco baroni and alessandro lenci. 2011. how we
blessed distributional semantic evaluation. in pro-
ceedings of the gems 2011 workshop on geometri-
cal models of natural language semantics, gems
   11, pages 1   10, edinburgh, scotland.

marco baroni, raffaella bernardi, ngoc-quynh do,
and chung-chieh shan. 2012. entailment above
in pro-
the word level in id65.
ceedings of the 13th conference of the eacl (eacl
2012), pages 23   32, avignon, france.

antoine bordes, nicolas usunier, alberto garcia-
duran, jason weston, and oksana yakhnenko.
2013. translating embeddings for modeling multi-
in advances in neural informa-
relational data.
tion processing systems 25 (nips-13), pages 2787   
2795.

ronan collobert and jason weston. 2008. a uni   ed
architecture for natural language processing: deep
in pro-
neural networks with multitask learning.
ceedings of the 25th international conference on
machine learning (icml 2008), pages 160   167,
helsinki, finland.

manaal faruqui, jesse dodge, sujay jauhar, chris
dyer, ed hovy, and noah smith. 2015. retro   tting
word vectors to semantic lexicons. in proceedings
of the 2015 conference of the north american chap-
ter of the association for computational linguis-
tics     human language technologies (naacl hlt
2015), pages 1351   1356, denver, usa.

christiane fellbaum, editor. 1998. id138: an elec-
tronic lexical database. mit press, cambridge,
usa.

daniel fried and kevin duh. 2015.

incorporating
both distributional and relational semantics in word
in proceedings of the third inter-
representations.
national conference on learning representations
(iclr 2015), san diego, usa.

ruiji fu, jiang guo, bing qin, wanxiang che, haifeng
wang, and ting liu. 2014. learning semantic hier-
archies via id27s. in proceedings of the
52nd annual meeting of the association for compu-
tational linguistics (acl 2014), pages 1199   1209,
baltimore, usa.

maayan geffet and ido dagan. 2005. the distribu-
tional inclusion hypotheses and lexical entailment.
in proceedings of the 43rd annual meeting of the as-
sociation for computational linguistics (acl 2005),
pages 107   114, ann arbor, usa.

roxana girju, preslav nakov, vivi nastase, stan sz-
pakowicz, peter turney, and deniz yuret.
2007.
semeval-2007 task 4: classi   cation of semantic re-
lations between nominals. in proceedings of the 4th
international workshop on semantic evaluation (se-
meval 2007), pages 13   18, prague, czech republic.

iris hendrickx, su nam kim, zornitsa kozareva,
preslav nakov, diarmuid   o s  eaghdha, sebastian
pad  o, marco pennacchiotti, lorenza romano, and
stan szpakowicz. 2010. semeval-2010 task 8:
multi-way classi   cation of semantic relations be-
tween pairs of nominals. in proceedings of the 5th
international workshop on semantic evaluation (se-
meval 2010), pages 33   38, uppsala, sweden.

2012.

david jurgens, saif mohammad, peter turney, and
keith holyoak.
semeval-2012 task 2:
in pro-
measuring degrees of relational similarity.
ceedings of the 6th international workshop on se-
mantic evaluation (semeval 2012), pages 356   364,
montr  eal, canada.

joo-kyung kim and marie-catherine de marneffe.
2013. deriving adjectival scales from continuous
in proceedings of the
space word representations.
2013 conference on empirical methods in natural
language processing (emnlp 2013), pages 1625   
1630, seattle, usa.

maximilian k  oper,

christian

scheible,

and
sabine schulte im walde.
2015. multilingual
reliability and    semantic    structure of continuous
word spaces. in proceedings of the eleventh inter-
national workshop on computational semantics
(iwcs-11), pages 40   45, london, uk.

lili kotlerman, ido dagan, idan szpektor, and maayan
zhitomirsky-geffet.
2010. directional distribu-
tional similarity for lexical id136. natural lan-
guage engineering, 16:359   389.

alessandro lenci and giulia benotto. 2012. identify-
ing hypernyms in distributional semantic spaces. in
proceedings of the first joint conference on lexical
and computational semantics (*sem 2012), pages
75   79, montr  eal, canada.

omer levy and yoav goldberg. 2014a. linguistic reg-
ularities in sparse and explicit word representations.
in proceedings of the 18th conference on natural
language learning (conll-2014), pages 171   180,
baltimore, usa.

omer levy and yoav goldberg. 2014b. neural word
embeddings as implicit id105. in ad-
vances in neural information processing systems 26
(nips-14).

omer levy, yoav goldberg, and ido dagan. 2015a.
improving distributional similarity with lessons
learned from id27s. transactions of the
association for computational linguistics, 3:211   
225.

patrick pantel, deepak ravichandran, and eduard
2004. towards terascale semantic acqui-
hovy.
in proceedings of the 20th international
sition.
conference on computational linguistics (coling
2004), pages 771   777, geneva, switzerland.

omer levy, steffen remus, chris biemann, ido da-
gan, and israel ramat-gan. 2015b. do supervised
distributional methods really learn lexical id136
relations? in proceedings of the 2015 conference
of the north american chapter of the association
for computational linguistics     human language
technologies (naacl hlt 2015), pages 970   976,
denver, usa.

m  arton makrai, d  avid nemeskey, and andr  as kornai.
2013. applicative structure in vector space mod-
in proceedings of the workshop on continu-
els.
ous vector space models and their compositionality
(cvsc), pages 59   63, so   a, bulgaria.

suresh manandhar, ioannis klapaftis, dmitriy dligach,
and sameer pradhan. 2010. semeval-2010 task
14: word sense induction & disambiguation. in pro-
ceedings of the 5th international workshop on se-
mantic evaluation, pages 63   68, uppsala, sweden.

tomas mikolov, kai chen, greg corrado, and jeffrey
dean. 2013a. ef   cient estimation of word repre-
in proceedings of the
sentations in vector space.
workshop of the first international conference on
learning representations (iclr 2013), scottsdale,
usa.

tomas mikolov, ilya sutskever, kai chen, greg cor-
rado, and jeffrey dean. 2013b. distributed represen-
tations of words and phrases and their composition-
ality. in advances in neural information processing
systems 25 (nips-13).

tomas mikolov, wen-tau yih, and geoffrey zweig.
2013c. linguistic regularities in continuous space
in proceedings of the 2013
word representations.
conference of the north american chapter of the
association for computational linguistics: human
language technologies (naacl hlt 2013), pages
746   751, atlanta, usa.

andriy mnih and geoffrey e hinton. 2009. a scal-
able hierarchical distributed language model. in ad-
vances in neural information processing systems 21
(nips-09), pages 1081   1088.

andriy mnih and koray kavukcuoglu. 2013. learning
id27s ef   ciently with noise-contrastive
estimation. in advances in neural information pro-
cessing systems 25 (nips-13).

silvia necs  ulescu, sara mendes, david jurgens, n  uria
bel, and roberto navigli. 2015. reading between
the lines: overcoming data sparsity for accurate clas-
in proceedings
si   cation of lexical relationships.
of the fourth joint conference on lexical and com-
putational semantics (*sem 2015), pages 182   192,
denver, usa.

2014.

jeffrey pennington, richard socher, and christopher d.
glove: global vectors for
manning.
in proceedings of the 2014
word representation.
conference on empirical methods in natural lan-
guage processing (emnlp 2014), pages 1532   
1543, doha, qatar.

laura rimell. 2014. distributional lexical entailment
by topic coherence. in proceedings of the 14th con-
ference of the european chapter of the association
for computational linguistics (eacl 2014), pages
511   519, gothenburg, sweden.

stephen roller and katrin erk. 2016. relations such
as hypernymy: identifying and exploiting hearst pat-
terns in distributional vectors for lexical entailment.
arxiv preprint arxiv:1605.05433.

stephen roller, katrin erk, and gemma boleda. 2014.
inclusive yet selective: supervised distributional hy-
pernymy detection. in proceedings of the 25th inter-
national conference on computational linguistics
(coling 2014), pages 1025   1036, dublin, ireland.

andrew rosenberg and julia hirschberg. 2007. v-
measure: a conditional id178-based external clus-
ter evaluation measure. in proceedings of the joint
conference on empirical methods in natural lan-
guage processing and computational natural lan-
guage learning 2007 (emnlp-conll 2007), pages
410   420, prague, czech republic.

enrico santus, alessandro lenci, qin lu, and sabine
schulte im walde. 2014. chasing hypernyms in vec-
tor spaces with id178. in proceedings of the 14th
conference of the european chapter of the associ-
ation for computational linguistics (eacl 2014),
pages 38   42, gothenburg, sweden.

richard socher, danqi chen, christopher d. manning,
and andrew y. ng. 2013. reasoning with neural
tensor networks for knowledge base completion. in
advances in neural information processing systems
25 (nips-13).

pang-ning tan, michael steinbach, and vipin kumar.
2006a. introduction to data mining. addison wes-
ley.

yee fan tan, min-yen kan, and hang cui. 2006b. ex-
tending corpus-based identi   cation of light verb con-
structions using a supervised learning framework. in
proceedings of the eacl 2006 workshop on multi-
word-expressions in a multilingual context, pages
49   56, trento, italy.

joseph turian, lev-arie ratinov, and yoshua bengio.
2010. word representations: a simple and general
in proceed-
method for semi-supervised learning.
ings of the 48th annual meeting of the acl (acl
2010), pages 384   394, uppsala, sweden.

peter d. turney. 2006. similarity of semantic relations.

computational linguistics, 32(3):379   416.

laurens van der maaten and geoffrey hinton. 2008.
visualizing data using id167. journal of machine
learning research, 9(2579-2605):85.

ulrike von luxburg. 2007. a tutorial on spectral clus-

tering. statistics and computing, 17(4):395   416.

julie weeds, daoud clarke, jeremy ref   n, david weir,
and bill keller. 2014. learning to distinguish hyper-
nyms and co-hyponyms. in proceedings of the 25th
international conference on computational linguis-
tics (coling 2014), pages 2249   2259, dublin, ire-
land.

gerhard weikum and martin theobald. 2010. from
information to knowledge: harvesting entities and re-
lationships from web sources. in proceedings of the
twenty ninth acm sigmod-sigact-sigart sym-
posium on principles of database systems, pages
65   76, indianapolis, usa.

chang xu, yanlong bai, jiang bian, bin gao, gang
wang, xiaoguang liu, and tie-yan liu. 2014. rc-
net: a general framework for incorporating knowl-
in proceedings
edge into word representations.
of the 23rd acm conference on information and
knowledge management (cikm 2014), pages 1219   
1228, shanghai, china.

ichiro yamada, kentaro torisawa, jun   ichi kazama,
kow kuroda, masaki murata, stijn de saeger, fran-
cis bond, and asuka sumida. 2009. hypernym dis-
covery based on distributional similarity and hierar-
chical structures. in proceedings of the 2009 con-
ference on empirical methods in natural language
processing (emnlp 2009), pages 929   937, singa-
pore.

mo yu and mark dredze. 2014. improving lexical em-
beddings with semantic knowledge. in proceedings
of the 52nd annual meeting of the association for
computational linguistics (acl 2014), pages 545   
550, baltimore, usa.

a. zhila, w.t. yih, c. meek, g. zweig, and t. mikolov.
2013. combining heterogeneous models for measur-
ing relational similarity. in proceedings of the 2013
conference of the north american chapter of the
association for computational linguistics: human
language technologies (naacl hlt 2013).

