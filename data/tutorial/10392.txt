6
1
0
2

 
r
p
a
6
2

 

 
 
]
l
c
.
s
c
[
 
 

4
v
9
0
9
7
0

.

5
0
5
1
:
v
i
x
r
a

solving verbal questions in iq test by knowledge-powered id27

huazheng wang

the university of virginia
huazhengwang@gmail.com

fei tian

university of science and technology of china

tianfei@mail.ustc.edu.cn

bin gao

microsoft research

bingao@microsoft.com

jiang bian
yidian inc

jiang.bian.prc@gmail.com

tie-yan liu

microsoft research
tyliu@microsoft.com

abstract

verbal comprehension questions appear very fre-
quently in intelligence quotient (iq) tests, which
measure human   s verbal ability including the un-
derstanding of the words with multiple senses, the
synonyms and antonyms, and the analogies among
words. in this work, we explore whether such tests
can be solved automatically by the deep learning
technologies for text data. we found that the task
was quite challenging, and simply applying exist-
ing technologies like id27 could not
achieve a good performance, due to the multiple
senses of words and the complex relations among
words. to tackle these challenges, we propose a
novel framework to automatically solve the ver-
bal iq questions by leveraging improved word em-
bedding by jointly considering the multi-sense na-
ture of words and the relational information among
words. experimental results have shown that the
proposed framework can not only outperform ex-
isting methods for solving verbal comprehension
questions but also exceed the average performance
of the amazon mechanical turk workers involved
in the study.

1 introduction
the intelligence quotient (iq) test [stern, 1914] is a test of
intelligence designed to formally study the success of an in-
dividual in adapting to a speci   c situation under certain con-
ditions. common iq tests measure various types of abilities
such as verbal, mathematical, logical, and reasoning skills.
these tests have been widely used in the study of psychology,
education, and career development. in the community of ar-
ti   cial intelligence, agents have been invented to ful   ll many
interesting and challenging tasks like face recognition, speech
recognition, handwriting recognition, and question answer-
ing. however, as far as we know, there are very limited stud-
ies of developing an agent to solve iq tests, which in some
sense is more challenging, since even common human beings
could not always succeed in the tests. considering that iq
test scores have been widely considered as a measure of in-
telligence, we think it is worth making further investigations
whether we can develop an agent that can solve iq tests.

the commonly used iq tests contain several types of ques-
tions like verbal, mathematical, logical, and picture ques-
tions, among which a large proportion (near 40%) are verbal
questions [carter, 2005]. the recent progress on deep learn-
ing for natural language processing (nlp), such as word em-
bedding technologies, has advanced the ability of machines
(or ai agents) to understand the meaning of words and the
relations among words. this inspires us to solve the verbal
questions in iq tests by leveraging the id27 tech-
nologies. however, our attempts show that a straightforward
application of id27 could not result in satisfac-
tory performances. this is actually understandable. standard
id27 technologies learn one embedding vector
for each word based on the co-occurrence information in a
text corpus. however, verbal comprehension questions in iq
tests usually consider the multiple senses of a word (and often
focus on the rare senses), and the complex relations among
(polysemous) words. this has clearly exceeded the capabil-
ity of standard id27 technologies.

to tackle the aforementioned challenges, we propose a

novel framework that consists of three components.

first, we build a classi   er to recognize the speci   c type
(e.g., analogy, classi   cation, synonym, and antonym) of ver-
bal questions. for different types of questions, different kinds
of relationships need to be considered and the solvers could
have different forms. therefore, with an effective question
type classi   er, we may solve the questions in a divide-and-
conquer manner.

second, we obtain distributed representations of words and
relations by leveraging a novel id27 method that
considers the multi-sense nature of words and the relational
knowledge among words (or their senses) contained in dictio-
naries. in particular, for each polysemous word, we retrieve
its number of senses from a dictionary, and conduct cluster-
ing on all its context windows in the corpus. then we attach
the example sentences for every sense in the dictionary to the
clusters, such that we can tag the polysemous word in each
context window with a speci   c word sense. on top of this,
instead of learning one embedding vector for each word, we
learn one vector for each pair of word-sense. furthermore,
in addition to learning the embedding vectors for words, we
also learn the embedding vectors for relations (e.g., synonym
and antonym) at the same time, by incorporating relational
knowledge into the objective function of the id27

learning algorithm. that is, the learning of word-sense rep-
resentations and relation representations interacts with each
other, such that the relational knowledge obtained from dic-
tionaries is effectively incorporated.

third, for each type of questions, we propose a speci   c
solver based on the obtained distributed word-sense represen-
tations and relation representations. for example, for analogy
questions, we    nd the answer by minimizing the distance be-
tween word-sense pairs in the question and the word-sense
pairs in the candidate answers.

we have conducted experiments using a combined iq test
set to test the performance of our proposed framework. the
experimental results show that our method can outperform
several baseline methods for verbal comprehension questions
in iq tests. we further deliver the questions in the test set to
human beings through amazon mechanical turk1. the av-
erage performance of the human beings is even a little lower
than that of our proposed method.

2 related work
2.1 verbal questions in iq test
in common iq tests, a large proportion of questions are ver-
bal comprehension questions, which play an important role
in deciding the    nal iq scores. for example, in wechsler
adult intelligence scale [wechsler, 2008], which is among
the most famous iq test systems, the full-scale iq is calcu-
lated from two iq scores: verbal iq and performance iq, and
around 40% questions in a typical test are verbal comprehen-
sion questions. verbal questions can test not only the verbal
ability (e.g., understanding polysemy of a word), but also the
reasoning ability and induction ability of an individual. ac-
cording to previous studies [carter, 2005], verbal questions
mainly have the types elaborated in table 1, in which the cor-
rect answers are highlighted in bold font.

analogy-i questions usually take the form    a is to b as
c is to ?   . one needs to choose a word d from a given list
of candidate words to form an analogical relation between
pair (a, b) and pair (c, d). such questions test the abil-
ity of identifying an implicit relation from word pair (a, b)
and apply it to compose word pair (c, d). note that the
analogy-i questions are also used as a major evaluation task
in the id97 models [mikolov et al., 2013]. analogy-ii
questions require two words to be identi   ed from two given
lists in order to form an analogical relation like    a is to ? as
c is to ?   . such questions are a bit more dif   cult than the
analogy-i questions since the analogical relation cannot be
observed directly from the questions, but need to be searched
in the word pair combinations from the candidate answers.
classi   cation questions require one to identify the word that
is different (or dissimilar) from others in a given word list.
such questions are also known as odd-one-out, which have
been studied in [pint  er et al., 2012]. classi   cation questions
test the ability of summarizing the majority sense of the words
and identifying the outlier. synonym questions require one to
pick one word out of a list of words such that it has the closest
meaning to a given word. synonym questions test the ability

1http://www.mturk.com/

of identifying all senses of the candidate words and selecting
the correct sense that can form a synonymous relation to the
given word. antonym questions require one to pick one word
out of a list of words such that it has the opposite meaning to
a given word. antonym questions test the ability of identify-
ing all senses of the candidate words and selecting the correct
sense that can form an antonymous relation to the given word.
solve
and
ques-
mathematical,
[sanghi and dowe, 2003;
tions
strannegard et al., 2012;
kushmany et al., 2014;
seo et al., 2014; hosseini et al., 2014; weston et al., 2015],
there has been very few efforts to develop automatic methods
to solve verbal questions.

are
logical,

although

picture

efforts

some

there

test

iq

in

to

a.k.a.

distributed
[bengio et al., 2003],

2.2 deep learning for id111
representa-
building
word
tions
id27s,
has attracted increasing attention in the area of machine
learning. different with conventional one-hot represen-
tations of words or distributional word representations
based on co-occurrence matrix between words such as
lsa [dumais et al., 1988] and lda [blei et al., 2003],
distributed word representations are usually low-dimensional
dense vectors trained with neural networks by maximizing
the likelihood of a text corpus. recently,
to show its
effectiveness in a variety of id111 tasks, a series
of works applied deep learning techniques to learn high-
quality word representations [collobert and weston, 2008;
mikolov et al., 2013; pennington et al., 2014].

it

is quite dif   cult

nevertheless, since the above works learn word rep-
resentations mainly based on the word co-occurrence
information,
to obtain high quality
embeddings for those words with very little context in-
formation; on the other hand,
large amount of noisy or
biased context could give rise to ineffective word embed-
dings either. therefore, it is necessary to introduce extra
knowledge into the learning process to regularize the quality
of id27.
some efforts have paid attention
to learn id27 in order to address knowledge
[bordes et al., 2011;
base completion and enhancement
socher et al., 2013; weston et al., 2013a], and some other
efforts have tried to leverage knowledge to enhance word
[luong et al., 2013; weston et al., 2013b;
representations
fried and duh, 2014; celikyilmaz et al., 2015]. moreover,
all the above models assume that one word has only one
embedding no matter whether the word is polysemous
or monosemous, which might bring some confusion for
the polysemous words.
there
are several efforts like [huang et al., ; tian et al., 2014;
neelakantan et al., 2014]. however, these models do not
leverage any extra knowledge (e.g., relational knowledge) to
enhance word representations.

to solve the problem,

3 solving verbal questions
in this section, we introduce our proposed framework to solve
the verbal questions, which consists of the following three
components.

table 1: types of verbal questions.

type

analogy-i
analogy-ii
classi   cation

synonym
antonym

example
isotherm is to temperature as isobar is to? (i) atmosphere, (ii) wind, (iii) pressure, (iv) latitude, (v) current.
identify two words (one from each set of brackets) that form a connection (analogy) when paired with the words in capitals: chapter (book, verse, read), act (stage, audience, play).
which is the odd one out? (i) calm, (ii) quiet, (iii) relaxed, (iv) serene, (v) unruf   ed.
which word is closest to irrational? (i)intransigent, (ii) irredeemable, (iii) unsafe, (iv) lost, (v) nonsensical.
which word is most opposite to musical? (i) discordant, (ii) loud, (iii) lyrical, (iv) verbal, (v) euphonious.

3.1 classi   cation of question types
the    rst component of the framework is a question classi   er,
which identi   es different types of verbal questions. since dif-
ferent types of questions usually have their unique ways of
expressions, the classi   cation task is relatively easy, and we
therefore take a simple approach to ful   ll the task. specif-
ically, we regard each verbal question as a short document
and use the tf  idf features to build its representation. then
we train an id166 classi   er with linear kernel on a portion
of labeled question data, and apply it to other questions. the
question labels include analogy-i, analogy-ii, classi   cation,
synonym, and antonym. we use the one-vs-rest training
strategy to obtain a linear id166 classi   er for each question
type.

3.2 embedding of word-senses and relations
the second component of our framework leverages deep
learning technologies to learn distributed representations for
words (i.e. id27). note that in the context of
verbal id53, we have some speci   c require-
ments on this learning process. verbal questions in iq tests
usually consider the multiple senses of a word (and focus on
the rare senses), and the complex relations among (polyse-
mous) words, such as synonym and antonym relation. these
challenges have exceeded the capability of standard word em-
bedding technologies. to address this problem, we propose a
novel approach that considers the multi-sense nature of words
and integrate the relational knowledge among words (or their
senses) into the learning process. in particular, our approach
consists of two steps. the    rst step aims at labeling a word
in the text corpus with its speci   c sense, and the second step
employs both the labeled text corpus and the relational knowl-
edge contained in dictionaries to simultaneously learn embed-
dings for both word-sense pairs and relations.

multi-sense identi   cation
first, we learn a single-sense id27 by using the
skip-gram method in id97 [mikolov et al., 2013].

second, we gather the context windows of all occurrences
of a word used in the skip-gram model, and represent each
context by a weighted average of the pre-learned embedding
vectors of the context words. we use tf  idf to de   ne the
weighting function, where we regard each context window
of the word as a short document to calculate the document
frequency. speci   cally, for a word w0, each of its context
window can be denoted by (w   n ,          , w0,          , wn ). then
we represent the window by calculating the weighted average
of the pre-learned embedding vectors of the context words as
below,

   =

1
2n

n

x

i=   n,i6=0

gwivwi ,

(1)

where gwi is the tf  idf score of wi, and vwi is the pre-
learned embedding vector of wi. after that, for each word,
we use spherical id116 to cluster all its context representa-
tions, where cluster number k is set as the number of senses
of this word in the online dictionary.

third, we match each cluster to the corresponding sense
in the dictionary. on one hand, we represent each cluster by
the average embedding vector of all those context windows
included in the cluster. for example, suppose word w0 has
k senses and thus it has k clusters of context windows, we
denote the average embedding vectors for these clusters as
    1,          ,     k. on the other hand, since the online dictionary
uses some descriptions and example sentences to interpret
each word sense, we can represent each word sense by the
average embedding of those words including its description
words and the words in the corresponding example sentences.
here, we assume the representation vectors (based on the on-
line dictionary) for the k senses of w0 are   1,          ,   k. after
that, we consecutively match each cluster to its closest word
sense in terms of the distance computed in the word embed-
ding space, i.e.,

(     i    ,   j    ) = argmin
i,j=1,       ,k

d(     i,   j),

(2)

where d(  ,   ) calculates the euclidean distance and (     i    ,   j    )
is the    rst matched pair of window cluster and word sense.
here, we simply take a greedy strategy. that is, we remove
    i    and   j    from the cluster vector set and the sense vector set,
and recursively run (2) to    nd the next matched pair till all the
pairs are found. finally, each word occurrence in the corpus
is relabeled by its associated word sense, which will be used
to learn the embeddings for word-sense pairs in the next step.
co-learning word-sense pair representations and
relation representations
after relabeling the text corpus, different occurrences of a
polysemous word may correspond to its different senses, or
more accurately word-sense pairs. we then learn the embed-
dings for word-sense pairs and relations (obtained from dic-
tionaries, such as synonym and antonym) simultaneously, by
integrating relational knowledge into the objective function
of the id27 learning model like skip-gram. we
propose to use a function er as described below to capture
the relational knowledge.

speci   cally, the existing relational knowledge extracted
from dictionaries, such as synonym, antonym, etc., can be
naturally represented in the form of a triplet (head, relation,
tail) (denoted by (hi, r, tj)     s, where s is the set of re-
lational knowledge), which consists of two word-sense pairs
(i.e. word h with its i-th sense and word t with its j-th sense),
h, t     w (w is the set of words) and a relationship r     r
(r is the set of relationships). to learn the relation represen-
tations, we make an assumption that relationships between
words can be interpreted as translation operations and they

can be represented by vectors. the principle in this model is
that if the relationship (hi, r, tj) exists, the representation of
the word-sense pair tj should be close to that of hi plus the
representation vector of the relationship r, i.e. hi + r; other-
wise, hi + r should be far away from tj. note that this model
learns word-sense pair representations and relation represen-
tations in a uni   ed continuous embedding space.

according to the above principle, we de   ne er as a
margin-based id173 function over the set of relational
knowledge s,

er = x

(hi ,r,tj )   s

   
(h

   
,r,t

x

)   s

   
(hi ,r,tj )

h   + d(hi + r, tj )     d(h

   

   
+ r, t

)i+

.

here [x]+ denotes the positive part of x,    > 0 is a mar-
gin hyperparameter, and d(  ,   ) is the distance measure for the
words in the embedding space. for simplicity, we again de-
   ne d(  ,   ) as the euclidean distance. the set of corrupted
triplets s

   

(h,r,t) is de   ned as:
(hi,r,tj) = n(h

s

   

   

, r, t)o[n(h, r, t

   

)o ,

(3)

which is constructed from s by replacing either the head
word-sense pair or the tail word-sense pair by another ran-
domly selected word with its randomly selected sense.

note that the optimization process might trivially minimize
er by simply increasing the norms of word-sense pair repre-
sentations and relation representations. to avoid this prob-
lem, we use an additional constraint on the norms, which is
a commonly-used trick in the literature [bordes et al., 2011].
however, instead of enforcing the l2-norm of the represen-
tations to 1 as used in [bordes et al., 2011], we adopt a soft
norm constraint on the relation representations as below:

ri = 2  (xi)     1,

(4)
where   (  ) is the sigmoid function   (xi) = 1/(1 + e   xi),
ri is the i-th dimension of relation vector r, and xi is a la-
tent variable, which guarantees that every dimension of the
relation representation vector is within the range (   1, 1).

by combining the skip-gram objective function and the
id173 function derived from relational knowledge,
we get the following combined objective jr that incorporates
relational knowledge into the word-sense pair embedding cal-
culation process,

jr =   er     l,

(5)
where    is the combination coef   cient. our goal is to mini-
mize the combined objective jr, which can be optimized us-
ing back propagation neural networks. by using this model,
we can obtain the distributed representations for both word-
sense pairs and relations simultaneously.

3.3 solvers for each type of questions
analogy-i
for the analogy-i questions like    a is to b as c is to ?   , we
answer them by optimizing:

d =

argmax

ib,ia,ic,id    ;d      t

cos(v(b,ib)     v(a,ia) + v(c,ic), v(d   ,id    )),

(6)

where t contains all the candidate answers, cos means co-
sine similarity, and ib, ia, ic, id    are the indexes for the word
senses of b, a, c, d    respectively. finally d is selected as
the answer.
analogy-ii
as the form of the analogy-ii questions is like    a is to ? as c
is to ?    with two lists of candidate answers, we can apply an
optimization method as below to select the best (b, d) pair,

argmax

cos(v(b   ,ib    )     v(a,ia) + v(c,ic), v(d   ,id    )),

ib    ,ia,ic,id    ;b      t1,d      t2

(7)
where t1, t2 are two lists of candidate words. thus we get
the answers b and d that can form an analogical relation be-
tween word pair (a, b) and word pair (c, d) under a certain
speci   c word sense combination.
classi   cation
for the classi   cation questions, we leverage the property that
words with similar co-occurrence information are distributed
close to each other in the embedding space. as there is one
word in the list that does not belong to others, it does not
have similar co-occurrence information with other words in
the training corpus, and thus this word should be far away
from other words in the id27 space.

according to the above discussion, we    rst calculate a
of all the candidate words

group of mean vectors miw1 ,       ,iwn
with any possible word senses as below,

miw1 ,       ,iwn

=

1
n x

wj    t

v(wj ,iwj ),

(8)

where t is the set of candidate words, n is the capacity of t ,
wj is a word in t ; iwj (j = 1,          , n ; iwj = 1,          , kwj ) is
the index for the word senses of wj, and kwj (j = 1,          , n )
is the number of word senses of wj. therefore, the number
of the mean vectors is m = qn
j=1 kwj . as both n and kwj
are very small, the computation cost is acceptable. then, we
choose the word with such a sense that its closest sense to the
corresponding mean vector is the largest among the candidate
words as the answer, i.e.,

w = argmax

wj    t

min

iwj ;l=1,       ,m

d(v(wj ,iwj ), ml).

(9)

synonym
for the synonym questions, we empirically explored two
solvers. for the    rst solver, we also leverage the property
that words with similar co-occurrence information are located
closely in the id27 space. therefore, given the
question word wq and the candidate words wi, we can    nd
the answer by the following optimization problem.

w = argmin

iwq ,iwj ;wj    t

d(v(wj ,iwj ), v(wq ,iwq )),

(10)

where t is the set of candidate words. the second solver
is based on the minimization objective of the translation dis-
tance between entities in the relational knowledge model (3).
speci   cally, we calculate the offset vector between the em-
bedding of question word wq and each word wj in the can-
didate list. then, we set the answer w as the candidate word

(12)

(13)

with which the offset is the closest to the representation vec-
tor of the synonym relation rs, i.e.,

w = argmin

iwq ,iwj ;wj    t (cid:12)(cid:12)|v(wj ,iwj )     v(wq ,iwq )|     rs(cid:12)(cid:12).

(11)

in practice, we found the second solver performs better (the
results are listed in section 4).
antonym
similar to solving the synonym questions, we explored two
solvers for antonym questions as well. that is, the    rst solver
(12) is based on the small offset distance between semanti-
cally close words whereas the second solver (13) leverages
the translation distance between two words    offset and the
embedding vector of the antonym relation. one might doubt
on the reasonableness of the    rst solver given that we aim
to    nd an answer word with opposite meaning for the target
word (i.e. antonym). we explain it here that since antonym
and its original word have similar co-occurrence information,
based on which the embedding vectors are derived, thus the
embedding vectors of both words with antonym relation will
still lie closely in the embedding space.

w = argmin

iwq ,iwj ;wj    t

d(v(wj ,iwj ), v(wq ,iwq )),

w = argmin

iwq ,iwj ;wj    t (cid:12)(cid:12)|v(wj ,iwj )     v(wq ,iwq )|     ra(cid:12)(cid:12),

where t is the set of candidate words and ra is the represen-
tation vector of the antonym relation. again we found that
the second solver performs better. similarly, for skip-gram,
only the    rst solver is applied.

4 experiments
we conduct experiments to examine whether our proposed
framework can achieve satisfying results on verbal compre-
hension questions.

4.1 data collection
training set for id27
we trained id27s on a publicly available text cor-
pus named wiki20142, which is a large text snapshot from
wikipedia. after being pre-processed by removing all the
html meta-data and replacing the digit numbers by english
words, the    nal training corpus contains more than 3.4 bil-
lion word tokens, and the number of unique words, i.e. the
vocabulary size, is about 2 million.
iq test set
according to our study, there is no online dataset speci   cally
released for verbal comprehension questions, although there
are many online iq tests for users to play with. in addition,
most of the online tests only calculate the    nal iq scores but
do not provide the correct answers. therefore, we only use
the online questions to train the verbal question classi   er de-
scribed in section 3.1. speci   cally, we manually collected
and labeled 30 verbal questions from the online iq test web-
sites3 for each of the    ve types (i.e. analogy-i, analogy-ii,

table 2: statistics of the verbal question test set.

type of questions number of questions

analogy-i
analogy-ii
classi   cation

synonym
antonym

total

50
29
53
51
49
232

classi   cation, synonym, and antonym) and trained an one-
vs-rest id166 classi   er for each type. the total accuracy on
the training set itself is 95.0%. the classi   er was then ap-
plied in the test set below.

we collected a set of verbal comprehension questions
associated with correct answers from the published iq
test books, such as [carter, 2005; carter, 2007; pape, 1993;
ken russell, 2002], and we used this collection as the test set
to evaluate the effectiveness of our new framework. in total,
this test set contains 232 questions with the corresponding
answers.the statistics of each question type are listed in ta-
ble 2.

4.2 compared methods
in the following experiments, we compare our new relation
knowledge powered model to several baselines.

random guess model (rg). random guess is the most
straightforward way for an agent to solve questions. in our
experiments, we used a random guess agent which would se-
lect an answer randomly regardless what the question was. to
measure the performance of random guess, we ran each task
for 5 times and calculated the average accuracy.

human performance (hp). since iq tests are designed to
evaluate human intelligence, it is quite natural to leverage hu-
man performance as a baseline. to collect human answers
on the test questions, we delivered them to human beings
through amazon mechanical turk, a crowd-sourcing internet
marketplace that allows people to participate human intelli-
gence tasks. in our study, we published    ve mechanical turk
jobs, one job corresponding to one speci   c question type. the
jobs were delivered to 200 people. to control the quality of
the collected results, we took several strategies: (i) we im-
posed high restrictions on the workers - we required all the
workers to be native english speakers in north american and
to be mechanical turk masters (who have demonstrated high
accuracy on previous human intelligence tasks on the me-
chanical turk marketplace); (ii) we recruited a large number
of workers in order to guarantee the statistical con   dence in
their performances; (iii) we tracked their age distribution and
education background, which are very similar to those of the
overall population in the u.s. while we can continue to im-
prove the design, we believe the current results already make
a lot of sense.

id44 model (lda). this base-
line model leveraged one of the most classical distribu-
tional word representations, i.e. id44
(lda) [blei et al., 2003]. in particular, we trained word rep-
resentations using lda on wiki2014 with the topic number
1000.

2http://en.wikipedia.org/wiki/wikipedia:database_download
3http://wechsleradultintelligencescale.com/

skip-gram model (sg). in this baseline, we applied the
id27 trained by skip-gram [mikolov et al., 2013]

table 3: accuracy of different methods among different hu-
man groups.

analogy-i

analogy-ii

classi   cation

synonym

antonym

rg
lda
hp
sg
sg-1
sg-2
glove
ms
ms-1
ms-2
ms-3
rk

24.60
28.00
45.87

38.00
38.00
45.09

36.36
40.00
17.65
48.00

11.72
13.79
34.37

24.14
20.69
24.14

19.05
20.69
20.69
34.48

20.75
39.62
47.23

37.74
39.62
32.08

41.30
41.51
47.17
52.83

19.27
27.45
50.38

45.10
47.06
47.06

50.00
49.02
47.06
60.78

23.13
30.61
53.30

40.82
44.90
40.82

36.59
40.82
30.61
51.02

total
20.51
29.31
46.23

38.36
39.66
39.03

38.67
40.09
36.73
50.86

(denoted by sg-1). in particular, when using skip-gram to
learn the embedding on wiki2014, we set the window size as
5, the embedding dimension as 500, the negative sampling
count as 3, and the epoch number as 3. in addition, we also
employed a pre-trained id27 by google4 with the
dimension of 300 (denoted by sg-2).

glove. this baseline algorithm uses another powerful
id27 model glove [pennington et al., 2014]. the
con   gurations of running glove are the same with those in
running sg-1.

(ms).

multi-sense model

in this baseline, we ap-
plied the multi-sense id27 models proposed
in [huang et al., ; tian et al., 2014; neelakantan et al., 2014]
(denoted by ms-1, ms-2 and ms-3 respectively). for ms-1,
we directly used the published multi-sense id27
vectors by the authors5, in which they set 10 senses for the
top 5% most frequent words. for ms-2 and ms-3, we get
the embedding vectors by the released codes from the authors
using the same con   gurations as ms-1.

relation knowledge powered model (rk). this is our
proposed method in section 3. in particular, when learning
the embedding on wiki2014, we set the window size as 5, the
embedding dimension as 500, the negative sampling count
as 3 (i.e.
the number of random selected negative triples
in s   ), and the epoch number as 3. we adopted the online
longman dictionary as the dictionary used in multi-sense
id91. we used a public relation knowledge set, wor-
drep [gao et al., 2014], for relation training.

4.3 experimental results
accuracy of question classi   er
we applied the question classi   er trained in section 4.1 on
the test set in table 2, and got the total accuracy 93.1%. for
rg and hp, the question classi   er was not needed. for other
methods, the wrongly classi   ed questions were also sent to
the corresponding wrong solver to    nd an answer.
if the
solver returned an empty result (which was usually caused
by invalid input format, e.g., an analogy-ii question was
wrongly input to the classi   cation solver), we would ran-
domly select an answer.

overall accuracy
table 3 demonstrates the accuracy of answering verbal ques-
tions by using all the approaches mentioned in section 4.2.

4https://code.google.com/p/id97/
5http://ai.stanford.edu/  ehhuang/

from this table, we have the following observations:
(i)
rk can achieve the best overall accuracy than all the other
methods.
in particular, rk can raise the overall accuracy
by about 4.63% over hp. (ii) rk is empirically superior
than the skip-gram models sg-1/sg-2 and glove. accord-
ing to our understanding, the improvement of rk over sg-
1/sg-2/glove comes from two aspects: multi-sense and rela-
tional knowledge. note that the performance difference be-
tween ms-1/ms-2/ms-3 and sg-1/sg-2/glove is not signif-
icant, showing that simply changing single-sense word em-
bedding to multi-sense id27 does not bring too
much bene   t. one reason is that the rare word-senses do not
have enough training data (contextual information) to pro-
duce high-quality id27. by further introducing
the relational knowledge among word-senses, the training for
rare word-senses will be linked to the training of their related
word-senses. as a result, the embedding quality of the rare
word-senses will be improved. (iii) rk is empirically supe-
rior than the two multi-sense algorithms ms-1, ms-2 and
ms-3, demonstrating the effectiveness brought by adopting
less model parameters and using online dictionary in build-
ing the multi-sense embedding model.

these results are quite impressive, indicating the potential
of using machine to comprehend human knowledge and even
achieve the comparable level of human intelligence.
accuracy in different question types
table 3 reports the accuracy of answering various types of
verbal questions by each comparing method. from the ta-
ble, we can observe that the sg and ms models can achieve
competitive accuracy on some certain question types (like
synonym) compared with hp. after incorporating knowledge
into learning id27, our rk model can improve
the accuracy over all question types. moreover, the table
shows that rk can result in a big improvement over hp on
the question types of synonym and classi   cation, while its
accuracy on the other question types is not so good as these
two types.

to sum up, the experimental results have demonstrated the
effectiveness of the proposed rk model compared with sev-
eral baseline methods. although the test set is not large, the
generalization of rk to other test sets should not be a concern
due to the unsupervised nature of our model.

5 conclusions
we investigated how to automatically solve verbal compre-
hension questions in iq tests by using the id27
techniques in deep learning.
in particular, we proposed a
three-step framework: (i) to recognize the speci   c type of a
verbal comprehension question by a classi   er, (ii) to leverage
a novel deep learning model to co-learn the representations
of both word-sense pairs and relations among words (or their
senses), (iii) to design dedicated solvers, based on the ob-
tained word-sense pair representations and relation represen-
tations, for addressing each type of questions. experimental
results have illustrated that this novel framework can achieve
better performance than existing methods for solving verbal
comprehension questions and even exceed the average per-
formance of the amazon mechanical turk workers involved

in the experiments.

references
[bengio et al., 2003] yoshua bengio, r  ejean ducharme,
pascal vincent, and christian jauvin. a neural proba-
bilistic language model. journal of machine learning re-
search, 3:1137   1155, 2003.

[blei et al., 2003] david m blei, andrew y ng, and
michael i jordan. id44. the journal
of machine learning research, 3:993   1022, 2003.

[bordes et al., 2011] antoine bordes, jason weston, ronan
collobert, yoshua bengio, et al. learning structured em-
beddings of knowledge bases. in aaai, 2011.

[carter, 2005] philip carter. the complete book of intelli-

gence tests. john wiley & sons ltd, 2005.

[carter, 2007] philip carter. the ultimate iq test book:
1,000 practice test questions to boost your brain power.
kogan page publishers, 2007.

[celikyilmaz et al., 2015] asli celikyilmaz, dilek hakkani-
tur, panupong pasupat, and ruhi sarikaya. enriching
id27s using id13 for semantic
tagging in conversational id71. in proceedings
of aaai, 2015.

[collobert and weston, 2008] ronan collobert and jason
weston. a uni   ed architecture for natural language pro-
cessing: deep neural networks with multitask learning. in
proceedings of icml, pages 160   167. acm, 2008.

[dumais et al., 1988] susan t dumais, george w furnas,
thomas k landauer, scott deerwester, and richard
harshman. using latent semantic analysis to improve ac-
cess to textual information.
in proceedings of sigchi,
1988.

[fried and duh, 2014] daniel fried and kevin duh. incor-
porating both distributional and relational semantics in
word representations. corr, abs/1412.4369, 2014.

[gao et al., 2014] bin gao, jiang bian, and tie-yan liu.
wordrep: a benchmark for research on learning word rep-
resentations. arxiv preprint arxiv:1407.1640, 2014.

[hosseini et al., 2014] mohammad javad hosseini, han-
naneh hajishirzi, oren etzioni, and nate kushman. learn-
ing to solve arithmetic word problems with verb catego-
rization. 2014.

[huang et al., ] eric h huang, richard socher, christo-
pher d manning, and andrew y ng.
improving word
representations via global context and multiple word pro-
totypes. in proceedings of acl.

[ken russell, 2002] philip carter ken russell. the times

book of iq tests. kogan page limited, 2002.

[kushmany et al., 2014] nate kushmany, yoav artziz, luke
zettlemoyerz, and regina barzilayy. learning to auto-
matically solve algebra word problems. in proceedings of
acl, 2014.

[luong et al., 2013] minh-thang luong, richard socher,
and christopher d manning. better word representations
with id56s for morphology. conll-
2013, 104, 2013.

[mikolov et al., 2013] tomas mikolov, ilya sutskever, kai
chen, greg s corrado, and jeff dean. distributed rep-
resentations of words and phrases and their composition-
ality. in nips, pages 3111   3119, 2013.

[neelakantan et al., 2014] arvind neelakantan,

jeevan
shankar, alexandre passos, and andrew mccallum.
ef   cient non-parametric estimation of multiple em-
beddings per word in vector space.
in proceedings of
emnlp, pages 1059   1069, doha, qatar, october 2014.
association for computational linguistics.

[pape, 1993] dan pape. the original cambridge self scor-

ing iq test. the magni group, inc, 1993.

[pennington et al., 2014] jeffrey

richard
socher, and christopher d manning. glove: global
vectors for word representation. proceedings of emnlp,
12:1532   1543, 2014.

pennington,

[pint  er et al., 2012] bal  azs pint  er, gyula v  or  os, zolt  an
szab  o, and andr  as l  orincz. automated word puzzle gen-
eration via topic dictionaries. corr, abs/1206.0377, 2012.
[sanghi and dowe, 2003] pritika sanghi and david dowe. a
computer program capable of passing i.q. tests.
in pro-
ceedings of the joint international conference on cogni-
tive science, 2003.

[seo et al., 2014] min joon seo, hannaneh hajishirzi, ali
farhadi, and oren etzioni. diagram understanding in ge-
ometry questions. 2014.

[socher et al., 2013] richard socher, danqi chen, christo-
pher d manning, and andrew ng. reasoning with neural
tensor networks for knowledge base completion. in nips,
pages 926   934, 2013.

[stern, 1914] william stern. the psychological methods of

testing intelligence. warwick & york, 1914.

[strannegard et al., 2012] claes

strannegard, mehrdad
amirghasemi, and simon ulfsbacker. an anthropomor-
phic method for number sequence problems. cognitive
systems research, 2012.

[tian et al., 2014] fei tian, hanjun dai, jiang bian, bin
gao, rui zhang, enhong chen, and tie-yan liu. a prob-
abilistic model for learning multi-prototype word embed-
dings. in proceedings of coling, 2014.

[wechsler, 2008] david wechsler. wechsler adult intelli-
gence scale   fourth edition (wais   iv). san antonio, tx:
ncs pearson, 2008.

[weston et al., 2013a] jason weston, antoine bordes, ok-
sana yakhnenko, and nicolas usunier. connecting lan-
guage and knowledge bases with embedding models for
id36. arxiv preprint arxiv:1307.7973, 2013.
[weston et al., 2013b] jason weston, antoine bordes, ok-
sana yakhnenko, and nicolas usunier. connecting lan-
guage and knowledge bases with embedding models for
id36. in proceedings of emnlp, 2013.

[weston et al., 2015] jason weston, antoine bordes, sumit
chopra, and tomas mikolov. towards ai-complete ques-
tion answering: a set of prerequisite toy tasks.
arxiv
preprint arxiv:1502.05698, 2015.

