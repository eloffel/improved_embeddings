6
1
0
2

 
r
a

m
4

 

 
 
]
l
c
.
s
c
[
 
 

3
v
8
9
1
8
0

.

1
1
5
1
:
v
i
x
r
a

published as a conference paper at iclr 2016

towards universal
paraphrastic sentence embeddings

john wieting mohit bansal kevin gimpel karen livescu
toyota technological institute at chicago, chicago, il, 60637, usa
{jwieting,mbansal,kgimpel,klivescu}@ttic.edu

abstract

we consider the problem of learning general-purpose, paraphrastic sentence em-
beddings based on supervision from the paraphrase database (ganitkevitch et al.,
2013). we compare six compositional architectures, evaluating them on annotated
textual similarity datasets drawn both from the same distribution as the training
data and from a wide range of other domains. we    nd that the most complex ar-
chitectures, such as long short-term memory (lstm) recurrent neural networks,
perform best on the in-domain data. however, in out-of-domain scenarios, sim-
ple architectures such as word averaging vastly outperform lstms. our simplest
averaging model is even competitive with systems tuned for the particular tasks
while also being extremely ef   cient and easy to use.
in order to better understand how these architectures compare, we conduct further
experiments on three supervised nlp tasks: sentence similarity, entailment, and
sentiment classi   cation. we again    nd that the word averaging models perform
well for sentence similarity and entailment, outperforming lstms. however, on
sentiment classi   cation, we    nd that the lstm performs very strongly   even
recording new state-of-the-art performance on the stanford sentiment treebank.
we then demonstrate how to combine our pretrained sentence embeddings with
these supervised tasks, using them both as a prior and as a black box feature
extractor. this leads to performance rivaling the state of the art on the sick
similarity and entailment tasks. we release all of our resources to the research
community1 with the hope that they can serve as the new baseline for further
work on universal sentence embeddings.

1

introduction

id27s have become ubiquitous in natural language processing (nlp). several re-
searchers have developed and shared id27s trained on large datasets (collobert et al.,
2011; mikolov et al., 2013; pennington et al., 2014), and these have been used effectively for many
downstream tasks (turian et al., 2010; socher et al., 2011; kim, 2014; bansal et al., 2014; tai et al.,
2015). there has also been recent work on creating representations for word sequences such as
phrases or sentences. many functional architectures have been proposed to model compositionality
in such sequences, ranging from those based on simple operations like addition (mitchell & lapata,
2010; yu & dredze, 2015; iyyer et al., 2015) to those based on richly-structured functions like re-
cursive neural networks (socher et al., 2011), convolutional neural networks (kalchbrenner et al.,
2014), and recurrent neural networks using long short-term memory (lstm) (tai et al., 2015).
however, there is little work on learning sentence representations that can be used across domains
with the same ease and effectiveness as id27s. in this paper, we explore compositional
models that can encode arbitrary word sequences into a vector with the property that sequences with
similar meaning have high cosine similarity, and that can, importantly, also transfer easily across
domains. we consider six compositional architectures based on neural networks and train them on
noisy phrase pairs from the paraphrase database (ppdb; ganitkevitch et al., 2013).

1trained

models

and

code

for

http://ttic.uchicago.edu/  wieting.

training

and

evaluation

are

available

at

1

published as a conference paper at iclr 2016

we consider models spanning the range of complexity from word averaging to lstms. with the
simplest word averaging model, there are no additional compositional parameters. the only param-
eters are the word vectors themselves, which are learned to produce effective sequence embeddings
when averaging is performed over the sequence. we add complexity by adding layers, leading to
variants of deep averaging networks (iyyer et al., 2015). we next consider several recurrent network
variants, culminating in lstms because they have been found to be effective for many types of
sequential data (graves et al., 2008; 2013; greff et al., 2015), including text (sutskever et al., 2014;
vinyals et al., 2014; xu et al., 2015a; hermann et al., 2015; ling et al., 2015; wen et al., 2015).

to evaluate our models, we consider two tasks drawn from the same distribution as the training data,
as well as 22 semeval textual similarity datasets from a variety of domains (such as news, tweets,
web forums, and image and video captions). interestingly, we    nd that the lstm performs well on
the in-domain task, but performs much worse on the out-of-domain tasks. we discover surprisingly
strong performance for the models based on word averaging, which perform well on both the in-
domain and out-of-domain tasks, beating the best lstm model by 16.5 pearson   s r on average.
moreover, we    nd that learning id27s in the context of vector averaging performs much
better than simply averaging pretrained, state-of-the-art id27s. our average pearson   s
r over all 22 semeval datasets is 17.1 points higher than averaging glove vectors2 and 12.8 points
higher than averaging paragram-sl999 vectors.3
our    nal sentence embeddings4 place in the top 25% of all submitted systems in every semeval
sts task from 2012 through 2015, being best or tied for best on 4 of the datasets.5 this is surprising
because the submitted systems were designed for those particular tasks, with access to training and
tuning data speci   cally developed for each task.

while the above experiments focus on transfer, we also consider the fully supervised setting (ta-
ble 5). we compare the same suite of compositional architectures for three supervised nlp tasks:
sentence similarity and id123 using the 2014 semeval sick dataset (marelli et al.,
2014), and sentiment classi   cation using the stanford sentiment treebank (socher et al., 2013). we
again    nd strong performance for the word averaging models for both similarity and entailment, out-
performing the lstm. however, for sentiment classi   cation, we see a different trend. the lstm
now performs best, achieving 89.2% on the coarse-grained sentiment classi   cation task. this result,
to our knowledge, is the new state of the art on this task.

we then demonstrate how to combine our ppdb-trained sentence embedding models with super-
vised nlp tasks. we    rst use our model as a prior, yielding performance on the similarity and
entailment tasks that rivals the state of the art. we also use our sentence embeddings as an effective
black box feature extractor for downstream tasks, comparing favorably to recent work (kiros et al.,
2015).

we release our strongest sentence embedding model, which we call paragram-phrase xxl, to
the research community.6 since it consists merely of a new set of id27s, it is extremely
ef   cient and easy to use for downstream applications. our hope is that this model can provide a new
simple and strong baseline in the quest for universal sentence embeddings.

2 related work

researchers have developed many ways to embed word sequences for nlp. they mostly focus
on the question of compositionality: given vectors for words, how should we create a vector for
a word sequence? mitchell & lapata (2008; 2010) considered bigram compositionality, compar-
ing many functions for composing two word vectors into a single vector to represent their bigram.
follow-up work by blacoe & lapata (2012) found again that simple operations such as vector ad-

2we used the publicly available 300-dimensional vectors that were trained on the 840 billion token common

crawl corpus, available at http://nlp.stanford.edu/projects/glove/.

3these

are

at
http://ttic.uchicago.edu/  wieting. they give human-level performance on two commonly
used word similarity datasets, wordsim353 (finkelstein et al., 2001) and siid113x-999 (hill et al., 2015).

from wieting et al.

300-dimensional

available

vectors

(2015)

and

are

4denoted paragram-phrase-xxl and discussed in section 4.3.
5as measured by the average pearson   s r over all datasets in each task; see table 4.
6available at http://ttic.uchicago.edu/  wieting.

2

published as a conference paper at iclr 2016

dition performed strongly. many other compositional architectures have been proposed. some have
been based on id65 (baroni et al., 2014; paperno et al., 2014; polajnar et al.,
2015; tian et al., 2015), while the current trend is toward development of neural network archi-
tectures. these include neural bag-of-words models (kalchbrenner et al., 2014), deep averaging
networks (dans) (iyyer et al., 2015), feature-weighted averaging (yu & dredze, 2015), recursive
neural networks based on parse structure (socher et al., 2011; 2012; 2013;   irsoy & cardie, 2014;
wieting et al., 2015), recursive networks based on non-syntactic hierarchical structure (zhao et al.,
2015; chen et al., 2015b), convolutional neural networks (kalchbrenner et al., 2014; kim, 2014;
hu et al., 2014; yin & sch  utze, 2015; he et al., 2015), and recurrent neural networks using long
short-term memory (tai et al., 2015; ling et al., 2015; liu et al., 2015). in this paper, we compare
six architectures: word averaging, word averaging followed by a single linear projection, dans, and
three variants of recurrent neural networks, including lstms.7
most of the work mentioned above learns compositional models in the context of supervised learn-
ing. that is, a training set is provided with annotations and the composition function is learned for
the purposes of optimizing an objective function based on those annotations. the models are then
evaluated on a test set drawn from the same distribution as the training set.

in this paper, in contrast, we are primarily interested in creating general purpose, domain indepen-
dent embeddings for word sequences. there have been research efforts also targeting this goal.
one approach is to train an autoencoder in an attempt to learn the latent structure of the sequence,
whether it be a sentence with a parse tree (socher et al., 2011), or a longer sequence such as a
paragraph or document (li et al., 2015b). other recently proposed methods, including paragraph
vectors (le & mikolov, 2014) and skip-thought vectors (kiros et al., 2015), learn sequence repre-
sentations that are predictive of words inside the sequence or in neighboring sequences. these
methods produce generic representations that can be used to provide features for text classi   cation
or sentence similarity tasks. while skip-thought vectors capture similarity in terms of discourse con-
text, in this paper we are interested in capturing paraphrastic similarity, i.e., whether two sentences
have the same meaning.

our learning formulation draws from a large body of related work on learning input repre-
sentations in order to maximize similarity in the learned space (weston et al., 2010; yih et al.,
2011; huang et al., 2013; hermann & blunsom, 2014; socher et al., 2014; faruqui & dyer, 2014;
bordes et al., 2014b;a; lu et al., 2015), including our prior work (wieting et al., 2015). we focus
our exploration here on modeling and keep the learning methodology mostly    xed, though we do
include certain choices about the learning procedure in our hyperparameter tuning space for each
model.

3 models and training

our goal is to embed sequences into a low-dimensional space such that cosine similarity in the
space corresponds to the strength of the paraphrase relationship between the sequences. we exper-
imented with six models of increasing complexity. the simplest model embeds a word sequence
x = hx1, x2, ..., xni by averaging the vectors of its tokens. the only parameters learned by this
model are the id27 matrix ww:

gparagram-phrase (x) =

1
n

w xi
w

n

xi

where w xi
phrase embeddings.

w is the id27 for word xi. we call the learned embeddings paragram-

in our second model, we learn a projection in addition to the id27s:

gproj(x) = wp  1

n

w xi

w ! + b

n

xi

7in prior work, we experimented with id56s on binarized parses of

the
ppdb (wieting et al., 2015), but we found that many of the phrases in ppdb are not sentences or even con-
stituents, causing the parser to have unexpected behavior.

3

published as a conference paper at iclr 2016

where wp is the projection matrix and b is a bias vector.
our third model is the deep averaging network (dan) of iyyer et al. (2015). this is a generalization
of the above models that typically uses multiple layers as well as nonlinear id180. in
our experiments below, we tune over the number of layers and choice of activation function.

our fourth model is a standard recurrent network (id56) with randomly initialized weight matrices
and nonlinear activations:

ht = f (wxw xt
gid56(x) = h   1

w + whht   1 + b)

where f is the activation function (either tanh or recti   ed linear unit; the choice is tuned), wx and
wh are parameter matrices, b is a bias vector, and h   1 refers to the hidden vector of the last token.
our    fth model is a special id56 which we call an identity-id56. in the identity-id56, the weight
matrices are initialized to identity, the bias is initialized to zero, and the activation is the identity
function. we divide the    nal output vector of the identity-id56 by the number of tokens in the
sequence. thus, before any updates to the parameters, the identity-id56 simply averages the word
embeddings. we also regularize the identity-id56 parameters to their initial values. the idea is that,
with high id173, the identity-id56 is simply averaging id27s. however, it is a
richer architecture and can take into account word order and hopefully improve upon the averaging
baseline.

is the most expressive. we use long short-term memory
our sixth and    nal model
(lstm) (hochreiter & schmidhuber, 1997), a recurrent neural network (id56) architecture de-
signed to model sequences with long-distance dependencies. lstms have recently been shown
to produce state-of-the-art results in a variety of sequence processing tasks (chen et al., 2015a;
filippova et al., 2015; xu et al., 2015c; belinkov & glass, 2015; wang & nyberg, 2015). we use
the version from gers et al. (2003) which has the following equations:

w + whiht   1 + wcict   1 + bi)
w + whf ht   1 + wcf ct   1 + bf )

w + whcht   1 + bc)

w + whoht   1 + wcoct + bo)

it =    (wxiw xt
ft =    (wxf w xt
ct = ftct   1 + it tanh (wxcw xt
ot =    (wxow xt
ht = ot tanh(ct)
glstm(x) = h   1

where    is the logistic sigmoid function. we found that the choice of whether or not to include the
output gate had a signi   cant impact on performance, so we used two versions of the lstm model,
one with the output gate and one without. for all models, we learn the id27s themselves,
denoting the trainable id27 parameters by ww. we denote all other trainable parameters
by wc (   compositional parameters   ), though the paragram-phrase model has no compositional
parameters. we initialize ww using some embeddings pretrained from large corpora.

3.1 training

we mostly follow the approach of wieting et al. (2015). the training data consists of (possibly
noisy) pairs taken directly from the original paraphrase database (ppdb) and we optimize a margin-
based loss.
our training data consists of a set x of phrase pairs hx1, x2i, where x1 and x2 are assumed to be
paraphrases. the objective function follows:

min
wc,ww

1

|x|  xhx1,x2i   x

max(0,        cos(g(x1), g(x2)) + cos(g(x1), g(t1)))

+ max(0,        cos(g(x1), g(x2)) + cos(g(x2), g(t2)))(cid:19)

+  c kwck2 +   w kwwinitial     wwk2

(1)

4

published as a conference paper at iclr 2016

where g is the embedding function in use (e.g., glstm),    is the margin,   c and   w are regulariza-
tion parameters, wwinitial is the initial id27 matrix, and t1 and t2 are carefully-selected
negative examples taken from a mini-batch during optimization. the intuition is that we want the
two phrases to be more similar to each other (cos(g(x1), g(x2))) than either is to their respective
negative examples t1 and t2, by a margin of at least   .

3.1.1 selecting negative examples

to select t1 and t2 in eq. 1, we tune the choice between two approaches. the    rst, max, simply
chooses the most similar phrase in some set of phrases (other than those in the given phrase pair).
for simplicity and to reduce the number of tunable parameters, we use the mini-batch for this set,
but it could be a separate set. formally, max corresponds to choosing t1 for a given hx1, x2i as
follows:

t1 =

argmax

cos(g(x1), g(t))

t:ht,  i   xb\{hx1,x2i}

where xb     x is the current mini-batch. that is, we want to choose a negative example ti that is
similar to xi according to the current model parameters. the downside of this approach is that we
may occasionally choose a phrase ti that is actually a true paraphrase of xi.
the second strategy selects negative examples using max with id203 0.5 and selects them
randomly from the mini-batch otherwise. we call this sampling strategy mix. we tune over the
strategy in our experiments.

4 experiments

4.1 data

we experiment on 24 textual similarity datasets, covering many domains, including all datasets
from every semeval semantic textual similarity (sts) task (2012-2015). we also evaluate on the
semeval 2015 twitter task (xu et al., 2015b) and the semeval 2014 semantic relatedness task
(marelli et al., 2014), as well as two tasks that use ppdb data (wieting et al., 2015; pavlick et al.,
2015).

the    rst sts task was held in 2012 and these tasks have been held every year since. given two sen-
tences, the objective of the task is to predict how similar they are on a 0-5 scale, where 0 indicates the
sentences are on different topics and 5 indicates that they are completely equivalent. each sts task
consists of 4-6 different datasets and the tasks cover a wide variety of domains which we have cat-
egorized below. most submissions for these tasks use supervised models that are trained and tuned
on either provided training data or similar datasets from older tasks. details on the number of teams
and submissions for each task and the performance of the submitted systems for each dataset are
included in table 1 and table 2 respectively. for more details on these tasks please refer to the rel-
evant publications for the 2012 (agirre et al., 2012), 2013 (agirre et al., 2013), 2014 (agirre et al.,
2014), and 2015 (agirre et al., 2015) tasks.

dataset
2012 sts
2013 sts
2014 sts
2015 sts
2014 sick
2015 twitter

no. of teams no. of submissions

35
34
15
29
17
19

88
89
38
74
66
26

table 1: details on numbers of teams and submissions in the sts tasks used for evaluation.

below are the textual domains contained in the sts tasks:
news: newswire was used in the 2012 task (msrpar) and the 2013 and 2014 tasks (deft news).
image and video descriptions: image descriptions generated via id104 were used in the
2013 and 2014 tasks (images). video descriptions were used in the 2012 task (msrvid).
glosses: glosses from id138, ontonotes, and framenet were used in the 2012, 2013, and 2014
tasks (onwn and fnwn).

5

published as a conference paper at iclr 2016

mt evaluation: the output of machine translation systems with their reference translations was
used in the 2012 task (smt-eur and smt-news) and the 2013 task (smt).
headlines: headlines of news articles were used in the 2013, 2014, and 2015 tasks (headline).
web forum: forum posts were used in the 2014 task (deft forum).
twitter: pairs containing a tweet related to a news headline and a sentence pertaining to the same
news headline. this dataset was used in the 2014 task (tweet news).
belief: text from the deft committed belief annotation (ldc2014e55) was used in the 2015 task
(belief).
questions and answers: paired answers to the same question from stackexchange (answers-
forums) and the beetle corpus (dzikovska et al., 2010) (answers-students) were used in 2015.

for tuning, we use two datasets that contain ppdb phrase pairs scored by human annotators on
the strength of their paraphrase relationship. one is a large sample of 26,456 annotated phrase pairs
developed by pavlick et al. (2015). the second, called annotated-ppdb, was developed in our prior
work (wieting et al., 2015) and is a small set of 1,000 annotated phrase pairs that were    ltered to
focus on challenging paraphrase phenomena.

4.2 id21

4.2.1 experimental settings

as training data, we used the xl section8 of ppdb which contains 3,033,753 unique phrase pairs.
however, for hyperparameter tuning we only used 100k examples sampled from ppdb xxl and
trained for 5 epochs. then after    nding the hyperparameters that maximize spearman   s    on the
pavlick et al. ppdb task, we trained on the entire xl section of ppdb for 10 epochs. we used
paragram-sl999 embeddings to initialize the id27 matrix (ww) for all models.
we chose the pavlick et al. task for tuning because we wanted our entire procedure to only make use
of ppdb and use no other resources. in particular, we did not want to use any sts tasks for training
or hyperparameter tuning. we chose the pavlick et al. dataset over annotated-ppdb due to its larger
size. but in practice the datasets are very similar and tuning on either produces similar results.

to learn model parameters for all experiments in this section, we minimize eq. 1. our models
have the following tunable hyperparameters:9   c, the l2 regularizer on the compositional param-
eters wc (not applicable for the word averaging model), the pool of phrases used to obtain neg-
ative examples (coupled with mini-batch size b, to reduce the number of tunable hyperparame-
ters),   w, the regularizer on the id27s, and   , the margin. we also tune over opti-
mization method (either adagrad (duchi et al., 2011) or adam (kingma & ba, 2014)), learning
rate (from {0.05, 0.005, 0.0005}), whether to clip the gradients with threshold 1 (pascanu et al.,
2012), and whether to use mix or max sampling. for the classic id56, we further tuned whether
to use tanh or recti   ed linear unit id180; for the identity-id56, we tuned   c over
{1000, 100, 10, 1} because we wanted higher id173 on the composition parameters; for the
dans we tuned over activation function (tanh or recti   ed linear unit) and the number of layers
(either 1 or 2); for the lstms we tuned on whether to include an output gate. we    x the output
dimensionalities of all models that require doing so to the dimensionality of our id27s
(300).

4.2.2 results

the results on all sts tasks as well as the sick and twitter tasks are shown in table 2. we include
results on the ppdb tasks in table 3. in table 2, we    rst show the median, 75th percentile, and
highest score from the of   cial task rankings. we then report the performance of our seven mod-
els: paragram-phrase (pp), identity-id56 (iid56), projection (proj.), deep-averaging network
(dan), recurrent neural network (id56), lstm with output gate (o.g.), and lstm without output

8ppdb comes in different sizes (s, m, l, xl, xxl, and xxxl), where each larger size subsumes all
smaller ones. the phrases are sorted by a con   dence measure and so the smaller sets contain higher precision
paraphrases.

9for   c we searched over {10   3, 10   4, 10   5, 10   6}, for b we searched over {25, 50, 100}, for   w we
searched over {10   5, 10   6, 10   7, 10   8} as well as the setting in which we do not update ww, and for    we
searched over {0.4, 0.6, 0.8}.

6

published as a conference paper at iclr 2016

gate (no o.g.). we compare to three baselines: skip-thought vectors10 (kiros et al., 2015), denoted
   st   , averaged glove11 vectors (pennington et al., 2014), and averaged paragram-sl999 vec-
tors (wieting et al., 2015), denoted    psl   . note that the glove vectors were used to initialize the
paragram-sl999 vectors which were, in turn, used to initialize our paragram-phrase embed-
dings. we compare to skip-thought vectors because trained models are publicly available and they
show impressive performance when used as features on several tasks including textual similarity.

dataset

50% 75% max

pp

proj. dan id56 iid56

lstm
(no o.g.)

msrpar
msrvid
smt-eur
onwn
smt-news
sts 2012 average
headline
onwn
fnwn
smt
sts 2013 average
deft forum
deft news
headline
images
onwn
tweet news
sts 2014 average
answers-forums
answers-students
belief
headline
images
sts 2015 average
2014 sick
2015 twitter

51.5
75.5
44.4
608
40.1
54.5
64.0
52.8
32.7
31.8
45.3
36.6
66.2
67.1
75.6
78.0
64.7
64.7
61.3
67.6
67.7
74.2
80.4
70.2
71.4
49.9

57.6
80.3
48.1
65.9
45.4
59.5
68.3
64.8
38.1
34.6
51.4
46.8
74.0
75.4
79.0
81.1
72.2
71.4
68.2
73.6
72.2
80.8
84.3
75.8
79.9
52.5

73.4
88.0
56.7
72.7
60.9
70.3
78.4
84.3
58.2
40.4
65.3
53.1
78.5
78.4
83.4
87.5
79.2
76.7
73.9
78.8
77.2
84.2
87.1
80.2
82.8
61.9

42.6
74.5
47.3
70.6
58.4
58.7
72.4
67.7
43.9
39.2
55.8
48.7
73.1
69.7
78.5
78.8
76.4
70.9
68.3
78.2
76.2
74.8
81.4
75.8
71.6
52.9

43.7
74.0
49.4
70.1
62.8
60.0
72.6
68.0
46.8
39.8
56.8
51.1
72.2
70.8
78.1
79.5
75.8
71.3
65.1
77.8
75.4
75.2
80.3
74.8
71.6
52.8

40.3
70.0
43.8
65.9
60.0
56.0
71.2
64.1
43.1
38.3
54.2
49.0
71.7
69.2
76.9
75.7
74.2
69.5
62.6
78.1
72.0
73.5
77.5
72.7
70.7
53.7

18.6
66.5
40.9
63.1
51.3
48.1
59.5
54.6
30.9
33.8
44.7
41.5
53.7
57.5
67.6
67.7
58.0
57.7
32.8
64.7
51.9
65.3
71.4
57.2
61.2
45.1

43.4
73.4
47.1
70.1
58.1
58.4
72.8
69.4
45.3
39.4
56.7
49.0
72.4
70.2
78.2
78.8
76.9
70.9
67.4
78.2
75.9
75.1
81.1
75.6
71.2
52.9

16.1
71.3
41.8
65.2
60.8
51.0
57.4
68.5
24.7
30.1
45.2
44.2
52.8
57.5
68.5
76.9
58.7
59.8
51.9
71.5
61.7
64.0
70.4
63.9
63.9
47.6

lstm
(o.g.)
9.3
71.3
44.3
56.4
51.0
46.4
48.5
50.4
38.4
28.8
41.5
46.1
39.1
50.9
62.9
61.7
48.2
51.5
50.7
55.7
52.6
56.6
64.2
56.0
59.0
36.1

st

glove

psl

16.8
41.7
35.2
29.7
30.8
30.8
34.6
10.0
30.4
24.3
24.8
12.9
23.5
37.8
51.2
23.3
39.9
31.4
36.1
33.0
24.6
43.6
17.7
31.0
49.8
24.7

47.7
63.9
46.0
55.1
49.6
52.5
63.8
49.0
34.2
22.3
42.3
27.1
68.0
59.5
61.0
58.4
51.2
54.2
30.5
63.0
40.5
61.8
67.5
52.7
65.9
30.3

41.6
60.0
42.4
63.0
57.0
52.8
68.8
48.0
37.9
31.0
46.4
37.2
67.0
65.3
62.0
61.1
64.7
59.5
38.8
69.2
53.2
69.0
69.9
60.0
66.4
36.3

table 2: results on semeval textual similarity datasets (pearson   s r    100). the highest score in
each row is in boldface (omitting the of   cial task score columns).
the results in table 2 show strong performance of our two simplest models:
the paragram-
phrase embeddings (pp) and our projection model (proj.). they outperform the other models
on all but 5 of the 22 datasets. the iid56 model has the next best performance, while the lstm
models lag behind. these results stand in marked contrast to those in table 3, which shows very
similar performance across models on the in-domain ppdb tasks, with the lstm models slightly
outperforming the others. for the lstm models, it is also interesting to note that removing the
output gate results in stronger performance on the textual similarity tasks. removing the output
gate improves performance on 18 of the 22 datasets. the lstm without output gate also performs
reasonably well compared to our strong paragram-sl999 addition baseline, beating it on 12 of
the 22 datasets.

4.3

paragram-phrase xxl

since we found that paragram-phrase embeddings have such strong performance, we trained
this model on more data from ppdb and also used more data for hyperparameter tuning. for tuning,
we used all of ppdb xl and trained for 10 epochs, then trained our    nal model for 10 epochs on
the entire phrase section of ppdb xxl, consisting of 9,123,575 unique phrase pairs.12 we show
the results of this improved model, which we call paragram-phrase xxl, in table 4. we also
report the median, 75th percentile, and maximum score from our suite of textual similarity tasks.

10note that we pre-processed the training data with the tokenizer from stanford corenlp (manning et al.,
2014) rather than the included nltk (bird et al., 2009) tokenizer. we found that doing so signi   cantly im-
proves the performance of the skip-thought vectors.

11we used the publicly available 300-dimensional vectors that were trained on the 840 billion token common

crawl corpus, available at http://nlp.stanford.edu/projects/glove/.

12we    xed batchsize to 100 and    to 0.4, as these were the optimal values for the experiment in table 2.
then, for   w we searched over {10   6, 10   7, 10   8}, and tuned over mix and max sampling. to optimize,
we used adagrad with a learning rate of 0.05.

7

published as a conference paper at iclr 2016

model

paragram-phrase
projection
dan
id56
iid56
lstm (no o.g.)
lstm (o.g.)
skip-thought
glove
paragram-sl999

pavlick et al.

(oracle)

60.3
61.0
60.9
60.5
60.3
61.6
61.5
39.3
44.8
55.3

pavlick et al.

(test)
60.0
58.4
60.1
60.3
60.0
61.3
60.9
39.3
44.8
55.3

annotated-ppdb

(test)
53.5
52.8
52.3
51.8
53.9
53.4
52.9
31.9
25.3
40.4

table 3: results on the ppdb tasks (spearman   s       100). for the task in pavlick et al. (2015), we
include the oracle result (the max spearman   s    on the dataset), since this dataset was used for model
selection for all other tasks, as well as test results where models were tuned on annotated-ppdb.

dataset

50% 75% max

msrpar
msrvid
smt-eur
onwn
smt-news
sts 2012 average
headline
onwn
fnwn
smt
sts 2013 average
deft forum
deft news
headline
images
onwn
tweet news
sts 2014 average
answers-forums
answers-students
belief
headline
images
sts 2015 average
2014 sick   
2015 twitter

51.5
75.5
44.4
60.8
40.1
54.5
64.0
52.8
32.7
31.8
45.3
36.6
66.2
67.1
75.6
78.0
64.7
64.7
61.3
67.6
67.7
74.2
80.4
70.2
71.4
49.9

57.6
80.3
48.1
65.9
45.4
59.5
68.3
64.8
38.1
34.6
51.4
46.8
74.0
75.4
79.0
81.1
72.2
71.4
68.2
73.6
72.2
80.8
84.3
75.8
79.9
52.5

73.4
88.0
56.7
72.7
60.9
70.3
78.4
84.3
58.2
40.4
65.3
53.1
78.5
78.4
83.4
87.5
79.2
76.7
73.9
78.8
77.2
84.2
87.1
80.2
82.8
61.9

paragram-

phrase-

xxl
44.8
79.6
49.5
70.4
63.3
61.5
73.9
73.8
47.7
40.4
58.9
53.4
74.4
71.5
80.4
81.5
77.4
73.1
69.1
78.0
78.2
76.4
83.4
77.0
72.7
52.4

table 4: results on semeval textual similarity datasets (pearson   s r   100) for paragram-phrase
xxl embeddings. results that match or exceed the best shared task system are shown in bold.    for
the 2014 sick task, the median, 75th percentile, and maximum include only the primary runs as the
full set of results was not available.

paragram-phrase xxl matches or exceeds the best performance on 4 of the datasets (smt-
news, smt, deft forum, and belief) and is within 3 points of the best performance on 8 out of 22.
we have made this trained model available to the research community.13

4.4 using representations in learned models

we explore two natural questions regarding our representations learned from ppdb: (1) can these
embeddings improve the performance of other models through initialization and id173? (2)
can they effectively be used as features for downstream tasks? to address these questions, we

13available at http://ttic.uchicago.edu/  wieting.

8

published as a conference paper at iclr 2016

task

similarity (sick)
entailment (sick)
binary sentiment (sst)

word

averaging

86.40
84.6
83.0

proj.

85.93
84.0
83.0

dan

85.96
84.5
83.4

id56

73.13
76.4
86.5

lstm lstm
(o.g.)
(no o.g.)
83.41
82.0
89.2

85.45
83.2
86.6

w/ universal
id173

86.84
85.3
86.9

table 5: results from supervised training of each compositional architecture on similarity, entail-
ment, and sentiment tasks. the last column shows results regularizing to our universal parameters
from the models in table 2. the    rst row shows pearson   s r    100 and the last two show accuracy.

used three tasks: the sick similarity task, the sick entailment task, and the stanford sentiment
treebank (sst) binary classi   cation task (socher et al., 2013). for the sick similarity task, we
minimize the objective function14 from tai et al. (2015). given a score for a sentence pair in the
range [1, k], where k is an integer, with sentence representations hl and hr, and model parameters
  , they    rst compute:

h   = hl     hr, h+ = |hl     hr|,

hs =   (cid:16)w (  )h   + w (+)h+ + b(h)(cid:17) ,
  p   = softmax(cid:16)w (p)hs + b(p)(cid:17) ,

  y = rt   p  ,

where rt = [1 2 . . . k]. they then de   ne a sparse target distribution p that satis   es y = rt p:

y        y   ,
   y        y + 1,
0

i =    y    + 1
i =    y   
otherwise

pi =      
   

for 1     i     k. then they use the following loss, the regularized kl-divergence between p and   p  :

j(  ) =

1
m

m

xk=1

kl(cid:16)p(k) (cid:13)(cid:13)(cid:13)

  p(k)

   (cid:17),

(2)

where m is the number of training pairs and where we always use l2 id173 on all composi-
tional parameters15 but omit these terms for clarity.
we use nearly the same model for the entailment task, with the only differences being that the    nal
softmax layer has three outputs and the cost function is the negative log-likelihood of the class
labels. for sentiment, since it is a binary sentence classi   cation task, we    rst encoded the sentence
and then used a fully-connected layer with a sigmoid activation followed by a softmax layer with
two outputs. we used negative log-likelihood of the class labels as the cost function. all models use
l2 id173 on all parameters, except for the id27s, which are regularized back to
their initial values with an l2 penalty.
we    rst investigated how these models performed in the standard setting, without using any models
trained using ppdb data. we tuned hyperparameters on the development set of each dataset16
as well as on two optimization schemes: adagrad with learning rate of 0.05 and adam with a
learning rate of 0.001. we trained the models for 10 epochs and initialized the id27s
with paragram-sl999 embeddings.

14this objective function has been shown to perform very strongly on text similarity tasks, signi   cantly

better than squared or absolute error.

15id27s are regularized toward their initial state.
16for all models, we tuned batch-size over {25, 50, 100}, output dimension over {50, 150, 300},   c over
{10   3, 10   4, 10   5, 10   6},   s =   c, and   w over {10   3, 10   4, 10   5, 10   6, 10   7, 10   8} as well as the
option of not updating the embeddings for all models except the word averaging model. we again    x the output
dimensionalities of all models which require this speci   cation, to the dimensionality of our id27s
(300). additionally, for the classic id56, we further tuned whether to use tanh or recti   ed linear unit activation
functions; for the dans we tuned over activation function (tanh or recti   ed linear unit) and the number of
layers (either 1 or 2).

9

published as a conference paper at iclr 2016

the results are shown in table 5. we    nd that using word averaging as the compositional archi-
tecture outperforms the other architectures for similarity and entailment. however, for sentiment
classi   cation, the lstm is much stronger than the averaging models. this suggests that the supe-
riority of a compositional architecture can vary widely depending on the evaluation, and motivates
future work to compare these architectures on additional tasks.

these results are very competitive with the state of the art on these tasks. recent strong results
on the sick similarity task include 86.86 using a convolutional neural network (he et al., 2015)
and 86.76 using a tree-lstm (tai et al., 2015). for entailment, the best result we are aware of is
85.1 (beltagy et al., 2015). on sentiment, the best previous result is 88.1 (kim, 2014), which our
lstm surprisingly outperforms by a signi   cant margin. we note that these experiments simply
compare compositional architectures using only the provided training data for each task, tuning on
the respective development sets. we did not use any ppdb data for these results, other than that used
to train the initial paragram-sl999 embeddings. our results appear to show that standard neural
architectures can perform surprisingly well given strong id27s and thorough tuning over
the hyperparameter space.

4.4.1 id173 and initialization to improve textual similarity models

in this setting, we initialize each respective model to the parameters learned from ppdb (calling
them universal parameters) and augment eq. 2 with three separate id173 terms with the
following weights:   s which regularizes the classi   cation parameters (the two layers used in the
classi   cation step after obtaining representations),   w for regularizing the word parameters toward
the learned ww from ppdb, and   c for regularizing the compositional parameters (for all models
except for the word averaging model) back to their initial values.17 in all cases, we regularize to the
universal parameters using l2 id173.
the results are shown in the last column of table 5, and we only show results for the best per-
forming models on each task (word averaging for similarity/entailment, lstm with output gate
for sentiment).
interestingly, it seems that regularizing to our universal parameters signi   cantly
improves results for the similarity and entailment tasks which are competitive or better than the
state-of-the-art, but harms the lstm   s performance on the sentiment classi   cation task.

4.4.2 representations as features

task

similarity (sick)
entailment (sick)
binary sentiment (sst)

paragram-phrase
300
2400
84.94
82.15
83.1
80.2
79.7
79.4

1200
82.85
80.1
78.8

skip-thought

uni-skip
84.77

bi-skip
84.05

-
-

-
-

table 6: results from supervised training on similarity, entailment, and sentiment tasks, except that
we keep the sentence representations    xed to our paragram-phrase model. the    rst row shows
pearson   s r    100 and the last two show accuracy, with boldface showing the highest score in each
row.

we also investigate how our paragram-phrase embeddings perform as features for supervised
tasks. we use a similar set-up as in kiros et al. (2015) and encode the sentences by averaging our
paragram-phrase embeddings and then just learn the classi   cation parameters without updating
the embeddings. to provide a more apt comparison to skip-thought vectors, we also learned a linear
projection matrix to increase dimensionality of our paragram-phrase embeddings. we chose
1200 and 2400 dimensions in order to both see the dependence of dimension on performance, and
so that they can be compared fairly with skip-thought vectors. note that 2400 dimensions is the
same dimensionality as the uni-skip and bi-skip models in kiros et al. (2015).

the 300 dimension case corresponds to the paragram-phrase embeddings from table 2. we
tuned our higher dimensional models on ppdb as described previously in section 4.2.2 before train-

17we tuned   s over {10   3, 10   4, 10   5, 10   6},   c over {10   2, 10   3, 10   4, 10   5, 10   6}, and   w over

{10   3, 10   4, 10   5, 10   6, 10   7, 10   8}. all other hyperparameters were tuned as previously described.

10

published as a conference paper at iclr 2016

ing on ppdb xl.18 then we trained the same models for the similarity, entailment, and sentiment
tasks as described in section 4.4 for 20 epochs. we again tuned   s over {10   3, 10   4, 10   5, 10   6}
and tuned over the two optimization schemes of adagrad with learning rate of 0.05 and adam with
a learning rate of 0.001. note that we are not updating the id27s or the projection matrix
during training.

the results are shown in table 6. the similarity and entailment tasks show clear improvements as
we project the embeddings into the 2400 dimensional space. in fact, our results outperform both
types of skip-thought embeddings on the single task that we overlap. however, the sentiment task
does not bene   t from higher dimensional representations, which is consistent with our regulariza-
tion experiments in which sentiment also did not show improvement. therefore, it seems that our
models learned from ppdb are more effective for similarity tasks than classi   cation tasks, but this
hypothesis requires further investigation.

5 discussion

it is interesting that the lstm, with or without output gates, is outperformed by much simpler
models on the similarity and entailment tasks studied in this paper. we now consider possible
explanations for this trend.

the    rst hypothesis we test is based on length. since ppdb contains short text snippets of a few
words, the lstm may not know how to handle the longer sentences that occur in our evaluation
tasks. if this is true, the lstm would perform much better on short text snippets and its performance
would degrade as their length increases. to test this hypothesis, we took all 12,108 pairs from the 20
semeval sts tasks and binned them by length.19 we then computed the pearson   s r for each bin.
the results are shown in table 7 and show that while the lstm models do perform better on the
shortest text pairs, they are still outperformed, at all lengths, by the paragram-phrase model.20

max
length

    4
5
6
7
8
9

    10

paragram-

phrase

72.7
74.0
70.5
73.7
75.5
73.0
72.6

lstm
(no o.g.)

63.4
54.5
52.6
56.9
60.2
58.0
55.6

lstm
(o.g.)
58.8
48.4
48.2
50.6
52.4
48.8
53.8

paragram-

sl999
66.3
65.0
50.1
56.4
60.1
58.8
58.4

table 7: performance (pearson   s r    100) as a function of the maximum number of tokens in the
sentence pairs over all 20 semeval sts datasets.

we next consider whether the lstm has worse generalization due to over   tting on the training
data. to test this, we analyzed how the models performed on the training data (ppdb xl) by
computing the average difference between the cosine similarity of the gold phrase pairs and the
negative examples.21 we found that all models had very similar scores: 0.7535, 0.7572, 0.7565, and
0.7463 for paragram-phrase, projection, lstm (o.g.), and lstm (no o.g.). this, along with
the similar performance of the models on the ppdb tasks in table 3, suggests that over   tting is not
the cause of the worse performance of the lstm model.

lastly, we consider whether the lstm   s weak performance was a result of insuf   cient tuning or
optimization. we    rst note that we actually ran more hyperparameter tuning experiments for the

18note that we    xed batch-size to 100,    to 0.4, and used max sampling as these were the optimal parameters
for the paragram-phrase embeddings. we tuned the other hyperparameters as described in section 4.2.2
with the exception of   c which was tuned over {10   4, 10   5, 10   6, 10   7, 10   8}.

19for each pair, we computed the number of tokens in each of the two pieces of text, took the max, and then

binned based on this value.

20note that for the analysis in sections 5 and 6, the models used were selected from earlier experiments.

they are not the same as those used to obtain the results in table 2.

21more precisely, for each gold pair hg1, g2i, and ni, the respective negative example of each gi, we com-

puted 2    cos(g1, g2)     cos(n1, g1)     cos(n2, g2) and averaged this value over all pairs.

11

published as a conference paper at iclr 2016

lstm models than either the paragram-phrase or projection models, since we tuned the deci-
sion to use an output gate. secondly, we note that tai et al. (2015) had a similar lstm result on
the sick dataset (pearson   s r of 85.28 to our 85.45) to show that our lstm implementation/tuning
procedure is able to match or exceed performance of another published lstm result. thirdly, the
similar performance across models on the ppdb tasks (table 3) suggests that no model had a large
advantage during tuning; all found hyperparameters that comfortably beat the paragram-sl999
addition baseline. finally, we point out that we tuned over learning rate and optimization strategy,
as well as experimented with clipping gradients, in order to rule out optimization issues.

5.1 under-trained embeddings

one limitation of our new paragram-phrase vectors is that many of our embeddings are under-
trained. the number of unique tokens occurring in our training data, ppdb xl, is 37,366. however,
the number of tokens appearing more than 100 times is just 7,113. thus, one clear source of im-
provement for our model would be to address under-trained embeddings for tokens appearing in our
test data.

in order to gauge the effect under-trained embeddings and unknown words have on our model, we
calculated the fraction of words in each of our 22 semeval datasets that do not occur at least 100
times in ppdb xl along with our performance deviation from the 75th percentile of each dataset.
we found that this fraction had a spearman   s    of -45.1 with the deviation from the 75th percentile
indicating that there is a signi   cant negative correlation between the fraction of oov words and
performance on these sts tasks.

5.2 using more ppdb

5.2.1 performance versus amount of training data

models in related work such as kiros et al. (2015) and li et al. (2015a) require signi   cant training
time on gpus, on the order of multiple weeks. moreover, dependence of model performance upon
training data size is unclear. to investigate this dependence for our paragram-phrase model, we
trained on different amounts of data and plotted the performance. the results are shown in figure 1.
we start with ppdb xl which has 3,033,753 unique phrase pairs and then divide by two until there
are fewer than 10 phrase pairs.22 for each data point (each division by two), we trained a model
with that number of phrase pairs for 10 epochs. we use the average pearson correlation for all 22
datasets in table 2 as the dependent variable in our plot.

we experimented with two different ways of selecting training data. the    rst (   ordered   ) retains
the order of the phrase pairs in ppdb, which ensures the smaller datasets contain higher con   dence
phrase pairs. the second (   random   ) randomly permutes ppdb xl before constructing the smaller
datasets. in both methods, each larger dataset contains the previous one plus as many new phase
pairs.

we make three observations about the plot in figure 1. the    rst is that performance continually
increases as more training data is added. this is encouraging as our embeddings can continually
improve with more data. secondly, we note the sizable improvement (4 points) over the paragram-
sl999 baseline by training on just 92 phrase pairs from ppdb. finally, we note the difference
between randomly permuting the training data and using the order from ppdb (which re   ects the
con   dence that the phrases in each pair possess the paraphrase relationship). performance of the
randomly permuted data is usually slightly better than that of the ordered data, until the performance
gap vanishes once half of ppdb xl is used. we suspect this behavior is due to the safe phrase pairs
that occur in the beginning of ppdb. these high-con   dence phrase pairs usually have only slight
differences and therefore are not as useful for training our model.

6 qualitative analysis

to explore other differences between our paragram-phrase vectors and the paragram-sl999
vectors that were used for initialization, we inspected lists of nearest neighbors in each vector space.

22the smallest dataset contained 5 pairs.

12

published as a conference paper at iclr 2016

performance vs. training data size

0.7

0.65

0.6

0.55

r

s
   
n
o
s
r
a
e
p
e
g
a
r
e
v
a

random
ordered

paragram-sl999

glove

0.5

0

1

2

3

4

5

6

7

log10 of training data size

figure 1: performance of the paragram-phrase embeddings as measured by the average pear-
son   s r on 22 textual similarity datasets versus the amount of training data from ppdb on a log scale.
each datapoint contains twice as much training data as the previous one. random and ordered refer
to whether we shuf   ed the xl paraphrase pairs from ppdb or kept them in order. we also show
baselines of averaging paragram-sl999 and glove embeddings.

word
unlike
2
ladies
lookin
disagree

paragram-phrase nearest neighbors
contrary, contrast, opposite, versa, conversely, opposed, contradiction
2.0, two, both, ii, 2nd, couple, 02
girls, daughters, honorable, females, girl, female, dear
staring, looking, watching, look, searching, iooking, seeking
agree, concur, agreeing, differ, accept

paragram-sl999 nearest neighbors
than, although, whilst, though, albeit, kinda, alike
2.0, 3, 1, b, ii, two, 2nd
gentlemen, colleague, fellow, girls, mr, madam, dear
doin, goin, talkin, sayin, comin, outta, somethin
disagreement, differ, dispute, difference, disagreements

table 8: nearest neighbors of paragram-phrase and paragram-sl999 id27s
sorted by cosine similarity.

when obtaining nearest neighbors, we restricted our search to the 10,000 most common tokens
in ppdb xl to ensure that the paragram-phrase vectors were not too under-trained. some
informative neighbors are shown in table 8. in the    rst four rows, we see that the paragram-
phrase embeddings have neighbors with a strong id141 relationship. they tend to avoid
having neighbors that are antonyms or co-hyponyms such as unlike and alike or 2 and 3 which are
an issue for the paragram-sl999 embeddings. in contrast to the    rst four rows, the last row
shows a problematic effect of our bag-of-words composition function: agree is the nearest neighbor
of disagree. the reason for this is that there are numerous pairs in ppdb xl such as i disagree
and i do not agree that encourage disagree and agree to have high cosine similarity. a model that
takes context into account could resolve this issue. the dif   culty would be    nding a model that does
so while still generalizing well, as we found that our paragram-phrase embeddings generalize
better than learning a weight matrix or using a recurrent neural network. we leave this for future
work.

when we take a closer look at our paragram-phrase embeddings, we    nd that information-
bearing content words, such as poverty, kidding, humanitarian, 18, and july have the largest l2
norms, while words such as of, it, to, hereby and the have the smallest. pham et al. (2015) noted this
same phenomenon in their closely-related compositional model. interestingly, we found that this
weighting explains much of the success of our model. in order to quantify exactly how much, we
calculated a weight for each token in our working vocabulary23 simply by summing up the absolute

23this corresponds to the 42,091 tokens that appear in the intersection of our paragram-sl999 vocabulary,

the test sets of all sts tasks in our evaluation, and ppdb xl plus an unknown word token.

13

published as a conference paper at iclr 2016

value of all components of its paragram-phrase vector. then we multiplied each weight by
its corresponding paragram-sl999 word vector. we computed the average pearson   s r over all
22 datasets in table 2. the paragram-sl999 vectors have an average correlation of 54.94, the
paragram-phrase vectors have 66.83, and the scaled paragram-sl999 vectors, where each
is multiplied by its computed weight, have an average pearson   s r of 62.64. therefore, it can be
surmised that at least 64.76% of the improvement over the initial paragram-sl999 vectors is due
to weighting tokens by their importance.24
we also investigated the connection between these multiplicative weights and word frequency. to
do so, we calculated the frequency of all tokens in ppdb xl.25 we then normalized these by the
total number of tokens in ppdb xl and used the reciprocal of these scores as the multiplicative
weights. thus less frequent words have more weight than more frequent words. with this baseline
weighting method, the average pearson   s r is 45.52, indicating that the weights we obtain for these
words are more sophisticated than mere word frequency. these weights are potentially useful for
other applications that can bene   t from modeling word importance, such as information retrieval.

7 conclusion

we introduced an approach to create universal sentence embeddings and propose our model as the
new baseline for embedding sentences, as it is simple, ef   cient, and performs strongly across a
broad range of tasks and domains. moreover, our representations do not require the use of any neu-
ral network architecture. the embeddings can be simply averaged for a given sentence in an nlp
application to create its sentence embedding. we also    nd that our representations can improve gen-
eral text similarity and entailment models when used as a prior and can achieve strong performance
even when used as    xed representations in a classi   er. future work will focus on improving our
embeddings by effectively handling undertrained words as well as by exploring new models that
generalize even better to the large suite of text similarity tasks used in our experiments.

acknowledgments

we would like to thank yoon kim, the anonymous reviewers, and the area chair for their valu-
able comments. we would also like to thank the developers of theano (bergstra et al., 2010;
bastien et al., 2012) and thank nvidia corporation for donating gpus used in this research.

references

agirre, eneko, diab, mona, cer, daniel, and gonzalez-agirre, aitor. semeval-2012 task 6: a
pilot on semantic textual similarity. in proceedings of the first joint conference on lexical and
computational semantics-volume 1: proceedings of the main conference and the shared task, and
volume 2: proceedings of the sixth international workshop on semantic evaluation. association
for computational linguistics, 2012.

agirre, eneko, cer, daniel, diab, mona, gonzalez-agirre, aitor, and guo, weiwei. *sem 2013
shared task: semantic textual similarity. in second joint conference on lexical and computa-
tional semantics (*sem), volume 1: proceedings of the main conference and the shared task:
semantic textual similarity, 2013.

agirre, eneko, banea, carmen, cardie, claire, cer, daniel, diab, mona, gonzalez-agirre, aitor,
guo, weiwei, mihalcea, rada, rigau, german, and wiebe, janyce. semeval-2014 task 10:
multilingual semantic textual similarity. in proceedings of the 8th international workshop on
semantic evaluation (semeval 2014), 2014.

24we also trained a model in which we only a learn a single multiplicative parameter for each word in
our vocabulary, keeping the id27s    xed to the paragram-sl999 embeddings. we trained for
10 epochs on all phrase pairs in ppdb xl. the resulting average pearson   s r, after tuning on the pavlick et
al. ppdb task, was 62.06, which is slightly lower than using the absolute value of each paragram-phrase
vector as its multiplicative weight.

25tokens that did not appear in ppdb xl were assigned a frequency of 1.

14

published as a conference paper at iclr 2016

agirre, eneko, banea, carmen, cardie, claire, cer, daniel, diab, mona, gonzalez-agirre, aitor,
guo, weiwei, lopez-gazpio, inigo, maritxalar, montse, mihalcea, rada, rigau, german, uria,
larraitz, and wiebe, janyce. semeval-2015 task 2: semantic textual similarity, english, span-
ish and pilot on interpretability. in proceedings of the 9th international workshop on semantic
evaluation (semeval 2015), 2015.

bansal, mohit, gimpel, kevin, and livescu, karen. tailoring continuous word representations for
id33. in proceedings of the annual meeting of the association for computational
linguistics, 2014.

baroni, marco, bernardi, raffaela, and zamparelli, roberto. frege in space: a program of compo-

sitional id65. linguistic issues in language technology, 9, 2014.

bastien, fr  ed  eric, lamblin, pascal, pascanu, razvan, bergstra, james, goodfellow, ian j., bergeron,
arnaud, bouchard, nicolas, and bengio, yoshua. theano: new features and speed improvements,
2012.

belinkov, yonatan and glass, james. arabic diacritization with recurrent neural networks. in pro-
ceedings of the 2015 conference on empirical methods in natural language processing, 2015.

beltagy, islam, roller, stephen, cheng, pengxiang, erk, katrin, and mooney, raymond j.
arxiv preprint

representing meaning with a combination of logical form and vectors.
arxiv:1505.06816, 2015.

bergstra, james, breuleux, olivier, bastien, fr  ed  eric, lamblin, pascal, pascanu, razvan, des-
jardins, guillaume, turian, joseph, warde-farley, david, and bengio, yoshua. theano: a cpu
and gpu math expression compiler. in proceedings of the python for scienti   c computing con-
ference (scipy), june 2010.

bird, steven, klein, ewan, and loper, edward. natural language processing with python. o   reilly

media, inc., 2009.

blacoe, william and lapata, mirella. a comparison of vector-based representations for semantic
in proceedings of the 2012 joint conference on empirical methods in natural

composition.
language processing and computational natural language learning, 2012.

bordes, antoine, chopra, sumit, and weston, jason. id53 with subgraph embed-
dings. in proceedings of the 2014 conference on empirical methods in natural language pro-
cessing (emnlp), 2014a.

bordes, antoine, weston, jason, and usunier, nicolas. open id53 with weakly super-
vised embedding models. in machine learning and knowledge discovery in databases. springer,
2014b.

chen, xinchi, qiu, xipeng, zhu, chenxi, liu, pengfei, and huang, xuanjing. long short-term
memory neural networks for chinese id40. in proceedings of the 2015 conference
on empirical methods in natural language processing, 2015a.

chen, xinchi, qiu, xipeng, zhu, chenxi, wu, shiyu, and huang, xuanjing. sentence modeling with
gated id56. in proceedings of the 2015 conference on empirical methods in
natural language processing, 2015b.

collobert, ronan, weston, jason, bottou, l  eon, karlen, michael, kavukcuoglu, koray, and kuksa,

pavel. natural language processing (almost) from scratch. j. mach. learn. res., 12, 2011.

duchi, john, hazan, elad, and singer, yoram. adaptive subgradient methods for online learning

and stochastic optimization. j. mach. learn. res., 12, 2011.

dzikovska, myroslava o, moore, johanna d, steinhauser, natalie, campbell, gwendolyn, farrow,
elaine, and callaway, charles b. beetle ii: a system for tutoring and computational linguistics
experimentation. in proceedings of the acl 2010 system demonstrations, 2010.

15

published as a conference paper at iclr 2016

faruqui, manaal and dyer, chris. improving vector space word representations using multilingual
correlation. in proceedings of the 14th conference of the european chapter of the association
for computational linguistics, 2014.

filippova, katja, alfonseca, enrique, colmenares, carlos a., kaiser, lukasz, and vinyals, oriol.
sentence compression by deletion with lstms. in proceedings of the 2015 conference on em-
pirical methods in natural language processing, 2015.

finkelstein, lev, gabrilovich, evgeniy, matias, yossi, rivlin, ehud, solan, zach, wolfman, gadi,
and ruppin, eytan. placing search in context: the concept revisited. in proceedings of the 10th
international conference on world wide web. acm, 2001.

ganitkevitch, juri, durme, benjamin van, and callison-burch, chris. ppdb: the paraphrase

database. in hlt-naacl. the association for computational linguistics, 2013.

gers, felix a, schraudolph, nicol n, and schmidhuber, j  urgen. learning precise timing with lstm

recurrent networks. the journal of machine learning research, 3, 2003.

graves, alex, liwicki, marcus, bunke, horst, schmidhuber, j  urgen, and fern  andez, santiago. un-
constrained on-line handwriting recognition with recurrent neural networks. in advances in neu-
ral information processing systems 20. 2008.

graves, alex, mohamed, abdel-rahman, and hinton, geoffrey. id103 with deep re-
current neural networks. in 2013 ieee international conference on acoustics, speech and signal
processing (icassp), 2013.

greff, klaus, srivastava, rupesh kumar, koutn    k, jan, steunebrink, bas r, and schmidhuber,

j  urgen. lstm: a search space odyssey. arxiv preprint arxiv:1503.04069, 2015.

he, hua, gimpel, kevin, and lin, jimmy. multi-perspective sentence similarity modeling with
convolutional neural networks. in proceedings of the 2015 conference on empirical methods in
natural language processing, 2015.

hermann, karl moritz and blunsom, phil. multilingual models for compositional distributed seman-
tics. in proceedings of the 52nd annual meeting of the association for computational linguistics
(volume 1: long papers), 2014.

hermann, karl moritz, ko  cisk  y, tom  a  s, grefenstette, edward, espeholt, lasse, kay, will, suley-
man, mustafa, and blunsom, phil. teaching machines to read and comprehend. in advances in
neural information processing systems, 2015.

hill, felix, reichart, roi, and korhonen, anna. siid113x-999: evaluating semantic models with

(genuine) similarity estimation. computational linguistics, 41(4), 2015.

hochreiter, sepp and schmidhuber, j  urgen. long short-term memory. neural computation, 9(8),

1997.

hu, baotian, lu, zhengdong, li, hang, and chen, qingcai. convolutional neural network archi-
tectures for matching natural language sentences. in advances in neural information processing
systems, 2014.

huang, po-sen, he, xiaodong, gao, jianfeng, deng, li, acero, alex, and heck, larry. learning
deep structured semantic models for web search using clickthrough data. in proceedings of the
22nd acm international conference on conference on information & knowledge management,
2013.

  irsoy, ozan and cardie, claire. deep id56s for compositionality in language.

in advances in neural information processing systems 27. 2014.

iyyer, mohit, manjunatha, varun, boyd-graber, jordan, and daum  e iii, hal. deep unordered com-
position rivals syntactic methods for text classi   cation. in proceedings of the 53rd annual meet-
ing of the association for computational linguistics and the 7th international joint conference
on natural language processing (volume 1: long papers), 2015.

16

published as a conference paper at iclr 2016

kalchbrenner, nal, grefenstette, edward, and blunsom, phil. a convolutional neural network for
modelling sentences. in proceedings of the 52nd annual meeting of the association for compu-
tational linguistics (volume 1: long papers), 2014.

kim, yoon. convolutional neural networks for sentence classi   cation. in proceedings of the 2014

conference on empirical methods in natural language processing (emnlp), 2014.

kingma, diederik and ba, jimmy. adam: a method for stochastic optimization. arxiv preprint

arxiv:1412.6980, 2014.

kiros, ryan, zhu, yukun, salakhutdinov, ruslan, zemel, richard s, torralba, antonio, urtasun,

raquel, and fidler, sanja. skip-thought vectors. arxiv preprint arxiv:1506.06726, 2015.

le, quoc v and mikolov, tomas. distributed representations of sentences and documents. arxiv

preprint arxiv:1405.4053, 2014.

li, jiwei, luong, minh-thang, and jurafsky, dan. a hierarchical neural autoencoder for paragraphs

and documents. arxiv preprint arxiv:1506.01057, 2015a.

li, jiwei, luong, thang, and jurafsky, dan. a hierarchical neural autoencoder for paragraphs and
documents. in proceedings of the 53rd annual meeting of the association for computational
linguistics and the 7th international joint conference on natural language processing (volume
1: long papers), 2015b.

ling, wang, dyer, chris, black, alan w, trancoso, isabel, fermandez, ramon, amir, silvio,
marujo, luis, and luis, tiago. finding function in form: compositional character models for
open vocabulary word representation. in proceedings of the 2015 conference on empirical meth-
ods in natural language processing, 2015.

liu, pengfei, qiu, xipeng, chen, xinchi, wu, shiyu, and huang, xuanjing. multi-timescale long
short-term memory neural network for modelling sentences and documents. in proceedings of
the 2015 conference on empirical methods in natural language processing, 2015.

lu, ang, wang, weiran, bansal, mohit, gimpel, kevin, and livescu, karen. deep multilingual
correlation for improved id27s. in proceedings of the 2015 conference of the north
american chapter of the association for computational linguistics: human language technolo-
gies, 2015.

manning, christopher d., surdeanu, mihai, bauer, john, finkel, jenny, bethard, steven j., and
mcclosky, david. the stanford corenlp natural language processing toolkit. in proceedings of
52nd annual meeting of the association for computational linguistics: system demonstrations,
2014.

marelli, marco, bentivogli, luisa, baroni, marco, bernardi, raffaella, menini, stefano, and zam-
parelli, roberto. semeval-2014 task 1: evaluation of compositional distributional semantic mod-
els on full sentences through semantic relatedness and id123. in proceedings of the
8th international workshop on semantic evaluation (semeval 2014), 2014.

mikolov, tomas, sutskever, ilya, chen, kai, corrado, greg s, and dean, jeff. distributed repre-
sentations of words and phrases and their compositionality. in advances in neural information
processing systems, 2013.

mitchell, jeff and lapata, mirella. vector-based models of semantic composition. in proceedings

of the 46th annual meeting of the association for computational linguistics, 2008.

mitchell, jeff and lapata, mirella. composition in distributional models of semantics. cognitive

science, 34(8), 2010.

paperno, denis, pham, nghia the, and baroni, marco. a practical and linguistically-motivated
approach to compositional id65. in proceedings of the 52nd annual meeting
of the association for computational linguistics (volume 1: long papers), 2014.

pascanu, razvan, mikolov, tomas, and bengio, yoshua. on the dif   culty of training recurrent

neural networks. arxiv preprint arxiv:1211.5063, 2012.

17

published as a conference paper at iclr 2016

pavlick, ellie, rastogi, pushpendre, ganitkevich, juri, durme, benjamin van, and callison-burch,
chris. ppdb 2.0: better paraphrase ranking,    ne-grained entailment relations, id27s,
and style classi   cation. in proceedings of the annual meeting of the association for computa-
tional linguistics, 2015.

pennington, jeffrey, socher, richard, and manning, christopher d. glove: global vectors for word
representation. proceedings of empirical methods in natural language processing (emnlp
2014), 2014.

pham, nghia the, kruszewski, germ  an, lazaridou, angeliki, and baroni, marco. jointly optimiz-
ing word representations for lexical and sentential tasks with the c-phrase model. in proceedings
of the 53rd annual meeting of the association for computational linguistics and the 7th interna-
tional joint conference on natural language processing (volume 1: long papers), 2015.

polajnar, tamara, rimell, laura, and clark, stephen. an exploration of discourse-based sentence
spaces for compositional id65. in proceedings of the first workshop on link-
ing computational models of lexical, sentential and discourse-level semantics, 2015.

socher, richard, huang, eric h, pennin, jeffrey, manning, christopher d, and ng, andrew y.
dynamic pooling and unfolding recursive autoencoders for paraphrase detection. in advances in
neural information processing systems, 2011.

socher, richard, huval, brody, manning, christopher d., and ng, andrew y. semantic compo-
sitionality through recursive matrix-vector spaces. in proceedings of the 2012 joint conference
on empirical methods in natural language processing and computational natural language
learning, 2012.

socher, richard, perelygin, alex, wu, jean, chuang, jason, manning, christopher d., ng, andrew,
and potts, christopher. recursive deep models for semantic compositionality over a sentiment
treebank. in proceedings of the 2013 conference on empirical methods in natural language
processing, 2013.

socher, richard, karpathy, andrej, le, quoc v., manning, christopher d., and ng, andrew y.
grounded id152 for    nding and describing images with sentences. tacl, 2,
2014.

sutskever, ilya, vinyals, oriol, and le, quoc vv. sequence to sequence learning with neural net-

works. in advances in neural information processing systems, 2014.

tai, kai sheng, socher, richard, and manning, christopher d. improved semantic representations
from tree-structured id137. arxiv preprint arxiv:1503.00075, 2015.

tian, ran, okazaki, naoaki, and inui, kentaro. the mechanism of additive composition. arxiv

preprint arxiv:1511.08407, 2015.

turian, joseph, ratinov, lev-arie, and bengio, yoshua. word representations: a simple and gen-
in proceedings of the 48th annual meeting of the

eral method for semi-supervised learning.
association for computational linguistics, 2010.

vinyals, oriol, kaiser, lukasz, koo, terry, petrov, slav, sutskever, ilya, and hinton, geoffrey.

grammar as a foreign language. arxiv preprint arxiv:1412.7449, 2014.

wang, di and nyberg, eric. a long short-term memory model for answer sentence selection in
id53. in proceedings of the 53rd annual meeting of the association for compu-
tational linguistics and the 7th international joint conference on natural language processing
(volume 2: short papers), 2015.

wen, tsung-hsien, gasic, milica, mrk  si  c, nikola, su, pei-hao, vandyke, david, and young, steve.
semantically conditioned lstm-based id86 for spoken dialogue systems.
in proceedings of the 2015 conference on empirical methods in natural language processing,
2015.

weston, jason, bengio, samy, and usunier, nicolas. large scale image annotation: learning to rank

with joint word-image embeddings. machine learning, 81(1), 2010.

18

published as a conference paper at iclr 2016

wieting, john, bansal, mohit, gimpel, kevin, livescu, karen, and roth, dan. from paraphrase
database to compositional paraphrase model and back. transactions of the association for com-
putational linguistics, 3, 2015.

xu, kelvin, ba, jimmy, kiros, ryan, cho, kyunghyun, courville, aaron c., salakhutdinov, ruslan,
zemel, richard s., and bengio, yoshua. show, attend and tell: neural image id134
with visual attention. in proceedings of the 32nd international conference on machine learning,
icml, 2015a.

xu, wei, callison-burch, chris, and dolan, william b. semeval-2015 task 1: paraphrase and se-
mantic similarity in twitter (pit). in proceedings of the 9th international workshop on semantic
evaluation (semeval), 2015b.

xu, yan, mou, lili, li, ge, chen, yunchuan, peng, hao, and jin, zhi. classifying relations via
long short term memory networks along shortest dependency paths. in proceedings of the 2015
conference on empirical methods in natural language processing, 2015c.

yih, wen-tau, toutanova, kristina, platt, john c., and meek, christopher. learning discriminative
projections for text similarity measures. in proceedings of the fifteenth conference on computa-
tional natural language learning, 2011.

yin, wenpeng and sch  utze, hinrich. convolutional neural network for paraphrase identi   cation.
in proceedings of the 2015 conference of the north american chapter of the association for
computational linguistics: human language technologies, 2015.

yu, mo and dredze, mark. learning composition models for phrase embeddings. transactions of

the association for computational linguistics, 3, 2015.

zhao, han, lu, zhengdong, and poupart, pascal. self-adaptive hierarchical sentence model.

proceedings of ijcai, 2015.

in

19

