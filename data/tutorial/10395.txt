classifying relations via long short term memory networks

along shortest dependency paths

yan xu,    lili mou,    ge li,       yunchuan chen,    hao peng,    zhi jin      

   software institute, peking university, 100871, p. r. china

{xuyan14,lige,zhijin}@sei.pku.edu.cn,{doublepower.mou,penghao.pku}@gmail.com
   university of chinese academy of sciences, chenyunchuan11@mails.ucas.ac.cn

5
1
0
2

 

g
u
a
5
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
0
2
7
3
0

.

8
0
5
1
:
v
i
x
r
a

abstract

relation classi   cation is an important re-
search arena in the    eld of natural lan-
guage processing (nlp). in this paper, we
present sdp-lstm, a novel neural net-
work to classify the relation of two enti-
ties in a sentence. our neural architecture
leverages the shortest dependency path
(sdp) between two entities; multichan-
nel recurrent neural networks, with long
short term memory (lstm) units, pick
up heterogeneous information along the
sdp. our proposed model has several dis-
tinct features: (1) the shortest dependency
paths retain most relevant information (to
relation classi   cation), while eliminating
irrelevant words in the sentence. (2) the
multichannel id137 allow ef-
fective information integration from het-
erogeneous sources over the dependency
paths. (3) a customized dropout strategy
regularizes the neural network to allevi-
ate over   tting. we test our model on the
semeval 2010 relation classi   cation task,
and achieve an f1-score of 83.7%, higher
than competing methods in the literature.

introduction

1
relation classi   cation is an important nlp task.
it plays a key role in various scenarios, e.g., in-
formation extraction (wu and weld, 2010), ques-
tion answering (yao and van durme, 2014), med-
ical informatics (wang and fan, 2014), ontol-
ogy learning (xu et al., 2014), etc. the aim
of relation classi   cation is to categorize into pre-
de   ned classes the relations between pairs of
marked entities in given texts. for instance, in
the sentence    a trillion gallons of [water]e1 have
been poured into an empty [region]e2 of outer

   corresponding authors.

space,    the entities water and region are of rela-
tion entity-destination(e1, e2).

traditional relation classi   cation approaches
rely largely on feature representation (kambhatla,
2004), or kernel design (zelenko et al., 2003;
bunescu and mooney, 2005). the former method
usually incorporates a large set of features; it is
dif   cult to improve the model performance if the
feature set is not very well chosen. the latter ap-
proach, on the other hand, depends largely on the
designed kernel, which summarizes all data infor-
mation. deep neural networks, emerging recently,
provide a way of highly automatic feature learn-
ing (bengio et al., 2013), and have exhibited con-
siderable potential (zeng et al., 2014; santos et
al., 2015). however, human engineering   that is,
incorporating human knowledge to the network   s
architecture   is still important and bene   cial.

this paper proposes a new neural network,
sdp-lstm, for relation classi   cation. our model
utilizes the shortest dependency path (sdp) be-
tween two entities in a sentence; we also design a
long short term memory (lstm)-based recurrent
neural network for information processing. the
neural architecture is mainly inspired by the fol-
lowing observations.

    shortest dependency paths are informative
(fundel et al., 2007; chen et al., 2014). to
determine the two entities    relation, we    nd it
mostly suf   cient to use only the words along
the sdp: they concentrate on most relevant
information while diminishing less relevant
noise. figure 1 depicts the dependency parse
tree of the aforementioned sentence. words
along the sdp form a trimmed phrase (gal-
lons of water poured into region) of the orig-
inal sentence, which conveys much informa-
tion about the target relation. other words,
such as a, trillion, outer space, are less infor-
mative and may bring noise if not dealt with
properly.

    direction matters. dependency trees are a
kind of directed graph. the dependency re-
lation between into and region is prep; such
relation hardly makes any sense if the di-
rected edge is reversed. moreover, the enti-
ties    relation distinguishes its directionality,
that is, r(a, b) differs from r(b, a), for a same
given relation r and two entities a, b. there-
fore, we think it necessary to let the neu-
ral model process information in a direction-
sensitive manner. out of this consideration,
we separate an sdp into two sub-paths, each
from an entity to the common ancestor node.
the extracted features along the two sub-
paths are concatenated to make    nal classi-
   cation.

    linguistic information helps.

for exam-
ple, with prior knowledge of hyponymy, we
know    water is a kind of substance.    this
is a hint that the entities, water and region,
are more of entity-destination rela-
tion than, say, communication-topic.
to gather heterogeneous information along
sdp, we design a multichannel recurrent neu-
ral network.
it makes use of information
from various sources, including words them-
selves, pos tags, id138 hypernyms, and
the grammatical relations between governing
words and their children.

for effective information propagation and inte-
gration, our model leverages lstm units during
recurrent propagation. we also customize a new
dropout strategy for our sdp-lstm network to
alleviate the problem of over   tting. to the best
of our knowledge, we are the    rst to use lstm-
based recurrent neural networks for the relation
classi   cation task.

we evaluate our proposed method on the
semeval 2010 relation classi   cation task, and
achieve an f1-score of 83.7%, higher than com-
peting methods in the literature.

in the rest of this paper, we review related work
in section 2. in section 3, we describe our sdp-
lstm model in detail. section 4 presents quan-
titative experimental results. finally, we have our
conclusion in section 5.

2 related work

relation classi   cation is a widely studied task
in the nlp community. various existing meth-

figure 1: the dependency parse tree correspond-
ing to the sentence    a trillion gallons of water
have been poured into an empty region of outer
space.    red lines indicate the shortest dependency
path between entities water and region. an edge
a     b refers to a being governed by b. depen-
dency types are labeled by the parser, but not pre-
sented in the    gure for clarity.

ods mainly fall into three classes: feature-based,
kernel-based, and neural network-based.

in feature-based approaches, different sets of
features are extracted and fed to a chosen classi   er
(e.g., id28). generally, three types of
features are often used. lexical features concen-
trate on the entities of interest, e.g., entities per se,
entity pos, entity neighboring information. syn-
tactic features include chunking, parse trees, etc.
semantic features are exempli   ed by the concept
hierarchy, entity class, entity mention. kamb-
hatla (2004) uses a maximum id178 model to
combine these features for relation classi   cation.
however, different sets of handcrafted features are
largely complementary to each other (e.g., hyper-
nyms versus named-entity tags), and thus it is hard
to improve performance in this way (zhou et al.,
2005).

kernel-based approaches specify some measure
of similarity between two data samples, with-
out explicit feature representation. zelenko et
al. (2003) compute the similarity of two trees by
utilizing their common subtrees. bunescu and
mooney (2005) propose a shortest path depen-
dency kernel for relation classi   cation. its main
idea is that the relation strongly relies on the de-
pendency path between two given entities. wang
(2008) provides a systematic analysis of several
kernels and show that id36 can bene-

pouredgallonshavebeenintotrillionof[region]a[water]anemptyofspaceoutere1e2   t from combining convolution kernel and syntac-
tic features. plank and moschitti (2013) introduce
semantic information into kernel methods in ad-
dition to considering structural information only.
one potential dif   culty of kernel methods is that
all data information is completely summarized by
the id81 (similarity measure), and thus
designing an effective kernel becomes crucial.

deep neural networks, emerging recently, can
learn underlying features automatically, and have
attracted growing interest in the literature. socher
et al. (2011) propose a id56
(id56) along sentences    parse trees for sentiment
analysis; such model can also be used to clas-
sify relations (socher et al., 2012). hashimoto et
al. (2013) explicitly weight phrases    importance
in id56s to improve performance. ebrahimi and
dou (2015) rebuild an id56 on the dependency
path between two marked entities. zeng et al.
(2014) explore convolutional neural networks, by
which they utilize sequential information of sen-
tences. santos et al. (2015) also use the convo-
lutional network; besides, they propose a ranking
id168 with data cleaning, and achieve the
state-of-the-art result in semeval-2010 task 8.

in addition to the above studies, which mainly
focus on relation classi   cation approaches and
models, other related research trends include in-
formation extraction from web documents in a
semi-supervised manner (bunescu and mooney,
2007; banko et al., 2007), dealing with small
datasets without enough labels by distant super-
vision techniques (mintz et al., 2009), etc.

3 the proposed sdp-lstm model

in this section, we describe our sdp-lstm model
in detail. subsection 3.1 delineates the overall ar-
chitecture of our model. subsection 3.2 presents
the rationale of using sdps. four different infor-
mation channels along the sdp are explained in
subsection 3.3. subsection 3.4 introduces the re-
current neural network with long short term mem-
ory, which is built upon the dependency path. sub-
section 3.5 customizes a dropout strategy for our
network to alleviate over   tting. we    nally present
our training objective in subsection 3.6.

3.1 overview
figure 2 depicts the overall architecture of our
sdp-lstm network.

first, a sentence is parsed to a dependency tree

by the stanford parser;1 the shortest dependency
path (sdp) is extracted as the input of our net-
work. along the sdp, four different types of
information   referred to as channels   are used,
including the words, pos tags, grammatical rela-
tions, and id138 hypernyms. (see figure 2a.)
in each channel, discrete inputs, e.g., words, are
mapped to real-valued vectors, called embeddings,
which capture the underlying meanings of the in-
puts.

two recurrent neural networks (figure 2b) pick
up information along the left and right sub-paths
of the sdp, respecitvely. (the path is separated by
the common ancestor node of two entities.) long
short term memory (lstm) units are used in the
recurrent networks for effective information prop-
agation. a max pooling layer thereafter gathers
information from lstm nodes in each path.

the pooling layers from different channels are
concatenated, and then connected to a hidden
layer. finally, we have a softmax output layer for
classi   cation. (see again figure 2a.)

3.2 the shortest dependency path
the dependency parse tree is naturally suitable for
relation classi   cation because it focuses on the ac-
tion and agents in a sentence (socher et al., 2014).
moreover, the shortest path between entities, as
discussed in section 1, condenses most illuminat-
ing information for entities    relation.

we also observe that the sub-paths, separated by
the common ancestor node of two entities, provide
strong hints for the relation   s directionality. take
figure 1 as an example. two entities water and
region have their common ancestor node, poured,
which separates the sdp into two parts:

[water]e1     of     gallons     poured

and

poured     into     [region]e2

the    rst sub-path captures information of e1,
whereas the second sub-path is mainly about
by examining the two sub-paths sepa-
e2.
rately, we know e1 and e2 are of
relation
entity-destination(e1, e2),
than
entity-destination(e2, e1).

rather

following the above intuition, we design
two recurrent neural networks, which propagate

1 http://nlp.stanford.edu/software/lex-parser.shtml

figure 2: (a) the overall architecture of sdp-lstm. (b) one channel of the recurrent neural networks
built upon the shortest dependency path. the channels are words, part-of-speech (pos) tags, grammatical
relations (abbreviated as gr in the    gure), and id138 hypernyms.

bottom-up from the entities to their common an-
cestor.
is direction-
sensitive.

in this way, our model

3.3 channels
we make use of four types of information along
the sdp for relation classi   cation. we call them
channels as these information sources do not inter-
act during recurrent propagation. detailed channel
descriptions are as follows.

    word representations. each word in a given
sentence is mapped to a real-valued vector by
looking up in a id27 table. un-
supervisedly trained on a large corpus, word
embeddings are thought to be able to well
capture words    syntactic and semantic infor-
mation (mikolov et al., 2013b).

    part-of-speech tags. since word embed-
dings are obtained on a generic corpus of a
large scale, the information they contain may
not agree with a speci   c sentence. we deal
with this problem by allying each input word
with its pos tag, e.g., noun, verb, etc.
in our experiment, we only take into use a
coarse-grained pos category, containing 15
different tags.

    grammatical relations. the dependency
relations between a governing word and its
children makes a difference in meaning. a
same word pair may have different depen-
dency relation types. for example,    beats
nsubj             it    is distinct from    beats dobj             it.   
thus, it is necessary to capture such gram-

matical relations in sdps.
in our experi-
ment, grammatical relations are grouped into
19 classes, mainly based on a coarse-grained
classi   cation (de marneffe et al., 2006).

    id138 hypernyms. as illustrated in sec-
tion 1, hyponymy information is also useful
for relation classi   cation. (details are not re-
peated here.) to leverage id138 hyper-
nyms, we use a tool developed by ciaramita
and altun (2006).2 the tool assigns a hy-
pernym to each word, from 41 prede   ned
concepts in id138, e.g., noun.food,
verb.motion, etc. given its hypernym,
each word gains a more abstract concept,
which helps to build a linkage between dif-
ferent but conceptual similar words.

as we can see, pos tags, grammatical rela-
tions, and id138 hypernyms are also discrete
(like words per se). however, no prevailing em-
bedding learning method exists for pos tags, say.
hence, we randomly initialize their embeddings,
and tune them in a supervised fashion during train-
ing. we notice that these information sources con-
tain much fewer symbols, 15, 19, and 41, than the
vocabulary size (greater than 25,000). hence, we
believe our strategy of random initialization is fea-
sible, because they can be adequately tuned during
supervised training.

3.4 recurrent neural network with long

short term memory units

the recurrent neural network is suitable for mod-
eling sequential data by nature, as it keeps a hid-

2http://sourceforge.net/projects/supersensetag

lstmoforwordoembeddingshiddenolayerdependencyooooooopathssoftmaxleftosub-pathpoolrightosub-pathpoolhiddenolayerlstmoforposoembeddingslstmoforgroembeddingslstmoforid138oembeddings(a)(b)lstmlstmlstmlstmlstmlstmlstmintroduced by zaremba and sutskever (2014), also
used in zhu et al. (2014).

concretely, the lstm-based recurrent neural
network comprises four components: an input gate
it, a forget gate ft, an output gate ot, and a mem-
ory cell ct (depicted in figure 3 and formalized
through equations 1   6 as bellow).

the three adaptive gates it, ft, and ot depend
on the previous state ht   1 and the current input
xt (equations 1   3). an extracted feature vector
gt is also computed, by equation 4, serving as the
candidate memory cell.

it =   (wi  xt + ui  ht   1 + bi)
ft =   (wf   xt + uf   ht   1 + bf )
ot =   (wo  xt + uo  ht   1 + bo)
gt = tanh(wg   xt + ug  ht   1 + bg)

(1)
(2)
(3)
(4)

the current memory cell ct is a combination of
the previous cell content ct   1 and the candidate
content gt, weighted by the input gate it and forget
gate ft, respectively. (see equation 5 below.)

ct = it     gt + ft     ct   1

(5)

the output of lstm units is the the recur-
rent network   s hidden state, which is computed by
equation 6 as follows.

ht = ot     tanh(ct)

(6)

in the above equations,    denotes a sigmoid
function;     denotes element-wise multiplication.
3.5 dropout strategies
a good id173 approach is needed to al-
leviate over   tting. dropout, proposed recently
by hinton et al. (2012), has been very successful
on feed-forward networks. by randomly omitting
feature detectors from the network during train-
ing, it can obtain less interdependent network units
and achieve better performance. however, the
conventional dropout does not work well with re-
current neural networks with lstm units, since
dropout may hurt the valuable memorization abil-
ity of memory units.

as there is no consensus on how to drop
out lstm units in the literature, we try several
dropout strategies for our sdp-lstm network:

    dropout embeddings;
    dropout inner cells in memory units, includ-

ing it, gt, ot, ct, and ht; and

figure 3: a long short term memory unit. h: hid-
den unit. c: memory cell. i: input gate. f: for-
get gate. o: output gate. g: candidate cell.    :
element-wise multiplication.    : activation func-
tion.

den state vector h, which changes with input data
at each step accordingly. we use the recurrent net-
work to gather information along each sub-path in
the sdp (figure 2b).

the hidden state ht, for the t-th word in the
sub-path, is a function of its previous state ht   1
and the current word xt. traditional recurrent net-
works have a basic interaction, that is, the input is
linearly transformed by a weight matrix and non-
linearly squashed by an activation function. for-
mally, we have

ht = f (winxt + wrecht   1 + bh)

where win and wrec are weight matrices for the
input and recurrent connections, respectively. bh
is a bias term for the hidden state vector, and fh a
non-linear activation function (e.g., tanh).

one problem of the above model is known
as gradient vanishing or exploding. the train-
ing of neural networks requires gradient back-
propagation. if the propagation sequence (path) is
too long, the gradient may probably either grow, or
decay, exponentially, depending on the magnitude
of wrec. this leads to the dif   culty of training.

long short term memory (lstm) units are pro-
posed in hochreiter (1998) to overcome this prob-
lem. the main idea is to introduce an adaptive gat-
ing mechanism, which decides the degree to which
lstm units keep the previous state and memo-
rize the extracted features of the current data in-
put. many lstm variants have been proposed in
the literature. we adopt in our method a variant

ct~~htgi~o~xtft-1cht-1xtht-1ht-1xt~ht-1xt    dropout the penultimate layer.

as we shall see in section 4.2, dropping out
lstm units turns out to be inimical to our model,
whereas the other two strategies boost in perfor-
mance.

the following equations formalize the dropout
operations on the embedding layers, where d de-
notes the dropout operator. each dimension in the
embedding vector, xt, is set to zero with a prede-
   ned dropout rate.

it =   (wi  d(xt) + ui  ht   1 + bi)
ft =   (wf   d(xt) + uf   ht   1 + bf )
ot =   (wo  d(xt) + uo  ht   1 + bo)
gt = tanh

wg  d(xt) + ug  ht   1 + bg

(cid:16)

(7)
(8)
(9)

(10)

(cid:17)

j =     nc(cid:88)

3.6 training objective
the sdp-lstm described above propagates in-
formation along a sub-path from an entity to the
common ancestor node (of the two entities). a
max pooling layer packs, for each sub-path, the
recurrent network   s states, h   s, to a    xed vector
by taking the maximum value in each dimension.
to all channels,
namely, words, pos tags, grammatical relations,
and id138 hypernyms. the pooling vectors in
these channels are concatenated, and fed to a fully
connected hidden layer. finally, we add a softmax
output layer for classi   cation. the training objec-
tive is the penalized cross-id178 error, given by

such architecture applies

(cid:32)   (cid:88)

(cid:33)

  (cid:88)

ti log yi+  

(cid:107)wi(cid:107)2

f +

(cid:107)ui(cid:107)2

f

i=1

i=1

i=1

where t     rnc is the one-hot represented ground
truth and y     rnc is the estimated id203 for
each class by softmax. (nc is the number of target
classes.) (cid:107)    (cid:107)f denotes the frobenius norm of a
matrix;    and    are the numbers of weight matri-
ces (for w    s and u   s, respectively).    is a hyper-
parameter that speci   es the magnitude of penalty
on weights. note that we do not add (cid:96)2 penalty to
biase parameters.

we pretrained id27s by id97
(mikolov et al., 2013a) on the english wikipedia
corpus; other parameters are initialized randomly.
we apply stochastic id119 (with mini-
batch 10) for optimization; gradients are computed
by standard back-propagation. training details are
further introduced in section 4.2.

4 experiments
in this section, we present our experiments in de-
tail. our implementation is built upon mou et al.
(2015). section 4.1 introduces the dataset; section
4.2 describes hyperparameter settings. in section
4.3, we compare sdp-lstm   s performance with
other methods in the literature. we also analyze
the effect of different channels in section 4.4.

4.1 dataset
the semeval-2010 task 8 dataset is a widely used
benchmark for relation classi   cation (hendrickx
et al., 2010). the dataset contains 8,000 sentences
for training, and 2,717 for testing. we split 1/10
samples out of the training set for validation.

the target contains 19 labels: 9 directed rela-
tions, and an undirected other class. the di-
rected relations are list as below.

    cause-effect
    component-whole
    content-container
    entity-destination
    entity-origin
    message-topic
    member-collection
    instrument-agency
    product-producer
in the following are illustrated two sample sen-

tences with directed relations.

[people]e1 have been moving back into
[downtown]e2.
financial [stress]e1 is one of the main
causes of [divorce]e2.

the target labels are entity-destination
(e1, e2), and cause-effect(e1, e2),
respec-
tively.

the dataset also contains an undirected other
class. hence, there are 19 target labels in total.
the undirected other class takes in entities that
do not    t into the above categories, illustrated by
the following example.

a misty [ridge]e1 uprises from the
[surge]e2.

we use the of   cial macro-averaged f1-score to
evaluate model performance. this of   cial mea-
surement excludes the other relation. nonethe-
less, we have no special treatment of other class
in our experiments, which is typical in other stud-
ies.

(a) dropout id27s

(b) dropout inner cells of memory units

(c) dropout the penultimate layer

figure 4: f1-scores versus dropout rates. we    rst evaluate the effect of dropout embeddings (a). then
the dropout of the inner cells (b) and the penultimate layer (c) is tested with id27s being
dropped out by 0.5.

4.2 hyperparameters and training details
this subsection presents hyperparameter tuning
for our model. we set word-embeddings to
be 200-dimensional; pos, id138 hyponymy,
and grammatical relation embeddings are 50-
dimensional. each channel of the lstm network
contains the same number of units as its source
embeddings (either 200 or 50). the penultimate
hidden layer is 100-dimensional. as it is not fea-
sible to perform full grid search for all hyperpa-
rameters, the above values are chosen empirically.
we add (cid:96)2 penalty for weights with coef   cient
10   5, which was chosen by validation from the set
{10   2, 10   3,       , 10   7}.

we thereafter validate the proposed dropout
strategies in section 3.5. since network units in
different channels do not interact with each other
during information propagation, we herein take
one channel of id137 to assess the ef-
   cacy. taking the word channel as an example,
we    rst drop out id27s. then with a
   xed dropout rate of id27s, we test the
effect of dropping out lstm inner cells and the
penultimate units, respectively.

we    nd that, dropout of lstm units hurts the
model, even if the dropout rate is small, 0.1,
say (figure 4b). dropout of embeddings im-
proves model performance by 2.16% (figure 4a);
dropout of the penultimate layer further improves
by 0.16% (figure 4c). this analysis also provides,
for other studies, some clues for dropout in lstm
networks.

4.3 results
table 4 compares our sdt-lstm with other state-
of-the-art methods. the    rst entry in the ta-

ble presents the highest performance achieved by
traditional feature engineering. hendrickx et al.
(2010) leverage a variety of handcrafted features,
and use id166 for classi   cation; they achieve an
f1-score of 82.2%.

neural networks are    rst used in this task in
socher et al. (2012). they build a recursive neural
network (id56) along a constituency tree for re-
lation classi   cation. they extend the basic id56
with matrix-vector interaction and achieve an f1-
score of 82.4%.

zeng et al. (2014) treat a sentence as sequen-
tial data and exploit the convolutional neural net-
work (id98); they also integrate word position in-
formation into their model. santos et al. (2015)
design a model called cr-id98; they propose a
ranking-based cost function and elaborately di-
minish the impact of the other class, which is
not counted in the of   cial f1-measure. in this way,
they achieve the state-of-the-art result with the f1-
score of 84.1%. without such special treatment,
their f1-score is 82.7%.

yu et al. (2014) propose a feature-rich com-
positional embedding model (fcm) for relation
classi   cation, which combines unlexicalized lin-
guistic contexts and id27s. they
achieve an f1-score of 83.0%.

our proposed sdt-lstm model yields an f1-
score of 83.7%. it outperforms existing compet-
ing approaches, in a fair condition of softmax with
cross-id178 error.

it is worth to note that we have also conducted
two controlled experiments: (1) traditional id56
without lstm units, achieving an f1-score of
82.8%; (2) lstm network over the entire depen-
dency path (instead of two sub-paths), achieving

00.10.20.30.40.50.6dropout rate70758085f1-score (%)00.10.20.30.40.50.6dropout rate70758085f1-score (%)00.10.20.30.40.50.6dropout rate70758085f1-score (%)classi   er

id166

id56

mvid56

id98

feature set
pos, id138, pre   xes and other morphological features,
depdency parse, levin classes, propbank, fanmenet,
noid113x-plus, google id165, paraphrases, textrunner
id27s
id27s, pos, ner, id138
id27s
id27s, pos, ner, id138
id27s
id27s, word position embeddings, id138

chain id98 id27s, pos, ner, id138

fcm

cr-id98

sdp-lstm

id27s
id27s, depedency parsing, ner
id27s
id27s, position embeddings
id27s, position embeddings
id27s
id27s, pos embeddings, id138 embeddings,
grammar relation embeddings

f1

82.2

74.8
77.6
79.1
82.4
69.7
82.7
82.7
80.6
83.0
82.8   
82.7
84.1   
82.4
83.7

table 1: comparison of relation classi   cation systems. the           remark refers to special treatment for
the other class.

an f1-score of 82.2%. these results demonstrate
the effectiveness of lstm and directionality in re-
lation classi   cation.

4.4 effect of different channels
this subsection analyzes how different channels
affect our model. we    rst used id27s
only as a baseline; then we added pos tags, gram-
matical relations, and id138 hypernyms, re-
spectively; we also combined all these channels
into our models. note that we did not try the latter
three channels alone, because each single of them
(e.g., pos) does not carry much information.

we see from table 2 that id27s
alone in sdp-lstm yield a remarkable perfor-
mance of 82.35%, compared with id98s 69.7%,
id56s 74.9   79.1%, and fcm 80.6%.

adding either grammatical relations or word-
net hypernyms outperforms other existing meth-
ods (data cleaning not considered here). pos tag-
ging is comparatively less informative, but still
boosts the f1-score by 0.63%.

we notice that, the boosts are not simply added
when channels are combined. this suggests that
these information sources are complementary to
each other in some linguistic aspects. nonethe-
less, incorporating all four channels further pushes
the f1-score to 83.70%.

channels
id27s
+ pos embeddings (only)
+ gr embeddings (only)
+ id138 embeddings (only)
+ pos + gr + id138 embeddings

f1
82.35
82.98
83.21
83.03
83.70

table 2: effect of different channels.

5 conclusion

in this paper, we propose a novel neural network
model, named sdp-lstm, for relation classi   -
cation.
it learns features for relation classi   ca-
tion iteratively along the shortest dependency path.
several types of information (word themselves,
pos tags, grammatical relations and id138 hy-
pernyms) along the path are used. meanwhile,
we leverage lstm units for long-range infor-
mation propagation and integration. we demon-
strate the effectiveness of sdp-lstm by evalu-
ating the model on semeval-2010 relation clas-
si   cation task, outperforming existing state-of-art
methods (in a fair condition without data clean-
ing). our result sheds some light in the relation
classi   cation task as follows.

    the shortest dependency path can be a valu-
able resource for relation classi   cation, cov-
ering mostly suf   cient information of target

relations.

    classifying relation is a challenging task due
to the inherent ambiguity of natural
lan-
guages and the diversity of sentence expres-
sion. thus, integrating heterogeneous lin-
guistic knowledge is bene   cial to the task.
    treating the shortest dependency path as two
sub-paths, mapping two different neural net-
works, helps to capture the directionality of
relations.
    lstm units are effective in feature detec-
tion and propagation along the shortest de-
pendency path.

acknowledgments
this research is supported by the national basic
research program of china (the 973 program) un-
der grant no. 2015cb352201 and the national
natural science foundation of china under grant
nos. 61232015 and 91318301.

references
[banko et al.2007] m. banko, m. j. cafarella, s. soder-
land, m. broadhead, and o. etzioni. 2007. open
in ijcai, vol-
information extraction for the web.
ume 7, pages 2670   2676.

[bengio et al.2013] y. bengio, a. courville, and p. vin-
cent. 2013. representation learning: a review and
ieee transactions on pattern
new perspectives.
analysis and machine intelligence, 35(8):1798   
1828.

[bunescu and mooney2005] r. c. bunescu and r. j.
mooney. 2005. a shortest path dependency kernel
in proceedings of the con-
for id36.
ference on human language technology and em-
pirical methods in natural language processing,
pages 724   731. association for computational lin-
guistics.

[bunescu and mooney2007] r.

and
r. mooney.
learning to extract rela-
tions from the web using minimal supervision.
in annual meeting-association for computational
linguistics, volume 45, page 576.

bunescu

2007.

[chen et al.2014] yun-nung chen, dilek hakkani-tur,
and gokan tur. 2014. deriving local relational
surface forms from dependency-based entity embed-
dings for unsupervised spoken language understand-
in spoken language technology workshop
ing.
(slt), 2014 ieee, pages 242   247. ieee.

empirical methods in natural language process-
ing, pages 594   602. association for computational
linguistics.

[de marneffe et al.2006] m. c. de marneffe, b. mac-
cartney, and c. d. manning.
2006. generat-
ing typed dependency parses from phrase structure
in proceedings of lrec, volume 6, pages
parses.
449   454.

[ebrahimi and dou2015] j. ebrahimi and d. dou.
2015. chain based id56 for relation classi   cation. in
hlt-naacl.

[fundel et al.2007] k. fundel, r. k  uffner, and r. zim-
mer. 2007. relex   id36 using de-
pendency parse trees. bioinformatics, 23(3):365   
371.

[hashimoto et al.2013] k. hashimoto, m. miwa,
y. tsuruoka, and t. chikayama. 2013. simple
id56s for
customization of
in emnlp, pages
semantic relation classi   cation.
1372   1376.

2010.

[hendrickx et al.2010] i. hendrickx, s. n. kim, and
z. et al. kozareva.
semeval-2010 task
8: multi-way classi   cation of semantic relations
in proceedings of
between pairs of nominals.
the workshop on semantic evaluations: recent
achievements and future directions). association
for computational linguistics.

[hinton et al.2012] g. e. hinton, n. srivastava,
a. krizhevsky, i. sutskever, and r. r. salakhut-
dinov.
improving neural networks by
preventing co-adaptation of feature detectors. arxiv
preprint arxiv:1207.0580.

2012.

[hochreiter1998] s. hochreiter. 1998. the vanishing
gradient problem during learning recurrent neural
nets and problem solutions. international journal of
uncertainty, fuzziness and knowledge-based sys-
tems, 6(02):107   116.

[kambhatla2004] n. kambhatla.

2004. combining
lexical, syntactic, and semantic features with max-
imum id178 models for extracting relations.
in
proceedings of the acl 2004 on interactive poster
and demonstration sessions, page 22. association
for computational linguistics.

[mikolov et al.2013a] t. mikolov,

sutskever,
k. chen, g. s. corrado, and j. dean.
2013a.
distributed representations of words and phrases
in advances in neural
and their compositionality.
information processing systems, pages 3111   3119.

i.

[mikolov et al.2013b] t. mikolov, w. t. yih, and
g. zweig. 2013b. linguistic regularities in continu-
ous space word representations. in hlt-naacl.

[ciaramita and altun2006] m. ciaramita and y. altun.
2006. broad-coverage sense disambiguation and
information extraction with a supersense sequence
in proceedings of the 2006 conference on
tagger.

[mintz et al.2009] m. mintz, s. bills, r. snow, and
d. jurafsky. 2009. distant supervision for relation
in proceedings of
extraction without labeled data.
the joint conference of the 47th annual meeting of

[yao and van durme2014] x. yao and b. van durme.
2014.
information extraction over structured data:
id53 with freebase. in proceedings
of the 52nd annual meeting of the association for
computational linguistics (volume 1: long pa-
pers), pages 956   966, baltimore, maryland, june.
association for computational linguistics.

[yu et al.2014] m. yu, m. r. gorid113y, and m. dredze.
2014. factor-based compositional embedding mod-
in the nips 2014 learning semantics work-
els.
shop, december.

[zaremba and sutskever2014] w.

and
i. sutskever. 2014. learning to execute. arxiv
preprint arxiv:1410.4615.

zaremba

[zelenko et al.2003] d. zelenko, c. aone,

and
a. richardella. 2003. kernel methods for relation
the journal of machine learning
extraction.
research, 3:1083   1106.

[zeng et al.2014] d. zeng, k. liu, s. lai, g. zhou, and
j. zhao. 2014. relation classi   cation via convolu-
tional deep neural network. in proceedings of col-
ing, pages 2335   2344.

[zhou et al.2005] g.d. zhou, j. su, j. zhang, and
m. zhang. 2005. exploring various knowledge in
id36. in proceedings of the 43rd an-
nual meeting on association for computational lin-
guistics, pages 427   434. association for computa-
tional linguistics.

[zhu et al.2014] x. zhu, p. sobhani, and h. guo. 2014.
long short-term memory over tree structures.
in
proceedings of the 32nd international conference
on machine learning.

the acl and the 4th international joint conference
on natural language processing of the afnlp: vol-
ume 2-volume 2, pages 1003   1011. association for
computational linguistics.

[mou et al.2015] l. mou, h. peng, g. li, y. xu,
l. zhang, and z. jin. 2015. discriminative neural
sentence modeling by tree-based convolution. arxiv
preprint arxiv:1504.01106.

[plank and moschitti2013] b. plank and a. moschitti.
2013. embedding semantic similarity in tree kernels
for id20 of id36. in acl
(1), pages 1498   1507.

[santos et al.2015] c. n. d. santos, b. xiang, and
b. zhou. 2015. classifying relations by ranking
with convolutional neural networks. in proceedings
of the 53rd annual meeting of the association for
computational linguistics.

[socher et al.2011] r. socher, j. pennington, e. h.
huang, a. y. ng, and c. d. manning.
2011.
semi-supervised recursive autoencoders for predict-
in proceedings of the
ing sentiment distributions.
conference on empirical methods in natural lan-
guage processing, pages 151   161. association for
computational linguistics.

[socher et al.2012] r. socher, b. huval, c. d. man-
ning, and a. y. ng. 2012. semantic compositional-
ity through recursive matrix-vector spaces. in pro-
ceedings of the 2012 joint conference on empiri-
cal methods in natural language processing and
computational natural language learning, pages
1201   1211. association for computational linguis-
tics.

[socher et al.2014] r. socher, a. karpathy, q. v. le,
c. d. manning, and a. y. ng. 2014. grounded
id152 for    nding and describing
images with sentences. transactions of the associa-
tion for computational linguistics, 2:207   218.

[wang and fan2014] c. wang and j. fan. 2014. med-
ical id36 with manifold models.
in
proceedings of the 52nd annual meeting of the as-
sociation for computational linguistics (volume 1:
long papers), pages 828   838, baltimore, maryland,
june. association for computational linguistics.

[wang2008] m. wang. 2008. a re-examination of de-
in

pendency path kernels for id36.
ijcnlp, pages 841   846.

[wu and weld2010] f. wu and d. s. weld. 2010. open
information extraction using wikipedia. in proceed-
ings of the 48th annual meeting of the association
for computational linguistics, pages 118   127. as-
sociation for computational linguistics.

[xu et al.2014] y. xu, g. li, l. mou, and y. lu.
2014. learning non-taxonomic relations on demand
international journal of
for ontology extension.
software engineering and knowledge engineering,
24(08):1159   1175.

