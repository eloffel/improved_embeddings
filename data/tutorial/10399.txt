learning meta-embeddings

by using ensembles of embedding sets

wenpeng yin and hinrich sch  utze

center for information and language processing

university of munich, germany

wenpeng@cis.uni-muenchen.de

5
1
0
2
 
c
e
d
0
3

 

 
 
]
l
c
.
s
c
[
 
 

2
v
7
5
2
4
0

.

8
0
5
1
:
v
i
x
r
a

abstract

id27s     distributed representa-
tions of words     in deep learning are bene   -
cial for many tasks in natural language pro-
cessing (nlp). however, different embedding
sets vary greatly in quality and characteris-
tics of the captured semantics. instead of re-
lying on a more advanced algorithm for em-
bedding learning, this paper proposes an en-
semble approach of combining different pub-
lic embedding sets with the aim of learning
meta-embeddings. experiments on word simi-
larity and analogy tasks and on part-of-speech
tagging show better performance of meta-
embeddings compared to individual embed-
ding sets. one advantage of meta-embeddings
is the increased vocabulary coverage. we will
release our meta-embeddings publicly.

introduction

1
recently, deep neural network (nn) models have
achieved remarkable results in nlp (collobert and
weston, 2008; sutskever et al., 2014; rockt  aschel et
al., 2015). one reason for these results are word em-
beddings, compact distributed word representations
learned in an unsupervised manner from large cor-
pora (bengio et al., 2003; mnih and hinton, 2009;
mikolov et al., 2010; mikolov, 2012; mikolov et al.,
2013a).

some prior work has studied differences in per-
formance of different embedding sets. for exam-
ple, chen et al. (2013) showed that the embed-
ding sets hlbl (mnih and hinton, 2009), senna
(collobert and weston, 2008), turian (turian et al.,
2010) and huang (huang et al., 2012) have great

variance in quality and characteristics of the seman-
tics captured. hill et al.
(2014; 2015a) showed
that embeddings learned by nn machine translation
models can outperform three representative mono-
lingual embedding sets: id97 (mikolov et al.,
2013b), glove (pennington et al., 2014) and cw
(collobert and weston, 2008). bansal et al. (2014)
found that brown id91, senna, cw, huang
and id97 yield signi   cant gains for dependency
parsing. moreover, using these representations to-
gether achieved the best results, suggesting their
complementarity. these prior studies motivate us to
explore an ensemble approach. since each embed-
ding set is trained by a different nn on a different
corpus and can be treated as a distinct description of
words, our expectation is that the ensemble contains
more information than each component embedding
set. we want to leverage this diversity to learn better-
performing id27s.

the

ensemble

approach has

two bene   ts.
first, enhancement of the representations: meta-
embeddings perform better than the individual em-
bedding sets. second, coverage: meta-embeddings
cover more words than the individual embedding
sets. the    rst three ensemble methods we introduce
are conc, svd and 1ton and they directly
only have the bene   t of enhancement. they learn
meta-embeddings on the overlapping vocabulary
of the embedding sets. conc concatenates the
vectors of a word from the different embedding sets.
svd performs dimension reduction on this con-
catenation. 1ton assumes that a meta-embedding
for the word exists and uses this meta-embedding
to predict representations of the word in the indi-

vidual embedding sets     the resulting    ne-tuned
meta-embedding is expected to contain knowledge
from all individual embedding sets.

to also address the objective of increased cover-
age of the vocabulary, we introduce 1ton+, a mod-
i   cation of 1ton that learns meta-embeddings for
all words in the vocabulary union in one step. let
an out-of-vocabulary (oov) word w of embedding
set es be a word that is not covered by es (i.e.,
es does not contain an embedding for w).1 1ton+
   rst randomly initializes the embeddings for oovs
and the meta-embeddings, then uses a prediction
setup similar to 1ton to update meta-embeddings
as well as oov embeddings. thus, 1ton+ si-
multaneously achieves two goals:
learning meta-
embeddings and extending the vocabulary (for both
meta-embeddings and invidual embedding sets).

an alternative method that increases coverage is
mutuallearning. mutuallearning learns
the embedding for a word that is an oov in em-
bedding set from its embeddings in other embed-
ding sets. we will use mutuallearning to in-
crease coverage for conc, svd and 1ton, so that
these three methods (when used together with mu-
tuallearning) have the advantages of both per-
formance enhancement and increased coverage.

in summary, meta-embeddings have two bene   ts
compared to individual embedding sets: enhance-
ment of performance and improved coverage of the
vocabulary. below, we demonstrate this experimen-
tally for three tasks: word similarity, word analogy
and id52.

if we simply view meta-embeddings as a way of
coming up with better embeddings, then the alter-
native is to develop a single embedding learning
algorithm that produces better embeddings. some
improvements proposed before have the disadvan-
tage of increasing the training time of embedding
learning substantially; e.g., the nnlm presented
in (bengio et al., 2003) is an order of magnitude
less ef   cient than an algorithm like id97 and,
more generally, replacing a linear objective function
with a nonlinear objective function increases train-
ing time. similarly,    ne-tuning the hyperparameters
of the embedding learning algorithm is complex and

1we do not consider words in this paper that are not covered
by any of the individual embedding sets. oov always refers to
a word that is covered by at least one embedding set.

time consuming. in many cases, it is not possible to
retrain using a different algorithm because the cor-
pus is not publicly available. but even if these ob-
stacles could be overcome, it is unlikely that there
ever will be a single    best    embedding learning al-
gorithm. so the current situation of multiple embed-
ding sets with different properties being available
is likely to persist for the forseeable future. meta-
embedding learning is a simple and ef   cient way
of taking advantage of this diversity. as we will
show below they combine several complementary
embedding sets and the resulting meta-embeddings
are stronger than each individual set.

2 related work

related work has focused on improving perfor-
mance on speci   c tasks by using several embed-
ding sets simultaneously. to our knowledge, there
is no work that aims to learn generally useful meta-
embeddings from individual embedding sets.

tsuboi (2014) incorporated id97 and glove
embeddings into a id52 system and found
that using these two embedding sets together was
better than using them individually.
similarly,
turian et al. (2010) found that using brown clusters,
cw embeddings and hlbl embeddings for ner
and chunking tasks together gave better performance
than using these representations individually.

luo et al. (2014) adapted cbow (mikolov et
al., 2013a) to train id27s on different
datasets     a wikipedia corpus, search click-through
data and user query data     for web search rank-
ing and for word similarity. they showed that us-
ing these embeddings together gives stronger results
than using them individually.

these papers show that using multiple embedding
sets is bene   cial. however, they either use embed-
ding sets trained on the same corpus (turian et al.,
2010) or enhance embedding sets by more training
data, not by innovative learning algorithms (luo et
al., 2014). in our work, we can leverage any pub-
licly available embedding set learned by any learn-
ing algorithm. our meta-embeddings are generi-
cally useful and are learned by supervised training of
an explicit model of the dependencies between em-
bedding sets and (except for conc) not by simple
concatenation.

3 experimental embedding sets
in this work, we use    ve released embedding sets.
(i) hlbl. hierarchical log-bilinear (mnih and hin-
ton, 2009) embeddings released by turian et al.
(2010);2 246,122 id27s, 100 dimen-
sions; training corpus: rcv1 corpus (reuters en-
glish newswire, august 1996     august 1997). (ii)
huang.3 huang et al. (2012) incorporated global
context to deal with challenges raised by words with
multiple meanings; 100,232 id27s, 50
dimensions; training corpus: april 2010 snapshot of
wikipedia. (iii) glove4 (pennington et al., 2014).
1,193,514 id27s, 300 dimensions; train-
ing corpus: 42 billion tokens of web data, from
common crawl. (iv) cw (collobert and weston,
2008). released by turian et al. (2010);5 268,810
id27s, 200 dimensions;
training cor-
pus: same as hlbl. (v) id97 (mikolov et al.,
2013b) cbow;6 929,022 id27s (we
discard phrase embeddings), 300 dimensions; train-
ing corpus: google news (about 100 billion words).
the intersection of the    ve vocabularies has size

35,965, the union has size 2,788,636.

4 ensemble methods
this section introduces the four ensemble methods:
conc, svd, 1ton and 1ton+.

4.1 conc: concatenation
in conc, the meta-embedding of w is the concate-
nation of    ve embeddings, one each from the    ve
embedding sets. for glove, we perform l2 normal-
ization for each dimension across the vocabulary as
recommended by the glove authors. then each em-
bedding of each embedding set is l2-normalized.
this ensures that each embedding set contributes
equally (a value between -1 and 1) when we com-
pute similarity via dot product.

we would like to make use of prior knowledge
and give more weight to well performing embed-
ding sets. in this work, we give glove and id97

2http://metaoptimize.com/projects/

wordreprs/

3 http://ai.stanford.edu/  ehhuang/
4http://nlp.stanford.edu/projects/glove/
5http://metaoptimize.com/projects/

wordreprs/

6http://code.google.com/p/id97/

figure 1: performance vs. weight scalar i

weight i > 1 and weight 1 to the other three embed-
ding sets. we use mc30 (miller and charles, 1991)
as dev set, since all embedding sets fully cover it.
we set i = 8, the value in figure 1 where perfor-
mance reaches a plateau. after l2 id172,
glove and id97 embeddings are multiplied by
i and remaining embedding sets are left unchanged.
the dimensionality of conc meta-embeddings

is k = 100 + 50 + 300 + 200 + 300 = 950.

4.2 svd: singular value decomposition
we do svd on above weighted concatenation vec-
tors of dimension k = 950.

given a set of conc representations for n words,
each of dimensionality k, we compute an svd de-
composition c = u sv t of the corresponding n  k
matrix c. we then use ud, the    rst d dimensions of
u, as the svd meta-embeddings of the n words. we
apply l2-id172 to embeddings; similarities
of svd vectors are computed as dot products.

d denotes the dimensionality of meta-embeddings
in svd, 1ton and 1ton+. we use d = 200
throughout and investigate the impact of d below.

1ton

4.3
figure 2 depicts the simple neural network we em-
ploy to learn meta-embeddings in 1ton. white rect-
angles denote known embeddings. the target to
learn is the meta-embedding (shown as shaded rect-
angle). meta-embeddings are initialized randomly.

figure 2: 1ton

 i2468101214161820performance0.650.70.750.80.850.9mc30let c be the number of embedding sets under con-
sideration, v1, v2, . . . , vi, . . . , vc their vocabularies
and v     =    c
i=1vi the intersection, used as train-
ing set. let v    denote the meta-embedding space.
we de   ne a projection f   i from space v    to space vi
(i = 1, 2, . . . , c) as follows:

oovs; thus, it does not have the limitation that it
can only be run on overlapping vocabulary.

(cid:88)

  wi = m   iw   

(1)
where m   i     rdi  d, w        rd is the meta-
embedding of word w in space v    and   wi     rdi
is the projected (or learned) representation of word
w in space vi. the training objective is minimizing
the sum of (i) squared error:

e =

|   wi     wi|2

(2)

i

and (ii) l2 cost (sum of squares) of the projection
weights m   i.

as for conc and svd, we weight glove and
id97 by i = 8. for 1ton, we implement this
by applying the factor i to the corresponding loss
part of the squared error.

the principle of 1ton is that we treat each in-
dividual embedding as a projection of the meta-
embedding, similar to principal component analysis.
an embedding is a description of the word based on
the corpus and the model that were used to create it.
the meta-embedding tries to recover a more com-
prehensive description of the word when it is trained
to predict the individual descriptions.

1ton can also be understood as a sentence mod-
eling process, similar to dbow (le and mikolov,
2014). the embedding of each word in a sentence
s is a partial description of s. dbow combines
all partial descriptions to form a comprehensive de-
scription of s. dbow initializes the sentence rep-
resentation randomly, then uses this representation
to predict the representations of individual words.
the sentence representation of s corresponds to the
meta-embedding in 1ton; and the representations
of the words in s correspond to the    ve embeddings
for a word in 1ton.

1ton+

4.4
recall that an oov (with respect to embedding set
es) is de   ned as a word unknown in es. 1ton+
is an extension of 1ton that learns embeddings for

figure 3: 1ton+

figure 3 depicts 1ton+.

in contrast to figure
2, we assume that the current word is an oov in
embedding sets 3 and 5. hence, in the new learning
task, embeddings 1, 2, 4 are known, and embeddings
3 and 5 and the meta-embedding are targets to learn.
we initialize all oov representations and meta-
embeddings randomly and use the same map-
ping formula as for 1ton to connect a meta-
embedding with the individual embeddings. both
meta-embedding and initialized oov embeddings
are updated during training.

each embedding set contains information about
only a part of the overall vocabulary. however, it
can predict what the remaining part should look like
by comparing words it knows with the information
other embedding sets provide about these words.
thus, 1ton+ learns a model of the dependencies
between the individual embedding sets and can use
these dependencies to infer what the embedding of
an oov should look like.

conc, svd and

1ton compute meta-
embeddings only for the intersection vocabulary.
1ton+ computes meta-embeddings for the union of
all individual vocabularies, thus greatly increasing
the coverage of individual embedding sets.

5 mutuallearning

mutuallearning is a method that extends
conc, svd and 1ton such that they have in-
creased coverage of the vocabulary. with mutu-
allearning, all four ensemble methods     conc,
svd, 1ton and 1ton+     have the bene   ts of both
performance enhancement and increased coverage
and we can use criteria like performance, compact-
ness and ef   ciency of training to select the best en-
semble method for a particular application.

1ton
mutuallearning (ml)
1ton+

bs
200
200
2000

lr

0.005
0.01
0.005

l2 weight
5    10   4
5    10   8
5    10   4

table 1: training setup. bs: batch size; lr: learning rate.

mutuallearning is applied to learn oov em-
beddings for all c embedding sets; however, for ease
of exposition, let us assume we want to compute em-
beddings for oovs for embedding set j only, based
on known embeddings in the other c     1 embedding
sets, with indexes i     {1 . . . j     1, j + 1 . . . c}. we
do this by learning c     1 mappings fij, each a pro-
jection from embedding set ei to embedding set ej.
similar to section 4.3, we train mapping fij on
the intersection vi     vj of the vocabularies cov-
ered by the two embedding sets. formally,   wj =
fij(wi) = mijwi where mij     rdj  di, wi     rdi
denotes the representation of word w in space vi and
  wj is the projected meta-embedding of word w in
space vj. training loss has the same form as for
1ton. a total of c     1 projections fij are trained to
learn oov embeddings for embedding set j.

let w be a word unknown in the vocabulary vj
of embedding set j, but known in v1, v2, . . . , vk.
to compute an embedding for w in vj, we    rst
compute the k projections f1j(w1), f2j(w2), . . .,
fkj(wk) from the source spaces v1, v2, . . . , vk to
the target space vj. then, the element-wise aver-
age of f1j(w1), f2j(w2), . . ., fkj(wk) is treated as
the representation of w in vj. our motivation is that
    assuming there is a true representation of w in vj
and assuming the projections were learned well     we
would expect all the projected vectors to be close
to the true representation. also, each source space
contributes potentially complementary information.
hence averaging them is a balance of knowledge
from all source spaces.

6 experiments

we train nns by back-propagation with adagrad
(duchi et al., 2011) and mini-batches. table 1 gives
hyperparameters.

we report results on three tasks: word similarity,

word analogy and id52.

6.1 word similarity and analogy tasks
we evaluate on siid113x-999 (hill et al., 2015b),
wordsim353 (finkelstein et al., 2001), rg (ruben-
stein and goodenough, 1965) and rw (luong et al.,
2013). for completeness, we also show results for
mc30, the validation set.

the word analogy task proposed in (mikolov et
al., 2013b) consists of questions like,    a is to b as c is
to ?   . the dataset contains 19,544 such questions,
divided into a semantic subset of size 8869 and a
syntactic subset of size 10,675.

table 2 gives results on similarity and anal-
ogy. numbers in parentheses are line numbers in
what follows. block    ind-full    (1-5) lists the perfor-
mance of individual embedding sets on the full vo-
cabulary. results on lines 6-34 are for the intersec-
tion of the vocabularies of the    ve embedding sets:
   ind-overlap    contains the performance of individ-
ual embedding sets,    ensemble    the performance of
our four ensemble methods and    discard    the perfor-
mance when one component set is removed.

the four ensemble approaches are very promis-
ing (31-34). for conc, discarding hlbl, huang
or cw does not hurt performance: conc (31),
conc(-hlbl)
(12) and
conc(-cw) (14) beat each individual embedding
set (6-10) in all tasks. glove contributes most in
siid113x-999, ws353, mc30 and rg; id97
contributes most in rw and word analogy tasks.

(11), conc(-huang)

svd (32) reduces the dimensionality of conc
from 950 to 200, but still gains performance in
siid113x-999 and rg. glove contributes most in
svd (larger losses on line 18 vs. lines 16-17, 19-
20). other embeddings contribute inconsistently.

1ton performs well only on word analogy, but it
gains great improvement when discarding cw em-
beddings (24). 1ton+ performs better than 1ton:
it has stronger results when considering all embed-
ding sets, and can still outperform individual embed-
ding sets while discarding hlbl (26), huang (27)
or cw (29).

these results demonstrate that ensemble methods
using multiple embedding sets produce stronger em-
beddings. however, it does not mean the more em-
bedding sets the better. whether an embedding set
helps, depends on the complementarity among the
sets as well as how we measure the ensemble results.

rg

rw

syn.

sem.

l
l
u
f
-
d
n
i

(0) 74.4

(0) 70.1

(21) 81.4

a
l
r
e
v
o
-
d
n
i

sl999 ws353 mc30
tot.
model
22.1 (1) 35.7 (3) 41.5 (0) 35.2 (1) 19.1 (892) 27.1 (423) 22.8 (198) 24.7
1 hlbl
9.7 (3) 61.7 (18) 65.9 (0) 63.0 (0) 6.4 (982) 8.4 (1016) 11.9 (326) 10.4
2 huang
45.3 (0) 75.4 (18) 83.6 (0) 82.9 (0) 48.7
(0) 75.2
3 glove
15.6 (1) 28.4 (3) 21.7 (0) 29.9 (1) 15.3 (896) 17.4 (423) 5.0 (198) 10.5
4 cw
44.2 (0) 69.8 (0) 78.9 (0) 76.1 (0) 53.4 (209) 77.1
(0) 75.6
5 w2v
22.3 (3) 34.8 (21) 41.5 (0) 35.2 (1) 22.2 (1212) 13.8 (8486) 15.4 (1859) 15.4
p 6 hlbl
9.7 (3) 62.0 (21) 65.9 (0) 64.1 (1) 3.9 (1212) 27.9 (8486) 9.9 (1859) 10.7
7 huang
45.0 (3) 75.5 (21) 83.6 (0) 82.4 (1) 59.1 (1212) 91.1 (8486) 68.2 (1859) 69.2
8 glove
16.0 (3) 30.8 (21) 21.7 (0) 29.9 (1) 17.4 (1212) 11.2 (8486) 2.3 (1859) 2.7
9 cw
44.1 (3) 69.3 (21) 78.9 (0) 75.4 (1) 61.5 (1212) 89.3 (8486) 72.6 (1859) 73.3
10 w2v
46.0 (3) 76.5 (21) 86.3 (0) 82.5 (1) 63.0 (1211) 93.2 (8486) 74.0 (1859) 74.8
11 conc (-hlbl)
12 conc (-huang) 46.1 (3) 76.5 (21) 86.3 (0) 82.5 (1) 62.9 (1212) 93.2 (8486) 74.0 (1859) 74.8
44.0 (3) 69.4 (21) 79.1 (0) 75.6 (1) 61.5 (1212) 89.3 (8486) 72.7 (1859) 73.4
13 conc (-glove)
46.0 (3) 76.5 (21) 86.6 (0) 82.5 (1) 62.9 (1212) 93.2 (8486) 73.9 (1859) 74.7
14 conc (-cw)
45.0 (3) 75.5 (21) 83.6 (0) 82.4 (1) 59.1 (1212) 90.9 (8486) 68.3 (1859) 69.2
15 conc (-w2v)
48.5 (3) 76.1 (21) 85.6 (0) 82.7 (1) 61.5 (1211) 90.6 (8486) 69.5 (1859) 70.4
16 svd (-hlbl)
48.8 (3) 76.5 (21) 85.4 (0) 83.5 (1) 61.7 (1212) 91.4 (8486) 69.8 (1859) 70.7
17 svd (-huang)
46.2 (3) 66.9 (21) 81.6 (0) 78.6 (1) 59.1 (1212) 88.8 (8486) 67.3 (1859) 68.2
18 svd (-glove)
48.5 (3) 76.1 (21) 85.7 (0) 82.7 (1) 61.5 (1212) 90.6 (8486) 69.5 (1859) 70.4
19 svd (-cw)
49.4 (3) 79.0 (21) 87.3 (0) 80.7 (1) 59.1 (1212) 90.3 (8486) 66.0 (1859) 67.1
20 svd (-w2v)
46.3 (3) 75.5 (21) 82.4 (0) 81.0 (1) 60.1 (1211) 91.9 (8486) 75.9 (1859) 76.5
21 1ton (-hlbl)
46.5 (3) 75.4 (21) 82.4 (0) 82.3 (1) 60.2 (1212) 93.5 (8486) 76.3 (1859) 77.0
22 1ton (-huang)
43.4 (3) 66.5 (21) 76.5 (0) 75.3 (1) 56.5 (1212) 89.0 (8486) 73.8 (1859) 74.5
23 1ton (-glove)
47.4 (3) 76.5 (21) 84.8 (0) 83.4 (1) 62.0 (1212) 91.4 (8486) 73.1 (1859) 73.8
24 1ton (-cw)
46.3 (3) 75.9 (21) 80.1 (0) 78.6 (1) 56.8 (1212) 92.2 (8486) 72.2 (1859) 73.0
25 1ton (-w2v)
26 1ton+ (-hlbl) 46.1 (3) 75.8 (21) 85.5 (0) 83.3 (1) 62.3 (1211) 92.2 (8486) 76.2 (1859) 76.9
27 1ton+ (-huang) 46.2 (3) 76.1 (21) 86.3 (0) 83.3 (1) 62.2 (1212) 93.8 (8486) 76.1 (1859) 76.8
28 1ton+ (-glove) 45.3 (3) 71.2 (21) 80.0 (0) 75.7 (1) 62.5 (1212) 90.0 (8486) 73.3 (1859) 74.0
46.9 (3) 78.1 (21) 85.5 (0) 83.9 (1) 62.7 (1212) 91.8 (8486) 73.3 (1859) 74.1
29 1ton+ (-cw)
45.8 (3) 76.2 (21) 84.4 (0) 83.1 (1) 60.9 (1212) 92.4 (8486) 72.4 (1859) 73.2
30 1ton+ (-w2v)
46.0 (3) 76.5 (21) 86.3 (0) 82.5 (1) 62.9 (1212) 93.2 (8486) 74.0 (1859) 74.8
48.5 (3) 76.0 (21) 85.7 (0) 82.7 (1) 61.5 (1212) 90.6 (8486) 69.5 (1859) 70.4
46.4 (3) 74.5 (21) 80.7 (0) 80.7 (1) 60.1 (1212) 91.9 (8486) 76.1 (1859) 76.8
46.3 (3) 75.3 (21) 85.2 (0) 82.7 (1) 61.6 (1212) 92.5 (8486) 76.3 (1859) 77.0

e 31 conc
l
b
m
e
s
n
e

32 svd
33 1ton
34 1ton+

d
r
a
c
s
i
d

table 2: results on    ve word similarity tasks and analogical reasoning. the number of oovs is given in parentheses for each result.
   ind-full/ind-overlap   : individual embedding sets with respective full/overlapping vocabulary;    ensemble   : ensemble results using
all    ve embedding sets;    discard   : one of the    ve embedding sets is removed. if a result is better than all methods in    ind-overlap   ,
then it is bolded.

rw(21)

semantic

syntactic

total

d
n
i

hlbl
huang
cw

6.9 17.3
7.4
4.4
4.3 6.4
7.1 10.6 17.3
e conc 14.2 16.5 48.3
l
12.4 15.7 47.9
b
m
16.7 11.7 48.5
e
s
n
   
e

rnd avg ml 1ton+ rnd avg ml 1ton+ rnd avg ml 1ton+ rnd avg ml 1ton+
24.5
16.2
11.4
   
   
   
81.1

26.4 22.4 22.4 22.7
4.1 10.9
22.0
18.4
5.0 5.0
    62.4 15.1 74.9
    54.3 13.6 70.1
    60.0 15.0 76.8
   

22.9 24.1 24.2 24.4
11.4
3.3 15.8
5.5 10.5 10.5 10.3
    36.2 16.3 81.0
    31.5 15.4 77.9
    34.7 16.1 82.0
   

17.5 26.3 26.4 26.3
6.4
2.7 21.8
17.7 17.2 17.2 16.7
4.6 18.0 88.1
4.1 17.5 87.3
4.2 17.6 88.2
   

svd
1ton
1ton+

   
   
   
48.8

7.7
4.9

88.4

76.3

1.2

4.8

   

   

   

   

   

   

   

   

table 3: comparison of effectiveness of four methods for learning oov embeddings. rnd: random initialization. avg: average
of embeddings of known words. ml: mutuallearning. rw(21) means there are still 21 oovs for the vocabulary union.

conc, the simplest ensemble, has robust perfor-
mance. however, using embeddings of size 950 as
input may mean too many parameters to tune for
deep learning. the other three methods     svd,
1ton, 1ton+     all have the advantage of smaller
dimensionality. svd reduces conc   s dimension-
ality dramatically and still keeps competitive per-
formance, especially on word similarity. 1ton is
competitive on analogy, but weak on word similar-
ity. 1ton+ performs consistently strongly on word
similarity and analogy.

not all state-of-the-art results are included in ta-
ble 2. one reason is that a fair comparison is
only possible on the shared vocabulary, so methods
without released embeddings cannot be included.
however, glove and id97 are widely recog-
nized as state-of-the-art embeddings.
in any case,
our main contribution is to present ensemble frame-
works which show that a combination of comple-
mentary embedding sets produces better-performing
meta-embeddings.

system comparison of learning oov embed-
dings.
in table 3, we extend the vocabularies of
each individual embedding set (   ind    block) and our
ensemble approaches (   ensemble    block) to the vo-
cabulary union, reporting results on rw and anal-
ogy     these tasks contain the most oovs. as
both id97 and glove have full coverage on
analogy, we do not rereport them in this table.
for each embedding set, we can compute the rep-
resentation of an oov (i) as a randomly initial-
ized vector (rnd); (ii) as the average of embed-
dings of all known words (avg); (iii) by mutu-
allearning (ml) and (iv) by 1ton+. 1ton+
learns oov embeddings for individual embedding
sets and meta-embeddings simultaneously, and it
would not make sense to replace these oov embed-
dings computed by 1ton+ with embeddings com-
puted by    rnd/avg/ml   . hence, we do not report
   rnd/avg/ml    results for 1ton+.

table 3 shows four interesting aspects. (i) mutu-
allearning helps much if an embedding set has
lots of oovs in certain task; e.g., mutuallearn-
ing is much better than avg and rnd on rw, and
outperforms rnd considerably for conc, svd
and 1ton on analogy. however, it cannot make big
difference for hlbl/cw on analogy, probably be-
cause these two embedding sets have much fewer

oovs, in which case avg and rnd work well
enough. (ii) avg produces bad results for conc,
svd and 1ton on analogy, especially in the syn-
tactic subtask. we notice that those systems have
large numbers of oovs in word analogy task. if for
analogy    a is to b as c is to d   , all four of a, b, c, d
are oovs, then they are represented with the same
average vector. hence, similarity between b     a + c
and each oov is 1.0. in this case, it is almost im-
possible to predict the correct answer d. unfortu-
nately, methods conc, svd and 1ton have many
oovs, resulting in the low numbers in table 3. (iii)
mutuallearning learns very effective embed-
dings for oovs. conc-ml, 1ton-ml and svd-
ml all get better results than id97 and glove
on analogy (e.g., for semantic analogy: 88.1, 87.3,
88.2 vs. 81.4 for glove). considering further their
bigger vocabulary, these ensemble methods are very
strong representation learning algorithms. (iv) the
performance of 1ton+ for learning embeddings for
oovs is competitive with mutuallearning. for
hlbl/huang/cw, 1ton+ performs slightly better
than mutuallearning in all four metrics. com-
paring 1ton-ml with 1ton+, 1ton+ is better than
   ml    on rw and semantic task, while performing
worse on syntactic task.

figure 4 shows the in   uence of dimensionality
d for svd, 1ton and 1ton+. peak performance
for different data sets and methods is reached for
d     [100, 500]. there are no big differences in
the averages across data sets and methods for high
enough d, roughly in the interval [150, 500]. in sum-
mary, as long as d is chosen to be large enough (e.g.,
    150), performance is robust.

we will release the meta-embeddings produced
by methods svd, 1ton and 1ton+ for d = 200
and also the meta-embeddings for method conc.

6.2 id20 for id52
this section evaluates individual embedding sets
and meta-embeddings on id52. flors
(schnabel and sch  utze, 2014), the best perform-
ing pos tagger for unsupervised domain adapta-
tion, acts as testbed, in which id52 is a
window-based, multilabel classi   cation problem us-
ing a linear id166. a word   s representation consists
of four feature vectors based on suf   x, shape, left
and right distributional neighbors respectively. we

(a) performance vs. d of svd

(b) performance vs. d of 1ton

(c) performance vs. d of 1ton+

figure 4: in   uence of dimensionality

wsj

emails

reviews

answers

weblogs

newsgroups
all oov all oov all oov all oov all oov all oov
88.66 54.73 90.40 56.75 93.33 74.17 88.55 48.32 88.14 58.09 95.76 88.30
tnt
89.11 56.02 91.43 58.66 94.15 77.13 88.92 49.30 88.68 58.42 96.83 90.25
stanford
89.14 53.82 91.30 54.20 94.21 76.44 88.96 47.25 88.64 56.37 96.63 87.96
id166tool
c&p
89.51 57.23 91.58 59.67 94.41 78.46 89.08 48.46 88.74 58.62 96.78 88.65
90.86 66.42 92.95 75.29 94.71 83.64 90.30 62.15 89.44 62.61 96.59 90.37
flors
flors+hlbl 90.01 62.64 92.54 74.19 94.19 79.55 90.25 62.06 89.33 62.32 96.53 91.03
flors+huang
90.68 68.53 92.86 77.88 94.71 84.66 90.62 65.04 89.62 64.46 96.65 91.69
90.99 70.64 92.84 78.19 94.69 86.16 90.54 65.16 89.75 65.61 96.65 92.03
flors+glove
90.37 69.31 92.56 77.65 94.62 84.82 90.23 64.97 89.32 65.75 96.58 91.36
flors+cw
flors+w2v
90.72 72.74 92.50 77.65 94.75 86.69 90.26 64.91 89.19 63.75 96.40 91.03
flors+conc 91.87 72.64 92.92 78.34 95.37 86.69 90.69 65.77 89.94 66.90 97.31 92.69
90.98 70.94 92.47 77.88 94.50 86.49 90.75 64.85 89.88 65.99 96.42 90.36
flors+svd
flors+1ton 91.53 72.84 93.58 78.19 95.65 87.62 91.36 65.36 90.31 66.48 97.66 92.86
flors+1ton+ 91.86 73.36 93.14 78.77 95.65 87.29 91.73 66.28 90.53 66.72 97.75 92.55

s
e
n
i
l
e
s
a
b

v
i
d
n
i
+

a
t
e
m
+

table 4: id52 results on six target domains.    baselines    lists representative systems for this task, including flors.    +indiv
/ +meta   : flors with individual embedding set / meta-embeddings. bold means higher than    baselines    and    +indiv   .

insert word   s embedding as the    fth feature vec-
tor. all embedding sets (except for 1ton+) are ex-
tended to the union vocabulary by mutuallearn-
ing. we follow schnabel and sch  utze (2014) for all
id171 and also train on sections 2-21 of
wall street journal (wsj) and evaluate on the de-
velopment sets of six different target domains:    ve
sancl (petrov and mcdonald, 2012) domains    
newsgroups, weblogs, reviews, answers, emails    
and sections 22-23 of wsj for in-domain testing.

table 4 gives results for some representative sys-
tems (   baselines   ), flors with individual em-
bedding sets (   +indiv   ) and flors with meta-
embeddings (   +meta   ). following conclusions can
be drawn.
(i) not all individual embedding sets
are bene   cial in this task; e.g., hlbl embeddings
make flors perform worse in 11 out of 12 cases.
(ii) however, in most cases, embeddings improve

system performance, which is consistent with prior
work on using embeddings for this type of task
(xiao and guo, 2013; yang and eisenstein, 2014;
tsuboi, 2014).
(iii) meta-embeddings generally
help more than the individual embedding sets, ex-
cept for svd (which only performs better in 3 out
of 12 cases).

7 conclusion

this work presented four ensemble methods    
conc, svd, 1ton and 1ton+     for learning
meta-embeddings from multiple embedding sets.
experiments on word similarity, word analogy and
id52 indicated the high quality of these
meta-embeddings. the ensemble methods have the
added advantage of increasing vocabulary coverage.
we will release the meta-embeddings.

501001502002503003504004505005560657075808590dimension of svdperformance (%)  wc353mcrgscwsrwdimension of o2m50100150200250300350400450500performance (%)5055606570758085wc353mcrgscwsrwdimension of o2m+50100150200250300350400450500performance (%)505560657075808590wc353mcrgscwsrwreferences
mohit bansal, kevin gimpel, and karen livescu. 2014.
tailoring continuous word representations for depen-
dency parsing. in proceedings of the annual meeting
of the association for computational linguistics.

yoshua bengio, r  ejean ducharme, pascal vincent, and
christian janvin. 2003. a neural probabilistic lan-
guage model. the journal of machine learning re-
search, 3:1137   1155.

yanqing chen, bryan perozzi, rami al-rfou, and steven
skiena. 2013. the expressive power of word embed-
dings. in icml workshop on deep learning for au-
dio, speech, and language processing.

ronan collobert and jason weston. 2008. a uni   ed ar-
chitecture for natural language processing: deep neu-
ral networks with multitask learning. in proceedings
of the 25th international conference on machine learn-
ing, pages 160   167. acm.

john duchi, elad hazan, and yoram singer.

2011.
adaptive subgradient methods for online learning and
the journal of machine
stochastic optimization.
learning research, 12:2121   2159.

lev finkelstein, evgeniy gabrilovich, yossi matias,
ehud rivlin, zach solan, gadi wolfman, and eytan
ruppin. 2001. placing search in context: the con-
cept revisited. in proceedings of the 10th international
conference on world wide web, pages 406   414. acm.
felix hill, kyunghyun cho, sebastien jean, coline
devin, and yoshua bengio. 2014. not all neural em-
beddings are born equal. in nips workshop on learn-
ing semantics.

felix hill, kyunghyun cho, sebastien jean, coline
devin, and yoshua bengio.
embedding
word similarity with id4. iclr
workshop.

2015a.

felix hill, roi reichart, and anna korhonen. 2015b.
siid113x-999: evaluating semantic models with (gen-
uine) similarity estimation. computational linguis-
tics.

eric h huang, richard socher, christopher d manning,
and andrew y ng. 2012. improving word representa-
tions via global context and multiple word prototypes.
in proceedings of the 50th annual meeting of the asso-
ciation for computational linguistics: long papers-
volume 1, pages 873   882. association for computa-
tional linguistics.

quoc v le and tomas mikolov. 2014. distributed rep-
resentations of sentences and documents. in proceed-
ings of the 31st international conference on machine
learning.

yong luo, jian tang, jun yan, chao xu, and zheng
chen. 2014. pre-trained multi-view word embed-
ding using two-side neural network. in twenty-eighth
aaai conference on arti   cial intelligence.

minh-thang luong, richard socher, and christopher d
manning. 2013. better word representations with
id56s for morphology. conll-
2013, 104.

tomas mikolov, martin kara     at, lukas burget, jan
cernock`y, and sanjeev khudanpur.
re-
current neural network based language model.
in
interspeech 2010, 11th annual conference of
the international speech communication association,
makuhari, chiba, japan, september 26-30, 2010,
pages 1045   1048.

2010.

tomas mikolov, kai chen, greg corrado, and jeffrey
dean. 2013a. ef   cient estimation of word representa-
tions in vector space. in proceedings of workshop at
iclr.

tomas mikolov, ilya sutskever, kai chen, greg s cor-
rado, and jeff dean. 2013b. distributed represen-
tations of words and phrases and their composition-
ality. in advances in neural information processing
systems, pages 3111   3119.

tom  a  s mikolov. 2012. statistical language models based
on neural networks. presentation at google, mountain
view, 2nd april.

george a miller and walter g charles. 1991. contex-
tual correlates of semantic similarity. language and
cognitive processes, 6(1):1   28.

andriy mnih and geoffrey e hinton. 2009. a scalable
hierarchical distributed language model. in advances
in neural information processing systems, pages 1081   
1088.

jeffrey pennington, richard socher, and christopher d
manning. 2014. glove: global vectors for word rep-
resentation. proceedings of the empiricial methods in
natural language processing (emnlp 2014), 12.

slav petrov and ryan mcdonald. 2012. overview of
in notes
the 2012 shared task on parsing the web.
of the first workshop on syntactic analysis of non-
canonical language (sancl), volume 59.

tim rockt  aschel, edward grefenstette, karl moritz her-
mann, tom  a  s ko  cisk`y, and phil blunsom. 2015. rea-
soning about entailment with neural attention. arxiv
preprint arxiv:1509.06664.

herbert rubenstein and john b goodenough.

1965.
contextual correlates of synonymy. communications
of the acm, 8(10):627   633.

tobias schnabel and hinrich sch  utze. 2014. flors: fast
and simple id20 for part-of-speech tag-
ging. transactions of the association for computa-
tional linguistics, 2:15   26.

ilya sutskever, oriol vinyals, and quoc vv le. 2014.
sequence to sequence learning with neural networks.
in advances in neural information processing systems,
pages 3104   3112.

yuta tsuboi. 2014. neural networks leverage corpus-
wide information for part-of-speech tagging. in pro-
ceedings of the 2014 conference on empirical meth-
ods in natural language processing (emnlp), pages
938   950.

joseph turian, lev ratinov, and yoshua bengio. 2010.
word representations: a simple and general method for
semi-supervised learning. in proceedings of the 48th
annual meeting of the association for computational
linguistics, pages 384   394. association for computa-
tional linguistics.

min xiao and yuhong guo. 2013. id20
for sequence labeling tasks with a probabilistic lan-
guage adaptation model. in proceedings of the 30th
international conference on machine learning, pages
293   301.

yi yang and jacob eisenstein. 2014. unsupervised do-
main adaptation with feature embeddings. iclr work-
shop.

