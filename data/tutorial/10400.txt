empirical study on deep learning models for qa

yang yu wei zhang chung-wei hang bing xiang bowen zhou

{yu, zhangwei, hangc, bingxia, zhou}@us.ibm.com

ibm watson

5
1
0
2

 

v
o
n
0
2

 

 
 
]
l
c
.
s
c
[
 
 

3
v
6
2
5
7
0

.

0
1
5
1
:
v
i
x
r
a

abstract

in this paper we explore deep learning models with memory component or at-
tention mechanism for id53 task. we combine and compare three
models, id4 [1], id63 [5], and mem-
ory networks [15] for a simulated qa data set [14]. this paper is the    rst one
that uses id4 and id63s for solving
qa tasks. our results suggest that the combination of attention and memory have
potential to solve certain qa problem.

1

introduction

id53 (qa) is a natural language processing (nlp) task that requires deep under-
standing of semantic abstraction and reasoning over facts that are relevant to a question [6]. there
are many different approaches to qa: constructing nlp pipeline where each component is sepa-
rately trained and then assembled [4], building large knowledge bases (kbs) [2] and reasoning with
facts therein, and machine    reading    approach to comprehend question and documents [6] where
answers are contained. recently, various deep learning (dl) models are proposed for different
learning problems. dl models are usually differentiable from end to end through id119.
they require neither any hand craft features nor separately tuned components. thus we think it is
important to study these models on addressing qa problem.
implicitly or explicitly, solving qa problem can be divided into two steps. the    rst step locates the
information that is relevant to the question, e.g. sentences in a text document or facts in a knowledge
graph. we call it the    search step.    the second step which we call    generation step   , extracts or
generates answer from the relevant pieces of information detected in the search step. this paper
focuses on reading comprehension type qa, where search and generation sometimes are coupled.
we focus on id4 (id4), memory network (memnn), and neural turing
machine (ntm) models, as they have representative state-of-the-art dl model architectures in cat-
egories they fall in. we conducted empirical studies in this work to better understand the strength
and places to improve in each model on solving qa and experiment settings follow the 2-step qa
framework. we will brie   y describe the id4, ntm and memnn in the next section.

2 deep learning models for id53

memnn memnns have been applied to qa [15, 11] and have shown promising results with dif-
ferent input transformation or model changes. its strength mainly lies in reasoning with id136
components combined with a long-term memory component and learning how to use these jointly.
a general memnn has four modules: input which converts the incoming input to the internal feature
representation, output which produces a new output, generalize which updates old memories given
the new input, and response which converts the output into the response format desired. the mem-
ory network described in [15] memorizes each fact in a memory slot and uses supporting facts for

1

(a) id4 model

(b) ntm model

figure 1: ntm and id4 models.

a given question labeled in the training data to learn to search facts. sainbayar et al. [11] described
another version of the memnn that can be trained end-to-end without knowing the supporting facts.
id4 using machine translation (mt) technique for qa is not new, as generating answer by given
question can be regarded as generating target text by given source text, or in other words, the answer
is a translation of the question. several previous works have used translation models to determine
answers [3, 12]. id4 brings new approaches to machine translation, for example two recurrent
neural network (id56) models are proposed [7, 13]. following previous success of applying mt
for qa, we think it is important to study if id4 could further help qa. as to the best of our
knowledge, no one has done this study yet. traditional translation systems usually are phrase-based
where small sub-components are tuned separately (e.g., koehn et al. [8]). id4 improves over
phrase-based systems by using a neural network to encode a sentence in source language and decode
it into a sentence in target language, which is a end-to-end system. however a main constraint
of this encoder-decoder approach is that the model needs to compress all information of a source
sentence into a    xed-length vector, which is dif   cult for long sentences or passages for reading
comprehension style qa. in order to address this issue, bahdanau et al. [1] introduced an extension
to the encoder-decoder model which uses bi-directional id56 and learns to align and translate jointly.
the id4 model we use is shown in the figure 1a. the input includes passage and question and
they are delimited by a marker. from the    gure, we see the two id56s read word by word in the
input of different directions. each time when the model generates an answer word, it searches for
multiple positions in the passage where the most relevant information is concentrated. the model
then predicts an answer word based on the context vectors associated with these positions and all
the previous generated answer words. formally, the i-th answer word yi (equation 1) is conditioned
on the words in answer before word yi and the passage x. in id56, the id155 is
modeled as a nonlinear function g() which depends on previous answer word yi   1, the id56 hidden
state si for time i and the context ci. the context vector ci is a weighted sum of a sequence of
annotations. each annotation has information about the complete passage with a focus on the parts
around the i-th word.

p(yi|y1, ..., yi   1, x) = g(yi   1, si, ci) = g(yi   1, f (si   1, yi   1, ci), ci)

(1)

ntm ntm [5] resembles turing machines in that it could learn arbitrary procedure in theory. as
we believe that qa problem can be solved with (probably sophisticated) programs, in this paper we
would examine how well ntm performs on reading comprehension tasks. ntms are essentially
id56s, which in turn are turing-complete [10] and capable of encoding any computer program in
theory, yet not always practical. ntm is end-to-end trainable with id119, due to the fact
that every component is differentiable that is enabled by converting the hard read and write into
   blurry    operations that interact to greater or lesser degree. the read and write heads, as well as the
memory component are recurrently updated through time, no matter if the controller is recurrent or
not. this paper is the    rst to examine id63s on qa problems. our implementation
of ntm (see figure 1b) internally uses a single-layer lstm network as controller. ntm inputs are
word distributed representations [9]. id27 is directed to lstm controller within ntm,
and the output of ntm is generated from softmax layer where each bit of output corresponds to an
answer. in doing so we regard the qa problem as a multi-class (multi-word) classi   cation problem

2

table 1: ai-complete 20 tasks results.

(i) memory network

(ii) support fact only

(iii) sup. fact highlighted

(iv) combination

(v) no support fact

memnn memnn-s memnn-r

memnn-s+id4

id4(10k)

task
1- 1 supporting fact
2- 2 supporting facts
3- 3 supporting facts
4- two arg. relations
5- three arg. relations
6- yes/no questions
7- counting
8- lists/sets
9- simple negation
10- indef. knowledge
11- basic coreference
12- conjunction
13- comp. coreference
14- time reasoning
15- basic deduction
16- basic induction
17- pos. reasoning
18- size reasoning
19- path finding
20- agents motivations
mean performance

a

100
100
100
69
83
52
78
90
71
57
100
100
100
100
73
100
46
50
9
100
78.9

b

100
66.4
49.3
66.9
98.5
100
58.3
n/a
100
100
79.5
100
89
99.9
100
92.8
100
27.2
30.7
100
82.0

c

100
100
100
100
82.5
49.4
69.5
n/a
63.6
49.3
55.5
100
49.5
100
100
100
46.7
52.9
24.6
100
76.0

d

id4
100
100
100
99.1
99.3
100
98.5
99
100
98.9
100
100
100
99.8
100
100
64.2
97.8
80.7
100
96.9

e

ntm
100
100
100
100
79.2
100
100
100
100
94.6
100
100
100
100
100
100
69.3
93
100
100
96.7

f

id4
100
99.6
99.5
97.5
90.6
99.8
96.6
92.7
99.7
96.8
100
100
100
97.5
92.7
88.1
58
91.8
29.7
93.3
91.2

g

ntm
100
100
100
100
73.7
100
100
98
100
85.9
100
100
100
100
100
100
61.2
93
100
100
95.6

h

100
69.7
60.4
66.5
98.1
100
44.8
n/a
100
98.9
84.4
100
91.7
99.8
100
96.1
56.9
58.3
16.9
100
81.2

i

id4
98.2
41.3
33.4
97.8
90.3
84.6
82.4
70.8
89.3
73.5
99.8
99.4
99.7
44.4
42.9
42.7
64.6
90.9
9.3
91.6
72.3

j

99.6
98.4
97.1
99.8
91.7
78.9
93.7
55.2
86.6
79.5
97.9
98.9
99.1
96.7
96
52.7
89.5
96.5
44.2
91.7
87.2

given that the number of answers is    nite and small. when multiple answers are needed, top n words
with top n probabilities will be selected.

3 experiments

the ai-complete data [14] is a synthetic dataset designed for a set of ai tasks. for each task, small
and large training sets have 1k and 10k questions, respectively, and test set has 1k questions. the
supervision in the training set is given by the true answers to questions, and the set of supporting facts
for answering a given question. all tasks are so clean that a human could achieve 100% accuracy,
as they are generated with a simulation. in this data, every statement in a passage has a number id
and each question is associated with the true answer and the ids of supporting facts.
memnn [15] has shown promising results on ai-complete data. so    rst we want to learn how
different memnn components contribute to it. we choose the memnn with adaptive memory [14]
as baseline (column a). we divide it into two virtual steps as the 2-step qa framework. we believe
searching supporting facts (memnn-s) is very important to memnn while the memnn response
module (memnn-r) has more room to improve, because it does not have any attention mechanism
to focus on words in retrieved facts. we train memnn-s by using complete passage as input and
supporting facts as prediction and train memnn-r by using true supporting facts as input and    nal
answer as prediction. column b,c in table 1 con   rm our hypothesis that the accuracy of searching
supporting facts from memnn-s is much better than the accuracy of predicting    nal answer words
from memnn-r.
following our    ndings in the    rst experiment, we want to see if id4 or ntm could do better than
memnn-r. using the same setting as testing memnn-r, both id4 and ntm show almost perfect
results (column d,e). as we analyzed, we think the main reason is that id4   s attention machanism
and ntm   s memory component help. then we wonder if the supporting facts input is not perfect,
for example if the search step can only mark some facts as supporting facts with high id203.
therefore in this experiment, we input the complete passage including non-supporting facts, and
use markers to annotate the begin and end of the supporting facts in the passage. including non-
supporting facts in input brings noise and it requires the model to memorize relevant information
and ignore non-relevant ones. from the results in group (iii) we see that id4 and ntm both
perform very good with just a little expected drop from using supporting facts only. ntm is shown
to be better in that ntm   s explicit memory component and content addressing mechanism could
directly target the relevant information stored.
although id4 and ntm showed good capability of solving the generation step in experiments
above. supporting facts still need to be identi   ed before the models could be applied. as we ana-
lyzed above, memnn-s is good at searching supporting facts. thus, we use memnn-s to generate
facts for those models and then apply id4 based on its fact-searching results. we see this combina-
tion (column h) improves the average performance over baseline (column a). it proves that this novel

3

architectural combination fusees the advantage of each, i.e. the memory component and attention
mechanism.
one major advantage of applying dl models to solve qa is that they can be learned/tuned end-to-
end. therefore the combination of two models through separate training may be less advantageous
than single model that has both key elements. as we analyzed, id4   s architecture is essentially
an attention mechanism over bidirectional id56 which has some memory functionality. thus we
wonder how id4 single model would perform compared to architectural combination shown above.
in this group of experiments, we use id4 model for end-to-end qa without the medium step
of    nding supporting facts to compare with previous architectural combination. that being said,
supporting facts are not marked up in both training and testing which reduced a lot information that
model can use to learn. so it requires that attention on supporting facts need to be learned as an
implicit byproduct by the id4 attention mechanism. we run id4 on both small and large training
sets. the experiment results (column i,j) show that it got 72% without any tuning or specialization
on qa problem. this result on small training set is within a reasonable gap to previous architectural
combination, considering the model does not use supporting facts at all. furthermore, when training
data is suf   cient, the accuracy is even comparable to the state-of-the-art accuracy (92%) from [11]
where specialized features are added and tuned speci   cally for this data. in sum, we think the id4
has potential to address certain qa problem, as it has both memory supported in id56 and the
attention over the memory.

4 conclusions

we studied several state-of-the-art dl models proposed for different learning tasks on solving qa
problem. experiment results suggest that a good agent need to remember and forget some facts
when necessary and external memory may be a choice. we are also convinced that to generate
answer, appropriate attention mechanism can do well. therefore, we believe a dl model combining
memory and attention mechanism great potential on handling qa problem.

references
[1] d. bahdanau, k. cho, and y. bengio. id4 by jointly learning to align

and translate. arxiv preprint arxiv:1409.0473, 2014.

[2] a. bordes, n. usunier, s. chopra, and j. weston. large-scale simple id53 with

memory networks. arxiv preprint arxiv:1506.02075, 2015.

[3] h. cui, r. sun, k. li, m.-y. kan, and t.-s. chua. id53 passage retrieval using
dependency relations. in proceedings of the 28th annual international acm sigir conference
on research and development in information retrieval, pages 400   407, new york, ny, usa,
2005. acm.

[4] d. a. ferrucci, e. w. brown, j. chu-carroll, j. fan, d. gondek, a. kalyanpur, a. lally,
j. w. murdock, e. nyberg, j. m. prager, n. schlaefer, and c. a. welty. building watson: an
overview of the deepqa project. ai magazine, 31(3):59   79, 2010.

[5] a. graves, g. wayne, and i. danihelka.

arxiv:1410.5401, 2014.

neural

turing machines.

arxiv preprint

[6] k. m. hermann, t. kocisk  y, e. grefenstette, l. espeholt, w. kay, m. suleyman, and p. blun-

som. teaching machines to read and comprehend. corr, abs/1506.03340, 2015.

[7] n. kalchbrenner and p. blunsom. recurrent continuous translation models. in emnlp 2013,
pages 1700   1709, seattle, washington, usa, october 2013. association for computational
linguistics.

[8] p. koehn, f. j. och, and d. marcu. statistical phrase-based translation. in proceedings of the
human language technology conference of the north american chapter of the association
for computational linguistics, pages 48   54, stroudsburg, pa, usa, 2003. association for
computational linguistics.

[9] t. mikolov, i. sutskever, k. chen, g. s. corrado, and j. dean. distributed representations of
words and phrases and their compositionality. in advances in neural information processing
systems, pages 3111   3119. 2013.

4

[10] h. t. siegelmann and e. d. sontag. on the computational power of neural nets. journal of

computer and system sciences, 50(1):132   150, 1995.

[11] s. sukhbaatar, a. szlam, j. weston, and r. fergus. end-to-end memory networks. arxiv

preprint arxiv:1503.08895, 2015.

[12] m. surdeanu, m. ciaramita, and h. zaragoza. learning to rank answers to non-factoid ques-

tions from web collections. computational linguistics, 37(2):351   383, june 2011.

[13] i. sutskever, o. vinyals, and q. v. v. le. sequence to sequence learning with neural networks.

in advances in neural information processing systems, pages 3104   3112. 2014.

[14] j. weston, a. bordes, s. chopra, and t. mikolov. towards ai-complete id53:

a set of prerequisite toy tasks. arxiv preprint arxiv:1502.05698, 2015.

[15] j. weston, s. chopra, and a. bordes. memory networks. arxiv preprint arxiv:1410.3916,

2014.

5

