text understanding from scratch

xiang zhang
yann lecun
computer science department, courant institute of mathematical sciences, new york university

xiang@cs.nyu.edu
yann@cs.nyu.edu

6
1
0
2

 
r
p
a
4

 

 
 
]

g
l
.
s
c
[
 
 

5
v
0
1
7
1
0

.

2
0
5
1
:
v
i
x
r
a

abstract

inputs all

the way up to abstract

this article demonstrates that we can apply deep
learning to text understanding from character-
level
text
concepts, using temporal convolutional net-
works(lecun et al., 1998) (convnets). we apply
convnets to various large-scale datasets, includ-
ing ontology classi   cation, id31,
and text categorization. we show that temporal
convnets can achieve astonishing performance
without the knowledge of words, phrases, sen-
tences and any other syntactic or semantic struc-
tures with regards to a human language. evi-
dence shows that our models can work for both
english and chinese.

1. introduction
text understanding consists in reading texts formed in nat-
ural languages, determining the explicit or implicit mean-
ing of each elements such as words, phrases, sentences
and paragraphs, and making id136s about the implicit
or explicit properties of these texts(norvig, 1987). this
problem has been traditionally dif   cult because of the ex-
treme variability in language formation(linell, 1982). to
date, most ways to handle text understanding, be it a hand-
crafted parsing program or a statistically learnt model, have
been resorted to the means of matching words statistics.
so far, most machine learning approaches to text under-
standing consist in tokenizing a string of characters into
structures such as words, phrases, sentences and para-
graphs, and then apply some statistical classi   cation al-
gorithm onto the statistics of such structures(soderland,
2001). these techniques work well enough when applied
to a narrowly de   ned domain, but the prior knowledge
required is not cheap     they need to pre-de   ne a dictio-

this technical report

is superseded by a paper entitled
   character-level convolutional networks for text classi   cation   ,
arxiv:1509.01626. it has considerably more experimental results
and a rewritten introduction.

nary of interested words, and the structural parser needs to
handle many special variations such as word morphologi-
cal changes and ambiguous chunking. these requirements
make text understanding more or less specialized to a par-
ticular language     if the language is changed, many things
must be engineered from scratch.
with the advancement of deep learning and availability
of large datasets, methods of handling text understand-
ing using deep learning techniques have gradually become
available. one technique which draws great interests is
id97(mikolov et al., 2013b). inspired by traditional
language models, this technique constructs representation
of words into a vector of    xed length trained under a
large corpus. based on the hope that machines may make
sense of languages in a formal fashion, many researchers
have tried to train a neural network for understanding texts
based the features extracted from it or similar techniques,
to name a few, (frome et al., 2013)(gao et al., 2013)(le &
mikolov, 2014)(mikolov et al., 2013a)(pennington et al.,
2014). most of these techniques try to apply id97 or
similar techniques with an engineered language model.
on the other hand, some researchers have also tried to train
a neural network from word level with little structural en-
gineering(collobert et al., 2011b)(kim, 2014)(johnson &
zhang, 2014)(dos santos & gatti, 2014). in these works, a
word level feature extractor such as lookup table(collobert
et al., 2011b) or id97(mikolov et al., 2013b) is used to
feed a temporal convnet(lecun et al., 1998). after train-
ing, convnets worked for both id170 tasks
such as part-of-speech tagging and named entity recogni-
tion, and text understanding tasks such as sentiment anal-
ysis and sentence classi   cation. they claim good results
for various tasks, but the datasets and models are relatively
small and there are still some engineered layers to represent
structures such as words, phrases and sentences.
in this article we show that text understanding can be han-
dled by a deep learning system without arti   cially embed-
ding knowledge about words, phrases, sentences or any
other syntactic or semantic structures associated with a lan-
guage. we apply temporal convnets(lecun et al., 1998)
to various large-scale text understanding tasks, in which the

text understanding from scratch

inputs are quantized characters and the outputs are abstract
properties of the text. our approach is one that    learns from
scratch   , in the following 2 senses

1. convnets do not require knowledge of words     work-
ing with characters is    ne. this renders a word-based
feature extractor
(such as lookuptable(collobert
et al., 2011b) or id97(mikolov et al., 2013b)) un-
necessary. all previous works start with words instead
of characters, which is dif   cult to apply a convolu-
tional layer directly due to its high dimension.

2. convnets do not require knowledge of syntax or se-
mantic structures     id136 directly to high-level tar-
gets is    ne. this also invalidates the assumption that
id170s and language models are neces-
sary for high-level text understanding.

our approach is partly inspired by convnet   s success in
id161. it has outstanding performance in various
image recognition tasks(girshick et al., 2013)(krizhevsky
et al., 2012)(sermanet et al., 2013). these successful re-
sults usually involve some end-to-end convnet model that
learns hierarchical representation from raw pixels(girshick
et al., 2013)(zeiler & fergus, 2014). similarly, we hy-
pothesize that when trained from raw characters, temporal
convnet is able to learn the hierarchical representations of
words, phrases and sentences in order to understand text.

2. convnet model design
in this section, we introduce the design of convnets for
text understanding. the design is modular, where the gra-
dients are obtained by back-propagation(rumelhart et al.,
1986) to perform optimization.

2.1. key modules

the main component in our model is the temporal convo-
lutional module, which simply computes a 1-d convolu-
tion between input and output. suppose we have a dis-
crete input function g(x)     [1, l]     r and a discrete
id81 f (x)     [1, k]     r. the convolution
h(y)     [1,(cid:98)(l     k)/d(cid:99) + 1]     r between f (x) and g(x)
with stride d is de   ned as

h(y) =

f (x)    g(y    d     x + c),

x=1

where c = k     d + 1 is an offset constant. just as in tradi-
tional convolutional networks in vision, the module is pa-
rameterized by a set of such id81s fij(x) (i =
1, 2, . . . , m and j = 1, 2, . . . , n) which we call weights, on
a set of inputs gi(x) and outputs hj(y). we call each gi
(or hj) an input (or output) frame, and m (or n) input (or

k(cid:88)

output) frame size. the outputs hj(y) is obtained by a sum
over i of the convolutions between gi(x) and fij(x).
one key module that helped us to train deeper models
is temporal max-pooling.
it is the same as spatial max-
pooling module used in id161(boureau et al.,
2010a), except that it is in 1-d. given a discrete input
function g(x)     [1, l]     r, the max-pooling function
h(y)     [1,(cid:98)(l     k)/d(cid:99) + 1]     r of g(x) is de   ned as

h(y) =

k

max
x=1

g(y    d     x + c),

where c = k     d + 1 is an offset constant. this very pool-
ing module enabled us to train convnets deeper than 6 lay-
ers, where all others fail. the analysis by (boureau et al.,
2010b) might shed some light on this.
the non-linearity used in our model is the recti   er or
thresholding function h(x) = max{0, x}, which makes
our convolutional layers similar to recti   ed linear units
(relus)(nair & hinton, 2010). we always apply this func-
tion after a convolutional or linear module, therefore we
omit its appearance in the following. the algorithm used
in training our model is stochastic id119 (sgd)
with a minibatch of size 128, using momentum(polyak,
1964)(sutskever et al., 2013) 0.9 and initial step size 0.01
which is halved every 3 epoches for 10 times. the training
method and parameters apply to all of our models. our im-
plementation is done using torch 7(collobert et al., 2011a).

2.2. character quantization

our model accepts a sequence of encoded characters as
input. the encoding is done by prescribing an alpha-
bet of size m for the input language, and then quan-
tize each character using 1-of-m encoding. then, the se-
quence of characters is transformed to a sequence of such
m sized vectors with    xed length l. any character exceed-
ing length l is ignored, and any characters that are not in
the alphabet including blank characters are quantized as
all-zero vectors.
inspired by how long-short term mem-
ory (lstm)(hochreiter & schmidhuber, 1997) work, we
quantize characters in backward order. this way, the latest
reading on characters is always placed near the beginning
of the output, making it easy for fully connected layers to
associate correlations with the latest memory. the input to
our model is then just a set of frames of length l, and the
frame size is the alphabet size m.
one interesting thing about this quantization is that visually
it is quite similar to braille(braille, 1829) used for assist-
ing blind reading, except that our encoding is more com-
pact. figure 1 depicts this fact. it seems that when trained
properly, humans can learn to read binary encoding of lan-
guages. this offers interesting insights and inspiration to
why our approach could work.

text understanding from scratch

(a) binary

(b) braille

table 1. convolutional layers used in our experiments. the con-
volutional layers do not use stride and pooling layers are all non-
overlapping ones, so we omit the description of their strides.

figure 1. comparison of our binary encoding and braille on the
text    international conference on machine learning   

layer large frame

small frame kernel

pool

the alphabet used in all of our models consists of 70 char-
acters, including 26 english letters, 10 digits, new line and
33 other characters. they include:

abcdefghijklmnopqrstuvwxyz0123456789
-,;.!?:         /\|_@#$%  &*     +-=<>()[]{}

before feeding the input to our model, no id172 is
done. this is because the input is already quite sparse by
itself, with many zeros scattered around. our models can
learn from this simple quantization without problems.

2.3. model design

we designed 2 convnets     one large and one small. they
are both 9 layers deep with 6 convolutional layers and 3
fully-connected layers, with different number of hidden
units and frame sizes. figure 2 gives an illustration.

figure 2. illustration of our model

the input have number of frames equal to 69 due to
our character quantization method, and the length of each
frame is dependent on the problem. we also insert 2
dropout(hinton et al., 2012) modules in between the 3
fully-connected layers to regularize. they have dropout
id203 of 0.5. table 1 lists the con   gurations for con-
volutional layers, and table 2 lists the con   gurations for
fully-connected (linear) layers.
before starting training the models, we randomize the
weights using gaussian distributions. the mean and stan-
dard deviation used for initializing the large model is
(0, 0.02), and small model (0, 0.05).
for different problems the input lengths are different, and
so are the frame lengths. from our model design, it is
easy to know that given input length l0, the output frame
length after the last convolutional layer (but before any of
the fully-connected layers) is l6 = (l0   96)/27. this num-
ber multiplied with the frame size at layer 6 will give the
input dimension the    rst fully-connected layer accepts.

1
2
3
4
5
6

1024
1024
1024
1024
1024
1024

256
256
256
256
256
256

7
7
3
3
3
3

3
3

n/a
n/a
n/a

3

table 2. fully-connected layers used in our experiments. the
number of output units for the last layer is determined by the prob-
lem. for example, for a 10-class classi   cation problem it will be
10.

layer output units large output units small

7
8
9

1024
2048
2048
1024
depends on the problem

2.4. data augmentation using thesaurus

many researchers have found that appropriate data aug-
mentation techniques are useful for controlling generaliza-
tion error for deep learning models. these techniques usu-
ally work well when we could    nd appropriate invariant
properties that the model should possess. for example, in
image recognition a model should have some controlled in-
variance towards changes in translating, scaling, rotating
and    ipping of the input image. similarly, in speech recog-
nition we usually augment data by adding arti   cial noise
background and changing the tone or speed of speech sig-
nal(hannun et al., 2014).
in terms of texts, it is not reasonable to augment the data
using signal transformations as done in image or speech
recognition, because the exact order of characters may form
rigorous syntactic and semantic meaning. therefore, the
best way to do data augmentation would have been using
human rephrases of sentences, but this is unrealistic and
expensive due the large volume of samples in our datasets.
as a result, the most natural choice in data augmentation
for us is to replace words or phrases with their synonyms.
we experimented data augmentation by using an english
thesaurus, which is obtained from the mytheas compo-
nent used in libreof   ce1 project. that thesaurus in turn

1http://www.libreoffice.org/

some textconvolutionsmax-poolinglengthframesquantization...conv. and pool. layersmax-poolingfully-connectedtext understanding from scratch

was obtained from id138(fellbaum, 2005), where ev-
ery synonym to a word or phrase is ranked by the semantic
closeness to the most frequently seen meaning.
to do synonym replacement for a given text, we need to
answer 2 questions: which words in the text should be re-
placed, and which synonym from the thesaurus should be
used for the replacement. to decide on the    rst question,
we extract all replaceable words from the given text and
randomly choose r of them to be replaced. the id203
of number r is determined by a geometric distribution with
parameter p in which p [r]     pr. the index s of the syn-
onym chosen given a word is also determined by a another
geometric distribution in which p [s]     qs. this way, the
id203 of a synonym chosen becomes smaller when it
moves distant from the most frequently seen meaning.
it is worth noting that models trained using our large-scale
datasets hardly require data augmentation, since their gen-
eralization errors are already pretty good. we will still re-
port the results using this new data augmentation technique
with p = 0.5 and q = 0.5.

2.5. comparison models

since we have constructed several large-scale datasets from
scratch, there is no previous publication for us to obtain a
comparison with other methods. therefore, we also imple-
mented two fairly standard models using previous methods:
the bag-of-words model, and a bag-of-centroids model via
id97(mikolov et al., 2013b).
the bag-of-words model is pretty straightforward. for each
dataset, we count how many times each word appears in the
training dataset, and choose 5000 most frequent ones as the
bag. then, we use multinomial id28 as the
classi   er for this bag of features.
as for the id97 model, we    rst ran id116 on the
word vectors learnt from google news corpus with k =
5000, and then use a bag of these centroids for multinomial
id28. this model is quite similar to the bag-
of-words model in that the number of features is also 5000.
one difference between these two models is that the fea-
tures for bag-of-words model are different for different
datasets, whereas for id97 they are the same. this
could be one reason behind the phenomenon that bag-of-
words consistently out-performs id97 in our experi-
ments. it might also be the case that the hope for linear
separability of id97 is not valid at all. that being said,
our own convnet models consistently out-perform both.

3. datasets and results
in this part we show the results obtained from various
datasets. the unfortunate fact in literature is that there is no

openly accessible dataset that is large enough or with labels
of suf   cient quality for us, although the research on text
understanding has been conducted for tens of years. there-
fore, we propose several large-scale datasets, in hopes that
text understanding can rival the success of image recog-
nition when large-scale datasets such as id163(deng
et al., 2009) became available.

3.1. dbpedia ontology classi   cation

dbpedia is a crowd-sourced community effort to extract
structured information from wikipedia(lehmann et al.,
2014). the english version of the dbpedia knowledge
base provides a consistent ontology, which is shallow and
cross-domain. it has been manually created based on the
most commonly used infoboxes within wikipedia. some
ontology classes in dbpedia contain hundreds of thousands
of samples, which are ideal candidates to construct an on-
tology classi   cation dataset.
the dbpedia ontology classi   cation dataset is constructed
by picking 14 non-overlapping classes from dbpedia 2014.
they are listed in table 3. from each of these 14 ontology
classes, we randomly choose 40,000 training samples and
5,000 testing samples. therefore, the total size of the train-
ing dataset is 560,000 and testing dataset 70,000.

table 3. dbpedia ontology classes. the numbers contain only
samples with both a title and a short abstract.

class

total

train

test

company
educational institution
artist
athlete
of   ce holder
mean of transportation
building
natural place
village
animal
plant
album
film
written work

63,058
50,450
95,505
268,104
47,417
47,473
67,788
60,091
159,977
187,587
50,585
117,683
86,486
55,174

40,000
40,000
40,000
40,000
40,000
40,000
40,000
40,000
40,000
40,000
40,000
40,000
40,000
40,000

5,000
5,000
5,000
5,000
5,000
5,000
5,000
5,000
5,000
5,000
5,000
5,000
5,000
5,000

before feeding the data to the models, we concatenate the
title and short abstract together to form a single input for
each sample. the length of input used was l0 = 1014,
therefore the frame length after last convolutional layer is
l6 = 34. using an nvidia tesla k40, training takes
about 5 hours per epoch for the large model, and 2 hours for
the small model. table 4 shows the classi   cation results.

text understanding from scratch

table 4. dbpedia results. the numbers are accuracy.
test

thesaurus

model

train

large convnet
large convnet
small convnet
small convnet
bag of words

id97

no
yes
no
yes
no
no

99.96% 98.27%
99.89% 98.40%
99.37% 98.02%
99.62% 98.15%
96.29% 96.19%
89.32% 89.09%

the results from table 4 indicate both good training and
testing errors from our models, with some improvement
from thesaurus augmentation. we believe this is a    rst ev-
idence that a learning machine does not require knowledge
about words, phrases, sentences, paragraphs or any other
syntactical or semantic structures to understand text. that
being said, we want to point out that convnets by their de-
sign have the capacity to learn such structured knowledge.

2,441,053 products(mcauley & leskovec, 2013). this
dataset contains review texts of extremely variate character
lengths from 3 to 32,788, in which the mean is around 764.
to construct a id31 dataset, we chose review
texts with character lengths between 100 and 1014. apart
from constructing from the original 5 score labels, we also
construct a sentiment polarity dataset in which labels 1 and
2 are converted to negative and 4 and 5 positive. there are
also large number of duplicated reviews in which the title
and review text are the same. we removed these duplicates.
table 5 lists the number of samples for each score and the
number sampled for the 2 dataset.

table 5. amazon review datasets. column    total    is the total
number of samples for each score. column    full    and    polarity   
are number of samples chosen for full score dataset and polarity
dataset, respectively.

total

full

polarity

1
2
3
4
5

2,746,559
1,791,219
2,892,566
6,551,166
20,705,260

730,000
730,000
730,000
730,000
730,000

1,275,000
725,000

0

725,000
1,275,000

figure 3. visualization of    rst layer weights

figure 3 is a visualization of some kernel weights in the
   rst layer of the large model trained without thesaurus aug-
mentation. each block represents a randomly chosen ker-
nel, with its horizontal direction iterates over input frames
and vertical direction over kernel size.
in the visualiza-
tion, black (or white) indicates large negative (or positive)
values, and gray indicates values near zero. it seems very
interesting that the network has learnt to care more about
the variations in letters than other characters. this phe-
nomenon is observed in models for all of the datasets.

3.2. amazon review id31

the purpose of id31 is to identify and extract
subjective information in different kinds of source mate-
rials. this task, when presented with the text written by
some user, could be formulated as a normal classi   cation
problem in which each class represents a degree indicator
for user   s subjective view. one example is the score sys-
tem used from amazon, which is a discrete score from 1 to
5 indicating user   s subjective rating of a product. the rat-
ing usually comes with a review text, which is a valuable
source for us to construct a id31 dataset.
we obtained an amazon review dataset from the stan-
ford network analysis project (snap), which spans 18
years with 34,686,770 reviews from 6,643,669 users on

we ignored score 3 for polarity dataset because some
texts in that score are not obviously negative or positive.
many researchers have shown that with some random text,
the inter-rater consensus on polarity is only about 60% -
80%(gamon & aue, 2005)(kim & hovy, 2004)(strappa-
rava & mihalcea, 2008)(viera et al., 2005)(wiebe et al.,
2001)(wilson et al., 2005). we believe that by picking out
score 3, the labels would have higher quality with a clearer
indication of positivity or negativity. we could have in-
cluded a third    neutral    class, but that would signi   cantly
reduce the number of samples for each class since sample
imbalance is not desirable.
for the full score dataset, we randomly selected 600,000
samples for each score for training and 130,000 samples
for testing. the size of training set is then 3,000,000 and
testing 650,000. for the polarity dataset, we randomly se-
lected 1,800,000 samples for each positive or negative la-
bel as training set and 200,000 samples for testing. in to-
tal, the polarity dataset has 3,600,000 training samples and
400,000 testing samples.
because we limit the maximum length of the text to be
1014, we can safely set the input length to be 1014 and
use the same con   guration as the dbpedia model. models
for amazon review datasets took signi   cantly more time to
go over each epoch. the time taken for the large model per
epoch is about a 5 days, and small model 2 days, with the

text understanding from scratch

table 6. result on amazon review full score dataset. the num-
bers are accuracy.

model

thesaurus

train

test

large convnet
large convnet
small convnet
small convnet
bag of words

id97

no
yes
no
yes
no
no

62.96% 58.69%
68.90% 59.55%
69.24% 59.47%
62.11% 59.57%
54.45% 54.17%
36.56% 36.50%

table 7. result on amazon review polarity dataset. the numbers
are accuracy.

model

thesaurus

train

test

large convnet
large convnet
small convnet
small convnet
bag of words

id97

no
yes
no
yes
no
no

97.57% 94.49%
96.82% 95.07%
96.03% 94.50%
95.44% 94.33%
89.96% 89.86%
72.95% 72.86%

polarity training taking a little bit longer. table 6 and table
7 list the results on full score dataset and polarity dataset,
respectively.

(a) train

(b) test

figure 4. confusion matrices on full score amazon review pre-
diction. white values are 1 and black 0. vertical direction iterates
over true score from top to bottom, and horizontal direction iter-
ates over predicted scores from left to right.

it seems that our models work much better on the polarity
dataset than the full score dataset. this is to be expected,
since full score prediction means more confusion between
nearby score labels. to demonstrate this,    gure 4 shows
the training and testing confusion matrices.

3.3. yahoo! answers topic classi   cation

yahoo! answers is a web site where people post questions
and answers, all of which are public to any web user will-

ing to browse or download them. we obtained yahoo! an-
swers comprehensive questions and answers version 1.0
dataset through the yahoo! webscope program. the data
they have collected is the yahoo! answers corpus as of oc-
tober 25th, 2007. it includes all the questions and their cor-
responding answers. the corpus contains 4,483,032 ques-
tions and their answers. in addition to question and answer
text, the corpus contains a small amount of metadata, i.e.,
which answer was selected as the best answer, and the cat-
egory and sub-category that was assigned to each question.
we constructed a topic classi   cation dataset from this cor-
pus using 10 largest main categories. they are listed in
table 8. each class contains 140,000 training samples and
6,000 testing samples. therefore, the total number of train-
ing samples is 1,400,000 and testing samples 60,000 in this
dataset. from all the answers and other meta-information,
we only used the best answer content and the main category
information.

table 8. yahoo! answers topic classi   cation dataset

category

total

train

test

society & culture
science & mathematics
health
education & reference
computers & internet
sports
business & finance
entertainment & music
family & relationships
politics & government

295,340
169,586
278,942
206,440
281,696
146,396
265,182
440,548
517,849
152,564

140,000
140,000
140,000
140,000
140,000
140,000
140,000
140,000
140,000
140,000

6,000
6,000
6,000
6,000
6,000
6,000
6,000
6,000
6,000
6,000

the yahoo! answers dataset also contains questions and
answers of various lengths, up to 4000 characters. during
training we still set the input length to be 1014 and truncate
the rest if necessary. but before truncation, we concate-
nated the question title, question content and best answer
content in reverse order so that the question title and con-
tent are less likely to be truncated. it takes about 1 day for
one epoch on the large model, and about 8 hours for the
small model. table 9 details the results on this dataset.
one interesting thing from the results on yahoo! answers
dataset is that both training and testing accuracy values are
quite small compared to the results we obtained from other
datasets, whereas the generalization error is pretty good.
one hypothesis for this is that there are some intrinsic con-
fusions in determining between some classes given a pair
of question and answer.
figure 5 shows the confusion matrix for the large model
without thesaurus augmentation.
it indicates relatively

text understanding from scratch

table 9. results on yahoo! answers dataset. the numbers are
accuracy.

model

thesaurus

train

test

large convnet
large convnet
small convnet
small convnet
bag of words

id97

no
yes
no
yes
no
no

73.42% 70.45%
75.55% 71.10%
72.84% 70.16%
72.51% 70.16%
66.83% 66.62%
56.37% 56.47%

(a) train

(b) test

figure 5. confusion matrices on yahoo! answers dataset. white
values are 1 and black 0. vertical direction iterates over true
classes from top to bottom, and horizontal direction iterates over
predicted classes from left to right.

table 10 is a summary of the dataset. from each category,
we randomly chose 30,000 samples as training and 1,900
as testing. the total number of training samples is then
120,000 and testing 7,600. compared to other datasets we
have constructed, this dataset is relatively small. therefore
the time taken for one epoch using the large model is only
3 hours, and about 1 hour for the small model.

table 11. result on ag   s news corpus. the numbers are accuracy

model

thesaurus

train

test

large convnet
large convnet
small convnet
small convnet
bag of words

id97

no
yes
no
yes
no
no

99.44% 87.18%
99.49% 86.61%
99.20% 84.35%
96.81% 85.20%
88.02% 86.69%
78.20% 76.73%

similarly as our previous experiments, we also use an input
length of 1014 for this dataset after title and description are
concatenated. the actual resulting maximum length of all
the inputs is 9843, but the mean is only around 232.
table 11 lists the results. it shows a sign of over   tting from
our models, which suggests that to achieve good text under-
standing results convnets require a large corpus in order to
learn from scratch.

large confusion for classes    society & culture   ,    educa-
tion & reference   , and    business & finance   .

3.5. news categorization in chinese

3.4. news categorization in english

news is one of the largest parts of the entire web today,
which makes it a good candidate to build text understand-
ing models. we obtained the ag   s corpus of news article
on the web2. it contains 496,835 categorized news articles
from more than 2000 news sources. we choose 4 largest
categories from this corpus to construct our dataset, using
only the title and description    elds.

table 10. ag   s news corpus. only categories used are listed.

category

total

train

test

world
sports
business
sci/tech

81,456
62,163
56,656
41,194

30,000
30,000
30,000
30,000

1,900
1,900
1,900
1,900

2http://www.di.unipi.it/  gulli/ag_corpus_

of_news_articles.html

one immediate advantage from our dictionary-free design
is its applicability to other kinds of human languages. our
simple approach only needs an alphabet of the target lan-
guage using one-of-n encoding. for languages such as
chinese, japanese and korean where there are too many
characters, one can simply use its romanized (or latinized)
transcription and quantize them just like in english. better
yet, the romanization or latinization is usually phonemic
or phonetic, which rivals the success of deep learning in
id103(hannun et al., 2014). here we investi-
gate one example: news categorization in chinese.
the dataset we obtained consists of the sogouca and so-
goucs news corpora(wang et al., 2008), containing in total
2,909,551 news articles in various topic channels. among
them, about 2,644,110 contain both a title and some con-
tent. we then labeled the each piece of news using its
url, by manually classify the their domain names. this
gives us a large corpus of news articles labeled with their
categories. there are a large number categories but most
of them contain only few articles. we choose 5 categories
       sports   ,       nance   ,    entertainment   ,    automobile    and
   technology   . the number of training samples selected for

text understanding from scratch

each class is 90,000 and testing 12,000, as table 12 shows.

table 12. sogou news dataset

category

total

train

test

sports
finance
entertainment
automobile
technology

645,931
315,551
160,409
167,647
188,111

90,000
90,000
90,000
90,000
90,000

12,000
12,000
12,000
12,000
12,000

the romanization or latinization form we have used is
pinyin, which is a phonetic system for transcribing the
mandarin pronunciations. during this procedure, we used
the pypinyin package combined with jieba chinese
segmentation system. the resulting pinyin text had each
tone appended their    nals as numbers between 1 and 4.
similar as before, we concatenate title and content to form
an input sample. the texts has a wide range of lengths from
14 to 810959. therefore, during data acquisition proce-
dure we constrain the length to stay between 100 and 1014
whenever possible. in the end, we also apply same models
as before to this dataset, for which the input length is 1014.
we ignored thesaurus augmentation for this dataset. table
13 lists the results.

table 13. result on sogou news corpus. the numbers are accu-
racy

model

thesaurus

train

test

large convnet
small convnet
bag of words

no
no
no

99.14% 95.12%
93.05% 91.35%
92.97% 92.78%

the input for a bag-of-words model is obtained by con-
sidering each pinyin at chinese character level as a word.
these results indicate consistently good performance from
our convnet models, even though it is completely a dif-
ferent kind of human language. this is one evidence to our
belief that convnets can be applied to any human language
in similar ways for text understanding tasks.

4. outlook and conclusion
in this article we provide a    rst evidence on convnets    ap-
plicability to text understanding tasks from scratch, that is,
convnets do not need any knowledge on the syntactic or
semantic structure of a language to give good benchmarks
text understanding. this evidence is in contrast with var-
ious previous approaches where a dictionary of words is

a necessary starting point, and usually structured parsing
is hard-wired into the model(collobert et al., 2011b)(kim,
2014)(johnson & zhang, 2014)(dos santos & gatti, 2014).
deep learning models have been known to have good rep-
resentations across domains or problems, in particular for
image recognition(razavian et al., 2014). how good the
learnt representations are for id38 is also one
interesting question to ask in the future. beyond that, we
can also consider how to apply unsupervised learning to
language models learnt from scratch. previous embedding
methods(collobert et al., 2011b)(mikolov et al., 2013b)(le
& mikolov, 2014) have shown that predicting words or
other patterns missing from the input could be useful. we
are eager to see how to apply these id21 and
unsupervised learning techniques with our models.
recent research shows that it is possible to generate text
description of images from the features learnt in a deep
image recognition model, using either fragment embed-
ding(karpathy et al., 2014) or recurrent neural networks
such as long-short term memory (lstm)(vinyals et al.,
2014). the models in this article show very good ability
for understanding natural languages, and we are interested
in using the features from our model to generate a response
sentence in similar ways. if this could be successful, con-
versational systems could have a big advancement.
it is also worth noting that natural language in its essence
is time-series in disguise. therefore, one natural extended
application for our approach is towards time-series data,
in which a hierarchical feature extraction mechanism could
bring some improvements over the recurrent and regression
models used widely today.
in this article we only apply convnets to text understand-
ing for its semantic or sentiment meaning. one other ap-
parent extension is towards traditional nlp tasks such as
chunking, id39 (ner) and part-of-
speech (pos) tagging. to do them, one would need to
adapt our models to structured outputs. this is very simi-
lar to the seminal work by collobert and weston(collobert
et al., 2011b), except that we probably no longer need to
construct a dictionary and start from words. our work also
makes it easy to extend these models to other human lan-
guages.
one    nal possibility from our model is learning from
symbolic systems such as mathematical equations, logic
expressions or programming languages. zaremba and
sutskever(zaremba & sutskever, 2014) have shown that it
is possible to approximate program executing using a recur-
rent neural network. we are also eager to see how similar
projects could work out using our convnet models.
with so many possibilities, we believe that convnet mod-
els for text understanding could go beyond from what this

text understanding from scratch

article shows and bring important insights towards arti   cial
intelligence in the future.

acknowledgement
we gratefully acknowledge the support of nvidia corpo-
ration with the donation of 2 tesla k40 gpus used for this
research.

references
boureau, y-l, bach, francis, lecun, yann, and ponce,
jean. learning mid-level features for recognition.
in
id161 and pattern recognition (cvpr), 2010
ieee conference on, pp. 2559   2566. ieee, 2010a.

boureau, y-lan, ponce, jean, and lecun, yann. a theo-
retical analysis of feature pooling in visual recognition.
in proceedings of the 27th international conference on
machine learning (icml-10), pp. 111   118, 2010b.

braille, louis. method of writing words, music, and plain
songs by means of dots, for use by the blind and ar-
ranged for them. 1829.

collobert, ronan, kavukcuoglu, koray, and farabet,
cl  ement. torch7: a matlab-like environment for ma-
in biglearn, nips workshop, number
chine learning.
epfl-conf-192376, 2011a.

collobert, ronan, weston, jason, bottou, l  eon, karlen,
michael, kavukcuoglu, koray, and kuksa, pavel. natu-
ral language processing (almost) from scratch. j. mach.
learn. res., 12:2493   2537, november 2011b.
issn
1532-4435.

deng, j., dong, w., socher, r., li, l.-j., li, k., and fei-
id163: a large-scale hierarchical image

fei, l.
database. in cvpr09, 2009.

dos santos, cicero and gatti, maira. deep convolutional
neural networks for id31 of short texts. in
proceedings of coling 2014, the 25th international
conference on computational linguistics: technical
papers, pp. 69   78, dublin, ireland, august 2014. dublin
city university and association for computational lin-
guistics.

fellbaum, christiane. id138 and id138s. in brown,
keith (ed.), encyclopedia of language and linguistics,
pp. 665   670, oxford, 2005. elsevier.

frome, andrea, corrado, greg s, shlens, jon, bengio,
samy, dean, jeff, mikolov, tomas, et al. devise: a
deep visual-semantic embedding model. in advances in
neural information processing systems, pp. 2121   2129,
2013.

gamon, michael and aue, anthony. automatic identi   -
cation of sentiment vocabulary: exploiting low associ-
ation with known sentiment terms. in in: proceedings
of the acl 2005 workshop on feature engineering for
machine learning in nlp, acl, pp. 57   64, 2005.

gao, jianfeng, he, xiaodong, yih, wen-tau, and deng, li.
learning semantic representations for the phrase trans-
lation model. arxiv preprint arxiv:1312.0482, 2013.

girshick, ross b., donahue, jeff, darrell, trevor, and
malik, jitendra. rich feature hierarchies for accurate
id164 and semantic segmentation. corr,
abs/1311.2524, 2013.

hannun, a., case, c., casper, j., catanzaro, b., diamos,
g., elsen, e., prenger, r., satheesh, s., sengupta, s.,
coates, a., and ng, a. y. deepspeech: scaling up end-
to-end id103. arxiv e-prints, december
2014.

hinton, geoffrey e, srivastava, nitish, krizhevsky, alex,
sutskever, ilya, and salakhutdinov, ruslan r. improving
neural networks by preventing co-adaptation of feature
detectors. arxiv preprint arxiv:1207.0580, 2012.

hochreiter, sepp and schmidhuber, j  urgen. long short-
neural comput., 9(8):1735   1780,

term memory.
november 1997. issn 0899-7667.

johnson, rie and zhang, tong. effective use of word or-
der for text categorization with convolutional neural net-
works. corr, abs/1412.1058, 2014.

karpathy, andrej, joulin, armand, and fei-fei, li. deep
fragment embeddings for bidirectional image sentence
mapping. corr, abs/1406.5679, 2014.

kim, soo-min and hovy, eduard. determining the senti-
in proceedings of the 20th interna-
ment of opinions.
tional conference on computational linguistics, col-
ing    04, stroudsburg, pa, usa, 2004. association for
computational linguistics.

kim, yoon. convolutional neural networks for sentence
in proceedings of the 2014 conference
classi   cation.
on empirical methods in natural language processing
(emnlp), pp. 1746   1751, doha, qatar, october 2014.
association for computational linguistics.

krizhevsky, alex, sutskever, ilya, and hinton, geoffrey e.
id163 classi   cation with deep convolutional neural
networks. in nips, pp. 1106   1114, 2012.

le, quoc v and mikolov, tomas. distributed represen-
arxiv preprint

tations of sentences and documents.
arxiv:1405.4053, 2014.

text understanding from scratch

lecun, y., bottou, l., bengio, y., and haffner, p. gradient-
based learning applied to document recognition. pro-
ceedings of the ieee, 86(11):2278   2324, november
1998.

sermanet, pierre, eigen, david, zhang, xiang, mathieu,
micha  el, fergus, rob, and lecun, yann. overfeat: inte-
grated recognition, localization and detection using con-
volutional networks. corr, abs/1312.6229, 2013.

lehmann, jens, isele, robert, jakob, max, jentzsch, anja,
kontokostas, dimitris, mendes, pablo n., hellmann, se-
bastian, morsey, mohamed, van kleef, patrick, auer,
s  oren, and bizer, christian. dbpedia - a large-scale,
multilingual knowledge base extracted from wikipedia.
semantic web journal, 2014.

linell, p. the written language bias in linguistics. 1982.

mcauley, julian and leskovec, jure. hidden factors and
hidden topics: understanding rating dimensions with re-
view text. in proceedings of the 7th acm conference on
recommender systems, recsys    13, pp. 165   172, new
york, ny, usa, 2013. acm. isbn 978-1-4503-2409-0.

mikolov, tomas, le, quoc v, and sutskever, ilya. ex-
ploiting similarities among languages for machine trans-
lation. arxiv preprint arxiv:1309.4168, 2013a.

mikolov, tomas, sutskever, ilya, chen, kai, corrado,
greg s., and dean, jeff. distributed representations of
words and phrases and their compositionality. in burges,
c.j.c., bottou, l., welling, m., ghahramani, z., and
weinberger, k.q. (eds.), advances in neural information
processing systems 26, pp. 3111   3119. 2013b.

nair, vinod and hinton, geoffrey e. recti   ed linear units
improve restricted id82s. in proceedings
of the 27th international conference on machine learn-
ing (icml-10), pp. 807   814, 2010.

norvig, peter. id136 in text understanding. in aaai,

pp. 561   565, 1987.

pennington,

jeffrey, socher, richard, and manning,
christopher d. glove: global vectors for word represen-
tation. proceedings of the empiricial methods in natural
language processing (emnlp 2014), 12, 2014.

polyak, b.t. some methods of speeding up the conver-
{ussr} computational
gence of iteration methods.
mathematics and mathematical physics, 4(5):1     17,
1964. issn 0041-5553.

razavian, ali sharif, azizpour, hossein, sullivan,
josephine, and carlsson, stefan. id98 features off-the-
shelf: an astounding baseline for recognition. corr,
abs/1403.6382, 2014.

soderland, stephen. building a machine learning based
in in proc. ijcai-2001
text understanding system.
workshop on adaptive text extraction and mining, pp.
64   70, 2001.

strapparava, carlo and mihalcea, rada. learning to iden-
tify emotions in text. in proceedings of the 2008 acm
symposium on applied computing, sac    08, pp. 1556   
1560, new york, ny, usa, 2008. acm. isbn 978-1-
59593-753-7.

sutskever, ilya, martens, james, dahl, george e., and hin-
ton, geoffrey e. on the importance of initialization and
momentum in deep learning. in dasgupta, sanjoy and
mcallester, david (eds.), proceedings of the 30th inter-
national conference on machine learning (icml-13),
volume 28, pp. 1139   1147. jmlr workshop and con-
ference proceedings, may 2013.

viera, anthony j, garrett, joanne m, et al. understanding
interobserver agreement: the kappa statistic. fam med,
37(5):360   363, 2005.

vinyals, oriol, toshev, alexander, bengio, samy, and er-
han, dumitru. show and tell: a neural image caption
generator. corr, abs/1411.4555, 2014.

wang, canhui, zhang, min, ma, shaoping, and ru, liyun.
automatic online news issue construction in web envi-
ronment. in proceedings of the 17th international con-
ference on world wide web, www    08, pp. 457   466,
new york, ny, usa, 2008. acm. isbn 978-1-60558-
085-2.

wiebe, janyce m., wilson, theresa, and bell, matthew.
identifying collocations for recognizing opinions. in
proceedings of the acl/eacl workshop on colloca-
tion, toulouse, fr, 2001.

wilson, theresa, wiebe, janyce, and hoffmann, paul. rec-
ognizing contextual polarity in phrase-level sentiment
in proceedings of the conference on hu-
analysis.
man language technology and empirical methods in
natural language processing, hlt    05, pp. 347   354,
stroudsburg, pa, usa, 2005. association for computa-
tional linguistics.

zaremba, wojciech and sutskever, ilya. learning to exe-

cute. corr, abs/1410.4615, 2014.

rumelhart, d.e., hintont, g.e., and williams, r.j. learn-
ing representations by back-propagating errors. nature,
323(6088):533   536, 1986.

zeiler, matthew d and fergus, rob. visualizing and under-
standing convolutional networks. in id161   
eccv 2014, pp. 818   833. springer, 2014.

