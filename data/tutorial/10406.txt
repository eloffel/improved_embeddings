a latent variable recurrent neural network

for discourse relation language models

yangfeng ji

georgia institute of technology

atlanta, ga 30308, usa
jiyfeng@gatech.edu

gholamreza haffari
monash university

clayton, vic, australia

gholamreza.haffari

@monash.edu

jacob eisenstein

georgia institute of technology

atlanta, ga 30308, usa
jacobe@gatech.edu

6
1
0
2

 
r
p
a
5

 

 
 
]
l
c
.
s
c
[
 
 

2
v
3
1
9
1
0

.

3
0
6
1
:
v
i
x
r
a

abstract

this paper presents a novel latent variable re-
current neural network architecture for jointly
modeling sequences of words and (possibly
latent) discourse relations between adjacent
sentences. a recurrent neural network gen-
erates individual words, thus reaping the ben-
e   ts of discriminatively-trained vector repre-
sentations. the discourse relations are rep-
resented with a latent variable, which can be
predicted or marginalized, depending on the
task. the resulting model can therefore em-
ploy a training objective that includes not only
discourse relation classi   cation, but also word
prediction. as a result, it outperforms state-of-
the-art alternatives for two tasks: implicit dis-
course relation classi   cation in the penn dis-
course treebank, and dialog act classi   cation
in the switchboard corpus. furthermore, by
marginalizing over latent discourse relations
at test time, we obtain a discourse informed
language model, which improves over a strong
lstm baseline.

1

introduction

natural language processing (nlp) has recently ex-
perienced a neural network    tsunami    (manning,
2016). a key advantage of these neural architec-
tures is that they employ discriminatively-trained
distributed representations, which can capture the
meaning of linguistic phenomena ranging from in-
dividual words (turian et al., 2010) to longer-range
linguistic contexts at the sentence level (socher et
al., 2013) and beyond (le and mikolov, 2014). be-
cause they are discriminatively trained, these meth-

ods can learn representations that yield very accurate
predictive models (e.g., dyer et al, 2015).

however, in comparison with the probabilistic
id114 that were previously the dominant
machine learning approach for nlp, neural archi-
tectures lack    exibility. by treating linguistic an-
notations as random variables, probabilistic graphi-
cal models can marginalize over annotations that are
unavailable at test or training time, elegantly model-
ing multiple linguistic phenomena in a joint frame-
work (finkel et al., 2006). but because these graph-
ical models represent uncertainty for every element
in the model, adding too many layers of latent vari-
ables makes them dif   cult to train.

in this paper, we present a hybrid architecture
that combines a recurrent neural network language
model with a latent variable model over shallow
discourse structure.
in this way, the model learns
a discriminatively-trained distributed representation
of the local contextual features that drive word
choice at the intra-sentence level, using techniques
that are now state-of-the-art in language model-
ing (mikolov et al., 2010). however, the model
treats shallow discourse structure     speci   cally, the
relationships between pairs of adjacent sentences    
as a latent variable. as a result, the model can act
as both a discourse relation classi   er and a language
model. speci   cally:

    if trained to maximize the conditional like-
lihood of the discourse relations,
it outper-
forms state-of-the-art methods for both im-
plicit discourse relation classi   cation in the
penn discourse treebank (rutherford and xue,
2015) and dialog act classi   cation in switch-

board (kalchbrenner and blunsom, 2013). the
model learns from both the discourse annota-
tions as well as the id38 objec-
tive, unlike previous recursive neural architec-
tures that learn only from annotated discourse
relations (ji and eisenstein, 2015).

    if the model is trained to maximize the joint
likelihood of the discourse relations and the
text, it is possible to marginalize over discourse
relations at test time, outperforming language
models that do not account for discourse struc-
ture.

in contrast to recent work on continuous latent
variables in recurrent neural networks (chung et al.,
2015), which require complex variational autoen-
coders to represent uncertainty over the latent vari-
ables, our model is simple to implement and train,
requiring only minimal modi   cations to existing re-
current neural network architectures that are imple-
mented in commonly-used toolkits such as theano,
torch, and id98.

we focus on a class of shallow discourse rela-
tions, which hold between pairs of adjacent sen-
tences (or utterances). these relations describe how
the adjacent sentences are related: for example, they
may be in contrast, or the latter sentence may of-
fer an answer to a question posed by the previous
sentence. shallow relations do not capture the full
range of discourse phenomena (webber et al., 2012),
but they account for two well-known problems: im-
plicit discourse relation classi   cation in the penn
discourse treebank, which was the 2015 conll
shared task (xue et al., 2015); and dialog act clas-
si   cation, which characterizes the structure of in-
terpersonal communication in the switchboard cor-
pus (stolcke et al., 2000), and is a key component of
contemporary id71 (williams and young,
2007). our model outperforms state-of-the-art alter-
natives for implicit discourse relation classi   cation
in the penn discourse treebank, and for dialog act
classi   cation in the switchboard corpus.

2 background
our model scaffolds on recurrent neural network
(id56) language models (mikolov et al., 2010), and
recent variants that exploit multiple levels of linguis-
tic detail (ji et al., 2015; lin et al., 2015).

nt(cid:89)

id56 language models let us denote token n in
a sentence t by yt,n     {1 . . . v }, and write yt =
{yt,n}n   {1...nt} to indicate the sequence of words in
sentence t. in an id56 language model, the proba-
bility of the sentence is decomposed as,

p(yt) =

p(yt,n | yt,<n),

(1)

n

where the id203 of each word yt,n is condi-
tioned on the entire preceding sequence of words
yt,<n through the summary vector ht,n   1. this vec-
tor is computed recurrently from ht,n   2 and from
the embedding of the current word, xyt,n   1, where
x     rk  v and k is the dimensionality of the word
embeddings. the language model can then be sum-
marized as,

ht,n =f(xyt,n , ht,n   1)

p(yt,n | yt,<n) =softmax (woht,n   1 + bo) ,

(2)
(3)
where the matrix wo     rv   k de   nes the output
embeddings, and bo     rv is an offset. the function
f(  ) is a deterministic non-linear transition function.
it typically takes an element-wise non-linear trans-
formation (e.g., tanh) of a vector resulting from the
sum of the id27 and a linear transforma-
tion of the previous hidden state.

the model as described thus far is identical to the
recurrent neural network language model (id56lm)
of mikolov et al. (2010). in this paper, we replace
the above simple hidden state units with the more
complex long short-term memory units (hochre-
iter and schmidhuber, 1997), which have consis-
tently been shown to yield much stronger perfor-
mance in id38 (pham et al., 2014).
for simplicity, we still use the term id56lm in re-
ferring to this model.

document context language model one draw-
back of the id56lm is that it cannot propagate long-
range information between the sentences. even if
we remove sentence boundaries, long-range infor-
mation will be attenuated by repeated application of
the non-linear transition function. ji et al. (2015)
propose the document context language model
(dclm) to address this issue. the core idea is to
represent context with two vectors: ht,n, represent-
ing intra-sentence word-level context, and ct, rep-
resenting inter-sentence context. these two vectors

figure 1: a fragment of our model with latent variable zt, which only illustrates discourse information    ow from sentence (t     1)
to t. the information from sentence (t     1) affects the distribution of zt and then the words prediction within sentence t.

p(yt,n+1 | zt, yt,<n, yt   1) = g

w(zt)

o ht,n

+ w(zt)

c ct   1

+

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

o(cid:124)(cid:123)(cid:122)(cid:125)

b(zt)

relation-speci   c

relation-speci   c

relation-speci   c

(cid:17)

(4)

(cid:16)

intra-sentential context

inter-sentential context

bias

figure 2: per-token generative probabilities in the discourse relation language model

are then linearly combined in the generation func-
tion for word yt,n,

p(yt,n | yt,<n, y<t)
= softmax (woht,n   1 + wcct   1 + bo) ,

where ct   1 is set to the last hidden state of the pre-
vious sentence. ji et al. (2015) show that this model
can improve language model perplexity.

3 discourse relation language models

we now present a probabilistic neural model over
sequences of words and shallow discourse relations.
discourse relations zt are treated as latent variables,
which are linked with a recurrent neural network
over words in a latent variable recurrent neural net-
work (chung et al., 2015).

3.1 the model
our model (see figure 1) is formulated as a two-step
generative story. in the    rst step, context informa-
tion from the sentence (t    1) is used to generate the
discourse relation between sentences (t     1) and t,

p(zt | yt   1) = softmax (u ct   1 + b) ,

(6)

where zt is a random variable capturing the dis-
course relation between the two sentences, and ct   1
is a vector summary of the contextual information
from sentence (t     1), just as in the dclm (equa-
tion 5). the model maintains a default context vec-
tor c0 for the    rst sentences of documents, and treats
it as a parameter learned with other model parame-
ters during training.

in the second step, the sentence yt is generated,
conditioning on the preceding sentence yt   1 and the
discourse relation zt:

(5)

p(yt | zt, yt   1) =

p(yt,n | yt,<n, yt   1, zt), (7)

nt(cid:89)

n

t(cid:89)

the generative id203 for the sentence yt de-
composes across tokens as usual (equation 7). the
per-token probabilities are shown in equation 4, in
figure 2. discourse relations are incorporated by pa-
rameterizing the output matrices w(zt)
and w(zt)
;
depending on the discourse relation that holds be-
tween (t     1) and t, these matrices will favor dif-
ferent parts of the embedding space. the bias term
b(zt)
is also parametrized by the discourse relation,
o
so that each relation can favor speci   c words.

overall, the joint id203 of the text and dis-

o

c

course relations is,

p(y1:t , z1:t ) =

p(zt | yt   1)    p(yt | zt, yt   1).

t

(8)

if the discourse relations zt are not observed, then
our model is a form of latent variable recurrent neu-
ral network (lvid56). connections to recent work
on lvid56s are discussed in    6; the key difference
is that the latent variables here correspond to linguis-
tically meaningful elements, which we may wish to
predict or marginalize, depending on the situation.
parameter tying as proposed, the discourse re-
lation language model has a large number of pa-
rameters. let k, h and v be the input dimension,

yt   1,nt   1   2yt   1,nt   1   1yt   1,nt   1yt,1yt,2ztyt,3c

and w(z)

hidden dimension and the size of vocabulary in lan-
guage modeling. the size of each prediction matrix
is v    h; there are two such matri-
w(z)
o
ces for each possible discourse relation. we reduce
the number of parameters by factoring each of these
matrices into two components:
o = wo    v(z), w(z)

c = wc    m(z),

w(z)

(9)

where v(z) and m(z) are relation-speci   c compo-
nents for intra-sentential and inter-sentential con-
texts; the size of these matrices is h    h, with
h (cid:28) v . the larger v    h matrices wo and wc
are shared across all relations.

id136

3.2
there are two possible id136 scenarios:
in-
ference over discourse relations, conditioning on
words; and id136 over words, marginalizing over
discourse relations.
id136 over discourse relations the prob-
ability of discourse relations given the sentences
p(z1:t | y1:t ) is decomposed into the product of
probabilities of individual discourse relations condi-
t p(zt | yt, yt   1).

tioned on the adjacent sentences(cid:81)

these probabilities are computed by bayes    rule:

p(zt | yt, yt   1) =

(cid:80)
p(yt | zt, yt   1)    p(zt | yt   1)
z(cid:48) p(yt | z(cid:48), yt   1)    p(z(cid:48) | yt   1)
.
(10)
the terms in each product are given in equations 6
and 7. normalizing involves only a sum over a small
   nite number of discourse relations. note that infer-
ence is easy in our case because all words are ob-
served and there is no probabilistic coupling of the
discourse relations.
id136 over words
in discourse-informed lan-
guage modeling, we marginalize over discourse re-
lations to compute the id203 of a sequence of
sentence y1:t , which can be written as,

t

zt

p(y1:t ) =

p(zt | yt   1)    p(yt | zt, yt   1),
(11)
because the word sequences are observed, decou-
pling each zt from its neighbors zt+1 and zt   1.
this decoupling ensures that we can compute the

t(cid:89)

(cid:88)

overall marginal likelihood as a product over local
marginals.

3.3 learning
the model can be trained in two ways: to maximize
the joint id203 p(y1:t , z1:t ), or to maximize
the id155 p(z1:t | y1:t ). the joint
training objective is more suitable for language mod-
eling scenarios, and the conditional objective is bet-
ter for discourse relation prediction. we now de-
scribe each objective in detail.

joint likelihood objective the joint likelihood
objective function is directly adopted from the joint
id203 de   ned in equation 8. the objective
function for a single document with t sentences or
utterances is,

(cid:96)(  ) =

log p(zt | yt   1)

t(cid:88)
nt(cid:88)

t

+

log p(yt,n | yt,<n, yt   1, zt),

(12)

n

where    represents the collection of all model pa-
rameters,
including the parameters in the lstm
units and the id27s.

maximizing the objective function (cid:96)(  ) will
jointly optimize the model on both language lan-
guage and discourse relation prediction. as such,
it can be viewed as a form of multi-task learn-
ing (caruana, 1997), where we learn a shared rep-
resentation that works well for discourse relation
prediction and for id38. however, in
practice, the large vocabulary size and number of to-
kens means that the id38 part of the
objective function tends to dominate.

conditional objective this training objective is
speci   c to the discourse relation prediction task, and
based on equation 10 can be written as:

(cid:96)r(  ) =

(cid:88)

log p(zt | yt   1) + log p(yt | zt, yt   1)
p(z(cid:48) | yt   1)    p(yt | z(cid:48), yt   1)

t

    log

z(cid:48)

(13)
the    rst line in equation 13 is the same as (cid:96)(  ),
but the second line re   ects the id172 over all

t(cid:88)

possible values of zt. this forces the objective func-
tion to attend speci   cally to the problem of maxi-
mizing the conditional likelihood of the discourse
relations and treat id38 as an auxil-
iary task (collobert et al., 2011).

alogue act corpus (stolcke et al., 2000, swda),
which is annotated on a collections of phone con-
versations. both corpora contain annotations of dis-
course relations and dialogue relations that hold be-
tween adjacent spans of text.

3.4 modeling limitations
the discourse relation language model is carefully
designed to decouple the discourse relations from
each other, after conditioning on the words.
it
is clear that text documents and spoken dialogues
have sequential discourse structures, and it seems
likely that modeling this structure could improve
performance. in a traditional hidden markov model
(id48) generative approach (stolcke et al., 2000),
modeling sequential dependencies is not dif   cult,
because training reduces to relative frequency es-
in the hybrid probabilistic-
timation. however,
neural architecture proposed here,
training is al-
ready expensive, due to the large number of param-
eters that must be estimated. adding probabilis-
tic couplings between adjacent discourse relations
(cid:104)zt   1, zt(cid:105) would require the use of dynamic pro-
gramming for both training and id136, increas-
ing time complexity by a factor that is quadratic in
the number of discourse relations. we did not at-
tempt this in this paper; we do compare against a
conventional id48 on the dialogue act prediction
task in    5.

ji et al. (2015) propose an alternative form of
the document context language model, in which the
contextual information ct impacts the hidden state
ht+1, rather than going directly to the outputs yt+1.
they obtain slightly better perplexity with this ap-
proach, which has fewer trainable parameters. how-
ever, this model would couple zt with all subsequent
sentences y>t, making prediction and marginaliza-
tion of discourse relations considerably more chal-
lenging. sequential monte carlo algorithms offer a
possible solution (de freitas et al., ; gu et al., 2015),
which may be considered in future work.

4 data and implementation

we evaluate our model on two benchmark datasets:
(1) the penn discourse treebank (prasad et al.,
2008, pdtb), which is annotated on a corpus of
wall street journal acticles; (2) the switchboard di-

the penn discourse treebank (pdtb)
provides
a low-level discourse annotation on written texts. in
the pdtb, each discourse relation is annotated be-
tween two argument spans, arg1 and arg2. there
are two types of relations: explicit and implicit.
explicit relations are signalled by discourse mark-
ers (e.g.,    however   ,    moreover   ), and the span of
arg1 is almost totally unconstrained: it can range
from a single clause to an entire paragraph, and
need not be adjacent to either arg2 nor the dis-
course marker. however, automatically classifying
these relations is considered to be relatively easy,
due to the constraints from the discourse marker it-
self (pitler et al., 2008). in addition, explicit rela-
tions are dif   cult to incorporate into language mod-
els which must generate each word exactly once. on
the contrary, implicit discourse relations are anno-
tated only between adjacent sentences, based on a
semantic understanding of the discourse arguments.
automatically classifying these discourse relations
is a challenging task (lin et al., 2009; pitler et al.,
2009; rutherford and xue, 2015; ji and eisenstein,
2015). we therefore focus on implicit discourse re-
lations, leaving to the future work the question of
how to apply our modeling framework to explicit
discourse relations. during training, we collapse all
relation types other than implicit (explicit, entrel,
and norel) into a single dummy relation type,
which holds between all adjacent sentence pairs that
do not share an implicit relation.

as in the prior work on    rst-level discourse re-
lation identi   cation (e.g., park and cardie, 2012),
we use sections 2-20 of the pdtb as the training
set, sections 0-1 as the development set for param-
eter tuning, and sections 21-22 for testing. for pre-
processing, we lower-cased all tokens, and substi-
tuted all numbers with a special token    num   . to
build the vocabulary, we kept the 10,000 most fre-
quent words from the training set, and replaced low-
frequency words with a special token    unk   . in
prior work that focuses on detecting individual rela-
tions, balanced training sets are constructed so that

there are an equal number of instances with and
without each relation type (park and cardie, ; biran
and mckeown, 2013; rutherford and xue, 2014).
in this paper, we target the more challenging multi-
way classi   cation problem, so this strategy is not ap-
plicable; in any case, since our method deals with
entire documents, it is not possible to balance the
training set in this way.

the switchboard dialog act corpus (swda)
is annotated on the switchboard corpus of human-
human conversational telephone speech (godfrey et
al., 1992). the annotations label each utterance
with one of 42 possible speech acts, such as agree,
hedge, and wh-question. because these speech
acts form the structure of the dialogue, most of them
pertain to both the preceding and succeeding utter-
ances (e.g., agree). the swda corpus includes
1155    ve-minute conversations. we adopted the
standard split from stolcke et al. (2000), using 1,115
conversations for training and nineteen conversa-
tions for test. for parameter tuning, we randomly
select nineteen conversations from the training set
as the development set. after parameter tuning, we
train the model on the full training set with the se-
lected con   guration. we use the same preprocessing
techniques here as in the pdtb.

implementation

4.1
we use a single-layer lstm to build the recur-
rent architecture of our models, which we im-
plement in the id98 package.1 our implemen-
tation is available on https://github.com/
jiyfeng/drlm. some additional details follow.

initialization following prior work on id56 ini-
tialization (bengio, 2012), all parameters except
the relation prediction parameters u and b are ini-
tialized with random values drawn from the range

[   (cid:112)6/(d1 + d2),(cid:112)6/(d1 + d2)], where d1 and d2

are the input and output dimensions of the parame-
ter matrix respectively. the matrix u is initialized
with random numbers from [   10   5, 10   5] and b is
initialized to 0.

learning online learning was performed using
adagrad (duchi et al., 2011) with initial learning

1https://github.com/clab/id98

rate    = 0.1. to avoid the exploding gradient prob-
lem, we used norm clipping trick with a threshold of
   = 5.0 (pascanu et al., 2012). in addition, we used
value dropout (srivastava et al., 2014) with rate 0.5,
on the input x, context vector c and hidden state h,
similar to the architecture proposed by pham et al.
(2014). the training procedure is monitored by the
performance on the development set. in our experi-
ments, 4 to 5 epochs were enough.

hyper-parameters our model includes two tun-
able hyper-parameters: the dimension of word rep-
resentation k, the hidden dimension of lstm unit
h. we consider the values {32, 48, 64, 96, 128} for
both k and h. for each corpus in experiments, the
best combination of k and h is selected via grid
search on the development set.

5 experiments

our main evaluation is discourse relation prediction,
using the pdtb and swda corpora. we also eval-
uate on id38, to determine whether
incorporating discourse annotations at training time
and then marginalizing them at test time can im-
prove performance.

5.1

implicit discourse relation prediction on
the pdtb

we    rst evaluate our model with implicit discourse
relation prediction on the pdtb dataset. most of the
prior work on    rst-level discourse relation predic-
tion focuses on the    one-versus-all    binary classi   -
cation setting, but we attack the more general four-
way classi   cation problem, as performed by ruther-
ford and xue (2015). we compare against the fol-
lowing methods:

rutherford and xue (2015) build a set of feature-
rich classi   ers on the pdtb, and then augment
these classi   ers with additional automatically-
labeled training instances. we compare against
their published results, which are state-of-the-art.
ji and eisenstein (2015) employ a recursive neural
network architecture. their experimental setting
is different, so we re-run their system using the
same setting as described in    4.

model
baseline
1. most common class
prior work
2. (rutherford and xue, 2015)
3. (rutherford and xue, 2015)

accuracy macro f1

54.7

   

55.0
57.1

38.4
40.5

40.0

56.4

with extra training data
4. (ji and eisenstein, 2015)
our work - drlm
5. joint training
6. conditional training
    signi   cantly better than lines 2 and 4 with p < 0.05
table 1: multiclass relation identi   cation on the    rst-level
pdtb relations.

57.1
59.5   

40.5
42.3

results as shown in table 1, the conditionally-
trained discourse relation language models (drlm)
outperforms all alternatives, on both metrics. while
the jointly-trained drlm is at the same level as the
previous state-of-the-art, conditional training on the
same model provides a signi   cant additional advan-
tage, indicated by a binomial test.

5.2 dialogue act tagging
dialogue act tagging has been widely studied in both
nlp and speech communities. we follow the setup
used by stolcke et al. (2000) to conduct experiments,
and adopt the following systems for comparison:

stolcke et al. (2000) employ a hidden markov
model, with each id48 state corresponding to a
dialogue act.

kalchbrenner and blunsom (2013) employ a
complex neural architecture, with a convolutional
network at each utterance and a recurrent net-
work over the length of the dialog. to our knowl-
edge, this model attains state-of-the-art accuracy
on this task, outperforming other prior work such
as (webb et al., 2005; milajevs and purver, 2014).

results as shown in table 2, the conditionally-
trained discourse relation language model (drlm)
outperforms all competitive systems on this task. a
binomial test shows the result in line 6 is signi   -
cantly better than the previous state-of-the-art (line
4). all comparisons are against published results,
and macro-f1 scores are not available. accuracy

1. model
baseline
2. most common class
prior work
3. (stolcke et al., 2000)
4.
2013)
our work - drlm
5. joint training
6. conditional training
    signi   cantly better than line 4 with p < 0.01

(kalchbrenner and blunsom,

accuracy

31.5

71.0
73.9

74.0
77.0   

table 2: the results of dialogue act tagging.

is more reliable on this evaluation, since no single
class dominates, unlike the pdtb task.

5.3 discourse-aware id38
as a joint model for discourse and language model-
ing, drlm can also function as a language model,
assigning probabilities to sequences of words while
marginalizing over discourse relations. to deter-
mine whether discourse-aware id38
can improve performance, we compare against the
following systems:

id56lm+lstm this is the same basic architec-
ture as the id56lm proposed by (mikolov et al.,
2010), which was shown to outperform a kneser-
ney smoothed 5-gram model on modeling wall
street journal text. following pham et al. (2014),
we replace the sigmoid nonlinearity with a long
short-term memory (lstm).

dclm we compare against the document context
language model (dclm) of ji et al. (2015). we
use the    context-to-output    variant, which is iden-
tical to the current modeling approach, except that
it is not parametrized by discourse relations. this
model achieves strong results on language model-
ing for small and medium-sized corpora, outper-
forming id56lm+lstm.

results the perplexities of id38 on
the pdtb and the swda are summarized in ta-
ble 3. the comparison between line 1 and line
2 shows the bene   t of considering multi-sentence
context information on id38. line
3 shows that adding discourse relation information

pdtb

swda

k

pplx

k h

128
96

h pplx

117.8
112.2

model
baseline
1. id56lm 96
2. dclm
96
our work
3. drlm
table 3: language model perplexities (pplx), lower is better.
the model dimensions k and h that gave best performance on
the dev set are also shown.

56.0
45.3

128
96

108.3

96
96

39.6

128

64

96

64

yields further improvements for both datasets. we
emphasize that discourse relations in the test doc-
uments are marginalized out, so no annotations are
required for the test set; the improvements are due
to the disambiguating power of discourse relations
in the training set.

because our training procedure requires discourse
annotations, this approach does not scale to the large
datasets typically used in id38. as a
consequence, the results obtained here are somewhat
academic, from the perspective of practical language
modeling. nonetheless, the positive results here mo-
tivate the investigation of training procedures that
are also capable of marginalizing over discourse re-
lations at training time.

6 related work

this paper draws on previous work in both discourse
modeling and id38.

discourse and dialog modeling early work on
discourse relation classi   cation utilizes rich, hand-
crafted feature sets (joty et al., 2012; lin et al.,
2009; sagae, 2009). recent representation learn-
ing approaches attempt to learn good representations
jointly with discourse relation classi   ers and dis-
course parsers (ji and eisenstein, 2014; li et al.,
2014). of particular relevance are applications of
neural architectures to pdtb implicit discourse re-
lation classi   cation (ji and eisenstein, 2015; zhang
et al., 2015; braud and denis, 2015). all of these
approaches are essentially classi   ers, and take su-
pervision only from the 16,000 annotated discourse
relations in the pdtb training set. in contrast, our
approach is a probabilistic model over the entire text.
probabilistic models are frequently used in dia-

log act tagging, where id48 have
been a dominant approach (stolcke et al., 2000). in
this work, the emission distribution is an id165
language model for each dialogue act; we use a
conditionally-trained recurrent neural network lan-
guage model. an alternative neural approach for di-
alogue act tagging is the combined convolutional-
recurrent architecture of kalchbrenner and blunsom
(2013). our modeling framework is simpler, rely-
ing on a latent variable parametrization of a purely
recurrent architecture.

id38 there are an increasing
number of attempts to incorporate document-level
context information into id38. for ex-
ample, mikolov and zweig (2012) introduce lda-
style topics into id56 based id38.
sordoni et al. (2015) use a convolutional structure
to summarize the context from previous two utter-
ances as context vector for id56 based language
modeling. our models in this paper provide a uni-
   ed framework to model the context and current sen-
tence. wang and cho (2015) and lin et al. (2015)
construct bag-of-words representations of previous
sentences, which are then used to inform the id56
language model that generates the current sentence.
the most relevant work is the document context
language model (ji et al., 2015, dclm); we de-
scribe the connection to this model in    2. by adding
discourse information as a latent variable, we attain
better perplexity on held-out data.

latent variable neural networks
introducing la-
tent variables to a neural network model increases
its representational capacity, which is the main goal
of prior efforts in this space (kingma and welling,
2014; chung et al., 2015). from this perspective,
our model with discourse relations as latent vari-
ables shares the same merit. unlike this prior work,
in our approach, the latent variables carry a lin-
guistic interpretation, and are at least partially ob-
served. also, these prior models employ continuous
latent variables, requiring complex id136 tech-
niques such as id5 (kingma
and welling, 2014; burda et al., 2016; chung et al.,
2015). in contrast, the discrete latent variables in our
model are easy to sum and maximize over.

7 conclusion
we have presented a probabilistic neural model
over sequences of words and shallow discourse re-
lations between adjacent sequences. this model
combines positive aspects of neural network ar-
chitectures with probabilistic id114:
it
can learn discriminatively-trained vector representa-
tions, while maintaining a probabilistic representa-
tion of the targeted linguistic element: in this case,
shallow discourse relations. this method outper-
forms state-of-the-art systems in two discourse rela-
tion detection tasks, and can also be applied as a lan-
guage model, marginalizing over discourse relations
on the test data. future work will investigate the
possibility of learning from partially-labeled train-
ing data, which would have at least two potential ad-
vantages. first, it would enable the model to scale up
to the large datasets needed for competitive language
modeling. second, by training on more data, the
resulting vector representations might support even
more accurate discourse relation prediction.

acknowledgments
thanks to trevor cohn, chris dyer, lingpeng kong,
and quoc v. le for helpful discussions, and to the
anonymous reviewers for their feedback. this work
was supported by a google faculty research award
to the third author. it was partially performed dur-
ing the 2015 jelinek memorial summer workshop
on speech and language technologies at the uni-
versity of washington, seattle, and was supported
by johns hopkins university via nsf grant no iis
1005411, darpa lorelei contract no hr0011-
15-2-0027, and gifts from google, microsoft re-
search, amazon and mitsubishi electric research
laboratory.

references
yoshua bengio. 2012. practical recommendations for
gradient-based training of deep architectures. in neu-
ral networks: tricks of the trade, pages 437   478.
springer.

or biran and kathleen mckeown. 2013.

in proceed-
ings of the association for computational linguistics
(acl), pages 69   73, sophia, bulgaria.

chlo  e braud and pascal denis. 2015. comparing word
representations for implicit discourse relation classi-

in proceedings of empirical methods for
   cation.
natural language processing (emnlp), pages 2201   
2211, lisbon, september.

yuri burda, roger grosse, and ruslan salakhutdinov.
in pro-
2016.
ceedings of the international conference on learning
representations (iclr).

importance weighted autoencoders.

rich caruana. 1997. multitask learning. machine learn-

ing, 28(1):41   75.

junyoung chung, kyle kastner, laurent dinh, kratarth
goel, aaron courville, and yoshua bengio. 2015.
a recurrent latent variable model for sequential data.
in neural information processing systems (nips),
montr  eal.

r. collobert,

j. weston, l. bottou, m. karlen,
k. kavukcuoglu, and p. kuksa. 2011. natural lan-
guage processing (almost) from scratch. journal of
machine learning research, 12:2493   2537.

jo  ao fg de freitas, mahesan niranjan, andrew h. gee,
and arnaud doucet. sequential monte carlo methods
to train neural network models. neural computation,
12(4):955   993.

john duchi, elad hazan, and yoram singer.

2011.
adaptive subgradient methods for online learning and
the journal of machine
stochastic optimization.
learning research, 12:2121   2159.

chris dyer, miguel ballesteros, wang ling, austin
matthews, and noah a. smith. 2015. transition-
based id33 with stack long short-term
memory. in proceedings of the association for com-
putational linguistics (acl), pages 334   343, beijing,
china.

jenny rose finkel, christopher d manning, and an-
drew y ng. 2006. solving the problem of cascading
errors: approximate bayesian id136 for linguis-
tic annotation pipelines. in proceedings of empirical
methods for natural language processing (emnlp),
pages 618   626.

john j godfrey, edward c holliman, and jane mcdaniel.
1992. switchboard: telephone speech corpus for re-
search and development. in icassp, volume 1, pages
517   520. ieee.

shixiang gu, zoubin ghahramani, and richard e turner.
2015. neural adaptive sequential monte carlo. in neu-
ral information processing systems (nips), montr  eal.
sepp hochreiter and j  urgen schmidhuber. 1997. long
short-term memory. neural computation, 9(8):1735   
1780.

yangfeng ji and jacob eisenstein. 2014. representation
learning for text-level discourse parsing. in proceed-
ings of the association for computational linguistics
(acl), baltimore, md.

yangfeng ji and jacob eisenstein. 2015. one vector is
not enough: entity-augmented distributional seman-
tics for discourse relations. transactions of the asso-
ciation for computational linguistics (tacl), june.

yangfeng ji, trevor cohn, lingpeng kong, chris dyer,
document con-
and jacob eisenstein.
in international conference
text language models.
on learning representations, poster paper, volume
abs/1511.03962.

2015.

sha   q joty, giuseppe carenini, and raymond ng. 2012.
a novel discriminative framework for sentence-level
discourse analysis. in proceedings of empirical meth-
ods for natural language processing (emnlp).

nal kalchbrenner and phil blunsom. 2013. recurrent
convolutional neural networks for discourse composi-
tionality. in proceedings of the workshop on continu-
ous vector space models and their compositionality,
pages 119   126, so   a, bulgaria, august. association
for computational linguistics.

diederik p kingma and max welling. 2014. auto-
encoding id58. in proceedings of the in-
ternational conference on learning representations
(iclr).

quoc le and tomas mikolov. 2014. distributed rep-
in pro-
resentations of sentences and documents.
ceedings of the international conference on machine
learning (icml).

jiwei li, rumeng li, and eduard hovy. 2014. recursive
deep models for discourse parsing. in proceedings of
empirical methods for natural language processing
(emnlp).

ziheng lin, min-yen kan, and hwee tou ng. 2009.
recognizing implicit discourse relations in the penn
in proceedings of empirical
discourse treebank.
methods for natural language processing (emnlp),
pages 343   351, singapore.

rui lin, shujie liu, muyun yang, mu li, ming zhou,
and sheng li. 2015. hierarchical recurrent neural
in proceedings of
network for document modeling.
empirical methods for natural language processing
(emnlp), pages 899   907, lisbon, september.

christopher d. manning. 2016. computational linguis-
tics and deep learning. computational linguistics,
41(4).

tomas mikolov and geoffrey zweig. 2012. context de-
pendent recurrent neural network language model. in
proceedings of spoken language technology (slt),
pages 234   239.

tomas mikolov, martin kara     at, lukas burget, jan cer-
2010. recurrent
in inter-

nock`y, and sanjeev khudanpur.
neural network based language model.
speech, pages 1045   1048.

dmitrijs milajevs and matthew purver. 2014.

investi-
gating the contribution of distributional semantic in-
formation for dialogue act classi   cation. in proceed-
ings of the 2nd workshop on continuous vector space
models and their compositionality (cvsc), pages 40   
47.

joonsuk park and claire cardie. improving implicit dis-
course relation recognition through feature set opti-
mization. in proceedings of the 13th annual meeting
of the special interest group on discourse and dia-
logue, pages 108   112, seoul, south korea, july. as-
sociation for computational linguistics.

razvan pascanu, tomas mikolov, and yoshua bengio.
2012. on the dif   culty of training recurrent neural
networks. arxiv preprint arxiv:1211.5063.

vu pham, th  eodore bluche, christopher kermorvant,
and j  er  ome louradour. 2014. dropout improves re-
current neural networks for handwriting recognition.
in frontiers in handwriting recognition (icfhr),
2014 14th international conference on, pages 285   
290. ieee.

emily pitler, mridhula raghupathy, hena mehta, ani
nenkova, alan lee, and aravind joshi. 2008. eas-
ily identi   able discourse relations. in proceedings of
the international conference on computational lin-
guistics (coling), pages 87   90, manchester, uk.

emily pitler, annie louis, and ani nenkova. 2009. au-
tomatic sense prediction for implicit discourse rela-
in proceedings of the association for
tions in text.
computational linguistics (acl), suntec, singapore.
rashmi prasad, nikhil dinesh, alan lee, eleni milt-
sakaki, livio robaldo, aravind joshi, and bonnie
webber. 2008. the penn discourse treebank 2.0. in
proceedings of lrec.

attapol t rutherford and nianwen xue. 2014. discov-
ering implicit discourse relations through brown clus-
ter pair representation and coreference patterns.
in
proceedings of the european chapter of the associ-
ation for computational linguistics (eacl).

attapol rutherford and nianwen xue. 2015. improving
the id136 of implicit discourse relations via classi-
fying explicit discourse connectives. pages 799   808,
may   june.

kenji sagae. 2009. analysis of discourse structure with
syntactic dependencies and data-driven shift-reduce
parsing. in proceedings of the 11th international con-
ference on parsing technologies (iwpt   09), pages
81   84, paris, france, october. association for com-
putational linguistics.

richard socher, alex perelygin, jean y wu, jason
chuang, christopher d manning, andrew y ng, and
christopher potts. 2013. recursive deep models for
semantic compositionality over a sentiment treebank.

in proceedings of empirical methods for natural lan-
guage processing (emnlp), seattle, wa.

alessandro sordoni, michel galley, michael auli, chris
brockett, yangfeng ji, meg mitchell, jian-yun nie,
jianfeng gao, and bill dolan. 2015. a neural network
approach to context-sensitive generation of conversa-
tional responses. in proceedings of the north ameri-
can chapter of the association for computational lin-
guistics (naacl), denver, co, may.

nitish srivastava, geoffrey hinton, alex krizhevsky,
ilya sutskever, and ruslan salakhutdinov.
2014.
dropout: a simple way to prevent neural networks
from over   tting. the journal of machine learning
research, 15(1):1929   1958.

andreas stolcke, klaus ries, noah coccaro, eliza-
beth shriberg, rebecca bates, daniel jurafsky, paul
taylor, rachel martin, carol van ess-dykema, and
marie meteer.
2000. dialogue act modeling for
automatic tagging and recognition of conversational
speech. computational linguistics, 26(3):339   373.

joseph turian, lev ratinov, and yoshua bengio. 2010.
word representations: a simple and general method
for semi-supervised learning. pages 384   394.

tian wang and kyunghyun cho.

context
arxiv:1511.03729.

language modelling.

2015.
larger-
arxiv preprint

nick webb, mark hepple, and yorick wilks. 2005. di-
alogue act classi   cation based on intra-utterance fea-
tures. in proceedings of the aaai workshop on spo-
ken language understanding.

bonnie webber, markus egg, and valia kordoni. 2012.
discourse structure and language technology. journal
of natural language engineering, 1.

jason d williams and steve young. 2007. partially ob-
servable id100 for spoken dialog
systems. computer speech & language, 21(2):393   
422.

nianwen xue, hwee tou ng, sameer pradhan, rashmi
prasad, christopher bryant, and attapol t rutherford.
2015. the conll-2015 shared task on shallow dis-
course parsing. in proceedings of the conference on
natural language learning (conll).

biao zhang, jinsong su, deyi xiong, yaojie lu, hong
duan, and junfeng yao. 2015. shallow convolu-
tional neural network for implicit discourse relation
recognition. in proceedings of empirical methods for
natural language processing (emnlp), pages 2230   
2235, lisbon, september.

