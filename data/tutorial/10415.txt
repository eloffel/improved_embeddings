learning natural language id136 with lstm

shuohang wang

school of information systems

singapore management university

jing jiang

school of information systems

singapore management university

6
1
0
2

 

v
o
n
0
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
9
4
8
8
0

.

2
1
5
1
:
v
i
x
r
a

shwang.2014@phdis.smu.edu.sg

jingjiang@smu.edu.sg

abstract

natural language id136 (nli) is a funda-
mentally important task in natural language
processing that has many applications. the
recently released stanford natural language
id136 (snli) corpus has made it possi-
ble to develop and evaluate learning-centered
methods such as deep neural networks for nat-
ural language id136 (nli). in this paper,
we propose a special long short-term mem-
ory (lstm) architecture for nli. our model
builds on top of a recently proposed neural at-
tention model for nli but is based on a sig-
ni   cantly different idea.
instead of deriving
sentence embeddings for the premise and the
hypothesis to be used for classi   cation, our so-
lution uses a match-lstm to perform word-
by-word matching of the hypothesis with the
premise. this lstm is able to place more
emphasis on important word-level matching
results.
in particular, we observe that this
lstm remembers important mismatches that
are critical for predicting the contradiction or
the neutral relationship label. on the snli
corpus, our model achieves an accuracy of
86.1%, outperforming the state of the art.

introduction

1
natural language id136 (nli) is the problem of
determining whether from a premise sentence p one
can infer another hypothesis sentence h (maccart-
ney, 2009). nli is a fundamentally important prob-
lem that has applications in many tasks including
id53, semantic search and automatic
text summarization. there has been much inter-
est in nli in the past decade, especially surround-

ing the pascal recognizing id123
(rte) challenge (dagan et al., 2005). existing so-
lutions to nli range from shallow approaches based
on lexical similarities (glickman et al., 2005) to ad-
vanced methods that consider syntax (mehdad et al.,
2009), perform explicit sentence alignment (mac-
cartney et al., 2008) or use formal logic (clark and
harrison, 2009).

recently, bowman et al. (2015) released the stan-
ford natural language id136 (snli) corpus for
the purpose of encouraging more learning-centered
approaches to nli. this corpus contains around
570k sentence pairs with three labels: entailment,
contradiction and neutral. the size of the corpus
makes it now feasible to train deep neural network
models, which typically require a large amount of
training data. bowman et al. (2015) tested a straight-
forward architecture of deep neural networks for
nli. in their architecture, the premise and the hy-
pothesis are each represented by a sentence embed-
ding vector. the two vectors are then fed into a
multi-layer neural network to train a classi   er. bow-
man et al. (2015) achieved an accuracy of 77.6%
when long short-term memory (lstm) networks
were used to obtain the sentence embeddings.

a more recent work by rockt  aschel et al. (2016)
improved the performance by applying a neural at-
tention model. while their basic architecture is
still based on sentence embeddings for the premise
and the hypothesis, a key difference is that the em-
bedding of the premise takes into consideration the
alignment between the premise and the hypothesis.
this so-called attention-weighted representation of
the premise was shown to help push the accuracy to

83.5% on the snli corpus.

a limitation of the aforementioned two models is
that they reduce both the premise and the hypoth-
esis to a single embedding vector before matching
them; i.e., in the end, they use two embedding vec-
tors to perform sentence-level matching. however,
not all word or phrase-level matching results are
equally important. for example, the matching be-
tween stop words in the two sentences is not likely
to contribute much to the    nal prediction. also, for
a hypothesis to contradict a premise, a single word
or phrase-level mismatch (e.g., a mismatch of the
subjects of the two sentences) may be suf   cient and
other matching results are less important, but this in-
tuition is hard to be captured if we directly match
two sentence embeddings.

in this paper, we propose a new lstm-based ar-
chitecture for learning natural language id136.
different from previous models, our prediction is
not based on whole sentence embeddings of the
premise and the hypothesis.
instead, we use an
lstm to perform word-by-word matching of the
hypothesis with the premise. our lstm sequen-
tially processes the hypothesis, and at each posi-
tion, it tries to match the current word in the hy-
pothesis with an attention-weighted representation
of the premise. matching results that are critical
for the    nal prediction will be    remembered    by the
lstm while less important matching results will be
   forgotten.    we refer to this architecture a match-
lstm, or mlstm for short.

experiments

show that our mlstm model
achieves an accuracy of 86.1% on the snli cor-
pus, outperforming the state of the art. furthermore,
through further analyses of the learned parameters,
we show that the mlstm architecture can indeed
pick up the more important word-level matching re-
sults that need to be remembered for the    nal pre-
diction. in particular, we observe that good word-
level matching results are generally    forgotten    but
important mismatches, which often indicate a con-
tradiction or a neutral relationship, tend to be    re-
membered.   

our code is available online1.

1https://github.com/shuohangwang/

seqmatchseq

section, we    rst

2 model
in this
review lstm. we
then review the word-by-word attention model by
rockt  aschel et al. (2016), which is their best per-
forming model. finally we present our mlstm ar-
chitecture for natural language id136.

2.1 background
lstm: let us    rst brie   y review lstm (hochre-
iter and schmidhuber, 1997). lstm is a special
form of recurrent neural networks (id56s), which
process sequence data. lstm uses a few gate vec-
tors at each position to control the passing of in-
formation along the sequence and thus improves
the modeling of long-range dependencies. while
there are different variations of lstms, here we
present the one adopted by rockt  aschel et al. (2016).
speci   cally, let us use x = (x1, x2, . . . , xn ) to de-
note an input sequence, where xk     rl (1     k    
n). at each position k, there is a set of internal vec-
tors, including an input gate ik, a forget gate fk, an
output gate ok and a memory cell ck. all these vec-
tors are used together to generate a d-dimensional
hidden state hk as follows:

ik =   (wixk + vihk   1 + bi),
fk =   (wfxk + vfhk   1 + bf),
ok =   (woxk + vohk   1 + bo),
ck = fk (cid:12) ck   1 + ik (cid:12) tanh(wcxk + vchk   1 + bc),
hk = ok (cid:12) tanh(ck),
(1)
where    is the sigmoid function, (cid:12) is the element-
wise multiplication of two vectors, and all w*    
rd  l,v*     rd  d and b*     rd are weight matrices
and vectors to be learned.
neural attention model: for the natural
lan-
guage id136 task, we have two sentences xs =
(xs
n ),
where xs is the premise and xt is the hypothesis.
here each x is an embedding vector of the corre-
sponding word. the goal is to predict a label y that
indicates the relationship between xs and xt. in this
paper, we assume y is one of entailment, contradic-
tion and neutral.

m ) and xt = (xt

2, . . . , xs

2, . . . , xt

1, xs

1, xt

rockt  aschel et al. (2016)    rst used two lstms
to process the premise and the hypothesis, respec-
tively, but initialized the second lstm (for the hy-

m(cid:88)

j and ht

pothesis) with the last cell state of the    rst lstm
(for the premise). let us use hs
k to denote
the resulting hidden states corresponding to xs
j and
xt
k, respectively. the main idea of the word-by-word
attention model by rockt  aschel et al. (2016) is to in-
troduce a series of attention-weighted combinations
of the hidden states of the premise, where each com-
bination is for a particular word in the hypothesis.
let us use ak to denote such an attention vector for
word xt
k in the hypothesis. speci   cally, ak is de-
   ned as follows2:

ak =

  kjhs
j,

(2)

j=1

where   kj is an attention weight that encodes the
degree to which xt
k in the hypothesis is aligned with
xs
j in the premise. the attention weight   kj is gen-
erated in the following way:

(cid:80)

exp(ekj)
j(cid:48) exp(ekj(cid:48))

  kj =

,

(3)

where

j + wtht

k + waha

ekj = we    tanh(wshs

(4)
here    is the dot-product between two vectors, the
vector we     rd and all matrices w*     rd  d con-
tain weights to be learned, and ha
k   1 is another hid-
den state which we will explain below.

k   1).

the attention-weighted premise ak essentially
tries to model the relevant parts in the premise with
respect to xt
k, i.e., the kth word in the hypothe-
sis. rockt  aschel et al. (2016) further built an id56
model over {ak}n
k=1 by de   ning the following hid-
den states:

k   1),

ha
k = ak + tanh(vaha

(5)
where va     rd  d is a weight matrix to be learned.
we can see that the last ha
n aggregates all the pre-
vious ak and can be seen as an attention-weighted
the word-by-word attention model by
rockt  aschel et al. (2016) in a different way but the underlying
model is the same. our ha
j) is their
y, our ht
k is their ht, and our   k is their   t. our presentation
is close to the one by bahdanau et al. (2015), with our attention
vectors a corresponding to the context vectors c in their paper.

k is their rt, our hs (all of hs

2we present

representation of the whole premise. rockt  aschel et
al. (2016) then used this ha
n , which represents the
whole premise, together with ht
n , which can be ap-
proximately regarded as an aggregated representa-
tion of the hypothesis3, to predict the label y.

2.2 our model
although the neural attention model by rockt  aschel
et al. (2016) achieved better results than bowman
et al. (2015), we see two limitations. first, the
model still uses a single vector representation of the
premise, namely ha
n , to match the entire hypothe-
sis. we speculate that if we instead use each of the
attention-weighted representations of the premise
for matching, i.e., use ak at position k to match
the hidden state ht
k of the hypothesis while we
go through the hypothesis, we could achieve better
matching results. this can be done using an id56
which at each position takes in both ak and ht
k as its
input and determines how well the overall matching
of the two sentences is up to the current position. in
the end the id56 will produce a single vector repre-
senting the matching of the two entire sentences.

the second limitation is that

the model by
rockt  aschel et al. (2016) does not explicitly allow
us to place more emphasis on the more important
matching results between the premise and the hy-
pothesis and down-weight the less critical ones. for
example, matching of stop words is presumably less
important than matching of content words. also,
some matching results may be particularly critical
for making the    nal prediction and thus should be
remembered. for example, consider the premise
   a dog jumping for a frisbee in the snow.    and
the hypothesis    a cat washes his face and whiskers
with his front paw.    when we sequentially pro-
cess the hypothesis, once we see that the subject
of the hypothesis cat does not match the subject of
the premise dog, we have a high id203 to be-
lieve that there is a contradiction. so this mismatch
should be remembered.

based on the two observations above, we propose
to use an lstm to sequentially match the two sen-
tences. at each position the lstm takes in both ak

3strictly speaking, in the model by rockt  aschel et al. (2016),
ht
n encodes both the premise and the hypothesis because the
two sentences are chained. but ht
n places a higher emphasis on
the hypothesis given the nature of id56s.

portant matching results will be    remembered    by
the lstm while non-essential ones will be    forgot-
ten.    we use the concatenation of ak, which is the
attention-weighted version of the premise for the kth
word in the hypothesis, and ht
k, the hidden state for
the kth word itself, as input to the mlstm.

speci   cally, let us de   ne

mk =

(cid:21)

(cid:20)ak

ht
k

.

we then build the mlstm as follows:

(7)

im
k =   (wmimk + vmihm
f m
k =   (wmfmk + vmfhm
om
k =   (wmomk + vmohm
cm
k = f m

k   1 + im

k   1 + bmi),
k   1 + bmf),
k   1 + bmo),

k (cid:12) tanh(wmcmk + vmchm
k   1

k (cid:12) cm
+bmc),
k (cid:12) tanh(cm
k ).

hm
k = om

(8)

with this mlstm,    nally we use only hm
hidden state, to predict the label y.

n , the last

implementation details

2.3
besides the difference of the lstm architecture, we
also introduce a few other changes from the model
by rockt  aschel et al. (2016). first, we insert a spe-
cial word null to the premise, and we allow words
in the hypothesis to be aligned with this null. this
is inspired by common practice in machine transla-
tion. speci   cally, we introduce a vector hs
0, which
is    xed to be a vector of 0s of dimension d. this hs
0
represents null and is used with other hs
j to derive
the attention vectors {ak}n

second, we use id27s trained from
glove (pennington et al., 2014) instead of id97
vectors. the main reason is that glove covers more
words in the snli corpus than id974.

k=1.

third, for words which do not have pre-trained
id27s, we take the average of the em-
beddings of all the words (in glove) surrounding the
unseen word within a window size of 9 (4 on the left
and 4 on the right) as an approximation of the em-
bedding of this unseen word. then we do not update

4the snli corpus contains 37k unique tokens. around
12.1k of them cannot be found in id97 but only around
4.1k of them cannot be found in glove.

figure 1: the top    gure depicts the model by rockt  aschel et al.
(2016) and the bottom    gure depicts our model. here hs rep-
resents all the hidden states hs
j. note that in the top model each
ha
k represents a weighted version of the premise only, while
in our model, each hm
k represents the matching between the
premise and the hypothesis up to position k.

and ht
k as its input. figure 1 gives an overview of
our model in contrast to the model by rockt  aschel
et al. (2016).

speci   cally, our model works as follows. first,
similar to rockt  aschel et al. (2016), we process the
premise and the hypothesis using two lstms, but
we do not feed the last cell state of the premise to
the lstm of the hypothesis. this is because we do
not need the lstm for the hypothesis to encode any
knowledge about the premise but we will match the
premise with the hypothesis using the hidden states
of the two lstms. again, we use hs
k to
represent these hidden states.

j and ht

next, we generate the attention vectors ak simi-
larly to eqn (2). however, eqn (4) will be replaced
by the following equation:

ekj = we    tanh(wshs

j + wtht

k + wmhm

k   1).

(6)

the only difference here is that we use a hidden state
hm instead of ha, and the way we de   ne hm is very
different from the de   nition of ha.

our hm

k is the hidden state at position k generated
from our mlstm. this lstm models the match-
ing between the premise and the hypothesis.
im-

any id27 when learning our model. al-
though this is a very crude approximation, it reduces
the number of parameters we need to update, and as
it turns out, we can still achieve better performance
than rockt  aschel et al. (2016).

3 experiments

3.1 experiment settings
data: we use the snli corpus to test the effective-
ness of our model. the original data set contains
570,152 sentence pairs, each labeled with one of the
following relationships: entailment, contradiction,
neutral and    , where     indicates a lack of consensus
from the human annotators. we discard the sentence
pairs labeled with     and keep the remaining ones for
our experiments. in the end, we have 549,367 pairs
for training, 9,842 pairs for development and 9,824
pairs for testing. this follows the same data partition
used by bowman et al. (2015) in their experiments.
we perform three-class classi   cation and use accu-
racy as our evaluation metric.
parameters: we use the adam method (kingma
and ba, 2014) with hyperparameters   1 set to 0.9
and   2 set to 0.999 for optimization. the initial
learning rate is set to be 0.001 with a decay ratio
of 0.95 for each iteration. the batch size is set to
be 30. we experiment with d = 150 and d = 300
where d is the dimension of all the hidden states.
methods for comparison: we mainly want to
compare our model with the word-by-word atten-
tion model by rockt  aschel et al. (2016) because
this model achieved the state-of-the-art performance
on the snli corpus. to ensure fair comparison,
besides comparing with the accuracy reported by
rockt  aschel et al. (2016), we also re-implemented
their model and report the performance of our im-
plementation. we also consider a few variations of
our model. speci   cally, the following models are
implemented and tested in our experiments:
    word-by-word attention (d = 150): this is
our implementation of the word-by-word at-
tention model by rockt  aschel et al. (2016),
where we set the dimension of the hidden states
to 150. the differences between our imple-
mentation and the original implementation by
rockt  aschel et al. (2016) are the following: (1)
we also add a null token to the premise for

prediction

n

n
e
c

2628
340
250

ground truth

e
286
3005
77

c
255
159
2823

table 2: the confusion matrix of the results by mlstm with
d = 300. n, e and c correspond to neutral, entailment and
contradiction, respectively.

j and ht

with d set to 150.

matching. (2) we do not feed the last cell state
of the lstm for the premise to the lstm for
the hypothesis, to keep it consistent with the
implementation of our model.
(3) for word
representation, we also use the glove word
embeddings and we do not update the word
embeddings. for unseen words, we adopt the
same strategy as described in section 2.3.
    mlstm (d = 150): this is our mlstm model
    mlstm with bi-lstm sentence modeling
(d = 150): this is the same as the model
above except that when we derive the hidden
states hs
k of the two sentences, we use
bi-lstms (graves, 2012) instead of lstms.
we implement this model to see whether bi-
lstms allow us to better align the sentences.
    mlstm (d = 300): this is our mlstm model
    mlstm with id27 (d = 300): this
is the same as the model above except that we
directly use the id27 vectors xs
j and
xt
k instead of the hidden states hs
k in our
model. in this case, each attention vector ak is
a weighted sum of {xs
j}m
j=1. we experiment
with this setting because we hypothesize that
the effectiveness of our model is largely related
to the mlstm architecture rather than the use
of lstms to process the original sentences.

with d set to 300.

j and ht

3.2 main results
table 1 compares the performance of the various
models we tested together with some previously re-
ported results.

we have the following observations: (1) first of
all, we can see that when we set d to 300, our model
achieves an accuracy of 86.1% on the test data,
which to the best of our knowledge is the highest on

train dev

model
lstm [bowman et al. (2015)]
classi   er [bowman et al. (2015)]
lstm shared [rockt  aschel et al. (2016)]
word-by-word attention [rockt  aschel et al. (2016)]
word-by-word attention (our implementation)
mlstm
mlstm with bi-lstm sentence modeling
mlstm
mlstm with id27

d
100
-
159
100
150
150
150
300
300

|  |w+m
10m
-
3.9m
3.9m
340k
544k
1.4m
1.9m
1.3m

|  |m
221k 84.4
-
99.7
252k 84.4
252k 85.3
340k 85.5
544k 91.0
1.4m 91.3
1.9m 92.0
1.3m 88.6

test
77.6
78.2
81.4
83.5
82.6
85.7
86.0
86.1
85.3

-
-
83.0
83.7
83.3
86.2
86.6
86.9
85.4

table 1: experiment results in terms of accuracy. d is the dimension of the hidden states. |  |w+m is the total number of parameters
and |  |m is the number of parameters excluding the id27s. note that the    ve models in the last section were implemented
by us while the other results were taken directly from previous papers. note also that for the    ve models in the last section, we do
not update id27s so |  |w+m is the same as |  |m. the three columns on the right are the accuracies of the trained models
on the training data, the development data and the test data, respectively.

id

premise

sentence
a dog jumping for a frisbee in the snow.

example 1 an animal is outside in the cold weather, playing with a plastic toy.

hypothesis example 2 a cat washed his face and whiskers with his front paw.

example 3 a pet is enjoying a game of fetch with his owner.

label

entailment
contradiction
neutral

table 3: three examples of sentence pairs with different relationship labels. the second hypothesis is a contradiction because it
mentions a completely different event. the third hypothesis is neutral to the premise because the phrase    with his owner    cannot
be inferred from the premise.

this data set. (2) if we compare our mlstm model
with our implementation of the word-by-word atten-
tion model by rockt  aschel et al. (2016) under the
same setting with d = 150, we can see that our per-
formance on the test data (85.7%) is higher than that
of their model (82.6%). we also tested statistical
signi   cance and found the improvement to be statis-
tically signi   cant at the 0.001 level. (3) the perfor-
mance of mlstm with bi-lstm sentence modeling
compared with the model with standard lstm sen-
tence modeling when d is set to 150 shows that us-
ing bi-lstm to process the original sentences helps
(86.0% vs. 85.7% on the test data), but the dif-
ference is small and the complexity of bi-lstm is
much higher than lstm. therefore when we in-
creased d to 300 we did not experiment with bi-
lstm sentence modeling. (4) interestingly, when
we experimented with the mlstm model using
the pre-trained id27s instead of lstm-
generated hidden states as initial representations of
the premise and the hypothesis, we were able to
achieve an accuracy of 85.3% on the test data, which

is still better than previously reported state of the art.
this suggests that the mlstm architecture coupled
with the attention model works well, regardless of
whether or not we use lstm to process the original
sentences.

because the nli task is a three-way classi   ca-
tion problem, to better understand the errors, we also
show the confusion matrix of the results obtained by
our mlstm model with d = 300 in table 2. we
can see that there is more confusion between neu-
tral and entailment and between neutral and contra-
diction than between entailment and contradiction.
this shows that neutral is relatively hard to capture.

3.3 further analyses

to obtain a better understanding of how our pro-
posed model actually performs the matching be-
tween a premise and a hypothesis, we further con-
duct the following analyses. first, we look at the
learned word-by-word alignment weights   kj
to
check whether the soft alignment makes sense. this
is the same as what was done by rockt  aschel et al.

figure 2: the alignment weights and the gate vectors of the three examples.

(2016). we then look at the values of the various
gate vectors of the mlstm. by looking at these val-
ues, we aim to check (1) whether the model is able
to differentiate between more important and less im-
portant word-level matching results, and (2) whether
the model forgets certain matching results and re-
members certain other ones.

relationship labels. they are given in table 3. the
values of the alignment weights and the gate vectors
are plotted in figure 2.

besides using the three examples, we will also
give some overall statistics of the parameter values
to con   rm our observations with the three examples.

word alignment

to conduct the analyses, we choose three ex-
amples and display the various learned parameter
values. these three sentence pairs share the same
premise but have different hypotheses and different

first, let us look at the top-most plots of fig-
ure 2. these plots show the alignment weights   kj
between the hypothesis and the premise, where a
darker color corresponds to a larger value of   kj.

recall that   kj is the degree to which the kth word
in the hypothesis is aligned with the jth word in the
premise. also recall that the weights   kj are con-
   gured such that for the same k all the   kj add up
to 1. this means the weights in the same row in
these plots add up to 1. from the three plots we can
see that the alignment weights generally make sense.
for example, in example 1,    animal    is strongly
aligned with    dog    and    toy    aligned with    frisbee.   
the phrase    cold weather    is aligned with    snow.   
in example 3, we also see that    pet    is strongly
aligned with    dog    and    game    aligned with    fris-
bee.   

in example 2,    cat    is strongly aligned with    dog   
and    washes    is aligned with    jumping.    it may ap-
pear that these matching results are wrong. how-
ever,    dog    is likely the best match for    cat    among
all the words in the premise, and as we will show
later, this match between    cat    and    dog    is actu-
ally a strong indication of a contradiction between
the two sentences. the same explanation applies to
the match between    washes    and    jumping.   

we also observe that some words are aligned
with the null token we inserted. for example,
the word    is    in the hypothesis in example 1 does
not correspond to any word in the premise and is
therefore aligned with null. the words    face    and
   whiskers    in example 2 and    owner    in example 3
are also aligned with null. intuitively, if some im-
portant content words in the hypothesis are aligned
with null, it is more likely that the relationship la-
bel is either contradiction or neutral.

values of gate vectors

next, let us look at the values of the learned gate
vectors of our mlstm for the three examples. we
show these values under the setting where d is set to
150. each row of these plots corresponds to one of
the 150 dimensions. again, a darker color indicates
a higher value.

an input gate controls whether the input at the
current position should be used in deriving the    nal
hidden state of the current position. from the three
plots of the input gates, we can observe that gener-
ally for stop words such as prepositions and articles
the input gates have lower values, suggesting that the
matching of these words is less important. on the
other hand, content words such as nouns and verbs

tend to have higher values of the input gates, which
also makes sense because these words are generally
more important for determining the    nal relation-
ship label.

to further verify the observation above, we com-
pute the average input gate values for stop words
and the other content words. we    nd that the former
has an average value of 0.287 with a standard devia-
tion of 0.084 while the latter has an average value of
0.347 with a standard deviation of 0.116. this shows
that indeed generally stop words have lower input
gate values.
interestingly, we also    nd that some
stop words may have higher input gate values if they
are critical for the classi   cation task. for example,
the negation word    not    has an average input gate
value of 0.444 with a standard deviation of 0.104.

overall, the values of the input gates con   rm that
the mlstm helps differentiate the more important
word-level matching results from the less important
ones.

next, let us look at the forget gates. recall that
a forget gate controls the importance of the previ-
ous cell state in deriving the    nal hidden state of the
current position. higher values of a forget gate indi-
cate that we need to remember the previous cell state
and pass it on whereas lower values indicate that we
should probably forget the previous cell. from the
three plots of the forget gates, we can see that overall
the colors are the lightest for example 1, which is an
entailment. this suggests that when the hypothesis
is an entailment of the premise, the mlstm tends
to forget the previous matching results. on the other
hand, for example 2 and example 3, which are con-
tradiction and neutral, we see generally darker col-
ors. in particular, in example 2, we can see that the
colors are consistently dark starting from the word
   his    in the hypothesis until the end. we believe the
explanation is that after the mlstm processes the
   rst three words of the hypothesis,    a cat washes,    it
sees that the matching between    cat    and    dog    and
between    washes    and    jumping    is a strong indica-
tion of a contradiction, and therefore these matching
results need to be remembered until the end of the
mlstm for the    nal prediction.

we have also checked the forget gates of the other
sentence pairs in the test data by computing the av-
erage forget gate values and the standard deviations
for entailment, neutral and contradiction, respec-

tively. we    nd that the values are 0.446  0.123,
0.507  0.148 and 0.536  0.170, respectively. for
contradiction and neutral, the forget gates start to
have higher values from certain positions of the hy-
potheses.

based on the observations above, we hypothesize
that the way the mlstm works is as follows. it re-
members important mismatches, which are useful
for predicting the contradiction or the neutral re-
lationship, and forgets good matching results. at
the end of the mlstm, if no important mismatch
is remembered, the    nal classi   er will likely pre-
dict entailment by default. otherwise, depending on
the kind of mismatch remembered, the classi   er will
predict either contradiction or neutral.

for the output gates, we are not able to draw any
important conclusion except that the output gates
seem to be positively correlated with the input gates
but they tend to be darker than the input gates.

4 related work

there has been much work on natural language in-
ference. shallow methods rely mostly on lexical
similarities but are shown to be robust. for example,
bowman et al. (2015) experimented with a lexical-
ized classi   er-based method, which only uses lexi-
cal information and achieves an accuracy of 78.2%
on the snli corpus. more advanced methods use
syntactic structures of the sentences to help match-
ing them. for example, mehdad et al. (2009) ap-
plied syntactic-semantic tree kernels for recogniz-
ing id123. because id136 is es-
sentially a logic problem, methods based on for-
mal logic (clark and harrison, 2009) or natural
logic (maccartney, 2009) have also been proposed.
a comprehensive review on existing work can be
found in the book by dagan et al. (2013).

the work most relevant to ours is the recently
proposed neural attention model-based method by
rockt  aschel et al. (2016), which we have detailed
in previous sections. neural id12 have
recently been applied to some natural language pro-
cessing tasks including machine translation (bah-
danau et al., 2015), abstractive summarization (rush
et al., 2015) and id53 (hermann et
al., 2015). rockt  aschel et al. (2016) showed that
the neural attention model could help derive a bet-

ter representation of the premise to be used to match
the hypothesis, whereas in our work we also use it to
derive representations of the premise that are used to
sequentially match the words in the hypothesis.

the snli corpus is new and so far it has
only been used in a few studies. besides the
work by bowman et al. (2015) themselves and by
rockt  aschel et al. (2016), there are two other studies
which used the snli corpus. vendrov et al. (2015)
used a skip-thought model proposed by kiros et al.
(2015) to the nli task and reported an accuracy of
81.5% on the test data. mou et al. (2015) used tree-
based id98 encoders to obtain sentence embeddings
and achieved an accuracy of 82.1%.

5 conclusions and future work

in this paper, we proposed a special lstm ar-
chitecture for the task of natural language infer-
ence. based on a recent work by rockt  aschel et al.
(2016), we    rst used neural id12 to de-
rive attention-weighted vector representations of the
premise. we then designed a match-lstm that pro-
cesses the hypothesis word by word while trying to
match the hypothesis with the premise. the last hid-
den state of this mlstm can be used for predicting
the relationship between the premise and the hypoth-
esis. experiments on the snli corpus showed that
the mlstm model outperformed the state-of-the-art
performance reported so far on this data set. more-
over, closer analyses on the gate vectors revealed
that our mlstm indeed remembers and passes on
important matching results, which are typically mis-
matches that indicate a contradiction or a neutral re-
lationship between the premise and the hypothesis.
with the large number of parameters to learn, an
inevitable limitation of our model is that a large
training data set is needed to learn good model pa-
rameters. indeed some preliminary experiments ap-
plying our mlstm to the sick corpus (marelli
et al., 2014), a smaller id123 bench-
mark data set, did not give very good results. we
believe that this is because our model learns ev-
erything from scratch except using the pre-trained
id27s. a future direction would be to
incorporate other resources such as the paraphrase
database (ganitkevitch et al., 2013) into the learning
process.

vectors. in advances in neural information process-
ing systems.

[maccartney et al.2008] bill maccartney, michel galley,
and christopher d manning. 2008. a phrase-based
alignment model for natural language id136.
in
proceedings of the conference on empirical methods
in natural language processing.

[maccartney2009] bill maccartney. 2009. natural lan-

guage id136. ph.d. thesis, stanford university.

[marelli et al.2014] marco marelli, stefano menini,
marco baroni, luisa bentivogli, raffaella bernardi,
and roberto zamparelli. 2014. a sick cure for the
evaluation of compositional distributional semantic
in proceedings of the ninth international
models.
conference on language resources and evaluation.

[mehdad et al.2009] yashar mehdad, alessandro mos-
chitti1, and fabio massiomo zanzotto.
2009.
semker: syntactic/semantic kernels for recognizing
in proceedings of the text anal-
id123.
ysis conference.

[mou et al.2015] lili mou, men rui, ge li, yan xu,
lu zhang, rui yan, and zhi jin. 2015. recogniz-
ing entailment and contradiction by tree-based convo-
lution. arxiv preprint arxiv:1512.08422.
pennington,

richard
socher, and christopher d manning. 2014. glove:
global vectors for word representation. proceedings
of the conference on empirical methods in natural
language processing.

[pennington et al.2014] jeffrey

[rockt  aschel et al.2016] tim rockt  aschel,

edward
grefenstette, karl moritz hermann, tom  a  s ko  cisk`y,
and phil blunsom. 2016. reasoning about entailment
with neural attention. in proceedings of the interna-
tional conference on learning representations.

[rush et al.2015] alexander m rush, sumit chopra, and
jason weston. 2015. a neural attention model for
abstractive sentence summarization. proceedings of
the conference on empirical methods in natural lan-
guage processing.

[vendrov et al.2015] ivan vendrov, ryan kiros, sanja
fidler,
order-
embeddings of images and language. arxiv preprint
arxiv:1511.06361.

and raquel urtasun.

2015.

references
[bahdanau et al.2015] dzmitry bahdanau, hyunghyun
cho, and yoshua bengio.
2015. neural machine
translation by jointly learning to align and translate. in
proceedings of the international conference on learn-
ing representations.

[bowman et al.2015] samuel r bowman, gabor angeli,
christopher potts, and christopher d manning. 2015.
a large annotated corpus for learning natural language
id136. in proceedings of the 2015 conference on
empirical methods in natural language processing.
[clark and harrison2009] peter clark and phil harrison.
2009. an id136-based approach to recognizing en-
tailment. in proceedings of the text analysis confer-
ence.

[dagan et al.2005] ido dagan, oren glickman,

and
bernardo magnini. 2005. the pascal recognising
id123 challenge. in proceedings of the
pascal challenges workshop on recognizing tex-
tual entailment.

[dagan et al.2013] ido dagan, dan roth, mark sam-
mons, and fabio massimo zanzotto. 2013. recog-
nizing id123: models and applications.
synthesis lectures on human language technolo-
gies. morgan & claypool publishers.

[ganitkevitch et al.2013] juri ganitkevitch, benjamin
van durme, and chris callison-burch. 2013. ppdb:
the paraphrase database. in proceedings of the 2013
conference of the north american chapter of the
association for computational linguistics.

[glickman et al.2005] oren glickman, ido dagan, and
moshe koppel. 2005. web based probabilistic textual
entailment. in proceedings of the pascal challenges
workshop on recognizing id123.

[graves2012] alex graves. 2012. supervised sequence
labelling with recurrent neural networks, volume 385.
springer.

[hermann et al.2015] karl moritz hermann, tomas ko-
cisky, edward grefenstette, lasse espeholt, will kay,
mustafa suleyman, and phil blunsom. 2015. teach-
ing machines to read and comprehend. in advances in
neural information processing systems.

[hochreiter and schmidhuber1997] sepp hochreiter and
j  urgen schmidhuber. 1997. long short-term memory.
neural computation, 9(8):1735   1780.

[kingma and ba2014] diederik kingma and jimmy ba.
2014. adam: a method for stochastic optimization.
proceedings of the international conference on learn-
ing representations.

[kiros et al.2015] ryan kiros, yukun zhu, ruslan r
salakhutdinov, richard zemel, raquel urtasun, an-
tonio torralba, and sanja fidler. 2015. skip-thought

