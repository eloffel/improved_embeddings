learning to compose neural networks for id53

jacob andreas and marcus rohrbach and trevor darrell and dan klein

department of electrical engineering and computer sciences

{jda,rohrbach,trevor,klein}@eecs.berkeley.edu

university of california, berkeley

6
1
0
2

 

n
u
j
 

7

 
 
]
l
c
.
s
c
[
 
 

4
v
5
0
7
1
0

.

1
0
6
1
:
v
i
x
r
a

abstract

we describe a id53 model that
applies to both images and structured knowl-
edge bases. the model uses natural lan-
guage strings to automatically assemble neu-
ral networks from a collection of composable
modules. parameters for these modules are
learned jointly with network-assembly param-
eters via id23, with only
(world, question, answer) triples as supervi-
sion. our approach, which we term a dynamic
neural module network, achieves state-of-the-
art results on benchmark datasets in both vi-
sual and structured domains.

introduction

1
this paper presents a compositional, attentional
model for answering questions about a variety of
world representations, including images and struc-
tured knowledge bases. the model translates from
questions to dynamically assembled neural net-
works, then applies these networks to world rep-
resentations (images or knowledge bases) to pro-
duce answers. we take advantage of two largely
independent lines of work: on one hand, an exten-
sive literature on answering questions by mapping
from strings to logical representations of meaning;
on the other, a series of recent successes in deep
neural models for image recognition and captioning.
by constructing neural networks instead of logical
forms, our model leverages the best aspects of both
linguistic compositionality and continuous represen-
tations.

our model has two components, trained jointly:
   rst, a collection of neural    modules    that can be
freely composed (figure 1a); second, a network lay-
out predictor that assembles modules into complete
deep networks tailored to each question (figure 1b).

figure 1: a learned syntactic analysis (a) is used to assemble a
collection of neural modules (b) into a deep neural network (c),
and applied to a world representation (d) to produce an answer.

previous work has used manually-speci   ed modular
structures for visual learning (andreas et al., 2016).
here we:

    learn a network structure predictor jointly with

module parameters themselves

    extend visual primitives from previous work to

reason over structured world representations

training data consists of (world, question, answer)
triples: our approach requires no supervision of net-
work layouts. we achieve state-of-the-art perfor-
mance on two markedly different question answer-
ing tasks: one with questions about natural im-
ages, and another with more compositional ques-
tions about united states geography.1

2 deep networks as functional programs
we begin with a high-level discussion of the kinds
of composed networks we would like to learn.

1we have released our code at http://github.com/

jacobandreas/nmn2

what cities are in georgia?atlantaandlookupgeorgiafindcitygeorgiaatlantamontgomeryknowledge sourcerelateinnetwork layout (section 4.2)find[city]lookup[georgia]relate[in]and(b)module inventory (section 4.1)findlookupandrelate(a)(c)(d)andreas et al. (2016) describe a heuristic ap-
proach for decomposing visual id53
tasks into sequence of modular sub-problems. for
example, the question what color is the bird? might
be answered in two steps:    rst,    where is the bird?   
(figure 2a), second,    what color is that part of the
image?    (figure 2c). this    rst step, a generic mod-
ule called find, can be expressed as a fragment of
a neural network that maps from image features and
a lexical item (here bird) to a distribution over pix-
els. this operation is commonly referred to as the
attention mechanism, and is a standard tool for ma-
nipulating images (xu et al., 2015) and text repre-
sentations (hermann et al., 2015).

the    rst contribution of this paper is an exten-
sion and generalization of this mechanism to enable
fully-differentiable reasoning about more structured
semantic representations. figure 2b shows how the
same module can be used to focus on the entity
georgia in a non-visual grounding domain; more
generally, by representing every entity in the uni-
verse of discourse as a feature vector, we can obtain
a distribution over entities that corresponds roughly
to a logical set-valued denotation.

having obtained such a distribution, existing neu-
ral approaches use it to immediately compute a
weighted average of image features and project back
into a labeling decision   a describe module (fig-
ure 2c). but the logical perspective suggests a num-
ber of novel modules that might operate on atten-
tions: e.g. combining them (by analogy to conjunc-
tion or disjunction) or inspecting them directly with-
out a return to feature space (by analogy to quanti   -
cation, figure 2d). these modules are discussed in
detail in section 4. unlike their formal counterparts,
they are differentiable end-to-end, facilitating their
integration into learned models. building on previ-
ous work, we learn behavior for a collection of het-
erogeneous modules from (world, question, answer)
triples.

the second contribution of this paper is a model
for learning to assemble such modules composition-
ally.
isolated modules are of limited use   to ob-
tain expressive power comparable to either formal
approaches or monolithic deep networks, they must
be composed into larger structures. figure 2 shows
simple examples of composed structures, but for
realistic question-answering tasks, even larger net-

figure 2: simple neural module networks, corresponding to
the questions what color is the bird? and are there any states?
(a) a neural find module for computing an attention over
pixels. (b) the same operation applied to a knowledge base.
(c) using an attention produced by a lower module to identify
the color of the region of the image attended to. (d) performing
quanti   cation by evaluating an attention directly.

works are required. thus our goal is to automati-
cally induce variable-free, tree-structured computa-
tion descriptors. we can use a familiar functional
notation from formal semantics (e.g. liang et al.,
2011) to represent these computations.2 we write
the two examples in figure 2 as

(describe[color] find[bird])

and

(exists find[state])

respectively. these are network layouts: they spec-
ify a structure for arranging modules (and their lex-
ical parameters) into a complete network. andreas
et al. (2016) use hand-written rules to deterministi-
cally transform dependency trees into layouts, and
are restricted to producing simple structures like the
above for non-synthetic data. for full generality, we
will need to solve harder problems, like transform-
ing what cities are in georgia? (figure 1) into

(and

find[city]
(relate[in] lookup[georgia]))

in this paper, we present a model for learning to se-
lect such structures from a set of automatically gen-
erated candidates. we call this model a dynamic
neural module network.

2but note that unlike formal semantics, the behavior of the

primitive functions here is itself unknown.

black	and	whitegeorgiaatlantamontgomerygeorgiaatlantamontgomeryexiststruefindbirddescribecolorfindstate(a)(b)(c)(d)3 related work

there is an extensive literature on database ques-
tion answering, in which strings are mapped to log-
ical forms, then evaluated by a black-box execu-
tion model to produce answers. supervision may be
provided either by annotated logical forms (wong
and mooney, 2007; kwiatkowski et al., 2010; an-
dreas et al., 2013) or from (world, question, answer)
triples alone (liang et al., 2011; pasupat and liang,
2015).
in general the set of primitive functions
from which these logical forms can be assembled is
   xed, but one recent line of work focuses on induc-
ing new predicates functions automatically, either
from perceptual features (krishnamurthy and kol-
lar, 2013) or the underlying schema (kwiatkowski
et al., 2013). the model we describe in this paper
has a uni   ed framework for handling both the per-
ceptual and schema cases, and differs from existing
work primarily in learning a differentiable execution
model with continuous evaluation results.

neural models for id53 are also a
subject of current interest. these include approaches
that model the task directly as a multiclass classi   -
cation problem (iyyer et al., 2014), models that at-
tempt to embed questions and answers in a shared
vector space (bordes et al., 2014) and attentional
models that select words from documents sources
(hermann et al., 2015). such approaches generally
require that answers can be retrieved directly based
on surface linguistic features, without requiring in-
termediate computation. a more structured ap-
proach described by yin et al. (2015) learns a query
execution model for database tables without any nat-
ural language component. previous efforts toward
unifying formal logic and representation learning in-
clude those of grefenstette (2013), krishnamurthy
and mitchell (2013), lewis and steedman (2013),
and beltagy et al. (2013).

the visually-grounded component of this work
relies on recent advances in convolutional net-
works for id161 (simonyan and zisser-
man, 2014), and in particular the fact that late convo-
lutional layers in networks trained for image recog-
nition contain rich features useful for other vision
tasks while preserving spatial information. these
features have been used for both image captioning
(xu et al., 2015) and visual qa (yang et al., 2015).

most previous approaches to visual question an-
swering either apply a recurrent model to deep rep-
resentations of both the image and the question (ren
et al., 2015; malinowski et al., 2015), or use the
question to compute an attention over the input im-
age, and then answer based on both the question and
the image features attended to (yang et al., 2015;
xu and saenko, 2015). other approaches include
the simple classi   cation model described by zhou
et al. (2015) and the dynamic parameter prediction
network described by noh et al. (2015). all of
these models assume that a    xed computation can
be performed on the image and question to compute
the answer, rather than adapting the structure of the
computation to the question.

as noted, andreas et al. (2016) previously con-
sidered a simple generalization of these attentional
approaches in which small variations in the net-
work structure per-question were permitted, with
the structure chosen by (deterministic) syntactic pro-
cessing of questions. other approaches in this gen-
eral family include the    universal parser    sketched
by bottou (2014), the graph transformer networks
of bottou et al. (1997), the knowledge-based neu-
ral networks of towell and shavlik (1994) and the
id56s of socher et al. (2013),
which use a    xed tree structure to perform further
linguistic analysis without any external world rep-
resentation. we are unaware of previous work that
simultaneously learns both parameters for and struc-
tures of instance-speci   c networks.
4 model
recall that our goal is to map from questions and
world representations to answers. this process in-
volves the following variables:
1. w a world representation
2. x a question
3. y an answer
4. z a network layout
5.    a collection of model parameters

our model is built around two distributions: a lay-
out model p(z|x;   (cid:96)) which chooses a layout for a
sentence, and a execution model pz(y|w;   e) which
applies the network speci   ed by z to w.

for ease of presentation, we introduce these mod-
els in reverse order. we    rst imagine that z is always

observed, and in section 4.1 describe how to evalu-
ate and learn modules parameterized by   e within
   xed structures. in section 4.2, we move to the real
scenario, where z is unknown. we describe how to
predict layouts from questions and learn   e and   (cid:96)
jointly without layout supervision.

4.1 evaluating modules
given a layout z, we assemble the corresponding
modules into a full neural network (figure 1c), and
apply it to the id99. interme-
diate results    ow between modules until an answer
is produced at the root. we denote the output of the

network with layout z on input world w as (cid:74)z(cid:75)w;
can alternatively write (cid:74)m(h1, h2)(cid:75) for a top-level

when explicitly referencing the substructure of z, we

module m with submodule outputs h1 and h2. we
then de   ne the execution model:

pz(y|w) = ((cid:74)z(cid:75)w)y

(1)

(this assumes that the root module of z produces
a distribution over labels y.) the set of possible
layouts z is restricted by module type constraints:
some modules (like find above) operate directly on
the input representation, while others (like describe
above) also depend on input from speci   c earlier
modules. two base types are considered in this pa-
per are attention (a distribution over pixels or enti-
ties) and labels (a distribution over answers).

parameters are tied across multiple instances of
the same module, so different instantiated networks
may share some parameters but not others. modules
have both parameter arguments (shown in square
brackets) and ordinary inputs (shown in parenthe-
ses). parameter arguments, like the running bird
example in section 2, are provided by the layout,
and are used to specialize module behavior for par-
ticular lexical items. ordinary inputs are the re-
sult of computation lower in the network.
in ad-
dition to parameter-speci   c weights, modules have
global weights shared across all instances of the
module (but not shared with other modules). we
write a, a, b, b, . . . for global weights and ui, vi for
weights associated with the parameter argument i.
    and (cid:12) denote (possibly broadcasted) elementwise
addition and multiplication respectively. the com-
plete set of global weights and parameter-speci   c
weights constitutes   e. every module has access to

the world representation, represented as a collection
of vectors w1, w2, . . . (or w expressed as a matrix).
the nonlinearity    denotes a recti   ed linear unit.

the modules used in this paper are shown below,
with names and type constraints in the    rst row and a
description of the module   s computation following.
(    attention)
lookup
lookup[i] produces an attention focused entirely at the
index f (i), where the relationship f between words
and positions in the input map is known ahead of time
(e.g. string matches on database    elds).

(cid:74)lookup[i](cid:75) = ef (i)

(2)
where ei is the basis vector that is 1 in the ith position
and 0 elsewhere.

(    attention)
find
find[i] computes a distribution over indices by con-
catenating the parameter argument with each position
of the input feature map, and passing the concatenated
vector through a mlp:

(cid:74)find[i](cid:75) = softmax(a (cid:12)   (bvi     cw     d)) (3)

(attention     attention)
relate
relate directs focus from one region of the input to
another. it behaves much like the find module, but
also conditions its behavior on the current region of
k hkwk, where hk is the

attention h. let   w(h) = (cid:80)

kth element of h. then,

(cid:74)relate[i](h)(cid:75) = softmax(a (cid:12)

  (bvi     cw     d   w(h)     e))

(4)
(attention*     attention)
and
and performs an operation analogous to set intersec-
tion for attentions. the analogy to probabilistic logic
suggests multiplying probabilities:

(cid:74)and(h1, h2, . . .)(cid:75) = h1 (cid:12) h2 (cid:12)       

(5)

(attention     labels)
describe
describe[i] computes a weighted average of w under
the input attention. this average is then used to predict
an answer representation. with   w as above,

(cid:74)describe[i](h)(cid:75) = softmax(a  (b   w(h) + vi)) (6)

(attention     labels)
exists
exists is the existential quanti   er, and inspects the
incoming attention directly to produce a label, rather
than an intermediate feature vector like describe:

(cid:16)(cid:0) max

k

(cid:17)

(cid:1)a + b

hk

(cid:74)exists](h)(cid:75) = softmax

(7)

mapped onto a (possibly smaller) set of semantic
primitives. second, these semantic primitives must
be combined into a structure that closely, but not ex-
actly, parallels the structure provided by syntax. for
example, state and province might need to be identi-
   ed with the same    eld in a database schema, while
all states have a capital might need to be identi   ed
with the correct (in situ) quanti   er scope.

while we cannot avoid the structure selection
problem, continuous representations simplify the
lexical selection problem. for modules that accept
a vector parameter, we associate these parameters
with words rather than semantic tokens, and thus
turn the combinatorial optimization problem asso-
ciated with lexicon induction into a continuous one.
now, in order to learn that province and state have
the same denotation, it is suf   cient to learn that their
associated parameters are close in some embedding
space   a task amenable to id119. (note
that this is easy only in an optimizability sense,
and not an information-theoretic one   we must still
learn to associate each independent lexical item with
the correct vector.) the remaining combinatorial
problem is to arrange the provided lexical items into
the right computational structure.
in this respect,
layout prediction is more like syntactic parsing than
ordinary id29, and we can rely on an
off-the-shelf syntactic parser to get most of the way
there. in this work, syntactic structure is provided by
the stanford dependency parser (de marneffe and
manning, 2008).

the construction of layout candidates is depicted

in figure 3, and proceeds as follows:

1. represent the input sentence as a dependency

tree.

2. collect all nouns, verbs, and prepositional
phrases that are attached directly to a wh-word
or copula.

3. associate each of these with a layout frag-
ment: ordinary nouns and verbs are mapped
to a single find module. proper nouns to a sin-
gle lookup module. prepositional phrases are
mapped to a depth-2 fragment, with a relate
module for the preposition above a find mod-
ule for the enclosed head noun.

4. form subsets of this set of layout fragments.
for each subset, construct a layout candidate by

figure 3: generation of layout candidates. the input sentence
(a) is represented as a dependency parse (b). fragments of this
dependency parse are then associated with appropriate modules
(c), and these fragments are assembled into full layouts (d).

with z observed, the model we have described
so far corresponds largely to that of andreas et al.
(2016), though the module inventory is different   
in particular, our new exists and relate modules
do not depend on the two-dimensional spatial struc-
ture of the input. this enables generalization to non-
visual world representations.

learning in this simpli   ed setting is straightfor-
ward. assuming the top-level module in each layout
is a describe or exists module, the fully- instan-
(cid:80)
tiated network corresponds to a distribution over la-
bels conditioned on layouts. to train, we maximize
(w,y,z) log pz(y|w;   e) directly. this can be under-
stood as a parameter-tying scheme, where the deci-
sions about which parameters to tie are governed by
the observed layouts z.

4.2 assembling networks
next we describe the layout model p(z|x;   (cid:96)). we
   rst use a    xed syntactic parse to generate a small
set of candidate layouts, analogously to the way
a semantic grammar generates candidate semantic
parses in previous work (berant and liang, 2014).

a semantic parse differs from a syntactic parse
in two primary ways. first, lexical items must be

what cities are in georgia?whatcitybeingeorgiafind[city]relate[in]lookup[georgia]relate[in]...lookup[georgia]find[city]and(a)(b)(c)(d)relate[in]lookup[georgia]joining all fragments with an and module, and
inserting either a measure or describe module
at the top (each subset thus results in two parse
candidates.)

all layouts resulting from this process feature a
relatively    at tree structure with at most one con-
junction and one quanti   er. this is a strong sim-
plifying assumption, but appears suf   cient to cover
most of the examples that appear in both of our
tasks. as our approach includes both categories, re-
lations and simple quanti   cation, the range of phe-
nomena considered is generally broader than pre-
vious perceptually-grounded qa work (krishna-
murthy and kollar, 2013; matuszek et al., 2012).

having generated a set of candidate parses, we
need to score them. this is a ranking problem;
as in the rest of our approach, we solve it using
standard neural machinery.
in particular, we pro-
duce an lstm representation of the question, a
feature-based representation of the query, and pass
both representations through a multilayer id88
(mlp). the query feature vector includes indicators
on the number of modules of each type present, as
well as their associated parameter arguments. while
one can easily imagine a more sophisticated parse-
scoring model, this simple approach works well for
our tasks.

formally, for a question x, let hq(x) be an lstm
encoding of the question (i.e. the last hidden layer of
an lstm applied word-by-word to the input ques-
tion). let {z1, z2, . . .} be the proposed layouts for
x, and let f (zi) be a feature vector representing the
ith layout. then the score s(zi|x) for the layout zi is

s(zi|x) = a(cid:62)  (bhq(x) + cf (zi) + d)

(8)
i.e. the output of an mlp with inputs hq(x) and
f (zi), and parameters   (cid:96) = {a, b, c, d}. finally,
we normalize these scores to obtain a distribution:

p(zi|x;   (cid:96)) = es(zi|x)(cid:46) n(cid:88)

es(zj|x)

(9)

a

and

having de   ned a layout
network

j=1
selection module
p(z|x;   (cid:96))
execution model
pz(y|w;   e), we are ready to de   ne a model
for predicting answers given only (world, question)
pairs. the key constraint is that we want to min-
imize evaluations of pz(y|w;   e) (which involves

expensive application of a deep network to a large
input representation), but can tractably evaluate
p(z|x;   (cid:96)) for all z (which involves application
of a shallow network to a relatively small set of
candidates). this is the opposite of the situation
usually encountered id29, where calls
to the query execution model are fast but the set of
candidate parses is too large to score exhaustively.

in fact, the problem more closely resembles the
scenario faced by agents in the reinforcement learn-
ing setting (where it is cheap to score actions, but
potentially expensive to execute them and obtain re-
wards). we adopt a common approach from that lit-
erature, and express our model as a stochastic pol-
icy. under this policy, we    rst sample a layout z
from a distribution p(z|x;   (cid:96)), and then apply z to
the knowledge source and obtain a distribution over
answers p(y|z, w;   e).
after z is chosen, we can train the execution
model directly by maximizing log p(y|z, w;   e) with
respect to   e as before (this is ordinary backprop-
agation). because the hard selection of z is non-
differentiable, we optimize p(z|x;   (cid:96)) using a policy
gradient method. the gradient of the reward surface
j with respect to the parameters of the policy is

   j(  (cid:96)) = e[    log p(z|x;   (cid:96))    r]

(10)

(this is the reinforce rule (williams, 1992)). here
the expectation is taken with respect to rollouts of
the policy, and r is the reward. because our goal is
to select the network that makes the most accurate
predictions, we take the reward to be identically the
negative log-id203 from the execution phase,
i.e.

e[(    log p(z|x;   (cid:96)))    log p(y|z, w;   e)]

(11)

thus the update to the layout-scoring model at each
timestep is simply the gradient of the log-id203
of the chosen layout, scaled by the accuracy of that
layout   s predictions. at training time, we approxi-
mate the expectation with a single rollout, so at each
step we update   (cid:96) in the direction (    log p(z|x;   (cid:96)))  
log p(y|z, w;   e) for a single z     p(z|x;   (cid:96)).   e and
  (cid:96) are optimized using adadelta (zeiler, 2012)
with    = 0.95,    = 1e   6 and gradient clipping at a
norm of 10.

test-dev

test-std

yes/no number other

zhou (2015) 76.6
noh (2015)
80.7
yang (2015) 79.3
81.2
nmn
d-nmn
81.1

35.0
37.2
36.6
38.0
38.6

42.6
41.7
46.1
44.0
45.5

all

55.7
57.2
58.7
58.6
59.4

all

55.9
57.4
58.9
58.7
59.4

table 1: results on the vqa test server. nmn is the
parameter-tying model from andreas et al. (2015), and d-nmn
is the model described in this paper.

best if the candidate layouts were relatively simple:
only describe, and and find modules are used, and
layouts contain at most two conjuncts.

one weakness of this basic framework is a dif   -
culty modeling prior knowledge about answers (of
the form most bears are brown). this kinds of lin-
guistic    prior    is essential for the vqa task, and
easily incorporated. we simply introduce an extra
hidden layer for recombining the    nal module net-
work output with the input sentence representation
hq(x) (see equation 8), replacing equation 1 with:

log pz(y|w, x) = (ahq(x) + b(cid:74)z(cid:75)w)y

(12)

(now modules with output
type labels should
be understood as producing an answer embedding
rather than a distribution over answers.) this allows
the question to in   uence the answer directly.

results are shown in table 1. the use of dynamic
networks provides a small gain, most noticeably on
   other    questions. we achieve state-of-the-art re-
sults on this task, outperforming a highly effective
visual bag-of-words model (zhou et al., 2015), a
model with dynamic network parameter prediction
(but    xed network structure) (noh et al., 2015), a
more conventional attentional model (yang et al.,
2015), and a previous approach using neural mod-
ule networks with no structure prediction (andreas
et al., 2016).

some examples are shown in figure 4. in general,
the model learns to focus on the correct region of the
image, and tends to consider a broad window around
the region. this facilitates answering questions like
where is the cat?, which requires knowledge of the
surroundings as well as the object in question.

what is in the sheep   s ear?

what color is she
wearing?

what is the man
dragging?

(describe[what]

(describe[color]

(describe[what]

(and find[sheep]
find[ear]))

find[wear])

find[man])

tag

white

boat (board)

figure 4: sample outputs for the visual id53
task. the second row shows the    nal attention provided as in-
put to the top-level describe module. for the    rst two exam-
ples, the model produces reasonable parses, attends to the cor-
rect region of the images (the ear and the woman   s clothing),
and generates the correct answer. in the third image, the verb is
discarded and a wrong answer is produced.

5 experiments

the framework described in this paper is general,
and we are interested in how well it performs on
datasets of varying domain, size and linguistic com-
plexity. to that end, we evaluate our model on tasks
at opposite extremes of both these criteria: a large
visual id53 dataset, and a small col-
lection of more structured geography questions.

5.1 questions about images
our    rst task is the recently-introduced visual ques-
tion answering challenge (vqa) (antol et al.,
2015). the vqa dataset consists of more than
200,000 images paired with human-annotated ques-
tions and answers, as in figure 4.

we use the vqa 1.0 release, employing the de-
velopment set for model selection and hyperparam-
eter tuning, and reporting    nal results from the eval-
uation server on the test-standard set. for the ex-
periments described in this section, the input feature
representations wi are computed by the the    fth con-
volutional layer of a 16-layer vggnet after pooling
(simonyan and zisserman, 2014). input images are
scaled to 448  448 before computing their represen-
tations. we found that performance on this task was

accuracy

model

geoqa geoqa+q

lsp-f
lsp-w
nmn
d-nmn

48
51
51.7
54.3

   
   

35.7
42.9

table 2: results on the geoqa dataset, and the geoqa
dataset with quanti   cation. our approach outperforms both a
purely logical model (lsp-f) and a model with learned percep-
tual predicates (lsp-w) on the original dataset, and a    xed-
structure nmn under both evaluation conditions.

5.2 questions about geography

the next set of experiments we consider focuses
on geoqa, a geographical question-answering
task    rst introduced by krishnamurthy and kollar
(2013). this task was originally paired with a vi-
sual id53 task much simpler than the
one just discussed, and is appealing for a number
of reasons. in contrast to the vqa dataset, geoqa
is quite small, containing only 263 examples. two
baselines are available: one using a classical se-
mantic parser backed by a database, and another
which induces logical predicates using linear clas-
si   ers over both spatial and distributional features.
this allows us to evaluate the quality of our model
relative to other perceptually grounded logical se-
mantics, as well as strictly logical approaches.

the geoqa domain consists of a set of entities
(e.g. states, cities, parks) which participate in vari-
ous relations (e.g. north-of, capital-of). here we take
the world representation to consist of two pieces: a
set of category features (used by the find module)
and a different set of relational features (used by the
relate module). for our experiments, we use a sub-
set of the features originally used by krishnamurthy
et al. the original dataset includes no quanti   ers,
and treats the questions what cities are in texas?
and are there any cities in texas? identically. be-
cause we are interested in testing the parser   s ability
to predict a variety of different structures, we intro-
duce a new version of the dataset, geoqa+q, which
distinguishes these two cases, and expects a boolean
answer to questions of the second kind.

results are shown in table 2. as in the orig-
inal work, we report
the results of leave-one-
environment-out cross-validation on the set of 10 en-

is key largo an island?
(exists (and lookup[key-largo] find[island]))
yes: correct

what national parks are in florida?
(and find[park] (relate[in] lookup[florida]))
everglades: correct

what are some beaches in florida?
(exists (and lookup[beach]

(relate[in] lookup[florida])))

yes (daytona-beach): wrong parse

what beach city is there in florida?
(and lookup[beach] lookup[city]

(relate[in] lookup[florida]))

[none] (daytona-beach): wrong module behavior

figure 5: example layouts and answers selected by the model
on the geoqa dataset. for incorrect predictions, the correct
answer is shown in parentheses.

vironments. our dynamic model (d-nmn) outper-
forms both the logical (lsp-f) and perceptual mod-
els (lsp-w) described by (krishnamurthy and kol-
lar, 2013), as well as a    xed-structure neural mod-
ule net (nmn). this improvement is particularly
notable on the dataset with quanti   ers, where dy-
namic structure prediction produces a 20% relative
improvement over the    xed baseline. a variety of
predicted layouts are shown in figure 5.

6 conclusion

we have introduced a new model, the dynamic neu-
ral module network, for answering queries about
both structured and unstructured sources of informa-
tion. given only (question, world, answer) triples
as training data, the model learns to assemble neu-
ral networks on the    y from an inventory of neural
models, and simultaneously learns weights for these
modules so that they can be composed into novel
structures. our approach achieves state-of-the-art
results on two tasks. we believe that the success of
this work derives from two factors:

continuous representations improve the expres-
siveness and learnability of semantic parsers: by re-
placing discrete predicates with differentiable neural
network fragments, we bypass the challenging com-
binatorial optimization problem associated with in-
duction of a semantic lexicon. in structured world

representations, neural predicate representations al-
low the model to invent reusable attributes and re-
lations not expressed in the schema. perhaps more
importantly, we can extend compositional question-
answering machinery to complex, continuous world
representations like images.

semantic structure prediction improves general-
ization in deep networks: by replacing a    xed net-
work topology with a dynamic one, we can tailor the
computation performed to each problem instance,
using deeper networks for more complex questions
and representing combinatorially many queries with
comparatively few parameters. in practice, this re-
sults in considerable gains in speed and sample ef   -
ciency, even with very little training data.

these observations are not limited to the question
answering domain, and we expect that they can be
applied similarly to tasks like instruction following,
game playing, and language generation.

acknowledgments

ja is supported by a national science foundation
graduate fellowship. mr is supported by a fellow-
ship within the fit weltweit-program of the german
academic exchange service (daad). this work
was additionally supported by darpa, afrl, dod
muri award n000141110688, nsf awards iis-
1427425 and iis-1212798, and the berkeley vision
and learning center.

references
jacob andreas, andreas vlachos, and stephen clark.
2013. id29 as machine translation.
in
proceedings of the annual meeting of the association
for computational linguistics, so   a, bulgaria.

jacob andreas, marcus rohrbach, trevor darrell, and
dan klein. 2016. neural module networks. in pro-
ceedings of the conference on id161 and
pattern recognition.

stanislaw antol, aishwarya agrawal, jiasen lu, mar-
garet mitchell, dhruv batra, c lawrence zitnick, and
devi parikh. 2015. vqa: visual question answer-
in proceedings of the international conference
ing.
on id161.

islam beltagy, cuong chau, gemma boleda, dan gar-
rette, katrin erk, and raymond mooney. 2013. mon-
tague meets markov: deep semantics with probabilis-
tic logical form. proceedings of the joint conference

on distributional and logical semantics, pages 11   
21.

jonathan berant and percy liang. 2014. semantic pars-
in proceedings of the annual
ing via id141.
meeting of the association for computational linguis-
tics, volume 7, page 92.

antoine bordes, sumit chopra, and jason weston. 2014.
id53 with subgraph embeddings. pro-
ceedings of the conference on empirical methods in
natural language processing.

l  eon bottou, yoshua bengio, and yann le cun. 1997.
global training of document processing systems us-
ing graph transformer networks. in proceedings of the
conference on id161 and pattern recogni-
tion, pages 489   494. ieee.

l  eon bottou. 2014. from machine learning to machine

reasoning. machine learning, 94(2):133   149.

marie-catherine de marneffe and christopher d man-
ning. 2008. the stanford typed dependencies repre-
sentation. in proceedings of the international confer-
ence on computational linguistics, pages 1   8.

edward grefenstette. 2013. towards a formal distribu-
tional semantics: simulating logical calculi with ten-
sors. joint conference on lexical and computational
semantics.

karl moritz hermann, tomas kocisky, edward grefen-
stette, lasse espeholt, will kay, mustafa suleyman,
and phil blunsom. 2015. teaching machines to read
and comprehend. in advances in neural information
processing systems, pages 1684   1692.

mohit iyyer, jordan boyd-graber, leonardo claudino,
richard socher, and hal daum  e iii. 2014. a neu-
ral network for factoid id53 over para-
graphs. in proceedings of the conference on empiri-
cal methods in natural language processing.

jayant krishnamurthy and thomas kollar. 2013. jointly
learning to parse and perceive: connecting natural lan-
guage to the physical world. transactions of the asso-
ciation for computational linguistics.

jayant krishnamurthy and tom mitchell. 2013. vec-
tor space id29: a framework for compo-
in proceedings of the
sitional vector space models.
acl workshop on continuous vector space models
and their compositionality.

tom kwiatkowski, luke zettlemoyer, sharon goldwa-
ter, and mark steedman. 2010. inducing probabilis-
tic id35 grammars from logical form with higher-
in proceedings of the conference
order uni   cation.
on empirical methods in natural language process-
ing, pages 1223   1233, cambridge, massachusetts.

tom kwiatkowski, eunsol choi, yoav artzi, and luke
zettlemoyer. 2013. scaling semantic parsers with on-
the-   y ontology matching. in proceedings of the con-

kelvin xu, jimmy ba, ryan kiros, kyunghyun cho,
aaron courville, ruslan salakhutdinov, richard
zemel, and yoshua bengio.
2015. show, attend
and tell: neural image id134 with visual
in international conference on machine
attention.
learning.

zichao yang, xiaodong he, jianfeng gao, li deng,
and alex smola.
stacked attention net-
works for image id53. arxiv preprint
arxiv:1511.02274.

2015.

pengcheng yin, zhengdong lu, hang li, and ben kao.
2015. neural enquirer: learning to query tables.
arxiv preprint arxiv:1512.00965.

matthew d zeiler.

2012.

adaptive learning rate method.
arxiv:1212.5701.

adadelta: an
arxiv preprint

bolei zhou, yuandong tian, sainbayar sukhbaatar,
arthur szlam, and rob fergus. 2015. simple base-
arxiv preprint
line for visual id53.
arxiv:1512.02167.

ference on empirical methods in natural language
processing.

mike lewis and mark steedman. 2013. combining
distributional and logical semantics. transactions of
the association for computational linguistics, 1:179   
192.

percy liang, michael jordan, and dan klein.

2011.
learning dependency-based id152.
in proceedings of the human language technology
conference of the association for computational lin-
guistics, pages 590   599, portland, oregon.

mateusz malinowski, marcus rohrbach, and mario fritz.
2015. ask your neurons: a neural-based approach to
answering questions about images. in proceedings of
the international conference on id161.

cynthia matuszek, nicholas fitzgerald, luke zettle-
moyer, liefeng bo, and dieter fox. 2012. a joint
model of language and perception for grounded at-
tribute learning. in international conference on ma-
chine learning.

hyeonwoo noh, paul hongsuck seo, and bohyung han.
2015. image id53 using convolutional
neural network with dynamic parameter prediction.
arxiv preprint arxiv:1511.05756.

panupong pasupat and percy liang. 2015. composi-
tional id29 on semi-structured tables. in
proceedings of the annual meeting of the association
for computational linguistics.

mengye ren, ryan kiros, and richard zemel. 2015. ex-
ploring models and data for image question answer-
in advances in neural information processing
ing.
systems.

k simonyan and a zisserman. 2014. very deep con-
volutional networks for large-scale image recognition.
arxiv preprint arxiv:1409.1556.

richard socher, john bauer, christopher d. manning,
and andrew y. ng. 2013. parsing with compositional
vector grammars. in proceedings of the annual meet-
ing of the association for computational linguistics.
1994.
knowledge-based arti   cial neural networks. arti   cial
intelligence, 70(1):119   165.

geoffrey g towell and jude w shavlik.

ronald j williams. 1992. simple statistical gradient-
following algorithms for connectionist reinforcement
learning. machine learning, 8(3-4):229   256.

yuk wah wong and raymond j. mooney. 2007. learn-
ing synchronous grammars for id29 with
id198. in proceedings of the annual meet-
ing of the association for computational linguistics,
volume 45, page 960.

huijuan xu and kate saenko.

2015. ask, attend
and answer: exploring question-guided spatial atten-
arxiv preprint
tion for visual id53.
arxiv:1511.05234.

