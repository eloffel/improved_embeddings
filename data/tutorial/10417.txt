leveraging sentence-level information with encoder lstm

for semantic slot filling

gakuto kurata
ibm research

bing xiang
ibm watson

gakuto@jp.ibm.com

bingxia@us.ibm.com

6
1
0
2

 

g
u
a
1
3

 

 
 
]
l
c
.
s
c
[
 
 

4
v
0
3
5
1
0

.

1
0
6
1
:
v
i
x
r
a

bowen zhou
ibm watson

zhou@us.ibm.com

abstract

recurrent neural network (id56) and one
of its speci   c architectures, long short-term
memory (lstm), have been widely used for
sequence labeling. explicitly modeling out-
put label dependencies on top of id56/lstm
is a widely-studied and effective extension.
we propose another extension to incorpo-
rate the global
information spanning over
the whole input sequence.
the proposed
method, encoder-labeler lstm,    rst encodes
the whole input sequence into a    xed length
vector with the encoder lstm, and then uses
this encoded vector as the initial state of an-
other lstm for sequence labeling. with this
method, we can predict the label sequence
while taking the whole input sequence in-
formation into consideration.
in the experi-
ments of a slot    lling task, which is an es-
sential component of natural language under-
standing, with using the standard atis cor-
pus, we achieved the state-of-the-art f1-score
of 95.66%.

1 introduction

natural language understanding (nlu) is an es-
sential component of natural human computer in-
teraction and typically consists of identifying the
intent of the users (intent classi   cation) and ex-
tracting the associated semantic slots (slot    ll-
ing) (de mori et al., 2008). we focus on the latter
semantic slot    lling task in this paper.

slot    lling can be framed as a sequential
labeling problem in which the most probable
semantic slot labels are estimated for each word

mo yu

ibm watson

yum@us.ibm.com

task and tremendous

slot    lling is a
of the given word sequence.
efforts have
traditional
been done, especially since the 1980s when the
defense advanced research program agency
(darpa) airline travel
information system
(atis) projects started (price, 1990). following the
success of deep learning (hinton et al., 2006;
bengio, 2009),
network
(id56)
and one
of its speci   c architectures, long short-term mem-
ory (lstm) (hochreiter and schmidhuber, 1997),
have been widely used since they can cap-
ture
(yao et al., 2013;
yao et al., 2014a; mesnil et al., 2015).
the
id56/lstm-based slot    lling has been extended to
be combined with explicit modeling of label depen-
dencies (yao et al., 2014b; liu and lane, 2015).

neural
jordan, 1997)

(elman, 1990;

dependencies

recurrent

temporal

in this paper, we extend the lstm-based
informa-
slot    lling to consider sentence-level
in the    eld of machine translation, an
tion.
encoder-decoder lstm has been gaining atten-
tion (sutskever et al., 2014), where the encoder
lstm encodes the global
information spanning
over the whole input sentence in its last hidden
state. inspired by this idea, we propose an encoder-
labeler lstm that leverages the encoder lstm for
slot    lling. first, we encode the input sentence
into a    xed length vector by the encoder lstm.
then, we predict the slot label sequence by the la-
beler lstm whose hidden state is initialized with
the encoded vector by the encoder lstm. with this
encoder-labeler lstm, we can predict the label se-
quence while taking the sentence-level information
into consideration.

the main contributions of this paper are two-

folds:

1. proposed an encoder-labeler lstm to leverage

sentence-level information for slot    lling.

2. achieved the state-of-the-art f1-score of
95.66% in the slot    lling task of the standard
atis corpus.

2 proposed method

we    rst revisit the lstm for slot    lling and enhance
this to explicitly model label dependencies. then we
explain the proposed encoder-labeler lstm.

2.1 lstm for slot filling
figure 1(a) shows a typical lstm for slot    lling and
we call this as labeler lstm(w) where words are fed
to the lstm (yao et al., 2014a).

slot    lling is a sequential labeling task to map a
1 to a sequence of t slot
sequence of t words xt
1 . each word xt is represented with a v
labels yt
dimensional one-hot-vector where v is the vocabu-
lary size and is transferred to de dimensional con-
tinuous space by the id27 matrix e    
rde  v as ext. instead of simply feeding ext into
the lstm, context window is a widely used tech-
nique to jointly consider k preceding and succeeding
words as ext+k
t   k     rde(2k+1). the lstm has the
architecture based on jozefowicz et al. (2015) that
does not have peephole connections and yields the
1 . for each time step t, the
hidden state sequence ht
posterior probabilities for each slot label are calcu-
lated by the softmax layer over the hidden state ht.
the id27 matrix e, lstm parameters,
and softmax layer parameters are estimated to mini-
mize the negative log likelihood over the correct la-
bel sequences with back-propagation through time
(bptt) (williams and peng, 1990).

2.2 explicit modeling of label dependency
a shortcoming of the labeler lstm(w) is that
it does not consider label dependencies. to ex-
plicitly model label dependencies, we introduce a
new architecture, labeler lstm (w+l), as shown
in figure 1(b), where the output label of previ-
ous time step is fed to the hidden state of current
time step, jointly with words, as mesnil et al. (2015)

and liu and lane (2015) tried with id56. for model
training, one-hot-vector of ground truth label of pre-
vious time step is fed to the hidden state of cur-
rent time step and for evaluation, left-to-right beam
search is used.

2.3 encoder-labeler lstm for slot filling
we propose two types of the encoder-labeler lstm
that uses the labeler lstm(w) and the labeler
lstm(w+l). figure 1(d) shows the encoder-
labeler lstm(w). the encoder lstm, to the left
of the dotted line, reads through the input sentence
backward.
its last hidden state contains the en-
coded information of the input sentence. the la-
beler lstm(w), to the right of the dotted line, is
the same with the labeler lstm(w) explained in
section 2.1, except that its hidden state is initialized
with the last hidden state of the encoder lstm. the
labeler lstm(w) predicts the slot label conditioned
on the encoded information by the encoder lstm,
which means that slot    lling is conducted with tak-
ing sentence-level information into consideration.
figure 1(e) shows the encoder-labeler lstm(w+l),
which uses the labeler lstm(w+l) and predicts
the slot label considering sentence-level information
and label dependencies jointly.

model training is basically the same as with the
baseline labeler lstm(w), as shown in section 2.1,
except that the error in the labeler lstm is propa-
gated to the encoder lstm with bptt.

this encoder-labeler lstm is motivated by the
encoder-decoder lstm that has been applied to ma-
chine translation (sutskever et al., 2014), grapheme-
to-phoneme conversion (yao and zweig, 2015), text
summarization (nallapati et al., 2016) and so on.
the difference is that the proposed encoder-labeler
lstm accepts the same input sequence twice while
the usual encoder-decoder lstm accepts the in-
put sequence once in the encoder. note that the
lstms for encoding and labeling are different in the
encoder-labeler lstm, but the same word embed-
ding matrix is used both for the encoder and labeler
since the same input sequence is fed twice.

2.4 related work on considering

sentence-level information

bi-directional id56/lstm have
posed

sentence-level

capture

to

been

pro-
informa-

o

o

o

o

o b-tocity

o

o

<b>

o

o

o

o

o

o b-tocity

o

o

seattle

i

a

need
(a) labeler lstm(w).

ticket

to

seattle

a

need

i
(b) labeler lstm(w+l).

ticket

to

seattle

to

ticket

need
encoder (backward) lstm

a

o

o

i

<b>

o

o

o

o

o

decoder lstm

o b-tocity

o

o

(c) encoder-decoder lstm.

o

o

o

o

o b-tocity

o

o

seattle

to

ticket

a

need

i

<b>

o

o

o

o

o

o b-tocity

o

o

seattle

to

ticket

a

need

i

i

need

a

ticket

to

seattle

i

encoder lstm (backward)

labeler lstm(w)

encoder lstm (backward)

need
labeler lstm(w+l)

ticket

a

to

seattle

(d) encoder-labeler lstm(w).

(e) encoder-labeler lstm(w+l).

figure 1: neural network architectures for slot    lling. input sentence is    i need a ticket to seattle   .    b-tocity    is slot label for
speci   c meaning and    o   is slot label without speci   c meaning.    <b>    is beginning symbol for slot sequence.

while

(mesnil et al., 2015;

zhou and xu, 2015;
tion
vu et al., 2016).
the
bi-directional
id56/lstm model the preceding and succeeding
contexts at a speci   c word and don   t explicitly
encode the whole sentence, our proposed encoder-
labeler lstm explicitly encodes whole sentence
and predicts slots conditioned on the encoded
information.

another method to consider the sentence-level in-
formation for slot    lling is the attention-based ap-
proach (simonnet et al., 2015). the attention-based
approach is novel in aligning two sequences of dif-
ferent length. however, in the slot    lling task where
the input and output sequences have the same length
and the input word and the output label has strong
relations, the effect of introducing    soft    attention
might become smaller. instead, we directly fed the
input word into the labeler part with using context
window method as explained in section 2.3.

3 experiments

we report two sets of experiments. first we use the
standard atis corpus to con   rm the improvement
by the proposed encoder-labeler lstm and com-
pare our results with the published results while dis-
cussing the related works. then we use a large-scale
data set to con   rm the effect of the proposed method
in a realistic use-case.

3.1 atis experiment
3.1.1 experimental setup

used
widely

the atis corpus, which

we
been
for
wang et al., 2006; tur et al., 2010).

as
(price, 1990;

has
the
benchmark
dahl et al., 1994;
figure 2

nlu

used

sentence

show

flights

from

boston

to

new

york

today

slots

o

o

o

b-fromcity

o

b-tocity

i-tocity

b-date

figure 2: example of atis sentence and annotated slots.

shows an example sentence and its semantic slot la-
bels in in-out-begin (iob) representation. the slot
   lling task was to predict the slot label sequences
from input word sequences.

the performance was measured by the f1-score:
f1 = 2  p recision  recall
, where precision is the ra-
p recision+recall
tio of the correct labels in the system   s output and
recall is the ratio of the correct labels in the ground
truth of the evaluation data (van rijsbergen, 1979).
the atis corpus contains the training data of
4,978 sentences and evaluation data of 893 sen-
tences. the unique number of slot labels is 127 and
the vocabulary size is 572. in the following exper-
iments, we randomly selected 80% of the original
training data to train the model and used the remain-
ing 20% as the heldout data (mesnil et al., 2015).
we reported the f1-score on the evaluation data with
hyper-parameters that achieved the best f1-score on
the heldout data.

for training, we randomly initialized parame-
ters in accordance with the normalized initializa-
tion (glorot and bengio, 2010). we used adam
for learning rate control (kingma and ba, 2014) and
dropout for generalization with a dropout rate of
0.5 (srivastava et al., 2014; zaremba et al., 2014).

3.1.2

improvement by encoder-labeler lstm
we conducted experiments to compare the labeler
lstm(w) (section 2.1), the labeler lstm(w+l)
(section 2.2), and the encoder-labeler lstm (sec-

tion 2.3). as for yet another baseline, we tried the
encoder-decoder lstm as shown in figure 1(c)1.

the initial

for all architectures, we set

learn-
ing rate to 0.001 (kingma and ba, 2014) and the
dimension of id27s to de = 30.
we changed the number of hidden units in the
lstm, dh     {100, 200, 300}2, and the size of
the context window, k     {0, 1, 2}3. we used
backward encoding for the encoder-decoder lstm
and the encoder-labeler lstm as suggested in
sutskever et al. (2014).
for the encoder-decoder
lstm, labeler lstm(w+l), and encoder-labeler
lstm(w+l), we used the left-to-right id125
decoder (sutskever et al., 2014) with beam sizes of
1, 2, 4, and 8 for evaluation where the best f1-score
was reported. during 100 training epochs, we re-
ported the f1-score on the evaluation data with the
epoch when the f1-score for the heldout data was
maximized. table 1 shows the results.

the proposed encoder-labeler lstm(w) and
encoder-labeler lstm(w+l) both outperformed
the labeler lstm(w) and labeler lstm(w+l),
which con   rms the novelty of considering sentence-
level information with the encoder lstm by our
proposed method.

contrary to expectations, f1-score by the
encoder-labeler lstm(w+l) was not
improved
from that by the encoder-labeler lstm(w). a pos-
sible reason for this is the propagation of label pre-
diction errors. we compared the label prediction ac-
curacy for the words after the    rst label prediction
error in the evaluation sentences and con   rmed that
the accuracy deteriorated from 84.0% to 82.6% by
using pthe label dependencies.

for the encoder-labeler lstm(w) which was bet-
ter than the encoder-labeler lstm(w+l), we tried
the deep architecture of 2 lstm layers (encoder-
labeler deep lstm(w)). we also trained the cor-
responding labeler deep lstm(w). as in table 1,
we obtained improvement from 94.91% to 95.47%
by the proposed encoder-labeler deep lstm(w),

1length of the output label sequence is equal to that of the
input word sequence in a slot    lling task. therefore, ending
symbol for slot sequence is not necessary.

2when using deep architecture later in this section, dh was

tuned for each layer.

3in our preliminary experiments with using the labeler

lstm(w), f1-scores deteriorated with k     3.

f1-score

(c) encoder-decoder lstm
(a) labeler lstm(w)
(d) encoder-labeler lstm(w)
(b) labeler lstm(w+l)
(e) encoder-labeler lstm(w+l)

80.11
94.80
95.29
94.91
95.19
94.91
95.47
table 1: experimental results on atis slot    lling task. left-
most column corresponds to figure 1. lines with bold fonts
use proposed encoder-labeler lstm. [%]

labeler deep lstm(w)
encoder-labeler deep lstm(w)

which was statistically signi   cant at the 90% level.
lastly, f1-score by the encoder-decoder lstm
was worse than other methods as shown in the    rst
row of table 1. since the slot label is closely related
with the input word, the encoder-decoder lstm was
not an appropriate approach for the slot    lling task.

3.1.3 comparison with published results

table 2 summarizes the recently published results
on the atis slot    lling task and compares them with
the results from the proposed methods.

recent research has been focusing on id56 and
its extensions. yao et al. (2013) used id56 and
outperformed methods that did not use neural net-
works, such as id166 (raymond and riccardi, 2007)
and crf (deng et al., 2012). mesnil et al. (2015)
tried bi-directional id56, but reported degrada-
tion comparing with their single-directional id56
(94.98%). yao et al. (2014a) introduced lstm and
deep lstm and obtained improvement over id56.
peng and yao (2015) proposed id56-em that used
an external memory architecture to improve the
memory capability of id56.

also

been

many

studies

explicitly

label
proposed

conducted
have
dependencies.
to
model
id98-crf
xu and sarikaya (2013)
that explicitly models the dependencies of
the
output from id98. mesnil et al. (2015) used hybrid
id56 that combined elman-type and jordan-type
id56s. liu and lane (2015) used the output label
for the previous word to model label dependencies
(id56-sop).

vu et al. (2016) recently proposed to use ranking
id168 over bi-directional id56s with achiev-
ing 95.47% (r-biid56) and reported 95.56% by en-

semble (5  r-biid56).

by comparing with these methods, the main dif-
ference of our proposed encoder-labeler lstm is
the use of encoder lstm to leverage sentence-level
information 4.

for our encoder-labeler lstm(w) and encoder-
labeler deep lstm(w), we further conducted
hyper-parameter search with a random search strat-
egy (bergstra and bengio, 2012). we tuned the di-
mension of id27s, de     {30, 50, 75},
number of hidden states in each layer, dh    
{100, 150, 200, 250, 300}, size of context window,
k     {0, 1, 2}, and initial learning rate sampled from
uniform distribution in range [0.0001, 0.01]. to the
best of our knowledge,
the previously published
best f1-score was 95.56%5 (vu et al., 2016). our
encoder-labeler deep lstm(w) achieved 95.66%
f1-score, outperforming the previously published
f1-score as shown in table 2.

note some of the previous results used whole
training data for model training while others used
randomly selected 80% of data for model training
and the remaining 20% for hyper-parameter tuning.
our results are based on the latter setup.

(liu et al., 2013a;

3.2 large-scale experiment
we prepared a large-scale data set by merging
the mit restaurant corpus and mit movie
liu et al., 2013b;
corpus
spoken laungage systems group, 2013)
with
the atis corpus. since users of the nlu system
may provide queries without explicitly specifying
their domain, building one nlu model for multiple
domains is necessary. the merged data set contains
30,229 training and 6,810 evaluation sentences.
the unique number of slot labels is 191 and the
vocabulary size is 16,049. with this merged data

4since simonnet et al. (2015) did not report the experimen-
tal results on atis, we could not experimentally compare our
result with their attention-based approach. theoretical compar-
ison is available in section 2.4.

5 there are other published results that achieved better f1-
scores by using other information on top of word features.
vukotic et al. (2015) achieved 96.16% f1-score by using the
named entity (ne) database when estimating id27s.
yao et al. (2013) and yao et al. (2014a) used ne features in ad-
dition to word features and obtained improvement with both the
id56 and lstm upto 96.60% f1-score. mesnil et al. (2015)
also used ne features and reported f1-score of 96.29% with
id56 and 96.46% with recurrent crf.

f1-score

94.11
id56 (yao et al., 2013)
94.35
id98-crf (xu and sarikaya, 2013)
94.73
bi-directional id56 (mesnil et al., 2015)
94.85
lstm (yao et al., 2014a)
94.89
id56-sop (liu and lane, 2015)
95.06
hybrid id56 (mesnil et al., 2015)
95.08
deep lstm (yao et al., 2014a)
95.25
id56-em (peng and yao, 2015)
95.47
r-biid56 (vu et al., 2016)
95.56
5  r-biid56 (vu et al., 2016)
95.40
encoder-labeler lstm(w)
encoder-labeler deep lstm(w)
95.66
table 2: comparison with published results on atis slot    lling
task. f1-scores by proposed method are improved from table 1
due to sophisticated hyper-parameters. [%]

set, we compared the labeler lstm(w) and the
proposed encoder-labeler lstm(w) according to
the experimental procedure explained in section
3.1.2. the labeler lstm(w) achieved the f1-score
of 72.80% and the encoder-labeler lstm(w)
improved it to 74.41%, which con   rmed the effect
of the proposed method in large and realistic data
set 6.

4 conclusion

we proposed an encoder-labeler lstm that can
conduct slot    lling conditioned on the encoded
sentence-level information. we applied this method
to the standard atis corpus and obtained the state-
of-the-art f1-score in a slot    lling task. we also
tried to explicitly model label dependencies, but it
was not bene   cial in our experiments, which should
be further investigated in our future work.

in this paper, we focused on the slot

label-
previous papers reported
ing in this paper.
that
jointly training the models for slot    lling
and intent classi   cation boosted the accuracy of
both tasks (xu and sarikaya, 2013; shi et al., 2015;
liu et al., 2015). leveraging our encoder-labeler
lstm approach in joint training should be worth

6the purpose of this experiment is to con   rm the effect of
the proposed method. the absolute f1-scores can not be com-
pared with the numbers in liu et al. (2013b) since the capital-
ization policy and the data size of the training data were differ-
ent.

trying.

acknowledgments

we are grateful to dr. yuta tsuboi, dr. ryuki
tachibana, and mr. nobuyasu itoh of ibm re-
search - tokyo for the fruitful discussion and their
comments on this and earlier versions of the paper.
we thank dr. ramesh m. nallapati and dr. cicero
nogueira dos santos of ibm watson for their valu-
able suggestions. we thank the anonymous review-
ers for their valuable comments.

references
[bengio2009] yoshua bengio. 2009. learning deep ar-
chitectures for ai. foundations and trends r(cid:13) in ma-
chine learning, 2(1):1   127.

[bergstra and bengio2012] james bergstra and yoshua
bengio. 2012. random search for hyper-parameter
optimization. the journal of machine learning re-
search, 13(1):281   305.

[dahl et al.1994] deborah a dahl, madeleine bates,
michael brown, william fisher, kate hunicke-smith,
david pallett, christine pao, alexander rudnicky, and
elizabeth shriberg. 1994. expanding the scope of the
atis task: the atis-3 corpus. in proc. hlt, pages
43   48.

[de mori et al.2008] renato de mori, fr  ed  eric bechet,
dilek hakkani-tur, michael mctear, giuseppe ric-
cardi, and gokhan tur.
spoken language
understanding.
ieee signal processing magazine,
3(25):50   58.

2008.

[deng et al.2012] li deng, gokhan tur, xiaodong he,
and dilek hakkani-tur. 2012. use of kernel deep con-
vex networks and end-to-end learning for spoken lan-
guage understanding. in proc. slt, pages 210   215.

[elman1990] jeffrey l elman. 1990. finding structure in

time. cognitive science, 14(2):179   211.

[glorot and bengio2010] xavier glorot and yoshua ben-
gio. 2010. understanding the dif   culty of training
deep feedforward neural networks. in proc. aistats,
pages 249   256.

[hinton et al.2006] geoffrey e hinton, simon osindero,
and yee-whye teh. 2006. a fast learning algorithm
for deep belief nets. neural computation, 18(7):1527   
1554.

[hochreiter and schmidhuber1997] sepp hochreiter and
j  urgen schmidhuber. 1997. long short-term memory.
neural computation, 9(8):1735   1780.

[jordan1997] michael i jordan. 1997. serial order: a
parallel distributed processing approach. advances in
psychology, 121:471   495.

[jozefowicz et al.2015] rafal

jozefowicz, wojciech
zaremba, and ilya sutskever. 2015. an empirical
exploration of recurrent network architectures.
in
proc. icml, pages 2342   2350.

[kingma and ba2014] diederik kingma and jimmy ba.
2014. adam: a method for stochastic optimization.
arxiv preprint arxiv:1412.6980.

[liu and lane2015] bing liu and ian lane. 2015. re-
current neural network structured output prediction for
spoken language understanding. in proc. nips work-
shop on machine learning for spoken language un-
derstanding and interactions.

[liu et al.2013a] jingjing liu, panupong pasupat, scott
cyphers, and james glass. 2013a. asgard: a portable
architecture for multilingual dialogue systems.
in
proc. icassp, pages 8386   8390.

[liu et al.2013b] jingjing liu, panupong pasupat, yining
wang, scott cyphers, and james glass. 2013b. query
understanding enhanced by hierarchical parsing struc-
tures. in proc. asru, pages 72   77.

[liu et al.2015] chunxi liu, puyang xu, and ruhi
sarikaya. 2015. deep contextual language under-
standing in spoken dialogue systems. in proc. inter-
speech, pages 120   124.

[mesnil et al.2015] gr  egoire mesnil, yann dauphin,
kaisheng yao, yoshua bengio, li deng, dilek
hakkani-tur, xiaodong he, larry heck, gokhan tur,
dong yu, et al. 2015. using recurrent neural net-
works for slot    lling in spoken language understand-
ing. ieee/acm transactions on audio, speech, and
language processing, 23(3):530   539.

[nallapati et al.2016] ramesh nallapati, bowen zhou,
c   a glar gulc  ehre, and bing xiang. 2016. abstractive
text summarization using sequence-to-sequence id56s
and beyond. in proc. conll.

[peng and yao2015] baolin peng and kaisheng yao.
2015. recurrent neural networks with external mem-
ory for language understanding.
arxiv preprint
arxiv:1506.00195.

[price1990] patti price. 1990. evaluation of spoken lan-
guage systems: the atis domain. in proc. darpa
speech and natural language workshop, pages 91   
95.

[raymond and riccardi2007] christian raymond and
giuseppe riccardi. 2007. generative and discrimina-
tive algorithms for spoken language understanding. in
proc. interspeech, pages 1605   1608.

[shi et al.2015] yangyang shi, kaisheng yao, hu chen,
yi-cheng pan, mei-yuh hwang, and baolin peng.
2015. contextual spoken language understanding us-
ing recurrent neural networks. in proc. icassp, pages
5271   5275.

[simonnet et al.2015] edwin

nathalie, del  eglise paul,

simonnet,
camelin
and est`eve yannick.

neural networks for language understanding. in proc.
interspeech, pages 2524   2528.

[yao et al.2014a] kaisheng yao, baolin peng, yu zhang,
dong yu, geoffrey zweig, and yangyang shi. 2014a.
spoken language understanding using long short-term
memory neural networks.
in proc. slt, pages 189   
194.

[yao et al.2014b] kaisheng yao, baolin peng, geoffrey
zweig, dong yu, xiaolong li, and feng gao. 2014b.
recurrent conditional random    eld for language un-
derstanding. in proc. icassp, pages 4077   4081.

[zaremba et al.2014] wojciech zaremba, ilya sutskever,
and oriol vinyals. 2014. recurrent neural network
id173. arxiv preprint arxiv:1409.2329.

[zhou and xu2015] jie zhou and wei xu. 2015. end-to-
end learning of id14 using recurrent
neural networks. in proc. acl, pages 1127   1137.

2015. exploring the use of attention-based recurrent
neural networks for spoken language understanding.
in proc. nips workshop on machine learning for
spoken language understanding and interactions.

[spoken laungage systems group2013] spoken

2013.

laungage systems group.
the mit
restaurant corpus and the mit movie corpus.
https://groups.csail.mit.edu/sls/downloads/,
mit
computer science and arti   cial intelligence labora-
tory.

[srivastava et al.2014] nitish srivastava, geoffrey hin-
ton, alex krizhevsky, ilya sutskever, and ruslan
salakhutdinov. 2014. dropout: a simple way to pre-
vent neural networks from over   tting. the journal of
machine learning research, 15(1):1929   1958.

[sutskever et al.2014] ilya sutskever, oriol vinyals, and
quoc vv le. 2014. sequence to sequence learning
with neural networks.
in proc. nips, pages 3104   
3112.

[tur et al.2010] gokhan tur, dilek hakkani-tur, and
larry heck. 2010. what is left to be understood in
atis? in proc. slt, pages 19   24.

[van rijsbergen1979] cornelis

joost van rijsbergen.

1979. information retrieval. butterworth.

[vu et al.2016] ngoc thang vu, pankaj gupta, heike
adel, and hinrich sch  utze.
2016. bi-directional
recurrent neural network with ranking loss for spo-
ken language understanding. in proc. icassp, pages
6060   6064.

[vukotic et al.2015] vedran vukotic, christian raymond,
and guillaume gravier. 2015.
is it time to switch
to id27 and recurrent neural networks for
spoken language understanding?
in proc. inter-
speech, pages 130   134.

[wang et al.2006] ye-yi wang, alex acero, milind ma-
hajan, and john lee. 2006. combining statistical
and knowledge-based spoken language understanding
in conditional models. in proc. coling-acl, pages
882   889.

[williams and peng1990] ronald j williams and jing
peng. 1990. an ef   cient gradient-based algorithm
for on-line training of recurrent network trajectories.
neural computation, 2(4):490   501.

[xu and sarikaya2013] puyang xu and ruhi sarikaya.
2013. convolutional neural network based triangular
crf for joint intent detection and slot    lling. in proc.
asru, pages 78   83.

[yao and zweig2015] kaisheng yao and geoffrey zweig.
2015. sequence-to-sequence neural net models for
grapheme-to-phoneme conversion.
proc. inter-
speech, pages 3330   3334.

[yao et al.2013] kaisheng yao, geoffrey zweig, mei-yuh
hwang, yangyang shi, and dong yu. 2013. recurrent

