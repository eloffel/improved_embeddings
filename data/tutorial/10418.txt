this figure "ec_in_unempl.png" is available in "png"(cid:10) format from:

http://arxiv.org/ps/1601.02502v1

6
1
0
2

 

n
a
j
 

1
1

 
 
]
l
c
.
s
c
[
 
 

1
v
2
0
5
2
0

.

1
0
6
1
:
v
i
x
r
a

trans-gram, fast cross-lingual word-embeddings

reyes     mannde = reginait     femmefr

jocelyn coulmance
105 rue la fayette

jean-marc marty    
105 rue la fayette

guillaume wenzek    
105 rue la fayette

amine benhalloum
105 rue la fayette

75010 paris

75010 paris

75010 paris

75010 paris

joc@proxem.com

jmm@proxem.com

guw@proxem.com

aba@proxem.com

abstract

a

we introduce trans-gram,
simple
and computationally-ef   cient method to
simultaneously learn and align word-
embeddings for a variety of languages, us-
ing only monolingual data and a smaller
set of sentence-aligned data. we use our
new method to compute aligned word-
embeddings for twenty-one languages us-
ing english as a pivot language. we show
that some linguistic features are aligned
across languages for which we do not have
aligned data, even though those properties
do not exist in the pivot language. we also
achieve state of the art results on standard
cross-lingual text classi   cation and word
translation tasks.

1 introduction

word-embeddings are a representation of words
with    xed-sized vectors. it is a distributed repre-
sentation (hinton, 1984) in the sense that there is
not necessarily a one-to-one correspondence be-
tween vector dimensions and linguistic properties.
the linguistic properties are distributed along the
dimensions of the space.
compute word-
a popular method
model
embeddings
the
(mikolov et al., 2013a).
this algorithm learns
high-quality word vectors with a computation cost
much lower than previous methods. this allows
the processing of very important amounts of data.
for instance, a 1.6 billion words dataset can be
processed in less than one day.

to
skip-gram

is

several authors came up with different methods
to align word-embeddings across two languages
(klementiev et al., 2012; mikolov et al., 2013b;
lauly et al., 2014; gouws et al., 2015).

   these authors contributed equally.

in this article, we introduce a new method
called trans-gram, which learns word embed-
dings aligned across many languages, in a simple
and ef   cient fashion, using only sentence align-
ments rather than word alignments. we compare
our method with previous approaches on a cross-
lingual document classi   cation task and on a word
translation task and obtain state of the art results
on these tasks. additionally, word-embeddings for
twenty-one languages are learned simultaneously
- to our knowledge - for the    rst time, in less than
two and a half hours. furthermore, we illustrate
some interesting properties that are captured such
as cross-lingual analogies, e.g ~reyes     ~mannde +
~femmefr     ~reginait which can be used for dis-
ambiguation.

2 review of previous work

a number of methods have been explored to
train and align bilingual word-embeddings. these
methods pursue two objectives:    rst, similar rep-
resentations (i.e. spatially close) must be assigned
to similar words (i.e.    semantically close   ) within
each language - this is the mono-lingual objec-
tive; second, similar representations must be as-
signed to similar words across languages - this is
the cross-lingual objective.

the simplest approach consists in separating the
mono-lingual optimization task from the cross-
lingual optimization task. this is for example the
case in (mikolov et al., 2013b). the idea is to sep-
arately train two sets of word-embeddings for each
language and then to do a parametric estimation
of the mapping between word-embeddings across
languages. this method was further extended by
(faruqui and dyer, 2014). even though those al-
gorithms proved to be viable and fast, it is not
clear whether or not a simple mapping between
whole languages exists. moreover, they require
word alignments which are a rare and expensive
resource.

another approach consists in focusing en-
tirely on the cross-lingual objective.
this
was explored in (hermann and blunsom, 2013;
lauly et al., 2014) where every couple of aligned
sentences is transformed into two    xed-size vec-
tors. then, the model minimizes the euclidean
distance between both vectors. this idea allows
processing corpus aligned at sentence-level rather
than word-level. however, it does not leverage the
abundance of existing mono-lingual corpora .

a popular approach is to jointly optimize the
mono-lingual and cross-lingual objectives simul-
taneously.
this is mostly done by minimiz-
ing the sum of mono-lingual id168s for
each language and the cross-lingual id168.
(klementiev et al., 2012) proved this approach to
be useful by obtaining state-of-the-art results on
several tasks.
(gouws et al., 2015) extends their
work with a more computationally-ef   cient imple-
mentation.

3 from skip-gram to trans-gram

3.1 skip-gram
we brie   y introduce the skip-gram algorithm, as
we will need it for further explanations. skip-
gram allows to train id27s for a lan-
guage using mono-lingual data. this method uses
a dual representation for words. each word w
has two embeddings: a target vector, ~w (    rd),
and a context vector, ~w (    rd). the algorithm
tries to estimate the id203 of a word w to
appear in the context of a word c. more pre-
cisely we are learning the embeddings ~w, ~c so that:
  ( ~w    ~c) = p (w|c) where    is the sigmoid func-
tion.

a simpli   ed version of the id168 mini-

mized by skip-gram is the following:

j = x

x

x

    log   ( ~w    ~c)

(1)

s   c

w   s

c   s[w   l:w+l]

where c is the set of sentences constituting the
training corpus, and s[w     l
is a
word window on the sentence s centered around
w. for the sake of simplicity this equation does
not
include the    negative-sampling    term, see
(mikolov et al., 2013a) for more details.

: w + l]

skip-gram can be seen as a materialization
of the distributional hypothesis (harris, 1968):
   words used in similar contexts have similar
meanings   . we will now see how to extend this
idea to cross-lingual contexts.

3.2 trans-gram

in this section we introduce trans-gram, a new
method to compute aligned word-embeddings for
a variety of languages.

our method will minimize the summation of
mono-lingual losses and cross-lingual losses. like
in bilbowa (gouws et al., 2015), we use skip-
gram as a mono-lingual loss. assuming we are
trying to learn aligned word vectors for languages
e (e.g. english) and f (e.g. french), we note je
and jf the two mono-lingual losses.
the cross-lingual

loss func-
tion is a distance between bag-of-words repre-
sentations of two aligned sentences.
but as
(levy and goldberg, 2014) showed that the skip-
gram id168 extracts interesting linguistic
features, we wanted to use a id168 for the
cross-lingual objective that will be closer to skip-
gram than bilbowa.

in bilbowa,

therefore, we introduce a new task, trans-
gram, similar to skip-gram. each english sen-
tence se in our aligned corpus ae,f is aligned with
a french sentence sf . in skip-gram, the context
picked for a target word we in a sentence se is the
set of words ce appearing in the window centered
around we: se[we     l : we + l]. in trans-gram, the
context picked for a target word we in a sentence
se will be all the words cf appearing in sf . the
loss can thus be written as:

   e,f = x

(se,sf )   ae,f

x
we   se

x
cf    sf

    log   ( ~we    ~cf )

(2)
this loss isn   t symmetric with respect to the lan-
guages. we, therefore, use two cross-lingual ob-
jectives:    e,f aligning e   s target vectors and f    s
context vectors and    f,e aligning f    s target vectors
and e   s context vectors. by comparison bilbowa
only aligns e   s target vectors and f    s target vec-
tors. the    gure 1 illustrates the four objectives.

notice that we make the assumption that the
meaning of a word is uniformly distributed in
the whole sentence. this assumption, although
a naive one, gave us in practice excellent results.
also our method uses only sentence-aligned cor-
pus and not word-aligned corpus which are rarer.
to add a third language i (e.g. italian), we just
have to add 3 new objectives (ji,    e,i and    i,e)
to the global loss. if available we could also add
   f,i or    i,f but in our case we only used corpora
aligned with english.

p

~sitse

je

~assisf

   f,e

~sitse

   e,f

~assisf

jf

~the ~cat

~on ~thee

~the ~cat ~sits ~on ~the

~mate

~le

~chat ~est

~assis ~sur ~le

~tapisf

~chat ~est

~sur ~lef

figure 1: the four partial objectives contributing to the alignment of english and french: a skip-gram
objective per language (je and jf) over a window surrounding a target word (blue) and two trans-gram
objectives (   e,f and    f,e) over the whole sentence aligned with the sentence from which the target word
is extracted (red).

4 implementation

in our experiments, we used the europarl
(koehn, 2005) aligned corpora. europarl-v7 has
two peculiarities:    rstly, the corpora are aligned
at sentence-level; secondly each pair of languages
contains english as one of its members: for in-
stance, there is no french/italian pair.
in other
words, english is used as a pivot language. no
bi-lingual lexicons nor other bi-lingual datasets
aligned at the word level were used.

it

using only the europarl-v7 texts as both mono-
lingual and bilingual data,
took 10 min-
utes to align 2 languages, and two and a half
hours to align the 21 languages of the corpus,
in a 40 dimensional space on a 6 core com-
puter. we also computed 300 dimensions vec-
tors using the wikipedia extracts provided by
(al-rfou et al., 2013) as monolingual data for
each language. the training time was 21 hours.

5 experiments

5.1 reuters cross-lingual document

classi   cation

we used a subset of the english and german
sections of the reuters rcv1/rcv2 corpora
(lewis and li, 2004) (10000 documents each), as
in (klementiev et al., 2012), and we replicated the
experimental setting. in the english dataset, there
are four topics: ccat (corporate/industrial),
ecat (economics), gcat (government/social),
and mcat (markets). we used these topics as
our labels and we only selected documents labeled
with a single topic. we trained our classi   er on
the articles of one language, where each document
was represented using an idf weighted sum of the
vectors of its words, we then tested it on the arti-
cles of the other language. the classi   er used was

an averaged id88, and we used the imple-
mentation from (klementiev et al., 2012)1. the
word vectors were computed on the europarl-v7
parallel corpus with size 40 like other methods.
for this task only the target vectors where used.

we report the percentage precision obtained
with our method, in comparison with other meth-
ods, in table 1. the table also include results
obtained with 300 dimensions vectors trained by
trans-gram with the europarl-v7 as parallel cor-
pus and the wikipedia as mono-lingual corpus.
the previous state of the art results were de-
tained (gouws et al., 2015) with bilbowa and
(lauly et al., 2014) with their bilingual auto-
encoder model. this model learns word em-
beddings during a translation task that uses an
encoder-decoder approach. we also report the
scores from klementiev et al. who introduced
the task and the bicvm model scores from
(hermann and blunsom, 2013).

the results show an overall signi   cant improve-
ment over the other methods, with the added ad-
vantage of being computationally ef   cient.

5.2 p@k word translation
next we evaluated our method on a word trans-
lation task, introduced in (mikolov et al., 2013b)
and used in (gouws et al., 2015). the words were
extracted from the publicly available wmt112
corpus. the experiments were done for two sets
of translation: english to spanish and spanish to
english. (mikolov et al., 2013b) extracted the top
6k most frequent words and translated them with
google translate. they used the top 5k pairs
to train a translation matrix, and evaluated their
method on the remaining 1k. as our english and

1thanks to s. gouws for providing this implementation
2http://www.statmt.org/wmt11/

method
klementiev et al.
bilingual auto-encoder
bicvm
bilbowa
trans-gram
trans-gram (size 300 vectors ep+wiki)

en     de de     en

speed-up in training time

77.6%
91.8%
83.7%
86,5%
87,8%
91,1%

71.1%
72.8%
71.4%
75%
78,7%
78,4%

  1

  3

  320

  800

  600

table 1: comparison of trans-gram with various methods for reuters english/german classi   cation

method
id153
bing
translation matrix
bilbowa
trans-gram

en     es p@1

en     es p@5

es     en p@1

es     en p@5

13%
55%
33%
39%
45%

18%

35%
44%
61%

24%
71%
51%
51%
47%

27%

52%
55%
62%

table 2: results on the translation task

spanish vectors are already aligned we don   t need
the 5k training pairs and use only the 1k test
pairs.

the reported score,

the translation precision
p @k, is the fraction of test-pairs where the tar-
get translation (google translate) is one of the k
translations proposed by our model. for a given
english word, w, our model takes its target vectors
~w and proposes the k closest spanish word using
the co-similarity of their vectors to ~w. we com-
pare ourselves to the    translation matrix    method
and to the bilbowa aligned vectors. we also re-
port the scores obtained by a trivial algorithm that
uses edit-distance to determine the closest transla-
tion and by the bing translator service.

6 interesting properties

6.1 cross-lingual disambiguation
we now present
the task of cross-lingual dis-
ambiguation as an example of possible uses of
aligned multilingual vectors. the goal of this task
is to    nd a suitable representation of each sense of
a given polysemous word. the idea of our method
is to look for a language in which the undesired
senses are represented by unambiguous words and
then to perform some arithmetic operation.

let   s illustrate the process with a concrete ex-
ample: consider the french word    train   , trainfr.
~trainfr translate
the three closest polish words to
in english into    now   ,    a train    and    when   . this
seems a poor matching. in fact, trainfr is polyse-
mous. it can name a line of railroad cars, but it is
also used to form progressive tenses. the french

   il est en train de manger    translates into    he is
eating   , or in italian    sta mangiando   .

as the italian word    sta    is used to form pro-
gressive tenses, it   s a good candidate to disam-
biguate trainfr. let   s introduce the vector ~v =
~trainfr     ~stait. now the three polish words clos-
est to ~v translate in english into    a train   ,    a train   
and    railroad   . therefore ~v is a better representa-
tion for the railroad sense of trainfr.

6.2 transfer of linguistic features

another interesting property of the vectors gener-
ated by trans-gram is the transfer of linguistic fea-
tures through a pivot language that does not pos-
sess these features.

let   s illustrate this by focusing on latin lan-
guages, which possess some features that en-
glish does not, like rich conjugations. for ex-
ample,
in french and italian the in   nitives of
   eat    are mangerfr and mangiareit, and the    rst
plural persons are mangeonsfr and mangiamoit.
actually in our models we observe the follow-
~mangiareit and
~mangerfr    
ing alignments:
~mangeonsfr    
~mangiamoit. it is thus remark-
able to see that features not present in english
match in languages aligned through english as
the only pivot language. we also found similar
transfers for the genders of adjectives and are cur-
rently studying other similar properties captured
by trans-gram.

[lewis and li2004] y.; rose t.; lewis, d. d.; yang and
f. li. 2004. rcv1: a new benchmark collection for
text categorization research. in journal of machine
learning research.

[mikolov et al.2013a] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013a. ef   cient estima-
tion of word representations in vector space. arxiv
preprint arxiv:1301.3781.

[mikolov et al.2013b] tomas mikolov, quoc v le, and
exploiting similarities
arxiv

ilya sutskever.
among languages for machine translation.
preprint arxiv:1309.4168.

2013b.

7 conclusion

in this paper we provided the following contri-
butions: trans-gram, a new method to compute
cross-lingual word-embeddings in a single word
space; state of the art results on cross-lingual nlp
tasks; a sketch of a cross-lingual calculus to help
disambiguate polysemous words;
the exhibition
of linguistic features transfers through a pivot-
language not possessing those features.

we are still exploring promising properties of
the generated vectors and their applications in
other nlp tasks (id31, ner...).

references
[al-rfou et al.2013] rami al-rfou, bryan perozzi, and
steven skiena. 2013. polyglot: distributed word
representations for multilingual nlp. arxiv preprint
arxiv:1307.1662.

[faruqui and dyer2014] manaal faruqui and chris
dyer. 2014. improving vector space word represen-
tations using multilingual correlation. association
for computational linguistics.

[gouws et al.2015] stephan gouws, yoshua bengio,
and greg corrado. 2015. bilbowa: fast bilingual
distributed representations without word alignments.
in proceedings of the 25th international conference
on machine learning, volume 15, pages 748   756.

[harris1968] zellig sabbettai harris. 1968. mathemat-

ical structures of language.

[hermann and blunsom2013] karl moritz hermann
and phil blunsom. 2013. multilingual distributed
representations without word alignment.
arxiv
preprint arxiv:1312.6173.

[hinton1984] geoffrey e hinton. 1984. distributed

representations.

[klementiev et al.2012] alexandre klementiev,

titov, and binod bhattarai.
crosslingual distributed representations of words.

2012.

ivan
inducing

[koehn2005] philipp koehn. 2005. a parallel corpus

for id151. in mt summit.

[lauly et al.2014] stanislas lauly, hugo larochelle,
mitesh khapra, balaraman ravindran, vikas c
raykar, and amrita saha. 2014. an autoencoder
approach to learning bilingual word representations.
in advances in neural information processing sys-
tems, pages 1853   1861.

[levy and goldberg2014] omer levy and yoav gold-
berg. 2014. neural id27 as implicit
id105. in advances in neural infor-
mation processing systems, pages 2177   2185.

method

no word

no prior on the

no explicit

computationally

can leverage

alignments

mapping between

alignments of

e   cient

mono-lingual

required

target vectors

target vectors

corpus

mikolov   s translation matrix
faruqui   s translation matrix

zou   s translation matrix

bicvm

bilingual auto-encoder

klementiev
bilbowa
trans-gram

  
  
  
x

x

x

x

x

  
  
  
x

x
  
x

x

x

x

x
  
  
x
  
x

x

x

x

x
  
  
x

x

x

x

x
  
  
x

x

x

table 1     comparative study of each method

beaufr
belit

lindopt

encantadores

bellefr
bellait
bonitapt
bellaes

frumosro

adormitaro

table 2     closest word in di   erent languages to the given french word. to the masculine    beau   
(pretty) the closest word are also masculine, while the closest words to the feminine    belle   
(pretty) are all feminine.

trainfr

trainfr     stait

pl

translation

pl

translation

6
1
0
2

 

n
a
j
 

1
1

 
 
]
l
c
.
s
c
[
 
 

1
v
2
0
5
2
0

.

1
0
6
1
:
v
i
x
r
a

obecnie
pocia  g
kiedy

pocia  gu

tym

kt  orym

na

aktualnie

perony
czym

now
train
when
train
this
where

on

currently
platforms

pocia  g
pocia  gu
kolejowa  
perony
tor  ow

train
train
train

platforms

tracks

wagon  ow

wagons bus

tory
torze
trasie

tracks
track
from
speed

what

pre  dko  scia  

table 3     top ten polish words and their english translations, for the ambiguous french word
   train    and its disambiguated version.

fr

innovation
innovations
cr  eativit  e

entrepreneurial

innovante

entrepreneuriat
comp  etitivit  e
technologies

it

innovazione
innovativa
innovazioni

imprenditoriale
imprenditorialit`a

pt

inova  c  ao

criatividade

inova  c  oes
inovadora
tecnologias

es

innovaci  on
innovaciones
innovadora
creatividad

en

non-technological

entrepreneurial

knowledge-driven

innovations

de

innovation

innovationen

unternehmergeist
wissensgesellschaft

tic

information-based

innovations-

pmi

competitividade

tecnolog    as

innovating

tic

competitividad

eco-e   ciency

innovationsf  ahigkeit
unternehmerischer

empreendedor

criadora

pci

estimulaci  on
emprendedor

research

forschungs-

policy-driven

unternehmerisches

punteras

internationalisation wettbewerbsf  ahigkeit

innovativo
creativit`a

entreprenariat

imprenditoria

innovant

tecnologie

bg

                    

                
                

                    

                                              

cs

inovaci
inovace
inovac    
inovac    m
inova  cn    

                                

internacionalizaci

pl

innowacji

innowacyjno  sci
innowacyjno  s  c

innowacje
innowacjom
innowacyjnej
innowacjami

el

                    
                      
                      
                      

            

                                      

                    

innowacyjnego

                                

tworzenia

kreatywno  s  c

hu

innov  aci  o

innov  aci  onak
innov  aci  ohoz

innov  aci  os
innov  aci  ot

kutat  as
tud  asba

innov  aci  on

kutat  as-fejleszt  es

                    
                  

nl

innovatie
innovaties

innovatievermogen
innovatiecapaciteit

innoverende

ondernemersklimaat

innovatief

   

kilpailukyvyn
teknologian
kilpailukyky  a
kilpailukyky
tutkimukseen
pk-yritysten
ty  ollisyytt  a

kasvun
kasvua

ty  opaikkojen

da

innovation

innovationen
videnbaseret
nyskabelse
innovationer

innovativ

innovations-

                        
                      
                    

                

et

innovatsiooni
innovatsioonile
teadmusp  ohise

uuendustele

uuendusi

innovatsioonipotentsiaali

uuendustest
uuenduste

uuendusteks

innovatiepotentieel
kennismaatschappij

innovationsevne
ivaerksaetter  and

  uj    t  asokat

ondernemerschap

innovativt

innovatsiooniks

sl

inovacij
inovacije

inovativnost
inovativnosti

inovacijam

inovativnostjo

inovacijske
ustvarjanja
inovacijah
inovacijami

sk

inov  acie
inov  aci    
inov  acii

inov  aci  am
inov  aciu
v  yskumu
inov  acia
inova  cnej
znalostnej

konkurencieschopnos  t

lt

naujoviu  
inovaciju  
naujoves
naujov  ems
inovacijoms
inovacijomis

inovacijas
naujov  emis
inovacijos
naujov  es

sv

innovation
nyskapande
innovationen
innovationer

innovationsf  orm  aga
entrepren  orsandan
kunskapsbaserad
kunskapsbaserat
innovationerna

innovationsomr  adet

znalostn    
v  yzkumu

znalostn    ho
inovacemi

ro

inovare
inova  tiei
inova  tie
inovarea
inov  arii
inova  tia

cunoa  sterii
cunoa  stere

inov  ari
inova  tii

lv

jaunin  ajumiem

inov  acijai
inov  aciju
inov  acijas

jaunin  ajumu

inov  acij  as
inov  acij  am

jaunin  ajumus

jaunrades

novatorisma

table 4     for each of the 21 languages of the europarl, the top ten words closest to innovationfr

this figure "spa-en.png" is available in "png"(cid:10) format from:

http://arxiv.org/ps/1601.02502v1

