7
1
0
2

 
t
c
o
9
1

 

 
 
]

v
c
.
s
c
[
 
 

6
v
8
0
1
7
0

.

2
1
5
1
:
v
i
x
r
a

recentadvancesinconvolutionalneuralnetworksjiuxianggua,   ,zhenhuawangb,   ,jasonkuenb,lianyangmab,amirshahroudyb,bingshuaib,tingliub,xingxingwangb,liwangb,gangwangb,jianfeicaic,tsuhanchencaroselab,interdisciplinarygraduateschool,nanyangtechnologicaluniversity,singaporebschoolofelectricalandelectronicengineering,nanyangtechnologicaluniversity,singaporecschoolofcomputerscienceandengineering,nanyangtechnologicaluniversity,singaporeabstractinthelastfewyears,deeplearninghasledtoverygoodperformanceonavarietyofproblems,suchasvisualrecognition,speechrecognitionandnaturallanguageprocessing.amongdi   erenttypesofdeepneuralnetworks,convolutionalneuralnetworkshavebeenmostextensivelystudied.leveragingontherapidgrowthintheamountoftheannotateddataandthegreatimprovementsinthestrengthsofgraphicsprocessorunits,theresearchonconvolutionalneuralnetworkshasbeenemergedswiftlyandachievedstate-of-the-artresultsonvarioustasks.inthispaper,weprovideabroadsurveyoftherecentadvancesinconvolutionalneuralnetworks.wedetailizetheimprovementsofid98ondi   erentaspects,includinglayerdesign,activationfunction,lossfunction,id173,optimizationandfastcomputation.besides,wealsointroducevariousapplicationsofconvolutionalneuralnetworksincomputervision,speechandnaturallanguageprocessing.keywords:convolutionalneuralnetwork,deeplearning1.introductionconvolutionalneuralnetwork(id98)isawell-knowndeeplearningarchitectureinspiredbythenaturalvisualperceptionmechanismofthelivingcreatures.in1959,hubel&wiesel[1]foundthatcellsinanimalvisualcortexareresponsiblefordetectinglightinreceptive   elds.inspiredbythisdiscovery,kunihikofukushimaproposedtheneocognitronin1980[2],whichcouldberegardedasthepredecessorofid98.in1990,lecunetal.[3]publishedtheseminalpaperestablishingthemodernframeworkofid98,andlaterimproveditin[4].theydevelopedamulti-layerarti   cialneuralnetworkcalledlenet-5whichcouldclassifyhandwrittendigits.likeotherneuralnetworks,lenet-5hasmultiplelayersandcanbetrainedwiththeid26algorithm[5].itcanobtaine   ectiverepresentationsoftheoriginalimage,whichmakesitpossibletorecognizevisualpatternsdirectlyfromrawpixelswithlittle-to-nonepreprocessing.aparallelstudyofzhangetal.[6]usedashift-invariantarti   cialneuralnetwork(siann)torecognizecharactersfromanimage.however,duetothelackoflargetrainingdataandcomputingpoweratthattime,theirnetworkscannotperformwellonmorecomplexproblems,e.g.,large-scaleimageandvideoclassi   cation.since2006,manymethodshavebeendevelopedtoovercomethedi   cultiesencounteredintrainingdeepid98s[7   10].mostnotably,krizhevskyetal.proposedaclassicid98architectureandshowedsigni   cantimprovementsuponpreviousmethodsontheimageclassi   cationtask.theoverallarchitectureoftheirmethod,i.e.,alexnet[8],issimilartolenet-5butwithadeeperstructure.withthesuccessofalexnet,manyworkshavebeenproposedtoimproveitsperformance.amongthem,fourrepresentativeworksare   equalcontributionemailaddresses:jgu004@ntu.edu.sg(jiuxianggu),zhwang.me@gmail.com(zhenhuawang),jasonkuen@ntu.edu.sg(jasonkuen),lyma@ntu.edu.sg(lianyangma),amir3@ntu.edu.sg(amirshahroudy),bshuai001@ntu.edu.sg(bingshuai),liut0016@e.ntu.edu.sg(tingliu),wangxx@ntu.edu.sg(xingxingwang),wa0002li@e.ntu.edu.sg(liwang),wanggang@ntu.edu.sg(gangwang),asjfcai@ntu.edu.sg(jianfeicai),tsuhan@ntu.edu.sg(tsuhanchen)figure1:hierarchically-structuredtaxonomyofthissurveyzfnet[11],vggnet[9],googlenet[10]andresnet[12].fromtheevolutionofthearchitectures,atypicaltrendisthatthenetworksaregettingdeeper,e.g.,resnet,whichwonthechampionofilsvrc2015,isabout20timesdeeperthanalexnetand8timesdeeperthanvggnet.byincreasingdepth,thenetworkcanbetterapproximatethetargetfunctionwithincreasednonlinearityandgetbetterfeaturerepresentations.however,italsoincreasesthecomplexityofthenetwork,whichmakesthenetworkbemoredi   culttooptimizeandeasiertogetover   tting.alongthisway,variousmethodshavebeenproposedtodealwiththeseproblemsinvariousaspects.inthispaper,wetrytogiveacomprehensivereviewofrecentadvancesandgivesomethoroughdiscussions.inthefollowingsections,weidentifybroadcategoriesofworksrelatedtoid98.figure1showsthehierarchically-structuredtaxonomyofthispaper.we   rstgiveanoverviewofthebasiccomponentsofid98insection2.then,weintroducesomerecentimprovementsondi   erentaspectsofid98includingconvolutionallayer,poolinglayer,activationfunction,lossfunction,id173andoptimizationinsec-tion3andintroducethefastcomputingtechniquesinsection4.next,wediscusssometypicalapplicationsofid98includingimageclassi   cation,objectdetection,objecttracking,poseestimation,textdetectionandrecognition,visualsaliencydetection,actionrecognition,scenelabeling,speechandnaturallanguageprocessinginsection5.finally,weconcludethispaperinsection6.2.basicid98componentstherearenumerousvariantsofid98architecturesintheliterature.however,theirbasiccomponentsareverysimilar.takingthefamouslenet-5asanexample,itconsistsofthreetypesoflayers,namelyconvolutional,pooling,andfully-connectedlayers.theconvolutionallayeraimstolearnfeaturerepresen-tationsoftheinputs.asshowninfigure2(a),convolutionlayeriscomposedofseveralconvolutionkernelswhichareusedtocomputedi   erentfeaturemaps.speci   cally,eachneuronofafeaturemapisconnectedtoaregionofneighbouringneuronsinthepreviouslayer.suchaneighbourhoodisreferredtoastheneuron   sreceptive   eldinthepreviouslayer.thenewfeaturemapcanbeobtainedby   rstconvolvingtheinputwithalearnedkernelandthenapplyinganelement-wisenonlinearactivationfunctionontheconvolvedresults.notethat,togenerateeachfeaturemap,thekernelissharedbyallspatiallocationsoftheinput.thecompletefeaturemapsareobtainedbyusingseveraldi   erentkernels.mathematically,thefeaturevalueatlocation(i,j)inthek-thfeaturemapofl-thlayer,zli,j,k,iscalculatedby:zli,j,k=wlktxli,j+blk(1)2(a)lenet-5network(b)learnedfeaturesfigure2:(a)thearchitectureofthelenet-5network,whichworkswellondigitclassi   cationtask.(b)visualizationoffeaturesinthelenet-5network.eachlayer   sfeaturemapsaredisplayedinadi   erentblock.wherewlkandblkaretheweightvectorandbiastermofthek-th   lterofthel-thlayerrespectively,andxli,jistheinputpatchcenteredatlocation(i,j)ofthel-thlayer.notethatthekernelwlkthatgeneratesthefeaturemapzl:,:,kisshared.suchaweightsharingmechanismhasseveraladvantagessuchasitcanreducethemodelcomplexityandmakethenetworkeasiertotrain.theactivationfunctionintroducesnonlinearitiestoid98,whicharedesirableformulti-layernetworkstodetectnonlinearfeatures.leta(  )denotethenonlinearactivationfunction.theactivationvalueali,j,kofconvolutionalfeaturezli,j,kcanbecomputedas:ali,j,k=a(zli,j,k)(2)typicalactivationfunctionsaresigmoid,tanh[13]andrelu[14].thepoolinglayeraimstoachieveshift-invariancebyreducingtheresolutionofthefeaturemaps.itisusuallyplacedbetweentwoconvolutionallayers.eachfeaturemapofapoolinglayerisconnectedtoitscorrespondingfeaturemapoftheprecedingconvolutionallayer.denotingthepoolingfunctionaspool(  ),foreachfeaturemapal:,:,kwehave:yli,j,k=pool(alm,n,k),   (m,n)   rij(3)whererijisalocalneighbourhoodaroundlocation(i,j).thetypicalpoolingoperationsareaveragepooling[15]andmaxpooling[16].figure2(b)showsthefeaturemapsofdigit7learnedbythe   rsttwoconvolutionallayers.thekernelsinthe1stconvolutionallayeraredesignedtodetectlow-levelfeaturessuchasedgesandcurves,whilethekernelsinhigherlayersarelearnedtoencodemoreabstractfeatures.bystackingseveralconvolutionalandpoolinglayers,wecouldgraduallyextracthigher-levelfeaturerepresen-tations.afterseveralconvolutionalandpoolinglayers,theremaybeoneormorefully-connectedlayerswhichaimtoperformhigh-levelreasoning[9,11,17].theytakeallneuronsinthepreviouslayerandconnectthemtoeverysingleneuronofcurrentlayertogenerateglobalsemanticinformation.notethatfully-connectedlayernotalwaysnecessaryasitcanbereplacedbya1  1convolutionlayer[18].thelastlayerofid98sisanoutputlayer.forclassi   cationtasks,thesoftmaxoperatoriscommonlyused[8].anothercommonlyusedmethodisid166,whichcanbecombinedwithid98featurestosolvedi   erentclassi   cationtasks[19,20].let      denotealltheparametersofaid98(e.g.,theweightvectorsandbiasterms).theoptimumparametersforaspeci   ctaskcanbeobtainedbyminimizinganappropriatelossfunctionde   nedonthattask.supposewehavendesiredinput-outputrelations{(xxx(n),yyy(n));n   [1,      ,n]},wherexxx(n)isthen-thinputdata,yyy(n)isitscorrespondingtargetlabelandooo(n)istheoutputofid98.thelossofid98canbecalculatedasfollows:l=1nnxn=1   (      ;yyy(n),ooo(n))(4)trainingid98isaproblemofglobaloptimization.byminimizingthelossfunction,wecan   ndthebest   ttingsetofparameters.stochasticgradientdescentisacommonsolutionforoptimizingid98network[21,22].3(a)convolution(b)tiledconvolution(c)dilatedconvolution(d)deconvolutionfigure3:illustrationof(a)convolution,(b)tiledconvolution,(c)dilatedconvolution,and(d)deconvolution3.improvementsonid98stherehavebeenvariousimprovementsonid98ssincethesuccessofalexnetin2012.inthissection,wedescribethemajorimprovementsonid98sfromsixaspects:convolutionallayer,poolinglayer,activationfunction,lossfunction,id173,andoptimization.3.1.convolutionallayerconvolution   lterinbasicid98sisageneralizedlinearmodel(glm)fortheunderlyinglocalimagepatch.itworkswellforabstractionwheninstancesoflatentconceptsarelinearlyseparable.hereweintroducesomeworkswhichaimtoenhanceitsrepresentationability.3.1.1.tiledconvolutionweightsharingmechanisminid98scandrasticallydecreasethenumberofparameters.however,itmayalsorestrictthemodelsfroid113arningotherkindsofinvariance.tiledid98[23]isavariationofid98thattilesandmultiplesfeaturemapstolearnrotationalandscaleinvariantfeatures.separatekernelsarelearnedwithinthesamelayer,andthecomplexinvariancescanbelearnedimplicitlybysquare-rootpoolingoverneighbouringunits.asillustratedinfigure3(b),theconvolutionoperationsareappliedeverykunit,wherekisthetilesizetocontrolthedistanceoverwhichweightsareshared.whenthetilesizekis1,theunitswithineachmapwillhavethesameweights,andtiledid98becomesidenticaltothetraditionalid98.in[23],theirexperimentsonthenorbandcifar-10datasetsshowthatk=2achievesthebestresults.wangetal.[24]   ndthattiledid98performsbetterthantraditionalid98[25]onsmalltimeseriesdatasets.3.1.2.transposedconvolutiontransposedconvolutioncanbeseenasthebackwardpassofacorrespondingtraditionalconvolution.itisalsoknownasdeconvolution[11,26   28]andfractionallystridedconvolution[29].tostayconsistentwithmostliterature[11,30],weusetheterm   deconvolution   .contrarytothetraditionalconvolutionthatconnectsmultipleinputactivationstoasingleactivation,deconvolutionassociatesasingleactivationwithmultipleoutputactivations.figure3(d)showsadeconvolutionoperationof3  3kernelovera4  4inputusingunitstrideandzeropadding.thestrideofdeconvolutiongivesthedilationfactorfortheinputfeaturemap.speci   cally,thedeconvolutionwill   rstupsampletheinputbyafactorofthestridevaluewithpadding,thenperformconvolutionoperationontheupsampledinput.recently,deconvolutionhasbeenwidelyusedforvisualization[11],recognition[31   33],localization[34],semanticsegmentation[30],visualquestionanswering[35],andsuper-resolution[36].3.1.3.dilatedconvolutiondilatedid98[37]isarecentdevelopmentofid98thatintroducesonemorehyper-parametertotheconvolutionallayer.byinsertingzerosbetween   lterelements,dilatedid98canincreasethenetwork   s4(a)linearconvolutionlayer(b)mlpconvlayerfigure4:thecomparisonoflinearconvolutionlayerandmlpconvlayer.receptive   eldsizeandletthenetworkcovermorerelevantinformation.thisisveryimportantfortaskswhichneedalargereceptive   eldwhenmakingtheprediction.formally,a1-ddilatedconvolutionwithdilationlthatconvolvessignalfwithkernelkofsizerisde   nedas(f   lk)t=p  k  ft   l  ,where   ldenotesl-dilatedconvolution.thisformulacanbestraightforwardlyextendedto2-ddilatedconvolution.figure3(c)showsanexampleofthreedilatedconvolutionlayerswherethedilationfactorlgrowsupexponentiallyateachlayer.themiddlefeaturemapf2isproducedfromthebottomfeaturemapf1byapplyinga1-dilatedconvolution,whereeachelementinf2hasareceptive   eldof3  3.f3isproducedfromf2byapplyinga2-dilatedconvolution,whereeachelementinf3hasareceptive   eldof(23   1)  (23   1).thetopfeaturemapf4isproducedfromf3byapplyinga4-dilatedconvolution,whereeachelementinf4hasareceptive   eldof(24   1)  (24   1).ascanbeseen,thesizeofreceptive   eldofeachelementinfi+1is(2i+2   1)  (2(i+2)   1).dilatedid98shaveachievedimpressiveperformanceintaskssuchasscenesegmentation[37],machinetranslation[38],speechsynthesis[39],andspeechrecognition[40].3.1.4.networkinnetworknetworkinnetwork(nin)isageneralnetworkstructureproposedbylinetal.[18].itreplacesthelinear   lteroftheconvolutionallayerbyamicronetwork,e.g.,multilayerid88convolution(mlpconv)layerinthepaper,whichmakesitcapableofapproximatingmoreabstractrepresentationsofthelatentconcepts.theoverallstructureofninisthestackingofsuchmicronetworks.figure4showsthedi   erencebetweenthelinearconvolutionallayerandthemlpconvlayer.formally,thefeaturemapofconvolutionlayer(withnonlinearactivationfunction,e.g.,relu[14])iscomputedas:ai,j,k=max(wtkxi,j+bk,0)(5)whereai,j,kistheactivationvalueofk-thfeaturemapatlocation(i,j),xi,jistheinputpatchcenteredatlocation(i,j),wkandbkareweightvectorandbiastermofthek-th   lter.asacomparison,thecomputationperformedbymlpconvlayerisformulatedas:ani,j,kn=max(wtknan   1i,j,:+bkn,0)(6)wheren   [1,n],nisthenumberoflayersinthemlpconvlayer,a0i,j,:isequaltoxi,j.inmlpconvlayer,1  1convolutionsareplacedafterthetraditionalconvolutionallayer.the1  1convolutionisequivalenttothecross-channelparametricpoolingoperationwhichissucceededbyrelu[14].therefore,themlpconvlayercanalsoberegardedasthecascadedcross-channelparametricpoolingonthenormalconvolutionallayer.intheend,theyalsoapplyaglobalaveragepoolingwhichspatiallyaveragesthefeaturemapsofthe   nallayer,anddirectlyfeedtheoutputvectorintosoftmaxlayer.comparedwiththefully-connectedlayer,globalaveragepoolinghasfewerparametersandthusreducestheover   ttingriskandcomputationalload.3.1.5.inceptionmoduleinceptionmoduleisintroducedbyszegedyetal.[10]whichcanbeseenasalogicalculminationofnin.theyusevariable   ltersizestocapturedi   erentvisualpatternsofdi   erentsizes,andapproximate5(a)(b)(c)(d)figure5:(a)inceptionmodule,naiveversion.(b)theinceptionmoduleusedin[10].(c)theimprovedinceptionmoduleusedin[41]whereeach5  5convolutionisreplacedbytwo3  3convolutions.(d)theinception-resnet-amoduleusedin[42].theoptimalsparsestructurebytheinceptionmodule.speci   cally,inceptionmoduleconsistsofonepoolingoperationandthreetypesofconvolutionoperations(seefigure5(b)),and1  1convolutionsareplacedbefore3  3and5  5convolutionsasdimensionreductionmodules,whichallowforincreasingthedepthandwidthofid98withoutincreasingthecomputationalcomplexity.withthehelpofinceptionmodule,thenetworkparameterscanbedramaticallyreducedto5millionswhicharemuchlessthanthoseofalexnet(60millions)andzfnet(75millions).intheirlaterpaper[41],to   ndhigh-performancenetworkswitharelativelymodestcomputationcost,theysuggesttherepresentationsizeshouldgentlydecreasefrominputstooutputsaswellasspatialaggre-gationcanbedoneoverlowerdimensionalembeddingswithoutmuchlossinrepresentationalpower.theoptimalperformanceofthenetworkcanbereachedbybalancingthenumberof   ltersperlayerandthedepthofthenetwork.inspiredbytheresnet[12],theirlatestinception-v4[42]combinestheinceptionar-chitecturewithshortcutconnections(seefigure5(d)).they   ndthatshortcutconnectionscansigni   cantlyacceleratethetrainingofinceptionnetworks.theirinception-v4modelarchitecture(with75trainablelay-ers)thatensemblesthreeresidualandoneinception-v4canachieve3.08%top-5errorrateonthevalidationdatasetofilsvrc2012.3.2.poolinglayerpoolingisanimportantconceptofid98.itlowersthecomputationalburdenbyreducingthenumberofconnectionsbetweenconvolutionallayers.inthissection,weintroducesomerecentpoolingmethodsusedinid98s.3.2.1.lppoolinglppoolingisabiologicallyinspiredpoolingprocessmodelledoncomplexcells[43].ithasbeentheoret-icallyanalyzedin[44],whichsuggestthatlppoolingprovidesbettergeneralizationthanmaxpooling.lppoolingcanberepresentedas:yi,j,k=[x(m,n)   rij(am,n,k)p]1/p(7)whereyi,j,kistheoutputofthepoolingoperatoratlocation(i,j)ink-thfeaturemap,andam,n,kisthefeaturevalueatlocation(m,n)withinthepoolingregionrijink-thfeaturemap.specially,whenp=1,lpcorrespondstoaveragepooling,andwhenp=   ,lpreducestomaxpooling.3.2.2.mixedpoolinginspiredbyrandomdropout[17]anddropconnect[45],yuetal.[46]proposeamixedpoolingmethodwhichisthecombinationofmaxpoolingandaveragepooling.thefunctionofmixedpoolingcanbe6formulatedasfollows:yi,j,k=  max(m,n)   rijam,n,k+(1     )1|rij|x(m,n)   rijam,n,k(8)where  isarandomvaluebeingeither0or1whichindicatesthechoiceofeitherusingaveragepoolingormaxpooling.duringforwardpropagationprocess,  isrecordedandwillbeusedfortheid26operation.experimentsin[46]showthatmixedpoolingcanbetteraddresstheover   ttingproblemsanditperformsbetterthanmaxpoolingandaveragepooling.3.2.3.stochasticpoolingstochasticpooling[47]isadropout-inspiredpoolingmethod.insteadofpickingthemaximumvaluewithineachpoolingregionasmaxpoolingdoes,stochasticpoolingrandomlypickstheactivationsaccordingtoamultinomialdistribution,whichensuresthatthenon-maximalactivationsoffeaturemapsarealsopossibletobeutilized.speci   cally,stochasticpooling   rstcomputestheprobabilitiespforeachregionrjbynormalizingtheactivationswithintheregion,i.e.,pi=ai/pk   rj(ak).afterobtainingthedistribu-tionp(p1,...,p|rj|),wecansamplefromthemultinomialdistributionbasedonptopickalocationlwithintheregion,andthensetthepooledactivationasyj=al,wherel   p(p1,...,p|rj|).comparedwithmaxpooling,stochasticpoolingcanavoidover   ttingduetothestochasticcomponent.3.2.4.spectralpoolingspectralpooling[48]performsdimensionalityreductionbycroppingtherepresentationofinputinfre-quencydomain.givenaninputfeaturemapx   rm  m,supposethedimensionofdesiredoutputfeaturemapish  w,spectralpooling   rstcomputesthediscretefouriertransform(dft)oftheinputfeaturemap,thencropsthefrequencyrepresentationbymaintainingonlythecentralh  wsubmatrixofthefre-quencies,and   nallyusesinversedfttomaptheapproximationbackintospatialdomain.comparedwithmaxpooling,thelinearlow-pass   lteringoperationofspectralpoolingcanpreservemoreinformationforthesameoutputdimensionality.meanwhile,italsodoesnotsu   erfromthesharpreductioninoutputmapdimensionalityexhibitedbyotherpoolingmethods.whatismore,theprocessofspectralpoolingisachievedbymatrixtruncation,whichmakesitcapableofbeingimplementedwithlittlecomputationalcostinid98s(e.g.,[49])thatemployfftforconvolutionkernels.3.2.5.spatialpyramidpoolingspatialpyramidpooling(spp)isintroducedbyheetal.[50].thekeyadvantageofsppisthatitcangeneratea   xed-lengthrepresentationregardlessoftheinputsizes.spppoolsinputfeaturemapinlocalspatialbinswithsizesproportionaltotheimagesize,resultingina   xednumberofbins.thisisdi   erentfromtheslidingwindowpoolinginthepreviousdeepnetworks,wherethenumberofslidingwindowsdependsontheinputsize.byreplacingthelastpoolinglayerwithspp,theyproposeanewspp-netwhichisabletodealwithimageswithdi   erentsizes.3.2.6.multi-scaleorderlesspoolinginspiredby[51],gongetal.[52]usemulti-scaleorderlesspooling(mop)toimprovetheinvarianceofid98swithoutdegradingtheirdiscriminativepower.theyextractdeepactivationfeaturesforboththewholeimageandlocalpatchesofseveralscales.theactivationsofthewholeimagearethesameasthoseofpreviousid98s,whichaimtocapturetheglobalspatiallayoutinformation.theactivationsoflocalpatchesareaggregatedbyvladencoding[53],whichaimtocapturemorelocal,   ne-graineddetailsoftheimageaswellasenhancinginvariance.thenewimagerepresentationisobtainedbyconcatenatingtheglobalactivationsandthevladfeaturesofthelocalpatchactivations.3.3.activationfunctionaproperactivationfunctionsigni   cantlyimprovestheperformanceofaid98foracertaintask.inthissection,weintroducetherecentlyusedactivationfunctionsinid98s.73.3.1.relurecti   edlinearunit(relu)[14]isoneofthemostnotablenon-saturatedactivationfunctions.thereluactivationfunctionisde   nedas:ai,j,k=max(zi,j,k,0)(9)wherezi,j,kistheinputoftheactivationfunctionatlocation(i,j)onthek-thchannel.reluisapiecewiselinearfunctionwhichprunesthenegativeparttozeroandretainsthepositivepart(seefigure6(a)).thesimplemax(  )operationofreluallowsittocomputemuchfasterthansigmoidortanhactivationfunctions,anditalsoinducesthesparsityinthehiddenunitsandallowsthenetworktoeasilyobtainsparserepresentations.ithasbeenshownthatdeepnetworkscanbetrainede   cientlyusingreluevenwithoutpre-training[8].eventhoughthediscontinuityofreluat0mayhurttheperformanceofid26,manyworkshaveshownthatreluworksbetterthansigmoidandtanhactivationfunctionsempirically[54,55].3.3.2.leakyreluapotentialdisadvantageofreluunitisthatithaszerogradientwhenevertheunitisnotactive.thismaycauseunitsthatdonotactiveinitiallyneveractiveasthegradient-basedoptimizationwillnotadjusttheirweights.also,itmayslowdownthetrainingprocessduetotheconstantzerogradients.toalleviatethisproblem,massetal.introduceleakyrelu(lrelu)[54]whichisde   nedas:ai,j,k=max(zi,j,k,0)+  min(zi,j,k,0)(10)where  isaprede   nedparameterinrange(0,1).comparedwithrelu,leakyrelucompressesthenegativepartratherthanmappingittoconstantzero,whichmakesitallowforasmall,non-zerogradientwhentheunitisnotactive.3.3.3.parametricreluratherthanusingaprede   nedparameterinleakyrelu,e.g.,  ineq.(10),heetal.[56]proposeparametricrecti   edlinearunit(prelu)whichadaptivelylearnstheparametersoftherecti   ersinordertoimproveaccuracy.mathematically,prelufunctionisde   nedas:ai,j,k=max(zi,j,k,0)+  kmin(zi,j,k,0)(11)where  kisthelearnedparameterforthek-thchannel.aspreluonlyintroducesaverysmallnumberofextraparameters,e.g.,thenumberofextraparametersisthesameasthenumberofchannelsofthewholenetwork,thereisnoextrariskofover   ttingandtheextracomputationalcostisnegligible.italsocanbesimultaneouslytrainedwithotherparametersbyid26.3.3.4.randomizedreluanothervariantofleakyreluisrandomizedleakyrecti   edlinearunit(rrelu)[57].inrrelu,theparametersofnegativepartsarerandomlysampledfromauniformdistributionintraining,andthen   xedintesting(seefigure6(c)).formally,rrelufunctionisde   nedas:a(n)i,j,k=max(z(n)i,j,k,0)+  (n)kmin(z(n)i,j,k,0)(12)wherez(n)i,j,kdenotestheinputofactivationfunctionatlocation(i,j)onthek-thchannelofn-thexample,  (n)kdenotesitscorrespondingsampledparameter,anda(n)i,j,kdenotesitscorrespondingoutput.itcouldreduceover   ttingduetoitsrandomizednature.xuetal.[57]alsoevaluaterelu,lrelu,preluandrreluonstandardimageclassi   cationtask,andconcludesthatincorporatinganon-zeroslopfornegativepartinrecti   edactivationunitscouldconsistentlyimprovetheperformance.8(a)relu(b)lrelu/prelu(c)rrelu(d)elufigure6:thecomparisonamongrelu,lrelu,prelu,rreluandelu.forleakyrelu,  isempiricallyprede   ned.forprelu,  kislearnedfromtrainingdata.forrrelu,  (n)kisarandomvariablewhichissampledfromagivenuniformdistributionintrainingandkeeps   xedintesting.forelu,  isempiricallyprede   ned.3.3.5.eluclevertetal.[58]introduceexponentiallinearunit(elu)whichenablesfasterlearningofdeepneuralnetworksandleadstohigherclassi   cationaccuracies.likerelu,lrelu,preluandrrelu,eluavoidsthevanishinggradientproblembysettingthepositiveparttoidentity.incontrasttorelu,eluhasanegativepartwhichisbene   cialforfastlearning.comparedwithlrelu,prelu,andrreluwhichalsohaveunsaturatednegativeparts,eluemploysasaturationfunctionasnegativepart.asthesaturationfunctionwilldecreasethevariationoftheunitsifdeactivated,itmakeselumorerobusttonoise.thefunctionofeluisde   nedas:ai,j,k=max(zi,j,k,0)+min(  (ezi,j,k   1),0)(13)where  isaprede   nedparameterforcontrollingthevaluetowhichanelusaturatefornegativeinputs.3.3.6.maxoutmaxout[59]isanalternativenonlinearfunctionthattakesthemaximumresponseacrossmultiplechan-nelsateachspatialposition.asstatedin[59],themaxoutfunctionisde   nedas:ai,j,k=maxk   [1,k]zi,j,k,wherezi,j,kisthek-thchannelofthefeaturemap.itisworthnotingthatmaxoutenjoysallthebene   tsofrelusincereluisactuallyaspecialcaseofmaxout,e.g.,max(wt1x+b1,wt2x+b2)wherew1isazerovectorandb1iszero.besides,maxoutisparticularlywellsuitedfortrainingwithdropout.3.3.7.proboutspringenbergetal.[60]proposeaprobabilisticvariantofmaxoutcalledprobout.theyreplacethemaximumoperationinmaxoutwithaprobabilisticsamplingprocedure.speci   cally,they   rstde   neaid203foreachoftheklinearunitsas:pi=e  zi/pkj=1e  zj,where  isahyperparameterforcontrollingthevarianceofthedistribution.then,theypickoneofthekunitsaccordingtoamultinomialdistribution{p1,...,pk}andsettheactivationvaluetobethevalueofthepickedunit.inordertoincorporatewithdropout,theyactuallyre-de   netheprobabilitiesas:  p0=0.5,  pi=e  zi/(2.kxj=1e  zj)(14)theactivationfunctionisthensampledas:ai=(0ifi=0zielse(15)wherei   multinomial{  p0,...,  pk}.proboutcanachievethebalancebetweenpreservingthedesirableprop-ertiesofmaxoutunitsandimprovingtheirinvarianceproperties.however,intestingprocess,proboutiscomputationallyexpensivethanmaxoutduetotheadditionalid203calculations.93.4.lossfunctionitisimportanttochooseanappropriatelossfunctionforaspeci   ctask.weintroducefourrepresentativeonesinthissubsection:hingeloss,softmaxloss,contrastiveloss,tripletloss.3.4.1.hingelosshingelossisusuallyusedtotrainlargemarginclassi   erssuchassupportvectormachine(id166).thehingelossfunctionofamulti-classid166isde   nedineq.(16),wherewistheweightvectorofclassi   erandyyy(i)   [1,...,k]indicatesitscorrectclasslabelamongthekclasses.lhinge=1nnxi=1kxj=1[max(0,1     (yyy(i),j)wtxi)]p(16)where  (yyy(i),j)=1ifyyy(i)=j,otherwise  (yyy(i),j)=   1.notethatifp=1,eq.(16)ishinge-loss(l1-loss),whileifp=2,itisthesquaredhinge-loss(l2-loss)[61].thel2-lossisdi   erentiableandimposesalargerlossforpointwhichviolatesthemargincomparingwithl1-loss.[19]investigatesandcomparestheperformanceofsoftmaxwithl2-id166sindeepnetworks.theresultsonmnist[62]demonstratethesuperiorityofl2-id166oversoftmax.3.4.2.softmaxlosssoftmaxlossisacommonlyusedlossfunctionwhichisessentiallyacombinationofmultinomiallogisticlossandsoftmax.givenatrainingset{(xxx(i),yyy(i));i   1,...,n,yyy(i)   1,...,k},wherexxx(i)isthei-thinputimagepatch,andyyy(i)isitstargetclasslabelamongthekclasses.thepredictionofj-thclassfori-thinputistransformedwiththesoftmaxfunction:p(i)j=ez(i)j/pkl=1ez(i)l,wherez(i)jisusuallytheactivationsofadenselyconnectedlayer,soz(i)jcanbewrittenasz(i)j=wtja(i)+bj.softmaxturnsthepredictionsintonon-negativevaluesandnormalizesthemtogetaid203distributionoverclasses.suchprobabilisticpredictionsareusedtocomputethemultinomiallogisticloss,i.e.,thesoftmaxloss,asfollows:lsoftmax=   1n[nxi=1kxj=11{yyy(i)=j}logp(i)j](17)recently,liuetal.[63]proposethelarge-marginsoftmax(l-softmax)loss,whichintroducesanangularmargintotheangle  jbetweeninputfeaturevectora(i)andthej-thcolumnwjofweightmatrix.thepredictionp(i)jforl-softmaxlossisde   nedas:p(i)j=ekwjkka(i)k  (  j)ekwjkka(i)k  (  j)+pl6=jekwlkka(i)kcos(  l)(18)  (  j)=(   1)kcos(m  j)   2k,  j   [k  /m,(k+1)  /m](19)wherek   [0,m   1]isaninteger,mcontrolsthemarginamongclasses.whenm=1,thel-softmaxlossreducestotheoriginalsoftmaxloss.byadjustingthemarginmbetweenclasses,arelativelydi   cultlearningobjectivewillbede   ned,whichcane   ectivelyavoidover   tting.theyverifythee   ectiveofl-softmaxonmnist,cifar-10,andcifar-100,and   ndthatthel-softmaxlossperformsbetterthantheoriginalsoftmax.3.4.3.contrastivelosscontrastivelossiscommonlyusedtotrainsiamesenetwork[64   67]whichisaweakly-supervisedschemeforlearningasimilaritymeasurefrompairsofdatainstanceslabelledasmatchingornon-matching.giventhei-thpairofdata(xxx(i)  ,xxx(i)  ),let(zzz(i,l)  ,zzz(i,l)  )denotesitscorrespondingoutputpairofthel-th(l   [1,      ,l])layer.in[65]and[66],theypasstheimagepairsthroughtwoidenticalid98s,andfeedthe10featurevectorsofthe   nallayertothecostmodule.thecontrastivelossfunctionthattheyusefortrainingsamplesis:lcontrastive=12nnxi=1(y)d(i,l)+(1   y)max(m   d(i,l),0)(20)whered(i,l)=||zzz(i,l)     zzz(i,l)  ||22,andmisamarginparametera   ectingnon-matchingpairs.if(xxx(i)  ,xxx(i)  )isamatchingpair,theny=1.otherwise,y=0.linetal.[68]   ndthatsuchasinglemarginlossfunctioncausesadramaticdropinretrievalresultswhen   ne-tuningthenetworkonallpairs.meanwhile,theperformanceisbetterretainedwhen   ne-tuningonlyonnon-matchingpairs.thisindicatesthatthehandlingofmatchingpairsinthelossfunctionisresponsibleforthedrop.whiletherecallrateonnon-matchingpairsaloneisstable,handlingthematchingpairsisthemainreasonforthedropinrecallrate.tosolvethisproblem,theyproposeadoublemarginlossfunctionwhichaddsanothermarginparametertoa   ectthematchingpairs.insteadofcalculatingthelossofthe   nallayer,theircontrastivelossisde   nedforeverylayerlandtheid26sforthelossofindividuallayersareperformedatthesametime.itisde   nedas:ld   contrastive=12nnxi=1lxl=1(y)max(d(i,l)   m1,0)+(1   y)max(m2   d(i,l),0)(21)inpractice,they   ndthatthesetwomarginparameterscansettobeequal(m1=m2=m)andbelearnedfromthedistributionofthesampledmatchingandnon-matchingimagepairs.3.4.4.tripletlosstripletloss[69]considersthreeinstancesperlossfunction.thetripletunits(xxx(i)a,xxx(i)p,xxx(i)n)usuallycontainananchorinstancexxx(i)aaswellasapositiveinstancexxx(i)pfromthesameclassofxxx(i)aandanegativeinstancexxx(i)n.let(zzz(i)a,zzz(i)p,zzz(i)n)denotethefeaturerepresentationofthetripletunits,thetripletlossisde   nedas:ltriplet=1nnxi=1max{d(i)(a,p)   d(i)(a,n)+m,0}(22)whered(i)(a,p)=kzzz(i)a   zzz(i)pk22andd(i)(a,n)=kzzz(i)a   zzz(i)nk22.theobjectiveoftripletlossistominimizethedistancebetweentheanchorandpositive,andmaximizethedistancebetweenthenegativeandtheanchor.however,randomlyselectedanchorsamplesmayjudgefalselyinsomespecialcases.forexample,whend(i)(n,p)<d(i)(a,p)<d(i)(a,n),thetripletlossmaystillbezero.thusthetripletunitswillbeneglectedduringthebackwardpropagation.liuetal.[70]proposethecoupledclusters(cc)losstosolvethisproblem.insteadofusingthetripletunits,thecoupledclusterslossfunctionisde   nedoverthepositivesetandthenegativeset.byreplacingtherandomlypickedanchorwiththeclustercenter,itmakesthesamplesinthepositivesetclustertogetherandsamplesinthenegativesetstayrelativelyfaraway,whichismorereliablethantheoriginaltripletloss.thecoupledclusterslossfunctionisde   nedas:lcc=1npnpxi=112max{kzzz(i)p   cpk22   kzzz(   )n   cpk22+m,0}(23)wherenpisthenumberofsamplesperset,zzz(   )nisthefeaturerepresentationofxxx(   )nwhichisthenearestnegativesampletotheestimatedcenterpointcp=(pnpizzz(i)p)/np.tripletlossanditsvariantshavebeenwidelyusedinvarioustasks,includingre-identi   cation[71],veri   cation[70],andimageretrieval[72].3.4.5.kullback-leiblerdivergencekullback-leiblerdivergence(kld)isanon-symmetricmeasureofthedi   erencebetweentwoid203distributionsp(x)andq(x)overthesamediscretevariablex(seefigure7(a)).thekldfromq(x)top(x)11(a)kldivergence(b)autoencodersvariantsandganvariantsfigure7:theillustrationof(a)thekullbackleiblerdivergencefortwonormalgaussiandistributions,(b)aevariants(ae,vae[73],dvae[74],andcvae[75])andganvariants(gan[76],cgan[77]).isde   nedas:dkl(p||q)=   h(p(x))   ep[logq(x)](24)=xxp(x)logp(x)   xxp(x)logq(x)=xxp(x)logp(x)q(x)(25)whereh(p(x))istheshannonid178ofp(x),ep(logq(x))isthecrossid178betweenp(x)andq(x).kldhasbeenwidelyusedasameasureofinformationlossintheobjectivefunctionofvariousautoen-coders(aes)[78   80].famousvariantsofaeincludesparseae[81,82],denoisingae[78]andvariationalae(vae)[73].vaeinterpretsthelatentrepresentationthroughbayesianid136.itconsistsoftwoparts:anencoderwhich   compresses   thedatasamplextothelatentrepresentationz   q  (z|x);andadecoder,whichmapssuchrepresentationbacktodataspace  x   p  (x|z)whichasclosetotheinputaspossible,where  and  aretheparametersofencoderanddecoderrespectively.asproposedin[73],vaestrytomaximizethevariationallowerboundofthelog-likelihoodoflogp(x|  ,  ):lvae=ez   q  (z|x)[logp  (x|z)]   dkl(q  (z|x)kp(z))(26)wherethe   rsttermisthereconstructioncost,andthekldtermenforcespriorp(z)ontheproposaldistributionq  (z|x).usuallyp(z)isthestandardnormaldistribution[73],discretedistribution[75],orsomedistributionswithgeometricinterpretation[83].followingtheoriginalvae,manyvariantshavebeenproposed[74,75,84].conditionalvae(cvae)[75,84]generatessamplesfromtheconditionaldistributionwith  x   p  (x|y,z).denoisingvae(dvae)[74]recoverstheoriginalinputxfromthecorruptedinput  x[78].jensen-shannondivergence(jsd)isasymmetricalformofkld.itmeasuresthesimilaritybetweenp(x)andq(x):djs(p||q)=12dkl(cid:18)p(x)(cid:13)(cid:13)(cid:13)(cid:13)p(x)+q(x)2(cid:19)+12dkl(cid:18)q(x)(cid:13)(cid:13)(cid:13)(cid:13)p(x)+q(x)2(cid:19)(27)byminimizingthejsd,wecanmakethetwodistributionsp(x)andq(x)ascloseaspossible.jsdhasbeensuccessfullyusedinthegenerativeadversarialnetworks(gans)[76,85,86].incontrasttovaesthatmodeltherelationshipbetweenxandzdirectly,gansareexplicitlysetuptooptimizeforgenerativetasks[85].theobjectiveofgansisto   ndthediscriminatordthatgivesthebestdiscriminationbetweentherealandgenerateddata,andsimultaneouslyencouragethegeneratorgto   ttherealdatadistribution.themin-maxgameplayedbetweenthediscriminatordandthegeneratorgisformalizedbythefollowingobjectivefunction:mingmaxdlgan(d,g)=ex   p(x)[logd(x)]+ez   q(z)[log(1   d(g(z)))](28)12theoriginalganpaper[76]showsthatfora   xedgeneratorg   ,wehavetheoptimaldiscriminatord   g(x)=p(x)p(x)+q(x).thentheequation28isequivalenttominimizethejsdbetweenp(x)andq(x).ifganddhaveenoughcapacity,thedistributionq(x)convergestop(x).likeconditionalvae,theconditionalgan(cgan)[77]alsoreceivesanadditionalinformationyasinputtogeneratesamplesconditioningony.inpractice,gansarenotoriouslyunstabletotrain[87,88].3.5.id173over   ttingisanunneglectableproblemindeepid98s,whichcanbee   ectivelyreducedbyid173.inthefollowingsubsection,weintroducesomee   ectiveid173techniques:   p-norm,dropout,anddropconnect.3.5.1.   p-normid173id173modi   estheobjectivefunctionbyaddingadditionaltermsthatpenalizethemodelcom-plexity.formally,ifthelossfunctionisl(  ,x,y),thentheregularizedlosswillbe:e(  ,x,y)=l(  ,x,y)+  r(  )(29)wherer(  )istheid173term,and  istheid173strength.   p-normid173functionisusuallyemployedasr(  )=pjk  jkpp.whenp   1,the   p-normisconvex,whichmakestheoptimizationeasierandrendersthisfunctionattractive[17].forp=2,the   2-normid173iscommonlyreferredtoasweightdecay.amoreprincipledalternativeof   2-normid173istikhonovid173[89],whichrewardsinvariancetonoiseintheinputs.whenp<1,the   p-normid173moreexploitsthesparsitye   ectoftheweightsbutconductstonon-convexfunction.3.5.2.dropoutdropoutis   rstintroducedbyhintonetal.[17],andithasbeenproventobeverye   ectiveinreducingover   tting.in[17],theyapplydropouttofully-connectedlayers.theoutputofdropoutisy=r   a(wtx),wherex=[x1,x2,...,xn]tistheinputtofully-connectedlayer,w   rn  disaweightmatrix,andrisabinaryvectorofsizedwhoseelementsareindependentlydrawnfromabernoullidistributionwithparameterp,i.e.ri   bernoulli(p).dropoutcanpreventthenetworkfrombecomingtoodependentonanyone(oranysmallcombination)ofneurons,andcanforcethenetworktobeaccurateevenintheabsenceofcertaininformation.severalmethodshavebeenproposedtoimprovedropout.wangetal.[90]proposeafastdropoutmethodwhichcanperformfastdropouttrainingbysamplingfromorintegratingagaussianapproximation.baetal.[91]proposeanadaptivedropoutmethod,wherethedropoutid203foreachhiddenvariableiscomputedusingabinarybeliefnetworkthatsharesparameterswiththedeepnetwork.in[92],they   ndthatapplyingstandarddropoutbefore1  1convolutionallayergenerallyincreasestrainingtimebutdoesnotpreventover   tting.therefore,theyproposeanewdropoutmethodcalledspatialdropout,whichextendsthedropoutvalueacrosstheentirefeaturemap.thisnewdropoutmethodworkswellespeciallywhenthetrainingdatasizeissmall.3.5.3.dropconnectdropconnect[45]takestheideaofdropoutastepfurther.insteadofrandomlysettingtheoutputsofneuronstozero,dropconnectrandomlysetstheelementsofweightmatrixwtozero.theoutputofdropconnectisgivenbyy=a((r   w)x),whererij   bernoulli(p).additionally,thebiasesarealsomaskedoutduringthetrainingprocess.figure8illustratesthedi   erencesamongno-drop,dropoutanddropconnectnetworks.3.6.optimizationinthissubsection,wediscusssomekeytechniquesforoptimizingid98s.13(a)no-drop(b)dropout(c)dropconnectfigure8:theillustrationofno-dropnetwork,dropoutnetworkanddropconnectnetwork.3.6.1.dataaugmentationdeepid98sareparticularlydependentontheavailabilityoflargequantitiesoftrainingdata.anelegantsolutiontoalleviatetherelativescarcityofthedatacomparedtothenumberofparametersinvolvedinid98sisdataaugmentation[8].dataaugmentationconsistsintransformingtheavailabledataintonewdatawithoutalteringtheirnatures.popularaugmentationmethodsincludesimplegeometrictransformationssuchassampling[8],mirroring[93],rotating[94],shifting[95],andvariousphotometrictransformations[96].paulinetal.[97]proposeagreedystrategythatselectsthebesttransformationfromasetofcandidatetransformations.however,theirstrategyinvolvesalargenumberofmodelre-trainingsteps,whichcanbecomputationallyexpensivewhenthenumberofcandidatetransformationsislarge.haubergetal.[98]proposeanelegantwayfordataaugmentationbyrandomlygeneratingdi   eomorphisms.xieetal.[99]andxuetal.[100]o   eradditionalmeansofcollectingimagesfromtheinternettoimprovelearningin   ne-grainedrecognitiontasks.3.6.2.weightinitializationdeepid98hasahugeamountofparametersanditslossfunctionisnon-convex[101],whichmakesitverydi   culttotrain.toachieveafastconvergenceintrainingandavoidthevanishinggradientproblem,apropernetworkinitializationisoneofthemostimportantprerequisites[102,103].thebiasparameterscanbeinitializedtozero,whiletheweightparametersshouldbeinitializedcarefullytobreakthesymmetryamonghiddenunitsofthesamelayer.ifthenetworkisnotproperlyinitialized,e.g.,eachlayerscalesitsinputbyk,the   naloutputwillscaletheoriginalinputbyklwherelisthenumberoflayers.inthiscase,thevalueofk>1leadstoextremelylargevaluesofoutputlayerswhilethevalueofk<1leadsadiminishingoutputvalueandgradients.krizhevskyetal.[8]initializetheweightsoftheirnetworkfromazero-meangaussiandistributionwithstandarddeviation0.01andsetthebiastermsofthesecond,fourthand   fthconvolutionallayersaswellasallthefully-connectedlayerstoconstantone.anotherfamousrandominitializationmethodis   xavier   ,whichisproposedin[104].theypicktheweightsfromagaussiandistributionwithzeromeanandavarianceof2/(nin+nout),whereninisthenumberofneuronsfeedingintoit,andnoutisthenumberofneuronstheresultisfedto.thus   xavier   canautomaticallydeterminethescaleofinitializationbasedonthenumberofinputandoutputneurons,andkeepthesignalinareasonablerangeofvaluesthroughmanylayers.oneofitsvariantsinca   e1usesthenin-onlyvariant,whichmakesitmucheasiertoimplement.   xavier   initializationmethodislaterextendedby[56]toaccountfortherectifyingnonlinearities,wheretheyderivearobustinitializationmethodthatparticularlyconsiderstherelunonlinearity.theirmethod,allowsforthetrainingofextremelydeepmodels(e.g.,[10])toconvergewhilethe   xavier   method[104]cannot.1https://github.com/bvlc/caffe14independently,saxeetal.[105]showthatorthonormalmatrixinitializationworksmuchbetterforlinearnetworksthangaussianinitialization,anditalsoworksfornetworkswithnonlinearities.mishkinetal.[102]extend[105]toaniterativeprocedure.speci   cally,itproposesalayer-sequentialunit-varianceprocessschemewhichcanbeviewedasanorthonormalinitializationcombinedwithbatchid172(seesection3.6.4)performedonlyonthe   rstmini-batch.itissimilartobatchid172asbothofthemtakeaunitvarianceid172procedure.di   erently,itusesortho-id172toinitializetheweightswhichhelpstoe   cientlyde-correlatelayeractivities.suchaninitializationtechniquehasbeenappliedto[106,107]witharemarkableincreaseinperformance.3.6.3.stochasticgradientdescenttheid26algorithmisthestandardtrainingmethodwhichusesgradientdescenttoupdatetheparameters.manygradientdescentoptimizationalgorithmshavebeenproposed[108,109].standardgradientdescentalgorithmupdatestheparameters      oftheobjectivel(      )as      t+1=      t              e[l(      t)],wheree[l(      t)]istheexpectationofl(      )overthefulltrainingsetand  isthelearningrate.insteadofcomputinge[l(      t)],stochasticgradientdescent(sgd)[21]estimatesthegradientsonthebasisofasinglerandomlypickedexample(xxx(t),yyy(t))fromthetrainingset:      t+1=      t     t         l(      t;xxx(t),yyy(t))(30)inpractice,eachparameterupdateinsgdiscomputedwithrespecttoamini-batchasopposedtoasingleexample.thiscouldhelptoreducethevarianceintheparameterupdateandcanleadtomorestableconvergence.theconvergencespeediscontrolledbythelearningrate  t.however,mini-batchsgddoesnotguaranteegoodconvergence,andtherearestillsomechallengesthatneedtobeaddressed.firstly,itisnoteasytochooseaproperlearningrate.onecommonmethodistouseaconstantlearningratethatgivesstableconvergenceintheinitialstage,andthenreducethelearningrateastheconvergenceslowsdown.additionally,learningrateschedules[110,111]havebeenproposedtoadjustthelearningrateduringthetraining.tomakethecurrentgradientupdatedependonhistoricalbatchesandacceleratetraining,momentum[108]isproposedtoaccumulateavelocityvectorintherelevantdirection.theclassicalmomentumupdateisgivenby:vvvt+1=  vvvt     t         l(      t;xxx(t),yyy(t))(31)      t+1=      t+vvvt+1(32)wherevvvt+1isthecurrentvelocityvector,  isthemomentumtermwhichisusuallysetto0.9.nesterovmomentum[103]isanotherwayofusingmomentumingradientdescentoptimization:vvvt+1=  vvvt     t         l(      t+  vvvt;xxx(t),yyy(t))(33)comparedwiththeclassicalmomentum[108]which   rstcomputesthecurrentgradientandthenmovesinthedirectionoftheupdatedaccumulatedgradient,nesterovmomentum   rstmovesinthedirectionofthepreviousaccumulatedgradient  vvvt,calculatesthegradientandthenmakesagradientupdate.thisanticipatoryupdatepreventstheoptimizationfrommovingtoofastandachievesbetterperformance[112].parallelizedsgdmethods[22,113]improvesgdtobesuitableforparallel,large-scalemachinelearning.unlikestandard(synchronous)sgdinwhichthetrainingwillbedelayedifoneofthemachinesisslow,theseparallelizedmethodsusetheasynchronousmechanismsothatnootheroptimizationswillbedelayedexceptfortheoneontheslowestmachine.je   reydeanetal.[114]useanotherasynchronoussgdprocedurecalleddownpoursgdtospeedupthelarge-scaledistributedtrainingprocessonclusterswithmanycpus.therearealsosomeworksthatuseasynchronoussgdwithmultiplegpus.paineetal.[115]basicallycombineasynchronoussgdwithgpustoacceleratethetrainingtimebyseveraltimescomparedtotrainingonasinglemachine.zhuangetal.[116]alsousemultiplegpustoasynchronouslycalculategradientsandupdatetheglobalmodelparameters,whichachieves3.2timesofspeedupon4gpuscomparedtotrainingonasinglegpu.15notethatsgdmethodsmaynotresultinconvergence.thetrainingprocesscanbeterminatedwhentheperformancestopsimproving.apopularremedytoover-trainingistouseearlystopping[117]inwhichoptimizationishaltedbasedontheperformanceonavalidationsetduringtraining.tocontrolthedurationofthetrainingprocess,variousstoppingcriteriacanbeconsidered.forexample,thetrainingmightbeperformedfora   xednumberofepochs,oruntilaprede   nedtrainingerrorisreached[118].thestoppingstrategyshouldbedonecarefully[119],aproperstoppingstrategyshouldletthetrainingprocesscontinueaslongasthenetworkgeneralizationabilityisimprovedandtheover   ttingisavoided.3.6.4.batchid172dataid172isusuallythe   rststepofdatapreprocessing.globaldataid172transformsallthedatatohavezero-meanandunitvariance.however,asthedata   owsthroughadeepnetwork,thedistributionofinputtointernallayerswillbechanged,whichwilllosethelearningcapacityandaccuracyofthenetwork.io   eetal.[120]proposeane   cientmethodcalledbatchid172(bn)topartiallyalleviatethisphenomenon.itaccomplishestheso-calledcovariateshiftproblembyaid172stepthat   xesthemeansandvariancesoflayerinputswheretheestimationsofmeanandvariancearecomputedaftereachmini-batchratherthantheentiretrainingset.supposethelayertonormalizehasaddimensionalinput,i.e.,x=[x1,x2,...,xd]t.we   rstnormalizethek-thdimensionasfollows:  xk=(xk     b)/q  2b+ (34)where  band  2barethemeanandvarianceofmini-batchrespectively,and isaconstantvalue.toenhancetherepresentationability,thenormalizedinput  xkisfurthertransformedinto:yk=bn  ,  (xk)=    xk+  (35)where  and  arelearnedparameters.batchid172hasmanyadvantagescomparedwithglobaldataid172.firstly,itreducesinternalcovariantshift.secondly,bnreducesthedependenceofgradientsonthescaleoftheparametersoroftheirinitialvalues,whichgivesabene   ciale   ectonthegradient   owthroughthenetwork.thisenablestheuseofhigherlearningratewithouttheriskofdivergence.furthermore,bnregularizesthemodel,andthusreducestheneedfordropout.finally,bnmakesitpossibletousesaturatingnonlinearactivationfunctionswithoutgettingstuckinthesaturatedmodel.3.6.5.shortcutconnectionsasmentionedabove,thevanishinggradientproblemofdeepid98scanbealleviatedbynormalizedinitialization[8]andbn[120].althoughthesemethodssuccessfullypreventdeepneuralnetworksfromover   tting,theyalsointroducedi   cultiesinoptimizingthenetworks,resultinginworseperformancesthanshallowernetworks[56,104,105].suchanoptimizationproblemsu   eredbydeeperid98sisregardedasthedegradationproblem.inspiredbylongshorttermmemory(lstm)networks[121]whichusegatefunctionstodeterminehowmuchofaneuron   sactivationvaluetotransformorjustpassthrough.srivastavaetal.[122]proposehighwaynetworkswhichenabletheoptimizationofnetworkswithvirtuallyarbitrarydepth.theoutputoftheirnetworkisgivenby:xl+1=  l+1(xl,wh)    l+1(xl,wt)+xl  (1     l+1(xl,wt))(36)wherexlandxl+1correspondtotheinputandoutputoflthhighwayblock,  (  )isthetransformgateand  (  )isusuallyana   netransformationfollowedbyanon-linearactivationfunction(ingeneralitmaytakeotherforms).thisgatingmechanismforcesthelayer   sinputsandoutputstobeofthesamesizeandallowshighwaynetworkswithtensorhundredsoflayerstobetrainede   ciently.theoutputsofgatesvarysigni   cantlywiththeinputexamples,demonstratingthatthenetworkdoesnotjustlearna   xedstructure,butdynamicallyroutesdatabasedonspeci   cexamples.independently,residualnets(resnets)[12]sharethesamecoreideathatworksinlstmunits.insteadofemployinglearnableweightsforneuron-speci   cgating,theshortcutconnectionsinresnetsarenotgated16anduntransformedinputisdirectlypropagatedtotheoutputwhichbringsfewerparameters.theoutputofresnetscanberepresentedasfollows:xl+1=xl+fl+1(xl,wf)(37)whereflistheweightlayer,itcanbeacompositefunctionofoperationssuchasconvolution,bn,relu,orpooling.withresidualblock,activationofanydeeperunitcanbewrittenasthesumoftheactivationofashallowerunitandaresidualfunction.thisalsoimpliesthatgradientscanbedirectlypropagatedtoshallowerunits,whichmakesdeepresnetsmucheasiertobeoptimizedthantheoriginalmappingfunctionandmoree   cienttotrainverydeepnets.thisisincontrasttousualfeedforwardnetworks,wheregradientsareessentiallyaseriesofmatrix-vectorproducts,thatmayvanishasnetworksgrowdeeper.aftertheoriginalresnets,heetal.[123]followupwithanotherpreactivationvariantofresnets,wheretheyconductasetofexperimentstoshowthatidentityshortcutconnectionsaretheeasiestfornetworkstolearn.theyalso   ndthatbringingbnforwardperformsconsiderablybetterthanusingbnafteraddition.intheircomparisons,theresidualnetwithbn+relupre-activationgetshigheraccuraciesthantheirpreviousresnets[12].inspiredby[123],shenetal.[124]introduceaweightingfactorfortheoutputfromtheconvolutionallayer,whichgraduallyintroducesthetrainablelayers.thelatestinception-v4paper[42]alsoreportsthattrainingisacceleratedandperformanceisimprovedbyusingidentityskipconnectionsacrossinceptionmodules.theoriginalresnetsandpreactivationresnetsareverydeepbutalsoverythin.bycontrast,wideresnets[125]proposestodecreasethedepthandincreasethewidth,whichachievesimpressiveresultsoncifar-10,cifar-100,andsvhn.however,theirclaimsarenotvalidatedonthelarge-scaleimageclassi   cationtaskonid163dataset2.stochasticdepthresnetsrandomlydropasubsetoflayersandbypassthemwiththeidentitymappingforeverymini-batch.bycombiningstochasticdepthresnetsanddropout,singhetal.[126]generalizedropoutandnetworkswithstochasticdepth,whichcanbeviewedasanensembleofresnets,dropoutresnets,andstochasticdepthresnets.theresnetsinresnets(rir)paper[127]describesanarchitecturethatmergesclassicalconvolutionalnetworksandresidualnetworks,whereeachblockofrircontainsresidualunitsandnon-residualblocks.therircanlearnhowmanyconvolutionallayersitshoulduseperresidualblock.resnetsofresnets(ror)[128]isamodi   cationtotheresnetsarchitecturewhichproposestousemulti-levelshortcutconnectionsasopposedtosingle-levelshortcutconnectionsinthepriorworkonresnets[12].densenet[129]canbeseenasanarchitecturetakestheinsightsoftheskipconnectiontotheextreme,inwhichtheoutputofalayerisconnectedtoallthesubsequentlayerinthemodule.inalloftheresnets[12,123],highway[122]andinceptionnetworks[42],wecanseeaprettycleartrendofusingshortcutconnectionstohelptrainverydeepnetworks.4.fastprocessingofid98swiththeincreasingchallengesinthecomputervisionandmachinelearningtasks,themodelsofdeepneuralnetworksgetmoreandmorecomplex.thesepowerfulmodelsrequiremoredatafortraininginordertoavoidover   tting.meanwhile,thebigtrainingdataalsobringsnewchallengessuchashowtotrainthenetworksinafeasibleamountoftime.inthissection,weintroducesomefastprocessingmethodsofid98s.4.1.fftmathieuetal.[49]carryouttheconvolutionaloperationinthefourierdomainwithffts.usingfft-basedmethodshasmanyadvantages.firstly,thefouriertransformationsof   lterscanbereusedasthe   ltersareconvolvedwithmultipleimagesinamini-batch.secondly,thefouriertransformationsoftheoutputgradientscanbereusedwhenbackpropagatinggradientstoboth   ltersandinputimages.finally,thesummationoverinputchannelscanbeperformedinthefourierdomain,sothatinversefouriertransformationsareonlyrequiredonceperoutputchannelperimage.therehavealreadybeensomegpu-basedlibrariesdevelopedtospeedupthetrainingandtestingprocess,suchascudnn[130]andfb   t[131].2http://www.image-net.org17however,usingffttoperformconvolutionneedsadditionalmemorytostorethefeaturemapsinthefourierdomain,sincethe   ltersmustbepaddedtobethesamesizeastheinputs.thisisespeciallycostlywhenthestridingparameterislargerthan1,whichiscommoninmanystate-of-artnetworks,suchastheearlylayersin[132]and[10].whilefftcanachievefastertrainingandtestingprocess,therisingprominenceoid122allsizeconvolutional   ltershavebecomeanimportantcomponentinid98ssuchasresnet[12]andgooglenet[10],whichmakesanewapproachspecializedforsmall   ltersizes:winograd   sminimal   lteringalgorithms[133].theinsightofwinogradislikefft,andthewinogradconvolutionscanbereducedacrosschannelsintransformspacebeforeapplyingtheinversetransformandthusmakestheid136moree   cient.4.2.structuredtransformslow-rankmatrixfactorizationhasbeenexploitedinavarietyofcontextstoimprovetheoptimizationproblems.givenanm  nmatrixcofrankr,thereexistsafactorizationc=abwhereaisanm  rfullcolumnrankmatrixandbisanr  nfullrowrankmatrix.thus,wecanreplacecbyaandb.toreducetheparametersofcbyafractionp,itisessentialtoensurethatmr+rn<pmn,i.e.,therankofcshouldsatisfythatr<pmn/(m+n).byapplyingthisfactorization,thespacecomplexityreducesfromo(mn)too(r(m+n)),andthetimecomplexityreducesfromo(mn)too(r(m+n)).tothisend,sainathetal.[134]applythelow-rankmatrixfactorizationtothe   nalweightlayerinadeepid98,resultingabout30-50%speedupintrainingtimewithlittlelossinaccuracy.similarly,xueetal.[135]applysingularvaluedecompositiononeachlayerofadeepid98toreducethemodelsizeby71%withlessthan1%relativeaccuracyloss.inspiredby[136]whichdemonstratestheredundancyintheparametersofdeepneuralnetworks,dentonetal.[137]andjaderbergetal.[138]independentlyinvestigatetheredundancywithintheconvolutional   ltersanddevelopapproximationstoreducedtherequiredcomputations.novikovetal.[139]generalizethelow-rankideas,wheretheytreattheweightmatrixasmulti-dimensionaltensorandapplyatensor-traindecomposition[140]toreducethenumberofparametersofthefully-connectedlayers.adaptivefastfoodtransformisgeneralizationofthefastfood[141]transformforapproximatingma-trix.itreparameterizestheweightmatrixc   rn  ninfully-connectedlayerswithanadaptivefastfoodtransformation:cx=(  d1h  d2  h  d3)x,where  d1,  d2and  d3arediagonalmatricesofparameters,  isarandompermutationmatrix,andhdenotesthewalsh-hadamardmatrix.thespacecomplexityofadaptivefastfoodtransformiso(n),andthetimecomplexityiso(nlogn).motivatedbythegreatadvantagesofcirculantmatrixinbothspaceandcomputatione   ciency[142,143],chengetal.[144]exploretheredundancyintheparametrizationoffully-connectedlayersbyimposingthecirculantstructureontheweightmatrixtospeedupthecomputation,andfurtherallowtheuseoffftforfastercomputation.withacirculantmatrixc   rn  nasthematrixofparametersinafully-connectedlayer,foraninputvectorx   rn,theoutputofcxcanbecalculatede   cientlyusingthefftandinverseifft:cdx=i   t(   t(v))      t(x),where   correspondstoelementwisemultiplicationoperation,v   rnisde   nedbyc,anddisarandomsign   ippingmatrixforimprovingthecapacityofthemodel.thismethodreducesthetimecomplexityfromo(n2)too(nlogn),andspacecomplexityfromo(n2)too(n).moczulskietal.[145]furthergeneralizethecirculantstructuresbyinterleavingdiagonalmatriceswiththeorthogonaldiscretecosinetransform(dct).theresultingtransform,acdc   1,haso(n)spacecomplexityando(nlogn)timecomplexity.4.3.lowprecisionfloatingpointnumbersareanaturalchoiceforhandlingthesmallupdatesoftheparametersofid98s.however,theresultingparametersmaycontainalotofredundantinformation[146].toreduceredundancy,binarizedneuralnetworks(bnns)restrictsomeorallthearithmeticsinvolvedincomputingtheoutputstobebinaryvalues.therearethreeaspectsofbinarizationforneuralnetworklayers:binaryinputactivations,binarysynapseweights,andbinaryoutputactivations.fullbinarizationrequiresallthethreecomponentsarebinarized,andthecaseswithoneortwocomponentsareconsideredaspartialbinarization.kimetal.[147]considerfullbinarizationwithapredeterminedportionofthesynapseshavingzeroweight,andallothersynapseswitha18weightofone.theirnetworkonlyneedsxnorandbitcountoperations,andtheyreport98.7%accuracyonthemnistdataset.xnor-net[148]appliesconvolutionalbnnsontheid163datasetwithtopologiesinspiredbyalexnet,resnetandgooglenet,reportingtop-1accuraciesofupto51.2%forfullbinarizationand65.5%forpartialbinarization.dorefa-net[149]exploresreducingprecisionduringtheforwardpassaswellasthebackwardpass.bothpartialandfullbinarizationareexploredintheirexperimentsandthecorrespondingtop-1accuraciesonid163are43%and53%.theworkbycourbariauxetal.[150]describeshowtotrainfully-connectednetworksandid98swithfullbinarizationandbatchid172layers,reportingcompetitiveaccuraciesonthemnist,svhn,andcifar-10datasets.4.4.weightcompressionmanyattemptshavebeenmadetoreducethenumberofparametersintheconvolutionlayersandfully-connectedlayers.here,webrie   yintroducesomemethodsunderthesetopics:vectorquantization,pruning,andhashing.vectorquantization(vq)isamethodforcompressingdenselyconnectedlayerstomakeid98modelssmaller.similartoscalarquantizationwherealargesetofnumbersismappedtoasmallerset[151],vqquantizesgroupsofnumberstogetherratherthanaddressingthemoneatatime.in2013,deniletal.[136]demonstratethepresenceofredundancyinneuralnetworkparameters,andusevqtosigni   cantlyreducethenumberofdynamicparametersindeepmodels.gongetal.[152]investigatetheinformationtheo-reticalvectorquantizationmethodsforcompressingtheparametersofid98s,andtheyobtainparameterpredictionresultssimilartothoseof[136].theyalso   ndthatvqmethodshaveacleargainoverexist-ingmatrixfactorizationmethods,andamongthevqmethods,structuredquantizationmethodssuchasproductquantizationworksigni   cantlybetterthanothermethods(e.g.,residualquantization[153],scalarquantization[154]).analternativeapproachtoweightcompressionispruning.itreducesthenumberofparametersandoperationsinid98sbypermanentlydroppinglessimportantconnections[155],whichenablessmallernet-workstoinheritknowledgefromthelargepredecessornetworksandmaintainscomparableofperformance.hanetal.[146,156]introduce   ne-grainedsparsityinanetworkbyamagnitude-basedpruningapproach.iftheabsolutemagnitudeofanyweightislessthanascalarthreshold,theweightispruned.gaoetal.[157]extendthemagnitude-basedapproachtoallowrestorationoftheprunedweightsinthepreviousiterations,withtightlycoupledpruningandretrainingstages,forgreatermodelcompression.yangetal.[158]takethecorrelationbetweenweightsintoconsiderationandproposeanenergy-awarepruningalgorithmthatdirectlyusesenergyconsumptionestimationofaid98toguidethepruningprocess.ratherthan   ne-grainedprun-ing,therearealsoworksthatinvestigatecoarse-grainedpruning.huetal.[159]proposeremoving   ltersthatfrequentlygeneratezerooutputactivationsonthevalidationset.srinivasetal.[160]mergesimilar   ltersintoone,whilemarietetal.[161]merge   lterswithsimilaroutputactivationsintoone.designingaproperhashingtechniquetoacceleratethetrainingofid98sorsavememoryspacealsoaninterestingproblem.hashednets[162]isarecenttechniquetoreducemodelsizesbyusingahashfunctiontogroupconnectionweightsintohashbuckets,andallconnectionswithinthesamehashbucketshareasingleparametervalue.theirnetworkshrinksthestoragecostsofneuralnetworkssigni   cantlywhilemostlypreservesthegeneralizationperformanceinimageclassi   cation.aspointedoutinshietal.[163]andweinbergeretal.[164],sparsitywillminimizehashcollisionmakingfeaturehashingevenmoree   ective.hashnetsmaybeusedtogetherwithpruningtogiveevenbetterparametersavings.4.5.sparseconvolutionrecently,severalattemptshavebeenmadetosparsifytheweightsofconvolutionallayers[165,166].liuetal.[165]considersparserepresentationsofthebasis   lters,andachieve90%sparsifyingbyexploitingbothinter-channelandintra-channelredundancyofconvolutionalkernels.insteadofsparsifyingtheweightsofconvolutionlayers,wenetal.[166]proposeastructuredsparsitylearning(ssl)approachtosimultaneouslyoptimizetheirhyperparameters(   ltersize,depth,andlocalconnectivity).bagherinezhadetal.[167]proposealookup-basedconvolutionalneuralnetwork(lid98)thatencodesconvolutionsbyfewlookupstoarichsetofdictionarythatistrainedtocoverthespaceofweightsinid98s.theydecodetheweightsofthe19convolutionallayerwithadictionaryandtwotensors.thedictionaryissharedamongallweight   ltersinalayer,whichallowsaid98tolearnfromveryfewtrainingexamples.lid98canachieveahigheraccuracyinasmallnumberofiterationscomparedtostandardid98.5.applicationsofid98sinthissection,weintroducesomerecentworksthatapplyid98stoachievestate-of-the-artperformance,includingimageclassi   cation,objecttracking,poseestimation,textdetection,visualsaliencydetection,actionrecognition,scenelabeling,speechandnaturallanguageprocessing.5.1.imageclassi   cationid98shavebeenappliedinimageclassi   cationforalongtime[168   171].comparedwithothermethods,id98scanachievebetterclassi   cationaccuracyonlargescaledatasets[8,9,172]duetotheircapabilityofjointfeatureandclassi   erlearning.thebreakthroughoflargescaleimageclassi   cationcomesin2012.krizhevskyetal.[8]developthealexnetandachievethebestperformanceinilsvrc2012.afterthesuccessofalexnet,severalworkshavemadesigni   cantimprovementsinclassi   cationaccuracybyeitherreducing   ltersize[11]orexpandingthenetworkdepth[9,10].buildingahierarchyofclassi   ersisacommonstrategyforimageclassi   cationwithalargenumberofclasses[173].theworkof[174]isoneoftheearliestattemptstointroducecategoryhierarchyinid98,inwhichadiscriminativetransferlearningwithtree-basedpriorsisproposed.theyuseahierarchyofclassesforsharinginformationamongrelatedclassesinordertoimproveperformanceforclasseswithveryfewtrainingexamples.similarly,wangetal.[175]buildatreestructuretolearn   ne-grainedfeaturesforsubcategoryrecognition.xiaoetal.[176]proposeatrainingmethodthatgrowsanetworknotonlyincrementallybutalsohierarchically.intheirmethod,classesaregroupedaccordingtosimilaritiesandareself-organizedintodi   erentlevels.yanetal.[177]introduceahierarchicaldeepid98s(hd-id98s)byembeddingdeepid98sintoacategoryhierarchy.theydecomposetheclassi   cationtaskintotwosteps.thecoarsecategoryid98classi   eris   rstusedtoseparateeasyclassesfromeachother,andthenthosemorechallengingclassesarerouteddownstreamto   necategoryclassi   ersforfurtherprediction.thisarchitecturefollowsthecoarse-to-   neclassi   cationparadigmandcanachievelowererroratthecostofana   ordableincreaseofcomplexity.subcategoryclassi   cationisanotherrapidlygrowingsub   eldofimageclassi   cation.therearealreadysome   ne-grainedimagedatasets(suchasbirds[178],dogs[179],cars[180],andplants[181]).usingobjectpartinformationisbene   cialfor   ne-grainedclassi   cation[182].generally,theaccuracycanbeimprovedbylocalizingimportantpartsofobjectsandrepresentingtheirappearancesdiscriminatively.alongthisway,bransonetal.[183]proposeamethodwhichdetectspartsandextractsid98featuresfrommultiplepose-normalizedregions.partannotationinformationisusedtolearnacompactposeid172space.theyalsobuildamodelthatintegrateslower-levelfeaturelayerswithpose-normalizedextractionroutinesandhigher-levelfeaturelayerswithunalignedimagefeaturestoimprovetheclassi   cationaccuracy.zhangetal.[184]proposeapart-basedr-id98whichcanlearnwhole-objectandpartdetectors.theyuseselectivesearch[185]togeneratethepartproposals,andapplynon-parametricgeometricconstraintstomoreaccuratelylocalizeparts.linetal.[186]incorporatepartlocalization,alignment,andclassi   cationintoonerecognitionsystemwhichiscalleddeeplac.theirsystemiscomposedofthreesub-networks:localizationsub-networkisusedtoestimatethepartlocation,alignmentsub-networkreceivesthelocationasinputandperformstemplatealignment[187],andclassi   cationsub-networktakesposealignedpartimagesasinputtopredictthecategorylabel.theyalsoproposeavaluelinkagefunctiontolinkthesub-networksandmakethemworkasawholeintrainingandtesting.ascanbenoted,alltheabove-mentionedmethodsmakeuseofpartannotationinformationforsupervisedtraining.however,theseannotationsarenoteasytocollectandthesesystemshavedi   cultyinscalingupandtohandlemanytypesof   ne-grainedclasses.toavoidthisproblem,someresearchersproposeto   ndlocalizedpartsorregionsinanunsupervisedmanner.krauseetal.[188]usetheensembleoflocalizedlearnedfeaturerepresentationsfor   ne-grainedclassi   cation,theyuseco-segmentationandalignmentto20generateparts,andthencomparetheappearanceofeachpartandaggregatethesimilaritiestogether.intheirlatestpaper[189],theycombineco-segmentationandalignmentinadiscriminativemixturetogeneratepartsforfacilitating   ne-grainedclassi   cation.zhangetal.[190]usetheunsupervisedselectivesearchtogenerateobjectproposals,andthenselecttheusefulpartsfromthemulti-scalegeneratedpartproposals.xiaoetal.[191]applyvisualattentioninid98for   ne-grainedclassi   cation.theirclassi   cationpipelineiscomposedofthreetypesofattentions:thebottom-upattentionproposescandidatepatches,theobject-leveltop-downattentionselectsrelevantpatchesofacertainobject,andthepart-leveltop-downattentionlocalizesdiscriminativeparts.theseattentionsarecombinedtotraindomain-speci   cnetworkswhichcanhelpto   ndforegroundobjectorobjectpartsandextractdiscriminativefeatures.linetal.[192]proposeabilinearmodelfor   ne-grainedimageclassi   cation.therecognitionarchitectureconsistsoftwofeatureextractors.theoutputsoftwofeatureextractorsaremultipliedusingtheouterproductateachlocationoftheimage,andarepooledtoobtainanimagedescriptor.5.2.objectdetectionobjectdetectionhasbeenalong-standingandimportantproblemincomputervision[193   195].gener-ally,thedi   cultiesmainlylieinhowtoaccuratelyande   cientlylocalizeobjectsinimagesorvideoframes.theuseofid98sfordetectionandlocalizationcanbetracedbackto1990s[196].however,duetothelackoftrainingdataandlimitedprocessingresources,theprogressofid98-basedobjectdetectionisslowbefore2012.since2012,thehugesuccessofid98sinid163challenge[8]rekindlesinterestinid98-basedobjectdetection[197].insomeearlyworks[196,198],theyusetheslidingwindowbasedapproachestodenselyevaluatetheid98classi   eronwindowssampledateachlocationandscale.sincethereareusuallyhundredsofthousandsofcandidatewindowsinaimage,thesemethodssu   erfromhighlycomputationalcost,whichmakesthemunsuitabletobeappliedonthelarge-scaledataset,e.g.,pascalvoc[172],id163[8]andmscoco[199].recently,objectproposalbasedmethodsattractalotofinterestsandarewidelystudiedintheliter-ature[185,193,200,201].thesemethodsusuallyexploitfastandgenericmeasurementstotestwhetherasampledwindowisapotentialobjectornot,andfurtherpasstheoutputobjectproposalstomoreso-phisticateddetectorstodeterminewhethertheyarebackgroundorbelongtoaspeci   cobjectclass.oneofthemostfamousobjectproposalbasedid98detectorisregion-basedid98(r-id98)[202].r-id98usesselectivesearch(ss)[185]toextractaround2000bottom-upregionproposalsthatarelikelytocontainobjects.then,theseregionproposalsarewarpedtoa   xedsize(227  227),andapre-trainedid98isusedtoextractfeaturesfromthem.finally,abinaryid166classi   erisusedfordetection.r-id98yieldsasigni   cantperformanceboost.however,itscomputationalcostisstillhighsincethetime-consumingid98featureextractorwillbeperformedforeachregionseparately.todealwiththisproblem,somerecentworksproposetosharethecomputationinfeatureextraction[9,28,132,202].over-feat[132]computesid98featuresfromanimagepyramidforlocalizationanddetection.hencethecompu-tationcanbeeasilysharedbetweenoverlappingwindows.spatialpyramidpoolingnetwork(sppnet)[203]isapyramid-basedversionofr-id98[202],whichintroducesanspplayertorelaxtheconstraintthatinputimagesmusthavea   xedsize.unliker-id98,sppnetextractsthefeaturemapsfromtheentireimageonlyonce,andthenappliesspatialpyramidpoolingoneachcandidatewindowtogeta   xed-lengthrepresentation.adrawbackofsppnetisthatitstrainingprocedureisamulti-stagepipeline,whichmakesitimpossibletotraintheid98featureextractorandid166classi   erjointlytofurtherimprovetheaccu-racy.fastrid98[204]improvessppnetbyusinganend-to-endtrainingmethod.allnetworklayerscanbeupdatedduring   ne-tuning,whichsimpli   esthelearningprocessandimprovesdetectionaccuracy.later,fasterr-id98[204]introducesaregionproposalnetwork(rpn)forobjectproposalsgenerationandachievesfurtherspeed-up.besider-id98basedmethods,gidarisetal.[205]proposeamulti-regionandsemanticsegmentation-awaremodelforobjectdetection.theyintegratethecombinedfeaturesonaniterativelocalizationmechanismaswellasabox-votingschemeafternon-maxsuppression.yooetal.[206]treattheobjectdetectionproblemasaniterativeclassi   cationproblem.itpredictsanaccurateobjectboundaryboxbyaggregatingquantizedweakdirectionsfromtheirdetectionnetwork.anotherimportantissueofobjectdetectionishowtoexploree   ectivetrainingsetsastheperformanceissomehowlargelydependsonquantityandqualityofbothpositiveandnegativesamples.onlinebootstrap-21ping(orhardnegativemining[207])forid98traininghasrecentlygainedinterestduetoitsimportanceforintelligentcognitivesystemsinteractingwithdynamicallychangingenvironments[208].[209]proposesanovelid64techniquecalledonlinehardexamplemining(ohem)fortrainingdetectionmodelsbasedonid98s.itsimpli   esthetrainingprocessbyautomaticallyselectingthehardexamples.mean-while,itonlycomputesthefeaturemapsofanimageonce,andthenforwardsallregion-of-interests(rois)oftheimageontopofthesefeaturemaps.thusitisableto   ndthehardexampleswithasmallextracomputationalcost.morerecently,yolo[210]andssd[211]allowsinglepipelinedetectionthatdirectlypredictsclasslabels.yolo[210]treatsobjectdetectionasaregressionproblemtospatiallyseparatedboundingboxesandassociatedclassprobabilities.thewholedetectionpipelineisasinglenetworkwhichpredictsboundingboxesandclassprobabilitiesfromthefullimageinoneevaluation,andcanbeoptimizedend-to-enddirectlyondetectionperformance.ssd[211]discretizestheoutputspaceofboundingboxesintoasetofdefaultboxesoverdi   erentaspectratiosandscalesperfeaturemaplocation.withthismultiplescalessettingandtheirmatchingstrategy,ssdissigni   cantlymoreaccuratethanyolo.withthebene   tsfromsuper-resolution,luetal.[212]proposeatop-downsearchstrategytodivideawindowintosub-windowsrecursively,inwhichanadditionalnetworkistrainedtoaccountforsuchdivisiondecisions.5.3.objecttrackingthesuccessinobjecttrackingreliesheavilyonhowrobusttherepresentationoftargetappearanceisagainstseveralchallengessuchasviewpointchanges,illuminationchanges,andocclusions[213   215].thereareseveralattemptstoemployid98sforvisualtracking.fanetal.[216]useid98asabaselearner.itlearnsaseparateclass-speci   cnetworktotrackobjects.in[216],theauthorsdesignaid98trackerwithashift-variantarchitecture.suchanarchitectureplaysakeyrolesothatitturnstheid98modelfromadetectorintoatracker.thefeaturesarelearnedduringo   inetraining.di   erentfromtraditionaltrackerswhichonlyextractlocalspatialstructures,thisid98basedtrackingmethodextractsbothspatialandtemporalstructuresbyconsideringtheimagesoftwoconsecutiveframes.becausethelargesignalsinthetemporalinformationtendtooccurnearobjectsthataremoving,thetemporalstructuresprovideacrudevelocitysignaltotracking.lietal.[217]proposeatarget-speci   cid98forobjecttracking,wheretheid98istrainedincrementallyduringtrackingwithnewexamplesobtainedonline.theyemployacandidatepoolofmultipleid98sasadata-drivenmodelofdi   erentinstancesofthetargetobject.individually,eachid98maintainsaspeci   csetofkernelsthatfavourablydiscriminateobjectpatchesfromtheirsurroundingbackgroundusingallavailablelow-levelcues.thesekernelsareupdatedinanonlinemannerateachframeafterbeingtrainedwithjustoneinstanceattheinitializationofthecorrespondingid98.insteadoflearningonecomplicatedandpowerfulid98modelforalltheappearanceobservationsinthepast,lietal.[217]usearelativelysmallnumberof   ltersintheid98withinaframeworkequippedwithatemporaladaptationmechanism.givenaframe,themostpromisingid98sinthepoolareselectedtoevaluatethehypothesisesforthetargetobject.thehypothesiswiththehighestscoreisassignedasthecurrentdetectionwindowandtheselectedmodelsareretrainedusingawarm-startid26whichoptimizesastructurallossfunction.in[218],aid98objecttrackingmethodisproposedtoaddresslimitationsofhandcraftedfeaturesandshallowclassi   erstructuresinobjecttrackingproblem.thediscriminativefeaturesare   rstautomaticallylearnedviaaid98.toalleviatethetrackerdriftingproblemcausedbymodelupdate,thetrackerexploitsthegroundtruthappearanceinformationoftheobjectlabeledintheinitialframesandtheimageobservationsobtainedonline.aheuristicschemaisusedtojudgewhetherupdatingtheobjectappearancemodelsornot.hongetal.[219]proposeavisualtrackingalgorithmbasedonapre-trainedid98,wherethenetworkistrainedoriginallyforlarge-scaleimageclassi   cationandthelearnedrepresentationistransferredtodescribetarget.ontopofthehiddenlayersintheid98,theyputanadditionallayerofanonlineid166tolearnatargetappearancediscriminativelyagainstbackground.themodellearnedbyid166isusedtocomputeatarget-speci   csaliencymapbyback-projectingtheinformationrelevanttotargettoinputimagespace.andtheyexploitthetarget-speci   csaliencymaptoobtaingenerativetargetappearancemodelsandperformtrackingwithunderstandingofspatialcon   gurationoftarget.225.4.poseestimationsincethebreakthroughindeepstructurelearning,manyrecentworkspaymoreattentiontolearnmultiplelevelsofrepresentationsandabstractionsforhuman-bodyposeestimationtaskwithid98s[220,221].deeppose[222]isthe   rstapplicationofid98stohumanposeestimationproblem.inthiswork,poseestimationisformulatedasaid98-basedregressionproblemtobodyjointcoordinates.acascadeof7-layeredid98sarepresentedtoreasonaboutposeinaholisticmanner.unlikethepreviousworksthatusuallyexplicitlydesigngraphicalmodelandpartdetectors,thedeepposecapturesthefullcontextofeachbodyjointbytakingthewholeimageastheinput.meanwhile,someworksexploitid98tolearnrepresentationoflocalbodyparts.ajrunetal.[223]presentaid98basedend-to-endlearningapproachforfull-bodyhumanposeestimation,inwhichid98partdetectorsandanmarkovrandomfield(mrf)-likespatialmodelarejointlytrained,andpair-wisepotentialsinthegrapharecomputedusingconvolutionalpriors.inaseriesofpapers,tompsonetal.[224]useamulti-resolutionid98tocomputeheat-mapforeachbodypart.di   erentfrom[223],tompsonetal.[224]learnthebodypartpriormodelandimplicitlythestructureofthespatialmodel.speci   cally,theystartbyconnectingeverybodyparttoitselfandtoeveryotherbodypartinapair-wisefashion,anduseafully-connectedgraphtomodelthespatialprior.asanextensionof[224],tompsonetal.[92]proposeaid98architecturewhichincludesapositionre   nementmodelafteraroughposeestimationid98.thisre   nementmodel,whichisasiamesenetwork[64],isjointlytrainedincascadewiththeo   -the-shelfmodel[224].inasimilarworkwith[224],chenetal.[225,226]alsocombinegraphicalmodelwithid98.theyexploitaid98tolearnconditionalprobabilitiesforthepresenceofpartsandtheirspatialrelationships,whichareusedinunaryandpairwisetermsofthegraphicalmodel.thelearnedconditionalprobabilitiescanberegardedaslow-dimensionalrepresentationsofthebodypose.thereisalsoaposeestimationmethodcalleddual-sourceid98[227]thatintegratesgraphicalmodelsandholisticstyle.ittakesthefullbodyimageandtheholisticviewofthelocalpartsasinputstocombinebothlocalandcontextualinformation.inadditiontostillimageposeestimationwithid98,recentlyresearchersalsoapplyid98tohumanposeestimationinvideos.basedonthework[224],jainetal.[228]alsoincorporatergbfeaturesandmotionfeaturestoamulti-resolutionid98architecturetofurtherimproveaccuracy.speci   cally,theid98worksinasliding-windowmannertoperformposeestimation.theinputoftheid98isa3dtensorwhichconsistsofanrgbimageanditscorrespondingmotionfeatures,andtheoutputisa3dtensorcontainingresponse-mapsofthejoints.ineachresponsemap,thevalueofeachlocationdenotetheenergyforpresencethecorrespondingjointatthatpixellocation.themulti-resolutionprocessingisachievedbysimplydownsamplingtheinputsandfeedingthemtothenetwork.5.5.textdetectionandrecognitionthetaskofrecognizingtextinimagehasbeenwidelystudiedforalongtime[229   232].traditionally,opticalcharacterrecognition(ocr)isthemajorfocus.ocrtechniquesmainlyperformtextrecognitiononimagesinratherconstrainedvisualenvironments(e.g.,cleanbackground,well-alignedtext).recently,thefocushasbeenshiftedtotextrecognitiononsceneimagesduetothegrowingtrendofhigh-levelvisualunderstandingincomputervisionresearch[233,234].thesceneimagesarecapturedinunconstrainedenvi-ronmentswherethereexistsalargeamountofappearancevariationswhichposesgreatdi   cultiestoexistingocrtechniques.suchaconcerncanbemitigatedbyusingstrongerandricherfeaturerepresentationssuchasthoselearnedbyid98models.alongthelineofimprovingtheperformanceofscenetextrecognitionwithid98,afewworkshavebeenproposed.theworkscanbecoarselycategorizedintothreetypes:(1)textdetectionandlocalizationwithoutrecognition,(2)textrecognitiononcroppedtextimages,and(3)end-to-endtextspottingthatintegratesbothtextdetectionandrecognition:5.5.1.textdetectiononeofthepioneeringworkstoapplyid98forscenetextdetectionis[235].theid98modelemployedby[235]learnsoncroppedtextpatchesandnon-textscenepatchestodiscriminatebetweenthetwo.thetextarethendetectedontheresponsemapsgeneratedbytheid98   ltersgiventhemultiscaleimagepyramidoftheinput.toreducethesearchspacefortextdetection,xuetal.[236]proposetoobtainasetofcharacter23candidatesviamaximallystableextremalregions(mser)and   lterthecandidatesbyid98classi   cation.anotherworkthatcombinesmserandid98fortextdetectionis[237].in[237],id98isusedtodistinguishtext-likemsercomponentsfromnon-textcomponents,andclutteredtextcomponentsaresplitbyapplyingid98inaslidingwindowmannerfollowedbynon-maximalsuppression(nms).otherthanlocalizationoftext,thereisaninterestingwork[238]thatmakesuseofid98todeterminewhethertheinputimagecontainstext,withouttellingwherethetextisexactlylocated.in[238],textcandidatesareobtainedusingmserwhicharethenpassedintoaid98togeneratevisualfeatures,andlastlytheglobalfeaturesoftheimagesareconstructedbyaggregatingtheid98featuresinabag-of-words(bow)framework.5.5.2.textrecognitiongoodfellowetal.[239]proposeaid98modelwithmultiplesoftmaxclassi   ersinits   nallayer,whichisformulatedinsuchawaythateachclassi   erisresponsibleforcharacterpredictionateachsequentiallocationinthemulti-digitinputimage.asanattempttorecognizetextwithoutusinglexiconanddictionary,jaderbergetal.[240]introduceanovelconditionalrandomfields(crf)-likeid98modeltojointlylearncharactersequencepredictionandbigramgenerationforscenetextrecognition.themorerecenttextrecognitionmethodssupplementconventionalid98modelswithvariantsofrecurrentneuralnetworks(id56)tobettermodelthesequencedependenciesbetweencharactersintext.in[241],id98extractsrichvisualfeaturesfromcharacter-levelimagepatchesobtainedviaslidingwindow,andthesequencelabellingiscarriedoutbylstm[242].themethodpresentedin[243]isverysimilarto[241],exceptthatin[243],lexiconcanbetakenintoconsiderationtoenhancetextrecognitionperformance.5.5.3.end-to-endtextspottingforend-to-endtextspotting,wangetal.[15]applyaid98modeloriginallytrainedforcharacterclassi   cationtoperformtextdetection.goinginasimilardirectionas[15],theid98modelproposedin[244]enablesfeaturesharingacrossthefourdi   erentsubtasksofanend-to-endtextspottingsystem:textdetection,charactercase-sensitiveandinsensitiveclassi   cation,andbigramclassi   cation.jaderbergetal.[245]makeuseofid98sinaverycomprehensivewaytoperformend-to-endtextspotting.in[245],themajorsubtasksofitsproposedsystem,namelytextboundingbox   ltering,textboundingboxregression,andtextrecognitionareeachtackledbyaseparateid98model.5.6.visualsaliencydetectionthetechniquetolocateimportantregionsinimageryisreferredtoasvisualsaliencyprediction.itisachallengingresearchtopic,withavastnumberofcomputervisionandimageprocessingapplicationsfacilitatedbyit.recently,acoupleofworkshavebeenproposedtoharnessthestrongvisualmodelingcapabilityofid98sforvisualsaliencyprediction.multi-contextualinformationisacrucialpriorinvisualsaliencyprediction,andithasbeenusedcon-currentlywithid98inmostoftheconsideredworks[246   250].wangetal.[246]introduceanovelsaliencydetectionalgorithmwhichsequentiallyexploitslocalcontextandglobalcontext.thelocalcontextishan-dledbyaid98modelwhichassignsalocalsaliencyvaluetoeachpixelgiventheinputoflocalimagepatches,whiletheglobalcontext(object-levelinformation)ishandledbyadeepfully-connectedfeedforwardnetwork.in[247],theid98parametersaresharedbetweentheglobal-contextandlocal-contextmodels,forpredictingthesaliencyofsuperpixelsfoundwithinobjectproposals.theid98modeladoptedin[248]ispre-trainedonlarge-scaleimageclassi   cationdatasetandthensharedamongdi   erentcontextuallevelsforfeatureex-traction.theoutputsoftheid98atdi   erentcontextuallevelsarethenconcatenatedasinputtobepassedintoatrainablefully-connectedfeedforwardnetworkforsaliencyprediction.similarto[247,248],theid98modelusedin[249]forsaliencypredictionaresharedacrossthreeid98streams,witheachstreamtakinginputofadi   erentcontextualscale.heetal.[250]deriveaspatialkernelandarangekerneltoproducetwomeaningfulsequencesas1-did98inputs,todescribecoloruniquenessandcolordistributionrespectively.theproposedsequencesareadvantageousoverinputsofrawimagepixelsbecausetheycanreducethetrainingcomplexityofid98,whilebeingabletoencodethecontextualinformationamongsuperpixels.therearealsoid98-basedsaliencypredictionapproaches[251   253]thatdonotconsidermulti-contextualinformation.instead,theyrelyverymuchonthepowerfulrepresentationcapabilityofid98.in[251],an24ensembleofid98sisderivedfromalargenumberofrandomlyinstantiatedid98models,togenerategoodfeaturesforsaliencydetection.theid98modelsinstantiatedin[251]arehowevernotdeepenoughbecausethemaximumnumberoflayersiscappedatthree.byusingapre-trainedanddeeperid98modelwith5convolutionallayers,[252](deepgaze)learnsaseparatesaliencymodeltojointlycombinetheresponsesfromeveryid98layerandpredictsaliencyvalues.[253]istheonlyworkmakinguseofid98toperformvisualsaliencypredictioninanend-to-endmanner,whichmeanstheid98modelacceptsrawpixelsasinputandproducessaliencymapasoutput.panetal.[253]arguethatthesuccessoftheproposedend-to-endmethodisattributedtoitsnot-so-deepid98architecturewhichattemptstopreventover   tting.5.7.actionrecognitionactionrecognition,thebehaviouranalysisofhumansubjectsandclassifyingtheiractivitiesbasedontheirvisualappearanceandmotiondynamics,isoneofthechallengingproblemsincomputervision[254   256].generally,thisproblemcanbedividedtotwomajorgroups:actionanalysisinstillimagesandinvideos.forbothofthesetwogroups,e   ectiveid98basedmethodshavebeenproposed.inthissubsectionwebrie   yintroducethelatestadvancesonthesetwogroups.5.7.1.actionrecognitioninstillimagestheworkof[257]hasshowntheoutputoflastfewlayersofatrainedid98canbeusedasageneralvisualfeaturedescriptorforavarietyoftasks.thesameintuitionisutilizedforactionrecognitionby[9,258],inwhichtheyusetheoutputsofthepenultimatelayerofapre-trainedid98torepresentfullimagesofactionsaswellasthehumanboundingboxesinsidethem,andachieveahighlevelofperformanceinactionclassi   cation.gkioxarietal.[259]addapartdetectiontothisframework.theirpartdetectorisaid98basedextensiontotheoriginalposelet[260]method.id98basedrepresentationofcontextualinformationisutilizedforactionrecognitionin[261].theysearchforthemostrepresentativesecondaryregionwithinalargenumberofobjectproposalregionsintheimageandaddcontextualfeaturestothedescriptionoftheprimaryregion(groundtruthboundingboxofhumansubject)inabottom-upmanner.theyutilizeaid98torepresentand   ne-tunetherepresentationsoftheprimaryandthecontextualregions.afterthat,theymoveastepforwardandshowthatitispossibletolocateandrecognizehumanactionsinimageswithoutusinghumanboundingboxes[262].however,theyneedtotrainhumandetectorstoguidetheirrecognitionattesttime.in[263],theyproposeamethodthatsegmentsouttheactionmaskofunderlyinghuman-objectinteractionswithminimumannotatione   orts.5.7.2.actionrecognitioninvideosequencesapplyingid98sonvideosischallengingbecausetraditionalid98saredesignedtorepresenttwodimen-sionalpurespatialsignalsbutinvideosanewtemporalaxisisaddedwhichisessentiallydi   erentfromthespatialvariationsinimages[256,264].thesizesofthevideosignalsarealsoinhigherordersincomparisontothoseofimageswhichmakesitmoredi   culttoapplyconvolutionalnetworkson.jietal.[265]proposetoconsiderthetemporalaxisinasimilarmannerasotherspatialaxesandintroduceanetworkof3dcon-volutionallayerstobeappliedonvideoinputs.recentlytranetal.[266]studytheperformance,e   ciency,ande   ectivenessofthisapproachandshowitsstrengthscomparedtootherapproaches.anotherapproachtoapplyid98sonvideosistokeeptheconvolutionsin2dandfusethefeaturemapsofconsecutiveframes,asproposedby[267].theyevaluatethreedi   erentfusionpolicies:latefusion,earlyfusion,andslowfusion,andcomparethemwithapplyingtheid98onindividualsingleframes.onemorestepforwardforbetteractionrecognitionviaid98sistoseparatetherepresentationtospatialandtemporalvariationsandtrainindividualid98sforeachofthem,asproposedbysimonyanandzisserman[268].firststreamofthisframeworkisatraditionalid98appliedonalltheframesandthesecondreceivesthedenseoptical   owoftheinputvideosandtrainsanotherid98whichisidenticaltothespatialstreaminsizeandstructure.theoutputofthetwostreamsarecombinedinaclassscorefusionstep.ch  eronetal.[269]utilizethetwostreamid98onthelocalizedpartsofthehumanbodyandshowtheaggregationofpart-basedlocalid98descriptorscane   ectivelyimprovetheperformanceofactionrecognition.anotherapproachtomodelthedynamicsofvideosdi   erentlyfromspatialvariations,istofeedtheid98basedfeaturesofindividual25framestoasequencelearningmodulee.g.,arecurrentneuralnetwork.donahueetal.[270]studydi   erentcon   gurationsofapplyinglstmunitsasthesequencelearnerinthisframework.5.8.scenelabelingscenelabelingaimstorelateonesemanticclass(road,water,seaetc.)toeachpixeloftheinputim-age[271   275].id98sareusedtomodeltheclasslikelihoodofpixelsdirectlyfromlocalimagepatches.theyareabletolearnstrongfeaturesandclassi   erstodiscriminatethelocalvisualsubtleties.farabetetal.[276]havepioneeredtoapplyid98stoscenelabelingtasks.theyfeedtheirmulti-scaleconvnetwithdi   erentscaleimagepatches,andtheyshowthatthelearnednetworkisabletoperformmuchbetterthansystemswithhand-craftedfeatures.besides,thisnetworkisalsosuccessfullyappliedtorgb-dscenelabeling[277].toenabletheid98stohavealarge   eldofviewoverpixels,pinheiroetal.[278]developtherecurrentid98s.morespeci   cally,theidenticalid98sareappliedrecurrentlytotheoutputmapsofid98sinthepreviousiterations.bydoingthis,theycanachieveslightlybetterlabelingresultswhilesigni   cantlyreducestheid136times.shuaietal.[279   281]traintheparametricid98sbysamplingimagepatches,whichspeedsupthetrainingtimedramatically.they   ndthatpatch-basedid98ssu   erfromlocalambiguityproblems,and[279]solveitbyintegratingglobalbeliefs.[280]and[281]usetherecurrentneuralnetworkstomodelthecontextualdependenciesamongimagefeaturesfromid98s,anddramaticallyboostthelabelingperformance.meanwhile,researchersareexploitingtousethepre-traineddeepid98sforobjectsemanticsegmentation.mostajabietal.[282]applythelocalandproximalfeaturesfromaconvnetandapplythealex-net[8]toobtainthedistantandglobalfeatures,andtheirconcatenationgivesrisetothezoom-outfeatures.theyachieveverycompetitiveresultsonthesemanticsegmentationtasks.longetal.[28]trainafullyconvolutionalnetworktodirectlypredicttheinputimagestodenselabelmaps.theconvolutionlayersofthefcnsareinitializedfromthemodelpre-trainedonid163classi   cationdataset,andthedeconvolutionlayersarelearnedtoupsampletheresolutionoflabelmaps.chenetal.[283]alsoapplythepre-traineddeepid98stoemitthelabelsofpixels.consideringthattheimperfectnessofboundaryalignment,theyfurtherusefullyconnectedcrftoboostthelabelingperformance.5.9.speechprocessing5.9.1.automaticspeechrecognitionautomaticspeechrecognition(asr)isthetechnologythatconvertshumanspeechintospokenwords[284].beforeapplyingid98toasr,thisdomainhaslongbeendominatedbythehiddenmarkovmodelandgaus-sianmixturemodel(gmm-id48)methods[285],whichusuallyrequireextractinghand-craftfeaturesonspeechsignals,e.g.,themostpopularmelfrequencycepstralcoe   cients(mfcc)features.meanwhile,someresearchershaveapplieddeepneuralnetworks(dnns)inlargevocabularycontinuousspeechrecog-nition(lvcsr)andobtainedencouragingresults[286,287],however,theirnetworksaresusceptibletoperformancedegradationsundermismatchedcondition[288],suchasdi   erentrecordingconditionsetc.id98shaveshownbetterperformanceovergmm-id48sandgeneraldnns[289,290],sincetheyarewellsuitedtoexploitthecorrelationsinbothtimeandfrequencydomainsthroughthelocalconnectivityandarecapableofcapturingfrequencyshiftinhumanspeechsignals.in[289],theyachievelowerspeechrecognitionerrorsbyapplyingid98onmel   lterbankfeatures.someattemptsusetherawwaveformwithid98s,andtolearn   lterstoprocesstherawwaveformjointlywiththerestofthenetwork[291,292].mostoftheearlyapplicationsofid98inasronlyusefewerconvolutionlayers.forexample,abdel-hamidetal.[290]useoneconvolutionallayerintheirnetwork,andamodeietal.[293]usethreeconvolutionallayersasthefeaturepreprocessinglayer.recently,verydeepid98shaveshownimpressiveperformanceinasr[294,295].besides,small   ltershavesuccessfullyappliedinacousticmodelinginhybridnn-id48asrsystem,andpoolingoperationsarereplacedbydenselyconnectedlayersforasrtasks[296].yuetal.[297]proposealayer-wisecontextexpansionwithattentionmodelforasr.itisavariantoftime-delayneuralnetwork[298]inwhichlowerlayersfocusonextractingsimplelocalpatternswhilehigherlayersexploitbroadercontextandextractcomplexpatternsthanthelowerlayers.asimilarideacanbefoundin[40].265.9.2.statisticalparametricspeechsynthesisinadditiontospeechrecognition,theimpactofid98shasalsospreadtostatisticalparametricspeechsynthesis(spss).thegoalofspeechsynthesisistogeneratespeechsoundsdirectlyfromthetextandpossiblywithadditionalinformation.ithasbeenknownformanyyearsthatthespeechsoundsgeneratedbyshallowstructuredid48networksareoftenmu   edcomparedwithnaturalspeech.manystudieshaveadopteddeeplearningtoovercomesuchde   ciency[299   301].oneadvantageofthesemethodsistheirstrongabilitytorepresenttheintrinsiccorrelationbyusingagenerativemodelingframework.inspiredbytherecentadvancesinneuralautoregressivegenerativemodelsthatmodelcomplexdistributionssuchasimages[302]andtext[303],wavenet[39]makesuseofthegenerativemodeloftheid98torepresenttheconditionaldistributionoftheacousticfeaturesgiventhelinguisticfeatures,whichcanbeseenasamilestoneinspss.inordertodealwithlong-rangetemporaldependencies,theydevelopanewarchitecturebasedondilatedcausalconvolutionstocaptureverylargereceptive   elds.byconditioninglinguisticfeaturesontext,itcanbeusedtodirectlysynthesizespeechfromtext.5.10.naturallanguageprocessing5.10.1.statisticallanguagemodelingforstatisticallanguagemodeling,theinputtypicallyconsistsofincompletesequencesofwordsratherthancompletesentences[304,305].kimetal.[304]usetheoutputofcharacter-levelid98astheinputtoanlstmateachtimestep.genid98[306]isaconvolutionalarchitectureforsequenceprediction,whichusesseparategatingnetworkstoreplacethemax-poolingoperations.recently,kalchbrenneretal.[38]proposeaid98-basedarchitectureforsequenceprocessingcalledbytenet,whichisastackoftwodilatedid98s.likewavenet[39],bytenetalsobene   tsfromconvolutionswithdilationstoincreasethereceptive   eldsize,thuscanmodelsequentialdatawithlong-termdependencies.italsohastheadvantagethatthecomputationaltimeonlylinearlydependsonthelengthofthesequences.comparedwithrecurrentneuralnetworks,id98snotonlycangetlong-rangeinformationbutalsogetahierarchicalrepresentationoftheinputwords.guetal.[307]andyannetal.[308]shareasimilarideathatbothofthemuseid98withoutpoolingtomodeltheinputwords.guetal.[307]combinethelanguageid98withrecurrenthighwaynetworksandachieveahugeimprovementcomparedtolstm-basedmethods.inspiredbythegatingmechanisminlstmnetworks,thegatedid98in[308]usesagatingmechanismtocontrolthepaththroughwhichinformation   owsinthenetwork,andachievesthestate-of-the-artonwikitext-103.however,theframeworksin[308]and[307]arestillundertherecurrentframework,andtheinputwindowsizeoftheirnetworkareoflimitedsize.howtocapturethespeci   clong-termdependenciesaswellashierarchicalrepresentationofhistorywordsisstillanopenproblem.5.10.2.textclassi   cationtextclassi   cationisacrucialtaskfornaturallanguageprocessing(nlp).naturallanguagesentenceshavecomplicatedstructures,bothsequentialandhierarchical,thatareessentialforunderstandingthem.owingtothepowerfulcapabilityofcapturinglocalrelationsoftemporalorhierarchicalstructures,id98shaveachievedtopperformanceinsentencemodeling.aproperid98architectureisimportantfortextclassi   cation.collobertetal.[309]andyuetal.[310]applyoneconvolutionallayertomodelthesentence,whilekalchbrenneretal.[311]stackmultiplelayersofconvolutiontomodelsentences.in[312],theyusemultichannelconvolutionandvariablekernelsforsentenceclassi   cation.itisshownthatmultipleconvolutionallayershelptoextracthigh-levelabstractfeatures,andmultiplelinear   lterscane   ectivelyconsiderdi   erentid165features.recently,yinetal.[313]extendthenetworkin[312]byhierarchicalconvolutionarchitectureandfurtherexplorationofmultichannelandvariablesizefeaturedetectors.thepoolingoperationcanhelpthenetworkdealwithvariablesentencelengths.in[312,314],theyusemax-poolingtokeepthemostimportantinformationtorepresentthesentence.however,max-poolingcannotdistinguishwhetherarelevantfeatureinoneoftherowsoccursjustoneormultipletimesanditignorestheorderinwhichthefeaturesoccur.in[311],theyproposethek-maxpoolingwhichreturnsthetopkactivationsintheoriginalorderintheinputsequence.dynamick-maxpoolingisageneralizationofthek-maxpoolingoperatorwherethekvalueisdependedontheinputfeaturemapsize.theid9827architecturesmentionedabovearerathershallowcomparedwiththedeepid98swhichareverysuccessfulincomputervision.recently,conneauetal.[315]implementadeepconvolutionalarchitecturewhichisupto29convolutionallayers.they   ndthatshortcutconnectionsgivebetterresultswhenthenetworkisverydeep(49layers).however,theydonotachievestate-of-the-artunderthissetting.6.conclusionsandoutlookdeepid98shavemadebreakthroughsinprocessingimage,video,speechandtext.inthispaper,wehavegivenanextensivesurveyonrecentadvancesofid98s.wehavediscussedtheimprovementsofid98ondi   erentaspects,namely,layerdesign,activationfunction,lossfunction,id173,optimizationandfastcomputation.beyondsurveyingtheadvancesofeachaspectofid98,wehavealsointroducedtheapplicationofid98onmanytasks,includingimageclassi   cation,objectdetection,objecttracking,poseestimation,textdetection,visualsaliencydetection,actionrecognition,scenelabeling,speechandnaturallanguageprocessing.althoughid98shaveachievedgreatsuccessinexperimentalevaluations,therearestilllotsofissuesthatdeservefurtherinvestigation.firstly,sincetherecentid98sarebecomingdeeperanddeeper,theyrequirelarge-scaledatasetandmassivecomputingpowerfortraining.manuallycollectinglabeleddatasetrequireshugeamountsofhumane   orts.thus,itisdesiredtoexploreunsupervisedlearningofid98s.meanwhile,tospeeduptrainingprocedure,althoughtherearealreadysomeasynchronoussgdalgorithmswhichhaveshownpromisingresultbyusingcpuandgpuclusters,itisstillworthtodevelope   ectiveandscalableparalleltrainingalgorithms.attestingtime,thesedeepmodelsarehighlymemorydemandingandtime-consuming,whichmakesthemnotsuitabletobedeployedonmobileplatformsthathavelimitedresources.itisimportanttoinvestigatehowtoreducethecomplexityandobtainfast-to-executemodelswithoutlossofaccuracy.furthermore,onemajorbarrierforapplyingid98onanewtaskisthatitrequiresconsiderableskillandexperiencetoselectsuitablehyperparameterssuchasthelearningrate,kernelsizesofconvolutional   lters,thenumberoflayersetc.thesehyper-parametershaveinternaldependencieswhichmakethemparticularlyexpensivefortuning.recentworkshaveshownthatthereexistsabigroomtoimprovecurrentoptimizationtechniquesforlearningdeepid98architectures[12,42,316].finally,thesolidtheoryofid98sisstilllacking.currentid98modelworksverywellforvariousapplications.however,wedonotevenknowwhyandhowitworksessentially.itisdesirabletomakemoree   ortsoninvestigatingthefundamentalprinciplesofid98s.meanwhile,itisalsoworthexploringhowtoleveragenaturalvisualperceptionmechanismtofurtherimprovethedesignofid98.wehopethatthispapernotonlyprovidesabetterunderstandingofid98sbutalsofacilitatesfutureresearchactivitiesandapplicationdevelopmentsinthe   eldofid98s.acknowledgmentthisresearchwascarriedoutattherapid-richobjectsearch(rose)labatthenanyangtechnolog-icaluniversity,singapore.theroselabissupportedbytheinfocommmediadevelopmentauthority,singapore.references[1]d.h.hubel,t.n.wiesel,receptive   eldsandfunctionalarchitectureofmonkeystriatecortex,thejournalofphysiology(1968)215   243.[2]k.fukushima,s.miyake,neocognitron:aself-organizingneuralnetworkmodelforamechanismofvisualpatternrecognition,in:competitionandcooperationinneuralnets,1982,pp.267   285.[3]b.b.lecun,j.s.denker,d.henderson,r.e.howard,w.hubbard,l.d.jackel,handwrittendigitrecognitionwithaback-propagationnetwork,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),1989,pp.396   404.[4]y.lecun,l.bottou,y.bengio,p.ha   ner,gradient-basedlearningappliedtodocumentrecognition,proceedingsofieee86(11)(1998)2278   2324.28[5]r.hecht-nielsen,theoryoftheid26neuralnetwork,neuralnetworks1(supplement-1)(1988)445   448.[6]w.zhang,k.itoh,j.tanida,y.ichioka,paralleldistributedprocessingmodelwithlocalspace-invariantinterconnectionsanditsopticalarchitecture,appliedoptics29(32)(1990)4790   4797.[7]x.-x.niu,c.y.suen,anovelhybridid98   id166classi   erforrecognizinghandwrittendigits,patternrecognition45(4)(2012)1318   1325.[8]o.russakovsky,j.deng,h.su,j.krause,s.satheesh,s.ma,z.huang,a.karpathy,a.khosla,m.bernstein,etal.,id163largescalevisualrecognitionchallenge,internationaljournalofcon   ictandviolence(ijcv)115(3)(2015)211   252.[9]k.simonyan,a.zisserman,verydeepconvolutionalnetworksforlarge-scaleimagerecognition,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2015.[10]c.szegedy,w.liu,y.jia,p.sermanet,s.reed,d.anguelov,d.erhan,v.vanhoucke,a.rabinovich,goingdeeperwithconvolutions,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.1   9.[11]m.d.zeiler,r.fergus,visualizingandunderstandingconvolutionalnetworks,in:proceedingsoftheeuropeanconfer-enceoncomputervision(eccv),2014,pp.818   833.[12]k.he,x.zhang,s.ren,j.sun,deepresiduallearningforimagerecognition,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2016,pp.770   778.[13]y.a.lecun,l.bottou,g.b.orr,k.-r.m  uller,e   cientbackprop,in:neuralnetworks:tricksofthetrade-secondedition,2012,pp.9   48.[14]v.nair,g.e.hinton,recti   edlinearunitsimproverestrictedboltzmannmachines,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2010,pp.807   814.[15]t.wang,d.j.wu,a.coates,a.y.ng,end-to-endtextrecognitionwithconvolutionalneuralnetworks,in:proceedingsoftheinternationalconferenceonpatternrecognition(icpr),2012,pp.3304   3308.[16]y.boureau,j.ponce,y.lecun,atheoreticalanalysisoffeaturepoolinginvisualrecognition,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2010,pp.111   118.[17]g.e.hinton,n.srivastava,a.krizhevsky,i.sutskever,r.r.salakhutdinov,improvingneuralnetworksbypreventingco-adaptationoffeaturedetectors,corrabs/1207.0580.[18]m.lin,q.chen,s.yan,networkinnetwork,in:proceedingsoftheinternationalconferenceonlearningrepresenta-tions(iclr),2014.[19]y.tang,deeplearningusinglinearsupportvectormachines,in:proceedingsoftheinternationalconferenceonmachinelearning(icml)workshops,2013.[20]g.madjarov,d.kocev,d.gjorgjevikj,s.d  zeroski,anextensiveexperimentalcomparisonofmethodsformulti-labellearning,patternrecognition45(9)(2012)3084   3104.[21]r.g.j.wijnhoven,p.h.n.dewith,fasttrainingofobjectdetectionusingstochasticgradientdescent,in:internationalconferenceonpatternrecognition(icpr),2010,pp.424   427.[22]m.zinkevich,m.weimer,l.li,a.j.smola,parallelizedstochasticgradientdescent,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2010,pp.2595   2603.[23]j.ngiam,z.chen,d.chia,p.w.koh,q.v.le,a.y.ng,tiledconvolutionalneuralnetworks,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2010,pp.1279   1287.[24]z.wang,t.oates,encodingtimeseriesasimagesforvisualinspectionandclassi   cationusingtiledconvolutionalneuralnetworks,in:proceedingsoftheassociationfortheadvancementofarti   cialintelligence(aaai)workshops,2015.[25]y.zheng,q.liu,e.chen,y.ge,j.l.zhao,timeseriesclassi   cationusingmulti-channelsdeepconvolutionalneuralnetworks,in:proceedingsoftheinternationalconferenceonweb-ageinformationmanagement(waim),2014,pp.298   310.[26]m.d.zeiler,d.krishnan,g.w.taylor,r.fergus,deconvolutionalnetworks,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2010,pp.2528   2535.[27]m.d.zeiler,g.w.taylor,r.fergus,adaptivedeconvolutionalnetworksformidandhighlevelfeaturelearning,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),2011,pp.2018   2025.[28]j.long,e.shelhamer,t.darrell,fullyconvolutionalnetworksforsemanticsegmentation,ieeetransactionsonpatternanalysisandmachineintelligence(pami)39(4)(2017)640   651.[29]f.visin,k.kastner,a.courville,y.bengio,m.matteucci,k.cho,reseg:arecurrentneuralnetworkforobjectsegmentation,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr)workshops,2015.[30]h.noh,s.hong,b.han,learningdeconvolutionnetworkforsemanticsegmentation,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),2015,pp.1520   1528.[31]c.cao,x.liu,y.yang,y.yu,j.wang,z.wang,y.huang,l.wang,c.huang,w.xu,etal.,lookandthinktwice:capturingtop-downvisualattentionwithfeedbackconvolutionalneuralnetworks,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),2015,pp.2956   2964.[32]j.zhang,z.lin,j.brandt,x.shen,s.sclaro   ,top-downneuralattentionbyexcitationbackprop,in:proceedingsoftheeuropeanconferenceoncomputervision(eccv),2016,pp.543   559.[33]y.zhang,e.k.lee,e.h.lee,u.edu,augmentingsupervisedneuralnetworkswithunsupervisedobjectivesforlarge-scaleimageclassi   cation,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2016,pp.612   621.[34]b.zhou,a.khosla,a.lapedriza,a.oliva,a.torralba,learningdeepfeaturesfordiscriminativelocalization,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2016,pp.2921   2929.29[35]a.das,h.agrawal,c.l.zitnick,d.parikh,d.batra,humanattentioninvisualquestionanswering:dohumansanddeepnetworkslookatthesameregions?,in:proceedingsoftheconferenceonempiricalmethodsinnaturallanguageprocessing(emnlp),2016,pp.932   937.[36]c.dong,c.c.loy,k.he,x.tang,imagesuper-resolutionusingdeepconvolutionalnetworks,ieeetransactionsonpatternanalysisandmachineintelligence(pami)38(2)(2016)295   307.[37]f.yu,v.koltun,multi-scalecontextaggregationbydilatedconvolutions,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2016.[38]n.kalchbrenner,l.espeholt,k.simonyan,a.v.d.oord,a.graves,k.kavukcuoglu,neuralmachinetranslationinlineartime,corrabs/1610.10099.[39]a.v.d.oord,s.dieleman,h.zen,k.simonyan,o.vinyals,a.graves,n.kalchbrenner,a.senior,k.kavukcuoglu,wavenet:agenerativemodelforrawaudio,corrabs/1609.03499.[40]t.sercu,v.goel,densepredictiononsequenceswithtime-dilatedconvolutionsforspeechrecognition,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips)workshops,2016.[41]c.szegedy,v.vanhoucke,s.io   e,j.shlens,z.wojna,rethinkingtheinceptionarchitectureforcomputervision,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2016,pp.2818   2826.[42]c.szegedy,s.io   e,v.vanhoucke,inception-v4,inception-resnetandtheimpactofresidualconnectionsonlearning,in:proceedingsoftheaaaiconferenceonarti   cialintelligence,2017,pp.4278   4284.[43]a.hyv  arinen,u.k  oster,complexcellpoolingandthestatisticsofnaturalimages,network:computationinneuralsystems18(2)(2007)81   100.[44]j.b.estrach,a.szlam,y.lecun,signalrecoveryfrompoolingrepresentations,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2014,pp.307   315.[45]l.wan,m.zeiler,s.zhang,y.l.cun,r.fergus,id173ofneuralnetworksusingdropconnect,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2013,pp.1058   1066.[46]d.yu,h.wang,p.chen,z.wei,mixedpoolingforconvolutionalneuralnetworks,in:proceedingsoftheroughsetsandknowledgetechnology(rskt),2014,pp.364   375.[47]m.d.zeiler,r.fergus,stochasticpoolingforid173ofdeepconvolutionalneuralnetworks,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2013.[48]o.rippel,j.snoek,r.p.adams,spectralrepresentationsforconvolutionalneuralnetworks,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2015,pp.2449   2457.[49]m.mathieu,m.hena   ,y.lecun,fasttrainingofconvolutionalnetworksthrough   ts,in:proceedingsoftheinterna-tionalconferenceonlearningrepresentations(iclr),2014.[50]k.he,x.zhang,s.ren,j.sun,spatialpyramidpoolingindeepconvolutionalnetworksforvisualrecognition,ieeetransactionsonpatternanalysisandmachineintelligence(pami)37(9)(2015)1904   1916.[51]s.singh,a.gupta,a.efros,unsuperviseddiscoveryofmid-leveldiscriminativepatches,in:proceedingsoftheeuropeanconferenceoncomputervision(eccv),2012,pp.73   86.[52]y.gong,l.wang,r.guo,s.lazebnik,multi-scaleorderlesspoolingofdeepconvolutionalactivationfeatures,in:proceedingsoftheeuropeanconferenceoncomputervision(eccv),2014,pp.392   407.[53]h.j  egou,f.perronnin,m.douze,j.sanchez,p.perez,c.schmid,aggregatinglocalimagedescriptorsintocompactcodes,ieeetransactionsonpatternanalysisandmachineintelligence(pami)34(9)(2012)1704   1716.[54]a.l.maas,a.y.hannun,a.y.ng,recti   ernonlinearitiesimproveneuralnetworkacousticmodels,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),vol.30,2013.[55]m.d.zeiler,m.ranzato,r.monga,m.mao,k.yang,q.v.le,p.nguyen,a.senior,v.vanhoucke,j.dean,etal.,onrecti   edlinearunitsforspeechprocessing,in:proceedingsoftheinternationalconferenceonacoustics,speech,andsignalprocessing(icassp),2013,pp.3517   3521.[56]k.he,x.zhang,s.ren,j.sun,delvingdeepintorecti   ers:surpassinghuman-levelperformanceonid163classi   -cation,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),2015,pp.1026   1034.[57]b.xu,n.wang,t.chen,m.li,empiricalevaluationofrecti   edactivationsinconvolutionalnetwork,in:proceedingsoftheinternationalconferenceonmachinelearning(icml)workshop,2015.[58]d.-a.clevert,t.unterthiner,s.hochreiter,fastandaccuratedeepnetworklearningbyexponentiallinearunits(elus),in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2016.[59]i.j.goodfellow,d.warde-farley,m.mirza,a.courville,y.bengio,maxoutnetworks,in:proceedingsoftheinterna-tionalconferenceonmachinelearning(icml),2013,pp.1319   1327.[60]j.t.springenberg,m.riedmiller,improvingdeepneuralnetworkswithprobabilisticmaxoutunits,corrabs/1312.6116.[61]t.zhang,solvinglargescalelinearpredictionproblemsusingstochasticgradientdescentalgorithms,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2004.[62]l.deng,themnistdatabaseofhandwrittendigitimagesformachinelearningresearch,ieeesignalprocessingmagazine29(6)(2012)141   142.[63]w.liu,y.wen,z.yu,m.yang,large-marginsoftmaxlossforconvolutionalneuralnetworks,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2016,pp.507   516.[64]j.broid113y,j.w.bentz,l.bottou,i.guyon,y.lecun,c.moore,e.s  ackinger,r.shah,signatureveri   cationusingasiamesetimedelayneuralnetwork,internationaljournalofpatternrecognitionandarti   cialintelligence(ijprai)7(4)(1993)669   688.[65]s.chopra,r.hadsell,y.lecun,learningasimilaritymetricdiscriminatively,withapplicationtofaceveri   cation,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2005,pp.539   546.[66]r.hadsell,s.chopra,y.lecun,dimensionalityreductionbylearninganinvariantmapping,in:proceedingsofthe30ieeeconferenceoncomputervisionandpatternrecognition(cvpr),2006,pp.1735   1742.[67]u.shaham,r.r.lederman,learningbycoincidence:siamesenetworksandcommonvariablelearning,patternrecog-nition.[68]j.lin,o.morere,v.chandrasekhar,a.veillard,h.goh,deephash:gettingid173,depthand   ne-tuningright,in:proceedingsoftheinternationalconferenceonmultimediaretrieval(icmr),2017,pp.133   141.[69]f.schro   ,d.kalenichenko,j.philbin,facenet:auni   edembeddingforfacerecognitionandid91,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.815   823.[70]h.liu,y.tian,y.yang,l.pang,t.huang,deeprelativedistancelearning:tellthedi   erencebetweensimilarvehicles,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2016,pp.2167   2175.[71]s.ding,l.lin,g.wang,h.chao,deepfeaturelearningwithrelativedistancecomparisonforpersonre-identi   cation,patternrecognition48(10)(2015)2993   3003.[72]z.liu,p.luo,s.qiu,x.wang,x.tang,deepfashion:poweringrobustclothesrecognitionandretrievalwithrichannotations,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2016,pp.1096   1104.[73]d.p.kingma,m.welling,auto-encodingvariationalbayes,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2014.[74]d.j.im,s.ahn,r.memisevic,y.bengio,denoisingcriterionforvariationalauto-encodingframework,in:proceedingsoftheassociationfortheadvancementofarti   cialintelligence(aaai),2017,pp.2059   2065.[75]d.p.kingma,s.mohamed,d.j.rezende,m.welling,semi-supervisedlearningwithdeepgenerativemodels,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2014,pp.3581   3589.[76]i.goodfellow,j.pouget-abadie,m.mirza,b.xu,d.warde-farley,s.ozair,a.courville,y.bengio,generativeadversarialnets,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2014,pp.2672   2680.[77]m.mirza,s.osindero,conditionalgenerativeadversarialnets,corrabs/1411.1784.[78]p.vincent,h.larochelle,y.bengio,p.-a.manzagol,extractingandcomposingrobustfeatureswithdenoisingautoen-coders,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2008,pp.1096   1103.[79]w.w.ng,g.zeng,j.zhang,d.s.yeung,w.pedrycz,dualautoencodersfeaturesforimbalanceclassi   cationproblem,patternrecognition60(2016)875   889.[80]j.mehta,a.majumdar,rodeo:robustde-aliasingautoencoderforreal-timemedicalimagereconstruction,patternrecognition63(2017)499   510.[81]b.a.olshausen,etal.,emergenceofsimple-cellreceptive   eldpropertiesbylearningasparsecodefornaturalimages,nature381(6583)(1996)607.[82]h.lee,a.battle,r.raina,a.y.ng,e   cientsparsecodingalgorithms,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2006,pp.801   808.[83]s.eslami,n.heess,t.weber,y.tassa,k.kavukcuoglu,g.e.hinton,attend,infer,repeat:fastsceneunderstandingwithgenerativemodels,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2016,pp.3225   3233.[84]k.sohn,h.lee,x.yan,learningstructuredoutputrepresentationusingdeepconditionalgenerativemodels,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2015,pp.3483   3491.[85]s.reed,z.akata,x.yan,l.logeswaran,b.schiele,h.lee,generativeadversarialtexttoimagesynthesis,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2016,pp.1060   1069.[86]e.l.denton,s.chintala,r.fergus,etal.,deepgenerativeimagemodelsusingalaplacianpyramidofadversarialnetworks,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2015,pp.1486   1494.[87]t.salimans,i.goodfellow,w.zaremba,v.cheung,a.radford,x.chen,improvedtechniquesfortraininggans,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2016,pp.2226   2234.[88]a.dosovitskiy,t.brox,generatingimageswithperceptualsimilaritymetricsbasedondeepnetworks,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2016,pp.658   666.[89]a.n.tikhonov,onthestabilityofinverseproblems,in:dokl.akad.nauksssr,vol.39,1943,pp.195   198.[90]s.wang,c.manning,fastdropouttraining,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2013,pp.118   126.[91]j.ba,b.frey,adaptivedropoutfortrainingdeepneuralnetworks,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2013,pp.3084   3092.[92]j.tompson,r.goroshin,a.jain,y.lecun,c.bregler,e   cientobjectlocalizationusingconvolutionalnetworks,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.648   656.[93]h.yang,i.patras,mirror,mirroronthewall,tellme,istheerrorsmall?,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.4685   4693.[94]s.xie,z.tu,holistically-nestededgedetection,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),2015,pp.1395   1403.[95]j.salamon,j.p.bello,deepconvolutionalneuralnetworksanddataaugmentationforenvironmentalsoundclassi   cation,signalprocessingletters(spl)24(3)(2017)279   283.[96]d.eigen,r.fergus,predictingdepth,surfacenormalsandsemanticlabelswithacommonmulti-scaleconvolutionalarchitecture,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),2015,pp.2650   2658.[97]m.paulin,j.revaud,z.harchaoui,f.perronnin,c.schmid,transformationpursuitforimageclassi   cation,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2014,pp.3646   3653.[98]s.hauberg,o.freifeld,a.b.l.larsen,j.w.fisheriii,l.k.hansen,dreamingmoredata:class-dependent31distributionsoverdi   eomorphismsforlearneddataaugmentation,in:proceedingsoftheinternationalconferenceonarti   cialintelligenceandstatistics(aistats),2016,pp.342   350.[99]s.xie,t.yang,x.wang,y.lin,hyper-classaugmentedandregularizeddeeplearningfor   ne-grainedimageclas-si   cation,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.2645   2654.[100]z.xu,s.huang,y.zhang,d.tao,augmentingstrongsupervisionusingwebdatafor   ne-grainedcategorization,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),2015,pp.2524   2532.[101]a.choromanska,m.hena   ,m.mathieu,g.b.arous,y.lecun,thelosssurfacesofmultilayernetworks,in:proceed-ingsoftheinternationalconferenceonarti   cialintelligenceandstatistics(aistats),2015.[102]d.mishkin,j.matas,allyouneedisagoodinit,in:proceedingsoftheinternationalconferenceonlearningrepre-sentations(iclr),2016.[103]i.sutskever,j.martens,g.dahl,g.hinton,ontheimportanceofinitializationandmomentumindeeplearning,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2013,pp.1139   1147.[104]x.glorot,y.bengio,understandingthedi   cultyoftrainingdeepfeedforwardneuralnetworks,in:proceedingsoftheinternationalconferenceonarti   cialintelligenceandstatistics(aistats),2010,pp.249   256.[105]a.m.saxe,j.l.mcclelland,s.ganguli,exactsolutionstothenonlineardynamicsoflearningindeeplinearneuralnetworks,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2014.[106]c.doersch,a.gupta,a.a.efros,unsupervisedvisualrepresentationlearningbycontextprediction,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),2015,pp.1422   1430.[107]p.agrawal,j.carreira,j.malik,learningtoseebymoving,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),2015,pp.37   45.[108]n.qian,onthemomentumtermingradientdescentlearningalgorithms,neuralnetworks12(1)(1999)145   151.[109]d.kingma,j.ba,adam:amethodforstochasticoptimization,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2015.[110]i.loshchilov,f.hutter,sgdr:stochasticgradientdescentwithwarmrestarts,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2017.[111]t.schaul,s.zhang,y.lecun,nomorepeskylearningrates,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2013,pp.343   351.[112]s.zhang,a.e.choromanska,y.lecun,deeplearningwithelasticaveragingsgd,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2015,pp.685   693.[113]b.recht,c.re,s.wright,f.niu,hogwild:alock-freeapproachtoparallelizingstochasticgradientdescent,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2011,pp.693   701.[114]j.dean,g.corrado,r.monga,k.chen,m.devin,m.mao,a.senior,p.tucker,k.yang,q.v.le,etal.,largescaledistributeddeepnetworks,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2012,pp.1232   1240.[115]t.paine,h.jin,j.yang,z.lin,t.huang,gpuasynchronousstochasticgradientdescenttospeedupneuralnetworktraining,corrabs/1107.2490.[116]y.zhuang,w.-s.chin,y.-c.juan,c.-j.lin,afastparallelsgdformatrixfactorizationinsharedmemorysystems,in:proceedingsoftheacmconferenceonrecommendersystemsrecsys,2013,pp.249   256.[117]y.yao,l.rosasco,a.caponnetto,onearlystoppingingradientdescentlearning,constructiveapproximation26(2)(2007)289   315.[118]l.prechelt,earlystopping-butwhen?,in:neuralnetworks:tricksofthetrade-secondedition,2012,pp.53   67.[119]c.zhang,s.bengio,m.hardt,b.recht,o.vinyals,understandingdeeplearningrequiresrethinkinggeneralization,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2017.[120]s.io   e,c.szegedy,batchid172:acceleratingdeepnetworktrainingbyreducinginternalcovariateshift,journalofmachinelearningresearch(jmlr)(2015)448   456.[121]s.hochreiter,j.schmidhuber,longshort-termmemory,neuralcomputation9(8)(1997)1735   1780.[122]r.k.srivastava,k.gre   ,j.schmidhuber,trainingverydeepnetworks,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2015,pp.2377   2385.[123]k.he,x.zhang,s.ren,j.sun,identitymappingsindeepresidualnetworks,in:proceedingsoftheeuropeanconferenceoncomputervision(eccv),2016,pp.630   645.[124]f.shen,r.gan,g.zeng,weightedresidualsforverydeepnetworks,in:proceedingsoftheinternationalconferenceonsystemsandinformatics(icsai),2016,pp.936   941.[125]s.zagoruyko,n.komodakis,wideresidualnetworks,in:proceedingsofthebritishmachinevisionconference(bmvc),2016,pp.87.1   87.12.[126]s.singh,d.hoiem,d.forsyth,swapout:learninganensembleofdeeparchitectures,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2016,pp.28   36.[127]s.targ,d.almeida,k.lyman,resnetinresnet:generalizingresidualarchitectures,corrabs/1603.08029.[128]k.zhang,m.sun,t.x.han,x.yuan,l.guo,t.liu,residualnetworksofresidualnetworks:multilevelresidualnetworks,ieeetransactionsoncircuitsandsystemsforvideotechnology(tcsvt)pp(99)(2016)1   1.[129]g.huang,z.liu,k.q.weinberger,denselyconnectedconvolutionalnetworks,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2016,pp.4700   4708.[130]s.chetlur,c.woolley,p.vandermersch,j.cohen,j.tran,b.catanzaro,e.shelhamer,cudnn:e   cientprimitivesfordeeplearningabs/1410.0759.[131]n.vasilache,j.johnson,m.mathieu,s.chintala,s.piantino,y.lecun,fastconvolutionalnetswithfb   t:agpu32performanceevaluation,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2015.[132]p.sermanet,d.eigen,x.zhang,m.mathieu,r.fergus,y.lecun,overfeat:integratedrecognition,localizationanddetectionusingconvolutionalnetworks.[133]a.lavin,fastalgorithmsforconvolutionalneuralnetworks,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2016,pp.4013   4021.[134]t.n.sainath,b.kingsbury,v.sindhwani,e.arisoy,b.ramabhadran,low-rankmatrixfactorizationfordeepneuralnetworktrainingwithhigh-dimensionaloutputtargets,in:proceedingsoftheinternationalconferenceonacoustics,speech,andsignalprocessing(icassp),2013,pp.6655   6659.[135]j.xue,j.li,y.gong,restructuringofdeepneuralnetworkacousticmodelswithsingularvaluedecomposition,in:proceedingsoftheinternationalspeechcommunicationassociation(interspeech),2013,pp.2365   2369.[136]m.denil,b.shakibi,l.dinh,n.defreitas,etal.,predictingparametersindeeplearning,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2013,pp.2148   2156.[137]e.l.denton,w.zaremba,j.bruna,y.lecun,r.fergus,exploitinglinearstructurewithinconvolutionalnetworksfore   cientevaluation,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2014,pp.1269   1277.[138]m.jaderberg,a.vedaldi,a.zisserman,speedingupconvolutionalneuralnetworkswithlowrankexpansions,in:proceedingsofthebritishmachinevisionconference(bmvc),2014.[139]a.novikov,d.podoprikhin,a.osokin,d.p.vetrov,tensorizingneuralnetworks,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2015,pp.442   450.[140]i.v.oseledets,tensor-traindecomposition,siamj.scienti   ccomputing33(5)(2011)2295   2317.[141]q.le,t.sarl  os,a.smola,fastfood-approximatingkernelexpansionsinloglineartime,in:proceedingsoftheinterna-tionalconferenceonmachinelearning(icml),vol.85,2013.[142]a.dasgupta,r.kumar,t.sarl  os,fastlocality-sensitivehashing,in:proceedingsoftheinternationalconferenceonknowledgediscoveryanddatamining(sigkdd),2011,pp.1073   1081.[143]f.x.yu,s.kumar,y.gong,s.-f.chang,circulantbinaryembedding,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2014,pp.946   954.[144]y.cheng,f.x.yu,r.s.feris,s.kumar,a.choudhary,s.-f.chang,anexplorationofparameterredundancyindeepnetworkswithcirculantprojections,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),2015,pp.2857   2865.[145]m.moczulski,m.denil,j.appleyard,n.defreitas,acdc:astructurede   cientlinearlayer,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2016.[146]s.han,h.mao,w.j.dally,deepcompression:compressingdeepneuralnetworkwithpruning,trainedquantizationandhu   mancoding,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2016.[147]m.kim,p.smaragdis,bitwiseneuralnetworks,in:proceedingsoftheinternationalconferenceonmachinelearning(icml)workshops,2016.[148]m.rastegari,v.ordonez,j.redmon,a.farhadi,xnor-net:id163classi   cationusingbinaryconvolutionalneuralnetworks,in:proceedingsoftheeuropeanconferenceoncomputervision(eccv),2016,pp.525   542.[149]s.zhou,y.wu,z.ni,x.zhou,h.wen,y.zou,dorefa-net:traininglowbitwidthconvolutionalneuralnetworkswithlowbitwidthgradients,corrabs/1606.06160.[150]m.courbariaux,y.bengio,binarynet:trainingdeepneuralnetworkswithweightsandactivationsconstrainedto+1or-1,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2016.[151]g.j.sullivan,e   cientscalarquantizationofexponentialandlaplacianrandomvariables,ieeetrans.informationtheory42(5)(1996)1365   1374.[152]y.gong,l.liu,m.yang,l.bourdev,compressingdeepconvolutionalnetworksusingvectorquantization,in:arxivpreprintarxiv:1412.6115,vol.abs/1412.6115,2014.[153]y.chen,t.guan,c.wang,approximatenearestneighborsearchbyresidualvectorquantization,sensors10(12)(2010)11259   11273.[154]w.zhou,y.lu,h.li,q.tian,scalarquantizationforlargescaleimagesearch,in:proceedingsofthe20thacminternationalconferenceonmultimedia,2012,pp.169   178.[155]l.y.pratt,comparingbiasesforminimalnetworkconstructionwithback-propagation,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),1988,pp.177   185.[156]s.han,j.pool,j.tran,w.dally,learningbothweightsandconnectionsfore   cientneuralnetwork,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2015,pp.1135   1143.[157]y.guo,a.yao,y.chen,dynamicnetworksurgeryfore   cientdnns,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2016,pp.1379   1387.[158]t.-j.yang,y.-h.chen,v.sze,designingenergy-e   cientconvolutionalneuralnetworksusingenergy-awarepruning,corrabs/1611.05128.[159]h.hu,r.peng,y.-w.tai,c.-k.tang,networktrimming:adata-drivenneuronpruningapproachtowardse   cientdeeparchitectures,vol.abs/1607.03250,2016.[160]s.srinivas,r.v.babu,data-freeparameterpruningfordeepneuralnetworks,in:proceedingsofthebritishmachinevisionconference(bmvc),2015.[161]z.mariet,s.sra,diversitynetworks,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2015.[162]w.chen,j.t.wilson,s.tyree,k.q.weinberger,y.chen,compressingneuralnetworkswiththehashingtrick,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2015,pp.2285   2294.33[163]q.shi,j.petterson,g.dror,j.langford,a.smola,s.vishwanathan,hashkernelsforstructureddata,journalofmachinelearningresearch(jmlr)10(2009)2615   2637.[164]k.weinberger,a.dasgupta,j.langford,a.smola,j.attenberg,featurehashingforlargescalemultitasklearning,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2009,pp.1113   1120.[165]b.liu,m.wang,h.foroosh,m.tappen,m.pensky,sparseconvolutionalneuralnetworks,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.806   814.[166]w.wen,c.wu,y.wang,y.chen,h.li,learningstructuredsparsityindeepneuralnetworks,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2016,pp.2074   2082.[167]h.bagherinezhad,m.rastegari,a.farhadi,lid98:lookup-basedconvolutionalneuralnetwork,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2017.[168]m.egmont-petersen,d.deridder,h.handels,imageprocessingwithneuralnetworksareview,patternrecognition35(10)(2002)2279   2301.[169]k.nogueira,o.a.penatti,j.a.dossantos,towardsbetterexploitingconvolutionalneuralnetworksforremotesensingsceneclassi   cation,patternrecognition61(2017)539   556.[170]z.zuo,g.wang,b.shuai,l.zhao,q.yang,exemplarbaseddeepdiscriminativeandshareablefeaturelearningforsceneimageclassi   cation,patternrecognition48(10)(2015)3004   3015.[171]a.t.lopes,e.deaguiar,a.f.desouza,t.oliveira-santos,facialexpressionrecognitionwithconvolutionalneuralnetworks:copingwithfewdataandthetrainingsampleorder,patternrecognition61(2017)610   628.[172]m.everingham,s.a.eslami,l.vangool,c.k.williams,j.winn,a.zisserman,thepascalvisualobjectclasseschallenge:aretrospective,internationaljournalofcon   ictandviolence(ijcv)111(1)(2015)98   136.[173]a.-m.tousch,s.herbin,j.-y.audibert,semantichierarchiesforimageannotation:asurvey,patternrecognition45(1)(2012)333   345.[174]n.srivastava,r.r.salakhutdinov,discriminativetransferlearningwithtree-basedpriors,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2013,pp.2094   2102.[175]z.wang,x.wang,g.wang,learning   ne-grainedfeaturesviaaid98treeforlarge-scaleclassi   cation,corrabs/1511.04534.[176]t.xiao,j.zhang,k.yang,y.peng,z.zhang,error-drivenincrementallearningindeepconvolutionalneuralnetworkforlarge-scaleimageclassi   cation,in:proceedingsoftheacmmultimediaconference,2014,pp.177   186.[177]z.yan,v.jagadeesh,d.decoste,w.di,r.piramuthu,hd-id98:hierarchicaldeepconvolutionalneuralnetworkforimageclassi   cation,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),pp.2740   2748.[178]t.berg,j.liu,s.w.lee,m.l.alexander,d.w.jacobs,p.n.belhumeur,birdsnap:large-scale   ne-grainedvisualcategorizationofbirds,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2014,pp.2019   2026.[179]a.khosla,n.jayadevaprakash,b.yao,f.-f.li,noveldatasetfor   ne-grainedimagecategorization:stanforddogs,in:proceedingsoftheieeeinternationalconferenceoncomputervision(cvprworkshops,vol.2,2011,p.1.[180]l.yang,p.luo,c.c.loy,x.tang,alarge-scalecardatasetfor   ne-grainedcategorizationandveri   cation,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.3973   3981.[181]m.minervini,a.fischbach,h.scharr,s.a.tsaftaris,finely-grainedannotateddatasetsforimage-basedplantpheno-typing,patternrecognitionletters81(2016)80   89.[182]g.-s.xie,x.-y.zhang,w.yang,m.-l.xu,s.yan,c.-l.liu,lg-id98:fromlocalpartstoglobaldiscriminationfor   ne-grainedrecognition,patternrecognition71(2017)118   131.[183]s.branson,g.vanhorn,p.perona,s.belongie,improvedbirdspeciesrecognitionusingposenormalizeddeepconvo-lutionalnets,in:proceedingsofthebritishmachinevisionconference(bmvc),2014.[184]n.zhang,j.donahue,r.girshick,t.darrell,part-basedr-id98sfor   ne-grainedcategorydetection,in:proceedingsoftheeuropeanconferenceoncomputervision(eccv),2014,pp.834   849.[185]j.r.uijlings,k.e.vandesande,t.gevers,a.w.smeulders,selectivesearchforobjectrecognition,internationaljournalofcon   ictandviolence(ijcv)104(2)(2013)154   171.[186]d.lin,x.shen,c.lu,j.jia,deeplac:deeplocalization,alignmentandclassi   cationfor   ne-grainedrecognition,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.1666   1674.[187]j.p.pluim,j.a.maintz,m.viergever,etal.,mutual-information-basedregistrationofmedicalimages:asurvey,ieeetrans.med.imaging22(8)(2003)986   1004.[188]j.krause,t.gebru,j.deng,l.-j.li,l.fei-fei,learningfeaturesandpartsfor   ne-grainedrecognition,in:proceedingsoftheinternationalconferenceonpatternrecognition(icpr),2014,pp.26   33.[189]j.krause,h.jin,j.yang,l.fei-fei,fine-grainedrecognitionwithoutpartannotations,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.5546   5555.[190]y.zhang,x.-s.wei,j.wu,j.cai,j.lu,v.-a.nguyen,m.n.do,weaklysupervised   ne-grainedcategorizationwithpart-basedimagerepresentation,ieeetransactionsonimageprocessing25(4)(2016)1713   1725.[191]t.xiao,y.xu,k.yang,j.zhang,y.peng,z.zhang,theapplicationoftwo-levelattentionmodelsindeepconvolutionalneuralnetworkfor   ne-grainedimageclassi   cation,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.842   850.[192]t.-y.lin,a.roychowdhury,s.maji,bilinearid98modelsfor   ne-grainedvisualrecognition,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),2015,pp.1449   1457.[193]d.t.nguyen,w.li,p.o.ogunbona,humandetectionfromimagesandvideos:asurvey,patternrecognition51(2016)148   175.[194]y.li,s.wang,q.tian,x.ding,featurerepresentationforstatistical-learning-basedobjectdetection:areview,pattern34recognition48(11)(2015)3542   3559.[195]m.pedersoli,a.vedaldi,j.gonzalez,x.roca,acoarse-to-   neapproachforfastdeformableobjectdetection,patternrecognition48(5)(2015)1844   1853.[196]s.j.nowlan,j.c.platt,aconvolutionalneuralnetworkhandtracker,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),1994,pp.901   908.[197]r.girshick,f.iandola,t.darrell,j.malik,deformablepartmodelsareconvolutionalneuralnetworks,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.437   446.[198]r.vaillant,c.monrocq,y.lecun,originalapproachforthelocalisationofobjectsinimages,ieeproceedings-vision,imageandsignalprocessing141(4)(1994)245   250.[199]t.-y.lin,m.maire,s.belongie,j.hays,p.perona,d.ramanan,p.doll  ar,c.l.zitnick,microsoftcoco:commonobjectsincontext,in:proceedingsoftheeuropeanconferenceoncomputervision(eccv),2014,pp.740   755.[200]i.endres,d.hoiem,categoryindependentobjectproposals,ieeetransactionsonpatternanalysisandmachineintelligence(pami)36(2)(2014)222   234.[201]l.g  omez,d.karatzas,textproposals:atext-speci   cselectivesearchalgorithmforwordspottinginthewild,patternrecognition70(2017)60   74.[202]r.girshick,j.donahue,t.darrell,j.malik,richfeaturehierarchiesforaccurateobjectdetectionandsemanticsegmentation,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2014,pp.580   587.[203]k.he,x.zhang,s.ren,j.sun,spatialpyramidpoolingindeepconvolutionalnetworksforvisualrecognition,ieeetransactionsonpatternanalysisandmachineintelligence(pami)37(9)(2015)1904   1916.[204]s.ren,k.he,r.girshick,j.sun,fasterr-id98:towardsreal-timeobjectdetectionwithregionproposalnetworks,ieeetransactionsonpatternanalysisandmachineintelligence(pami)39(6)(2017)1137   1149.[205]s.gidaris,n.komodakis,objectdetectionviaamulti-regionandsemanticsegmentation-awareid98model,in:proceed-ingsoftheinternationalconferenceoncomputervision(iccv),2015,pp.1134   1142.[206]d.yoo,s.park,j.-y.lee,a.s.paek,i.sokweon,attentionnet:aggregatingweakdirectionsforaccurateobjectdetection,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),2015,pp.2659   2667.[207]p.f.felzenszwalb,r.b.girshick,d.mcallester,d.ramanan,objectdetectionwithdiscriminativelytrainedpart-basedmodels,ieeetransactionsonpatternanalysisandmachineintelligence(pami)32(9)(2010)1627   1645.[208]e.simo-serra,e.trulls,l.ferraz,i.kokkinos,f.moreno-noguer,frackingdeepconvolutionalimagedescriptors,corrabs/1412.6537.[209]a.shrivastava,a.gupta,r.girshick,trainingregion-basedobjectdetectorswithonlinehardexamplemining,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2016,pp.761   769.[210]j.redmon,s.divvala,r.girshick,a.farhadi,youonlylookonce:uni   ed,real-timeobjectdetection,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2016,pp.779   788.[211]w.liu,d.anguelov,d.erhan,c.szegedy,s.reed,ssd:singleshotmultiboxdetector,in:proceedingsoftheeuropeanconferenceoncomputervision(eccv),2016,pp.21   37.[212]y.lu,t.javidi,s.lazebnik,adaptiveobjectdetectionusingadjacencyandzoomprediction,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2016,pp.2351   2359.[213]k.zhang,h.song,real-timevisualtrackingviaonlineweightedmultipleinstancelearning,patternrecognition46(1)(2013)397   411.[214]s.zhang,h.yao,x.sun,x.lu,sparsecodingbasedvisualtracking:reviewandexperimentalcomparison,patternrecognition46(7)(2013)1772   1788.[215]s.zhang,j.wang,z.wang,y.gong,y.liu,multi-targettrackingbylearninglocal-to-globaltrajectorymodels,patternrecognition48(2)(2015)580   590.[216]j.fan,w.xu,y.wu,y.gong,humantrackingusingconvolutionalneuralnetworks,ieeetrans.neuralnetworks(tnn)21(10)(2010)1610   1623.[217]h.li,y.li,f.porikli,deeptrack:learningdiscriminativefeaturerepresentationsbyconvolutionalneuralnetworksforvisualtracking,in:proceedingsofthebritishmachinevisionconference(bmvc),2014.[218]y.chen,x.yang,b.zhong,s.pan,d.chen,h.zhang,id98tracker:onlinediscriminativeobjecttrackingviadeepconvolutionalneuralnetwork,appl.softcomput.38(2016)1088   1098.[219]s.hong,t.you,s.kwak,b.han,onlinetrackingbylearningdiscriminativesaliencymapwithconvolutionalneuralnetwork,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2015,pp.597   606.[220]m.patacchiola,a.cangelosi,headposeestimationinthewildusingconvolutionalneuralnetworksandadaptivegradientmethods,patternrecognition71(2017)132   143.[221]k.nishi,j.miura,generationofhumandepthimageswithbodypartlabelsforcomplexhumanposerecognition,patternrecognition.[222]a.toshev,c.szegedy,deeppose:humanposeestimationviadeepneuralnetworks,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2014,pp.1653   1660.[223]a.jain,j.tompson,m.andriluka,g.w.taylor,c.bregler,learninghumanposeestimationfeatureswithconvolutionalnetworks,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2014.[224]j.j.tompson,a.jain,y.lecun,c.bregler,jointtrainingofaconvolutionalnetworkandagraphicalmodelforhumanposeestimation,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2014,pp.1799   1807.[225]x.chen,a.l.yuille,articulatedposeestimationbyagraphicalmodelwithimagedependentpairwiserelations,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2014,pp.1736   1744.[226]x.chen,a.yuille,parsingoccludedpeopleby   exiblecompositions,in:proceedingsoftheieeeconferenceon35computervisionandpatternrecognition(cvpr),2015,pp.3945   3954.[227]x.fan,k.zheng,y.lin,s.wang,combininglocalappearanceandholisticview:dual-sourcedeepneuralnetworksforhumanposeestimation,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.1347   1355.[228]a.jain,j.tompson,y.lecun,c.bregler,modeep:adeeplearningframeworkusingmotionfeaturesforhumanposeestimation,in:proceedingsoftheasianconferenceoncomputervision(accv),2014,pp.302   315.[229]y.y.tang,s.-w.lee,c.y.suen,automaticdocumentprocessing:asurvey,patternrecognition29(12)(1996)1931   1952.[230]a.vinciarelli,asurveyono   -linecursivewordrecognition,patternrecognition35(7)(2002)1433   1446.[231]k.jung,k.i.kim,a.k.jain,textinformationextractioninimagesandvideo:asurvey,patternrecognition37(5)(2004)977   997.[232]s.eskenazi,p.gomez-kr  amer,j.-m.ogier,acomprehensivesurveyofmostlytextualdocumentsegmentationalgorithmssince2008,patternrecognition64(2017)1   14.[233]x.bai,b.shi,c.zhang,x.cai,l.qi,text/non-textimageclassi   cationinthewildwithconvolutionalneuralnetworks,patternrecognition66(2017)437   446.[234]l.gomez,a.nicolaou,d.karatzas,improvingpatch-basedscenetextscriptidenti   cationwithensemblesofconjoinednetworks,patternrecognition67(2017)85   96.[235]m.delakis,c.garcia,textdetectionwithconvolutionalneuralnetworks,in:proceedingsoftheinternationalconferenceoncomputervisiontheoryandapplications(visapp),2008,pp.290   294.[236]h.xu,f.su,robustseedlocalizationandgrowingwithdeepconvolutionalfeaturesforscenetextdetection,in:pro-ceedingsoftheinternationalconferenceonmultimediaretrieval(icmr),2015,pp.387   394.[237]w.huang,y.qiao,x.tang,robustscenetextdetectionwithconvolutionneuralnetworkinducedmsertrees,in:proceedingsoftheeuropeanconferenceoncomputervision(eccv),2014,pp.497   511.[238]c.zhang,c.yao,b.shi,x.bai,automaticdiscriminationoftextandnon-textnaturalimages,in:proceedingsoftheinternationalconferenceondocumentanalysisandrecognition(icdar),2015,pp.886   890.[239]i.j.goodfellow,j.ibarz,s.arnoud,v.shet,multi-digitnumberrecognitionfromstreetviewimageryusingdeepconvolutionalneuralnetworks,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2014.[240]m.jaderberg,k.simonyan,a.vedaldi,a.zisserman,deepstructuredoutputlearningforunconstrainedtextrecogni-tion,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2015.[241]p.he,w.huang,y.qiao,c.c.loy,x.tang,readingscenetextindeepconvolutionalsequences,in:proceedingsoftheaaaiconferenceonarti   cialintelligence,2016,pp.3501   3508.[242]f.a.gers,j.schmidhuber,f.cummins,learningtoforget:continualpredictionwithlstm,neuralcomputation12(10)(2000)2451   2471.[243]b.shi,x.bai,c.yao,anend-to-endtrainableneuralnetworkforimage-basedsequencerecognitionanditsapplicationtoscenetextrecognition,corrabs/1507.05717.[244]m.jaderberg,a.vedaldi,a.zisserman,deepfeaturesfortextspotting,in:proceedingsoftheeuropeanconferenceoncomputervision(eccv),2014,pp.512   528.[245]m.jaderberg,k.simonyan,a.vedaldi,a.zisserman,readingtextinthewildwithconvolutionalneuralnetworks,vol.116,2016,pp.1   20.[246]l.wang,h.lu,x.ruan,m.-h.yang,deepnetworksforsaliencydetectionvialocalestimationandglobalsearch,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.3183   3192.[247]r.zhao,w.ouyang,h.li,x.wang,saliencydetectionbymulti-contextdeeplearning,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.1265   1274.[248]g.li,y.yu,visualsaliencybasedonmultiscaledeepfeatures,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.5455   5463.[249]n.liu,j.han,d.zhang,s.wen,t.liu,predictingeye   xationsusingconvolutionalneuralnetworks,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.362   370.[250]s.he,r.w.lau,w.liu,z.huang,q.yang,superid98:asuperpixelwiseconvolutionalneuralnetworkforsalientobjectdetection,internationaljournalofcon   ictandviolence(ijcv)115(3)(2015)330   344.[251]e.vig,m.dorr,d.cox,large-scaleoptimizationofhierarchicalfeaturesforsaliencypredictioninnaturalimages,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2014,pp.2798   2805.[252]m.kmmerer,l.theis,m.bethge,deepgazei:boostingsaliencypredictionwithfeaturemapstrainedonid163,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr)workshops,2015.[253]j.pan,x.gir-inieto,end-to-endconvolutionalnetworkforsaliencyprediction,corrabs/1507.01422.[254]g.guo,a.lai,asurveyonstillimagebasedhumanactionrecognition,patternrecognition47(10)(2014)3343   3361.[255]l.l.presti,m.lacascia,3dskeleton-basedhumanactionclassi   cation:asurvey,patternrecognition53(2016)130   147.[256]j.zhang,w.li,p.o.ogunbona,p.wang,c.tang,rgb-d-basedactionrecognitiondatasets:asurvey,patternrecognition60(2016)86   105.[257]j.donahue,y.jia,o.vinyals,j.ho   man,n.zhang,e.tzeng,t.darrell,decaf:adeepconvolutionalactivationfeatureforgenericvisualrecognition(2014).[258]m.oquab,l.bottou,i.laptev,j.sivic,learningandtransferringmid-levelimagerepresentationsusingconvolutionalneuralnetworks,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2014,pp.1717   1724.36[259]g.gkioxari,r.girshick,j.malik,actionsandattributesfromwholesandparts,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),2015,pp.2470   2478.[260]l.pishchulin,m.andriluka,p.gehler,b.schiele,poseletconditionedpictorialstructures,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2013,pp.588   595.[261]g.gkioxari,r.b.girshick,j.malik,contextualactionrecognitionwithr*id98,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),2015,pp.1080   1088.[262]g.gkioxari,r.girshick,j.malik,actionsandattributesfromwholesandparts,in:proceedingsoftheieeeinterna-tionalconferenceoncomputervision(iccv),2015,pp.2470   2478.[263]y.zhang,l.cheng,j.wu,j.cai,m.n.do,j.lu,actionrecognitioninstillimageswithminimumannotatione   orts,ieeetransactionsonimageprocessing25(11)(2016)5479   5490.[264]l.wang,l.ge,r.li,y.fang,three-streamid98sforactionrecognition,patternrecognitionletters92(2017)33   40.[265]s.ji,w.xu,m.yang,k.yu,3dconvolutionalneuralnetworksforhumanactionrecognition,ieeetransactionsonpatternanalysisandmachineintelligence(pami)35(1)(2013)221   231.[266]d.tran,l.bourdev,r.fergus,l.torresani,m.paluri,learningspatiotemporalfeatureswith3dconvolutionalnetworks,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),2015,pp.4489   4497.[267]a.karpathy,g.toderici,s.shetty,t.leung,r.sukthankar,l.fei-fei,large-scalevideoclassi   cationwithconvolu-tionalneuralnetworks,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2014,pp.1725   1732.[268]k.simonyan,a.zisserman,two-streamconvolutionalnetworksforactionrecognitioninvideos,in:z.ghahramani,m.welling,c.cortes,n.lawrence,k.weinberger(eds.),proceedingsoftheadvancesinneuralinformationprocessingsystems(nips),2014,pp.568   576.[269]g.ch  eron,i.laptev,c.schmid,p-id98:pose-basedid98featuresforactionrecognition,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),2015,pp.3218   3226.[270]j.donahue,l.annehendricks,s.guadarrama,m.rohrbach,s.venugopalan,k.saenko,t.darrell,long-termrecurrentconvolutionalnetworksforvisualrecognitionanddescription,ieeetransactionsonpatternanalysisandmachineintelligence(pami)39(4)(2017)677   691.[271]k.-s.fu,j.mui,asurveyonimagesegmentation,patternrecognition13(1)(1981)3   16.[272]q.zhou,b.zheng,w.zhu,l.j.latecki,multi-scalecontextforscenelabelingvia   exiblesegmentationgraph,patternrecognition59(2016)312   324.[273]f.liu,g.lin,c.shen,crflearningwithid98featuresforimagesegmentation,patternrecognition48(10)(2015)2983   2992.[274]s.bu,p.han,z.liu,j.han,sceneparsingusingid136embeddeddeepnetworks,patternrecognition59(2016)188   198.[275]b.peng,l.zhang,d.zhang,asurveyofgraphtheoreticalapproachestoimagesegmentation,patternrecognition46(3)(2013)1020   1038.[276]c.farabet,c.couprie,l.najman,y.lecun,learninghierarchicalfeaturesforscenelabeling,ieeetransactionsonpatternanalysisandmachineintelligence(pami)35(8)(2013)1915   1929.[277]c.couprie,c.farabet,l.najman,y.lecun,indoorsemanticsegmentationusingdepthinformation,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2013.[278]p.pinheiro,r.collobert,recurrentconvolutionalneuralnetworksforscenelabeling,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2014,pp.82   90.[279]b.shuai,g.wang,z.zuo,b.wang,l.zhao,integratingparametricandnon-parametricmodelsforscenelabeling,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.4249   4258.[280]b.shuai,z.zuo,w.gang,quaddirectional2d-recurrentneuralnetworksforimagelabeling22(11)(2015)1990   1994.[281]b.shuai,z.zuo,g.wang,b.wang,dag-recurrentneuralnetworksforscenelabeling,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2016,pp.3620   3629.[282]m.mostajabi,p.yadollahpour,g.shakhnarovich,feedforwardsemanticsegmentationwithzoom-outfeatures,in:proceedingsoftheieeeconferenceoncomputervisionandpatternrecognition(cvpr),2015,pp.3376   3385.[283]l.-c.chen,g.papandreou,i.kokkinos,k.murphy,a.l.yuille,semanticimagesegmentationwithdeepconvolutionalnetsandfullyconnectedcrfs,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2015.[284]m.elayadi,m.s.kamel,f.karray,surveyonspeechemotionrecognition:features,classi   cationschemes,anddatabases,patternrecognition44(3)(2011)572   587.[285]l.deng,p.kenny,m.lennig,v.gupta,f.seitz,p.mermelstein,phonemichiddenmarkovmodelswithcontinuousmixtureoutputdensitiesforlargevocabularywordrecognition,ieeetrans.signalprocessing39(7)(1991)1677   1681.[286]g.hinton,l.deng,d.yu,g.e.dahl,a.-r.mohamed,n.jaitly,a.senior,v.vanhoucke,p.nguyen,t.n.sainath,etal.,deepneuralnetworksforacousticmodelinginspeechrecognition:thesharedviewsoffourresearchgroups,ieeesignalprocess.mag.29(6)(2012)82   97.[287]l.deng,j.li,j.-t.huang,k.yao,d.yu,f.seide,m.seltzer,g.zweig,x.he,j.williams,etal.,recentadvancesindeeplearningforspeechresearchatmicrosoft,in:proceedingsoftheinternationalconferenceonacoustics,speech,andsignalprocessing(icassp),2013,pp.8604   8608.[288]k.yao,d.yu,f.seide,h.su,l.deng,y.gong,adaptationofcontext-dependentdeepneuralnetworksforautomaticspeechrecognition,in:proceedingsofthespokenlanguagetechnology(slt),2012,pp.366   369.[289]o.abdel-hamid,a.-r.mohamed,h.jiang,g.penn,applyingconvolutionalneuralnetworksconceptstohybridnn-id48modelforspeechrecognition,in:proceedingsoftheinternationalconferenceonacoustics,speech,andsignalprocessing(icassp),2012,pp.4277   4280.37[290]o.abdel-hamid,a.-r.mohamed,h.jiang,l.deng,g.penn,d.yu,convolutionalneuralnetworksforspeechrecog-nition,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2014.[291]d.palaz,r.collobert,m.m.doss,estimatingphonemeclassconditionalprobabilitiesfromrawspeechsignalusingcon-volutionalneuralnetworks,in:proceedingsoftheinternationalspeechcommunicationassociation(interspeech),2013,pp.1766   1770.[292]y.hoshen,r.j.weiss,k.w.wilson,speechacousticmodelingfromrawmultichannelwaveforms,in:proceedingsoftheinternationalconferenceonacoustics,speech,andsignalprocessing(icassp),2015,pp.4624   4628.[293]d.amodei,r.anubhai,e.battenberg,c.case,j.casper,b.catanzaro,j.chen,m.chrzanowski,a.coates,g.diamos,etal.,deepspeech2:end-to-endspeechrecognitioninenglishandmandarin,2016,pp.173   182.[294]t.sercu,v.goel,advancesinverydeepconvolutionalneuralnetworksforlvcsr,in:proceedingsoftheinternationalspeechcommunicationassociation(interspeech),2016,pp.3429   3433.[295]l.t  oth,convolutionaldeepmaxoutnetworksforphonerecognition.,in:proceedingsoftheinternationalspeechcom-municationassociation(interspeech),2014,pp.1078   1082.[296]t.n.sainath,b.kingsbury,a.mohamed,g.e.dahl,g.saon,h.soltau,t.beran,a.y.aravkin,b.ramabhadran,improvementstodeepconvolutionalneuralnetworksforlvcsr,in:proceedingsoftheautomaticspeechrecognitionandunderstanding(asru)workshops,2013,pp.315   320.[297]d.yu,w.xiong,j.droppo,a.stolcke,g.ye,j.li,g.zweig,deepconvolutionalneuralnetworkswithlayer-wisecontextexpansionandattention,in:proceedingsoftheinternationalspeechcommunicationassociation(interspeech),2016,pp.17   21.[298]a.waibel,t.hanazawa,g.hinton,k.shikano,k.j.lang,phonemerecognitionusingtime-delayneuralnetworks,ieeetrans.acoustics,speech,andsignalprocessing37(3)(1989)328   339.[299]l.-h.chen,t.raitio,c.valentini-botinhao,j.yamagishi,z.-h.ling,dnn-basedstochasticpost   lterforid48-basedspeechsynthesis.,in:proceedingsoftheinternationalspeechcommunicationassociation(interspeech),2014,pp.1954   1958.[300]b.uria,i.murray,s.renals,c.valentini-botinhao,j.bridle,modellingacousticfeaturedependencieswitharti   cialneuralnetworks:trajectory-rnade,in:proceedingsoftheinternationalconferenceonacoustics,speech,andsignalprocessing(icassp),2015,pp.4465   4469.[301]z.huang,s.m.siniscalchi,c.-h.lee,hierarchicalbayesiancombinationofplug-inmaximumaposterioridecodersindeepneuralnetworks-basedspeechrecognitionandspeakeradaptation,patternrecognitionletters.[302]a.vandenoord,n.kalchbrenner,k.kavukcuoglu,pixelrecurrentneuralnetworks,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2016,pp.1747   1756.[303]r.jozefowicz,o.vinyals,m.schuster,n.shazeer,y.wu,exploringthelimitsoflanguagemodeling,in:proceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2016.[304]y.kim,y.jernite,d.sontag,a.m.rush,character-awareneurallanguagemodels,in:proceedingsoftheassociationfortheadvancementofarti   cialintelligence(aaai),2016,pp.2741   2749.[305]j.gu,c.jianfei,g.wang,t.chen,stack-captioning:coarse-to-   nelearningforimagecaptioning,vol.abs/1709.03376,2017.[306]m.wang,z.lu,h.li,w.jiang,q.liu,genid98:aconvolutionalarchitectureforwordsequenceprediction,in:proceedingsoftheassociationforcomputationallinguistics(acl),2015,pp.1567   1576.[307]j.gu,g.wang,c.jianfei,t.chen,anempiricalstudyoflanguageid98forimagecaptioning,in:proceedingsoftheinternationalconferenceoncomputervision(iccv),2017.[308]m.a.d.g.yannn.dauphin,angelafan,languagemodelingwithgatedconvolutionalnetworks,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2017,pp.933   941.[309]r.collobert,j.weston,auni   edarchitecturefornaturallanguageprocessing:deepneuralnetworkswithmultitasklearning,in:proceedingsoftheinternationalconferenceonmachinelearning(icml),2008,pp.160   167.[310]l.yu,k.m.hermann,p.blunsom,s.pulman,deeplearningforanswersentenceselection,in:proceedingsoftheadvancesinneuralinformationprocessingsystems(nips)workshop,2014.[311]n.kalchbrenner,e.grefenstette,p.blunsom,aconvolutionalneuralnetworkformodellingsentences,in:proceedingsoftheassociationforcomputationallinguistics(acl),2014,pp.655   665.[312]y.kim,convolutionalneuralnetworksforsentenceclassi   cation,in:proceedingsoftheconferenceonempiricalmethodsinnaturallanguageprocessing(emnlp),2014,pp.1746   1751.[313]w.yin,h.sch  utze,multichannelvariable-sizeconvolutionforsentenceclassi   cation,in:proceedingsoftheconferenceonnaturallanguagelearning(conll),2015,pp.204   214.[314]r.collobert,j.weston,l.bottou,m.karlen,k.kavukcuoglu,p.kuksa,naturallanguageprocessing(almost)fromscratch,journalofmachinelearningresearch(jmlr)12(2011)2493   2537.[315]a.conneau,h.schwenk,l.barrault,y.lecun,verydeepconvolutionalnetworksfornaturallanguageprocessing,corrabs/1606.01781.[316]g.huang,y.sun,z.liu,d.sedra,k.weinberger,deepnetworkswithstochasticdepth,in:proceedingsoftheeuropeanconferenceoncomputervision(eccv),2016,pp.646   661.38