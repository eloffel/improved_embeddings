morphological in   ection generation using
character sequence to sequence learning

manaal faruqui1 yulia tsvetkov1 graham neubig2 chris dyer1

2graduate school of information science, nara institute of science and technology, japan

1language technologies institute, carnegie mellon university, usa
{mfaruqui,ytsvetko,cdyer}@cs.cmu.edu neubig@is.naist.jp

6
1
0
2

 
r
a

 

m
2
2

 
 
]
l
c
.
s
c
[
 
 

3
v
0
1
1
6
0

.

2
1
5
1
:
v
i
x
r
a

abstract

morphological
in   ection generation is the
task of generating the in   ected form of a given
lemma corresponding to a particular linguis-
tic transformation. we model the problem of
in   ection generation as a character sequence
to sequence learning problem and present a
variant of the neural encoder-decoder model
for solving it. our model is language inde-
pendent and can be trained in both supervised
and semi-supervised settings. we evaluate our
system on seven datasets of morphologically
rich languages and achieve either better or
comparable results to existing state-of-the-art
models of in   ection generation.

1

introduction

in   ection is the word-formation mechanism to ex-
press different grammatical categories such as tense,
mood, voice, aspect, person, gender, number and
case. in   ectional morphology is often realized by
the concatenation of bound morphemes (pre   xes
and suf   xes) to a root form or stem, but noncon-
catenative processes such as ablaut and in   xation are
found in many languages as well. table 1 shows the
possible in   ected forms of the german stem kalb
(calf) when it is used in different cases and numbers.
the in   ected forms are the result of both ablaut (e.g.,
a     a) and suf   xation (e.g., +ern).

in   ection generation is useful for reducing data
sparsity in morphologically complex languages. for
example, id151 suffers from
data sparsity when translating morphologically-rich
languages, since every surface form is considered an

singular

nominative kalb
accusative kalb
kalb
dative
genitive
kalbes

plural
k  alber
k  alber
k  albern
k  alber

table 1: an example of an in   ection table from the german
noun dataset for the word kalb (calf).

independent entity. translating into lemmas in the
target language, and then applying in   ection gener-
ation as a post-processing step, has been shown to
alleviate the sparsity problem (minkov et al., 2007;
toutanova et al., 2008; clifton and sarkar, 2011;
fraser et al., 2012; chahuneau et al., 2013a). mod-
eling in   ection generation has also been used to im-
prove id38 (chahuneau et al., 2013b),
identi   cation of multi-word expressions (o   azer et
al., 2004), among other applications.

the traditional approach to modeling in   ec-
tion relies on hand-crafted    nite state transducers
and id69, e.g., using two-level morphology
(koskenniemi, 1983; kaplan and kay, 1994). such
systems are appealing since they correspond to lin-
guistic theories, but they are expensive to create,
they can be fragile (o   azer, 1996), and the com-
posed transducers can be impractically large. as
an alternative, machine learning models have been
proposed to generate in   ections from root forms
as string transduction (yarowsky and wicentowski,
2000; wicentowski, 2004; dreyer and eisner, 2011;
durrett and denero, 2013; ahlberg et al., 2014;
hulden, 2014; ahlberg et al., 2015; nicolai et al.,
2015). however, these impose either assumptions
about the set of possible morphological processes

figure 1: a general in   ection generation model.

(e.g. af   xation) or require careful feature engineer-
ing.

in this paper, we present a model of in   ection
generation based on a neural network sequence to
sequence transducer. the root form is represented
as sequence of characters, and this is the input to
an encoder-decoder architecture (cho et al., 2014;
sutskever et al., 2014). the model transforms its in-
put to a sequence of output characters representing
the in   ected form (  4). our model makes no as-
sumptions about morphological processes, and our
features are simply the individual characters. the
model is trained on pairs of root form and in   ected
forms obtained from in   ection tables extracted from
wiktionary.1 we improve the supervised model with
unlabeled data, by integrating a character language
model trained on the vocabulary of the language.

our experiments show that the model achieves
better or comparable results to state-of-the-art meth-
ods on the benchmark in   ection generation tasks
(  5). for example, our model is able to learn long-
range relations between character sequences in the
string aiding the in   ection generation process re-
quired by finnish vowel harmony (  6), which helps
it obtain the current best results in that language. we
have publicly released our code for in   ection gener-
ation.2

2

in   ection generation: background

durrett and denero (2013) formulate the task of su-
pervised in   ection generation for a given root form,
based on a large number of training in   ection ta-
bles extracted from wiktionary. every in   ection ta-
ble contains the in   ected form of a given root word
corresponding to different linguistic transformations
(cf. table 1). figure 1 shows the in   ection genera-
tion framework. since the release of the wiktionary

1www.wiktionary.org
2https://github.com/mfaruqui/morph-_

trans

dataset, several different models have reported per-
formance on this dataset. as we are also using this
dataset, we will now review these models.

we denote the models of durrett and denero
(2013), ahlberg et al. (2014), ahlberg et al. (2015),
and nicolai et al. (2015), by ddn13, afh14,
afh15, and nck15 respectively. these models
perform in   ection generation as string transduction
and largely consist of three major components: (1)
character alignment of word forms in a table; (2)
extraction of string transformation rules; (3) appli-
cation of rules to new root forms.

the    rst step is learning character alignments
across in   ected forms in a table. figure 2 (a) shows
alignment between three word forms of kalb. dif-
ferent models use different heuristic algorithms for
alignments such as id153, dynamic edit dis-
tance (eisner, 2002; oncina and sebban, 2006),
and longest subsequence alignment (bergroth et al.,
2000). aligning characters across word forms pro-
vide spans of characters that have changed and spans
that remain unchanged. these spans are used to ex-
tract rules for in   ection generation for different in-
   ection types as shown in figure 2 (b)   (d).

by applying the extracted rules to new root forms,
in   ected words can be generated. ddn13 use a
semi-markov model (sarawagi and cohen, 2004) to
predict what rules should be applied, using charac-
ter id165s (n = 1 to 4) as features. afh14 and
afh15 use substring features extracted from words
to match an input word to a rule table. nck15 use
a semi-markov model inspired by ddn13, but ad-
ditionally use target id165s and joint id165s as
features sequences while selecting the rules.

motivation for our model. morphology often
makes references to segmental features, like place
or manner of articulation, or voicing status (chom-
sky and halle, 1968). while these can be encoded
as features in existing work, our approach treats seg-
ments as vectors of features    natively   . our ap-
proach represents every character as a bundle of con-
tinuous features, instead of using discrete surface
character sequence features. also, our model uses
features as part of the transduction rules themselves,
whereas in existing work features are only used to
rescore rule applications.

in existing work,

the learner implicitly speci-

in   ection generationkalbk  lbercase=nominative number=plurale := ht +1 as the input representation. the decoder
is trained to predict the next output yt given the
encoded input vector e and all the previously pre-
dicted outputs (cid:104)y1,       yt   1(cid:105). in other words, the de-
coder de   nes a id203 over the output sequence
(cid:126)y = (cid:104)y1,       , yt (cid:48)(cid:105) by decomposing the joint proba-
bility into ordered conditionals:

p((cid:126)y|(cid:126)x) =

p(yt|e,(cid:104)y1,       , yt   1(cid:105))

(2)

(cid:89)t (cid:48)

t=1

figure 2: rule extraction:
(a) character aligned-table; (b)
table-level rule of afh14, afh15 (c) vertical rules of ddn13
and (d) atomic rules of nck15.

   es the class of rules that can be learned, such
as    delete    or    concatenate   . to deal with phe-
nomenona like segment lengthening in english: run
    running; or reduplication in hebrew: kelev    
klavlav, chatul     chataltul; (or consonant grada-
tion in finnish), where the af   xes are induced from
characters of the root form, one must engineer a new
rule class, which leads to poorer estimates due to
data sparsity. by modeling in   ection generation as
a task of generating a character sequence, one char-
acter at a time, we do away with such problems.

3 neural encoder-decoder models
here, we describe brie   y the underlying framework
of our in   ection generation model, called the recur-
rent neural network (id56) encoder-decoder (cho et
al., 2014; sutskever et al., 2014) which is used to
transform an input sequence (cid:126)x to output sequence (cid:126)y.
we represent an item by x, a sequence of items by
(cid:126)x, vectors by x, matrices by x, and sequences of
vectors by (cid:126)x.

3.1 formulation
in the encoder-decoder framework, an encoder reads
a variable length input sequence, a sequence of vec-
tors (cid:126)x = (cid:104)x1,       , xt(cid:105) (corresponding to a sequence
of input symbols (cid:126)x = (cid:104)x1,       , xt(cid:105)) and generates
a    xed-dimensional vector representation of the se-
quence. xt     rl is an input vector of length l. the
most common approach is to use an id56 such that:

ht = f (ht   1, xt)

(1)
where ht     rn is a hidden state at time t, and f
is generally a non-linear transformation, producing

with a decoder id56, we can    rst obtain the hidden
layer at time t as: st = g(st   1,{e, yt   1}) and feed
this into a softmax layer to obtain the conditional
id203 as:

p(yt = i|e, (cid:126)y<t) = softmax(wsst + bs)i

(3)
where, (cid:126)y<t = (cid:104)y1,       , yt   1(cid:105). in recent work, both
f and g are generally lstms, a kind of id56 which
we describe next.

3.2 long short-term memory (lstm)
in principle, id56s allow retaining information from
time steps in the distant past, but the nonlinear
   squashing    functions applied in the calculation of
each ht result in a decay of the error signal used in
training with id26. lstms are a vari-
ant of id56s designed to cope with this    vanish-
ing gradient    problem using an extra memory    cell   
(hochreiter and schmidhuber, 1997; graves, 2013).
past work explains the computation within an lstm
through the metaphors of deciding how much of the
current input to pass into memory or forget. we refer
interested readers to the original papers for details.

4

in   ection generation model

we frame the problem of in   ection generation as a
sequence to sequence learning problem of charac-
ter sequences. the standard encoder-decoder mod-
els were designed for machine translation where
the objective is to translate a sentence (sequence of
words) from one language to a semantically equiv-
alent sentence (sequence of words) in another lan-
guage. we can easily port the encoder-decoder
translation model for in   ection generation. our
model predicts the sequence of characters in the in-
   ected string given the characters in the root word
(input).

kalb<w></w>k  lber<w></w>kalb<w></w>es(a)x1ax2<w></w>  er<w></w>a<w></w>esx2x1x1x2(b)</w>er</w></w>esa  a(c)(d)a  aa</w>er</w></w>es</w>kkllbbfigure 3: the modi   ed encoder-decoder architecture for in   ection generation. input characters are shown in black and predicted

characters are shown in red.   indicates the append operation.

however, our problem differs from the above set-
ting in two ways: (1) the input and output character
sequences are mostly similar except for the in   ec-
tions; (2) the input and output character sequences
have different semantics. regarding the    rst differ-
ence, taking the word play as an example, the in-
   ected forms corresponding to past tense and con-
tinuous forms are played and playing. to better use
this correspondence between the input and output
sequence, we also feed the input sequence directly
into the decoder:

st = g(st   1,{e, yt   1, xt})

(4)

where, g is the decoder lstm, and xt and yt are the
input and output character vectors respectively. be-
cause the lengths of the input and output sequences
are not equal, we feed an   character in the decoder,
indicating null input, once the input sequence runs
out of characters. these   character vectors are pa-
rameters that are learned by our model, exactly as
other character vectors.

regarding the second difference, to provide the
model the ability to learn the transformation of se-
mantics from input to output, we apply an af   ne
transformation on the encoded vector e:
e     wtranse + btrans

(5)

where, wtrans, btrans are the transformation pa-
rameters. also,
in the encoder we use a bi-
directional lstm (graves et al., 2005) instead of
a uni-directional lstm, as it has been shown to
capture the sequence information more effectively

(ling et al., 2015; ballesteros et al., 2015; bah-
danau et al., 2015). our resultant in   ection gener-
ation model is shown in figure 3.

4.1 supervised learning
the parameters of our model are the set of
character vectors,
the transformation parameters
(wtrans, btrans), and the parameters of the encoder
and decoder lstms (  3.2). we use negative log-
likelihood of the output character sequence as the
id168:

    log p((cid:126)y|(cid:126)x) =    (cid:88)t (cid:48)

log p(yt|e, (cid:126)y<t)

(6)

t=1

we minimize the loss using stochastic updates with
adadelta (zeiler, 2012). this is our purely super-
vised model for in   ection generation and we evalu-
ate it in two different settings as established by pre-
vious work:

factored model.
in the    rst setting, we learn a
separate model for each type of in   ection indepen-
dent of the other possible in   ections. for example,
in case of german nouns, we learn 8, and for ger-
man verbs, we learn 27 individual encoder-decoder
in   ection models (cf. table 3). there is no param-
eter sharing across these models. we call these fac-
tored models of in   ection generation.

joint model.
in the second setting, while learn-
ing a model for an in   ection type, we also use the
information of how the lemma in   ects across all
other in   ection types i.e., the in   ection table of a
root form is used to learn different in   ection mod-
els. we model this, by having the same encoder

kalb<w></w>ekea<w>kk  lber</w>el  eble  be  ee  replm((cid:126)y)

len((cid:126)y) - len((cid:126)x)

same-suf   x((cid:126)y, (cid:126)x)?
same-pre   x((cid:126)y, (cid:126)x)?

p((cid:126)y|(cid:126)x)

levenshtein((cid:126)y, (cid:126)x)
subsequence((cid:126)y, (cid:126)x)?
subsequence((cid:126)x, (cid:126)y)?

table 2: features used to rerank the in   ected outputs. (cid:126)x, (cid:126)y
denote the root and in   ected character sequences resp.

in the encoder-decoder model across all in   ection
models.3 the encoder in our model is learning a
representation of the input character sequence. be-
cause all in   ection models take the same input but
produce different outputs, we hypothesize that hav-
ing the same encoder can lead to better estimates.

4.2 semi-supervised learning

the model we described so far relies entirely on the
availability of pairs of root form and in   ected word
form for learning to generate in   ections. although
such supervised models can be used to obtain in   ec-
tion generation models (durrett and denero, 2013;
ahlberg et al., 2015), it has been shown that unla-
beled data can generally improve the performance
of such systems (ahlberg et al., 2014; nicolai et al.,
2015). the vocabulary of the words of a language
encode information about what correct sequences of
characters in a language look like. thus, we learn
a language model over the character sequences in
a vocabulary extracted from a large unlabeled cor-
pus. we use this language model to make predic-
tions about the next character in the sequence given
the previous characters, in following two settings.

output reranking.
in the    rst setting, we    rst
train the in   ection generation model using the su-
pervised setting as described in   4.1. while mak-
ing predictions for in   ections, we use id125
to generate possible output character sequences and
rerank them using the language model id203
along with other easily extractable features as de-
scribed in table 2. we use pairwise ranking opti-
mization (pro) to learn the reranking model (hop-
kins and may, 2011). the reranker is trained on the
beam output of dev set and evaluated on test set.

3we also tried having the same encoder and decoder across
in   ection types, with just the transformation matrix being dif-
ferent (equ. 5), and observed consistently worse results.

dataset
german nouns (de-n)
german verbs (de-v)
spanish verbs (es-v)
finnish nn & adj. (fi-na)
finnish verbs (fi-v)
dutch verbs (nl-v)
french verbs (fr-v)

root forms
2764
2027
4055
6400
7249
11200
6957

in   .
8
27
57
28
53
9
48

table 3: the number of root forms and types of in   ections
across datasets.

t=1

1
z

(cid:88)t (cid:48)

language model interpolation.
in the second
setting, we interpolate the id203 of observing
the next character according to the language model
with the id203 according to our in   ection gen-
eration model. thus, the id168 becomes:
   log p((cid:126)y|(cid:126)x) =

    log p(yt|e, (cid:126)y<t)
      log plm(yt|(cid:126)y<t) (7)
where plm (yt|(cid:126)y<t) is the id203 of observing
the word yt given the history estimated according to
a language model,        0 is the interpolation pa-
rameter which is learned during training and z is
the id172 factor. this formulation lets us
use any off-the-shelf pre-trained character language
model easily (details in   5).
4.3 ensembling
our id168s (equ. 6 & 7) formulated using a
neural network architecture are non-convex in nature
and are thus dif   cult to optimize. it has been shown
that taking an ensemble of models which were ini-
tialized differently and trained independently leads
to improved performance (hansen and salamon,
1990; collobert et al., 2011). thus, for each model
type used in this work, we report results obtained
using an ensemble of models. so, while decoding
(cid:81)k
we compute the id203 of emitting a charac-
ter as the product-of-experts of the individual mod-
els in the ensemble: pens(yt|  ) = 1
i=1 pi(yt|  )
where, pi(yt|  ) is the id203 according to i-th
model and z is the id172 factor.

1
k

z

5 experiments
we now conduct experiments using the described
models. note that not all previously published mod-

els present results on all settings, and thus we com-
pare our results to them wherever appropriate.

hyperparameters. across all models described
in this paper, we use the following hyperparameters.
in both the encoder and decoder models we use sin-
gle layer lstms with the hidden vector of length
100. the length of character vectors is the size of
character vocabulary according to each dataset. the
parameters are regularized with (cid:96)2, with the regular-
ization constant 10   5.4 the number of models for
ensembling are k = 5. models are trained for at
most 30 epochs and the model with best result on
development set is selected.

5.1 data

durrett and denero (2013) published the wik-
tionary in   ection dataset with training, development
and test splits. the development and test sets con-
tain 200 in   ection tables each and the training sets
consist of the remaining data. this dataset con-
tains in   ections for german, finnish and span-
ish. this dataset was further augmented by (nico-
lai et al., 2015), by adding dutch verbs extracted
from celex lexical database (baayen et al., 1995),
french verbs from verbsite, an online french con-
jugation dictionary and czech nouns and verbs
from the prague dependnecy treebank (haji  c et al.,
2001). as the dataset for czech contains many in-
complete tables, we do not use it for our experi-
ments. these datasets come with pre-speci   ed train-
ing/dev/test splits, which we use(cf. table 3). for
each of these sets, the training data is restricted to
80% of the total in   ection tables, with 10% for de-
velopment and 10% for testing.

for semi-supervised experiments, we train a 5-
gram character language model with witten-bell
smoothing (bell et al., 1990) using the srilm
toolkit (stolcke, 2002). we train the character lan-
guage models on the list of unique word types ex-
tracted from the wikipedia dump for each language
after    ltering out words with characters unseen in
the in   ection generation training dataset. we ob-
tained    2 million unique words for each language.

ddn13 nck15 ours
96.72
88.12
99.81
97.81
95.44
96.71
98.82
96.20

97.50
88.60
99.80
98.10
93.00
96.10
99.20
96.04

94.76
88.31
99.61
97.23
92.14
90.50
98.80
94.47

de-v
de-n
es-v
fi-v
fi-na
nl-v
fr-v
avg.

table 4: individual form prediction accuracy for factored su-
pervised models.

ddn13 afh14 afh15 ours
97.25
88.37
99.86
97.97
94.71
95.63
96.16
98.74
96.15

98.11
89.88
99.92
97.14
93.68
95.74
   
   
   

97.01
87.81
99.52
96.36
91.91
94.53
   
   
   

96.19
88.94
99.67
96.43
93.41
94.93
93.88
98.60
95.30

de-v
de-n
es-v
fi-v
fi-na
avg.
nl-v
fr-v
avg.

table 5: individual form prediction accuracy for joint super-
vised models.

individual

5.2 results
supervised models. the
in   ected
form accuracy for the factored model (  4.1) is shown
in table 4. across datasets, we obtain either com-
parable or better results than nck15 while obtain-
ing on average an accuracy of 96.20% which is
higher than both ddn13 and nck15. our factored
model performs better than ddn13 and nck15
on datasets with large training set (es-v, fi-v, fi-
na, nl-v, fr-v) as opposed to datasets with small
training set (de-n, de-v). in the joint model set-
ting (cf. table 5), on average, we perform better
than ddn13 and afh14 but are behind afh15
by 0.11%. our model improves in performance
over our factored model for de-n, de-v, and es-v,
which are the three smallest training datasets. thus,
parameter sharing across different in   ection types
helps the low-resourced scenarios.5

4using dropout did not improve our results.
5although nck15 provide results in the joint model setting,
they also use raw data in the joint model which makes it incom-
parable to our model and other previous models.

afh14 nck15
97.90
89.90
99.90
98.10
93.60
95.88
96.60
99.20
96.45

de-v
de-n
es-v
fi-v
fi-na
avg.
nl-v
fr-v
avg.
table 6:
semi-supervised models.

97.87
91.81
99.58
96.63
93.82
95.93
   
   
   

interpol rerank
97.11
89.31
99.94
97.62
95.66
95.93
96.64
98.94
96.45

96.79
88.31
99.78
96.66
94.60
95.42
96.66
98.81
96.08

individual form prediction accuracy for factored

model
encoder-decoder
encoder-decoder attention
ours w/o encoder
ours

accuracy
79.08
95.64
84.04
96.20

table 7: avg. accuracy across datasets of the encoder-decoder,
attentional encoder-decoder & our model without encoder.

semi-supervised models. we now evaluate the
utility of character language models in in   ection
generation, in two different settings as described ear-
lier (  4.2). we use the factored model as our base
model in the following experiments as it performed
better than the joint model (cf. table 4 & 5). our
reranking model which uses the character language
model along with other features (cf. table 2) to se-
lect the best answer from a beam of predictions, im-
proves over almost all the datasets with respect to the
supervised model and is equal on average to afh14
and nck15 semi-supervised models with 96.45%
accuracy. we obtain the best reported results on
es-v and fi-na datasets (99.94% and 95.66% re-
spectively). however, our second semi-supervised
model, the interpolation model, on average obtains
96.08% and is surprisingly worse than our super-
vised model (96.20%).

comparison to other architectures. finally it
is of interest how our proposed model compares to
more traditional neural models. we compare our
model against a standard encoder-decoder model,
and an encoder-decoder model with attention, both
trained on root form to in   ected form character
sequences.
in a standard encoder-decoder model
(sutskever et al., 2014), the encoded input sequence

figure 4: plot of in   ection prediction accuracy against the
length of gold in   ected forms. the points are shown with minor
offset along the x-axis to enhance clarity.

vector is fed into the hidden layer of the decoder as
input, and is not available at every time step in con-
trast to our model, where we additionally feed in xt
at every time step as in equ. 4. an attentional model
computes a weighted average of the hidden layer of
the input sequence, which is then used along with
the decoder hidden layer to make a prediction (bah-
danau et al., 2015). these models also do not take
the root form character sequence as inputs to the de-
coder. we also evaluate the utility of having an en-
coder which computes a representation of the input
character sequence in a vector e by removing the en-
coder from our model in figure 3. the results in ta-
ble 7 show that we outperform the encoder-decoder
model, and the model without an encoder substan-
tially. our model is slightly better than the atten-
tional encoder-decoder model, and is simpler as it
does not have the additional attention layer.

6 analysis
length of in   ected forms.
in figure 4 we show
how the prediction accuracy of an in   ected form
varies with respect to the length of the correct in-
   ected form.to get stable estimates, we bin the in-
   ected forms according to their length: < 5, [5, 10),
[10, 15), and     15. the accuracy for each bin is
macro-averaged across 6 datasets6 for our factored
model and the best models of ddn13 and nck15.
6we remove de-n as its the smallest and shows high vari-

ance in results.

5101520word length9092949698100accuracy (%)oursddn13nck15our model consistently shows improvement in per-
formance as word length increases and is signi   -
cantly better than ddn13 on words of length more
than 20 and is approximately equal to nck15. on
words of length < 5, we perform worse than ddn13
but better than nck15. on average, our model has
the least error margin across bins of different word
length as compared to both ddn13 and nck15.
using lstms in our model helps us make better
predictions for long sequences, since they have the
ability to capture long-range dependencies.

finnish vowel harmony. our model obtains the
current best result on the finnish noun and adjective
dataset, this dataset has the longest in   ected words,
some of which are > 30 characters long. finnish ex-
hibits vowel harmony, i.e, the occurrence of a vowel
is controlled by other vowels in the word. finnish
vowels are divided into three groups: front (  a,   o,
y), back (a, o, u), and neutral (e, i). if back vow-
els are present in a stem, then the harmony is back
(i.e, front vowels will be absent), else the harmony is
front (i.e, back vowels will be absent). in compound
words the suf   x harmony is determined by the    nal
stem in the compound. for example, our model cor-
rectly in   ects the word fasisti (fascist) to obtain fa-
sisteissa and the compound t  arkkelyspitoinen (starch
containing) to t  arkkelyspitoisissa. the ability of our
model to learn such relations between these vowels
helps capture vowel harmony. for fi-na, our model
obtains 99.87% for correctly predicting vowel har-
mony, and nck15 obtains 98.50%.we plot the char-
acter vectors of these finnish vowels (cf. figure 5)
using id167 projection (van der maaten and hin-
ton, 2008) and observe that the vowels are correctly
grouped with visible transition from the back to the
front vowels.

7 related work

similar to the encoder in our framework, rastogi et
al. (2016) extract sub-word features using a forward-
backward lstm from a word, and use them in a tra-
ditional weighted fst to generate in   ected forms.
neural encoder-decoder models of string transduc-
tion have also been used for sub-word level transfor-
mations like grapheme-to-phoneme conversion (yao
and zweig, 2015; rao et al., 2015).

generation of in   ectional morphology has been

figure 5: plot of character vectors of finnish vowels. their
organization shows that front, back and neutral vowel groups
have been discovered. the arrows show back and front vowel
correspondences.

particularly useful
in statistical machine transla-
tion, both in translation from morphologically rich
languages (goldwater and mcclosky, 2005), and
into morphologically rich languages (minkov et al.,
2007; toutanova et al., 2008; clifton and sarkar,
2011; fraser et al., 2012). modeling the morpholog-
ical structure of a word has also shown to improve
the quality of word clusters (clark, 2003) and word
vector representations (cotterell and sch  utze, 2015).
in   ection generation is complementary to the task
of morphological and phonological segmentation,
where the existing word form needs to be segmented
to obtained meaningful sub-word units (creutz and
lagus, 2005; snyder and barzilay, 2008; poon et
al., 2009; narasimhan et al., 2015; cotterell et al.,
2015; cotterell et al., 2016). an additional line of
work that bene   ts from implicit modeling of mor-
phology is neural character-based natural language
processing, e.g., part-of-speech tagging (santos and
zadrozny, 2014; ling et al., 2015) and dependency
parsing (ballesteros et al., 2015). these models
have been successful when applied to morphologi-
cally rich languages, as they are able to capture word
formation patterns.

8 conclusion

we have presented a model that generates in   ected
forms of a given root form using a neural network
sequence to sequence string transducer. our model
obtains state-of-the-art results and performs at par or

better than existing in   ection generation models on
seven different datasets. our model is able to learn
long-range dependencies within character sequences
for in   ection generation which makes it specially
suitable for morphologically rich languages.

acknowledgements

we thank mans hulden for help in explaining
finnish vowel harmony, and garrett nicolai for
making the output of his system available for com-
parison. this work was sponsored in part by the
national science foundation through award iis-
1526745.

references
[ahlberg et al.2014] malin ahlberg, markus forsberg,
and mans hulden. 2014. semi-supervised learning
of morphological paradigms and lexicons. in proc. of
eacl.

[ahlberg et al.2015] malin ahlberg, markus forsberg,
and mans hulden. 2015. paradigm classi   cation in
supervised learning of morphology. proc. of naacl.
[baayen et al.1995] harald r. baayen, richard piepen-
brock, and leon gulikers. 1995. the celex lexical
database. release 2 (cd-rom). ldc, university of
pennsylvania.

[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio.
2015. neural machine
translation by jointly learning to align and translate.
in proc. of iclr.

[ballesteros et al.2015] miguel ballesteros, chris dyer,
and noah a. smith. 2015. improved transition-based
parsing by modeling characters instead of words with
lstms. in proc. of emnlp.

[bell et al.1990] timothy c bell, john g cleary, and
ian h witten. 1990. text compression. prentice-hall,
inc.

[bergroth et al.2000] lasse bergroth, harri hakonen, and
timo raita. 2000. a survey of longest common sub-
sequence algorithms. in proc. of spire.

[chahuneau et al.2013a] victor

eva
schlinger, noah a. smith, and chris dyer. 2013a.
translating into morphologically rich languages with
synthetic phrases. in proc. of emnlp.

chahuneau,

[chahuneau et al.2013b] victor chahuneau, noah a
smith, and chris dyer. 2013b. knowledge-rich mor-
phological priors for bayesian language models.
in
proc. of naacl.

[cho et al.2014] kyunghyun cho, bart van merrienboer,
caglar gulcehre, dzmitry bahdanau, fethi bougares,

holger schwenk, and yoshua bengio. 2014. learning
phrase representations using id56 encoder   decoder for
id151. in proc. of emnlp.

[chomsky and halle1968] n. chomsky and m. halle.
1968. the sound pattern of english. harper & row,
new york, ny.

[clark2003] alexander clark.

2003. combining dis-
tributional and morphological information for part of
speech induction. in proc. of eacl.

[clifton and sarkar2011] ann clifton and anoop sarkar.
2011. combining morpheme-based machine transla-
tion with post-processing morpheme prediction.
in
proc. of acl.

[collobert et al.2011] ronan collobert, jason weston,
l  eon bottou, michael karlen, koray kavukcuoglu,
and pavel kuksa. 2011. natural language process-
ing (almost) from scratch. the journal of machine
learning research, 12:2493   2537.

[cotterell and sch  utze2015] ryan cotterell and hinrich
sch  utze. 2015. morphological word-embeddings. in
proc. of naacl.

[cotterell et al.2015] ryan cotterell, nanyun peng, and
jason eisner. 2015. modeling word forms using latent
underlying morphs and phonology. transactions of
the association for computational linguistics, 3:433   
447.

[cotterell et al.2016] ryan cotterell, tim vieria, and
hinrich sch  utze. 2016. a joint model of orthography
and morphological segmentation. in proc. of naacl.
[creutz and lagus2005] mathias creutz and krista la-
gus. 2005. unsupervised morpheme segmentation
and morphology induction from text corpora using
morfessor 1.0. helsinki university of technology.

[dreyer and eisner2011] markus dreyer and jason eis-
ner.
2011. discovering morphological paradigms
from plain text using a dirichlet process mixture
model. in proc. of emnlp.

[durrett and denero2013] greg durrett and john den-
ero. 2013. supervised learning of complete morpho-
logical paradigms. in proc. of naacl.

[eisner2002] jason eisner. 2002. parameter estimation
in proc. of

for probabilistic    nite-state transducers.
acl.

[fraser et al.2012] alexander fraser, marion weller,
aoife cahill, and fabienne cap. 2012. modeling
in proc. of
in   ection and word-formation in smt.
eacl.

[goldwater and mcclosky2005] sharon goldwater and
david mcclosky. 2005.
improving statistical mt
through morphological analysis. in proc. of emnlp,
pages 676   683.

[graves et al.2005] alex graves, santiago fern  andez,
and j  urgen schmidhuber. 2005. bidirectional lstm

networks for improved phoneme classi   cation and
recognition. in proc. of icann.

[graves2013] alex graves.

2013.

quences with recurrent neural networks.
abs/1308.0850.

generating se-
corr,

[haji  c et al.2001] jan haji  c, barbora vidov  a-hladk  a, and
petr pajas. 2001. the prague dependency treebank:
annotation structure and support. in proc. of the ircs
workshop on linguistic databases.

[hansen and salamon1990] lars kai hansen and peter
salamon. 1990. neural network ensembles. in proc.
of pami.

[hochreiter and schmidhuber1997] sepp hochreiter and
j  urgen schmidhuber. 1997. long short-term memory.
neural computation, 9(8):1735   1780.

[hopkins and may2011] mark hopkins and jonathan

may. 2011. tuning as ranking. in proc. of emnlp.

[hulden2014] mans hulden. 2014. generalizing in   ec-
tion tables into paradigms with    nite state operations.
in proc. of the joint meeting of sigmorphon and
sigid122.

[kaplan and kay1994] ronald m kaplan and martin
kay. 1994. regular models of phonological rule sys-
tems. computational linguistics, 20(3):331   378.

[koskenniemi1983] kimmo koskenniemi. 1983. two-
level morphology: a general computational model for
word-form recognition and production. university of
helsinki.

[ling et al.2015] wang ling, tiago lu    s, lu    s marujo,
r  amon fernandez astudillo, silvio amir, chris dyer,
alan w black, and isabel trancoso. 2015. find-
ing function in form: compositional character models
for open vocabulary word representation. in proc. of
emnlp.

[minkov et al.2007] einat minkov, kristina toutanova,
and hisami suzuki. 2007. generating complex mor-
phology for machine translation. in proc. of acl.

[narasimhan et al.2015] karthik narasimhan, regina
barzilay, and tommi jaakkola. 2015. an unsuper-
vised method for uncovering morphological chains.
tacl.

[nicolai et al.2015] garrett nicolai, colin cherry, and
grzegorz kondrak. 2015. in   ection generation as dis-
criminative string transduction. in proc. of naacl.

[o   azer et al.2004] kemal o   azer,   ozlem c  etino  glu, and
bilge say. 2004. integrating morphology with multi-
word expression processing in turkish. in proc. of the
workshop on multiword expressions.

[o   azer1996] kemal o   azer. 1996. error-tolerant    nite-
state recognition with applications to morphological
analysis and id147. computational lin-
guistics, 22(1):73   89.

[oncina and sebban2006] jose oncina and marc sebban.
2006. learning stochastic id153: application
in handwritten character recognition. pattern recogni-
tion, 39(9):1575   1587.

[poon et al.2009] hoifung poon, colin cherry,

and
kristina toutanova. 2009. unsupervised morpholog-
ical segmentation with id148. in proc. of
naacl.

[rao et al.2015] kanishka rao, fuchun peng, hasim sak,
and franc  oise beaufays.
grapheme-to-
phoneme conversion using long short-term memory
recurrent neural networks. in proc. of icassp.

2015.

[rastogi et al.2016] pushpendre rastogi, ryan cotterell,
and jason eisner. 2016. weighting    nite-state trans-
ductions with neural context. in proc. of naacl.

[santos and zadrozny2014] cicero d. santos and bianca
zadrozny. 2014. learning character-level representa-
tions for part-of-speech tagging. in proc. of icml.

[sarawagi and cohen2004] sunita

william w cohen.
tional random    elds for information extraction.
proc. of nips.

2004.

sarawagi

and
semi-markov condi-
in

[snyder and barzilay2008] benjamin snyder and regina
barzilay. 2008. unsupervised multilingual learning
in in the annual
for morphological segmentation.
conference of the.

[stolcke2002] andreas stolcke. 2002. srilm-an extensi-
ble id38 toolkit. in proc. of interspeech.
[sutskever et al.2014] ilya sutskever, oriol vinyals, and
quoc vv le. 2014. sequence to sequence learning
with neural networks. in proc. of nips.

[toutanova et al.2008] kristina

hisami
suzuki, and achim ruopp. 2008. applying mor-
phology generation models to machine translation. in
proc. of acl, pages 514   522.

toutanova,

[van der maaten and hinton2008] laurens

der
maaten and geoffrey hinton. 2008. visualizing data
using id167. journal of machine learning research,
9:2579   2605.

van

[wicentowski2004] richard wicentowski. 2004. multi-
lingual noise-robust supervised morphological analy-
sis using the wordframe model. in proc. of sigphon.
[yao and zweig2015] kaisheng yao and geoffrey zweig.
sequence-to-sequence neural net models
in proc. of

2015.
for grapheme-to-phoneme conversion.
icassp.

[yarowsky and wicentowski2000] david yarowsky and
richard wicentowski. 2000. minimally supervised
morphological analysis by multimodal alignment. in
proc. of acl.

[zeiler2012] matthew d zeiler.

an adaptive learning rate method.
arxiv:1212.5701.

2012.

adadelta:
arxiv preprint

