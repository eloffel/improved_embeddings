6
1
0
2

 
r
a

m
1

 

 
 
]

g
l
.
s
c
[
 
 

6
v
1
6
3
6
0

.

1
1
5
1
:
v
i
x
r
a

published as a conference paper at iclr 2016

order-embeddings of images and language

ivan vendrov, ryan kiros, sanja fidler, raquel urtasun
department of computer science
university of toronto
{vendrov,rkiros,fidler,urtasun}@cs.toronto.edu

abstract

hypernymy, id123, and image captioning can be seen as special cases
of a single visual-semantic hierarchy over words, sentences, and images. in this
paper we advocate for explicitly modeling the partial order structure of this hierar-
chy. towards this goal, we introduce a general method for learning ordered repre-
sentations, and show how it can be applied to a variety of tasks involving images
and language. we show that the resulting representations improve performance
over current approaches for hypernym prediction and image-caption retrieval.

1

introduction

id161 and natural language processing are becoming increasingly intertwined. recent
work in vision has moved beyond discriminating between a    xed set of object classes, to automati-
cally generating open-ended lingual descriptions of images (vinyals et al., 2015). recent methods
for natural language processing such as young et al. (2014) learn the semantics of language by
grounding it in the visual world. looking to the future, autonomous arti   cial agents will need to
jointly model vision and language in order to parse the visual world and communicate with people.
but what, precisely, is the relationship between images and the words or captions we use to describe
them? it is akin to the hypernym relation between words, and id123 among phrases:
captions are simply abstractions of images. in fact, all three relations can be seen as special cases
of a partial order over images and language, illustrated in figure 1, which we refer to as the visual-
semantic hierarchy. as a partial order, this relation is transitive:    woman walking her dog   ,    woman
walking   ,    person walking   ,    person   , and    entity    are all valid abstractions of the rightmost image.
our goal in this work is to learn representations that respect this partial order structure.

figure 1: a slice of the visual-semantic hierarchy

most recent approaches to modeling the hypernym, entailment, and image-caption relations involve
learning distributed representations or embeddings. this is a very powerful and general approach
which maps the objects of interest   words, phrases, images    to points in a high-dimensional vector
space. one line of work, exempli   ed by chopra et al. (2005) and    rst applied to the caption-image
relationship by socher et al. (2014), requires the mapping to be distance-preserving: semantically

1

published as a conference paper at iclr 2016

similar objects are mapped to points that are nearby in the embedding space. a symmetric distance
measure such as euclidean or cosine distance is typically used. since the visual-semantic hierarchy
is an antisymmetric relation, we expect this approach to introduce systematic model error.
other approaches do not have such explicit constraints, learning a more-or-less general binary rela-
tion between the objects of interest, e.g. bordes et al. (2011); socher et al. (2013); ma et al. (2015).
notably, no existing approach directly imposes the transitivity and antisymmetry of the partial order,
leaving the model to induce these properties from data.
in contrast, we propose to exploit the partial order structure of the visual-semantic hierarchy by
learning a mapping which is not distance-preserving but order-preserving between the visual-
semantic hierarchy and a partial order over the embedding space. we call embeddings learned
in this way order-embeddings. this idea can be integrated into existing relational learning meth-
ods simply by replacing their comparison operation with ours. by modifying existing methods in
this way, we    nd that order-embeddings provide a marked improvement over the state-of-art for
hypernymy prediction and caption-id162, and near state-of-the-art performance for natural
language id136.
this paper is structured as follows. we begin, in section 2, by giving a uni   ed mathematical treat-
ment of our tasks, and describing the general approach of learning order-embeddings. in the next
three sections we describe in detail the tasks we tackle, how we apply the order-embeddings idea
to each of them, and the results we obtain. the tasks are hypernym prediction (section 3), caption-
id162 (section 4), and id123 (section 5).
in the supplementary material, we visualize novel vector regularities that emerge in our learned
embeddings of images and language.

2 learning order-embeddings

to unify our treatment of various tasks, we introduce the problem of partial order completion. in
partial order completion, we are given a set of positive examples p = {(u, v)} of ordered pairs
drawn from a partially ordered set (x,(cid:22)x ), and a set of negative examples n which we know to
be unordered. our goal is to predict whether an unseen pair (u(cid:48), v(cid:48)) is ordered. note that hypernym
prediction, caption-id162, and id123 are all special cases of this task, since
they all involve classifying pairs of concepts in the (partially ordered) visual-semantic hierarchy.
we tackle this problem by learning a mapping from x into a partially ordered embedding space
(y,(cid:22)y ). the idea is to predict the ordering of an unseen pair in x based on its ordering in the
embedding space. this is possible only if the mapping satis   es the following crucial property:
de   nition 1. a function f : (x,(cid:22)x )     (y,(cid:22)y ) is an order-embedding if for all u, v     x,

u (cid:22)x v if and only if f (u) (cid:22)y f (v)

this de   nition implies that each combination of embedding space y , order (cid:22)y , and order-
embedding f determines a unique completion of our data as a partial order (cid:22)x. in the following,
we    rst consider the choice of y and (cid:22)y , and then discuss how to    nd an appropriate f.

+

2.1 the reversed product order on rn
the choice of y and (cid:22)y is somewhat application-dependent. for the purpose of modeling the
semantic hierarchy, our choices are narrowed by the following considerations.
much of the expressive power of human language comes from abstraction and composition. for any
two concepts, say    dog    and    cat   , we can name a concept that is an abstraction of the two, such
as    mammal   , as well as a concept that composes the two, such as    dog chasing cat   . so, in order
to represent the visual-semantic hierarchy, we need to choose an order (cid:22)y that is rich enough to
embed these two relations.
we also restrict ourselves to orders (cid:22)y with a top element, which is above every other element in the
order. in the visual-semantic hierarchy, this element represents the most general possible concept;
practically, it provides an anchor for the embedding.

2

published as a conference paper at iclr 2016

n(cid:94)

finally, we choose the embedding space y to be continuous in order to allow optimization with
gradient-based methods.
a natural choice that satis   es all three properties is the reversed product order on rn
the conjunction of total orders on each coordinate:

+ , de   ned by

x (cid:22) y if and only if

xi     yi

(1)

i=1

for all vectors x, y with nonnegative coordinates. note the reversal of direction: smaller coordinates
imply higher position in the partial order. the origin is then the top element of the order, representing
the most general concept.
instead of viewing our embeddings as single points x     rn
+ , we can also view them as sets {y :
x (cid:22) y}. the meaning of a word is then the union of all concepts of which it is a hypernym, and the
meaning of a sentence is the union of all sentences that entail it. the visual-semantic hierarchy can
then be seen as a special case of the subset relation, a connection also used by young et al. (2014).

2.2 penalizing order violations

having    xed the embedding space and order, we now consider the problem of    nding an order-
embedding into this space. in practice, the order embedding condition (de   nition 1) is too restrictive
to impose as a hard constraint. instead, we aim to    nd an approximate order-embedding: a mapping
which violates the order-embedding condition, imposed as a soft constraint, as little as possible.
more precisely, we de   ne a penalty that measures the degree to which a pair of points violates the
product order. in particular, we de   ne the penalty for an ordered pair (x, y) of points in rn

e(x, y) = || max(0, y     x)||2

(2)
crucially, e(x, y) = 0        x (cid:22) y according to the reversed product order; if the order is not
satis   ed, e(x, y) is positive. this effectively imposes a strong prior on the space of relations, en-
couraging our learned relation to satisfy the partial order properties of transitivity and antisymmetry.
this penalty is key to our method. throughout the remainder of the paper, we will use it where
previous work has used symmetric distances or learned comparison operators.
recall that p and n are our positive and negative examples, respectively. then, to learn an approx-
imate order-embedding f, we could use a max-margin loss which encourages positive examples to
have zero penalty, and negative examples to have penalty greater than a margin:

+ as

e(f (u), f (v)) +

max{0,        e(f (u(cid:48)), f (v(cid:48)))}

(3)

(cid:88)

(u(cid:48),v(cid:48))   n

(cid:88)

(u,v)   p

in practice we are often not given negative examples, in which case this loss admits the trivial
solution of mapping all objects to the same point. the best way of dealing with this problem depends
on the application, so we will describe task-speci   c variations of the loss in the next several sections.

3 hypernym prediction

to test the ability of our model to learn partial orders from incomplete data, our    rst task is to predict
withheld hypernym pairs in id138 (miller, 1995). a hypernym pair is a pair of concepts where
the    rst concept is a specialization or an instance of the second, e.g., (woman, person) or (new york,
city). our setup differs signi   cantly from previous work in that we use only the id138 hierarchy
as training data. the most similar evaluation has been that of baroni et al. (2012), who use external
linguistic data in the form of distributional semantic vectors. bordes et al. (2011) and socher et al.
(2013) also evaluate on the id138 hierarchy, but they use other relations in id138 as training
data (and external linguistic data, in socher   s case).
additionally, the latter two consider only direct hypernyms, rather than the full, transitive hyper-
nymy relation. but predicting the transitive hypernym relation is a better-de   ned problem because
individual hypernym edges in id138 vary dramatically in the degree of abstraction they require.
for instance, (person, organism) is a direct hypernym pair, but it takes eight hypernym edges to get
from cat to organism.

3

published as a conference paper at iclr 2016

3.1 id168

to apply order-embeddings to hypernymy, we follow the setup of socher et al. (2013) in learning
an n-dimensional vector for each concept in id138, but we replace their neural tensor network
with our order-violation penalty de   ned in eq. (2). just like them, we corrupt each hypernym pair
by replacing one of the two concepts with a randomly chosen concept, and use these corrupted
pairs as negative examples for both training and evaluation. we use their max-margin loss, which
encourages the order-violation penalty to be zero for positive examples, and greater than a margin   

e(f (u), f (v)) + max{0,        e(f (u(cid:48)), f (v(cid:48)))}

(4)

for negative examples: (cid:88)

(u,v)   w ordn et

where e is our order-violation penalty, and (u(cid:48), v(cid:48)) is a corrupted version of (u, v). since we learn
an independent embedding for each concept, the mapping f is simply a lookup table.

3.2 dataset

the transitive closure of the id138 hierarchy gives us 838073 edges between 82192 concepts in
id138. like bordes et al. (2011), we randomly select 4000 edges for the test split, and another
4000 for the development set. note that the majority of test set edges can be inferred simply by
applying transitivity, giving us a strong baseline.

3.3 details of training

we learn a 50-dimensional nonnegative vector for each concept in id138 using the max-margin
objective (4) with margin    = 1, sampling 500 true and 500 false hypernym pairs in each batch. we
train for 30-50 epochs using the adam optimizer (kingma & ba, 2015) with learning rate 0.01 and
early stopping on the validation set. during evaluation, we    nd the optimal classi   cation threshold
on the validation set, then apply it to the test set.

3.4 results

since our setup is novel, there are no published numbers
to compare to. we therefore compare three variants of our
model to two baselines, with results shown in table 1.
the transitive closure baseline involves no learning; it
simply classi   es hypernyms pairs as positive if they are
in the transitive closure of the union of edges in the train-
ing and validation sets. the word2gauss baseline eval-
uates the approach of vilnis & mccallum (2015) to rep-
resent words as gaussian densities rather than points in
the embedding space. this allows a natural representa-
tion of hierarchies using the kl divergence. we used
50-dimensional diagonal gaussian embeddings, trained
for 200 epochs on a max-margin objective with margin
7, chosen by grid search1.
order-embeddings (symmetric) is our full model, but
using symmetric cosine distance instead of our asymmet-
ric penalty. order-embeddings (bilinear) replaces our
penalty with the bilinear model used by socher et al.
(2013). order-embeddings is our full model.
only our full model can do better than the transitive baseline, showing the value of exploiting partial
order structure in contrast to using symmetric similarity or learning a general binary relation as most
previous work and our bilinear baseline do.
the resulting 50-dimensional embeddings are dif   cult to visualize. to give some intuition for the
structure being learned, figure 2 shows the results of a toy 2d experiment.

figure 2:
2-dim order-embedding of a
small subset of the id138 hypernym rela-
tion. all the true hypernym pairs (green ar-
rows) are correctly embedded, but two spu-
rious pairs (pink arrows), are introduced.
only direct hypernyms are shown.

1we used the code of http://github.com/seomoz/word2gauss

4

published as a conference paper at iclr 2016

method

accuracy (%)

transitive closure
word2gauss
order-embeddings (symmetric)
order-embeddings (bilinear)
order-embeddings

88.2
86.6
84.2
86.3
90.6

table 1: binary classi   cation accuracy on 4000 withheld edges from id138.

4 caption-id162

the caption-id162 task has become a standard evaluation of joint models of vision and
language (hodosh et al., 2013; lin et al., 2014a). the task involves ranking a large dataset of
images by relevance for a query caption (id162), and ranking captions by relevance for
a query image (caption retrieval). given a set of aligned image-caption pairs as training data, the
goal is then to learn a caption-image compatibility score s(c, i) to be used at test time.
many modern approaches model the caption-image relationship symmetrically, either by embedding
into a common    visual-semantic    space with inner-product similarity (socher et al., 2014; kiros
et al., 2014), or by using canonical correlations analysis between distributed representations of
images and captions (klein et al., 2015). while karpathy & li (2015) and plummer et al. (2015)
model a    ner-grained alignment between regions in the image and segments of the caption, the
similarity they use is still symmetric. an alternative is to learn an unconstrained binary relation,
either with a neural language model conditioned on the image (vinyals et al., 2015; mao et al.,
2015) or using a multimodal id98 (ma et al., 2015).
in contrast to these lines of work, we propose to treat the caption-image pairs as a two-level partial
order with captions above the images they describe, and let

s(c, i) =    e(fi(i), fc(c))

with e our order-violation penalty de   ned in eq (2), and fc, fi are embedding functions from cap-
+ .
tions and images into rn

4.1 id168

to facilitate comparison, we use the same pairwise ranking loss that socher et al. (2014), kiros et al.
(2014) and karpathy & li (2015) have used on this task   simply replacing their symmetric similar-
ity measure with our asymmetric order-violation penalty. this id168 encourages s(c, i) for
ground truth caption-image pairs to be greater than that for all other pairs, by a margin:

max{0,        s(c, i) + s(c(cid:48), i)} +

max{0,        s(c, i) + s(c, i(cid:48))}

(5)

(c,i)

c(cid:48)

i(cid:48)

where (c, i) is a ground truth caption-image pair, c(cid:48) goes over captions that no describe i, and i(cid:48) goes
over image not described by c.

4.2

image and caption embeddings

to learn fc and fi, we use the approach of kiros et al. (2014) except, since we are embedding into
rn
+ , we constrain the embedding vectors to have nonnegative entries by taking their absolute value.
thus, to embed images, we use
(6)
where wi is a learned n    4096 matrix, n being the dimensionality of the embedding space.
cn n (i) is the same image feature used by klein et al. (2015): we rescale images to have smallest
side 256 pixels, we take 224    224 crops from the corners, center, and their horizontal re   ections,
run the 10 crops through the 19-layer vgg network of simonyan & zisserman (2015) (weights
pre-trained on id163 and    xed during training), and average their fc7 features.

fi(i) = |wi    cn n (i)|

5

(cid:32)(cid:88)

(cid:88)

(cid:88)

(cid:33)

published as a conference paper at iclr 2016

model

caption retrieval

r@1 r@10 med

r

mean

r

id162

r@1 r@10 med

r

mnlm (kiros et al., 2014)
m-id56 (mao et al., 2015)
dvsa (karpathy & li,
2015)
stv (kiros et al., 2015)
fv (klein et al., 2015)
m-id98 (ma et al., 2015)
m-id98en s
order-embeddings (reversed)
order-embeddings (1-crop)
order-embeddings (symm.)
order-embeddings

dvsa
fv
order-embeddings (symm.)
order-embeddings

43.4
41.0
38.4

33.8
39.4
38.3
42.8
11.2
41.4
45.4
46.7

11.8
17.3
21.5
23.3

85.8
83.5
80.5

82.1
80.9
81.0
84.1
44.0
84.2
88.7
88.9

45.4
50.2
62.9
65.0

2
2
1

3
2
2
2

14.2
2.0
2.0
2.0

12.2
10.0
6.0
5.0

1k test images
31.0
29.0
27.4

*
*
*

*

*
*

10.4

25.9
25.1
27.4
32.6
12.3
86.6
33.5
8.7
36.3
5.8
5.7
37.9
5k test images

*

46.4
24.4
24.4

8.9
10.8
16.8
18.0

79.9
77.0
74.8

74.6
76.6
79.5
82.8
53.5
82.2
85.8
85.9

36.3
40.1
56.3
57.6

3
3
3

4
4
3
3
9.0
2.6
2.0
2.0

19.5
17.0
8.0
7.0

mean

r

*
*
*

11.1

*

*
*

30.1
10.0
9.0
8.1

*

49.3
40.4
35.9

table 2: results of caption-id162 evaluation on coco. r@k is recall@k, in %. med r
is median rank. metrics for our models on 1k test images are averages over    ve 1000-image splits of
the 5000-image test set, as in (klein et al., 2015). best results overall are in bold; best results using
1-crop vgg features are underlined.

to embed the captions, we use a recurrent neural net encoder with gru activations (cho et al.,
2014), so fc(c) = |gru (c)|, the absolute value of hidden state after processing the last word.

4.3 dataset

we evaluate on the microsoft coco dataset (lin et al., 2014b), which has over 120,000 images,
each with at least    ve human-annotated captions per image. this is by far the largest dataset com-
monly used for caption-id162. we use the data splits of karpathy & li (2015) for training
(113,287 images), validation (5000 images), and test (5000 images).

4.4 details of training

to train the model, we use the standard pairwise ranking objective from eq. (5). we sample mini-
batches of 128 random image-caption pairs, and draw all contrastive terms from the minibatch,
giving us 127 contrastive images for each caption and captions for each image. we train for 15-30
epochs using the adam optimizer with learning rate 0.001, and early stopping on the validation set.
we set the dimension of the embedding space and the gru hidden state n to 1024, the dimension of
the learned id27s to 300, and the margin    to 0.05. all these hyperparameters, as well
as the learning rate and batchsize, were selected using the validation set. for consistency with kiros
et al. (2014) and to mitigate over   tting, we constrain the caption and image embeddings to have unit
l2 norm. this constraint implies that no two points can be exactly ordered with zero order-violation
penalty, but since we use a ranking loss, only the relative size of the penalties matters.

4.5 results

given a query caption or image, we sort all the images or captions of the test set in order of increasing
penalty. we use standard ranking metrics for evaluation. we measure recall@k, the percent of
queries for which the gt term is one of the    rst k retrieved; and median and mean rank, which are
statistics over the position of the gt term in the retrieval order.

6

published as a conference paper at iclr 2016

table 2 shows a comparison between all state-of-the-art and some older methods2 along with our
own; see ma et al. (2015) for a more complete listing.
the best results overall are in bold, and the best results using 1-crop vgg image features are under-
lined. note that the comparison is additionally complicated by the following:

    m-id98en s is an ensemble of four different models, whereas the other entries are all

single models.

    stv and fv use external text corpora to learn their language features, whereas the other

methods learn them from scratch.

to facilitate the comparison and to evaluate the contributions of various components of our model,
we evaluate four variations of order-embeddings:
order-embeddings is our full model as described above.
order-embeddings (reversed) reverses the order of captions and image embeddings in our order-
violation penalty   placing images above captions in the partial order learned by our model. this
seemingly slight variation performs atrociously, con   rming our prior that captions are much more
abstract than images, and should be placed higher in the semantic hierarchy.
order-embeddings (1-crop) computes the image feature using just the center crop, instead of aver-
aging over 10 crops.
order-embeddings (symm.) replaces our asymmetric penalty with the symmetric cosine distance,
and allows embedding coordinates to be negative   essentially replicating mnlm, but with better
image features. here we    nd that a different margin (   = 0.2) works best.
between these four models, the only previous work whose results are incommensurable with ours is
dvsa, since it uses the less discriminative id98 of krizhevsky et al. (2012) but 20 region features
instead of a single whole-image feature.
aside from this limitation, and if only single models are considered, order-embeddings signi   cantly
outperform the state-of-art approaches for id162 even when we control for image features.

4.6 exploration

why would order-embeddings do well on such a shallow partial order? why are they much more
helpful for id162 than for caption retrieval?
intuitively, symmetric similarity should fail when an image has captions with very different levels
of detail, because the captions are so dissimilar that it is impossible to map both their embeddings
close to the same image embedding. order-embeddings don   t have this problem: the less detailed
caption can be embedded very far away from the image while remaining above it in the partial order.
to evaluate this intuition, we use caption length as a proxy for level of detail and select, among pairs
of co-referring captions in our validation set, the 100 pairs with the biggest length difference. for
id162 with 1000 target images, the mean rank over captions in this set is 6.4 for order-
embeddings and 9.7 for cosine similarity, a much bigger difference than over the entire dataset.
some particularly dramatic examples of this are shown in figure 3. moreover, if we use the shorter
caption as a query, and retrieve captions in order of increasing error, the mean rank of the longer
caption is 34.0 for order-embeddings and 47.6 for cosine similarity, showing that order-embeddings
are able to capture the relatedness of co-referring captions with very different lengths.
this also explains why order-embeddings provide a much smaller improvement for caption retrieval
than for id162: all the caption retrieval metrics are based on the position of the    rst ground
truth caption in the retrieval order, so the embeddings need only learn to retrieve one of each image   s
   ve captions well, which symmetric similarity is well suited for.

2note that the numbers for mnlm come not from the published paper but from the recently released code

at http://github.com/ryankiros/visual-semantic-embedding.

7

published as a conference paper at iclr 2016

captions

image rank

cosine

order-emb

a sitting area with furniture and    owers makes a back-
drop for a boy with headphones sitting in the fore-
ground at one of the chairs at a dining table that holds
glasses and a handbag working at a laptop

a kid is wearing headphone while on a laptop

view of top of a white building with tan speckled area
an uncovered awning with a pigeon in    ght below and
a red umbrella behind balcony wall

a pigeon    ying near white beams of a building

4

286

3

91

8

24

5

6

figure 3: images with captions of very different lengths, and the rank of the gt image when using
each caption as a query.

5 id123 / natural language id136

natural language id136 can be seen as a generalization of hypernymy from words to sentences.
for example, from    woman walking her dog in a park    we can infer both    woman walking her dog   
and    dog in a park   , but not    old woman    or    black dog   . given a pair of sentences, our task is to
predict whether we can infer the second sentence (the hypothesis) from the    rst (the premise).

5.1 id168

(cid:88)

(cid:88)

to apply order-embeddings to this task, we again view it as partial order completion   we can infer
a hypothesis from a premise exactly when the hypothesis is above the premise in the visual-semantic
hierarchy.
unlike our other tasks, for which we had to generate contrastive negatives, datasets for natural
language id136 include labeled negative examples. so, we can simply use a max-margin loss:

e(f (p), f (h)) +

max{0,        e(f (p(cid:48)), f (h(cid:48)))}

(7)

(p,h)

(p(cid:48),h(cid:48))

where (p, h) are positive and (p(cid:48), h(cid:48)) negative pairs of premise and hypothesis. to embed sentences,
we use the same gru encoder as in the caption-id162 task.

5.2 dataset

to evaluate order-embeddings on the natural language id136 task, we use the recently proposed
snli corpus (bowman et al., 2015), which contains 570,000 pairs of sentences, each labeled with
   entailment    if the id136 is valid,    contradiction    if the two sentences contradict, or    neutral   
if the id136 is invalid but there is no contradiction. our method only allows us to discrimi-
nate between entailment and non-entailment, so we merge the    contradiction    and    neutral    classes
together to serve as our negative examples.

5.3

implementation details

just as for caption-image ranking, we set the dimensions of the embedding space and gru hidden
state to be 1024, the dimension of the id27s to be 300, and constrain the embeddings to
have unit l2 norm. we train for 10 epochs with batches of 128 sentence pairs. we use the adam
optimizer with learning rate 0.001 and early stopping on the validation set. during evaluation, we
   nd the optimal classi   cation threshold on validation, then use the threshold to classify the test set.

8

published as a conference paper at iclr 2016

method

2-class

3-class

neural attention (rockt  aschel et al., 2015)
eop classi   er (bowman et al., 2015)
skip-thoughts
order-embeddings (symmetric)
order-embeddings

*

75.0
87.7
79.3
88.6

table 3: test accuracy (%) on snli.

83.5
*

81.5

*
*

5.4 results

the state-of-the-art method for 3-class classi   cation on snli is that of rockt  aschel et al. (2015).
unfortunately, they do not compute 2-class accuracy, so we cannot compare to them directly.
as a bridge to facilitate comparison, we use a challenging baseline which can be evaluated on both
the 2-class and 3-class problems. the baseline, referred to as skip-thoughts, involves a feedfor-
ward neural network on top of skip-thought vectors (kiros et al., 2015), a state-of-the-art semantic
representation of sentences. given pairs of sentence vectors u and v, the input to the network is
the concatenation of u, v and the absolute difference |u     v|. we tuned the number of layers, layer
dimensionality and dropout rates to optimize performance on the development set, using the adam
optimizer. batch id172 (ioffe & szegedy, 2015) and prelu units (he et al., 2015) were
used. our best network used 2 hidden layers of 1000 units each, with dropout rate of 0.5 across both
the input and hidden layers. we did not backpropagate through the skip-thought encoder.
we also evaluate against eop classi   er, a 2-class baseline introduced by (bowman et al., 2015),
and against a version of our model where our order-violation penalty is replaced with the symmetric
cosine distance, order-embeddings (symmetric).
the results for all models are shown in table 3. we see that order-embeddings outperform the skip-
thought baseline despite not using external text corpora. while our method is almost certainly worse
than the state-of-the-art method of rockt  aschel et al. (2015), which uses a word-by-word attention
mechanism, it is also much simpler.

6 conclusion and future work

we introduced a simple method to encode order into learned distributed representations, which al-
lows us to explicitly model the partial order structure of the visual-semantic hierarchy. our method
can be easily integrated into existing relational learning methods, as we demonstrated on three chal-
lenging tasks involving id161 and natural language processing. on two of these tasks,
hypernym prediction and caption-id162, our methods outperform all previous work.
a promising direction of future work is to learn better classi   ers on id163 (deng et al., 2009),
which has over 21k image classes arranged by the id138 hierarchy. previous approaches, includ-
ing frome et al. (2013) and norouzi et al. (2014) have embedded words and images into a shared
semantic space with symmetric similarity   which our experiments suggest to be a poor    t with the
partial order structure of id138. we expect signi   cant progress on id163 classi   cation, and
the related problems of one-shot and zero-shot learning, to be possible using order-embeddings.
going further, order-embeddings may enable learning the entire semantic hierarchy in a single model
which jointly reasons about hypernymy, entailment, and the relationship between perception and
language, unifying what have been until now almost independent lines of work.

acknowledgments

we thank kaustav kundu for many fruitful discussions throughout the development of this paper.
the work was supported in part by an nserc graduate scholarship.

9

published as a conference paper at iclr 2016

references
baroni, marco, bernardi, raffaella, do, ngoc-quynh, and shan, chung-chieh. entailment above

the word level in id65. in eacl, 2012.

bordes, antoine, weston, jason, collobert, ronan, and bengio, yoshua. learning structured em-

beddings of knowledge bases. in aaai, 2011.

bowman, samuel r., angeli, gabor, potts, christopher, and manning, christopher d. a large

annotated corpus for learning natural language id136. in emnlp, 2015.

cho, kyunghyun, van merri  enboer, bart, gulcehre, caglar, bahdanau, dzmitry, bougares, fethi,
schwenk, holger, and bengio, yoshua. learning phrase representations using id56 encoder-
decoder for id151. in emnlp, 2014.

chopra, sumit, hadsell, raia, and lecun, yann. learning a similarity metric discriminatively, with

application to face veri   cation. in cvpr, 2005.

deng, jia, dong, wei, socher, richard, li, li-jia, li, kai, and fei-fei, li. id163: a large-scale

hierarchical image database. in cvpr, 2009.

frome, andrea, corrado, greg s, shlens, jon, bengio, samy, dean, jeff, mikolov, tomas, et al.

devise: a deep visual-semantic embedding model. in nips, 2013.

he, kaiming, zhang, xiangyu, ren, shaoqing, and sun, jian. delving deep into recti   ers: surpass-

ing human-level performance on id163 classi   cation. iccv, 2015.

hodosh, micah, young, peter, and hockenmaier, julia. framing image description as a ranking

task: data, models and id74. jair, 2013.

ioffe, sergey and szegedy, christian. batch id172: accelerating deep network training by

reducing internal covariate shift. icml, 2015.

karpathy, andrej and li, fei-fei. deep visual-semantic alignments for generating image descrip-

tions. in cvpr, 2015.

kingma, diederik and ba, jimmy. adam: a method for stochastic optimization. in iclr, 2015.

kiros, ryan, salakhutdinov, ruslan, and zemel, richard s. unifying visual-semantic embeddings

with multimodal neural language models. arxiv preprint arxiv:1411.2539, 2014.

kiros, ryan, zhu, yukun, salakhutdinov, ruslan, zemel, richard s, torralba, antonio, urtasun,

raquel, and fidler, sanja. skip-thought vectors. nips, 2015.

klein, benjamin, lev, guy, sadeh, gil, and wolf, lior. associating neural id27s with

deep image representations using    sher vectors. in cvpr, 2015.

krizhevsky, alex, sutskever, ilya, and hinton, geoffrey e. id163 classi   cation with deep con-

volutional neural networks. in nips, 2012.

lin, dahua, fidler, sanja, kong, chen, and urtasun, raquel. visual semantic search: retrieving
videos via complex textual queries. in proceedings of the ieee conference on id161
and pattern recognition, 2014a.

lin, tsung-yi, maire, michael, belongie, serge, hays, james, perona, pietro, ramanan, deva,
doll  ar, piotr, and zitnick, c lawrence. microsoft coco: common objects in context. in eccv,
2014b.

ma, lin, lu, zhengdong, shang, lifeng, and li, hang. multimodal convolutional neural networks

for matching image and sentence. iccv, 2015.

mao, junhua, xu, wei, yang, yi, wang, jiang, and yuille, alan. deep captioning with multimodal

recurrent neural networks (m-id56). in iclr, 2015.

mikolov, tomas, yih, wen-tau, and zweig, geoffrey. linguistic regularities in continuous space

word representations. in hlt-naacl, pp. 746   751, 2013.

10

published as a conference paper at iclr 2016

miller, george a. id138: a lexical database for english. communications of the acm, 1995.

norouzi, mohammad, mikolov, tomas, bengio, samy, singer, yoram, shlens, jonathon, frome,
andrea, corrado, greg s, and dean, jeffrey. zero-shot learning by convex combination of se-
mantic embeddings. in iclr, 2014.

plummer, bryan, wang, liwei, cervantes, chris, caicedo, juan, hockenmaier, julia, and lazebnik,
svetlana. flickr30k entities: collecting region-to-phrase correspondences for richer image-to-
sentence models. arxiv preprint arxiv:1505.04870, 2015.

rockt  aschel, tim, grefenstette, edward, hermann, karl moritz, ko  cisk`y, tom  a  s, and blunsom,
phil. reasoning about entailment with neural attention. arxiv preprint arxiv:1509.06664, 2015.

simonyan, k. and zisserman, a. very deep convolutional networks for large-scale image recogni-

tion. in iclr, 2015.

socher, richard, chen, danqi, manning, christopher d, and ng, andrew. reasoning with neural

tensor networks for knowledge base completion. in nips, 2013.

socher, richard, karpathy, andrej, le, quoc v, manning, christopher d, and ng, andrew y.
grounded id152 for    nding and describing images with sentences. tacl,
2014.

van der maaten, laurens and hinton, geoffrey. visualizing data using id167. journal of machine

learning research, 9(2579-2605):85, 2008.

vilnis, luke and mccallum, andrew. word representations via gaussian embedding. in iclr, 2015.

vinyals, oriol, toshev, alexander, bengio, samy, and erhan, dumitru. show and tell: a neural

image caption generator. in cvpr, 2015.

young, peter, lai, alice, hodosh, micah, and hockenmaier, julia. from image descriptions to
visual denotations: new similarity metrics for semantic id136 over event descriptions. tacl,
2014.

11

published as a conference paper at iclr 2016

7 supplementary material

mikolov et al. (2013) showed that word representations learned using id97 exhibit semantic
regularities, such as king     man + woman     queen. kiros et al. (2014) showed that similar
regularities hold for joint image-language models. we    nd that order-embeddings exhibit a novel
form of regularity, shown in figure 4. the elementwise max and min operations in the embedding
space roughly correspond to composition and abstraction, respectively.

figure 4: multimodal regularities found with embeddings learned for the caption-id162
task. note that some images have been slightly cropped for easier viewing, but no relevant objects
have been removed.

12

nearest non-query images in coco trainmax(   man   ,    cat   )  max(   black dog   ,    park   ) querymax(               ),min(               ),min(               ),   dog   max(               ),   man   