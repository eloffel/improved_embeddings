mutual information and diverse decoding improve neural machine

translation

jiwei li and dan jurafsky
computer science department

stanford university, stanford, ca, 94305, usa

jiweil,jurafsky@stanford.edu

6
1
0
2

 
r
a

 

m
2
2

 
 
]
l
c
.
s
c
[
 
 

2
v
2
7
3
0
0

.

1
0
6
1
:
v
i
x
r
a

abstract

sequence-to-sequence neural translation
models learn semantic and syntactic rela-
tions between sentence pairs by optimiz-
ing the likelihood of the target given the
source, i.e., p(y|x), an objective that ig-
nores other potentially useful sources of
information. we introduce an alternative
objective function for neural mt that max-
imizes the mutual information between the
source and target sentences, modeling the
bi-directional dependency of sources and
targets. we implement the model with a
simple re-ranking method, and also intro-
duce a decoding algorithm that increases
diversity in the n-best list produced by
the    rst pass. applied to the wmt ger-
man/english and french/english tasks, the
proposed models offers a consistent perfor-
mance boost on both standard lstm and
attention-based neural mt architectures.

1

introduction

sequence-to-sequence models for machine transla-
tion (id195) (sutskever et al., 2014; bahdanau
et al., 2014; cho et al., 2014; kalchbrenner and
blunsom, 2013; sennrich et al., 2015a; sennrich
et al., 2015b; gulcehre et al., 2015) are of growing
interest for their capacity to learn semantic and syn-
tactic relations between sequence pairs, capturing
contextual dependencies in a more continuous way
than phrase-based smt approaches. id195
models require minimal domain knowledge, can be
trained end-to-end, have a much smaller memory
footprint than the large phrase tables needed for
phrase-based smt, and achieve state-of-the-art per-
formance in large-scale tasks like english to french

(luong et al., 2015b) and english to german (lu-
ong et al., 2015a; jean et al., 2014) translation.

id195 models are implemented as an
encoder-decoder network, in which a source se-
quence input x is mapped (encoded) to a continu-
ous vector representation from which a target out-
put y will be generated (decoded). the framework
is optimized through maximizing the log-likelihood
of observing the paired output y given x:

loss =     log p(y|x)

(1)

while standard id195 models thus capture the
unidirectional dependency from source to target,
i.e., p(y|x), they ignore p(x|y), the dependency
from the target to the source, which has long been
an important feature in phrase-based translation
(och and ney, 2002; shen et al., 2010). phrase
based systems that combine p(x|y), p(y|x) and
other features like sentence length yield signi   cant
performance boost.

we propose to incorporate this bi-directional
dependency and model the maximum mutual in-
formation (mmi) between source and target into
id195 models. as li et al. (2015) recently
showed in the context of conversational response
generation, the mmi based objective function is
equivalent to linearly combining p(x|y) and p(y|x).
with a tuning weight   , such a id168 can be
written as :

p(x, y)

y

y

p(x)p(y)  

= arg max

  y = arg max

log
(1       ) log p(y|x) +    log p(x|y)
(2)
but as also discussed in li et al. (2015), direct
decoding from (2) is infeasible because computing
p(x|y) cannot be done until the target has been

computed1.

to avoid this enormous search space, we propose
to use a reranking approach to approximate the
mutual information between source and target in
id4 models. we separately
trained two id195 models, one for p(y|x) and
one for p(x|y). the p(y|x) model is used to gen-
erate n-best lists from the source sentence x. the
lists are followed by a reranking process using the
second term of the objective function, p(x|y).

because reranking approaches are dependent
on having a diverse n-best list to rerank, we also
propose a diversity-promoting decoding model tai-
lored to neural mt systems. we tested the mutual
information objective function and the diversity-
promoting decoding model on english   french,
english   german and german   english transla-
tion tasks, using both standard lstm settings and
the more advanced attention-model based settings
that have recently shown to result in higher perfor-
mance.

the next section presents related work, fol-
lowed by a background section 3 introducing
lstm/attention machine translation models. our
proposed model will be described in detail in sec-
tions 4, with datasets and experimental results in
section 6 followed by conclusions.

2 related work

this paper draws on three prior lines of research:
id195 models, modeling mutual information,
and promoting translation diversity.
id195 models id195 models map
source sequences to vector space representations,
from which a target sequence is then generated.
they yield good performance in a variety of nlp
generation tasks including conversational response
generation (vinyals and le, 2015; serban et al.,
2015a; li et al., 2015), and parsing (vinyals et al.,
2014; ?).

a id4 system uses dis-
tributed representations to model the conditional

1as demonstrated in (li et al., 2015)

log

p(x, y)

p(x)p(y)   = log p(y|x)        log p(y)

(3)

equ. 2 can be immediately achieved by applying bayesian
rules

log p(y) = log p(y|x) + log p(x)     log p(x|y)

id203 of targets given sources, using two com-
ponents, an encoder and a decoder. kalchbrenner
and blunsom (2013) used an encoding model akin
to convolutional networks for encoding and stan-
dard hidden unit recurrent nets for decoding. simi-
lar convolutional networks are used in (meng et al.,
2015) for encoding. sutskever et al. (2014; luong
et al. (2015a) employed a stacking lstm model
for both encoding and decoding. bahdanau et al.
(2014), jean et al. (2014) adopted bi-directional
recurrent nets for the encoder.

maximum mutual information maximum mu-
tual information (mmi) was introduced in speech
recognition (bahl et al., 1986) as a way of measur-
ing the mutual dependence between inputs (acous-
tic feature vectors) and outputs (words) and improv-
ing discriminative training (woodland and povey,
2002). li et al. (2015) show that mmi could
solve an important problem in id195 conversa-
tional response generation. prior id195 models
tended to generate highly generic, dull responses
(e.g., i don   t know) regardless of the inputs (sor-
doni et al., 2015; vinyals and le, 2015; serban et
al., 2015b). li et al. (2015) show that modeling
the mutual dependency between messages and re-
sponse promotes the diversity of response outputs.
our goal, distinct from these previous uses of
mmi, is to see whether the mutual information
objective improves translation by bidirectionally
modeling source-target dependencies. in that sense,
our work is designed to incorporate into id195
models features that have proved useful in phrase-
based mt, like the reverse translation id203
or sentence length (och and ney, 2002; shen et al.,
2010; devlin et al., 2014).

generating diverse translations various algo-
rithms have been proposed for generated diverse
translations in phrase-based mt, including com-
pact representations like lattices and hypergraphs
(macherey et al., 2008; tromble et al., 2008;
kumar and byrne, 2004),    traits    like transla-
tion length (devlin and matsoukas, 2012), bag-
ging/boosting (xiao et al., 2013), or multiple sys-
tems (cer et al., 2013). gimpel et al. (2013; ba-
tra et al. (2012), produce diverse n-best lists by
adding a dissimilarity function based on id165
overlaps, distancing the current translation from
already-generated ones by choosing translations
that have higher scores but distinct from previous
ones. while we draw on these intuitions, these ex-
isting diversity promoting algorithms are tailored to

phrase-based translation frameworks and not easily
transplanted to neural mt decoding which requires
batched computation.

3 background: neural machine

translation

id4 models map source
x = {x1, x2, ...xnx} to a continuous vector
representation, from which target output y =
{y1, y2, ..., yny} is to be generated.
3.1 lstm models
a long-short term memory model (hochreiter and
schmidhuber, 1997) associates each time step with
an input gate, a memory gate and an output gate, de-
noted respectively as it, ft and ot. let et denote the
vector for the current word wt, ht the vector com-
puted by the lstm model at time t by combining
et and ht   1., ct the cell state vector at time t, and
   the sigmoid function. the vector representation
ht for each time step t is given by:
it =   (wi    [ht   1, et])
ft =   (wf    [ht   1, et])
ot =   (wo    [ht   1, et])
lt = tanh(wl    [ht   1, et])
ct = ft    ct   1 + it    lt
t = ot    tanh(ct)
hs

(4)
(5)
(6)
(7)
(8)
(9)
where wi, wf , wo, wl     rk  2k. the lstm de-
   nes a distribution over outputs y and sequentially
predicts tokens using a softmax function:

nt(cid:89)

t=1

(cid:80)

p(y|x) =

exp(f (ht   1, eyt))
w(cid:48) exp(f (ht   1, ew(cid:48)))

where f (ht   1, eyt) denotes the activation function
between ht   1 and ewt, where ht   1 is the represen-
tation output from the lstm at time t     1. each
sentence concludes with a special end-of-sentence
symbol eos. commonly, the input and output each
use different lstms with separate sets of compo-
sitional parameters to capture different composi-
tional patterns. during decoding, the algorithm
terminates when an eos token is predicted.

3.2 id12
id12 adopt a look-back strategy that
links the current decoding stage with input time
steps to represent which portions of the input are
most responsible for the current decoding state (xu

et al., 2015; luong et al., 2015b; bahdanau et al.,
2014).
let h = {  h1,   h2, ...,   hnx} be the collection of
hidden vectors outputted from lstms during en-
coding. each element in h contains information
about the input sequences, focusing on the parts
surrounding each speci   c token. let ht   1 be the
lstm outputs for decoding at time t     1. atten-
tion models link the current-step decoding infor-
mation, i.e., ht with each of the representations
at decoding step   ht(cid:48) using a weight variable at.
at can be constructed from different scoring func-
tions such as the dot product between the two vec-
t   1      ht, a general model akin to tensor
tors, i.e., ht
t   1    w      ht, and the concatena-
operation i.e., ht
tion model by concatenating the two vectors i.e.,
u t  tanh(w    [ht   1,   ht]). the behavior of different
attention scoring functions have been extensively
studied in luong et al. (2015a). for all experi-
ments in this paper, we adopt the general strategy
where the relevance score between the current step
of the decoding representation and the encoding
representation is given by:

the attention vector is created by averaging
weights over all input time-steps:

vt(cid:48) = ht

ai =

t   1    w      ht
(cid:80)
exp(vt   )
t    exp(vt   )
(cid:88)

ai  ht(cid:48)

t(cid:48)   [1,ns ]

mt =

(10)

(11)

id12 predict subsequent tokens based
on the combination of the last step outputted lstm
vectors ht   1 and attention vectors mt:

(cid:126)ht   1 = tanh(wc    [ht   1, mt])
p(yt|y<, x) = softmax(ws    (cid:126)ht   1)

(12)

where wc     rk  2k, ws     rv   k with v denot-
ing vocabulary size. luong et al. (2015a) reported
a signi   cant performance boost by integrating (cid:126)ht   1
into the next step lstm hidden state computation
(referred to as the input-feeding model), making
lstm compositions in decoding as follows:

it =   (wi    [ht   1, et, (cid:126)ht   1])
ft =   (wf    [ht   1, et,(cid:126)ht   1])
ot =   (wo    [ht   1, et, (cid:126)ht   1])
lt = tanh(wl    [ht   1, et, (cid:126)ht   1])

(13)

where wi, wf , wo, wl     rk  3k. for the atten-
tion models implemented in this work, we adopt
the input-feeding strategy.

3.3 unknown word replacements
one of the major issues in neural mt models is the
computational complexity of the softmax function
for target word prediction, which requires summing
over all tokens in the vocabulary. neural models
tend to keep a shortlist of 50,00-80,000 most fre-
quent words and use an unknown (unk) token to
represent all infrequent tokens, which signi   cantly
impairs id7 scores. recent work has proposed
to deal with this issue: (luong et al., 2015b) adopt
a post-processing strategy based on aligner from
ibm models, while (jean et al., 2014) approximates
softmax functions by selecting a small subset of
target vocabulary.

in this paper, we use a strategy similar to that
of jean et al. (2014), thus avoiding the reliance on
external ibm model word aligner. from the atten-
tion models, we obtain word alignments from the
training dataset, from which a bilingual dictionary
is extracted. at test time, we    rst generate target
sequences. once a translation is generated, we link
the generated unk tokens back to positions in the
source inputs, and replace each unk token with the
translation word of its correspondent source token
using the pre-constructed dictionary.

as the unknown word replacement mecha-
nism relies on automatic word alignment extrac-
tion which is not explicitly modeled in vanilla
id195 models, it can not be immediately ap-
plied to vanilla id195 models. however, since
unknown word replacement can be viewed as a
post-processing technique, we can apply a pre-
trained attention-model to any given translation.
for id195 models, we    rst generate transla-
tions and replace unk tokens within the transla-
tions using the pre-trained id12 to post-
process the translations.

4 mutual information via reranking

as discussed in li et al. (2015), direct decoding
from (2) is infeasible since the second part, p(x|y),
requires completely generating the target before it
can be computed. we therefore use an approxima-
tion approach:
1. train p(y|x) and p(x|y) separately using
vanilla id195 models or attention mod-
els.

2. generate n-best lists from p(y|x).
3. rerank the n-best list by linearly adding

p(x|y).

4.1 standard id125 for n-best lists
n-best lists are generated using a id125 de-
coder with beam size set to k = 200 from p(y|x)
models. as illustrated in figure 1, at time step t    1
in decoding, we keep record of n hypotheses based
on score s(yt   1|x) = log p(y1, y2, ..., yt   1|x). as
we move on to time step t, we expand each of the k
t   1},
hypotheses (denoted as y k
k     [1, k]), by selecting top k of the translations,
denoted as yk,k(cid:48)
, k(cid:48)     [1, k], leading to the con-
struction of k    k new hypotheses:

t   1 = {yk

2 , ..., yk

1 , yk

t

t   1, yk,k(cid:48)
[y k

t

], k     [1, k], k(cid:48)     [1, k]

the score for each of the k    k hypotheses is
computed as follows:

t

t

s(y k

t   1, yk,k(cid:48)

|x) = s(y k

t   1|x)+log p(yk,k(cid:48)

|x, y k
t   1)
(14)
in a standard id125 model, the top k hy-
potheses are selected (from the k    k hypothe-
ses computed in the last step) based on the score
|x). the remaining hypotheses are
s(y k
ignored as we proceed to the next time step.

t   1, yk,k(cid:48)

t

we set the minimum length and maximum length
to 0.75 and 1.5 times the length of sources. beam
size n is set to 200. to be speci   c, at each time
step of decoding, we are presented with k    k
word candidates. we    rst add all hypotheses with
an eos token being generated at current time step
to the n-best list. next we preserve the top k
un   nished hypotheses and move to next time step.
we therefore maintain batch size of 200 constant
when some hypotheses are completed and taken
down by adding in more un   nished hypotheses.
this will lead the size of    nal n-best list for each
input much larger than the beam size2.

4.2 generating a diverse n-best list
unfortunately, the n-best lists outputted from stan-
dard id125 are a poor surrogate for the entire
search space (finkel et al., 2006; huang, 2008).
the id125 algorithm can only keep a small
proportion of candidates in the search space and
most of the generated translations in n-best list

2for example, for the development set of the english-
german wmt14 task, each input has an average of 2,500
candidates in the n-best list.

figure 1: illustration of standard id125 and proposed diversity promoting id125.

t

t   1, yk,k(cid:48)

we propose to change the way s(y k

are similar, differing only by punctuation or minor
morphological variations, with most of the words
overlapping. because this lack of diversity in the
n-best list will signi   cantly decrease the impact of
our reranking process, it is important to    nd a way
to generate a more diverse n-best list.
|x)
is computed in an attempt to promote diversity, as
shown in figure 1. for each of the hypotheses
t   1 (he and it), we generate the top k transla-
y k
tions, yk,k(cid:48)
, k(cid:48)     [1, k] as in the standard beam
search model. next we rank the k translated to-
kens generated from the same parental hypothesis
based on p(yk,k(cid:48)
t   1) in descending order: he
is ranks the    rst among he is and he has, and he
has ranks second; similarly for it is and it has.
t   1, yk,k(cid:48)

] by
adding an additional part   k(cid:48), where k(cid:48) denotes
the ranking of the current hypothesis among its
siblings, which is    rst for he is and it is, second for
he has and it has.

next we rewrite the score for [y k

|x, y k

t

t

t

  s(y k

t   1, yk,k(cid:48)

t

|x) = s(y k

t   1, yk,k(cid:48)

t

|x)       k(cid:48)

(15)

t

t   1, yk,k(cid:48)

the top k hypothesis are selected based on
|x) as we move on to the next time
  s(y k
step. by adding the additional term   k(cid:48), the model
punishes bottom ranked hypotheses among sib-
lings (hypotheses descended from the same parent).
when we compare newly generated hypotheses de-
scended from different ancestors, the model gives
more credit to top hypotheses from each of different
ancestors. for instance, even though the original
score for it is is lower than he has, the model favors
the former as the latter is more severely punished

by the intra-sibling ranking part   k(cid:48). the model
thus generally favors choosing hypotheses from
diverse parents, leading to a more diverse n-best
list.

the proposed model is straightforwardly imple-
mented with minor adjustment to the standard beam
search model3.

we employ the diversity id74 in
(li et al., 2015) to evaluate the degree of diversity
of the n-best lists: calculating the average number
of distinct unigrams distinct-1 and bigrams distinct-
2 in the n-best list given each source sentence,
scaled by the total number of tokens. by employing
the diversity promoting model with    tuned from
the development set based on id7 score, the
value of distinct-1 increases from 0.54% to 0.95%,
and distinct-2 increases from 1.55% to 2.84% for
english-german translation. similar phenomenon
are observed from english-french translation tasks
and details are omitted for brevity.

4.3 reranking
the generated n-best list is then reranked by lin-
early combining log p(y|x) with log p(x|y). the
score of the source given each generated translation
can be immediately computed from the previously
trained p(x|y).
other than log p(y|x), we also consider log p(y),
which denotes the average language model proba-
bility trained from monolingual data. it is worth

3decoding for neural based mt model using large batch-
size can be expensive resulted from softmax word prediction
function. the proposed model supports batched decoding us-
ing gpu, signi   cantly speed up decoding process than other
diversity fostering models tailored to phrase based mt sys-
tems.

nothing that integrating log p(y|x) and log p(y)
into reranking is not a new one and has long been
employed by in id87s in standard
mt. in neural mt literature, recent progress has
demonstrated the effectiveness of modeling rerank-
ing with language model (gulcehre et al., 2015).
we also consider an additional term that takes
into account the length of targets (denotes as lt )
in decoding. we thus linearly combine the three
parts, making the    nal ranking score for a given
target candidate y as follows:

score(y) = log p(y|x) +    log p(x|y)

+    log p(y) +   lt

(16)

we optimize   ,    and    using mert (och, 2003)
id7 score (papineni et al., 2002) on the develop-
ment set.

5 experiments

our models are trained on the wmt   14 training
dataset containing 4.5 million pairs for english-
german and german-english translation, and 12
million pairs for english-french translation. for
english-german translation, we limit our vocabu-
laries to the top 50k most frequent words for both
languages. for english-french translation, we keep
the top 200k most frequent words for the source
language and 80k for the target language. words
that are not in the vocabulary list are noted as the
universal unknown token.

for the english-german and english-german
translation, we use newstest2013 (3000 sentence
pairs) as the development set and translation per-
formances are reported in id7 (papineni et al.,
2002) on newstest2014 (2737) sentences. for
english-french translation, we concatenate news-
test-2012 and news-test-2013 to make a develop-
ment set (6,003 pairs in total) and evaluate the mod-
els on news-test-2014 with 3,003 pairs4.
5.1 training details for p(x|y) and p(y|x)
we trained neural models on standard id195
models and id12. we trained p(y|x)
following the standard training protocols described
in (sutskever et al., 2014). p(x|y) is trained identi-
cally but with sources and targets swapped.

we adopt a deep structure with four lstm lay-
ers for encoding and four lstm layers for decod-

4as in (luong et al., 2015a). all texts are tokenized with
tokenizer.perl and id7 scores are computed with multi-
id7.perl

ing, each of which consists of a different set of pa-
rameters. we followed the detailed protocols from
luong et al. (2015a): each lstm layer consists
of 1,000 hidden neurons, and the dimensionality
of id27s is set to 1,000. other training
details include: lstm parameters and word em-
beddings are initialized from a uniform distribution
between [-0.1,0.1]; for english-german transla-
tion, we run 12 epochs in total. after 8 epochs,
we start halving the learning rate after each epoch;
for english-french translation, the total number of
epochs is set to 8, and we start halving the learn-
ing rate after 5 iterations. batch size is set to 128;
gradient clipping is adopted by scaling gradients
when the norm exceeded a threshold of 5. inputs
are reversed.

our implementation on a single gpu5 processes
approximately 800-1200 tokens per second. train-
ing for the english-german dataset (4.5 million
pairs) takes roughly 12-15 days. for the french-
english dataset, comprised of 12 million pairs,
training takes roughly 4-6 weeks.

5.2 training p(y) from monolingual data
we respectively train single-layer lstm recurrent
models with 500 units for german and french us-
ing monolingual data. we news crawl corpora
from wmt136 as additional training data to train
monolingual language models. we used a subset
of the original dataset which roughly contains 50-
60 millions sentences. following (gulcehre et al.,
2015; sennrich et al., 2015a), we remove sentences
with more than 10% unknown words based on the
vocabulary constructed using parallel datasets. we
adopted similar protocols as we trained id195
models, such as gradient clipping and mini batch.

5.3 english-german results
we reported progressive performances as we add
in more features for reranking. results for differ-
ent models on wmt2014 english-german trans-
lation task are shown in figure 1. among all the
features, reverse id203 from mutual informa-
tion (i.e., p(x|y)) yields the most signi   cant perfor-
mance boost, +1.4 and +1.1 for standard id195
models without and with unknown word replace-
ment, +0.9 for id127. in line with (gul-

5tesla k40m, 1 kepler gk110b, 2880 cuda cores.
6http://www.statmt.org/wmt13/

translation-task.html

7target length has long proved to be one of the most im-
portant features in phrase based mt due to the id7 score   s
signi   cant sensitiveness to target lengths. however, here we

model
standard
standard
standard
standard
standard

features
p(y|x)
p(y|x)+length
p(y|x)+p(x|y)+length
p(y|x)+p(x|y)+p(y)+length
p(y|x)+p(x|y)+p(y)+length+diver decoding

standard+unkrep
standard+unkrep
standard+unkrep
standard+unkrep
standard+unkrep

p(y|x)
p(y|x)+length
p(y|x)+p(x|y)+length
p(y|x)+p(x|y)+p(y)+length
p(y|x)+p(x|y)+p(y)+length+diver decoding

attention+unkrep
attention+unkrep
attention+unkrep
attention+unkrep
attention+unkrep

p(y|x)
p(y|x)+length
p(y|x)+p(x|y)+length
p(y|x)+p(x|y)+p(y)+length
p(y|x)+p(x|y)+p(y)+length+diver decoding

jean et al., 2015 (without ensemble)

jean et al., 2015 (with ensemble)

luong et al. (2015a) (with unkrep, without ensemble)

luong et al. (2015a) (with unkrep, with ensemble)

id7 scores
13.2
13.6 (+0.4)
15.0 (+1.4)
15.4 (+0.4)
15.8 (+0.4)
+2.6 in total
14.7
15.2 (+0.7)
16.3 (+1.1)
16.7 (+0.4)
17.3 (+0.3)
+2.6 in total
20.5
20.9 (+0.4)
21.8 (+0.9)
22.1 (+0.3)
22.6 (+0.3)
+2.1 in total
19.4
21.6
20.9
23.0

table 1: id7 scores from different models for on wmt14 english-german results. unkrep denotes applying unknown word
replacement strategy. diversity indicates diversity-promoting model for decoding being adopted. baselines performances are
reprinted from jean et al. (2014), luong et al. 2015a.

cehre et al., 2015; sennrich et al., 2015a), we ob-
serve consistent performance boost introduced by
language model.

we see the bene   t from our diverse n-best list by
comparing mutual+diversity models with diversity
models. on top of the improvements from standard
id125 due to reranking, the diversity models
introduce additional gains of +0.4, +0.3 and +0.3,
leading the total gains roughly up to +2.6, +2.6,
+2.1 for different models. the unknown token re-
placement technique yields signi   cant gains, in line
with observations from jean et al. (2014; luong et
al. (2015a).

we compare our english-german system with
various others: (1) the end-to-end neural mt sys-
tem from jean et al. (2014) using a large vocabu-
lary size. (2) models from luong et al. (2015a)
that combines different id12. for the
models described in (jean et al., 2014) and (lu-

do not observe as large performance boost here as in phrase
based mt. this is due to the fact that during decoding, target
length has already been strictly constrained. as described in
4.1, we only consider candidates of lengths between 0.75 and
1.5 times that of the source.

ong et al., 2015a), we reprint their results from
both the single model setting and the ensemble set-
ting, which a set of (usually 8) neural models that
differ in random initializations and the order of
minibatches are trained, the combination of which
jointly contributes in the decoding process. the
ensemble procedure is known to result in improved
performance (luong et al., 2015a; jean et al., 2014;
sutskever et al., 2014).

note that the reported results from the standard
id195 models and id12 in table
1 (those without considering mutual information)
are from models identical in structure to the corre-
sponding models described in (luong et al., 2015a),
and achieve similar performances (13.2 vs 14.0 for
standard id195 models and 20.5 vs 20.7 for
id12). due to time and computational
constraints, we did not implement an ensemble
mechanism, making our results incomparable to
the ensemble mechanisms in these papers.

model
standard
standard
standard
standard
standard

features
p(y|x)
p(y|x)+length
p(y|x)+p(x|y)+length
p(y|x)+p(x|y)+p(y)+length
p(y|x)+p(x|y)+p(y)+length+diver decoding

standard+unkrep
standard+unkrep
standard+unkrep
standard+unkrep
standard+unkrep

p(y|x)
p(y|x)+length
p(y|x)+p(x|y)+length
p(y|x)+p(x|y)+p(y)+length
p(y|x)+p(x|y)+p(y)+length+diver decoding

attention+unkrep
attention+unkrep
attention+unkrep
attention+unkrep
attention+unkrep

p(y|x)
p(y|x)+length
p(y|x)+p(x|y)+length
p(y|x)+p(x|y)+p(y)+length
p(y|x)+p(x|y)+p(y)+length+diver decoding

lstm (google) (without ensemble))

lstm (google) (with ensemble)

luong et al. (2015b), unkrep (without ensemble)

luong et al. (2015b), unkrep (with ensemble)

id7 scores
29.0
29.7 (+0.7)
31.2 (+1.5)
31.7 (+0.5)
32.2 (+0.5)
+3.2 in total
31.0
31.5 (+0.5)
32.9 (+1.4)
33.3 (+0.4)
33.6 (+0.3)
+2.6 in total
33.4
34.3 (+0.9)
35.2 (+0.9)
35.7 (+0.5)
36.3 (+0.4)
+2.7 in total
30.6
33.0
32.7
37.5

table 2: id7 scores from different models for on wmt   14 english-french results. google is the lstm-based model proposed
in sutskever et al. (2014). luong et al. (2015) is the extension of google models with unknown token replacements.

5.4 french-english results
results from the wmt   14 french-english datasets
are shown in table 2, along with results reprinted
from sutskever et al. (2014; luong et al. (2015b).
we again observe that applying mutual information
yields better performance than the corresponding
standard neural mt models.

relative to the english-german dataset, the
english-french translation task shows a larger gap
between our new model and vanilla models where
reranking information is not considered; our mod-
els respectively yield up to +3.2, +2.6, +2.7 boost in
id7 compared to standard neural models without
and with unknown word replacement, and atten-
tion models.

6 discussion

in this paper, we introduce a new objective for
neural mt based on the mutual dependency be-
tween the source and target sentences, inspired
by recent work in neural conversation generation
(li et al., 2015). we build an approximate imple-
mentation of our model using reranking, and then
to make reranking more powerful we introduce a

new decoding method that promotes diversity in
the    rst-pass n-best list. on english   french and
english   german translation tasks, we show that
the id4 models trained us-
ing the proposed method perform better than corre-
sponding standard models, and that both the mutual
information objective and the diversity-increasing
decoding methods contribute to the performance
boost..

the new models come with the advantages of
easy implementation with sources and targets in-
terchanged, and of offering a general solution that
can be integrated into any neural generation mod-
els with minor adjustments. indeed, our diversity-
enhancing decoder can be applied to generate more
diverse n-best lists for any nlp reranking task.
finding a way to introduce mutual information
based decoding directly into a    rst-pass decoder
without reranking naturally constitutes our future
work.

references
dzmitry bahdanau, kyunghyun cho, and yoshua ben-
gio. 2014. id4 by jointly

learning to align and translate.
arxiv:1409.0473.

arxiv preprint

lr bahl, peter f brown, peter v de souza, and
robert l mercer. 1986. maximum mutual informa-
tion estimation of hidden markov model parameters
for id103. in proc. icassp, volume 86,
pages 49   52.

dhruv batra, payman yadollahpour, abner guzman-
rivera, and gregory shakhnarovich. 2012. diverse
m-best solutions in markov random    elds. in com-
puter vision   eccv 2012, pages 1   16. springer.

daniel cer, christopher d manning, and daniel juraf-
sky. 2013. positive diversity tuning for machine
translation system combination. in proceedings of
the eighth workshop on statistical machine trans-
lation, pages 320   328.

kyunghyun cho, bart van merri  enboer, caglar gul-
cehre, dzmitry bahdanau, fethi bougares, holger
schwenk, and yoshua bengio.
2014. learning
phrase representations using id56 encoder-decoder
for id151. arxiv preprint
arxiv:1406.1078.

jacob devlin and spyros matsoukas.

2012. trait-
based hypothesis selection for machine translation.
in proceedings of the 2012 conference of the north
american chapter of the association for computa-
tional linguistics: human language technologies,
pages 528   532. association for computational lin-
guistics.

jacob devlin, rabih zbib, zhongqiang huang, thomas
lamar, richard m schwartz, and john makhoul.
2014. fast and robust neural network joint models
for id151. in acl (1), pages
1370   1380. citeseer.

jenny rose finkel, christopher d manning, and an-
drew y ng. 2006. solving the problem of cascading
errors: approximate bayesian id136 for linguis-
tic annotation pipelines. in proceedings of the 2006
conference on empirical methods in natural lan-
guage processing, pages 618   626. association for
computational linguistics.

kevin gimpel, dhruv batra, chris dyer, gregory
shakhnarovich, and virginia tech. 2013. a system-
atic exploration of diversity in machine translation.
in proceedings of the 2013 conference on empirical
methods in natural language processing, october.

caglar gulcehre, orhan firat, kelvin xu, kyunghyun
cho, loic barrault, huei-chi lin, fethi bougares,
holger schwenk, and yoshua bengio. 2015. on
using monolingual corpora in neural machine trans-
lation. arxiv preprint arxiv:1503.03535.

sepp hochreiter and j  urgen schmidhuber.

1997.
neural computation,

long short-term memory.
9(8):1735   1780.

liang huang. 2008. forest reranking: discriminative
parsing with non-local features. in acl, pages 586   
594.

s  ebastien jean, kyunghyun cho, roland memisevic,
and yoshua bengio. 2014. on using very large tar-
get vocabulary for id4. arxiv
preprint arxiv:1412.2007.

nal kalchbrenner and phil blunsom. 2013. recurrent
in emnlp, pages

continuous translation models.
1700   1709.

shankar kumar and william byrne. 2004. minimum
bayes-risk decoding for statistical machine transla-
tion. technical report, dtic document.

jiwei li, michel galley, chris brockett, jianfeng gao,
and bill dolan. 2015. a diversity-promoting objec-
tive function for neural conversation models. arxiv
preprint arxiv:1510.03055.

minh-thang luong, hieu pham, and christopher d
manning. 2015a. effective approaches to attention-
based id4. emnlp.

minh-thang luong, ilya sutskever, quoc v le, oriol
vinyals, and wojciech zaremba. 2015b. address-
ing the rare word problem in neural machine transla-
tion. in proceedings of acl.

wolfgang macherey, franz josef och, ignacio thayer,
and jakob uszkoreit. 2008. lattice-based mini-
mum error rate training for statistical machine trans-
in proceedings of the conference on em-
lation.
pirical methods in natural language processing,
pages 725   734. association for computational lin-
guistics.

fandong meng, zhengdong lu, mingxuan wang,
hang li, wenbin jiang, and qun liu. 2015. en-
coding source language with convolutional neural
arxiv preprint
network for machine translation.
arxiv:1503.01838.

franz josef och and hermann ney. 2002. discrimina-
tive training and maximum id178 models for sta-
tistical machine translation. in proceedings of acl
2002, pages 295   302.

franz josef och. 2003. minimum error rate training
in id151. in proceedings of
the 41st annual meeting on association for compu-
tational linguistics-volume 1, pages 160   167. asso-
ciation for computational linguistics.

kishore papineni, salim roukos, todd ward, and wei-
jing zhu. 2002. id7: a method for automatic eval-
in proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics, pages 311   318. association for
computational linguistics.

rico sennrich, barry haddow, and alexandra birch.
improving id4
arxiv preprint

2015a.
models with monolingual data.
arxiv:1511.06709.

rico sennrich, barry haddow, and alexandra birch.
rare
arxiv preprint

2015b.
words with subword units.
arxiv:1508.07909.

id4 of

iulian v serban, alessandro sordoni, yoshua bengio,
aaron courville, and joelle pineau. 2015a. build-
ing end-to-end dialogue systems using generative hi-
erarchical neural network models. arxiv preprint
arxiv:1507.04808.

iulian vlad serban, ryan lowe, laurent charlin, and
joelle pineau. 2015b. a survey of available corpora
for building data-driven dialogue systems. arxiv
preprint arxiv:1512.05742.

libin shen, jinxi xu, and ralph weischedel. 2010.
string-to-dependency id151.
computational linguistics, 36(4):649   671.

alessandro sordoni, michel galley, michael auli,
chris brockett, yangfeng ji, margaret mitchell,
jian-yun nie, jianfeng gao, and bill dolan. 2015.
a neural network approach to context-sensitive gen-
eration of conversational responses. arxiv preprint
arxiv:1506.06714.

ilya sutskever, oriol vinyals, and quoc vv le. 2014.
sequence to sequence learning with neural networks.
in advances in neural information processing sys-
tems, pages 3104   3112.

roy w tromble, shankar kumar, franz och, and wolf-
gang macherey. 2008. lattice minimum bayes-risk
decoding for id151. in pro-
ceedings of the conference on empirical methods in
natural language processing, pages 620   629. as-
sociation for computational linguistics.

oriol vinyals and quoc le. 2015. a neural conversa-

tional model. arxiv preprint arxiv:1506.05869.

oriol vinyals, lukasz kaiser, terry koo, slav petrov,
2014.
arxiv preprint

ilya sutskever, and geoffrey hinton.
grammar as a foreign language.
arxiv:1412.7449.

p. c. woodland and d. povey. 2002. large scale
discriminative training of id48
for id103. computer speech and lan-
guage, 16:25   47.

tong xiao, jingbo zhu, and tongran liu. 2013. bag-
ging and boosting id151 sys-
tems. arti   cial intelligence, 195:496   527.

kelvin xu, jimmy ba, ryan kiros, aaron courville,
ruslan salakhutdinov, richard zemel, and yoshua
bengio. 2015. show, attend and tell: neural im-
age id134 with visual attention. arxiv
preprint arxiv:1502.03044.

