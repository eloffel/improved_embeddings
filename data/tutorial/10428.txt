the role of context types and dimensionality

in learning id27s

oren melamud       david mcclosky      

siddharth patwardhan    mohit bansal  

   computer science department, bar-ilan university, ramat-gan, israel

7
1
0
2

 
l
u
j
 

9
1

 
 
]
l
c
.
s
c
[
 
 

2
v
3
9
8
0
0

.

1
0
6
1
:
v
i
x
r
a

melamuo@cs.biu.ac.il
   google, new york, ny, usa

dmcc@google.com

   ibm watson, yorktown heights, ny, usa

siddharth@us.ibm.com

  toyota technological institute at chicago, chicago, il, 60637, usa

mbansal@ttic.edu

abstract

we provide the    rst extensive evaluation of
how using different types of context to learn
skip-gram id27s affects perfor-
mance on a wide range of intrinsic and ex-
trinsic nlp tasks. our results suggest that
while intrinsic tasks tend to exhibit a clear
preference to particular types of contexts and
higher dimensionality, more careful tuning is
required for    nding the optimal settings for
most of the extrinsic tasks that we consid-
ered. furthermore, for these extrinsic tasks,
we    nd that once the bene   t from increas-
ing the embedding dimensionality is mostly
exhausted, simple concatenation of word em-
beddings, learned with different context types,
can yield further performance gains. as an ad-
ditional contribution, we propose a new vari-
ant of the skip-gram model that learns word
embeddings from weighted contexts of substi-
tute words.

1

introduction

id27s have become increasingly pop-
ular lately, proving to be valuable as a source of
features in a broad range of nlp tasks with lim-
ited supervision (turian et al., 2010; collobert et
al., 2011; socher et al., 2013; bansal et al., 2014).
id971 skip-gram (mikolov et al., 2013a)

   majority of work performed while at ibm watson.
1http://code.google.com/p/id97/

and glove2 (pennington et al., 2014) are among
the most widely used id27 models to-
day. their success is largely due to an ef   cient
and user-friendly implementation that learns high-
quality id27s from very large corpora.

both id97 and glove learn low-
dimensional continuous vector representations for
words by considering window-based contexts, i.e.,
context words within some    xed distance of each
side of the target word. however, the underlying
models are equally applicable to different choices
of context types. for example, bansal et al. (2014)
and levy and goldberg (2014) showed that using
syntactic contexts rather than window contexts in
id97 captures functional similarity (as in
lion:cat) rather than topical similarity or relatedness
(as in lion:zoo). further, bansal et al. (2014) and
melamud et al. (2015b) showed the bene   ts of
such modi   ed-context embeddings in dependency
parsing and lexical substitution tasks. however,
to the best of our knowledge, there has not been
an extensive evaluation of the effect of multiple,
diverse context types on a wide range of nlp tasks.
id27s are typically evaluated on in-
trinsic and extrinsic tasks. intrinsic tasks mostly in-
clude predicting human judgments of semantic re-
lations between words, e.g., as in wordsim-353
(finkelstein et al., 2001), while extrinsic tasks in-
clude various    real    downstream nlp tasks, such as
coreference resolution and id31. re-

2http://nlp.stanford.edu/projects/glove/

cent works have shown that while intrinsic evalu-
ations are easier to perform, their correlation with
results on extrinsic evaluations is not very reliable
(schnabel et al., 2015; tsvetkov et al., 2015), stress-
ing the importance of the latter.

in this work, we provide the    rst extensive eval-
uation of id27s learned with different
types of context, on a wide range of intrinsic simi-
larity and relatedness tasks, and extrinsic nlp tasks,
namely id33, named entity recogni-
tion, coreference resolution, and sentiment analy-
sis. we employ contexts based of different word
window sizes, syntactic dependencies, and a lesser-
known substitute words approach (yatbaz et al.,
2012). finally, we experiment with combinations
of the above id27s, comparing two ap-
proaches: (1) simple vector concatenation that offers
a wider variety of features for a classi   er to choose
and learn weighted combinations from, and (2) di-
mensionality reduction via either singular value
decomposition or canonical correlation analysis,
which tries to    nd a smaller subset of features.

our results suggest that it is worthwhile to care-
fully choose the right type of id27s for
an extrinsic nlp task, rather than rely on intrinsic
benchmark results. speci   cally, picking the optimal
context type and dimensionality is critical. further-
more, once the bene   t from increasing the embed-
ding dimensionality is mostly exhausted, concatena-
tion of id27s learned with different con-
text types can yield further performance gains.

2 id27 context types

2.1 learning corpus

we use a    xed learning corpus for a fair compari-
son of all embedding types: a concatenation of three
large english corpora: (1) english wikipedia 2015,
(2) umbc web corpus (han et al., 2013), and (3)
english gigaword (ldc2011t07) newswire corpus
(parker et al., 2011). our concatenated corpus is
diverse and substantial in size with approximately
10b words. this allows us to learn high quality em-
beddings that cover a large vocabulary. after ex-
tracting clean text from these corpora, we used stan-
ford corenlp (manning et al., 2014) for sentence
splitting, id121, part-of-speech tagging and

id33.3 then, all tokens were lower-
cased, and sentences were shuf   ed to prevent struc-
tured bias. when learning id27s, we ig-
nored words with corpus frequency lower than 100,
yielding a vocabulary of about 500k words.4

2.2 window-based id27s
we used id97   s skip-gram model with neg-
ative sampling (mikolov et al., 2013b) to learn
window-based id27s. 5 this popular
method embeds both target words and contexts in
the same low-dimensional space, where the embed-
dings of a target and context are pushed closer to-
gether the more frequently they co-occur in a learn-
ing corpus. indirectly, this also results in similar em-
beddings for target words that co-occur with similar
contexts. more formally, this method optimizes the
following objective function:

l =

lt,c

(t,c)   pairs

(cid:88)
(cid:88)

(1)

(2)

lt,c = log   (v(cid:48)

c    vt) +

log   (   v(cid:48)

neg    vt)

neg   negs (t,c)

where vt and v(cid:48)
c are the vector representations of tar-
get word t and context word c. pairs is the set
of window-based co-occurring target-context pairs
considered by the model that depends on the win-
dow size, and negs (t,c) is a set of randomly sam-
pled context words used with the pair (t, c).6

we experimented with window sizes of 1, 5,
and 10, and various dimensionalities. we denote a
window-based id27 with window size
of n and dimensionality of m with wnm. for ex-
ample, w5300 is a id27 learned using a
window size of 5 and dimensionality of 300.

2.3 dependency-based id27s
we used id97f7 (levy and goldberg, 2014),
to learn dependency-based id27s from

3parses follow the universal dependencies formalism and

were produced by stanford corenlp, version 3.5.2

4our id27s are available at: www.cs.biu.ac.
il/nlp/resources/downloads/embeddings-contexts/
5we used negative sampling = 5 and iterations = 3 in all of

the experiments described in this paper.

6for more details refer to mikolov et al. (2013b).
7http://bitbucket.org/yoavgo/id97f

the parsed version of our corpus, similar to the ap-
proach of bansal et al. (2014). id97f ac-
cepts as its input arbitrary target-context pairs.
in
the case of dependency-based id27s,
the context elements are the syntactic contexts of
the target word, rather than the words in a win-
dow around it. speci   cally, following levy and
goldberg (2014), we    rst    collapsed    prepositions
(as implemented in id97f). then, for a tar-
get word t with modi   ers m1,...,mk and head h,
we paired the target word with the context elements
(m1, r1),...,(mk, rk),(h, r   1
h ), where r is the type of
the dependency relation between the head and the
modi   er (e.g., dobj, prep of ) and r   1 denotes an in-
verse relation. we denote a dependency-based word
embedding with dimensionality of m by depm. we
note that under this setting id97f optimizes
the same objective function described in equa-
tion (1), with pairs now comprising dependency-
based pairs instead of window-based ones.

2.4 substitute-based id27s

substitute vectors are a recent approach to represent-
ing contexts of target words, proposed in yatbaz et
al. (2012). instead of the neighboring words them-
selves, a substitute vector includes the potential    ller
words for the target word slot, weighted according
to how       t    they are to    ll the target slot given the
neighboring words. for example, the substitute vec-
tor representing the context of the word love in    i
love my job   , could look like: [quit 0.5, love 0.3,
hate 0.1, lost 0.1]. substitute-based contexts are
generated using a language model and were suc-
cessfully used in id65 models for
part-of-speech induction (yatbaz et al., 2012), word
sense induction (baskaya et al., 2013), functional se-
mantic similarity (melamud et al., 2014) and lexical
substitution tasks (melamud et al., 2015a).

similar to yatbaz et al. (2012), we consider the
words in a substitute vector, as a weighted set of con-
texts    co-occurring    with the observed target word.
for example, the above substitute vector is consid-
ered as the following set of weighted target-context
pairs: {(love, quit, 0.5), (love, love, 0.3), (love,
hate, 0.1), (love, lost, 0.1)}. to learn word embed-
dings from such weighted target-context pairs, we
extended id97f by modifying the objective

w10300
played
play
plays
professionally
player

dep300
play
played
understudying
caddying
plays

sub300
singing
rehearsing
performing
composing
running

table 1: the top    ve words closest to target word
playing in different embedding spaces.

function in equation (1) as follows:

(cid:88)

l =

(t,c)   pairs

  t,c    lt,c

(3)

where   t,c
is the weight of the target-context
pair (t, c). with this simple modi   cation, the effect
of target-context pairs on the learned word represen-
tations becomes proportional to their weights.

to generate the substitute vectors we followed
the methodology in (yatbaz et al., 2012; mela-
mud et al., 2015a). we learned a 4-gram kneser-
ney language model from our learning corpus us-
ing kenlm (hea   eld et al., 2013). then, we used
fastsubs (yuret, 2012) with this language model
to ef   ciently generate substitute vectors, where the
weight of each substitute s is the conditional proba-
bility p(s|c) for this substitute to    ll the target slot
given the sentential context c. for ef   ciency, we
pruned the substitute vectors to their top-10 sub-
stitutes, s1..s10, and normalized their probabilities
i=1..10 p(si|c) = 1. we also gener-
ated only up to 20,000 substitute vectors for each tar-
get word type. finally, we converted each substitute
vector into weighted target-substitute pairs and used
our extended version of id97f to learn the
substitute-based id27s, denoted subm.

such that(cid:80)

2.5 qualitative effect of context type
to motivate the rest of our work, we    rst qualita-
tively inspect the top most-similar words to some
target words, using cosine similarity of their respec-
tive embeddings. as illustrated in table 1, in embed-
dings learned with large window contexts, we see
both functionally similar words and topically similar
words, sometimes with a different part-of-speech.
with small windows and dependency contexts, we
generally see much fewer topically similar words,
which is consistent with previous    ndings (bansal et

al., 2014; levy and goldberg, 2014). finally, with
substitute-based contexts, there appears to be even a
stronger preference for functional similarity, with a
tendency to also strictly preserve verb tense.

3 id27 combinations

as different choices of context type yield word
embeddings with different properties, we hypothe-
size that combinations of such embeddings could be
more informative for some extrinsic tasks. we ex-
perimented with two alternative approaches to com-
bine different sets of id27s: (1) sim-
ple vector concatenation, which is a lossless com-
bination that comes at the cost of increased dimen-
sionality, and (2) svd and cca, which are lossy
combinations that attempt to capture the most use-
ful information from the different embeddings sets
with lower dimensionality. the methods used are
described in more detail next.

3.1 concatenation

perhaps the simplest way to combine two different
sets of id27s (sharing the same vocabu-
lary) is to concatenate their word vectors for every
word type. we denote such a combination of word
embedding set a with id27 set b using
the symbol (+). for example w10+dep600 is the
concatenation of w10300 with dep300. naturally,
the dimensionality of the concatenated embeddings
is the sum of the dimensionalities of the component
embeddings. in our experiments, we only ever com-
bine id27s of equal dimensionality.

the motivation behind concatenation relates pri-
marily to supervised models in extrinsic tasks.
in
such settings, we hypothesize that using concate-
nated id27s as input features to a classi-
   er could let it choose and combine (i.e., via learned
weights) the most suitable features for the task. con-
sider a situation where the concatenated embedding
w10+dep600 is used to represent the word inputs
to a id39 classi   er. in this case,
the classi   er could choose, for instance, to represent
entity words mostly with dependency-based embed-
ding features (re   ecting functional semantics), and
surrounding words with large window-based embed-
ding features (re   ecting topical semantics).

3.2 singular value decomposition
singular value decomposition (svd) has been
shown to be effective in compressing sparse word
representations (levy et al., 2015). in this work, we
use this technique in the same way to reduce the di-
mensionality of concatenated id27s.

3.3 canonical correlation analysis
recent work used canonical correlation analysis
(cca) to derive an improved set of word embed-
dings. the main idea is that two distinct sets of
id27s, learned with different types of in-
put data, are considered as multi-views of the same
vocabulary. then, cca is used to project each onto
a lower dimensional space, where correlation be-
tween the two is maximized. the correlated infor-
mation is presumably more reliable. dhillon et al.
(2011) considered their two cca views as embed-
dings learned from the left and from the right con-
text of the target words, showing improvements on
chunking and id39. faruqui and
dyer (2014) and lu et al. (2015) considered multi-
lingual views, showing improvements in several in-
trinsic tasks, such as word and phrase similarity.

inspired by this prior work, we consider pairs of
id27 sets, learned with different types
of context, as different views and correlate them
using linear cca.8 we use either the siid113x-
999 or wordsim-353-r intrinsic benchmark (sec-
tion 4.1) to tune the cca hyperparameters9 with
the spearmint bayesian optimization tool10 (snoek
et al., 2012). this results in different projections
for each of these tuning objectives, where siid113x-
999/wordsim-353-r is expected to give some bias
towards functional/topical similarity, respectively.

4 evaluation

intrinsic benchmarks

4.1
we employ several commonly used intrinsic bench-
marks for assessing how well id27s
mimic human judgements of semantic similarity of
words. the popular wordsim-353 dataset (finkel-
stein et al., 2001) includes 353 word pairs manually

8see faruqui and dyer (2014), lu et al. (2015) for details.
9these are projection dimensionality and id173.
10github.com/jaspersnoek/spearmint

annotated with a degree of similarity. for exam-
ple, computer:keyboard is annotated with 7.62, in-
dicating a relatively high degree of similarity. while
wordsim-353 does not make a distinction between
different       avors    of similarity, agirre et al. (2009)
proposed two subsets of this dataset, wordsim-353-
s and wordsim-353-r, which focus on functional
and topical similarities, respectively. siid113x-999
(hill et al., 2014) is a larger word pair similarity
dataset with 999 annotated pairs, purposely built to
focus on functional similarity. we evaluate our em-
beddings on these datasets by computing a score
for each pair as the cosine similarity of two word
vectors. the spearman   s correlation11 between the
ranking of word pairs induced from the human an-
notations and that from the embeddings is reported.
the toefl task contains 80 synonym selection
items, where a synonym of a target word is to be se-
lected out of four possible choices. we report the
overall accuracy of a system that uses cosine dis-
tance between the embeddings of the target word
and each of the choices to select the one most similar
to the target word as the answer.

4.2 extrinsic benchmarks

the following four diverse downstream nlp tasks
serve as our extrinsic benchmarks.12

1) id33 (parse) the stanford
neural network dependency (nndep) parser
(chen and manning, 2014) uses dense continuous
representations of words, parts-of-speech and de-
pendency labels. while it can learn these repre-
sentations entirely during the training on labeled
data, chen and manning (2014) show that initializa-
tion with id27s, which were pre-trained
on unlabeled data, yields improved performance.
hence, we used our different types of embeddings to
initialize the nndep parser and compared their per-
formance on a standard id32 benchmark.
we used wsj sections 2   21 for training and 22 for
development. we used predicted tags produced via
20-fold jackkni   ng on sections 2   21 with the stan-
ford corenlp tagger.

11we used spearmanr, scipy version 0.15.1.
12since our goal is to explore performance trends, we mostly

experimented with the tasks    development sets.

2) id39 (ner) we used the
ner system of turian et al. (2010), which allows
adding id27 features (on top of various
other features) to a regularized averaged id88
classi   er, and achieves near state-of-the-art results
using several off-the-shelf word representations. we
varied the type of id27s used as features
when training the ner model, to evaluate their ef-
fect on ner benchmarks results. following turian
et al. (2010), we used the conll-2003 shared task
dataset (tjong kim sang and de meulder, 2003)
with 204k/51k train/dev words, as our main bench-
mark. we also performed an out-of-domain eval-
uation, using conll-2003 as the train set and the
muc7 formal run (59k words) as the test set.13

3) coreference resolution (coref) we used the
berkeley coreference system (durrett and klein,
2013), which achieves near state-of-the-art results
with a log-linear supervised model. most of the fea-
tures in this model are associated with pairs of cur-
rent and antecedent reference mentions, for which a
coreference decision needs to be made. to evaluate
the contribution of different id27 types
to this model, we extended it to support the follow-
ing additional features: {ai}i=1..m, {ci}i=1..m and
{ai    ci}i=1..m, where ai or ci is the value of the ith
dimension in a id27 vector represent-
ing the antecedent or current mention, respectively.
we considered two different id27 repre-
sentations for a mention: (1) the embedding of the
head word of the mention and (2) the average em-
bedding of all words in the mention. the features
of both types of representations were presented to
the learning model as inputs at the same time. they
were added on top of berkeley   s full feature list (   fi-
nal   ) as described in durrett and klein (2013). we
evaluated our features on the conll-2012 corefer-
ence shared task (pradhan et al., 2012).

id31

4)
(senti) following
faruqui et al. (2014), we used a sentence-level
binary decision version of the id31
task from socher et al. (2013).
in this setting,
neutral sentences were discarded and all remaining
sentences were labeled coarsely as positive or neg-
ative. maintaining the original split into train/dev

13see turian et al. (2010) for more details on this setting.

figure 1: intrinsic tasks    results for embeddings learned with different types of contexts.

results, we get a dataset containing 6920/872
sentences. to evaluate different types of word
embeddings, we represented each sentence as an
average of its id27s and then used an
l2-regularized id28 classi   er trained
on these features to predict the sentiment labels.

intrinsic results for context types

5 results
5.1
the results on the intrinsic tasks are illustrated in
figure 1. first, we see that the performance on all
tasks generally increases with the number of dimen-
sions, reaching near-optimal performance at around
300 dimensions, for all types of contexts. this is
in line with similar observations on skip-gram word
embeddings (mikolov et al., 2013a).

looking further, we observe that there are sig-
ni   cant differences in the results when using dif-
ferent types of contexts. the effect of context

choice is perhaps most evident in the wordsim-353-
r task, which captures topical similarity. as might
be expected, in this benchmark, the largest-window
id27s perform best. the performance
decreases with the decrease in window size and
then reaches signi   cantly lower levels for depen-
dency (dep) and substitute-based (sub) embed-
dings. conversely, in wordsim-353-s and siid113x-
999, both of which capture a more functional simi-
larity, the dep embeddings are the ones that perform
best, strengthening similar observations in levy and
goldberg (2014). finally, in the toefl benchmark,
all contexts except for sub, perform comparably.

5.2 extrinsic results for context types

the extrinsic tasks results are illustrated in figure 2.
a    rst observation is that optimal extrinsic results
may be reached with as few as 50 dimensions. fur-
thermore, performance may even degrade when us-

figure 2: extrinsic tasks    development set results for embeddings learned with different types of contexts.
   base    denotes the results with no id27 features. due to computational limitations we tested
ner and parse with only up to 300 dimensions embeddings, and coref with up to 100.

ing too many dimensions, as is most evident in the
ner task. this behavior presumably depends on
various factors, such as the size of the labeled train-
ing data or the type of classi   er used, and highlights
the importance of tuning the dimensionality of word
embeddings in extrinsic tasks. this is in contrast
to intrinsic tasks, where higher dimensionality typi-
cally yields better results.

next, comparing the results of different types of
contexts, we see, as might be expected, that de-
pendency embeddings work best in the parse task.
more generally, embeddings that do well in func-
tional similarity intrinsic benchmarks and badly in
topical ones (dep, sub and w1) work best for
parse, while large window contexts perform worst,
similar to observations in bansal et al. (2014).

in the rest of the tasks it   s dif   cult to say which
context works best for what. one possible expla-

context type
dep
w1
sub
w10
w5
none

f1 x 100

79.8
79.3
79.0
78.1
77.4
71.8

table 2: ner muc out-of-domain results for dif-
ferent embeddings with dimensionality = 25.

nation to this in the case of ner and coref is that
the embedding features are used as add-ons to an al-
ready competitive learning system. therefore, the
total improvement on top of a    no embedding    base-
line is relatively small, leaving little room for signif-
icant differences between different contexts.

we did    nd a more notable contribution of word

gles with identical dimensionality. for example, the
200-dimensional concat w10+dep200, which is a
concatenation of w10100 and dep100, is compared
against 200-dimensional singles, such as w10200.

looking at the results, it seems like the bene-
   t from concatenation depends on the dimension-
ality and task at hand, as also illustrated in fig-
ure 3. given task x and dimensionality d, if d
2 is in
the range where increasing the dimensionality yields
signi   cant improvement on task x, then it   s better
to simply increase dimensionality of singles from d
2
to d rather than concatenate. the most evident ex-
ample for this are the results on the senti task with
d = 50. in this case, the bene   t from concatenat-
ing two 25-dimensional singles is notably lower than
that of using a single 50-dimensional word embed-
ding. on the other hand, if d
2 is in the range where
near-optimal performance is reached on task x, then
concatenation seems to pay off. this can be seen in
senti with d = 600, parse with d = 200, and ner
with d = 50. more concretely, looking at the best
performing concatenations, it seems like combina-
tions of the topical w10 embedding with one of the
more functional ones, sub, dep or w1, typically
perform best, suggesting that there is added value in
combining embeddings of different nature.

finally, our experiments with the methods using
svd (section 3.2) and cca (section 3.3) yielded
degraded performance compared to single word em-
beddings for all extrinsic tasks and therefore are not
reported for brevity. these results seem to further
strengthen the hypothesis that the information cap-
tured with varied types of context is different and
complementary, and therefore it is bene   cial to pre-
serve these differences as in our concatenation ap-
proach.

6 related work

there are a number of recent works whose goal is
a broad evaluation of the performance of different
id27s on a range of tasks. however,
to the best of our knowledge, none of them focus
on embeddings learned with diverse context types
as we do. levy et al. (2015), lapesa and evert
(2014), and lai et al. (2015) evaluate several design
choices when learning word representations. how-
ever, levy et al. (2015) and lapesa and evert (2014)

figure 3: mean development set results for the tasks
parse and senti.
   mean    and    mean+    stand for
mean results across all single context types and con-
text concatenations, respectively.

embedding features to the overall system perfor-
mance in the out-of-domain ner muc evaluation,
described in table 2. in this out-of-domain setting,
all types of contexts achieve at least    ve points im-
provement over the baseline. presumably, this is be-
cause continuous id27 features are more
robust to differences between train and test data,
such as the typical vocabulary used. however, a de-
tailed investigation of out-of-domain settings is out
of scope for this paper and left for future work.

5.3 extrinsic results for combinations
a comparison of the results obtained on the ex-
trinsic tasks using the id27 concate-
nations (concats), described in section 3.1, versus
the original single context id27s (sin-
gles), appears in table 3. to control for dimen-
sionality, concats are always compared against sin-

50

dimensions result
best+
best
mean+
mean
best+
best
mean+
mean
best+
best
mean+
mean

200

600

parse

88.7 (w10+sub)
88.9 (w1)
88.3
88.4
89.1 (w1+dep)
88.8 (sub)
88.9
88.6

ner

93.6 (w1+dep)
93.3 (w1)
93.3
93.1
93.1 (w10+dep)
92.8 (w10)
92.8
92.4

coref

62.4 (w10+w1)
62.3 (dep)
62.1
62.2

senti

74.3 (w10+w1)
77.3 (sub)
72.7
74.7
81.0 (w10+sub)
80.2 (w10)
79.1
79.9
82.6 (w10+sub)
82.3 (w1)
82.0
81.5

table 3: extrinsic tasks development set results obtained with id27s concatenations.    best   
and    best+    are the best results achieved across all single context types and context concatenations, respec-
tively (best performing embedding indicated in parenthesis).    mean    and    mean+    are the mean results for
the same. due to computational limitations of the employed systems, some of the evaluations were not
performed.

perform only intrinsic evaluations and restrict con-
text representation to word windows, while lai et
al. (2015) do perform extrinsic evaluations, but re-
strict their context representation to a word window
with the default size of 5. schnabel et al. (2015)
and tsvetkov et al. (2015) report low correlation be-
tween intrinsic and extrinsic results with different
id27s (they did not evaluate different
context types), which is consistent with differences
we found between intrinsic and extrinsic perfor-
mance patterns in all tasks, except parsing. bansal et
al. (2014) show that functional (dependency-based
and small-window) embeddings yield higher pars-
ing improvements than topical (large-window) em-
beddings, which is consistent with our    ndings.

several works focus on particular types of con-
texts for learning id27s. cirik and
yuret (2014) investigates s-code word embed-
dings based on substitute word contexts. ling et al.
(2015b) and ling et al. (2015a) propose extensions
to the standard window-based context modeling.
alternatively, another recent popular line of work
(faruqui et al., 2014; kiela et al., 2015) attempts
to improve id27s by using manually-
constructed resources, such as id138. these
techniques could be complementary to our work. fi-
nally, yin and sch  utze (2015) and goikoetxea et al.
(2016) propose id27s combinations, us-
ing methods such as concatenation and cca, but

evaluate mostly on intrinsic tasks and do not con-
sider different types of contexts.

7 conclusions

in this paper we evaluated skip-gram word embed-
dings on multiple intrinsic and extrinsic nlp tasks,
varying dimensionality and type of context. we
show that while the best practices for setting skip-
gram hyperparameters typically yield good results
on intrinsic tasks, success on extrinsic tasks requires
more careful thought. speci   cally, we suggest that
picking the optimal dimensionality and context type
are critical for obtaining the best accuracy on ex-
trinsic tasks and are typically task-speci   c. further
improvements can often be achieved by combining
complementary id27s of different con-
text types with the right dimensionality.

acknowledgments

we thank do kook choe for providing us the jack-
knifed version of wsj. we also wish to thank the
ibm watson team for helpful discussions and our
anonymous reviewers for their comments. this
work was partially supported by the israel science
foundation grant 880/12 and the german research
foundation through the german-israeli project co-
operation (dip, grant da 1600/1-1).

references
[agirre et al.2009] eneko agirre, enrique alfonseca,
keith hall, jana kravalova, marius pas  ca, and aitor
soroa. 2009. a study on similarity and relatedness
using distributional and id138-based approaches.
in proceedings of naacl. association for computa-
tional linguistics.

[bansal et al.2014] mohit bansal, kevin gimpel, and
karen livescu. 2014. tailoring continuous word rep-
resentations for id33. in proceedings
of the annual meeting of the association for compu-
tational linguistics.

[baskaya et al.2013] osman baskaya, enis sert, volkan
cirik, and deniz yuret. 2013. ai-ku: using substitute
vectors and co-occurrence modeling for word sense in-
duction and disambiguation. in proceedings of the se-
meval.

[chen and manning2014] danqi chen and christopher d
2014. a fast and accurate dependency
manning.
in proceedings of
parser using neural networks.
the 2014 conference on empirical methods in natu-
ral language processing (emnlp), volume 1, pages
740   750.

[cirik and yuret2014] volkan cirik and deniz yuret.
2014. substitute based scode id27s in su-
pervised nlp tasks. arxiv preprint arxiv:1407.6853.

[collobert et al.2011] ronan collobert, jason weston,
l  eon bottou, michael karlen, koray kavukcuoglu,
and pavel kuksa. 2011. natural language process-
ing (almost) from scratch. the journal of machine
learning research, 12:2493   2537.

[dhillon et al.2011] paramveer dhillon, dean p foster,
and lyle h ungar. 2011. multi-view learning of word
embeddings via cca. in advances in neural informa-
tion processing systems, pages 199   207.

[durrett and klein2013] greg durrett and dan klein.
2013. easy victories and uphill battles in coreference
resolution. in proc. of emnlp.

[faruqui and dyer2014] manaal faruqui and chris dyer.
improving vector space word representations
in proceedings of

2014.
using multilingual correlation.
eacl.

[faruqui et al.2014] manaal faruqui, jesse dodge, su-
jay k jauhar, chris dyer, eduard hovy, and noah a
smith. 2014. retro   tting word vectors to semantic
lexicons. in proceedings of deep learning and rep-
resentation learning workshop, nips.

[finkelstein et al.2001] lev

finkelstein,

evgeniy
gabrilovich, yossi matias, ehud rivlin, zach
solan, gadi wolfman, and eytan ruppin.
2001.
placing search in context: the concept revisited.
in
proceedings of the 10th international conference on
world wide web, pages 406   414. acm.

[goikoetxea et al.2016] josu goikoetxea, eneko agirre,
and aitor soroa. 2016. single or multiple? combining
word representations independently learned from text
and id138. in proceedings of aaai.

[han et al.2013] lushan han, abhay l. kashyap, tim
finin, james may   eld, and johnathan weese. 2013.
umbc ebiquity-core: semantic textual simi-
in proceedings of the second joint
larity systems.
conference on lexical and computational semantics.
association for computational linguistics, june.

[hea   eld et al.2013] kenneth

ivan
pouzyrevsky, jonathan h. clark, and philipp koehn.
2013. scalable modi   ed kneser-ney language model
estimation. in proceedings of acl.

hea   eld,

[hill et al.2014] felix hill, roi reichart, and anna ko-
rhonen.
siid113x-999: evaluating semantic
models with (genuine) similarity estimation. arxiv
preprint arxiv:1408.3456.

2014.

[kiela et al.2015] douwe kiela, felix hill, and stephen
clark. 2015. specializing id27s for simi-
larity or relatedness.

[lai et al.2015] siwei lai, kang liu, liheng xu, and jun
zhao. 2015. how to generate a good word embed-
ding? arxiv preprint arxiv:1507.05523.

[lapesa and evert2014] gabriella lapesa and stefan ev-
ert. 2014. a large scale evaluation of distributional
semantic models: parameters, interactions and model
selection. transactions of the association for compu-
tational linguistics, 2:531   545.

[levy and goldberg2014] omer levy and yoav gold-
berg. 2014. dependencybased id27s. in
proceedings of acl.

2015.

[levy et al.2015] omer levy, yoav goldberg, and ido
dagan.
improving distributional similarity
with lessons learned from id27s. transac-
tions of the association for computational linguistics,
3:211   225.

[ling et al.2015a] wang ling, lin chu-cheng, yulia
tsvetkov, silvio amir, ram  on fernandez astudillo,
chris dyer, alan w black, and isabel trancoso.
2015a. not all contexts are created equal: better word
in proceed-
representations with variable attention.
ings of emnlp.

[ling et al.2015b] wang ling, chris dyer, alan black,
and isabel trancoso. 2015b. two/too simple adap-
tations of id97 for syntax problems. in proceed-
ings of naacl-hlt.

[lu et al.2015] ang lu, weiran wang, mohit bansal,
kevin gimpel, , and karen livescu. 2015. deep mul-
tilingual correlation for improved id27s.
in proceedings of naacl.

[manning et al.2014] christopher d. manning, mihai
jenny finkel, steven j.

john bauer,

surdeanu,

sentiment treebank. in proceedings of the conference
on empirical methods in natural language processing
(emnlp), volume 1631, page 1642. citeseer.

[tjong kim sang and de meulder2003] erik f tjong
kim sang and fien de meulder. 2003. introduction
to the conll-2003 shared task: language-independent
the
id39.
seventh conference on natural
language learning
at hlt-naacl 2003-volume 4, pages 142   147.
association for computational linguistics.

in proceedings of

[tsvetkov et al.2015] yulia tsvetkov, manaal faruqui,
wang ling, guillaume lample, and chris dyer. 2015.
evaluation of word vector representations by subspace
alignment. in proc. of emnlp.

[turian et al.2010] j. turian, l. ratinov, and y. bengio.
2010. word representations: a simple and general
method for semisupervised learning. in proc. of acl,
pages 384   394.

[yatbaz et al.2012] mehmet ali yatbaz, enis sert, and
deniz yuret. 2012. learning syntactic categories us-
ing paradigmatic representations of word context. in
proceedings of emnlp.

[yin and sch  utze2015] wenpeng yin

and hinrich
sch  utze. 2015. learning word meta-embeddings by
using ensembles of embedding sets. arxiv preprint
arxiv:1508.04257.

[yuret2012] deniz yuret.

2012. fastsubs: an ef-
   cient and exact procedure for    nding the most
likely lexical substitutes based on an id165 language
model. signal processing letters, ieee, 19(11):725   
728.

2014. the stan-
bethard, and david mcclosky.
ford corenlp natural language processing toolkit. in
proceedings of 52nd annual meeting of the associa-
tion for computational linguistics: system demon-
strations, pages 55   60.

[melamud et al.2014] oren melamud, ido dagan, jacob
goldberger, idan szpektor, and deniz yuret. 2014.
probabilistic modeling of joint-context in distribu-
tional similarity. in proceedings of conll.

[melamud et al.2015a] oren melamud, ido dagan, and
jacob goldberger. 2015a. modeling word meaning
in context with substitute vectors. in proceedings of
naacl.

[melamud et al.2015b] oren melamud, omer levy, and
ido dagan. 2015b. a simple id27 model
for lexical substitution. in proceedings of the vector
space modeling for nlp workshop, naacl.

[mikolov et al.2013a] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013a. ef   cient estima-
tion of word representations in vector space. in pro-
ceedings of iclr.

[mikolov et al.2013b] tomas mikolov,

ilya sutskever,
kai chen, greg corrado, and jeffrey dean. 2013b.
distributed representations of words and phrases and
their compositionality. in proceedings of nips.

[parker et al.2011] robert parker, david graff, junbo
kong, ke chen, and kazuaki maeda. 2011. english
gigaword fifth edition. linguistic data consortium,
ldc2011t07, june.

[pennington et al.2014] jeffrey

pennington,

richard
socher, and christopher d manning. 2014. glove:
in proceed-
global vectors for word representation.
ings of the empiricial methods in natural language
processing (emnlp 2014), volume 12.

[pradhan et al.2012] sameer pradhan, alessandro mos-
chitti, nianwen xue, olga uryupina, and yuchen
zhang.
2012. conll-2012 shared task: modeling
multilingual unrestricted coreference in ontonotes. in
joint conference on emnlp and conll-shared task,
pages 1   40. association for computational linguis-
tics.

[schnabel et al.2015] tobias schnabel,

igor labutov,
david mimno, and thorsten joachims. 2015. evalu-
ation methods for unsupervised id27s. in
proc. of emnlp.

[snoek et al.2012] jasper snoek, hugo larochelle, and
ryan p adams. 2012. practical bayesian optimization
of machine learning algorithms. in advances in neural
information processing systems, pages 2951   2959.

[socher et al.2013] richard socher, alex perelygin,
jean y wu, jason chuang, christopher d manning,
andrew y ng, and christopher potts. 2013. recur-
sive deep models for semantic compositionality over a

