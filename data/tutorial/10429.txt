multi-source neural translation

barret zoph and kevin knight
information sciences institute

department of computer science
university of southern california
{zoph,knight}@isi.edu

6
1
0
2

 

n
a
j
 

5

 
 
]
l
c
.
s
c
[
 
 

1
v
0
1
7
0
0

.

1
0
6
1
:
v
i
x
r
a

abstract

we build a multi-source machine transla-
tion model and train it to maximize the
id203 of a target english string given
french and german sources. using the
neural encoder-decoder framework, we
explore several combination methods and
report up to +4.8 id7 increases on top of
a very strong attention-based neural trans-
lation model.

1

introduction

kay (2000) points out that if a document is trans-
lated once, it is likely to be translated again and
again into other languages. this gives rise to an
interesting idea: a human does the    rst transla-
tion by hand, then turns the rest over to machine
translation (mt). the translation system now has
two strings as input, which can reduce ambiguity
via    triangulation    (kay   s term). for example, the
normally ambiguous english word    bank    may be
more easily translated into french in the presence
of a second, german input string containing the
word    flussufer    (river bank).

och and ney (2001) describe such a multi-
source mt system. they    rst train separate bilin-
gual mt systems f   e, g   e, etc. at run-
time, they separately translate input strings f and
g into candidate target strings e1 and e2, then se-
lect the best one of the two. a typical selec-
tion factor is the product of the system scores.
schwartz (2008) revisits such factors in the con-
text of id148 and id7 score, while
max et al. (2010) re-rank f   e n-best lists us-
ing id165 precision with respect to g   e trans-
lations. callison-burch (2002) exploits hypothesis
selection in multi-source mt to expand available
corpora, via co-training.

others use system combination techniques to
merge hypotheses at the word level, creating the

ability to synthesize new translations outside those
proposed by the single-source translators. these
methods include confusion networks (matusov et
al., 2006; schroeder et al., 2009), source-side
string combination (schroeder et al., 2009), and
median strings (gonz  alez-rubio and casacuberta,
2010).

the above work all relies on base mt systems
trained on bilingual data, using traditional meth-
ods. this follows early work in sentence align-
ment (gale and church, 1993) and word align-
ment (simard, 1999), which exploited trilingual
text, but did not build trilingual models. previous
authors possibly considered a three-dimensional
translation table t(e|f, g) to be prohibitive.
in this paper, by contrast, we train a p(e|f, g)
model directly on trilingual data, and we use that
model to decode an (f, g) pair simultaneously. we
view this as a kind of multi-tape transduction (el-
got and mezei, 1965; kaplan and kay, 1994; deri
and knight, 2015) with two input tapes and one
output tape. our contributions are as follows:
    we train a p(e|f, g) model directly on trilin-
gual data, and we use it to decode a new
source string pair (f, g) into target string e.
    we show positive id7 improvements over
    we show that improvements are best when
the two source languages are more distant
from each other.

strong single-source baselines.

we are able to achieve these results using
the framework of neural encoder-decoder models,
where multi-target mt (dong et al., 2015) and
multi-source, cross-modal mappings have been
explored (luong et al., 2015a).

2 multi-source neural mt

in the neural encoder-decoder framework for mt
(neco and forcada, 1997; casta  no and casacu-
berta, 1997; sutskever et al., 2014; bahdanau et
al., 2014; luong et al., 2015b), we use a recur-

figure 1: the encoder-decoder framework for id4 (id4) (sutskever et al., 2014).
here, a source sentence c b a (presented in reverse order as a b c) is translated into a target sentence
w x y z. at each step, an evolving real-valued vector summarizes the state of the encoder (white) and
decoder (gray).

rent neural network (encoder) to convert a source
sentence into a dense,    xed-length vector. we then
use another recurrent network (decoder) to convert
that vector in a target sentence.1

in this paper, we use a four-layer encoder-
decoder system (figure 1) with long short-term
memory (lstm) units (hochreiter and schmid-
huber, 1997) trained for maximum likelihood (via
a softmax layer) with back-propagation through
time (werbos, 1990). for our baseline single-
source mt system we use two different models,
one of which implements the local attention plus
feed-input model from luong et al. (2015b).

figure 2 shows our approach to multi-source
mt. each source language has its own encoder.
the question is how to combine the hidden states
and cell states from each encoder, to pass on to
the decoder. black combiner blocks implement a
function whose input is two hidden states (h1 and
h2) and two cell states (c1 and c2), and whose out-
put is a single hidden state h and cell state c. we
propose two combination methods.

2.1 basic combination method
the basic method works by concatenating the two
hidden states from the source encoders, applying a
linear transformation wc (size 2000 x 1000), then
sending its output through a tanh non-linearity.
this operation is represented by the equation:

(cid:16)

(cid:17)

h = tanh

wc[h1; h2]

(1)

wc and all other weights in the network are

1we follow previous authors in presenting the source sen-

tence to the encoder in reverse order.

learned from example string triples drawn from a
trilingual training corpus.

the new cell state is simply the sum of the two

cell states from the encoders.

c = c1 + c2

(2)

we also attempted to concatenate cell states and
apply a linear transformation, but training diverges
due to large cell values.

2.2 child-sum method
our second combination method is inspired by the
child-sum tree-lstms of tai et al. (2015). here,
we use an lstm variant to combine the two hid-
den states and cells. the standard lstm input,
output, and new cell value are all calculated. then
cell states from each encoder get their own forget
gates. the    nal cell state and hidden state are cal-
culated as in a normal lstm. more precisely:

(cid:16)
(cid:16)

(cid:17)
(cid:17)

i = sigmoid

w i

1h1 + w i

2h2

(cid:16)

(cid:17)

f = sigmoid

w f

i hi

w o

2 h2

(cid:16)

o = sigmoid

1 h1 + w o

(cid:17)
c = if (cid:12) uf + f1 (cid:12) c1 + f2 (cid:12) c2

1 h1 + w u

u = tanh

2 h2

w u

h = of (cid:12) tanh(cf )

(3)

(4)

(5)

(6)

(7)

(8)

this method employs eight new matrices (the
w    s in the above equations), each of size
1000 x 1000. the (cid:12) symbol represents an elemen-
twise multiplication.
in equation 3, i represents

a b c <eos> w x y z <eos> z y x w figure 2: multi-source encoder-decoder model for mt. we have two source sentences (c b a and k j i)
in different languages. each language has its own encoder; it passes its    nal hidden and cell state to a set
of combiners (in black). the output of a combiner is a hidden state and cell state of the same dimension.

the input gate of a typical lstm cell.
in equa-
tion 4, there are two forget gates indexed by the
subscript i that serve as the forget gates for each
of the incoming cells for each of the encoders. in
equation 5, o represents the output gate of a nor-
mal lstm. i, f, o, and u are all size-1000 vectors.

2.3 multi-source attention
our single-source attention model is modeled off
the local-p attention model with feed input from
luong et al. (2015b), where hidden states from the
top decoder layer can look back at the top hidden
states from the encoder. the top decoder hidden
state is combined with a weighted sum of the en-
coder hidden states, to make a better hidden state
vector (   ht), which is passed to the softmax output
layer. with input-feeding, the hidden state from
the attention model is sent down to the bottom de-
coder layer at the next time step.

the local-p attention model from luong et al.
(2015b) works as follows. first, a position to look
at in the source encoder is predicted by equation 9:

pt = s    sigmoid(vt

p tanh(wpht))

(9)

s is the source sentence length, and vp and wp
are learned parameters, with vp being a vector of
dimension 1000, and wp being a matrix of dimen-
sion 1000 x 1000.

after pt is computed, a window of size 2d + 1
is looked at in the top layer of the source encoder
centered around pt (d = 10). for each hidden
state in this window, we compute an alignment

score at(s), between 0 and 1. this alignment score
is computed by equations 10, 11 and 12:

(cid:16)   (s     pt)2

(cid:17)

2  2

at(s) = align(ht, hs)exp

align(ht, hs) =

(cid:80)

exp(score(ht, hs))
s(cid:48) exp(score(ht, hs(cid:48)))

score(ht, hs) = ht

t wahs

(10)

(11)

(12)

in equation 10,    is set to be d/2 and s is the
source index for that hidden state. wa is a learn-
able parameter of dimension 1000 x 1000.

once all of the alignments are calculated, ct is
created by taking a weighted sum of all source hid-
den states multiplied by their alignment weight.

the    nal hidden state sent to the softmax layer

is given by:

  ht = tanh

wc[ht; ct]

(13)

we modify this attention model to look at both
source encoders simultaneously. we create a con-
text vector from each source encoder named c1
t
and c2
t instead of the just ct in the single-source
attention model:

  ht = tanh

wc[ht; c1

t ; c2
t ]

(14)

in our multi-source attention model we now
have two pt variables, one for each source encoder.

(cid:17)

(cid:17)

(cid:16)

(cid:16)

a b c <eos> w x y z <eos> z y x w a b c <eos> w x y z <eos> z y x w i j k french
66.2m
424,832

27.8

english german
59.4m
57.0m
865,806
381,062
2,378,112
25.0

24.0

word tokens
word types
segment pairs
ave. segment
length (tokens)

figure 3: trilingual corpus statistics.

target = english

method
   
   

ppl id7
source
21.0
10.3
french
15.9
17.3
german
23.2
8.7
french+german basic
22.5
9.0
french+german child-sum
child-sum 10.9
20.7
french+french
25.2
8.1
french
attention
30.0
5.7
french+german b-attent.
french+german cs-attent.
6.0
29.6

figure 5: action of the multi-attention model as
the neural decoder generates target english from
french/german sources (test set). lines show
strengths of at(s).

target = german

method
   
   

source
french
english
french+english basic
french+english child-sum
english
attention
french+english b-attent.
french+english cs-attent.

ppl id7
12.3
10.6
13.4
9.6
14.5
9.1
9.5
14.4
17.6
7.3
18.6
6.9
7.1
18.2

figure 4: multi-source mt for target english, with
source languages french and german. ppl reports
test-set perplexity as the system predicts english
tokens. id7 is scored using the multi-id7.perl
script from moses. for our evaluation we use a
single reference and they are case sensitive.

we also have two separate sets of alignments and
therefore now have two ct values denoted by c1
t
and c2
t as mentioned above. we also have distinct
wa, vp, and wp parameters for each encoder.

3 experiments

we use english, french, and german data from
a subset of the wmt 2014 dataset (bojar et al.,
2014). figure 3 shows statistics for our training
set. for development, we use the 3000 sentences
supplied by wmt. for testing, we use a 1503-line
trilingual subset of the wmt test set.

for the single-source models, we follow the
training procedure used in luong et al. (2015b),
but with 15 epochs and halving the learning rate
every full epoch after the 10th epoch. we also re-
scale the normalized gradient when norm > 5. for
training, we use a minibatch size of 128, a hidden
state size of 1000, and dropout as in zaremba et al.
(2014). the dropout rate is 0.2, the initial parame-
ter range is [-0.1, +0.1], and the learning rate is 1.0.
for the normal and multi-source id12,
we adjust these parameters to 0.3, [-0.08, +0.08],
and 0.7, respectively, to adjust for over   tting.

figure 6: multi-source mt results for target ger-
man, with source languages french and english.

figure 4 show our results for target english,
with source languages being french and german).
we see that the basic combination method yields a
+4.8 id7 improvement over the strongest single-
source, attention-based system. it also improves
id7 by +2.2 over the non-attention baseline. the
child-sum method gives improvements of +4.4
and +1.4. we also con   rm that two copies of the
same french input yields no id7 improvement.
figure 5 shows the action of the multi-attention

model during decoding.

when our source languages are english and
french (figure 6), we observe smaller id7
gains (up to +1.1). this is evidence that the more
distinct the source languages, the better they dis-
ambiguate each other.

4 conclusion

we describe a multi-source neural mt system that
gets up to +4.8 id7 gains over a very strong
attention-based, single-source baseline. we ob-
tain this result through a novel encoder-vector
combination method and a novel multi-attention
system. we release the code for these experi-
ments at https://github.com/isi-nlp/
zoph_id56.

source 1: unk aspekte sind ebenfalls wichtig .  target:     unk aspects are important , too .  source 2: les aspects unk sont   galement importants . r. neco and m. forcada. 1997. asynchronous trans-
lations with recurrent neural nets. in international
conf. on neural networks, volume 4, pages 2535   
2540.

f. j. och and h. ney. 2001. statistical multi-source

translation. in proc. mt summit.

j. schroeder, t. cohn, and p. koehn. 2009. word lat-

tices for multi-source translation. in proc. eacl.

l. schwartz. 2008. multi-source translation methods.

proc. amta.

m. simard.

1999.

three languages are better than two.
emnlp/vlc.

text-translation alignment:
in proc.

i. sutskever, o. vinyals, and q. v. le. 2014. sequence
to sequence learning with neural networks. in proc.
nips.

k. s. tai, r. socher, and c. manning. 2015. improved
semantic representations from tree-structured long
short-term memory networks. in proc. acl.

p. j. werbos. 1990. id26 through time:
what it does and how to do it. proceedings of the
ieee, 78(10):1550   1560.

w. zaremba, i. sutskever, and o. vinyals.

2014.
recurrent neural network id173. corr,
abs/1409.2329.

references
d. bahdanau, k. cho, and y. bengio. 2014. neural
machine translation by jointly learning to align and
translate. in proc. iclr.

o. bojar, c. buck, c. federmann, b. haddow,
p. koehn, c. monz, m. post, and l. specia, edi-
tors. 2014. proc. of the ninth workshop on sta-
tistical machine translation. association for com-
putational linguistics.

c. callison-burch. 2002. co-training for statistical
machine translation. master   s thesis, school of in-
formatics, university of edinburgh.

m. a. casta  no and f. casacuberta. 1997. a con-
nectionist approach to machine translation. in eu-
rospeech.

a. deri and k. knight. 2015. how to make a fren-
emy: multitape fsts for portmanteau generation.
in proc. naacl.

d. dong, h. wu, w. he, d. yu, and h. wang. 2015.
id72 for multiple language transla-
tion. in proc. acl.

c. elgot and j. mezei. 1965. on relations de   ned
by generalized    nite automata. ibm journal of re-
search and development, 9(1):47   68.

w. a gale and k. w church. 1993. a program for
aligning sentences in bilingual corpora. computa-
tional linguistics, 19(1):75   102.

j. gonz  alez-rubio and f. casacuberta. 2010. on the
use of median string for multi-source translation. in
proc. icpr.

s. hochreiter and j. schmidhuber. 1997. long short-

term memory. neural computation, 9(8).

r. kaplan and m. kay. 1994. regular models of
phonological rule systems. computational linguis-
tics, 20(3):331   378.

m. kay. 2000. triangulation in translation. keynote

at mt 2000 conference, university of exeter.

m. luong, q. v. le, i. sutskever, o. vinyals, and
l. kaiser. 2015a. multi-task sequence to sequence
learning. in arxiv. http://arxiv.org/abs/1511.06114.

m. luong, h. pham, and c. manning. 2015b. effec-
tive approaches to attention-based neural machine
translation. in proc. emnlp.

e. matusov, n. uef   ng, and h. ney. 2006. computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. in proc. eacl.

a. max, j. crego, and f. yvon. 2010. contrastive
lexical evaluation of machine translation. in proc.
lrec.

