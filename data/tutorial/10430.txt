end-to-end id36 using lstms

on sequences and tree structures

makoto miwa

toyota technological institute

nagoya, 468-8511, japan

makoto-miwa@toyota-ti.ac.jp

mohit bansal

toyota technological institute at chicago

chicago, il, 60637, usa
mbansal@ttic.edu

6
1
0
2

 

n
u
j
 

8

 
 
]
l
c
.
s
c
[
 
 

3
v
0
7
7
0
0

.

1
0
6
1
:
v
i
x
r
a

abstract

we present a novel end-to-end neural
model to extract entities and relations be-
tween them. our recurrent neural net-
work based model captures both word se-
quence and dependency tree substructure
information by stacking bidirectional tree-
structured lstm-id56s on bidirectional
sequential lstm-id56s. this allows our
model to jointly represent both entities and
relations with shared parameters in a sin-
gle model. we further encourage detec-
tion of entities during training and use of
entity information in id36
via entity pretraining and scheduled sam-
pling. our model improves over the state-
of-the-art feature-based model on end-to-
end id36, achieving 12.1%
and 5.7% relative error reductions in f1-
score on ace2005 and ace2004, respec-
tively. we also show that our lstm-
id56 based model compares favorably to
the state-of-the-art id98 based model (in
f1-score) on nominal relation classi   ca-
tion (semeval-2010 task 8). finally, we
present an extensive ablation analysis of
several model components.

1

introduction

extracting semantic relations between entities in
text is an important and well-studied task in in-
formation extraction and natural language pro-
cessing (nlp). traditional systems treat this task
as a pipeline of two separated tasks, i.e., named
entity recognition (ner) (nadeau and sekine,
2007; ratinov and roth, 2009) and relation
extraction (zelenko et al., 2003; zhou et al.,
2005), but recent studies show that end-to-end

(joint) modeling of entity and relation is impor-
tant for high performance (li and ji, 2014; miwa
and sasaki, 2014) since relations interact closely
with entity information. for instance, to learn
that toefting and bolton have an organization-
af   liation (org-aff) relation in the sentence
toefting transferred to bolton, the entity informa-
tion that toefting and bolton are person and orga-
nization entities is important. extraction of these
entities is in turn encouraged by the presence of
the context words transferred to, which indicate an
employment relation. previous joint models have
employed feature-based structured learning. an
alternative approach to this end-to-end relation ex-
traction task is to employ automatic feature learn-
ing via neural network (nn) based models.

there are two ways to represent relations be-
recur-
tween entities using neural networks:
rent/id56s (id56s) and convo-
lutional neural networks (id98s). among these,
id56s can directly represent essential
linguis-
tic structures, i.e., word sequences (hammerton,
2001) and constituent/dependency trees (tai et
al., 2015). despite this representation ability,
for relation classi   cation tasks, the previously re-
ported performance using long short-term memory
(lstm) based id56s (xu et al., 2015b; li et al.,
2015) is worse than one using id98s (dos santos
et al., 2015). these previous lstm-based sys-
tems mostly include limited linguistic structures
and neural architectures, and do not model entities
and relations jointly. we are able to achieve im-
provements over state-of-the-art models via end-
to-end modeling of entities and relations based on
richer lstm-id56 architectures that incorporate
complementary linguistic structures.

word sequence and tree structure are known to
be complementary information for extracting rela-
tions. for instance, dependencies between words

are not enough to predict that source and u.s.
have an org-aff relation in the sentence    this
is ...   , one u.s. source said, and the context word
said is required for this prediction. many tradi-
tional, feature-based relation classi   cation mod-
els extract features from both sequences and parse
trees (zhou et al., 2005). however, previous id56-
based models focus on only one of these linguistic
structures (socher et al., 2012).

we present a novel end-to-end model to extract
relations between entities on both word sequence
and dependency tree structures. our model allows
joint modeling of entities and relations in a sin-
gle model by using both bidirectional sequential
(left-to-right and right-to-left) and bidirectional
tree-structured (bottom-up and top-down) lstm-
id56s. our model    rst detects entities and then
extracts relations between the detected entities us-
ing a single incrementally-decoded nn structure,
and the nn parameters are jointly updated using
both entity and relation labels. unlike traditional
incremental end-to-end id36 models,
our model further incorporates two enhancements
into training: entity pretraining, which pretrains
the entity model, and scheduled sampling (ben-
gio et al., 2015), which replaces (unreliable) pre-
dicted labels with gold labels in a certain probabil-
ity. these enhancements alleviate the problem of
low-performance entity detection in early stages
of training, as well as allow entity information to
further help downstream relation classi   cation.

on end-to-end id36, we improve
over the state-of-the-art feature-based model, with
12.1% (ace2005) and 5.7% (ace2004) relative
error reductions in f1-score. on nominal relation
classi   cation (semeval-2010 task 8), our model
compares favorably to the state-of-the-art id98-
based model in f1-score. finally, we also ab-
late and compare our various model components,
which leads to some key    ndings (both positive
and negative) about the contribution and effec-
tiveness of different id56 structures, input depen-
dency relation structures, different parsing mod-
els, external resources, and joint learning settings.

2 related work

lstm-id56s have been widely used for sequen-
tial labeling, such as clause identi   cation (ham-
merton, 2001), phonetic labeling (graves and
schmidhuber, 2005), and ner (hammerton,
2003). recently, huang et al. (2015) showed that

building a conditional random    eld (crf) layer on
top of bidirectional lstm-id56s performs com-
parably to the state-of-the-art methods in the part-
of-speech (pos) tagging, chunking, and ner.

for relation classi   cation, in addition to tra-
ditional feature/kernel-based approaches (zelenko
et al., 2003; bunescu and mooney, 2005), sev-
eral neural models have been proposed in the
semeval-2010 task 8 (hendrickx et al., 2010),
including embedding-based models (hashimoto
et al., 2015), id98-based models (dos santos et
al., 2015), and id56-based models (socher et al.,
2012). recently, xu et al. (2015a) and xu et
al. (2015b) showed that the shortest dependency
paths between relation arguments, which were
used in feature/kernel-based systems (bunescu
and mooney, 2005), are also useful in nn-based
models. xu et al. (2015b) also showed that lstm-
id56s are useful for relation classi   cation, but the
performance was worse than id98-based models.
li et al. (2015) compared separate sequence-based
and tree-structured lstm-id56s on relation clas-
si   cation, using basic id56 model structures.

research on tree-structured lstm-id56s (tai
et al., 2015)    xes the direction of information
propagation from bottom to top, and also cannot
handle an arbitrary number of typed children as in
a typed dependency tree. furthermore, no id56-
based relation classi   cation model simultaneously
uses word sequence and dependency tree informa-
tion. we propose several such novel model struc-
tures and training settings, investigating the simul-
taneous use of bidirectional sequential and bidi-
rectional tree-structured lstm-id56s to jointly
capture linear and dependency context for end-to-
end extraction of relations between entities.

as for end-to-end (joint) extraction of relations
between entities, all existing models are feature-
based systems (and no nn-based model has been
proposed). such models include structured pre-
diction (li and ji, 2014; miwa and sasaki,
2014), integer id135 (roth and yih,
2007; yang and cardie, 2013), card-pyramid pars-
ing (kate and mooney, 2010), and global prob-
abilistic id114 (yu and lam, 2010;
singh et al., 2013). among these, structured pre-
diction methods are state-of-the-art on several cor-
pora. we present an improved, nn-based alterna-
tive for the end-to-end id36.

fig. 1: our incrementally-decoded end-to-end id36 model, with bidirectional sequential
and bidirectional tree-structured lstm-id56s.

3 model

we design our model with lstm-id56s that rep-
resent both word sequences and dependency tree
structures, and perform end-to-end extraction of
relations between entities on top of these id56s.
fig. 1 illustrates the overview of the model. the
model mainly consists of three representation lay-
ers: a id27s layer (embedding layer),
a word sequence based lstm-id56 layer (se-
quence layer), and    nally a dependency subtree
based lstm-id56 layer (dependency layer). dur-
ing decoding, we build greedy, left-to-right entity
detection on the sequence layer and realize rela-
tion classi   cation on the dependency layers, where
each subtree based lstm-id56 corresponds to
a relation candidate between two detected enti-
ties. after decoding the entire model structure, we
update the parameters simultaneously via back-
propagation through time (bptt) (werbos, 1990).
the dependency layers are stacked on the se-
quence layer, so the embedding and sequence lay-
ers are shared by both entity detection and rela-
tion classi   cation, and the shared parameters are
affected by both entity and relation labels.

3.1 embedding layer

the embedding layer handles embedding repre-
sentations. nw, np, nd and ne-dimensional vectors
v(w), v(p), v(d) and v(e) are embedded to words,
part-of-speech (pos) tags, dependency types, and
entity labels, respectively.

3.2 sequence layer
the sequence layer represents words in a linear se-
quence using the representations from the embed-
ding layer. this layer represents sentential con-
text information and maintains entities, as shown
in bottom-left part of fig. 1.

we represent the word sequence in a sentence
with bidirectional lstm-id56s (graves et al.,
2013). the lstm unit at t-th word consists of
a collection of nls-dimensional vectors: an input
gate it, a forget gate ft, an output gate ot, a mem-
ory cell ct, and a hidden state ht. the unit re-
ceives an n-dimensional input vector xt, the previ-
ous hidden state ht   1, and the memory cell ct   1,
and calculates the new vectors using the following
equations:

(cid:16)
(cid:16)
(cid:16)

w (i)xt + u (i)ht   1 + b(i)(cid:17)
w (f )xt + u (f )ht   1 + b(f )(cid:17)
w (o)xt + u (o)ht   1 + b(o)(cid:17)
w (u)xt + u (u)ht   1 + b(u)(cid:17)
(cid:16)

,

,

,

(1)

,

ut = tanh
ct = it(cid:12)ut + ft(cid:12)ct   1,
ht = ot(cid:12) tanh(ct),
where    denotes the logistic function, (cid:12) denotes
element-wise multiplication, w and u are weight
matrices, and b are bias vectors. the lstm unit
at t-th word receives the concatenation of word
and pos embeddings as its input vector: xt =
v(w)
. we also concatenate the hidden state
t
vectors of the two directions    lstm units corre-
      
ht) as
sponding to each word (denoted as

      
ht and

; v(p)

(cid:104)

(cid:105)

t

it =   

ft =   

ot =   

in1909,sidneyyateswasborninchicago.b-perl-perword/pos embeddingsbi-lstmhiddensoftmaxnsubjpasspreppobjyatesborninchicagophysbi-treelstmhiddensoftmaxsequence (entity)dependency (relation)lstm unitdropouttanhtanhdependency embeddingstanhlabel embeddingsembeddingsneural net / softmax                  (cid:104)      

ht;

(cid:105)
      
ht

, and pass it to the

its output vector, st =
subsequent layers.

3.3 entity detection
we treat entity detection as a sequence labeling
task. we assign an entity tag to each word us-
ing a commonly used encoding scheme bilou
(begin, inside, last, outside, unit) (ratinov and
roth, 2009), where each entity tag represents the
entity type and the position of a word in the entity.
for example, in fig. 1, we assign b-per and l-
per (which denote the beginning and last words
of a person entity type, respectively) to each word
in sidney yates to represent this phrase as a per
(person) entity type.

we perform entity detection on top of the se-
quence layer. we employ a two-layered nn with
an nhe-dimensional hidden layer h(e) and a soft-
max output layer for entity detection.

(cid:16)

(cid:16)

t   1] + b(eh)(cid:17)
t + b(ey)(cid:17)

(2)

(3)

= tanh

h(e)
t
yt = softmax

w (eh)[st; v(e)
w (ey)h(e)

here, w are weight matrices and b are bias vec-
tors.

we assign entity labels to words in a greedy,
left-to-right manner.1 during this decoding, we
use the predicted label of a word to predict the
label of the next word so as to take label depen-
dencies into account. the nn above receives the
concatenation of its corresponding outputs in the
sequence layer and the label embedding for its pre-
vious word (fig. 1).

3.4 dependency layer
the dependency layer represents a relation be-
tween a pair of two target words (corresponding
to a relation candidate in relation classi   cation) in
the dependency tree, and is in charge of relation-
speci   c representations, as is shown in top-right
part of fig. 1. this layer mainly focuses on the
shortest path between a pair of target words in the
dependency tree (i.e., the path between the least
common node and the two target words) since
these paths are shown to be effective in relation
classi   cation (xu et al., 2015a). for example, we
show the shortest path between yates and chicago
in the bottom of fig. 1, and this path well captures
the key phrase of their relation, i.e., born in.

1we also tried id125 but this did not show improve-

ments in initial experiments.

we employ bidirectional tree-structured lstm-
id56s (i.e., bottom-up and top-down) to represent
a relation candidate by capturing the dependency
structure around the target word pair. this bidirec-
tional structure propagates to each node not only
the information from the leaves but also informa-
tion from the root. this is especially important
for relation classi   cation, which makes use of ar-
gument nodes near the bottom of the tree, and our
top-down lstm-id56 sends information from the
top of the tree to such near-leaf nodes (unlike in
standard bottom-up lstm-id56s).2 note that the
two variants of tree-structured lstm-id56s by
tai et al. (2015) are not able to represent our tar-
get structures which have a variable number of
typed children: the child-sum tree-lstm does
not deal with types and the n-ary tree assumes
a    xed number of children. we thus propose a
new variant of tree-structured lstm-id56 that
shares weight matrices us for same-type children
and also allows variable number of children. for
this variant, we calculate nlt-dimensional vectors
in the lstm unit at t-th node with c(t) children
using following equations:

       ,
       ,

u (i)
m(l)htl + b(i)

it =   

l   c(t)

l   c(t)

ftk =   

      w (i)xt +
(cid:88)
      w (f )xt +
(cid:88)
      w (o)xt +
(cid:88)
      w (u)xt +
(cid:88)
ct = it(cid:12)ut +
l   c(t)
ht = ot(cid:12) tanh(ct),
where m(  ) is a type mapping function.

(cid:88)
l   c(t)
ftl(cid:12)ctl,

ut = tanh

ot =   

l   c(t)

u (o)
m(l)htl + b(o)

u (u)
m(l)htl + b(u)

u (f )
m(k)m(l)htl + b(f )

(4)

       ,
       ,

to investigate appropriate structures to repre-
sent relations between two target word pairs, we
experiment with three structure options. we pri-
marily employ the shortest path structure (sp-
tree), which captures the core dependency path
between a target word pair and is widely used in
relation classi   cation models, e.g., (bunescu and

2we also tried to use one lstm-id56 by connecting the
root (paulus et al., 2014), but preparing two lstm-id56s
showed slightly better performance in our initial experiments.

mooney, 2005; xu et al., 2015a). we also try two
other dependency structures: subtree and full-
tree. subtree is the subtree under the lowest
common ancestor of the target word pair. this pro-
vides additional modi   er information to the path
and the word pair in sptree. fulltree is the full
dependency tree. this captures context from the
entire sentence. while we use one node type for
sptree, we de   ne two node types for subtree and
fulltree, i.e., one for nodes on shortest paths and
one for all other nodes. we use the type mapping
function m(  ) to distinguish these two nodes types.
3.5 stacking sequence and dependency

layers

(cid:104)

we stack the dependency layers (corresponding to
relation candidates) on top of the sequence layer to
incorporate both word sequence and dependency
tree structure information into the output. the
dependency-layer lstm unit at the t-th word re-
ceives as input xt =
, i.e., the con-
catenation of its corresponding hidden state vec-
tors st in the sequence layer, dependency type
embedding v(d)
(denotes the type of dependency
to the parent3), and label embedding v(e)
(corre-
sponds to the predicted entity label).

st; v(d)

; v(e)

(cid:105)

t

t

t

t

3.6 relation classi   cation
we incrementally build relation candidates using
all possible combinations of the last words of de-
tected entities, i.e., words with l or u labels in
the bilou scheme, during decoding. for in-
stance, in fig. 1, we build a relation candidate us-
ing yates with an l-per label and chicago with
an u-loc label. for each relation candidate, we
realize the dependency layer dp (described above)
corresponding to the path between the word pair
p in the relation candidate, and the nn receives a
relation candidate vector constructed from the out-
put of the dependency tree layer, and predicts its
relation label. we treat a pair as a negative relation
when the detected entities are wrong or when the
pair has no relation. we represent relation labels
by type and direction, except for negative relations
that have no direction.
the relation candidate vector is constructed as
the concatenation dp = [   hpa;   hp1;   hp2], where
   hpa is the hidden state vector of the top lstm
3we use the dependency to the parent since the number of
children varies. dependency types can also be incorporated
into m(  ), but this did not help in initial experiments.

unit in the bottom-up lstm-id56 (representing
the lowest common ancestor of the target word
pair p), and    hp1,    hp2 are the hidden state vec-
tors of the two lstm units representing the    rst
and second target words in the top-down lstm-
id56.4 all the corresponding arrows are shown in
fig. 1.

similarly to the entity detection, we employ a
two-layered nn with an nhr-dimensional hidden
layer h(r) and a softmax output layer (with weight
matrices w , bias vectors b).

(cid:16)

w (rh)dp + b(rh)(cid:17)
t + b(ry)(cid:17)
(cid:16)

w (ry)h(r)

(5)

(6)

h(r)
p = tanh
yp = softmax

we construct the input dp for relation classi   -
cation from tree-structured lstm-id56s stacked
on sequential lstm-id56s, so the contribution
of sequence layer to the input is indirect. fur-
thermore, our model uses words for represent-
ing entities, so it cannot fully use the entity in-
formation. to alleviate these problems, we di-
rectly concatenate the average of hidden state vec-
(cid:104)
tors for each entity from the sequence layer to
the input dp to relation classi   cation, i.e., d(cid:48)
p =
(pair), where
1|ip2|
dp;
ip1 and ip2 represent sets of word indices in the
   rst and second entities.5

(cid:80)

(cid:80)

i   ip1

i   ip2

1|ip1|

(cid:105)

si;

si

also, we assign two labels to each word pair in
prediction since we consider both left-to-right and
right-to-left directions. when the predicted labels
are inconsistent, we select the positive and more
con   dent label, similar to xu et al. (2015a).

3.7 training
we update the model parameters
including
weights, biases, and embeddings by bptt and
adam (kingma and ba, 2015) with gradient clip-
ping, parameter averaging, and l2-id173
(we regularize weights w and u, not the bias
terms b). we also apply dropout (srivastava et al.,
2014) to the embedding layer and to the    nal hid-
den layers for entity detection and relation classi-
   cation.

we employ two enhancements, scheduled sam-
pling (bengio et al., 2015) and entity pretrain-
ing, to alleviate the problem of unreliable pre-
diction of entities in the early stage of training,
4note that the order of the target words corresponds to the

direction of the relation, not the positions in the sentence.

5note that we do not show this pair in fig.1 for simplic-

ity.

and to encourage building positive relation in-
stances from the detected entities.
in scheduled
sampling, we use gold labels as prediction in the
id203 of  i that depends on the number of
epochs i during training if the gold labels are le-
gal. as for  i, we choose the inverse sigmoid de-
cay  i = k/(k + exp(i/k)), where k(    1) is a
hyper-parameter that adjusts how often we use the
gold labels as prediction. entity pretraining is in-
spired by (pentina et al., 2015), and we pretrain
the entity detection model using the training data
before training the entire model parameters.

4 results and discussion
4.1 data and task settings
we evaluate on three datasets: ace05 and ace04
for end-to-end id36, and semeval-
2010 task 8 for relation classi   cation. we use the
   rst two datasets as our primary target, and use
the last one to thoroughly analyze and ablate the
relation classi   cation part of our model.

ace05 de   nes 7 coarse-grained entity types
and 6 coarse-grained relation types between enti-
ties. we use the same data splits, preprocessing,
and task settings as li and ji (2014). we report
the primary micro f1-scores as well as micro pre-
cision and recall on both entity and relation extrac-
tion to better explain model performance. we treat
an entity as correct when its type and the region of
its head are correct. we treat a relation as correct
when its type and argument entities are correct; we
thus treat all non-negative relations on wrong en-
tities as false positives.

ace04 de   nes the same 7 coarse-grained en-
tity types as ace05 (doddington et al., 2004), but
de   nes 7 coarse-grained relation types. we fol-
low the cross-validation setting of chan and roth
(2011) and li and ji (2014), and the preprocessing
and id74 of ace05.

semeval-2010 task 8 de   nes 9 relation types
between nominals and a tenth type other when
two nouns have none of these relations (hendrickx
et al., 2010). we treat this other type as a nega-
tive relation type, and no direction is considered.
the dataset consists of 8,000 training and 2,717
test sentences, and each sentence is annotated with
a relation between two given nominals. we ran-
domly selected 800 sentences from the training set
as our development set. we followed the of   cial
task setting, and report the of   cial macro-averaged
f1-score (macro-f1) on the 9 relation types.

for more details of the data and task settings,

please refer to the supplementary material.

4.2 experimental settings
we implemented our model using the id98 library.6
we parsed the texts using the stanford neural de-
pendency parser7 (chen and manning, 2014) with
the original stanford dependencies. based on pre-
liminary tuning, we    xed embedding dimensions
nw to 200, np, nd, ne to 25, and dimensions of
intermediate layers (nls, nlt of lstm-id56s and
nhe, nhr of hidden layers) to 100. we initialized
word vectors via id97 (mikolov et al., 2013)
trained on wikipedia8 and randomly initialized all
other parameters. we tuned hyper-parameters us-
ing development sets for ace05 and semeval-
2010 task 8 to achieve high primary (micro- and
macro-) f1-scores.9 for ace04, we directly em-
ployed the best parameters for ace05. the hyper-
parameter settings are shown in the supplementary
material. for semeval-2010 task 8, we also omit-
ted the entity detection and label embeddings since
only target nominals are annotated and the task de-
   nes no entity types. our statistical signi   cance
results are based on the approximate randomiza-
tion (ar) test (noreen, 1989).

4.3 end-to-end id36 results
table 1 compares our model with the state-of-the-
art feature-based model of li and ji (2014)10 on
   nal test sets, and shows that our model performs
better than the state-of-the-art model.

to analyze the contributions and effects of the
various components of our end-to-end relation ex-
traction model, we perform ablation tests on the
ace05 development set (table 2). the perfor-
mance slightly degraded without scheduled sam-
pling, and the performance signi   cantly degraded
when we removed entity pretraining or removed
both (p<0.05). this is reasonable because the
model can only create relation instances when
both of the entities are found and, without these
enhancements, it may get too late to    nd some re-
lations. removing label embeddings did not affect

6https://github.com/clab/id98
7http://nlp.stanford.edu/software/
stanford-corenlp-full-2015-04-20.zip

8https://dumps.wikimedia.org/enwiki/

20150901/

9we did not tune the precision-recall trade-offs, but doing

so can speci   cally improve precision further.

10other work on ace is not comparable or performs worse

than the model by li and ji (2014).

corpus

settings

ace05 our model (sptree)

li and ji (2014)

ace04 our model (sptree)

li and ji (2014)

entity
r
0.839
0.769
0.829
0.762

f1
0.834
0.808
0.818
0.797

relation
r
0.540
0.398
0.481
0.361

f1
0.556
0.495
0.484
0.453

p
0.572
0.654
0.487
0.608

p
0.829
0.852
0.808
0.835

table 1: comparison with the state-of-the-art on the ace05 test set and ace04 dataset.

settings

our model (sptree)
   entity pretraining (ep)
   scheduled sampling (ss)
   label embeddings (le)
   shared parameters (shared)
   ep, ss
   ep, ss, le, shared

entity
r
0.821
0.798
0.818
0.821
0.820
0.804
0.815

p
0.815
0.793
0.812
0.811
0.796
0.781
0.800

f1
0.818
0.796
0.815
0.816
0.808
0.792
0.807

p
0.506
0.494
0.522
0.512
0.541
0.509
0.520

relation

r
0.529
0.491
0.490
0.499
0.482
0.479
0.452

f1
0.518
0.492*
0.505
0.505
0.510
0.494*
0.484**

table 2: ablation tests on the ace05 development dataset. * denotes signi   cance at p<0.05, ** denotes
p<0.01.

settings

sptree
subtree
fulltree
subtree (-sp)
fulltree (-sp)
child-sum
spseq
spxu

entity
r
0.821
0.818
0.816
0.816
0.817
0.819
0.813
0.818

f1
0.818
0.815
0.811
0.810
0.811
0.8122
0.807
0.813

relation

p
0.506
0.525
0.536
0.533
0.517
0.514
0.500
0.494

r
0.529
0.506
0.507
0.495
0.470
0.499
0.523
0.522

f1
0.518
0.515
0.521
0.514
0.492*
0.506
0.511
0.508

p
0.815
0.812
0.806
0.803
0.804
0.806
0.801
0.809

table 3: comparison of lstm-id56 structures on the ace05 development dataset.

the entity detection performance, but this degraded
the recall in relation classi   cation. this indicates
that entity label information is helpful in detecting
relations.

we also show the performance without shar-
ing parameters, i.e., embedding and sequence lay-
ers, for detecting entities and relations (   shared
parameters); we    rst train the entity detection
model, detect entities with the model, and build
a separate id36 model using the
detected entities,
i.e., without entity detection.
this setting can be regarded as a pipeline model
since two separate models are trained sequentially.
without the shared parameters, both the perfor-
mance in entity detection and relation classi   ca-
tion drops slightly, although the differences are

not signi   cant. when we removed all the en-
hancements, i.e., scheduled sampling, entity pre-
training, label embedding, and shared parameters,
the performance is signi   cantly worse than sp-
tree (p<0.01), showing that these enhancements
provide complementary bene   ts to end-to-end re-
lation extraction.

next, we show the performance with differ-
ent lstm-id56 structures in table 3. we    rst
compare the three input dependency structures
(sptree, subtree, fulltree) for tree-structured
lstm-id56s. performances on these three struc-
tures are almost same when we distinguish the
nodes in the shortest paths from other nodes,
but when we do not distinguish them (-sp), the
information outside of the shortest path,
i.e.,

fulltree (-sp), signi   cantly hurts performance
(p<0.05). we then compare our tree-structured
lstm-id56 (sptree) with the child-sum tree-
structured lstm-id56 on the shortest path of tai
et al. (2015). child-sum performs worse than our
sptree model, but not with as big of a decrease
as above. this may be because the difference in
the models appears only on nodes that have multi-
ple children and all the nodes except for the least
common node have one child.

we    nally show results with two counterparts
of sequence-based lstm-id56s using the short-
est path (last two rows in table 3). spseq is a bidi-
rectional lstm-id56 on the shortest path. the
lstm unit receives input from the sequence layer
concatenated with embeddings for the surround-
ing dependency types and directions. we concate-
nate the outputs of the two id56s for the relation
candidate. spxu is our adaptation of the shortest
path lstm-id56 proposed by xu et al. (2015b)
to match our sequence-layer based model.11 this
has two lstm-id56s for the left and right sub-
paths of the shortest path. we    rst calculate the
max pooling of the lstm units for each of these
two id56s, and then concatenate the outputs of the
pooling for the relation candidate. the compar-
ison with these sequence-based lstm-id56s in-
dicates that a tree-structured lstm-id56 is com-
parable to sequence-based ones in representing
shortest paths.

overall,

the performance comparison of the
lstm-id56 structures in table 3 show that for
end-to-end id36, selecting the ap-
propriate tree structure representation of the input
(i.e., the shortest path) is more important than the
choice of the lstm-id56 structure on that input
(i.e., sequential versus tree-based).

4.4 relation classi   cation analysis results
to thoroughly analyze the relation classi   cation
part alone, e.g., comparing different lstm struc-
tures, architecture components such as hidden lay-
ers and input information, and classi   cation task
settings, we use the semeval-2010 task 8. this
dataset, often used to evaluate nn models for rela-
tion classi   cation, annotates only relation-related
nominals (unlike ace datasets), so we can focus
cleanly on the relation classi   cation part.

11this is different from the original one in that we use the
sequence layer and we concatenate the embeddings for the in-
put, while the original one prepared individual lstm-id56s
for different inputs and concatenated their outputs.

settings

macro-f1

no external knowledge resources

our model (sptree)
dos santos et al. (2015)
xu et al. (2015a)

+id138

0.844
0.841
0.840

our model (sptree + id138)
xu et al. (2015a)
xu et al. (2015b)

0.855
0.856
0.837

table 4: comparison with state-of-the-art models
on semeval-2010 task 8 test-set.

settings
sptree
subtree
fulltree
subtree (-sp)
fulltree (-sp)
child-sum
spseq
spxu

macro-f1

0.851
0.839
0.829   
0.840
0.828   
0.838
0.844
0.847

table 5: comparison of lstm-id56 structures on
semeval-2010 task 8 development set.

we    rst report of   cial test set results in ta-
ble 4. our novel lstm-id56 model is compara-
ble to both the state-of-the-art id98-based models
on this task with or without external sources, i.e.,
id138, unlike the previous best lstm-id56
model (xu et al., 2015b).12

next, we compare different lstm-id56 struc-
tures in table 5. as for the three input de-
pendency structures (sptree, subtree, fulltree),
fulltree performs signi   cantly worse than other
structures regardless of whether or not we dis-
tinguish the nodes in the shortest paths from the
other nodes, which hints that the information out-
side of the shortest path signi   cantly hurts the per-
formance (p<0.05). we also compare our tree-
structured lstm-id56 (sptree) with sequence-
based lstm-id56s (spseq and spxu) and tree-
structured lstm-id56s (child-sum). all these
lstm-id56s perform slightly worse than our sp-

12when incorporating id138

information into our
model, we prepared embeddings for id138 hypernyms ex-
tracted by supersensetagger (ciaramita and altun, 2006)
and concatenated the embeddings to the input vector (the con-
catenation of word and pos embeddings) of the sequence
lstm. we tuned the dimension of the id138 embeddings
and set it to 15 using the development dataset.

settings
sptree
   hidden layer
   sequence layer
   pair
   pair, sequence layer
stanford pid18
+id138
left-to-right candidates
neg. sampling (xu et al., 2015a)

macro-f1

0.851
0.839
0.840
0.844
0.827   
0.844
0.854
0.843
0.848

table 6: model setting ablations on semeval-
2010 development set.

tree model, but the differences are small.

overall,

for relation classi   cation, although
the performance comparison of the lstm-id56
structures in table 5 produces different results on
fulltree as compared to the results on ace05 in
table 3, the trend still holds that selecting the ap-
propriate tree structure representation of the input
is more important than the choice of the lstm-
id56 structure on that input.

finally, table 6 summarizes the contribution
of several model components and training set-
tings on semeval relation classi   cation. we    rst
remove the hidden layer by directly connecting
the lstm-id56 layers to the softmax layers, and
found that this slightly degraded performance, but
the difference was small. we then skip the se-
quence layer and directly use the word and pos
embeddings for the dependency layer. removing
the sequence layer13 or entity-related information
from the sequence layer (   pair) slightly degraded
performance, and, on removing both, the perfor-
mance dropped signi   cantly (p<0.05). this indi-
cates that the sequence layer is necessary but the
last words of nominals are almost enough for ex-
pressing the relations in this task.

when we replace the stanford neural depen-
dency parser with the stanford lexicalized pid18
parser (stanford pid18), the performance slightly
dropped, but the difference was small. this in-
dicates that the selection of parsing models is
not critical. we also included id138, and this
slightly improved the performance (+id138),
but the difference was small. lastly, for the gener-
ation of relation candidates, generating only left-
to-right candidates slightly degraded the perfor-

13note that this setting still uses some sequence layer in-
formation since it uses the entity-related information (pair).

mance, but the difference was small and hence the
creation of right-to-left candidates was not critical.
treating the inverse relation candidate as a nega-
tive instance (negative sampling) also performed
comparably to other generation methods in our
model (unlike xu et al. (2015a), which showed
a signi   cance improvement over generating only
left-to-right candidates).

5 conclusion

we presented a novel end-to-end relation extrac-
tion model that represents both word sequence
and dependency tree structures by using bidirec-
tional sequential and bidirectional tree-structured
lstm-id56s. this allowed us to represent both
entities and relations in a single model, achiev-
ing gains over the state-of-the-art, feature-based
system on end-to-end id36 (ace04
and ace05), and showing favorably compara-
ble performance to recent state-of-the-art id98-
based models on nominal relation classi   cation
(semeval-2010 task 8).

our evaluation and ablation led to three key
   ndings. first, the use of both word sequence
and dependency tree structures is effective. sec-
ond, training with the shared parameters improves
id36 accuracy, especially when em-
ployed with entity pretraining, scheduled sam-
pling, and label embeddings. finally, the shortest
path, which has been widely used in relation clas-
si   cation, is also appropriate for representing tree
structures in neural lstm models.

acknowledgments

we thank qi li, kevin gimpel, and the anony-
mous reviewers for dataset details and helpful dis-
cussions.

references

[bengio et al.2015] samy bengio, oriol vinyals,
navdeep jaitly, and noam shazeer.
2015.
scheduled sampling for sequence prediction
with recurrent neural networks. arxiv preprint
arxiv:1506.03099.

[bunescu and mooney2005] razvan c bunescu
and raymond mooney. 2005. a shortest path
dependency kernel for id36.
in
proceedings of the conference on human lan-
guage technology and empirical methods in

natural language processing, pages 724   731.
acl.

[chan and roth2011] yee seng chan and dan
roth. 2011. exploiting syntactico-semantic
in proceed-
structures for id36.
ings of the 49th annual meeting of the asso-
ciation for computational linguistics: human
language technologies, pages 551   560, port-
land, oregon, usa, june. acl.

chen

[chen and manning2014] danqi

and
2014. a fast and
christopher manning.
accurate dependency parser using neural net-
works. in proceedings of the 2014 conference
on empirical methods in natural language
processing (emnlp), pages 740   750, doha,
qatar, october. acl.

[ciaramita and altun2006] massimiliano

cia-
2006. broad-
ramita and yasemin altun.
coverage sense disambiguation and information
extraction with a supersense sequence tagger.
in proceedings of
the 2006 conference on
empirical methods
in natural language
processing, pages 594   602, sydney, australia,
july. acl.

[doddington et al.2004] george

doddington,
alexis mitchell, mark przybocki, lance
ramshaw, stephanie strassel,
and ralph
weischedel.
2004. the automatic content
extraction (ace) program     tasks, data, and
the fourth
evaluation.
international conference on language re-
sources and evaluation (lrec-2004), lisbon,
portugal, may. european language resources
association (elra).

in proceedings of

[dos santos et al.2015] cicero dos santos, bing
xiang, and bowen zhou. 2015. classifying
relations by ranking with convolutional neural
in proceedings of the 53rd annual
networks.
meeting of the association for computational
linguistics and the 7th international joint con-
ference on natural language processing (vol-
ume 1: long papers), pages 626   634, beijing,
china, july. acl.

[graves and schmidhuber2005] alex graves and
j  urgen schmidhuber.
framewise
phoneme classi   cation with bidirectional lstm
and other neural network architectures. neural
networks, 18(5):602   610.

2005.

[graves et al.2013] alan graves, abdel-rahman
mohamed, and geoffrey hinton. 2013. speech
recognition with deep recurrent neural net-
in acoustics, speech and signal pro-
works.
cessing (icassp), 2013 ieee international
conference on, pages 6645   6649. ieee.

[hammerton2001] james hammerton.

2001.
clause identi   cation with long short-term mem-
in proceedings of the 2001 workshop
ory.
on computational natural language learning-
volume 7, page 22. acl.

[hammerton2003] james hammerton.

2003.
id39 with long short-term
memory.
in walter daelemans and miles os-
borne, editors, proceedings of the seventh con-
ference on natural language learning at hlt-
naacl 2003, pages 172   175. acl.

[hashimoto et al.2015] kazuma hashimoto, pon-
tus stenetorp, makoto miwa, and yoshimasa
tsuruoka.
2015. task-oriented learning of
id27s for semantic relation classi-
   cation. in proceedings of the nineteenth con-
ference on computational natural language
learning, pages 268   278, beijing, china, july.
acl.

[hendrickx et al.2010] iris hendrickx, su nam
kim, zornitsa kozareva, preslav nakov, di-
armuid   o s  eaghdha, sebastian pad  o, marco
pennacchiotti, lorenza romano, and stan sz-
pakowicz. 2010. semeval-2010 task 8: multi-
way classi   cation of semantic relations between
pairs of nominals. in proceedings of the 5th in-
ternational workshop on semantic evaluation,
pages 33   38, uppsala, sweden, july. acl.

[huang et al.2015] zhiheng huang, wei xu, and
2015. bidirectional lstm-crf mod-
arxiv preprint

kai yu.
els for sequence tagging.
arxiv:1508.01991.

[kate and mooney2010] rohit j. kate and ray-
mond mooney. 2010. joint entity and relation
extraction using card-pyramid parsing. in pro-
ceedings of the fourteenth conference on com-
putational natural language learning, pages
203   212, uppsala, sweden, july. acl.

[kingma and ba2015] diederik

kingma

and
2015. adam: a method for
in iclr 2015, san

jimmy ba.
stochastic optimization.
diego, ca, may.

[li and ji2014] qi li and heng ji. 2014.

incre-
mental joint extraction of entity mentions and
in proceedings of the 52nd annual
relations.
meeting of the association for computational
linguistics (volume 1: long papers), pages
402   412, baltimore, maryland, june. acl.

[pentina et al.2015] anastasia pentina, viktoriia
sharmanska, and christoph h. lampert. 2015.
curriculum learning of multiple tasks. in ieee
conference on id161 and pattern
recognition cvpr, pages 5492   5500, boston,
ma, usa, june.

[li et al.2015] jiwei li, thang luong, dan juraf-
sky, and eduard hovy. 2015. when are tree
structures necessary for deep learning of repre-
in proceedings of the 2015 con-
sentations?
ference on empirical methods in natural lan-
guage processing, pages 2304   2314, lisbon,
portugal, september. acl.

[lu and roth2015] wei lu and dan roth. 2015.
joint mention extraction and classi   cation with
in proceedings of the
mention hypergraphs.
2015 conference on empirical methods in nat-
ural language processing, pages 857   867, lis-
bon, portugal, september. acl.

[mikolov et al.2013] tomas mikolov,

ilya
sutskever, kai chen, greg s corrado, and
jeff dean. 2013. distributed representations of
words and phrases and their compositionality.
in advances in neural information processing
systems, pages 3111   3119.

[miwa and sasaki2014] makoto miwa and yutaka
sasaki. 2014. modeling joint entity and re-
lation extraction with table representation.
in
proceedings of the 2014 conference on empir-
ical methods in natural language processing
(emnlp), pages 1858   1869, doha, qatar, oc-
tober. acl.

[nadeau and sekine2007] david nadeau

and
satoshi sekine. 2007. a survey of named en-
tity recognition and classi   cation. lingvisticae
investigationes, 30(1):3   26.

[noreen1989] eric w. noreen. 1989. computer-
intensive methods for testing hypotheses : an
introduction. wiley-interscience, april.

[paulus et al.2014] romain

paulus,

richard
2014.
socher, and christopher d manning.
global belief id56s.
in
z. ghahramani, m. welling, c. cortes, n.d.
lawrence,
editors,
advances
information process-
ing systems 27, pages 2888   2896. curran
associates, inc.

and k.q. weinberger,
in neural

[ratinov and roth2009] lev ratinov and dan
roth. 2009. design challenges and misconcep-
tions in id39. in proceed-
ings of the thirteenth conference on compu-
tational natural language learning (conll-
2009), pages 147   155, boulder, colorado,
june. acl.

[roth and yih2007] dan roth and wen-tau yih,
2007. global id136 for entity and relation
identi   cation via a id135 for-
mulation. mit press.

singh,
sebastian
[singh et al.2013] sameer
riedel, brian martin,
jiaping zheng, and
andrew mccallum. 2013. joint id136 of
in pro-
entities, relations, and coreference.
ceedings of the 2013 workshop on automated
knowledge base construction, pages 1   6. acm.

[socher et al.2012] richard socher, brody huval,
christopher d. manning, and andrew y. ng.
2012. semantic compositionality through re-
cursive matrix-vector spaces. in proceedings of
the 2012 joint conference on empirical meth-
ods in natural language processing and com-
putational natural language learning, pages
1201   1211, jeju island, korea, july. acl.

[srivastava et al.2014] nitish srivastava, geoffrey
hinton, alex krizhevsky, ilya sutskever, and
ruslan salakhutdinov. 2014. dropout: a sim-
ple way to prevent neural networks from over-
   tting. the journal of machine learning re-
search, 15(1):1929   1958.

[tai et al.2015] kai sheng tai, richard socher,
and christopher d. manning. 2015. improved
semantic representations from tree-structured
id137. in proceed-
ings of the 53rd annual meeting of the asso-
ciation for computational linguistics and the
7th international joint conference on natural
language processing (volume 1: long papers),
pages 1556   1566, beijing, china, july. acl.

[werbos1990] paul j werbos. 1990. backpropa-
gation through time: what it does and how to do
it. proceedings of the ieee, 78(10):1550   1560.

[xu et al.2015a] kun xu, yansong feng, song-
fang huang, and dongyan zhao. 2015a. se-
mantic relation classi   cation via convolutional
neural networks with simple negative sampling.
in proceedings of the 2015 conference on em-
pirical methods in natural language process-
ing, pages 536   540, lisbon, portugal, septem-
ber. acl.

[xu et al.2015b] yan xu, lili mou, ge li,
yunchuan chen, hao peng, and zhi jin. 2015b.
classifying relations via long short term mem-
ory networks along shortest dependency paths.
in proceedings of
the 2015 conference on
empirical methods in natural language pro-
cessing, pages 1785   1794, lisbon, portugal,
september. acl.

[yang and cardie2013] bishan yang and claire
cardie. 2013. joint id136 for    ne-grained
opinion extraction. in proceedings of the 51st
annual meeting of the association for com-
putational linguistics (volume 1: long pa-
pers), pages 1640   1649, so   a, bulgaria, au-
gust. acl.

[yu and lam2010] xiaofeng yu and wai lam.
2010. jointly identifying entities and extract-
ing relations in encyclopedia text via a graphi-
cal model approach. in coling 2010: posters,
pages 1399   1407, beijing, china, august. col-
ing 2010 organizing committee.

[zelenko et al.2003] dmitry zelenko, chinatsu
aone, and anthony richardella. 2003. kernel
methods for id36. the journal of
machine learning research, 3:1083   1106.

[zhou et al.2005] guodong zhou, jian su, jie
zhang, and min zhang.
2005. exploring
various knowledge in id36.
in
proceedings of the 43rd annual meeting of
the association for computational linguistics
(acl   05), pages 427   434, ann arbor, michi-
gan, june. acl.

a supplemental material
a.1 data and task settings
ace05 de   nes 7 coarse-grained entity types:
facility (fac), geo-political entities (gpe),

location (loc), organization (org), person
(per), vehicle (veh) and weapon (wea), and
6 coarse-grained relation types between enti-
ties: artifact (art), gen-af   liation (gen-aff),
org-af   liation (org-aff), part-whole (part-
whole), person-social (per-soc) and physical
(phys). we removed the cts, un subsets, and used
a 351/80/80 train/dev/test split. we removed du-
plicated entities and relations, and resolved nested
entities. we used head spans for entities. we fol-
low the settings by (li and ji, 2014), and we did
not use the full mention boundary unlike lu and
roth (2015). we use entities and relations to refer
to entity mentions and relation mentions in ace
for brevity.

ace04 de   nes the same 7 coarse-grained entity
types as ace05 (doddington et al., 2004), but de-
   nes 7 coarse-grained relation types: pys, per-
soc, employment
/ membership / subsidiary
(emp-org), art, per/org af   liation (other-
aff), gpe af   liation (gpe-aff), and discourse
(disc). we follow the cross-validation setting of
chan and roth (2011) and li and ji (2014). we
removed disc and did 5-fold cv on bnews and
nwire subsets (348 documents). we use the same
preprocessing and id74 of ace05.

semeval-2010 task 8 de   nes 9 relation types
between nominals ( cause-effect,
instrument-
agency, product-producer, content-container,
entity-origin, entity-destination, component-
whole, member-collection and message-topic),
and a tenth type other when two nouns have none
of these relations (hendrickx et al., 2010). we
treat this other type as a negative relation type,
and no direction is considered. the dataset con-
sists of 8,000 training and 2,717 test sentences,
and each sentence is annotated with a relation be-
tween two given nominals. we randomly selected
800 sentences from the training set as our devel-
opment set. we followed the of   cial task setting,
and report the of   cial macro-averaged f1-score
(macro-f1) on the 9 relation types.

a.2 hyper-parameter settings

here we show the hyper-parameters and the range
tried for the hyper-parameters in parentheses.
hyper-parameters include the initial learning rate
(5e-3, 2e-3, 1e-3, 5e-4, 2e-4, 1e-4), the regular-
ization parameter (1e-4, 1e-5, 1e-6, 1e-7), dropout
probabilities (0.0, 0.1, 0.2, 0.3, 0.4, 0.5), the size
of gradient clipping (1, 5, 10, 50, 100), scheduled

sampling parameter k (1, 5, 10, 50, 100), the num-
ber of epochs for training and entity pretraining (   
100), and the embedding dimension of id138
hypernym (5, 10, 15, 20, 25, 30).

