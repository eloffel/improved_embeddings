abid98: attention-based convolutional neural network

for modeling sentence pairs

wenpeng yin, hinrich sch  utze

bing xiang, bowen zhou

center for information and language processing

ibm watson

lmu munich, germany
wenpeng@cis.lmu.de

8
1
0
2

 

n
u
j
 

5
2

 
 
]
l
c
.
s
c
[
 
 

4
v
3
9
1
5
0

.

2
1
5
1
:
v
i
x
r
a

abstract

how to model a pair of sentences is a critical
issue in many nlp tasks such as answer selec-
tion (as), paraphrase identi   cation (pi) and
id123 (te). most prior work (i)
deals with one individual task by    ne-tuning
a speci   c system; (ii) models each sentence   s
representation separately, rarely considering
the impact of the other sentence; or (iii) re-
lies fully on manually designed, task-speci   c
linguistic features. this work presents a gen-
eral attention based convolutional neural
network (abid98) for modeling a pair of
sentences. we make three contributions. (i)
the abid98 can be applied to a wide va-
riety of tasks that require modeling of sen-
tence pairs.
(ii) we propose three attention
schemes that integrate mutual in   uence be-
tween sentences into id98s;
thus, the rep-
resentation of each sentence takes into con-
sideration its counterpart. these interdepen-
dent sentence pair representations are more
powerful than isolated sentence representa-
tions.
(iii) abid98s achieve state-of-the-art
performance on as, pi and te tasks. we
release code at: https://github.com/
yinwenpeng/answer_selection.

1

introduction

how to model a pair of sentences is a critical is-
sue in many nlp tasks such as answer selection
(as) (yu et al., 2014; feng et al., 2015), paraphrase
identi   cation (pi) (madnani et al., 2012; yin and
sch  utze, 2015a), id123 (te) (marelli et
al., 2014a; bowman et al., 2015a) etc.

yorktown heights, ny, usa

bingxia,zhou@us.ibm.com

s
a

i
p

e
t

s0 how much did waterboy gross?
1 the movie earned $161.5 million
s+
   
1 this was jerry reed   s    nal    lm appearance
s
s0 she struck a deal with rh to pen a book today
1 she signed a contract with rh to write a book
s+
   
1 she denied today that she struck a deal with rh
s
s0 an ice skating rink placed outdoors is full of people
1 a lot of people are in an ice skating park
s+
   
1 an ice skating rink placed indoors is full of people
s

figure 1: positive (< s0, s+
1 >)
examples for as, pi and te tasks. rh = random house

1 >) and negative (< s0, s   

most prior work derives each sentence   s represen-
tation separately, rarely considering the impact of
the other sentence. this neglects the mutual in   u-
ence of the two sentences in the context of the task.
it also contradicts what humans do when comparing
two sentences. we usually focus on key parts of one
sentence by extracting parts from the other sentence
that are related by identity, synonymy, antonymy
and other relations. thus, human beings model the
two sentences together, using the content of one sen-
tence to guide the representation of the other.

figure 1 demonstrates that each sentence of a pair
partially determines which parts of the other sen-
tence we must focus on. for as, correctly answer-
ing s0 requires attention on    gross   : s+
1 contains
a corresponding unit (   earned   ) while s   
1 does not.
for pi, focus should be removed from    today    to
correctly recognize < s0, s+
1 > as paraphrases and
< s0, s   
1 > as non-paraphrases. for te, we need
to focus on    full of people    (to recognize te for
< s0, s+
1 >) and on    outdoors    /    indoors    (to recog-
nize non-te for < s0, s   
1 >). these examples show
the need for an architecture that computes different
representations of si for different s1   i (i     {0, 1}).

convolutional neural networks (id98s) (lecun
et al., 1998) are widely used to model sentences
(kalchbrenner et al., 2014; kim, 2014) and sen-
tence pairs (socher et al., 2011; yin and sch  utze,
2015a), especially in classi   cation tasks. id98s
are supposed to be good at extracting robust and
abstract features of input. this work presents the
abid98, an attention-based convolutional neural
network, that has a powerful mechanism for mod-
eling a sentence pair by taking into account the
interdependence between the two sentences. the
abid98 is a general architecture that can handle a
wide variety of sentence pair modeling tasks.

some prior work proposes simple mechanisms
that can be interpreted as controlling varying atten-
tion; e.g., yih et al. (2013) employ word alignment
to match related parts of the two sentences. in con-
trast, our attention scheme based on id98s models
relatedness between two parts fully automatically.
moreover, attention at multiple levels of granularity,
not only at word level, is achieved as we stack mul-
tiple convolution layers that increase abstraction.

prior work on attention in deep learning (dl)
mostly addresses id137
(lstms)
(hochreiter and schmidhuber, 1997).
lstms achieve attention usually in a word-to-word
scheme, and word representations mostly encode
the whole context within the sentence (bahdanau et
al., 2015; rockt  aschel et al., 2016). it is not clear
whether this is the best strategy; e.g., in the as ex-
ample in figure 1, it is possible to determine that
   how much    in s0 matches    $161.5 million    in s1
without taking the entire sentence contexts into ac-
count. this observation was also investigated by
yao et al. (2013b) where an information retrieval
system retrieves sentences with tokens labeled as
date by id39 or as cd by pos
tagging if there is a    when    question. however, la-
bels or pos tags require extra tools. id98s bene   t
from incorporating attention into representations of
local phrases detected by    lters; in contrast, lstms
encode the whole context to form attention-based
word representations     a strategy that is more com-
plex than the id98 strategy and (as our experiments
suggest) performs less well for some tasks.

apart from these differences, it is clear that atten-
tion has as much potential for id98s as it does for
lstms. as far as we know, this is the    rst nlp

paper that incorporates attention into id98s. our
abid98s get state-of-the-art in as and te tasks,
and competitive performance in pi, then obtains fur-
ther improvements over all three tasks when linguis-
tic features are used.
2 related work
non-dl on sentence pair modeling. sentence
pair modeling has attracted lots of attention in the
past decades. many tasks can be reduced to a se-
mantic text matching problem.
in this paper, we
adopt the arguments by yih et al. (2013) who ar-
gue against shallow approaches as well as against
semantic text matching approaches that can be com-
putationally expensive:

due to the variety of word choices
and inherent ambiguities in natural lan-
guage, bag-of-word approaches with sim-
ple surface-form word matching tend to
produce brittle results with poor predic-
tion accuracy (bilotti et al., 2007). as a
result, researchers put more emphasis on
exploiting syntactic and semantic struc-
ture. representative examples include
methods based on deeper semantic anal-
ysis (shen and lapata, 2007; moldovan et
al., 2007), tree edit-distance (punyakanok
et al., 2004; heilman and smith, 2010)
and quasi-synchronous grammars (wang
et al., 2007) that match the dependency
parse trees of the two sentences.

instead of focusing on the high-level semantic rep-
resentation, yih et al. (2013) turn their attention to
improving the shallow semantic component, lexical
semantics, by performing semantic matching based
on a latent word-alignment structure (cf. chang et al.
(2010)). lai and hockenmaier (2014) explore    ner-
grained word overlap and alignment between two
sentences using negation, hypernym, synonym and
antonym relations. yao et al. (2013a) extend word-
to-word alignment to phrase-to-phrase alignment by
a semi-markov crf. however, such approaches of-
ten require more computational resources.
in ad-
dition, employing syntactic or semantic parsers    
which produce errors on many sentences     to    nd
the best match between the structured representa-
tions of two sentences is not trivial.

dl on sentence pair modeling. to address
some of the challenges of non-dl work, much re-
cent work uses neural networks to model sentence
pairs for as, pi and te.

for as, yu et al. (2014) present a bigram id98 to
model question and answer candidates. yang et al.
(2015) extend this method and get state-of-the-art
performance on the wikiqa dataset (section 5.1).
feng et al. (2015) test various setups of a bi-id98 ar-
chitecture on an insurance domain qa dataset. tan
et al. (2016) explore bidirectional lstms on the
same dataset. our approach is different because we
do not model the sentences by two independent neu-
ral networks in parallel, but instead as an interdepen-
dent sentence pair, using attention.

for pi, blacoe and lapata (2012) form sentence
representations by summing up id27s.
socher et al. (2011) use recursive autoencoders
(raes) to model representations of local phrases
in sentences, then pool similarity values of phrases
from the two sentences as features for binary classi-
   cation. yin and sch  utze (2015a) similarly replace
an rae with a id98. in all three papers, the rep-
resentation of one sentence is not in   uenced by the
other     in contrast to our attention-based model.

for te, bowman et al. (2015b) use recursive neu-
ral networks to encode entailment on sick (marelli
et al., 2014b). rockt  aschel et al. (2016) present an
attention-based lstm for the stanford natural lan-
guage id136 corpus (bowman et al., 2015a). our
system is the    rst id98-based work on te.

some prior work aims to solve a general sen-
tence matching problem. hu et al. (2014) present
two id98 architectures, arc-i and arc-ii, for sen-
tence matching. arc-i focuses on sentence repre-
sentation learning while arc-ii focuses on match-
ing features on phrase level. both systems were
tested on pi, sentence completion (sc) and tweet-
response matching. yin and sch  utze (2015b) pro-
pose the multigranid98 architecture to model gen-
eral sentence matching based on phrase matching on
multiple levels of granularity and get promising re-
sults for pi and sc. wan et al. (2015) try to match
two sentences in as and sc by multiple sentence
representations, each coming from the local repre-
sentations of two lstms. our work is the    rst
one to investigate attention for the general sentence
matching task.

attention-based dl in non-nlp domains.
even though there is little if any work on atten-
tion mechanisms in id98s for nlp, attention-based
id98s have been used in id161 for visual
id53 (chen et al., 2015), image clas-
si   cation (xiao et al., 2015), id134 (xu
et al., 2015), image segmentation (hong et al., 2016)
and object localization (cao et al., 2015).

mnih et al. (2014) apply attention in recurrent
neural networks (id56s) to extract    information
from an image or video by adaptively selecting a
sequence of regions or locations and only process-
ing the selected regions at high resolution.    gre-
gor et al. (2015) combine a spatial attention mech-
anism with id56s for image generation. ba et al.
(2015) investigate attention-based id56s for recog-
nizing multiple objects in images. chorowski et al.
(2014) and chorowski et al. (2015) use attention in
id56s for id103.

attention-based dl in nlp. attention-based
dl systems have been applied to nlp after their
success in id161 and id103.
they mainly rely on id56s and end-to-end encoder-
decoders for tasks such as machine translation (bah-
danau et al., 2015; luong et al., 2015) and text re-
construction (li et al., 2015; rush et al., 2015). our
work takes the lead in exploring attention mecha-
nisms in id98s for nlp tasks.

3 bid98: basic bi-id98

we now introduce our basic (non-attention) id98
that is based on the siamese architecture (brom-
ley et al., 1993), i.e., it consists of two weight-
sharing id98s, each processing one of the two sen-
tences, and a    nal layer that solves the sentence pair
task. see figure 2. we refer to this architecture as
the bid98. the next section will then introduce the
abid98, an attention architecture that extends the
bid98. table 1 gives our notational conventions.

in our implementation and also in the mathemat-
ical formalization of the model given below, we
pad the two sentences to have the same length s =
max(s0, s1). however, in the    gures we show dif-
ferent lengths because this gives a better intuition of
how the model works.

we now describe the bid98   s four types of lay-
ers: input, convolution, average pooling and output.

symbol
s, s0, s1
v
w
di
w

description
sentence or sentence length
word
   lter width
dimensionality of input to layer i + 1
weight matrix

table 1: notation

figure 2: bid98: abid98 without attention

input layer. in the example in the    gure, the two
input sentences have 5 and 7 words, respectively.
each word is represented as a d0-dimensional pre-
computed id97 (mikolov et al., 2013) embed-
ding, d0 = 300. as a result, each sentence is repre-
sented as a feature map of dimension d0    s.
convolution layer. let v1, v2, . . . , vs be the
words of a sentence and ci     rw  d0, 0 < i < s +
w, the concatenated embeddings of vi   w+1, . . . , vi
where embeddings for vj are set to zero when j < 1
or j > s. we then generate the representation
pi     rd1 for the phrase vi   w+1, . . . , vi using the
convolution weights w     rd1  wd0 as follows:

pi = tanh(w    ci + b)

where b     rd1 is the bias.

average pooling layer. pooling (including min,
max, average pooling) is commonly used to extract

robust features from convolution. in this paper, we
introduce attention weighting as an alternative, but
use average pooling as a baseline as follows.

for the output feature map of the last convolu-
tion layer, we do column-wise averaging over all
columns, denoted as all-ap. this generates a rep-
resentation vector for each of the two sentences,
shown as the top    average pooling (all-ap)    layer
below    id28    in figure 2. these two
vectors are the basis for the sentence pair decision.
for the output feature map of non-   nal convolu-
tion layers, we do column-wise averaging over win-
dows of w consecutive columns, denoted as w-ap;
shown as the lower    average pooling (w-ap)    layer
in figure 2. for    lter width w, a convolution layer
transforms an input feature map of s columns into
a new feature map of s + w     1 columns; average
pooling transforms this back to s columns. this ar-
chitecture supports stacking an arbitrary number of
convolution-pooling blocks to extract increasingly
abstract features. input features to the bottom layer
are words, input features to the next layer are short
phrases and so on. each level generates more ab-
stract features of higher granularity.

the last layer is an output layer, chosen accord-
ing to the task; e.g., for binary classi   cation tasks,
this layer is id28 (see figure 2). other
types of output layers are introduced below.

we found that in most cases, performance is
boosted if we provide the output of all pooling lay-
ers as input to the output layer. for each non-   nal
average pooling layer, we perform w-ap (pooling
over windows of w columns) as described above, but
we also perform all-ap (pooling over all columns)
and forward the result to the output layer. this
improves performance because representations from
different layers cover the properties of the sentences
at different levels of abstraction and all of these lev-
els can be important for a particular sentence pair.

4 abid98: attention-based bid98

the abid98-1,

we now describe three architectures based on the
bid98,
the abid98-2 and the
abid98-3, that each introduces an attention mech-
anism for modeling sentence pairs; see figure 3.

abid98-1. the abid98-1 (figure 3(a)) em-
ploys an attention feature matrix a to in   uence con-

(a) one block in abid98-1

(b) one block in abid98-2

(c) one block in abid98-3

figure 3: three abid98 architectures

volution. attention features are intended to weight
those units of si more highly in convolution that are
relevant to a unit of s1   i (i     {0, 1}); we use the
term    unit    here to refer to words on the lowest level
and to phrases on higher levels of the network. fig-
ure 3(a) shows two unit representation feature maps
in red:
this part of the abid98-1 is the same as
in the bid98 (see figure 2). each column is the
representation of a unit, a word on the lowest level
and a phrase on higher levels. we    rst describe the
attention feature matrix a informally (layer    conv
input   , middle column, in figure 3(a)). a is gener-
ated by matching units of the left representation fea-
ture map with units of the right representation fea-
ture map such that the attention values of row i in
a denote the attention distribution of the i-th unit of
s0 with respect to s1, and the attention values of col-
umn j in a denote the attention distribution of the
j-th unit of s1 with respect to s0. a can be viewed as
a new feature map of s0 (resp. s1) in row (resp. col-
umn) direction because each row (resp. column) is a
new feature vector of a unit in s0 (resp. s1). thus, it
makes sense to combine this new feature map with
the representation feature maps and use both as in-
put to the convolution operation. we achieve this by
transforming a into the two blue matrices in figure
3(a) that have the same format as the representation
feature maps. as a result, the new input of convolu-
tion has two feature maps for each sentence (shown
in red and blue). our motivation is that the atten-
tion feature map will guide the convolution to learn
   counterpart-biased    sentence representations.
more formally, let fi,r     rd  s be the represen-
tation feature map of sentence i (i     {0, 1}). then
we de   ne the attention matrix a     rs  s as follows:
ai,j = match-score(f0,r[:, i], f1,r[:, j])
(1)
the function match-score can be de   ned in a variety
of ways. we found that 1/(1 + |x     y|) works well
where |    | is euclidean distance.

given attention matrix a, we generate the atten-

tion feature map fi,a for si as follows:

f0,a = w0    a(cid:62), f1,a = w1    a

the weight matrices w0     rd  s, w1     rd  s are
parameters of the model to be learned in training.1

1the weights of the two matrices are shared in our imple-

mentation to reduce the number of parameters of the model.

we stack the representation feature map fi,r and
the attention feature map fi,a as an order 3 tensor
and feed it into convolution to generate a higher-
level representation feature map for si (i     {0, 1}).
in figure 3(a), s0 has 5 units, s1 has 7. the output
of convolution (shown in the top layer,    lter width
w = 3) is a higher-level representation feature map
with 7 columns for s0 and 9 columns for s1.

abid98-2. the abid98-1 computes attention
weights directly on the input representation with the
aim of improving the features computed by convolu-
tion. the abid98-2 (figure 3(b)) instead computes
attention weights on the output of convolution with
the aim of reweighting this convolution output. in
the example shown in figure 3(b), the feature maps
output by convolution for s0 and s1 (layer marked
   convolution    in figure 3(b)) have 7 and 9 columns,
respectively; each column is the representation of a
unit. the attention matrix a compares all units in s0
with all units of s1. we sum all attention values for a
unit to derive a single attention weight for that unit.
this corresponds to summing all values in a row of
a for s0 (   col-wise sum   , resulting in the column
vector of size 7 shown) and summing all values in a
column for s1 (   row-wise sum   , resulting in the row
vector of size 9 shown).
more formally, let a     rs  s be the attention

matrix, a0,j =(cid:80) a[j, :] the attention weight of unit
j in s0, a1,j = (cid:80) a[:, j] the attention weight of

i,r     rd  (si+w   1) the output of
unit j in s1 and fc
convolution for si. then the j-th column of the new
(cid:88)
feature map fp

i,r generated by w-ap is derived by:

ai,kfc

i,r[:, k],

j = 1 . . . si

fp

i,r[:, j] =

k=j:j+w

i,r     rd  si, i.e., abid98-2 pooling
note that fp
generates an output feature map of the same size as
the input feature map of convolution. this allows
us to stack multiple convolution-pooling blocks to
extract features of increasing abstraction.

there are three main differences between the
abid98-1 and the abid98-2. (i) attention in the
abid98-1 impacts convolution indirectly while at-
tention in the abid98-2 in   uences pooling through
direct attention weighting. (ii) the abid98-1 re-
quires the two matrices wi to convert the attention
matrix into attention feature maps; and the input to

convolution has two times as many feature maps.
thus, the abid98-1 has more parameters than the
abid98-2 and is more vulnerable to over   tting.
(iii) as pooling is performed after convolution, pool-
ing handles larger-granularity units than convolu-
tion; e.g., if the input to convolution has word level
granularity, then the input to pooling has phrase level
granularity, the phrase size being equal to    lter size
w. thus, the abid98-1 and the abid98-2 imple-
ment attention mechanisms for linguistic units of
different granularity. the complementarity of the
abid98-1 and the abid98-2 motivates us to pro-
pose the abid98-3, a third architecture that com-
bines elements of the two.

abid98-3 (figure 3(c)) combines the abid98-1
and the abid98-2 by stacking them; it combines the
strengths of the abid98-1 and -2 by allowing the
attention mechanism to operate (i) both on the con-
volution and on the pooling parts of a convolution-
pooling block and (ii) both on the input granularity
and on the more abstract output granularity.

5 experiments

we test the proposed architectures on three tasks:
answer selection (as), paraphrase identi   cation (pi)
and id123 (te).

common training setup. words are initialized
by 300-dimensional id97 embeddings and not
changed during training. a single randomly initial-
ized embedding is created for all unknown words by
uniform sampling from [-.01,.01]. we employ ada-
grad (duchi et al., 2011) and l2 id173.

network con   guration. each network in the
experiments below consists of (i) an initialization
block b1 that initializes words by id97 em-
beddings, (ii) a stack of k     1 convolution-pooling
blocks b2, . . . , bk, computing increasingly abstract
features, and (iii) one    nal lr layer (logistic regres-
sion layer) as shown in figure 2.

the input to the lr layer consists of kn features
    each block provides n similarity scores, e.g., n
cosine similarity scores. figure 2 shows the two
sentence vectors output by the    nal block bk of the
stack (   sentence representation 0   ,    sentence repre-
sentation 1   ); this is the basis of the last n similarity
scores. as we explained in the    nal paragraph of
section 3, we perform all-ap pooling for all blocks,

as

pi

te

l
c
#

lr w l2

lr w l2

lr w l2

abid98-1 1 .08 4 .0004 .08 3 .0002 .08 3 .0006
abid98-1 2 .085 4 .0006 .085 3 .0003 .085 3 .0006
abid98-2 1 .05 4 .0003 .085 3 .0001 .09 3 .00065
abid98-2 2 .06 4 .0006 .085 3 .0001 .085 3 .0007
abid98-3 1 .05 4 .0003 .05 3 .0003 .09 3 .0007
abid98-3 2 .06 4 .0006 .055 3 .0005 .09 3 .0007

table 2: hyperparameters. lr: learning rate. #cl: num-
ber convolution layers. w:    lter width. the number of
convolution kernels di (i > 0) is 50 throughout.
not just for bk. thus we get one sentence representa-
tion each for s0 and s1 for each block b1, . . . , bk. we
compute n similarity scores for each block (based
on the block   s two sentence representations). thus,
we compute a total of kn similarity scores and these
scores are input to the lr layer.

depending on the task, we use different methods

for computing the similarity score: see below.

layerwise training.

in our training regime,
we    rst
train a network consisting of just one
convolution-pooling block b2. we then create a
new network by adding a block b3, initialize its b2
block with the previously learned weights for b2 and
train b3 keeping the previously learned weights for
b2    xed. we repeat this procedure until all k     1
convolution-pooling blocks are trained. we found
that this training regime gives us good performance
and shortens training times considerably. since sim-
ilarity scores of lower blocks are kept unchanged
once they have been learned, this also has the nice
effect that    simple    similarity scores (those based
on surface features) are learned    rst and subsequent
training phases can focus on complementary scores
derived from more complex abstract features.

classi   er. we found that performance increases
if we do not use the output of the lr layer as the
   nal decision, but instead train a linear id166 or a
id28 with default parameters2 directly
on the input to the lr layer (i.e., on the kn similarity
scores that are generated by the k-block stack after
network training is completed). direct training of
id166s/lr seems to get closer to the global optimum
than id119 training of id98s.

table 2 shows hyperparameters, tuned on dev.
we use addition and lstms as two shared base-
lines for all three tasks, i.e., for as, pi and te. we

2 http://scikit-learn.org/stable/ for both.

now describe these two shared baselines.

(i) addition. we sum up id27s
element-wise to form each sentence representation.
the classi   er input is then the concatenation of the
(ii) a-lstm. be-
two sentence representations.
fore this work, most attention mechanisms in nlp
were implemented in recurrent neural networks for
text generation tasks such as machine translation
(e.g., bahdanau et al. (2015), luong et al. (2015)).
rockt  aschel et al. (2016) present an attention-lstm
for natural language id136. since this model is
the pioneering attention based id56 system for sen-
tence pair classi   cation, we consider it as a baseline
system (   a-lstm   ) for all our three tasks. the a-
lstm has the same con   guration as our abid98s
in terms of word initialization (300-dimensional
id97 embeddings) and the dimensionality of all
hidden layers (50).

5.1 answer selection
we use wikiqa,3 an open domain question-answer
dataset. we use the subtask that assumes that there
is at least one correct answer for a question. the
corresponding dataset consists of 20,360 question-
candidate pairs in train, 1,130 pairs in dev and 2,352
pairs in test where we adopt the standard setup of
only considering questions with correct answers in
test. following yang et al. (2015), we truncate an-
swers to 40 tokens.

the task is to rank the candidate answers based
on their relatedness to the question. evaluation mea-
sures are mean average precision (map) and mean
reciprocal rank (mrr).

task-speci   c setup. we use cosine similarity as
the similarity score for as. in addition, we use sen-
tence lengths, wordcnt (count of the number of non-
stopwords in the question that also occur in the an-
swer) and wgtwordcnt (reweight the counts by the
idf values of the question words). thus, the    nal
input to the lr layer has size k + 4: one cosine for
each of the k blocks and the four additional features.
we compare with seven baselines. the    rst three
are considered by yang et al. (2015): (i) wordcnt;
(ii) wgtwordcnt; (iii) id98-cnt (the state-of-the-
art system): combine id98 with (i) and (ii). apart
from the baselines considered by yang et al. (2015),

3http://aka.ms/wikiqa (yang et al., 2015)

map mrr
method
0.4924
wordcnt
0.4891
0.5132
wgtwordcnt 0.5099
0.6520
id98-cnt
0.6652
0.5069
0.5021
addition
addition(+)
0.5888
0.5929
0.5483
0.5347
a-lstm
0.6537
0.6381
a-lstm(+)
0.6813
0.6629
one-conv
two-conv
0.6593
0.6738
0.6810    0.6979   
one-conv
0.6855    0.7023   
two-conv
0.6885    0.7054   
one-conv
0.6879    0.7068   
two-conv
0.6914    0.7127   
one-conv
0.6921    0.7108   
two-conv

s
e
n
i
l
e
s
a
b

bid98

abid98-1

abid98-2

abid98-3

table 3: results on wikiqa. best result per column
is bold. signi   cant improvements over state-of-the-art
baselines (underlined) are marked with     (t-test, p < .05).

we compare with two addition baselines and two
lstm baselines. addition and a-lstm are the
shared baselines described before. we also combine
both with the four extra features; this gives us two
additional baselines that we refer to as addition(+)
and a-lstm(+).

results. table 3 shows performance of the base-
lines, of the bid98 and of the three abid98s. for
id98s, we test one (one-conv) and two (two-conv)
convolution-pooling blocks.

the non-attention network bid98 already per-
forms better than the baselines. if we add attention
mechanisms, then the performance further improves
by several points. comparing the abid98-2 with
the abid98-1, we    nd the abid98-2 is slightly
better even though the abid98-2 is the simpler ar-
chitecture.
if we combine the abid98-1 and the
abid98-2 to form the abid98-3, we get further
improvement.4

this can be explained by the abid98-3   s abil-
ity to take attention of    ner-grained granularity into
consideration in each convolution-pooling block
while the abid98-1 and the abid98-2 consider at-
tention only at convolution input or only at pooling
input, respectively. we also    nd that stacking two
convolution-pooling blocks does not bring consis-
tent improvement and therefore do not test deeper
architectures.

4if we limit the input to the lr layer to the k similarity
scores in the abid98-3 (two-conv), results are .660 (map) /
.677 (mrr).

5.2 paraphrase identi   cation
we use the microsoft research paraphrase (msrp)
corpus (dolan et al., 2004). the training set contains
2753 true / 1323 false and the test set 1147 true /
578 false paraphrase pairs. we randomly select 400
pairs from train and use them as dev; but we still
report results for training on the entire training set.
for each triple (label, s0, s1) in the training set, we
also add (label, s1, s0) to the training set to make
best use of the training data. systems are evaluated
by accuracy and f1.

task-speci   c setup.

in this task, we add the
15 mt features from (madnani et al., 2012) and
the lengths of the two sentences.
in addition, we
compute id8-1, id8-2 and id8-su4
(lin, 2004), which are scores measuring the match
between the two sentences on (i) unigrams, (ii) bi-
grams and (iii) unigrams and skip-bigrams (maxi-
mum skip distance of four), respectively.
in this
task, we found transforming euclidean distance into
similarity score by 1/(1 + |x     y|) performs better
than cosine similarity. additionally, we use dynamic
pooling (yin and sch  utze, 2015a) of the attention
matrix a in equation (1) and forward pooled val-
ues of all blocks to the classi   er. this gives us bet-
ter performance than only forwarding sentence-level
matching features.

we compare our system with representative dl
approaches:
(i) a-lstm; (ii) a-lstm(+): a-
lstm plus handcrafted features; (iii) rae (socher
et al., 2011), recursive autoencoder; (iv) bi-id98-
mi (yin and sch  utze, 2015a), a bi-id98 architec-
ture; and (v) mpssm-id98 (he et al., 2015), the
state-of-the-art nn system for pi, and the follow-
ing four non-dl systems: (vi) addition; (vii) ad-
dition(+): addition plus handcrafted features; (viii)
mt (madnani et al., 2012), a system that combines
machine translation metrics;5 (ix) mf-tf-kld (ji
and eisenstein, 2013), the state-of-the-art non-nn
system.

results. table 4 shows that the bid98 is slightly
worse than the state-of-the-art whereas the abid98-
1 roughly matches it. the abid98-2 is slightly
above the state-of-the-art. the abid98-3 outper-

method

acc f1

s
e
n
i
l
e
s
a
b

majority voting 66.5 79.9
76.8 83.6
rae
bi-id98-mi
78.4 84.6
mpssm-id98 78.6 84.7
mt
76.8 83.8
78.6 84.6
mf-tf-kld
70.8 80.9
addition
77.3 84.1
addition (+)
69.5 80.1
a-lstm
a-lstm (+)
77.1 84.0
78.1 84.1
one-conv
78.3 84.3
two-conv
78.5 84.5
one-conv
two-conv
78.5 84.6
78.6 84.7
one-conv
78.8 84.7
two-conv
78.8 84.8
one-conv
78.9 84.8
two-conv
table 4: results for pi on msrp

abid98-1

abid98-2

abid98-3

bid98

forms the state-of-the-art in accuracy and f1.6 two
convolution layers only bring small improvements
over one.

5.3 id123
semeval 2014 task 1 (marelli et al., 2014a) eval-
uates system predictions of id123 (te)
relations on sentence pairs from the sick dataset
(marelli et al., 2014b). the three classes are entail-
ment, contradiction and neutral. the sizes of sick
train, dev and test sets are 4439, 495 and 4906 pairs,
respectively. we call this dataset orig.

we also create nonover, a copy of orig in
which words occurring in both sentences are re-
moved. a sentence in nonover is denoted by the
special token <empty> if all words are removed.
table 5 shows three pairs from orig and their trans-
formation in nonover. we observe that focusing
on the non-overlapping parts provides clearer hints
for te than orig. in this task, we run two copies of
each network, one for orig, one for nonover;
these two networks have a single common lr layer.
like lai and hockenmaier (2014), we train our
   nal system (after    xing hyperparameters) on train
and dev (4934 pairs). eval measure is accuracy.

task-speci   c setup. we found that for this task
forwarding two similarity scores from each block

5for better comparability of approaches in our experiments,
we use a simple id166 classi   er, which performs slightly worse
than madnani et al. (2012)   s more complex meta-classi   er.

6improvement of .3 (acc) and .1 (f1) over state-of-the-art is
not signi   cant. the abid98-3 (two-conv) without    linguistic   
features (i.e., mt and id8) achieves 75.1/82.7.

orig
children in red shirts are
playing in the leaves
three kids are sitting in the leaves
three boys are jumping in the leaves boys
three kids are jumping in the leaves
kids
a man is jumping into an empty pool an empty
a man is jumping into a full pool

a full

nonover
children red shirts
playing
three kids sitting

0

1

2

table 5: sick data: converting the original sentences
(orig) into the nonover format

(instead of just one) is helpful. we use cosine sim-
ilarity and euclidean distance. as we did for para-
phrase identi   cation, we add the 15 mt features for
each sentence pair for this task as well; our motiva-
tion is that entailed sentences resemble paraphrases
more than contradictory sentences do.

we use the following linguistic features. nega-
tion is important for detecting contradiction. fea-
ture neg is set to 1 if either sentence contains    no   ,
   not   ,    nobody   ,    isn   t    and to 0 otherwise. fol-
lowing lai and hockenmaier (2014), we use word-
net (miller, 1995) to detect nyms: synonyms, hy-
pernyms and antonyms in the pairs. but we do this
on nonover (not on orig) to focus on what
is critical for te. speci   cally, feature syn is the
number of word pairs in s0 and s1 that are syn-
onyms. hyp0 (resp. hyp1) is the number of words
in s0 (resp. s1) that have a hypernym in s1 (resp.
in addition, we collect all potential antonym
s0).
pairs (pap) in nonover. we identify the matched
chunks that occur in contradictory and neutral, but
not in entailed pairs. we exclude synonyms and hy-
pernyms and apply a frequency    lter of n = 2. in
contrast to lai and hockenmaier (2014), we con-
strain the pap pairs to cosine similarity above 0.4
in id97 embedding space as this discards many
noise pairs. feature ant is the number of matched
pap antonyms in a sentence pair. as before we use
sentence lengths, both for orig (len0o:
length
s0, len1o: length s1) and for nonover (len0n:
length s0, len1n: length s1).

on the whole, we have 24 extra features: 15
mt metrics, neg, syn, hyp0, hyp1, ant, len0o,
len1o, len0n and len1n.

apart from the addition and lstm baselines, we
further compare with the top-3 systems in semeval
and trrntn (bowman et al., 2015b), a recursive
neural network developed for this sick task.

3 (jimenez et al., 2014)

trrntn (bowman et al., 2015b)

-

m
e
s

l
a
v
e

p
o
t

addition

a-lstm

bid98

abid98-1

abid98-2

abid98-3

method

acc
83.1
(zhao et al., 2014)
83.6
(lai and hockenmaier, 2014) 84.6
76.9
73.1
79.4
78.0
81.7
84.8
85.0
85.6
85.8
85.7
85.8
86.0   
86.2   

no features
plus features
no features
plus features
one-conv
two-conv
one-conv
two-conv
one-conv
two-conv
one-conv
two-conv

table 6: results on sick. signi   cant improvements over
(lai and hockenmaier, 2014) are marked with     (test of
equal proportions, p < .05).

results. table 6 shows that our id98s outper-
form a-lstm (with or without linguistic features
added) and the top three semeval systems. compar-
ing abid98s with the bid98, attention mechanisms
consistently improve performance. the abid98-1
has performance comparable to the abid98-2 while
the abid98-3 is better still: a boost of 1.6 points
compared to the previous state of the art.7

visual analysis. figure 4 visualizes the attention
matrices for one te sentence pair in the abid98-
2 for blocks b1 (unigrams), b2 (   rst convolutional
layer) and b3 (second convolutional layer). darker
shades of blue indicate stronger attention values.

in figure 4 (top), each word corresponds to ex-
actly one row or column. we can see that words in
si with semantic equivalents in s1   i get high atten-
tion while words without semantic equivalents get
low attention, e.g.,    walking    and    murals    in s0 and
   front    and    colorful    in s1. this behavior seems
reasonable for the unigram level.

rows/columns of the attention matrix in figure 4
(middle) correspond to phrases of length three since
   lter width w = 3. high attention values generally
correlate with close semantic correspondence:
the
phrase    people are    in s0 matches    several people
are    in s1; both    are walking outside    and    walking
outside the    in s0 match    are in front    in s1;    the
building that    in s0 matches    a colorful building    in

7if we run the abid98-3 (two-conv) without the 24 linguis-

tic features, performance is 84.6.

and    of a colorful building    in s1.

rows/columns of the attention matrix in figure 4
(bottom, second layer of convolution) correspond
to phrases of length 5 since    lter width w = 3 in
both convolution layers (5 = 1 + 2     (3     1)). we
use    . . .    to denote words in the middle if a phrase
like    several...front    has more than two words. we
can see that attention distribution in the matrix has
focused on some local regions. as granularity of
phrases is larger, it makes sense that the attention
values are smoother. but we still can    nd some
interesting clues: at the two ends of the main di-
agonal, higher attentions hint that the    rst part of
s0 matches well with the    rst part of s1;    several
murals on it    in s0 matches well with    of a color-
ful building    in s1, which satis   es the intuition that
these two phrases are crucial for making a decision
on te in this case. this again shows the potential
strength of our system in    guring out which parts of
the two sentences refer to the same object. in ad-
dition, in the central part of the matrix, we can see
that the long phrase    people are walking outside the
building    in s0 matches well with the long phrase
   are in front of a colorful building    in s1.

6 summary

we presented three mechanisms to integrate atten-
tion into id98s for general sentence pair modeling
tasks.

our experiments on as, pi and te show that
attention-based id98s perform better than id98s
without attention mechanisms. the abid98-2 gen-
erally outperforms the abid98-1 and the abid98-
3 surpasses both.

in all tasks, we did not    nd any big improvement
of two layers of convolution over one layer. this is
probably due to the limited size of training data. we
expect that, as larger training sets become available,
deep abid98s will show even better performance.
in addition, linguistic features contribute in all
three tasks:
improvements by 0.0321 (map) and
0.0338 (mrr) for as, improvements by 3.8 (acc)
and 2.1 (f1) for pi and an improvement by 1.6 (acc)
for te. but our abid98s can still reach or surpass
state-of-the-art even without those features in as
and te tasks. this indicates that abid98s are gen-
erally strong nn systems.

figure 4: attention visualization for te. top: unigrams,
b1. middle: conv1, b2. bottom: conv2, b3.

s1. more interestingly, looking at the bottom right
corner, both    on it    and    it    in s0 match    building   
in s1; this indicates that abid98s are able to detect
some coreference across sentences.    building    in
s1 has two places in which higher attentions appear,
one is with    it    in s0, the other is with    the building
that    in s0. this may indicate that abid98s recog-
nize that    building    in s1 and    the building that    /
   it    in s0 refer to the same object. hence, corefer-
ence resolution across sentences as well as within a
sentence both are detected. for the attention vectors
on the left and the top, we can see that attention has
focused on the key parts:    people are walking out-
side the building that    in s0,    several people are in   

severalpeopleareinfrontofacolorfulbuildingpeoplearewalkingoutsidethebuildingthathasseveralmuralsonitseveralseveral peopleseveral people are people are inare in frontin front offront of aof a colorfula corlorful buildingcorlorful buildingbuildingpeoplepeople arepeople are walkingare walking outsidewalking outside theoutside the buildingthe building thatbuilding that hasthat has severalhas several muralsseveral murals onmurals on iton ititseveralseveral peopleseveral...areseveral...inseveral...frontpeople...ofare...ain...colorfulfront...buildingof...buildinga...buildingpeoplepeople arepeople...walkingpeople...outsidepeople...theare...buildingwalking...thatoutside...hasthe...severalbuilding...muralsthat...onhas...itseveral...itmurals...itattention-based lstms are especially successful
in tasks with a strong generation component like ma-
chine translation (discussed in sec. 2). id98s have
not been used for this type of task. this is an inter-
esting area of future work for attention-based id98s.

acknowledgments
we gratefully acknowledge the support of deutsche
forschungsgemeinschaft
grant schu
2246/8-2.

(dfg):

we would like to thank the anonymous reviewers

for their helpful comments.

ming-wei chang, dan goldwasser, dan roth, and vivek
srikumar. 2010. discriminative learning over con-
in proceedings of
strained latent representations.
naacl-hlt, pages 429   437.

kan chen, jiang wang, liang-chieh chen, haoyuan
gao, wei xu, and ram nevatia.
2015. abc-
id98: an attention based convolutional neural net-
work for visual id53. arxiv preprint
arxiv:1511.05960.

jan chorowski, dzmitry bahdanau, kyunghyun cho, and
yoshua bengio. 2014. end-to-end continuous speech
recognition using attention-based recurrent nn: first
results. in proceedings of deep learning and repre-
sentation learning workshop, nips.

references

jimmy ba, volodymyr mnih, and koray kavukcuoglu.
2015. multiple object recognition with visual atten-
tion. in proceedings of iclr.

dzmitry bahdanau, kyunghyun cho, and yoshua ben-
2015. id4 by jointly
in proceedings of

gio.
learning to align and translate.
iclr.

matthew w bilotti, paul ogilvie, jamie callan, and eric
nyberg. 2007. structured retrieval for question an-
swering. in proceedings of sigir, pages 351   358.

william blacoe and mirella lapata. 2012. a comparison
of vector-based representations for semantic composi-
tion. in proceedings of emnlp-conll, pages 546   
556.

samuel r bowman, gabor angeli, christopher potts, and
christopher d manning. 2015a. a large annotated
corpus for learning natural language id136. in pro-
ceedings of emnlp, pages 632   642.

samuel r bowman, christopher potts, and christopher d
manning. 2015b. id56s can learn
logical semantics. in proceedings of cvsc workshop,
pages 12   21.

jane broid113y, james w bentz, l  eon bottou, isabelle
guyon, yann lecun, cliff moore, eduard s  ackinger,
and roopak shah. 1993. signature veri   cation using
international
a siamese time delay neural network.
journal of pattern recognition and arti   cial intelli-
gence, 7(04):669   688.

chunshui cao, xianming liu, yi yang, yinan yu,
jiang wang, zilei wang, yongzhen huang, liang
wang, chang huang, wei xu, deva ramanan, and
thomas s. huang. 2015. look and think twice: cap-
turing top-down visual attention with feedback con-
volutional neural networks. in proceedings of iccv,
pages 2956   2964.

jan k chorowski, dzmitry bahdanau, dmitriy serdyuk,
2015.
in

kyunghyun cho,
attention-based models for id103.
proceedings of nips, pages 577   585.

and yoshua bengio.

bill dolan, chris quirk, and chris brockett. 2004. un-
supervised construction of large paraphrase corpora:
in pro-
exploiting massively parallel news sources.
ceedings of coling, pages 350   356.

john duchi, elad hazan, and yoram singer.

2011.
adaptive subgradient methods for online learning and
stochastic optimization. jmlr, 12:2121   2159.

minwei feng, bing xiang, michael r glass, lidan
wang, and bowen zhou. 2015. applying deep learn-
ing to answer selection: a study and an open task.
proceedings of ieee asru workshop.

karol gregor,

ivo danihelka,

alex graves,
and daan wierstra.
draw: a recurrent neural network for
in proceedings of icml, pages

danilo jimenez rezende,
2015.
image generation.
1462   1471.

hua he, kevin gimpel, and jimmy lin. 2015. multi-
perspective sentence similarity modeling with convo-
lutional neural networks. in proceedings of emnlp,
pages 1576   1586.

michael heilman and noah a smith.

2010. tree
edit models for recognizing id123s, para-
phrases, and answers to questions. in proceedings of
naacl-hlt, pages 1011   1019.

sepp hochreiter and j  urgen schmidhuber. 1997. long
short-term memory. neural computation, 9(8):1735   
1780.

seunghoon hong, junhyuk oh, bohyung han, and
honglak lee. 2016. learning transferrable knowl-
edge for semantic segmentation with deep convolu-
tional neural network. in proceedings of cvpr.

baotian hu, zhengdong lu, hang li, and qingcai chen.
2014. convolutional neural network architectures for
matching natural language sentences. in proceedings
of nips, pages 2042   2050.

yangfeng ji and jacob eisenstein. 2013. discriminative
improvements to distributional sentence similarity. in
proceedings of emnlp, pages 891   896.

sergio jimenez, george duenas, julia baquero, alexan-
der gelbukh, av juan dios b  atiz, and av mendiz  abal.
2014. unal-nlp: combining soft cardinality fea-
tures for semantic textual similarity, relatedness and
in proceedings of semeval, pages 732   
entailment.
742.

nal kalchbrenner, edward grefenstette, and phil blun-
som. 2014. a convolutional neural network for mod-
elling sentences. in proceedings of acl, pages 655   
665.

yoon kim. 2014. convolutional neural networks for sen-
tence classi   cation. in proceedings of emnlp, pages
1746   1751.

alice lai and julia hockenmaier. 2014. illinois-lh: a
denotational and distributional approach to semantics.
in proceedings of semeval, pages 329   334.

yann lecun, l  eon bottou, yoshua bengio, and patrick
haffner. 1998. gradient-based learning applied to
in proceedings of the ieee,
document recognition.
pages 2278   2324.

jiwei li, minh-thang luong, and dan jurafsky. 2015.
a hierarchical neural autoencoder for paragraphs and
documents. in proceedings of acl, pages 1106   1115.
chin-yew lin. 2004. id8: a package for automatic
evaluation of summaries. in proceedings of the acl
text summarization workshop.

minh-thang luong, hieu pham, and christopher d
manning. 2015. effective approaches to attention-
in proceedings of
based id4.
emnlp, pages 1412   1421.

nitin madnani, joel tetreault, and martin chodorow.
2012. re-examining machine translation metrics for
paraphrase identi   cation. in proceedings of naacl,
pages 182   190.

marco marelli, luisa bentivogli, marco baroni, raf-
faella bernardi, stefano menini, and roberto zampar-
elli. 2014a. semeval-2014 task 1: evaluation of com-
positional distributional semantic models on full sen-
tences through semantic relatedness and textual entail-
ment. in proceedings of semeval, pages 1   8.

marco marelli, stefano menini, marco baroni, luisa
bentivogli, raffaella bernardi, and roberto zampar-
elli. 2014b. a sick cure for the evaluation of com-
positional distributional semantic models. in proceed-
ings of lrec, pages 216   223.

tomas mikolov, ilya sutskever, kai chen, greg s cor-
rado, and jeff dean. 2013. distributed representa-
tions of words and phrases and their compositionality.
in proceedings of nips, pages 3111   3119.

george a miller. 1995. id138: a lexical database for
english. communications of the acm, 38(11):39   41.

volodymyr mnih, nicolas heess, alex graves, and ko-
ray kavukcuoglu. 2014. recurrent models of visual
attention. in proceedings of nips, pages 2204   2212.
dan moldovan, christine clark, sanda harabagiu, and
daniel hodges. 2007. cogex: a semantically and
contextually enriched logic prover for question an-
swering. journal of applied logic, 5(1):49   69.

vasin punyakanok, dan roth, and wen-tau yih. 2004.
mapping dependencies trees: an application to ques-
in proceedings of ai&math, pages
tion answering.
1   10.

tim rockt  aschel, edward grefenstette, karl moritz her-
mann, tom  a  s ko  cisk`y, and phil blunsom. 2016. rea-
soning about entailment with neural attention. in pro-
ceedings of iclr.

alexander m rush, sumit chopra, and jason weston.
2015. a neural attention model for abstractive sen-
in proceedings of emnlp,
tence summarization.
pages 379   389.

dan shen and mirella lapata. 2007. using semantic
roles to improve id53. in proceedings
of emnlp-conll, pages 12   21.

richard socher, eric h huang, jeffrey pennin, christo-
pher d manning, and andrew y ng. 2011. dynamic
pooling and unfolding recursive autoencoders for para-
phrase detection. in proceedings of nips, pages 801   
809.

ming tan, bing xiang, and bowen zhou. 2016. lstm-
based deep learning models for non-factoid answer se-
lection. in proceedings of iclr workshop.

shengxian wan, yanyan lan, jiafeng guo, jun xu, liang
pang, and xueqi cheng. 2015. a deep architecture for
semantic matching with multiple positional sentence
representations. arxiv preprint arxiv:1511.08277.

mengqiu wang, noah a smith, and teruko mitamura.
is the jeopardy model? a quasi-
in proceedings of

2007. what
synchronous grammar for qa.
emnlp-conll, pages 22   32.

tianjun xiao, yichong xu, kuiyuan yang, jiaxing
zhang, yuxin peng, and zheng zhang. 2015. the
application of two-level id12 in deep con-
volutional neural network for    ne-grained image clas-
si   cation. in proceedings of cvpr, pages 842   850.

kelvin xu, jimmy ba, ryan kiros, kyunghyun cho,
aaron c. courville, ruslan salakhutdinov, richard s.
zemel, and yoshua bengio. 2015. show, attend and
tell: neural image id134 with visual at-
tention. in proceedings of icml, pages 2048   2057.

yi yang, wen-tau yih, and christopher meek. 2015.
wikiqa: a challenge dataset for open-domain ques-
in proceedings of emnlp, pages
tion answering.
2013   2018.

xuchen yao, benjamin van durme, chris callison-
burch, and peter clark. 2013a. semi-markov phrase-
in proceedings of
based monolingual alignment.
emnlp, pages 590   600.

xuchen yao, benjamin van durme, and peter clark.
2013b. automatic coupling of answer extraction and
in proceedings of acl, pages
information retrieval.
159   165.

wen-tau yih, ming-wei chang, christopher meek, and
andrzej pastusiak. 2013. id53 using
enhanced lexical semantic models. in proceedings of
acl, pages 1744   1753.

wenpeng yin and hinrich sch  utze. 2015a. convolu-
tional neural network for paraphrase identi   cation. in
proceedings of naacl, pages 901   911.

wenpeng yin and hinrich sch  utze.

2015b. multi-
granid98: an architecture for general matching of
text chunks on multiple levels of granularity. in pro-
ceedings of acl-ijcnlp, pages 63   73.

lei yu, karl moritz hermann, phil blunsom, and
stephen pulman. 2014. deep learning for answer sen-
tence selection. in proceedings of nips deep learning
workshop.

jiang zhao, tian tian zhu, and man lan. 2014. ecnu:
one stone two birds: ensemble of heterogenous mea-
sures for semantic relatedness and id123.
in proceedings of semeval, pages 271   277.

