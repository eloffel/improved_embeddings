character-aware neural language models

yacine jernite   
yoon kim   
   school of engineering and applied sciences
{yoonkim,srush}@seas.harvard.edu

harvard university

david sontag   

alexander m. rush   

   courant institute of mathematical sciences
{jernite,dsontag}@cs.nyu.edu

new york university

5
1
0
2
 
c
e
d
1

 

 
 
]
l
c
.
s
c
[
 
 

4
v
5
1
6
6
0

.

8
0
5
1
:
v
i
x
r
a

abstract

we describe a simple neural language model that re-
lies only on character-level inputs. predictions are still
made at the word-level. our model employs a con-
volutional neural network (id98) and a highway net-
work over characters, whose output
is given to a
long short-term memory (lstm) recurrent neural net-
work language model (id56-lm). on the english
id32 the model is on par with the existing
state-of-the-art despite having 60% fewer parameters.
on languages with rich morphology (arabic, czech,
french, german, spanish, russian), the model out-
performs word-level/morpheme-level lstm baselines,
again with fewer parameters. the results suggest that on
many languages, character inputs are suf   cient for lan-
guage modeling. analysis of word representations ob-
tained from the character composition part of the model
reveals that the model is able to encode, from characters
only, both semantic and orthographic information.

introduction

id38 is a fundamental task in arti   cial intel-
ligence and natural language processing (nlp), with appli-
cations in id103, text generation, and machine
translation. a language model is formalized as a id203
distribution over a sequence of strings (words), and tradi-
tional methods usually involve making an n-th order markov
assumption and estimating id165 probabilities via count-
ing and subsequent smoothing (chen and goodman 1998).
the count-based models are simple to train, but probabilities
of rare id165s can be poorly estimated due to data sparsity
(despite smoothing techniques).

neural language models (nlm) address the id165 data
sparsity issue through parameterization of words as vectors
(id27s) and using them as inputs to a neural net-
work (bengio, ducharme, and vincent 2003; mikolov et al.
2010). the parameters are learned as part of the training
process. id27s obtained through nlms exhibit
the property whereby semantically close words are likewise
close in the induced vector space (as is the case with non-
neural techniques such as latent semantic analysis (deer-
wester, dumais, and harshman 1990)).
copyright c(cid:13) 2016, association for the advancement of arti   cial
intelligence (www.aaai.org). all rights reserved.

while nlms have been shown to outperform count-based
id165 language models (mikolov et al. 2011), they are
blind to subword information (e.g. morphemes). for exam-
ple, they do not know, a priori, that eventful, eventfully, un-
eventful, and uneventfully should have structurally related
embeddings in the vector space. embeddings of rare words
can thus be poorly estimated, leading to high perplexities
for rare words (and words surrounding them). this is espe-
cially problematic in morphologically rich languages with
long-tailed frequency distributions or domains with dynamic
vocabularies (e.g. social media).

in this work, we propose a language model that lever-
ages subword information through a character-level con-
volutional neural network (id98), whose output is used
as an input to a recurrent neural network language model
(id56-lm). unlike previous works that utilize subword in-
formation via morphemes (botha and blunsom 2014; lu-
ong, socher, and manning 2013), our model does not require
morphological tagging as a pre-processing step. and, unlike
the recent line of work which combines input word embed-
dings with features from a character-level model (dos santos
and zadrozny 2014; dos santos and guimaraes 2015), our
model does not utilize id27s at all in the input
layer. given that most of the parameters in nlms are from
the id27s, the proposed model has signi   cantly
fewer parameters than previous nlms, making it attractive
for applications where model size may be an issue (e.g. cell
phones).

to summarize, our contributions are as follows:

    on english, we achieve results on par with the existing
state-of-the-art on the id32 (ptb), despite hav-
ing approximately 60% fewer parameters, and

    on morphologically rich languages (arabic, czech,
french, german, spanish, and russian), our model
outperforms various baselines
(kneser-ney, word-
level/morpheme-level lstm), again with fewer parame-
ters.

we have released all the code for the models described in
this paper.1

1https://github.com/yoonkim/lstm-char-id98

model

the architecture of our model, shown in figure 1, is straight-
forward. whereas a conventional nlm takes word embed-
dings as inputs, our model instead takes the output from
a single-layer character-level convolutional neural network
with max-over-time pooling.

for notation, we denote vectors with bold lower-case (e.g.
xt, b), matrices with bold upper-case (e.g. w, uo), scalars
with italic lower-case (e.g. x, b), and sets with cursive upper-
case (e.g. v,c) letters. for notational convenience we as-
sume that words and characters have already been converted
into indices.
recurrent neural network
a recurrent neural network (id56) is a type of neural net-
work architecture particularly suited for modeling sequen-
tial phenomena. at each time step t, an id56 takes the input
vector xt     rn and the hidden state vector ht   1     rm and
produces the next hidden state ht by applying the following
recursive operation:

ht = f (wxt + uht   1 + b)

(1)
here w     rm  n, u     rm  m, b     rm are parameters
of an af   ne transformation and f is an element-wise nonlin-
earity. in theory the id56 can summarize all historical in-
formation up to time t with the hidden state ht. in practice
however, learning long-range dependencies with a vanilla
id56 is dif   cult due to vanishing/exploding gradients (ben-
gio, simard, and frasconi 1994), which occurs as a result of
the jacobian   s multiplicativity with respect to time.

long short-term memory (lstm)

(hochreiter and
schmidhuber 1997) addresses the problem of learning long
range dependencies by augmenting the id56 with a memory
cell vector ct     rn at each time step. concretely, one step
of an lstm takes as input xt, ht   1, ct   1 and produces ht,
ct via the following intermediate calculations:

it =   (wixt + uiht   1 + bi)
ft =   (wf xt + uf ht   1 + bf )
ot =   (woxt + uoht   1 + bo)
gt = tanh(wgxt + ught   1 + bg)
ct = ft (cid:12) ct   1 + it (cid:12) gt
ht = ot (cid:12) tanh(ct)

(2)

here   (  ) and tanh(  ) are the element-wise sigmoid and hy-
perbolic tangent functions, (cid:12) is the element-wise multipli-
cation operator, and it, ft, ot are referred to as input, for-
get, and output gates. at t = 1, h0 and c0 are initialized to
zero vectors. parameters of the lstm are wj, uj, bj for
j     {i, f, o, g}.

memory cells in the lstm are additive with respect to
time, alleviating the gradient vanishing problem. gradient
exploding is still an issue, though in practice simple opti-
mization strategies (such as gradient clipping) work well.
lstms have been shown to outperform vanilla id56s on
many tasks, including on id38 (sundermeyer,
schluter, and ney 2012). it is easy to extend the id56/lstm
to two (or more) layers by having another network whose

figure 1: architecture of our language model applied to an exam-
ple sentence. best viewed in color. here the model takes absurdity
as the current input and combines it with the history (as represented
by the hidden state) to predict the next word, is. first layer performs
a lookup of character embeddings (of dimension four) and stacks
them to form the matrix ck. then convolution operations are ap-
plied between ck and multiple    lter matrices. note that in the
above example we have twelve    lters   three    lters of width two
(blue), four    lters of width three (yellow), and    ve    lters of width
four (red). a max-over-time pooling operation is applied to obtain
a    xed-dimensional representation of the word, which is given to
the highway network. the highway network   s output is used as the
input to a multi-layer lstm. finally, an af   ne transformation fol-
lowed by a softmax is applied over the hidden representation of
the lstm to obtain the distribution over the next word. cross en-
tropy loss between the (predicted) distribution over next word and
the actual next word is minimized. element-wise addition, multi-
plication, and sigmoid operators are depicted in circles, and af   ne
transformations (plus nonlinearities where appropriate) are repre-
sented by solid arrows.

input at t is ht (from the    rst network). indeed, having mul-
tiple layers is often crucial for obtaining competitive perfor-
mance on various tasks (pascanu et al. 2013).

recurrent neural network language model
let v be the    xed size vocabulary of words. a language
model speci   es a distribution over wt+1 (whose support is
v) given the historical sequence w1:t = [w1, . . . , wt]. a re-
current neural network language model (id56-lm) does this

n ll =     t(cid:88)

by applying an af   ne transformation to the hidden layer fol-
lowed by a softmax:

(cid:80)
exp(ht    pj + qj)
j(cid:48)   v exp(ht    pj(cid:48)

)

pr(wt+1 = j|w1:t) =

+ qj(cid:48)

(3)
where pj is the j-th column of p     rm  |v| (also referred to
as the output embedding),2 and qj is a bias term. similarly,
for a conventional id56-lm which usually takes words as
inputs, if wt = k, then the input to the id56-lm at t is
the input embedding xk, the k-th column of the embedding
matrix x     rn  |v|. our model simply replaces the input
embeddings x with the output from a character-level con-
volutional neural network, to be described below.
if we denote w1:t = [w1,       , wt ] to be the sequence of
words in the training corpus, training involves minimizing
the negative log-likelihood (n ll) of the sequence

log pr(wt|w1:t   1)

(4)

t=1

which is typically done by truncated id26
through time (werbos 1990; graves 2013).
character-level convolutional neural network
in our model, the input at time t is an output from a
character-level convolutional neural network (charid98),
which we describe in this section. id98s (lecun et al.
1989) have achieved state-of-the-art results on computer vi-
sion (krizhevsky, sutskever, and hinton 2012) and have also
been shown to be effective for various nlp tasks (collobert
et al. 2011). architectures employed for nlp applications
differ in that they typically involve temporal rather than spa-
tial convolutions.
let c be the vocabulary of characters, d be the dimen-
sionality of character embeddings,3 and q     rd  |c| be the
matrix character embeddings. suppose that word k     v is
made up of a sequence of characters [c1, . . . , cl], where l is
the length of word k. then the character-level representation
of k is given by the matrix ck     rd  l, where the j-th col-
umn corresponds to the character embedding for cj (i.e. the
cj-th column of q).4
we apply a narrow convolution between ck and a    lter
(or kernel) h     rd  w of width w, after which we add a
bias and apply a nonlinearity to obtain a feature map f k    
rl   w+1. speci   cally, the i-th element of f k is given by:

f k[i] = tanh((cid:104)ck[   , i : i + w     1], h(cid:105) + b)

(5)

still utilize id27s in the output layer.

2in our work, predictions are at the word-level, and hence we
3given that |c| is usually small, some authors work with one-
hot representations of characters. however we found that using
lower dimensional representations of characters (i.e. d < |c|) per-
formed slightly better.

4two technical details warrant mention here: (1) we append
start-of-word and end-of-word characters to each word to better
represent pre   xes and suf   xes and hence ck actually has l + 2
columns; (2) for batch processing, we zero-pad ck so that the num-
ber of columns is constant (equal to the max word length) for all
words in v.

where ck[   , i : i+w   1] is the i-to-(i+w   1)-th column of
ck and (cid:104)a, b(cid:105) = tr(abt ) is the frobenius inner product.
finally, we take the max-over-time

yk = max

i

f k[i]

(6)

as the feature corresponding to the    lter h (when applied to
word k). the idea is to capture the most important feature   
the one with the highest value   for a given    lter. a    lter is
essentially picking out a character id165, where the size of
the id165 corresponds to the    lter width.

1 , . . . , yk

we have described the process by which one feature is
obtained from one    lter matrix. our charid98 uses multiple
   lters of varying widths to obtain the feature vector for k.
so if we have a total of h    lters h1, . . . , hh, then yk =
h] is the input representation of k. for many nlp
[yk
applications h is typically chosen to be in [100, 1000].
highway network
we could simply replace xk (the id27) with yk
at each t in the id56-lm, and as we show later, this simple
model performs well on its own (table 7). one could also
have a multilayer id88 (mlp) over yk to model in-
teractions between the character id165s picked up by the
   lters, but we found that this resulted in worse performance.
instead we obtained improvements by running yk through
a highway network, recently proposed by srivastava et al.
(2015). whereas one layer of an mlp applies an af   ne trans-
formation followed by a nonlinearity to obtain a new set of
features,

z = g(wy + b)

(7)

one layer of a highway network does the following:
z = t (cid:12) g(wh y + bh ) + (1     t) (cid:12) y

(8)
where g is a nonlinearity, t =   (wt y + bt ) is called the
transform gate, and (1   t) is called the carry gate. similar to
the memory cells in id137, highway layers allow
for training of deep networks by adaptively carrying some
dimensions of the input directly to the output.5 by construc-
tion the dimensions of y and z have to match, and hence
wt and wh are square matrices.

experimental setup

as is standard in id38, we use perplexity
(p p l) to evaluate the performance of our models. perplex-
ity of a model over a sequence [w1, . . . , wt ] is given by

p p l = exp

(9)

where n ll is calculated over the test set. we test the model
on corpora of varying languages and sizes (statistics avail-
able in table 1).

we conduct hyperparameter search, model introspection,
and ablation studies on the english id32 (ptb)
(marcus, santorini, and marcinkiewicz 1993), utilizing the

5srivastava et al. (2015) recommend initializing bt to a neg-
ative value, in order to militate the initial behavior towards carry.
we initialized bt to a small interval around    2.

(cid:16) n ll

(cid:17)

t

t

data-s
|c|
51
101
74
72
76
62
132

data-l
|c|
197
195
260
222
225
111
   

|v|
10 k
46 k
37 k
27 k
25 k
62 k
86 k

|v|
t
1 m
60 k
1 m 206 k
1 m 339 k
1 m 152 k
1 m 137 k
1 m 497 k
4 m

   

20 m
17 m
51 m
56 m
57 m
25 m

english (en)
czech (cs)
german (de)
spanish (es)
french (fr)
russian (ru)
arabic (ar)
   
table 1: corpus statistics. |v| = word vocabulary size; |c| = char-
acter vocabulary size; t = number of tokens in training set. the
small english data is from the id32 and the arabic data
is from the news-commentary corpus. the rest are from the 2013
acl workshop on machine translation. |c| is large because of
(rarely occurring) special characters.

standard training (0-20), validation (21-22), and test (23-24)
splits along with pre-processing by mikolov et al. (2010).
with approximately 1m tokens and |v| = 10k, this version
has been extensively used by the id38 com-
munity and is publicly available.6

with the optimal hyperparameters tuned on ptb, we ap-
ply the model to various morphologically rich languages:
czech, german, french, spanish, russian, and arabic. non-
arabic data comes from the 2013 acl workshop on ma-
chine translation,7 and we use the same train/validation/test
splits as in botha and blunsom (2014). while the raw data
are publicly available, we obtained the preprocessed ver-
sions from the authors,8 whose morphological nlm serves
as a baseline for our work. we train on both the small
datasets (data-s) with 1m tokens per language, and the
large datasets (data-l) including the large english data
which has a much bigger |v| than the ptb. arabic data
comes from the news-commentary corpus,9 and we per-
form our own preprocessing and train/validation/test splits.
in these datasets only singleton words were replaced with
<unk> and hence we effectively use the full vocabulary. it
is worth noting that the character model can utilize surface
forms of oov tokens (which were replaced with <unk>), but
we do not do this and stick to the preprocessed versions (de-
spite disadvantaging the character models) for exact com-
parison against prior work.

optimization
the models are trained by truncated id26
through time (werbos 1990; graves 2013). we backprop-
agate for 35 time steps using stochastic id119
where the learning rate is initially set to 1.0 and halved if
the perplexity does not decrease by more than 1.0 on the
validation set after an epoch. on data-s we use a batch
size of 20 and on data-l we use a batch size of 100 (for

6http://www.   t.vutbr.cz/   imikolov/id56lm/
7http://www.statmt.org/wmt13/translation-task.html
8http://bothameister.github.io/
9http://opus.ling   l.uu.se/news-commentary.php

small

large

id98

d
w [1, 2, 3, 4, 5, 6]
h
f

15
[25    w]
tanh
1
relu
2

highway

l
g

lstm

l
m 300

15
[1, 2, 3, 4, 5, 6, 7]
[min{200, 50    w}]
tanh
2
relu
2
650

table 2: architecture of the small and large models. d =
dimensionality of character embeddings; w =    lter widths;
h = number of    lter matrices, as a function of    lter width
(so the large model has    lters of width [1, 2, 3, 4, 5, 6, 7] of
size [50, 100, 150, 200, 200, 200, 200] for a total of 1100    lters);
f, g = nonlinearity functions; l = number of layers; m = number
of hidden units.

greater ef   ciency). gradients are averaged over each batch.
we train for 25 epochs on non-arabic and 30 epochs on ara-
bic data (which was suf   cient for convergence), picking the
best performing model on the validation set. parameters of
the model are randomly initialized over a uniform distribu-
tion with support [   0.05, 0.05].

for id173 we use dropout (hinton et al. 2012)
with id203 0.5 on the lstm input-to-hidden layers
(except on the initial highway to lstm layer) and the
hidden-to-output softmax layer. we further constrain the
norm of the gradients to be below 5, so that if the l2 norm
of the gradient exceeds 5 then we renormalize it to have
||    || = 5 before updating. the gradient norm constraint
was crucial in training the model. these choices were largely
guided by previous work of zaremba et al. (2014) on word-
level id38 with lstms.

of clusters c = (cid:100)(cid:112)|v|(cid:101) and randomly split v into mutually

finally, in order to speed up training on data-l we em-
ploy a hierarchical softmax (morin and bengio 2005)   a
common strategy for training language models with very
large |v|   instead of the usual softmax. we pick the number
exclusive and collectively exhaustive subsets v1, . . . ,vc of
(approximately) equal size.10 then pr(wt+1 = j|w1:t) be-
comes,

pr(wt+1 = j|w1:t) =

  

(cid:80)c
exp(ht    sr + tr)
r(cid:48)=1 exp(ht    sr(cid:48)
+ tr(cid:48)
(cid:80)
exp(ht    pj
r + qj
r)
exp(ht    pj(cid:48)
r + qj(cid:48)
r )
j(cid:48)   vr

)

(10)

where r is the cluster index such that j     vr. the    rst term
is simply the id203 of picking cluster r, and the second

10while brown id91/frequency-based id91 is com-
monly used in the literature (e.g. botha and blunsom (2014) use
brown clusering), we used random clusters as our implementation
enjoys the best speed-up when the number of words in each clus-
ter is approximately equal. we found random id91 to work
surprisingly well.

lstm-word-small
lstm-char-small
lstm-word-large
lstm-char-large
kn-5 (mikolov et al. 2012)
id56    (mikolov et al. 2012)
id56-lda    (mikolov et al. 2012)
genid98    (wang et al. 2015)
fofe-fnnlm    (zhang et al. 2015)
deep id56 (pascanu et al. 2013)
sum-prod net    (cheng et al. 2014)
lstm-1    (zaremba et al. 2014)
lstm-2    (zaremba et al. 2014)

p p l

97.6
92.3
85.4
78.9

141.2
124.7
113.7
116.4
108.0
107.5
100.0
82.7
78.4

size
5 m
5 m
20 m
19 m
2 m
6 m
7 m
8 m
6 m
6 m
5 m
20 m
52 m

table 3: performance of our model versus other neural language
models on the english id32 test set. p p l refers to per-
plexity (lower is better) and size refers to the approximate number
of parameters in the model. kn-5 is a kneser-ney 5-gram language
model which serves as a non-neural baseline.    for these models the
authors did not explicitly state the number of parameters, and hence
sizes shown here are estimates based on our understanding of their
papers or private correspondence with the respective authors.

term is the id203 of picking word j given that cluster r
is picked. we found that hierarchical softmax was not nec-
essary for models trained on data-s.

results

english id32
we train two versions of our model to assess the trade-off
between performance and size. architecture of the small
(lstm-char-small) and large (lstm-char-large) models
is summarized in table 2. as another baseline, we also
train two comparable lstm models that use word em-
beddings only (lstm-word-small, lstm-word-large).
lstm-word-small uses 200 hidden units and lstm-word-
large uses 650 hidden units. id27 sizes are
also 200 and 650 respectively. these were chosen to keep
the number of parameters similar to the corresponding
character-level model.

as can be seen from table 3, our large model is on
par with the existing state-of-the-art (zaremba et al. 2014),
despite having approximately 60% fewer parameters. our
small model signi   cantly outperforms other nlms of sim-
ilar size, even though it is penalized by the fact that the
dataset already has oov words replaced with <unk> (other
models are purely word-level models). while lower perplex-
ities have been reported with model ensembles (mikolov and
zweig 2012), we do not include them here as they are not
comparable to the current work.

other languages
the model   s performance on the english ptb is informative
to the extent that it facilitates comparison against the large
body of existing work. however, english is relatively simple

cs
545
465

503
414
401

493
398
371

de
366
296

305
278
260

286
263
239

data-s
es
241
200

fr
274
225

212
197
182

200
177
165

229
216
189

222
196
184

ru
396
304

352
290
278

357
271
261

ar
323
   
216
230
196

172
148
148

small

botha kn-4
mlbl
word
morph
char
word
morph
char

large

table 4: test set perplexities for data-s. first two rows are from
botha (2014) (except on arabic where we trained our own kn-4
model) while the last six are from this paper. kn-4 is a kneser-
ney 4-gram language model, and mlbl is the best performing
morphological logbilinear model from botha (2014). small/large
refer to model size (see table 2), and word/morph/char are models
with words/morphemes/characters as inputs respectively.

from a morphological standpoint, and thus our next set of
results (and arguably the main contribution of this paper)
is focused on languages with richer morphology (table 4,
table 5).

we compare our results against the morphological log-
bilinear (mlbl) model from botha and blunsom (2014),
whose model also takes into account subword information
through morpheme embeddings that are summed at the input
and output layers. as comparison against the mlbl mod-
els is confounded by our use of lstms   widely known
to outperform their feed-forward/log-bilinear cousins   we
also train an lstm version of the morphological nlm,
where the input representation of a word given to the lstm
is a summation of the word   s morpheme embeddings. con-
cretely, suppose that m is the set of morphemes in a lan-
guage, m     rn  |m| is the matrix of morpheme embed-
dings, and mj is the j-th column of m (i.e. a morpheme
embedding). given the input word k, we feed the following
representation to the lstm:

xk +

mj

(11)

(cid:88)

j   mk

where xk is the id27 (as in a word-level model)
and mk     m is the set of morphemes for word k. the
morphemes are obtained by running an unsupervised mor-
phological tagger as a preprocessing step.11 we emphasize
that the id27 itself (i.e. xk) is added on top of the
morpheme embeddings, as was done in botha and blunsom
(2014). the morpheme embeddings are of size 200/650 for
the small/large models respectively. we further train word-
level lstm models as another baseline.

on data-s it is clear from table 4 that the character-
level models outperform their word-level counterparts de-

11we use morfessor cat-map (creutz and lagus 2007), as in

botha and blunsom (2014).

cs
862
643

701
615
578

de
463
404

347
331
305

data-l
es
219
203

fr
243
227

186
189
169

202
209
190

ru
390
300

353
331
313

en
291
273

236
233
216

botha kn-4
mlbl
word
morph
char

small

table 5: test set perplexities on data-l. first two rows are from
botha (2014), while the last three rows are from the small lstm
models described in the paper. kn-4 is a kneser-ney 4-gram lan-
guage model, and mlbl is the best performing morphological log-
bilinear model from botha (2014). word/morph/char are models
with words/morphemes/characters as inputs respectively.

spite, again, being smaller.12 the character models also out-
perform their morphological counterparts (both mlbl and
lstm architectures), although improvements over the mor-
phological lstms are more measured. note that the mor-
pheme models have strictly more parameters than the word
models because id27s are used as part of the in-
put.

due to memory constraints13 we only train the small
models on data-l (table 5). interestingly we do not ob-
serve signi   cant differences going from word to morpheme
lstms on spanish, french, and english. the character
models again outperform the word/morpheme models. we
also observe signi   cant perplexity reductions even on en-
glish when v is large. we conclude this section by noting
that we used the same architecture for all languages and did
not perform any language-speci   c tuning of hyperparame-
ters.

discussion
learned word representations
we explore the word representations learned by the models
on the ptb. table 6 has the nearest neighbors of word rep-
resentations learned from both the word-level and character-
level models. for the character models we compare the rep-
resentations obtained before and after highway layers.

before the highway layers the representations seem to
solely rely on surface forms   for example the nearest neigh-
bors of you are your, young, four, youth, which are close to
you in terms of id153. the highway layers however,
seem to enable encoding of semantic features that are not
discernable from orthography alone. after highway layers
the nearest neighbor of you is we, which is orthographically
distinct from you. another example is while and though   
these words are far apart id153-wise yet the composi-
tion model is able to place them near each other. the model

12the difference in parameters is greater for non-ptb corpora
as the size of the word model scales faster with |v|. for example,
on arabic the small/large word models have 35m/121m parameters
while the corresponding character models have 29m/69m parame-
ters respectively.

13all models were trained on gpus with 2gb memory.

figure 2: plot of character id165 representations via pca for
english. colors correspond to: pre   xes (red), suf   xes (blue), hy-
phenated (orange), and all others (grey). pre   xes refer to character
id165s which start with the start-of-word character. suf   xes like-
wise refer to character id165s which end with the end-of-word
character.

also makes some clear mistakes (e.g. his and hhs), highlight-
ing the limits of our approach, although this could be due to
the small dataset.

the learned representations of oov words (computer-
aided, misinformed) are positioned near words with the
same part-of-speech. the model is also able to correct for
incorrect/non-standard spelling (looooook), indicating po-
tential applications for text id172 in noisy domains.
learned character id165 representations
as discussed previously, each    lter of the charid98 is es-
sentially learning to detect particular character id165s. our
initial expectation was that each    lter would learn to activate
on different morphemes and then build up semantic repre-
sentations of words from the identi   ed morphemes. how-
ever, upon reviewing the character id165s picked up by
the    lters (i.e. those that maximized the value of the    lter),
we found that they did not (in general) correspond to valid
morphemes.

to get a better intuition for what the character composi-
tion model is learning, we plot the learned representations
of all character id165s (that occurred as part of at least two
words in v) via principal components analysis (figure 2).
we feed each character id165 into the charid98 and use
the charid98   s output as the    xed dimensional representa-
tion for the corresponding character id165. as is appar-
ent from figure 2, the model learns to differentiate between
pre   xes (red), suf   xes (blue), and others (grey). we also    nd
that the representations are particularly sensitive to character
id165s containing hyphens (orange), presumably because
this is a strong signal of a word   s part-of-speech.
highway layers
we quantitatively investigate the effect of highway network
layers via ablation studies (table 7). we train a model with-
out any highway layers, and    nd that performance decreases
signi   cantly. as the difference in performance could be
due to the decrease in model size, we also train a model
that feeds yk (i.e. word representation from the charid98)

while

although
letting
though
minute

chile
whole

meanwhile

white

meanwhile

whole
though

nevertheless

in vocabulary

you

conservatives

we
guys

i

your
young
four
youth

we
your
doug

i

richard
jonathan
robert
neil
nancy

trading

advertised
advertising
turnover
turnover

hard
rich
richer
richter

eduard
gerard
edward

carl

heading
training
reading
leading

trade
training
traded
trader

out-of-vocabulary
misinformed

computer-aided

looooook

   
   
   
   

computer-guided

computerized

disk-drive
computer

computer-guided
computer-driven

computerized

computer

   
   
   
   

informed
performed
transformed

inform

informed
performed

outperformed
transformed

   
   
   
   

look
cook
looks
shook

look
looks
looked
looking

his
your
her
my
their

this
hhs
is
has

hhs
this
their
your

lstm-word

lstm-char

(before highway)

lstm-char

(after highway)

table 6: nearest neighbor words (based on cosine similarity) of word representations from the large word-level and character-level (before
and after highway layers) models trained on the ptb. last three words are oov words, and therefore they do not have representations in the
word-level model.

no highway layers
one highway layer
two highway layers
one mlp layer

lstm-char
small large
84.6
100.3
79.7
92.3
78.9
90.1
111.2
92.6

t

|v|

25 k

10 k

50 k
1 m 17% 16% 21%
5 m
10 m
25 m

8% 14% 16% 21%
9% 12% 15%
9%
9%
8%
9% 10%

100 k

   

table 7: perplexity on the id32 for small/large models
trained with/without highway layers.

through a one-layer multilayer id88 (mlp) to use as
input into the lstm. we    nd that the mlp does poorly, al-
though this could be due to optimization issues.

we hypothesize that id199 are especially
well-suited to work with id98s, adaptively combining lo-
cal features detected by the individual    lters. id98s have
already proven to be been successful for many nlp tasks
(collobert et al. 2011; shen et al. 2014; kalchbrenner,
grefenstette, and blunsom 2014; kim 2014; zhang, zhao,
and lecun 2015; lei, barzilay, and jaakola 2015), and we
posit that further gains could be achieved by employing
highway layers on top of existing id98 architectures.

we also anecdotally note that (1) having one to two high-
way layers was important, but more highway layers gener-
ally resulted in similar performance (though this may de-
pend on the size of the datasets), (2) having more convolu-
tional layers before max-pooling did not help, and (3) high-
way layers did not improve models that only used word em-
beddings as inputs.

effect of corpus/vocab sizes
we next study the effect of training corpus/vocabulary sizes
on the relative performance between the different models.
we take the german (de) dataset from data-l and vary the
training corpus/vocabulary sizes, calculating the perplex-

table 8: perplexity reductions by going from small word-level to
character-level models based on different corpus/vocabulary sizes
on german (de). |v| is the vocabulary size and t is the number
of tokens in the training set. the full vocabulary of the 1m dataset
was less than 100k and hence that scenario is unavailable.

ity reductions as a result of going from a small word-level
model to a small character-level model. to vary the vocabu-
lary size we take the most frequent k words and replace the
rest with <unk>. as with previous experiments the character
model does not utilize surface forms of <unk> and simply
treats it as another token. although table 8 suggests that the
perplexity reductions become less pronounced as the corpus
size increases, we nonetheless    nd that the character-level
model outperforms the word-level model in all scenarios.

further observations
we report on some further experiments and observations:
    combining id27s with the charid98   s out-
put to form a combined representation of a word (to be
used as input to the lstm) resulted in slightly worse
performance (81 on ptb with a large model). this was
surprising, as improvements have been reported on part-
of-speech tagging (dos santos and zadrozny 2014) and
id39 (dos santos and guimaraes
2015) by concatenating id27s with the out-
put from a character-level id98. while this could be due

to insuf   cient experimentation on our part,14 it suggests
that for some tasks, id27s are super   uous   
character inputs are good enough.
    while our model requires additional convolution opera-
tions over characters and is thus slower than a comparable
word-level model which can perform a simple lookup at
the input layer, we found that the difference was manage-
able with optimized gpu implementations   for example
on ptb the large character-level model trained at 1500 to-
kens/sec compared to the word-level model which trained
at 3000 tokens/sec. for scoring, our model can have the
same running time as a pure word-level model, as the
charid98   s outputs can be pre-computed for all words in
v. this would, however, be at the expense of increased
model size, and thus a trade-off can be made between
run-time speed and memory (e.g. one could restrict the
pre-computation to the most frequent words).

related work

neural language models (nlm) encompass a rich fam-
ily of neural network architectures for id38.
some example architectures include feed-forward (bengio,
ducharme, and vincent 2003), recurrent (mikolov et al.
2010), sum-product (cheng et al. 2014), log-bilinear (mnih
and hinton 2007), and convolutional (wang et al. 2015) net-
works.

in order to address the rare word problem, alexandrescu
and kirchhoff (2006)   building on analogous work on
count-based id165 language models by bilmes and kirch-
hoff (2003)   represent a word as a set of shared factor em-
beddings. their factored neural language model (fnlm)
can incorporate morphemes, word shape information (e.g.
capitalization) or any other annotation (e.g. part-of-speech
tags) to represent words.

a speci   c class of fnlms leverages morphemic infor-
mation by viewing a word as a function of its (learned)
morpheme embeddings (luong, socher, and manning 2013;
botha and blunsom 2014; qui et al. 2014). for example lu-
ong, socher, and manning (2013) apply a recursive neural
network over morpheme embeddings to obtain the embed-
ding for a single word. while such models have proved use-
ful, they require morphological tagging as a preprocessing
step.

another direction of work has involved purely character-
level nlms, wherein both input and output are charac-
ters (sutskever, martens, and hinton 2011; graves 2013).
character-level models obviate the need for morphological
tagging or manual feature engineering, and have the attrac-
tive property of being able to generate novel words. how-
ever they are generally outperformed by word-level models
(mikolov et al. 2012).

outside of

id38,

improvements have
been reported on part-of-speech tagging (dos santos and
zadrozny 2014) and id39 (dos santos

14we experimented with (1) concatenation, (2) tensor products,
(3) averaging, and (4) adaptive weighting schemes whereby the
model learns a convex combination of id27s and the
charid98 outputs.

and guimaraes 2015) by representing a word as a concatena-
tion of its id27 and an output from a character-
level id98, and using the combined representation as fea-
tures in a conditional random field (crf). zhang, zhao,
and lecun (2015) do away with id27s com-
pletely and show that for text classi   cation, a deep id98
over characters performs well. ballesteros, dyer, and smith
(2015) use an id56 over characters only to train a transition-
based parser, obtaining improvements on many morpholog-
ically rich languages.

finally, ling et al. (2015) apply a bi-directional lstm
over characters to use as inputs for id38 and
part-of-speech tagging. they show improvements on various
languages (english, portuguese, catalan, german, turkish).
it remains open as to which character composition model
(i.e. id98 or lstm) performs better.

conclusion

we have introduced a neural language model that utilizes
only character-level inputs. predictions are still made at the
word-level. despite having fewer parameters, our model
outperforms baseline models that utilize word/morpheme
embeddings in the input layer. our work questions the ne-
cessity of id27s (as inputs) for neural language
modeling.

analysis of word representations obtained from the char-
acter composition part of the model further indicates that
the model is able to encode, from characters only, rich se-
mantic and orthographic features. using the charid98 and
highway layers for representation learning (e.g. as input into
id97 (mikolov et al. 2013)) remains an avenue for fu-
ture work.

insofar as sequential processing of words as inputs is
ubiquitous in natural language processing, it would be in-
teresting to see if the architecture introduced in this paper is
viable for other tasks   for example, as an encoder/decoder
in id4 (cho et al. 2014; sutskever,
vinyals, and le 2014).

acknowledgments

we are especially grateful to jan botha for providing the
preprocessed datasets and the model results.

references

2015.

alexandrescu, a., and kirchhoff, k. 2006. factored neural lan-
guage models. in proceedings of naacl.
ballesteros, m.; dyer, c.; and smith, n. a.
im-
proved transition-based parsing by modeling characters instead
of words with lstms. in proceedings of emnlp.
bengio, y.; ducharme, r.; and vincent, p. 2003. a neural prob-
abilistic language model. journal of machine learning research
3:1137   1155.
bengio, y.; simard, p.; and frasconi, p. 1994. learning long-term
dependencies with id119 is dif   cult. ieee transac-
tions on neural networks 5:157   166.
bilmes, j., and kirchhoff, k. 2003. factored language models
and generalized parallel backoff. in proceedings of naacl.

marcus, m.; santorini, b.; and marcinkiewicz, m. 1993. building
a large annotated corpus of english: the id32. compu-
tational linguistics 19:331   330.
mikolov, t., and zweig, g. 2012. context dependent recurrent
neural network language model. in proceedings of slt.
mikolov, t.; kara   at, m.; burget, l.; cernocky, j.; and khudanpur,
s. 2010. recurrent neural network based language model. in
proceedings of interspeech.
mikolov, t.; deoras, a.; kombrink, s.; burget, l.; and cernocky,
j. 2011. empirical evaluation and combination of advanced lan-
guage modeling techniques. in proceedings of interspeech.
mikolov, t.; sutskever, i.; deoras, a.; le, h.-s.; kombrink, s.;
and cernocky, j. 2012. subword id38 with neural
networks. preprint: www.   t.vutbr.cz/  imikolov/id56lm/char.pdf.
mikolov, t.; chen, k.; corrado, g.; and dean, j. 2013. ef-
   cient estimation of word representations in vector space.
arxiv:1301.3781.
mnih, a., and hinton, g. 2007. three new id114 for
statistical language modelling. in proceedings of icml.
morin, f., and bengio, y. 2005. hierarchical probabilistic neural
network language model. in proceedings of aistats.
pascanu, r.; culcehre, c.; cho, k.; and bengio, y. 2013. how to
construct deep neural networks. arxiv:1312.6026.
qui, s.; cui, q.; bian, j.; and gao, b. 2014. co-learning of word
representations and morpheme representations. in proceedings
of coling.
shen, y.; he, x.; gao, j.; deng, l.; and mesnil, g. 2014. a latent
semantic model with convolutional-pooling structure for infor-
mation retrieval. in proceedings of cikm.
srivastava, r. k.; greff, k.; and schmidhuber, j. 2015. training
very deep networks. arxiv:1507.06228.
sundermeyer, m.; schluter, r.; and ney, h. 2012. lstm neural
networks for id38.
sutskever, i.; martens, j.; and hinton, g. 2011. generating text
with recurrent neural networks.
sutskever, i.; vinyals, o.; and le, q. 2014. sequence to sequence
learning with neural networks.
wang, m.; lu, z.; li, h.; jiang, w.; and liu, q. 2015. genid98:
a convolutional architecture for word sequence prediction.
in
proceedings of acl.
werbos, p. 1990. back-propagation through time: what it does
and how to do it. in proceedings of ieee.
zaremba, w.; sutskever, i.; and vinyals, o. 2014. recurrent neural
network id173. arxiv:1409.2329.
zhang, s.; jiang, h.; xu, m.; hou, j.; and dai, l. 2015. the fixed-
size ordinally-forgetting encoding method for neural network
language models. in proceedings of acl.
zhang, x.; zhao, j.; and lecun, y. 2015. character-level convo-
lutional networks for text classi   cation. in proceedings of nips.

botha, j., and blunsom, p. 2014. compositional morphology for
word representations and language modelling. in proceedings
of icml.
botha, j. 2014. probabilistic modelling of morphologically rich
languages. dphil dissertation, oxford university.
chen, s., and goodman, j. 1998. an empirical study of smooth-
ing techniques for id38. technical report, har-
vard university.
cheng, w. c.; kok, s.; pham, h. v.; chieu, h. l.; and chai, k. m.
2014. id38 with sum-product networks. in pro-
ceedings of interspeech.
cho, k.; van merrienboer, b.; gulcehre, c.; bahdanau, d.;
bougares, f.; schwenk, h.; and bengio, y. 2014. learning phrase
representations using id56 encoder-decoder for statistical ma-
chine translation. in proceedings of emnlp.
collobert, r.; weston, j.; bottou, l.; karlen, m.; kavukcuoglu, k.;
and kuksa, p. 2011. natural language processing (almost) from
scratch. journal of machine learning research 12:2493   2537.
creutz, m., and lagus, k. 2007. unsupervised models for mor-
pheme segmentation and morphology learning. in proceedings of
the acm transations on speech and language processing.
deerwester, s.; dumais, s.; and harshman, r. 1990. indexing by
latent semantic analysis. journal of american society of infor-
mation science 41:391   407.
dos santos, c. n., and guimaraes, v. 2015. boosting named entity
recognition with neural character embeddings. in proceedings of
acl named entities workshop.
dos santos, c. n., and zadrozny, b. 2014. learning character-
level representations for part-of-speech tagging. in proceedings
of icml.
graves, a. 2013. generating sequences with recurrent neural
networks. arxiv:1308.0850.
hinton, g.; srivastava, n.; krizhevsky, a.; sutskever, i.; and
salakhutdinov, r. 2012. improving neural networks by prevent-
ing co-adaptation of feature detectors. arxiv:1207.0580.
hochreiter, s., and schmidhuber, j. 1997. long short-term mem-
ory. neural computation 9:1735   1780.
kalchbrenner, n.; grefenstette, e.; and blunsom, p. 2014. a con-
volutional neural network for modelling sentences. in proceed-
ings of acl.
kim, y. 2014. convolutional neural networks for sentence clas-
si   cation. in proceedings of emnlp.
krizhevsky, a.; sutskever, i.; and hinton, g. 2012.
id163
classi   cation with deep convolutional neural networks. in pro-
ceedings of nips.
lecun, y.; boser, b.; denker, j. s.; henderson, d.; howard, r. e.;
hubbard, w.; and jackel, l. d. 1989. handwritten digit recogni-
tion with a id26 network. in proceedings of nips.
lei, t.; barzilay, r.; and jaakola, t. 2015. molding id98s for
text: non-linear, non-consecutive convolutions. in proceedings
of emnlp.
ling, w.; lui, t.; marujo, l.; astudillo, r. f.; amir, s.; dyer, c.;
black, a. w.; and trancoso, i. 2015. finding function in form:
compositional character models for open vocabulary word rep-
resentation. in proceedings of emnlp.
luong, m.-t.; socher, r.; and manning, c. 2013. better word
representations with id56s for morphology.
in proceedings of conll.

