recurrent neural network grammars

chris dyer    adhiguna kuncoro    miguel ballesteros       noah a. smith   
   school of computer science, carnegie mellon university, pittsburgh, pa, usa
   computer science & engineering, university of washington, seattle, wa, usa

   nlp group, pompeu fabra university, barcelona, spain

{cdyer,akuncoro}@cs.cmu.edu, miguel.ballesteros@upf.edu, nasmith@cs.washington.edu

6
1
0
2

 
t
c
o
2
1

 

 
 
]
l
c
.
s
c
[
 
 

4
v
6
7
7
7
0

.

2
0
6
1
:
v
i
x
r
a

this is modi   ed version of a paper originally
published at naacl 2016 that contains a cor-
rigendum at the end, with improved results af-
ter    xing an implementation bug in the id56g
composition function.

abstract

we introduce recurrent neural network gram-
mars, probabilistic models of sentences with
explicit phrase structure. we explain ef   cient
id136 procedures that allow application to
both parsing and id38. experi-
ments show that they provide better parsing in
english than any single previously published
supervised generative model and better lan-
guage modeling than state-of-the-art sequen-
tial id56s in english and chinese1.

introduction

1
sequential recurrent neural networks (id56s) are
remarkably effective models of natural language.
in the last few years, language model results that
substantially improve over long-established state-of-
the-art baselines have been obtained using id56s
(zaremba et al., 2015; mikolov et al., 2010) as well
as in various conditional id38 tasks
such as machine translation (bahdanau et al., 2015),
image id134 (xu et al., 2015), and dia-
logue generation (wen et al., 2015). despite these
impressive results, sequential models are a priori
inappropriate models of natural language, since re-
lationships among words are largely organized in
terms of latent nested structures rather than sequen-
tial surface order (chomsky, 1957).
in this paper, we introduce recurrent neural net-
work grammars (id56gs;   2), a new generative
1the code to reproduce our results after the bug    x is pub-

licly available at https://github.com/clab/id56g.

probabilistic model of sentences that explicitly mod-
els nested, hierarchical relationships among words
and phrases. id56gs operate via a recursive syntac-
tic process reminiscent of probabilistic context-free
grammar generation, but decisions are parameter-
ized using id56s that condition on the entire syntac-
tic derivation history, greatly relaxing context-free
independence assumptions.
the foundation of this work is a top-down vari-
ant of transition-based parsing (  3). we give two
variants of the algorithm, one for parsing (given an
observed sentence, transform it into a tree), and one
for generation. while several transition-based neu-
ral models of syntactic generation exist (hender-
son, 2003, 2004; emami and jelinek, 2005; titov
and henderson, 2007; buys and blunsom, 2015b),
these have relied on structure building operations
based on parsing actions in shift-reduce and left-
corner parsers which operate in a largely bottom-
up fashion. while this construction is appealing be-
cause id136 is relatively straightforward, it lim-
its the use of top-down grammar information, which
is helpful for generation (roark, 2001).2 id56gs
maintain the algorithmic convenience of transition-
based parsing but incorporate top-down (i.e., root-
to-terminal) syntactic information (  4).

the top-down transition set

that id56gs are
based on lends itself to discriminative modeling as
well, where sequences of transitions are modeled
conditional on the full input sentence along with the
incrementally constructed syntactic structures. sim-
ilar to previously published discriminative bottom-
up transition-based parsers (henderson, 2004; sagae
and lavie, 2005; zhang and clark, 2011, inter alia),
greedy prediction with our model yields a linear-

2the left-corner parsers used by henderson (2003, 2004)
incorporate limited top-down information, but a complete path
from the root of the tree to a terminal is not generally present
when a terminal is generated. refer to henderson (2003, fig.
1) for an example.

time deterministic parser (provided an upper bound
on the number of actions taken between process-
ing subsequent terminal symbols is imposed); how-
ever, our algorithm generates arbitrary tree struc-
tures directly, without the binarization required by
shift-reduce parsers. the discriminative model also
lets us use ancestor sampling to obtain samples of
parse trees for sentences, and this is used to solve
a second practical challenge with id56gs: approx-
imating the marginal likelihood and map tree of a
sentence under the generative model. we present a
simple importance sampling algorithm which uses
samples from the discriminative parser to solve in-
ference problems in the generative model (  5).
experiments show that id56gs are effective for
both id38 and parsing (  6). our gen-
erative model obtains (i) the best-known parsing re-
sults using a single supervised generative model and
(ii) better perplexities in id38 than
state-of-the-art sequential lstm language models.
surprisingly   although in line with previous pars-
ing results showing the effectiveness of genera-
tive models (henderson, 2004; johnson, 2001)   
parsing with the generative model obtains signi   -
cantly better results than parsing with the discrim-
inative model.

2 id56 grammars

formally, an id56g is a triple (n,   ,   ) consisting
of a    nite set of nonterminal symbols (n), a    nite
set of terminal symbols (  ) such that n        =    ,
and a collection of neural network parameters   . it
does not explicitly de   ne rules since these are im-
plicitly characterized by   . the algorithm that the
grammar uses to generate trees and strings in the lan-
guage is characterized in terms of a transition-based
algorithm, which is outlined in the next section. in
the section after that, the semantics of the param-
eters that are used to turn this into a stochastic al-
gorithm that generates pairs of trees and strings are
discussed.

3 top-down parsing and generation

id56gs are based on a top-down generation algo-
rithm that relies on a stack data structure of par-
tially completed syntactic constituents. to empha-
size the similarity of our algorithm to more familiar

bottom-up shift-reduce recognition algorithms, we
   rst present the parsing (rather than generation) ver-
sion of our algorithm (  3.1) and then present modi-
   cations to turn it into a generator (  3.2).
3.1 parser transitions
the parsing algorithm transforms a sequence of
words x into a parse tree y using two data structures
(a stack and an input buffer). as with the bottom-
up algorithm of sagae and lavie (2005), our algo-
rithm begins with the stack (s) empty and the com-
plete sequence of words in the input buffer (b). the
buffer contains unprocessed terminal symbols, and
the stack contains terminal symbols,    open    nonter-
minal symbols, and completed constituents. at each
timestep, one of the following three classes of op-
erations (fig. 1) is selected by a classi   er, based on
the current contents on the stack and buffer:
    nt(x) introduces an    open nonterminal    x onto
the top of the stack. open nonterminals are
written as a nonterminal symbol preceded by an
open parenthesis, e.g.,    (vp   , and they represent
a nonterminal whose child nodes have not yet
been fully constructed. open nonterminals are
   closed    to form complete constituents by subse-
quent reduce operations.
    shift removes the terminal symbol x from the
front of the input buffer, and pushes it onto the
top of the stack.
    reduce repeatedly pops completed subtrees or
terminal symbols from the stack until an open
nonterminal is encountered, and then this open
nt is popped and used as the label of a new con-
stituent that has the popped subtrees as its chil-
dren. this new completed constituent is pushed
onto the stack as a single composite item. a single
reduce operation can thus create constituents
with an unbounded number of children.

the parsing algorithm terminates when there is a
single completed constituent on the stack and the
buffer is empty. fig. 2 shows an example parse
using our transition set. note that in this paper
we do not model preterminal symbols (i.e., part-of-
speech tags) and our examples therefore do not in-
clude them.3

3preterminal symbols are, from the parsing algorithm   s
point of view, just another kind of nonterminal symbol that re-

our transition set is closely related to the op-
erations used in earley   s algorithm which likewise
introduces nonterminals symbols with its predict
operation and later completes them after consum-
ing terminal symbols one at a time using scan
(earley, 1970). it is likewise closely related to the
   linearized    parse trees proposed by vinyals et al.
(2015) and to the top-down, left-to-right decompo-
sitions of trees used in previous generative parsing
and id38 work (roark, 2001, 2004;
charniak, 2010).
a further connection is to ll(   ) parsing which
uses an unbounded lookahead (compactly repre-
sented by a dfa) to distinguish between parse alter-
natives in a top-down parser (parr and fisher, 2011);
however, our parser uses an id56 encoding of the
lookahead rather than a dfa.

constraints on parser transitions. to guarantee
that only well-formed phrase-structure trees are pro-
duced by the parser, we impose the following con-
straints on the transitions that can be applied at each
step which are a function of the parser state (b, s, n)
where n is the number of open nonterminals on the
stack:
    the nt(x) operation can only be applied if b is

not empty and n < 100.4

    the shift operation can only be applied if b is

not empty and n     1.

    the reduce operation can only be applied if the
top of the stack is not an open nonterminal sym-
bol.

    the reduce operation can only be applied if n    

2 or if the buffer is empty.

to designate the set of valid parser transitions, we
write ad(b, s, n).
quires no special handling. however, leaving them out reduces
the number of transitions by o(n) and also reduces the number
of action types, both of which reduce the runtime. furthermore,
standard parsing evaluation scores do not depend on preterminal
prediction accuracy.

4since our parser allows unary nonterminal productions,
there are an in   nite number of valid trees for    nite-length sen-
tences. the n < 100 constraint prevents the classi   er from
misbehaving and generating excessively large numbers of non-
terminals. similar constraints have been proposed to deal with
the analogous problem in bottom-up shift-reduce parsers (sagae
and lavie, 2005).

3.2 generator transitions
the parsing algorithm that maps from sequences
of words to parse trees can be adapted with mi-
nor changes to produce an algorithm that stochas-
tically generates trees and terminal symbols. two
changes are required: (i) there is no input buffer of
unprocessed words, rather there is an output buffer
(t ), and (ii) instead of a shift operation there are
gen(x) operations which generate terminal symbol
x        and add it to the top of the stack and the out-
put buffer. at each timestep an action is stochasti-
cally selected according to a conditional distribution
that depends on the current contents of s and t . the
algorithm terminates when a single completed con-
stituent remains on the stack. fig. 4 shows an exam-
ple generation sequence.
constraints on generator transitions. the gen-
eration algorithm also requires slightly modi   ed
constraints. these are:
    the gen(x) operation can only be applied if n    
    the reduce operation can only be applied if the
top of the stack is not an open nonterminal symbol
and n     1.

1.

to designate the set of valid generator transitions,
we write ag(t, s, n).

this transition set generates trees using nearly the
same structure building actions and stack con   gura-
tions as the    top-down pda    construction proposed
by abney et al. (1999), albeit without the restriction
that the trees be in chomsky normal form.

3.3 transition sequences from trees
any parse tree can be converted to a sequence of
transitions via a depth-   rst, left-to-right traversal of
a parse tree. since there is a unique depth-   rst, left-
ro-right traversal of a tree, there is exactly one tran-
sition sequence of each tree. for a tree y and a
sequence of symbols x, we write a(x, y) to indi-
cate the corresponding sequence of generation tran-
sitions, and b(x, y) to indicate the parser transitions.

3.4 runtime analysis
a detailed analysis of the algorithmic properties of
our top-down parser is beyond the scope of this pa-
per; however, we brie   y state several facts. as-

stackt
buffert open ntst action
buffert+1 open ntst+1
nt(x)
s
b
b
x | b
shift
s
b
s | (x |   1 | . . . |   (cid:96) b
reduce s | (x   1 . . .   (cid:96)) b
figure 1: parser transitions showing the stack, buffer, and open nonterminal count before and after each action type. s represents
the stack, which contains open nonterminals and completed subtrees; b represents the buffer of unprocessed terminal symbols; x
is a terminal symbol, x is a nonterminal symbol, and each    is a completed subtree. the top of the stack is to the right, and the
buffer is consumed from left to right. elements on the stack and buffer are delimited by a vertical bar (| ).

stackt+1
s | (x
s | x

n + 1
n
n     1

n
n
n

input: the hungry cat meows .

stack

0

1

2

3

4

5

6

7

8

9

10

11

(s
(s| (np
(s| (np| the
(s| (np| the| hungry
(s| (np| the| hungry| cat
(s| (np the hungry cat)
(s| (np the hungry cat)| (vp
(s| (np the hungry cat)| (vp meows
(s| (np the hungry cat)| (vp meows)
(s| (np the hungry cat)| (vp meows)| .
(s (np the hungry cat) (vp meows) .)

action
buffer
the| hungry| cat| meows| . nt(s)
the| hungry| cat| meows| . nt(np)
the| hungry| cat| meows| .
shift
hungry| cat| meows| .
shift
cat| meows| .
shift
meows| .
reduce
meows| .
nt(vp)
meows| .
shift
.
reduce
.
shift
reduce

figure 2: top-down parsing example.

stackt
termst open ntst action
termst+1 open ntst+1
nt(x)
s
t
t
t | x
gen(x)
s
t
s | (x |   1 | . . . |   (cid:96) t
reduce s | (x   1 . . .   (cid:96)) t
figure 3: generator transitions. symbols de   ned as in fig. 1 with the addition of t representing the history of generated terminals.

stackt+1
s | (x
s | x

n + 1
n
n     1

n
n
n

stack

terminals

0

1

2

3

4

5

6

7

8

9

10

11

(s
(s| (np
(s| (np| the
the
the| hungry
(s| (np| the| hungry
the| hungry| cat
(s| (np| the| hungry| cat
the| hungry| cat
(s| (np the hungry cat)
the| hungry| cat
(s| (np the hungry cat)| (vp
the| hungry| cat| meows
(s| (np the hungry cat)| (vp meows
the| hungry| cat| meows
(s| (np the hungry cat)| (vp meows)
(s| (np the hungry cat)| (vp meows)| . the| hungry| cat| meows| .
the| hungry| cat| meows| .
(s (np the hungry cat) (vp meows) .)

figure 4: joint generation of a parse tree and sentence.

action
nt(s)
nt(np)
gen(the)
gen(hungry)
gen(cat)
reduce
nt(vp)
gen(meows)
reduce
gen(.)
reduce

history of actions (a<t) taken by the generator, rep-
resented by an embedding ht,

ut = tanh (w[ot; st; ht] + c) ,

where w and c are parameters. refer to figure 5
for an illustration of the architecture.

the output buffer, stack, and history are se-
quences that grow unboundedly, and to obtain rep-
resentations of them we use recurrent neural net-
works to    encode    their contents (cho et al., 2014).
since the output buffer and history of actions are
only appended to and only contain symbols from a
   nite alphabet, it is straightforward to apply a stan-
dard id56 encoding architecture. the stack (s) is
more complicated for two reasons. first, the ele-
ments of the stack are more complicated objects than
symbols from a discrete alphabet: open nontermi-
nals, terminals, and full trees, are all present on the
stack. second, it is manipulated using both push and
pop operations. to ef   ciently obtain representations
of s under push and pop operations, we use stack
lstms (dyer et al., 2015). to represent complex
parse trees, we de   ne a new syntactic composition
function that recursively de   nes representations of
trees.

4.1 syntactic composition function
when a reduce operation is executed, the parser
pops a sequence of completed subtrees and/or to-
kens (together with their vector embeddings) from
the stack and makes them children of the most recent
open nonterminal on the stack,    completing    the
constituent. to compute an embedding of this new
subtree, we use a composition function based on
bidirectional lstms, which is illustrated in fig. 6.

suming the availability of constant time push and
pop operations, the runtime is linear in the number
of the nodes in the parse tree that is generated by
the parser/generator (intuitively, this is true since al-
though an individual reduce operation may require
applying a number of pops that is linear in the num-
ber of input symbols, the total number of pop opera-
tions across an entire parse/generation run will also
be linear). since there is no way to bound the num-
ber of output nodes in a parse tree as a function of
the number of input words, stating the runtime com-
plexity of the parsing algorithm as a function of the
input size requires further assumptions. assuming
our    xed constraint on maximum depth, it is linear.

3.5 comparison to other models
our generation algorithm algorithm differs from
previous stack-based parsing/generation algorithms
in two ways. first, it constructs rooted tree struc-
tures top down (rather than bottom up), and sec-
ond, the transition operators are capable of directly
generating arbitrary tree structures rather than, e.g.,
assuming binarized trees, as is the case in much
prior work that has used transition-based algorithms
to produce phrase-structure trees (sagae and lavie,
2005; zhang and clark, 2011; zhu et al., 2013).
4 generative model
id56gs use the generator transition set just pre-
sented to de   ne a joint distribution on syntax trees
(y) and words (x). this distribution is de   ned as a
sequence model over generator transitions that is pa-
rameterized using a continuous space embedding of
the algorithm state at each time step (ut); i.e.,

p(at | a<t)

p(x, y) =

=

|a(x,y)|

|a(x,y)|

(cid:89)t=1
(cid:89)t=1

exp r(cid:62)

atut + bat

(cid:80)a(cid:48)   ag(tt,st,nt) exp r(cid:62)

,

a(cid:48)ut + ba(cid:48)

and where action-speci   c embeddings ra and bias
vector b are parameters in   .

the representation of the algorithm state at time
t, ut, is computed by combining the representation
of the generator   s three data structures: the output
buffer (tt), represented by an embedding ot, the
stack (st), represented by an embedding st, and the

figure 6: syntactic composition function based on bidirec-
tional lstms that is executed during a reduce operation; the
network on the right models the structure on the left.

npuvwnpuvwnpxxfigure 5: neural architecture for de   ning a distribution over at given representations of the stack (st), output buffer (tt) and
history of actions (a<t). details of the composition architecture of the np, the action history lstm, and the other elements of the
stack are not shown. this architecture corresponds to the generator state at line 7 of figure 4.

the    rst vector read by the lstm in both the for-
ward and reverse directions is an embedding of the
label on the constituent being constructed (in the    g-
ure, np). this is followed by the embeddings of the
child subtrees (or tokens) in forward or reverse or-
der.
intuitively, this order serves to    notify    each
lstm what sort of head it should be looking for as it
processes the child node embeddings. the    nal state
of the forward and reverse lstms are concatenated,
passed through an af   ne transformation and a tanh
nonlinearity to become the subtree embedding.5 be-
cause each of the child node embeddings (u, v, w in
fig. 6) is computed similarly (if it corresponds to an
internal node), this composition function is a kind of
id56.

4.2 word generation
to reduce the size of ag(s, t, n), word genera-
tion is broken into two parts. first, the decision to
generate is made (by predicting gen as an action),
and then choosing the word, conditional on the cur-
rent parser state. to further reduce the computa-
tional complexity of modeling the generation of a
word, we use a class-factored softmax (baltescu and
classes for a vocabulary of size |  |, this prediction

blunsom, 2015; goodman, 2001). by using(cid:112)|  |

5we found the many previously proposed syntactic compo-
sition functions inadequate for our purposes. first, we must
contend with an unbounded number of children, and many
previously proposed functions are limited to binary branching
nodes (socher et al., 2013b; dyer et al., 2015). second, those
that could deal with n-ary nodes made poor use of nonterminal
information (tai et al., 2015), which is crucial for our task.

step runs in time o((cid:112)|  |) rather than the o(|  |) of

the full-vocabulary softmax. to obtain clusters, we
use the greedy agglomerative id91 algorithm
of brown et al. (1992).

4.3 training
the parameters in the model are learned to maxi-
mize the likelihood of a corpus of trees.

4.4 discriminative parsing model
a discriminative parsing model can be obtained by
replacing the embedding of tt at each time step with
an embedding of the input buffer bt. to train this
model, the conditional likelihood of each sequence
of actions given the input string is maximized.6

5

id136 via importance sampling

our generative model p(x, y) de   nes a joint dis-
tribution on trees (y) and sequences of words (x).
to evaluate this as a language model, it is neces-
sary to compute the marginal id203 p(x) =

(cid:80)y(cid:48)   y(x) p(x, y(cid:48)). and, to evaluate the model as

a parser, we need to be able to    nd the map parse
tree, i.e., the tree y     y(x) that maximizes p(x, y).
however, because of the unbounded dependencies
across the sequence of parsing actions in our model,
exactly solving either of these id136 problems
is intractable. to obtain estimates of these, we use

6for the discriminative parser, the pos tags are processed
similarly as in (dyer et al., 2015); they are predicted for english
with the stanford tagger (toutanova et al., 2003) and chinese
with marmot (mueller et al., 2013).

thehungrycatnp(vp(sreducegennt(np)nt(vp)   cathungrythea<tp(at)utttz}|{stz}|{a variant of importance sampling (doucet and jo-
hansen, 2011).
our importance sampling algorithm uses a condi-
tional proposal distribution q(y | x) with the fol-
lowing properties: (i) p(x, y) > 0 =    q(y |
x) > 0; (ii) samples y     q(y | x) can be ob-
tained ef   ciently; and (iii) the conditional probabil-
ities q(y | x) of these samples are known. while
many such distributions are available, the discrim-
inatively trained variant of our parser (  4.4) ful-
   lls these requirements: sequences of actions can
be sampled using a simple ancestral sampling ap-
proach, and, since parse trees and action sequences
exist in a one-to-one relationship, the product of the
action probabilities is the id155 of
the parse tree under q. we therefore use our discrim-
inative parser as our proposal distribution.
importance sampling uses importance weights,
which we de   ne as w(x, y) = p(x, y)/q(y | x), to
compute this estimate. under this de   nition, we can
derive the estimator as follows:

p(x) = (cid:88)y   y(x)

p(x, y) = (cid:88)y   y(x)

= eq(y|x)w(x, y).

q(y | x)w(x, y)

we now replace this expectation with its monte
carlo estimate as follows, using n samples from q:

y(i)     q(y | x)

eq(y|x)w(x, y)

for i     {1, 2, . . . , n}
mc    1
n

w(x, y(i))

n(cid:88)i=1

to obtain an estimate of the map tree   y, we choose
the sampled tree with the highest id203 under
the joint model p(x, y).

6 experiments

we present results of our two models both on parsing
(discriminative and generative) and as a language
model (generative only) in english and chinese.
data. for english,   2   21 of the id32
are used as training corpus for both, with   24 held
out as validation, and   23 used for evaluation. sin-
gleton words in the training corpus with unknown

word classes using the the berkeley parser   s map-
ping rules.7 orthographic case distinctions are pre-
served, and numbers (beyond singletons) are not
normalized. for chinese, we use the penn chinese
treebank version 5.1 (ctb) (xue et al., 2005).8 for
the chinese experiments, we use a single unknown
word class. corpus statistics are given in table 1.9

table 1: corpus statistics.

sequences
tokens
types
unk-types

ptb-train
39,831
950,012
23,815
49

ptb-test
2,416
56,684
6,823
42

ctb-train
50,734
1,184,532
31,358
1

ctb-test
348
8,008
1,637
1

model and training parameters. for the dis-
criminative model, we used hidden dimensions of
128 and 2-layer lstms (larger numbers of dimen-
sions reduced validation set performance). for the
generative model, we used 256 dimensions and 2-
layer lstms.
for both models, we tuned the
dropout rate to maximize validation set likelihood,
obtaining optimal rates of 0.2 (discriminative) and
0.3 (generative). for the sequential lstm baseline
for the language model, we also found an optimal
dropout rate of 0.3. for training we used stochas-
tic id119 with a learning rate of 0.1. all
parameters were initialized according to recommen-
dations given by glorot and bengio (2010).

english parsing results. table 2 (last two rows)
gives the performance of our parser on section 23,
as well as the performance of several representa-
tive models. for the discriminative model, we used
a greedy decoding rule as opposed to id125
in some shift-reduce baselines. for the generative
model, we obtained 100 independent samples from
a    attened distribution of the discriminative parser
(by exponentiating each id203 by    = 0.8 and
renormalizing) and reranked them according to the

7http://github.com/slavpetrov/
8  001   270 and 440   1151 for training,   301   325 develop-

berkeleyparser
ment data, and   271   300 for evaluation.

9this preprocessing scheme is more similar to what is stan-
dard in parsing than what is standard in id38.
however, since our model is both a parser and a language
model, we opted for the parser id172.

generative model.10

table 2: parsing results on ptb   23 (d=discriminative,
g=generative, s=semisupervised). (cid:63) indicates the (vinyals et
al., 2015) result with trained only on the wsj corpus without
ensembling.
model
vinyals et al. (2015)(cid:63)     wsj only
henderson (2004)
socher et al. (2013a)
zhu et al. (2013)
petrov and klein (2007)
bod (2003)
shindo et al. (2012)     single
shindo et al. (2012)     ensemble
zhu et al. (2013)
mcclosky et al. (2006)
vinyals et al. (2015)     single
discriminative, q(y | x)
generative,   p(y | x)

type
d
d
d
d
g
g
g
g
s
s
s
d
g

f1
88.3
89.4
90.4
90.4
90.1
90.7
91.1
92.4
91.3
92.1
92.1
89.8
92.4

chinese parsing results. chinese parsing results
were obtained with the same methodology as in en-
glish and show the same pattern (table 6).

table 3: parsing results on ctb 5.1.

model
zhu et al. (2013)
wang et al. (2015)
huang and harper (2009)
charniak (2000)
bikel (2004)
petrov and klein (2007)
zhu et al. (2013)
wang and xue (2014)
wang et al. (2015)
discriminative, q(y | x)
generative,   p(y | x)

type
d
d
d
g
g
g
s
s
s
d
g

f1
82.6
83.2
84.2
80.8
80.6
83.3
85.6
86.3
86.6
80.7
82.7

language model results. we report held-out per-
word perplexities of three language models, both se-
quential and syntactic. log probabilities are normal-
ized by the number of words (excluding the stop

10the value    = 0.8 was chosen based on the diversity of

the samples generated on the development set.

symbol), inverted, and exponentiated to yield the
perplexity. results are summarized in table 4.

table 4: language model perplexity results.

model
ikn 5-gram
lstm lm
id56g

test ppl (ptb)

test ppl (ctb)

169.3
113.4
102.4

255.2
207.3
171.9

7 discussion
it is clear from our experiments that the proposed
generative model is quite effective both as a parser
and as a language model. this is the result of
(i) relaxing conventional independence assumptions
(e.g., context-freeness) and (ii) inferring continu-
ous representations of symbols alongside non-linear
models of their syntactic relationships. the most
signi   cant question that remains is why the dis-
criminative model   which has more information
available to it than the generative model   performs
worse than the generative model. this pattern has
been observed before in neural parsing by hender-
son (2004), who hypothesized that larger, unstruc-
tured conditioning contexts are harder to learn from,
and provide opportunities to over   t. our discrimi-
native model conditions on the entire history, stack,
and buffer, while our generative model only ac-
cesses the history and stack. the fully discrimina-
tive model of vinyals et al. (2015) was able to obtain
results similar to those of our generative model (al-
beit using much larger training sets obtained through
semisupervision) but similar results to those of our
discriminative parser using the same data. in light of
their results, we believe henderson   s hypothesis is
correct, and that generative models should be con-
sidered as a more statistically ef   cient method for
learning neural networks from small data.

8 related work
our language model combines work from two mod-
eling traditions:
(i) recurrent neural network lan-
guage models and (ii) syntactic language model-
ing. recurrent neural network language models
use id56s to compute representations of an un-
bounded history of words in a left-to-right language
model (zaremba et al., 2015; mikolov et al., 2010;

elman, 1990). syntactic language models jointly
generate a syntactic structure and a sequence of
words (baker, 1979; jelinek and lafferty, 1991).
there is an extensive literature here, but one strand
of work has emphasized a bottom-up generation of
the tree, using variants of shift-reduce parser ac-
tions to de   ne the id203 space (chelba and
jelinek, 2000; emami and jelinek, 2005). the
neural-network   based model of henderson (2004)
is particularly similar to ours in using an unbounded
history in a neural network architecture to param-
eterize generative parsing based on a left-corner
model. dependency-only language models have
also been explored (titov and henderson, 2007;
buys and blunsom, 2015a,b). modeling generation
top-down as a rooted branching process that recur-
sively rewrites nonterminals has been explored by
charniak (2000) and roark (2001). of particular
note is the work of charniak (2010), which uses ran-
dom forests and hand-engineered features over the
entire syntactic derivation history to make decisions
over the next action to take.

the neural networks we use to model sentences
are structured according to the syntax of the sen-
tence being generated. syntactically structured neu-
ral architectures have been explored in a num-
ber of applications, including discriminative pars-
ing (socher et al., 2013a; kiperwasser and gold-
berg, 2016), id31 (tai et al., 2015;
socher et al., 2013b), and sentence representa-
tion (socher et al., 2011; bowman et al., 2006).
however, these models have been, without excep-
tion, discriminative; this is the    rst work to use syn-
tactically structured neural models to generate lan-
guage. earlier work has demonstrated that sequen-
tial id56s have the capacity to recognize context-
free (and beyond) languages (sun et al., 1998;
siegelmann and sontag, 1995).
in contrast, our
work may be understood as a way of incorporating a
context-free inductive bias into the model structure.

9 outlook

id56gs can be combined with a particle    lter infer-
ence scheme (rather than the importance sampling
method based on a discriminative parser,   5) to pro-
duce a left-to-right marginalization algorithm that
runs in expected linear time. thus, they could be

used in applications that require language models.

a second possibility is to replace the sequential
generation architectures found in many neural net-
work transduction problems that produce sentences
conditioned on some input. previous work in ma-
chine translation has showed that conditional syn-
tactic models can function quite well without the
computationally expensive marginalization process
at decoding time (galley et al., 2006; gimpel and
smith, 2014).

a third consideration regarding how id56gs, hu-
man sentence processing takes place in a left-to-
right, incremental order. while an id56g is not a
processing model (it is a grammar), the fact that it is
left-to-right opens up several possibilities for devel-
oping new sentence processing models based on an
explicit grammars, similar to the processing model
of charniak (2010).

finally, although we considered only the super-
vised learning scenario, id56gs are joint models
that could be trained without trees, for example, us-
ing expectation maximization.

10 conclusion

we introduced recurrent neural network grammars,
a probabilistic model of phrase-structure trees that
can be trained generatively and used as a language
model or a parser, and a corresponding discrimina-
tive model that can be used as a parser. apart from
out-of-vocabulary preprocessing, the approach re-
quires no feature design or transformations to tree-
bank data. the generative model outperforms ev-
ery previously published parser built on a single su-
pervised generative model in english, and a bit be-
hind the best-reported generative model in chinese.
as language models, id56gs outperform the best
single-sentence language models.

acknowledgments

thank

brendan
o   connor,
and brian roark for

we
swabha
swayamdipta,
feedback
on drafts of this paper, and jan buys, phil blunsom,
and yue zhang for help with data preparation.
this work was sponsored in part by the defense
advanced research projects agency (darpa)
information innovation of   ce (i2o) under
the
low resource languages for emergent incidents

(lorelei) program issued by darpa/i2o under
contract no. hr0011-15-c-0114; it was also sup-
ported in part by contract no. w911nf-15-1-0543
with the darpa and the army research of   ce
(aro). approved for public release, distribution
unlimited. the views expressed are those of the au-
thors and do not re   ect the of   cial policy or position
of the department of defense or the u.s. govern-
ment. miguel ballesteros was supported by the
european commission under the contract numbers
fp7-ict-610411 (project multisensor) and
h2020-ria-645012 (project kristina).

references

steven abney, david mcallester, and fernando pereira.
1999. relating probabilistic grammars and automata.
in proc. acl.

dzmitry bahdanau, kyunghyun cho, and yoshua ben-
gio. 2015. neural machine transation by jointly learn-
ing to align and translate. in proc. iclr.

james k. baker. 1979. trainable grammars for speech
recognition. the journal of the acoustical society of
america, 65(s1):s132   s132.

paul baltescu and phil blunsom. 2015. pragmatic neural

modelling in machine translation. in proc. naacl.

dan bikel. 2004. on the parameter space of generative
lexicalized statistical parsing models. ph.d. thesis,
university of pennsylvania.

rens bod. 2003. an ef   cient implementation of a new

dop model. in proc. eacl.

samuel r. bowman, jon gauthier, abhinav rastogi,
raghav gupta, christopher d. manning, and christo-
pher potts. 2006. a fast uni   ed model for parsing and
sentence understanding. corr, abs/1603.06021.

peter f. brown, peter v. desouza, robert l. mercer, vin-
cent j. della pietra, and jenifer c. lai. 1992. class-
based id165 models of natural language. computa-
tional linguistics, 18(4):467   479.

jan buys and phil blunsom. 2015a. a bayesian model
for generative transition-based id33.
corr, abs/1506.04334.

jan buys and phil blunsom. 2015b. generative incre-
mental id33 with neural networks. in
proc. acl.

eugene charniak. 2000. a maximum-id178-inspired

parser. in proc. naacl.
eugene charniak. 2010.

top-down nearly-context-

sensitive parsing. in proc. emnlp.

ciprian chelba and frederick jelinek. 2000. structured

id38. computer speech and language,
14(4).

kyunghyun cho, bart van merri  enboer, caglar gulcehre,
dzmitry bahdanau, fethi bougares, holger schwenk,
and yoshua bengio. 2014. learning phrase represen-
tations using id56 encoder   decoder for statistical ma-
chine translation. in proc. emnlp.

noam chomsky. 1957. syntactic structures. mouton,

the hague/paris.

arnaud doucet and adam m. johansen. 2011. a tutorial
on particle    ltering and smoothing: fifteen years later.
in handbook of nonlinear filtering. oxford.

chris dyer, miguel ballesteros, wang ling, austin
matthews, and noah a. smith. 2015. transition-based
id33 with stack long short-term mem-
ory. in proc. acl.

jay earley. 1970. an ef   cient context-free parsing algo-

rithm. communications of the acm, 13(2):94   102.

jeffrey l. elman. 1990. finding structure in time. cog-

nitive science, 14:179   211.

ahmad emami and frederick jelinek. 2005. a neural
syntactic language model. machine learning, 60:195   
227.

michel galley, jonathan graehl, kevin knight, daniel
marcu, steve deneefe, wei wang, and ignacio
scalable id136 and training of
thayer. 2006.
in proc.
context-rich syntactic translation models.
acl.

kevin gimpel and noah a. smith. 2014. phrase de-
pendency machine translation with quasi-synchronous
computational linguistics,
tree-to-tree features.
40(2).

xavier glorot and yoshua bengio. 2010. understanding
the dif   culty of training deep feedforward neural net-
works. in proc. icml.

joshua goodman. 2001. classes for fast maximum en-

tropy training. corr, cs.cl/0108006.

james henderson. 2003. inducing history representations
for broad coverage statistical parsing. in proc. naacl.
james henderson. 2004. discriminative training of a neu-

ral network statistical parser. in proc. acl.

zhongqiang huang and mary harper. 2009. self-training
pid18 grammars with latent annotations across lan-
guages. in proc. emnlp.

frederick jelinek and john d. lafferty. 1991. compu-
tation of the id203 of initial substring generation
by stochastic context-free grammars. computational
linguistics, 17(3):315   323.

mark johnson. 2001. joint and conditional estimation of

tagging and parsing models. in proc. acl.

eliyahu kiperwasser and yoav goldberg. 2016. easy-
tree

   rst id33 with hierarchical
lstms. arxiv:1603.00375.

david mcclosky, eugene charniak, and mark johnson.
in proc.

2006. effective self-training for parsing.
naacl.

tom  a  s mikolov, martin kara     at, luk  a  s burget, jan
  cernock  y, and sanjeev khudanpur. 2010. recurrent
neural network based language model. in proc. inter-
speech.

thomas mueller, helmut schmid, and hinrich sch  utze.
2013. ef   cient higher-order crfs for morphologi-
in proceedings of the 2013 conference
cal tagging.
on empirical methods in natural language process-
ing, pages 322   332. association for computational
linguistics, seattle, washington, usa. url http:
//www.aclweb.org/anthology/d13-1032.
terence parr and kathleen fisher. 2011. ll(*): the
foundation of the antlr parser generator. in proc.
pldi.

slav petrov and dan klein. 2007. improved id136 for

unlexicalized parsing. in proc. naacl.

brian roark. 2001. probabilistic top-down parsing and
id38. computational linguistics, 27(2).
brian roark. 2004. robust garden path parsing. jnle,

10(1):1   24.

kenji sagae and alon lavie. 2005. a classi   er-based
parser with linear run-time complexity. in proc. iwpt.
hiroyuki shindo, yusuke miyao, akinori fujino, and
masaaki nagata. 2012. bayesian symbol-re   ned tree
substitution grammars for syntactic parsing. in proc.
acl.

hava t. siegelmann and eduardo d. sontag. 1995. on
the computational power of neural nets. journal of
computer and system sciences, 50.

richard socher, john bauer, christopher d. manning,
and andrew y. ng. 2013a. parsing with compositional
vectors. in proc. acl.

richard socher, eric h. huang, jeffrey pennington, an-
drew y. ng, and christopher d. manning. 2011. dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. in proc. nips.

richard socher, alex perelygin, jean y. wu, jason
chuang, christopher d. manning, andrew y. ng, and
christopher potts. 2013b. recursive deep models for
semantic compositionality over a sentiment treebank.
in proc. emnlp.

guo-zheng sun, c. lee giles, and hsing-hen chen.
1998. the neural network pushdown automaton: ar-
in adaptive pro-
chitecture, dynamics and training.

cessing of sequences and data structures, volume
1387 of lecture notes in computer science, pages
296   345.

kai sheng tai, richard socher, and christopher d. man-
ning. 2015. improved semantic representations from
tree-structured id137. in
proc. acl.

ivan titov and james henderson. 2007. a latent variable
in proc.

model for generative id33.
iwpt.

kristina toutanova, dan klein, christopher d. manning,
and yoram singer. 2003. feature-rich part-of-speech
in proc.
tagging with a cyclic dependency network.
naacl.

oriol vinyals, lukasz kaiser, terry koo, slav petrov,
ilya sutskever, and geoffrey hinton. 2015. grammar
as a foreign language. in proc. iclr.

zhiguo wang, haitao mi, and nianwen xue. 2015. fea-
ture optimization for constituent parsing via neural
networks. in proc. acl-ijcnlp.

zhiguo wang and nianwen xue. 2014. joint pos tag-
ging and transition-based constituent parsing in chi-
nese with non-local features. in proc. acl.

tsung-hsien wen, milica ga  si  c, nikola mrk  si  c, pei-hao
su, david vandyke, and steve young. 2015. semanti-
cally conditioned lstm-based natural language gen-
eration for spoken dialogue systems. in proc. emnlp.
kelvin xu, jimmy lei ba, ryan kiros, kyunghyun cho,
aaron courville, ruslan salakhutdinov, richard s.
zemel, and yoshua bengio. 2015. show, attend and
tell: neural image id134 with visual at-
tention. in proc. icml.

naiwen xue, fei xia, fu-dong chiou, and marta palmer.
2005. the penn chinese treebank: phrase structure
annotation of a large corpus. nat. lang. eng., 11(2).

wojciech zaremba, ilya sutskever, and oriol vinyals.
in

2015. recurrent neural network id173.
proc. iclr.

yue zhang and stephen clark. 2011. syntactic process-
ing using the generalized id88 and id125.
computational linguistics, 37(1).

muhua zhu, yue zhang, wenliang chen, min zhang, and
jingbo zhu. 2013. fast and accurate shift-reduce con-
stituent parsing. in proc. acl.

corrigendum to recurrent neural network grammars

properly composing the constituent and propagat-
ing information upwards is crucial. despite slightly
higher id38 perplexity on ptb   23,
the    xed id56g still outperforms a highly optimized
sequential lstm baseline.

f1
88.3
89.4
90.4
90.4
90.1
90.7
91.1
92.4
91.3
92.1
92.1
89.8
92.4
91.7
93.3

type
d
d
d
d
g
g
g
g
s
s
s
d
g
d
g

model
vinyals et al. (2015)(cid:63)     wsj only
henderson (2004)
socher et al. (2013a)
zhu et al. (2013)
petrov and klein (2007)
bod (2003)
shindo et al. (2012)     single
shindo et al. (2012)     ensemble
zhu et al. (2013)
mcclosky et al. (2006)
vinyals et al. (2015)
discriminative, q(y | x)        buggy
generative,   p(y | x)        buggy
discriminative, q(y | x)     correct
generative,   p(y | x)     correct
table 5: parsing results with    xed composition function on
ptb   23 (d=discriminative, g=generative, s=semisupervised).
(cid:63) indicates the (vinyals et al., 2015) model trained only on the
wsj corpus without ensembling.     indicates id56g models
with the buggy composition function implementation.

abstract

due to an implentation bug in the id56g   s
recursive composition function, the results re-
ported in dyer et al. (2016) did not correspond
to the model as it was presented. this corri-
gendum describes the buggy implementation
and reports results with a corrected implemen-
tation. after correction, on the ptb   23 and
ctb 5.1 test sets, respectively, the generative
model achieves id38 perplexi-
ties of 105.2 and 148.5, and phrase-structure
parsing f1 of 93.3 and 86.9, a new state of
the art in phrase-structure parsing for both lan-
guages.

id56g composition function and
implementation error

the composition function reduces a completed con-
stituent into a single vector representation using a
bidirectional lstm (figure 7) over embeddings of
the constituent   s children as well as an embedding of
the resulting nonterminal symbol type. the imple-
mentation error (figure 8) composed the constituent
(np the hungry cat) by reading the sequence    np the
hungry np   , that is, it discarded the rightmost child
of every constituent and replaced it with a second
copy of the constituent   s nonterminal symbol. this
error occurs for every constituent and means crucial
information is not properly propagated upwards in
the tree.

results after correction

the implementation error affected both the gener-
ative and discriminative id56gs.11 we summarize
corrected english phrase-structure ptb   23 parsing
result in table 5, chinese (ctb 5.1   271   300) in
table 6 (achieving the the best reported result on
both datasets), and english and chinese language
modeling perplexities in table 7. the consider-
able improvement in parsing accuracy indicates that

11the discriminative model can only be used for parsing and

not for id38, since it only models p(y | x).

figure 7: correct id56g composition function for the con-
stituent (np the hungry cat).

figure 8: buggy implementation of the id56g composition
function for the constituent (np the hungry cat). note that
the right-most child, cat, has been replaced by a second np.

model
zhu et al. (2013)
wang et al. (2015)
huang and harper (2009)
charniak (2000)
bikel (2004)
petrov and klein (2007)
zhu et al. (2013)
wang and xue (2014)
wang et al. (2015)
discriminative, q(y | x)    - buggy
generative,   p(y | x)    - buggy
discriminative, q(y | x)     correct
generative,   p(y | x)     correct
table 6: parsing results on ctb 5.1 including results with the
buggy composition function implementation (indicated by    )
and with the correct implementation.

type
d
d
d
g
g
g
s
s
s
d
g
d
g

f1
82.6
83.2
84.2
80.8
80.6
83.3
85.6
86.3
86.6
80.7
82.7
84.6
86.9

model
ikn 5-gram
lstm lm
id56g     buggy   
id56g     correct

test ppl (ptb)

test ppl (ctb)

169.3
113.4
102.4
105.2

255.2
207.3
171.9
148.5

table 7: ptb and ctb id38 results including re-
sults with the buggy composition function implementation (in-
dicated by    ) and with the correct implementation.

