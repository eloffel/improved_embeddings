semi-supervised id91 for short text via deep representation

learning

zhiguo wang and haitao mi and abraham ittycheriah

ibm t.j. watson research center

yorktown heights, ny, usa

{zhigwang, hmi, abei}@us.ibm.com

7
1
0
2

 
l
u
j
 

4
1

 
 
]
l
c
.
s
c
[
 
 

2
v
7
9
7
6
0

.

2
0
6
1
:
v
i
x
r
a

abstract

in this work, we propose a semi-supervised
method for short text id91, where we
represent texts as distributed vectors with neu-
ral networks, and use a small amount of la-
beled data to specify our intention for cluster-
ing. we design a novel objective to combine
the representation learning process and the k-
means id91 process together, and opti-
mize the objective with both labeled data and
unlabeled data iteratively until convergence
through three steps: (1) assign each short text
to its nearest centroid based on its representa-
tion from the current neural networks; (2) re-
estimate the cluster centroids based on cluster
assignments from step (1); (3) update neural
networks according to the objective by keep-
ing centroids and cluster assignments    xed.
experimental results on four datasets show
that our method works signi   cantly better than
several other text id91 methods.

introduction

1
text id91 is a fundamental problem in text
mining and information retrieval.
its task is to
group similar texts together such that texts within
a cluster are more similar to texts in other clus-
ters. usually, a text is represented as a bag-of-words
or term frequency-inverse document frequency (tf-
idf) vector, and then the id116 algorithm (mac-
queen, 1967) is performed to partition a set of texts
into homogeneous groups.

however, when dealing with short texts, the char-
acteristics of short text and id91 task raise sev-
eral issues for the conventional unsupervised clus-
tering algorithms. first, the number of uniqe words

(a) what   s the color of apples?
(b) when will this apple be ripe?
(c) do you like apples?
(d) what   s the color of oranges?
(e) when will this orange be ripe?
(f) do you like oranges?
table 1: examples for short text id91.

in each short text is small, as a result, the lexcical
sparsity issue usually leads to poor id91 qual-
ity (dhillon and guan, 2003). second, for a speci   c
short text id91 task, we have prior knowledge
or paticular intenstions before id91, while fully
unsupervised approaches may learn some classes the
other way around. take the sentences in table 1 for
example, those sentences can be clustered into dif-
ferent partitions based on different intentions: apple
{a, b, c} and orange {d, e, f} with a fruit type inten-
sion, or what-question {a, d}, when-question {b, e},
and yes/no-question cluster {c, f} with a question
type intension.

to address the lexical sparity issue, one direction
is to enrich text representations by extracting fea-
tures and relations from wikipedia (banerjee et al.,
2007) or an ontology (fodeh et al., 2011). but this
approach requires the annotated knowlege, which is
also language dependent. so the other direction,
which directly encode texts into distributed vectors
with neural networks (hinton and salakhutdinov,
2006; xu et al., 2015), becomes more interesing.
to tackle the second problem, semi-supervised ap-
proaches (e.g. (bilenko et al., 2004; davidson and
basu, 2007; bair, 2013)) have gained signi   cant
popularity in the past decades. our question is can

we have a uni   ed model to integrate netural net-
works into the semi-supervied framework?

in this paper, we propose a uni   ed framework for
the short text id91 task. we employ a deep neu-
ral network model to represent short sentences, and
integrate it into a semi-supervised algorithm. con-
cretely, we extend the objective in the classical un-
supervised id116 algorithm by adding a penalty
term from labeled data. thus, the new objective
covers three key groups of parameters: centroids of
clusters, the cluster assignment for each text, and
the parameters within deep neural networks. in the
training procedure, we start from random initializa-
tion of centroids and neural networks, and then op-
timize the objective iteratively through three steps
until converge:

(1) assign each short text to its nearest centroid
based on its representation from the current
neural networks;

(2) re-estimate cluster centroids based on cluster

assignments from step (1);

(3) update neural networks according to the objec-
tive by keeping centroids and cluster assign-
ments    xed.

experimental results on four different datasets show
that our method achieves signi   cant improvements
over several other text id91 methods.

in following parts, we    rst describe our neural
network models for text representaion (section 2).
then we introduce our semi-supervised id91
method and the learning algorithm (section 3). fi-
nally, we evaluate our method on four different
datasets (section 4).

2 representation learning for short texts

we represent each word with a dense vector w, so
that a short text s is    rst represented as a matrix
s = [w1, ..., w|s|], which is a concatenation of all
vectors of w in s, |s| is the length of s. then we de-
sign two different types of neural networks to ingest
the word vector sequence s: the convolutional neu-
ral networks (id98) and the long short-term memory
(lstm). more formally, we de   ne the presentation
function as x = f (s), where x is the represent vector

figure 1: id98 for text representation learning.

of the text s. we test two encoding functions (id98
and lstm) in our experiments.

inspired from kim (2014), our id98 model views
the sequence of word vectors as a matrix, and applies
two sequential operations: convolution and max-
pooling. then, a fully connected layer is employed
to convert the    nal representation vector into a    xed
size. figure 1 gives the diagram of the id98 model.
in the convolution operation, we de   ne a list of    l-
ters {wo}, where the shape of each    lter is d    h, d
is the dimension of word vectors and h is the win-
dow size. each    lter is applied to a patch (a window
size h of vectors) of s, and generates a feature. we
apply this    lter to all possible patches in s, and pro-
duce a series of features. the number of features
depends on the shape of the    lter wo and the length
of the input short text. to deal with variable fea-
ture size, we perform a max-pooling operation over
all the features to select the maximum value. there-
fore, after the two operations, each    lter generates
only one feature. we de   ne several    lters by vary-
ing the window size and the initial values. thus, a
vector of features is captured after the max-pooling
operation, and the feature dimension is equal to the
number of    lters.

figure 2 gives the diagram of our lstm model.
we implement the standard lstm block described
in graves (2012). each word vector is fed into the

   	
      	
      	
      	
   convolution operation max-pooling operation fully connected layer  n(cid:88)

n=1

by keeping {rnk}    xed. junsup is a quadratic func-
tion of {  k}, and it can be minimized by setting its
derivative with respect to {  k} to zero.

   junsup

     k

= 2

rnk(xn       k) = 0

(2)

then, we can easily solve {  k} as

(cid:80)n
(cid:80)n

n=1 rnkxn
n=1 rnk

  k =

(3)

in other words,   k is equal to the mean of all the
data points assigned to cluster k.

3.2 semi-supervised id116 with neural

networks

the classical id116 algorithm only uses unlabeled
data, and solves the id91 problem under the
unsupervised learning framework. as already men-
tioned, the id91 results may not be consistent
to our intention. in order to acquire useful cluster-
ing results, some supervised information should be
introduced into the learning procedure. to this end,
we employ a small amount of labeled data to guide
the id91 process.

following section 2, we represent each text s as
a dense vector x via neural networks f (s). instead
of training the text representation model separately,
we integrate the training process into the id116
algorithm, so that both the labeled data and the un-
labeled data can be used for representation learning
and text id91. let us denote the labeled data
set as {(s1, y1), (s2, y2), ..., (sl, yl)}, and the unla-
beled data set as {sl+1, sl+2, ..., sn}, where yi is
the given label for si. we then de   ne the objective
function as:

n(cid:88)
l(cid:88)

n=1

k(cid:88)

k=1

rnk(cid:107)f (sn)       k(cid:107)2

{(cid:107)f (sn)       gn(cid:107)2+

jsemi =   

+ (1       )

(cid:88)

j(cid:54)=gn

n=1

[l + (cid:107)f (sn)       gn(cid:107)2     (cid:107)f (sn)       j(cid:107)2]+}

(4)

figure 2: lstm for text representation learning.

lstm model sequentially, and the mean of the hid-
den states over the entire sentence is taken as the
   nal representation vector.

3 semi-supervised id91 for short

texts

3.1 revisiting id116 id91
given a set of texts {s1, s2, ..., sn}, we represent
them as a set of data points {x1, x2, ..., xn}, where
xi can be a bag-of-words or tf-idf vector in tra-
ditional approaches, or a dense vector in section 2.
the task of text id91 is to partition the data set
into some number k of clusters, such that the sum of
the squared distance of each data point to its closest
cluster centroid is minimized. for each data point
xn, we de   ne a set of binary variables rnk     {0, 1},
where k     {1, ..., k} describing which of the k
clusters xn is assigned to. so that if xn is assigned
to cluster k, then rnk = 1, and rnj = 0 for j (cid:54)= k.
let   s de   ne   k as the centroid of the k-th cluster.
we can then formulate the objective function as

junsup =

rnk(cid:107)xn       k(cid:107)2

(1)

n=1

k=1

our goal is the    nd the values of {rnk} and {  k} so
as to minimize junsup.

the id116 algorithm optimizes junsup through
the id119 approach, and results in an it-
erative procedure (bishop, 2006). each iteration
involves two steps: e-step and m-step.
in the e-
step, the algorithm minimizes junsup with respect
to {rnk} by keeping {  k}    xed. junsup is a linear
function for {rnk}, so we can optimize for each data
point separately by simply assigning the n-th data
point to the closest cluster centroid. in the m-step,
the algorithm minimizes junsup with respect to {  k}

n(cid:88)

k(cid:88)

lstm lstm lstm       	
   w1 w2 wn mean 1. initialize {  k} and f (  ).
2. assign cluster: assign each text to its nearest cluster centroid.
3. estimate centroid: estimate the cluster centroids based on the cluster assignments from step 2.
4. update parameter: update parameters in neural networks.
5. repeat step 2 to 4 until convergence.

table 2: pseudocode for semi-supervised id91

the objective function contains two terms. the    rst
term is adapted from the unsupervised id116 al-
gorithm in eq. (1), and the second term is de   ned
to encourage labeled data being clustered in correla-
tion with the given labels.        [0, 1] is used to tune
the importance of unlabeled data. the second term
contains two parts. the    rst part penalizes large dis-
tance between each labeled instance and its correct
cluster centroid, where gn = g(yn) is the cluster
id mapped from the given label yn, and the map-
ping function g(  ) is implemented with the hun-
garian algorithm (munkres, 1957). the second part
is denoted as a hinge loss with a margin l, where
[x]+ = max(x, 0). this part incurs some loss if the
distance to the correct centroid is not shorter (by the
margin l) than distances to any of incorrect cluster
centroids.

there are three groups of parameters in jsemi: the
cluster assignment of each text {rnk}, the cluster
centroids {  k}, and the parameters within the neural
network model f (  ). our goal is the    nd the values
of {rnk}, {  k} and parameters in f (  ), so as to min-
imize jsemi. inspired from the id116 algorithm,
we design an algorithm to successively minimize
jsemi with respect to {rnk}, {  k}, and parameters in
f (  ). table 2 gives the corresponding pseudocode.
first, we initialize the cluster centroids {  k} with
the id116++ strategy (arthur and vassilvitskii,
2007), and randomly initialize all the parameters in
the neural network model. then, the algorithm iter-
atively goes through three steps (assign cluster, es-
timate centroid, and update parameter) until jsemi
converges.

the assign cluster step minimizes jsemi with re-
spect to {rnk} by keeping f (  ) and {  k}    xed. its
goal is to assign a cluster id for each data point. we
can see that the second term in eq. (4) has no rela-
tion with {rnk}. thus, we only need to minimize the
   rst term by assigning each text to its nearest clus-
ter centroid, which is identical to the e-step in the

id116 algorithm. in this step, we also calculate
the mappings between the given labels {yi} and the
cluster ids (with the hungarian algorithm) based on
cluster assignments of all labeled data.
the estimate centroid step minimizes jsemi with
respect to {  k} by keeping {rnk} and f (  )    xed,
which corresponds to the m-step in the id116 al-
gorithm.
it aims to estimate the cluster centroids
{  k} based on the cluster assignments {rnk} from
the assign cluster step. the second term in eq.
(4) makes each labeled instance involved in the es-
timating process of cluster centroids. by solving
   jsemi/     k = 0, we get

n=1   rnkf (sn) +(cid:80)l
(cid:80)n
(cid:80)n
n=1   rnk +(cid:80)l

  k =

n=1 wnkf (sn)
n=1 wnk

(5)

(cid:88)

j(cid:54)=gn

nkj     (cid:88)

(cid:48)(cid:48)

i

j(cid:54)=gn

(cid:48)(cid:48)(cid:48)
i
nkj)

wnk = (1       )(i

(cid:48)
nk +

(cid:48)

(cid:48)(cid:48)(cid:48)

(cid:48)
nj

(cid:48)
nj

(cid:48)
i
nk =   (k, gn)
nkj =   (k, j)      
(cid:48)(cid:48)
i
nkj = (1       (k, j))      
i
nj =   (l + (cid:107)f (sn)       gn(cid:107)2     (cid:107)f (sn)       j(cid:107)2 > 0)
  
(6)
where   (x1, x2)=1 if x1 is equal to x2, otherwise
  (x1, x2)=0, and   (x)=1 if x is true, otherwise
  (x)=0. the    rst term in the numerator of eq. (5)
is the contributions from all data points, and   rnk is
the weight of sn for   k. the second term is acquired
from labeled data, and wnk is the weight of a labeled
instance sn for   k.
the update parameter step minimizes jsemi with
respect to f (  ) by keeping {rnk} and {  k}    xed,
which has no counterpart in the id116 algorithm.
the main goal is to update parameters for the text
representation model. we take jsemi as the loss
function, and train neural networks with the adam
algorithm (kingma and ba, 2014).

dataset
question type
ag news
dbpedia
yahoo answer

class#
6
4
14
10

total#
5,953
4,000
14,000
10,000

labeled#
595
400
1,400
1,000

table 3: statistics for the short text datasets

4 experiment
4.1 experimental setting
we evaluate our method on four short text datasets.
(1) question type is the trec question dataset (li
and roth, 2002), where all the questions are clas-
si   ed into 6 categories: abbreviation, description,
(2) ag news
entity, human, location and numeric.
dataset contains short texts extracted from the ag   s
news corpus, where all the texts are classi   ed into
4 categories: world, sports, business, and sci/tech
(zhang and lecun, 2015). (3) dbpedia is the dbpe-
dia ontology dataset, which is constructed by pick-
ing 14 non-overlapping classes from dbpedia 2014
(4) yahoo answer is the
(lehmann et al., 2014).
10 topics classi   cation dataset extracted from ya-
hoo! answers comprehensive questions and an-
swers version 1.0 dataset by zhang and lecun
(2015). we use all the 5,952 questions for the ques-
tion type dataset. but the other three datasets con-
tain too many instances (e.g. 1,400,000 instances in
yahoo answer). running id91 experiments on
such a large dataset is quite inef   cient. following
the same solution in (xu et al., 2015), we randomly
choose 1,000 samples for each classes individually
for the other three datasets. within each dataset, we
randomly sample 10% of the instances as labeled
data, and evaluate the performance on the remain-
ing 90% instances. table 3 summarizes the statistics
of these datasets.

in all experiments, we set the size of word vector
dimension as d=300 1, and pre-train the word vec-
tors with the id97 toolkit (mikolov et al., 2013)
on the english gigaword (ldc2011t07). the num-
ber of clusters is set to be the same number of labels
in the dataset. the id91 performance is eval-

1we tuned different dimensions for word vectors. when the
size is small (50 or 100), performance drops signi   cantly. when
the size is larger (300, 500 or 1000), the curve    attens out. to
make our model more ef   cient, we    xed it as 300.

figure 3:
in   uence of the short text representation model,
where the x-axis is the output dimension of the text represen-
tation models.

uated with two metrics: adjusted mutual informa-
tion (ami) (vinh et al., 2009) and accuracy (acc)
(amig  o et al., 2009). in order to show the statistical
signi   cance, the performance of each experiment is
the average of 10 trials.

4.2 model properties
there are several hyper-parameters in our model,
e.g., the output dimension of the text representation
models, and the    in eq. (4). the choice of these
hyper-parameters may affect the    nal performance.
in this subsection, we present some experiments to
demonstrate the properties of our model, and    nd a
good con   guration that we use to evaluate our    nal
model. all the experiments in this subsection were
performed on the question type dataset.

first, we evaluated the effectiveness of the out-
put dimension in text representation models. we
switched the dimension size among {50, 100, 300,
500, 1000}, and    xed the other options as:    =
0.5, the    lter types in the id98 model including
{unigram, bigram, trigram} and 500    lters for each
type. figure 3 presents the amis from both id98
and lstm models. we found that 100 is the best
output dimension for both id98 and lstm models.
therefore, we set the output dimension as 100 in the
following experiments.

second, we studied the effect of    in eq.

(4),
which tunes the importance of unlabeled data. we
varied    among {0.00001, 0.0001, 0.001, 0.01, 0.1},
and remain the other options as the last experi-
ment. figure 4 shows the amis from both id98 and
lstm models. we found that the id91 per-
formance is not good when using a very small   .

	
   	
   num_dim	
   	
   	
   alpha	
   	
   	
   	
   0.2 0.25 0.3 0.35 0.4 0.45 50 100 300 500 1000 ami id98 lstm 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.00001 0.0001 0.001 0.01 0.1 ami id98 lstm 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 ami id98 lstm figure 4: in   uence of unlabeled data, where the x-axis is    in
eq. (4).

figure 6: in   uence of the pre-training strategy.

labels in the dataset. we then trained the model
through the classi   cation task using all labeled data.
after this process, we removed the top layer, and
used the remaining parameters to initialize our id98
and lstm models. the performance for our mod-
els with and without pre-training strategy are given
in figure 6. we can see that the pre-training strat-
egy is quite effective for our models. therefore, we
use the pre-training strategy in the following experi-
ments.

4.3 comparing with other models
in this subsection, we compared our method with
some representative systems. we implemented a se-
ries of id91 systems. all of these systems are
based on the id116 algorithm, but they represent
short texts differently:

bow represents each text as a bag-of-words vector.

tf-idf represents each text as a tf-idf vector.

average-vec represents each text with the average

of all word vectors within the text.

metric-learn-bow employs

the metric learning
method proposed by weinberger et al. (2005),
and learns to project a bag-of-words vector
into a 300-dimensional vector based on labeled
data.

metric-learn-idf uses the same metric learning
method, and learns to map a tf-idf vector

figure 5: in   uence of labeled data, where the x-axis is the ratio
of data with given labels.

by increasing the value of   , we acquired progres-
sive improvements, and reached to the peak point
at   =0.01. after that, the performance dropped.
therefore, we choose   =0.01 in the following ex-
periments. this results also indicate that the un-
labeled data are useful for the text representation
learning process.

third, we tested the in   uence of the size of la-
beled data. we tuned the ratio of labeled instances
from the whole dataset among [1%, 10%], and kept
the other con   gurations as the previous experiment.
the amis are shown in figure 5. we can see that the
more labeled data we use, the better performance we
get. therefore, the labeled data are quite useful for
the id91 process.

fourth, we checked the effect of the pre-training
strategy for our models. we added a softmax layer
on top of our id98 and lstm models, where the
size of the output layer is equal to the number of

	
   	
   num_dim	
   	
   	
   alpha	
   	
   	
   	
   0.2 0.25 0.3 0.35 0.4 0.45 50 100 300 500 1000 ami id98 lstm 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.00001 0.0001 0.001 0.01 0.1 ami id98 lstm 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 ami id98 lstm 	
   	
   num_dim	
   	
   	
   alpha	
   	
   	
   	
   0.2 0.25 0.3 0.35 0.4 0.45 50 100 300 500 1000 ami id98 lstm 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.00001 0.0001 0.001 0.01 0.1 ami id98 lstm 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 ami id98 lstm ratio	
   	
   	
   	
   pretrian	
   0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 ami acc unsup.

sup.

semisup.

bow
tf-idf
average-vec
metric-learn-bow
metric-learn-idf
metric-learn-ave-vec
id98-classi   er
id98-represent.
lstm-classi   er
lstm-represent.
semi-id98
semi-lstm

dbpedia

ag news

question type
yahoo answer
ami acc ami acc ami acc ami acc
0.140
0.028
0.031
0.145
0.222
0.135
0.329
0.104
0.114
0.368
0.400
0.304
0.501
0.511
0.334
0.442
0.482
0.512
0.272
0.421
0.554
0.529
0.492
0.337

0.257
0.259
0.356
0.380
0.379
0.553
0.771
0.618
0.741
0.618
0.739
0.712

0.029
0.168
0.457
0.459
0.443
0.606
0.554
0.604
0.524
0.535
0.662
0.599

0.578
0.558
0.610
0.808
0.821
0.829
0.879
0.864
0.862
0.667
0.894
0.788

0.019
0.023
0.077
0.125
0.150
0.221
0.285
0.210
0.283
0.152
0.338
0.187

0.311
0.449
0.737
0.776
0.765
0.851
0.771
0.833
0.763
0.771
0.876
0.830

0.546
0.527
0.619
0.854
0.876
0.879
0.938
0.899
0.928
0.706
0.945
0.802

table 4: performance of all systems on each dataset.

figure 7: id167 visualizations of id91 results.

************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************(a) metric-learn-ave-vec************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************(b) id98-represent.************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************(c) semi-lstm************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************(d) semi-id98into a 300-dimensional vector based on labeled
data.

metric-learn-ave-vec also uses the metric learning
method, and learns to project an averaged word
vector into a 100-dimensional vector based on
labeled data.

we designed two classi   ers (id98-classi   er and
lstm-classi   er) by adding a softmax layer on top
of our id98 and lstm models. we trained these
two classi   ers with labeled data, and utilized them
to predict labels for unlabeled data. we also built
two text representation models (   id98-represent.    and
   lstm-represent.   ) by setting parameters of our id98
and lstm models with the corresponding parame-
ters in id98-classi   er and lstm-classi   er. then, we
used them to represent short texts into vectors, and
applied the id116 algorithm for id91.

table 4 summarizes the results of all systems
on each dataset, where    semi-id98    is our semi-
supervised id91 algorithm with the id98
model, and    semi-lstm    is our semi-supervised clus-
tering algorithm with the lstm model. we grouped
all the systems into three categories: unsupervised
(unsup.), supervised (sup.), and semi-supervised
(semisup.) 2. we found that the supervised systems
worked much better than the unsupervised counter-
parts, which implies that the small amount of labeled
data is necessary for better performance. we also
noticed that within the supervised systems, the sys-
tems using deep learning (id98 or lstm) models
worked better than the systems using metric learn-
ing method, which shows the power of deep learn-
ing models for short text modeling. our    semi-id98   
system got the best performance on almost all the
datasets.

figure 7 visualizes id91 results on the ques-
tion type dataset from four representative systems.
in figure 7(a), clusters severely overlap with each
other. when using the id98 sentence representa-
tion model, we can clearly identify all clusters in
figure 7(b), but the boundaries between clusters are

2all id91 systems are based on the same number of
instances (total# in table 3). for the semi-supervised and su-
pervised systems, the labels for 1% of the instances are given
(labeled# in table 3). and the evaluation was conducted only
on the unlabeled portion.

still obscure. the id91 results from our semi-
supervised id91 algorithm are given in figure
7(c) and figure 7(d). we can see that the boundaries
between clusters become much clearer. therefore,
our algorithm is very effective for short text cluster-
ing.

5 related work

semi-supervised
into two categories:

id91 methods
existing
constraint-based and
fall
representation-based. in constraint-based methods
(davidson and basu, 2007), some labeled informa-
tion is used to constrain the id91 process. in
representation-based methods (bair, 2013), a repre-
sentation model is    rst trained to satisfy the labeled
information, and all data points are clustered based
on representations from the representation model.
bilenko et al. (2004) proposed to integrate there two
methods into a uni   ed framework, which shares
the same idea of our proposed method. however,
they only employed the metric learning model for
representation learning, which is a linear projection.
whereas, our method utilized deep learning models
to learn representations in a more    exible non-linear
space. xu et al. (2015) also employed deep learning
models for short text id91. however, their
method separated the representation learning pro-
cess from the id91 process, so it belongs to the
representation-based method. whereas, our method
combined the representation learning process and
the id91 process together, and utilized both
labeled data and unlabeled data for representation
learning and id91.

6 conclusion

in this paper, we proposed a semi-supervised clus-
tering algorithm for short texts. we utilized deep
learning models to learn representations for short
texts, and employed a small amount of labeled data
to specify our intention for id91. we integrated
the representation learning process and the cluster-
ing process into a uni   ed framework, so that both
of the two processes get some bene   ts from labeled
data and unlabeled data. experimental results on
four datasets show that our method is more effective
than other competitors.

[lehmann et al.2014] jens lehmann, robert isele, max
jakob, anja jentzsch, dimitris kontokostas, pablo n
mendes, sebastian hellmann, mohamed morsey,
patrick van kleef, s  oren auer, et al. 2014. dbpedia-
a large-scale, multilingual knowledge base extracted
from wikipedia. semantic web journal, 5:1   29.

[li and roth2002] xin li and dan roth. 2002. learn-
ing question classi   ers. in proceedings of the 19th in-
ternational conference on computational linguistics-
volume 1, pages 1   7. association for computational
linguistics.

[macqueen1967] james macqueen. 1967. some meth-
ods for classi   cation and analysis of multivariate ob-
servations. in proceedings of the    fth berkeley sympo-
sium on mathematical statistics and id203, vol-
ume 1, pages 281   297. oakland, ca, usa.

[mikolov et al.2013] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013. ef   cient estimation
of word representations in vector space. arxiv preprint
arxiv:1301.3781.

[munkres1957] james munkres. 1957. algorithms for
the assignment and transportation problems. journal
of the society for industrial and applied mathematics,
5(1):32   38.

[vinh et al.2009] nguyen xuan vinh, julien epps, and
james bailey. 2009. information theoretic measures
for id91s comparison: is a correction for chance
in proceedings of the 26th annual in-
necessary?
ternational conference on machine learning, pages
1073   1080. acm.

[weinberger et al.2005] kilian q weinberger,

john
blitzer, and lawrence k saul.
2005. distance
metric learning for large margin nearest neighbor
in advances in neural information
classi   cation.
processing systems, pages 1473   1480.

[xu et al.2015] jiaming xu, peng wang, guanhua tian,
bo xu, jun zhao, fangyuan wang, and hongwei hao.
2015. short text id91 via convolutional neural
networks. in proceedings of naacl-hlt, pages 62   
69.

[zhang and lecun2015] xiang zhang and yann lecun.
2015. text understanding from scratch. arxiv preprint
arxiv:1502.01710.

references
[amig  o et al.2009] enrique amig  o, julio gonzalo, javier
artiles, and felisa verdejo. 2009. a comparison of
extrinsic id91 id74 based on formal
constraints. information retrieval, 12(4):461   486.

[arthur and vassilvitskii2007] david arthur and sergei
vassilvitskii. 2007. id116++: the advantages of
careful seeding. in proceedings of the eighteenth an-
nual acm-siam symposium on discrete algorithms,
pages 1027   1035. society for industrial and applied
mathematics.

[bair2013] eric bair. 2013. semi-supervised id91
methods. wiley interdisciplinary reviews: computa-
tional statistics, 5(5):349   361.

[banerjee et al.2007] somnath banerjee, krishnan ra-
id91
manathan, and ajay gupta.
in proceedings of the
short texts using wikipedia.
30th annual international acm sigir conference on
research and development in information retrieval,
pages 787   788. acm.

2007.

[bilenko et al.2004] mikhail bilenko, sugato basu, and
raymond j mooney. 2004.
integrating constraints
and metric learning in semi-supervised id91. in
proceedings of the twenty-   rst international confer-
ence on machine learning, page 11. acm.

[bishop2006] christopher m bishop.

2006. pattern

recognition and machine learning. springer.

[davidson and basu2007] ian davidson and sugato basu.
2007. a survey of id91 with instance level con-
straints. acm transactions on knowledge discovery
from data, 1:1   41.

[dhillon and guan2003] inderjit s. dhillon and yuqiang
information theoretic id91 of
pages 517   520. ieee

guan.
sparse co-occurrence data.
computer society.

2003.

[fodeh et al.2011] samah fodeh, bill punch, and pang-
ning tan. 2011. on ontology-driven document clus-
tering using core semantic features. knowledge and
information systems, 28(2):395   421.

[graves2012] alex graves. 2012. supervised sequence
labelling with recurrent neural networks, volume 385.
springer.

[hinton and salakhutdinov2006] geoffrey e hinton and
ruslan r salakhutdinov.
2006. reducing the di-
mensionality of data with neural networks. science,
313(5786):504   507.

[kim2014] yoon kim. 2014. convolutional neural net-
in proceedings of
works for sentence classi   cation.
the 2014 conference on empirical methods in natural
language processing, pages 1746   1751.

[kingma and ba2014] diederik kingma and jimmy ba.
2014. adam: a method for stochastic optimization.
arxiv preprint arxiv:1412.6980.

