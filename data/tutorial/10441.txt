authorship attribution using a neural network language model

zhenhao ge, yufang sun and mark j.t. smith

school of electrical and computer engineering, purdue university
465 northwestern ave, west lafayette, indiana, usa, 47907-2035
emails: {zge, sun361, mjts}@purdue.edu, phone: +1 (317) 457-9348

6
1
0
2

 

b
e
f
7
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
2
9
2
5
0

.

2
0
6
1
:
v
i
x
r
a

abstract

in practice, training language models for individual authors
is often expensive because of limited data resources. in such
cases, neural network language models (nnlms), gener-
ally outperform the traditional non-parametric id165 mod-
els. here we investigate the performance of a feed-forward
nnlm on an authorship attribution problem, with moderate
author set size and relatively limited data. we also consider
how the text topics impact performance. compared with a
well-constructed id165 baseline method with kneser-ney
smoothing, the proposed method achieves nearly 2.5% re-
duction in perplexity and increases author classi   cation ac-
curacy by 3.43% on average, given as few as 5 test sen-
tences. the performance is very competitive with the state
of the art in terms of accuracy and demand on test data.
the source code, preprocessed datasets, a detailed descrip-
tion of the methodology and results are available at https:
//github.com/zge/authorship-attribution.

introduction

authorship attribution refers to identifying authors from
given texts by their unique textual features. it is challenging
since the author   s style may vary from time to time by topics,
mood and environment. many methods have been explored
to address this problem, such as id44
for id96 (seroussi, zukerman, and bohnert 2011)
and naive bayes for text classi   cation (coyotl-morales et
al. 2006). regarding id38 methods, there
is mixed advocacy for the conventional id165 methods
(ke  selj et al. 2003) and methods using more compact and
distributed representations, like neural network language
models (nnlms), which was claimed to capture semantics
better with limited training data (bengio et al. 2003).

most nnlm toolkits available (mikolov et al. 2010) are
designed for recurrent nnlms which are better for captur-
ing complex and longer text patterns and require more train-
ing data. in contrast, the feed-forward nnlm framework
we proposed is less computationally expensive and more
suitable for id38 with limited data. it is devel-
oped in matlab with full network tuning functionalities.
the database we use is composed of transcripts of 16
video courses taken from coursera, collected one sentence
per line into a text    le for each course. to reduce the in   u-
ence of    topic    on author/instructor classi   cation, courses
were all selected from science and engineering    elds, such
copyright c(cid:13) 2015, association for the advancement of arti   cial
intelligence (www.aaai.org). all rights reserved.

as algorithm, dsp, data mining, it, machine learning,
nlp, etc. there are 8000+ sentences/course and about 20
words/sentence on average. the vocabulary size of each
author varies from 3000 to 9000. after id30 with
porter   s algorithm and pruning words with frequency less
than 1/100, 000, author vocabulary size is reduced to a range
from 1800 to 2700, with average size around 2000. fig. 1
shows the vocabulary size for each course, under various
conditions and the database coverage with the most frequent
k words (k = 500, 1000, 2000) after id30 and pruning.

fig. 1: vocabulary size and word coverage in various stages

neural network language model (nnlm)

similar to id165 methods, the nnlm is also used to an-
swer one of the fundamental questions in language model-
ing: predicting the best target word w   , given a context of
n     1 words. the target word is typically the last word
within context size n. however, it theoretically can be in
any position. fig. 2 demonstrates the structure of the pro-
posed nnlm with multinomial classi   cation cost function:

tj log yj, j     v,

(1)

c =    (cid:88)

v

where v is the vocabulary size, yj and tj are the    nal out-
put and the target label. this nnlm setup contains 4 types
of layers. the word layer contains n     1 input words rep-
resented by v -dimensional index vectors with v     1    0   s
and one    1    positioned in a different location to differenti-
ate it from all other words. words are then transformed to
their distributed representation and concatenated in the em-
bedding layer. outputs from this layer forward propagate to
the hidden sigmoid layer, then softmax layer to predict the
probabilities of the possible target words. weights/biases
between layers are initiated randomly and with zeros re-
spectively, and their error derivatives are computed through
backward propagation. the network is iteratively updated
with parameters, such as learning rate and momentum.

02468101214160500010000dataset index (c)vocabulary size (v)vocabulary size for each dataset  voriginalvstemmedvstemmed   pruned02468101214160.80.850.90.951dataset index (c)database coverage (dc)database coverage from most frequent k words for each datasetstemmed & pruned datasets, k = 500, 1000, 2000  dc2000dc1000dc500fig. 2: a feed-forward nnlm setup (i: index, w: word,
n: number of context words, w : weight, b: bias)

in implementation, the processed text data for each course
are randomly split into training, validation, and test sets with
ratio 8:1:1. this segmentation is performed 10 times with
different randomization seeds, so the mean/variance of per-
formance of nnlms can be measured later. we optimized
a 4-gram nnlm (predicting the 4th word using the previous
3) with mini-batch training through 10 to 20 epochs for each
course. the model parameters, such as number of nodes in
each layer, learning rate, and momentum are customized for
obtaining the best individual models.

classi   cation with perplexity measurement
1 as a word sequence (w1,w2, . . . ,wn ) and
denote w n
p (w n
1 given a lm, perplexity
is an intrinsic measurement of the lm    tness de   ned by:

1 ) as the id203 of w n

p p (w n

1 ) = p (w n
using markov chain theory, p (w n
by the id203 of the closest n words p (w n
n(cid:89)
p p (w n

1 ) can be approximated by

1 )    1
(2)
1 ) can be approximated
n   n +1), so

n

p (wk|w k   1

k   n +1))   1/n

p p (w n

n   n +1) = (

(3)

k=1

the mean perplexity of applying trained 4-gram nnlms
to their corresponding test sets are 67.3    2.4. this is lower
(better) than the traditional id165 method (69.0   2.4 with
4-gram srilm). the classi   cation is performed by    nding
the author with his/her nnlm that maximizes the accumu-
lative perplexity of the test sentences. by randomly select-
ing 1 to 20 test sentences from the test set, fig. 3 shows
the 16-way classi   cation accuracy using 3 methods, for one
particular course/instructor and for all courses on average.
there are 2 courses taught by the same instructor, intention-
ally added for investigating the topic impact on accuracy.
they are excluded when computing the average accuracy
in fig. 3. similarly, the accuracies for courses using two
methods with differing text lengths are compared in fig. 4.
both    gures show the nnlm method is slightly better than
the sri baselines at the 4-gram level. a classi   cation con-
fusion matrix (not included due to space limits) was also
computed to show the similarity between authors. the re-
sults show higher confusion on similar courses, which indi-
cates the topic does impact accuracy. the nnlm has higher
confusion values than the sri baseline on the two different
courses from the same instructor, so it is more biased toward
the author rather than the topic in that sense.

fig. 3: individual and mean accuracies vs.
terms of the number of sentences

text length in

fig. 4: accuracies at 3 stages differed by text length for 14
courses (2 courses from the same instructor are excluded)

.

conclusion and future work

the nnlm-based work achieves promising results com-
pared with the id165 baseline. the nearly perfect accura-
cies given 10+ test sentences are competitive with the state-
of-the-art, which achieved 95%+ accuracy on a similar au-
thor size (coyotl-morales et al. 2006), or 80%+ with tens of
authors and limited training data (seroussi, zukerman, and
bohnert 2011). however, it may also indicate this dataset is
not suf   ciently challenging, probably due to the training and
test data consistency and the topic distinction. in the future,
datasets with more authors can be used, for example, taken
from collections of books or transcribed speeches. we also
plan to integrate a nonlinear function optimization scheme
using the conjugate gradient (rasmussen 2006), which auto-
matically selects the best training parameters and saves time
in model customization. to compensate for the relatively
small size of the training set, lms may also be trained with
a group of authors and then adapted to the individuals.

references

[bengio et al. 2003] bengio, y.; ducharme, r.; vincent, p.; and
janvin, c. 2003. a neural probabilistic language model. the
journal of machine learning research 3:1137   1155.
[coyotl-morales et al. 2006] coyotl-morales, r. m.; villase  nor-
pineda, l.; montes-y g  omez, m.; and rosso, p. 2006. authorship
attribution using word sequences. in progress in pattern recogni-
tion, image analysis and applications. springer.
[ke  selj et al. 2003] ke  selj, v.; peng, f.; cercone, n.; and thomas,
c. 2003. id165-based author pro   les for authorship attribution.
in pacling.
[mikolov et al. 2010] mikolov, t.; kara     at, m.; burget, l.; cer-
nock`y, j.; and khudanpur, s. 2010. recurrent neural network
based language model. in interspeech 2010.
[rasmussen 2006] rasmussen, c. e. 2006. gaussian processes for
machine learning.
[seroussi, zukerman, and bohnert 2011] seroussi, y.; zukerman,
i.; and bohnert, f. 2011. authorship attribution with latent dirich-
let allocation. in conll.

index  of  context  word	(cid:1861)	(cid:1835)(cid:4666)(cid:1875)(cid:3036)(cid:4667)index  of  context  word	(cid:1840)	(cid:1835)(cid:4666)(cid:1875)(cid:3015)(cid:4667)index  of  context  word	1	(cid:1835)(cid:4666)(cid:1875)(cid:2869)(cid:4667)index  of  context  word	2	(cid:1835)(cid:4666)(cid:1875)(cid:2870)(cid:4667)      hidden  layer  (sigmoid)(cid:1849)(cid:2933)(cid:2925)(cid:2928)(cid:2914)(cid:2879)(cid:2915)(cid:2923)(cid:2912)(cid:1849)(cid:2933)(cid:2925)(cid:2928)(cid:2914)(cid:2879)(cid:2915)(cid:2923)(cid:2912)(cid:1849)(cid:2933)(cid:2925)(cid:2928)(cid:2914)(cid:2879)(cid:2915)(cid:2923)(cid:2912)(cid:1849)(cid:2933)(cid:2925)(cid:2928)(cid:2914)(cid:2879)(cid:2915)(cid:2923)(cid:2912)(cid:1849)(cid:2915)(cid:2923)(cid:2912)(cid:2879)(cid:2918)(cid:2919)(cid:2914)(cid:1854)(cid:2918)(cid:2919)(cid:2914)(cid:2914)(cid:2915)(cid:2924)output  layer  (softmax)(cid:1849)(cid:2918)(cid:2919)(cid:2914)(cid:2879)(cid:2925)(cid:2931)(cid:2930)(cid:1854)(cid:2925)(cid:2931)(cid:2930)(cid:2926)(cid:2931)(cid:2930)embedding  layerrepresentation  of  word	(cid:1861)representation  of  word	(cid:1840)representation  of  word	1representation  of  word	2      index  of  target  word	(cid:1872)(cid:1835)(cid:1875)(cid:3047),(cid:1872)   1,(cid:1840),(cid:1872)(cid:3405)(cid:1861)24681012141618200.40.60.81no. of sentencesavgerage accuracy1   of   16 classfication accuracy vs. text length, course id: 3  unigram (sri)4gram (sri)4gram (nnlm)24681012141618200.60.70.80.91no. of sentencescourse avgerage accuracy1   of   16 course average classfication accuracy vs. text length  4gram (sri)4gram (nnlm)0510150.60.81dataset index (c)accuracysri 4   gram  10 sentences5 sentences1 sentence0510150.60.81dataset index (c)accuracynnlm 4   gram  