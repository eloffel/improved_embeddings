6
1
0
2

 

b
e
f
0
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
3
8
4
3
0

.

2
0
6
1
:
v
i
x
r
a

learning distributed representations of sentences from unlabelled data

felix hill

computer laboratory

university of cambridge

felix.hill@cl.cam.ac.uk

kyunghyun cho
courant institute of

mathematical sciences

& centre for data science

new york university

kyunghyun.cho@nyu.edu

anna korhonen

department of theoretical

& applied linguistics
university of cambridge

alk23@cam.ac.uk

abstract

unsupervised methods for learning distributed
representations of words are ubiquitous in to-
day   s nlp research, but far less is known
about the best ways to learn distributed phrase
or sentence representations from unlabelled
data. this paper is a systematic comparison
of models that learn such representations. we
   nd that the optimal approach depends crit-
ically on the intended application. deeper,
more complex models are preferable for rep-
resentations to be used in supervised sys-
tems, but shallow id148 work best
for building representation spaces that can
be decoded with simple spatial distance met-
rics. we also propose two new unsupervised
representation-learning objectives designed to
optimise the trade-off between training time,
domain portability and performance.

1 introduction

distributed representations - dense real-valued vec-
tors that encode the semantics of linguistic units
- are ubiquitous in today   s nlp research.
for
single-words or word-like entities, there are estab-
lished ways to acquire such representations from
naturally occurring (unlabelled) training data based
on comparatively task-agnostic objectives (such as
predicting adjacent words). these methods are
well understood empirically (baroni et al., 2014b)
and theoretically (levy and goldberg, 2014). the
best word representation spaces re   ect consistently-
observed aspects of human conceptual organisa-
tion (hill et al., 2015b), and can be added as features

to improve the performance of numerous language
processing systems (collobert et al., 2011).

by contrast, there is comparatively little consen-
sus on the best ways to learn distributed represen-
tations of phrases or sentences.1 with the advent
of deeper language processing techniques, it is rel-
atively common for models to represent phrases or
sentences as continuous-valued vectors. examples
include machine translation (sutskever et al., 2014),
image captioning (mao et al., 2015) and dialogue
systems (serban et al., 2015). while it has been ob-
served informally that the internal sentence repre-
sentations of such models can re   ect semantic in-
tuitions (cho et al., 2014), it is not known which ar-
chitectures or objectives yield the    best    or most use-
ful representations. resolving this question could
ultimately have a signi   cant impact on language
processing systems. indeed, it is phrases and sen-
tences, rather than individual words, that encode the
human-like general world knowledge (or    common
sense   ) (norman, 1972) that is a critical missing part
of most current language understanding systems.

we address this issue with a systematic compari-
son of cutting-edge methods for learning distributed
representations of sentences. we constrain our com-
parison to methods that do not require labelled data
gathered for the purpose of training models, since
such methods are more cost-effective and applica-
ble across languages and domains. we also propose
two new phrase or sentence representation learn-
ing objectives - sequential denoising autoencoders

1see

the

contrasting

(mitchell and lapata, 2008;
baroni et al., 2014a; milajevs et al., 2014) among others.

conclusions

in
clark and pulman, 2007;

(sdaes) and fastsent, a sentence-level log-linear
bag-of-words model. we compare all methods on
two types of task - supervised and unsupervised
evaluations - re   ecting different ways in which rep-
resentations are ultimately to be used. in the former
setting, a classi   er or regression model is applied
to representations and trained with task-speci   c la-
belled data, while in the latter, representation spaces
are directly queried using cosine distance.

we observe notable differences in approaches de-
pending on the nature of the evaluation metric. in
particular, deeper or more complex models (which
require greater time and resources to train) gener-
ally perform best in the supervised setting, whereas
shallow id148 work best on unsuper-
vised benchmarks. speci   cally, skipthought vec-
tors (kiros et al., 2015) perform best on the major-
ity of supervised evaluations, but sdaes are the top
performer on paraphrase identi   cation. in contrast,
on the (unsupervised) sick sentence relatedness
benchmark, fastsent, a simple, log-linear variant of
the skipthought objective, performs better than all
other models. interestingly, the method that exhibits
strongest performance across both supervised and
unsupervised benchmarks is a bag-of-words model
trained to compose id27s using dictio-
nary de   nitions (hill et al., 2015a). taken together,
these    ndings constitute valuable guidelines for the
application of phrasal or sentential representation-
learning to language understanding systems.

2 distributed sentence representations

to constrain the analysis, we compare neural lan-
guage models that compute sentence representations
from unlabelled, naturally-ocurring data, as with
the predominant methods for word representations.2
likewise, we do not focus on    bottom up    models
where phrase or sentence representations are built
from    xed mathematical operations on word vec-
tors (although we do consider a canonical case -
see cbow below); these were already compared
by milajevs et al. (2014). most space is devoted to
our novel approaches, and we refer the reader to the
original papers for more details of existing models.

2this

excludes

innovative

level
kalchbrenner et al., 2014) and many others.

architectures

including

supervised

sentence-
(socher et al., 2011;

2.1 existing models trained on text

skipthought vectors for consecutive sentences
si   1, si, si+1 in some document, the skipthought
model (kiros et al., 2015) is trained to predict target
sentences si   1 and si+1 given source sentence si.
as with all sequence-to-sequence models, in train-
ing the source sentence is    encoded    by a recurrent
neural network (id56) (with gated recurrent uu-
nits (cho et al., 2014)) and then    decoded    into the
two target sentences in turn.
importantly, because
id56s employ a single set of update weights at each
time-step, both the encoder and decoder are sensitive
to the order of words in the source sentence.

for each position in a target sentence st, the
decoder computes a softmax distribution over the
model   s vocabulary. the cost of a training exam-
ple is the sum of the negative log-likelihood of each
correct word in the target sentences si   1 and si+1.
this cost is backpropagated to train the encoder (and
decoder), which, when trained, can map sequences
of words to a single vector.

paragraphvector le and mikolov (2014) proposed
two id148 of sentence representation.
the dbow model learns a vector s for every sen-
tence s in the training corpus which, together with
id27s vw, de   ne a softmax distribution
optimised to predict words w     s given s. the
vw are shared across all sentences in the corpus.
in the dm model, k-grams of consecutive words
{wi . . . wi+k     s} are selected and s is combined
with {vwi . . . vwi+k } to make a softmax prediction
(parameterised by additional weights) of wi+k+1.

we used the gensim implementation,3 treating
each sentence in the training data as a    paragraph    as
suggested by the authors. during training, both dm
and dbow models store representations for every
sentence (as well as word) in the training corpus.
even on large servers it was therefore only possi-
ble to train models with representation size 200, and
dm models whose combination operation was aver-
aging (rather than concatenation).

bottom-up methods we train cbow and skip-
gram id27s (mikolov et al., 2013b) on
the books corpus, and compose by elementwise ad-

3https://radimrehurek.com/gensim/

to

we

also

compare

dition as proposed by mitchell and lapata (2010).4
c-
phrase (pham et al., 2015), an approach that
exploits a (supervised) parser to infer distributed
semantic representations based on a syntactic parse
of sentences. c-phrase achieves state-of-the-art
results for distributed representations on several
evaluations used in this study.5
non-distributed baseline we implement a tfidf
bow model in which the representation of sentence
s encodes the count in s of a set of feature-words
weighted by their t   df in c, the corpus. the feature-
words are the 200,000 most common words in c.

2.2 models trained on structured resources

the following models rely on (freely-available) data
that has more structure than raw text.

dictrep hill et al. (2015a) trained neural language
models to map dictionary de   nitions to pre-trained
id27s of the words de   ned by those def-
initions. they experimented with bow and id56
(with lstm) encoding architectures and variants
in which the input id27s were either
learned or pre-trained (+embs.)
to match the tar-
get id27s. we implement their models
using the available code and training data.6
captionrep using the same overall architecture,
we trained (bow and id56) models to map cap-
tions in the coco dataset (chen et al., 2015) to
pre-trained vector representations of images. the
image representations were encoded by a deep
convolutional network (szegedy et al., 2014) trained
on the ilsvrc 2014 object
recognition task
(russakovsky et al., 2014). multi-modal distributed
representations can be encoded by feeding test sen-
tences forward through the trained model.

id4 we consider the sentence representations
learned by neural mt models.
these models

code

4we also tried multiplication but this gave very poor results.
5since
is
publicly-
pre-trained model

for c-phrase
available

available we
(http://clic.cimec.unitn.it/composes/cphrase-vectors.html).
note this model is trained on 3   more text than others in this
study.

use

not

the

6https://www.cl.cam.ac.uk/  fh295/.

de   nitions
from the training data matching those in the id138 sts 2014
evaluation (used in this study) were excluded.

have identical architecture to skipthought, but are
trained on sentence-aligned translated texts. we
used a standard architecture (cho et al., 2014) on
all available en-fr and en-de data from the 2015
workshop on statistical mt (wmt).7

(denoising) autoencoders

2.3 novel text-based models
we introduce two new approaches designed to ad-
dress certain limitations with the existing models.
sequential
the
skipthought objective requires training text with
a coherent
inter-sentence narrative, making it
problematic to port to domains such as social media
or arti   cial
language generated from symbolic
knowledge. to avoid this restriction, we experiment
with a representation-learning objective based
on denoising autoencoders (daes).
in a dae,
high-dimensional input data is corrupted according
to some noise function, and the model is trained to
recover the original data from the corrupted version.
as a result of this process, daes learn to repre-
sent the data in terms of features that explain its
important factors of variation (vincent et al., 2008).
transforming data into dae representations (as
a    pre-training    or initialisation step) gives more
robust (supervised) classi   cation performance in
deep feedforward networks (vincent et al., 2010).

the original daes were feedforward nets applied
to (image) data of    xed size. here, we adapt the ap-
proach to variable-length sentences by means of a
noise function n (s|po, px), determined by free pa-
rameters po, px     [0, 1]. first, for each word w in
s, n deletes w with (independent) id203 po.
then, for each non-overlapping bigram wiwi+1 in
s, n swaps wi and wi+1 with id203 px. we
then train the same lstm-based encoder-decoder
architecture as id4, but with the denoising objec-
tive to predict (as target) the original source sentence
s given a corrupted version n (s|po, px) (as source).
the trained model can then encode novel word se-
quences into distributed representations. we call
this model the sequential denoising autoencoder
(sdae). note that, unlike skipthought, sdaes can
be trained on sets of sentences in arbitrary order.

we label the case with no noise (i.e.

po =
px = 0 and n     id) sae. this set-

7www.statmt.org/wmt15/translation-task.html

ting matches the method applied to text classi-
   cation tasks by dai and le (2015). the    word
dropout    effect when po     0 has also been
used as a regulariser for deep nets in supervised
language tasks (iyyer et al., 2015), and for large
px the objective is similar to word-level    debag-
ging    (sutskever et al., 2011). for the sdae, we
tuned po, px on the validation set (see section 3.2).8
we also tried a variant (+embs) in which words are
represented by (   xed) pre-trained embeddings.

s(d)ae
paragraphvec
cbow
skipthought
fastsent
dictrep
captionrep
id4

s
o

o
r w

d
d w

s

2400
100
500
4800
100
500
500
2400

100
100
500
620
100
256
256
512

r
t

72*
4
2

336*

2
24*
24*
72*

e
t

640
1130
145
890
140
470
470
720

that

shows

fastsent the performance of skipthought vec-
tors
rich sentence semantics can
be inferred from the content of adjacent sen-
tences.
the model could be said to exploit
a type of sentence-level distributional hypothe-
sis (harris, 1954; polajnar et al., 2015). never-
theless,
like many deep neural language models,
skipthought is very slow to train (see table 1).
fastsent is a simple additive (log-linear) sentence
model designed to exploit
the same signal, but
at much lower computational expense. given a
bow representation of some sentence in context,
the model simply predicts adjacent sentences (also
represented as bow) .

more formally, fastsent learns a source uw and
target vw embedding for each word in the model vo-
cabulary. for a training example si   1, si, si+1 of
consecutive sentences, si is represented as the sum
of its source embeddings si = pw   si uw. the cost
of the example is then simply:

x

  (si, vw)

(1)

w   si   1   si+1

where   (v1, v2) is the softmax function.

we also experiment with a variant (+ae) in which
the encoded (source) representation must predict its
own words as target in addition to those of adjacent
sentences. thus in fastsent+ae, (1) becomes

table 1:
properties of models compared in this study
os: requires training corpus of sentences in order. r: requires
structured resource for training. wo: encoder sensitive to word
order. sd: dimension of sentence representation. wd: dimen-
sion of word representation. tr: approximate training time
(hours) on the dataset in this paper. * indicates trained on gpu.
te: approximate time (s) taken to encode 0.5m sentences.

2.4 training and model selection
unless stated above, all models were trained on
the toronto books corpus,9 which has the inter-
sentential coherence required for skipthought and
fastsent. the corpus consists of 70m ordered sen-
tences from over 7,000 books.

speci   cations of the models are shown in ta-
ble 1. the id148 (skipgram, cbow,
paragraphvec and fastsent) were trained for one
epoch on one cpu core. the representation di-
mension d for these models was found after tun-
ing d     {100, 200, 300, 400, 500} on the validation
set.10 all other models were trained on one gpu.
the s(d)ae models were trained for one epoch
(    8 days). the skipthought model was trained
for two weeks, covering just under one epoch.11 for
captionrep and dictrep, performance was mon-
itored on held-out training data and training was
stopped after 24 hours after a plateau in cost. the
id4 models were trained for 72 hours.

x

  (si, vw).

(2)

3 evaluating sentence representations

w   si   1   si   si+1

at test time the trained model (very quickly) en-
codes unseen word sequences into distributed rep-
resentations with s = pw   s uw.

8we searched po, px     {0.1, 0.2, 0.3} and observed best

results with po = px = 0.1.

in previous work, distributed representations of
language were evaluated either by measuring the
effect of adding representations as features in

9http://www.cs.toronto.edu/  mbweb/
10for paragraphvec only d     {100, 200} was possible due

to the high memory footprint.

11downloaded from https://github.com/ryankiros/skip-thoughts

   
   
   
   
   
   
   
   
   
   
dataset

sentence 1

news mexico wishes to guarantee citizens    safety.
forum the problem is simpler than that.
sts id138 a social set or clique of friends.
2014

twitter taking aim #stopgunviolence #congress #nra obama, gun policy and the n.r.a.
images a woman riding a brown horse.
a young girl riding a brown horse.
keita wins mali presidential election.
a man is jumping into a full pool.

iranians vote in presidential election.

sick (test+train) a lone biker is jumping in the air.

headlines

sentence 2

/5
4
mexico wishes to avoid more violence.
3.8
the problem is simple.
an unof   cial association of people or groups. 3.6
1.6
4.4
0.4
1.7

table 2: example sentence pairs and    similarity    ratings from the unsupervised evaluations used in this study.

-

judgements

(hill et al., 2015a;

some classi   cation task - supervised evaluation
(collobert et al., 2011;
mikolov et al., 2013a;
kiros et al., 2015) - or by comparing with human
relatedness
unspervised
evalu-
baroni et al., 2014b;
ation
the former setting re   ects
levy et al., 2015).
a scenario in which representations are used to
inject general knowledge (sometimes considered
as pre-training) into a supervised model.
the
latter pertains to applications in which the sentence
representation space is used for direct comparisons,
lookup or retrieval. here, we apply and compare
both evaluation paradigms.

3.1 supervised evaluations

representations are applied to 6 sentence classi-
   cation tasks: paraphrase identi   cation (msrp)
review sentiment
(dolan et al., 2004), movie
(mr)
(pang and lee, 2005),
product
reviews
(cr) (hu and liu, 2004), subjectivity classi   cation
(pang and lee, 2004),
(subj)
opinion polarity
(mpqa)
(wiebe et al., 2005) and question type
classi   cation (trec) (voorhees, 2002). we follow
the procedure (and code) of kiros et al. (2015): a
id28 classi   er is trained on top of sen-
tence representations, with 10-fold cross-validation
used when a train-test split is not pre-de   ned.

3.2 unsupervised evaluations

we also measure how well representation spaces
re   ect human intuitions of
the semantic sen-
tence relatedness, by computing the cosine dis-
tance between vectors for the two sentences in
each test pair, and correlating these distances
with gold-standard human judgements. the sick
dataset (marelli et al., 2014) consists of 10,000 pairs
of sentences and relatedness judgements. the sts
2014 dataset (agirre et al., 2014) consists of 3,750

pairs and ratings from six linguistic domains. exam-
ple ratings are shown in table 2. all available pairs
are used for testing apart from the 500 sick    trial   
pairs, which are held-out for tuning hyperparameters
(representation size of id148, and noise
parameters in sdae). the optimal settings on this
task are then applied to both supervised and unsu-
pervised evaluations.

4 results

performance of the models on the supervised eval-
uations (grouped according to the data required
by their objective) is shown in table 3. overall,
skipthought vectors perform best on three of the
six evaluations, the bow dictrep model with pre-
trained id27s performs best on two, and
the sdae on one. sdaes perform notably well on
the id141 task, going beyond skipthought
by three percentage points and approaching state-
of-the-art performance of models designed speci   -
cally for the task (ji and eisenstein, 2013). sdae
is also consistently better than sae, which aligns
with other    ndings that adding noise to aes pro-
duces richer representations (vincent et al., 2008).

results on the unsupervised evaluations are
shown in table 4. the same dictrep model per-
forms best on four of the six sts categories (and
overall) and is joint-top performer on sick. of
the models trained on raw text, simply adding
cbow word vectors works best on sts. the best
performing raw text model on sick is fastsent,
which achieves almost identical performance to c-
phrase   s state-of-the-art performance for a dis-
tributed model (pham et al., 2015). further, it uses
less than a third of the training text and does not
require access to (supervised) syntactic representa-
tions for training. together, the results of fastsent
on the unsupervised evaluations and skipthought

data

model

unordered
sentences

(toronto books:

70m sents,
0.9b words)

ordered
sentences

(toronto books)

other

structured

data

resource

2.8b words

sae
sae+embs.
sdae
sdae+embs.
paragraphvec dbow
paragraphvec dm
skipgram
cbow
unigram tfidf
skipthought
fastsent
fastsent+ae
id4 en to fr
id4 en to de
captionrep bow
captionrep id56
dictrep bow
dictrep bow+embs.
dictrep id56
dictrep id56+embs.
cphrase

msrp (acc / f1) mr
62.6
73.2
67.6
74.6
60.2
61.5
73.6
73.6
73.7
76.5
70.8
71.8
64.7
61.0
61.9
55.0
71.3
76.7
67.8
72.5
75.7

74.3 / 81.7
70.6 / 77.9
76.4 / 83.4
73.7 / 80.7
72.9 / 81.1
73.6 / 81.9
69.3 / 77.2
67.6 / 76.1
73.6 / 81.7
73.0 / 82.0
72.2 / 80.3
71.2 / 79.1
69.1 / 77.1
65.2 / 73.3
73.6 / 81.9
72.6 / 81.1
73.7 / 81.6
68.4 / 76.8
73.2 / 81.6
66.8 / 76.0
72.2 / 79.6

cr
68.0
75.3
74.0
78.0
66.9
68.6
77.3
7730
79.2
80.1
78.4
76.7
70.1
67.6
69.3
64.9
75.6
78.7
72.7
73.5
78.8

subj mpqa trec
80.2
86.1
80.4
89.8
77.6
89.3
90.8
78.4
59.4
76.3
55.8
76.4
82.2
89.2
89.1
82.2
85.0
90.3
92.2
93.6
76.8
88.7
80.4
88.8
82.8
84.9
81.6
78.2
72.2
77.4
62.4
64.9
86.6
73.8
90.7
81.0
75.8
81.4
72.0
85.6
91.1
78.8

76.8
86.2
81.3
86.9
70.7
78.1
85.0
85.0
82.4
87.1
80.6
81.5
81.5
72.9
70.8
71.0
82.5
87.2
82.5
85.7
86.2

table 3: performance of sentence representation models on supervised evaluations (section 3.1). bold numbers indicate best
performance in class. underlined indicates best overall.

on the supervised benchmarks provide strong sup-
port for the sentence-level distributional hypothe-
sis: the context in which a sentence occurs provides
valuable information about its semantics.

across both unsupervised and supervised evalua-
tions, the bow dictrep with pre-trained word em-
beddings exhibits by some margin the most con-
sistent performance. ths robust performance sug-
gests that dictrep representations may be particu-
larly valuable when the ultimate application is non-
speci   c or unknown, and con   rms that dictionary
de   nitions (where available) can be a powerful re-
source for representation learning.

dataset are determined by the language immediately
following the represented question (i.e.
the an-
swer) (voorhees, 2002). paraphrase detection, on
the other hand, may be better served by a model
that focused entirely on the content within a sen-
tence, such as sdaes. similar variation can be
observed in the unsupervised evaluations. for in-
stance, the (multimodal) representations produced
by the captionrep model do not perform particu-
larly well apart from on the image category of sts
where they beat all other models, demonstrating a
clear effect of the well-studied modality differences
in representation learning (bruni et al., 2014).

5 discussion

many additional conclusions can be drawn from the
results in tables 3 and 4.

different objectives yield different representa-
tions it may seem obvious, but the results con   rm
that different learning methods are preferable for
different intended applications (and this variation
appears greater than for word representations). for
instance, it is perhaps unsurprising that skipthought
performs best on trec because the labels in this

the nearest neighbours in table 5 give a more
concrete sense of the representation spaces. one
notable difference is between (ae-style) models
whose semantics come from within-sentence rela-
tionships (cbow, sdae, dictrep, paragraphvec)
and skipthought/fastsent, which exploit the con-
text around sentences.
in the former case, nearby
sentences generally have a high proportion of words
in common, whereas for the latter it is the general
concepts and/or function of the sentence that is sim-
ilar, and word overlap is often minimal.
indeed,

model

news
17/.16
sae
.52/.54
sae+embs.
.07/.04
sdae
sdae+embs.
.51/.54
paragraphvec dbow .31/.34
.42/.46
paragraphvec dm
skipgram
.56/.59
.57/.61
cbow
.48/.48
unigram tfidf
.44/.45
skipthought
.58/.59
fastsent
.56/ .59
fastsent+ae
.35/.32
id4 en to fr
.47/.43
id4 en to de
.26/.26
captionrep bow
captionrep id56
.05/.05
.62/.67
dictrep bow
.65/.72
dictrep bow+embs.
.40/.46
dictrep id56
dictrep id56+embs.
.51/.60
.69/.71
cphrase

sts 2014

forum id138 twitter
.28/.22
.12/.12
.60/.60
.22/.23
.11/.13
.44/.42
.57/.58
.29/.29
.43/.46
.32/.32
.54/.57
.33/.34
.71/.74
.42/.42
.71/.75
.43/.44
.63/.65
.40/.38
.42/.43
.14/.15
.41/.36
.63/.66
.41/.40
.70/.74
.55/.53
.18/.18
.49/.45
.26/.25
.37/.31
.29/.22
.13/.09
.36/.30
.62/.66
.42/.40
.67/.72
.49/.47
.42/.42
.26/.23
.29/.27
.44/.47
.60/.65
.43/.41

.30/.23
.60/.55
.33/.24
.56/.50
.53/.5
.51/.48
.73/.70
.72/.69
.60/.59
.39/.34
.74/.70
.69/.64
.47/.43
.34/.31
.50/.35
.40/.33
.81/.81
.85/.86
.78/.78
.80/.81
.76/.73

images headlines
.49/.46
. 64/.64
.44/.38
.59/.59
.46/.44
.32/.30
.65/.67
.71/.73
72/.74
.55/.60
.74/.78
.63/.65
.44/.45
.44/.43
.78/.81
.76/.82
.66/.68
.71/.74
.56/.56
.65/.70
.75/.79

.13/.11
.41/.41
.36/.36
.43/.44
.39/.41
.46/.47
.55/.58
.55/.59
.49/.49
.43/.44
.57/.59
.58/.60
.43/.43
.38/.37
.39/.36
.30/.28
.53/.58
.57/.61
.38/.40
.42/.46
.60/.65

sick

all

test + train

.12/.13
.42/.43
.17/.15
.37/.38
.42/.43
.44/.44
.62/.63
.64/.65
.58/.57
.27/.29
.63/.64
.62/.62
.43/.42
.40/.38
.46/.42
.39/.36
.62/.65
.67/.70
.49/.50
.54/.57
.65/.67

.32/.31
.47/.49
.46/.46
.46/.46
.42/.46
.44/.46
.60/.69
.60/.69
.52/.58
.57/.60
.61/.72
.60/.65
.47/.49
.46/46
.56/.65
.53/.62
.57/.66
.61/.70
.49/.56
.49/.59
.60/.72

table 4: performance of sentence representation models (spearman/pearson correlations) on unsupervised (relatedness) evalua-
tions (section 3.2). models are grouped according to training data as indicated in table 3.

this may be a more important trait of fastsent than
the marginal improvement on the sick task. read-
ers can compare the cbow and fastsent spaces at
http://45.55.60.98/.

in the unsupervised setting.

differences between supervised and unsuper-
vised performance many of the best performing
models on the supervised evaluations do not per-
form well
in the
skipthought, s(d)ae and id4 models, the cost is
computed based on a non-linear decoding of the in-
ternal sentence representations, so, as also observed
by (almahairi et al., 2015), the informative geome-
try of the representation space may not be re   ected
in a simple cosine distance. the id148
generally perform better in this unsupervised setting.

differences in resource requirements as shown in
table 1, different models require different resources
to train and use. this can limit their possible appli-
cations. for instance, while it was easy to make an
online demo for fast querying of near neighbours in
the cbow and fastsent spaces, it was not practical

for other models owing to memory footprint, encod-
ing time and representation dimension.

the role of word order is unclear the aver-
age scores of models that are sensitive to word
order (76.3) and of those that are not (76.6) are
approximately the same across supervised evalua-
tions. across the unsupervised evaluations, how-
ever, bow models score 0.55 on average compared
with 0.42 for id56-based (order sensitive) models.
this seems at odds with the widely held view that
word order plays an important role in determining
the meaning of english sentences. one possibility
is that order-critical sentences that cannot be dis-
ambiguated by a robust conceptual semantics (that
could be encoded in distributed lexical representa-
tions) are in fact relatively rare. however, it is also
plausible that current available evaluations do not
adequately re   ect order-dependent aspects of mean-
ing (see below). this latter conjecture is supported
by the comparatively strong performance of tfidf
bow vectors, in which the effective lexical seman-
tics are limited to simple relative frequencies.

query

cbow

skip
thought

fastsent

sdae

dictrep
(ff+embs.)
paragraph
vector (dm)

if he had a weapon, he could maybe take out
their last imp, and then beat up errol and vanessa.
then rob and i would duke it out, and every
once in a while, he would actually beat me.
if he could ram them from behind, send them saling over
the far side of the levee, he had a chance of stopping them.
isak   s close enough to pick off any one of them,
maybe all of them, if he had his ri   e and a mind to.
he   d even killed some of the most dangerous criminals
in the galaxy, but none of those men had gotten to him like vitktis.
kevin put a gun to the man   s head, but even though
he cried, he couldn   t tell kevin anything more.
i take a deep breath and open the doors.

an annoying buzz started to ring in my ears, becoming
louder and louder as my vision began to swim.
louder.

a weighty pressure landed on my lungs and my vision blurred
at the edges, threatening my consciousness altogether.
the noise grew louder, the quaking increased as the
sidewalk beneath my feet began to tremble even more.
i smile because i   m familiar with the knock,
pausing to take a deep breath before dashing down the stairs.
then gradually i began to hear a ringing in my ears.

they listened as the motorcycle-like roar
of an engine got louder and louder then stopped.

table 5: sample nearest neighbour queries selected from a randomly sampled 0.5m sentences of the toronto books corpus.

msrp
0.94 (6)

supervised (combined    = 0.90)
mr

cr

subj mpaq
0.85 (1)
0.86 (3)

0.85 (1)

0.86 (4)

trec
0.89 (5)

news
0.92 (4)

forum id138 twitter
0.92 (3)
0.93 (6)

0.92 (4)

images headlines all sts
0.95 (8)
0.91 (1)

0.92 (2)

sick
0.93 (7)

unsupervised (combined    = 0.93)

table 6: internal consistency (chronbach   s   ) among evaluations when individual benchmarks are left out of the (supervised or unsuper-
vised) cohorts. consistency rank within cohort is in parentheses (1 = most consistent with other evaluations).

the evaluations have limitations the internal con-
sistency (chronbach   s   ) of all evaluations consid-
ered together is 0.81 (just above    acceptable   ).12
table 6 shows that consistency is far higher (   ex-
cellent   ) when considering the supervised or unsu-
pervised tasks as independent cohorts. this indi-
cates that, with respect to common characteristics of
sentence representations, the supervised and unsu-
pervised benchmarks do indeed prioritise different
properties.
it is also interesting that, by this met-
ric, the properties measured by msrp and image-
caption relatedness are the furthest removed from
other evaluations in their respective cohorts.

while these consistency scores are a promis-
ing sign,
they could also be symptomatic of a
set of evaluations that are all limited in the same
way. the inter-rater agreement is only reported
for one of the 8 evaluations considered (mpqa,
0.72 (wiebe et al., 2005)), and for mr, subj and
trec, each item is only rated by one or two an-
notators to maximise coverage. table 2 illustrates
why this may be an issue for the unsupervised eval-
uations; the notion of sentential    relatedness    seems
very subjective.
it should be emphasised, how-
ever, that the tasks considered in this study are all
frequently used for evaluation, and, to our knowl-
edge, there are no existing benchmarks that over-

12wikipedia.org/wiki/cronbach   s_alpha

come these limitations.

6 conclusion

advances in deep learning algorithms, software and
hardware mean that many architectures and objec-
tives for learning distributed sentence representa-
tions from unlabelled data are now available to nlp
researchers. we have presented the    rst (to our
knowledge) systematic comparison of these meth-
ods. we showed notable variation in the perfor-
mance of approaches across a range of evaluations.
among other conclusions, we found that the op-
timal approach depends critically on whether rep-
resentations will be applied in supervised or unsu-
pervised settings - in the latter case, fast, shallow
bow models can still achieve the best performance.
further, we proposed two new objectives, fastsent
and sequential denoising autoencoders, which per-
form particularly well on speci   c tasks (msrp and
sick sentence relatedness respectively).13 if the ap-
plication is unknown, however, the best all round
choice may be dictrep: learning a mapping of pre-
trained id27s from the word-phrase sig-
nal in dictionary de   nitions. while we have focused
on models using naturally-occurring training data,

13we make all code for training and evaluating these new
models publicly available, together with pre-trained models and
an online demo of the fastsent sentence space.

in future work we will also consider supervised ar-
chitectures (including convolutional, recursive and
character-level models), potentially training them on
multiple supervised tasks as an alternative way to
induce the    general knowledge    needed to give lan-
guage technology the elusive human touch.

acknowledgments

this work was supported by a google faculty
award to ak and fh and a google european doc-
toral fellowship to fh. thanks also to marek rei,
tamara polajnar, laural rimell, jamie ryan kiros
and piotr bojanowski for helpful discussion and
comments.

references

[agirre et al.2014] eneko agirre, carmen banea, claire
cardie, daniel cer, mona diab, aitor gonzalez-
agirre, weiwei guo, rada mihalcea, german rigau,
and janyce wiebe. 2014. semeval-2014 task 10: mul-
tilingual semantic textual similarity. in proceedings of
the 8th international workshop on semantic evalua-
tion (semeval 2014), pages 81   91.

[almahairi et al.2015] amjad almahairi, kyle kastner,
kyunghyun cho, and aaron courville. 2015. learn-
ing distributed representations from reviews for col-
laborative    ltering.
in proceedings of the 9th acm
conference on recommender systems, pages 147   
154. acm.

[baroni et al.2014a] marco baroni, raffaela bernardi,
and roberto zamparelli.
2014a. frege in space:
a program of compositional id65.
linguistic issues in language technology, 9.

[baroni et al.2014b] marco baroni, georgiana dinu, and
germ  an kruszewski.
2014b. don   t count, pre-
dict! a systematic comparison of context-counting vs.
context-predicting semantic vectors.
in proceedings
of the 52nd annual meeting of the association for
computational linguistics, volume 1, pages 238   247.
[bruni et al.2014] elia bruni, nam-khanh tran, and
marco baroni. 2014. multimodal distributional se-
mantics. j. artif. intell. res. (jair), 49:1   47.

[chen et al.2015] xinlei chen, hao fang, tsung-yi lin,
ramakrishna vedantam, saurabh gupta, piotr dollar,
and c lawrence zitnick. 2015. microsoft coco cap-
tions: data collection and evaluation server. arxiv
preprint arxiv:1504.00325.

[cho et al.2014] kyunghyun cho, bart van merri  enboer,
caglar gulcehre, dzmitry bahdanau, fethi bougares,

holger schwenk, and yoshua bengio. 2014. learn-
ing phrase representations using id56 encoder-decoder
for id151. in proceedings of
emnlp.

[clark and pulman2007] stephen clark and stephen pul-
man. 2007. combining symbolic and distributional
models of meaning.
in aaai spring symposium:
quantum interaction, pages 52   55.

[collobert et al.2011] ronan collobert, jason weston,
l  eon bottou, michael karlen, koray kavukcuoglu,
and pavel kuksa. 2011. natural language process-
ing (almost) from scratch. the journal of machine
learning research, 12:2493   2537.

[dai and le2015] andrew m dai and quoc v le. 2015.
semi-supervised sequence learning.
in advances in
neural information processing systems, pages 3061   
3069.

[dolan et al.2004] bill dolan, chris quirk, and chris
brockett. 2004. unsupervised construction of large
paraphrase corpora: exploiting massively parallel
news sources. in proceedings of the 20th international
conference on computational linguistics, page 350.
association for computational linguistics.

[harris1954] zellig s harris. 1954. distributional struc-

ture. word.

[hill et al.2015a] felix hill, kyunghyun cho, anna ko-
rhonen, and yoshua bengio. 2015a. learning to un-
derstand phrases by embedding the dictionary. trans-
actions of the association for computational linguis-
tics.

[hill et al.2015b] felix hill, roi reichart, and anna ko-
rhonen. 2015b. siid113x-999: evaluating semantic
models with (genuine) similarity estimation. compu-
tational linguistics.

[hu and liu2004] minqing hu and bing liu. 2004. min-
ing and summarizing customer reviews. in proceed-
ings of the tenth acm sigkdd international confer-
ence on knowledge discovery and data mining, pages
168   177. acm.

[iyyer et al.2015] mohit iyyer, varun manjunatha, jordan
boyd-graber, and hal daum  e iii. 2015. deep un-
ordered composition rivals syntactic methods for text
classi   cation. proceedings of the 53rd annual meet-
ing of the association for computational linguistics.
[ji and eisenstein2013] yangfeng ji and jacob eisenstein.
2013. discriminative improvements to distributional
sentence similarity. in emnlp, pages 891   896.

[kalchbrenner et al.2014] nal kalchbrenner, edward
grefenstette, and phil blunsom. 2014. a convolu-
tional neural network for modelling sentences.
in
proceedings of emnlp.

[kiros et al.2015] ryan kiros, yukun zhu, ruslan r
salakhutdinov, richard zemel, raquel urtasun, an-
tonio torralba, and sanja fidler. 2015. skip-thought

vectors. in advances in neural information process-
ing systems, pages 3276   3284.

[le and mikolov2014] quoc v le and tomas mikolov.
2014. distributed representations of sentences and
documents. in proceedings of icml.

[levy and goldberg2014] omer levy and yoav gold-
berg. 2014. neural id27 as implicit ma-
trix factorization. in advances in neural information
processing systems, pages 2177   2185.

2015.

[levy et al.2015] omer levy, yoav goldberg, and ido
dagan.
improving distributional similarity
with lessons learned from id27s. transac-
tions of the association for computational linguistics,
3:211   225.

[mao et al.2015] junhua mao, wei xu, yi yang, jiang
wang, and alan yulle. 2015. deep captioning with
multimodal recurrent neural networks (m-id56).
in
proceedings of iclr.

[marelli et al.2014] marco marelli, stefano menini,
marco baroni, luisa bentivogli, raffaella bernardi,
and roberto zamparelli. 2014. a sick cure for the
evaluation of compositional distributional semantic
models.
in proceedings of lrec, pages 216   223.
citeseer.

[mikolov et al.2013a] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013a. ef   cient estima-
tion of word representations in vector space. arxiv
preprint arxiv:1301.3781.

[mikolov et al.2013b] tomas mikolov,

ilya sutskever,
kai chen, greg s corrado, and jeff dean. 2013b.
distributed representations of words and phrases and
their compositionality.
in advances in neural infor-
mation processing systems, pages 3111   3119.

[milajevs et al.2014] dmitrijs milajevs, dimitri kartsak-
lis, mehrnoosh sadrzadeh, and matthew purver. 2014.
evaluating neural word representations in tensor-based
compositional settings. in proceedings of emnlp.

[mitchell and lapata2008] jeff mitchell and mirella lap-
ata. 2008. vector-based models of semantic composi-
tion. in acl, pages 236   244.

[mitchell and lapata2010] jeff mitchell and mirella la-
pata. 2010. composition in distributional models of
semantics. cognitive science, 34(8):1388   1429.

[norman1972] donald a norman.

1972. memory,

knowledge, and the answering of questions.

[pang and lee2004] bo pang and lillian lee. 2004. a
sentimental education: id31 using sub-
jectivity summarization based on minimum cuts.
in
proceedings of the 42nd annual meeting on associa-
tion for computational linguistics, page 271. associ-
ation for computational linguistics.

[pang and lee2005] bo pang and lillian lee. 2005. see-
ing stars: exploiting class relationships for sentiment

in pro-
categorization with respect to rating scales.
ceedings of the 43rd annual meeting on association
for computational linguistics, pages 115   124. asso-
ciation for computational linguistics.

[pham et al.2015] nghia the pham, germ  an kruszewski,
angeliki lazaridou, and marco baroni. 2015. jointly
optimizing word representations for lexical and sen-
tential tasks with the c-phrase model. in proceedings
of alc.

[polajnar et al.2015] tamara polajnar, laura rimell, and
stephen clark. 2015. an exploration of discourse-
based sentence spaces for compositional distributional
semantics. in workshop on linking models of lexical,
sentential and discourse-level semantics (lsdsem),
page 1.

[russakovsky et al.2014] olga russakovsky, jia deng,
hao su, jonathan krause, sanjeev satheesh, sean
ma, zhiheng huang, andrej karpathy, aditya khosla,
michael bernstein, et al. 2014. id163 large scale
visual recognition challenge. international journal of
id161, pages 1   42.

[serban et al.2015] iulian v serban, alessandro sordoni,
yoshua bengio, aaron courville, and joelle pineau.
2015. building end-to-end dialogue systems using
generative hierarchical neural network models. arxiv
preprint arxiv:1507.04808.

[socher et al.2011] richard socher, jeffrey pennington,
eric h huang, andrew y ng, and christopher d man-
ning. 2011. semi-supervised recursive autoencoders
for predicting sentiment distributions. in proceedings
of the conference on empirical methods in natural
language processing, pages 151   161. association for
computational linguistics.

[sutskever et al.2011] ilya sutskever, james martens, and
geoffrey e hinton. 2011. generating text with recur-
rent neural networks. in proceedings of the 28th inter-
national conference on machine learning (icml-11),
pages 1017   1024.

[sutskever et al.2014] ilya sutskever, oriol vinyals, and
quoc vv le. 2014. sequence to sequence learning
with neural networks. in advances in neural informa-
tion processing systems, pages 3104   3112.

[szegedy et al.2014] christian

szegedy, wei

liu,
yangqing jia, pierre sermanet, scott reed, dragomir
anguelov, dumitru erhan, vincent vanhoucke, and
andrew rabinovich.
2014. going deeper with
convolutions. arxiv preprint arxiv:1409.4842.

[vincent et al.2008] pascal vincent, hugo larochelle,
yoshua bengio, and pierre-antoine manzagol. 2008.
extracting and composing robust features with denois-
ing autoencoders. in proceedings of the 25th interna-
tional conference on machine learning, pages 1096   
1103. acm.

[vincent et al.2010] pascal vincent, hugo larochelle, is-
abelle lajoie, yoshua bengio, and pierre-antoine
manzagol. 2010. stacked denoising autoencoders:
learning useful representations in a deep network with
a local denoising criterion. the journal of machine
learning research, 11:3371   3408.

[voorhees2002] ellen m voorhees. 2002. overview of
the trec 2001 id53 track. nist special
publication, pages 42   51.

[wiebe et al.2005] janyce wiebe, theresa wilson, and
claire cardie. 2005. annotating expressions of opin-
ions and emotions in language. language resources
and evaluation, 39(2-3):165   210.

