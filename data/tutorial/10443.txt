5
1
0
2

 

g
u
a
0
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
1
5
0
5
0

.

8
0
5
1
:
v
i
x
r
a

auto-sizingneuralnetworks:withapplicationstoid165languagemodelskentonmurrayanddavidchiangdepartmentofcomputerscienceandengineeringuniversityofnotredame{kmurray4,dchiang}@nd.eduabstractneuralnetworkshavebeenshowntoimproveperformanceacrossarangeofnatural-languagetasks.however,design-ingandtrainingthemcanbecomplicated.frequently,researchersresorttorepeatedexperimentationtopickoptimalsettings.inthispaper,weaddresstheissueofchoosingthecorrectnumberofunitsinhiddenlayers.weintroduceamethodforautomaticallyadjustingnetworksizebypruningouthiddenunitsthrough      ,1and   2,1id173.weapplythismethodtolanguagemodelinganddemonstrateitsabilitytocorrectlychoosethenumberofhiddenunitswhilemaintainingperplexity.wealsoincludethesemodelsinamachinetranslationdecoderandshowthatthesesmallerneuralmodelsmaintainthesignif-icantimprovementsoftheirunprunedver-sions.1introductionneuralnetworkshaveproventobehighlyef-fectiveatmanytasksinnaturallanguage.forexample,neurallanguagemodelsandjointlan-guage/translationmodelsimprovemachinetrans-lationqualitysigni   cantly(vaswanietal.,2013;devlinetal.,2014).however,neuralnetworkscanbecomplicatedtodesignandtrainwell.manyde-cisionsneedtobemade,andperformancecanbehighlydependentonmakingthemcorrectly.yettheoptimalsettingsarenon-obviousandcanbelaboriousto   nd,oftenrequiringanextensivegridsearchinvolvingnumerousexperiments.inthispaper,wefocusonthechoiceofthesizesofhiddenlayers.weintroduceamethodforautomaticallypruningouthiddenlayerunits,byaddingasparsity-inducingregularizerthaten-couragesunitstodeactivateifnotneeded,sothattheycanberemovedfromthenetwork.thus,af-tertrainingwithmoreunitsthannecessary,anet-workisproducedthathashiddenlayerscorrectlysized,savingbothtimeandmemorywhenactuallyputtingthenetworktouse.usinganeuralid165languagemodel(bengioetal.,2003),weareabletoshowthatournovelauto-sizingmethodisabletolearnmodelsthataresmallerthanmodelstrainedwithoutthemethod,whilemaintainingnearlythesameperplexity.themethodhasonlyasinglehyperparametertoadjust(asopposedtoadjustingthesizesofeachofthehiddenlayers),andwe   ndthatthesamesettingworksconsistentlywellacrossdifferenttrainingdatasizes,vocabularysizes,andid165sizes.inaddition,weshowthatincorporatingthesemod-elsintoamachinetranslationdecoderstillresultsinlargeid7pointimprovements.theresultisthatfewerexperimentsareneededtoobtainmod-elsthatperformwellandarecorrectlysized.2backgroundlanguagemodelsareoftenusedinnaturallan-guageprocessingtasksinvolvinggenerationoftext.forinstance,inmachinetranslation,thelan-guagemodelhelpstooutput   uenttranslations,andinspeechrecognition,thelanguagemodelhelpstodisambiguateamongpossiid7tterances.currentlanguagemodelsareusuallyid165models,whichlookattheprevious(n   1)wordstopredictthenthwordinasequence,basedon(smoothed)countsofid165scollectedfromtrainingdata.thesemodelsaresimplebutveryeffectiveinimprovingtheperformanceofnaturallanguagesystems.however,id165modelssufferfromsomelim-itations,suchasdatasparsityandmemoryusage.asanalternative,researchershavebegunexplor-ingtheuseofneuralnetworksforlanguagemod-eling.formodelingid165s,themostcommonapproachisthefeedforwardnetworkofbengioetal.(2003),showninfigure1.eachnoderepresentsaunitor   neuron,   whichhasarealvaluedactivation.theunitsareorga-nizedintoreal-vectorvaluedlayers.theactiva-tionsateachlayerarecomputedasfollows.(weassumen=3;thegeneralizationiseasy.)thetwoprecedingwords,w1,w2,aremappedintolower-dimensionalwordembeddings,x1=a:w1x2=a:w2thenpassedthroughtwohiddenlayers,y=f(b1x1+b2x2+b)z=f(cy+c)wherefisanelementwisenonlinearactivation(ortransfer)function.commonlyusedactivationfunctionsarethehyperbolictangent,logisticfunc-tion,andrecti   edlinearunits,tonameafew.fi-nally,theresultismappedviaasoftmaxtoanout-putid203distribution,p(wn|w1      wn   1)   exp([dz+d]wn).theparametersofthemodelarea,b1,b2,b,c,c,d,andd,whicharelearnedbyminimizingthenegativelog-likelihoodofthethetrainingdatausingstochasticgradientdescent(alsoknownasid26)orvariants.vaswanietal.(2013)showedthatthismodel,withsomeimprovements,canbeusedeffectivelyduringdecodinginmachinetranslation.inthispa-per,weuseandextendtheirimplementation.3methodsourmethodisfocusedonthechallengeofchoos-ingthenumberofunitsinthehiddenlayersofafeed-forwardneuralnetwork.thenetworksusedfordifferenttasksrequiredifferentnumbersofunits,andthelayersinasinglenetworkalsore-quiredifferentnumbersofunits.choosingtoofewunitscanimpairtheperformanceofthenetwork,andchoosingtoomanyunitscanleadtoover   t-ting.itcanalsoslowdowncomputationswiththenetwork,whichcanbeamajorconcernformanyapplicationssuchasintegratingneurallanguagemodelsintoamachinetranslationdecoder.ourmethodstartsoutwithalargenumberofunitsineachlayerandthenjointlytrainsthenet-workwhilepruningoutindividualunitswhenpos-sible.thegoalistoendupwithatrainednetworkwordsw1,w2inputembeddingsx1,x2hiddenyhiddenzoutputp(w3|w1w2)dcb1b2afigure1:neuralprobabilisticlanguagemodel(bengioetal.,2003),adaptedfromvaswanietal.(2013).thatalsohastheoptimalnumberofunitsineachlayer.wedothisbyaddingaregularizertotheob-jectivefunction.forsimplicity,considerasinglelayerwithoutbias,y=f(wx).letl(w)bethenegativelog-likelihoodofthemodel.insteadofminimizingl(w)alone,wewanttomini-mizel(w)+  r(w),wherer(w)isacon-vexregularizer.the   1norm,r(w)=   w   1=   i,j|wij|,isacommonchoiceforpushingpa-rameterstozero,whichcanbeusefulforprevent-ingover   ttingandreducingmodelsize.however,weareinterestednotonlyinreducingthenumberofparametersbutthenumberofunits.todothis,weneedadifferentregularizer.weassumeactivationfunctionsthatsatisfyf(0)=0,suchasthehyperbolictangentorrec-ti   edlinearunit(f(x)=max{0,x}).then,ifwepushtheincomingweightsofaunityitozero,thatis,wij=0forallj(aswellasthebias,ifany:bi=0),thenyi=f(0)=0isindependentofthepreviouslayersandcontributesnothingtosubse-quentlayers.sotheunitcanberemovedwithoutaffectingthenetworkatall.therefore,weneedaregularizerthatpushesalltheincomingconnec-tionweightstoaunittogethertowardszero.here,weexperimentwithtwo,the   2,1normandthe      ,1norm.1the   2,1normonama-1inthenotation   p,q,thesubscriptpcorrespondstothenormovereachgroupofparameters,andqcorrespondstothenormoverthegroupnorms.contrarytomorecommonusage,inthispaper,thegroupsarerows,notcolumns.x1x2x1x2   2      figure2:the(unsquared)   2normand      normbothhavesharptipsattheoriginthatencouragesparsity.trixwisr(w)=   i   wi:   2=   i         jw2ij      12.(1)(iftherearebiasesbi,theyshouldbeincludedaswell.)thisputsequalpressureoneachrow,butwithineachrow,thelargervaluescontributemore,andthereforethereismorepressureonlargerval-uestowardszero.the      ,1normisr(w)=   i   wi:      =   imaxj|wij|.(2)again,thisputsequalpressureoneachrow,butwithineachrow,onlythemaximumvalue(orval-ues)matter,andthereforethepressuretowardszeroisentirelyonthemaximumvalue(s).figure2visualizesthesparsity-inducingbehav-iorofthetworegularizersonasinglerow.bothhaveasharptipattheoriginthatencouragesalltheparametersinarowtobecomeexactlyzero.4optimizationhowever,thisalsomeansthatsparsity-inducingregularizersarenotdifferentiableatzero,mak-inggradient-basedoptimizationmethodstrickiertoapply.themethodsweusearediscussedindetailelsewhere(duchietal.,2008;duchiandsinger,2009);inthissection,weincludeashortdescriptionofthesemethodsforcompleteness.4.1proximalgradientmethodmostworkonlearningwithregularizers,includ-ingthiswork,canbethoughtofasinstancesoftheproximalgradientmethod(parikhandboyd,2014).ourobjectivefunctioncanbesplitintotwoparts,aconvexanddifferentiablepart(l)andaconvexbutnon-differentiablepart(  r).inprox-imalgradientdescent,wealternatebetweenim-provinglaloneand  ralone.letubethepa-rametervaluesfromthepreviousiteration.wecomputenewparametervalueswusing:v   u        l(u)(3)w   argmaxw   12     w   v   2+  r(w)   (4)andrepeatuntilconvergence.the   rstupdateisjustastandardgradientdescentupdateonl;thesecondisknownastheproximaloperatorfor  randinmanycaseshasaclosed-formsolution.intherestofthissection,weprovidesomejusti   ca-tionforthismethod,andinsections4.2and4.3weshowhowtocomputetheproximaloperatorforthe   2and      norms.wecanthinkofthegradientdescentupdate(3)onlasfollows.approximatelaroundubythetangentplane,  l(v)=l(u)+   l(u)(v   u)(5)andmovevtominimize  l,butdon   tmoveittoofarfromu;thatis,minimizef(v)=12     v   u   2+  l(v).settingpartialderivativestozero,weget   f   v=1  (v   u)+   l(u)=0v=u        l(u).byasimilarstrategy,wecanderivethesecondstep(4).againwewanttomovewtominimizetheobjectivefunction,butdon   twanttomoveittoofarfromu;thatis,wewanttominimize:g(w)=12     w   u   2+  l(w)+  r(w).notethatwehavenotapproximatedrbyatan-gentplane.wecansimplifythisbysubstitutingin(3).the   rsttermbecomes12     w   u   2=12     w   v        l(u)   2=12     w   v   2      l(u)(w   v)+  2      l(u)   2andthesecondtermbecomes  l(w)=l(u)+   l(u)(w   u)=l(u)+   l(u)(w   v        l(u)).the   l(u)(w   v)termscancelout,andwecanignoretermsnotinvolvingw,givingg(w)=12     w   v   2+  r(w)+const.whichisminimizedbytheupdate(4).thus,wehavesplittheoptimizationstepintotwoeasiersteps:   rst,dotheupdateforl(3),thendotheupdatefor  r(4).thelattercanoftenbedoneexactly(withoutapproximatingrbyatangentplane).weshownexthowtodothisforthe   2and      norms.4.2   2and   2,1id173sincethe   2,1normonmatrices(1)isseparableintothe   2normofeachrow,wecantreateachrowseparately.thus,forsimplicity,assumethatwehaveasinglerowandwanttominimizeg(w)=12     w   v   2+     w   +const.theminimumiseitheratw=0(thetipofthecone)orwherethepartialderivativesarezero(figure3):   g   w=1  (w   v)+  w   w   =0.clearly,wandvmusthavethesamedirectionanddifferonlyinmagnitude,thatis,w=  v   v   .sub-stitutingthisintotheaboveequation,wegetthesolution  =   v          .thereforetheupdateisw=  v   v     =max(0,   v          ).4.3      and      ,1id173asabove,sincethe      ,1normonmatrices(2)isseparableintothe      normofeachrow,wecantreateachrowseparately;thus,wewanttomini-mizeg(w)=12     w   v   2+  maxj|xj|+const.   w   >0   w   =0figure3:examplesofthetwopossiblecasesforthe   2gradientupdate.pointvisdrawnwithahol-lowdot,andpointwisdrawnwithasoliddot.before      prox.op.   1projectionfigure4:theproximaloperatorforthe      norm(withstrength    )decreasesthemaximalcompo-nentsuntilthetotaldecreasesumsto    .projec-tionontothe   1-ball(ofradius    )decreaseseachcomponentbyanequalamountuntiltheysumto    .intuitively,thesolutioncanbecharacterizedas:decreaseallofthemaximal|xj|untilthetotalde-creasereaches    orallthexjarezero.seefig-ure4.ifwepre-sortthe|xj|innonincreasingorder,it   seasytoseehowtocomputethis:for  =1,...,n,seeifthereisavalue     x  suchthatdecreasingallthex1,...,x  to  amountstoato-taldecreaseof    .thelargest  forwhichthisispossiblegivesthecorrectsolution.butthissituationseemssimilartoanotherop-timizationproblem,projectionontothe   1-ball,whichduchietal.(2008)solveinlineartimewithoutpre-sorting.infact,thetwoproblemscanbesolvedbynearlyidenticalalgorithms,becausetheyareconvexconjugatesofeachother(duchiandsinger,2009;bachetal.,2012).intuitively,the   1projectionofvisexactlywhatiscutoutbythe      proximaloperator,andviceversa(fig-ure4).duchietal.   salgoritid48odi   edforthepresentproblemisshownasalgorithm1.itpartitionsthexjaboutapivotelement(line6)andtestswhetheritandtheelementstoitsleftcanbedecreasedtoavalue  suchthatthetotaldecreaseis  (line8).ifso,itrecursivelysearchestherightside;ifnot,theleftside.attheconclusionofthealgorithm,  issettothelargestvaluethatpassesthetest(line13),and   nallythenewxjarecomputed(line16)   theonlydifferencefromduchietal.   salgorithm.thisalgorithmisasymptoticallyfasterthanthatofquattonietal.(2009).theyreformulate      ,1id173asaconstrainedoptimizationprob-lem(inwhichthe      ,1normisboundedby  )andprovideasolutionino(nlogn)time.themethodshownhereissimplerandfasterbecauseitcanworkoneachrowseparately.algorithm1linear-timealgorithmfortheproxi-maloperatorofthe      norm.1:procedureupdate(w,  )2:lo,hi   1,n3:s   04:whilelo   hido5:selectmdrandomlyfromlo,...,hi6:     partition(w,lo,md,hi)7:     1     s+     i=lo|xi|        8:if     |x  |then9:s   s+     i=lo|xi|10:lo     +111:else12:hi        113:     hi14:     1  (s     )15:fori   1,...,ndo16:xi   min(max(xi,     ),  )17:procedurepartition(w,lo,md,hi)18:swapxloandxmd19:i   lo+120:forj   lo+1,...,hido21:ifxj   xlothen22:swapxiandxj23:i   i+124:swapxloandxi   125:returni   15experimentsweevaluateourmodelusingtheopen-sourcenplmtoolkitreleasedbyvaswanietal.(2013),extendingittousetheadditionalregularizersasdescribedinthispaper.2weuseavocabularysizeof100kandwordembeddingswith50dimen-sions.weusetwohiddenlayersofrecti   edlinearunits(nairandhinton,2010).2theseextensionshavebeencontributedtothenplmproject.wetrainneurallanguagemodels(lms)ontwonaturallanguagecorpora,europarlv7englishandtheafpportionofenglishgigaword5.aftertok-enization,europarlhas56mtokensandgigawordafphas870mtokens.forbothcorpora,weholdoutavalidationsetof5,000tokens.wetraineachmodelfor10iterationsoverthetrainingdata.ourexperimentsbreakdownintothreeparts.first,welookattheimpactofourpruningmethodonperplexityofaheld-outvalidationset,acrossavarietyofsettings.second,wetakeacloserlookathowthemodelevolvesthroughthetrainingpro-cess.finally,weexplorethedownstreamimpactofourmethodonastatisticalphrase-basedma-chinetranslationsystem.5.1evaluatingperplexityandnetworksizewe   rstlookattheimpactthatthe      ,1regular-izerhasontheperplexityofourvalidationset.themainresultsareshownintable1.for     0.01,theregularizerseemstohavelittleimpact:nohid-denunitsarepruned,andperplexityisalsonotaf-fected.for  =1,ontheotherhand,mosthiddenunitsarepruned   apparentlytoomany,sinceper-plexityisworse.butfor  =0.1,weseethatweareabletopruneoutmanyhiddenunits:uptohalfofthe   rstlayer,withlittleimpactonperplexity.wefoundthistobeconsistentacrossallourexper-iments,varyingid165size,initialhiddenlayersize,andvocabularysize.table2showsthesameinformationfor5-grammodelstrainedonthelargergigawordafpcor-pus.thesenumberslookverysimilartothoseoneuroparl:again  =0.1worksbest,and,countertoexpectation,eventhe   nalnumberofunitsissimilar.table3showstheresultofvaryingthevocabu-larysize:again  =0.1worksbest,and,althoughitisnotshowninthetable,wealsofoundthatthe   nalnumberofunitsdidnotdependstronglyonthevocabularysize.table4showsresultsusingthe   2,1norm(eu-roparlcorpus,5-grams,100kvocabulary).sincethisisadifferentregularizer,thereisn   tanyrea-sontoexpectthat  behavesthesameway,andindeed,asmallervalueof  seemstoworkbest.5.2acloserlookattrainingwealsostudiedtheevolutionofthenetworkoverthetrainingprocesstogainsomeinsightsintohowthemethodworks.the   rstquestionwewantto2-gram3-gram5-gram  layer1layer2ppllayer1layer2ppllayer1layer2ppl01,000501031,00050661,00050550.0011,000501041,00050661,00050540.011,000501041,00050631,00050550.149947105652496678450551.0502411112832761442968table1:comparisonof      ,1id173on2-gram,3-gram,and5-gramneurallanguagemodels.thenetworkinitiallystartedwith1,000unitsinthe   rsthiddenlayerand50inthesecond.aid173strengthof  =0.1consistentlyisabletopruneunitswhilemaintainingperplexity,eventhoughthe   nalnumberofunitsvariesconsiderablyacrossmodels.thevocabularysizeis100k.  layer1layer2perplexity01,000501000.0011,00050990.011,000501010.1742501071.02417173table2:resultsfromtraininga5-gramneurallmontheafpportionofthegigaworddataset.aswiththesmallereuroparlcorpus(table1),areg-ularizationstrengthof  =0.1isabletopruneunitswhilemaintainingperplexity.vocabularysize  10k25k50k100k0476054550.001475454540.01475855550.1486255551.061646568table3:aid173strengthof  =0.1isbestacrossdifferentvocabularysizes.  layer1layer2perplexity01,000501000.00011,00050540.0011,00050550.0161650570.11993265table4:resultsusing   2,1id173.024681005001,000epochnonzerounitsinhiddenlayer11000900800700figure5:numberofunitsin   rsthiddenlayerovertime,withvariousstartingsizes(  =0.1).ifwestartwithtoomanyunits,weendupwiththesamenumber,althoughifwestartwithasmallernumberofunits,afewarestillprunedaway.answeriswhetherthemethodissimplyremov-ingunits,orconvergingonanoptimalnumberofunits.figure5suggeststhatitisalittleofboth:ifwestartwithtoomanyunits(900or1000),themethodconvergestothesamenumberregardlessofhowmanyextraunitstherewereinitially.butifwestartwithasmallernumberofunits,themethodstillprunesawayabout50units.next,welookatthebehaviorovertimeofdif-ferentid173strengths  .wefoundthatnotonlydoes  =1pruneouttoomanyunits,itdoessoatthevery   rstiteration(figure6,above),perhapsprematurely.bycontrast,the  =0.1runprunesoutunitsgradually.byplottingthesecurvestogetherwithperplexity(figure6,below),wecanseethatthe  =0.1runis   ttingthemodelandpruningitatthesametime,whichseemspreferableto   ttingwithoutanypruning(  =024681005001,000epochnonzerounitsinhiddenlayer1     0.01  =0.1  =10246810050100epochperplexity  =0.01  =0.1  =1figure6:above:numberofunitsin   rsthid-denlayerovertime,forvariousid173strengths  .aid173strengthof   0.01doesnotzerooutanyrows,whileastrengthof1zerosoutrowsrightaway.below:perplexityovertime.therunswith     0.1haveverysimilarlearningcurves,whereas  =1isworsefromthebeginning.neurallm  noneeuroparlgigawordafp0(none)23.224.7(+1.5)25.2(+2.0)0.124.6(+1.4)24.9(+1.7)table5:theimprovementsintranslationaccuracyduetotheneurallm(showninparentheses)areaffectedonlyslightlyby      ,1id173.fortheeuroparllm,thereisnostatisticallysigni   -cantdifference,andforthegigawordafplm,astatisticallysigni   cantbutsmalldecreaseof   0.3.0.01)orpruning   rstandthen   tting(  =1).wecanalsovisualizetheweightmatrixitselfovertime(figure7),for  =0.1.itisstrikingthatalthoughthissetting   tsthemodelandprunesitatthesametime,asarguedabove,bythe   rstiterationitalreadyseemstohavedecidedroughlyhowmanyunitsitwilleventuallyprune.5.3evaluatingonmachinetranslationwealsolookedattheimpactofourmethodonstatisticalmachinetranslationsystems.weusedthemosestoolkit(koehnetal.,2007)tobuildaphrasebasedmachinetranslationsystemwithatraditional5-gramlmtrainedonthetargetsideofourbitext.weaugmentedthissystemwithneu-rallmstrainedontheeuroparldataandthegi-gawordafpdata.basedontheresultsfromtheperplexityexperiments,welookedatmodelsbothbuiltwitha  =0.1regularizer,andwithoutregu-larization(  =0).webuiltoursystemusingthenewscommentarydatasetv8.wetunedourmodelusingnewstest13andevaluatedusingnewstest14.afterstandardcleaningandid121,therewere155kparal-lelsentencesinthenewscommentarydataset,and3,000sentenceseachforthetuningandtestsets.table5showsthattheadditionofaneurallmhelpssubstantiallyoverthebaseline,withim-provementsofupto2id7.usingtheeuroparlmodel,theid7scoresobtainedwithoutandwithid173werenotsigni   cantlydiffer-ent(p   0.05),consistentwiththenegligibleper-plexitydifferencebetweenthesemodels.onthegigawordafpmodel,id173diddecreasetheid7scoreby0.3,consistentwiththesmallperplexityincreaseoftheregularizedmodel.thedecreaseisstatisticallysigni   cant,butsmallcom-paredwiththeoverallbene   tofaddinganeu-rallm.1iteration5iterations10iterationsfigure7:evolutionofthe   rsthiddenlayerweightmatrixafter1,5,and10iterations(withrowssortedby      norm).anonlinearcolorscaleisusedtoshowsmallvaluesmoreclearly.thefourverticalblockscorrespondtothefourcontextwords.thelightbaratthebottomistherowsthatareclosetozero,andthewhitebaristherowsthatareexactlyzero.6relatedworkresearchershavebeenexploringtheuseofneu-ralnetworksforlanguagemodelingforalongtime.schmidhuberandheil(1996)proposedacharacterid165modelusingneuralnetworkswhichtheyusedfortextcompression.xuandrudnicky(2000)proposedaword-basedproba-bilitymodelusingasoftmaxoutputlayertrainedusingcross-id178,butonlyforbigrams.bengioetal.(2003)de   nedaprobabilisticwordid165modelanddemonstratedimprovementsovercon-ventionalsmoothedlanguagemodels.mnihandteh(2012)speduptrainingoflog-bilinearlan-guagemodelsthroughtheuseofnoise-contrastiveestimation(nce).vaswanietal.(2013)alsousedncetotrainthearchitectureofbengioetal.(2003),andwereabletointegratealarge-vocabularylanguagemodeldirectlyintoama-chinetranslationdecoder.baltescuetal.(2014)describeasimilarmodel,withextensionslikeahierarchicalsoftmax(basedonbrownid91)anddirectid165features.beyondfeed-forwardneuralnetworklan-guagemodels,researchershaveexploredusingmorecomplicatedneuralnetworkarchitectures.id56lmisanopen-sourceimplementationofalanguagemodelusingrecurrentneuralnetworks(id56)whereconnectionsbetweenunitscanformdirectedcycles(mikolovetal.,2011).sunder-meyeretal.(2015)usethelong-shorttermmem-ory(lstm)neuralarchitecturetoshowaper-plexityimprovementovertheid56lmtoolkit.infuturework,weplanonexploringhowourmethodcouldimprovethesemorecomplicatedneuralmodelsaswell.automaticallylimitingthesizeofneuralnet-worksisanoldidea.the   optimalbraindam-age   (obd)technique(lecunetal.,1989)com-putesasaliencybasedonthesecondderivativeoftheobjectivefunctionwithrespecttoeachparame-ter.theparametersarethensortedbysaliency,andthelowest-saliencyparametersarepruned.thepruningprocessisseparatefromthetrainingpro-cess,whereasid173performstrainingandpruningsimultaneously.id173inneuralnetworksisalsoanoldidea;forexample,now-landandhinton(1992)mentionboth   22and   0id173.ourmethoddevelopsonthisideabyusingamixednormtopruneunits,ratherthanparameters.srivastavaetal.introduceamethodcalleddropoutinwhichunitsaredirectlydeactivatedatrandomduringtraining(srivastavaetal.,2014),whichinducessparsityinthehiddenunitactiva-tions.however,attheendoftraining,allunitsarereactivated,asthegoalofdropoutistore-duceover   tting,nottoreducenetworksize.thus,dropoutandourmethodseemtobecomplemen-tary.7conclusionwehavepresentedamethodforauto-sizinganeu-ralnetworkduringtrainingbyremovingunitsus-inga      ,1regularizer.thisregularizerdrivesaunit   sinputweightsasagroupdowntozero,al-lowingtheunittobepruned.wecanthuspruneunitsoutofournetworkduringtrainingwithmin-imalimpacttoheld-outperplexityordownstreamperformanceofamachinetranslationsystem.ourresultsshowedempiricallythatthechoiceofaid173coef   cientof0.1wasrobusttoinitialcon   gurationparametersofinitialnetworksize,vocabularysize,id165order,andtrainingcorpus.furthermore,imposingasingleregularizerontheobjectivefunctioncantuneallofthehiddenlayersofanetworkwithonesetting.thisreducestheneedtoconductexpensive,multi-dimensionalgridsearchesinordertodetermineoptimalsizes.wehavedemonstratedthepowerandef   cacyofthismethodonafeed-forwardneuralnetworkforlanguagemodelingthoughexperimentsonper-plexityandmachinetranslation.however,thismethodisgeneralenoughthatitshouldbeapplica-bletootherdomains,bothinsidenaturallanguageprocessingandoutside.asneuralmodelsbecomemorepervasiveinnaturallanguageprocessing,theabilitytoauto-sizenetworksforfastexperimen-tationandquickexplorationwillbecomeincreas-inglyimportant.acknowledgmentswewouldliketothanktomerlevinboim,anto-niosanastasopoulos,andashishvaswanifortheirhelpfuldiscussions,aswellasthereviewersfortheirassistanceandfeedback.referencesfrancisbach,rodolphejenatton,julienmairal,andguillaumeobozinski.2012.optimizationwithsparsity-inducingpenalties.foundationsandtrendsinmachinelearning,4(1):1   106.paulbaltescu,philblunsom,andhieuhoang.2014.oxlm:aneurallanguagemodellingframeworkformachinetranslation.praguebulletinofmathemati-callinguistics,102(1):81   92.yoshuabengio,r  ejeanducharme,pascalvincent,andchristianjanvin.2003.aneuralprobabilisticlanguagemodel.j.machinelearningresearch,3:1137   1155.jacobdevlin,rabihzbib,zhongqianghuang,thomaslamar,richardschwartz,andjohnmakhoul.2014.fastandrobustneuralnetworkjointmodelsforsta-tisticalmachinetranslation.inproc.acl,pages1370   1380.johnduchiandyoramsinger.2009.ef   cientonlineandbatchlearningusingforwardbackwardsplitting.j.machinelearningresearch,10:2899   2934.johnduchi,shaishalev-shwartz,yoramsinger,andtusharchandra.2008.ef   cientprojectionsontothe   1-ballforlearninginhighdimensions.inproc.icml,pages272   279.philippkoehn,hieuhoang,alexandrabirch,chriscallison-burch,marcellofederico,nicolabertoldi,brookecowan,wadeshen,christinemoran,richardzens,chrisdyer,ond  rejbojar,alexan-draconstantin,andevanherbst.2007.moses:opensourcetoolkitforstatisticalmachinetransla-tion.inproc.acl,interactiveposteranddemon-strationsessions,pages177   180.yannlecun,johns.denker,saraa.solla,richarde.howard,andlawrenced.jackel.1989.optimalbraindamage.inproc.nips,volume2,pages598   605.tomasmikolov,stefankombrink,anoopdeoras,lukarburget,andjancernocky.2011.id56lm-recurrentneuralnetworklanguagemodelingtoolkit.inproc.asru,pages196   201.andriymnihandyeewhyeteh.2012.afastandsimplealgorithmfortrainingneuralprobabilisticlanguagemodels.inproc.icml,pages1751   1758.vinodnairandgeoffreyehinton.2010.recti-   edlinearunitsimproverestrictedboltzmannma-chines.inproc.icml,pages807   814.stevenj.nowlandandgeoffreye.hinton.1992.simplifyingneuralnetworksbysoftweight-sharing.neuralcomputation,4:473   493.nealparikhandstephenboyd.2014.proximalal-gorithms.foundationsandtrendsinoptimization,1(3):127   239.ariadnaquattoni,xaviercarreras,michaelcollins,andtrevordarrell.2009.anef   cientprojectionforl1,   id173.inproc.icml,pages857   864.jurgenschmidhuberandstefanheil.1996.sequen-tialneuraltextcompression.ieeetransactionsonneuralnetworks,7:142   146.nitishsrivastava,geoffreyhinton,alexkrizhevsky,ilyasutskever,andruslansalakhutdinov.2014.dropout:asimplewaytopreventneuralnetworksfromover   tting.j.machinelearningresearch,15(1):1929   1958.martinsundermeyer,hermannney,andralfschl  uter.2015.fromfeedforwardtorecurrentlstmneu-ralnetworksforlanguagemodeling.trans.audio,speech,andlanguage,23(3):517   529.ashishvaswani,yinggongzhao,victoriafossum,anddavidchiang.2013.decodingwithlarge-scaleneurallanguagemodelsimprovestranslation.inproc.emnlp,pages1387   1392.weixuandalexanderi.rudnicky.2000.canar-ti   cialneuralnetworkslearnlanguagemodels?inproc.internationalconferenceonstatisticallan-guageprocessing,pagesm1   13.