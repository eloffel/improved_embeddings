6
1
0
2

 

y
a
m
1
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
5
2
9
1
0

.

2
0
6
1
:
v
i
x
r
a

massively multilingual id27s

waleed ammar    george mulcaire    yulia tsvetkov   

guillaume lample    chris dyer    noah a. smith   

   school of computer science, carnegie mellon university, pittsburgh, pa, usa
   computer science & engineering, university of washington, seattle, wa, usa

wammar@cs.cmu.edu, gmulc@uw.edu, ytsvetko@cs.cmu.edu
{glample,cdyer}@cs.cmu.edu, nasmith@cs.washington.edu

abstract

we introduce new methods for estimat-
ing and evaluating embeddings of words
in more than    fty languages in a sin-
gle shared embedding space. our esti-
mation methods, multicluster and mul-
ticca, use dictionaries and monolingual
data;
they do not require parallel data.
our new evaluation method, multiqvec-
cca,
is shown to correlate better than
previous ones with two downstream tasks
(text categorization and parsing). we also
describe a web portal for evaluation that
will facilitate further research in this area,
along with open-source releases of all our
methods.

1 introduction

in machine translation,

vector-space representations of words are widely
used in statistical models of natural
language.
in addition to improving the performance on
standard monolingual nlp tasks, shared rep-
resentation of words across languages offers
intriguing possibilities (klementiev et al., 2012).
for example,
translat-
ing a word never seen in parallel data may be
overcome by seeking its vector-space neighbors,
provided the embeddings are learned from both
plentiful monolingual corpora and more limited
parallel data. a second opportunity comes from
id21, in which models trained in one
language can be deployed in other languages.
while previous work has used hand-engineered
features that are cross-linguistically stable as the
transfer (zeman and resnik, 2008;
basis model
tsvetkov et al., 2014),
mcdonald et al., 2011;
automatically
of-
fer
generalization
(klementiev et al., 2012;
at

promise
cost

learned
of

the
lower

embeddings

better

hermann and blunsom, 2014; guo et al., 2016).
we therefore conjecture that developing estima-
tion methods for massively id73
embeddings (i.e., embeddings for words in a large
number of languages) will play an important role
in the future of multilingual nlp.

this paper builds on previous work in multilin-
gual embeddings and makes the following contri-
butions:

    we propose two dictionary-based methods   
multicluster and multicca   for estimating
multilingual embeddings which only require
monolingual data and pairwise parallel dictio-
naries, and use them to train embeddings in 59
languages for which these resources are avail-
able (  2). parallel corpora are not required but
can be used when available. we show that the
proposed methods work well in some settings
and id74.

    we adapt qvec (tsvetkov et al., 2015)1 to eval-
uating multilingual embeddings (multiqvec).
we also develop a new evaluation method
multiqvec-cca which addresses a theoretical
shortcoming of multiqvec (  3). compared to
other intrinsic metrics used in the literature, we
show that both multiqvec and multiqvec-cca
achieve better correlations with extrinsic tasks.

    we develop an easy-to-use web portal2 for
evaluating arbitrary multilingual embeddings
using a suite of intrinsic and extrinsic metrics
(  4). together with the provided benchmarks,
the evaluation portal will substantially facilitate
future research in this area.

1a method for evaluating monolingual id27s.
2http://128.2.220.95/multilingual

2 estimating multilingual embeddings

let l be a set of languages, and let v m be the
set of surface forms (word types) in m     l. let
v = sm   l v m. our goal is to estimate a partial
embedding function e : l    v 7    rd (allowing
a surface form that appears in two languages to
have different vectors in each). we would like to
estimate this function such that: (i) semantically
similar words in the same language are nearby, (ii)
translationally equivalent words in different lan-
guages are nearby, and (iii) the domain of the func-
tion covers as many words in v as possible.

we use distributional similarity in a monolin-
gual corpus mm to model semantic similarity be-
tween words in the same language. for cross-
lingual similarity, either a parallel corpus pm,n or a
bilingual dictionary dm,n     v m    v n can be used.
our methods focus on the latter, in some cases ex-
tracting dm,n from a parallel corpus.3

most previous work on multilingual embed-
dings only considered the bilingual case, | l |=
2. we focus on estimating multilingual em-
beddings for
| l |> 2 and describe two
novel dictionary-based methods (multicluster
and multicca). we then describe our base-
lines: a variant of coulmance et al. (2015) and
guo et al. (2016) (henceforth referred to as mul-
tiskip),4 and the translation-invariance matrix fac-
torization method (gardner et al., 2015).

2.1 multicluster
in this approach, we decompose the problem into
two simpler subproblems: e = eembed     ecluster,
where ecluster : l    v 7    c deterministically maps
words to multilingual clusters c, and eembed : c    
rd assigns a vector to each cluster. we use a bilin-
gual dictionary to    nd clusters of translationally
equivalent words, then use distributional similari-
ties of the clusters in monolingual corpora from all
languages in l to estimate an embedding for each
cluster. by forcing words from different languages

in

both

directions.

3to do this, we align the corpus using fast align
es-
dis-
=

(dyer et al., 2013)
timated
tributions
are
(cid:8)(u, v) | u     v m
where the threshold    trades off dictionary recall and preci-
sion. we    xed    = 0.1 early on based on manual inspection
of the resulting dictionaries.

of
to
, pm|n(u | v)    pn|m(v | u) >    (cid:9),

parameters
used
, v     v n

the
translation
dm,n

the word
select

pairs:

4we

multiskip

developed

of
coulmance et al. (2015) and guo et al. (2016). one impor-
tant distinction is that multiskip is only trained on parallel
corpora, while coulmance et al. (2015) and guo et al. (2016)
also use monolingual corpora.

independently

in a cluster to share the same embedding, we cre-
ate anchor points in the vector space to bridge lan-
guages.

more speci   cally, we de   ne the clusters as the
connected components in a graph where nodes are
(language, surface form) pairs and edges corre-
spond to translation entries in dm,n. we assign ar-
bitrary ids to the clusters and replace each word
token in each monolingual corpus with the corre-
sponding cluster id, and concatenate all modi   ed
corpora. the resulting corpus consists of multilin-
gual cluster id sequences. we can then apply any
monolingual embedding estimator; here, we use
the skipgram model from mikolov et al. (2013a).

2.2 multicca
our proposed method (multicca) extends the
bilingual embeddings of faruqui and dyer (2014).
first, they use monolingual corpora to train mono-
lingual embeddings for each language indepen-
dently (em and en), capturing semantic similar-
ity within each language separately. then, us-
ing a bilingual dictionary dm,n, they use canonical
correlation analysis (cca) to estimate linear pro-
jections from the ranges of the monolingual em-
beddings em and en, yielding a bilingual embed-
ding em,n. the linear projections are de   ned by
tm   m,n and tn   m,n     rd  d; they are selected to
maximize the correlation between tm   m,nem(u)
and tn   m,nen(v) where (u, v)     dm,n. the bilin-
gual embedding is then de   ned as ecca(m, u) =
tm   m,nem(u) (and likewise for ecca(n, v)).

in this work, we use a simple extension (in hind-
sight) to construct multilingual embeddings for
more languages. we let the vector space of the
initial (monolingual) english embeddings serve as
the multilingual vector space (since english typ-
ically offers the largest corpora and wide avail-
ability of bilingual dictionaries). we then estimate
projections from the monolingual embeddings of
the other languages into the english space.

we start by estimating, for each m     l \
{en}, the two projection matrices: tm   m,en and
ten   m,en; these are guaranteed to be non-singular.
we then de   ne the multilingual embedding as
ecca(en, u) = een(u) for u     ven, and
en   m,entm   m,enem(v) for v    
ecca(m, v) = t    1
v m

, m     l \ {en}.

2.3 multiskip
luong et al. (2015b) proposed a method for esti-
mating bilingual embeddings which only makes

use of parallel data; it extends the skipgram model
of mikolov et al. (2013a). the skipgram model
de   nes a distribution over words u that occur in
a context window (of size k) of a word v:

gardner et al. (2015) solves for a low-rank de-
composition uv     which both approximates x as
well as its transformations a   x, xa and a   xa by
de   ning the following objective:

p(u | v) =

exp eskipgram(m, v)   econtext(m, u)

pu      v m exp eskipgram(m, v)   econtext(m, u   )

minu ,v kx     uv    k2 + kxa     uv    k2
+ ka   x     uv    k2 + ka   xa     uv    k2

in practice, this distribution can be estimated us-
ing a noise contrastive estimation approximation
(gutmann and hyv  arinen, 2012) while maximiz-
ing the log-likelihood:

x

x

log p(ui+k | ui)

i   pos(mm)

k   {   k ,...,   1,1,...,k}

where pos(mm) are the indices of words in the
monolingual corpus mm.

to establish a bilingual embedding, with a
parallel corpus pm,n of source language m and
target language n, luong et al. (2015b) estimate
conditional models of words in both source and
target positions. the source positions are se-
lected as sentential contexts (similar to monolin-
gual skipgram), and the bilingual contexts come
from aligned words. the bilingual objective is to
maximize:

x

x

i   m-pos(pm,n)

k   {   k ,...,   1,1,...,k}

log p(ui+k | ui)
+ log p(va(i)+k | ui)

+ x

x

j   n-pos(pm,n)

k   {   k ,...,   1,1,...,k}

log p(vj+k | vj)
+ log p(ua(j)+k | vj)

where m-pos(pm,n) and n-pos(pm,n) are the in-
deces of the source and target tokens in the parallel
corpus respectively, a(i) and a(j) are the positions
of words that align to i and j in the other language.
it is easy to see how this method can be extended
for more than two languages by summing up the
bilingual objectives for all available parallel cor-
pora.

2.4 translation-invariance
gardner et al. (2015) proposed that multilingual
embeddings should be translation invariant. con-
sider a matrix x     r|v|  |v| which summarizes
the pointwise mutual information statistics be-
tween pairs of words in monolingual corpora,
and let uv     be a low-rank decomposition of x
where u, v     r|v|  d. now, consider another
matrix a     r|v|  |v| which summarizes bilin-
gual alignment frequencies in a parallel corpus.

the multilingual embeddings are then taken to be
the rows of the matrix u.

3 evaluating multilingual embeddings

one of our contributions is to streamline the eval-
uation of multilingual embeddings. in addition to
assessing goals (i   iii) stated in   2, a good evalua-
tion metric should also (iv) show good correlation
with performance in downstream applications and
(v) be computationally ef   cient.

it is easy to evaluate the coverage (iii) by count-
ing the number of words covered by an embedding
function in a closed vocabulary. intrinsic evalua-
tion metrics are generally designed to be computa-
tionally ef   cient (v) but may or may not meet the
goals (i, ii, iv). although intrinsic evaluations will
never be perfect, a standard set of evaluation met-
rics will help drive research. by design, standard
(monolingual) word similarity tasks meet (i) while
cross-lingual word similarity tasks and the word
translation tasks meet (ii). we propose another
evaluation method (multiqvec-cca), designed to
simultaneously assess goals (i, ii). multiqvec-
cca extends qvec (tsvetkov et al., 2015), a re-
cently proposed monolingual evaluation method,
addressing fundamental    aws and extending it to
multiple languages. to assess the degree to which
these id74 meet (iv), in   5 we per-
form a correlation analysis looking at which intrin-
sic metrics are best correlated with downstream
task performance   i.e., we evaluate the evaluation
metrics.

3.1 word similarity
word similarity datasets such as wordsim-353
(agirre et al., 2009) and men (bruni et al., 2014)
provide human judgments of semantic similarity.
by ranking words by cosine similarity and by their
empirical similarity judgments, a ranking correla-
tion can be computed that assesses how well the
estimated vectors capture human intuitions about
semantic relatedness.

some previous work on bilingual and multilin-
gual embeddings focuses on monolingual word

similarity to evaluate embeddings (e.g., faruqui
and dyer, 2014). this approach is limited be-
cause it cannot measure the degree to which em-
beddings from different languages are similar (ii).
for this paper, we report results on an english
word similarity task,
the stanford rw dataset
(luong et al., 2013), as well as a combination
of several cross-lingual word similarity datasets
(camacho-collados et al., 2015).

3.2 word translation
this task directly assesses the degree to which
translationally equivalent words in different lan-
guages are nearby in the embedding space.
the evaluation data consists of word pairs
which are known to be translationally equiva-
lent. the score for one word pair (l1, w1), (l2, w2)
both of which are covered by an embed-
ding e is 1 if cosine(e(l1, w1), e(l2, w2))    
2     gl2 where gl2
cosine(e(l1, w1), e(l2, w   
is the set of words of language l2 in the evalu-
ation dataset, and cosine is the cosine similarity
function. otherwise, the score for this word pair
is 0. the overall score is the average score for
all word pairs covered by the embedding func-
tion. this is a variant of the method used by
mikolov et al. (2013b) to evaluate bilingual em-
beddings.

2))     w   

3.3 correlation-based evaluation
we introduce qvec-cca   an intrinsic evaluation
measure of the quality of id27s. our
method is an improvement of qvec   a monolin-
gual evaluation based on alignment of embeddings
to a matrix of features extracted from a linguis-
tic resource (tsvetkov et al., 2015). we review
qvec, and then describe qvec-cca.

qvec. the main idea behind qvec is to quan-
tify the linguistic content of id27s
by maximizing the correlation with a manually-
annotated linguistic resource. let the number of
common words in the vocabulary of the word em-
beddings and the linguistic resource be n. to
quantify the semantic content of embeddings, a se-
mantic linguistic matrix s     rp  n is constructed
from a semantic database, with a column vector
for each word. each word vector is a distribu-
tion of the word over p linguistic properties, based
on annotations of the word in the database. let
x     rd  n be embedding matrix with every row
as a dimension vector x     r1  n. d denotes the

dimensionality of id27s. then, s and
x are aligned to maximize the cumulative corre-
lation between the aligned dimensions of the two
matrices. speci   cally, let a     {0, 1}d  p be a ma-
trix of alignments such that aij = 1 iff xi is aligned
to sj, otherwise aij = 0. if r(xi, sj) is the pearson   s
correlation between vectors xi and sj, then qvec
is de   ned as:

qvec = maxa:pj aij   1

x
x
i=1

s
x
j=1

r(xi, sj)    aij

the constraint pj aij     1, warrants that one dis-
tributional dimension is aligned to at most one lin-
guistic dimension.

to

qvec

shown

been
downstream semantic

correlate
has
strongly with
tasks
(tsvetkov et al., 2015). however, it suffers from
two major weaknesses. first, it is not invariant to
linear transformations of the embeddings    basis,
whereas the bases in id27s are gen-
erally arbitrary (szegedy et al., 2014). second,
a sum of correlations produces an unnormalized
score:
the more dimensions in the embedding
matrix the higher the score.
this precludes
comparison of models of different dimensional-
ity. qvec-cca simultaneously addresses both
problems.

qvec-cca. to measure correlation between
the embedding matrix x and the linguistic matrix
s, instead of cumulative dimension-wise correla-
tion we employ cca. cca    nds two sets of basis
vectors, one for x    and the other for s   , such that
the correlations between the projections of the ma-
trices onto these basis vectors are maximized. for-
mally, cca    nds a pair of basis vectors v and w
such that

qvec-cca = cca(x   

, s   )

= maxv,w r(x   v, s   w)

thus, qvec-cca ensures
invariance to the
matrices    bases    rotation, and since it is a sin-
gle correlation,
it produces a score in [   1, 1].
both qvec and qvec-cca rely on a matrix of
linguistic properties constructed from a man-
ually crafted linguistic resource. we extend
both methods
evaluations   
multiqvec and multiqvec-cca   by construct-
ing the linguistic matrix using supersense tag
annotations
(miller et al., 1993),

to multilingual

for english

danish
mart    nez alonso et al., 2016)
(montemagni et al., 2003).

(mart    nez alonso et al., 2015;
italian

and

3.4 extrinsic tasks

in order to evaluate how useful the word embed-
dings are for a downstream task, we use the em-
bedding vector as a dense feature representation
of each word in the input, and deliberately remove
any other feature available for this word (e.g., pre-
   xes, suf   xes, part-of-speech). for each task, we
train one model on the aggregate training data
available for several languages, and evaluate on
the aggregate evaluation data in the same set of
languages. we apply this for multilingual doc-
ument classi   cation and multilingual dependency
parsing.

for

document

classi   cation, we

follow
klementiev et al. (2012) in using the rcv corpus
of newswire text, and train a classi   er which
differentiates between four topics. while most
previous work used this data only in a bilingual
setup, we simultaneously train the classi   er on
documents in seven languages,5 and evaluate on
the development/test section of those languages.
for this task, we report the average classi   cation
accuracy on the test set.

for id33, we train the stack-
lstm parser of dyer et al. (2015) on a subset of
the languages in the universal dependencies v1.1,6
and test on the same languages, reporting unla-
beled attachment scores. we remove all part-of-
speech and morphology features from the data,
and prevent the model from optimizing the word
embeddings used to represent each word in the
corpus, thereby forcing the parser to rely com-
pletely on the provided (pretrained) embeddings
as the token representation. although omitting
other features (e.g., parts of speech) hurts the per-
formance of the parser, it emphasizes the contribu-
tion of the id27s being studied.

4 evaluation portal

in order to facilitate future research on multilin-
gual id27s, we developed a web portal
to enable researchers who develop new estimation
methods to evaluate them using a suite of evalu-
ation tasks. the portal serves the following pur-

5danish, german, english, spanish, french, italian and

swedish.

6http://hdl.handle.net/11234/lrt-1478

poses:

    download the monolingual and bilingual data
we used to estimate multilingual embeddings in
this paper,

    download standard development/test data sets
for each of the id74 to help re-
searchers working in this area report trustwor-
thy and replicable results,7

    upload arbitrary multilingual embeddings, scan
which languages are covered by the embed-
dings, allow the user to pick among the com-
patible evaluation tasks, and receive evaluation
scores for the selected tasks, and

    register a new evaluation data set or a new eval-
uation metric via the github repository which
mirrors the backend of the web portal.

5 experiments

our experiments are designed to show two pri-
mary sets of results: (i) how well the proposed
intrinsic id74 correlate with down-
stream tasks (  5.1) and (ii) which estimation meth-
ods work best according to each metric (  5.2). the
data used for training and evaluation are available
for download on the evaluation portal.

5.1 correlations between intrinsic vs.

extrinsic id74

in this experiment, we consider four intrinsic
id74 (cross-lingual word similar-
ity, word translation, multiqvec and multiqvec-
cca) and two extrinsic id74 (mul-
tilingual document classi   cation and multilingual
parsing).

data: for the cross-lingual word similarity task,
we use disjoint subsets of the en-it mws353
dataset (leviant and reichart, 2015) for develop-
ment (308 word pairs) and testing (307 word
pairs).
for the word translation task, we use
wiktionary to extract a development set (647
translations) and a test set (647 translations) of
translationally-equivalent word pairs in en-it, en-
da and da-it. for both multiqvec and multiqvec-
cca, we used disjoint subsets of the multilingual
(en, da, it) supersense tag annotations described
in   3 for development (12,513 types) and testing
(12,512 types).

7except for the original rcv documents, which are re-
stricted by the reuters license and cannot be republished. all
other data is available for download.

(   ) extrinsic task
(   ) intrinsic metric
word similarity
word translation
multiqvec
multiqvec-cca

document
classi   cation
0.386
0.066
0.635
0.896

dependency
parsing
0.007
-0.292
0.444
0.273

table 1: correlations between intrinsic evaluation
metrics (rows) and downstream task performance
(columns).

for the document classi   cation task, we use the
multilingual rcv corpus (en, it, da). for the de-
pendency parsing task, we use the universal de-
pendencies v1.1 (agi  c et al., 2015) in three lan-
guages (en, da, it).

setup: to estimate correlations between the
proposed intrinsic id74 and down-
stream task performance, we train a total of 17
different multilingual embeddings for three lan-
guages (english, italian and danish). to com-
pute the correlations, we evaluate each of the 17
embeddings (12 multicluster embeddings, 1 mul-
ticca embeddings, 1 multiskip embeddings, 2
translation-invariance embeddings) according to
each of the six id74 (4 intrinsic, 2
extrinsic).8

results: table 1 shows pearson   s correlation co-
ef   cients of eight (intrinsic metric, extrinsic met-
ric) pairs. although each of two proposed meth-
ods multiqvec and multiqvec-cca correlate bet-
ter with a different extrinsic task, we establish (i)
that intrinsic methods previously used in the litera-
ture (cross-lingual word similarity and word trans-
lation) correlate poorly with downstream tasks,
and (ii) that the intrinsic methods proposed in this
paper (multiqvec and multiqvec-cca) correlate
better with both downstream tasks, compared to
cross-lingual word similarity and word transla-
tion.9

8the 102 (17    6) values used to compute pearson   s cor-
relation coef   cient are provided in the supplementary mate-
rial.

9although supersense annotations exist for other lan-
guages, the annotations are inconsistent across languages and
may not be publicly available, which is a disadvantage of the
multiqvec and multiqvec-cca metrics. therefore, we rec-
ommend that future multilingual supersense annotation ef-
forts use the same set of supersense tags used in other lan-
guages. if the id27s are primarily needed for en-
coding syntactic information, one could use tag dictionaries
based on the universal pos tag set (petrov et al., 2012) in-
stead of supersense tags.

id33
doc. classi   cation
mono. wordsim
cross. wordsim
word translation
mono. qvec
multiqvec
mono. qvec-cca
multiqvec-cca

task multicluster multicca
48.8 [69.3]
91.6 [52.6]
43.0 [71.0]
66.8 [78.2]
83.6 [31.8]
10.7 [99.0]
8.7 [87.0]
63.4 [99.0]
42.0 [87.0]

48.4 [72.1]
90.3 [52.3]
14.9 [71.0]
12.8 [78.2]
30.0 [38.9]
7.6 [99.6]
8.3 [86.4]
53.8 [99.6]
37.4 [86.4]

table 2: results for multilingual embeddings that
cover 59 languages. each row corresponds to
one of the embedding id74 we use
(higher is better). each column corresponds to
one of the embedding estimation methods we con-
sider; i.e., numbers in the same row are compa-
rable. numbers in square brackets are coverage
percentages.

5.2 evaluating multilingual estimation

methods

we now turn to evaluating the four estimation
methods described in   2. we use the proposed
methods (i.e., multicluster and multicca) to
train multilingual embeddings in 59 languages for
which bilingual translation dictionaries are avail-
able.10 in order to compare our methods to base-
lines which use parallel data (i.e., multiskip and
translation-invariance), we also train multilingual
embeddings in a smaller set of 12 languages for
which high-quality parallel data are available.11

training data: we use europarl en-xx parallel
data for the set of 12 languages. we obtain en-xx
bilingual dictionaries from two different sources.
for the set of 12 languages, we extract the bilin-
gual dictionaries from the europarl parallel cor-
pora. for the remaining 47 languages, dictio-
naries were formed by translating the 20k most
common words in the english monolingual corpus
with google translate, ignoring translation pairs
with identical surface forms and multi-word trans-
lations.

evaluation data: monolingual word similarity
uses the men dataset in bruni et al. (2014) as

10the 59-language set is { bg, cs, da, de, el, en, es,    , fr,
hu, it, sv, zh, af, ca, iw, cy, ar, ga, zu, et, gl, id, ru, nl, pt, la, tr,
ne, lv, lt, tg, ro, is, pl, yi, be, hy, hr, jw, ka, ht, fa, mi, bs, ja,
mg, tl, ms, uz, kk, sr, mn, ko, mk, so, uk, sl, sw }.

11the 12-language set is {bg, cs, da, de, el, en, es,    , fr,

hu, it, sv}.

a development set and stanford   s rare words
dataset in luong et al. (2013) as a test set. for
the cross-lingual word similarity task, we aggre-
gate the rg-65 datasets in six language pairs (fr-
es, fr-de, en-fr, en-es, en-de, de-es).
for the
word translation task, we use wiktionary to ex-
tract translationally-equivalent word pairs to eval-
uate multilingual embeddings for the set of 12 lan-
guages. since wiktionary-based translations do
not cover all 59 languages, we use google trans-
late to obtain en-xx bilingual dictionaries to eval-
uate the embeddings of 59 languages. for qvec
and qvec-cca, we split the english supersense
annotations used in tsvetkov et al. (2015) into a
development set and a test set. for multiqvec and
multiqvec-cca, we use supersense annotations
in english, italian and danish. for the document
classi   cation task, we use the multilingual rcv
corpus in seven languages (da, de, en, es, fr, it,
sv). for the id33 task, we use the
universal dependencies v1.1 in twelve languages
(bg, cs, da, de, el, en, es,    , fr, hu, it, sv).

setup: all id27s in the follow-
ing results are 512-dimensional vectors. meth-
ods which indirectly use skipgram (i.e., multi-
cca, multiskip, and multicluster) are trained
using 10 epochs of stochastic id119,
and use a context window of size 5.
the
translation-invariance method use a context win-
dow of size 3.12 we only estimate embeddings
for words/clusters which occur 5 times or more in
the monolingual corpora. in a postprocessing step,
all vectors are normalized to unit length. multi-
cluster uses a maximum cluster size of 1,000 and
10,000 for the set of 12 and 59 languages, respec-
tively.
in the english tasks (monolingual word
similarity, qvec, qvec-cca), skipgram embed-
dings (mikolov et al., 2013a) and multicca em-
beddings give identical results (since we project
words in other languages to the english vector
space, estimated using the skipgram model). the
software used to train all embeddings as well as
the trained embeddings are available for download
on the evaluation portal.13

we note that intrinsic evaluation of word em-
beddings (e.g., word similarity) typically ignores

12training translation-invariance embeddings with larger
context window sizes using the matlab implementation pro-
vided by gardner et al. (2015) is computationally challeng-
ing.

13urls to software libraries on github are redacted to

comply with the double-blind reviewing of conll.

test instances which are not covered by the embed-
dings being studied. when the vocabulary used in
two sets of id27s is different, which
is often the case, the intrinsic evaluation score for
each set may be computed based on a different set
of test instances, which may bias the results in un-
expected ways. for instance, if one set of embed-
dings only covers frequent words while the other
set also covers infrequent words, the scores of the
   rst set may be in   ated because frequent words
appear in many different contexts and are there-
fore easier to estimate than infrequent words. to
partially address this problem, we report the cov-
erage of each set of embeddings in square brack-
ets. when the difference in coverage is large, we
repeat the evaluation using only the intersection
of vocabularies covered by all embeddings being
evaluated. extrinsic evaluations are immune to
this problem because the score is computed based
on all test instances regardless of the coverage.

results [59 languages]. we train the proposed
dictionary-based estimation methods (multiclus-
ter and multicca) for 59 languages, and evalu-
ate the trained embeddings according to nine dif-
ferent metrics in table 2. the results show that,
when trained on a large number of languages, mul-
ticca consistently outperforms multicluster ac-
cording to all id74. note that most
differences in coverage between multicluster and
multicca are relatively small.

it is worth noting that the mainstream approach
of estimating one vector representation per word
type (rather than word token) ignores the fact that
the same word may have different semantics in dif-
ferent contexts. the multicluster method exacer-
bates this problem by estimating one vector repre-
sentation per cluster of translationally equivalent
words. the added semantic ambiguity severely
hurts the performance of multicluster with 59 lan-
guages, but it is still competitive with 12 languages
(see below).

results [12 languages]. we compare the pro-
posed dictionary-based estimation methods to par-
allel text-based methods in table 3. the ranking
of the four estimation methods is not consistent
across all id74. this is unsurprising
since each metric evaluates different traits of word
embeddings, as detailed in   3. however, some pat-
terns are worth noting in table 3.

in    ve of the evaluations (including both ex-

extrinsic
metrics

intrinsic
metrics

id33
document classi   cation
monolingual word similarity
id73 similarity
word translation
monolingual qvec
multiqvec
monolingual qvec-cca
multiqvec-cca

task multicluster multicca multiskip
57.7 [68.9]
90.4 [45.7]
33.9 [55.4]
59.5 [67.5]
46.7 [39.5]
8.4 [98.0]
8.7 [87.0]
58.9 [98.0]
36.3 [75.6]

61.0 [70.9]
92.1 [48.1]
38.0 [57.5]
58.1 [74.1]
43.7 [45.2]
10.3 [98.6]
9.3 [82.0]
62.4 [98.6]
43.3 [82.0]

58.7 [69.3]
92.1 [62.8]
43.0 [71.0]
66.6 [78.2]
35.7 [53.2]
10.7 [99.0]
8.7 [87.0]
63.4 [99.0]
41.5 [87.0]

invariance
59.8 [68.6]
91.1 [31.3]
51.0 [23.0]
58.7 [63.0]
63.9 [30.3]
8.1 [91.7]
5.3 [74.7]
65.8 [91.7]
46.2 [74.7]

table 3: results for multilingual embeddings that cover bulgarian, czech, danish, greek, english,
spanish, german, finnish, french, hungarian, italian and swedish. each row corresponds to one of
the embedding id74 we use (higher is better). each column corresponds to one of the
embedding estimation methods we consider; i.e., numbers in the same row are comparable. numbers in
square brackets are coverage percentages.

trinsic tasks),
the best performing method is a
dictionary-based one proposed in this paper.
in
the remaining four intrinsic methods,
the best
performing method is the translation-invariance
method. multiskip ranks last in    ve evaluations,
and never ranks    rst. since our implementation of
multiskip does not make use of monolingual data,
it only learns from monolingual contexts observed
in parallel corpora, it misses the opportunity to
learn from contexts in the much larger monolin-
gual corpora. trained for 12 languages, mul-
ticluster is competitive in four evaluations (and
ranks    rst in three).

we note that multicca consistently achieves
better coverage than the translation-invariance
method. for intrinsic measures, this confounds the
performance comparison. a partial solution is to
test only on word types for which all four methods
have a vector; this subset is in no sense a represen-
tative sample of the vocabulary. in this compari-
son (provided in the supplementary material), we
   nd a similar pattern of results, though multicca
outperforms the translation-invariance method on
the monolingual word similarity task. also,
the gap (between multicca and the translation-
invariance method) reduces to 0.7 in monolingual
qvec-cca and 2.5 in multiqvec-cca.

6 related work

there is a rich body of literature on bilingual em-
beddings, including work on machine translation
(zou et al., 2013; hermann and blunsom, 2014;
cho et al., 2014;
luong et al., 2015b;

alia),14

and

inter

cross-lingual

classi   cation

luong et al., 2015a,
cross-
lingual id33 (guo et al., 2015;
guo et al., 2016),
docu-
(klementiev et al., 2012;
ment
gouws et al., 2014;
kocisk`y et al., 2014).
al-rfou    et al. (2013) trained id27s
for more than 100 languages, but the embeddings
of each language are trained independently (i.e.,
embeddings of words in different languages do
not share the same vector space). word clusters
are a related form of distributional representation;
in id91, cross-lingual distributional repre-
sentations were proposed as well (och, 1999;
t  ackstr  om et al., 2012).
haghighi et al. (2008)
used cca to learn bilingual
lexicons from
monolingual corpora.

7 conclusion

we proposed two dictionary-based estimation
methods for multilingual id27s, mul-
ticca and multicluster, and used them to train
embeddings for 59 languages. we characterized
important shortcomings of the qvec previously
used to evaluate monolingual embeddings, and
proposed an improved metric multiqvec-cca.
both multiqvec and multiqvec-cca obtain bet-
ter correlations with downstream tasks compared
to intrinsic methods previously used in the litera-
ture. finally, in order to help future research in this
area, we created a web portal for users to upload
their multilingual embeddings and easily evaluate

14hermann and blunsom (2014) showed that the bicvm
method can be extended to more than two languages, but the
released software library only supports bilingual embeddings.

them on nine id74, with two modes
of operation (development and test) to encourage
sound experimentation practices.

acknowledgments

waleed ammar is supported by the google fellow-
ship in natural language processing. part of this
material is based upon work supported by a sub-
contract with raytheon bbn technologies corp.
under darpa prime contract no. hr0011-15-
c-0013. this work was supported in part by the
national science foundation through award iis-
1526745. we thank manaal faruqui, wang ling,
kazuya kawakami, matt gardner, benjamin wil-
son and the anonymous reviewers of the nw-nlp
workshop for helpful comments. we are also
grateful to h  ector mart    nez alonso for his help
with danish resources.

references
[agi  c et al.2015]   zeljko agi  c, maria jesus aranzabe,
aitziber atutxa, cristina bosco, jinho choi, marie-
catherine de marneffe, timothy dozat, rich  ard
farkas, jennifer foster, filip ginter, iakes goe-
naga, koldo gojenola, yoav goldberg, jan haji  c,
anders tr  rup johannsen, jenna kanerva, juha
kuokkala, veronika laippala, alessandro lenci,
krister lind  en, nikola ljube  si  c, teresa lynn,
christopher manning, h  ector alonso mart    nez,
ryan mcdonald, anna missil  a, simonetta monte-
magni, joakim nivre, hanna nurmi, petya osen-
ova, slav petrov, jussi piitulainen, barbara plank,
prokopis prokopidis, sampo pyysalo, wolfgang
seeker, mojgan seraji, natalia silveira, maria simi,
kiril simov, aaron smith, reut tsarfaty, veronika
vincze, and daniel zeman. 2015. universal de-
pendencies 1.1. lindat/clarin digital library at
institute of formal and applied linguistics, charles
university in prague.

[agirre et al.2009] eneko agirre, enrique alfonseca,
keith hall, jana kravalova, marius pas  ca, and aitor
soroa. 2009. a study on similarity and relatedness
using distributional and id138-based approaches.
in proc. of naacl, pages 19   27.

[al-rfou    et al.2013] rami al-rfou   , bryan perozzi,
and steven skiena. 2013. polyglot: distributed
word representations for multilingual nlp.
in
conll.

[bruni et al.2014] elia bruni, nam-khanh tran, and
marco baroni. 2014. multimodal distributional se-
mantics. jair.

[camacho-collados et al.2015] jos  e

camacho-
collados, mohammad taher pilehvar, and roberto
navigli. 2015. a framework for the construction

of monolingual and cross-lingual word similarity
datasets. in proc. of acl.

cho,

[cho et al.2014] kyunghyun

bart
van merri  enboer, caglar gulcehre, dzmitry
bahdanau, fethi bougares, holger schwenk, and
yoshua bengio. 2014. learning phrase represen-
tations using id56 encoder-decoder for statistical
machine translation. in proc. of emnlp.

[coulmance et al.2015] jocelyn coulmance, jean-marc
marty, guillaume wenzek, and amine benhal-
loum. 2015. trans-gram, fast cross-lingual word-
embeddings. in proc. of emnlp.

[dyer et al.2013] chris dyer, victor chahuneau, and
noah a. smith. 2013. a simple, fast, and effec-
tive reparameterization of ibm model 2. in proc. of
naacl.

[dyer et al.2015] chris dyer, miguel ballesteros,
wang ling, austin matthews, and noah a smith.
2015. transition-based id33 with
stack long short-term memory. in proc. of acl.

[faruqui and dyer2014] manaal faruqui and chris
dyer. 2014.
improving vector space word repre-
sentations using multilingual correlation. proc. of
eacl.

[gardner et al.2015] matt gardner, kejun huang,
evangelos papalexakis, xiao fu, partha talukdar,
christos faloutsos, nicholas sidiropoulos, and
tom mitchell. 2015. translation invariant word
embeddings. in proc. of emnlp.

[gouws et al.2014] stephan gouws, yoshua bengio,
and greg corrado. 2014. bilbowa: fast bilingual
distributed representations without word alignments.
arxiv preprint arxiv:1410.2455.

[guo et al.2015] jiang guo, wanxiang che, david
yarowsky, haifeng wang, and ting liu.
2015.
cross-lingual id33 based on dis-
tributed representations. in proc. of acl.

[guo et al.2016] jiang guo, wanxiang che, david
yarowsky, haifeng wang, and ting liu. 2016. a
representation learning framework for multi-source
transfer parsing. in proc. of aaai.

[gutmann and hyv  arinen2012] michael u gutmann
and aapo hyv  arinen. 2012. noise-contrastive esti-
mation of unnormalized statistical models, with ap-
plications to natural image statistics. jmlr.

[haghighi et al.2008] aria haghighi, percy liang, tay-
lor berg-kirkpatrick, and dan klein. 2008. learn-
ing bilingual lexicons from monolingual corpora. in
proc. of acl.

[hermann and blunsom2014] karl moritz hermann
and phil blunsom. 2014. multilingual models for
compositional id65. in proc. of
acl.

antonio zampolli, francesca fanciulli, maria mas-
setani, remo raffaelli, et al. 2003. building the
italian syntactic-semantic treebank.
in treebanks,
pages 189   210. springer.

[och1999] franz joseph och.

1999. an ef   cient
method for determining bilingual word classes. in
eacl.

[petrov et al.2012] slav petrov, dipanjan das, and
ryan mcdonald. 2012. a universal part-of-speech
tagset. in proc. of lrec.

[szegedy et al.2014] christian

szegedy, wojciech
ilya sutskever, joan bruna, dumitru
zaremba,
2014.
erhan, ian goodfellow, and rob fergus.
intriguing properties of neural networks. in proc. of
iclr.

[t  ackstr  om et al.2012] oscar t  ackstr  om, ryan mc-
donald, and jakob uszkoreit. 2012. cross-lingual
word clusters for direct transfer of linguistic struc-
ture. in proc. of naacl, pages 477   487.

[tsvetkov et al.2014] yulia tsvetkov, leonid boytsov,
anatole gershman, eric nyberg, and chris dyer.
2014. metaphor detection with cross-lingual model
transfer. in proc. of acl.

[tsvetkov et al.2015] yulia tsvetkov, manaal faruqui,
wang ling, guillaume lample, and chris dyer.
2015. evaluation of word vector representations by
subspace alignment. in proc. of emnlp.

[zeman and resnik2008] daniel zeman and philip
resnik. 2008. cross-language parser adaptation be-
tween related languages. in proc. of ijcnlp, pages
35   42.

[zou et al.2013] will y zou, richard socher, daniel m
cer, and christopher d manning. 2013. bilingual
id27s for phrase-based machine transla-
tion. in proc. of emnlp, pages 1393   1398.

[klementiev et al.2012] alexandre klementiev,

ivan
titov, and binod bhattarai.
inducing
crosslingual distributed representations of words. in
proc. of coling.

2012.

[kocisk`y et al.2014] tom  a  s kocisk`y, karl moritz her-
mann, and phil blunsom. 2014. learning bilingual
word representations by marginalizing alignments.
in arxiv preprint arxiv:1405.0947.

[leviant and reichart2015] ira leviant and roi re-
ichart. 2015. judgment language matters: towards
judgment language informed vector space modeling.
in arxiv preprint arxiv:1508.00106.

[luong et al.2013] minh-thang

richard
socher, and christopher d. manning. 2013. better
word representations with id56s
for morphology. in proc. of conll.

luong,

[luong et al.2015a] minh-thang

ilya
sutskever, quoc v le, oriol vinyals, and wo-
jciech zaremba. 2015a. addressing the rare word
problem in id4. in proc. of
acl.

luong,

[luong et al.2015b] thang luong, hieu pham, and
christopher d manning. 2015b. bilingual word
representations with monolingual quality in mind.
in proc. of naacl.

[mart    nez alonso et al.2015] h  ector mart    nez alonso,
anders johannsen, sussi olsen, sanni nimb,
nicolai hartvig s  rensen, anna braasch, anders
s  gaard, and bolette sandford pedersen. 2015. su-
persense tagging for danish.
in proc. of nodal-
ida, page 21.

[mart    nez alonso et al.2016] h  ector mart    nez alonso,
anders johannsen, sussi olsen, sanni nimb, and
bolette sandford pedersen. 2016. an empirically
grounded expansion of the supersense inventory. in
proc. of the global id138 conference.

[mcdonald et al.2011] ryan mcdonald, slav petrov,
2011. multi-source transfer
in proc. of

and keith hall.
of delexicalized dependency parsers.
emnlp, pages 62   72.

[mikolov et al.2013a] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013a. ef   cient esti-
mation of word representations in vector space. in
proc. of iclr.

[mikolov et al.2013b] tomas mikolov, quoc v. le, and
ilya sutskever.
exploiting similarities
among languages for machine translation. in arxiv
preprint arxiv:1309.4168v1.

2013b.

[miller et al.1993] george a. miller, claudia leacock,
randee tengi, and ross t. bunker. 1993. a seman-
tic concordance. in proc. of hlt, pages 303   308.

[montemagni et al.2003] simonetta

montemagni,
francesco barsotti, marco battista, nicoletta
calzolari, ornella corazzari, alessandro lenci,

