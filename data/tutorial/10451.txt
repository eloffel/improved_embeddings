on the properties of id4: encoder   decoder

approaches

kyunghyun cho

bart van merri  enboer

universit  e de montr  eal

dzmitry bahdanau   

jacobs university, germany

4
1
0
2

 
t
c
o
7

 

 
 
]
l
c
.
s
c
[
 
 

2
v
9
5
2
1

.

9
0
4
1
:
v
i
x
r
a

yoshua bengio

universit  e de montr  eal, cifar senior fellow

abstract

id4 is a relatively
new approach to statistical machine trans-
lation based purely on neural networks.
the id4 models of-
ten consist of an encoder and a decoder.
the encoder extracts a    xed-length repre-
sentation from a variable-length input sen-
tence, and the decoder generates a correct
translation from this representation. in this
paper, we focus on analyzing the proper-
ties of the id4 us-
ing two models; id56 encoder   decoder
and a newly proposed gated recursive con-
volutional neural network. we show that
the id4 performs
relatively well on short sentences without
unknown words, but its performance de-
grades rapidly as the length of the sentence
and the number of unknown words in-
crease. furthermore, we    nd that the pro-
posed gated recursive convolutional net-
work learns a grammatical structure of a
sentence automatically.

1

introduction

a new approach for statistical machine transla-
tion based purely on neural networks has recently
been proposed (kalchbrenner and blunsom, 2013;
sutskever et al., 2014). this new approach, which
we refer to as id4, is in-
spired by the recent trend of deep representational
learning. all the neural network models used in
(kalchbrenner and blunsom, 2013; sutskever et
al., 2014; cho et al., 2014) consist of an encoder
and a decoder. the encoder extracts a    xed-length
vector representation from a variable-length input
sentence, and from this representation the decoder

    research done while visiting universit  e de montr  eal

generates a correct, variable-length target transla-
tion.

the emergence of the neural machine transla-
tion is highly signi   cant, both practically and the-
oretically. id4 models re-
quire only a fraction of the memory needed by
traditional id151 (smt)
models. the models we trained for this paper
require only 500mb of memory in total. this
stands in stark contrast with existing smt sys-
tems, which often require tens of gigabytes of
memory. this makes the neural machine trans-
lation appealing in practice. furthermore, un-
like conventional translation systems, each and ev-
ery component of the neural translation model is
trained jointly to maximize the translation perfor-
mance.

as this approach is relatively new, there has not
been much work on analyzing the properties and
behavior of these models. for instance: what
are the properties of sentences on which this ap-
proach performs better? how does the choice of
source/target vocabulary affect the performance?
in which cases does the id4
fail?

it is crucial to understand the properties and be-
havior of this new id4 ap-
proach in order to determine future research di-
rections. also, understanding the weaknesses and
strengths of id4 might lead
to better ways of integrating smt and neural ma-
chine translation systems.

in this paper, we analyze two neural machine
translation models. one of them is the id56
encoder   decoder that was proposed recently in
(cho et al., 2014). the other model replaces the
encoder in the id56 encoder   decoder model with
a novel neural network, which we call a gated
recursive convolutional neural network (grconv).
we evaluate these two models on the task of trans-
lation from french to english.

our analysis shows that the performance of
the id4 model degrades
quickly as the length of a source sentence in-
creases. furthermore, we    nd that the vocabulary
size has a high impact on the translation perfor-
mance. nonetheless, qualitatively we    nd that the
both models are able to generate correct transla-
tions most of the time. furthermore, the newly
proposed grconv model is able to learn, without
supervision, a kind of syntactic structure over the
source language.
2 neural networks for variable-length

sequences

in this section, we describe two types of neural
networks that are able to process variable-length
sequences. these are the recurrent neural net-
work and the proposed gated recursive convolu-
tional neural network.

2.1 recurrent neural network with gated

hidden neurons

(a)

(b)

figure 1: the graphical illustration of (a) the re-
current neural network and (b) the hidden unit that
adaptively forgets and remembers.

a recurrent neural network (id56, fig. 1 (a))
works on a variable-length sequence x =
(x1, x2,       , xt ) by maintaining a hidden state h
over time. at each timestep t, the hidden state h(t)
is updated by

(cid:16)

(cid:17)

h(t) = f

h(t   1), xt

,

where f is an activation function. often f is as
simple as performing a linear transformation on
the input vectors, summing them, and applying an
element-wise logistic sigmoid function.

an id56 can be used effectively to learn a dis-
tribution over a variable-length sequence by learn-
ing the distribution over the next input p(xt+1 |
xt,       , x1). for instance, in the case of a se-
quence of 1-of-k vectors, the distribution can be
learned by an id56 which has as an output
p(xt,j = 1 | xt   1, . . . , x1) =

exp(cid:0)wjh(cid:104)t(cid:105)(cid:1)
j(cid:48)=1 exp(cid:0)wj(cid:48)h(cid:104)t(cid:105)(cid:1) ,
(cid:80)k

t(cid:89)

for all possible symbols j = 1, . . . , k, where wj
are the rows of a weight matrix w. this results in
the joint distribution

p(x) =

p(xt | xt   1, . . . , x1).

t=1

recently, in (cho et al., 2014) a new activation
function for id56s was proposed. the new activa-
tion function augments the usual logistic sigmoid
activation function with two gating units called re-
set, r, and update, z, gates. each gate depends on
the previous hidden state h(t   1), and the current
input xt controls the    ow of information. this is
reminiscent of long short-term memory (lstm)
units (hochreiter and schmidhuber, 1997). for
details about this unit, we refer the reader to (cho
et al., 2014) and fig. 1 (b). for the remainder of
this paper, we always use this new activation func-
tion.

2.2 gated recursive convolutional neural

network

besides id56s, another natural approach to deal-
ing with variable-length sequences is to use a re-
cursive convolutional neural network where the
parameters at each level are shared through the
whole network (see fig. 2 (a)). in this section, we
introduce a binary convolutional neural network
whose weights are recursively applied to the input
sequence until it outputs a single    xed-length vec-
tor. in addition to a usual convolutional architec-
ture, we propose to use the previously mentioned
gating mechanism, which allows the recursive net-
work to learn the structure of the source sentences
on the    y.
let x = (x1, x2,       , xt ) be an input sequence,
where xt     rd. the proposed gated recursive
convolutional neural network (grconv) consists of
four weight matrices wl, wr, gl and gr. at
each recursion level t     [1, t     1], the activation
of the j-th hidden unit h(t)
j

h(t)
j =   c  h(t)

j +   lh(t   1)

is computed by
j   1 +   rh(t   1)

j

,

(1)

where   c,   l and   r are the values of a gater that
sum to 1. the hidden unit is initialized as

h(0)
j = uxj,

where u projects the input into a hidden space.

zrhh~x(a)

(b)

(c)

(d)

figure 2: the graphical illustration of (a) the recursive convolutional neural network and (b) the proposed
gated unit for the recursive convolutional neural network. (c   d) the example structures that may be
learned with the proposed gated unit.

1
z

exp

glh(t)

j   1 + grh(t)

j

figure 3: the encoder   decoder architecture

,

(cid:16)

the new activation   h(t)
j

is computed as usual:

  h(t)
j =   

wlh(t)

j   1 + wrh(t)

j

,

where    is an element-wise nonlinearity.

the gating coef   cients      s are computed by

         c

  l
  r

       =
(cid:104)
3(cid:88)

k=1

(cid:16)

(cid:16)

where gl, gr     r3  d and

(cid:17)

(cid:17)

(cid:17)(cid:105)

.

k

z =

exp

glh(t)

j   1 + grh(t)

j

according to this activation, one can think of
the activation of a single node at recursion level t
as a choice between either a new activation com-
puted from both left and right children, the acti-
vation from the left child, or the activation from
the right child. this choice allows the overall
structure of the recursive convolution to change
adaptively with respect to an input sample. see
fig. 2 (b) for an illustration.

in this respect, we may even consider the pro-
posed grconv as doing a kind of unsupervised
parsing.
if we consider the case where the gat-
ing unit makes a hard decision, i.e.,    follows an
1-of-k coding, it is easy to see that the network
adapts to the input and forms a tree-like structure
(see fig. 2 (c   d)). however, we leave the further
investigation of the structure learned by this model
for future research.

3 purely id4
3.1 encoder   decoder approach
the task of translation can be understood from the
perspective of machine learning as learning the

conditional distribution p(f | e) of a target sen-
tence (translation) f given a source sentence e.
once the conditional distribution is learned by a
model, one can use the model to directly sample
a target sentence given a source sentence, either
by actual sampling or by using a (approximate)
search algorithm to    nd the maximum of the dis-
tribution.

a number of recent papers have proposed to
use neural networks to directly learn the condi-
tional distribution from a bilingual, parallel cor-
pus (kalchbrenner and blunsom, 2013; cho et al.,
2014; sutskever et al., 2014). for instance, the au-
thors of (kalchbrenner and blunsom, 2013) pro-
posed an approach involving a convolutional n-
gram model to extract a    xed-length vector of a
source sentence which is decoded with an inverse
convolutional id165 model augmented with an
id56. in (sutskever et al., 2014), an id56 with
lstm units was used to encode a source sentence
and starting from the last hidden state, to decode a
target sentence. similarly, the authors of (cho et
al., 2014) proposed to use an id56 to encode and
decode a pair of source and target phrases.

at the core of all these recent works lies an
encoder   decoder architecture (see fig. 3). the
encoder processes a variable-length input (source
sentence) and builds a    xed-length vector repre-
sentation (denoted as z in fig. 3). conditioned on

  h~economic growth has slowed down in recent years .la croissance   conomique a ralenti ces derni  res ann  es .[z  ,z  , ... ,z  ]12dencodedecodethe encoded representation, the decoder generates
a variable-length sequence (target sentence).

before (sutskever et al., 2014) this encoder   
decoder approach was used mainly as a part of the
existing id151 (smt) sys-
tem. this approach was used to re-rank the n-best
list generated by the smt system in (kalchbren-
ner and blunsom, 2013), and the authors of (cho
et al., 2014) used this approach to provide an ad-
ditional score for the existing phrase table.

in this paper, we concentrate on analyzing the
direct translation performance, as in (sutskever et
al., 2014), with two model con   gurations. in both
models, we use an id56 with the gated hidden
unit (cho et al., 2014), as this is one of the only
options that does not require a non-trivial way to
determine the target length. the    rst model will
use the same id56 with the gated hidden unit as
an encoder, as in (cho et al., 2014), and the second
one will use the proposed gated recursive convo-
lutional neural network (grconv). we aim to un-
derstand the inductive bias of the encoder   decoder
approach on the translation performance measured
by id7.

4 experiment settings
4.1 dataset
we evaluate the encoder   decoder models on the
task of english-to-french translation. we use the
bilingual, parallel corpus which is a set of 348m
selected by the method in (axelrod et al., 2011)
from a combination of europarl (61m words),
news commentary (5.5m), un (421m) and two
crawled corpora of 90m and 780m words respec-
tively.1 we did not use separate monolingual data.
the performance of the neural machien transla-
tion models was measured on the news-test2012,
news-test2013 and news-test2014 sets ( 3000 lines
each). when comparing to the smt system, we
use news-test2012 and news-test2013 as our de-
velopment set for tuning the smt system, and
news-test2014 as our test set.

among all the sentence pairs in the prepared
parallel corpus, for reasons of computational ef-
   ciency we only use the pairs where both english
and french sentences are at most 30 words long to
train neural networks. furthermore, we use only
the 30,000 most frequent words for both english

1all

the data

can be downloaded from http:
//www-lium.univ-lemans.fr/  schwenk/cslm_
joint_paper/.

and french. all the other rare words are consid-
ered unknown and are mapped to a special token
([unk]).

4.2 models
we train two models: the id56 encoder   
decoder (id56enc)(cho et al., 2014) and the
newly proposed gated recursive convolutional
neural network (grconv). note that both models
use an id56 with gated hidden units as a decoder
(see sec. 2.1).

we use minibatch stochastic id119
with adadelta (zeiler, 2012) to train our two mod-
els. we initialize the square weight matrix (transi-
tion matrix) as an orthogonal matrix with its spec-
tral radius set to 1 in the case of the id56enc and
0.4 in the case of the grconv. tanh and a recti   er
(max(0, x)) are used as the element-wise nonlin-
ear functions for the id56enc and grconv respec-
tively.

the grconv has 2000 hidden neurons, whereas
the id56enc has 1000 hidden neurons. the word
embeddings are 620-dimensional in both cases.2
both models were trained for approximately 110
hours, which is equivalent to 296,144 updates and
846,322 updates for the grconv and id56enc, re-
spectively.3

4.2.1 translation using beam-search
we use a basic form of beam-search to    nd a trans-
lation that maximizes the id155
given by a speci   c model (in this case, either the
id56enc or the grconv). at each time step of
the decoder, we keep the s translation candidates
with the highest log-id203, where s = 10
is the beam-width. during the beam-search, we
exclude any hypothesis that includes an unknown
word. for each end-of-sequence symbol that is se-
lected among the highest scoring candidates the
beam-width is reduced by one, until the beam-
width reaches zero.

the beam-search to (approximately)    nd a se-
quence of maximum log-id203 under id56
was proposed and used successfully in (graves,
2012) and (boulanger-lewandowski et al., 2013).
recently, the authors of (sutskever et al., 2014)
found this approach to be effective in purely neu-
ral machine translation based on lstm units.

2in all cases, we train the whole network including the

id27 matrix.

3the code will be available online, should the paper be

accepted.

model
id56enc
grconv
moses

moses+id56enc(cid:63)
moses+lstm   

development

13.15
9.97
30.64
31.48

32

id56enc
grconv
moses

21.01
17.19
32.77

(a) all lengths

test
13.92
9.97
33.30
34.64
35.65
23.45
18.22
35.63

l
l

a

model
id56enc
grconv
moses
k id56enc
n
grconv
u
moses
o
n

development

19.12
16.60
28.92
24.73
21.74
32.20

test
20.99
17.50
32.00
27.03
22.94
35.40

(b) 10   20 words

l
l

a

k
n
u
o
n

table 1: id7 scores computed on the development and test sets. the top three rows show the scores on
all the sentences, and the bottom three rows on the sentences having no unknown words. ((cid:63)) the result
reported in (cho et al., 2014) where the id56enc was used to score phrase pairs in the phrase table. (   )
the result reported in (sutskever et al., 2014) where an encoder   decoder with lstm units was used to
re-rank the n-best list generated by moses.

when we use the beam-search to    nd the k best
translations, we do not use a usual log-id203
but one normalized with respect to the length of
the translation. this prevents the id56 decoder
from favoring shorter translations, behavior which
was observed earlier in, e.g., (graves, 2013).
5 results and analysis
5.1 quantitative analysis
in this paper, we are interested in the properties
of the id4 models. specif-
ically, the translation quality with respect to the
length of source and/or target sentences and with
respect to the number of words unknown to the
model in each source/target sentence.

first, we look at how the id7 score, re   ect-
ing the translation performance, changes with re-
spect to the length of the sentences (see fig. 4 (a)   
(b)). clearly, both models perform relatively well
on short sentences, but suffer signi   cantly as the
length of the sentences increases.

we observe a similar trend with the number of
unknown words, in fig. 4 (c). as expected, the
performance degrades rapidly as the number of
unknown words increases. this suggests that it
will be an important challenge to increase the size
of vocabularies used by the neural machine trans-
lation system in the future. although we only
present the result with the id56enc, we observed
similar behavior for the grconv as well.

in table 1 (a), we present the translation perfor-
mances obtained using the two models along with
the baseline phrase-based smt system.4 clearly
4we used moses as a baseline, trained with additional

the phrase-based smt system still shows the su-
perior performance over the proposed purely neu-
ral machine translation system, but we can see that
under certain conditions (no unknown words in
both source and reference sentences), the differ-
ence diminishes quite signi   cantly. furthermore,
if we consider only short sentences (10   20 words
per sentence), the difference further decreases (see
table 1 (b).

furthermore, it is possible to use the neural ma-
chine translation models together with the existing
phrase-based system, which was found recently in
(cho et al., 2014; sutskever et al., 2014) to im-
prove the overall translation performance (see ta-
ble 1 (a)).

this analysis suggests that that the current neu-
ral translation approach has its weakness in han-
dling long sentences. the most obvious explana-
tory hypothesis is that the    xed-length vector rep-
resentation does not have enough capacity to en-
code a long sentence with complicated structure
and meaning. in order to encode a variable-length
sequence, a neural network may    sacri   ce    some
of the important topics in the input sentence in or-
der to remember others.

this is in stark contrast to the conventional
phrase-based machine translation system (koehn
et al., 2003). as we can see from fig. 5, the
conventional system trained on the same dataset
(with additional monolingual data for the language
model) tends to get a higher id7 score on longer
sentences.

in fact, if we limit the lengths of both the source

monolingual data for a 4-gram language model.

source

reference

id56enc

grconv

moses

source

reference

id56enc

grconv

moses

source

reference

id56enc

grconv

moses

she explained her new position of foreign affairs and security policy representative as a reply to a
question:    who is the european union? which phone number should i call?   ; i.e. as an important step
to uni   cation and better clarity of union   s policy towards countries such as china or india.
elle a expliqu  e le nouveau poste de la haute repr  esentante pour les affaires   etrang`eres et la politique de
d  efense dans le cadre d   une r  eponse `a la question:    qui est qui `a l   union europ  eenne?       a quel num  ero
de t  el  ephone dois-je appeler?   , donc comme un pas important vers l   unicit  e et une plus grande lisibilit  e
de la politique de l   union face aux   etats, comme est la chine ou bien l   inde.
elle a d  ecrit sa position en mati`ere de politique   etrang`ere et de s  ecurit  e ainsi que la politique de l   union
europ  eenne en mati`ere de gouvernance et de d  emocratie .
elle a expliqu  e sa nouvelle politique   etrang`ere et de s  ecurit  e en r  eponse `a un certain nombre de questions
:    qu   est-ce que l   union europ  eenne ?     .
elle a expliqu  e son nouveau poste des affaires   etrang`eres et la politique de s  ecurit  e repr  esentant en
r  eponse `a une question:    qui est l   union europ  eenne? quel num  ero de t  el  ephone dois-je appeler?   ;
c   est comme une   etape importante de l   uni   cation et une meilleure lisibilit  e de la politique de l   union
`a des pays comme la chine ou l   inde .

the investigation should be complete by the end of the year when the    ndings will be presented to
deutsche bank   s board of managing directors - with recommendations for action.
l   examen doit   etre termin  e d   ici la    n de l   ann  ee, ensuite les r  esultats du conseil d   administration de la
deutsche bank doivent   etre pr  esent  es - avec recommandation, d    habitude.
l     etude devrait   etre termin  ee `a la    n de l    ann  ee, lorsque les conclusions seront pr  esent  ees au conseil
d   administration de la deutsche bank, conseil d   association avec des mesures.
l   enqu  ete devrait   etre termin  ee `a la    n de l   ann  ee o`u les conclusions seront pr  esent  ees par le conseil
d   administration de la bce `a la direction des recommandations.
l   enqu  ete devrait   etre termin  e d   ici la    n de l   ann  ee lorsque les r  esultats seront pr  esent  es `a la deutsche
bank conseil des directeurs g  en  eraux - avec des recommandations .

and there are thorny mechanical questions that must be resolved during that time, like how to bal-
ance the state   s mandate of    adequate access    to licensed marijuana with its prohibitions on cannabis
businesses within 1,000 feet of a school, park, playground or child care center.
pendant ce temps, des questions pratiques restent en suspens: comment   equilibrer le mandat de l     etat
qui garantit un acc`es appropri  e `a la marijuana agr  e  ee et interdit l   installation de commerces de vente de
cannabis dans un rayon de 30 km autour d   une   ecole, d   un parc, d   un terrain de jeu ou d   une cr`eche.
il y a des questions pr  eventives qui se posent quant `a l     equilibre des droits de l   enfant dans les limites
d   une entreprise de collecte de sang.
de fac  on g  en  erale, il y a des raisons de s  ecurit  e pour que les entreprises aient acc`es `a des milliers de
centres de p  eche, d   eau ou de recherche.
et il y a des probl`emes m  ecaniques complexes qui doivent   etre r  esolues au cours de cette p  eriode,
comme la mani`ere d     equilibrer le mandat de    l   acc`es ad  equat    permis de marijuana avec l   interdiction
du cannabis aux entreprises de 1000 pieds d   une   ecole de jeu ou de parc, le service de garde.

(a) long sentences

source

reference
id56enc
grconv
moses

there is still no agreement as to which election rules to follow.
toutefois il n   existe toujours pas d   accord selon quel r`eglement de vote il faut proc  eder.
il n   y a pas encore d   accord sur les r`egles   electorales.
il n   y a pas encore d   accord sur la question des   elections `a suivre.
il y a toujours pas d   accord sur l     election des r`egles `a suivre.

source

many of these ideas may have been creative, but they didn   t necessarily work.

reference beaucoup de ces id  ees   etaient cr  eatives mais elles n   ont pas forc  ement fonctionn  e.
id56enc
grconv
moses

bon nombre de ces id  ees ont peut-  etre   et  e cr  eatrices, mais elles ne s   appliquaient pas n  ecessairement.
beaucoup de ces id  ees peuvent   etre cr  eatives, mais elles n   ont pas fonctionn  e.
beaucoup de ces id  ees ont pu   etre cr  eatif, mais ils n   ont pas n  ecessairement.

source

there is a lot of consensus between the left and the right on this subject.
reference c   est qu   il y a sur ce sujet un assez large consensus entre gauche et droite.
id56enc
grconv
moses

il existe beaucoup de consensus entre la gauche et le droit `a la question.
il y a un consensus entre la gauche et le droit sur cette question.
il y a beaucoup de consensus entre la gauche et la droite sur ce sujet.

source

reference
id56enc
grconv
moses

according to them, one can    nd any weapon at a low price right now.
selon eux, on peut trouver aujourd   hui `a moscou n   importe quelle arme pour un prix raisonnable.
selon eux, on peut se trouver de l   arme `a un prix trop bas.
en tout cas, ils peuvent trouver une arme `a un prix tr`es bas `a la fois.
selon eux, on trouve une arme `a bas prix pour l   instant.
(b) short sentences

table 2: the sample translations along with the source sentences and the reference translations.

(a) id56enc

(b) grconv

(c) id56enc

figure 4: the id7 scores achieved by (a) the id56enc and (b) the grconv for sentences of a given
length. the plot is smoothed by taking a window of size 10. (c) the id7 scores achieved by the id56
model for sentences with less than a given number of unknown words.

sentence and the reference translation to be be-
tween 10 and 20 words and use only the sentences
with no unknown words, the id7 scores on the
test set are 27.81 and 33.08 for the id56enc and
moses, respectively.

note that we observed a similar trend even
when we used sentences of up to 50 words to train
these models.

5.2 qualitative analysis
although id7 score is used as a de-facto stan-
dard metric for evaluating the performance of a
machine translation system, it is not the perfect
metric (see, e.g., (song et al., 2013; liu et al.,
2011)). hence, here we present some of the ac-
tual translations generated from the two models,
id56enc and grconv.

in table. 2 (a)   (b), we show the translations of
some randomly selected sentences from the de-
velopment and test sets. we chose the ones that
have no unknown words. (a) lists long sentences
(longer than 30 words), and (b) short sentences
(shorter than 10 words). we can see that, despite
the difference in the id7 scores, all three mod-
els (id56enc, grconv and moses) do a decent job
at translating, especially, short sentences. when
the source sentences are long, however, we no-
tice the performance degradation of the neural ma-
chine translation models.

additionally, we present here what

type of
structure the proposed gated recursive convolu-
tional network learns to represent. with a sample
sentence    obama is the president of the united
states   , we present the parsing structure learned
by the grconv encoder and the generated transla-
tions, in fig. 6. the    gure suggests that the gr-
conv extracts the vector representation of the sen-

figure 5: the id7 scores achieved by an smt
system for sentences of a given length. the plot
is smoothed by taking a window of size 10. we
use the solid, dotted and dashed lines to show the
effect of different lengths of source, reference or
both of them, respectively.

tence by    rst merging    of the united states    to-
gether with    is the president of    and    nally com-
bining this with    obama is    and    .   , which is well
correlated with our intuition.

despite the lower performance the grconv
showed compared to the id56 encoder   decoder,5
we    nd this property of the grconv learning a
grammar structure automatically interesting and
believe further investigation is needed.

6 conclusion and discussion

in this paper, we have investigated the property
of a recently introduced family of machine trans-

5however, it should be noted that the number of gradient
updates used to train the grconv was a third of that used to
train the id56enc. longer training may change the result,
but for a fair comparison we chose to compare models which
were trained for an equal amount of time. neither model was
trained to convergence.

01020304050607080sentencelength05101520id7scoresourcetextreferencetextboth01020304050607080sentencelength05101520id7scoresourcetextreferencetextboth0246810max.numberofunknownwords1012141618202224id7scoresourcetextreferencetextboth01020304050607080sentencelength0510152025303540id7scoresourcetextreferencetextbothtranslations
obama est le pr  esident des   etats-unis . (2.06)
obama est le pr  esident des   etats-unis . (2.09)
obama est le pr  esident des etats-unis . (2.61)
obama est le pr  esident des etats-unis . (3.33)
barack obama est le pr  esident des   etats-unis . (4.41)
barack obama est le pr  esident des   etats-unis . (4.48)
barack obama est le pr  esident des etats-unis . (4.54)
l   obama est le pr  esident des   etats-unis . (4.59)
l   obama est le pr  esident des   etats-unis . (4.67)
obama est pr  esident du congr`es des   etats-unis .(5.09)

(a)

(b)

figure 6: (a) the visualization of the grconv structure when the input is    obama is the president of
the united states.   . only edges with gating coef   cient    higher than 0.1 are shown. (b) the top-10
translations generated by the grconv. the numbers in parentheses are the negative log-id203 of the
translations.

lation system based purely on neural networks.
we focused on evaluating an encoder   decoder ap-
proach, proposed recently in (kalchbrenner and
blunsom, 2013; cho et al., 2014; sutskever et al.,
2014), on the task of sentence-to-sentence trans-
lation. among many possible encoder   decoder
models we speci   cally chose two models that dif-
fer in the choice of the encoder; (1) id56 with
gated hidden units and (2) the newly proposed
gated recursive convolutional neural network.

after training those two models on pairs of
english and french sentences, we analyzed their
performance using id7 scores with respect to
the lengths of sentences and the existence of un-
known/rare words in sentences. our analysis re-
vealed that the performance of the neural machine
translation suffers signi   cantly from the length of
sentences. however, qualitatively, we found that
the both models are able to generate correct trans-
lations very well.

these analyses suggest a number of future re-
search directions in machine translation purely
based on neural networks.

firstly, it is important to    nd a way to scale up
training a neural network both in terms of com-
putation and memory so that much larger vocabu-
laries for both source and target languages can be
used. especially, when it comes to languages with
rich morphology, we may be required to come up
with a radically different approach in dealing with

words.

secondly, more research is needed to prevent
the id4 system from under-
performing with long sentences. lastly, we need
to explore different neural architectures, especially
for the decoder. despite the radical difference in
the architecture between id56 and grconv which
were used as an encoder, both models suffer from
the curse of sentence length. this suggests that it
may be due to the lack of representational power
in the decoder. further investigation and research
are required.

in addition to the property of a general neural
machine translation system, we observed one in-
teresting property of the proposed gated recursive
convolutional neural network (grconv). the gr-
conv was found to mimic the grammatical struc-
ture of an input sentence without any supervision
on syntactic structure of language. we believe this
property makes it appropriate for natural language
processing applications other than machine trans-
lation.

acknowledgments

the authors would like to acknowledge the sup-
port of the following agencies for research funding
and computing support: nserc, calcul qu  ebec,
compute canada,
the canada research chairs
and cifar.

obamaisthepresidentoftheunitedstates.++++++++++++++++++++++++++++++++++++matthew d. zeiler.

2012. adadelta: an adap-
tive learning rate method. technical report, arxiv
1212.5701.

references
amittai axelrod, xiaodong he, and jianfeng gao.
2011. id20 via pseudo in-domain data
selection. in proceedings of the acl conference on
empirical methods in natural language processing
(emnlp), pages 355   362. association for compu-
tational linguistics.

nicolas boulanger-lewandowski, yoshua bengio, and
pascal vincent. 2013. audio chord recognition with
recurrent neural networks. in ismir.

kyunghyun cho, bart van merrienboer, caglar gul-
cehre, fethi bougares, holger schwenk, and yoshua
bengio.
2014. learning phrase representations
using id56 encoder-decoder for statistical machine
the empiricial
translation.
methods in natural language processing (emnlp
2014), october. to appear.

in proceedings of

alex graves. 2012. sequence transduction with re-
in proceedings of the
current neural networks.
29th international conference on machine learning
(icml 2012).

a. graves. 2013. generating sequences with recurrent
neural networks. arxiv:1308.0850 [cs.ne],
august.

s. hochreiter and j. schmidhuber. 1997. long short-
term memory. neural computation, 9(8):1735   
1780.

nal kalchbrenner and phil blunsom. 2013. two re-
current continuous translation models. in proceed-
ings of the acl conference on empirical methods
in natural language processing (emnlp), pages
1700   1709. association for computational linguis-
tics.

philipp koehn, franz josef och, and daniel marcu.
2003. statistical phrase-based translation. in pro-
ceedings of the 2003 conference of the north amer-
ican chapter of the association for computational
linguistics on human language technology - vol-
ume 1, naacl    03, pages 48   54, stroudsburg, pa,
usa. association for computational linguistics.

chang liu, daniel dahlmeier, and hwee tou ng.
2011. better id74 lead to better ma-
chine translation. in proceedings of the conference
on empirical methods in natural language pro-
cessing, pages 375   384. association for computa-
tional linguistics.

xingyi song, trevor cohn, and lucia specia. 2013.
id7 deconstructed: designing a better mt eval-
uation metric. in proceedings of the 14th interna-
tional conference on intelligent text processing and
computational linguistics (cicling), march.

ilya sutskever, oriol vinyals, and quoc le. 2014. se-
quence to sequence learning with neural networks.
in advances in neural information processing sys-
tems (nips 2014), december.

