improved semantic representations from

tree-structured id137

kai sheng tai, richard socher*, christopher d. manning

computer science department, stanford university, *metamind inc.

kst@cs.stanford.edu, richard@metamind.io, manning@stanford.edu

abstract

because of their superior ability to pre-
serve sequence information over time,
long short-term memory (lstm) net-
works, a type of recurrent neural net-
work with a more complex computational
unit, have obtained strong results on a va-
riety of sequence modeling tasks. the
only underlying lstm structure that has
been explored so far is a linear chain.
however, natural language exhibits syn-
tactic properties that would naturally com-
bine words to phrases. we introduce the
tree-lstm, a generalization of lstms to
tree-structured network topologies. tree-
lstms outperform all existing systems
and strong lstm baselines on two tasks:
predicting the semantic relatedness of two
sentences (semeval 2014, task 1) and
sentiment classi   cation (stanford senti-
ment treebank).
introduction

1
most models for distributed representations of
phrases and sentences   that is, models where real-
valued vectors are used to represent meaning   fall
into one of three classes: bag-of-words models,
sequence models, and tree-structured models. in
bag-of-words models, phrase and sentence repre-
sentations are independent of word order; for ex-
ample, they can be generated by averaging con-
stituent word representations (landauer and du-
mais, 1997; foltz et al., 1998).
in contrast, se-
quence models construct sentence representations
as an order-sensitive function of the sequence of
tokens (elman, 1990; mikolov, 2012). lastly,
tree-structured models compose each phrase and
sentence representation from its constituent sub-
phrases according to a given syntactic structure
over the sentence (goller and kuchler, 1996;
socher et al., 2011).

y1

x1

y2

x2

y2

x2
y1

x1

y4

x4

y3

x3

y3

x5

y4

x4

y6

x6

figure 1: top: a chain-structured lstm net-
work. bottom: a tree-structured lstm network
with arbitrary branching factor.

order-insensitive models are insuf   cient

to
fully capture the semantics of natural language
due to their inability to account for differences in
meaning as a result of differences in word order
or syntactic structure (e.g.,    cats climb trees    vs.
   trees climb cats   ). we therefore turn to order-
sensitive sequential or tree-structured models. in
particular, tree-structured models are a linguisti-
cally attractive option due to their relation to syn-
tactic interpretations of sentence structure. a nat-
ural question, then, is the following: to what ex-
tent (if at all) can we do better with tree-structured
models as opposed to sequential models for sen-
tence representation? in this paper, we work to-
wards addressing this question by directly com-
paring a type of sequential model that has recently
been used to achieve state-of-the-art results in sev-
eral nlp tasks against its tree-structured general-
ization.

due to their capability for processing arbitrary-
recurrent neural networks

length sequences,

5
1
0
2

 

y
a
m
0
3

 

 
 
]
l
c
.
s
c
[
 
 

3
v
5
7
0
0
0

.

3
0
5
1
:
v
i
x
r
a

(id56s) are a natural choice for sequence model-
ing tasks. recently, id56s with long short-term
memory (lstm) units (hochreiter and schmid-
huber, 1997) have re-emerged as a popular archi-
tecture due to their representational power and ef-
fectiveness at capturing long-term dependencies.
id137, which we review in sec. 2, have
been successfully applied to a variety of sequence
modeling and prediction tasks, notably machine
translation (bahdanau et al., 2014; sutskever et al.,
2014), id103 (graves et al., 2013),
image id134 (vinyals et al., 2014),
and program execution (zaremba and sutskever,
2014).

in this paper, we introduce a generalization of
the standard lstm architecture to tree-structured
network topologies and show its superiority for
representing sentence meaning over a sequential
lstm. while the standard lstm composes its
hidden state from the input at the current time
step and the hidden state of the lstm unit in the
previous time step, the tree-structured lstm, or
tree-lstm, composes its state from an input vec-
tor and the hidden states of arbitrarily many child
units. the standard lstm can then be considered
a special case of the tree-lstm where each inter-
nal node has exactly one child.

in our evaluations, we demonstrate the empiri-
cal strength of tree-lstms as models for repre-
senting sentences. we evaluate the tree-lstm
architecture on two tasks: semantic relatedness
prediction on sentence pairs and sentiment clas-
si   cation of sentences drawn from movie reviews.
our experiments show that tree-lstms outper-
form existing systems and sequential lstm base-
lines on both tasks. implementations of our mod-
els and experiments are available at https://
github.com/stanfordnlp/treelstm.

2 id137
2.1 overview
recurrent neural networks (id56s) are able to pro-
cess input sequences of arbitrary length via the re-
cursive application of a transition function on a
hidden state vector ht. at each time step t, the
hidden state ht is a function of the input vector xt
that the network receives at time t and its previous
hidden state ht   1. for example, the input vector xt
could be a vector representation of the t-th word in
body of text (elman, 1990; mikolov, 2012). the
hidden state ht     rd can be interpreted as a d-

dimensional distributed representation of the se-
quence of tokens observed up to time t.

commonly, the id56 transition function is an
af   ne transformation followed by a pointwise non-
linearity such as the hyperbolic tangent function:

ht = tanh (w xt + u ht   1 + b) .

unfortunately, a problem with id56s with transi-
tion functions of this form is that during training,
components of the gradient vector can grow or de-
cay exponentially over long sequences (hochre-
iter, 1998; bengio et al., 1994). this problem with
exploding or vanishing gradients makes it dif   cult
for the id56 model to learn long-distance correla-
tions in a sequence.

(hochreiter

the lstm architecture

and
schmidhuber, 1997) addresses this problem of
learning long-term dependencies by introducing a
memory cell that is able to preserve state over long
periods of time. while numerous lstm variants
have been described, here we describe the version
used by zaremba and sutskever (2014).

we de   ne the lstm unit at each time step t to
be a collection of vectors in rd: an input gate it, a
forget gate ft, an output gate ot, a memory cell ct
and a hidden state ht. the entries of the gating
vectors it, ft and ot are in [0, 1]. we refer to d as
the memory dimension of the lstm.

the lstm transition equations are the follow-

ing:

it =   

ft =   

ot =   

(cid:16)
(cid:16)
(cid:16)

w (i)xt + u (i)ht   1 + b(i)(cid:17)
w (f )xt + u (f )ht   1 + b(f )(cid:17)
w (o)xt + u (o)ht   1 + b(o)(cid:17)
(cid:16)
w (u)xt + u (u)ht   1 + b(u)(cid:17)

,

,

,

(1)

,

ut = tanh
ct = it (cid:12) ut + ft (cid:12) ct   1,
ht = ot (cid:12) tanh(ct),

where xt is the input at the current time step,    de-
notes the logistic sigmoid function and (cid:12) denotes
elementwise multiplication.
intuitively, the for-
get gate controls the extent to which the previous
memory cell is forgotten, the input gate controls
how much each unit is updated, and the output gate
controls the exposure of the internal memory state.
the hidden state vector in an lstm unit is there-
fore a gated, partial view of the state of the unit   s
internal memory cell. since the value of the gating
variables vary for each vector element, the model

can learn to represent information over multiple
time scales.

2.2 variants
two commonly-used variants of the basic lstm
architecture are the bidirectional lstm and the
multilayer lstm (also known as the stacked or
deep lstm).

bidirectional lstm. a bidirectional lstm
(graves et al., 2013) consists of two lstms that
are run in parallel: one on the input sequence and
the other on the reverse of the input sequence. at
each time step, the hidden state of the bidirec-
tional lstm is the concatenation of the forward
and backward hidden states. this setup allows the
hidden state to capture both past and future infor-
mation.

multilayer lstm.
in multilayer lstm archi-
tectures, the hidden state of an lstm unit in layer
(cid:96) is used as input to the lstm unit in layer (cid:96)+1 in
the same time step (graves et al., 2013; sutskever
et al., 2014; zaremba and sutskever, 2014). here,
the idea is to let the higher layers capture longer-
term dependencies of the input sequence.

these two variants can be combined as a multi-

layer bidirectional lstm (graves et al., 2013).

3 tree-structured lstms

a limitation of the lstm architectures described
in the previous section is that they only allow for
strictly sequential information propagation. here,
we propose two natural extensions to the basic
the child-sum tree-lstm
lstm architecture:
and the n-ary tree-lstm. both variants allow for
richer network topologies where each lstm unit
is able to incorporate information from multiple
child units.

as in standard lstm units, each tree-lstm
unit (indexed by j) contains input and output
gates ij and oj, a memory cell cj and hidden
state hj. the difference between the standard
lstm unit and tree-lstm units is that gating
vectors and memory cell updates are dependent
on the states of possibly many child units. ad-
ditionally, instead of a single forget gate, the tree-
lstm unit contains one forget gate fjk for each
child k. this allows the tree-lstm unit to se-
lectively incorporate information from each child.
for example, a tree-lstm model can learn to em-
phasize semantic heads in a semantic relatedness

h2

x1

h3

c2

u1

c3

f2

i1

f3

o1

c1

h1

figure 2: composing the memory cell c1 and hid-
den state h1 of a tree-lstm unit with two chil-
dren (subscripts 2 and 3). labeled edges cor-
respond to gating by the indicated gating vector,
with dependencies omitted for compactness.

task, or it can learn to preserve the representation
of sentiment-rich children for sentiment classi   ca-
tion.

as with the standard lstm, each tree-lstm
unit takes an input vector xj. in our applications,
each xj is a vector representation of a word in a
sentence. the input word at each node depends
on the tree structure used for the network. for in-
stance, in a tree-lstm over a dependency tree,
each node in the tree takes the vector correspond-
ing to the head word as input, whereas in a tree-
lstm over a constituency tree, the leaf nodes take
the corresponding word vectors as input.

3.1 child-sum tree-lstms
given a tree, let c(j) denote the set of children
of node j. the child-sum tree-lstm transition
equations are the following:

  hj =

ij =   

fjk =   

oj =   

k   c(j)

hk,

(cid:88)
(cid:16)
w (i)xj + u (i)  hj + b(i)(cid:17)
w (f )xj + u (f )hk + b(f )(cid:17)
(cid:16)
(cid:16)
w (o)xj + u (o)  hj + b(o)(cid:17)
(cid:16)
w (u)xj + u (u)  hj + b(u)(cid:17)
(cid:88)

,

,

,

fjk (cid:12) ck,

uj = tanh
cj = ij (cid:12) uj +

k   c(j)
hj = oj (cid:12) tanh(cj),
where in eq. 4, k     c(j).

(2)

(3)

(4)

(5)

(6)

(7)

(8)

,

intuitively, we can interpret each parameter ma-
trix in these equations as encoding correlations be-
tween the component vectors of the tree-lstm

unit, the input xj, and the hidden states hk of the
unit   s children. for example, in a dependency tree
application, the model can learn parameters w (i)
such that the components of the input gate ij have
values close to 1 (i.e.,    open   ) when a semanti-
cally important content word (such as a verb) is
given as input, and values close to 0 (i.e.,    closed   )
when the input is a relatively unimportant word
(such as a determiner).
dependency tree-lstms. since the child-
sum tree-lstm unit conditions its components
on the sum of child hidden states hk, it is well-
suited for trees with high branching factor or
whose children are unordered. for example, it is a
good choice for dependency trees, where the num-
ber of dependents of a head can be highly variable.
we refer to a child-sum tree-lstm applied to a
dependency tree as a dependency tree-lstm.

3.2 n-ary tree-lstms
the n-ary tree-lstm can be used on tree struc-
tures where the branching factor is at most n and
where children are ordered, i.e., they can be in-
dexed from 1 to n. for any node j, write the hid-
den state and memory cell of its kth child as hjk
and cjk respectively. the n-ary tree-lstm tran-
sition equations are the following:

w (i)xj +

u (i)
(cid:96) hj(cid:96) + b(i)

w (f )xj +

u (f )
k(cid:96) hj(cid:96) + b(f )

(cid:32)
(cid:32)
(cid:32)

ij =   

fjk =   

oj =   

w (o)xj +

u (o)
(cid:96) hj(cid:96) + b(o)

, (11)

uj = tanh

w (u)xj +

u (u)
(cid:96) hj(cid:96) + b(u)

(cid:32)

n(cid:88)

cj = ij (cid:12) uj +
hj = oj (cid:12) tanh(cj),

(cid:96)=1

fj(cid:96) (cid:12) cj(cid:96),

where in eq. 10, k = 1, 2, . . . , n. note that
when the tree is simply a chain, both eqs. 2   8
and eqs. 9   14 reduce to the standard lstm tran-
sitions, eqs. 1.

the introduction of separate parameter matri-
ces for each child k allows the n-ary tree-lstm

,

(cid:33)
(cid:33)
(cid:33)

(9)

,

(10)

(cid:33)

,

(12)

(13)

(14)

(cid:96)=1

n(cid:88)
n(cid:88)
n(cid:88)

(cid:96)=1

(cid:96)=1

n(cid:88)

(cid:96)=1

model to learn more    ne-grained conditioning on
the states of a unit   s children than the child-
sum tree-lstm. consider, for example, a con-
stituency tree application where the left child of a
node corresponds to a noun phrase, and the right
child to a verb phrase. suppose that in this case
it is advantageous to emphasize the verb phrase
in the representation. then the u (f )
k(cid:96) parameters
can be trained such that the components of fj1 are
close to 0 (i.e.,    forget   ), while the components of
fj2 are close to 1 (i.e.,    preserve   ).

forget gate parameterization.
in eq. 10, we
de   ne a parameterization of the kth child   s for-
get gate fjk that contains    off-diagonal    param-
k(cid:96) , k (cid:54)= (cid:96). this parameteriza-
eter matrices u (f )
tion allows for more    exible control of informa-
tion propagation from child to parent. for exam-
ple, this allows the left hidden state in a binary tree
to have either an excitatory or inhibitory effect on
the forget gate of the right child. however, for
large values of n, these additional parameters are
impractical and may be tied or    xed to zero.

constituency tree-lstms. we can naturally
apply binary tree-lstm units to binarized con-
stituency trees since left and right child nodes are
distinguished. we refer to this application of bi-
nary tree-lstms as a constituency tree-lstm.
note that in constituency tree-lstms, a node j
receives an input vector xj only if it is a leaf node.

in the remainder of this paper, we focus on
the special cases of dependency tree-lstms and
constituency tree-lstms. these architectures
are in fact closely related; since we consider only
binarized constituency trees, the parameterizations
of the two models are very similar. the key dif-
ference is in the application of the compositional
parameters: dependent vs. head for dependency
tree-lstms, and left child vs. right child for con-
stituency tree-lstms.

4 models

we now describe two speci   c models that apply
the tree-lstm architectures described in the pre-
vious section.

4.1 tree-lstm classi   cation
in this setting, we wish to predict labels   y from a
discrete set of classes y for some subset of nodes
in a tree. for example, the label for a node in a

  p   (y | {x}j) .

pi =

parse tree could correspond to some property of
the phrase spanned by that node.
at each node j, we use a softmax classi   er to
predict the label   yj given the inputs {x}j observed
at nodes in the subtree rooted at j. the classi   er
takes the hidden state hj at the node as input:

(cid:16)

w (s)hj + b(s)(cid:17)

,

  p  (y | {x}j) = softmax
  yj = arg max

y

the cost function is the negative log-likelihood

of the true class labels y(k) at each labeled node:

(cid:16)

y(k)(cid:12)(cid:12)(cid:12) {x}(k)(cid:17)

+

(cid:107)  (cid:107)2
2,

  
2

j(  ) =     1
m

log   p  

m(cid:88)

k=1

where m is the number of labeled nodes in the
training set, the superscript k indicates the kth la-
beled node, and    is an l2 id173 hyperpa-
rameter.

4.2 semantic relatedness of sentence pairs
given a sentence pair, we wish to predict a
real-valued similarity score in some range [1, k],
where k > 1 is an integer. the sequence
{1, 2, . . . , k} is some ordinal scale of similarity,
where higher scores indicate greater degrees of
similarity, and we allow real-valued scores to ac-
count for ground-truth ratings that are an average
over the evaluations of several human annotators.
we    rst produce sentence representations hl
and hr for each sentence in the pair using a
tree-lstm model over each sentence   s parse tree.
given these sentence representations, we predict
the similarity score   y using a neural network that
considers both the distance and angle between the
pair (hl, hr):

h   = hl (cid:12) hr,
h+ = |hl     hr|,
hs =   

(cid:16)

w (  )h   + w (+)h+ + b(h)(cid:17)
(cid:16)

w (p)hs + b(p)(cid:17)

,

  p   = softmax
  y = rt   p  ,

(15)

,

where rt = [1 2 . . . k] and the absolute value
function is applied elementwise. the use of both
distance measures h   and h+ is empirically mo-
tivated: we    nd that the combination outperforms
the use of either measure alone. the multiplicative
measure h   can be interpreted as an elementwise

i = (cid:98)y(cid:99) + 1
i = (cid:98)y(cid:99)
otherwise

comparison of the signs of the input representa-
tions.

we want the expected rating under the predicted
distribution   p   given model parameters    to be
close to the gold rating y     [1, k]:   y = rt   p       y.
we therefore de   ne a sparse target distribution1 p
that satis   es y = rt p:

(cid:98)y(cid:99)     y + 1,
0

               y     (cid:98)y(cid:99),
(cid:16)
m(cid:88)

kl

k=1

1
m

for 1     i     k. the cost function is the regular-
ized kl-divergence between p and   p  :

p(k)(cid:13)(cid:13)(cid:13)   p(k)

  

(cid:17)

+

(cid:107)  (cid:107)2
2,

  
2

j(  ) =

where m is the number of training pairs and the
superscript k indicates the kth sentence pair.

5 experiments

we evaluate our tree-lstm architectures on two
tasks:
(1) sentiment classi   cation of sentences
sampled from movie reviews and (2) predicting
the semantic relatedness of sentence pairs.

in comparing our tree-lstms against sequen-
tial lstms, we control for the number of lstm
parameters by varying the dimensionality of the
hidden states2. details for each model variant are
summarized in table 1.

5.1 sentiment classi   cation
in this task, we predict the sentiment of sen-
tences sampled from movie reviews. we use
the stanford sentiment treebank (socher et al.,
2013). there are two subtasks: binary classi   ca-
tion of sentences, and    ne-grained classi   cation
over    ve classes: very negative, negative, neu-
tral, positive, and very positive. we use the stan-
dard train/dev/test splits of 6920/872/1821 for the
binary classi   cation subtask and 8544/1101/2210
for the    ne-grained classi   cation subtask (there
are fewer examples for the binary subtask since

1in the subsequent experiments, we found that optimizing
this objective yielded better performance than a mean squared
error objective.

2for our bidirectional lstms, the parameters of the for-
ward and backward transition functions are shared.
in our
experiments, this achieved superior performance to bidirec-
tional lstms with untied weights and the same number of
parameters (and therefore smaller hidden vector dimension-
ality).

lstm variant
standard
bidirectional
2-layer
bidirectional 2-layer
constituency tree
dependency tree

d
150
150
108
108
142
150

relatedness
|  |

sentiment
|  |

d
168
168
120
120
150
168

315,840
315,840
318,720
318,720
316,800
315,840

203,400
203,400
203,472
203,472
205,190
203,400

table 1: memory dimensions d and composition
function parameter counts |  | for each lstm vari-
ant that we evaluate.

neutral sentences are excluded). standard bina-
rized constituency parse trees are provided for
each sentence in the dataset, and each node in
these trees is annotated with a sentiment label.

for the sequential lstm baselines, we predict
the sentiment of a phrase using the representation
given by the    nal lstm hidden state. the sequen-
tial lstm models are trained on the spans corre-
sponding to labeled nodes in the training set.

3.1)

we use the classi   cation model described in
sec. 4.1 with both dependency tree-lstms
(sec.
and constituency tree-lstms
(sec. 3.2). the constituency tree-lstms are
structured according to the provided parse trees.
for the dependency tree-lstms, we produce
dependency parses3 of each sentence; each node
in a tree is given a sentiment label if its span
matches a labeled span in the training set.

5.2 semantic relatedness
for a given pair of sentences, the semantic relat-
edness task is to predict a human-generated rating
of the similarity of the two sentences in meaning.
we use the sentences involving composi-
tional knowledge (sick) dataset (marelli et al.,
2014), consisting of 9927 sentence pairs in a
4500/500/4927 train/dev/test split. the sentences
are derived from existing image and video descrip-
tion datasets. each sentence pair is annotated with
a relatedness score y     [1, 5], with 1 indicating
that the two sentences are completely unrelated,
and 5 indicating that the two sentences are very
related. each label is the average of 10 ratings as-
signed by different human annotators.

here, we use the similarity model described in
sec. 4.2. for the similarity prediction network
(eqs. 15) we use a hidden layer of size 50. we

3dependency parses produced by the stanford neural

network dependency parser (chen and manning, 2014).

method
rae (socher et al., 2013)
mv-id56 (socher et al., 2013)
rntn (socher et al., 2013)
did98 (blunsom et al., 2014)
paragraph-vec (le and mikolov, 2014)
id98-non-static (kim, 2014)
id98-multichannel (kim, 2014)
did56 (irsoy and cardie, 2014)
lstm
bidirectional lstm
2-layer lstm
2-layer bidirectional lstm
dependency tree-lstm
constituency tree-lstm

    randomly initialized vectors
    glove vectors,    xed
    glove vectors, tuned

fine-grained

43.2
44.4
45.7
48.5
48.7
48.0
47.4
49.8

46.4 (1.1)
49.1 (1.0)
46.0 (1.3)
48.5 (1.0)
48.4 (0.4)

43.9 (0.6)
49.7 (0.4)
51.0 (0.5)

binary
82.4
82.9
85.4
86.8
87.8
87.2
88.1
86.6

84.9 (0.6)
87.5 (0.5)
86.3 (0.6)
87.2 (1.0)
85.7 (0.4)

82.0 (0.5)
87.5 (0.8)
88.0 (0.3)

table 2: test set accuracies on the stanford sen-
timent treebank. for our experiments, we report
mean accuracies over 5 runs (standard deviations
in parentheses). fine-grained: 5-class sentiment
classi   cation. binary: positive/negative senti-
ment classi   cation.

produce binarized constituency parses4 and depen-
dency parses of the sentences in the dataset for our
constituency tree-lstm and dependency tree-
lstm models.

5.3 hyperparameters and training details
the hyperparameters for our models were tuned
on the development set for each task.

we initialized our word representations using
publicly available 300-dimensional glove vec-
tors5 (pennington et al., 2014). for the sentiment
classi   cation task, word representations were up-
dated during training with a learning rate of 0.1.
for the semantic relatedness task, word represen-
tations were held    xed as we did not observe any
signi   cant improvement when the representations
were tuned.

our models were trained using adagrad (duchi
et al., 2011) with a learning rate of 0.05 and a
minibatch size of 25. the model parameters were
regularized with a per-minibatch l2 id173
strength of 10   4. the sentiment classi   er was ad-
ditionally regularized using dropout (hinton et al.,
2012) with a dropout rate of 0.5. we did not ob-
serve performance gains using dropout on the se-
mantic relatedness task.

4constituency parses produced by the stanford pid18

parser (klein and manning, 2003).

5trained on 840 billion tokens of common crawl data,

http://nlp.stanford.edu/projects/glove/.

method
illinois-lh (lai and hockenmaier, 2014)
unal-nlp (jimenez et al., 2014)
meaning factory (bjerva et al., 2014)
ecnu (zhao et al., 2014)
mean vectors
dt-id56 (socher et al., 2014)
sdt-id56 (socher et al., 2014)
lstm
bidirectional lstm
2-layer lstm
2-layer bidirectional lstm
constituency tree-lstm
dependency tree-lstm

pearson   s r

spearman   s   

0.7993
0.8070
0.8268
0.8414

0.7538
0.7489
0.7721

   

mse
0.3692
0.3550
0.3224

   

0.7577 (0.0013)
0.7923 (0.0070)
0.7900 (0.0042)
0.8528 (0.0031)
0.8567 (0.0028)
0.8515 (0.0066)
0.8558 (0.0014)
0.8582 (0.0038)
0.8676 (0.0030)

0.6738 (0.0027)
0.7319 (0.0071)
0.7304 (0.0076)
0.7911 (0.0059)
0.7966 (0.0053)
0.7896 (0.0088)
0.7965 (0.0018)
0.7966 (0.0053)
0.8083 (0.0042)

0.4557 (0.0090)
0.3822 (0.0137)
0.3848 (0.0074)
0.2831 (0.0092)
0.2736 (0.0063)
0.2838 (0.0150)
0.2762 (0.0020)
0.2734 (0.0108)
0.2532 (0.0052)

table 3: test set results on the sick semantic relatedness subtask. for our experiments, we report mean
scores over 5 runs (standard deviations in parentheses). results are grouped as follows: (1) semeval
2014 submissions; (2) our own baselines; (3) sequential lstms; (4) tree-structured lstms.

6 results
6.1 sentiment classi   cation
our results are summarized in table 2. the con-
stituency tree-lstm outperforms existing sys-
tems on the    ne-grained classi   cation subtask and
achieves accuracy comparable to the state-of-the-
art on the binary subtask. in particular, we    nd that
it outperforms the dependency tree-lstm. this
performance gap is at least partially attributable to
the fact that the dependency tree-lstm is trained
on less data: about 150k labeled nodes vs. 319k
for the constituency tree-lstm. this difference
is due to (1) the dependency representations con-
taining fewer nodes than the corresponding con-
stituency representations, and (2) the inability to
match about 9% of the dependency nodes to a cor-
responding span in the training data.

we found that updating the word representa-
tions during training (      ne-tuning    the word em-
bedding) yields a signi   cant boost in performance
on the    ne-grained classi   cation subtask and gives
a minor gain on the binary classi   cation subtask
(this    nding is consistent with previous work on
this task by kim (2014)). these gains are to be
expected since the glove vectors used to initial-
ize our word representations were not originally
trained to capture sentiment.

6.2 semantic relatedness
our results are summarized in table 3. following
marelli et al. (2014), we use pearson   s r, spear-
man   s    and mean squared error (mse) as evalua-

tion metrics. the    rst two metrics are measures of
correlation against human evaluations of semantic
relatedness.

we compare our models against a number of
non-lstm baselines. the mean vector baseline
computes sentence representations as a mean of
the representations of the constituent words. the
dt-id56 and sdt-id56 models (socher et al.,
2014) both compose vector representations for the
nodes in a dependency tree as a sum over af   ne-
transformed child vectors, followed by a nonlin-
earity. the sdt-id56 is an extension of the dt-
id56 that uses a separate transformation for each
dependency relation. for each of our baselines,
including the lstm models, we use the similarity
model described in sec. 4.2.

we also compare against four of the top-
performing systems6 submitted to the semeval
2014 semantic relatedness shared task: ecnu
(zhao et al., 2014), the meaning factory (bjerva
et al., 2014), unal-nlp (jimenez et al., 2014),
and illinois-lh (lai and hockenmaier, 2014).
these systems are heavily feature engineered,
generally using a combination of surface form
overlap features and lexical distance features de-
rived from id138 or the paraphrase database
(ganitkevitch et al., 2013).

our lstm models outperform all these sys-

6we list the strongest results we were able to    nd for this
task; in some cases, these results are stronger than the of   cial
performance by the team on the shared task. for example,
the listed result by zhao et al. (2014) is stronger than their
submitted system   s pearson correlation score of 0.8280.

figure 3: fine-grained sentiment classi   cation ac-
curacy vs. sentence length. for each (cid:96), we plot
accuracy for the test set sentences with length in
the window [(cid:96)     2, (cid:96) + 2]. examples in the tail
of the length distribution are batched in the    nal
window ((cid:96) = 45).

figure 4: pearson correlations r between pre-
dicted similarities and gold ratings vs. sentence
length. for each (cid:96), we plot r for the pairs with
mean length in the window [(cid:96)   2, (cid:96)+2]. examples
in the tail of the length distribution are batched in
the    nal window ((cid:96) = 18.5).

tems without any additional feature engineering,
with the best results achieved by the dependency
tree-lstm. recall that in this task, both tree-
lstm models only receive supervision at the root
of the tree, in contrast to the sentiment classi   -
cation task where supervision was also provided
at the intermediate nodes. we conjecture that in
this setting, the dependency tree-lstm bene   ts
from its more compact structure relative to the
constituency tree-lstm, in the sense that paths
from input word vectors to the root of the tree
are shorter on aggregate for the dependency tree-
lstm.

7 discussion and qualitative analysis

7.1 modeling semantic relatedness
in table 4, we list nearest-neighbor sentences re-
trieved from a 1000-sentence sample of the sick
test set. we compare the neighbors ranked by the
dependency tree-lstm model against a baseline
ranking by cosine similarity of the mean word vec-
tors for each sentence.

the dependency tree-lstm model exhibits
several desirable properties. note that in the de-
pendency parse of the second query sentence, the
word    ocean    is the second-furthest word from the
root (   waving   ), with a depth of 4. regardless, the
retrieved sentences are all semantically related to
the word    ocean   , which indicates that the tree-
lstm is able to both preserve and emphasize in-
formation from relatively distant nodes. addi-
tionally, the tree-lstm model shows greater ro-

bustness to differences in sentence length. given
the query    two men are playing guitar   , the tree-
lstm associates the phrase    playing guitar    with
the longer, related phrase    dancing and singing in
front of a crowd    (note as well that there is zero
token overlap between the two phrases).

7.2 effect of sentence length
one hypothesis to explain the empirical strength
of tree-lstms is that tree structures help miti-
gate the problem of preserving state over long se-
quences of words. if this were true, we would ex-
pect to see the greatest improvement over sequen-
tial lstms on longer sentences. in figs. 3 and 4,
we show the relationship between sentence length
and performance as measured by the relevant task-
speci   c metric. each data point is a mean score
over 5 runs, and error bars have been omitted for
clarity.

we observe that while the dependency tree-
lstm does signi   cantly outperform its sequen-
tial counterparts on the relatedness task for
longer sentences of length 13 to 15 (fig. 4), it
also achieves consistently strong performance on
shorter sentences. this suggests that unlike se-
quential lstms, tree-lstms are able to encode
semantically-useful structural information in the
sentence representations that they compose.

8 related work
distributed representations of words (rumelhart
et al., 1988; collobert et al., 2011; turian et al.,
2010; huang et al., 2012; mikolov et al., 2013;

051015202530354045sentencelength0.300.350.400.450.500.550.600.650.70accuracydt-lstmct-lstmlstmbi-lstm468101214161820meansentencelength0.780.800.820.840.860.880.90rdt-lstmct-lstmlstmbi-lstmranking by mean word vector cosine similarity
a woman is slicing potatoes
a woman is cutting potatoes
a woman is slicing herbs
a woman is slicing tofu
a boy is waving at some young runners from the ocean
a man and a boy are standing at the bottom of some stairs ,

which are outdoors

a group of children in uniforms is standing at a gate and

one is kissing the mother

a group of children in uniforms is standing at a gate and

there is no one kissing the mother

two men are playing guitar
some men are playing rugby
two men are talking

two dogs are playing with each other

score

0.96
0.92
0.92

0.92

0.90

0.90

0.88
0.87

0.87

ranking by dependency tree-lstm model
a woman is slicing potatoes
a woman is cutting potatoes
potatoes are being sliced by a woman
tofu is being sliced by a woman
a boy is waving at some young runners from the ocean
a group of men is playing with a ball on the beach

a young boy wearing a red swimsuit is jumping out of a

blue kiddies pool

the man is tossing a kid into the swimming pool that is

near the ocean

two men are playing guitar
the man is singing and playing the guitar
the man is opening the guitar for donations and plays

with the case

two men are dancing and singing in front of a crowd

score

4.82
4.70
4.39

3.79

3.37

3.19

4.08
4.01

4.00

table 4: most similar sentences from a 1000-sentence sample drawn from the sick test set. the tree-
lstm model is able to pick up on more subtle relationships, such as that between    beach    and    ocean   
in the second example.

pennington et al., 2014) have found wide appli-
cability in a variety of nlp tasks. following
this success, there has been substantial interest in
the area of learning distributed phrase and sen-
tence representations (mitchell and lapata, 2010;
yessenalina and cardie, 2011; grefenstette et al.,
2013; mikolov et al., 2013), as well as distributed
representations of longer bodies of text such as
paragraphs and documents (srivastava et al., 2013;
le and mikolov, 2014).

our approach builds on recursive neural net-
works (goller and kuchler, 1996; socher et al.,
2011), which we abbreviate as tree-id56s in or-
der to avoid confusion with recurrent neural net-
works. under the tree-id56 framework, the vec-
tor representation associated with each node of
a tree is composed as a function of the vectors
corresponding to the children of the node. the
choice of composition function gives rise to nu-
merous variants of this basic framework. tree-
id56s have been used to parse images of natu-
ral scenes (socher et al., 2011), compose phrase
representations from word vectors (socher et al.,
2012), and classify the sentiment polarity of sen-
tences (socher et al., 2013).
9 conclusion
in this paper, we introduced a generalization of
lstms to tree-structured network topologies. the
tree-lstm architecture can be applied to trees
with arbitrary branching factor. we demonstrated
the effectiveness of the tree-lstm by applying
the architecture in two tasks: semantic relatedness

and sentiment classi   cation, outperforming exist-
ing systems on both. controlling for model di-
mensionality, we demonstrated that tree-lstm
models are able to outperform their sequential
counterparts. our results suggest further lines of
work in characterizing the role of structure in pro-
ducing distributed representations of sentences.

acknowledgements
we thank our anonymous reviewers for their valu-
able feedback. stanford university gratefully ac-
knowledges the support of a natural language
understanding-focused gift from google inc. and
the defense advanced research projects agency
(darpa) deep exploration and filtering of text
(deft) program under air force research lab-
oratory (afrl) contract no. fa8750-13-2-0040.
any opinions,    ndings, and conclusion or recom-
mendations expressed in this material are those of
the authors and do not necessarily re   ect the view
of the darpa, afrl, or the us government.

references
bahdanau, dzmitry, kyunghyun cho, and yoshua
bengio. 2014. id4 by
jointly learning to align and translate. arxiv
preprint arxiv:1409.0473 .

bengio, yoshua, patrice simard, and paolo fras-
coni. 1994. learning long-term dependencies
with id119 is dif   cult. ieee trans-
actions on neural networks 5(2):157   166.

bjerva, johannes, johan bos, rob van der goot,

and malvina nissim. 2014. the meaning fac-
tory: formal semantics for recognizing textual
entailment and determining semantic similarity.
semeval 2014 .

blunsom, phil, edward grefenstette, nal kalch-
brenner, et al. 2014. a convolutional neural net-
work for modelling sentences. in proceedings
of the 52nd annual meeting of the association
for computational linguistics.

chen, danqi and christopher d manning. 2014. a
fast and accurate dependency parser using neu-
ral networks. in proceedings of the 2014 con-
ference on empirical methods in natural lan-
guage processing (emnlp). pages 740   750.

collobert, ronan, jason weston, l  eon bottou,
michael karlen, koray kavukcuoglu, and pavel
kuksa. 2011. natural language processing (al-
most) from scratch. the journal of machine
learning research 12:2493   2537.

duchi, john, elad hazan, and yoram singer. 2011.
adaptive subgradient methods for online learn-
ing and stochastic optimization. the journal of
machine learning research 12:2121   2159.

elman, jeffrey l. 1990. finding structure in time.

cognitive science 14(2):179   211.

foltz, peter w, walter kintsch, and thomas k
landauer. 1998. the measurement of textual
coherence with latent semantic analysis. dis-
course processes 25(2-3):285   307.

ganitkevitch, juri, benjamin van durme, and
chris callison-burch. 2013. ppdb: the para-
phrase database. in hlt-naacl. pages 758   
764.

goller, christoph and andreas kuchler. 1996.
learning task-dependent distributed representa-
tions by id26 through structure. in
ieee international conference on neural net-
works. volume 1, pages 347   352.

graves, alex, navdeep jaitly, and a-r mohamed.
2013. hybrid id103 with deep
bidirectional lstm. in ieee workshop on au-
tomatic id103 and understanding
(asru). pages 273   278.

grefenstette, edward, georgiana dinu, yao-
zhong zhang, mehrnoosh sadrzadeh, and
marco baroni. 2013. multi-step regression
learning for compositional distributional se-
mantics. arxiv preprint arxiv:1301.6939 .

hinton, geoffrey e, nitish srivastava, alex
ilya sutskever, and ruslan r
krizhevsky,
salakhutdinov. 2012.
improving neural net-
works by preventing co-adaptation of feature
detectors. arxiv preprint arxiv:1207.0580 .

hochreiter, sepp. 1998. the vanishing gradient
problem during learning recurrent neural nets
and problem solutions. international journal of
uncertainty, fuzziness and knowledge-based
systems 6(02):107   116.

hochreiter, sepp and j  urgen schmidhuber. 1997.
long short-term memory. neural computa-
tion 9(8):1735   1780.

huang, eric h., richard socher, christopher d.
manning, and andrew y. ng. 2012.
improv-
ing word representations via global context and
in annual meeting
multiple word prototypes.
of the association for computational linguis-
tics (acl).

irsoy, ozan and claire cardie. 2014. deep re-
cursive neural networks for compositionality in
in advances in neural information
language.
processing systems. pages 2096   2104.

jimenez, sergio, george duenas, julia baquero,
alexander gelbukh, av juan dios b  atiz, and
av mendiz  abal. 2014. unal-nlp: combin-
ing soft cardinality features for semantic textual
similarity, relatedness and entailment. semeval
2014 .

kim, yoon. 2014.

convolutional neural net-
works for sentence classi   cation. arxiv preprint
arxiv:1408.5882 .

klein, dan and christopher d manning. 2003.
accurate unlexicalized parsing. in proceedings
of the 41st annual meeting on association for
computational linguistics-volume 1. associa-
tion for computational linguistics, pages 423   
430.

lai, alice and julia hockenmaier. 2014. illinois-
lh: a denotational and distributional approach
to semantics. semeval 2014 .

landauer, thomas k and susan t dumais. 1997.
a solution to plato   s problem: the latent se-
mantic analysis theory of acquisition, induction,
and representation of knowledge. psychological
review 104(2):211.

le, quoc v and tomas mikolov. 2014. dis-
tributed representations of sentences and doc-
uments. arxiv preprint arxiv:1405.4053 .

drew y ng, and christopher potts. 2013. re-
cursive deep models for semantic composition-
ality over a sentiment treebank. in proceedings
of the conference on empirical methods in nat-
ural language processing (emnlp).

srivastava, nitish, ruslan r salakhutdinov, and
geoffrey e hinton. 2013. modeling documents
with deep id82s. arxiv preprint
arxiv:1309.6865 .

sutskever, ilya, oriol vinyals, and quoc vv le.
2014. sequence to sequence learning with neu-
ral networks. in advances in neural informa-
tion processing systems. pages 3104   3112.

turian, joseph, lev ratinov, and yoshua bengio.
2010. word representations: a simple and gen-
eral method for semi-supervised learning.
in
proceedings of the 48th annual meeting of the
association for computational linguistics. as-
sociation for computational linguistics, pages
384   394.

vinyals, oriol, alexander toshev, samy bengio,
and dumitru erhan. 2014. show and tell: a
neural image caption generator. arxiv preprint
arxiv:1411.4555 .

yessenalina, ainur and claire cardie. 2011. com-
positional matrix-space models for sentiment
in proceedings of the conference
analysis.
on empirical methods in natural language
processing. association for computational lin-
guistics, pages 172   182.

zaremba, wojciech

and

2014. learning to execute.
arxiv:1410.4615 .

ilya

sutskever.
arxiv preprint

zhao, jiang, tian tian zhu, and man lan. 2014.
ecnu: one stone two birds: ensemble of het-
erogenous measures for semantic relatedness
and id123. semeval 2014 .

marelli, marco, luisa bentivogli, marco ba-
roni, raffaella bernardi, stefano menini, and
roberto zamparelli. 2014. semeval-2014 task
1: evaluation of compositional distributional
semantic models on full sentences through se-
mantic relatedness and id123.
in
semeval 2014.

mikolov, tom  a  s. 2012. statistical language mod-
els based on neural networks. ph.d. thesis,
brno university of technology.

mikolov, tomas, ilya sutskever, kai chen, greg s
corrado, and jeff dean. 2013. distributed
representations of words and phrases and their
compositionality. in advances in neural infor-
mation processing systems. pages 3111   3119.
mitchell, jeff and mirella lapata. 2010. composi-
tion in distributional models of semantics. cog-
nitive science 34(8):1388   1429.

pennington, jeffrey, richard socher, and christo-
pher d manning. 2014. glove: global vectors
for word representation. proceedings of the em-
piricial methods in natural language process-
ing (emnlp 2014) 12.

rumelhart, david e, geoffrey e hinton, and
ronald j williams. 1988. learning represen-
tations by back-propagating errors. cognitive
modeling 5.

socher, richard, brody huval, christopher d
manning, and andrew y ng. 2012. seman-
tic compositionality through recursive matrix-
vector spaces. in proceedings of the 2012 joint
conference on empirical methods in natural
language processing and computational nat-
ural language learning. association for com-
putational linguistics, pages 1201   1211.

socher, richard, andrej karpathy, quoc v le,
christopher d manning, and andrew y ng.
2014. grounded id152 for
   nding and describing images with sentences.
transactions of the association for computa-
tional linguistics 2:207   218.

socher, richard, cliff c lin, chris manning, and
andrew y ng. 2011. parsing natural scenes
and natural language with recursive neural net-
works. in proceedings of the 28th international
conference on machine learning (icml-11).
pages 129   136.

socher, richard, alex perelygin, jean y wu,
jason chuang, christopher d manning, an-

