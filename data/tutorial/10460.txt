long short-term memory-networks for machine reading

jianpeng cheng, li dong and mirella lapata
school of informatics, university of edinburgh

10 crichton street, edinburgh eh8 9ab

{jianpeng.cheng,li.dong}@ed.ac.uk, mlap@inf.ed.ac.uk

6
1
0
2

 

p
e
s
0
2

 

 
 
]
l
c
.
s
c
[
 
 

7
v
3
3
7
6
0

.

1
0
6
1
:
v
i
x
r
a

abstract

in this paper we address the question of how
to render sequence-level networks better at
handling structured input. we propose a ma-
chine reading simulator which processes text
incrementally from left to right and performs
shallow reasoning with memory and atten-
tion. the reader extends the long short-term
memory architecture with a memory network
in place of a single memory cell. this en-
ables adaptive memory usage during recur-
rence with neural attention, offering a way to
weakly induce relations among tokens. the
system is initially designed to process a single
sequence but we also demonstrate how to inte-
grate it with an encoder-decoder architecture.
experiments on id38, sentiment
analysis, and natural language id136 show
that our model matches or outperforms the
state of the art.

1

introduction

how can a sequence-level network induce relations
which are presumed latent during text processing?
how can a recurrent network attentively memorize
longer sequences in a way that humans do? in this
paper we design a machine reader that automatically
learns to understand text. the term machine read-
ing is related to a wide range of tasks from answer-
ing reading comprehension questions (clark et al.,
2013), to fact and id36 (etzioni et al.,
2011; fader et al., 2011), ontology learning (poon
and domingos, 2010), and id123 (da-
gan et al., 2005). rather than focusing on a speci   c
task, we develop a general-purpose reading simula-

tor, drawing inspiration from human language pro-
cessing and the fact language comprehension is in-
cremental with readers continuously extracting the
meaning of utterances on a word-by-word basis.

in order to understand texts, our machine reader
should provide facilities for extracting and repre-
senting meaning from natural language text, storing
meanings internally, and working with stored mean-
ings to derive further consequences.
ideally, such
a system should be robust, open-domain, and de-
grade gracefully in the presence of semantic rep-
resentations which may be incomplete, inaccurate,
or incomprehensible. it would also be desirable to
simulate the behavior of english speakers who pro-
cess text sequentially, from left to right,    xating
nearly every word while they read (rayner, 1998)
and creating partial representations for sentence pre-
   xes (konieczny, 2000; tanenhaus et al., 1995).

id38 tools such as recurrent neural
networks (id56) bode well with human reading be-
havior (frank and bod, 2011). id56s treat each sen-
tence as a sequence of words and recursively com-
pose each word with its previous memory, until the
meaning of the whole sentence has been derived. in
practice, however, sequence-level networks are met
with at least three challenges. the    rst one concerns
model training problems associated with vanishing
and exploding gradients (hochreiter, 1991; bengio
et al., 1994), which can be partially ameliorated with
gated id180, such as the long short-
term memory (lstm) (hochreiter and schmidhu-
ber, 1997), and gradient clipping (pascanu et al.,
2013). the second issue relates to memory com-
pression problems. as the input sequence gets com-
pressed and blended into a single dense vector, suf-

the fbi is chasing a criminal on the run .
thethe fbi is chasing a criminal on the run .
thethe fbifbi is chasing a criminal on the run .
thethe fbifbi
thethe fbifbi
thethe fbifbi
thethe fbifbi
thethe fbifbi
thethe fbifbi
thethe fbifbi

isis chasing a criminal on the run .
isis
isis
isis
isis
isis
isis

chasing
chasing a criminal on the run .
chasing
chasing aa criminal on the run .
chasing
chasing aa
chasing
chasing aa
chasing
chasing aa
chasing
chasing aa

criminal
criminal on the run .
criminal
criminal onon the run .
criminal
criminal onon thethe run .
criminal
criminal onon thethe

runrun .

figure 1: illustration of our model while reading the
sentence the fbi is chasing a criminal on the run.
color red represents the current word being    xated,
blue represents memories. shading indicates the de-
gree of memory activation.

   ciently large memory capacity is required to store
past information. as a result, the network general-
izes poorly to long sequences while wasting memory
on shorter ones. finally, it should be acknowledged
that sequence-level networks lack a mechanism for
handling the structure of the input. this imposes
an inductive bias which is at odds with the fact that
language has inherent structure.
in this paper, we
develop a text processing system which addresses
these limitations while maintaining the incremental,
generative property of a recurrent language model.
recent attempts to render neural networks more
structure aware have seen the incorporation of exter-
nal memories in the context of recurrent neural net-
works (weston et al., 2015; sukhbaatar et al., 2015;
grefenstette et al., 2015). the idea is to use multiple
memory slots outside the recurrence to piece-wise
store representations of the input; read and write
operations for each slot can be modeled as an at-
tention mechanism with a recurrent controller. we
also leverage memory and attention to empower a
recurrent network with stronger memorization capa-
bility and more importantly the ability to discover
relations among tokens. this is realized by insert-
ing a memory network module in the update of a re-
current network together with attention for memory
addressing. the attention acts as a weak inductive
module discovering relations between input tokens,
and is trained without direct supervision. as a point
of departure from previous work, the memory net-
work we employ is internal to the recurrence, thus
strengthening the interaction of the two and lead-
ing to a representation learner which is able to rea-

son over shallow structures. the resulting model,
which we term long short-term memory-network
(lstmn), is a reading simulator that can be used
for sequence processing tasks.

figure 1 illustrates the reading behavior of the
lstmn. the model processes text incrementally
while learning which past tokens in the memory and
to what extent they relate to the current token being
processed. as a result, the model induces undirected
relations among tokens as an intermediate step of
learning representations. we validate the perfor-
mance of the lstmn in id38, sen-
timent analysis, and natural language id136. in
all cases, we train lstmn models end-to-end with
task-speci   c supervision signals, achieving perfor-
mance comparable or better to state-of-the-art mod-
els and superior to vanilla lstms.

2 related work

our machine reader is a recurrent neural network ex-
hibiting two important properties: it is incremental,
simulating human behavior, and performs shallow
structure reasoning over input streams.

recurrent neural network (id56s) have been suc-
cessfully applied to various sequence modeling and
sequence-to-sequence transduction tasks. the latter
have assumed several guises in the literature such
as machine translation (bahdanau et al., 2014), sen-
tence compression (rush et al., 2015), and reading
comprehension (hermann et al., 2015). a key con-
tributing factor to their success has been the abil-
ity to handle well-known problems with exploding
or vanishing gradients (bengio et al., 1994), leading
to models with gated id180 (hochre-
iter and schmidhuber, 1997; cho et al., 2014), and
more advanced architectures that enhance the in-
formation    ow within the network (koutn    k et al.,
2014; chung et al., 2015; yao et al., 2015).

a remaining practical bottleneck for id56s is
memory compression (bahdanau et al., 2014): since
the inputs are recursively combined into a single
memory representation which is typically too small
in terms of parameters, it becomes dif   cult to accu-
rately memorize sequences (zaremba and sutskever,
2014).
this
problem can be sidestepped with an attention mech-
anism which learns soft alignments between the de-
coding states and the encoded memories (bahdanau

in the encoder-decoder architecture,

et al., 2014). in our model, memory and attention
are added within a sequence encoder allowing the
network to uncover lexical relations between tokens.
the idea of introducing a structural bias to neu-
ral models is by no means new. for example, it is
re   ected in the work of socher et al. (2013a) who
apply id56s for learning natural
language representations.
in the context of recur-
rent neural networks, efforts to build modular, struc-
tured neural models date back to das et al. (1992)
who connect a recurrent neural network with an ex-
ternal memory stack for learning context free gram-
mars. recently, weston et al. (2015) propose mem-
ory networks to explicitly segregate memory stor-
age from the computation of neural networks in gen-
eral. their model is trained end-to-end with a mem-
ory addressing mechanism closely related to soft at-
tention (sukhbaatar et al., 2015) and has been ap-
plied to machine translation (meng et al., 2015).
grefenstette et al. (2015) de   ne a set of differen-
tiable data structures (stacks, queues, and dequeues)
as memories controlled by a recurrent neural net-
work. tran et al. (2016) combine the lstm with an
external memory block component which interacts
with its hidden state. kumar et al. (2016) employ
a structured neural network with episodic memory
modules for natural language and also visual ques-
tion answering (xiong et al., 2016).

similar to the above work, we leverage memory
and attention in a recurrent neural network for induc-
ing relations between tokens as a module in a larger
network responsible for representation learning. as
a property of soft attention, all intermediate rela-
tions we aim to capture are soft and differentiable.
this is in contrast to shift-reduce type neural mod-
els (dyer et al., 2015; bowman et al., 2016) where
the intermediate decisions are hard and induction is
more dif   cult. finally, note that our model captures
undirected lexical relations and is thus distinct from
work on dependency grammar induction (klein and
manning, 2004) where the learned head-modi   er re-
lations are directed.

3 the machine reader

in this section we present our machine reader which
is designed to process structured input while retain-
ing the incrementality of a recurrent neural network.
the core of our model is a long short-term mem-

ory (lstm) unit with an extended memory tape that
explicitly simulates the human memory span. the
model performs implicit relation analysis between
tokens with an attention-based memory addressing
mechanism at every time step. in the following, we
   rst review the standard long short-term memory
and then describe our model.

3.1 long short-term memory
a long short-term memory (lstm) recurrent neu-
ral network processes a variable-length sequence
x = (x1,x2,       ,xn) by incrementally adding new
content into a single memory slot, with gates con-
trolling the extent to which new content should be
memorized, old content should be erased, and cur-
rent content should be exposed. at time step t, the
memory ct and the hidden state ht are updated with
the following equations:

            it

             =

               

            w    [ht   1, xt]

  
  
tanh

ft
ot
  ct
ct = ft (cid:12) ct   1 + it (cid:12)   ct
ht = ot (cid:12) tanh(ct)

(1)

(2)

(3)

where i,
f , and o are gate activations. compared
to the standard id56, the lstm uses additive mem-
ory updates and it separates the memory c from the
hidden state h, which interacts with the environment
when making predictions.

3.2 long short-term memory-network
the    rst question that arises with lstms is the ex-
tent to which they are able to memorize sequences
under recursive compression. lstms can produce
a list of state representations during composition,
however, the next state is always computed from the
current state. that is to say, given the current state
ht, the next state ht+1 is conditionally independent of
states h1      ht   1 and tokens x1      xt. while the recur-
sive state update is performed in a markov manner, it
is assumed that lstms maintain unbounded mem-
ory (i.e., the current state alone summarizes well the
tokens it has seen so far). this assumption may fail
in practice, for example when the sequence is long

environment (e.g., computing attention), and a mem-
ory tape used to represent what is actually stored in
memory.1 therefore, each token is associated with
a hidden vector and a memory vector. let xt de-
note the current input; ct   1 = (c1,       ,ct   1) denotes
the current memory tape, and ht   1 = (h1,       ,ht   1)
the previous hidden tape. at time step t, the model
computes the relation between xt and x1      xt   1
through h1      ht   1 with an attention layer:
  ht   1)

at
i = vt tanh(whhi +wxxt +w  h

(4)

i = softmax(at
st
i)

(5)
this yields a id203 distribution over the hidden
state vectors of previous tokens. we can then com-
pute an adaptive summary vector for the previous
hidden tape and memory tape denoted by   ct and   ht,
respectively:

and use them for computing the values of ct and ht
in the recurrent update as:

t   1
   

i=1

=

  ct

(cid:20)  ht
(cid:21)
               
             =

ci

i   
st

(cid:20)hi
(cid:21)
            w    [  ht, xt]

  
  
tanh

ft
ot
  ct
ct = ft (cid:12)   ct + it (cid:12)   ct
ht = ot (cid:12) tanh(ct)

            it

(6)

(7)

(8)
(9)
where v, wh, wx and w  h are the new weight terms of
the network.

a key idea behind the lstmn is to use attention
for inducing relations between tokens. these rela-
tions are soft and differentiable, and components of
a larger representation learning network. although
it is appealing to provide direct supervision for the
attention layer, e.g., with evidence collected from
a dependency treebank, we treat it as a submod-
ule being optimized within the larger network in a
downstream task. it is also possible to have a more
structured relational reasoning module by stacking
multiple memory and hidden layers in an alternat-
ing fashion, resembling a stacked lstm (graves,
1for comparison, lstms maintain a hidden vector and a
memory vector; memory networks (weston et al., 2015) have a
set of key vectors and a set of value vectors.

figure 2:
color indicates degree of memory activation.

long short-term memory-network.

or when the memory size is not large enough. an-
other undesired property of lstms concerns model-
ing structured input. an lstm aggregates informa-
tion on a token-by-token basis in sequential order,
but there is no explicit mechanism for reasoning over
structure and modeling relations between tokens.

our model aims to address both limitations. our
solution is to modify the standard lstm structure
by replacing the memory cell with a memory net-
work (weston et al., 2015). the resulting long
short-term memory-network (lstmn) stores the
contextual representation of each input token with
a unique memory slot and the size of the memory
grows with time until an upper bound of the memory
span is reached. this design enables the lstm to
reason about relations between tokens with a neural
attention layer and then perform non-markov state
updates. although it is feasible to apply both write
and read operations to the memories with attention,
we concentrate on the latter. we conceptualize the
read operation as attentively linking the current to-
ken to previous memories and selecting useful con-
tent when processing it. although not the focus of
this work, the signi   cance of the write operation
can be analogously justi   ed as a way of incremen-
tally updating previous memories, e.g., to correct
wrong interpretations when processing garden path
sentences (ferreira and henderson, 1991).

the architecture of the lstmn is shown in fig-
ure 2 and the formal de   nition is provided as fol-
lows. the model maintains two sets of vectors
stored in a hidden state tape used to interact with the

(11)

(12)

(13)

2013) or a multi-hop memory network (sukhbaatar
et al., 2015). this can be achieved by feeding the
output hk
t of the lower layer k as input to the upper
layer (k + 1). the attention at the (k + 1)th layer is
computed as:

inter-attention between the input at time step t and
tokens in the entire source sequence as follows:

bt
j = ut tanh(w     j +wxxt +w        t   1)

at
i,k+1 = vt tanh(whhk+1

i +wlhk

t +w  h

  hk+1
t   1 )

(10)

j = softmax(bt
pt
j)

skip-connections (graves, 2013) can be applied to
feed xt to upper layers as well.

4 modeling two sequences with lstmn

natural language processing tasks such as machine
translation and id123 are concerned
with modeling two sequences rather than a single
one. a standard tool for modeling two sequences
with recurrent networks is the encoder-decoder ar-
chitecture where the second sequence (also known
as the target) is being processed conditioned on the
   rst one (also known as the source). in this section
we explain how to combine the lstmn which ap-
plies attention for intra-relation reasoning, with the
encoder-decoder network whose attention module
learns the inter-alignment between two sequences.
figures 3a and 3b illustrate two types of combina-
tion. we describe the models more formally below.

shallow attention fusion shallow fusion simply
treats the lstmn as a separate module that can
be readily used in an encoder-decoder architecture,
in lieu of a standard id56 or lstm. as shown in
figure 3a, both encoder and decoder are modeled
as lstmns with intra-attention. meanwhile, inter-
attention is triggered when the decoder reads a tar-
get token, similar to the inter-attention introduced in
bahdanau et al. (2014).

deep attention fusion deep fusion combines
inter- and intra-attention (initiated by the decoder)
when computing state updates. we use different no-
tation to represent the two sets of attention. follow-
ing section 3.2, c and h denote the target memory
tape and hidden tape, which store representations of
the target symbols that have been processed so far.
the computation of intra-attention follows equa-
tions (4)   (9). additionally, we use a = [  1,       ,  m]
and y = [  1,       ,  m] to represent the source mem-
ory tape and hidden tape, with m being the length of
the source sequence conditioned upon. we compute

after that we compute the adaptive representation of
the source memory tape     t and hidden tape     t as:

(cid:21)

(cid:20)     t

    t

(cid:21)

(cid:20)   j

   j

=

j   
pt

m

   

j=1

we can then transfer the adaptive source represen-
tation     t to the target memory with another gating
operation rt, analogous to the gates in equation (7).

rt =   (wr    [    t,xt])

(14)

the new target memory includes inter-alignment
rt (cid:12)     t, intra-relation ft (cid:12)   ct, and the new input in-
formation it (cid:12)   ct:

ct = rt (cid:12)     t + ft (cid:12)   ct + it (cid:12)   ct

ht = ot (cid:12) tanh(ct)

(15)

(16)

as shown in the equations above and figure 3b, the
major change of deep fusion lies in the recurrent
storage of the inter-alignment vector in the target
memory network, as a way to help the target net-
work review source information.

5 experiments

in this section we present our experiments for eval-
uating the performance of the lstmn machine
reader. we start with id38 as it
is a natural testbed for our model. we then as-
sess the model   s ability to extract meaning repre-
sentations for generic sentence classi   cation tasks
such as id31. finally, we examine
whether the lstmn can recognize the semantic
relationship between two sentences by applying it
to a natural language id136 task. our code
is available at https://github.com/cheng6076/
snli-attention.

(a) decoder with shallow attention fusion.

(b) decoder with deep attention fusion.

figure 3: lstmns for sequence-to-sequence modeling. the encoder uses intra-attention, while the decoder
incorporates both intra- and inter-attention. the two    gures present two ways to combine the intra- and
inter-attention in the decoder.

models
kn5
id56
lstm
lstmn
slstm
glstm
dlstm
lstmn

layers

perplexity

   
1
1
1
3
3
3
3

141
129
115
108
115
107
109
102

table 1: language model perplexity on the penn
treebank. the size of memory is 300 for all models.

5.1 id38
our id38 experiments were con-
ducted on the english id32 dataset. fol-
lowing common practice (mikolov et al., 2010), we
trained on sections 0   20 (1m words), used sec-
tions 21   22 for validation (80k words), and sec-
tions 23   24 (90k words for testing). the dataset
contains approximately 1 million tokens and a vo-
cabulary size of 10k. the average sentence length
is 21. we use perplexity as our evaluation metric:
ppl = exp(nll/t ), where nll denotes the nega-
tive log likelihood of the entire test set and t the
corresponding number of tokens. we used stochas-
tic id119 for optimization with an ini-
tial learning rate of 0.65, which decays by a factor
of 0.85 per epoch if no signi   cant improvement has
been observed on the validation set. we renormal-
ize the gradient if its norm is greater than 5. the
mini-batch size was set to 40. the dimensions of

the id27s were set to 150 for all models.
in this suite of experiments we compared the
lstmn against a variety of baselines. the    rst
one is a kneser-ney 5-gram language model (kn5)
which generally serves as a non-neural baseline for
the id38 task. we also present per-
plexity results for the standard id56 and lstm
models. we also implemented more sophisti-
cated lstm architectures, such as a stacked lstm
(slstm), a gated-feedback lstm (glstm; chung
et al. (2015)) and a depth-gated lstm (dlstm;
yao et al. (2015)). the gated-feedback lstm has
feedback gates connecting the hidden states across
multiple time steps as an adaptive control of the in-
formation    ow. the depth-gated lstm uses a depth
gate to connect memory cells of vertically adjacent
layers.
in general, both glstm and dlstm are
able to capture long-term dependencies to some de-
gree, but they do not explicitly keep past memories.
we set the number of layers to 3 in this experiment,
mainly to agree with the id38 exper-
iments of chung et al. (2015). also note that that
there are no single-layer variants for glstm and
dlstm; they have to be implemented as multi-layer
systems. the hidden unit size of the lstmn and all
comparison models (except kn5) was set to 300.

the results of the id38 task are
shown in table 1. perplexity results for kn5 and
id56 are taken from mikolov et al. (2015). as can
be seen, the single-layer lstmn outperforms these

models
rae (socher et al., 2011)
rntn (socher et al., 2013b)
did56 (irsoy and cardie, 2014)
did98 (blunsom et al., 2014)
id98-mc (kim, 2014)
t-id98 (lei et al., 2015)
pv (le and mikolov, 2014)
ct-lstm (tai et al., 2015)
lstm (tai et al., 2015)
2-layer lstm (tai et al., 2015)
lstmn
2-layer lstmn

fine-grained binary
82.4
85.4
86.6
86.8
88.1
88.6
87.8
88.0
84.9
86.3
86.3
87.0

43.2
45.7
49.8
48.5
48.0
51.2
48.7
51.0
46.4
46.0
47.6
47.9

table 2: model accuracy (%) on the sentiment tree-
bank (test set). the memory size of lstmn models
is set to 168 to be compatible with previously pub-
lished lstm variants (tai et al., 2015).

tences for training, 872 for validation and 1,821 for
testing. table 2 reports results on both    ne-grained
and binary classi   cation tasks.

we experimented with 1- and 2-layer lstmns.
for the latter model, we predict the sentiment la-
bel of the sentence based on the averaged hidden
vector passed to a 2-layer neural network classi   er
with relu as the activation function. the mem-
ory size for both lstmn models was set to 168 to
be compatible with previous lstm models (tai et
al., 2015) applied to the same task. we used pre-
trained 300-d glove 840b vectors (pennington et
al., 2014) to initialize the id27s. the
gradient for words with glove embeddings, was
scaled by 0.35 in the    rst epoch after which all word
embeddings were updated normally.

we used adam (kingma and ba, 2015) for op-
timization with the two momentum parameters set
to 0.9 and 0.999 respectively. the initial learning
rate was set to 2e-3. the id173 constant was
1e-4 and the mini-batch size was 5. a dropout rate
of 0.5 was applied to the neural network classi   er.

we compared our model with a wide range of top-
performing systems. most of these models (includ-
ing ours) are lstm variants (third block in table 2),
id56s (   rst block), or convolu-
tional neural networks (id98s; second block). re-
cursive models assume the input sentences are rep-
resented as parse trees and can take advantage of
annotations at the phrase level. lstm-type models
and id98s are trained on sequential input, with the

figure 4: examples of intra-attention (language
modeling). bold lines indicate higher attention
scores. arrows denote which word is being focused
when attention is computed, but not the direction of
the relation.

two baselines and the lstm by a signi   cant mar-
gin. amongst all deep architectures, the three-layer
lstmn also performs best. we can study the mem-
ory activation mechanism of the machine reader by
visualizing the attention scores. figure 4 shows
four sentences sampled from the id32 val-
idation set. although we explicitly encourage the
reader to attend to any memory slot, much attention
focuses on recent memories. this agrees with the
linguistic intuition that long-term dependencies are
relatively rare. as illustrated in figure 4 the model
captures some valid lexical relations (e.g., the de-
pendency between sits and at, sits and plays, every-
one and is, is and watching). note that arcs here
are undirected and are different from the directed
arcs denoting head-modi   er relations in dependency
graphs.

5.2 id31
our second task concerns the prediction of senti-
ment labels of sentences. we used the stanford sen-
timent treebank (socher et al., 2013a), which con-
tains    ne-grained sentiment labels (very positive,
positive, neutral, negative, very negative) for 11,855
sentences. following previous work on this dataset,
we used 8,544 sentences for training, 1,101 for val-
idation, and 2,210 for testing. the average sentence
length is 19.1. in addition, we also performed a bi-
nary classi   cation task (positive, negative) after re-
moving the neutral label. this resulted in 6,920 sen-

recent approaches use two sequential lstms to
encode the premise and the hypothesis respectively,
and apply neural attention to reason about their logi-
cal relationship (rockt  aschel et al., 2016; wang and
jiang, 2016). furthermore, rockt  aschel et al. (2016)
show that a non-standard encoder-decoder architec-
ture which processes the hypothesis conditioned on
the premise results signi   cantly boosts performance.
we use a similar approach to tackle this task with
lstmns. speci   cally, we use two lstmns to read
the premise and hypothesis, and then match them
by comparing their hidden state tapes. we perform
average pooling for the hidden state tape of each
lstmn, and concatenate the two averages to form
the input to a 2-layer neural network classi   er with
relu as the activation function.

we used pre-trained 300-d glove 840b vectors
(pennington et al., 2014) to initialize the word em-
beddings. out-of-vocabulary (oov) words were
initialized randomly with gaussian samples (  =0,
  =1). we only updated oov vectors in the    rst
epoch, after which all id27s were up-
dated normally. the dropout rate was selected from
[0.1, 0.2, 0.3, 0.4]. we used adam (kingma and ba,
2015) for optimization with the two momentum pa-
rameters set to 0.9 and 0.999 respectively, and the
initial learning rate set to 1e-3. the mini-batch size
was set to 16 or 32. for a fair comparison against
previous work, we report results with different hid-
den/memory dimensions (i.e., 100, 300, and 450).

we compared variants of our model against dif-
ferent types of lstms (see the second block in ta-
ble 3). speci   cally, these include a model which
encodes the premise and hypothesis independently
with two lstms (bowman et al., 2015), a shared
lstm (rockt  aschel et al., 2016), a word-by-word
attention model (rockt  aschel et al., 2016), and a
matching lstm (mlstm; wang and jiang (2016)).
this model sequentially processes the hypothesis,
and at each position tries to match the current word
with an attention-weighted representation of the
premise (rather than basing its predictions on whole
sentence embeddings). we also compared our mod-
els with a bag-of-words baseline which averages the
pre-trained embeddings for the words in each sen-
tence and concatenates them to create features for a
id28 classi   er (   rst block in table 3).
lstmns achieve better performance compared

figure 5: examples of intra-attention (sentiment
analysis). bold lines (red) indicate attention be-
tween sentiment important words.

exception of ct-lstm (tai et al., 2015) which op-
erates over tree-structured network topologies such
as constituent trees. for comparison, we also report
the performance of the paragraph vector model (pv;
le and mikolov (2014); see table 2, second block)
which neither operates on trees nor sequences but
learns distributed id194s param-
eterized directly.

the results in table 2 show that both 1- and
2-layer lstmns outperform the lstm baselines
while achieving numbers comparable to state of the
art. the number of layers for our models was set to
be comparable to previously published results. on
the    ne-grained and binary classi   cation tasks our
2-layer lstmn performs close to the best system
t-id98 (lei et al., 2015). figure 5 shows examples
of intra-attention for sentiment words. interestingly,
the network learns to associate sentiment important
words such as though and fantastic or not and good.

5.3 natural language id136
the ability to reason about the semantic relation-
ship between two sentences is an integral part of
text understanding. we therefore evaluate our model
on recognizing id123, i.e., whether two
premise-hypothesis pairs are entailing, contradic-
tory, or neutral. for this task we used the stan-
ford natural language id136 (snli) dataset
(bowman et al., 2015), which contains premise-
hypothesis pairs and target labels indicating their
relation. after removing sentences with unknown
labels, we end up with 549,367 pairs for training,
9,842 for development and 9,824 for testing. the
vocabulary size is 36,809 and the average sentence
length is 22. we performed lower-casing and tok-
enization for the entire dataset.

|  |m

h
test
models
        59.8
bow concatenation
77.6
100
221k
lstm (bowman et al., 2015)
252k
100
83.5
lstm-att (rockt  aschel et al., 2016)
300
1.9m 86.1
mlstm (wang and jiang, 2016)
81.5
260k
100
lstmn
84.3
280k
100
lstmn shallow fusion
100
330k
84.5
lstmn deep fusion
1.4m 85.2
300
lstmn shallow fusion
1.7m 85.7
300
lstmn deep fusion
2.8m 86.0
450
lstmn shallow fusion
3.4m 86.3
450
lstmn deep fusion
table 3: parameter counts |  |m, size of hidden
unit h, and model accuracy (%) on the natural lan-
guage id136 task.

to lstms (with and without attention; 2nd block
in table 3). we also observe that fusion is gen-
erally bene   cial, and that deep fusion slightly im-
proves over shallow fusion. one explanation is that
with deep fusion the inter-attention vectors are re-
currently memorized by the decoder with a gating
operation, which also improves the information    ow
of the network. with standard training, our deep fu-
sion yields the state-of-the-art performance in this
task. although encouraging, this result should be in-
terpreted with caution since our model has substan-
tially more parameters compared to related systems.
we could compare different models using the same
number of total parameters. however, this would in-
evitably introduce other biases, e.g., the number of
hyper-parameters would become different.

6 conclusions
in this paper we proposed a machine reading simula-
tor to address the limitations of recurrent neural net-
works when processing inherently structured input.
our model is based on a long short-term mem-
ory architecture embedded with a memory network,
explicitly storing contextual representations of in-
put tokens without recursively compressing them.
more importantly, an intra-attention mechanism is
employed for memory addressing, as a way to in-
duce undirected relations among tokens. the at-
tention layer is not optimized with a direct super-
vision signal but with the entire network in down-
stream tasks. experimental results across three tasks
show that our model yields performance comparable

or superior to state of the art.

although our experiments focused on lstms, the
idea of building more structure aware neural models
is general and can be applied to other types of net-
works. when direct supervision is provided, simi-
lar architectures can be adapted to tasks such as de-
pendency parsing and id36. in the fu-
ture, we hope to develop more linguistically plausi-
ble neural architectures able to reason over nested
structures and neural models that learn to discover
compositionality with weak or indirect supervision.

acknowledgments

we thank members of the ilcc at the school of
informatics and the anonymous reviewers for help-
ful comments. the support of the european re-
search council under award number 681760    trans-
lating multiple modalities into text    is gratefully
acknowledged.

references
[andreas et al.2016] jacob andreas, marcus rohrbach,
trevor darrell, and dan klein. 2016. learning to
compose neural networks for id53. in
proceedings of the 2016 naacl: hlt, pages 1545   
1554, san diego, california.

[bahdanau et al.2014] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio.
2014. neural machine
translation by jointly learning to align and translate.
in proceedings of the 2014 iclr, banff, alberta.

[bengio et al.1994] yoshua bengio, patrice simard, and
paolo frasconi. 1994. learning long-term depen-
dencies with id119 is dif   cult. neural net-
works, ieee transactions on, 5(2):157   166.

[blunsom et al.2014] phil blunsom, edward grefenstette,
and nal kalchbrenner. 2014. a convolutional neural
in proceedings of
network for modelling sentences.
the 52nd acl, pages 655   665, baltimore, maryland.
[bowman et al.2015] samuel r bowman, gabor angeli,
christopher potts, and christopher d manning. 2015.
a large annotated corpus for learning natural language
id136. in proceedings of the 2015 emnlp, pages
22   32, lisbon, portugal.

[bowman et al.2016] samuel r bowman, jon gauthier,
abhinav rastogi, raghav gupta, christopher d man-
ning, and christopher potts. 2016. a fast uni   ed
model for parsing and sentence understanding. in pro-
ceedings of the 54th acl, pages 1466   1477, berlin,
germany.

[cho et al.2014] kyunghyun cho, bart van merri  enboer,
caglar gulcehre, dzmitry bahdanau, fethi bougares,
holger schwenk, and yoshua bengio. 2014. learning
phrase representations using id56 encoder-decoder
for id151. in proceedings of
the 2014 emnlp, pages 1724   1734, doha, qatar.

[chung et al.2015] junyoung chung, caglar gulcehre,
kyunghyun cho, and yoshua bengio. 2015. gated
feedback recurrent neural networks. in proceedings of
the 32nd icml, pages 2067   2075, lille, france.

[clark et al.2013] peter clark, phil harrison, and niran-
jan balasubramanian. 2013. a study of the knowl-
edge base requirements for passing an elementary sci-
ence test. in proceedings of the 3rd workshop on au-
tomated kb construction, san francisco, california.
and
bernardo magnini. 2005. the pascal recognising
in proceedings of the
id123 challenge.
pascal challenges workshop on recognising tex-
tual entailment.

[dagan et al.2005] ido dagan, oren glickman,

[das et al.1992] sreerupa das, c. lee giles, and guo
zheng sun. 1992. learning context-free grammars:
capabilities and limitations of a recurrent neural net-
work with an external stack memory. in proceedings
of the 14th annual conference of the cognitive sci-
ence society, pages 791   795. morgan kaufmann pub-
lishers.

[dyer et al.2015] chris dyer, miguel ballesteros, wang
ling, austin matthews, and noah a smith. 2015.
transition-based id33 with stack long
short-term memory. in proceedings of the 53rd acl,
pages 334   343, beijing, china.

[etzioni et al.2011] oren etzioni, anthony fader, janara
christensen, stephen soderland, and mausam. 2011.
id10: the second genera-
tion. in proceedings of the 22nd ijcai, pages 3   10,
barcelona, spain.

[fader et al.2011] anthony fader, stephen soderland,
and oren etzioni. 2011. identifying relations for open
in proceedings of the 2011
information extraction.
emnlp, pages 1535   1545, edinburgh, scotland, uk.
and
john m. henderson. 1991. recovery from misanaly-
ses of garden-path sentences. journal of memory and
language, 30:725   745.

[ferreira and henderson1991] fernanda

ferreira

[frank and bod2011] stefan l. frank and rens bod.
2011. insensitivity of the human sentence-processing
system to hierarchical structure. pyschological sci-
ence, 22(6):829   834.

[graves2013] alex graves. 2013. generating sequences
arxiv preprint

with recurrent neural networks.
arxiv:1308.0850.

[grefenstette et al.2015] edward

karl moritz hermann, mustafa suleyman,

grefenstette,
and

phil blunsom. 2015. learning to transduce with un-
bounded memory. in advances in neural information
processing systems, pages 1819   1827.

[hermann et al.2015] karl moritz hermann, tomas ko-
cisky, edward grefenstette, lasse espeholt, will kay,
mustafa suleyman, and phil blunsom. 2015. teach-
ing machines to read and comprehend. in advances in
neural information processing systems, pages 1684   
1692.

[hochreiter and schmidhuber1997] sepp hochreiter and
j  urgen schmidhuber. 1997. long short-term memory.
neural computation, 9(8):1735   1780.

[hochreiter1991] sepp hochreiter. 1991. untersuchun-
gen zu dynamischen neuronalen netzen. diploma,
technische universit  at m  unchen.

[irsoy and cardie2014] ozan irsoy and claire cardie.
2014. deep id56s for composi-
tionality in language. in advances in neural informa-
tion processing systems, pages 2096   2104.

[kim2014] yoon kim. 2014. convolutional neural net-
in proceedings of

works for sentence classi   cation.
the 2014 emnlp, pages 1746   1751, doha, qatar.

[kingma and ba2015] diederik kingma and jimmy ba.
2015. adam: a method for stochastic optimization.
in proceedings of the 2015 iclr, san diego, califor-
nia.

[klein and manning2004] dan klein and christopher
manning. 2004. corpus-based induction of syntac-
tic structure: models of dependency and constituency.
in proceedings of the 42nd acl, pages 478   485,
barcelona, spain.

[konieczny2000] lars konieczny. 2000. locality and
journal of psycholinguistics,

parsing complexity.
29(6):627   645.

[koutn    k et al.2014] jan koutn    k, klaus greff, faustino
gomez, and j  urgen schmidhuber. 2014. a clockwork
id56. in proceedings of the 31st icml, pages 1863   
1871, beijing, china.

[kumar et al.2016] ankit kumar, ozan irsoy, jonathan
su, james bradbury, robert english, brian pierce, pe-
ter ondruska, ishaan gulrajani, and richard socher.
2016. ask me anything: dynamic memory networks
for natural language processing. in proceedings of the
33rd icml, new york, ny.

[le and mikolov2014] quoc v le and tomas mikolov.
2014. distributed representations of sentences and
documents. in proceedings of the 31st icml, pages
1188   1196, beijing, china.

[lei et al.2015] tao lei, regina barzilay, and tommi
jaakkola. 2015. molding id98s for text: non-linear,
non-consecutive convolutions. in proceedings of the
2015 emnlp, pages 1565   1575, lisbon, portugal.

[socher et al.2013b] richard socher, alex perelygin,
jean y wu, jason chuang, christopher d manning,
andrew y ng, and christopher potts. 2013b. recur-
sive deep models for semantic compositionality over
in proceedings of the 2013
a sentiment treebank.
emnlp, pages 1631   1642, seattle, washingtton.

[sukhbaatar et al.2015] sainbayar sukhbaatar, jason we-
ston, rob fergus, et al. 2015. end-to-end memory
networks. in advances in neural information process-
ing systems, pages 2431   2439.

[tai et al.2015] kai sheng tai, richard socher, and
christopher d manning. 2015.
improved semantic
representations from tree-structured long short-term
memory networks. in proceedings of the 53rd acl,
pages 1556   1566, beijing, china.

[tanenhaus et al.1995] michael k. tanenhaus, michael j.
spivey-knowlton, kathleen m. eberhard, and julue c.
sedivy. 1995. integration of visual and linguistic in-
formation in spoken language comprehension. sci-
ence, 268:1632   1634.

[tran et al.2016] ke tran, arianna bisazza, and christof
monz. 2016. recurrent memory network for language
in proceedings of the 15th naacl, san
modeling.
diego, ca.

[wang and jiang2016] shuohang wang and jing jiang.
2016. learning natural language id136 with lstm.
in proceedings of the 2016 naacl: hlt, pages 1442   
1451, san diego, california.

[weston et al.2015] jason weston, sumit chopra, and
in pro-

antoine bordes. 2015. memory networks.
ceedings of the 2015 iclr, san diego, usa.

[xiong et al.2016] caiming xiong, stephen merity, and
richard socher. 2016. dynamic memory networks
for visual and textual id53. in proceed-
ings of the 33rd icml, new york, ny.

[yao et al.2015] kaisheng yao, trevor cohn, katerina
vylomova, kevin duh, and chris dyer.
2015.
depth-gated recurrent neural networks. arxiv preprint
arxiv:1508.03790.

[zaremba and sutskever2014] wojciech zaremba

and
ilya sutskever. 2014. learning to execute. arxiv
preprint arxiv:1410.4615.

[meng et al.2015] fandong meng,

zhengdong lu,
zhaopeng tu, hang li, and qun liu. 2015. a deep
memory-based architecture for sequence-to-sequence
in proceedings of iclr-workshop 2016,
learning.
san juan, puerto rico.

[mikolov et al.2010] tomas mikolov, martin kara     at,
lukas burget, jan cernock`y, and sanjeev khudan-
pur. 2010. recurrent neural network based language
in proceedings of 11th interspeech, pages
model.
1045   1048, makuhari, japan.

[mikolov et al.2015] tomas mikolov, armand joulin,
sumit chopra, michael mathieu, and marc   aurelio
ranzato. 2015. learning longer memory in recurrent
neural networks. in proceedings of iclr workshop,
san diego, california.

[pascanu et al.2013] razvan pascanu, tomas mikolov,
and yoshua bengio. 2013. on the dif   culty of train-
ing recurrent neural networks. in proceedings of the
30th icml, pages 1310   1318, atlanta, georgia.

[pennington et al.2014] jeffrey

pennington,

richard
socher, and christopher d. manning. 2014. glove:
in proceed-
global vectors for word representation.
ings of the 2014 emnlp, pages 1532   1543, doha,
qatar.

[poon and domingos2010] hoifung poon and pedro
domingos. 2010. unsupervised ontology induction
from text. in proceedings of the 48th annual meeting
of
the association for computational linguistics,
pages 296   305, uppsala.

[rayner1998] keith rayner. 1998. eye movements in
reading and information processing: 20 years of re-
search. psychological bulletin, 124(3):372   422.

[rockt  aschel et al.2016] tim rockt  aschel,

edward
grefenstette, karl moritz hermann, tom  a  s ko  cisk`y,
and phil blunsom. 2016. reasoning about entailment
in proceedings of the 2016
with neural attention.
iclr, san juan, puerto rico.

[rush et al.2015] alexander m rush, sumit chopra, and
jason weston. 2015. a neural attention model for
abstractive sentence summarization. in proceedings of
the 2015 emnlp, pages 379   389, lisbon, portugal.

[socher et al.2011] richard socher, eric h huang, jef-
frey pennin, christopher d manning, and andrew y
ng. 2011. dynamic pooling and unfolding recursive
autoencoders for paraphrase detection. in advances in
neural information processing systems, pages 801   
809.

[socher et al.2013a] richard socher, alex perelygin,
jean wu, jason chuang, christopher d. manning, an-
drew ng, and christopher potts. 2013a. recursive
deep models for semantic compositionality over a sen-
timent treebank. in proceedings of the 2013 emnlp,
pages 1631   1642, seattle, washington.

