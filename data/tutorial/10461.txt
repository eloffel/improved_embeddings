 
 

 

item2vec: neural item embedding for id185 

oren barkan^* and noam koenigstein* 

 

 

^tel aviv university 

*microsoft 

abstract 

 

 

             

 

 

many  collaborative  filtering  (cf)  algorithms  are  item-
based in the sense that they analyze item-item relations in 
order to produce item similarities. recently, several works 
in  the  field  of  natural  language  processing  (nlp) 
suggested  to  learn  a  latent  representation  of  words  using 
neural embedding algorithms. among them, the skip-gram 
with  negative  sampling 
(sgns),  also  known  as 
id97, was shown to provide state-of-the-art results on 
various linguistics tasks. in this paper, we show that item-
based  cf  can  be  cast  in  the  same  framework  of  neural 
word  embedding.  inspired  by  sgns,  we  describe  a 
method  we  name  item2vec  for  item-based  cf  that 
produces  embedding  for  items  in  a  latent  space.  the 
method  is  capable  of  inferring  item-item  relations  even 
when  user  information  is  not  available.  we  present 
experimental  results  that  demonstrate  the  effectiveness  of 
the item2vec method and show it is competitive with svd. 
 
index  terms       skip-gram,  id97,  neural  word 
embedding,  collaborative  filtering, 
item  similarity, 
recommender  systems,  market  basket  analysis,  item-
item id185, item recommendations. 
 

 
1. introduction and related work 

recommender 

computing  item  similarities  is  a  key  building  block  in 
modern 
systems.  while  many 
recommendation  algorithms  are  focused  on  learning  a 
low  dimensional  embedding  of  users  and 
items 
simultaneously  [1,  2,  3],  computing  item  similarities  is 
an  end  in  itself.  item  similarities  are  extensively  used 
by  online  retailers  for  many  different  recommendation 
tasks.  this  paper  deals  with  the  overlooked  task  of 
learning  item  similarities  by  embedding  items  in  a  low 
dimensional space.  
     item-based  similarities  are  used  by  online  retailers 
for  recommendations  based  on  a  single  item.  for 
example,  in  the  windows  10  app  store,  the  details 
page  of  each  app  or  game  includes  a  list  of  other 
similar  apps  titled     people  also  like   .  this  list  can  be 

1 

 

 

 

 

 

 

 

 

 
 
 
 

 
fig. 1. recommendations in windows 10 store based on similar 
items to need for speed. 

extended  to  a  full  page  recommendation  list  of  items 
similar  to  the  original  app  as  shown  in  fig.  1.  similar 
recommendation  lists  which  are  based  merely  on 
similarities  to  a  single  item  exist  in  most  online  stores 
e.g.,  amazon,  netflix,  google  play,  itunes  store  and 
many others.  
    the  single  item  recommendations  are  different  than 
the  more     traditional     user-to-item  recommendations 
because  they  are  usually  shown  in  the  context  of  an 
explicit  user  interest  in  a  specific  item  and  in  the 
context  of  an  explicit  user 
to  purchase. 
therefore,  single  item  recommendations  based  on  item 
similarities  often  have  higher  click-through  rates 
(ctr) 
and 
consequently  responsible  for  a  larger  share  of  sales  or 
revenue.   

recommendations 

user-to-item 

intent 

than 

 
 

for  users,  would  produce  better 

single item recommendations based on item similarities 
are  used  also  for  a  variety  of  other  recommendation 
tasks:  in     candy  rank     recommendations  for  similar 
items  (usually  of  lower  price)  are  suggested  at  the 
check-out  page  right  before  the  payment.  in     bundle    
recommendations  a  set  of  several  items  are  grouped 
and  recommended  together.  finally,  item  similarities 
are  used  in  online  stores  for  better  exploration  and 
discovery and improve the overall user experience. it is 
unlikely  that  a  user-item  cf  method,  that  learns  the 
connections  between  items  implicitly  by  defining  slack 
variables 
item 
representations than a method that is optimized to learn 
the item relations directly. 
     item  similarities  are  also  at  the  heart  of  item-based 
cf  algorithms  that  aim  at  learning  the  representation 
directly  from  the  item-item  relations  [4,  5].  there  are 
several  scenarios  where  item-based  cf  methods  are 
desired:  in  a  large  scale  dataset,  when  the  number  of 
users  is  significantly  larger  than  the  number  of  items, 
the  computational  complexity  of  methods  that  model 
items  solely  is  significantly  lower  than  methods  that 
model  both  users  and 
items  simultaneously.  for 
example,  online  music  services  may  have  hundreds  of 
millions of enrolled users with just tens of thousands of 
artists (items). 
     in  certain  scenarios,  the  user-item  relations  are  not 
available.  for  instance,  a  significant  portion  of  today   s 
online  shopping  is  done  without  an  explicit  user 
identification 
available 
information  is  per  session.  treating  these  sessions  as 
   users     would  be  prohibitively  expensive  as  well  as 
less informative. 
     recent  progress  in  neural  embedding  methods  for 
linguistic  tasks  have  dramatically  advanced  state-of-
the-art  nlp  capabilities  [6,  7,  8,  12].  these  methods 
attempt to map words and phrases to a low dimensional 
vector  space  that  captures  semantic  relations  between 
words. specifically, skip-gram with negative sampling 
(sgns),  known  also  as  id97  [8],  set  new  records 
in  various  nlp  tasks  [7,  8]  and  its  applications  have 
been extended to other domains beyond nlp [9, 10]. 
     in  this  paper,  we  propose  to  apply  sgns  to  item-
based  cf.  motivated  by  its  great  success  in  other 
that  sgns  with  minor 
domains,  we 
modifications  may  capture 
relations  between 
different  items  in  collaborative  filtering  datasets.  to 
this  end,  we  propose  a  modified  version  of  sgns 
named  item2vec.  we  show  that  item2vec  can  induce  a 
similarity  measure  that  is  competitive  with  an  item-
based  cf  using  svd,  while  leaving  the  comparison  to 
other more complex methods to a future research.  
     the  rest  of  the  paper  is  organized  as  follows: 
section  2  overviews  the  sgns  method.  section  3 

process. 

instead, 

suggest 

the 

the 

describes  how  to  apply  sgns  to  item-based  cf.  in 
section  4,  we  describe  the  experimental  setup  and 
present qualitative and quantitative results. 
 

2. skip-gram with negative sampling 

that  was 

skip-gram  with  negative  sampling  (sgns)  is  a  neural 
word  embedding  method 
introduced  by 
mikolov et. al in [8]. the method aims at finding words 
representation that captures the relation between a word 
to  its  surrounding  words  in  a  sentence.  in  the  rest  of 
this  section,  we  provide  a  brief  overview  of  the  sgns 
method. 

given  a  sequence  of  words 

(

iw =   from  a  finite 
,  the  skip-gram  objective  aims 

1

i

)k

vocabulary 

{ }w
w w =

=

i

i

1

at maximizing the following term: 

     

1

k

k

       

i

=            

c j c j

1

,

   

0

log (

p w w
i

|

+

i

j

)

                 (1) 

where  c   is  the  context  window  size  (that  may  depend 
on 

p w w  is the softmax function: 

iw ) and (

)

|

j

i

                    

p w w
i

(

|

j

)

exp(

u v

t
i

j

)

exp(

u v
k

t
i

=    

                  (2) 

)

   
k i

w

(

(

iv

)m

)m

where 

iu u              and 

v              are  latent 
vectors  that  correspond  to  the  target  and  context 
iw w    ,  respectively, 
representations  for  the  word 
is  chosen 

wi
empirically and according to the size of the dataset. 

the  parameter  m  

  and 

{1,...,

w

   

}

using eq. (2) is impractical due to the computational 
, which is a linear function of 

complexity of 

   

(

)

|

p w w
i

j

the  vocabulary  size  w   that  is  usually  in  size  of 

5

10

6

10   

 . 

negative  sampling  comes  to  alleviate  the  above 
computational  problem  by  the  replacement  of  the 
softmax function from eq.(2) with 

n

p w w
i

(

|

j

)

=

  
(
u v

t
i

j

      
  
)
(

u v
k

t
i

k

=

1

)

 

where 

   =

( ) 1 / 1 exp(
x

+

    ,  n   is  a  parameter  that 

x

)

determines  the  number  of  negative  examples  to  be 
drawn  per  a  positive  example.  a  negative  word 
iw   is 
sampled  from  the  unigram  distribution  raised  to  the 
3/4rd  power.  this  distribution  was 
to 
significantly  outperform 
the  unigram  distribution, 
empirically [8]. 

found 

in order to overcome the imbalance between rare and 
frequent  words the following subsampling procedure is 

2 

 
 

proposed  [8]:  given  the  input  word  sequence,  we 
id203 
discard 

each  word 

w   with 

a 

p discard w

(

|

) 1

=    

  

)
f w

(

 

  where 

 

f w  

)

(

is 

the 

frequency  of  the  word  w   and       is  a  prescribed 

threshold.  this  procedure  was  reported  to  accelerate 
the  learning  process  and  to  improve  the  representation 
of rare words significantly [8]. 

  finally,  u   and  v   are  estimated  by  applying 
stochastic  gradient  ascent  with  respect  to  the  objective 
in eq. (1).  
 

3. item2vec     sgns for item similarity 

in  the  context  of  cf  data,  the  items  are  given  as  user 
generated sets. note that the information about the relation 
between  a  user  and  a  set of items is not always available. 
for example, we might be given a dataset of orders that a 
store received, without the information about the user that 
made the order. in other words, there are scenarios where 
multiple  sets  of  items  might  belong  to  the  same  user,  but 
this  information  is  not  provided.  in  section  4,  we  present 
experimental  results  that  show  that  our  method  handles 
these scenarios as well. 
    we  propose  to  apply  sgns  to  item-based  cf.  the 
application of sgns to cf data is straightforward once we 
realize  that  a  sequence  of  words  is  equivalent  to  a  set  or 
basket  of  items.  therefore,  from  now  on,  we  will  use  the 
terms    word    and    item    interchangeably. 
     by  moving  from  sequences  to  sets,  the  spatial  /  time 
information is lost. we choose to discard this information, 
since in this paper, we assume a static environment where 
items  that  share  the  same  set  are  considered  similar,  no 
matter  in  what  order  /  time  they  were  generated  by  the 
user. this assumption may not hold in other scenarios, but 
we  keep  the  treatment  of  these  scenarios  out  of  scope  of 
this paper. 
     since  we  ignore  the  spatial  information,  we  treat  each 
pair of items that share the same set as a positive example. 
this implies a window size that is determined from the set 
size.  specifically,  for  a  given  set  of  items,  the  objective 
from eq. (1) is modified as follows: 

1

k

k

      

k =
1
i

j

   

i

log (

p w w
i

|

j

)

. 

another  option  is  to  keep  the objective in eq. (1) as 
is,  and  shuffle  each  set  of  items  during  runtime.  in our 
experiments we observed that both options perform the 
same. 
     the  rest  of  the  process  remains  identical  to  the 
method  described  in  section  2. we name the described 
method item2vec.   
     in  this  work,  we  used 

iu   as  the  final representation 

3 

for  the  i -th  item  and  the  affinity  between  a  pair  of 
items  is  computed  by  the  cosine  similarity.  other 
v+  
options  are  to  use 

iv ,  the  additive  composition, 

u
i

i

or  the  concatenation 

   
   

t
u v
i

t
i

t

      

.  note  that  the  last  two 

options sometimes produce superior representation. 

  

4. experimental setup and results 

in  this  section,  we  present  an  empirical  evaluation  of  the 
item2vec  method.  we  provide  both  qualitative  and 
quantitative  results  depending  whether  a  metadata  about 
the items exists. as a baseline item-based cf algorithm we 
used item-item svd.  

4.1  datasets 

we  evaluate  the  methods  on  two  different  datasets,  both 
private. the first dataset is user-artist data that is retrieved 
from  the  microsoft  xbox  music  service.  this  dataset 
consist  of  9m  events.  each  event  consists  of  a  user-artist 
relation,  which  means  the  user  played  a  song  by  the 
specific  artist.  the  dataset  contains  732k  users  and  49k 
distinct artists.  
     the  second  dataset  contains  orders  of  products  from 
microsoft  store.  an  order  is  given  by  a  basket  of  items 
without  any  information  about  the  user  that  made  it. 
therefore,  the  information  in this dataset is weaker in the 
sense  that  we  cannot  bind  between  users  and  items.  the 
dataset  consist  of  379k  orders  (that  contains  more  than  a 
single item) and 1706  distinct items. 

4.2  systems and parameters 

we applied item2vec to both datasets. the optimization 
is  done  by  stochastic  gradient  decent.  we  ran  the 
algorithm  for  20  epochs.  we  set  the  negative  sampling 
value  to 
  for  both  datasets.  the  dimension 
parameter  m   was  set  to  100  and  40  for  the  music  and 
further  applied 
store  datasets, 
310      to  the 
subsampling  with       values  of 

respectively.  we 

510      and 

n =

15

music  and  store  datasets,  respectively.  the  reason  we 
set  different  parameter  values  is  due  to  different  sizes 
of the datasets. 

j   entry  contains  the  number  of  times  (

we  compare  our  method  to  a  svd  based  item-item 
similarity  system.  to  this  end,  we  apply  svd  to  a 
square  matrix  in  size  of  number  of  items,  where  the 
)
jw w  
( , )
i
appears  as  a  positive  pair  in  the  dataset.  then,  we 
normalized  each  entry  according  to  the  square  root  of 
the  product  of  its  row  and  column  sums.  finally,  the 
latent  representation  is  given  by  the  rows  of 
, 
where  s  is a diagonal matrix that its diagonal contains 
the  top  m   singular  values  and  u   is  a  matrix  that 
contains  the  corresponding  left  singular  vectors  as 

1/ 2us

,

i

 , 

 
 

 

 - 

 

 

 

 

 

 

 

 

 
 
 
 

fig.2: id167 embedding for the item vectors produced by item2vec (a) and svd (b). 
the items are colored according to a web retrieved genre metadata. 
 

columns.  the  affinity  between  items  is  computed  by 
cosine  similarity  of  their  representations.  throughout 
this section we name this method    svd   . 

table  1:  inconsistencies  between  genres  from 
the  web  catalog  and  the  item2vec  based  knn 
predictions 

4.3  experiments and results 

the  music  dataset  does  not  provide  genre  metadata. 
therefore,  for  each  artist  we retrieved the genre metadata 
from the web to form a genre-artist catalog. then we used 
this  catalog  in  order  to  visualize  the  relation  between  the 
learnt  representation  and  the  genres.  this  is  motivated  by 
the  assumption  that  a  useful  representation  would  cluster 
artists according to their genre. to this end, we generated a 
subset  that  contains  the  top  100  popular  artists  per  genre 
for  the  following  distinct  genres:  'r&b  /  soul',  'kids', 
'classical',  'country',  'electronic  /  dance',  'jazz',  'latin', 
'hip hop', 'reggae / dancehall', 'rock', 'world', 'christian 
/ gospel' and 'blues / folk'. we applied id167 [11] with a 
cosine  kernel  to  reduce  the  dimensionality  of  the  item 
vectors to 2. then, we colored each artist point according 
to its genre. 
figures  2(a)  and  2(b)  present  the  2d  embedding  that  was 
produced  by  id167,  for  item2vec  and  svd,  respectively. 
as  we  can  see,  item2vec  provides  a  better  id91. we 
further  observe  that  some  of  the  relatively  homogenous 
areas  in  fig.  2(a)  are  contaminated  with  items  that  are 
colored differently. we found out that many of these cases 
originate in artists that were mislabeled in the web or have 
a mixed genre. 
     table  1  presents  several  examples,  where  the  genre 
associated  with  a  given  artist  (according  to  metadata 
that we retrieved from the web) is inaccurate or at least 
inconsistent  with  wikipedia.  therefore,  we  conclude 
that  usage  based  models  such  as  item2vec  may  be 
useful  for  the  detection  of  mislabeled  data  and  even 

artist name 

dmx        
llj 
walter beasley 
sevendust        
big bill roonzy 
anita baker 
cassandra 
wilson 
notixx 

genre from 
web catalog 
(incorrect) 
r&b / soul 
rock /metal 
blues / folk 

hip hop  
reggae 
rock 

genre predicted by 
item2vec based knn 

(correct) 
hip hop  
hip hop 

jazz  

rock / metal  
blues / folk 
r&b / soul 

r&b / soul 

jazz 

reggae 

electronic 

table  2: 
  a  comparison  between  svd  and 
item2vec on genre classification task for various 
sizes of top popular artist sets 

top (q) popular 

artists  

svd 

accuracy 

item2vec 
accuracy 

2.5k 
5k 
10k 
15k    
20k 
10k unpopular (see 
text) 

85% 
83.4% 
80.2% 
76.8% 
73.8% 

58.4% 

86.4%  
84.2% 
82% 
79.5% 
77.9% 

68% 

provide  a  suggestion  for  the  correct  label  using  a 
simple k nearest neighbor (knn) classifier. 
     in  order  to quantify the similarity quality, we tested 
the  genre  consistency  between  an  item  and  its  nearest 

4 

 
 

 

table 3: a qualitative comparison between item2vec and svd for selected items from the music dataset 

seed item (genre) 

item2vec     top 4 recommendations 

svd     top 4 recommendations 

david guetta 
(dance) 
katy perry (pop) 

dr. dre (hip hop) 

johnny cash 
(country) 
guns n' roses 
(rock) 
justin timberlake 
(pop) 

avicii ,calvin harris, martin solveig, deorro 

brothers, the blue rose, jwj, akcent 

miley cyrus, kelly clarkson, p!nk, taylor swift 

game, snoop dogg, n.w.a, dmx 

willie nelson, jerry reed, dolly parton, merle 
haggard 

last friday night, winx club, boots on cats, thaman 
s. 
jack the smoker, royal goon, hoova slim, man 
power 
hank williams, the highwaymen, johnny horton, 
hoyt axton 

aerosmith, ozzy osbourne, bon jovi, ac/dc 

bon jovi, gilby clarke, def leppard, mtley cre 

rihanna, beyonce, the black eyed peas, bruno 
mars 

jc chasez. jordan knight, shontelle, nsync 

table 4: a qualitative comparison between item2vec and svd for selected items from the store dataset 

seed item 

item2vec     top 4 recommendations 

svd     top 4 recommendations 

lego emmet 

lego bad cop, lego simpsons: bart, lego 
ninjago, lego scooby-doo 

minecraft foam, disney toy box, minecraft (xbox 
one), terraria (xbox one) 

minecraft 
lanyard 

gopro lcd 
touch 
bacpac 

surface pro 4 
type cover  

disney 
baymax 

windows 
server 2012 
r2 

minecraft diamond earrings, minecraft periodic 
table, minecraft crafting table, minecraft enderman 
plush  

rabbids invasion, mortal kombat, minecraft periodic 
table 

gopro anti-fog inserts, gopro the frame mount, 
gopro floaty backdoor, gopro 3-way 

titanfall (xbox one), gopro the frame mount, call of 
duty (pc), evolve (pc) 

uag surface pro 4 case, zip sleeve for surface, 
surface 65w power supply, surface pro 4 screen 
protection 

farming simulator (pc), dell 17 gaming laptop, bose 
wireless headphones, uag surface pro 4  case 

disney maleficent, disney hiro, disney stich, disney 
marvel super heroes 

disney stich, mega bloks halo unsc firebase, lego 
simpsons: bart,  mega bloks halo unsc gungoose 

windows server remote desktop services 1-user, 
exchange server 5-client, windows server 5-user 
client access, exchange server 5-user client access 

nba live (xbox one)     600 points download code, 
windows 10 home, mega bloks halo covenant drone 
outbreak,  mega bloks halo unsc vulture gunship 

neighbors.  we  do  that  by  iterating  over  the  top  q  

popular  items  (for  various  values  of  q )  and  check 

whether  their  genre  is consistent with the genres of the 
k   nearest  items  that  surround  them.  this  is  done  by  a 
simple majority voting. we ran the same experiment for 
different neighborhood sizes ( k  = 6, 8, 10, 12 and 16)  
and no significant change in the results was observed.  
k = .  we 
     table  2  presents  the  results  obtained  for 
observe  that  item2vec  is  consistently  better  than  the 
svd  model,  where  the  gap  between  the  two  keeps 
growing  as  q  
that 

increases.  this  might 

imply 

8

item2vec  produces  a  better  representation  for  less 
popular items than the one produced by svd, which is 
unsurprising  since  item2vec  subsamples  popular  items 
and  samples  the  negative  examples  according  to  their 

5 

popularity. 
we  further  validate  this  hypothesis  by  applying  the 
same     genre  consistency     test  to  a  subset  of  10k 
unpopular items (the last row in table 2). we define an 
unpopular  item  in  case  it  has  less  than  15  users  that 
played  its  corresponding  artist.  the  accuracy  obtained 
by  item2vec  was  68%,  compared  to  58.4%  by  svd. 
qualitative  comparisons  between  item2vec  and  svd  are 
presented  in  tables  3-4  for  music  and  store  datasets, 
respectively.  the  tables  present  seed  items  and  their  4 
nearest  neighbors  (in 
latent  space).  the  main 
advantage  of  this  comparison  is  that  it  enables  the 
inspection  of  item  similarities  in  higher  resolutions  than 
genres.  moreover,  since  the  store  dataset  lacks  any 
informative  tags  /  labels,  a  qualitative  evaluation  is 
inevitable.  we  observe  that  for  both  datasets,  item2vec 

the 

 
 

provides  lists  that  are  better  related  to  the  seed  item  than 
the  ones  that  are  provided  by  svd.  furthermore,  we  see 
that  even  though  the  store  dataset  contains  weaker 
information, item2vec manages to infer item relations quite 
well. 
 

5. conclusion 

in this paper, we proposed item2vec - a neural embedding 
algorithm  for  item-based  collaborative  filtering.  item2vec 
is based on sgns with minor modifications. 
     we  present  both  quantitative 
and  qualitative 
evaluations that demonstrate the effectiveness of item2vec 
when compared to a svd-based item similarity model. we 
observed  that  item2vec  produces  a  better  representation 
for  items  than  the  one  obtained  by  the  baseline  svd 
model,  where  the  gap  between  the  two  becomes  more 
significant for unpopular items. we explain this by the fact 
that  item2vec  employs  negative  sampling  together  with 
subsampling of popular items. 
     in  future  we  plan  to  investigate  more  complex  cf 
models  such  as  [1,  2,  3]  and  compare  between  them  and 
item2vec.  we  will  further  explore  bayesian  variants  [12] 
of  sg for the application of item similarity. 

 

6. references 

[1]  paquet,  u.,  koenigstein,  n.  (2013,  may).  one-class 
id185 with random graphs. in proceedings of 
the  22nd  international  conference  on  world  wide  web  (pp. 
999-1008).  

[2]  koren  y,  bell  r,  volinsky  c.  matrix  factorization 
techniques  for  recommender  systems.  computer.  2009  aug 
1(8):30-7.  

[3]  salakhutdinov  r,  mnih a.  bayesian  probabilistic  matrix 
factorization  using  markov  chain  monte  carlo. 
in 
proceedings icml 2008 jul 5 (pp. 880-887). 

[4]  sarwar  b,  karypis  g,  konstan  j,  riedl  j.  item-based 
collaborative 
in 
proceedings www 2001 apr 1 (pp. 285-295). acm.  

recommendation  algorithms. 

filtering 

[5]  linden  g,  smith  b,  york 
recommendations: 
internet computing, ieee. 2003 jan;7(1):76-80. 

item-to-item 

collaborative 

j.  amazon.com 
filtering. 

[6]  collobert  r,  weston  j. a  unified  architecture  for  natural 
language  processing:  deep  neural  networks  with  multitask 
learning. in proceedings of icml 2008 jul 5 (pp. 160-167). 

[7]  mnih  a,  hinton  ge.  a  scalable  hierarchical  distributed 
language  model.  in  proceedings  of  nips  2009  (pp.  1081-
1088). 

[8]  mikolov  t,  sutskever  i,  chen  k,  corrado  gs,  dean  j. 
distributed  representations  of  words  and  phrases  and  their 
compositionality.  in  proceedings  of  nips  2013  (pp.  3111-
3119). 

[9]  frome  a,  corrado  gs,  shlens  j,  bengio  s,  dean  j, 
mikolov  t.  devise:  a  deep  visual-semantic  embedding 
model. proceedings of nips 2013 (pp. 2121-2129). 

[10] lazaridou a, pham nt, baroni m. combining language 

and  vision  with  a  multimodal  skip-gram  model.  arxiv 
preprint arxiv:1501.02598. 2015 jan 12. 

[11] van der maaten, l., & hinton, g. visualizing data using 
id167.  journal  of  machine  learning  research,  (2008) 
9(2579-2605), 85. 

[12]  barkan  o.  bayesian  neural  word  embedding.  arxiv 
preprint arxiv: 1603.06571. 2015. 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

6 

