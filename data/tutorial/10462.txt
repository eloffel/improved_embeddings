simple and accurate id33

using bidirectional lstm feature representations

eliyahu kiperwasser

computer science department

bar-ilan university
ramat-gan, israel

yoav goldberg

computer science department

bar-ilan university
ramat-gan, israel

6
1
0
2

 
l
u
j
 

0
2

 
 
]
l
c
.
s
c
[
 
 

3
v
1
5
3
4
0

.

3
0
6
1
:
v
i
x
r
a

elikip@gmail.com

yoav.goldberg@gmail.com

abstract

we present a simple and effective scheme
for id33 which is based on
bidirectional-lstms (bilstms). each sen-
tence token is associated with a bilstm vec-
tor representing the token in its sentential con-
text, and feature vectors are constructed by
concatenating a few bilstm vectors. the
bilstm is trained jointly with the parser ob-
jective, resulting in very effective feature ex-
tractors for parsing. we demonstrate the ef-
fectiveness of the approach by applying it to
a greedy transition-based parser as well as to
a globally optimized graph-based parser. the
resulting parsers have very simple architec-
tures, and match or surpass the state-of-the-art
accuracies on english and chinese.

introduction

1
the focus of this paper is on feature represen-
tation for id33, using recent tech-
niques from the neural-networks (   deep learning   )
literature. modern approaches to dependency pars-
ing can be broadly categorized into graph-based
and transition-based parsers (k  bler et al., 2009).
graph-based parsers (mcdonald, 2006) treat pars-
ing as a search-based id170 prob-
lem in which the goal is learning a scoring func-
tion over dependency trees such that the correct tree
is scored above all other trees. transition-based
parsers (nivre, 2004; nivre, 2008) treat parsing as
a sequence of actions that produce a parse tree, and
a classi   er is trained to score the possible actions at
each stage of the process and guide the parsing pro-
cess. perhaps the simplest graph-based parsers are

arc-factored (   rst order) models (mcdonald, 2006),
in which the scoring function for a tree decomposes
over the individual arcs of the tree. more elaborate
models look at larger (overlapping) parts, requiring
more sophisticated id136 and training algorithms
(martins et al., 2009; koo and collins, 2010). the
basic transition-based parsers work in a greedy man-
ner, performing a series of locally-optimal decisions,
and boast very fast parsing speeds. more advanced
transition-based parsers introduce some search into
the process using a beam (zhang and clark, 2008)
or id145 (huang and sagae, 2010).
regardless of the details of the parsing frame-
work being used, a crucial step in parser design is
choosing the right feature function for the underly-
ing statistical model. recent work (see section 2.2
for an overview) attempt to alleviate parts of the fea-
ture function design problem by moving from lin-
ear to non-linear models, enabling the modeler to
focus on a small set of    core    features and leav-
ing it up to the machine-learning machinery to come
up with good feature combinations (chen and man-
ning, 2014; pei et al., 2015; lei et al., 2014; taub-
tabib et al., 2015). however, the need to carefully
de   ne a set of core features remains. for exam-
ple, the work of chen and manning (2014) uses 18
different elements in its feature function, while the
work of pei et al. (2015) uses 21 different elements.
other works, notably dyer et al. (2015) and le and
zuidema (2014), propose more sophisticated feature
representations, in which the feature engineering is
replaced with architecture engineering.

in this work, we suggest an approach which is
much simpler in terms of both feature engineering

and architecture engineering. our proposal (section
3) is centered around biid56s (irsoy and cardie,
2014; schuster and paliwal, 1997), and more specif-
ically bilstms (graves, 2008), which are strong
and trainable sequence models (see section 2.3).
the bilstm excels at representing elements in a
sequence (i.e., words) together with their contexts,
capturing the element and an    in   nite    window
around it. we represent each word by its bilstm
encoding, and use a concatenation of a minimal set
of such bilstm encodings as our feature function,
which is then passed to a non-linear scoring function
(multi-layer id88). crucially, the bilstm is
trained with the rest of the parser in order to learn
a good feature representation for the parsing prob-
lem. if we set aside the inherent complexity of the
bilstm itself and treat it as a black box, our pro-
posal results in a pleasingly simple feature extractor.
we demonstrate the effectiveness of the approach
by using the bilstm feature extractor in two pars-
transition-based (section 4) as
ing architectures,
well as a graph-based (section 5).
in the graph-
based parser, we jointly train a structured-prediction
model on top of a bilstm, propagating errors from
the structured objective all the way back to the
bilstm feature-encoder. to the best of our knowl-
edge, we are the    rst to perform such end-to-end
training of a id170 model and a recur-
rent feature extractor for non-sequential outputs.1

aside from the novelty of the bilstm feature
extractor and the end-to-end structured training, we
rely on existing models and techniques from the
parsing and id170 literature. we
stick to the simplest parsers in each category    
greedy id136 for the transition-based architec-
ture, and a    rst-order, arc-factored model for the
graph-based architecture. despite the simplicity
of the parsing architectures and the feature func-
tions, we achieve near state-of-the-art parsing ac-
curacies in both english (93.1 uas) and chinese
(86.6 uas), using a    rst-order parser with two fea-
tures and while training solely on treebank data,
without relying on semi-supervised signals such as
pre-trained id27s (chen and manning,
2014), word-clusters (koo et al., 2008), or tech-

1structured training of sequence tagging models over id56-
based representations was explored by chiu and nichols (2016)
and lample et al. (2016).

niques such as tri-training (weiss et al., 2015).
when also including pre-trained id27s,
we obtain further improvements, with accuracies of
93.9 uas (english) and 87.6 uas (chinese) for a
greedy transition-based parser with 11 features, and
93.6 uas (en) / 87.4 (ch) for a greedy transition-
based parser with 4 features.

2 background and notation

notation we use x1:n to denote a sequence of n
vectors x1,       , xn. f  (  ) is a function parameter-
ized with parameters   . we write fl(  ) as shorthand
for f  l     an instantiation of f with a speci   c set of
parameters   l. we use     to denote a vector con-
catenation operation, and v[i] to denote an indexing
operation taking the ith element of a vector v.

2.1 feature functions in id33

traditionally, state-of-the-art parsers rely on linear
models over hand-crafted feature functions. the fea-
ture functions look at core components (e.g.    word
on top of stack   ,    leftmost child of the second-to-
top word on the stack   ,    distance between the head
and the modi   er words   ), and are comprised of sev-
eral templates, where each template instantiates a bi-
nary indicator function over a conjunction of core
elements (resulting in features of the form    word on
top of stack is x and leftmost child is y and . . .    ).
the design of the feature function     which compo-
nents to consider and which combinations of com-
ponents to include     is a major challenge in parser
design. once a good feature function is proposed
in a paper it is usually adopted in later works, and
sometimes tweaked to improve performance. ex-
amples of good feature functions are the feature-set
proposed by zhang and nivre (2011) for transition-
based parsing (including roughly 20 core compo-
nents and 72 feature templates), and the feature-
set proposed by mcdonald et al. (2005) for graph-
based parsing, with the paper listing 18 templates
for a    rst-order parser, while the    rst order feature-
extractor in the actual implementation   s code (mst-
parser2) includes roughly a hundred feature tem-
plates.

2http://www.seas.upenn.edu/~strctlrn/

mstparser/mstparser.html

the core features in a transition-based parser usu-
ally look at information such as the word-identity
and part-of-speech (pos) tags of a    xed number of
words on top of the stack, a    xed number of words
on the top of the buffer, the modi   ers (usually left-
most and right-most) of items on the stack and on the
buffer, the number of modi   ers of these elements,
parents of words on the stack, and the length of the
spans spanned by the words on the stack. the core
features of a    rst-order graph-based parser usually
take into account the word and pos of the head
and modi   er items, as well as pos-tags of the items
around the head and modi   er, pos tags of items be-
tween the head and modi   er, and the distance and
direction between the head and modi   er.

2.2 related research efforts

coming up with a good feature-set for a parser is a
hard and time consuming task, and many researchers
attempt to reduce the required manual effort. the
work of lei et al. (2014) suggests a low-rank ten-
sor representation to automatically    nd good feature
combinations. taub-tabib et al. (2015) suggest a
kernel-based approach to implicitly consider all pos-
sible feature combinations over sets of core-features.
the recent popularity of neural networks prompted
a move from templates of sparse, binary indicator
features to dense core feature encodings fed into
non-linear classi   ers. chen and manning (2014) en-
code each core feature of a greedy transition-based
parser as a dense low-dimensional vector, and the
vectors are then concatenated and fed into a non-
linear classi   er (multi-layer id88) which can
potentially capture arbitrary feature combinations.
weiss et al. (2015) showed further gains using the
same approach coupled with a somewhat improved
set of core features, a more involved network archi-
tecture with skip-layers, id125-decoding, and
careful hyper-parameter tuning. pei et al. (2015)
apply a similar methodology to graph-based pars-
ing. while the move to neural-network classi-
   ers alleviates the need for hand-crafting feature-
combinations, the need to carefully de   ne a set of
core features remain. for example, the feature rep-
resentation in chen and manning (2014) is a con-
catenation of 18 word vectors, 18 pos vectors and

12 dependency-label vectors.3

the above works tackle the effort in hand-crafting
effective feature combinations. a different line of
work attacks the feature-engineering problem by
suggesting novel neural-network architectures for
encoding the parser state, including intermediately-
built subtrees, as vectors which are then fed to non-
linear classi   ers. titov and henderson encode the
parser state using incremental sigmoid-belief net-
works (2007). in the work of dyer et al. (2015), the
entire stack and buffer of a transition-based parser
are encoded as a stack-lstms, where each stack el-
ement is itself based on a compositional represen-
tation of parse trees. le and zuidema (2014) en-
code each tree node as two compositional represen-
tations capturing the inside and outside structures
around the node, and feed the representations into
a reranker. a similar reranking approach, this time
based on convolutional neural networks, is taken by
zhu et al. (2015). finally, in kiperwasser and gold-
berg (2016) we present an easy-first parser based
on a novel hierarchical-lstm tree encoding.

in contrast to these, the approach we present in
this work results in much simpler feature functions,
without resorting to elaborate network architectures
or compositional tree representations.

work by vinyals et al.

(2015) employs a
sequence-to-sequence with attention architecture for
constituency parsing. each token in the input sen-
tence is encoded in a deep-bilstm representation,
and then the tokens are fed as input to a deep-
lstm that predicts a sequence of bracketing ac-
tions based on the already predicted bracketing as
well as the encoded bilstm vectors. a trainable
attention mechanism is used to guide the parser to
relevant bilstm vectors at each stage. this ar-
chitecture shares with ours the use of bilstm en-
coding and end-to-end training. the sequence of
bracketing actions can be interpreted as a sequence
of shift and reduce operations of a transition-based
parser. however, while the parser of vinyals et al.

3in all of these neural-network based approaches, the vec-
tor representations of words were initialized using pre-trained
word-embeddings derived from a large corpus external to the
training data. this puts the approaches in the semi-supervised
category, making it hard to tease apart the contribution of the au-
tomatic feature-combination component from that of the semi-
supervised component.

relies on a trainable attention mechanism for fo-
cusing on speci   c bilstm vectors, parsers in the
transition-based family we use in section 4 use a hu-
man designed stack and buffer mechanism to manu-
ally direct the parser   s attention. while the effec-
tiveness of the trainable attention approach is im-
pressive, the stack-and-buffer guidance of transition-
based parsers results in more robust learning.
in-
deed, work by cross and huang (2016), published
while working on the camera-ready version of this
paper, show that the same methodology as ours
is highly effective also for greedy, transition-based
constituency parsing, surpassing the beam-based ar-
chitecture of vinyals et al. (88.3f vs. 89.8f points)
when trained on the id32 dataset and with-
out using orthogonal methods such as ensembling
and up-training.

2.3 id182
recurrent neural networks (id56s) are statistical
learners for modeling sequential data. an id56 al-
lows one to model the ith element in the sequence
based on the past     the elements x1:i up to and in-
cluding it. the id56 model provides a framework
for conditioning on the entire history x1:i without
resorting to the markov assumption which is tradi-
tionally used for modeling sequences. id56s were
shown to be capable of learning to count, as well as
to model line lengths and complex phenomena such
as bracketing and code indentation (karpathy et al.,
2015). our proposed feature extractors are based on
a bidirectional recurrent neural network (biid56),
an extension of id56s that take into account both the
past x1:i and the future xi:n. we use a speci   c    avor
of id56 called a long short-term memory network
(lstm). for brevity, we treat id56 as an abstrac-
tion, without getting into the mathematical details of
the implementation of the id56s and lstms. for
further details on id56s and lstms, the reader is
referred to goldberg (2015) and cho (2015).

the recurrent neural network (id56) abstraction
is a parameterized function id56  (x1:n) mapping a
sequence of n input vectors x1:n, xi     rdin to a se-
quence of n output vectors h1:n, hi     rdout. each
output vector hi is conditioned on all the input vec-
tors x1:i, and can be thought of as a summary of the
pre   x x1:i of x1:n. in our notation, we ignore the
intermediate vectors h1:n   1 and take the output of

id56  (x1:n) to be the vector hn.

a bidirectional id56 is composed of two id56s,
id56f and id56r, one reading the sequence in its
regular order, and the other reading it in reverse.
concretely, given a sequence of vectors x1:n and a
desired index i, the function biid56  (x1:n, i) is de-
   ned as:

biid56  (x1:n, i) = id56f (x1:i)     id56r(xn:i)
the vector vi = biid56(x1:n, i) is then a represen-
tation of the ith item in x1:n, taking into account
both the entire history x1:i and the entire future xi:n
by concatenating the matching id56s. we can view
the biid56 encoding of an item i as representing the
item i together with a context of an in   nite window
around it.
computational complexity computing
the
biid56 vectors encoding of the ith element of a
sequence x1:n requires o(n) time for computing
the two id56s and concatenating their outputs.
a naive approach of computing the bidirectional
representation of all n elements result in o(n2)
computation. however,
it is trivial to compute
the biid56 encoding of all sequence items in
linear time by pre-computing id56f (x1:n) and
id56r(xn:1), keeping the intermediate representa-
tions, and concatenating the required elements as
needed.
biid56 training initially, the biid56 encodings
vi do not capture any particular information. during
training, the encoded vectors vi are fed into further
network layers, until at some point a prediction is
made, and a loss is incurred. the back-propagation
algorithm is used to compute the gradients of all the
parameters in the network (including the biid56 pa-
rameters) with respect to the loss, and an optimizer
is used to update the parameters according to the
gradients. the training procedure causes the biid56
function to extract from the input sequence x1:n the
relevant information for the task task at hand.
going deeper we use
a variant of deep
bidirectional
biid56)
(or
composed of k biid56 functions
which is
biid561,       , biid56k that feed into each other: the
output biid56(cid:96)(x1:n, 1), . . . , biid56(cid:96)(x1:n, n) of
biid56(cid:96) becomes the input of biid56(cid:96)+1. stacking

k-layer

id56

biid56s in this way has been empirically shown to
be effective (irsoy and cardie, 2014). in this work,
we use biid56s and deep-biid56s interchangeably,
specifying the number of layers when needed.

historical notes id56s were introduced by el-
man (1990), and extended to biid56s by schus-
ter and paliwal (1997).
the lstm variant of
id56s is due to hochreiter and schmidhuber (1997).
bilstms were recently popularized by graves
(2008), and deep biid56s were introduced to nlp
by irsoy and cardie (2014), who used them for se-
quence tagging. in the context of parsing, lewis et
al. (2016) and vaswani et al. (2016) use a bilstm
sequence tagging model to assign a id35 supertag
for each token in the sentence. lewis et al. (2016)
feeds the resulting supertags sequence into an a*
id35 parser. vaswani et al. (2016) adds an addi-
tional layer of lstm which receives the bilstm
representation together with the k-best supertags
for each word and outputs the most likely supertag
given previous tags, and then feeds the predicted su-
pertags to a discriminitively trained parser. in both
works, the bilstm is trained to produce accurate
id35 supertags, and is not aware of the global pars-
ing objective.

3 our approach

we propose to replace the hand-crafted feature func-
tions in favor of minimally-de   ned feature functions
which make use of automatically learned bidirec-
tional lstm representations.

given n-words input sentence s with words
w1, . . . , wn together with the corresponding pos
tags t1, . . . , tn,4 we associate each word wi and pos
ti with embedding vectors e(wi) and e(ti), and cre-
ate a sequence of input vectors x1:n in which each
xi is a concatenation of the corresponding word and
pos vectors:

xi = e(wi)     e(pi)

the embeddings are trained together with the model.
this encodes each word in isolation, disregarding its
context. we introduce context by representing each

4 in this work the tag sequence is assumed to be given, and
in practice is predicted by an external model. future work will
address relaxing this assumption.

input element as its (deep) bilstm vector, vi:

vi = bilstm(x1:n, i)

our feature function    is then a concatenation of a
small number of bilstm vectors. the exact fea-
ture function is parser dependent and will be dis-
cussed when discussing the corresponding parsers.
the resulting feature vectors are then scored using a
non-linear function, namely a multi-layer id88
with one hidden layer (mlp):

m lp  (x) = w 2    tanh(w 1    x + b1) + b2

where    = {w 1, w 2, b1, b2} are the model parame-
ters.

beside using the bilstm-based feature func-
tions, we make use of standard parsing techniques.
crucially, the bilstm is trained jointly with the rest
of the parsing objective. this allows it to learn rep-
resentations which are suitable for the parsing task.
consider a concatenation of two bilstm vectors
(vi     vj) scored using an mlp. the scoring function
has access to the words and pos-tags of vi and vj, as
well as the words and pos-tags of the words in an
in   nite window surrounding them. as lstms are
known to capture length and sequence position in-
formation, it is very plausible that the scoring func-
tion can be sensitive also to the distance between i
and j, their ordering, and the sequential material be-
tween them.

parsing-time complexity once the bilstm is
trained, parsing is performed by    rst computing the
bilstm encoding vi for each word in the sentence
(a linear time operation).5 then, parsing proceeds as
usual, where the feature extraction involves a con-
catenation of a small number of the pre-computed vi
vectors.

4 transition-based parser

we begin by integrating the feature extractor in a
transition-based parser (nivre, 2008). we follow
the notation in goldberg and nivre (2013). the

5 while the bilstm computation is quite ef   cient as it is,
as demonstrated by lewis et al. (2016), if using a gpu imple-
mentation the bilstm encoding can be ef   ciently performed
over many of sentences in parallel, making its computation cost
almost negligible.

figure 1: illustration of the neural model scheme of the transition-based parser when calculating the scores of the
possible transitions in a given con   guration. the con   guration (stack and buffer) is depicted on the top. each transition
is scored using an mlp that is fed the bilstm encodings of the    rst word in the buffer and the three words at the top
of the stack (the colors of the words correspond to colors of the mlp inputs above), and a transition is picked greedily.
each xi is a concatenation of a word and a pos vector, and possibly an additional external embedding vector for the
word. the    gure depicts a single-layer bilstm, while in practice we use two layers. when parsing a sentence, we
iteratively compute scores for all possible transitions and apply the best scoring action until the    nal con   guration is
reached.

transition-based parsing framework assumes a tran-
sition system, an abstract machine that processes
sentences and produces parse trees. the transition
system has a set of con   gurations and a set of tran-
sitions which are applied to con   gurations. when
parsing a sentence, the system is initialized to an ini-
tial con   guration based on the input sentence, and
transitions are repeatedly applied to this con   gura-
tion. after a    nite number of transitions, the system
arrives at a terminal con   guration, and a parse tree
is read off the terminal con   guration. in a greedy
parser, a classi   er is used to choose the transition
to take in each con   guration, based on features ex-
tracted from the con   guration itself. the parsing al-
gorithm is presented in algorithm 1 below.

given a sentence s, the parser is initialized with
the con   guration c (line 2). then, a feature func-
tion   (c) represents the con   guration c as a vector,
which is fed to a scoring function score assign-
ing scores to (con   guration,transition) pairs. score
scores the possible transitions t, and the highest

algorithm 1 greedy transition-based parsing
1: input: sentence s = w1, . . . , xw, t1, . . . , tn,
parameterized function score  (  ) with param-
eters   .

2: c     initial(s)
3: while not terminal(c) do
4:
5:
6: return tree(c)

  t     arg maxt   legal(c) score  
c       t(c)

(cid:0)  (c), t(cid:1)

scoring transition   t is chosen (line 4). the transition
  t is applied to the con   guration, resulting in a new
parser con   guration. the process ends when reach-
ing a    nal con   guration, from which the resulting
parse tree is read and returned (line 6).

transition systems differ by the way they de   ne
con   gurations, and by the particular set of transi-
tions available to them. a parser is determined by
the choice of a transition system, a feature function

thes2jumpeds1overs0theb0lazyb1dogb2rootb3foxbrowncon   guration:scoring:lstmfxtheconcatlstmfxbrownconcatlstmfxfoxconcatlstmfxjumpedconcatlstmfxoverconcatlstmfxtheconcatlstmfxlazyconcatlstmfxdogconcatlstmfxrootconcatlstmbs0lstmbs1lstmbs2lstmbs3lstmbs4lstmbs5lstmbs6lstmbs7lstmbs8vthevbrownvfoxvjumpedvovervthevlazyvdogvrootmlp(scoreleftarc,scorerightarc,scoreshift)   and a scoring function score. our choices are
detailed below.

(. . .|s2|s1|s0,
is de   ned as:

b0| . . . ,

t ) the feature extractor

the arc-hybrid system many transition systems
exist in the literature. in this work, we use the arc-
hybrid transition system (kuhlmann et al., 2011),
which is similar to the more popular arc-standard
system (nivre, 2004), but for which an ef   cient dy-
namic oracle is available (goldberg and nivre, 2012;
goldberg and nivre, 2013). in the arc-hybrid sys-
tem, a con   guration c = (  ,   , t ) consists of a
stack   , a buffer   , and a set t of dependency arcs.
both the stack and the buffer hold integer indices
pointing to sentence elements. given a sentence
s = w1, . . . , wn, t1, . . . , tn, the system is initial-
ized with an empty stack, an empty arc set, and
   = 1, . . . , n, root , where root is the special root
index. any con   guration c with an empty stack and
a buffer containing only root is terminal, and the
parse tree is given by the arc set tc of c. the arc-
hybrid system allows 3 possible transitions, shift,
left(cid:96) and right(cid:96), de   ned as:
shift[(  , b0|  , t )]
left(cid:96)[(  |s1|s0, b0|  , t )] = (  |s1, b0|  , t     {(b0, s0, (cid:96))})
right(cid:96)[(  |s1|s0,   , t )]

= (  |b0,   , t )
= (  |s1,   , t     {(s1, s0, (cid:96))})

the shift transition moves the    rst item of the
buffer (b0) to the stack. the left(cid:96) transition re-
moves the    rst item on top of the stack (s0) and
attaches it as a modi   er to b0 with label (cid:96), adding
the arc (b0, s0, (cid:96)). the right(cid:96) transition removes
s0 from the stack and attaches it as a modi   er to the
next item on the stack (s1), adding the arc (s1, s0, (cid:96)).

scoring function traditionally, the scoring func-
tion score  (x, t) is a discriminative linear model
of the form scorew (x, t) = (w    x)[t]. the lin-
earity of score required the feature function   (  )
to encode non-linearities in the form of combination
features. we follow chen and manning (2014) and
replace the linear scoring model with an mlp.

score  (x, t) = m lp  (x)[t]

simple feature function the feature function
  (c) is typically complex (see section 2.1). our
feature function is the concatenated bilstm vec-
tors of the top 3 items on the stack and the    rst
item on the buffer.
i.e., for a con   guration c =

  (c) = vs2     vs1     vs0     vb0

vi = bilstm(x1:n, i)

this feature function is rather minimal: it takes
into account the bilstm representations of s1, s0
and b0, which are the items affected by the possible
transitions being scored, as well as one extra stack
context s2.6 figure 1 depicts transition scoring with
our architecture and this feature function. note that,
unlike previous work, this feature function does not
take into account t , the already built structure. the
high parsing accuracies in the experimental sections
suggest that the bilstm encoding is capable of es-
timating a lot of the missing information based on
the provided stack and buffer elements and the se-
quential content between them.

while not explored in this work,

relying on
only four word indices for scoring an action re-
sults in very compact state signatures, making
our proposed feature representation very appeal-
ing for use in transition-based parsers that employ
dynamic-programming search (huang and sagae,
2010; kuhlmann et al., 2011).

extended feature function one of the bene   ts
of the greedy transition-based parsing framework is
precisely its ability to look at arbitrary features from
the already built tree.
if we allow somewhat less
minimal feature function, we could add the bilstm
vectors corresponding to the right-most and left-
most modi   ers of s0, s1 and s2, as well as the left-
most modi   er of b0, reaching a total of 11 bilstm
vectors. we refer to this as the extended feature set.
as we   ll see in section 6, using the extended set
does indeed improve parsing accuracies when using
pre-trained id27s, but has a minimal ef-
fect in the fully-supervised case.7

6an additional buffer context is not needed, as b1 is by def-
inition adjacent to b0, a fact that we expect the bilstm en-
coding of b0 to capture. in contrast, b0, s0, s1 and s2 are not
necessarily adjacent to each other in the original sentence.

7we did not experiment with other feature con   gurations. it
is well possible that not all of the additional 7 child encodings
are needed for the observed accuracy gains, and that a smaller
feature set will yield similar or even better improvements.

4.1 details of the training algorithm
the training objective is to set the score of correct
transitions above the scores of incorrect transitions.
we use a margin-based objective, aiming to maxi-
mize the margin between the highest scoring correct
action and the highest scoring incorrect action. the
hinge loss at each parsing con   guration c is de   ned
as:

(cid:16)

m lp(cid:0)  (c)(cid:1)[to]
m lp(cid:0)  (c)(cid:1)[tp]

(cid:17)

max

0, 1    max
to   g

+ max
tp   a\g

where a is the set of possible transitions and g
is the set of correct (gold) transitions at the cur-
rent stage. at each stage of the training process
the parser scores the possible transitions a, incurs
a loss, selects a transition to follow, and moves to
the next con   guration based on it. the local losses
are summed throughout the parsing process of a sen-
tence, and the parameters are updated with respect
to the sum of the losses at sentence boundaries.8
the gradients of the entire network (including the
mlp and the bilstm) with respect to the sum of
the losses are calculated using the id26
algorithm. as usual, we perform several training it-
erations over the training corpus, shuf   ing the order
of sentences in each iteration.

error-exploration and dynamic oracle training
we follow goldberg and nivre (2013);goldberg and
nivre (2012) in using error exploration training with
a dynamic-oracle, which we brie   y describe below.
at each stage in the training process, the parser
assigns scores to all the possible transitions t     a. it
then selects a transition, applies it, and moves to the
next step. which transition should be followed? a
common approach follows the highest scoring tran-
sition that can lead to the gold tree. however, when
training in this way the parser sees only con   gura-
tions that result from following correct actions, and
as a result tends to suffer from error propagation at

8to increase gradient stability and training speed, we simu-
late mini-batch updates by only updating the parameters when
the sum of local losses contains at least 50 non-zero elements.
sums of fewer elements are carried across sentences. this as-
sures us a suf   cient number of gradient samples for every up-
date thus minimizing the effect of gradient instability.

test time. instead, in error-exploration training the
parser follows the highest scoring action in a dur-
ing training even if this action is incorrect, exposing
it to con   gurations that result from erroneous deci-
sions. this strategy requires de   ning the set g such
that the correct actions to take are well-de   ned also
for states that cannot lead to the gold tree. such
a set g is called a dynamic oracle. we perform
error-exploration training using the dynamic-oracle
de   ned by goldberg and nivre (2013).
aggressive exploration we found that even when
using error-exploration, after one iteration the model
remembers the training set quite well, and does not
make enough errors to make error-exploration effec-
tive. in order to expose the parser to more errors,
we follow an aggressive-exploration scheme: we
sometimes follow incorrect transitions also if they
score below correct transitions. speci   cally, when
the score of the correct transition is greater than that
of the wrong transition but the difference is smaller
than a margin constant, we chose to follow the incor-
rect action with id203 pagg (we use pagg = 0.1
in our experiments).
summary the greedy transition-based parser
follows standard techniques from the literature
(margin-based objective, dynamic oracle training,
error exploration, mlp-based non-linear scoring
function). we depart from the literature by re-
placing the hand-crafted feature function over care-
fully selected components of the con   guration with
a concatenation of bilstm representations of a few
prominent items on the stack and the buffer, and
training the bilstm encoder jointly with the rest
of the network.

5 graph-based parser

graph-based parsing follows the common structured
prediction paradigm (taskar et al., 2005; mcdonald
et al., 2005):

predict(s) = arg max
y   y(s)

scoreglobal(s, y)

scoreglobal(s, y) =

scorelocal(s, part)

(cid:88)

part   y

given an input sentence s (and the corresponding
sequence of vectors x1:n) we look for the highest-

figure 2: illustration of the neural model scheme of the graph-based parser when calculating the score of a given parse
tree. the parse tree is depicted below the sentence. each dependency arc in the sentence is scored using an mlp that
is fed the bilstm encoding of the words at the arc   s end points (the colors of the arcs correspond to colors of the
mlp inputs above), and the individual arc scores are summed to produce the    nal score. all the mlps share the same
parameters. the    gure depicts a single-layer bilstm, while in practice we use two layers. when parsing a sentence,
we compute scores for all possible n2 arcs, and    nd the best scoring tree using a dynamic-programming algorithm.

scoring parse tree y in the space y(s) of valid de-
pendency trees over s. in order to make the search
tractable, the scoring function is decomposed to the
sum of local scores for each part independently.

in this work, we focus on arc-factored graph
based approach presented in mcdonald et al. (2005).
arc-factored parsing decomposes the score of a tree
to the sum of the score of its head-modi   er arcs
(h, m):

(cid:88)

score(cid:0)  (s, h, m)(cid:1)

parse(s) = arg max
y   y(s)

(h,m)   y

given the scores of the arcs the highest scoring pro-
jective tree can be ef   ciently found using eisner   s
decoding algorithm (1996). mcdonald et al. and
most subsequent work estimate the local score of an
arc by a linear model parameterized by a weight vec-
tor w, and a feature function   (s, h, m) assigning a
sparse feature vector for an arc linking modi   er m
to head h. we follow pei et al. (2015) and replace
the linear scoring function with an mlp.

the feature extractor   (s, h, m) is usually com-
plex, involving many elements (see section 2.1).
in contrast, our feature extractor uses merely the

bilstm encoding of the head word and the mod-
i   er word:

  (s, h, m) = biid56(x1:n, h)     biid56(x1:n, m)

the    nal model is:

parse(s) = arg max
y   y(s)

= arg max
y   y(s)

scoreglobal(s, y)

score(cid:0)  (s, h, m)(cid:1)

(cid:88)
(cid:88)

(h,m)   y

m lp (vh     vm)

= arg max
y   y(s)

(h,m)   y
vi = biid56(x1:n, i)

the architecture is illustrated in figure 2.

training the training objective is to set the score
function such that correct tree y is scored above in-
correct ones. we use a margin-based objective (mc-
donald et al., 2005; lecun et al., 2006), aiming to
maximize the margin between the score of the gold
tree y and the highest scoring incorrect tree y(cid:48). we
de   ne a hinge loss with respect to a gold tree y as:

lstmfxtheconcatlstmfxbrownconcatlstmfxfoxconcatlstmfxjumpedconcatlstmfx   concatlstmbs0lstmbs1lstmbs2lstmbs3lstmbs4vthevbrownvfoxvjumpedv   mlpmlpmlpmlp+(cid:16)

max

0, 1     max
y(cid:48)(cid:54)=y

+

(cid:88)
(cid:88)

(h,m)   y(cid:48)

(h,m)   y

m lp (vh     vm)

m lp (vh     vm)

(cid:17)

each of the tree scores is then calculated by acti-
vating the mlp on the arc representations. the en-
tire loss can viewed as the sum of multiple neural
networks, which is sub-differentiable. we calculate
the gradients of the entire network (including to the
bilstm encoder and id27s).
labeled parsing up to now, we described unla-
beled parsing. a possible approach for adding la-
bels is to score the combination of an unlabeled arc
(h, m) and its label (cid:96) by considering the label as part
of the arc (h, m, (cid:96)). this results in |labels|  |arcs|
parts that need to be scored, leading to slow parsing
speeds and arguably a harder learning problem.

instead, we chose to    rst predict the unlabeled
structure using the model given above, and then pre-
dict the label of each resulting arc. using this ap-
proach, the number of parts stays small, enabling
fast parsing.

the labeling of an arc (h, m) is performed using
the same feature representation   (s, h, m) fed into a
different mlp predictor:

label(h, m) = arg max
(cid:96)   labels

m lplbl(vh     vm)[(cid:96)]

as before we use a margin based hinge loss. the la-
beler is trained on the gold trees.9 the bilstm en-
coder responsible for producing vh and vm is shared
with the arc-factored parser: the same bilstm en-
coder is used in the parer and the labeler. this
sharing of parameters can be seen as an instance of
id72 (caruana, 1997). as we show
in section 6, the sharing is effective:
training the
bilstm feature encoder to be good at predicting
arc-labels signi   cantly improves the parser   s unla-
beled accuracy.
loss augmented id136
in initial experiments,
the network learned quickly and over   t the data. in

9when training the labeled parser, we calculate the structure
loss and the labeling loss for each training sentence, and sum
the losses prior to computing the gradients.

order to remedy this, we found it useful to use loss
augmented id136 (taskar et al., 2005). the in-
tuition behind loss augmented id136 is to update
against trees which have high model scores and are
also very wrong. this is done by augmenting the
score of each part not belonging to the gold tree by
adding a constant to its score. formally, the loss
transforms as follows:

(cid:88)

max(0, 1 + score(x, y)   
max
y(cid:48)(cid:54)=y

part   y(cid:48)

(scorelocal(x, part) + 1part(cid:54)   y))

speed improvements the arc-factored model re-
quires the scoring of n2 arcs. scoring is performed
using an mlp with one hidden layer, resulting in n2
matrix-vector multiplications from the input to the
hidden layer, and n2 multiplications from the hid-
den to the output layer. the    rst n2 multiplications
involve larger dimensional input and output vectors,
and are the most time consuming. fortunately, these
can be reduced to 2n multiplications and n2 vec-
tor additions, by observing that the multiplication
w    (vh     vm) can be written as w 1    vh + w 2    vm
where w 1 and w 1 are are the    rst and second half
of the matrix w and reusing the products across dif-
ferent pairs.
summary the graph-based parser
is straight-
forward    rst-order parser, trained with a margin-
based hinge-loss and loss-augmented id136. we
depart from the literature by replacing the hand-
crafted feature function with a concatenation of
bilstm representations of the head and modi   er
words, and training the bilstm encoder jointly
with the structured objective. we also introduce a
novel id72 approach for labeled pars-
ing by training a second-stage arc-labeler sharing the
same bilstm encoder with the unlabeled parser.

6 experiments and results

we evaluated our parsing model on english and chi-
nese data. for comparison purposes we follow the
setup of dyer et al. (2015).

data for english, we used the stanford depen-
dency (sd) (de marneffe and manning, 2008) con-
version of the id32 (marcus et al., 1993),
using the standard train/dev/test splits with the

system

method

representation

emb

ptb-ym

ptb-sd

ctb

this work
this work
this work
zhangnivre11
martins13 (turboparser)
pei15
dyer15
ballesteros16
this work
this work
this work
weiss15
weiss15
pei15
dyer15
ballesteros16
lezuidema14
zhu15

   
   
   
   
   
   
   
   

graph, 1st order

transition (greedy, dyn-oracle)
transition (greedy, dyn-oracle)

2 bilstm vectors
4 bilstm vectors
11 bilstm vectors

transition (beam)
graph, 3rd order+
graph, 2nd order
transition (greedy)

transition (greedy, dyn-oracle)

graph, 1st order

transition (greedy, dyn-oracle)
transition (greedy, dyn-oracle)

transition (greedy)
transition (beam)
graph, 2nd order
transition (greedy)

transition (greedy, dyn-oracle)

reranking /blend
reranking /blend

large feature set (sparse)
large feature set (sparse)
large feature set (dense)

stack-lstm + composition
stack-lstm + composition

2 bilstm vectors
4 bilstm vectors
11 bilstm vectors

large feature set (dense)
large feature set (dense)
large feature set (dense)

yes
yes
yes
yes
yes
yes
stack-lstm + composition yes
stack-lstm + composition yes
inside-outside recursive net yes
yes

recursive conv-net

uas

   
   
   

92.9
92.8
93.0

   
   
   
   
   
   
   

93.3

   
   

93.1
93.8

uas las uas las
85.1
93.1
85.0
93.1
93.2
84.9
   
84.4

86.6
86.2
86.5
86.0

91.0
91.0
91.2
   
   
   

90.0
90.6
90.9
91.5
91.9
91.2
92.0
   

90.9
91.4
91.5

   

93.1

   

92.4
92.7
93.0
93.6
93.9
93.2
94.0
   

93.1
93.6
93.8

   

   
   

85.7
86.1
86.5
87.4
87.6
   
   
   

87.1
87.6
   

85.7

   
   

84.1
84.5
84.9
85.9
86.1

   
   
   

85.5
86.2
   
   

table 1: test-set parsing results of various state-of-the-art parsing systems on the english (ptb) and chinese (ctb) datasets. the
systems that use embeddings may use different pre-trained embeddings. english results use predicted pos tags (different systems
use different taggers), while chinese results use gold pos tags. ptb-ym: english ptb, yamada and matsumoto head rules.
ptb-sd: english ptb, stanford dependencies (different systems may use different versions of the stanford converter). ctb:
chinese treebank. reranking /blend in method column indicates a reranking system where the reranker score is interpolated with
the base-parser   s score. the different systems and the numbers reported from them are taken from: zhangnivre11: (zhang and
nivre, 2011); martins13: (martins et al., 2013); weiss15 (weiss et al., 2015); pei15: (pei et al., 2015); dyer15 (dyer et al., 2015);
ballesteros16 (ballesteros et al., 2016); lezuidema14 (le and zuidema, 2014); zhu15: (zhu et al., 2015).

same predicted pos-tags as used in dyer et al.
(2015);chen and manning (2014). this dataset con-
tains a few non-projective trees. punctuation sym-
bols are excluded from the evaluation.

for chinese, we use the penn chinese treebank
5.1 (ctb5), using the train/test/dev splits of (zhang
and clark, 2008; dyer et al., 2015) with gold part-
of-speech tags, also following (dyer et al., 2015;
chen and manning, 2014).

when using external id27s, we also

use the same data as dyer et al. (2015).10
implementation details the parsers are imple-
mented in python, using the pyid98 toolkit11 for
neural network training. the code is available at
the github repository https://github.com/
elikip/bist-parser. we use the lstm vari-
ant implemented in pyid98, and optimize using the
adam optimizer (kingma and ba, 2015). unless
otherwise noted, we use the default values provided
by pyid98 (e.g. for random initialization, learning
rates etc).

10we thank dyer et al. for sharing their data with us.
11https://github.com/clab/id98/tree/

master/pyid98

the word and pos embeddings e(wi) and e(pi)
are initialized to random values and trained together
with the rest of the parsers    networks. in some ex-
periments, we introduce also pre-trained word em-
beddings.
in those cases, the vector representa-
tion of a word is a concatenation of its randomly-
initialized vector embedding with its pre-trained
word vector. both are tuned during training. we
use the same word vectors as in dyer et al. (2015)

during training, we employ a variant of word
dropout (iyyer et al., 2015), and replace a word with
the unknown-word symbol with id203 that is
inversely proportional to the frequency of the word.
a word w appearing #(w) times in the training cor-
pus is replaced with the unknown symbol with prob-
ability punk(w) =
#(w)+  . if a word was dropped
the external embedding of the word is also dropped
with id203 0.5.

  

we train the parsers for up to 30 iterations, and
choose the best model according to the uas accu-
racy on the development set.

hyperparameter tuning we performed a very
minimal hyper-parameter search with the graph-

based parser, and use the same hyper-parameters for
both parsers. the hyper-parameters of the    nal net-
works used for all the reported experiments are de-
tailed in table 2.

sults, with the extended feature set yielding the best
reported results for chinese, and ranked second for
english, after the heavily-tuned beam-based parser
of weiss et al. (2015).

id27 dimension
pos tag embedding dimension

hidden units in m lp

hidden units in m lplbl

bi-lstm layers

100
25
100
100
2

bi-lstm dimensions (hidden/output)

125 / 125

   (for word dropout)

pagg (for exploration training)

0.25
0.1

table 2: hyper-parameter values used in experiments

main results table 1 lists the test-set accuracies of
our best parsing models, compared to other state-of-
the-art parsers from the literature.12

it is clear that our parsers are very competitive,
despite using very simple parsing architectures and
minimal feature extractors. when not using external
embeddings, the    rst-order graph-based parser with
2 features outperforms all other systems that are not
using external resources, including the third-order
turboparser. the greedy transition based parser
with 4 features also matches or outperforms most
other parsers, including the beam-based transition
parser with heavily engineered features of zhang
and nivre (2011) and the stack-lstm parser of
dyer et al. (2015), as well as the same parser when
trained using a dynamic oracle (ballesteros et al.,
2016). moving from the simple (4 features) to the
extended (11 features) feature set leads to some
gains in accuracy for both english and chinese.

interestingly, when adding external word embed-
dings the accuracy of the graph-based parser de-
grades. we are not sure why this happens, and leave
the exploration of effective semi-supervised parsing
with the graph-based model for future work. the
greedy parser does manage to bene   t from the ex-
ternal embeddings, and using them we also see gains
from moving from the simple to the extended feature
set. both feature sets result in very competitive re-

12unfortunately, many papers still report english parsing
results on the de   cient yamada and matsumoto head rules
(ptb-ym) rather than the more modern stanford-dependencies
(ptb-sd). we note that the ptb-ym and ptb-sd results are
not strictly comparable, and in our experience the ptb-ym re-
sults are usually about half a uas point higher.

additional results we perform some ablation ex-
periments in order to quantify the effect of the dif-
ferent components on our best models (table 3).

ptb

ctb

graph (no ext. emb)
   pos
   arclabeler
   loss aug.
greedy (ext. emb)
   pos
   dynoracle

91.0
89.8

uas las uas las
85.4
93.3
92.9
76.8
92.7
81.3
93.8
93.4
93.5

87.0
80.6
86.2
52.6
87.8
83.4
87.5

79.4
91.5
91.2
91.4

51.7
86.0
81.6
85.9

   

   

table 3: ablation experiments results (dev set) for the graph-
based parser without external embeddings and the greedy parser
with external embeddings and extended feature set.

loss augmented id136 is crucial for the success
of the graph-based parser, and the multi-task learn-
ing scheme for the arc-labeler contributes nicely
to the unlabeled scores. dynamic oracle training
yields nice gains for both english and chinese.

7 conclusion

we presented a pleasingly effective approach for
feature extraction for id33 based on
a bilstm encoder that is trained jointly with the
parser, and demonstrated its effectiveness by inte-
grating it into two simple parsing models: a greedy
transition-based parser and a globally optimized
   rst-order graph-based parser, yielding very com-
petitive parsing accuracies in both cases.

acknowledgements this research is supported by
the intel collaborative research institute for com-
putational intelligence (icri-ci) and the israeli sci-
ence foundation (grant number 1555/15). we thank
lillian lee for her important feedback and efforts
invested in editing this paper. we also thank the re-
viewers for their valuable comments.

references
miguel ballesteros, yoav goldberg, chris dyer, and
training with explo-

noah a. smith.

2016.

ration improves a greedy stack-lstm parser. corr,
abs/1603.03793.

rich caruana.

1997. multitask learning. machine

learning, 28:41   75, july.

danqi chen and christopher manning. 2014. a fast and
accurate dependency parser using neural networks.
in proceedings of the 2014 conference on empirical
methods in natural language processing (emnlp),
pages 740   750, doha, qatar, october. association for
computational linguistics.

jason p.c. chiu and eric nichols. 2016. named entity
recognition with bidirectional lstm-id98s. transac-
tions of the association for computational linguistics,
4. to appear.

kyunghyun cho.

2015.

natural

standing with distributed representation.
abs/1511.07916.

language under-
corr,

james cross and liang huang. 2016. incremental pars-
ing with minimal features using bi-directional lstm.
in proceedings of the 54th annual meeting of the as-
sociation for computational linguistics, berlin, ger-
many, august. association for computational lin-
guistics.

marie-catherine de marneffe and christopher d. man-
ning. 2008. stanford dependencies manual. techni-
cal report, stanford university.

chris dyer, miguel ballesteros, wang ling, austin
matthews, and noah a. smith. 2015. transition-
based id33 with stack long short-term
in proceedings of the 53rd annual meet-
memory.
ing of the association for computational linguistics
and the 7th international joint conference on natural
language processing (volume 1: long papers), pages
334   343, beijing, china, july. association for com-
putational linguistics.

jason eisner. 1996. three new probabilistic models for
id33: an exploration. in 16th interna-
tional conference on computational linguistics, pro-
ceedings of the conference, coling 1996, center for
sprogteknologi, copenhagen, denmark, august 5-9,
1996, pages 340   345.

jeffrey l. elman. 1990. finding structure in time. cog-

nitive science, 14(2):179   211.

yoav goldberg and joakim nivre. 2012. a dynamic ora-
cle for arc-eager id33. in proceedings
of coling 2012, pages 959   976, mumbai, india, de-
cember. the coling 2012 organizing committee.

yoav goldberg and joakim nivre.

2013. training
deterministic parsers with non-deterministic oracles.
transactions of the association for computational
linguistics, 1:403   414.
2015.

a primer on neural net-
work models for natural language processing. corr,
abs/1510.00726.

yoav goldberg.

alex graves. 2008. supervised sequence labelling with
recurrent neural networks. ph.d. thesis, technical
university munich.

sepp hochreiter and j  rgen schmidhuber. 1997. long
short-term memory. neural computation, 9(8):1735   
1780.

liang huang and kenji sagae. 2010. dynamic pro-
gramming for linear-time incremental parsing. in pro-
ceedings of the 48th annual meeting of the associa-
tion for computational linguistics, pages 1077   1086,
uppsala, sweden, july. association for computational
linguistics.

ozan irsoy and claire cardie. 2014. opinion mining
with deep recurrent neural networks. in proceedings
of the 2014 conference on empirical methods in nat-
ural language processing (emnlp), pages 720   728,
doha, qatar, october. association for computational
linguistics.

mohit iyyer, varun manjunatha, jordan boyd-graber,
and hal daum   iii. 2015. deep unordered composi-
tion rivals syntactic methods for text classi   cation. in
proceedings of the 53rd annual meeting of the associ-
ation for computational linguistics and the 7th inter-
national joint conference on natural language pro-
cessing (volume 1: long papers), pages 1681   1691,
beijing, china, july. association for computational
linguistics.

andrej karpathy, justin johnson, and fei-fei li. 2015.
visualizing and understanding recurrent networks.
corr, abs/1506.02078.

diederik p. kingma and jimmy ba. 2015. adam: a
method for stochastic optimization. in proceedings of
the 3rd international conference for learning repre-
sentations, san diego, california.

eliyahu kiperwasser and yoav goldberg.

2016.
easy-   rst id33 with hierarchical tree
lstms. transactions of the association for compu-
tational linguistics, 4. to appear.

terry koo and michael collins. 2010. ef   cient third-
order dependency parsers. in proceedings of the 48th
annual meeting of the association for computational
linguistics, pages 1   11, uppsala, sweden, july. asso-
ciation for computational linguistics.

terry koo, xavier carreras, and michael collins. 2008.
simple semi-supervised id33. in pro-
ceedings of the 46th annual meeting of the associ-
ation for computational linguistics, pages 595   603,
columbus, ohio, june. association for computational
linguistics.

sandra k  bler, ryan t. mcdonald, and joakim nivre.
2009. id33. synthesis lectures on
human language technologies. morgan & claypool
publishers.

marco kuhlmann, carlos g  mez-rodr  guez, and gior-
gio satta. 2011. id145 algorithms
for transition-based dependency parsers. in proceed-
ings of the 49th annual meeting of the association for
computational linguistics: human language tech-
nologies, pages 673   682, portland, oregon, usa,
june. association for computational linguistics.

guillaume lample, miguel ballesteros, sandeep subra-
manian, kazuya kawakami, and chris dyer. 2016.
neural architectures for id39. in
proceedings of the 2016 conference of the north
american chapter of the association for computa-
tional linguistics: human language technologies,
pages 260   270, san diego, california, june. associ-
ation for computational linguistics.

phong le and willem zuidema.

2014. the inside-
outside id56 model for depen-
dency parsing. in proceedings of the 2014 conference
on empirical methods in natural language process-
ing (emnlp), pages 729   739, doha, qatar, october.
association for computational linguistics.

yann lecun, sumit chopra, raia hadsell, marc   aurelio
ranzato, and fu jie huang.
2006. a tutorial on
energy-based learning. predicting structured data, 1.
tao lei, yu xin, yuan zhang, regina barzilay, and
tommi jaakkola. 2014. low-rank tensors for scor-
in proceedings of the
ing dependency structures.
52nd annual meeting of the association for compu-
tational linguistics (volume 1: long papers), pages
1381   1391, baltimore, maryland, june. association
for computational linguistics.

mike lewis, kenton lee, and luke zettlemoyer. 2016.
lstm id35 parsing. in proceedings of the 2016 con-
ference of the north american chapter of the associa-
tion for computational linguistics: human language
technologies, pages 221   231, san diego, california,
june. association for computational linguistics.

mitchell p. marcus, beatrice santorini, and mary ann
marcinkiewicz. 1993. building a large annotated cor-
pus of english: the id32. computational
linguistics, 19(2):313   330.

andre martins, noah a. smith, and eric xing. 2009.
concise integer id135 formulations for
id33. in proceedings of the joint con-
ference of the 47th annual meeting of the acl and
the 4th international joint conference on natural lan-
guage processing of the afnlp, pages 342   350, sun-
tec, singapore, august. association for computational
linguistics.

andre martins, miguel almeida, and noah a. smith.
2013. turning on the turbo: fast third-order non-
in proceedings of the 51st
projective turbo parsers.
annual meeting of the association for computational
linguistics (volume 2: short papers), pages 617   622,

so   a, bulgaria, august. association for computa-
tional linguistics.

ryan mcdonald, koby crammer, and fernando pereira.
2005. online large-margin training of dependency
in proceedings of the 43rd annual meet-
parsers.
ing of the association for computational linguistics
(acl   05), pages 91   98, ann arbor, michigan, june.
association for computational linguistics.

ryan mcdonald. 2006. discriminative training and
spanning tree algorithms for id33.
ph.d. thesis, university of pennsylvania.

joakim nivre. 2004. incrementality in deterministic de-
in frank keller, stephen clark,
pendency parsing.
matthew crocker, and mark steedman, editors, pro-
ceedings of the acl workshop incremental parsing:
bringing engineering and cognition together, pages
50   57, barcelona, spain, july. association for com-
putational linguistics.

joakim nivre. 2008. algorithms for deterministic incre-
mental id33. computational linguis-
tics, 34(4):513   553.

wenzhe pei, tao ge, and baobao chang. 2015. an ef-
fective neural network model for graph-based depen-
in proceedings of the 53rd annual
dency parsing.
meeting of the association for computational linguis-
tics and the 7th international joint conference on nat-
ural language processing (volume 1: long papers),
pages 313   322, beijing, china, july. association for
computational linguistics.

mike schuster and kuldip k. paliwal. 1997. bidirec-
tional recurrent neural networks. ieee trans. signal
processing, 45(11):2673   2681.

benjamin taskar, vassil chatalbashev, daphne koller,
and carlos guestrin. 2005. learning structured pre-
diction models: a large margin approach. in machine
learning, proceedings of the twenty-second interna-
tional conference (icml 2005), bonn, germany, au-
gust 7-11, 2005, pages 896   903.

hillel taub-tabib, yoav goldberg, and amir glober-
son. 2015. template kernels for dependency pars-
in proceedings of the 2015 conference of the
ing.
north american chapter of the association for com-
putational linguistics: human language technolo-
gies, pages 1422   1427, denver, colorado, may   june.
association for computational linguistics.

ivan titov and james henderson. 2007. a latent variable
model for generative id33. in proceed-
ings of the tenth international conference on parsing
technologies, pages 144   155, prague, czech repub-
lic, june. association for computational linguistics.
ashish vaswani, yonatan bisk, kenji sagae, and ryan
in pro-
musa. 2016. id55 with lstms.
ceedings of the 15th annual conference of the north

american chapter of the association for computa-
tional linguistics (short papers), san diego, califor-
nia, june.

oriol vinyals, lukasz kaiser, terry koo, slav petrov,
ilya sutskever, and geoffrey e. hinton. 2015. gram-
mar as a foreign language. in advances in neural in-
formation processing systems 28: annual conference
on neural information processing systems 2015, de-
cember 7-12, 2015, montreal, quebec, canada, pages
2773   2781.

david weiss, chris alberti, michael collins, and slav
petrov. 2015. structured training for neural network
in proceedings of the 53rd
transition-based parsing.
annual meeting of the association for computational
linguistics and the 7th international joint conference
on natural language processing (volume 1: long pa-
pers), pages 323   333, beijing, china, july. associa-
tion for computational linguistics.

yue zhang and stephen clark. 2008. a tale of two
parsers: investigating and combining graph-based and
transition-based id33. in proceedings
of the 2008 conference on empirical methods in nat-
ural language processing, pages 562   571, honolulu,
hawaii, october. association for computational lin-
guistics.

yue zhang and joakim nivre. 2011. transition-based
id33 with rich non-local features.
in
proceedings of the 49th annual meeting of the asso-
ciation for computational linguistics: human lan-
guage technologies, pages 188   193, portland, ore-
gon, usa, june. association for computational lin-
guistics.

chenxi zhu, xipeng qiu, xinchi chen, and xuanjing
huang. 2015. a re-ranking model for dependency
parser with recursive convolutional neural network. in
proceedings of the 53rd annual meeting of the associ-
ation for computational linguistics and the 7th inter-
national joint conference on natural language pro-
cessing (volume 1: long papers), pages 1159   1168,
beijing, china, july. association for computational
linguistics.

