6
1
0
2

 

n
u
j
 

8

 
 
]
l
c
.
s
c
[
 
 

2
v
2
4
0
6
0

.

3
0
6
1
:
v
i
x
r
a

globally normalized transition-based neural networks

daniel andor, chris alberti, david weiss, aliaksei severyn,

alessandro presta, kuzman ganchev, slav petrov and michael collins   

google inc

new york, ny

{andor,chrisalberti,djweiss,severyn,apresta,kuzman,slav,mjcollins}@google.com

abstract

achieves

state-of-the-art

we introduce
a globally normalized
transition-based neural network model
that
part-of-
speech tagging, id33 and
sentence compression results. our model
is a simple feed-forward neural network
that operates on a task-speci   c transition
system, yet achieves comparable or better
accuracies than recurrent models. we dis-
cuss the importance of global as opposed
to local id172: a key insight is
that the label bias problem implies that
globally normalized models can be strictly
more expressive than locally normalized
models.

1 introduction

of

have

natural

network

language

approaches

taken
neural
processing
the    eld
(nlp) by storm.
in particular, variants of
long short-term memory (lstm) networks
have
(hochreiter and schmidhuber, 1997)
produced impressive results on some of
the
part-of-speech
such
classic nlp tasks
as
tagging (ling et al., 2015),
syntactic parsing
(vinyals et al., 2015) and id14
(zhou and xu, 2015). one might speculate that
it is the recurrent nature of these models that
enables these results.

in this work we demonstrate that simple
feed-forward networks without any recurrence
can achieve comparable or better accuracies
than lstms, as long as they are globally nor-
malized.
in
section 2, uses a transition system (nivre, 2006)
by
and feature

our model, described in detail

embeddings

introduced

as

    on leave from columbia university.

chen and manning (2014). we do not use any re-
currence, but perform id125 for maintaining
multiple hypotheses and introduce global normal-
ization with a conditional random    eld (crf) ob-
jective (bottou et al., 1997; le cun et al., 1998;
lafferty et al., 2001; collobert et al., 2011)
to
overcome the label bias problem that locally nor-
malized models suffer from. since we use beam
id136, we approximate the partition function
by summing over the elements in the beam,
and use early updates (collins and roark, 2004;
zhou et al., 2015). we compute gradients based
on this approximate global id172 and
perform full id26 training of all neural
network parameters based on the crf loss.

in section 3 we revisit the label bias problem
and the implication that globally normalized mod-
els are strictly more expressive than locally nor-
malized models. lookahead features can par-
tially mitigate this discrepancy, but cannot fully
compensate for it   a point to which we return
later. to empirically demonstrate the effective-
ness of global id172, we evaluate our
model on part-of-speech tagging, syntactic de-
pendency parsing and sentence compression (sec-
tion 4). our model achieves state-of-the-art ac-
curacy on all of these tasks, matching or outper-
forming lstms while being signi   cantly faster.
in particular for id33 on the wall
street journal we achieve the best-ever published
unlabeled attachment score of 94.61%.

as discussed in more detail

in section 5,
we also outperform previous structured training
approaches used for neural network transition-
based parsing.
our ablation experiments
show that we outperform weiss et al. (2015) and
alberti et al. (2015) because we do global back-
propagation training of all model parameters,
while they    x the neural network parameters when
training the global part of their model. we

also outperform zhou et al. (2015) despite using a
smaller beam. to shed additional light on the la-
bel bias problem in practice, we provide a sentence
compression example where the local model com-
pletely fails. we then demonstrate that a globally
normalized parsing model without any lookahead
features is almost as accurate as our best model,
while a locally normalized model loses more than
10% absolute in accuracy because it cannot effec-
tively incorporate evidence as it becomes avail-
able.

finally, we provide an open-source implemen-
tation of our method, called syntaxnet,1 which
we have integrated into the popular tensorflow2
framework. we also provide a pre-trained,
state-of-the art english dependency parser called
   parsey mcparseface,    which we tuned for a bal-
ance of speed, simplicity, and accuracy.

2 model

at its core, our model is an incremental transition-
based parser (nivre, 2006). to apply it to different
tasks we only need to adjust the transition system
and the input features.

2.1 transition system

given an input x, most often a sentence, we de   ne:

    a set of states s(x).
    a special start state s        s(x).
    a set of allowed decisions a(s, x) for all s    

s(x).

    a transition function t(s, d, x) returning a

new state s    for any decision d     a(s, x).

we will use a function   (s, d, x;   ) to compute the
score of decision d in state s for input x. the
vector    contains the model parameters and we
assume that   (s, d, x;   ) is differentiable with re-
spect to   .

in this section, for brevity, we will drop the de-
pendence of x in the functions given above, simply
writing s, a(s), t(s, d), and   (s, d;   ).

throughout this work we will use transition sys-
tems in which all complete structures for the same
input x have the same number of decisions n(x)
(or n for brevity). in id33 for ex-
ample, this is true for both the arc-standard and
arc-eager transition systems (nivre, 2006), where
for a sentence x of length m, the number of deci-

1http://github.com/tensor   ow/models/tree/master/syntaxnet
2http://www.tensor   ow.org

sions for any complete parse is n(x) = 2    m.3
a complete structure is then a sequence of deci-
sion/state pairs (s1, d1) . . . (sn, dn) such that s1 =
s   , di     s(si) for i = 1 . . . n, and si+1 =
t(si, di). we use the notation d1:j to refer to a de-
cision sequence d1 . . . dj.

we assume that there is a one-to-one mapping
between decision sequences d1:j   1 and states sj:
that is, we essentially assume that a state encodes
the entire history of decisions. thus, each state
can be reached by a unique decision sequence
from s   .4 we will use decision sequences d1:j   1
and states interchangeably:
in a slight abuse of
notation, we de   ne   (d1:j   1, d;   ) to be equal to
  (s, d;   ) where s is the state reached by the deci-
sion sequence d1:j   1.

the scoring function   (s, d;   ) can be de   ned
in a number of ways.
in this work, following
chen and manning (2014), weiss et al. (2015),
and zhou et al. (2015), we de   ne it via a feed-
forward neural network as

  (s, d;   ) =   (s;   (l))      (d).

here   (l) are the parameters of the neural network,
excluding the parameters at the    nal layer.   (d) are
the    nal layer parameters for decision d.   (s;   (l))
is the representation for state s computed by the
neural network under parameters   (l). note that
the score is linear in the parameters   (d). we next
describe how softmax-style id172 can be
performed at the local or global level.

2.2 global vs. local id172

in the chen and manning (2014) style of greedy
neural network parsing, the conditional probabil-
ity distribution over decisions dj given context
d1:j   1 is de   ned as

p(dj|d1:j   1;   ) =

exp   (d1:j   1, dj;   )

zl(d1:j   1;   )

,

(1)

where

zl(d1:j   1;   ) = x

exp   (d1:j   1, d   ;   ).

d      a(d1:j   1)

3note that this is not true for the swap transition system

de   ned in nivre (2009).

4it is straightforward to extend the approach to make use
of id145 in the case where the same state
can be reached by multiple decision sequences.

each zl(d1:j   1;   ) is a local id172 term.
the id203 of a sequence of decisions d1:n is

pl(d1:n) =

n

y
j=1

p(dj|d1:j   1;   )

=

exp pn
qn

j=1   (d1:j   1, dj;   )
j=1 zl(d1:j   1;   )

.

(2)

id125 can be used to attempt to    nd the
maximum of eq. (2) with respect to d1:n. the
additive scores used in id125 are the log-
softmax of each decision, ln p(dj|d1:j   1;   ), not
the raw scores   (d1:j   1, dj;   ).

in contrast, a conditional random field (crf)

de   nes a distribution pg(d1:n) as follows:

a signi   cant practical advantange of the locally
normalized cost eq. (4) is that the local parti-
tion function zl and its derivative can usually be
computed ef   ciently. in contrast, the zg term in
eq. (5) contains a sum over d   
1:n     dn that is in
many cases intractable.

early

updates

to make learning tractable with the glob-
ally normalized model, we use id125
and
(collins and roark, 2004;
zhou et al., 2015). as the training sequence is
being decoded, we keep track of the location of
the gold path in the beam. if the gold path falls
out of the beam at step j, a stochastic gradient
step is taken on the following objective:

lglobal   beam(d   

1:j;   ) =

pg(d1:n) =

exp pn

j=1   (d1:j   1, dj;   )

zg(  )

,

(3)

   

j

x

i=1

  (d   

1:i   1, d   

i ;   ) + ln x
   bj

d   

1:j

exp

j

x

i=1

  (d   

1:i   1, d   

i;   ).(6)

where

zg(  ) = x

exp

d   
1:n   dn

n

x
j=1

  (d   

1:j   1, d   

j;   )

and dn is the set of all valid sequences of deci-
sions of length n. zg(  ) is a global id172
term. the id136 problem is now to    nd

here the set bj contains all paths in the beam
at step j, together with the gold path pre   x d   
1:j.
it is straightforward to derive gradients of the
loss in eq. (6) and to back-propagate gradients to
all levels of a neural network de   ning the score
  (s, d;   ).
if the gold path remains in the beam
throughout decoding, a gradient step is performed
using bn, the beam at the end of decoding.

argmax
d1:n   dn

pg(d1:n) = argmax
d1:n   dn

n

x
j=1

  (d1:j   1, dj;   ).

3 the label bias problem

id125 can again be used to approximately
   nd the argmax.

2.3 training
training data consists of inputs x paired with gold
decision sequences d   
1:n. we use stochastic gradi-
ent descent on the negative log-likelihood of the
data under the model. under a locally normalized
model, the negative log-likelihood is

llocal(d   

1:n;   ) =     ln pl(d   

1:n;   ) =

(4)

   

n

x
j=1

  (d   

1:j   1, d   

j ;   ) +

n

x
j=1

ln zl(d   

1:j   1;   ),

whereas under a globally normalized model it is

to be
intuitively, we would like the model
able to revise an earlier decision made during
search, when later evidence becomes available that
rules out the earlier decision as incorrect. at
   rst glance, it might appear that a locally nor-
malized model used in conjunction with beam
search or exact search is able to revise ear-
lier decisions. however the label bias problem
(see bottou (1991), collins (1999) pages 222-226,
lafferty et al. (2001), bottou and lecun (2005),
smith and johnson (2007)) means
locally
normalized models often have a very weak ability
to revise earlier decisions.

that

this section gives a formal perspective on the
label bias problem, through a proof that globally
normalized models are strictly more expressive
than locally normalized models. the theorem was
originally proved5 by smith and johnson (2007).

lglobal(d   

1:n;   ) =     ln pg(d   

1:n;   ) =

   

n

x
j=1

  (d   

1:j   1, d   

j ;   ) + ln zg(  ).

(5)

5more precisely smith and johnson (2007) prove the the-
orem for models with potential functions of the form
  (di   1, di, xi); the generalization to potential functions of
the form   (d1:i   1, di, x1:i) is straightforward.

the example underlying the proof gives a clear il-
lustration of the label bias problem.6

global models can be strictly more expressive
than local models consider a tagging problem
where the task is to map an input sequence x1:n
to a decision sequence d1:n. first, consider a lo-
cally normalized model where we restrict the scor-
ing function to access only the    rst i input sym-
bols x1:i when scoring decision di. we will re-
turn to this restriction soon. the scoring function
   can be an otherwise arbitrary function of the tu-
ple hd1:i   1, di, x1:ii:

pl(d1:n|x1:n) =

n

y
i=1

pl(di|d1:i   1, x1:i)

=

exp pn
qn

i=1   (d1:i   1, di, x1:i)

i=1 zl(d1:i   1, x1:i)

second, consider a globally normalized model

pg(d1:n|x1:n) =

exp pn

i=1   (d1:i   1, di, x1:i)

zg(x1:n)

.

this model again makes use of a scoring function
  (d1:i   1, di, x1:i) restricted to the    rst i input sym-
bols when scoring decision di.

de   ne pl to be the set of all possible distribu-
tions pl(d1:n|x1:n) under the local model obtained
as the scores    vary. similarly, de   ne pg to be the
set of all possible distributions pg(d1:n|x1:n) un-
der the global model. here a    distribution    is a
function from a pair (x1:n, d1:n) to a id203
p(d1:n|x1:n). our main result is the following:
theorem 3.1 see
smith and johnson (2007).
subset of pg, that is pl ( pg.

also
pl is a strict

to prove this we will    rst prove that pl     pg.
this step is straightforward. we then show that
pg * pl; that is, there are distributions in pg
that are not in pl. the proof that pg * pl gives
a clear illustration of the label bias problem.

proof that pl     pg: we need to show that
for any locally normalized distribution pl, we can

6smith and johnson (2007) cite michael collins as the
source of the example underlying the proof. note that
the theorem refers to conditional models of
the form
p(d1:n|x1:n) with global or local id172. equiva-
lence (or non-equivalence) results for joint models of the
form p(d1:n, x1:n) are quite different: for example results
from chi (1999) and abney et al. (1999) imply that weighted
context-free grammars (a globally normalized joint model)
and id140 (a locally normal-
ized joint model) are equally expressive.

construct a globally normalized model pg such
that pg = pl. consider a locally normalized
model with scores   (d1:i   1, di, x1:i). de   ne a
global model pg with scores

     (d1:i   1, di, x1:i) = log pl(di|d1:i   1, x1:i).

then it is easily veri   ed that

pg(d1:n|x1:n) = pl(d1:n|x1:n)

for all x1:n, d1:n. (cid:3)

in proving pg * pl we will use a simple prob-
lem where every example seen in training or test
data is one of the following two tagged sentences:

x1x2x3 = a b c, d1d2d3 = a b c
x1x2x3 = a b e, d1d2d3 = a d e

(7)

.

note that the input x2 = b is ambiguous: it can
take tags b or d. this ambiguity is resolved when
the next input symbol, c or e, is observed.

follows.

now consider a globally normalized model,
scores   (d1:i   1, di, x1:i) are de-
where the
the set
   ned as
{(a, b), (b, c), (a, d), (d, e)} of bigram tag
transitions seen in the data. similarly, de   ne e
as the set {(a, a), (b, b), (c, c), (b, d), (e, e)} of
(word, tag) pairs seen in the data. we de   ne

de   ne t

as

  (d1:i   1, di, x1:i)
=       j(di   1, di)     t k +       j(xi, di)     ek

(8)

where    is the single scalar parameter of the
model, and j  k = 1 if    is true, 0 otherwise.

proof that pg * pl: we will construct a glob-
ally normalized model pg such that there is no lo-
cally normalized model such that pl = pg.

under the de   nition in eq. (8), it is straightfor-

ward to show that

lim
        

pg(a b c|a b c) = lim
        

pg(a d e|a b e) = 1.

in

contrast,

any
  (d1:i   1, di, x1:i), we must have

under

de   nition

for

pl(a b c|a b c) + pl(a d e|a b e)     1

(9)

follows

because

pl(a d e|a b e)

pl(a b c|a b c)

this
=
pl(a|a)    pl(b|a, a b)    pl(c|a b, a b c)
and
pl(a|a)   
pl(d|a, a b)    pl(e|a d, a b e).
the in-
equality pl(b|a, a b) + pl(d|a, a b)     1 then
immediately implies eq. (9).

=

a

it

(9)

the

under

locally

pg(a b c|a b c)

it follows that for suf   ciently large values of   ,
we have pg(a b c|a b c) + pg(a d e|a b e) > 1,
is impossible to de-
and given eq.
normalized model with
   ne
pl(a b c|a b c)
and
=
pl(a d e|a b e) = pg(a d e|a b e). (cid:3)
that

scores
  (d1:i   1, di, x1:i) depend only on the    rst
i
input symbols, the globally normalized model is
still able to model the data in eq. (7), while the
locally normalized model fails (see eq. 9). the
ambiguity at input symbol b is naturally resolved
when the next symbol (c or e) is observed, but
the locally normalized model is not able to revise
its prediction.

restriction

it is easy to    x the locally normalized model
for the example in eq. (7) by allowing scores
  (d1:i   1, di, x1:i+1) that take into account the in-
put symbol xi+1. more generally we can have a
model of the form   (d1:i   1, di, x1:i+k) where the
integer k speci   es the amount of lookahead in the
model. such lookahead is common in practice, but
insuf   cient in general. for every amount of looka-
head k, we can construct examples that cannot be
modeled with a locally normalized model by du-
plicating the middle input b in (7) k + 1 times.
only a local model with scores   (d1:i   1, di, x1:n)
that considers the entire input can capture any dis-
tribution p(d1:n|x1:n): in this case the decompo-
sition pl(d1:n|x1:n) = qn
i=1 pl(di|d1:i   1, x1:n)
makes no independence assumptions.

however, increasing the amount of context used
as input comes at a cost, requiring more powerful
learning algorithms, and potentially more train-
ing data. for a detailed analysis of the trade-
offs between structural features in crfs and more
powerful local classi   ers without structural con-
straints, see liang et al. (2008);
in these exper-
iments local classi   ers are unable to reach the
performance of crfs on problems such as pars-
ing and id39 where structural
constraints are important. note that there is noth-
ing to preclude an approach that makes use of both
global id172 and more powerful scoring
functions   (d1:i   1, di, x1:n), obtaining the best of
both worlds. the experiments that follow make
use of both.

4 experiments

to demonstrate the    exibility and modeling power
of our approach, we provide experimental results

on a diverse set of id170 tasks. we
apply our approach to id52, syntactic de-
pendency parsing, and sentence compression.

while directly optimizing the global model de-
   ned by eq. (5) works well, we found that train-
ing the model in two steps achieves the same pre-
cision much faster: we    rst pretrain the network
using the local objective given in eq. (4), and then
perform additional training steps using the global
objective given in eq. (6). we pretrain all layers
except the softmax layer in this way. we purpose-
fully abstain from complicated hand engineering
of input features, which might improve perfor-
mance further (durrett and klein, 2015).
recipe

from
the
for each training stage of
weiss et al. (2015)
our model. speci   cally, we use averaged stochas-
tic id119 with momentum, and we
tune the learning rate,
learning rate schedule,
momentum, and early stopping time using a
separate held-out corpus for each task. we tune
again with a different set of hyperparameters for
training with the global objective.

training

use

we

4.1 id52
part of speech (pos) tagging is a classic nlp task,
where modeling the structure of the output is im-
portant for achieving state-of-the-art performance.

data & evaluation. we conducted experi-
ments on a number of different datasets:
(1)
(wsj) part
the english wall street journal
the id32 (marcus et al., 1993)
of
with standard id52 splits;
(2) the en-
glish    treebank union    multi-domain corpus
containing data from the ontonotes corpus
the english web
version 5 (hovy et al., 2006),
treebank
and
the updated and corrected question treebank
(judge et al., 2006) with
to
weiss et al. (2015);
the conll    09
multi-lingual shared task (haji  c et al., 2009).

(petrov and mcdonald, 2012),

identical

and (3)

setup

model con   guration.
inspired by the inte-
grated id52 and parsing transition system
of bohnet and nivre (2012), we employ a simple
transition system that uses only a shift action and
predicts the pos tag of the current word on the
buffer as it gets shifted to the stack. we extract the
following features on a window   3 tokens cen-
tered at the current focus token: word, cluster,
character id165 up to length 3. we also extract
the tag predicted for the previous 4 tokens. the

method

linear crf
ling et al. (2015)

our local (b=1)
our local (b=8)
our global (b=8)

en
wsj

97.17
97.78

97.44
97.45
97.44

en-union

news web qtb

ca

ch

cz

conll    09

en

ge

ja

sp

97.60 94.58 96.04
97.44 94.03 96.18

98.81 94.45 98.90 97.50 97.14 97.90 98.79
98.77 94.38 99.00 97.60 97.84 97.06 98.71

97.66 94.46 96.59
97.69 94.46 96.64
97.77 94.80 96.86

98.91 94.56 98.96 97.36 97.35 98.02 98.88
98.88 94.56 98.96 97.40 97.35 98.02 98.89
99.03 94.72 99.02 97.65 97.52 98.37 98.97

parsey mcparseface

-

97.52 94.24 96.45 -

-

-

-

-

-

-

-

avg

-

97.17
97.16

97.29
97.30
97.47

-

table 1: final id52 test set results on english wsj and treebank union as well as conll   09. we also show the
performance of our pre-trained open source model,    parsey mcparseface.   

network in these experiments has a single hidden
layer with 256 units on wsj and treebank union
and 64 on conll   09.

results.
in table 1 we compare our model to
a linear crf and to the compositional character-
to-word lstm model of ling et al. (2015). the
crf is a    rst-order linear model with exact infer-
ence and the same emission features as our model.
it additionally also has transition features of the
word, cluster and character id165 up to length 3
on both endpoints of the transition. the results for
ling et al. (2015) were solicited from the authors.
our local model already compares favorably
against these methods on average. using beam
search with a locally normalized model does not
help, but with global id172 it leads to a
7% reduction in relative error, empirically demon-
strating the effect of label bias. the set of char-
acter ngrams feature is very important, increasing
average accuracy on the conll   09 datasets by
about 0.5% absolute. this shows that character-
level modeling can also be done with a simple
feed-forward network without recurrence.

4.2 id33

in id33 the goal is to produce a di-
rected tree representing the syntactic structure of
the input sentence.

data & evaluation. we use the same corpora
as in our id52 experiments, except that
we use the standard parsing splits of the wsj. to
avoid over-   tting to the development set (sec. 22),
we use sec. 24 for tuning the hyperparameters
of our models. we convert
the english con-
stituency trees to stanford style dependencies
(de marneffe et al., 2006) using version 3.3.0 of
the converter. for english, we use predicted pos
tags (the same pos tags are used for all models)
and exclude punctuation from the evaluation, as

is standard. for the conll    09 datasets we fol-
low standard practice and include all punctuation
in the evaluation. we follow alberti et al. (2015)
and use our own predicted pos tags so that we
can include a k-best tag feature (see below) but
use the supplied predicted morphological features.
we report unlabeled and labeled attachment scores
(uas/las).

model con   guration. our model con   guration
is basically the same as the one originally pro-
posed by chen and manning (2014) and then re-
   ned by weiss et al. (2015). in particular, we use
the arc-standard transition system and extract the
same set of features as prior work: words, part of
speech tags, and dependency arcs and labels in the
surrounding context of the state, as well as k-best
tags as proposed by alberti et al. (2015). we use
two hidden layers of 1,024 dimensions each.

results. tables 2 and 3 show our    nal pars-
ing results and a comparison to the best sys-
tems from the literature. we obtain the best ever
published results on almost all datasets, includ-
ing the wsj. our main results use the same pre-
trained id27s as weiss et al. (2015)
and alberti et al. (2015), but no tri-training. when
we arti   cially restrict ourselves to not use pre-
trained id27s, we observe only a mod-
est drop of    0.5% uas; for example, training
only on the wsj yields 94.08% uas and 92.15%
las for our global model with a beam of size 32.
even though we do not use tri-training, our
model compares favorably to the 94.26% las
and 92.41% uas reported by weiss et al. (2015)
with tri-training. as we show in sec. 5, these
gains can be attributed to the full id26
training that differentiates our approach from that
of weiss et al. (2015) and alberti et al. (2015).
our
results also signi   cantly outperform the
lstm-based approaches of dyer et al. (2015) and

method

martins et al. (2013)   
zhang and mcdonald (2014)   
weiss et al. (2015)
alberti et al. (2015)

our local (b=1)
our local (b=32)
our global (b=32)

wsj

uas

las

92.89 90.55
93.22 91.02
93.99 92.05
94.23 92.36

92.95 91.02
93.59 91.70
94.61 92.79

union-news
uas
las

93.10 91.13
93.32 91.48
93.91 92.25
94.10 92.55

93.11 91.46
93.65 92.03
94.44 92.93

union-web
uas
las

union-qtb
uas
las

88.23
88.65
89.29
89.55

88.42
88.96
90.17

85.04
85.59
86.44
86.85

85.58
86.17
87.54

94.21
93.37
94.17
94.74

92.49
93.22
95.40

94.77

91.54
90.69
92.06
93.04

90.38
91.17
93.64

93.17

parsey mcparseface (b=8)

-

-

94.15 92.51

89.08

86.29

table 2: final english id33 test set results. we note that training our system using only the wsj corpus (i.e. no
pre-trained embeddings or other external resources) yields 94.08% uas and 92.15% las for our global model with beam 32.

method

catalan

uas las

chinese

uas las

czech

uas las

english

uas las

german

uas las

japanese
uas las

spanish
uas las

best shared task result

-

87.86

-

79.17

-

80.38

-

89.88

-

87.48

-

92.57

-

87.64

ballesteros et al. (2015)
90.22 86.42
zhang and mcdonald (2014) 91.41 87.91
91.33 87.22
lei et al. (2014)
92.44 89.60
bohnet and nivre (2012)
alberti et al. (2015)
92.31 89.17

our local (b=1)
our local (b=16)
our global (b=16)

91.24 88.21
91.91 88.93
92.67 89.83

80.64 76.52
82.87 78.57
81.67 76.71
82.52 78.51
83.57 79.90

81.29 77.29
82.22 78.26
84.72 80.85

79.87 73.62
86.62 80.59
88.76 81.77
88.82 83.73
88.45 83.57

85.78 80.63
86.25 81.28
88.94 84.56

90.56 88.01
92.69 90.01
92.75 90.00
92.87 90.60
92.70 90.56

91.44 89.29
92.16 90.05
93.22 91.23

88.83 86.10
89.88 87.38
90.81 87.81
91.37 89.38
90.58 88.20

89.12 86.95
89.53 87.4
90.91 89.15

93.47 92.55
92.82 91.87
94.04 91.84
93.67 92.63
93.99 93.10

93.71 92.85
93.61 92.74
93.65 92.84

90.38 86.59
90.82 87.34
91.16 87.38
92.24 89.60
92.26 89.33

91.01 88.14
91.64 88.88
92.62 89.95

table 3: final conll    09 id33 test set results.

ballesteros et al. (2015).

4.3 sentence compression

our    nal id170 task is extractive
sentence compression.

a

&

evaluation. we

data
follow
filippova et al. (2015), where
large news
collection is used to heuristically generate com-
pression instances. our    nal corpus contains
about 2.3m compression instances: we use 2m
examples for training, 130k for development and
160k for the    nal test. we report per-token f1
score and per-sentence accuracy (a),
i.e. per-
centage of instances that fully match the golden
compressions. following filippova et al. (2015)
we also run a human evaluation on 200 sentences
where we ask the raters to score compressions for
readability (read) and informativeness (info)
on a scale from 0 to 5.

model con   guration. the transition system
for sentence compression is similar to pos tag-
ging: we scan sentences from left-to-right and la-
bel each token as keep or drop. we extract fea-
tures from words, pos tags, and dependency la-
bels from a window of tokens centered on the in-

method

generated corpus human eval
info

read

f1

a

filippova et al. (2015)
automatic

our local (b=1)
our local (b=8)
our global (b=8)

35.36

82.83

-

30.51
31.19
35.16

-

78.72
75.69
81.41

4.66
4.31

4.58

-

4.03
3.77

4.03

-

4.67

4.07

table 4: sentence compression results on news data. auto-
matic refers to application of the same automatic extraction
rules used to generate the news training corpus.

put, as well as features from the history of predic-
tions. we use a single hidden layer of size 400.

results. table 4 shows our sentence compres-
sion results. our globally normalized model again
signi   cantly outperforms the local model. beam
search with a locally normalized model suffers
from severe label bias issues that we discuss on
a concrete example in section 5. we also com-
pare to the sentence compression system from
filippova et al. (2015), a 3-layer stacked lstm
which uses dependency label information. the
lstm and our global model perform on par on
both the automatic evaluation as well as the hu-
man ratings, but our model is roughly 100   faster.

all compressions kept approximately 42% of the
tokens on average and all the models are signi   -
cantly better than the automatic extractions (p <
0.05).

5 discussion

we derived a proof for the label bias problem
and the advantages of global models. we then
emprirically veri   ed this theoretical superiority
by demonstrating state-of-the-art performance on
three different tasks. in this section we situate and
compare our model to previous work and provide
two examples of the label bias problem in practice.

5.1 related neural crf work

neural network models have been been combined
with conditional
random    elds and globally
normalized models before. bottou et al. (1997)
and le cun et al. (1998) describe global
train-
ing of neural network models for structured
prediction problems.
peng et al. (2009) add
a non-linear neural network layer to a linear-
chain crf and do and artires (2010) apply
a similar approach to more general markov
network
and
zheng et al. (2015) introduce recurrence into the
model and huang et al. (2015)    nally combine
crfs and lstms. these neural crf models are
limited to sequence labeling tasks where exact
id136 is possible, while our model works well
when exact id136 is intractable.

yao et al. (2014)

structures.

5.2 related transition-based parsing work

on

early work

neural-networks

zhou et al. (2015)

for
for
transition-based parsing, see henderson (2003;
to the work of
2004). our work is closest
weiss et al. (2015),
and
watanabe and sumita (2015); in these approaches
global id172 is added to the local model
of chen and manning (2014).
empirically,
weiss et al. (2015) achieves the best performance,
even though their model keeps the parameters of
the locally normalized neural network    xed and
only trains a id88 that uses the activations
as features. their model is therefore limited in
its ability to revise the predictions of the locally
normalized model. in table 5 we show that full
id26 training all the way to the word
embeddings is very important and signi   cantly
contributes to the performance of our model. we
also compared training under the crf objective

method

local (b=1)
local (b=16)

global (b=16) {  (d)}
global (b=16) {w2,   (d)}
global (b=16) {w1, w2,   (d)}
global (b=16) (full)

uas

las

92.85
93.32

93.45
94.01
94.09
94.38

90.59
91.09

91.21
91.77
91.81
92.17

table 5: wsj dev set scores for successively deeper levels
of id26. the full parameter set corresponds to
id26 all the way to the embeddings. wi: hidden
layer i weights.

with a id88-like hinge loss between the
gold and best elements of the beam. when we
limited the id26 depth to training only
the top layer   (d), we found negligible differences
in accuracy: 93.20% and 93.28% for the crf
objective and hinge loss respectively. however,
when training with full id26 the crf
accuracy is 0.2% higher and training converged
more than 4   faster.

zhou et al. (2015) perform full backpropaga-
tion training like us, but even with a much
larger beam,
their performance is signi   cantly
lower than ours. we also apply our model
to two additional
tasks, while they experi-
ment only with id33.
finally,
watanabe and sumita (2015) introduce recurrent
components and additional techniques like max-
violation updates for a corresponding constituency
parsing model. in contrast, our model does not re-
quire any recurrence or specialized training.

5.3 label bias in practice

we observed several instances of severe label bias
in the sentence compression task. although us-
ing id125 with the local model outperforms
greedy id136 on average, id125 leads
the local model to occasionally produce empty
compressions (table 6).
it is important to note
that these are not search errors: the empty com-
pression has higher id203 under pl than the
prediction from greedy id136. however, the
more expressive globally normalized model does
not suffer from this limitation, and correctly gives
the empty compression almost zero id203.

we also present some empirical evidence that
the label bias problem is severe in parsing. we
trained models where the scoring functions in
parsing at position i in the sentence are limited to

method

predicted compression

pl pg

local (b=1)
local (b=8)
global (b=8) in pakistan, former leader pervez musharraf has appeared in court for the    rst time, on treason charges. 0.06 0.07

in pakistan, former leader pervez musharraf has appeared in court for the    rst time, on treason charges. 0.13 0.05
in pakistan, former leader pervez musharraf has appeared in court for the    rst time, on treason charges. 0.16 <10   4

table 6: example sentence compressions where the label bias of the locally normalized model leads to a breakdown during
id125. the id203 of each compression under the local (pl) and global (pg) models shows that only the global
model can properly represent zero id203 for the empty compression.

considering only tokens x1:i; hence unlike the full
parsing model, there is no ability to look ahead
in the sentence when making a decision.7 the
result for a greedy model under this constraint
is 76.96% uas; for a locally normalized model
with id125 is 81.35%; and for a globally
normalized model is 93.60%. thus the globally
normalized model gets very close to the perfor-
mance of a model with full lookahead, while the
locally normalized model with a beam gives dra-
matically lower performance. in our    nal exper-
iments with full lookahead, the globally normal-
ized model achieves 94.01% accuracy, compared
to 93.07% accuracy for a local model with beam
search. thus adding lookahead allows the lo-
cal model to close the gap in performance to the
global model; however there is still a signi   cant
difference in accuracy, which may in large part be
due to the label bias problem.

a number of authors have considered modi   ed
training procedures for greedy models, or for lo-
cally normalized models. daum  e iii et al. (2009)
introduce searn, an algorithm that allows a
classi   er making greedy decisions to become
more robust
to errors made in previous deci-
sions. goldberg and nivre (2013) describe im-
provements to a greedy parsing approach that
makes use of methods from imitation learn-
ing (ross et al., 2011) to augment
the training
these methods are focused on
set. note that
greedy models:
they are unlikely to solve the
label bias problem when used in conjunction
with id125, given that
the problem is
one of expressivity of the underlying model.
more recent work (yazdani and henderson, 2015;
vaswani and sagae, 2016) has augmented locally
normalized models with correctness probabilities
or error states, effectively adding a step after every
decision where the id203 of correctness of
the resulting structure is evaluated. this gives con-

siderable gains over a locally normalized model,
although performance is lower than our full glob-
ally normalized approach.

6 conclusions

we presented a simple and yet powerful model ar-
chitecture that produces state-of-the-art results for
id52, id33 and sentence
compression. our model combines the    exibil-
ity of transition-based algorithms and the model-
ing power of neural networks. our results demon-
strate that feed-forward network without recur-
rence can outperform recurrent models such as
lstms when they are trained with global normal-
ization. we further support our empirical    ndings
with a proof showing that global id172
helps the model overcome the label bias problem
from which locally normalized models suffer.

acknowledgements

we would like to thank ling wang for training
his c2w part-of-speech tagger on our setup, and
emily pitler, ryan mcdonald, greg coppola and
fernando pereira for tremendously helpful discus-
sions. finally, we are grateful to all members of
the google parsing team.

references
[abney et al.1999] steven abney, david mcallester,
and fernando pereira. 1999. relating probabilis-
tic grammars and automata. proceedings of the 37th
annual meeting of the association for computa-
tional linguistics, pages 131   160.

[alberti et al.2015] chris alberti, david weiss, greg
coppola, and slav petrov.
improved
transition-based parsing and tagging with neural net-
works. in proceedings of the 2015 conference on
empirical methods in natural language process-
ing, pages 1354   1359.

2015.

7this setting may be important in some applications,
where for example parse structures for sentence pre   xes are
required, or where the input is received one word at a time
and online processing is bene   cial.

[ballesteros et al.2015] miguel ballesteros, chris dyer,
and noah a. smith. 2015.
improved transition-
based parsing by modeling characters instead of
words with lstms.
in proceedings of the 2015

conference on empirical methods in natural lan-
guage processing, pages 349   359.

[bohnet and nivre2012] bernd bohnet and joakim
nivre. 2012. a transition-based system for joint
part-of-speech tagging and labeled non-projective
id33.
in proceedings of the 2012
joint conference on empirical methods in natural
language processing and computational natural
language learning, pages 1455   1465.

[bottou and lecun2005] l  eon bottou and yann le-
cun. 2005. graph transformer networks for image
recognition. bulletin of the international statistical
institute (isi).

[bottou et al.1997] l  eon bottou, yann le cun, and
yoshua bengio. 1997. global training of docu-
ment processing systems using graph transformer
networks. in proceedings of id161 and
pattern recognition (cvpr), pages 489   493.

[bottou1991] l  eon bottou.

1991. une approche
th  eorique de lapprentissage connexionniste: appli-
cations `a la reconnaissance de la parole. ph.d. the-
sis, doctoral dissertation, universite de paris xi.

[chen and manning2014] danqi chen and christo-
pher d. manning. 2014. a fast and accurate de-
pendency parser using neural networks. in proceed-
ings of the 2014 conference on empirical methods
in natural language processing, pages 740   750.

[chi1999] zhiyi chi.

statistical properties
of id140. computa-
tional linguistics, pages 131   160.

1999.

[collins and roark2004] michael collins and brian
roark. 2004. incremental parsing with the percep-
tron algorithm.
in proceedings of the 42nd meet-
ing of the association for computational linguistics
(acl   04), pages 111   118.

[collins1999] michael collins. 1999. head-driven
statistical models for natural language parsing.
ph.d. thesis, university of pennsylvania.

[collobert et al.2011] ronan collobert, jason weston,
l  eon bottou, michael karlen, koray kavukcuoglu,
and pavel kuksa. 2011. natural language process-
ing (almost) from scratch. the journal of machine
learning research, 12:2493   2537.

[daum  e iii et al.2009] hal daum  e iii, john langford,
and daniel marcu.
search-based struc-
tured prediction. machine learning journal (mlj),
75(3):297   325.

2009.

[de marneffe et al.2006] marie-catherine de marn-
effe, bill maccartney, and christopher d. manning.
2006. generating typed dependency parses from
phrase structure parses. in proceedings of fifth in-
ternational conference on language resources and
evaluation, pages 449   454.

[do and artires2010] trinh minh tri do and thierry
artires. 2010. neural conditional random    elds. in
international conference on arti   cial intelligence
and statistics, volume 9, pages 177   184.

[durrett and klein2015] greg durrett and dan klein.
2015. neural crf parsing.
in proceedings of the
53rd annual meeting of the association for compu-
tational linguistics and the 7th international joint
conference on natural language processing, pages
302   312.

[dyer et al.2015] chris dyer, miguel ballesteros,
wang ling, austin matthews, and noah a. smith.
2015. transition-based id33 with
stack long short-term memory.
in proceedings of
the 53rd annual meeting of the association for
computational linguistics, pages 334   343.

[filippova et al.2015] katja filippova, enrique alfon-
seca, carlos a. colmenares,   ukasz kaiser, and
oriol vinyals. 2015. sentence compression by dele-
tion with lstms.
in proceedings of the 2015 con-
ference on empirical methods in natural language
processing, pages 360   368.

[goldberg and nivre2013] yoav goldberg and joakim
nivre. 2013. training deterministic parsers with
non-deterministic oracles. transactions of the asso-
ciation for computational linguistics, 1:403   414.

[haji  c et al.2009] jan haji  c, massimiliano cia-
ramita, richard johansson, daisuke kawahara,
maria ant`onia mart    , llu    s m`arquez, adam mey-
ers, joakim nivre, sebastian pad  o, jan   st  ep  anek,
pavel stra  n  ak, mihai surdeanu, nianwen xue,
and yi zhang. 2009. the conll-2009 shared task:
syntactic and semantic dependencies in multi-
ple languages.
in proceedings of the thirteenth
conference on computational natural language
learning: shared task, pages 1   18.

[henderson2003] james henderson. 2003.

inducing
history representations for broad coverage statistical
parsing.
in proceedings of the 2003 human lan-
guage technology conference of the north ameri-
can chapter of the association for computational
linguistics, pages 24   31.

[henderson2004] james henderson. 2004. discrimi-
native training of a neural network statistical parser.
in proceedings of the 42nd meeting of the associa-
tion for computational linguistics (acl   04), pages
95   102.

[hochreiter and schmidhuber1997] sepp hochreiter
and j  urgen schmidhuber. 1997. long short-term
memory. neural computation, 9(8):1735   1780.

[hovy et al.2006] eduard hovy, mitchell marcus,
martha palmer, lance ramshaw,
and ralph
weischedel. 2006. ontonotes: the 90% solution.
in proceedings of the human language technology
conference of the naacl, short papers, pages
57   60.

[huang et al.2015] zhiheng huang, wei xu, and kai
yu. 2015. bidirectional lstm-crf models for se-
quence tagging. arxiv preprint arxiv:1508.01991.

[judge et al.2006] john judge, aoife cahill, and josef
van genabith. 2006. questionbank: creating a cor-
pus of parse-annotated questions. in proceedings of
the 21st international conference on computational
linguistics and 44th annual meeting of the associa-
tion for computational linguistics, pages 497   504.

[lafferty et al.2001] john lafferty, andrew mccallum,
and fernando pereira. 2001. conditional random
   elds: probabilistic models for segmenting and la-
beling sequence data. in proceedings of the eigh-
teenth international conference on machine learn-
ing, pages 282   289.

[le cun et al.1998] yann le cun, l  eon bottou, yoshua
bengio, and patrick haffner. 1998. gradient based
learning applied to document recognition. proceed-
ings of ieee, 86(11):2278   2324.

[lei et al.2014] tao lei, yu xin, yuan zhang, regina
barzilay, and tommi jaakkola. 2014. low-rank
tensors for scoring dependency structures. in pro-
ceedings of the 52nd annual meeting of the asso-
ciation for computational linguistics, pages 1381   
1391.

[liang et al.2008] percy liang, hal daum  e, iii, and
dan klein. 2008. structure compilation: trading
structure for features. in proceedings of the 25th in-
ternational conference on machine learning, pages
592   599.

[ling et al.2015] wang ling, chris dyer, alan w
black, isabel trancoso, ramon fermandez, silvio
amir, luis marujo, and tiago luis. 2015. finding
function in form: compositional character models
for open vocabulary word representation.
in pro-
ceedings of the 2015 conference on empirical meth-
ods in natural language processing, pages 1520   
1530.

[marcus et al.1993] mitchell p. marcus, beatrice san-
torini, and mary ann marcinkiewicz. 1993. build-
ing a large annotated corpus of english: the penn
treebank. computational linguistics, 19(2):313   
330.

[martins et al.2013] andre martins, miguel almeida,
and noah a. smith. 2013. turning on the turbo:
fast third-order non-projective turbo parsers.
in
proceedings of the 51st annual meeting of the as-
sociation for computational linguistics, pages 617   
622.

[nivre2006] joakim nivre. 2006.

inductive depen-

dency parsing. springer-verlag new york, inc.

[peng et al.2009] jian peng, liefeng bo, and jinbo xu.
2009. conditional neural    elds.
in advances in
neural information processing systems 22, pages
1419   1427.

[petrov and mcdonald2012] slav petrov and ryan mc-
donald. 2012. overview of the 2012 shared task
on parsing the web. notes of the first workshop
on syntactic analysis of non-canonical language
(sancl).

[ross et al.2011] st  ephane ross, geoffrey j. gordon,
and j. andrew bagnell. 2011. no-regret reduc-
tions for imitation learning and structured predic-
tion. aistats.

[smith and johnson2007] noah smith and mark john-
son. 2007. weighted and probabilistic context-free
grammars are equally expressive. computational
linguistics, pages 477   491.

[vaswani and sagae2016] ashish vaswani and kenji
sagae.
2016. ef   cient structured id136 for
transition-based parsing with neural networks and
error states. transactions of the association for
computational linguistics, 4:183   196.

[vinyals et al.2015] oriol vinyals,   ukasz kaiser,
terry koo, slav petrov, ilya sutskever, and geof-
frey hinton. 2015. grammar as a foreign language.
in advances in neural
information processing
systems 28, pages 2755   2763.

[watanabe and sumita2015] taro watanabe and ei-
ichiro sumita. 2015. transition-based neural con-
stituent parsing. in proceedings of the 53rd annual
meeting of the association for computational lin-
guistics and the 7th international joint conference
on natural language processing, pages 1169   1179.

[weiss et al.2015] david weiss, chris alberti, michael
collins, and slav petrov. 2015. structured training
for neural network transition-based parsing. in pro-
ceedings of the 53rd annual meeting of the associa-
tion for computational linguistics, pages 323   333.

[yao et al.2014] kaisheng yao, baolin peng, geoffrey
zweig, dong yu, xiaolong li, and feng gao. 2014.
recurrent conditional random    eld for language un-
derstanding. in ieee international conference on
acoustics, speech, and signal processing (icassp
   14).

[yazdani and henderson2015] majid yazdani

and
james henderson.
incremental recurrent
neural network dependency parser with search-
based discriminative training. in proceedings of the
nineteenth conference on computational natural
language learning, pages 142   152.

2015.

[nivre2009] joakim nivre. 2009. non-projective de-
pendency parsing in expected linear time.
in pro-
ceedings of the joint conference of the 47th annual
meeting of the acl and the 4th international joint
conference on natural language processing of the
afnlp, pages 351   359.

[zhang and mcdonald2014] hao zhang and ryan mc-
donald. 2014. enforcing structural diversity in
cube-pruned id33.
in proceedings
of the 52nd annual meeting of the association for
computational linguistics, pages 656   661.

[zheng et al.2015] shuai zheng, sadeep jayasumana,
bernardino romera-paredes, vibhav vineet,
zhizhong su, dalong du, chang huang, and philip
h. s. torr. 2015. conditional random    elds as re-
current neural networks. in the ieee international
conference on id161 (iccv), pages
1529   1537.

[zhou and xu2015] jie zhou and wei xu. 2015. end-
to-end learning of id14 using re-
current neural networks. in proceedings of the 53rd
annual meeting of the association for computa-

tional linguistics and the 7th international joint
conference on natural language processing, pages
1127   1137.

[zhou et al.2015] hao zhou, yue zhang, and jiajun
chen.
2015. a neural probabilistic structured-
prediction model for transition-based dependency
parsing. in proceedings of the 53rd annual meet-
ing of the association for computational linguis-
tics, pages 1213   1222.

