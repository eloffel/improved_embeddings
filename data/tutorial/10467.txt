9
1
0
2

 
r
a

 

m
6
2

 
 
]

g
l
.
s
c
[
 
 

5
v
8
1
3
6
0

.

3
0
6
1
:
v
i
x
r
a

harnessing deep neural networks with logic rules

zhiting hu

xuezhe ma

zhitingh@cs.cmu.edu

xuezhem@cs.cmu.edu

zhengzhong liu
liu@cs.cmu.edu

eduard hovy
hovy@cmu.edu

eric p. xing

epxing@cs.cmu.edu

school of computer science, carnegie mellon university

abstract

combining deep neural networks with structured logic rules is desirable to har-
ness    exibility and reduce uninterpretability of the neural models. we propose a gen-
eral framework capable of enhancing various types of neural networks (e.g., id98s and
id56s) with declarative    rst-order logic rules. speci   cally, we develop an iterative dis-
tillation method that transfers the structured information of logic rules into the weights
of neural networks. we deploy the framework on a id98 for id31, and an
id56 for id39. with a few highly intuitive rules, we obtain sub-
stantial improvements and achieve state-of-the-art or comparable results to previous
best-performing systems.

1 introduction

deep neural networks provide a powerful mechanism for learning patterns from massive
data, achieving new levels of performance on image classi   cation (krizhevsky et al., 2012),
id103 (hinton et al., 2012), machine translation (bahdanau et al., 2014), play-
ing strategic board games (silver et al., 2016), and so forth.

despite the impressive advances, the widely-used dnn methods still have limitations. the
high predictive accuracy has heavily relied on large amounts of labeled data; and the
purely data-driven learning can lead to uninterpretable and sometimes counter-intuitive
results (szegedy et al., 2014; nguyen et al., 2015).
it is also di   cult to encode human
intention to guide the models to capture desired patterns, without expensive direct super-
vision or ad-hoc initialization.

on the other hand, the cognitive process of human beings have indicated that people learn
not only from concrete examples (as dnns do) but also from di   erent forms of general
knowledge and rich experiences (minksy, 1980; lake et al., 2015). logic rules provide a
   exible declarative language for communicating high-level cognition and expressing struc-
tured knowledge. it is therefore desirable to integrate logic rules into dnns, to transfer
human intention and domain knowledge to neural models, and regulate the learning process.

1

in this paper, we present a framework capable of enhancing general types of neural net-
works, such as convolutional networks (id98s) and recurrent networks (id56s), on various
tasks, with logic rule knowledge. combining symbolic representations with neural methods
have been considered in di   erent contexts. neural-symbolic systems (garcez et al., 2012)
construct a network from a given rule set to execute reasoning. to exploit a priori knowl-
edge in general neural architectures, recent work augments each raw data instance with
useful features (collobert et al., 2011), while network training, however, is still limited to
instance-label supervision and su   ers from the same issues mentioned above. besides, a
large variety of structural knowledge cannot be naturally encoded in the feature-label form.

our framework enables a neural network to learn simultaneously from labeled instances as
well as logic rules, through an iterative rule knowledge distillation procedure that transfers
the structured information encoded in the logic rules into the network parameters. since the
general logic rules are complementary to the speci   c data labels, a natural    side-product   
of the integration is the support for semi-supervised learning where unlabeled data is used
to better absorb the logical knowledge. methodologically, our approach can be seen as a
combination of the knowledge distillation (hinton et al., 2015; bucilu et al., 2006) and the
posterior id173 (pr) method (ganchev et al., 2010). in particular, at each iteration
we adapt the posterior constraint principle from pr to construct a rule-regularized teacher,
and train the student network of interest to imitate the predictions of the teacher network.
we leverage soft logic to support    exible rule encoding.

we apply the proposed framework on both id98 and id56, and deploy on the task of
id31 (sa) and id39 (ner), respectively. with only a
few (one or two) very intuitive rules, both the distilled networks and the joint teacher
networks strongly improve over their basic forms (without rules), and achieve better or
comparable performance to state-of-the-art models which typically have more parameters
and complicated architectures.

to the best of our knowledge, this is the    rst work to integrate logic rules with general
workhorse types of deep neural networks in a principled framework. the encouraging re-
sults indicate our method can be potentially useful for incorporating richer types of human
knowledge, and improving other application domains.

2 related work

combination of logic rules and neural networks has been considered in di   erent contexts.
neural-symbolic systems (garcez et al., 2012), such as kbann (towell et al., 1990) and
cilp++ (fran  ca et al., 2014), construct network architectures from given rules to perform
reasoning and knowledge acquisition. a related line of research, such as markov logic
networks (richardson and domingos, 2006), derives probabilistic id114 (rather
than neural networks) from the rule set.

with the recent success of deep neural networks in a vast variety of application domains,
it is increasingly desirable to incorporate structured logic knowledge into general types of
networks to harness    exibility and reduce uninterpretability. recent work that trains on

2

extra features from domain knowledge (collobert et al., 2011), while producing improved
results, does not go beyond the data-label paradigm. kulkarni et al. (2015) uses a specialized
training procedure with careful ordering of training instances to obtain an interpretable
neural layer of an image network. karaletsos et al. (2016) develops a generative model jointly
over data-labels and similarity knowledge expressed in triplet format to learn improved
disentangled representations.

though there do exist general frameworks that allow encoding various structured constraints
on latent variable models (ganchev et al., 2010; zhu et al., 2014; liang et al., 2009), they
either are not directly applicable to the nn case, or could yield inferior performance as in
our empirical study. liang et al. (2008) transfers predictive power of pre-trained structured
models to unstructured ones in a pipelined fashion.

our proposed approach is distinct in that we use an iterative rule distillation process to
e   ectively transfer rich structured knowledge, expressed in the declarative    rst-order logic
language, into parameters of general neural networks. we show that the proposed approach
strongly outperforms an extensive array of other either ad-hoc or general integration meth-
ods.

3 method

in this section we present our framework which encapsulates the logical structured knowl-
edge into a neural network. this is achieved by forcing the network to emulate the predic-
tions of a rule-regularized teacher, and evolving both models iteratively throughout training
(section 3.2). the process is agnostic to the network architecture, and thus applicable to gen-
eral types of neural models including id98s and id56s. we construct the teacher network
in each iteration by adapting the posterior id173 principle in our logical constraint
setting (section 3.3), where our formulation provides a closed-form solution. figure 1 shows
an overview of the proposed framework.

figure 1: framework overview. at each iteration, the teacher network is obtained by
projecting the student network to a rule-regularized subspace (red dashed arrow); and
the student network is updated to balance between emulating the teacher   s output and
predicting the true labels (black/blue solid arrows).

3

losslabeled datalogic rules    (    |    )        (    |    )projectionunlabeled datateacher network constructionrule knowledge distillationback propagationteacher    (    |    )student         (    |    )3.1 learning resources: instances and rules

our approach allows neural networks to learn from both speci   c examples and general rules.
here we give the settings of these    learning resources   .
assume we have input variable x     x and target variable y     y. for clarity, we focus
on k-way classi   cation, where y =    k is the k-dimensional id203 simplex and
y     {0, 1}k     y is a one-hot encoding of the class label. however, our method speci   cation
can straightforwardly be applied to other contexts such as regression and sequence learning
(e.g., ner tagging, which is a sequence of classi   cation decisions). the training data
d = {(xn, yn)}n
further consider a set of    rst-order logic (fol) rules with con   dences, denoted as r =
{(rl,   l)}l
l=1, where rl is the lth rule over the input-target space (x ,y), and   l     [0,   ] is
the con   dence level with   l =     indicating a hard rule, i.e., all groundings are required to
be true (=1). here a grounding is the logic expression with all variables being instantiated.
given a set of examples (x, y)     (x ,y) (e.g., a minibatch from d), the set of groundings
of rl are denoted as {rlg(x, y)}gl
g=1. in practice a rule grounding is typically relevant to
only a single or subset of examples, though here we give the most general form on the entire
set.

n=1 is a set of instantiations of (x, y).

we encode the fol rules using soft logic (bach et al., 2015) for    exible encoding and stable
optimization. speci   cally, soft logic allows continuous truth values from the interval [0, 1]
instead of {0, 1}, and the boolean logic operators are reformulated as:

a&b = max{a + b     1, 0}
a     b = min{a + b, 1}
a1                an =
  a = 1     a

(cid:88)

ai/n

i

(1)

here & and     are two di   erent approximations to logical conjunction (foulds et al., 2015):
& is useful as a selection operator (e.g., a&b = b when a = 1, and a&b = 0 when a = 0),
while     is an averaging operator.

3.2 rule knowledge distillation
a neural network de   nes a id155 p  (y|x) by using a softmax output layer
that produces a k-dimensional soft prediction vector denoted as     (x). the network is
parameterized by weights   . standard neural network training has been to iteratively
update    to produce the correct labels of training instances. to integrate the information
encoded in the rules, we propose to train the network to also imitate the outputs of a
rule-regularized projection of p  (y|x), denoted as q(y|x), which explicitly includes rule
constraints as id173 terms.
in each iteration q is constructed by projecting p  
into a subspace constrained by the rules, and thus has desirable properties. we present the
construction in the next section. the prediction behavior of q reveals the information of the
regularized subspace and structured rules. emulating the q outputs serves to transfer this

4

knowledge into p  . the new objective is then formulated as a balancing between imitating
the soft predictions of q and predicting the true hard labels:

n(cid:88)

  (t+1) = arg min
       

1
n

(1       )(cid:96)(yn,     (xn))

n=1

+   (cid:96)(s(t)

n ,     (xn)),

(2)

where (cid:96) denotes the id168 selected according to speci   c applications (e.g., the cross
id178 loss for classi   cation); s(t)
n is the soft prediction vector of q on xn at iteration t; and
   is the imitation parameter calibrating the relative importance of the two objectives.

a similar imitation procedure has been used in other settings such as model compres-
sion (bucilu et al., 2006; hinton et al., 2015) where the process is termed distillation.
following them we call p  (y|x) the    student    and q(y|x) the    teacher   , which can be intu-
itively explained in analogous to human education where a teacher is aware of systematic
general rules and she instructs students by providing her solutions to particular questions
(i.e., the soft predictions). an important di   erence from previous distillation work, where
the teacher is obtained beforehand and the student is trained thereafter, is that our teacher
and student are learned simultaneously during training.

though it is possible to combine a neural network with rule constraints by projecting
the network to the rule-regularized subspace after it is fully trained as before with only
data-label instances, or by optimizing projected network directly, we found our iterative
teacher-student distillation approach provides a much superior performance, as shown in
the experiments. moreover, since p   distills the rule information into the weights    instead
of relying on explicit rule representations, we can use p   for predicting new examples at test
time when the rule assessment is expensive or even unavailable (i.e., the privileged informa-
tion setting (lopez-paz et al., 2016)) while still enjoying the bene   t of integration. besides,
the second loss term in eq.(2) can be augmented with rich unlabeled data in addition to
the labeled examples, which enables semi-supervised learning for better absorbing the rule
knowledge.

3.3 teacher network construction
we now proceed to construct the teacher network q(y|x) at each iteration from p  (y|x).
the iteration index t is omitted for clarity. we adapt the posterior id173 principle
in our logic constraint setting. our formulation ensures a closed-form solution for q and
thus avoids any signi   cant increases in computational overhead.
recall the set of fol rules r = {(rl,   l)}l
l=1. our goal is to    nd the optimal q that
   ts the rules while at the same time staying close to p  . for the    rst property, we apply
a commonly-used strategy that imposes the rule constraints on q through an expectation
operator. that is, for each rule (indexed by l) and each of its groundings (indexed by g)
on (x, y), we expect eq(y|x)[rlg(x, y)] = 1, with con   dence   l. the constraints de   ne a
rule-regularized space of all valid distributions. for the second property, we measure the
closeness between q and p   with kl-divergence, and wish to minimize it. combining the

5

two factors together and further allowing slackness for the constraints, we    nally get the
following optimization problem:

min
q,     0

(cid:88)
kl(q(y|x)(cid:107)p  (y|x)) + c
s.t.   l(1     eq[rl,gl(x, y)])       l,gl
l = 1, . . . , l,

gl = 1, . . . , gl,

l,gl

  l,gl

(3)

where   l,gl     0 is the slack variable for respective logic constraint; and c is the id173
parameter. the problem can be seen as projecting p   into the constrained subspace. the
problem is convex and can be e   ciently solved in its dual form with closed-form solutions.
we provide the detailed derivation in the supplementary materials and directly give the
solution here:

q   (y|x)     p  (y|x) exp

c  l(1     rl,gl (x, y))

(4)

            (cid:88)

l,gl

         

intuitively, a strong rule with large   l will lead to low probabilities of predictions that fail to
meet the constraints. we discuss the computation of the id172 factor in section 3.4.

our framework is related to the posterior id173 (pr) method (ganchev et al., 2010)
which places constraints over model posterior in unsupervised setting. in classi   cation, our
optimization procedure is analogous to the modi   ed em algorithm for pr, by using cross-
id178 loss in eq.(2) and evaluating the second loss term on unlabeled data di   ering from
d, so that eq.(4) corresponds to the e-step and eq.(2) is analogous to the m-step. this
sheds light from another perspective on why our framework would work. however, we found
in our experiments (section 5) that to produce strong performance it is crucial to use the
same labeled data xn in the two losses of eq.(2) so as to form a direct trade-o    between
imitating soft predictions and predicting correct hard labels.

3.4

implementations

the procedure of iterative distilling optimization of our framework is summarized in algo-
rithm 1.

during training we need to compute the soft predictions of q at each iteration, which is
straightforward through direct enumeration if the rule constraints in eq.(4) are factored in
the same way as the base neural model p   (e.g., the    but   -rule of sentiment classi   cation in
section 4.1). if the constraints introduce additional dependencies, e.g., bi-gram dependency
as the transition rule in the ner task (section 4.2), we can use id145
for e   cient computation. for higher-order constraints (e.g., the listing rule in ner), we
approximate through id150 that iteratively samples from q(yi|y   i, x) for each
position i. if the constraints span multiple instances, we group the relevant instances in
minibatches for joint id136 (and randomly break some dependencies when a group is
too large). note that calculating the soft predictions is e   cient since only one nn forward
pass is required to compute the base distribution p  (y|x) (and few more, if needed, for
calculating the truth values of relevant rules).

6

algorithm 1 harnessing nn with rules
input: the training data d = {(xn, yn)}n

the rule set r = {(rl,   l)}l
parameters:        imitation parameter

l=1,

n=1,

c     id173 strength

1: initialize neural network parameter   
2: repeat
3:
4:
5:
6: until convergence
output: distill student network p   and teacher network q

sample a minibatch (x, y)     d
construct teacher network q with eq.(4)
transfer knowledge into p   by updating    with eq.(2)

p v.s. q at test time at test time we can use either the distilled student network p,
or the teacher network q after a    nal projection. our empirical results show that both
models substantially improve over the base network that is trained with only data-label
instances. in general q performs better than p. particularly, q is more suitable when the logic
rules introduce additional dependencies (e.g., spanning over multiple examples), requiring
joint id136. in contrast, as mentioned above, p is more lightweight and e   cient, and
useful when rule evaluation is expensive or impossible at prediction time. our experiments
compare the performance of p and q extensively.

imitation strength    the imitation parameter    in eq.(2) balances between emulating
the teacher soft predictions and predicting the true hard labels. since the teacher network
is constructed from p  , which, at the beginning of training, would produce low-quality
predictions, we thus favor predicting the true labels more at initial stage. as training goes
on, we gradually bias towards emulating the teacher predictions to e   ectively distill the
structured knowledge. speci   cally, we de   ne   (t) = min{  0, 1       t} at iteration t     0,
where        1 speci   es the speed of decay and   0 < 1 is a lower bound.

4 applications

we have presented our framework that is general enough to improve various types of neural
networks with rules, and easy to use in that users are allowed to impose their knowledge and
intentions through the declarative    rst-order logic. in this section we illustrate the versatil-
ity of our approach by applying it on two workhorse network architectures, i.e., convolutional
network and recurrent network, on two representative applications, i.e., sentence-level sen-
timent analysis which is a classi   cation problem, and id39 which is a
sequence learning problem.

for each task, we    rst brie   y describe the base neural network. since we are not focusing
on tuning network architectures, we largely use the same or similar networks to previous
successful neural models. we then design the linguistically-motivated rules to be integrated.

7

figure 2: left: the id98 architecture for sentence-level id31. the sentence
representation vector is followed by a fully-connected layer with softmax output activa-
tion, to output sentiment predictions. right: the architecture of the bidirectional lstm
recurrent network for ner. the id98 for extracting character representation is omitted.

4.1 sentiment classi   cation

sentence-level id31 is to identify the sentiment (e.g., positive or negative)
underlying an individual sentence. the task is crucial for many opinion mining applications.
one challenging point of the task is to capture the contrastive sense (e.g., by conjunction
   but   ) within a sentence.

base network we use the single-channel convolutional network proposed in (kim, 2014).
the simple model has achieved compelling performance on various sentiment classi   cation
benchmarks. the network contains a convolutional layer on top of word vectors of a given
sentence, followed by a max-over-time pooling layer and then a fully-connected layer with
softmax output activation. a convolution operation is to apply a    lter to word windows.
multiple    lters with varying window sizes are used to obtain multiple features. figure 2,
left panel, shows the network architecture.

logic rules one di   culty for the plain neural network is to identify contrastive sense
in order to capture the dominant sentiment precisely. the conjunction word    but    is one
of the strong indicators for such sentiment changes in a sentence, where the sentiment of
clauses following    but    generally dominates. we thus consider sentences s with an    a-
but-b    structure, and expect the sentiment of the whole sentence to be consistent with the
sentiment of clause b. the logic rule is written as:
has-   a-but-b   -structure(s)    
(1(y = +)         (b)+         (b)+     1(y = +)) ,

(5)

where 1(  ) is an indicator function that takes 1 when its argument is true, and 0 otherwise;
class    +    represents    positive   ; and     (b)+ is the element of     (b) for class    +   . by eq.(1),
when s has the    a-but-b    structure, the truth value of the above logic rule equals to

8

ilikethisbookstorealotpaddingpaddingid27convolutionmax poolingsentence representationchar+wordrepresentationbackwardlstmforwardlstmlstmlstmlstmlstmlstmlstmlstmlstmoutputrepresentationnyclocatesinusa(1 +     (b)+)/2 when y = +, and (2         (b)+)/2 otherwise 1. note that here we assume
two-way classi   cation (i.e., positive and negative), though it is straightforward to design
rules for    ner grained sentiment classi   cation.

4.2 id39

ner is to locate and classify elements in text into entity categories such as    persons   
and    organizations   . it is an essential    rst step for downstream language understanding
applications. the task assigns to each word a named entity tag in an    x-y    format where
x is one of bieos (beginning, inside, end, outside, and singleton) and y is the entity
category. a valid tag sequence has to follow certain constraints by the de   nition of the
tagging scheme. besides, text with structures (e.g., lists) within or across sentences can
usually expose some consistency patterns.

base network the base network has a similar architecture with the bi-directional lstm
recurrent network (called blstm-id98) proposed in (chiu and nichols, 2015) for ner
which has outperformed most of previous neural models. the model uses a id98 and pre-
trained word vectors to capture character- and word-level information, respectively. these
features are then fed into a bi-directional id56 with lstm units for sequence tagging.
compared to (chiu and nichols, 2015) we omit the character type and capitalization fea-
tures, as well as the additive transition matrix in the output layer. figure 2, right panel,
shows the network architecture.

logic rules the base network largely makes independent tagging decisions at each po-
sition, ignoring the constraints on successive labels for a valid tag sequence (e.g., i-org
cannot follow b-per). in contrast to recent work (lample et al., 2016) which adds a con-
ditional random    eld (crf) to capture bi-gram dependencies between outputs, we instead
apply logic rules which does not introduce extra parameters to learn. an example rule is:

equal(yi   1, i-org)        equal(yi, b-per)

(6)

the con   dence levels are set to     to prevent any violation.

we further leverage the list structures within and across sentences of the same documents.
speci   cally, named entities at corresponding positions in a list are likely to be in the same
categories. for instance, in    1. juventus, 2. barcelona, 3. ...    we know    barcelona    must
be an organization rather than a location, since its counterpart entity    juventus    is an
organization. we describe our simple procedure for identifying lists and counterparts in the
supplementary materials. the logic rule is encoded as:

(7)
1replacing     with & in eq.(5) leads to a probably more intuitive rule which takes the

is-counterpart(x, a)     1     (cid:107)c(ey)     c(    (a))(cid:107)2,

value     (b)+ when y = +, and 1         (b)+ otherwise.

9

where ey is the one-hot encoding of y (the class prediction of x); c(  ) collapses the proba-
bility mass on the labels with the same categories into a single id203, yielding a vector
with length equaling to the number of categories. we use (cid:96)2 distance as a measure for the
closeness between predictions of x and its counterpart a. note that the distance takes
value in [0, 1] which is a proper soft truth value. the list rule can span multiple sentences
(within the same document). we found the teacher network q that enables explicit joint
id136 provides much better performance over the distilled student network p (section 5).

5 experiments

we validate our framework by evaluating its applications of sentiment classi   cation and
id39 on a variety of public benchmarks. by integrating the simple yet
e   ective rules with the base networks, we obtain substantial improvements on both tasks
and achieve state-of-the-art or comparable results to previous best-performing systems.
comparison with a diverse set of other rule integration methods demonstrates the unique
e   ectiveness of our framework. our approach also shows promising potentials in the semi-
supervised learning and sparse data context.

throughout the experiments we set the id173 parameter to c = 6. in sentiment
classi   cation we set the imitation parameter to   (t) = 1     0.95t, while in ner   (t) =
min{0.9, 1   0.9t} to downplay the noisy listing rule. the con   dence levels of rules are set to
  l = 1, except for hard constraints whose con   dence is    . for neural network con   guration,
we largely followed the reference work, as speci   ed in the following respective sections. all
experiments were performed on a linux machine with eight 4.0ghz cpu cores, one tesla
k40c gpu, and 32gb ram. we implemented neural networks using theano 2, a popular
deep learning platform.

5.1 sentiment classi   cation

5.1.1 setup

we test our method on a number of commonly used benchmarks, including 1) sst2,
stanford sentiment treebank (socher et al., 2013) which contains 2 classes (negative and
positive), and 6920/872/1821 sentences in the train/dev/test sets respectively. following
(kim, 2014) we train models on both sentences and phrases since all labels are provided.
2) mr (pang and lee, 2005), a set of 10,662 one-sentence movie reviews with negative
or positive sentiment. 3) cr (hu and liu, 2004), customer reviews of various products,
containing 2 classes and 3,775 instances. for mr and cr, we use 10-fold cross validation
as in previous work. in each of the three datasets, around 15% sentences contains the word
   but   .

2http://deeplearning.net/software/theano

10

model

1 id98 (kim, 2014)
2 id98-rule-p
3 id98-rule-q

4 mgnc-id98 (zhang et al., 2016)
5 mvid98 (yin and schutze, 2015)
6 id98-multichannel (kim, 2014)
7 paragraph-vec (le and mikolov, 2014)
8 crf-pr (yang and cardie, 2014)
9 rntn (socher et al., 2013)

10 g-dropout (wang and manning, 2013)

sst2

87.2
88.8
89.3

88.4
89.4
88.1
87.8
   
85.4
   

mr
81.3  0.1
81.6  0.1
81.7  0.1
   
   
81.1
   
   
   
79.0

cr
84.3  0.2
85.0  0.3
85.3  0.3
   
   
85.0
   
82.7
   
82.1

table 1: accuracy (%) of sentiment classi   cation. row 1, id98 (kim, 2014) is the base
network corresponding to the    id98-non-static    model in (kim, 2014). rows 2-3 are the
networks enhanced by our framework: id98-rule-p is the student network and id98-rule-
q is the teacher network. for mr and cr, we report the average accuracy  one standard
deviation using 10-fold cross validation.

for the base neural network we use the    non-static    version in (kim, 2014) with the exact
same con   gurations. speci   cally, word vectors are initialized using id97 (mikolov et al.,
2013) and    ne-tuned throughout training, and the neural parameters are trained using sgd
with the adadelta update rule (zeiler, 2012).

5.1.2 results

table 1 shows the sentiment classi   cation performance. rows 1-3 compare the base neural
model with the models enhanced by our framework with the    but   -rule (eq.(5)). we see
that our method provides a strong boost on accuracy over all three datasets. the teacher
network q further improves over the student network p, though the student network is more
widely applicable in certain contexts as discussed in sections 3.2 and 3.4. rows 4-10 show
the accuracy of recent top-performing methods. on the mr and cr datasets, our model
outperforms all the baselines. on sst2, mvid98 (yin and schutze, 2015) (row 5) is
the only system that shows a slightly better result than ours. their neural network has
combined diverse sets of pre-trained id27s (while we use only id97) and
contained more neural layers and parameters than our model.

to further investigate the e   ectiveness of our framework in integrating structured rule
knowledge, we compare with an extensive array of other possible integration approaches.
table 2 lists these methods and their performance on the sst2 task. we see that: 1)
although all methods lead to di   erent degrees of improvement, our framework outperforms
all other competitors with a large margin. 2) in particular, compared to the pipelined
method in row 6 which is in analogous to the structure compilation work (liang et al., 2008),
our iterative distillation (section 3.2) provides better performance. another advantage of
our method is that we only train one set of neural parameters, as opposed to two separate

11

model

accuracy (%)

1 id98 (kim, 2014)
2
3
4
5
6

-but-clause
-(cid:96)2-reg
-project
-opt-project
-pipeline

7
8

-rule-p
-rule-q

87.2
87.3
87.5
87.9
88.3
87.9

88.8
89.3

table 2: performance of di   erent rule integration methods on sst2. 1) id98 is the base
network; 2)    -but-clause    takes the clause after    but    as input; 3)    -(cid:96)2-reg    imposes a
id173 term   (cid:107)    (s)         (y )(cid:107)2 to the id98 objective, with the strength    selected
on dev set; 4)    -project    projects the trained base id98 to the rule-regularized subspace
with eq.(3); 5)    -opt-project    directly optimizes the projected id98; 6)    -pipeline    distills
the pre-trained    -opt-project    to a plain id98; 7-8)    -rule-p    and    -rule-q    are our models
with p being the distilled student network and q the teacher network. note that    -but-
clause    and    -(cid:96)2-reg    are ad-hoc methods applicable speci   cally to the    but   -rule.

sets as in the pipelined approach. 3) the distilled student network    -rule-p    achieves much
superior accuracy compared to the base id98, as well as    -project    and    -opt-project    which
explicitly project id98 to the rule-constrained subspace. this validates that our distillation
procedure transfers the structured knowledge into the neural parameters e   ectively. the
inferior accuracy of    -opt-project    can be partially attributed to the poor performance of its
neural network part which achieves only 85.1% accuracy and leads to inaccurate evaluation
of the    but   -rule in eq.(5).

we next explore the performance of our framework with varying numbers of labeled in-
stances as well as the e   ect of exploiting unlabeled data.
intuitively, with less labeled
examples we expect the general rules would contribute more to the performance, and unla-
beled data should help better learn from the rules. this can be a useful property especially
when data are sparse and labels are expensive to obtain. table 3 shows the results. the
subsampling is conducted on the sentence level. that is, for instance, in    5%    we    rst
selected 5% training sentences uniformly at random, then trained the models on these sen-
tences as well as their phrases. the results verify our expectations. 1) rows 1-3 give
the accuracy of using only data-label subsets for training. in every setting our methods
consistently outperform the base id98. 2)    -rule-q    provides higher improvement on 5%
data (with margin 2.6%) than on larger data (e.g., 2.3% on 10% data, and 2.0% on 30%
data), showing promising potential in the sparse data context. 3) by adding unlabeled
instances for semi-supervised learning as in rows 5-6, we get further improved accuracy.
4) row 4,    -semi-pr    is the posterior id173 (ganchev et al., 2010) which imposes
the rule constraint through only unlabeled data during training. our distillation framework
consistently provides substantially better results.

12

data size

1 id98
2
3

-rule-p
-rule-q

4
5
6

-semi-pr
-semi-rule-p
-semi-rule-q

5%

79.9
81.5
82.5

81.5
81.7
82.7

10% 30% 100%

81.6
83.2
83.9

83.1
83.3
84.2

83.6
84.5
85.6

84.6
84.7
85.7

87.2
88.8
89.3

   
   
   

table 3: accuracy (%) on sst2 with varying sizes of labeled data and semi-supervised
learning. the header row is the percentage of labeled examples for training. rows 1-3
use only the supervised data. rows 4-6 use semi-supervised learning where the remaining
training data are used as unlabeled examples. for    -semi-pr    we only report its projected
solution (in analogous to q) which performs better than the non-projected one (in analogous
to p).

model

1 blstm
2 blstm-rule-trans
3 blstm-rules

f1

89.55
p: 89.80, q: 91.11
p: 89.93, q: 91.18

4 nn-lex (collobert et al., 2011)
5
s-lstm (lample et al., 2016)
6 blstm-lex (chiu and nichols, 2015)
7 blstm-crf1 (lample et al., 2016)
8
9 blstm-crf2 (ma and hovy, 2016)

joint-ner-el (luo et al., 2015)

89.59
90.33
90.77
90.94
91.20
91.21

table 4: performance of ner on conll-2003. row 2, blstm-rule-trans imposes the
transition rules (eq.(6)) on the base blstm. row 3, blstm-rules further incorporates
the list rule (eq.(7)). we report the performance of both the student model p and the
teacher model q.

5.2 id39

5.2.1 setup

we evaluate on the well-established conll-2003 ner benchmark (tjong kim sang and
de meulder, 2003), which contains 14,987/3,466/3,684 sentences and 204,567/51,578/46,666
tokens in train/dev/test sets, respectively. the dataset includes 4 categories, i.e., person,
location, organization, and misc. bioes tagging scheme is used. around 1.7% named
entities occur in lists.

we use the mostly same con   gurations for the base blstm network as in (chiu and
nichols, 2015), except that, besides the slight architecture di   erence (section 4.2), we apply
adadelta for parameter updating. glove (pennington et al., 2014) word vectors are used
to initialize word features.

13

5.2.2 results

table 4 presents the performance on the ner task. by incorporating the bi-gram transi-
tion rules (row 2), the joint teacher model q achieves 1.56 improvement in f1 score that
outperforms most previous neural based methods (rows 4-7), including the blstm-crf
model (lample et al., 2016) which applies a conditional random    eld (crf) on top of a
blstm in order to capture the transition patterns and encourage valid sequences. in con-
trast, our method implements the desired constraints in a more straightforward way by using
the declarative logic rule language, and at the same time does not introduce extra model
parameters to learn. further integration of the list rule (row 3) provides a second boost
in performance, achieving an f1 score very close to the best-performing systems including
joint-ner-el (luo et al., 2015) (row 8), a probabilistic graphical model optimizing ner
and entity linking jointly with massive external resources, and blstm-crf (ma and hovy,
2016), a combination of blstm and crf with more parameters than our rule-enhanced
neural networks.

from the table we see that the accuracy gap between the joint teacher model q and the
distilled student p is relatively larger than in the sentiment classi   cation task (table 1).
this is because in the ner task we have used logic rules that introduce extra dependen-
cies between adjacent tag positions as well as multiple instances, making the explicit joint
id136 of q useful for ful   lling these structured constraints.

6 discussion and future work

we have developed a framework which combines deep neural networks with    rst-order
logic rules to allow integrating human knowledge and intentions into the neural models.
in particular, we proposed an iterative distillation procedure that transfers the structured
information of logic rules into the weights of neural networks. the transferring is done via a
teacher network constructed using the posterior id173 principle. our framework is
general and applicable to various types of neural architectures. with a few intuitive rules,
our framework signi   cantly improves base networks on id31 and named entity
recognition, demonstrating the practical signi   cance of our approach.

though we have focused on    rst-order logic rules, we leveraged soft logic formulation which
can be easily extended to general probabilistic models for expressing structured distributions
and performing id136 and reasoning (lake et al., 2015). we plan to explore these diverse
id99s to guide the dnn learning. the proposed iterative distillation
procedure also reveals connections to recent neural autoencoders (kingma and welling,
2014; rezende et al., 2014) where generative models encode probabilistic structures and
neural recognition models distill the information through iterative optimization (rezende
et al., 2016; johnson et al., 2016; karaletsos et al., 2016).

the encouraging empirical results indicate a strong potential of our approach for improving
other application domains such as vision tasks, which we plan to explore in the future.

14

finally, we also would like to generalize our framework to automatically learn the con   dence
of di   erent rules, and derive new rules from data.

acknowledgments

we thank the anonymous reviewers for their valuable comments. this work is supported
by nsf iis1218282, nsf iis1447676, air force fa8721-05-c-0003, and fa8750-12-2-0342.

15

references

bach, s. h., broecheler, m., huang, b., and getoor, l. (2015). hinge-loss markov random

   elds and probabilistic soft logic. arxiv preprint arxiv:1505.04406.

bahdanau, d., cho, k., and bengio, y. (2014). id4 by jointly

learning to align and translate. proc. of iclr.

bucilu, c., caruana, r., and niculescu-mizil, a. (2006). model compression. in proc. of

kdd, pages 535   541. acm.

chiu, j. p. and nichols, e. (2015). id39 with bidirectional lstm-

id98s. arxiv preprint arxiv:1511.08308.

collobert, r., weston, j., bottou, l., karlen, m., kavukcuoglu, k., and kuksa, p. (2011).

natural language processing (almost) from scratch. jmlr, 12:2493   2537.

foulds, j., kumar, s., and getoor, l. (2015). latent topic networks: a versatile proba-

bilistic programming framework for topic models. in proc. of icml, pages 777   786.

fran  ca, m. v., zaverucha, g., and garcez, a. s. d. (2014). fast relational learning using
bottom clause propositionalization with arti   cial neural networks. machine learning,
94(1):81   104.

ganchev, k., gra  ca, j., gillenwater, j., and taskar, b. (2010). posterior id173 for

structured latent variable models. jmlr, 11:2001   2049.

garcez, a. s. d., broda, k., and gabbay, d. m. (2012). neural-symbolic learning systems:

foundations and applications. springer science & business media.

hinton, g., deng, l., yu, d., dahl, g. e., mohamed, a.-r., jaitly, n., senior, a., van-
houcke, v., nguyen, p., sainath, t. n., et al. (2012). deep neural networks for acoustic
modeling in id103: the shared views of four research groups. signal pro-
cessing magazine, ieee, 29(6):82   97.

hinton, g., vinyals, o., and dean, j. (2015). distilling the knowledge in a neural network.

arxiv preprint arxiv:1503.02531.

hu, m. and liu, b. (2004). mining and summarizing customer reviews. in proc. of kdd,

pages 168   177. acm.

johnson, m. j., duvenaud, d., wiltschko, a. b., datta, s. r., and adams, r. p. (2016).
structured vaes: composing probabilistic id114 and variational autoen-
coders. arxiv preprint arxiv:1603.06277.

karaletsos, t., belongie, s., tech, c., and r  atsch, g. (2016). bayesian representation

learning with oracle constraints. in proc. of iclr.

kim, y. (2014). convolutional neural networks for sentence classi   cation. proc. of emnlp.

kingma, d. p. and welling, m. (2014). auto-encoding id58. in proc. of iclr.

16

krizhevsky, a., sutskever, i., and hinton, g. e. (2012). id163 classi   cation with deep

convolutional neural networks. in proc. of nips, pages 1097   1105.

kulkarni, t. d., whitney, w. f., kohli, p., and tenenbaum, j. (2015). deep convolutional

inverse graphics network. in proc. of nips, pages 2530   2538.

lake, b. m., salakhutdinov, r., and tenenbaum, j. b. (2015). human-level concept learning

through probabilistic program induction. science, 350(6266):1332   1338.

lample, g., ballesteros, m., subramanian, s., kawakami, k., and dyer, c. (2016). neural

architectures for id39. in proc. of naacl.

le, q. v. and mikolov, t. (2014). distributed representations of sentences and documents.

proc. of icml.

liang, p., daum  e iii, h., and klein, d. (2008). structure compilation: trading structure

for features. in proc. of icml, pages 592   599. acm.

liang, p., jordan, m. i., and klein, d. (2009). learning from measurements in exponential

families. in proc. of icml, pages 641   648. acm.

lopez-paz, d., bottou, l., sch  olkopf, b., and vapnik, v. (2016). unifying distillation and

privileged information. prof. of iclr.

luo, g., huang, x., lin, c.-y., and nie, z. (2015). joint id39 and

disambiguation. in proc. of emnlp.

ma, x. and hovy, e. (2016). end-to-end sequence labeling via bi-directional lstm-id98s-

crf. in proc. of acl.

mikolov, t., sutskever, i., chen, k., corrado, g. s., and dean, j. (2013). distributed
representations of words and phrases and their compositionality. in proc. of nips, pages
3111   3119.

minksy, m. (1980). learning meaning. technical report ai lab memo. project mac. mit.

nguyen, a., yosinski, j., and clune, j. (2015). deep neural networks are easily fooled:
high con   dence predictions for unrecognizable images. in proc. of cvpr, pages 427   
436. ieee.

pang, b. and lee, l. (2005). seeing stars: exploiting class relationships for sentiment

categorization with respect to rating scales. in proc. of acl, pages 115   124.

pennington, j., socher, r., and manning, c. d. (2014). glove: global vectors for word

representation. in proc. of emnlp, volume 14, pages 1532   1543.

rezende, d. j., mohamed, s., danihelka, i., gregor, k., and wierstra, d. (2016). one-shot

generalization in deep generative models. arxiv preprint arxiv:1603.05106.

rezende, d. j., mohamed, s., and wierstra, d. (2014). stochastic id26 and

approximate id136 in deep generative models. proc. of icml.

17

richardson, m. and domingos, p. (2006). markov logic networks. machine learning, 62(1-

2):107   136.

silver, d., huang, a., maddison, c. j., guez, a., sifre, l., van den driessche, g., schrit-
twieser, j., antonoglou, i., panneershelvam, v., lanctot, m., et al. (2016). mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484   489.

socher, r., perelygin, a., wu, j. y., chuang, j., manning, c. d., ng, a. y., and potts, c.
(2013). recursive deep models for semantic compositionality over a sentiment treebank.
in proc. of emnlp, volume 1631, page 1642. citeseer.

szegedy, c., zaremba, w., sutskever, i., bruna, j., erhan, d., goodfellow, i., and fergus,

r. (2014). intriguing properties of neural networks. proc. of iclr.

tjong kim sang, e. f. and de meulder, f. (2003). introduction to the conll-2003 shared
task: language-independent id39. in proc. of conll, pages 142   
147. association for computational linguistics.

towell, g. g., shavlik, j. w., and noordewier, m. o. (1990). re   nement of approxi-
mate domain theories by knowledge-based neural networks. in proceedings of the eighth
national conference on arti   cial intelligence, pages 861   866. boston, ma.

wang, s. and manning, c. (2013). fast dropout training. in proc. of icml, pages 118   126.

yang, b. and cardie, c. (2014). context-aware learning for sentence-level id31

with posterior id173. in proc. of acl, pages 325   335.

yin, w. and schutze, h. (2015). multichannel variable-size convolution for sentence classi-

   cation. proc. of conll.

zeiler, m. d. (2012). adadelta: an adaptive learning rate method.

arxiv preprint

arxiv:1212.5701.

zhang, y., roller, s., and wallace, b. (2016). mgnc-id98: a simple approach to exploiting

multiple id27s for sentence classi   cation. proc. of naacl.

zhu, j., chen, n., and xing, e. p. (2014). bayesian id136 with posterior id173

and applications to in   nite latent id166s. jmlr, 15(1):1799   1847.

18

a appendix

a.1 solving problem eq.(3), section 3.3

we provide the detailed derivation for solving the problem in eq.(3), section 3.3, which we
repeat here:

min
q,     0

(cid:88)
kl(q(y|x)(cid:107)p  (y|x)) + c
s.t.   l(1     eq[rl,gl(x, y)])       l,gl
l = 1, . . . , l,

gl = 1, . . . , gl,

l,gl

  l,gl

the following derivation is largely adapted from (ganchev et al., 2010) for the logic rule
constraint setting, with some reformulation that produces closed-form solution.

the lagrangian is

where

(cid:88)

max

     0,     0,     0

min
q(y),  

l,

(a.2)

l = kl(q(y|x)(cid:107)p  (y|x)) +

(cid:88)

(c +   l,gl)  l,gl

(cid:88)
  l,gl (eq[  l(1     rl,gl(x, y))]       l,gl) +   (

l,gl

+

l,gl

y

q(y|x)     1)

(a.3)

solving eq.(a.2), we obtain

   ql = log q(y|x) + 1     log p  (y|x) +

(cid:88)

=   

q(y|x) =

p  (y|x) exp{   (cid:80)

  l,gl [  l(1     rl,gl(x, y))] +    = 0
l   l  l(1     rl,gl(x, y))}

l,gl

e exp(  )

(a.1)

(a.4)

(a.5)

     l,gl

l = c +   l,gl       l,gl = 0

(cid:88)

y

     l =

p  (y|x) exp

(cid:110)   (cid:80)

=   

   = log

=   

  l,gl =   l,gl     c
(cid:111)
  l,gl  l(1     rl,gl(x, y))
(cid:110)   (cid:80)

y p(y|x) exp

    1 = 0

(cid:111)
  l,gl  l(1     rl,gl(x, y))

l,gl
e exp(  )

      (cid:80)

      

l,gl
e

19

(a.6)

let z   =(cid:80)

y p(y|x) exp

(cid:110)   (cid:80)

l,gl

(cid:111)
  l,gl  l(1     rl,gl(x, y))
(cid:88)
(c +   l,gl)  l,gl    (cid:88)

  l,gl  l,gl

. plugging    into l

l =     log z   +

l,gl

l,gl

(a.7)

=     log z  

since z   monotonically decreases as    increases, and from eq.(a.6) we have   l,gl     c,
therefore:

max
c        0

    log z  
=   

     

l,gl

= c

(a.8)

plugging eqs.(a.6) and (a.8) into eq.(a.5) we obtain the solution of q as in eq.(4).

a.2 identifying lists for ner

we design a simple pattern-matching based method to identify lists and counterparts in the
ner task. we ensure high precision and do not expect high recall. in particular, we only
retrieve lists that with the pattern    1. ... 2. ... 3. ...    (i.e., indexed by numbers), and    - ...
- ... - ...    (i.e., each item marked with    -   ). we require at least 3 items to form a list.

we further require the text of each item follows certain patterns to ensure the text is
highly likely to be named entities, and rule out those lists whose item text is largely free
text. speci   cally, we require 1) all words of the item text all start with capital letters; 2)
referring the text between punctuations as    block   , each block includes no more than 3
words.

we detect both intra-sentence lists and inter-sentence lists in documents. we found the
above patterns are e   ective to identify true lists. a better list detection method is expected
to further improve our ner results.

20

