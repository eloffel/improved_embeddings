recurrent neural network encoder with attention for community

id53

wei-ning hsu and yu zhang and james glass

computer science and arti   cial intelligence laboratory

massachusetts institute of technology

cambridge, ma 02139, usa

{wnhsu,yzhang87,jrg}@csail.mit.edu

6
1
0
2

 
r
a

 

m
3
2

 
 
]
l
c
.
s
c
[
 
 

1
v
4
4
0
7
0

.

3
0
6
1
:
v
i
x
r
a

abstract

we apply a general recurrent neural net-
work (id56) encoder framework to commu-
nity id53 (cqa) tasks. our ap-
proach does not rely on any linguistic pro-
cessing, and can be applied to different lan-
guages or domains. further improvements are
observed when we extend the id56 encoders
with a neural attention mechanism that en-
courages reasoning over entire sequences. to
deal with practical issues such as data spar-
sity and imbalanced labels, we apply vari-
ous techniques such as id21 and
multitask learning. our experiments on the
semeval-2016 cqa task show 10% improve-
ment on a map score compared to an infor-
mation retrieval-based approach, and achieve
comparable performance to a strong hand-
crafted feature-based method.

is

introduction

in the past decade,

1
community id53 (cqa)
a
paradigm that provides forums for users to ask
or answer questions on any topic with barely any
restrictions.
these websites
have attracted a great number of users, and have
accumulated a large collection of question-comment
threads generated by these users. however,
the
low restriction results in a high variation in answer
quality, which makes it time-consuming to search
for useful information from the existing content. it
would therefore be valuable to automate the pro-
cedure of ranking related questions and comments
for users with a new question, or when looking for
solutions from comments of an existing question.

automation of cqa forums can be divided into
three tasks: question-comment relevance (task a),
question-question relevance (task b), and question-
external comment relevance (task c). one might
think that classic retrieval models like language
models for information retrieval (zhai and lafferty,
2004) could solve these tasks. however, a big
challenge for cqa tasks is that users are used to
expressing similar meanings with different words,
which creates gaps when matching questions based
on common words. other challenges include in-
formal usage of language, highly diverse content of
comments, and variation in the length of both ques-
tions and comments.

to overcome these issues, most previous work
(e.g. semeval 2015 (nakov et al., 2015)) relied
heavily on additional features and reasoning capa-
in (rockt  aschel et al., 2015), a neural
bilities.
attention-based model was proposed for automati-
cally recognizing entailment relations between pairs
of natural language sentences. in this study, we    rst
modify this model for all three cqa tasks. we also
extend this framework into a jointly trained model
when the external resources are available, i.e. select-
ing an external comment when we know the ques-
tion that the external comment answers (task c).

our ultimate objective is to classify relevant
questions and comments without complicated hand-
crafted features. by applying id56-based encoders,
we avoid heavily engineered features and learn the
representation automatically. in addition, an atten-
tion mechanism augments encoders with the ability
to attend to past outputs directly. this becomes help-
ful when encoding longer sequences, since we no

longer need to compress all information into a    xed-
length vector representation.

in our view, existing annotated cqa corpora are
generally too small to properly train an end-to-end
neural network. to address this, we investigate
id21 by pretraining the recurrent sys-
tems on other corpora, and also generating addi-
tional instances from existing cqa corpus.

2 related work

earlier work of community id53 re-
lied heavily on feature engineering, linguistic tools,
and external resource. (jeon et al., 2006) and (shah
and pomerantz, 2010) utilized rich non-textual fea-
(grundstr  om and
tures such as answer   s pro   le.
nugues, 2014) syntactically analyzed the question
and extracted name entity features. (harabagiu and
hickl, 2006) demonstrated a id123 sys-
tem can enhance cqa task by casting question an-
swering to logical entailment.

more recent work incorporated word vector into
their feature extraction system and based on it de-
signed different distance metric for question and an-
swer (tran et al., 2015) (belinkov et al., 2015).
while these approaches showed effectiveness, it is
dif   cult to generalize them to common cqa tasks
since linguistic tools and external resource may be
restrictive in other languages and features are highly
customized for each cqa task.

very recent work on answer selection also in-
volved the use of neural networks. (wang and ny-
berg, 2015) used lstm to construct a joint vector
based on both the question and the answer and then
converted it into a learning to rank problem. (feng
et al., 2015) proposed several convolutional neural
network (id98) architectures for cqa. our method
differs in that id56 encoder is applied here and by
adding attention mechanism we jointly learn which
words in question to focus and hence available to
conduct qualitative analysis. during classi   cation,
we feed the extracted vector into a feed-forward
neural network directly instead of using mean/max
pooling on top of each time steps.

3 method

in this section, we    rst discuss long short-term mem-
ory (lstm) units and an associated attention mech-

anism. next, we explain how we can encode a
pair of sentences into a dense vector for predict-
ing relationships using an lstm with an attention
mechanism. finally, we apply these models to pre-
dict question-question similarity, question-comment
similarity, and question-external comment similar-
ity.

3.1 lstm models
lstms have shown great success in many differ-
ent    elds. an lstm unit contains a memory cell
with self-connections, as well as three multiplicative
gates to control information    ow. given input vector
xt, previous hidden outputs ht   1, and previous cell
state ct   1, lstm units operate as follows:

x =(cid:34) xt
ht   1(cid:35)

it =   (wixx + wicct   1 + bi)
ft =   (wfxx + wfcct   1 + bf )
ot =   (woxx + wocct   1 + bo)
ct = ft (cid:12) ct   1 + it (cid:12) tanh(wcxx + bc)
ht = ot (cid:12) tanh(ct)

(1)

(2)
(3)
(4)
(5)
(6)

where it, ft, ot are input, forget, and output gates,
respectively. the sigmoid function   () is a soft gate
function controlling the amount of information    ow.
w s and bs are model parameters to learn.

traditional

encoder-decoder

3.2 neural attention
a
ap-
id56
proach (sutskever et al., 2014)    rst encodes an
arbitrary length input sequence into a    xed-length
dense vector that can be used as input to subsequent
classi   cation models, or to initialize the hidden state
of a secondary decoder. however, the requirement
to compress all necessary information into a single
   xed length vector can be problematic. a neural
attention model (bahdanau et al., 2014) (cho et
al., 2014) has been recently proposed to alleviate
this issue by enabling the network to attend to
past outputs when decoding. thus, the encoder no
longer needs to represent an entire sequence with
one vector;
it encodes information into
a sequence of vectors, and adaptively chooses a
subset of the vectors when decoding.

instead,

figure 1: id56 encoder for related question/comment selection.

figure 2: neural attention model for related question/comment selection.

3.3 predicting relationships of object pairs

with an attention model

in our cqa tasks, the pair of objects are (question,
question) or (question, comment), and the relation-
ship is relevant/irrelevant. the left side of figure 1
shows one intuitive way to predict relationships us-
ing id56s. parallel lstms encode two objects inde-
pendently, and then concatenate their outputs as an
input to a feed-forward neural network (fnn) with
a softmax output layer for classi   cation.

the representations of the two objects are gener-
ated independently in this manner. however, we are
more interested in the relationship instead of the ob-
ject representations themselves. therefore, we con-
sider a serialized lstm-encoder model in the right
side of figure 1 that is similar to that in (rockt  aschel
et al., 2015), but also allows an augmented feature
input to the fnn classi   er.

figure 2 illustrates our attention framework in
more detail. the    rst lstm reads one object, and
passes information through hidden units to the sec-
ond lstm. the second lstm then reads the other

object and generates the representation of this pair
after the entire sequence is processed. we build an-
other fnn that takes this representation as input to
classify the relationship of this pair.

by adding an attention mechanism to the encoder,
we allow the second lstm to attend to the sequence
of output vectors from the    rst lstm, and hence
generate a weighted representation of    rst object ac-
cording to both objects. let hn be the last output
of second lstm and m = [h1, h2,       , hl] be the
sequence of output vectors of the    rst object. the
weighted representation of the    rst object is

(cid:48)
h

=

l(cid:88)i=1

  ihi

the weight is computed by

exp(a(hi, hn ))
j=1 exp(a(hj, hn ))

  i =

(cid:80)l

where a() is the importance model that produces a
higher score for (hi, hn ) if hi is useful to determine

(7)

(8)

lstm1lstm2relevantirrelevantparallellstmencoderlstm1lstm2relevantirrelevantsequentiallstmencodersharedornot?w1w2wi...q1...qiwhatis...bank:thebestbankin...h1h2hiattention[h0=lxi=1   ihi]hnlstm1lstm2hlaugmentedfeaturesrelevantirrelevantthe object pair   s relationship. we parametrize this
model using another fnn. note that in our frame-
work, we also allow other augmented features (e.g.,
the ranking score from the ir system) to enhance the
classi   er. so the    nal input to the classi   er will be
hn , h(cid:48), as well as augmented features.
3.4 modeling question-external comments
for task c, in addition to an original question (oriq)
and an external comment (relc), the question which
relc commented on is also given (relq). to incor-
porate this extra information, we consider a multi-
task learning framework which jointly learns to pre-
dict the relationships of the three pairs (oriq/relq,
oriq/relc, relq/relc).

figure 3 shows our framework: the three lower
models are separate serialized lstm-encoders for
the three respective object pairs, whereas the upper
model is an fnn that takes as input the concatena-
tion of the outputs of three encoders, and predicts
the relationships for all three pairs. more speci   -
cally, the output layer consists of three softmax lay-
ers where each one is intended to predict the rela-
tionship of one particular pair.

for the overall id168, we combine three
separate id168s using a heuristic weight vec-
tor    that allocates a higher weight to the main task
(oriq-relc relationship prediction) as follows:

l =   1l1 +   2l2 +   3l3

(9)

by doing so, we hypothesize that the related tasks
can improve the main task by leveraging common-
ality among all tasks.

4 experiments
we evaluate our approach on all three cqa tasks.
we use the cqa datasets provided by the semeval
2016 task 1. the cqa data is organized as follows:
there are 267 original questions, each question has
10 related question, and each related question has
10 comments. therefore, for task a, there are a total
number of 26,700 question-comment pairs. for task
b, there are 2,670 question-question pairs. for task
c, there are 26,700 question-comment pairs. the
test dataset includes 50 questions, 500 related ques-
tions and 5,000 comments which do not overlap with

1http://alt.qcri.org/semeval2016/task3

the training set. to evaluate the performance, we use
mean average precision (map) and f1 score.
baseline system: figure 4 illustrates our base-
line systems. the ir-based system is scored by the
google search engine. for each question-comment
pair, or question-question pair, we use google   s rank
to calculate the map. while there is no training on
the target data, we expect that google used many ex-
ternal resources to produce these ranks. the feature-
rich system is that proposed by (belinkov et al.,
2015) in semeval-2015. in this approach, they com-
pute text-based, vector-based, metadata-based and
rank-based features from the pre-processed data.
the features are used by a linear id166 for com-
ment selection. this system includes traditional
handcrafted features, and some id56-based features
(word vectors). it also includes the information from
the ir system (ranked-based). so we believe it is a
strong baseline to compare with our model.
id56 encoder: our
on
theano (bastien et al., 2012; bergstra et al.,
2010). table 1 gives a list of hyper-parameters
we considered. as suggested by (greff et al.,
2015),
the hyper-parameters for lstms can be
tuned independently. we tuned each parameter
separately on a development set (split from the
training set) and simply picked the best setting.
our experiments show that using id27s
from google-news provides modest improvements,
but    xing the embedding degrades performance a
lot. also, using separate parameters for lstms is
better than sharing. for the optimization method,
adadelta converged faster, but adagrad gives
better performance. note that all the parameters
were tuned on task a, and we simply applied them
to task b and c. this is for saving computation, and
also because task a is more well-de   ned compared
to b and c in terms of dataset size and label balance.

system is

based

4.1 preliminary results
table 2 shows the initial results using the id56 en-
coder for different tasks. we observe that the atten-
tion model always gets better results than the id56
without attention, especially for task c. however,
the id56 model achieves a very low f1 score. for
task b, it is even worse than the random baseline.

figure 3: joint learning for external comment selection.

figure 4: ir-based system and feature-rich based system.

task a

task b

task c

model
random

map
0.4860
parallel lstm 0.6123
0.6175
0.6239

seq lstm
w/ attention

f1

0.5004
0.6091
0.6063
0.6323

map
0.5595
0.5553
0.5620
0.5723

f1

0.4691
0.4087
0.4299
0.4334

map
0.1383
0.2413
0.2356
0.2837

f1

0.1277
0.0057
0.0115
0.1449

table 2: the id56 encoder results for cqa tasks (bold is best).

embedding
two lstm

#cells for lstm
# nodes for mlp

optimizer
learning rate
regularizer
dropout rate

l2

init or random,    x or update

shared or not
64, 128, 256

128, 256

adagrad, adadelta, sgd

0.001,0.01,0.1

dropout, l2 id173

0.0, 0.2, 0.3, 0.4, 0.5

0, 0.001, 0.0001, 0.00001

table 1: the hyper-parameters we tuned. terms in
bold represent the selected    nal parameters.

we believe the reason is because for task b, there
are only 2,670 pairs for training which is very lim-
ited training for a reasonable neural network. for
task c, we believe the problem is highly imbalanced
data. since the related comments did not directly
comment on the original question, more than 90% of
the comments are labeled as irrelevant to the original

question. the low f1 (with high precision and low
recall) means our system tends to label most com-
ments as irrelevant. in the following section, we in-
vestigate methods to address these issues.

4.2 robust parameter initialization
one way to improve models trained on limited data
is to use external data to pretrain the neural network.
we therefore considered two different datasets for
this task.

    cross-domain: the stanford natural language
id136 (snli) corpus (bowman et al., 2015)
has a huge amount of cleaned premise and hy-
pothesis pairs. unfortunately the pairs are for
a different task. the relationship between the
premise and hypothesis may be similar to the
relation between questions and comments, but
may also be different.

    in-domain:

since task a seems has reason-

taskataskbtaskcmodelmodelmodeloriqrelcrelqrelqoriqrelcuserrelated q1original qgooglerelated q2related q3rankedrankedir-based systemrelated c1related c2related c3related c10related c1related c2related c3related c10vector-based featurestext-based featuresmetadata-based featuresrank-based featuresfeature-rich based systemfeature extractorid166wordsphrasessentencesentire q&acommon stringjaro second strcosinejaccard coe   task btask atask cable performance, and the network is also well-
trained, we could use it directly to initialize task
b.

to utilize the data, we    rst trained the model on each
auxiliary data (snli or task a) and then removed
the softmax layer. after that, we retrain the network
using the target data with a softmax layer that was
randomly initialized.

for task a, the snli cannot improve map or f1
scores. actually it slightly hurts the performance.
we surmise that it is probably because the domain is
different. further investigation is needed: for exam-
ple, we could only use the parameter for embedding
layers etc. for task b, the snli yields a slight im-
provement on map (0.2%), and task a could give
(1.2%) on top of that. no improvement was ob-
served on f1. for task c, pretraining by task a is
also better than using snli (task a is 1% better than
the baseline, while snli is almost the same).

in summary, the in-domain pretraining seems bet-
ter, but overall, the improvement is less than we ex-
pected, especially for task b, which only has very
limited target data. we will not make a conclusion
here since more investigation is needed.

4.3 multitask learning
as mentioned in section 3.4, we also explored a
multitask learning framework that jointly learns to
predict the relationships of all three tasks. we set
0.8 for the main task (task c) and 0.1 for the other
auxiliary tasks. the map score did not improve,
but f1 increases to 0.1617. we believe this is be-
cause other tasks have more balanced labels, which
improves the shared parameters for task c.

4.4 augmented data
there are many sources of external question-answer
pairs that could be used in our tasks. for exam-
ple: webquestion (was introduced by the authors
of sempre system (berant et al., 2013)) and the
simplequestions dataset 2. all of them are positive
examples for our task and we can easily create neg-
ative examples from it. initial experiments indicate
that it is very easy to over   t these obvious negative
examples. we believe this is because our negative

2http://fb.ai/babi.

examples are non-informative for our task and just
introduce noise.

since the external data seems to hurt the perfor-
mance, we try to use the in-domain pairs to enhance
task b and task c. for task b, if relative question
1 (rel1) and relative question 2 (rel2) are both rele-
vant to the original question, then we add a positive
sample (rel1, rel2, 1). if either rel1 and rel2 is ir-
relevant and the other is relevant, we add a negative
sample (rel1, rel2, 0). after doing this, the samples
of task b increase from 2, 670 to 11, 810. by apply-
ing this method, the map score increased slightly
from 0.5723 to 0.5789 but the f1 score improved
from 0.4334 to 0.5860.

for task c, we used task a   s data directly. the
results are very similar with a slight improvement
on map, but large improvement on f1 score from
0.1449 to 0.2064.

4.5 augmented features

to further enhance the system, we incorporate a one
hot vector of the original ir ranking as an additional
feature into the fnn classi   er. table 3 shows the
results. in comparing the models with and without
augmented features, we can see large improvement
for task b and c. the f1 score for task a degrades
slightly but map improves. this might be because
task a already had a substantial amount of training
data.

4.6 comparison with other systems

table 4 gives the    nal comparison between differ-
ent models (we only list the map score because it
is the of   cial score for the challenge). since the two
baseline models did not use any additional data, in
this table our system was also restricted to the pro-
vided training data. for task a, we can see that if
there is enough training data our single system al-
ready performs better than a very strong feature-rich
based system. for task b, since only limited train-
ing data is given, both feature-rich based system and
our system are worse than the ir system. for task
c, our system also got comparable results with the
feature-rich based system. if we do a simple system
combination (average the rank score) between our
system and the ir system, the combined system will

task a

task b

task c

w/ attention

model

map
0.2837
0.3236
table 3: cqa task results with augmented features (bold is best).

map
0.6239
0.6385

map
0.5723
0.6585

0.6323
0.6218

0.4334
0.5382

f1

f1

w/ attention + aug features

f1

0.1449
0.1963

give large gains on tasks b and c3. this implies that
our system is complimentary with the ir system.

model

ir

attention

feature-rich & ir

attention & ir

task a task b task c
map map map
0.307
0.538
0.324
0.639
0.632
0.339
0.394
0.639

0.714
0.659
0.685
0.717

table 4: compared with other systems (bold is best).

5 analysis of attention mechanism

in addition to quantitative analysis, it is natural to
qualitatively evaluate the performance of the atten-
tion mechanism by visualizing the weight distribu-
tion of each instance. we randomly picked several
instances from the test set in task a, for which the
sentence lengths are more moderate for demonstra-
tion. these examples are shown in figure 5, and
categorized into short, long, and noisy sentences for
discussion. a darker blue patch refers to a larger
weight relative to other words in the same sentence.

5.1 short sentences

figure 5a illustrates two cqa examples whose ques-
tions are relatively short. the comments corre-
sponding to these questions are    ...snorkeling two
days ago off the coast of dukhan...    and    the doha
international airport...   . we can observe that our
model successfully learns to focus on the most rep-
resentative part of the question pertaining to classi-
fying the relationship, which is    place for snorkel-
ing    for the    rst example and    place can ... visited
in qatar    for the second example.

3the feature-rich based system was already combined with

the ir system)

5.2 long sentences
in figure 5b, we investigate two examples with
longer questions, which both contain 63 words. in-
terestingly, the distribution of weights does not be-
come more uniform; the model still focuses atten-
tion on a small number of hot words, for example,
   puppy dog for ... mall    and    hectic driving in doha
... car insurance ... quite costly   . additionally,
some words that appear frequently but carry little in-
formation for classi   cation are assigned very small
weights, such as i/we/my, is/am, like, and to.

5.3 noisy sentence
due to the open nature of cqa forums, some con-
tent is noisy. figure 5c is an example with excessive
usage of question marks. again, our model exhibits
its robustness by allocating very low weights to the
noise symbols and therefore excludes the noninfor-
mative content.

6 conclusion

in this paper, we demonstrate that a general id56 en-
coder framework can be applied to community ques-
tion answering tasks. by adding a neural attention
mechanism, we showed quantitatively and qualita-
tively that attention can improve the id56 encoder
framework. to deal with a more realistic scenario,
we expanded the framework to incorporate metadata
as augmented inputs to a fnn classi   er, and pre-
trained models on larger datasets, increasing both
stability and performance. our model is consistently
better than or comparable to a strong feature-rich
baseline system, and is superior to an ir-based sys-
tem when there is a reasonable amount of training
data.

our model is complimentary with an ir-based
system that uses vast amounts of external resources
but trained for general purposes. by combining
the two systems, it exceeds the feature-rich and ir-
based system in all three tasks.

(a) short sentences

(b) long sentences

figure 5: visualization of attention mechanism on short, long, and noisy sentences.

(c) noisy sentence

moreover, our approach is also language indepen-
dent. we have also performed preliminary experi-
ments on the arabic portion of the semeval-2016
cqa task. the results are competitive with a hand-
tuned strong baseline from semeval-2015.

tention mechanisms instead of attending only when
reading the last word.

references

future work could proceed in two directions:    rst,
we can enrich the existing system by incorporat-
ing available metadata and preprocessing data with
morphological id172 and out-of-vocabulary
mappings; second, we can reinforce our model by
carrying out word-by-word and history-aware at-

[bahdanau et al.2014] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio.
2014. neural machine
translation by jointly learning to align and translate.
arxiv preprint arxiv:1409.0473.

[bastien et al.2012] fr  ed  eric bastien, pascal lamblin,
razvan pascanu, james bergstra, ian j. goodfellow,
arnaud bergeron, nicolas bouchard, and yoshua ben-

sigir conference on research and development in in-
formation retrieval, pages 228   235. acm.

[nakov et al.2015] r. nakov, l. marquez, w. magdy,
a. moschitti, and j. glass. 2015. semeval-2015 task
3: answer selection in community question answer-
ing. in proc. sameval, pages 282   287.

[rockt  aschel et al.2015] tim rockt  aschel,

edward
grefenstette, karl moritz hermann, tom  a  s ko  cisk`y,
and phil blunsom.
2015. reasoning about en-
arxiv preprint
tailment with neural attention.
arxiv:1509.06664.

[shah and pomerantz2010] chirag shah and jefferey
pomerantz. 2010. evaluating and predicting answer
quality in community qa. in proceedings of the 33rd
international acm sigir conference on research and
development in information retrieval, pages 411   418.
acm.

[sutskever et al.2014] ilya sutskever, oriol vinyals, and
quoc vv le. 2014. sequence to sequence learning
with neural networks. in advances in neural informa-
tion processing systems, pages 3104   3112.

[tran et al.2015] quan hung tran, vu tran, tu vu, minh
le nguyen, and son bao pham. 2015. jaist: com-
bining multiple features for answer selection in com-
munity id53. in proceedings of the 9th
international workshop on semantic evaluation, se-
meval, volume 15.

[wang and nyberg2015] di wang and eric nyberg.
2015. a long short-term memory model for answer
sentence selection in id53. in acl.

[zhai and lafferty2004] c. zhai and j. lafferty. 2004. a
study of smoothing methods for language models ap-
plied to information retrieval. in acm trans. inf. syst.

2012. theano: new features and speed im-
gio.
provements. deep learning and unsupervised fea-
ture learning nips 2012 workshop.

[belinkov et al.2015] yonatan belinkov, mitra mo-
2015.
htarami, scott cyphers, and james glass.
vectorslu: a continuous word vector approach to
answer selection in community id53
in proceedings of the 9th international
systems.
workshop on semantic evaluation, semeval, vol-
ume 15.

[berant et al.2013] j. berant, a. chou, r. frostig, and
p. liang. 2013. id29 on freebase from
question-answer pairs. in emnlp.

[bergstra et al.2010] james bergstra, olivier breuleux,
fr  ed  eric bastien, pascal lamblin, razvan pascanu,
guillaume desjardins, joseph turian, david warde-
farley, and yoshua bengio. 2010. theano: a cpu and
gpu math expression compiler. in proceedings of the
python for scienti   c computing conference (scipy),
june. oral presentation.

[bowman et al.2015] s. bowman, g. angeli, c. potts,
and c. manning. 2015. a large annotated corpus for
learning natural language id136. in emnlp.

[cho et al.2014] kyunghyun cho, bart van merri  enboer,
caglar gulcehre, dzmitry bahdanau, fethi bougares,
holger schwenk, and yoshua bengio. 2014. learn-
ing phrase representations using id56 encoder-decoder
arxiv preprint
for id151.
arxiv:1406.1078.

[feng et al.2015] minwei feng, bing xiang, michael r
glass, lidan wang, and bowen zhou. 2015. apply-
ing deep learning to answer selection: a study and an
open task. arxiv preprint arxiv:1508.01585.

[greff et al.2015] klaus greff, rupesh kumar srivastava,
jan koutn    k, bas r. steunebrink, and j  urgen schmid-
huber. 2015. lstm: a search space odyssey. corr,
abs/1503.04069.

[grundstr  om and nugues2014] jakob grundstr  om and
pierre nugues. 2014. using syntactic features in an-
swer reranking. in aaai 2014 workshop on cognitive
computing for augmented human intelligence, pages
13   19.

[harabagiu and hickl2006] sanda harabagiu and an-
drew hickl. 2006. methods for using textual entail-
ment in open-domain id53. in proceed-
ings of the 21st international conference on compu-
tational linguistics and the 44th annual meeting of
the association for computational linguistics, pages
905   912. association for computational linguistics.
[jeon et al.2006] jiwoon jeon, w bruce croft, joon ho
lee, and soyeon park. 2006. a framework to pre-
dict the quality of answers with non-textual features.
in proceedings of the 29th annual international acm

