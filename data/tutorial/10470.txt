6
1
0
2

 
r
a

 

m
3
2

 
 
]
e
n
.
s
c
[
 
 

1
v
9
4
2
7
0

.

3
0
6
1
:
v
i
x
r
a

a tutorial on deep neural networks for

intelligent systems

juan c. cuevas-tello1,2

manuel valenzuela-rend  on 1

juan a. nolazco-flores1

1tecnol  ogico de monterrey, campus monterrey
av. eugenio garza sada 2501 sur, c.p. 64849,

monterrey, n.l., mexico

{cuevastello,valenzuela,jnolazco}@itesm.mx

2engineering faculty, uaslp

dr. manuel nava no. 8, zona universitaria, c.p. 78290

san luis potosi, slp, mexico

cuevas@uaslp.mx

march 24, 2016

abstract

developing intelligent systems involves arti   cial intelligence approaches
including arti   cial neural networks. here, we present a tutorial of deep
neural networks (dnns), and some insights about the origin of the term
   deep   ; references to deep learning are also given. restricted boltzmann
machines, which are the core of dnns, are discussed in detail. an ex-
ample of a simple two-layer network, performing unsupervised learning
for unlabeled data, is shown. id50 (dbns), which are
used to build networks with more than two layers, are also described.
moreover, examples for supervised learning with dnns performing sim-
ple prediction and classi   cation tasks, are presented and explained. this
tutorial includes two intelligent pattern recognition applications: hand-
written digits (benchmark known as mnist) and id103.

1

introduction

intelligent systems involve arti   cial intelligence approaches including arti   cial
neural networks. this paper focus mainly on deep neural networks (dnns).

the core of dnns are the restricted id82s (rbms) proposed
by smolensky [23, 10], and widely studied by hinton et al. [13, 12, 11], where
the term deep comes from deep beliefs networks (dbn) [12]. the next section
describes the relationship among rbms, dbn and dnns.

1

nowadays, the term deep learning (dl) is becoming popular in the machine
learning literature [15, 3, 22]. however, dl mainly refers to deep neural net-
works (dnns) and in particular to dbns and rbms [15]. some work related
to dl is focusing on high performance computing to speed up the learning
of dnns, i.e. graphics processing units (known as gpus), message passing
interface (mpi) among other parallelization technologies [3].

a wide survey on arti   cial intelligence and in particular dl has been pub-
lished recently, which covers dnns, convolutional neural networks, recurrent
neural networks, among many other learning strategies [22].

a restricted id82 (rbm) is de   ned as

a single layer of hidden units which are not connected to each
other and have undirected, symmetrical connections to a layer of
visible units. the visible units and the hidden states are sampled
from their conditional distribution using id150 by running
a markov chain until it reaches its stationary distribution. the
learning rule is the same as the maximum likelihood learning rule
[contrastive divergence] for the in   nite logistic belief net with tied
weights [12].

products of experts (poe) and id82s are probabilistic gener-
ative models, and their intersection comes up with rbms [10]. learning by
contrastive divergence of poe is the basis of the learning algorithm of dbns
[10, 12].

we recommend [6] as a gentle introduction that explains the training of
rbms and their relationship to id114 including markov random
fields (mrfs); it also presents markov chains to explain how a rbm draws
samples from id203 distributions such as gibbs distribution of a mrf.

the building blocks of a rbm are binary stochastic neurons [12]. nev-
ertheless, there are several ways to de   ne real-valued visible neurons, where
gaussian-binary-rbm are widely used [6].
we use a publicly available matlab r(cid:13)/octave toolbox for rbms developed
by tanaka and okutomi [24]. this toolbox implements sparsity [16], dropout
[4] and a novel id136 for rbm [24].

the main contribution of this tutorial are the dnns examples along the
source code (matlab/octave) to build intelligent systems. therefore, the spirit
of this tutorial is that people can easily execute the examples and see what
kind of results are obtained. there are examples with either unsupervised or
supervised learning, and examples for prediction and classi   cation tasks are also
provided. moreover, the parameter setting of dnns with an example is shown.
this tutorial is organized as follows: the following section (  2) describes
rbms. section   3 describes the toolbox developed by tanaka and okutomi [24]
and the database mnist. section   4 presents a discussion about the parameter
settings of dnns. section   5 explains some simple examples of dnns. the
last section presents speech processing with dnns;   6. finally, a summary and
references are given.

2

figure 1: a rbm with a visible layer and a hidden layer.

2 restricted id82s (rbms)

a rbm is depicted in fig. 1. the visible layer is the input, unlabeled data,
to the neural network. the hidden layer grabs features from the input data,
and each neuron captures a di   erent feature [12]. by de   nition, a rbm is a
bipartite undirected graph. a rbm has m visible units (cid:126)v = (v1, v2, . . . , vm),
the input data, and n hidden units (cid:126)h = (h1, h2, . . . , hn), the features [6]. a
joint con   guration, ((cid:126)v,(cid:126)h) of the visible and hidden units has an energy given by
[14]

e((cid:126)v,(cid:126)h) =     m(cid:88)

aivi     n(cid:88)

bjhj     m(cid:88)

n(cid:88)

vihjwij ,

(1)

i=1

j=1

i=1

j=1

where vi and hj are the binary states of the visible and hidden units, respec-
tively; ai, bj are the biases, and wij is a real valued weight associated with each
edge in the network [11], see fig. 1.

the building block of a rbm is a binary stochastic neuron [12]. fig. 2 shows

how to obtain the state of a hidden neuron given a visible layer (data).

a rbm can be seen as a stochastic neural network. first, weights wij are
randomly initialized. then, the data to be learned is set at the visible layer;
this data can be an image, a signal, etcetera. now, the state of the neurons at

3

figure 2: the building block of a rbm: a binary stochastic neuron. in this
example, visible to hidden, hj is the id203 of producing a spike [11].

the hidden layer is obtained by

p(hj = 1|(cid:126)v) = sig

(cid:32)

bj +

(cid:88)

i

(cid:33)

viwij

,

(2)

so the id155 of hj being 1 is the    ring rate of a stochastic
neuron with a sigmoid activation function, sig(x) = 1/(1     e   x), see fig. 2.
this step, visible to hidden, is represented as (cid:104)vihj(cid:105)0, at time t = 0 [12, 6, 24].

2.1 contrastive divergence algorithm

learning in a rbm is achieved by the contrastive divergence (cd) algorithm,
see fig. 3 [12]. the    rst step of the cd algorithm is (cid:104)vihj(cid:105)0, as shown above.
the next step is the    reconstruction    of the visible layer by

      ai +

(cid:88)

j

       ,

p(vi = 1|(cid:126)h) = sig

hjwi,j

(3)

i.e., hidden to visible. this step is denoted as (cid:104)hjvi(cid:105)0. the new state of the
hidden layer is obtained using the result of the reconstruction as the input data,
and this step is denoted as (cid:104)vihj(cid:105)1; at time t = 1. finally, the weights and
biases are adjusted in the following way [12]:

   wij =   (cid:0)(cid:104)vihj(cid:105)0     (cid:104)vihj(cid:105)1(cid:1) ;
   ai =   (cid:0)v0
   bj =   (cid:0)h0

i     v1
j     h1

(cid:1) ;
(cid:1) ;

j

i

4

(4)

(5)

(6)

figure 3: contrastive divergence (cd) algorithm.

where    is the learning rate. rbms    nd better models if more steps of the cd
algorithm are performed; cdk is used to denote the learning in k steps/iterations
[11]. the cd algorithm is summarized in algorithm 1, and it uses the complete
training data, batch learning [6].

2.2 deep belief network

a deep belief network (dbn) [12] is depicted in fig. 4. comparing fig. 1 with
fig. 4, we can see that a dbn is built by stacking rbms. thus, the more levels
the dbn has, the deeper the dbn is. the hidden neurons in a rbm1 capture
the features from the visible neurons. then, those features become the input to
rbm2, and so on until the rbmr is reached; see also fig. 5. a dbn extracts
features from features in an unsupervised manner (deep learning).

5

figure 4: an in   nite belief network. the more rbms, the deeper the learning;
i.e. id50 (dbn).

6

figure 5: an hybrid dbn for supervised learning.

a hybrid dbn has been proposed for supervised learning, see fig. 5. this
network adds labels to the top layer. the weights (cid:126)wl between the top level and
the last layer of hidden neurons, associative memory, are learned in a supervised
manner. this process is called    ne-tunning [12], and it can be achieved by many
di   erent algorithms including id26 [21, 1, 19, 8]. this hybrid dbn
is referred as deep neural networks [16, 4, 6, 24].

hinton et al. applied a dnn to the minst handwritten digits database1 [12],
see fig. 6. at that time, the dnn produced the best performance with an error
rate of 1.25% compared with other methods including support vector machines
(id166) which had an error rate of 1.4% [12].

3 toolbox for dnn
we use a publicly available toolbox for matlab r(cid:13) developed by tanaka and
okutomi [24], and which can be downloaded online2. this toolbox is based
on [12]. this toolbox includes sparsity [16], dropout [4] and a novel id136

1http://yann.lecun.com/exdb/mnist/
2http://www.mathworks.com/matlabcentral/   leexchange/42853-deep-neural-network

7

figure 6: an hybrid dbn for supervised learning [12]; the mnist database.

for rbm devised by tanaka [24]. once the toolbox has been downloaded and
unzipped, it will generate the following directories:

    /deepneuralnetwork/
    /deepneuralnetwork/mnist

3.1 mnist

the mnist database3 of handwritten digits has a training set of 60,000 exam-
ples, and a test set of 10,000 examples. once the mnist has been downloaded
and unzipped, we will come up with the following    les:
    train-images-idx3-ubyte: training set images
    train-labels-idx1-ubyte: training set labels
    t10k-images-idx3-ubyte: test set images
    t10k-labels-idx1-ubyte: test set labels

note that when you uncompress the *.gz    les, then you will need to check
the    le names, and replace    .    by    -   . you must locate the    les within the
/deepneuralnetwork/mnist/ directory.

3http://yann.lecun.com/exdb/mnist/

8

figure 7: another dnn architecture for the mnist database [24].

3.2 running the example: dnn-mnist

the    le /mnist/testmnist.m is the main    le of the example provided by the
toolbox to train a dnn for the mnist database. the example uses a hybrid
network with only two hidden layers of 800 neurons each layer, see fig. 7. we
have tested the toolbox on octave4 3.2.4 and matlab r(cid:13) 7.11.0.584 (2010b),
both in linux operating systems.

the script testmnist.m will generate the    le mnistbbdbn.mat with the
dnn already trained. once testmnist.m has    nished it will appear something
like:

    for training data: rmse = 0.0155251; errorrate = 0.00196667 (0.196%);

tanaka et al. reported 0.158% [24].

    for test data: rmse = 0.0552593; errorrate = 0.0161 (1.6%); tanaka et

al. reported 1.76% [24].

the computational time required to train 60,000 mnist examples and
10,000 examples for testing is about 3 days on a computer with 384 gb memory,
4 cpus 2.3ghz with 16 cores each (total of 64 cores).

3.3 understanding the toolbox example for mnist

once the example script (testmnist.m) has been successfully executed, we run
the script in fig. 8.

4https://www.gnu.org/software/octave/

9

% matlab/octave script

mnistfilenames = cell(4,1);
mnistfilenames{1} =    train-images-idx3-ubyte   ;
mnistfilenames{2} =    train-labels-idx1-ubyte   ;
mnistfilenames{3} =    t10k-images-idx3-ubyte   ;
mnistfilenames{4} =    t10k-labels-idx1-ubyte   ;
[trainimages trainlabels testimages testlabels]=mnistread(mnistfilenames);
% load data for training and testing from files
load mnistbbdbn;
dbn = bbdbn; % set dbn as the trained net
n = 10; %number of test data to analyze
in = testimages(1:n,:);
out = testlabels(1:n,:);
for i=1:n,

%load only n records of testing data

%load only n records of testing data

% load the trained dnn

imshow(reshape(in(i,:),28,28));
name = [   print img-training-   ,num2str(i),   .jpg -djpeg   ]
eval(name); %save the plot, file in jpeg format

end
% v2h: get the output of the dnn
out = v2h( dbn, in );
% get the maximum values (m) of out and indexes (ind)
[m ind] = max(out,[],2);
out = zeros(size(out)); % initialize the variable out
% now, fill with ones where the maximum values where located (ind):
for i=1:size(out,1)

out(i,ind(i)) = 1;

end
% now compare out vs out. let say the output of the dnn (out) vs
% the desired output (out)
errorrate = abs(out-out);
% sum(errorrate,2) performs the sum in two dimensions,
% first by row then the resulting column.
% it is divided by 2; if some output fails, the sum will count twice.
% mean gives us the percentage of error, known as error rate.
errorrate = mean(sum(errorrate,2)/2) % finally the error rate is obtained

% analytically compare out vs out

figure 8: matlab/octave script: analysis of n = 10 test images from mnist
database.

this script generates n = 10 images via imshow, see fig. 9. the images are
part of the 10    rst testing samples. each image is stored in a vector of size 784,
which corresponds to an image size of 28  28 pixels. and each pixel stores a
number between 0 and 255, where 0 means background (white) and 255 means
foreground (black); see fig. 9.

10

figure 9: the    rst 10 testing examples of the mnist database. the    rst one
is the digit 7, then 2; the last one is the digit 9.

figure 10:

inputs and outputs of a dnn for the mnist database.

in fig. 10, we depict the inputs and outputs of the dnn for the mnist
database. the variable in represents the inputs for either training or testing
samples. the variable out is the output for training, and the variable out
the output for testing. both output variables represent the labels of the digits
to learn/recognize (digits from 0 to 9).

for example, the script in fig. 8 generates the following result:

11

out =
0
0
0
1
0
0
0
0
0
0

0
0
1
0
0
1
0
0
0
0

0
1
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
1
0
1
0
0
0

0
0
0
0
0
0
0
0
1
0

0
0
0
0
0
0
0
0
0
0

1
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
1
0
1

the variable out represents the labels. each row correspond to each digit
image5, and each column means the activation or not of the digits 0   9. that
is, the    rst column represents the digit 0, and the last column the digit 9. for
example, see top-left in fig. 9, the image representing the digit 7 activates only
column 8 (i.e. 0 0 0 0 0 0 0 1 0 0) of the    rst row of variable out. the image at
the right side of digit 7 corresponds to digit 2, so the third column is activated
on the second row of variable out, and so on. in this example the errorrate is
zero, because the    rst ten samples of testing are all recognized successfully. let
us create an hypothetical scenario where the dnn fails to recognize the    rst
image (digit 7), i.e. imagine that the output looks like the following:

out =
0
0
0
1
0
0
0
0
0
0

0
0
1
0
0
1
0
0
0
0

0
1
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
1
0
1
0
0
0

0
0
0
0
0
0
0
0
1
0

1
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
1
0
1

look at the    rst row, now this row indicates that the    rst digit image in fig. 9
corresponds to the digit 6, instead of digit 7. therefore, errorrate measures
this error via the abs function along the di   erence between the desired output
out and the output of the dnn, which is out. then the error rate is obtained
as follows:

5note that there are 60,000 images for training, 10,000 images for testing and 10 images

for illustration purposes (fig. 9), i.e. only 10 rows.

12

errorrate = abs(out-out)
errorrate =

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

1
0
0
0
0
0
0
0
0
0

1
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

now, we sum out by row. this is to detect how many di   erences are found

by test sample.

sum(errorrate,2)
ans =
2
0
0
0
0
0
0
0
0
0

if an error exists, then the result is divided by 2. this is because if there is

some di   erence per each sample we will have two 1   s as show above.

13

sum(errorrate,2)/2
ans =
1
0
0
0
0
0
0
0
0
0

finally, the error rate is given as the mean value:

mean(sum(errorrate,2)/2)
ans =

0.10000

since the dnn fails to recognize one digit out of ten, the error rate is 10%

(i.e. 0.10000).

4 parameter setting

there are several parameters to set up when working with dnns, including
statistics to monitor the contrastive divergence algorithm, batch sizes, moni-
toring over   tting (iterations), learning rate   , initial weights, number of hidden
unit and hidden layers, types of units (e.g. binary or gaussian), dropout, among
others [16, 11, 4, 6]. in practice, we can only focus on the following:

    maximum of iterations (maxiter), which is also know as k for the con-

trastive divergence algorithm.

    the learning rate   (stepratio).
    type units (e.g. bernoulli or gaussian distribution).

we analyze the impact of these dnn parameters through the xor example;
see   5.2.4 below.

5 further examples

besides the mnist database example, described above, this section presents
examples for unsupervised and supervised learning; including prediction and

14

figure 11: example of unsupervised learning with a single rbm.

classi   cation tasks.

5.1 unsupervised learning

we now show an example for unsupervised learning. the network architecture
is a single rbm with six visible units and eight hidden units; see fig. 11. the
goal is to learn a simple pattern (pattern) as shown below within the script in
fig. 12. this pattern is a very simple example of unlabeled data.

all the computational times reported throughout this section were obtained
running on a personal computer with the following characteristics: 2.3 ghz
intel core i7 and 4gb memory; linux-ubuntu 12.04 and gnu octave 3.2.4.

5.1.1 script

fig. 12 shows the matlab/octave script for our example of unsupervised learn-
ing. we used the same toolbox than for the mnist database [24].

5.1.2 results

as the script in fig. 12 is executed, several data is displayed. first, the training
data (our pattern) is shown:

15

% matlab/octave script

clear all;
addpath(   ..   );
ini = clock;
pattern = [1
0
0

1
0
0

0
1
0

0
1
0

0
0
1

0 ;
0 ;
1 ];

% #no variables as input

inputs = 6;
traindata = pattern(:,1:inputs)
testdata = traindata;
nodes = [inputs 8]; % [#inputs #hidden]
bbdbn = randdbn( nodes,    bbdbn    ); % bernoulli-bernoulli rbms
nrbm = numel(bbdbn.rbm);
% meta-paramters or hyper-parameters
opts.maxiter = 50;
opts.batchsize = 1;
opts.verbose = false;
opts.stepratio = 2.5;
opts.object =    crossentorpy   ;
%learning stage
fprintf(    training...\n    );
opts.layer = nrbm-1;
bbdbn = pretraindbn(bbdbn, traindata, opts);
%testing stage
fprintf(    testing...\n    );
h = v2h( bbdbn, testdata); % visible layer to hidden layer
out = h2v(bbdbn,h); % hidden to visible layers (reconstruction)
fprintf(    results...\n    );
out
round(out)
theend = clock;
fprintf(   \nelapsed time: %f\n   ,etime(theend,ini));

figure 12: matlab/octave script: unsupervised learning example.

16

traindata =

1
0
0

1
0
0

0
1
0

0
1
0

0
0
1

0
0
1

then the output after pretraindbn is:

out =

9.9e-01
5.0e-04
4.2e-06

9.9e-01
4.3e-04
3.6e-06

1.0e-04
9.9e-01
5.9e-06

1.2e-04
9.9e-01
5.4e-06

2.5e-05
6.1e-04
9.9e-01

2.6e-05
6.0e-04
9.9e-01

this output are the probabilities [0,1], known as reconstructions, so we apply

the function round, and then we obtain:

out =
1
0
0

1
0
0

0
1
0

0
1
0

0
0
1

0
0
1

the total elapsed time is 0.254730 seconds.

5.1.3 discussion

we found that the dnn in fig. 11 is able to learn the given pattern in an
unsupervised manner. we use a stepratio of 2.5 because this allow us to
have fewer iterations, i.e. maxiter = 50. some authors recommend a learning
rate (stepratio) of 0.01 [11, 24], but with this setting, we need at least 1,000
iterations to learn the pattern; see   5.2.4 for parameter setting issues.

5.2 predicting patterns

the following example simulates a time series prediction scenario. we test two
di   erent patterns (pattern1 and pattern2). our training data is a matrix with
eight columns, which is the number of variables. we use only six variables as
input, and the last two column variables as output. the main idea is that we
feed the network only with six variables, then the network must    predict    the
next two variables; see fig. 13. compared with the previous example, we use
supervised learning as shown above in the mnist example, see   3. therefore,
the labels are our two last columns of the pattern (outputs), i.e. trainlabels.

17

figure 13: a dnn architecture for the example of supervised learning: pre-
dicting patterns.

5.2.1 script

the script for this example is shown in fig. 13.

5.2.2 results

the script in fig. 14 generates the following output. first, it prints out the
traindata and trainlabels together with the instruction [traindata trainlabels]:

training...
ans =
0
0
0
0
0
1

0
0
0
0
1
0

0
0
0
1
0
1

0
0
1
0
1
0

0
1
0
1
0
0

1
0
1
0
0
0

0
1
0
0
0
0

1
0
0
0
0
0

the reconstructions (probabilities) are given in the last two columns:

testing...
ans =
0.0000 0.0000 0.0000 0.0000 0.0000 1.0000 0.0031 0.9881
0.0000 0.0000 0.0000 0.0000 1.0000 0.0000 0.9911 0.0011
0.0000 0.0000 0.0000 1.0000 0.0000 1.0000 0.0044 0.0112
0.0000 0.0000 1.0000 0.0000 1.0000 0.0000 0.0092 0.0073
0.0000 1.0000 0.0000 1.0000 0.0000 0.0000 0.0065 0.0002
1.0000 0.0000 1.0000 0.0000 0.0000 0.0000 0.0003 0.0041

18

% matlab/octave script

clear all;
addpath(   ..   );
ini = clock();
pattern1 = [ 0 0 0 0 0 1 0 1;
0 0 0 0 1 0 1 0;
0 0 0 1 0 1 0 0;
0 0 1 0 1 0 0 0;
0 1 0 1 0 0 0 0;
1 0 1 0 0 0 0 0];

pattern2 = [ 1 1 0 0 0 0 0 0;
0 1 1 0 0 0 0 0;
0 0 1 1 0 0 0 0;
0 0 0 1 1 0 0 0;
0 0 0 0 1 1 0 0;
0 0 0 0 0 1 1 0;
0 0 0 0 0 0 1 1;
1 0 0 0 0 0 0 1];

% get the training data, inputs

= pattern(:,1:inputs);

% set the pattern to simulate

% we test with the same training data

% #no variables as input
% #no. variables as ouputs

pattern=pattern1;
inputs = 6;
outputs = 2;
traindata
trainlabels = pattern(:,inputs+1:inputs+outputs); % show the labels
testdata = traindata;
testlabels = trainlabels;
nodes = [inputs 20 outputs]; % [#inputs #hidden #outputs]
bbdbn = randdbn( nodes,    bbdbn    ); % bernoulli-bernoulli rbms
nrbm = numel(bbdbn.rbm);
% meta-parameters or hyper-parameters
opts.maxiter = 1000;
opts.batchsize = 6;
opts.verbose = false;
opts.stepratio = 2.5;
opts.object =    crossentorpy   ;
%learning stage
fprintf(    training...\n    );
[traindata trainlabels]
opts.layer = nrbm-1;
bbdbn = pretraindbn(bbdbn, traindata, opts);
bbdbn= setlinearmapping(bbdbn, traindata, trainlabels);
opts.layer = 0;
bbdbn = traindbn(bbdbn, traindata, trainlabels, opts);
fprintf(    testing...\n    );
out = v2h( bbdbn, testdata );
[testdata out]
[testdata round(out)]
theend = clock();
fprintf(   \nelapsed time: %f\n   ,etime(theend,ini));

19

figure 14: matlab/octave script: predicting patterns example.

as in the previous example, we apply the function round, so we obtain:

[testdata round(out)]
ans =
0
0
0
0
0
1

0
0
0
0
1
0

0
0
0
1
0
1

0
0
1
0
1
0

0
1
0
1
0
0

1
0
1
0
0
0

0
1
0
0
0
0

1
0
0
0
0
0

5.2.3 discussion

in this example, we set a dnn to predict two variables given six input variables.
we found that the dnn is able to successfully predict the given patterns. here,
we show only results for pattern1, but the results for pattern2 are similar. we
started to test the dnn with a di   erent pattern, and we came across accidentally
with a pattern that the dnn cannot predict (4 inputs, 2 outputs):

training...
ans =
0
0
0
0
0
1

0
0
0
0
1
0

0
0
0
1
0
0

0
0
1
0
0
0

0
1
0
0
0
0

1
0
0
0
0
0

testing...
ans =

0.00000
0.00000
0.00000
0.00000
0.00000
1.00000

0.00000
0.00000
0.00000
0.00000
1.00000
0.00000

0.00000
0.00000
0.00000
1.00000
0.00000
0.00000

0.00000
0.00000
1.00000
0.00000
0.00000
0.00000

0.49922
0.49922
0.00993
0.01070
0.01142
0.01109

0.49922
0.49922
0.00993
0.01070
0.01142
0.01109

the two    rst rows of the training data have the same input values, but they
have di   erent output. therefore, the dnn    intelligently    suggest an output of
0.499 (id203).

20

figure 15: a dnn architecture for the xor example.

5.2.4 xor problem

the xor problem is a non-linear problem that is typical test for a classi   er
because it is a problem that a simple linear classi   er cannot learn.
in the
neural networks literature, an example of a linear classi   er is the id88
introduced by frank rosenblatt in 1957 [20]. a decade later, marvin minsky
and seymour paper wrote their famous book id88s, and they showed
that id88s cannot solve the xor problem [18]. perhaps partly due to the
publication of id88s , there was a decline of research in neural networks
until the id26 algorithm appeared about twenty year after minsky
and paper   s publication.

here, we analyze the xor problem with a dnn; see fig. 15.

5.2.5 script

the script for the xor problem is shown in fig. 16.

21

%matlab/octave script

bernoulli-bernoulli rbms

= [0 0; 0 1; 1 0; 1 1];

traindata
trainlabels = [0; 1; 1; 0];
testdata = traindata;
testlabels = trainlabels;
nodes = [2 12 1]; % [#inputs #hidden #outputs]
bbdbn = randdbn( nodes,    bbdbn    ); %
nrbm = numel(bbdbn.rbm);
% meta-paramters or hyper-parameters
opts.maxiter = 100;
opts.batchsize = 4;
opts.verbose = false;
opts.stepratio = 2.5;
opts.object =    crossentorpy   ;
%learning stage
fprintf(    training...\n    );
opts.layer = nrbm-1;
bbdbn = pretraindbn(bbdbn, traindata, opts);
bbdbn= setlinearmapping(bbdbn, traindata, trainlabels);
opts.layer = 0;
bbdbn = traindbn(bbdbn, traindata, trainlabels, opts);
%testing stage
fprintf(    testing...\n    );
testdata = [0.1 0.1; 0 0.9; 1 0.2; 0.8 1];
testdata
out = v2h( bbdbn, testdata )
%printing results
fprintf(    \nresults:\n\n    );
rmse= calcrmse(bbdbn, testdata, testlabels);
errorrate= calcerrorrate(bbdbn, testdata, testlabels);
fprintf(    for test data:\n    );
fprintf(    rmse: %g\n   , rmse );
fprintf(    errorrate: %g\n   , errorrate );

figure 16: matlab/octave script: xor example.

5.2.6 results

the script in fig. 16 involves only two input variables and a single output, so
the traindata and trainlabels are as follows:

22

traindata =

0
0
1
1

0
1
0
1

trainlabels =

0
1
1
0

before coming up with the script of fig. 16, we tested di   erent con   gurations

for the dnn. we started with the following hyperparameter setting:

nodes = [2 3 3 1]; % [#inputs #hidden #hidden #outputs]
pts.maxiter = 10;
opts.batchsize = 4;
opts.verbose = true;
opts.stepratio = 0.1;
opts.object =    crossid178   ;
testdata = traindata;

the trainlabels is a column vector, and out    is a row vector, where out   
0 1 1 0. the

is the transpose of out. therefore, the desired output is out    =
output of the dnn for this setting is:

out    = 0.49994

0.49991

0.50009

0.50006

this setting does not work with the above parameters. if we add more iterations
opts.maxiter = 100, it still does not work properly, and we obtain the output:

out    = 0.47035

0.51976

0.49161

0.50654

if we add more hidden neurons nodes = [2 12 12 1] with the same number
of iterations, then the performance improves. the output now is:

out    = 0.118510

0.906046

0.878771

0.096262

the performance is still better if we add more iterations opts.maxiter = 1000.
the output now is:

out    = 0.014325
elapsed time: 32.607048 seconds

0.982409

0.990972

0.012630

23

in order to decrease the
the previous experiment takes about 33 seconds.
complexity of the network, we reduce the number of hidden neurons. now we
have a single hidden layer nodes = [2 12 1], and with the same number of
iterations opts.maxiter = 1000. in about 24 seconds, the output is:

out    = 0.043396
elapsed time: 23.640984 seconds

0.950205

0.947391

0.059305

now, if we reduce the the number of iterations with less neurons as the
previous experiments, i.e. opts.maxiter = 100, it is faster but the performance
decays. so the output now is:

out    = 0.16363
elapsed time: 2.617439 seconds

0.80535

0.82647

0.20440

other hyperparameter is opts.stepratio, the learning rate, so we tunned
this parameter to opts.stepratio = 0.01, and the number of iterations is
opts.maxiter = 1000. we found similar results to the previous experiment,
i.e. we do not reach the desired output.

nevertheless, if we use opts.stepratio = 2.5 and opts.maxiter = 100,

then we obtain a good performance in about one second. the output is:

out    = 0.022711
elapsed time: 0.806343

0.955236

0.955606

0.065202

another important experiment is to test the performance with real values.

so we change the test data, and we obtain the following results:

testdata =

0.10000
0.00000
1.00000
0.80000

0.10000
0.90000
0.20000
1.00000

out =

0.213813
0.956192
0.889679
0.053432

elapsed time: 0.686760

finally, we ran some experiments by tuning the hyperparameter opts.object
to    square    or    crossentorpy   , and we found no di   erence in performance. the
   ag opts.verbose is only for showing or not the training performance.

24

5.2.7 discussion

for the xor problem, we found that the best performance is with the following
combination: opts.stepratio = 2.5 and opts.maxiter = 100. a large step
ratio with few iterations allow us to obtain results faster than other settings.
the performance is good enough to solve the xor problem. moreover, this
setting classi   es correctly when the inputs are real data. for this reason, this
setting is used in the previous examples; see   5.1 and   5.2.

6 speech processing

speech processing has several applications including id103, lan-
guage identi   cation and speaker recognition; see fig. 17. sometimes, addi-
tional information is stored and associated to speech. therefore, speaker recog-
nition can be either text dependent or text independent. moreover, speaker
recognition involves di   erent tasks such as [2]:

    speaker identi   cation
    speaker detection
    speaker veri   cation.

6.1 speech features

the    rst step for most id103 systems is the feature extraction
from the time-domain sampled acoustic waveform (audio); see figure 18a. the
time-domain waveform is represented by overlapping frames. each frame is
generated every 10ms with a duration of 25ms. then, a feature is extracted
for every frame. several methods have been investigated for feature extraction
(acoustic representation) including linear prediction coe   cients (lpcs), per-
ceptual linear prediction (plp) coe   cients and mel-frequency spaced cepstral
coe   cients(mfccs) [5, 25].

6.2 dnn and speech processing

as we shown above, dnns have the    exibility to be used as either unsuper-
vised or supervised learning. therefore, dnns can be used for regression or
classi   cation problems in id103; see fig. 19.

nowadays, dnns have been applied successfully on speech processing includ-
ing speaker recognition [9, 26], language identi   cation [7] and speech generation
[17].
voicebox6 is a speech processing toolbox for matlab r(cid:13), which is also

publicly available.

6http://www.ee.ic.ac.uk/hp/sta   /dmb/voicebox/voicebox.html

25

figure 17: speech processing

7 summary

neural networks approaches have been used widely to build intelligent systems.
we introduced deep neural networks (dnns) and restricted boltzmann ma-
chines (rbms), and their relationship to deep learning (dl) and deep belief
nets (dbns). across the literature, there are some introductory papers for
rbms [11, 6]. one of the contributions of this tutorial are the simple examples
for a better understanding of rbms and dnns. the examples cover unsuper-
vised and supervised learning, therefore, we cover both unlabeled and labeled
data, for prediction and classi   cation. moreover, we introduce a publicly avail-
able matlab r(cid:13) toolbox to show the performance of dnns and rbms [24].
the toolbox and the examples have been tested on octave, the open source
version of matlab r(cid:13). the last example, xor problem, presents some results
by di   erent setting of some hyperparameters of dnns. finally, two applications
for intelligent pattern recognition are also covered on this tutorial: the mnist
benchmarking and id103.

references

[1] c. m. bishop and g. e. hinton. neural networks for pattern recognition.

clarendon press, 1995.

26

figure 18: audio sample. (a) audio represented as a time-domain waveform;
(b) features obtained from the time-domain waveform.

27

figure 19: dnn and speech

28

[2] j.p. jr. campbell. speaker recognition: a tutorial. proceedings of ieee,

85(6):1437   1462, 1997.

[3] a. coates, b. huval, t. wang, and a. y. wu, d. j.and ng. deep learn-
in proceedings of the 30th international

ing with cots hpc systems.
conference on machine learning (icml-2013), 2013.

[4] g. e. dahl, t. n. sainath, and g. e. hinton. improving deep neural net-
works for lvcsr using recti   ed linear units and dropout. in ieee inter-
national conference on acoustics, speech and signal processing (icassp-
2013), pages 8609   8613, 2013.

[5] s. davis and p. mermelstein. comparison of parametric representations for
monosyllabic word recognition in continuously spoken sentences. in ieee
transactions on acoustics, speech and signal processing, volume 28, 1980.

[6] a. fischer and c. igel. training restricted id82s: an intro-

duction. pattern recognition, 14:25   39, 2014.

[7] j. gonzalez-dominguez, i. lopez-mreno, p.j. moreno, and j. gonzalez-
rodriguez. frame-by-frame language identi   cation in short utterances us-
ing deep neural networks. neural networks, 64:49   58, 2015.

[8] s. haykin. neural networks: a comprehensive foundation. prentice hall,

1999.

[9] g. hinton, l. deng, d. yu, g.e. dahl, a. mohamed, n. jaitly, a. senior,
v. vanhoucke, p. nguyen, t.n. sainath, and b. kingsbury. deep neural
networks for acoustic modeling in id103: the shared views
of four research groups. ieee signal processing magazine, 29(6):82   97,
2012.

[10] g. e. hinton. training products of experts by minimizing contrastive di-

vergence. neural computation, 14(8):1711   1800, 2002.

[11] g. e. hinton. a practical guide to training restricted id82s
version 1. department of computer science, university of toronto, 2010.

[12] g. e. hinton, s. osindero, and y. w. teh. a fast learning algorithm for

deep belief nets. neural computation, 18(7):1527   1554, 2006.

[13] g. e. hinton and r. r. salakhutdinov. reducing the dimensionality of

data with neural networks. science, 313(1):504   507, 2006.

[14] j. j. hop   eld. neural networks and physical systems with emergent col-
lective computational abilities. in proceedings of the national academy of
sciences, volume 79, pages 2554      2558, 1982.

29

[15] q. le, m. ranzato, r. monga, m. devin, k. chen, g. corrado, j. dean,
, and a. ng. building high-level features using large scale unsupervised
learning. in proceedings of international conference on machine learning
(icml2012), 2012.

[16] h. lee, c. ekanadham, and a. y. ng. sparse deep belief net model for
in advances in neural information processing systems

visual area v2.
(nips-2008), volume 20, 2008.

[17] z.h. ling, s.y. kang, h. zen, a. senior, m. schuster, x.j. qian, h. meng,
and l. deng. deep learning for acoustic modeling in parametric speech
generation. ieee signal processing magazine, 32(3):35   52, 2015.

[18] m. l. minsky and s. a. papert. id88s. cambridge, ma: mit press,

1969.

[19] r. rojas. neural networks: a systematic introduction. springer-verlag,

1996.

[20] f. rosenblatt. the id88   a perceiving and recognizing automaton.

technical report 85-460-1, cornell aeronautical laboratory, 1957.

[21] d. e. rumelhart, g. e. hinton, and r.j. williams. learning representa-

tions by back-propagating errors. nature, 323(1):533   536, 1986.

[22] j schmidhuber. deep learning in neural networks: an overview. neural

networks, 61:85   117, 2015.

[23] p. smolensky. parallel distributed processing, volume 1, chapter informa-
tion processing in dynamical systems: foundations of harmony theory,
pages 194   281. mit press, cambridge, 1986.

[24] m. tanaka and m. okutomi. a novel id136 of a restricted boltzmann
machine. in international conference on pattern recognition (icpr2014),
2014.

[25] r. togneri and d. pullella. an overview of speaker identi   cation: accuracy
and robustness issues. ieee circuits and systems magazine, 2(1):23   61,
2011.

[26] c. weng, d. yu, m.l. seltzer, and j. droppo. single-channel mixed speech
recognition using deep neural networks. in ieee international acoustics,
speech and signal processing (icassp), pages 5632   5636, 2014.

30

