1545

san diego, california, june 12-17, 2016. c(cid:13)2016 association for computational linguistics

proceedings of naacl-hlt 2016, pages 1545   1554,

learningtocomposeneuralnetworksforquestionansweringjacobandreasandmarcusrohrbachandtrevordarrellanddankleindepartmentofelectricalengineeringandcomputersciencesuniversityofcalifornia,berkeley{jda,rohrbach,trevor,klein}@eecs.berkeley.eduabstractwedescribeaquestionansweringmodelthatappliestobothimagesandstructuredknowl-edgebases.themodelusesnaturallan-guagestringstoautomaticallyassembleneu-ralnetworksfromacollectionofcomposablemodules.parametersforthesemodulesarelearnedjointlywithnetwork-assemblyparam-etersviareinforcementlearning,withonly(world,question,answer)triplesassupervi-sion.ourapproach,whichwetermadynamicneuralmodulenetwork,achievesstate-of-the-artresultsonbenchmarkdatasetsinbothvi-sualandstructureddomains.1introductionthispaperpresentsacompositional,attentionalmodelforansweringquestionsaboutavarietyofworldrepresentations,includingimagesandstruc-turedknowledgebases.themodeltranslatesfromquestionstodynamicallyassembledneuralnet-works,thenappliesthesenetworkstoworldrep-resentations(imagesorknowledgebases)topro-duceanswers.wetakeadvantageoftwolargelyindependentlinesofwork:ononehand,anexten-siveliteratureonansweringquestionsbymappingfromstringstologicalrepresentationsofmeaning;ontheother,aseriesofrecentsuccessesindeepneuralmodelsforimagerecognitionandcaptioning.byconstructingneuralnetworksinsteadoflogicalforms,ourmodelleveragesthebestaspectsofbothlinguisticcompositionalityandcontinuousrepresen-tations.ourmodelhastwocomponents,trainedjointly:   rst,acollectionofneural   modules   thatcanbefreelycomposed(figure1b);second,anetworklay-outpredictorthatassemblesmodulesintocompletedeepnetworkstailoredtoeachquestion(figure1a).module inventory (section 4.2)findlookupandwhat cities are in georgia?atlantanetwork layout (section 4.1)andlookupgeorgiafindcitygeorgiaatlantamontgomeryknowledge sourcerelaterelateinfind[city]lookup[georgia]relate[in]and(a)(b)(c)(d)figure1:alearnedsyntacticanalysis(a)isusedtoassembleacollectionofneuralmodules(b)intoadeepneuralnetwork(c),andappliedtoaworldrepresentation(d)toproduceananswer.previousworkhasusedmanually-speci   edmodularstructuresforvisuallearning(andreasetal.,2016).herewe:   learnanetworkstructurepredictorjointlywithmoduleparametersthemselves   extendvisualprimitivesfrompreviousworktoreasonoverstructuredworldrepresentationstrainingdataconsistsof(world,question,answer)triples:ourapproachrequiresnosupervisionofnet-worklayouts.weachievestate-of-the-artperfor-manceontwomarkedlydifferentquestionanswer-ingtasks:onewithquestionsaboutnaturalim-ages,andanotherwithmorecompositionalques-tionsaboutunitedstatesgeography.12deepnetworksasfunctionalprogramswebeginwithahigh-leveldiscussionofthekindsofcomposednetworkswewouldliketolearn.1wehavereleasedourcodeathttp://github.com/jacobandreas/nmn21546

andreasetal.(2016)describeaheuristicap-proachfordecomposingvisualquestionansweringtasksintosequenceofmodularsub-problems.forexample,thequestionwhatcoloristhebird?mightbeansweredintwosteps:   rst,   whereisthebird?   (figure2a),second,   whatcoloristhatpartoftheimage?   (figure2c).this   rststep,agenericmod-ulecalledfind,canbeexpressedasafragmentofaneuralnetworkthatmapsfromimagefeaturesandalexicalitem(herebird)toadistributionoverpix-els.thisoperationiscommonlyreferredtoastheattentionmechanism,andisastandardtoolforma-nipulatingimages(xuetal.,2015)andtextrepre-sentations(hermannetal.,2015).the   rstcontributionofthispaperisanexten-sionandgeneralizationofthismechanismtoenablefully-differentiablereasoningaboutmorestructuredsemanticrepresentations.figure2bshowshowthesamemodulecanbeusedtofocusontheentitygeorgiainanon-visualgroundingdomain;moregenerally,byrepresentingeveryentityintheuni-verseofdiscourseasafeaturevector,wecanobtainadistributionoverentitiesthatcorrespondsroughlytoalogicalset-valueddenotation.havingobtainedsuchadistribution,existingneu-ralapproachesuseittoimmediatelycomputeaweightedaverageofimagefeaturesandprojectbackintoalabelingdecision   adescribemodule(fig-ure2c).butthelogicalperspectivesuggestsanum-berofnovelmodulesthatmightoperateonatten-tions:e.g.combiningthem(byanalogytoconjunc-tionordisjunction)orinspectingthemdirectlywith-outareturntofeaturespace(byanalogytoquanti   -cation,figure2d).thesemodulesarediscussedindetailinsection4.unliketheirformalcounterparts,theyaredifferentiableend-to-end,facilitatingtheirintegrationintolearnedmodels.buildingonprevi-ouswork,welearnbehaviorforacollectionofhet-erogeneousmodulesfrom(world,question,answer)triples.thesecondcontributionofthispaperisamodelforlearningtoassemblesuchmodulescomposition-ally.isolatedmodulesareoflimiteduse   toob-tainexpressivepowercomparabletoeitherformalapproachesormonolithicdeepnetworks,theymustbecomposedintolargerstructures.figure2showssimpleexamplesofcomposedstructures,butforrealisticquestion-answeringtasks,evenlargernet-black	and	whitegeorgiaatlantamontgomerygeorgiaatlantamontgomeryexiststruefindbirddescribecolorfindstate(a)(b)(c)(d)figure2:simpleneuralmodulenetworks,correspondingtothequestionswhatcoloristhebird?andarethereanystates?(a)aneuralfindmoduleforcomputinganattentionoverpixels.(b)thesameoperationappliedtoaknowledgebase.(c)usinganattentionproducedbyalowermoduletoidentifythecoloroftheregionoftheimageattendedto.(d)performingquanti   cationbyevaluatinganattentiondirectly.worksarerequired.thusourgoalistoautomati-callyinducevariable-free,tree-structuredcomputa-tiondescriptors.wecanuseafamiliarfunctionalnotationfromformalsemantics(e.g.liangetal.,2011)torepresentthesecomputations.2wewritethetwoexamplesinfigure2as(describe[color]find[bird])and(existsfind[state])respectively.thesearenetworklayouts:theyspec-ifyastructureforarrangingmodules(andtheirlex-icalparameters)intoacompletenetwork.andreasetal.(2016)usehand-writtenrulestodeterministi-callytransformdependencytreesintolayouts,andarerestrictedtoproducingsimplestructuresliketheabovefornon-syntheticdata.forfullgenerality,wewillneedtosolveharderproblems,liketransform-ingwhatcitiesareingeorgia?(figure1)into(andfind[city](relate[in]lookup[georgia]))inthispaper,wepresentamodelforlearningtose-lectsuchstructuresfromasetofautomaticallygen-eratedcandidates.wecallthismodeladynamicneuralmodulenetwork.2butnotethatunlikeformalsemantics,thebehavioroftheprimitivefunctionshereisitselfunknown.1547

3relatedworkthereisanextensiveliteratureondatabaseques-tionanswering,inwhichstringsaremappedtolog-icalforms,thenevaluatedbyablack-boxexecu-tionmodeltoproduceanswers.supervisionmaybeprovidedeitherbyannotatedlogicalforms(wongandmooney,2007;kwiatkowskietal.,2010;an-dreasetal.,2013)orfrom(world,question,answer)triplesalone(liangetal.,2011;pasupatandliang,2015).ingeneralthesetofprimitivefunctionsfromwhichtheselogicalformscanbeassembledis   xed,butonerecentlineofworkfocusesoninduc-ingnewpredicatesfunctionsautomatically,eitherfromperceptualfeatures(krishnamurthyandkol-lar,2013)ortheunderlyingschema(kwiatkowskietal.,2013).themodelwedescribeinthispaperhasauni   edframeworkforhandlingboththeper-ceptualandschemacases,anddiffersfromexistingworkprimarilyinlearningadifferentiableexecutionmodelwithcontinuousevaluationresults.neuralmodelsforquestionansweringarealsoasubjectofcurrentinterest.theseincludeap-proachesthatmodelthetaskdirectlyasamulticlassclassi   cationproblem(iyyeretal.,2014),modelsthatattempttoembedquestionsandanswersinasharedvectorspace(bordesetal.,2014)andat-tentionalmodelsthatselectwordsfromdocumentssources(hermannetal.,2015).suchapproachesgenerallyrequirethatanswerscanberetrieveddi-rectlybasedonsurfacelinguisticfeatures,withoutrequiringintermediatecomputation.amorestruc-turedapproachdescribedbyyinetal.(2015)learnsaqueryexecutionmodelfordatabasetableswith-outanynaturallanguagecomponent.previousef-fortstowardunifyingformallogicandrepresenta-tionlearningincludethoseofgrefenstette(2013)andkrishnamurthyandmitchell(2013).thevisually-groundedcomponentofthisworkreliesonrecentadvancesinconvolutionalnet-worksforcomputervision(simonyanandzisser-man,2014),andinparticularthefactthatlateconvo-lutionallayersinnetworkstrainedforimagerecog-nitioncontainrichfeaturesusefulforotherdown-streamvisiontasks,whilepreservingspatialinfor-mation.thesefeatureshavebeenusedforbothim-agecaptioning(xuetal.,2015)andvisualquestionanswering(yangetal.,2015).mostpreviousapproachestovisualquestionan-sweringeitherapplyarecurrentmodeltodeeprep-resentationsofboththeimageandthequestion(renetal.,;malinowskietal.,2015),orusethequestiontocomputeanattentionovertheinputimage,andthenanswerbasedonboththequestionandtheim-agefeaturesattendedto(yangetal.,2015;xuandsaenko,2015).otherapproachesincludethesimpleclassi   cationmodeldescribedbyzhouetal.(2015)andthedynamicparameterpredictionnetworkde-scribedbynohetal.(2015).allofthesemodelsassumethata   xedcomputationcanbeperformedontheimageandquestiontocomputetheanswer,ratherthanadaptingthestructureofthecomputationtothequestion.asnoted,andreasetal.(2016)previouslycon-sideredasimplegeneralizationoftheseattentionalapproachesinwhichsmallvariationsinthenet-workstructureper-questionwerepermitted,withthestructurechosenby(deterministic)syntacticpro-cessingofquestions.otherapproachesinthisgen-eralfamilyincludethe   universalparser   sketchedbybottou(2014),andtherecursiveneuralnetworksofsocheretal.(2013),whichusea   xedtreestruc-turetoperformfurtherlinguisticanalysiswithoutanyexternalworldrepresentation.weareunawareofpreviousworkthatsucceedsinsimultaneouslylearningboththeparametersforandstructuresofinstance-speci   cneuralnetworks.4modelrecallthatourgoalistomapfromquestionsandworldrepresentationstoanswers.thisprocessin-volvesthefollowingvariables:1.waworldrepresentation2.xaquestion3.yananswer4.zanetworklayout5.  acollectionofmodelparametersourmodelisbuiltaroundtwodistributions:alay-outmodelp(z|x;     )whichchoosesalayoutforasentence,andaexecutionmodelpz(y|w;  e)whichappliesthenetworkspeci   edbyztow.foreaseofpresentation,weintroducethesemod-elsinreverseorder.we   rstimaginethatzisalwaysobserved,andinsection4.1describehowtoevalu-ateandlearnmodulesparameterizedby  ewithin1548

   xedstructures.insection4.2,wemovetotherealscenario,wherezisunknown.wedescribehowtopredictlayoutsfromquestionsandlearn  eand     jointlywithoutlayoutsupervision.4.1evaluatingmodulesgivenalayoutz,weassemblethecorrespondingmodulesintoafullneuralnetwork(figure1c),andapplyittotheknowledgerepresentation.interme-diateresults   owbetweenmodulesuntilananswerisproducedattheroot.wedenotetheoutputofthenetworkwithlayoutzoninputworldwasjzkw;whenexplicitlyreferencingthesubstructureofz,wecanalternativelywritejm(h1,h2)kforatop-levelmodulemwithsubmoduleoutputsh1andh2.wethende   netheexecutionmodel:pz(y|w)=(jzkw)y(1)(thisassumesthattherootmoduleofzproducesadistributionoverlabelsy.)thesetofpossiblelayoutszisrestrictedbymoduletypeconstraints:somemodules(likefindabove)operatedirectlyontheinputrepresentation,whileothers(likedescribeabove)alsodependoninputfromspeci   cearliermodules.twobasetypesareconsideredinthispa-perareattention(adistributionoverpixelsorenti-ties)andlabels(adistributionoveranswers).parametersaretiedacrossmultipleinstancesofthesamemodule,sodifferentinstantiatednetworksmaysharesomeparametersbutnotothers.moduleshavebothparameterarguments(showninsquarebrackets)andordinaryinputs(showninparenthe-ses).parameterarguments,liketherunningbirdexampleinsection2,areprovidedbythelayout,andareusedtospecializemodulebehaviorforpar-ticularlexicalitems.ordinaryinputsarethere-sultofcomputationlowerinthenetwork.inad-ditiontoparameter-speci   cweights,moduleshaveglobalweightssharedacrossallinstancesofthemodule(butnotsharedwithothermodules).wewritea,a,b,b,...forglobalweightsandui,viforweightsassociatedwiththeparameterargumenti.   and(cid:12)denote(possiblybroadcasted)elementwiseadditionandmultiplicationrespectively.thecom-pletesetofglobalweightsandparameter-speci   cweightsconstitutes  e.everymodulehasaccesstotheworldrepresentation,representedasacollectionofvectorsw1,w2,...(orwexpressedasamatrix).thenonlinearity  denotesarecti   edlinearunit.themodulesusedinthispaperareshownbelow,withnamesandtypeconstraintsinthe   rstrowandadescriptionofthemodule   scomputationfollowing.lookup(   attention)lookup[i]producesanattentionfocusedentirelyattheindexf(i),wheretherelationshipfbetweenwordsandpositionsintheinputmapisknownaheadoftime(e.g.stringmatchesondatabase   elds).jlookup[i]k=ef(i)(2)whereeiisthebasisvectorthatis1intheithpositionand0elsewhere.find(   attention)find[i]computesadistributionoverindicesbycon-catenatingtheparameterargumentwitheachpositionoftheinputfeaturemap,andpassingtheconcatenatedvectorthroughamlp:jfind[i]k=softmax(a(cid:12)  (bvi   cw   d))(3)relate(attention   attention)relatedirectsfocusfromoneregionoftheinputtoanother.itbehavesmuchlikethefindmodule,butalsoconditionsitsbehavioronthecurrentregionofattentionh.let  w(h)=pkhkwk,wherehkisthekthelementofh.then,jrelate[i](h)k=softmax(a(cid:12)  (bvi   cw   d  w(h)   e))(4)and(attention*   attention)andperformsanoperationanalogoustosetintersec-tionforattentions.theanalogytoprobabilisticlogicsuggestsmultiplyingprobabilities:jand(h1,h2,...)k=h1(cid:12)h2(cid:12)      (5)describe(attention   labels)describe[i]computesaweightedaverageofwundertheinputattention.thisaverageisthenusedtopredictananswerrepresentation.with  wasabove,jdescribe[i](h)k=softmax(a  (b  w(h)+vi))(6)exists(attention   labels)existsistheexistentialquanti   er,andinspectstheincomingattentiondirectlytoproducealabel,ratherthananintermediatefeaturevectorlikedescribe:jexists](h)k=softmax(cid:16)(cid:0)maxkhk(cid:1)a+b(cid:17)(7)1549

what cities are in georgia?whatcitybeingeorgiafind[city]relate[in]lookup[georgia]relate[in]...lookup[georgia]find[city]and(a)(b)(c)(d)relate[in]lookup[georgia]figure3:generationoflayoutcandidates.theinputsentence(a)isrepresentedasadependencyparse(b).fragmentsofthisdependencyparsearethenassociatedwithappropriatemodules(c),andthesefragmentsareassembledintofulllayouts(d).withzobserved,themodelwehavedescribedsofarcorrespondslargelytothatofandreasetal.(2016),thoughthemoduleinventoryisdifferent   inparticular,ournewexistsandrelatemodulesdonotdependonthetwo-dimensionalspatialstruc-tureoftheinput.thisenablesgeneralizationtonon-visualworldrepresentations.learninginthissimpli   edsettingisstraightfor-ward.assumingthetop-levelmoduleineachlayoutisadescribeorexistsmodule,thefully-instan-tiatednetworkcorrespondstoadistributionoverla-belsconditionedonlayouts.totrain,wemaximizep(w,y,z)logpz(y|w;  e)directly.thiscanbeunder-stoodasaparameter-tyingscheme,wherethedeci-sionsaboutwhichparameterstotiearegovernedbytheobservedlayoutsz.4.2assemblingnetworksnextwedescribethelayoutmodelp(z|x;     ).we   rstusea   xedsyntacticparsetogenerateasmallsetofcandidatelayouts,analogouslytothewayasemanticgrammargeneratescandidatesemanticparsesinpreviouswork(berantandliang,2014).asemanticparsediffersfromasyntacticparseintwoprimaryways.first,lexicalitemsmustbemappedontoa(possiblysmaller)setofsemanticprimitives.second,thesesemanticprimitivesmustbecombinedintoastructurethatclosely,butnotex-actly,parallelsthestructureprovidedbysyntax.forexample,stateandprovincemightneedtobeidenti-   edwiththesame   eldinadatabaseschema,whileallstateshaveacapitalmightneedtobeidenti   edwiththecorrect(insitu)quanti   erscope.whilewecannotavoidthestructureselectionproblem,continuousrepresentationssimplifythelexicalselectionproblem.formodulesthatacceptavectorparameter,weassociatetheseparameterswithwordsratherthansemantictokens,andthusturnthecombinatorialoptimizationproblemasso-ciatedwithlexiconinductionintoacontinuousone.now,inordertolearnthatprovinceandstatehavethesamedenotation,itissuf   cienttolearnthattheirassociatedparametersarecloseinsomeembeddingspace   ataskamenabletogradientdescent.(notethatthisiseasyonlyinanoptimizabilitysense,andnotaninformation-theoreticone   wemuststilllearntoassociateeachindependentlexicalitemwiththecorrectvector.)theremainingcombinatorialproblemistoarrangetheprovidedlexicalitemsintotherightcomputationalstructure.inthisrespect,layoutpredictionismorelikesyntacticparsingthanordinarysemanticparsing,andwecanrelyonanoff-the-shelfsyntacticparsertogetmostofthewaythere.inthiswork,syntacticstructureisprovidedbythestanforddependencyparser(demarneffeandmanning,2008).theconstructionoflayoutcandidatesisdepictedinfigure3,andproceedsasfollows:1.representtheinputsentenceasadependencytree.2.collectallnouns,verbs,andprepositionalphrasesthatareattacheddirectlytoawh-wordorcopula.3.associateeachofthesewithalayoutfrag-ment:ordinarynounsandverbsaremappedtoasinglefindmodule.propernounstoasin-glelookupmodule.prepositionalphrasesaremappedtoadepth-2fragment,witharelatemodulefortheprepositionaboveafindmod-ulefortheenclosedheadnoun.4.formsubsetsofthissetoflayoutfragments.foreachsubset,constructalayoutcandidateby1550

joiningallfragmentswithanandmodule,andinsertingeitherameasureordescribemoduleatthetop(eachsubsetthusresultsintwoparsecandidates.)alllayoutsresultingfromthisprocessfeaturearelatively   attreestructurewithatmostonecon-junctionandonequanti   er.thisisastrongsim-plifyingassumption,butappearssuf   cienttocovermostoftheexamplesthatappearinbothofourtasks.asourapproachincludesbothcategories,re-lationsandsimplequanti   cation,therangeofphe-nomenaconsideredisgenerallybroaderthanpre-viousperceptually-groundedqawork(krishna-murthyandkollar,2013;matuszeketal.,2012).havinggeneratedasetofcandidateparses,weneedtoscorethem.thisisarankingproblem;asintherestofourapproach,wesolveitusingstandardneuralmachinery.inparticular,wepro-duceanlstmrepresentationofthequestion,afeature-basedrepresentationofthequery,andpassbothrepresentationsthroughamultilayerid88(mlp).thequeryfeaturevectorincludesindicatorsonthenumberofmodulesofeachtypepresent,aswellastheirassociatedparameterarguments.whileonecaneasilyimagineamoresophisticatedparse-scoringmodel,thissimpleapproachworkswellforourtasks.formally,foraquestionx,lethq(x)beanlstmencodingofthequestion(i.e.thelasthiddenlayerofanlstmappliedword-by-wordtotheinputques-tion).let{z1,z2,...}betheproposedlayoutsforx,andletf(zi)beafeaturevectorrepresentingtheithlayout.thenthescores(zi|x)forthelayoutziiss(zi|x)=a>  (bhq(x)+cf(zi)+d)(8)i.e.theoutputofanmlpwithinputshq(x)andf(zi),andparameters     ={a,b,c,d}.finally,wenormalizethesescorestoobtainadistribution:p(zi|x;     )=es(zi|x).nxj=1es(zj|x)(9)havingde   nedalayoutselectionmodulep(z|x;     )andanetworkexecutionmodelpz(y|w;  e),wearereadytode   neamodelforpredictinganswersgivenonly(world,question)pairs.thekeyconstraintisthatwewanttomin-imizeevaluationsofpz(y|w;  e)(whichinvolvesexpensiveapplicationofadeepnetworktoalargeinputrepresentation),butcantractablyevaluatep(z|x;     )forallz(whichinvolvesapplicationofashallownetworktoarelativelysmallsetofcandidates).thisistheoppositeofthesituationusuallyencounteredsemanticparsing,wherecallstothequeryexecutionmodelarefastbutthesetofcandidateparsesistoolargetoscoreexhaustively.infact,theproblemmorecloselyresemblesthescenariofacedbyagentsinthereinforcementlearn-ingsetting(whereitischeaptoscoreactions,butpotentiallyexpensivetoexecutethemandobtainre-wards).weadoptacommonapproachfromthatlit-erature,andexpressourmodelasastochasticpol-icy.underthispolicy,we   rstsamplealayoutzfromadistributionp(z|x;     ),andthenapplyztotheknowledgesourceandobtainadistributionoveranswersp(y|z,w;  e).afterzischosen,wecantraintheexecutionmodeldirectlybymaximizinglogp(y|z,w;  e)withrespectto  easbefore(thisisordinarybackprop-agation).becausethehardselectionofzisnon-differentiable,weoptimizep(z|x;     )usingapolicygradientmethod.thegradientoftherewardsurfacejwithrespecttotheparametersofthepolicyis   j(     )=e[   logp(z|x;     )  r](10)(thisisthereinforcerule(williams,1992)).heretheexpectationistakenwithrespecttorolloutsofthepolicy,andristhereward.becauseourgoalistoselectthenetworkthatmakesthemostaccuratepredictions,wetaketherewardtobeidenticallythenegativelog-id203fromtheexecutionphase,i.e.e[(   logp(z|x;     ))  logp(y|z,w;  e)](11)thustheupdatetothelayout-scoringmodelateachtimestepissimplythegradientofthelog-id203ofthechosenlayout,scaledbytheaccuracyofthatlayout   spredictions.attrainingtime,weapproxi-matetheexpectationwithasinglerollout,soateachstepweupdate     inthedirection(   logp(z|x;     ))  logp(y|z,w;  e)forasinglez   p(z|x;     ).  eand     areoptimizedusingadadelta(zeiler,2012)with  =0.95,  =1e   6andgradientclippingatanormof10.1551

whatisinthesheep   sear?whatcolorisshewearing?whatisthemandragging?(describe[what](andfind[sheep]find[ear]))(describe[color]find[wear])(describe[what]find[man])tagwhiteboat(board)figure4:sampleoutputsforthevisualquestionansweringtask.thesecondrowshowsthe   nalattentionprovidedasin-puttothetop-leveldescribemodule.forthe   rsttwoexam-ples,themodelproducesreasonableparses,attendstothecor-rectregionoftheimages(theearandthewoman   sclothing),andgeneratesthecorrectanswer.inthethirdimage,theverbisdiscardedandawronganswerisproduced.5experimentstheframeworkdescribedinthispaperisgeneral,andweareinterestedinhowwellitperformsondatasetsofvaryingdomain,sizeandlinguisticcom-plexity.tothatend,weevaluateourmodelontasksatoppositeextremesofboththesecriteria:alargevisualquestionansweringdataset,andasmallcol-lectionofmorestructuredgeographyquestions.5.1questionsaboutimagesour   rsttaskistherecently-introducedvisualques-tionansweringchallenge(vqa)(antoletal.,2015).thevqadatasetconsistsofmorethan200,000imagespairedwithhuman-annotatedques-tionsandanswers,asinfigure4.weusethevqa1.0release,employingthede-velopmentsetformodelselectionandhyperparam-etertuning,andreporting   nalresultsfromtheeval-uationserveronthetest-standardset.fortheex-perimentsdescribedinthissection,theinputfeaturerepresentationswiarecomputedbythethe   fthcon-volutionallayerofa16-layervggnetafterpooling(simonyanandzisserman,2014).inputimagesarescaledto448  448beforecomputingtheirrepresen-tations.wefoundthatperformanceonthistaskwastest-devtest-stdyes/nonumberotherallallzhou(2015)76.635.042.655.755.9noh(2015)80.737.241.757.257.4nmn77.737.239.354.855.1nmn*79.737.142.857.3   d-nmn80.537.443.157.958.0table1:resultsonthevqatestserver.nmnistheparameter-tyingmodelfromandreasetal.(2015),whilenmn*isareimplementationusingthesameimageprocessingpipelineasd-nmn.themodelwithdynamicnetworkstructurepredictionachievesthebestpublishedresultsonthistask.bestifthecandidatelayoutswererelativelysimple:onlydescribe,andandfindmodulesareused,andlayoutscontainatmosttwoconjuncts.oneweaknessofthisbasicframeworkisadif   -cultymodelingpriorknowledgeaboutanswers(oftheformbearsarebrown).thiskindsoflinguis-tic   prior   isessentialforthevqatask,andeasilyincorporated.wesimplyintroduceanextrahiddenlayerforrecombiningthe   nalmodulenetworkout-putwiththeinputsentencerepresentationhq(x)(seeequation8),replacingequation1with:logpz(y|w,x)=(ahq(x)+bjzkw)y(12)(nowmoduleswithoutputtypelabelsshouldbeunderstoodasproducingananswerembeddingratherthanadistributionoveranswers.)thisallowsthequestiontoin   uencetheanswerdirectly.resultsareshownintable1.theuseofdy-namicnetworksprovidesasmallgain,mostnotice-ablyonyes/noquestions.weachievestate-of-the-artresultsonthistask,outperformingahighlyeffec-tivevisualbag-of-wordsmodel(zhouetal.,2015),amodelwithdynamicnetworkparameterprediction(but   xednetworkstructure)(nohetal.,2015),andapreviousapproachusingneuralmodulenetworkswithnostructureprediction(andreasetal.,2016).forthislastmodel,wereportboththenumbersfromtheoriginalpaper,andareimplementationofthemodelthatusesthesameimagepreprocessingasthedynamicmodulenetworkexperimentsinthispaper.amoreconventionalattentionalmodelhasalsobeenappliedtothistask(yangetal.,2015);whilewealsooutperformtheirreportedperformance,theevalua-tionusesdifferenttrain/testsplit,soresultsarenotdirectlycomparable.1552

accuracymodelgeoqageoqa+qlsp-f48   lsp-w51   nmn51.735.7d-nmn54.342.9table2:resultsonthegeoqadataset,andthegeoqadatasetwithquanti   cation.ourapproachoutperformsbothapurelylogicalmodel(lsp-f)andamodelwithlearnedpercep-tualpredicates(lsp-w)ontheoriginaldataset,anda   xed-structurenmnunderbothevaluationconditions.someexamplesareshowninfigure4.ingeneral,themodellearnstofocusonthecorrectregionoftheimage,andtendstoconsiderabroadwindowaroundtheregion.thisfacilitatesansweringquestionslikewhereisthecat?,whichrequiresknowledgeofthesurroundingsaswellastheobjectinquestion.5.2questionsaboutgeographythenextsetofexperimentsweconsiderfocusesongeoqa,ageographicalquestion-answeringtask   rstintroducedbykrishnamurthyandkollar(2013).thistaskwasoriginallypairedwithavi-sualquestionansweringtaskmuchsimplerthantheonejustdiscussed,andisappealingforanumberofreasons.incontrasttothevqadataset,geoqaisquitesmall,containingonly263examples.twobaselinesareavailable:oneusingaclassicalse-manticparserbackedbyadatabase,andanotherwhichinduceslogicalpredicatesusinglinearclas-si   ersoverbothspatialanddistributionalfeatures.thisallowsustoevaluatethequalityofourmodelrelativetootherperceptuallygroundedlogicalse-mantics,aswellasstrictlylogicalapproaches.thegeoqadomainconsistsofasetofentities(e.g.states,cities,parks)whichparticipateinvari-ousrelations(e.g.north-of,capital-of).herewetaketheworldrepresentationtoconsistoftwopieces:asetofcategoryfeatures(usedbythefindmodule)andadifferentsetofrelationalfeatures(usedbytherelatemodule).forourexperiments,weuseasub-setofthefeaturesoriginallyusedbykrishnamurthyetal.theoriginaldatasetincludesnoquanti   ers,andtreatsthequestionswhatcitiesareintexas?andarethereanycitiesintexas?identically.be-causeweareinterestedintestingtheparser   sabilitytopredictavarietyofdifferentstructures,weintro-iskeylargoanisland?(exists(andlookup[key-largo]find[island]))yes:correctwhatnationalparksareinflorida?(andfind[park](relate[in]lookup[florida]))everglades:correctwhataresomebeachesinflorida?(exists(andlookup[beach](relate[in]lookup[florida])))yes(daytona-beach):wrongparsewhatbeachcityisthereinflorida?(andlookup[beach]lookup[city](relate[in]lookup[florida]))[none](daytona-beach):wrongmodulebehaviorfigure5:examplelayoutsandanswersselectedbythemodelonthegeoqadataset.forincorrectpredictions,thecorrectanswerisshowninparentheses.duceanewversionofthedataset,geoqa+q,whichdistinguishesthesetwocases,andexpectsabooleananswertoquestionsofthesecondkind.resultsareshownintable2.asintheorig-inalwork,wereporttheresultsofleave-one-environment-outcross-validationonthesetof10en-vironments.ourdynamicmodel(d-nmn)outper-formsboththelogical(lsp-f)andperceptualmod-els(lsp-w)describedby(krishnamurthyandkol-lar,2013),aswellasa   xed-structureneuralmod-ulenet(nmn).thisimprovementisparticularlynotableonthedatasetwithquanti   ers,wheredy-namicstructurepredictionproducesa20%relativeimprovementoverthe   xedbaseline.avarietyofpredictedlayoutsareshowninfigure5.6conclusionwehaveintroducedanewmodel,thedynamicneu-ralmodulenetwork,foransweringqueriesaboutbothstructuredandunstructuredsourcesofinforma-tion.givenonly(question,world,answer)triplesastrainingdata,themodellearnstoassembleneu-ralnetworksonthe   yfromaninventoryofneuralmodels,andsimultaneouslylearnsweightsforthesemodulessothattheycanbecomposedintonovelstructures.ourapproachachievesstate-of-the-artresultsontwotasks.webelievethatthesuccessofthisworkderivesfromtwofactors:1553

continuousrepresentationsimprovetheexpres-sivenessandlearnabilityofsemanticparsers:byre-placingdiscretepredicateswithdifferentiableneuralnetworkfragments,webypassthechallengingcom-binatorialoptimizationproblemassociatedwithin-ductionofasemanticlexicon.instructuredworldrepresentations,neuralpredicaterepresentationsal-lowthemodeltoinventreusableattributesandre-lationsnotexpressedintheschema.perhapsmoreimportantly,wecanextendcompositionalquestion-answeringmachinerytocomplex,continuousworldrepresentationslikeimages.semanticstructurepredictionimprovesgeneral-izationindeepnetworks:byreplacinga   xednet-worktopologywithadynamicone,wecantailorthecomputationperformedtoeachprobleminstance,usingdeepernetworksformorecomplexquestionsandrepresentingcombinatoriallymanyquerieswithcomparativelyfewparameters.inpractice,thisre-sultsinconsiderablegainsinspeedandsampleef   -ciency,evenwithverylittletrainingdata.theseobservationsarenotlimitedtothequestionansweringdomain,andweexpectthattheycanbeappliedsimilarlytotaskslikeinstructionfollowing,gameplaying,andlanguagegeneration.acknowledgmentsjaissupportedbyanationalsciencefoundationgraduatefellowship.mrissupportedbyafellow-shipwithinthefitweltweit-programofthegermanacademicexchangeservice(daad).thisworkwasadditionallysupportedbydarpa,afrl,dodmuriawardn000141110688,nsfawardsiis-1427425andiis-1212798,andtheberkeleyvisionandlearningcenter.referencesjacobandreas,andreasvlachos,andstephenclark.2013.semanticparsingasmachinetranslation.inproceedingsoftheannualmeetingoftheassociationforcomputationallinguistics,so   a,bulgaria.jacobandreas,marcusrohrbach,trevordarrell,anddanklein.2016.neuralmodulenetworks.inpro-ceedingsoftheconferenceoncomputervisionandpatternrecognition.stanislawantol,aishwaryaagrawal,jiasenlu,mar-garetmitchell,dhruvbatra,clawrencezitnick,anddeviparikh.2015.vqa:visualquestionanswer-ing.inproceedingsoftheinternationalconferenceoncomputervision.jonathanberantandpercyliang.2014.semanticpars-ingviaid141.inproceedingsoftheannualmeetingoftheassociationforcomputationallinguis-tics,volume7,page92.antoinebordes,sumitchopra,andjasonweston.2014.questionansweringwithsubgraphembeddings.pro-ceedingsoftheconferenceonempiricalmethodsinnaturallanguageprocessing.l  eonbottou.2014.frommachinelearningtomachinereasoning.machinelearning,94(2):133   149.marie-catherinedemarneffeandchristopherdman-ning.2008.thestanfordtypeddependenciesrepre-sentation.inproceedingsoftheinternationalconfer-enceoncomputationallinguistics,pages1   8.edwardgrefenstette.2013.towardsaformaldistribu-tionalsemantics:simulatinglogicalcalculiwithten-sors.jointconferenceonlexicalandcomputationalsemantics.karlmoritzhermann,tomaskocisky,edwardgrefen-stette,lasseespeholt,willkay,mustafasuleyman,andphilblunsom.2015.teachingmachinestoreadandcomprehend.inadvancesinneuralinformationprocessingsystems,pages1684   1692.mohitiyyer,jordanboyd-graber,leonardoclaudino,richardsocher,andhaldaum  eiii.2014.aneu-ralnetworkforfactoidquestionansweringoverpara-graphs.inproceedingsoftheconferenceonempiri-calmethodsinnaturallanguageprocessing.jayantkrishnamurthyandthomaskollar.2013.jointlylearningtoparseandperceive:connectingnaturallan-guagetothephysicalworld.transactionsoftheasso-ciationforcomputationallinguistics.jayantkrishnamurthyandtommitchell.2013.vec-torspacesemanticparsing:aframeworkforcompo-sitionalvectorspacemodels.inproceedingsoftheaclworkshoponcontinuousvectorspacemodelsandtheircompositionality.tomkwiatkowski,lukezettlemoyer,sharongoldwa-ter,andmarksteedman.2010.inducingprobabilis-ticid35grammarsfromlogicalformwithhigher-orderuni   cation.inproceedingsoftheconferenceonempiricalmethodsinnaturallanguageprocess-ing,pages1223   1233,cambridge,massachusetts.tomkwiatkowski,eunsolchoi,yoavartzi,andlukezettlemoyer.2013.scalingsemanticparserswithon-the-   yontologymatching.inproceedingsofthecon-ferenceonempiricalmethodsinnaturallanguageprocessing.percyliang,michaeljordan,anddanklein.2011.learningdependency-basedcompositionalsemantics.1554

inproceedingsofthehumanlanguagetechnologyconferenceoftheassociationforcomputationallin-guistics,pages590   599,portland,oregon.mateuszmalinowski,marcusrohrbach,andmariofritz.2015.askyourneurons:aneural-basedapproachtoansweringquestionsaboutimages.inproceedingsoftheinternationalconferenceoncomputervision.cynthiamatuszek,nicholasfitzgerald,lukezettle-moyer,liefengbo,anddieterfox.2012.ajointmodeloflanguageandperceptionforgroundedat-tributelearning.ininternationalconferenceonma-chinelearning.hyeonwoonoh,paulhongsuckseo,andbohyunghan.2015.imagequestionansweringusingconvolutionalneuralnetworkwithdynamicparameterprediction.arxivpreprintarxiv:1511.05756.panupongpasupatandpercyliang.2015.composi-tionalsemanticparsingonsemi-structuredtables.inproceedingsoftheannualmeetingoftheassociationforcomputationallinguistics.mengyeren,ryankiros,andrichardzemel.explor-ingmodelsanddataforimagequestionanswering.inadvancesinneuralinformationprocessingsystems.ksimonyanandazisserman.2014.verydeepcon-volutionalnetworksforlarge-scaleimagerecognition.arxivpreprintarxiv:1409.1556.richardsocher,johnbauer,christopherd.manning,andandrewy.ng.2013.parsingwithcompositionalvectorgrammars.inproceedingsoftheannualmeet-ingoftheassociationforcomputationallinguistics.ronaldjwilliams.1992.simplestatisticalgradient-followingalgorithmsforconnectionistreinforcementlearning.machinelearning,8(3-4):229   256.yukwahwongandraymondj.mooney.2007.learn-ingsynchronousgrammarsforsemanticparsingwithlambdacalculus.inproceedingsoftheannualmeet-ingoftheassociationforcomputationallinguistics,volume45,page960.huijuanxuandkatesaenko.2015.ask,attendandanswer:exploringquestion-guidedspatialatten-tionforvisualquestionanswering.arxivpreprintarxiv:1511.05234.kelvinxu,jimmyba,ryankiros,kyunghyuncho,aaroncourville,ruslansalakhutdinov,richardzemel,andyoshuabengio.2015.show,attendandtell:neuralimagecaptiongenerationwithvisualattention.ininternationalconferenceonmachinelearning.zichaoyang,xiaodonghe,jianfenggao,lideng,andalexsmola.2015.stackedattentionnet-worksforimagequestionanswering.arxivpreprintarxiv:1511.02274.pengchengyin,zhengdonglu,hangli,andbenkao.2015.neuralenquirer:learningtoquerytables.arxivpreprintarxiv:1512.00965.matthewdzeiler.2012.adadelta:anadaptivelearningratemethod.arxivpreprintarxiv:1212.5701.boleizhou,yuandongtian,sainbayarsukhbaatar,arthurszlam,androbfergus.2015.simplebase-lineforvisualquestionanswering.arxivpreprintarxiv:1512.02167.