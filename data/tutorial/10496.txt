reasoning about pragmatics with neural listeners and speakers

jacob andreas and dan klein

computer science division

university of california, berkeley

{jda,klein}@cs.berkeley.edu

6
1
0
2

 

p
e
s
6
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
2
6
5
0
0

.

4
0
6
1
:
v
i
x
r
a

abstract

we present a model for contrastively describ-
ing scenes, in which context-speci   c behav-
ior results from a combination of id136-
driven pragmatics and learned semantics. like
previous learned approaches to language gen-
eration, our model uses a simple feature-
driven architecture (here a pair of neural    lis-
tener    and    speaker    models) to ground lan-
guage in the world. like id136-driven ap-
proaches to pragmatics, our model actively
reasons about listener behavior when select-
ing utterances. for training, our approach re-
quires only ordinary captions, annotated with-
out demonstration of the pragmatic behavior
the model ultimately exhibits. in human eval-
uations on a referring expression game, our
approach succeeds 81% of the time, compared
to 69% using existing techniques.

introduction

1
we present a model for describing scenes and ob-
jects by reasoning about context and listener behav-
ior. by incorporating standard neural modules for
id162 and id38 into a prob-
abilistic framework for pragmatics, our model gen-
erates rich, contextually appropriate descriptions of
structured world representations.

this paper focuses on a reference game rg

played between a listener l and a speaker s.

1. reference candidates r1 and r2 are re-

vealed to both players.

2. s is secretly assigned a random target t    

{1, 2}.

3. s produces a description d = s(t, r1, r2),

which is shown to l.

4. l chooses c = l(d, r1, r2).
5. both players win if c = t.

(rg)

(a) target

(b) distractor

the owl is sitting in the tree

(c) description

figure 1: sample output from our model. when presented with
a target image (a) in contrast with a distractor image (b), the
model generates a description (c). this description mentions a
tree, the distinguishing object present in (a) but not in (b), and
situates it with respect to other objects and events in the scene.

figure 1 shows an example drawn from a standard
captioning dataset (zitnick et al., 2014).

in order for the players to win, s   s description d
must be pragmatic: it must be informative,    uent,
concise, and must ultimately encode an understand-
ing of l   s behavior. in figure 1, for example, the
owl is wearing a hat and the owl is sitting in the tree
are both accurate descriptions of the target image,
but only the second allows a human listener to suc-
ceed with high id203. rg is the focus of many
papers in the computational pragmatics literature: it
provides a concrete generation task while eliciting a
broad range of pragmatic behaviors, including con-
versational implicature (benotti and traum, 2009)
and context dependence (smith et al., 2013). exist-
ing computational models of pragmatics can be di-
vided into two broad lines of work, which we term
the direct and derived approaches.

direct models (see section 2 for examples) are
based on a representation of s. they learn prag-
matic behavior by example. beginning with datasets
annotated for the speci   c task they are trying to
solve (e.g. examples of humans playing rg), direct

models use feature-based architectures to predict ap-
propriate behavior without a listener representation.
while quite general in principle, such models re-
quire training data annotated speci   cally with prag-
matics in mind; such data is scarce in practice.

derived models, by contrast, are based on a repre-
sentation of l. they    rst instantiate a base listener
l0 (intended to simulate a na    ve, non-pragmatic
they then form a reasoning speaker
listener).
s1, which chooses a description that causes l0
to behave correctly. existing derived models cou-
ple hand-written grammars and hand-engineered lis-
tener models with sophisticated id136 proce-
dures. they exhibit complex behavior, but are re-
stricted to small domains where grammar engineer-
ing is practical.

the approach we present in this paper aims to
capture the best aspects of both lines of work. like
direct approaches, we use machine learning to ac-
quire a complete grounded generation model from
data, without domain knowledge in the form of a
hand-written grammar or hand-engineered listener
model. but like derived approaches, we use this
learning to construct a base model, and embed it
within a higher-order model that reasons about lis-
tener responses. as will be seen, this reasoning step
allows the model to make use of weaker supervision
than previous data-driven approaches, while exhibit-
ing robust behavior in a variety of contexts.

our goal is to build a derived model that scales to
real-world datasets without domain engineering. in-
dependent of the application to rg, our model also
belongs to the family of neural image captioning
models that have been a popular subject of recent
study (xu et al., 2015). nevertheless, our approach
appears to be:

    the    rst such captioning model

to reason

explicitly about listeners

    the    rst learned approach to pragmatics that re-

quires only non-pragmatic training data

following previous work, we evaluate our model
on rg, though the general architecture could be ap-
plied to other tasks where pragmatics plays a core
role. using a large dataset of abstract scenes like
the one shown in figure 1, we run a series of games
with humans in the role of l and our system in the

role of s. we    nd that the descriptions generated by
our model result in correct interpretation 17% more
often than a recent learned baseline system. we use
these experiments to explore various other aspects
of computational pragmatics, including tradeoffs be-
tween adequacy and    uency, and between computa-
tional ef   ciency and expressive power.1

2 related work
direct pragmatics as an example of the direct
approach mentioned in the introduction, fitzgerald
et al. (2013) collect a set of human-generated re-
ferring expressions about abstract representations of
sets of colored blocks. given a set of blocks to
describe, their model directly learns a maximum-
id178 distribution over the set of logical expres-
sions whose denotation is the target set. other re-
search, focused on referring expression generation
from a id161 perspective, includes that of
mao et al. (2015) and kazemzadeh et al. (2014).

derived pragmatics derived approaches, some-
times referred to as    rational speech acts    models,
include those of smith et al. (2013), vogel et al.
(2013), golland et al. (2010), and monroe and potts
(2015). these couple template-driven language gen-
eration with probabilistic or game-theoretic reason-
ing frameworks to produce contextually appropriate
language: intelligent listeners reason about the be-
havior of re   exive speakers, and even higher-order
speakers reason about these listeners. experiments
(frank et al., 2009) show that derived approaches ex-
plain human behavior well, but both computational
and representational issues restrict their application
to simple reference games. they require domain-
speci   c engineering, controlled world representa-
tions, and pragmatically annotated training data.

an extensive literature on computational prag-
matics considers its application to tasks other than
rg, including instruction following (anderson et
al., 1991) and discourse analysis (jurafsky et al.,
1997).

representing language and the world in addi-
tion to the pragmatics literature, the approach pro-

1models, human annotations, and code to generate all tables
and    gures in this paper can be found at http://github.
com/jacobandreas/pragma.

posed in this paper relies extensively on recently de-
veloped tools for multimodal processing of language
and unstructured representations like images. these
includes both id162 models, which select
an image from a collection given a textual descrip-
tion (socher et al., 2014), and neural conditional lan-
guage models, which take a content representation
and emit a string (donahue et al., 2015).

3 approach

our goal is to produce a model that can play the
role of the speaker s in rg. speci   cally, given a
target referent (e.g. scene or object) r and a dis-
tractor r(cid:48), the model must produce a description d
that uniquely identi   es r. for training, we have ac-
cess to a set of non-contrastively captioned referents
{(ri, di)}: each training description di is generated
for its associated referent ri in isolation. there is
no guarantee that di would actually serve as a good
referring expression for ri in any particular context.
we must thus use the training data to ground lan-
guage in referent representations, but rely on reason-
ing to produce pragmatics.

our model architecture is compositional and hi-
erarchical. we begin in section 3.2 by describing a
collection of    modules   : basic computational prim-
itives for mapping between referents, descriptions,
and reference judgments, here implemented as lin-
ear operators or small neural networks. while these
modules appear as substructures in neural architec-
tures for a variety of tasks, we put them to novel use
in constructing a reasoning pragmatic speaker.

section 3.3 describes how to assemble two base
models: a literal speaker, which maps from refer-
ents to strings, and a literal listener, which maps
from strings to reference judgments. section 3.4 de-
scribes how these base models are used to imple-
ment a top-level reasoning speaker: a learned, prob-
abilistic, derived model of pragmatics.

3.1 preliminaries
formally, we take a description d to consist of a se-
quence of words d1, d2, . . . , dn, drawn from a vo-
cabulary of known size. for encoding, we also as-
sume access to a feature representation f (d) of the
sentence (for purposes of this paper, a vector of in-
dicator features on id165s). these two views   as

figure 2: diagrams of modules used to construct speaker and
listener models.    fc    is a fully-connected layer (a matrix multi-
ply) and    relu    is a recti   ed linear unit. the encoder modules
(a,b) map from feature representations (in gray) to embeddings
(in black), while the ranker (c) and describer modules (d) re-
spectively map from embeddings to decisions and strings.

a sequence of words di and a feature vector f (d)   
form the basis of module interactions with language.
referent representations are similarly simple. be-
cause the model never generates referents   only
conditions on them and scores them   a vector-
valued feature representation of referents suf   ces.
our approach is completely indifferent to the na-
ture of this representation. while the experiments
in this paper use a vector of indicator features on
objects and actions present in abstract scenes (fig-
ure 1), it would be easy to instead use pre-trained
convolutional representations for referring to natural
images. as with descriptions, we denote this feature
representation f (r) for referents.

3.2 modules
all listener and speaker models are built from a kit
of simple building blocks for working with multi-
modal representations of images and text:

1. a referent encoder er
2. a description encoder ed
3. a choice ranker r
4. a referent describer d

these are depicted in figure 2, and speci   ed more
formally below. all modules are parameterized by
weight matrices, written with capital letters w1, w2,

sentencefcfcrelusumfcsoftmaxrelufcsoftmaxfcngram	featuresdescref	featuresreferentreferentwordnword<nwordn+1choicereferentdesc(d) referent describer d(a) referent encoder er(b) description encoder ed(c) choice ranker retc.; we refer to the collection of weights for all
modules together as w .

encoders the referent and description encoders
produce a linear embedding of referents and descrip-
tions in a common vector space.

referent encoder:
er(r) = w1f (r)
description encoder: ed(d) = w2f (d)

(1)
(2)

choice ranker the choice ranker takes a string
encoding and a collection of referent encodings, as-
signs a score to each (string, referent) pair, and then
transforms these scores into a distribution over ref-
erents. we write r(ei|e   i, ed) for the id203 of
choosing i in contrast to the alternative; for exam-
ple, r(e2|e1, ed) is the id203 of answering    2   
when presented with encodings e1 and e2.

s1 = w(cid:62)
s2 = w(cid:62)

3   (w4e1 + w5ed)
3   (w4e2 + w5ed)

r(ei|e   i, ed) =

esi

es1 + es2

(3)

(here    is a recti   ed linear activation function.)

referent describer the referent describer takes
an image encoding and outputs a description us-
ing a (feedforward) conditional neural
language
model. we express this model as a distribution
p(dn+1|dn, d<n, er), where dn is an indicator fea-
ture on the last description word generated, d<n is a
vector of indicator features on all other words pre-
viously generated, and er is a referent embedding.
this is a    2-plus-skip-gram    model, with local posi-
tional history features, global position-independent
history features, and features on the referent being
described. to implement this id203 distribu-
tion, we    rst use a multilayer id88 to com-
pute a vector of scores s (one si for each vocabulary
item): s = w6  (w7[dn, d<n, ei]). we then normal-
j esj .
finally, p(dn+1|dn, d<n, er) = pdn+1.
3.3 base models
from these building blocks, we construct a pair of
base models. the    rst of these is a literal listener
l0, which takes a description and a set of referents,
and chooses the referent most likely to be described.

ize these to obtain probabilities: pi = esi/(cid:80)

figure 3: schematic depictions of models. the literal listener
l0 maps from descriptions and reference candidates to ref-
erence decisions. the literal speaker s0 maps directly from
scenes to descriptions, ignoring context, while the reasoning
speaker uses samples from s0 and scores from both l0 and s0
to produce contextually-appropriate captions.

this serves the same purpose as the base listener in
the general derived approach described in the intro-
duction. we additionally construct a literal speaker
s0, which takes a referent in isolation and outputs a
description. the literal speaker is used for ef   cient
id136 over the space of possible descriptions, as
described in section 3.4. l0 is, in essence, a retrieval
model, and s0 is neural captioning model.

both of the base models are probabilistic: l0 pro-
duces a distribution over referent choices, and s0
produces a distribution over strings. they are de-
picted with shaded backgrounds in figure 3.
literal listener given a description d and a pair of
candidate referents r1 and r2, the literal listener em-
beds both referents and passes them to the ranking
module, producing a distribution over choices i.

ed = ed(d)
e1 = er(r1)
e2 = er(r2)

pl0(i|d, r1, r2) = r(ei|e   i, ed)

(4)
that is, pl0(1|d, r1, r2) = r(e1|e2, ed) and vice-
versa. this model is trained contrastively, by solving
the following optimization problem:

log pl0(1|dj, rj, r(cid:48))

(5)

(cid:88)

j

max

w

here r(cid:48) is a random distractor chosen uniformly
from the training set.
for each training exam-
ple (ri, di), this objective attempts to maximize the

desc.	encoderref.	encoderref.	encoderrankerref.	decoderref.	encoderliteral listener (l0)literal speaker (s0)reasoning speaker (s1)s0l0samplerid203 that the model chooses ri as the referent
of di over a random distractor.

this contrastive objective ensures that our ap-
proach is applicable even when there is not a
naturally-occurring source of target   distractor pairs,
as previous work (golland et al., 2010; monroe and
potts, 2015) has required. note that this can also be
viewed as a version of the loss described by smith
and eisner (2005), where it approximates a likeli-
hood objective that encourages l0 to prefer ri to ev-
ery other possible referent simultaneously.

literal speaker as in the    gure,
the literal
speaker is obtained by composing a referent encoder
with a describer, as follows:

e = er(f (r))
ps0(d|r) = dd(d|e)

as with the listener, the literal speaker should be un-
derstood as producing a distribution over strings. it
is trained by maximizing the conditional likelihood
of captions in the training data:

log ps0(di|ri)

(6)

(cid:88)

i

max

w

these base models are intended to be the minimal
learned equivalents of the hand-engineered speak-
ers and hand-written grammars employed in previ-
ous derived approaches (golland et al., 2010). the
neural encoding/decoding framework implemented
by the modules in the previous subsection provides
a simple way to map from referents to descriptions
and descriptions to judgments without worrying too
much about the details of syntax or semantics. past
work amply demonstrates that neural conditional
language models are powerful enough to generate
   uent and accurate (though not necessarily prag-
matic) descriptions of images or structured represen-
tations (donahue et al., 2015).

3.4 reasoning model
as described in the introduction, the general derived
approach to pragmatics constructs a base listener
and then selects a description that makes it behave
correctly. since the assumption that listeners will
behave deterministically is often a poor one, it is
common for such derived approaches to implement

probabilistic base listeners, and maximize the prob-
ability of correct behavior.

the neural literal listener l0 described in the pre-
ceding section is such a probabilistic listener. given
a target i and a pair of candidate referents r1 and r2,
it is natural to specify the behavior of a reasoning
speaker as simply:

pl0(i|d, r1, r2)

max

d

(7)

at a    rst glance, the only thing necessary to im-
plement this model is the representation of the literal
listener itself. when the set of possible utterances
comes from a    xed vocabulary (vogel et al., 2013)
or a grammar small enough to exhaustively enumer-
ate (smith et al., 2013) the operation maxd in equa-
tion 7 is practical.

for our purposes, however, we would like the
model to be capable of producing arbitrary utter-
ances. because the score pl0 is produced by a
discriminative listener model, and does not factor
along the words of the description, there is no dy-
namic program that enables ef   cient id136 over
the space of all strings.

we instead use a sampling-based optimization
procedure. the key ingredient here is a good pro-
posal distribution from which to sample sentences
likely to be assigned high weight by the model lis-
tener. for this we turn to the literal speaker s0
described in the previous section. recall that this
speaker produces a distribution over plausible de-
scriptions of isolated images, while ignoring prag-
matic context. we can use it as a source of candi-
date descriptions, to be reweighted according to the
expected behavior of l0. the full speci   cation of a
sampling neural reasoning speaker is as follows:
1. draw samples d1, . . . dn     ps0(  |ri).
2. score samples: pk = pl0(i|dk, r1, r2).
3. select dk with k = arg max pk.

while primarily to enable ef   cient id136, we
can also use the literal speaker to serve a differ-
ent purpose:    regularizing    model behavior towards
choices that are adequate and    uent, rather than ex-
ploiting strange model behavior. past work has re-
stricted the set of utterances in a way that guaran-
tees    uency. but with an imperfect learned listener
model, and a procedure that optimizes this listener   s

judgments directly, the speaker model might acci-
dentally discover the kinds of pathological optima
that neural classi   cation models are known to ex-
hibit (goodfellow et al., 2014)   in this case, sen-
tences that cause exactly the right response from l0,
but no longer bear any resemblance to human lan-
guage use. to correct this, we allow the model to
consider two questions: as before,    how likely is
it that a listener would interpret this sentence cor-
rectly?   , but additionally    how likely is it that a
speaker would produce it?   

formally, we introduce a parameter    that trades
off between l0 and s0, and take the reasoning model
score in step 2 above to be:

pk = ps0(dk|ri)      pl0(i|dk, r1, r2)1     

(8)
this can be viewed as a weighted joint id203
that a sentence is both uttered by the literal speaker
and correctly interpreted by the literal listener, or al-
ternatively in terms of grice   s conversational max-
ims (grice, 1970): l0 encodes the maxims of qual-
ity and relation, ensuring that the description con-
tains enough information for l to make the right
choice, while s0 encodes the maxim of manner, en-
suring that the description conforms with patterns of
human language use. responsibility for the maxim
of quantity is shared: l0 ensures that the model
doesn   t say too little, and s0 ensures that the model
doesn   t say too much.
4 evaluation
we evaluate our model on the reference game rg
described in the introduction. in particular, we con-
struct instances of rg using the abstract scenes
dataset introduced by zitnick and parikh (2013).
example scenes are shown in figure 1 and figure
4. the dataset contains pictures constructed by hu-
mans and described in natural language. scene rep-
resentations are available both as rendered images
and as feature representations containing the identity
and location of each object; as noted in section 3.1,
we use this feature set to produce our referent rep-
resentation f (r). this dataset was previously used
for a variety of language and vision tasks (e.g. or-
tiz et al. (2015), zitnick et al. (2014)). it consists of
10,020 scenes, each annotated with up to 6 captions.
the abstract scenes dataset provides a more chal-
lenging version of rg than anything we are aware of

in the existing computational pragmatics literature,
which has largely used the tuna corpus of isolated
object descriptions (gatt et al., 2007) or small syn-
thetic datasets (smith et al., 2013). by contrast, the
abstract scenes data was generated by humans look-
ing at complex images with numerous objects, and
features grammatical errors, misspellings, and a vo-
cabulary an order of magnitude larger than tuna.
unlike previous work, we have no prespeci   ed in-
domain grammar, and no direct supervision of the
relationship between scene features and lexemes.

we perform a human evaluation using amazon
mechanical turk. we begin by holding out a de-
velopment set and a test set; each held-out set con-
tains 1000 scenes and their accompanying descrip-
tions. for each held-out set, we construct two sets of
200 paired (target, distractor) scenes: all, with up to
four differences between paired scenes, and hard,
with exactly one difference between paired scenes.
(we take the number of differences between scenes
to be the number of objects that appear in one scene
but not the other.)

we report two id74. fluency is
determined by showing human raters isolated sen-
tences, and asking them to rate linguistic quality on
a scale from 1   5. accuracy is success rate at rg:
as in figure 1, humans are shown two images and a
model-generated description, and asked to select the
image matching the description.

in the remainder of this section, we measure the
tradeoff between    uency and accuracy that results
from different mixtures of the base models (sec-
tion 4.1), measure the number of samples needed
to obtain good performance from the reasoning lis-
tener (section 4.2), and attempt to approximate the
reasoning listener with a monolithic    compiled    lis-
tener (section 4.3). in section 4.4 we report    nal
accuracies for our approach and baselines.

4.1 how good are the base models?
to measure the performance of the base models,
we draw 10 samples djk for a subset of 100 pairs

# samples

accuracy (%)

1
66

10
75

100
83

1000
85

table 1: s1 accuracy vs. number of samples.

4.2 how many samples are needed?
next we turn to the computational ef   ciency of the
reasoning model. as in all sampling-based infer-
ence, the number of samples that must be drawn
from the proposal is of critical interest   if too many
samples are needed, the model will be too slow to
use in practice. having    xed    = 0.02 in the pre-
ceding section, we measure accuracy for versions of
the reasoning model that draw 1, 10, 100, and 1000
samples. results are shown in table 1. we    nd that
gains continue up to 100 samples.

is reasoning necessary?

4.3
because they do not require complicated id136
procedures, direct approaches to pragmatics typi-
cally enjoy better computational ef   ciency than de-
rived ones. having built an accurate derived speaker,
can we bootstrap a more ef   cient direct speaker?

to explore this, we constructed a    compiled   
speaker model as follows: given reference candi-
dates r1 and r2 and target t, this model produces
embeddings e1 and e2, concatenates them together
into a    contrast embedding    [et, e   t], and then feeds
this whole embedding into a string decoder mod-
ule. like s0, this model generates captions without
the need for discriminative rescoring; unlike s0, the
contrast embedding means this model can in prin-
ciple learn to produce pragmatic captions, if given
access to pragmatic training data. since no such
training data exists, we train the compiled model on
captions sampled from the reasoning speaker itself.
this model is evaluated in table 3. while the
distribution of scores is quite different from that

(a) target

(b) distractor

(prefer l0)

(prefer s0)

a hamburger on the ground
0.0
0.1 mike is holding the burger
0.2

the airplane is in the sky

figure 5: captions for the same pair with varying   . changing
   alters both the naturalness and speci   city of the output.

figure 5: tradeoff between speaker and listener models, con-
trolled by the parameter    in equation 8. with    = 0, all weight
is placed on the literal listener, and the model produces highly
discriminative but somewhat dis   uent captions. with    = 1, all
weight is placed on the literal speaker, and the model produces
   uent but generic captions.

(r1,j, r2,j) in the dev-all set. we collect human    u-
ency and accuracy judgments for each of the 1000
total samples. this allows us to conduct a post-hoc
search over values of   : for a range of   , we com-
pute the average accuracy and    uency of the high-
est scoring sample. by varying   , we can view the
tradeoff between accuracy and    uency that results
from interpolating between the listener and speaker
model   setting    = 0 gives samples from pl0, and
   = 1 gives samples from ps0.

figure 5 shows the resulting accuracy and    uency
for various values of   . it can be seen that relying
entirely on the listener gives the highest accuracy
but degraded    uency. however, by adding only a
very small weight to the speaker model, it is possible
to achieve near-perfect    uency without a substantial
decrease in accuracy. example sentences for an in-
dividual reference game are shown in figure 5; in-
creasing    causes captions to become more generic.
for the remaining experiments in this paper, we take
   = 0.02,    nding that this gives excellent perfor-
mance on both metrics.

on the development set,    = 0.02 results in an
average    uency of 4.8 (compared to 4.8 for the lit-
eral speaker    = 1). this high    uency can be con-
   rmed by inspection of model samples (figure 4).
we thus focus on accuracy or the remainder of the
evaluation.

(a) the sun is in the sky

[contrastive]

(d) the plane is    ying in the sky

[contrastive]

(c) the dog is standing beside jenny

[contrastive]

(b) mike is wearing a chef   s hat

[non-contrastive]

figure 4: figure 4: four randomly-chosen samples from our model. for each, the target image is shown on the left, the distractor
image is shown on the right, and description generated by the model is shown below. all descriptions are    uent, and generally
succeed in uniquely identifying the target scene, even when they do not perfectly describe it (e.g. (c)). these samples are broadly
representative of the model   s performance (table 2).

dev acc. (%)

test acc. (%)

model

literal (s0)
contrastive
reasoning (s1)

all

66
71
83

hard

54
54
73

all

64
69
81

hard

53
58
68

# of differences

1

50
64
44

2

66
86
72

3

70
88
80

4 mean

78
94
80

66 (%)
83
69

literal (s0)
reasoning
compiled (s1)

table 2: success rates at rg on abstract scenes.    literal    is
a captioning baseline corresponding to the base speaker s0.
   contrastive    is a reimplementation of the approach of mao
et al. (2015).    reasoning    is the model from this paper. all
differences between our model and baselines are signi   cant
(p < 0.05, binomial).

of the base model (it improves noticeably over s0
on scenes with 2   3 differences), the overall gain is
negligible (the difference in mean scores is not sig-
ni   cant). the compiled model signi   cantly under-
performs the reasoning model. these results sug-
gest either that the reasoning procedure is not easily
approximated by a shallow neural network, or that
example descriptions of randomly-sampled training
pairs (which are usually easy to discriminate) do not
provide a strong enough signal for a re   ex learner to
recover pragmatic behavior.

table 3: comparison of the    compiled    pragmatic speaker
model with literal and explicitly reasoning speakers. the mod-
els are evaluated on subsets of the development set, arranged by
dif   culty: column headings indicate the number of differences
between the target and distractor scenes.

evaluate on the test set, comparing this reason-
ing model s1 to two baselines: literal, an image
captioning model trained normally on the abstract
scene captions (corresponding to our l0), and con-
trastive, a model trained with a soft contrastive ob-
jective, and previously used for visual referring ex-
pression generation (mao et al., 2015).

results are shown in table 2. our reasoning
model outperforms both the literal baseline and pre-
vious work by a substantial margin, achieving an im-
provement of 17% on all pairs set and 15% on hard
pairs.2 figures 4 and 6 show various representative

4.4 final evaluation
based on the following sections, we keep    = 0.02
and use 100 samples to generate predictions. we

2 for comparison, a model with hand-engineered pragmatic
behavior   trained using a feature representation with indicators
on only those objects that appear in the target image but not the
distractor   produces an accuracy of 78% and 69% on all and

(a)

(b)

(c)

(b vs. a) mike is holding a baseball bat
(b vs. c)

the snake is slithering away from mike and jenny

figure 6: descriptions of the same image in different contexts.
when the target scene (b) is contrasted with the left (a), the
system describes a bat; when the target scene is contrasted with
the right (c), the system describes a snake.

descriptions from the model.

5 conclusion

we have presented an approach for learning to gen-
erate pragmatic descriptions about general referents,
even without training data collected in a pragmatic
context. our approach is built from a pair of sim-
ple neural base models, a listener and a speaker, and
a high-level model that reasons about their outputs
in order to produce pragmatic descriptions.
in an
evaluation on a standard referring expression game,
our model   s descriptions produced correct behavior
in human listeners signi   cantly more often than ex-
isting baselines.

it is generally true of existing derived approaches
to pragmatics that much of the system   s behavior re-
quires hand-engineering, and generally true of di-
rect approaches (and neural networks in particular)
that training is only possible when supervision is
available for the precise target task. by synthesiz-
ing these two approaches, we address both prob-
lems, obtaining pragmatic behavior without domain
knowledge and without targeted training data. we
believe that this general strategy of using reasoning
to obtain novel contextual behavior from neural de-
coding models might be more broadly applied.

references
anne h. anderson, miles bader, ellen gurman bard,
elizabeth boyle, gwyneth doherty, simon garrod,

hard development pairs respectively.
in addition to perform-
ing slightly worse than our reasoning model, this alternative
approach relies on the structure of scene representations and
cannot be applied to more general pragmatics tasks.

stephen isard, jacqueline kowtko, jan mcallister, jim
miller, et al. 1991. the hcrc map task corpus. lan-
guage and speech, 34(4):351   366.

luciana benotti and david traum. 2009. a computa-
tional account of comparative implicatures for a spo-
ken dialogue agent. in proceedings of the eighth in-
ternational conference on computational semantics,
pages 4   17. proceedings of the annual meeting of the
association for computational linguistics.

jeffrey donahue, lisa anne hendricks, sergio guadar-
rama, marcus rohrbach, subhashini venugopalan,
kate saenko, and trevor darrell. 2015. long-term
recurrent convolutional networks for visual recogni-
tion and description. in proceedings of the conference
on id161 and pattern recognition, pages
2625   2634.

nicholas fitzgerald, yoav artzi, and luke zettlemoyer.
2013. learning distributions over logical forms for
in proceedings of
referring expression generation.
the conference on empirical methods in natural lan-
guage processing.

michael c frank, noah d goodman, peter lai, and
joshua b tenenbaum. 2009. informative communi-
cation in word production and word learning. in pro-
ceedings of the 31st annual conference of the cognitive
science society, pages 1228   1233.

albert gatt, ielka van der sluis, and kees van deemter.
2007. evaluating algorithms for the generation of re-
ferring expressions using a balanced corpus. in pro-
ceedings of the eleventh european workshop on nat-
ural language generation, pages 49   56. proceedings
of the annual meeting of the association for compu-
tational linguistics.

dave golland, percy liang, and dan klein. 2010. a
game-theoretic approach to generating spatial descrip-
in proceedings of the 2010 conference on
tions.
empirical methods in natural language processing,
pages 410   419. association for computational lin-
guistics.

ian goodfellow, jonathon shlens, and christian szegedy.
2014. explaining and harnessing adversarial exam-
ples. arxiv preprint arxiv:1412.6572.

herbert p grice. 1970. logic and conversation.
daniel jurafsky, rebecca bates, noah coccaro, rachel
martin, marie meteer, klaus ries, elizabeth shriberg,
audreas stolcke, paul taylor, van ess-dykema, et al.
1997. automatic detection of discourse structure for
id103 and understanding. in ieee work-
shop on automatic id103 and under-
standing, pages 88   95. ieee.

sahar kazemzadeh, vicente ordonez, mark matten, and
tamara l berg. 2014. referitgame: referring to ob-
jects in photographs of natural scenes. in proceedings

of the conference on empirical methods in natural
language processing, pages 787   798.

junhua mao, jonathan huang, alexander toshev, oana
camburu, alan yuille, and kevin murphy.
2015.
generation and comprehension of unambiguous object
descriptions. arxiv preprint arxiv:1511.02283.

will monroe and christopher potts. 2015. learning in
in proceedings of
the rational speech acts model.
20th amsterdam colloquium, amsterdam, december.
illc.

luis gilberto mateos ortiz, clemens wolff, and mirella
lapata. 2015. learning to interpret and describe ab-
stract scenes. in proceedings of the human language
technology conference of the north american chap-
ter of the association for computational linguistics,
pages 1505   1515.

noah a. smith and jason eisner. 2005. contrastive esti-
mation: training id148 on unlabeled data.
in proceedings of the annual meeting of the associa-
tion for computational linguistics.

nathaniel j smith, noah goodman, and michael frank.
2013. learning and using language via recursive prag-
in advances in
matic reasoning about other agents.
neural information processing systems, pages 3039   
3047.

richard socher, andrej karpathy, quoc v le, christo-
pher d manning, and andrew y ng. 2014. grounded
id152 for    nding and describing
images with sentences. transactions of the associa-
tion for computational linguistics, 2:207   218.

adam vogel, max bodoia, christopher potts, and daniel
jurafsky. 2013. emergence of gricean maxims from
multi-agent decision theory. in proceedings of the hu-
man language technology conference of the north
american chapter of the association for computa-
tional linguistics, pages 1072   1081.

kelvin xu, jimmy ba, ryan kiros, aaron courville,
ruslan salakhutdinov, richard zemel, and yoshua
bengio. 2015. show, attend and tell: neural im-
age id134 with visual attention. arxiv
preprint arxiv:1502.03044.

c zitnick and devi parikh. 2013. bringing semantics
in proceedings
into focus using visual abstraction.
of the conference on id161 and pattern
recognition, pages 3009   3016.

c lawrence zitnick, ramakrishna vedantam, and devi
parikh. 2014. adopting abstract images for semantic
scene understanding.

