accepted as workshop contribution at iclr 2015

diverse embedding neural network
language models

kartik audhkhasi   , abhinav sethy    & bhuvana ramabhadran
ibm t. j. watson research center
yorktown heights, ny 10598, usa
{kaudhkha,asethy,bhuvana}@us.ibm.com
   kartik audhkhasi and abhinav sethy are joint    rst authors.

abstract

we propose diverse embedding neural network (denn), a novel architecture
for language models (lms). a dennlm projects the input word history vec-
tor onto multiple diverse low-dimensional sub-spaces instead of a single higher-
dimensional sub-space as in conventional feed-forward neural network lms. we
encourage these sub-spaces to be diverse during network training through an aug-
mented id168. our id38 experiments on the id32
data set show the performance bene   t of using a dennlm.

1

introduction

diversity of systems trained to perform a given machine learning task is crucial to obtaining a
performance improvement upon fusion (kuncheva (2004)). the problem of id38,
that aims to design predictive models of text, is no exception. several language models (lms)
have been proposed over many years of research (rosen   eld (2000)). simple id165 models esti-
mate the id155 of the i-th word wi in a sentence given the previous n     1 words
(wi   n +1, . . . , wi   1) as

  pid165(wi|wi   n +1, . . . , wi   1) =

c(wi, wi   1, . . . , wi   n +1)

c(wi   1, . . . , wi   n +1)

(1)

where c(.) computes the count of a given word phrase or id165 in the training text1. more com-
plex lms such as feed-forward neural networks (nns) (bengio et al. (2006)) estimate this probabil-
ity as a non-linear function

  pnn(wi|wi   n +1, . . . , wi   1) = f (wi   1, . . . , wi   n +1;   )

(2)
parameterized by   2. researchers have found that fusing different kind of id165 language models
together (goodman (2001), mikolov et al. (2011)) often signi   cantly improves performance. table 1
shows the perplexity3 of 4-gram and nnlms on a standard split of the id32 data set
(marcus et al. (1993)). interpolation of a nnlm with a 4-gram lm gives a 16.9% reduction in
perplexity over a single nnlm even though the two lms have relatively close perplexities. we
use the correlation coef   cient between the posterior probabilities of the predicted word over the
test set from the two models as a simple measure to predict whether the models are diverse. if the
posterior probabilities are highly correlated, then the models are less diverse and smaller gains are
expected from fusion. the posteriors from the id165 and nnlm have a correlation coef   cient of
0.869 which is signi   cantly lower than the correlation coef   cient of 0.988 for a pair of randomly
initialized nnlms. this higher diversity of the nnlm and id165 lm combination results in
signi   cant perplexity improvement upon interpolation.

5
1
0
2

 
r
p
a
5
1

 

 
 
]
l
c
.
s
c
[
 
 

5
v
3
6
0
7

.

2
1
4
1
:
v
i
x
r
a

1almost all id165 models are smoothed in various ways to assign non-zero id203 estimates for word
phrases unseen in training data. we use kneser-ney smoothing (kneser & ney (1995)) for all id165s models
in this paper.

2we will describe the typical architecture of a nnlm in the next section
3perplexity is a standard measure of lm performance and is 2   l where l is the negative average log-

likelihood of the test set text estimated by the lm.

1

accepted as workshop contribution at iclr 2015

table 1: this table shows the test set perplexities of several lms on the id32 test set. a
(x,y) id165 nnlm projects the one-hot vector5of the previous n-1 words onto an x-dimensional
linear sub-space. it then inputs this projected vector into a nn with one hidden layer with y neu-
rons that outputs the posterior id203 of all words in the vocabulary. k nn-randinit denotes
interpolation of k randomly initialized and independently-trained nnlms. the last column shows
the average correlation coef   cient between posteriors of the test set words estimated by the lms in
the ensemble.

lm name

4-gram
1 nn

4 nn-randinit

4-gram + 1 nn

4-gram + 4 nn-randinit

description

perplexity

posterior
corr. coeff.

4-gram kneser-ney smoothed

1x(600,800) 4gm nn-lm

randomly initialized

4x(150,200) 4gm nn-lms

interpolation
interpolation

142.04

140.06 (-1.4%)
134.80 (-5.1%)

116.33 (-18.1%)
116.17 (-18.2%)

n/a
n/a
0.99

0.87
0.88

random initialization and modifying the neural net topology in terms of the embedding and hidden
layer size can be used to build diverse nnlm models. as we can see from table 1, 4 randomly ini-
tialized nnlms (4 nn-randinit) when fused together provide a 5% improvement in perplexity over
the baseline. recurrent nnlm models of different topologies can be fused to get signi   cant gains
as well as demonstrated in (mikolov et al. (2011)). the remarkable bene   t of such simple diversity-
promoting strategies leads us to the central question of this paper - is there a way to explicitly enforce
diversity during nnlm model training? we show that modifying the nnlm architecture and aug-
menting the training id168 achieves that. the fact that a nnlm learns a low-dimensional
continuous space embedding of input words motivates the architecture and training of our proposed
model - diverse embedding neural network (denn) lm.
we    rst give an overview of conventional nnlms in the next section. section 3 presents the
dennlm architecture and its training id168. we presents experiments and results in sec-
tion 4 and conclude the paper in section 5.

2 feed-forward neural network language model (nnlm)

a feed-forward neural network lm (nnlm) converts the one-hot encoding of each word in the
history to a continuous low-dimensional vector representation (bengio et al. (2006)). figure 1 shows
the schematic diagram of a typical nnlm. let wi, . . . , wi   n +1 denote the 1-in-v vectors of the
history words. let rh denote the d    v matrix that projects a history word vectors onto d-
dimensional vectors rh wi, . . . , rh wi   n +1. d is typically much smaller than the size v of the
vocabulary with typical values being v = 10, 000 and d = 500.
this resulting (n     1)d-dimensional continuous representation of input words feeds into a neural
network with one hidden layer that estimates the 1-in-v vector of the target word wi. the hid-
den neuron activation function is hyperbolic tangent or logistic sigmoid while the output neuron
activation function is a v -way softmax. the back-propagation algorithm (rumelhart et al. (1986))
trains an nnlm, often using stochastic id119 where the gradient is computed over several
random subsets or batches of id165s from the input training data.
researchers have proposed several variants of the feed forward nnlm, especially to model a longer
input word history. prominent examples include the recurrent neural network lm (mikolov et al.
(2010)) and bidirectional long-short term memory (lstm) lm (sundermeyer et al. (2012)). these
models offer improved performance but can be slower and more dif   cult to train. even though
we restrict our attention to feed forward nnlms in this paper, the general principles proposed are
applicable to other nnlm architectures as well. the next section introduces the diverse embedding
nnlm (dennlm).

5a one-hot vector of the i-th word in the vocabulary contains 1 at index i and 0 everywhere else.

2

accepted as workshop contribution at iclr 2015

figure 1: this    gure shows the schematic diagram of a 3-gram nnlm. the matrix rh projects the
input history word vectors onto a continuous space. these representations then pass through a nn
with one hidden layer to predict the next word.

3 diverse embedding nnlm

a diverse embedding nnlm (dennlm) aims to learn multiple diverse representations of the in-
put words rather than a single representation. it is    rst important to understand the intuition of a
representation in the context of a nnlm. given a set of n input word vectors wi   1, . . . , wi   n +1,
consider the set of d-dimensional vectors rh wi   1, . . . , rh wi   n +1. the pairwise distances be-
tween vectors of this set constitute the representation of the input words. a good representation
captures the contextual and semantic similarity between pairs of words. similar words are located
close to each other in representation while dissimilar words are located far apart. a nnlm uses this
representation of the input words to predict the next word. the most natural way to ensure diversity
of two nnlms is through the diversity in the representation itself (representational diversity). the
next section discusses an intuitive score to capture representational diversity.

3.1 diversity between nnlm embeddings

maximizing the representational diversity between two nnlms    rst requires an objective score to
capture this diversity. consider the representation rh wi   1, . . . , rh wi   n +1 of the n     1 input
words to the nnlm. since this representation lies in a d-dimensional euclidean space, the set of
all (n     1)(n     2)/2 pairwise angles between n     1 words are suf   cient to uniquely de   ne the
representation even under any af   ne transformation such as translation, rotation, and scaling. the
matrix of pairwise cosine angles between all pairs of points in the representation de   ned by rh1 is

                        

c(rh1) =

1

wt

i   2rt

h1rh1wi   1

||rh1wi   2|| ||rh1wi   2||

...

wt

i   n +1rt

h1rh1wi   1

||rh1wi   n +1|| ||rh1wi   1||

. . .

. . .
...
. . .

wt

i   1rt

h1rh1wi   n +1

||rh1wi   1|| ||rh1wi   n +1||
||rh1wi   2|| ||rh1wi   n +1||

h1rh1wi   n +1

i   2rt

wt

...

1

                         .

this matrix completely de   nes the representation of the word history produced by rh. it is also
independent of the dimensionality of the representation, i.e.
the number of rows of rh. this is
useful when comparing two representations with different dimensionality of the same set of input
data points. given two such representations computed using matrices rh1 and rh2, we de   ne the

3

accepted as workshop contribution at iclr 2015

(cid:16)

(cid:17)t

(cid:16)

(cid:17)

representational diversity as the negative correlation between cosine angles

drep(c(rh1), c(rh2)) =    vec

c(rh1)

vec

c(rh2)

(3)

across the two representations, where vec raster-scans a matrix into a column vector. we note
that drep is bounded because the cosine is bounded between    1 and 1. representational diversity
between rh1 and rh2 increases as drep decreases. in our implementation of distance diversity, for
computation ef   ciency reasons we consider distances over a randomly chosen set of 500 words in
each minibatch instead of the full vocabulary. our experiments show that using the entire vocabulary
for diversity computation gave only minor improvements in perplexity at the expense of much longer
training time.
we are currently exploring several other potential ways to compute diversity between two nnlms
beyond the representational diversity score presented in this paper. the next section discusses the
dennlm architecture and training id168.

3.2 dennlm architecture and training id168

as discussed earlier, a dennlm attempts to learn multiple diverse low-dimensional representations
of the input word history. figure 2 shows the schematic diagram of a dennlm with two diverse
representations. the two representations pass through two separate nns and produce separate pre-
dictions of the next word. the model merges the two predictions to produce the    nal prediction.

figure 2: this    gure shows the schematic diagram of a 3-gram dennlm with two diverse repre-
sentations.

it is important to note that the dennlm is equivalent to a single nnlm with the following block-
structured representation matrix

          r1h
(cid:19)

r2h

0

         
(cid:18) r1p

0

rh =

0 r1h

0 r2h

(cid:18) w1

w =

0
0 w2

rp =

0
0 r2p

(cid:19)

.

and block-diagonal connection weight matrices

the presence of zero entries in the above matrices implies that a dennlm has a smaller number
of parameters than a comparable nnlm. the equivalence between a dennlm and a conventional
nnlm makes the implementation of a dennlm easy within conventional nnlm toolkits.

4

accepted as workshop contribution at iclr 2015

merely constraining the architecture of a nnlm does not guarantee that it learns multiple diverse
representations. hence we augment the training id168 of a conventional nnlm with addi-
tional terms to promote diversity. we use the negative log-likelihood or cross id178 for v -way
classi   cation as the id168 for a conventional nnlm. a dennlm instead uses the following
augmented id168:

t(cid:88)

i=1

(cid:16) m(cid:88)
m(cid:88)

m=1

log

t(cid:88)

(cid:16)

ldennlm =      

  mpm(wi|wi   1, . . . , wi   n +1)

    (1       )
      drep(c(rh1), . . . , c(rhm ))

  m log

m=1

i=1

pm(wi|wi   1, . . . , wi   n +1)

(cid:17)

(cid:17)

(4)

the    rst term of the above id168 is a mixture model loss that ensures that the fused pre-
diction is accurate. using only the    rst term trains a mixture of neural networks. however, this
will not ensure high discrimination ability of the representations learned by the individual networks
because the different representations will only capture the modes of the data distribution. we thus
include the second term, motivated by a recent work on deeply supervised neural networks (lee et al.
(2014)), where the authors augment the conventional loss at the    nal layer with discriminative loses
computed at previous layers. the second term in (4) plays a similar role and makes the individual
representations discriminative as well. the    nal term is the representational diversity of the nnlm
as described in section 3.1. minimizing the dennlm id168 in (4) gives representations that
are jointly discriminative, individually discriminative, and diverse.
we can add l1 and/or l2 id173 penalties to the id168 as well, which are often useful
to prevent over-   tting in case the network size is large compared to the number of training set n-
grams. the next section discusses the experimental setup and results.

4 experiments and results

we conducted id38 experiments on a standard split of the id32 (upenn) data
set (marcus et al. (1993)), as used in previous works such as (mikolov et al. (2011)). the upenn
data set has a vocabulary of 10k words, and contains approximately 1m id165s in the training set.
we used the upenn data set since it is well-studied for id38 and also small enough
to conduct several experiments for understanding the dennlm model better. we implemented
the dennlm in theano (bergstra et al. (2011)) and trained it by optimizing the augmented loss
function in (4) using root-mean square propagation (rmsprop) (tieleman & hinton (2012)), a
variant of stochastic id119. we tuned all hyper-parameters on the standard upenn held-
out set.
table 4 shows the test set perplexities of the baseline and the dennlms. we kept the nn model
size comparable by reducing the size of each component nnlm. our results show a signi   cant
improvement in perplexity by using a dennlm over the 4-gram model, a single nnlm, and in-
terpolation of randomly initialized nnlms. the posterior correlation coef   cients are signi   cantly
less than 0.99, which is the correlation coef   cient for randomly initialized nnlms.
note that the dennlm signi   cantly outperforms a standard nnlm of size (600,800) which has
similar number of parameters. it is also clearly better then a randomly initialized set of 4 nnlm
models. the perplexity results in table 4 for our diverse feed-forward nnlms are especially encour-
aging given the fact that the more advanced recurrent neural network (id56) lm gives a perplexity
of 124.7 by itself and 105.7 upon interpolation with a 5-gram lm on the same task (mikolov et al.
(2011)).

4.1 sensitivity to hyperparameters

we further studied the dependence of dennlm performance on some sets of hyper-parameters and
list our observations below:

5

accepted as workshop contribution at iclr 2015

table 2: this table shows the test set perplexities of baseline and dennlms on the id32
test set.

lm name

description

perplexity

posterior
corr. coeff.

4-gram
1 nn

4 nn-randinit

4 denn

8 denn

4-gram + 1 nn

4-gram + 4 denn
4-gram + 8 denn

4-gram kneser-ney smoothed

1x(600,800) 4gm nn-lm

randomly initialized

4x(150,200) 4gm nn-lms

diversely trained

4x(150,200) 4gm nn-lms

diversely trained

8x(75,100) 4gm nn-lms

interpolation
interpolation
interpolation

142.04

137.32 (-3.3%)
134.85 (-5.1%)

122.69 (-13.6%)

116.41 (-18.0%)

116.33 (-18.1%)
112.79 (-20.6%)
109.32 (-23.0%)

n/a
n/a
0.99

0.89

0.83

0.87
0.88
0.92

    diversity parameters: the performance was fairly stable over a range of    between 0.3 to
0.7 with minor differences of around 2 points in perplexity. the performance with respect
to diversity weight    was also stable in the range 1 to 3
    model parameters: we explored different model sizes for both the single model and
dennlm by changing the dennlm topology (number of hidden neurons, layers etc.).
we observed that in general, a dennlm with the same total size as a single nnlm works
well. we can achieve smaller gains of around 3 to 4 points in perplexity by increasing the
size of the dennlm.
    optimization parameters: we observed that the choice of the optimization algorithm can
have a signi   cant impact on the diversity of models generated with random initialization.
we found that using rmsprop with a signi   cantly higher learning rate, gradient clipping,
and scaling of bias updates while training four randomly-initialized nnlms leads to a
lower posterior posterior correlation of 0.95 compared to 0.99 with our standard optimiza-
tion setting. this higher diversity between models translates to a lower perplexity of 120
with the interpolated nnlm which is comparable to our best results in table 4. this indi-
cates that the choice of hyper-parameters and optimizer settings for building diverse models
via random initialization can be different from the ones used for training the models indi-
vidually.

to further understand the impact of hyper-parameters on dennlm diversity and perplexity, we
performed a thorough grid search of    and    in (4), the learning rate of rmsprop, and the weight of
an l2 penalty on the dennlm connection weights. we then computed the log perplexity and av-
erage cross-correlation between posteriors of individual models in a dennlm. figure 3 shows the
scatter plot between average posterior cross-correlation and percent improvement of the dennlm
log perplexity over the best model   s perplexity. a strong negative correlation of    0.97 in this    gure
shows that more diverse models give a bigger improvement in log perplexity upon interpolation.
this highlights the merit of training diverse nnlm and the fact that one can achieve this diver-
sity by either informed objective functions such as the one in (4) or an exhaustive hyper-parameter
search. the latter becomes especially tedious for deep and complex neural networks trained on big
data sets.

5 conclusion

in this work, we introduced a neural network architecture and training objective function that en-
courages the individual models to be diverse in terms of their output distribution as well as the
underlying word representations. we demonstrated its usefulness on the well-studied upenn lan-
guage modeling task. the proposed training criterion is general enough and does not constrain the
nnlm models to be of any speci   c architecture. given the promising results, our next step is to
evaluate the improved language models on id103 and spoken-term detection tasks. we

6

accepted as workshop contribution at iclr 2015

figure 3: this    gure shows the scatter plot between average posterior cross-correlation and percent
improvement of the dennlm log perplexity over the best model   s log perplexity. we observed
a strong correlation coef   cient of -0.97 between the two variables indicating that more diverse
nnlms give a bigger reduction in perplexity upon interpolation.

also plan to explore the utility of these diverse representations to measure semantic similarity and
for sentence completion, where id27-based models have been shown to be effective.

6 acknowledgement

this work was supported by the intelligence advanced research projects activity (iarpa) via de-
partment of defense u.s. army research laboratory (dod / arl) contract number w911nf-12-
c-0012. the u.s. government is authorized to reproduce and distribute reprints for governmental
purposes notwithstanding any copyright annotation thereon. disclaimer: the views and conclusions
contained herein are those of the authors and should not be interpreted as necessarily representing
the of   cial policies or endorsements, either expressed or implied, of iarpa, dod/arl, or the u.s.
government.

references
bengio, y., schwenk, h., sen  ecal, j., morin, f., and gauvain, j. neural probabilistic language

models. in innovations in machine learning, pp. 137   186. springer, 2006.

bergstra, j., bastien, f., breuleux, o., lamblin, p., pascanu, r., delalleau, o., desjardins, g.,
warde-farley, d., goodfellow, i. j., bergeron, a., and bengio, y. theano: deep learning on gpus
with python. in big learn workshop, nips   11, 2011.

goodman, j. t. a bit of progress in id38. technical report, 2001.

kneser, r. and ney, h. improved backing-off for m-gram id38. in proc. icassp,

volume 1, pp. 181   184. ieee, 1995.

kuncheva, l. i. combining pattern classi   ers: methods and algorithms. john wiley & sons, 2004.

lee, c.-y., xie, s., gallagher, p., zhang, z., and tu, z. deeply-supervised nets. arxiv e-prints,

september 2014.

marcus, m. p., marcinkiewicz, m. a., and santorini, b. building a large annotated corpus of en-

glish: the id32. computational linguistics, 19(2):313   330, 1993.

7

accepted as workshop contribution at iclr 2015

mikolov, t., kara     at, m., burget, l., cernock`y, j., and khudanpur, s. recurrent neural network

based language model. in interspeech, pp. 1045   1048, 2010.

mikolov, t., deoras, a., kombrink, s., burget, l., and cernock`y, j. empirical evaluation and

combination of advanced id38 techniques. in interspeech, pp. 605   608, 2011.

rosen   eld, r. two decades of statistical id38: where do we go from here? proceed-

ings of the ieee, 88, 2000.

rumelhart, d. e., hinton, g. e., and williams, r. j. learning representations by back-propagating

errors. nature, 323:533   536, 1986.

sundermeyer, m., schl  uter, r., and ney, h. lstm neural networks for id38.

interspeech, 2012.

in

tieleman, t. and hinton, g. lecture 6.5   rmsprop: divide the gradient by a running average of its

recent magnitude. coursera: neural networks for machine learning, 2012.

8

