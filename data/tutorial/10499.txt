6
1
0
2

 

p
e
s
3
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
3
9
7
3
0

.

3
0
6
1
:
v
i
x
r
a

training with exploration improves a greedy id200 parser

miguel ballesteros    yoav goldberg    chris dyer    noah a. smith   

   nlp group, pompeu fabra university, barcelona, spain

   computer science department, bar-ilan university, ramat gan, israel

   google deepmind, london, uk

   computer science & engineering, university of washington, seattle, wa, usa

miguel.ballesteros@upf.edu, yoav.goldberg@gmail.com,

cdyer@google.com, nasmith@cs.washington.edu

abstract

the greedy id200 depen-
we adapt
dency parser of dyer et al. (2015) to support a
training-with-exploration procedure using dy-
namic oracles (goldberg and nivre, 2013) in-
stead of assuming an error-free action his-
tory. this form of training, which accounts for
model predictions at training time, improves
parsing accuracies. we discuss some modi   -
cations needed in order to get training with ex-
ploration to work well for a probabilistic neu-
ral network dependency parser.

1 introduction

natural language parsing can be formulated as a se-
ries of decisions that read words in sequence and in-
crementally combine them to form syntactic struc-
tures;
this formalization is known as transition-
based parsing, and is often coupled with a greedy
search procedure (yamada and matsumoto, 2003;
nivre, 2003; nivre, 2004; nivre, 2008). the lit-
erature on transition-based parsing is vast, but all
works share in common a classi   cation component
that takes into account features of the current parser
state1 and predicts the next action to take condi-
tioned on the state. the state is of unbounded size.
dyer et al. (2015) presented a parser in which the
parser   s unbounded state is embedded in a    xed-
dimensional continuous space using recurrent neu-
ral networks. coupled with a recursive tree com-
position function, the feature representation is able

1the term    state    refers to the collection of previous de-
cisions (sometimes called the history), resulting partial struc-
tures, which are typically stored in a stack data structure, and
the words remaining to be processed.

to capture information from the entirety of the state,
without resorting to locality assumptions that were
common in most other transition-based parsers. the
use of a novel id200 data structure allows the
parser to maintain a constant time per-state update,
and retain an overall linear parsing time.

the dyer et al. parser was trained to maximize
the likelihood of gold-standard transition sequences,
given words. at test time, the parser makes greedy
decisions according to the learned model. although
this setup obtains very good performance, the train-
ing and testing conditions are mismatched in the fol-
lowing way: at training time the historical context of
an action is always derived from the gold standard
(i.e., perfectly correct past actions), but at test time,
it will be a model prediction.

in this work, we adapt the training criterion so
as to explore parser states drawn not only from the
training data, but also from the model as it is be-
ing learned. to do so, we use the method of gold-
berg and nivre (2012; 2013) to dynamically chose
an optimal (relative to the    nal attachment accuracy)
action given an imperfect history. by interpolating
between algorithm states sampled from the model
and those sampled from the training data, more ro-
bust predictions at test time can be made. we show
that the technique can be used to improve the strong
parser of dyer et al.

2 parsing model and parameter learning

our departure point is the parsing model described
by dyer et al. (2015). we do not describe the model
in detail, and refer the reader to the original work. at
each stage t of the parsing process, the parser state is

encoded into a vector pt, which is used to compute
the id203 of the parser action at time t as:

p(zt | pt) =

exp (cid:0)g

   
zt pt + qzt(cid:1)

pz       a(s,b) exp (cid:0)g

   
z     pt + qz    (cid:1)

,

(1)

where gz is a column vector representing the (out-
put) embedding of the parser action z, and qz is a
bias term for action z. the set a(s, b) represents
the valid transition actions that may be taken in the
current state. since pt encodes information about all
previous decisions made by the parser, the chain rule
gives the id203 of any valid sequence of parse
transitions z conditional on the input:

p(z | w) =

|z|
y
t=1

p(zt | pt).

(2)

the parser is trained to maximize the conditional
id203 of taking a    correct    action at each pars-
ing state. the de   nition of what constitutes a    cor-
rect    action is the major difference between a static
oracle as used by dyer et al. (2015) and the dynamic
oracle explored here.

regardless of the oracle, our training implemen-
tation constructs a computation graph (nodes that
represent values,
linked by directed edges from
each function   s inputs to its outputs) for the neg-
ative log id203 for the oracle transition se-
quence as a function of the current model parame-
ters and uses forward- and id26 to ob-
tain the gradients respect to the model parameters
(lecun et al., 1998, section 4).

2.1 training with static oracles
with a static oracle, the training procedure com-
putes a canonical reference series of transitions for
each gold parse tree. it then runs the parser through
this canonical sequence of transitions, while keep-
ing track of the state representation pt at each step t,
as well as the distribution over transitions p(zt | pt)
which is predicted by the current classi   er for the
state representation. once the end of the sentence is
reached, the parameters are updated towards maxi-
mizing the likelihood of the reference transition se-
quence (equation 2), which equates to maximizing
the id203 of the correct transition, p(zgt | pt),
at each state along the path.

is

test

the best

problematic

2.2 training with dynamic oracles
the parser is trained
in the static oracle case,
to predict
transition to take at each
parsing step, assuming all previous transitions
were correct. since the parser is likely to make
time and encounter states it
mistakes at
has not seen during training,
this training cri-
(daum  e iii et al., 2009;
terion
ross et al., 2011;
goldberg and nivre, 2012;
goldberg and nivre, 2013, inter alia). instead, we
would prefer to train the parser to behave optimally
even after making a mistake (under the constraint
that it cannot backtrack or    x any previous de-
cision). we thus need to include in the training
examples states that result from wrong parsing
decisions,
transitions
to take in these states. to this end we reconsider
which training examples to show, and what it means
to behave optimally on these training examples.
the framework of training with exploration using
dynamic oracles suggested by goldberg and nivre
(2012; 2013) provides answers to these questions.
while the application of dynamic oracle training
is
some adaptations
were needed to accommodate the probabilistic
training objective. these adaptations mostly follow
goldberg (2013).

together with the optimal

relatively straightforward,

dynamic oracles. a dynamic oracle is the com-
ponent that, given a gold parse tree, provides the
optimal set of possible actions to take for any valid
parser state. in contrast to static oracles that derive
a canonical state sequence for each gold parse tree
and say nothing about states that deviate from this
canonical path, the dynamic oracle is well de   ned
for states that result from parsing mistakes, and they
may produce more than a single gold action for a
given state. under the dynamic oracle framework,
an action is said to be optimal for a state if the best
tree that can be reached after taking the action is no
worse (in terms of accuracy with respect to the gold
tree) than the best tree that could be reached prior to
taking that action.

goldberg and nivre (2013) de   ne the arc-
decomposition property of transition systems, and
show how to derive ef   cient dynamic oracles for
transition systems that are arc-decomposable.2 un-
2speci   cally: for every parser con   guration p and group of

fortunately, the arc-standard transition system does
not have this property. while it is possible to com-
pute dynamic oracles for the arc-standard system
(goldberg et al., 2014), the computation relies on a
id145 algorithm which is polyno-
mial in the length of the stack. as the dynamic ora-
cle has to be queried for each parser state seen during
training, the use of this dynamic oracle will make
the training runtime several times longer. we chose
instead to switch to the arc-hybrid transition sys-
tem (kuhlmann et al., 2011), which is very similar
to the arc-standard system but is arc-decomposable
and hence admits an ef   cient o(1) dynamic oracle,
resulting in only negligible increase to training run-
time. we implemented the dynamic oracle to the
arc-hybrid system as described by goldberg (2013).

training with exploration.
in order to expose
the parser to con   gurations that are likely to result
from incorrect parsing decisions, we make use of the
probabilistic nature of the classi   er. during training,
instead of following the gold action, we sample the
next transition according to the output distribution
the classi   er assigns to the current con   guration.
another option, taken by goldberg and nivre, is to
follow the one-best action predicted by the classi   er.
however, initial experiments showed that the one-
best approach did not work well. because the neural
network classi   er becomes accurate early on in the
training process, the one-best action is likely to be
correct, and the parser is then exposed to very few
error states in its training process. by sampling from
the predicted distribution, we are effectively increas-
ing the chance of straying from the gold path during
training, while still focusing on mistakes that receive
relatively high parser scores. we believe further for-
mal analysis of this method will reveal connections
to id23 and, perhaps, other meth-
ods for learning complex policies.

taking this idea further, we could increase the
number of error-states observed in the training pro-
cess by changing the sampling distribution so as
to bias it toward more low-id203 states. we
do this by raising each id203 to the power of

   (0 <        1) and re-normalizing. this trans-
formation keeps the relative ordering of the events,
while shifting id203 mass towards less frequent
events. as we show below, this turns out to be very
bene   cial for the con   gurations that make use of
external embeddings.
indeed, these con   gurations
achieve high accuracies and sharp class distributions
early on in the training process.

the parser is trained to maximize the likelihood of
a correct action zg at each parsing state pt according
to equation 1. when using the dynamic oracle, a
state pt may admit multiple correct actions z
g =
{zgi , . . . , zgk }. our objective in such cases is the
marginal likelihood of all correct actions,3

p(z

g | pt) = x
zgi    zg

p(zgi | pt).

(3)

3 experiments

following the same settings of chen and manning
(2014) and dyer et al (2015) we report results4 in
the english ptb and chinese ctb-5. table 1 shows
the results of the parser in its different con   gura-
tions. the table also shows the best result obtained
with the static oracle (obtained by rerunning dyer et
al. parser) for the sake of comparison between static
and dynamic training strategies.

method
arc-standard (dyer et al.)
arc-hybrid (static)
arc-hybrid (dynamic)
arc-hybrid (dyn.,    = 0.75)
+ pre-training:
arc-standard (dyer et al.)
arc-hybrid (static)
arc-hybrid (dynamic)
arc-hybrid (dyn.,    = 0.75)

english

chinese

uas
92.40
92.08
92.66
92.73

93.04
92.78
93.15
93.56

las uas
85.48
90.04
85.66
89.80
90.43
86.07
86.13
90.60

90.87
90.67
91.05
91.42

86.85
86.94
87.05
87.65

las
83.94
84.03
84.46
84.53

85.36
85.46
85.63
86.21

table 1: id33: english (sd) and chinese.

the score achieved by the dynamic oracle for
english is 93.56 uas. this is remarkable given
that the parser uses a completely greedy search
procedure. moreover,
the chinese score estab-
lishes the state-of-the-art, using the same settings as
chen and manning (2014).

arcs a, if each arc in a can be derived from p, then a valid
tree structure containing all of the arcs in a can also be derived
from p. this is a suf   cient condition, but whether it is necessary
is unknown; hence the question of an ef   cient, o(1) dynamic
oracle for the augmented system is open.

3a similar objective was used by riezler et al (2000), char-
niak and johnson (2005) and goldberg (2013) in the context of
log-linear probabilistic models.

4the results on the development sets are similar and only

used for optimization and validation.

catalan

chinese

czech

english

german

japanese

spanish

method
uas
arc-standard, static + pp 89.60
+ pre-training
arc-hybrid, dyn. + pp
+ pre-training
y   15
a   16 + pre-training
a   16-beam

91.24
92.67

90.45

   
   

   

las uas
79.68
85.45
82.45
80.74
83.54

86.38

   

las uas
75.08
77.96
78.55
76.52
79.66

85.68

   

   

   

81.29
84.72

   

77.29
80.85

85.2
85.78
88.94

   
   

88.21
89.83

   

79.38

las uas
91.12
71.06
91.59
91.62
92.22
90.75
91.44
93.22

77.5
80.63
84.56

   

las uas
88.09
88.69
88.56
89.15
89.80
89.23
90.34
89.87
88.14
89.6
89.12
89.29
91.23
90.91

   

93.47

las uas
85.24
93.10
86.15
87.29
88.17
86.0
86.95
89.15

93.71
93.65

   
   

   

92.70

las uas
89.08
92.28
90.76
89.53
91.09
88.3
91.01
92.62

92.85
92.84

   
   

las
85.03
87.48
85.69
87.95
85.4
88.14
89.95

table 2: id33 results. the dynamic oracle uses    = 0.75 (selected on english; see table 1). pp refers to pseudo-
projective parsing. y   15 and a   16 are beam = 1 parsers from yazdani and henderson (2015) and andor et al. (2016), respectively.
a   16-beam is the parser with beam larger than 1 by andor et al. (2016). bold numbers indicate the best results among the greedy
parsers.

the error-exploring dynamic-oracle training al-
ways improves over static oracle training control-
ling for the transition system, but the arc-hybrid sys-
tem slightly under-performs the arc-standard system
when trained with static oracle. flattening the sam-
pling distribution (   = 0.75) is especially bene   cial
when training with pretrained id27s.

in order

to be able to compare with simi-
lar greedy parsers (yazdani and henderson, 2015;
andor et al., 2016)5 we report the performance of
the parser on the multilingual
treebanks of the
conll 2009 shared task (haji  c et al., 2009). since
some of the treebanks contain nonprojective sen-
tences and arc-hybrid does not allow nonprojec-
tive trees, we use the pseudo-projective approach
(nivre and nilsson, 2005). we used predicted part-
of-speech tags provided by the conll 2009 shared
task organizers. we also include results with pre-
trained id27s for english, chinese, ger-
man, and spanish following the same training setup
as dyer et al. (2015); for english and chinese we
used the same pretrained id27s as in ta-
ble 1, for german we used the monolingual training
data from the wmt 2015 dataset and for spanish we
used the spanish gigaword version 3. see table 2.

4 related work

on

greedy

parsers

explored

non-gold
facilitated by dynamic oracles,
researchers

out-
training
has
comes,
several
in
been
different
(goldberg and nivre, 2012;
goldberg and nivre, 2013; goldberg et al., 2014;
honnibal et al., 2013; honnibal and johnson, 2014;

ways

by

5we report the performance of these parsers in the most

comparable setup, that is, with beam size 1 or greedy search.

id203

frameworks

chang et al., 2015).
the

g  omez-rodr    guez et al., 2014;
bj  orkelund and nivre, 2015;
tokg  oz and eryi  git, 2015;
g  omez-rodr    guez and fern  andez-gonz  alez, 2015;
vaswani and sagae, 2016). more generally, training
greedy search systems by paying attention to the ex-
pected classi   er behavior during test time has been
explored under the imitation learning and learning-
(abbeel and ng, 2004;
to-search
daum  e iii and marcu, 2005;
vlachos, 2012;
daum  e iii et al., 2009;
he et al., 2012;
ross et al., 2011;
di-
rectly modeling
of making
a mistake has also been explored for pars-
ing (yazdani and henderson, 2015).
generally,
the use of id56s to conditionally predict ac-
tions in sequence given a history is spurring
increased interest in training regimens that make the
learned model more robust to test-time prediction
errors.
solutions based on curriculum learn-
ing (bengio et al., 2015), expected loss training
(shen et al., 2015), and id23 have
been proposed (ranzato et al., 2016). finally, aban-
doning greedy search in favor of approximate global
search offers an alternative solution to the problems
with greedy search (andor et al., 2016), and has
been analyzed as well (kulesza and pereira, 2007;
finley and joachims, 2008),
including for parsing
(martins et al., 2009).

5 conclusions

dyer et al. (2015) presented id200s and used
them to implement a transition-based dependency
parser. the parser uses a greedy learning strat-
egy which potentially provides very high parsing

speed while still achieving state-of-the-art results.
we have demonstrated that improvement by training
the greedy parser on non-gold outcomes; dynamic
oracles improve the id200 parser, achieving
93.56 uas for english, maintaining greedy search.

acknowledgments

this work was sponsored in part by the u. s. army
research laboratory and the u. s. army research
of   ce under contract/grant number w911nf-10-
1-0533, and in part by nsf career grant iis-
1054319. miguel ballesteros was supported by
the european commission under the contract num-
bers fp7-ict-610411 (project multisensor)
and h2020-ria-645012 (project kristina). yoav
goldberg is supported by the intel collaborative
research institute for computational intelligence
(icri-ci), a google research award and the israeli
science foundation (grant number 1555/15).

references

[abbeel and ng2004] pieter abbeel and andrew y. ng.
2004. apprenticeship learning via inverse reinforce-
ment learning. in proc. of icml.

[andor et al.2016] daniel andor, chris alberti, david
weiss, aliaksei severyn, alessandro presta, kuzman
ganchev, slav petrov, and michael collins. 2016.
globally normalized transition-based neural networks.
in proc. of acl.

[bengio et al.2015] samy bengio,

oriol vinyals,
navdeep jaitly, and noam shazeer. 2015. scheduled
sampling for sequence prediction with recurrent
neural networks. arxiv:1506.03099.

[bj  orkelund and nivre2015] anders bj  orkelund

joakim nivre.
for
id33. in proc. of iwpt.

unrestricted

and
2015. non-deterministic oracles
transition-based

non-projective

[chang et al.2015] kai-wei chang, akshay krishna-
murthy, alekh agarwal, hal daume, and john lang-
ford.
2015. learning to search better than your
teacher. in proc. of icml.

[charniak and johnson2005] eugene charniak and mark
johnson. 2005. coarse-to-   ne n-best parsing and
maxent discriminative reranking. in proc. of acl.

[chen and manning2014] danqi chen

and christo-
pher d. manning.
2014. a fast and accurate
dependency parser using neural networks. in proc. of
emnlp.

[daum  e iii and marcu2005] hal daum  e iii and daniel
marcu. 2005. learning as search optimization: ap-
proximate large margin methods for structured predic-
tion. in proc. of icml.

[daum  e iii et al.2009] hal daum  e iii, john langford,
and daniel marcu. 2009. search-based structured pre-
diction. machine learning, 75:297   325.

[dyer et al.2015] chris dyer, miguel ballesteros, wang
ling, austin matthews, and noah a. smith. 2015.
transition-based id33 with stack long
short-term memory. in proc. of acl.

[finley and joachims2008] t. finley and t. joachims.
2008. training structural id166s when exact id136
is intractable. in in proc. of icml.

[goldberg and nivre2012] yoav goldberg and joakim
nivre. 2012. a dynamic oracle for arc-eager depen-
dency parsing. in proc. of coling.

[goldberg and nivre2013] yoav goldberg and joakim
nivre. 2013. training deterministic parsers with non-
deterministic oracles. transactions of the association
for computational linguistics, 1:403   414.

[goldberg et al.2014] yoav goldberg, francesco sarto-
rio, and giorgio satta. 2014. a tabular method for
dynamic oracles in transition-based parsing. transac-
tions of the association for computational linguistics,
2.

[goldberg2013] yoav goldberg. 2013. dynamic-oracle
transition-based parsing with calibrated probabilistic
output. in proc. of iwpt.

[g  omez-rodr    guez and fern  andez-gonz  alez2015]

carlos g  omez-rodr    guez and daniel fern  andez-
gonz  alez.
2015. an ef   cient dynamic oracle for
unrestricted non-projective parsing. in proc. of acl.

[g  omez-rodr    guez et al.2014] carlos g  omez-rodr    guez,
francesco sartorio, and giorgio satta.
2014. a
polynomial-time dynamic oracle for non-projective
id33. in proc. of emnlp.

[haji  c et al.2009] jan haji  c, massimiliano ciaramita,
richard johansson, daisuke kawahara, maria ant`onia
mart    , llu    s m`arquez, adam meyers, joakim nivre,
sebastian pad  o, jan   st  ep  anek, pavel stra  n  ak, mihai
surdeanu, nianwen xue, and yi zhang. 2009. the
conll-2009 shared task: syntactic and semantic de-
pendencies in multiple languages. in proc. of conll.
[he et al.2012] he he, hal daum  e iii, and jason eisner.

2012. imitation learning by coaching. in nips.
[honnibal and johnson2014] matthew honnibal

and
joint incremental dis   uency
transactions
the association for computational linguistics,

mark johnson. 2014.
detection and id33.
of
2:131   142.

[honnibal et al.2013] matthew honnibal, yoav goldberg,
and mark johnson. 2013. a non-monotonic arc-eager

[vlachos2012] andreas vlachos. 2012. an investigation
of imitation learning algorithms for structured predic-
tion. in proc. of the european workshop on reinforce-
ment learning.

[yamada and matsumoto2003] hiroyasu yamada

yuji matsumoto.
analysis with support vector machines.
iwpt.

2003.

and
statistical dependency
in proc. of

[yazdani and henderson2015] majid yazdani and james
henderson. 2015. incremental recurrent neural net-
work dependency parser with search-based discrimi-
native training. in proc. of conll.

transition system for id33. in proc. of
conll.

[kuhlmann et al.2011] marco kuhlmann, carlos g  omez-
rodr    guez, and giorgio satta. 2011. dynamic pro-
gramming algorithms for transition-based dependency
parsers. in proc. of acl.

[kulesza and pereira2007] a. kulesza and f. pereira.
2007. structured learning with approximate id136.
in nips.

[lecun et al.1998] yann lecun, l  eon bottou, yoshua
bengio, and patrick haffner. 1998. gradient-based
learning applied to document recognition. proceed-
ings of the ieee, 86(11):2278   2324.

[martins et al.2009] andr  e f. t. martins, noah a. smith,
and eric p. xing. 2009. polyhedral outer approxima-
tions with application to natural language parsing. in
proc. of icml.

[nivre and nilsson2005] joakim nivre and jens nilsson.
in

pseudo-projective id33.

2005.
proc. of acl.

[nivre2003] joakim nivre. 2003. an ef   cient algorithm
for projective id33. in proc. of iwpt.
[nivre2004] joakim nivre. 2004. incrementality in de-
terministic id33.
in proceedings of
the workshop on incremental parsing: bringing en-
gineering and cognition together.

[nivre2008] joakim nivre. 2008. algorithms for deter-
ministic incremental id33. computa-
tional linguistics, 34(4):513   553.

[ranzato et al.2016] marc   aurelio

sumit
chopra, michael auli,
and wojciech zaremba.
2016. sequence level training with recurrent neural
networks. in proc. of iclr.

ranzato,

[riezler et al.2000] stefan riezler, detlef prescher, jonas
kuhn, and mark johnson. 2000. lexicalized stochas-
tic modeling of constraint-based grammars using log-
linear measures and em training. in proc. of acl.

[ross et al.2011] st  ephane ross, geoffrey j. gordon, and
j. andrew bagnell. 2011. a reduction of imitation
learning and id170 to no-regret online
learning. in proc. of aistat.

[shen et al.2015] shiqi shen, yong cheng, zhongjun he,
wei he, hua wu, maosong sun, and yang liu. 2015.
minimum risk training for id4.
in proc. of acl.

[tokg  oz and eryi  git2015] alper tokg  oz

and g  uls  en
eryi  git. 2015. transition-based dependency dag
parsing using dynamic oracles. in proc. of acl srw.
[vaswani and sagae2016] ashish vaswani and kenji
ef   cient structured id136 for
sagae.
transition-based parsing with neural networks and er-
ror states. transactions of the association for compu-
tational linguistics, 4:183   196.

2016.

