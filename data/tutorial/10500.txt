 

 

bayesian neural id27 

oren barkan 

tel aviv university, israel 

microsoft, israel  

 
 
 
 

abstract 

recently,  several  works  in  the  domain  of  natural  language 
processing  presented  successful  methods  for  word  embed-
ding. among them, the skip-gram with negative sampling, 
known  also  as  id97,  advanced  the  state-of-the-art  of 
various  linguistics  tasks.  in  this paper,  we  propose  a  scala-
ble  bayesian  neural  word  embedding  algorithm.  the  algo-
rithm  relies  on  a  variational  bayes  solution  for  the  skip-
gram  objective  and  a  detailed  step  by  step  description  is 
provided. we present experimental results that demonstrate 
the performance of the proposed algorithm for word analogy 
and  similarity  tasks  on  six  different  datasets  and  show  it  is 
competitive with the original skip-gram method. 

1    introduction and related work   

(nlp) 

recent  progress  in  neural  word  embedding  methods  has 
advanced  the  state-of-the-art  in  the  domain  of  natural  lan-
guage  processing 
(pennington,  socher,  and 
manning  2014;  collobert  and  weston  2008;  mnih  and 
hinton  2008;  mikolov  et  al.  2013;  vilnis  and  mccallum 
2015; zhang et al. 2014). these methods attempt to learn a 
low  dimensional  representation  for  words  that  captures 
semantic  and  syntactic  relations.  specifically,  skip-gram 
(sg)  with  negative  sampling,  known  also  as  id97 
(mikolov et al. 2013), set new records in various linguistic 
tasks  and  its  applications  have  been  extended  to  other do-
mains  beyond  nlp  such  as  computer  vision  (frome, 
corrado,  and  shlens  2013;  lazaridou,  nghia,  and  baroni 
2015)  and  collaborative  filtering  (cf)  (barkan  and 
koenigstein  2016;  barkan,  brumer,  and  koenigstein 
2016). 
 
in  this  paper,  we  propose  a  scalable  bayesian  neural 
id27 algorithm that in principle, can be applied 
to  any  dataset  that  is  given  as  sets  /  sequences  of  items. 
hence,  the  proposed  method  is  not  limited  to  the  task  of 
word  embedding  and  may  be  applicable  to  general  item 
similarity tasks as well. we provide a fully detailed step by 

                                                 
 

step algorithm,  which is  straightforward to implement and 
requires negligible amount of parameter tuning. 
  bayesian methods for words representation are proposed 
in (vilnis and mccallum 2015; zhang et al. 2014). differ-
ent  from  these  methods,  we  propose  a  variational  bayes 
(vb)  (bishop  2006)  solution  to  the  sg  objective.  there-
fore, we name our method bayesian skip-gram (bsg). 
  vb solutions provides a stable and robust behavior that 
require negligible effort in hyperparameter tuning (bishop 
2006).  this  is  in  contrast  to  point  estimate  solutions  that 
are  more  sensitive  to  singularities  in  the  objective  and  of-
ten require significant amount of hyperparameter tuning. 
  while the sg method maps words to vectors, bsg maps 
words  to  densities  in  a  latent  space.  moreover,  bsg  pro-
vides for a confidence level in the embedding and opens up 
for density based similarities measures.  
  our  contribution  is  twofold:  first,  we  derive  a  tractable 
(yet  scalable)  bayesian  solution  to  the  sg  objective  and 
provide  a  detailed  step  by  step  algorithm.  secondly,  we 
propose several density based similarity measures that can 
be investigated in further research.  
  the rest of the paper is organized as follows:  section  2 
overviews  the  sg  method.  in  section  3,  we  provide  the 
mathematical derivation of the bsg solution. in section 4, 
we describe the bsg algorithm in detail. in section 5,  we 
present  evaluations  on  six  different  datasets,  where  we 
show that in most cases bsg outperforms sg. 

 

2   skip-gram with negative sampling 

sg  with  negative  sampling  is  a  neural  word  embedding 
method  that  was  introduced  in  (mikolov  et  al.  2013). the 
method  aims  at  estimating  words  representation  that  cap-
tures semantic and syntactic relations between a word to its 
surrounding  words in a sentence. note that sg can be ap-
plied  with  hierarchical  softmax  (mikolov  et  al.  2013),  but 
in this work we refer to sg as sg with negative sampling. 
the rest of this section provides a brief overview of the sg 
method. 
  given a sequence of words 

iw =  from a finite vo-

) l

(

1

i

cabulary 

{ }l
w w =
i
lowing objective 

=

i

1

, sg aims at maximizing the fol-

work,  we  do  not  use  biases,  since  in  our  initial  experi-
ments their contribution was found to be marginal. 

                  

1

l

l

       

i

=            

c j c j

1

,

   

0

log (

p w w
i

|

+

i

j

)

                   (1) 

2.2   data subsampling 

where  c   is  defined  as  the  context  window  size  and
p w w  is the softmax function: 

(

)

|

j

i

                    

p w w
i

(

|

j

)

exp(

u v

t
i

)

j

=    

exp(

u v

t
i

k

)

                   (2) 

   
k i

w

(

iu u              and 
v              are  latent  vectors  that 
correspond  to  the  target  and  context  representations  for 

)m

)m

iv

(

in  order  to  overcome  the  imbalance  between  rare  and 
frequent  words  the  following  subsampling  procedure  is 
suggested:  given  the  input  words  sequence,  we  discard 
in  a  sentence  with  a  id203 
each  word  w  

(

p discard w
f w   is  the  fre-
quency of the word  w  and     is a prescribed threshold. 

    where   

) 1

f w  
)

=    

(

(

)

/

|

this  technique  is  reported  to  accelerate  the  learning 
process  and  improve  the  representation  of  rare  words 
(mikolov et al. 2013). 

the  word 

iw w    ,  respectively. 

wi

   

{1,..., }

l

  and  the 

2.3   word representation and similarity 

parameter  m   is  chosen  empirically  and  according  to 
the size of the dataset. 
    using eq. (2) is impractical due to the computational 
   
,  which  is  linear  in  l   that  is 
complexity  of 
usually in size of 
. 

|
(
)
p w w
i
6
5
10   
10

j

2.1   negative sampling 

negative  sampling  (mikolov  et  al.  2013)  is  introduced 
in  order  to  overcome  the  above  computational  problem 
by the replacement of the softmax function from eq. (2) 
with                                               

n

                 

p w w
i

(

|

j

)

=

  
(
u v

t
i

j

      
  
)
(

u v
k

t
i

k

=
1

)

            (3) 

where 

   =

( ) 1 / 1 exp(
x

+

    ,  n   is  a  parameter  that 

x

)

)

)

(

unip

3/ 4 (

w ,  where 

3/4rd  power 

determines  the  number  of  negative  examples  to  be 
kw  is 
sampled per a positive example. a negative word 
sampled  from  the  unigram  distribution  raised  to  the 
unip w   is  defined  as  the 
number  of  times  w   appear  in the  entire  corpus  divided 
by  the  total  length  (in  words)  of  the  corpus.  this 
distribution  was  found  to  outperform  both  the  uniform 
and  unigram  distributions  (mikolov  et  al.  2013).  the 
latent  representations  u   and  v   are  estimated  by 
applying a stochastic gradient ascent with respect to the 
objective in eq. (1). 
    it is worth noting that some versions of word embed-
ding  algorithms  incorporate  bias  terms  into  eq.  (3)  as 
follows  

n

       

p w w
i

(

|

j

)

=

  
(
u v

t
i

j

+ +

b
i

b

j

  
(

   
)

k

=
1

   

u v
k

t
i

       

b
i

b
k

)

. 

these biases often explain properties such as frequency 
of a word in the text (pennington, socher, and manning 
2014;  mnih  and  hinton  2008)  or  popularity  of  an  item 
in  the  dataset  (paquet  and  koenigstein  2013).  in  this 

sg produces two different representations 

the word 

iw . in this work, we use 

sentation for 

iw . alternatively, we can use 

iu  and 

iv  for 
iu  as the final repre-
iv , the addi-

tive  composition 

u
i

v+   or  the  concatenation 

i

   
   

t
u v
i

t
i

t

      

. 

the  last  two  options  are  reported  to  procure  superior 
representation  (garten  et  al.  2015).  the  similarity  be-
w w   is  computed  by  applying 
tween  a  pair  of  words 

,a

b

the  cosine  similarity  to  their  corresponding  representa-
tions  

,a
u u  as follows 

b

sim w w
b

(

,

a

)

=

t
a

u u
b
u
b

a

u

. 

3   bayesian skip-gram (bsg) 

as described in section 2, sg produces point estimates 
for  u  and  v  . in this section, we propose a method for 
deriving  full  distributions  for  u   and  v (in  this  paper, 
we  use  the  terms  distribution  and  density  interchangea-
bly).  we  assume  that  each  target  and  context  random 
vectors  are  independent  and  have  normal  priors  with  a 
zero  mean  and  diagonal  covariance  with  a  precision 
parameter     as follows 

i       

i

:w

 

p u
i

(

)

=

n

u
i

(0,

1

i     

)

 and 

(
p v
i

)

=

n

v
i

(0,

1

i     

)

 . 

note  that  different  values  of      can  be  set  to  the  priors 
over  u   and  v .  furthermore,  these  hyperparameters 
can be treated as random  variables and be learned from 
the  data  (paquet  and  koenigstein  2013).  however,  in 
this  work,  we  assume  these  hyperparameters  are  given 
and identical. 
    we  define 

(

)ic w   as  a  multiset  that  contains  the 
iw   in  the  corpus.  let 
i
  be  the 

{( , ) |

  and 

)}

)}

   
j c w
i

=

(

i

j

n

indices  of  the  context  words  of 

i

p

=

{( , ) |

i

j

   
j c w
i

(

positive  and  negative  sets,  respectively.  note  that 

pi   is  a 
   s  size  might  be  quadratic  in  the 

multiset  too  and 

ni

that  log (

p d   is  independent  of  q .  hence,  minimizing 

)

(
kld q

  
( )

  (cid:3)

p

(

|

d

)

)

  is  equivalent  to  maximizing ( )l q . 

vocabulary  size  l   .  therefore,  we  approximate 

ni

  by 

it was shown (bishop 2006) that 

( )l q  is maximized by 

negative sampling as described in section 2.1. 
   
   let 

 and define 

=    

| ( ,
i

d

=

d

{

i

i

i

)

j

ij

i

d

}

, where 

an  iterative  procedure  that  is  guaranteed  to  converge  to 
'q s  
a  local  optimum. this  is  done  by  updating  each  of 

 is a random variable 

factors, sequentially and according to the update rule 

d

p
d i        

{1, 1}

:

d

n

                       

d

ij

   

d i

(( , ))

j

   
=    
   

1

( , )
i
j

   

i

   

1 ( , )
j

i

   

i

.  

p

n

then, 

the 

likelihood 

of 

ijd  

is 

given 

by 

p d

(

ij

|

u v

,

i

)

  =
(

j

d u v

ij

t
i

)

j

. note that when applied to mul-

tisets,  the  operator        is  defined  as  the  multiset  sum  and 
not as the multiset union.  
   the joint distribution of 

,u v  and  d  is given by 

    

|

u v

,

i

)

j

ij

p v
i

(

)

=

        (4) 

)

=

p d u v p u p v

(

(

)

=

,

(

)

|
)
       

p u
i

(

)

   
i
i
w

   
i

i
w

d u v

ij

t
i

)

j

   

   
i
i
w

(0,

   
1
  

i

)

n

u
i

   

   
i
i
w

n

v
i

(0,

   
  

1

i

).

,

,

(
p u v d
   

p d

(

( , )
j
i

   
i

   

( , )
j
i

   
i

d

d

  
(

3.1   variational approximation 

we  aim  at  computing  the  posterior 

p u v d .  how-

)

(

,

|

ever, a direct computation is hard. the posterior can be 
approximated  using  mcmc  approaches  such  as  gibbs 
sampling  or  by  using  vb  methods.  in  this  work,  we 
choose  to  apply  vb  approximation  (bishop  2006), 
since  it  was  shown  to  converge  faster  to  an  accurate 
solution, empirically.  
    let 
d  
(
p

   =     ,  vb  approximates 
)
 by finding a fully factorized distribution 

the  posterior 

u v

|

l

q

  
( )

=

q u v

(

,

)

=

q u q v

) (

(

)

=    

i

=
1

) (
q u q v
i

(

i

)

 

that minimizes the kl divergence (bishop 2006)  

        

(
d q

kl

  
( ) ||

p

  
(

|

d

)

)

=    

q

  
( ) log

p

  
( )
q
  
(

|

d

  
d

. 

)

to this end, we define the following expression: 

( )
l q

   

   

q

  
( ) log

)

  
( ,
p d
  
( )

q

  
d

=

                

*
q
u
i

=

exp

(

e

q
  
\

ui

  
[log ( ,

p d

)]

+

const

)

           (6) 

where  the  update  for 

*

ivq   is  performed  by  the  replace-

ment of 

iu  with 

iv  in eq. (6). 
p d

   =
( ,

)

    recall  that  the  term 

p u v d

(

,

,

)

  in  eq.  (6) 

contains the likelihood 

p d u v  from eq. (4),  which 

(

)

,

|

is  a  product  of  sigmoid  functions  of  u   and  v . there-
fore,  a  conjugate  relation  between  the  likelihood  and 
the  priors  does  not  hold  and  the  distribution  that  is  im-
plied by 
 in eq. (6) does not belong to 

  
[log ( ,

p d

e

)]

q
  
\

ui

the exponential family. 
    next,  we  show  that  by  the  introduction  of  an  addi-
ij     we  are  able  to  bring  eq.  (6)  to  a 
tional  parameter 

form that is recognized as the gaussian distribution. 
    we  start  by  lower  bounding 

p d u v   using  the 

(

)

,

|

following logistic bound (jaakkola and jordan 1996): 

         

log ( )
a

  

   

a

   

  

2

   

    

( )(

a

2

   

2
  

)

+

     
log ( )

          (7) 

where 

    
( )

=

.  by  applying  the  lower  bound 

1

   

   
1
     
( )
   
   
  
2
p d    we get 

   
   
   

2

)

|

from (7) to  log (

log (

p d

|

  
)

   

log

p d
  

(

|

  
)

=

 

   

( , )
j
i

   
i

d

d u v

ij

t
i

   

  
ij

j

2

   

    
ij

(

)(

u v v u
i

j

t
i

t
j

   

2
  
ij

)

+

log (

     
ij

)

.  (8) 

by using eq. (8) we bound 

( )l q  as follows 

           

( )
l q

   

( )
l q
  

=

   

q

  
( ) log

p
  

  
( ,

)
d

q

  
( )

  
d

=

   

q

  
( ) log

p d
  

(

|

     
) ( )

p

q

  
( )

  
d

.            

  
(

        

   

q

  
( ) log

p

   

(
d q

kl

  
( )

q
(cid:3)

p

  
(

|

d

)

)

+

log (

p d

)

d

|
  
( )

)

  
d

+

   

q

  
( ) log (

p d d

)

  

=

        (5) 

furthermore,  it  was  shown  (jaakkola  and  jordan  1996) 
that the bound in eq. (6) is tight when 

where  the  last  transition  in  eq.  (5)  is  due  to  the  fact  q  

 

is  a  pdf.  by  rearranging  eq.  (5)  we  get  the  relation 
(
kld q

,  where  we  notice 

( )
p d l q

log (

  
( )

  
(

d

=

   

p

(cid:3)

)

)

)

|

2
  
ij

=

e

q

[
u v v u
i

j

t
i

t
j

] var(

=

u v

t
i

var(

u v

t
i

)

+

j

           
u
i

t
u
i

t
v

v

j

j

)

+

e

[
u v

t
i

j

q

e
]

[

t
v u
i
j

q

]

=

j

    (9) 

where 

        

jv

[

jv

]

q

  and  the  last  transition  in  eq.  (9) 

holds  since 

iu and 

jv   are  independent.  by  assuming 

3.2   stochastic updates 

diagonal covariance matrices, the term  var(

t
i

ju v

(8) is computed by 

         

var(

u v

t
i

j

)

=

var

   
   
   

m

   

k

=
1

u v

ik

jk

   
   
   

=

m

   

k

=
1

var(

u v

ik

jk

)

=

m

   

2
                 
u

+

+

2
u

2
u

2
v

2
v

2
v

ik

jk

ik

jk

jk

ik

k

=
1

finally, by combining eqs. (9) and (10) we get 

)

 in eq. 

.   (10) 

due to data sampling, the effective dataset changes be-
tween  the  iterations  and  the  optimization  becomes  sto-
chastic. since we do not want to ignore the information 
from  previous  steps,  we  need  to  figure  out  a  way  to 
consider  this  information  in  our  updates.  a  common 
practice is to apply updates in the spirit of the robbins-
monro method (robbins and monro 1951). this is per-
formed  by  the  introduction  of  an  iteration  dependent 
variable 

)k    that controls the updates as follows 

(

                  

  
ij

=

m

   

k

=
1

2
           
(
v

)(

+

+

2
u

2
u

2
v

ik

jk

ik

jk

                     

( )
k
p
u
i

=

( )
k

  

p
u
i

+    

(1

( )
k

  

k

   

1)

)

(
p
u
i

                  

)

.           (11) 

                     

( )
k
r
u
i

=

( )
k

  

r
u
i

+    

(1

( )
k

  

k

   

1)

)

(
r
u
i

. 

therefore,  instead  of  maximizing 

( )l q   we  can  maxim-

  and  this  is  done  by  replacing  the  term 

ize 

( )
l q  
p d   from eq. (6) with  log

)

log ( ,

d      as follows 
( ,
p

)

               

*
q
u
i

=

exp

(

e

q
  
\

ui

[log

     
( ,
p

d

)]

+

const

)

.       (12) 

by applying the natural logarithm to eq. (12) we get 

log

*
q
u
i

=

e

q
  
\

ui

[log

p
  

  
( ,

d

)]

+

const

=

p d u v
  

(

,

|

)]

+

e

q
  
\

u
i

[log (

p u v

,

)]

+

const

=

   (13) 

in  practice,  this  means  that  during  the  runtime  of  the 
algorithm we need to keep the results from the previous 
iteration. 
    robbins  and  monro  showed  several  conditions  for 
)k     needs 
convergence,  where  one  of  them  states  that

(

to satisfy: 

               

   

   

k

=

0

k

)

(
  

=    

  and   

   

   

k

=

0

  
(

( ) 2

)k

<    

.           (15) 

to  this  end,  we  suggest  to  use 

with  a  decay 
1  <       as  this  ensures  the  conditions  in 

)k

(
  

k   
   =

parameter  0.5

e

q
  
\

u
i

[log

u r
u

t
i

i

   

1

2

where 

u p u

t
i

u

i

+

const

i

              

p
u
i

=

r
u
i

=

   
   

   

   
j

i

ui

1

2

   

   
j

i

ui

    
2 (
ij

  

)

[

v v

j

q

t
j

]

   
   

+

  
i

d

  
v

ij

j

(15) hold. we further suggest to set 

k   =  for the first 
few  iterations,  in  order  to  avoid  too  early  convergence. 
specifically, in our implementation, we did not perform 
stochastic updates in the first 10 iterations. 

1

(

)

                 (14)       

4   the bsg algorithm 

with 

i

iu

=

{ | ( , )
j

j

i

   

i

d

}

  and 

  

[

v v

j

t
j

q

]

=     +

v

j

t
     
v

v

j

j

. 

note that in the last transition in eq. (13), all terms that 
iu are absorbed into the  const  term. 
are independent of 
iuq is 

    by  inspecting  eqs.  (13)  and  (14),  we  see  that 

*

in this section,  we provide a detailed description of the 
bsg  algorithm  that  is  based  on  sections  2  and  3.  the 
algorithm is described in fig. 1 and includes three main 
stages. the first stage is an initialization, then the algo-
rithm  iterates  between  data  sampling  and  parameter 
updates till a convergence criterion is met or number of 
iterations is exceeded. in what follows, we explain each 
of these stages in detail. 

normally  distributed  with 

the  natural  parameters                               

p
u
i

   =       (the  precision  matrix)  and 

1

u
i

r
u
i

p   =
u
i

u
i

(the 

means  times  precision  vector).  note  that  the  computa-
ivq    s  parameters  is  symmetric.  moreover,  since 

tion  of 

*

the updates for 

l
{
}
iq

u
i

=  depend only on 
1

l
{ }
iq

v
i

=  and vice 
1

versa,  they  can  be  performed  in  parallel.  this  gives  an 
alternating updates scheme that is embarrassingly paral-
lel and (under the assumption of constant dataset) guar-
anteed  to  converge  to  a  local  optimum  (bishop  2006): 
=   (in  parallel),  then  update  all 
1

first,  update  all 

l
{
}
iq

u
i

l
{ }
iq

v
i

=  (in parallel) and repeat until convergence. 
1

4.1   stage 1 - initialization 

the  algorithm  is  given  the  following  hyperparameters: 
the  input  text  t   (set  of  senstences),  target  representa-
tion  dimension  m ,  the  number  of  maximum  iterations 
k ,  the  maximal  window  size 
,  the  negative  to 
positive  ratio  n         ,  the  subsampling  parameter    ,  a 
stopping  threshold    ,  the  decay  parameter       and  the 
prior precision parameter    . as described in section 3, 
different  values  of     can  be  learned  for  u   and  v , 
however  in  our  implementation,  we  chose  to  use  a 
unique parameter    . 

maxc

in this stage, we further need to determine the effective 
set  of  words  w to  learn  representation  for. this  can  be 
done  by  considering  all  words  in  the  data  that  appear 
more  than  a  prescribed  number  of  times,  or  by  consid-
ering  the  l   most  popular  words.  in  this  work,  we  stick 
with  the  latter.  then,  every  word  w w      is  discarded 
from the data (step 1). 

step 
=

{

q q q
v
i

u
i

,

2 
l
}
i

initializes 

the 

target 

distributions 

  parameters.  specifically,  the  means  are 

=
1

drawn  from  the  multivariate  standard  normal  distribu-
tion and the covariance matrices are set to identity. 

in  step  3,  we  compute 

unip   according  to  the  descrip-
tion in section 2.1, then we raise it to the    rd power.  
    step  4  updates  k   and  k   according  to    .  this  en-
sures  the  stochastic  updates  are  not  performed  in  the 
first    iterations. 

4.2   stage 2     data sampling 

at  the  beginning  of  every  iteration,  we  subsample  the 
data  (step  5.1)  as  described  in  section  2.2.  then,  we 
follow the description in section 3: for each instance of 
iw   in  t ,  we  sample  a  window  size  c     from 
the  word 
}
 

the  uniform  discrete  distribution  over  the  set

{1,...,

c
max

(

and consider the  c  words to the left and to the right of 
iw   as  context  words  for 
iw .  this  results  in  a  multiset 
)ic w  that contains the indices of the context  words of 
iw   (an  index  may  appear  multiple  times).  then,  we 
create 
tuples 
i

positive 
(

 (step 5.3). 

multiset 

{( , ) |

of 

)}

a 

=

i

j

   
j c w
i

p

next, for each tuple  ( , )
j

i

examples  ( , )

i z  such that  ( , )
i z

is  sampled  according  to 

i     we sample  n  negative 

p
i    . a negative word 

zw  
w .  we  further  update 

3/ 4 (

)

p

p

uni

z

i

u
i

,

i

v

j

,

i

v

z

,

d d  accordingly (step 5.4).  

,

ij

iz

  an  alternative  implementation  of  step  5.4  is  to  save 
for  each  tuple  a  counter  for  the  number  of  times  it  ap-
pears. this  can  be  done  by  maintaining  dictionary  data 
structures  that  count  positive  and  negative  examples. 
this  avoids  the  need  of  maintaining 
=   as  mul-

l
}
i

{

i

i

,

1

u
i

v
i

tisets  (list  data  structures)  and  replace  them  with  set 
data structures. 

   

4.3   stage 3     parameter updates 

in  this  stage,  we  update  the  parameters  of  the  distribu-

tions 

=

q q q
v
i

{

u
i

,

l
}
i

=
1

.  the  updates  are  performed  first 

for 

l
{
}
iq

u
i

=  (step 5.6) and then  for 

1

l
{
}
iq

v
i

=  in a symmet-

1

ric  manner  (step  5.7).  moreover,  each  sequence  of  up-
dates  is  performed  in  parallel.  note  that  step  5.6.1  in-

volves the computation of 

ij   , 

)ij      and 

(

  

[

q

jv v

j

]t

 that 

are given by eqs. (11), (7) and (14), respectively. 

1  + . this is ensured by step 5.5. 

    due  to  data  sampling  the  dataset  is  changed  per  it-
eration.  therefore,  we  apply  stochastic  updates  (step 
5.6.2).  the  stochastic  updates  are  performed  starting 
from the iteration 
    a  crucial  point  to  notice  is  the  computation  of  the 
mean  and  the  covariance:  first,  we  compute  the  covari-
ance  matrix  by  the  inversion  of  the  precision  matrix 
(step  5.6.3).  this  is  performed  by  using  cholesky  de-
composition.  then,  we  extract  the  mean  (step  5.6.4). 
iu   to zeros 

finally, we set all the off diagonal values in 

(step 5.6.5) while keeping 

iup as is. 

    the  algorithm  stops  if  the  convergence  criterion  is 
met or number of iterations is exceeded (last line). 

4.4   similarity measures 

bsg  maps  words  to  normal  distributions.  in  this  work, 
we choose to use the distributions 
=  for represent-

l
{
}
iq

1

u
i

ing  words.  the  similarity  between  a  pair  of  words 
w w     can  be  computed  by  the  cosine  similarity  of 

,i

j

,

their  means 

u      .  by  using  the  covariance,  a  confi-
dence  level  can  be  computed  as  well.  to  this  end,  we 
define  a  random  variable 
.  though  the  distri-

u u

=

u
i

j

y
ij

t
i

j

bution  of 

ijy   is  not  normal,  it  has  the  following  mean 

and variance                                        

         

        
u

=

y
ij

t
u
i

j

2
  
y
ij

=

tr

[

     

u
i

u

j

]

+

           
u

  

  

+

u

t
u

t
u
i

u
i

u
i

j

j

            (16)        

.

j

hence,  we  choose  to  approximate 
with 
dence level of the similarity score. 

n       . then, 

ijy     

2
y
ij

y
ij

(

)

y
ij

,

2

ijy    s  distribution 
  can  be  used  as  a  confi-

bsg further enables the applications of other similar-
approximate 

example,  we 

can 

 by approximating the marginalization  

ity 
(
p d

ij

types.  for 
d=

1 |

)

=

1|

d

)

=

ij

   

p d

(

ij

=

1,

u u d du du

)

,

|

i

j

j

=

   

j

j

i

i

p d

=

1 |

ij

u u p u d p u d du du

)

(

)

(

)

,

|

|

i

j

i

u u q u q u du du

) (

) (

)

j

i

j

i

t
i

   

j

   

  
(

y p y dy
ij
ij

)

(

)

ij

    (17) 

   

(

p d
   
   

  
(

(

(

     
y
ij

1

+

     

2
y
ij

/ 8 .

)

2

where 

ijy     and 

ijy     are  given  by  eq.  (16)  and  the  last 
three  approximations  follow  from  the  vb  approxima-
tion,  eq. (16) and (mackay 1992), respectively.  
    another  option  is  to  apply  a  similarity  measure  that 
is  based  on  a  symmetric  version  of  the  kl  divergence 
between two multivariate normal distributions 

- target representation dimension 
- input text, given as sequence of sequences of words 

bsg algorithm 

input:  
m  
t  
      - prior precision 
k    - maximum number of iterations 
maxc
l    
   

 - maximal window size 

- number of most frequent words to be considered in the vocabulary 
- subsampling parameter 

- number of iterations to apply without performing stochastic updates (in the beginning) 

 n   - negative to positive ratio 
   
      - stopping threshold 
       - decay parameter 

output:  
q      
,
v
i

=

{

u
i

,

       

,

u
i

l
}
i

=
1

v
i

- parameters of the distributions 

=

q q q
v
i

{

u
i

,

l
}
i

=
1

 

 
1. create a set 

=
i      to  l  
  
(0, )
i
iu

1
~

n

2. for 

2.1 

3/ 4

3. compute 

unip
          ,  k

1

k

4. 
5. repeat 
  5.1.

{ }l
w w =

i

i

1

 of the  l  most frequent words in  t  and discard all other words from  t . 

, 

  
iv

~

(0, )
n i

, 

iup

i    , 

ivp

i     

 over  w  using  t  as described in section 2.1. 
k            // first   iterations are performed without stochastic updates 

subt     subsample ( t ), as described in section 2.2 

  5.2. for 
     5.2.1. 

i      to  l  
i

1
i  
   
,

u
i

  
     

v
i

     5.2.2. 

prev

p
u
i

p    , 

u
i

prev

p
v
i

p    , 

v
i

prev

r
u
i

r    , 

u
i

prev

r
v
i

r     // save values from the previous iteration 

v
i

  5.3. create  pi  based on 
  5.4. for  ( , )

i

j  in  pi  
       
j

i

{ }

u
i

    5.4.1. 

i

u
i

, 

i

v

j

        , 

{ }
i

i

v

j

ijd       

1

subt  as described in section 4.2   // positive sampling 

    5.4.2. for 

n       to  n  // negative sampling 

1

  5.4.2.1. sample a negative word index  z  according to 

p

3/ 4 (

uni

w  s.t.  ( , )
i z

)

z

i     
p

  5.4.2.2. 

i

u
i

       

i

u
i

{ }

z

, 

i

v

z

        , 

{ }
i

i

v

z

izd          
1

5.5. if 

k >  then 

0

  

        else 

k   

1       // stochastic updates condition 

5.6. parfor 

i      to  l   // parallel for loop 

1

  5.6.1.  compute 

up r using eq. (14) 

,

u
i

i

  5.6.2. 

p
u
i

   

  

p
u
i

+    

(1

  
)

prev

p
u
i

,

r
u
i

   

  
r
u
i

+    

(1

  
)

prev

r
u
i

 

  5.6.3. 

1
        

up    

u
i

i

  5.6.4. 

         

u
i

u ur

i

i

 

  5.6.5. 

      

u
i

diag diag

[

[

  

]]

 

u
i

5.7. apply a symmetric version of step 5.6 to 

l
{
}
iq

v
i

= parameters 

1

 until  k k>

 or 

l

   

i

=
1

   

r
u
i

prev

r
u
i

2

<

  

 and 

l

   

i

=
1

   

r
v
i

prev

r
v
i

<

  

 

2

 

figure 1: the bsg algorithm 

  

sim

symkl

(

q
u

i

,

q
u

j

)

=    

d q
u

kl

(

||

q
u

j

i

)

   

d q

kl

(

||

q
u
i

u

j

)

  (18) 

where 

d q
u
i

kl

(

||

q
u

j

)

  has  the  following  closed  form 

solution (bishop 2006) 

d q
u
i

kl

(

||

q
u

j

)

=

1

2

{

log

      

u

j

log

   +

u
i

     
(
u
i

   

u

j

t
)

  

   
1
u

j

     
(
u
i

   

u

j

t

)

+

tr

   
   

     

   
1
u

j

   
   

   

m

}

.

u
i

 

note  that  the  application  of  the  bsg  algorithm  to  gen-
eral  item  similarity  tasks  is  straightforward.  the  only 
requirement is that the data is given in the same format. 
specifically,  every  sentence  of  words  in  the  data  is  re-
placed with a sequence of items. moreover, if the items 
are  given  as  sets,  for  each  sequence,  the  window  size 
should be set to the length of the sequence. this results 
in  a  bayesian  version  of 
item2vec  (barkan  and 
koenigstein 2016). 

5   experimental setup and results 

in this section, we compare between the bsg and sg 
algorithms (for sg we used the id971 implementa-
tion).  the  algorithms  are  evaluated  on  two  different 
tasks:  the  word  similarity  task  (finkelstein  et  al.  2002) 
and  the  word  analogy  task  (pennington,  socher,  and 
manning 2014).  
    the  word  similarity  task  requires  to  score  pairs  of 
words  according  to  their  relatedness.  for  each  pair,  a 
ground  truth  similarity  score  is  given.  the  similarity 
score  we  used  for  both  models  is  the  cosine  similarity. 
specifically,  for  bsg  we  observed  no  significant  im-
provement,  when  applying  the  similarities  from  eqs. 
(17)  and  (18),  instead  of  the  cosine  similarity.  in  order 
to  compare  between  the  bsg  and  sg  methods,  we 
compute  for  each  method  the  spearman  (spearman 
1987)  rank  correlation  coefficient  with  respect  to  the 
ground truth. 
    the  word  analogy  task  is  essentially  a  completion 
bw  as 
task: a bank of questions in the form of    
cw  is to ?    is given, where  the task is to replace ? with 
dw . the questions are divided to  syn-
the correct  word 
tactic questions such as    onion is to onions as lion is to 
lions    and semantic questions, e.g.    berlin is to germa-
ny as london is to england   .  
    the  method  we use to answer the questions is by re-
dw  that gives the highest cosine simi-
porting the word 
  .  for  the 

larity  score  between 

aw is to 

u

u

u

+

a

c

du     and 
bsg  and  sg  models  we  used 

   
u
?
iu   and 
sentation for the word iw , respectively. 

=

b

iu   as  the  repre-

                                                 
1 https://code.google.com/p/id97 

5.1   datasets 

we  trained  both  models  on  the  corpus  from  (chelba  et 
al. 2014). in order to accelerate the training process, we 
limited our  vocabulary to the 30k most  frequent  words 
in  the  corpus  and  discarded  all  other  words.  then,  we 
randomly  sampled  a  subset  of  2.8m     sentences     that 
results in a total text length of 66m words. 
    the  word  similarity  evaluation  includes  several  dif-
ferent  datasets:  wordsim353  (finkelstein  et  al.  2002), 
scws  (huang  et  al.  2012),  rare  words  (luong, 
socher,  and  manning  2013),  men  (bruni,  tran,  and 
baroni  2014)  and  siid113x999  (hill,  reichart,  and 
korhonen  2015).  the  reader  is  referred  to  the  refer-
ences  for  further  details  about  these  datasets.  for  each 
combination  of  dataset  and  method,  we  report  the 
spearman rank correlation (x100). 
    the  word  analogy  evaluation  dataset  (mikolov  et  al. 
2013)  consists  of  14  distinct  groups  of  analogy  ques-
tions,  where  each  group  contains  a  different  number  of 
questions.  both  models  were  evaluated  on  an  effective 
set  that  contains  14122  questions  (all  questions  that 
contain out-of-vocabulary words were discarded). 

5.2   parameter configuration 

4

=

40

m =

c
max

30000

   
510  

,  maximal  window  size 

  and  negative  to  positive  ratio 

the  same  parameters  configuration  was  set  for  both 
systems.  specifically,  we  set  the  target  representation 
= , 
dimension 
,  vocabulary  size 
subsampling  parameter 
1n = .  for 
l =
bsg,  we  further  set 
  (note 
that  bsg  is  quite  robust  to  the  choice  of       as  long  as 
0.5
40
  
iterations  (we  verified  their  convergence  after  ~30  it-
erations).  in  order  to  mitigate  the  effect  of  noise  in  the 
results,  we  trained  10  different  instances  of  bsg  and 
sg  and  report  the  average  score  that  is  obtained  for 
each entry in the tables. 

1  <     ).    both  models  were  trained  for 

1   =   , 

10   =

  and 

k =

   =

0.7

5.3   results 

table  1  presents  the  (average)  spearman  rank  correla-
tion score (x100) obtained by bsg and sg on the word 
similarity task for various datasets. table 2 presents the 
(average) percentage of correct answers for each model 
per  questions  group  on  the  word  analogy  task.  we  see 
that the models are competitive  where bsg results in a 
better  total  accuracy.  examining  the  results  from  both 
tables,  we notice that in  most cases, bsg achieves bet-
ter results than sg. this might be explained by the fact 
that  bsg  leverages  information  from  second  moments 
as well. 
    comparing 
literature 
(pennington, socher, and manning 2014; mikolov et al. 
2013),  we  see  that  the  scores  obtained  by  both  models 
are  lower.  this  might  be  explained  by  several  reasons: 

results  with 

our 

the 

table 1: a comparison between bsg and sg on various word similarity datasets 

wordsim353 

scws 

rare words 

men 

siid113x999 

method 

 (finkelstein et al. 

sg  

bsg 

2002) 

58.6 

61.1 

 (huang et 
al. 2012) 

59.4 

59.3 

 (luong, socher, 

 (bruni, tran, 

 (hill, reichart, and 

and manning 2013) 

and baroni 2014) 

korhonen 2015) 

51.2 

52.5 

60.9 

61.1 

26.4  

27.3 

table 2: a comparison between bsg and sg on 

the word analogy task 

questions group name 

bsg (%) 

sg (%) 

capital common countries       

capital world 

currency 

city in state 

family      

adjective to adverb 

opposite 

comparable 

superlative 

present participle 

nationality adjective 

past tense 

plural 

plural verbs 

total 

62.4 

53.2 

7.1 
14.3 

55.7 

20.1 
6.7 

47.2 

41 

39 

88.9 

43.7 

46.9 

29.4 

45.1 

59.5  

50.2 

3.6 

17.9  

63.3 
15.2 

6.7 

43 

48.1 
36.2 

82.3 

39.2 

42.3 

29.1 

42.5 

first,  we  use a  smaller corpus of 66m  words  vs. 1-30b 
in (pennington, socher, and manning 2014; mikolov et 
al. 2013). secondly, the target representation dimension 
we  use  is  40  vs.  100-600  in  (pennington,  socher,  and 
manning  2014;  mikolov  et  al.  2013).  therefore,  we 
believe  that  the  performance  of  our  models  can  be  im-
proved  significantly  by  increasing  the  representation 
dimension as well as the amount of training data. recall 
that  our  main  goal  is  to  show  that  bsg  is  an  effective 
word  embedding  method  and  provides  competitive  re-
sults when compared to the sg method. 

6   conclusion 

in  this  paper,  we  introduced  the  bsg  algorithm  that  is 
based on a vb solution to the sg objective. we provide 
the mathematical derivation of the proposed solution as 
well as step by step algorithm that is straightforward to 
implement.  furthermore,  we  propose  several  density 
based  similarities  measures.  we  demonstrate  the  appli-
cation of bsg on various linguistic datasets and present 
experimental  results  that  show  bsg  and  sg  are  com-
petitive. 

references 

barkan,  oren,  yael  brumer,  and  noam  koenigstein.  2016. 
   modelling session activity with neural embedding.    in recsys 
posters. 

barkan,  oren,  and  noam  koenigstein.  2016.     item2vec:  neural 
item  embedding  for  collaborative  filtering.     article.  arxiv 
preprint arxiv:1603.04259. 

bishop,  christopher  m.  2006.  pattern  recognition  and  machine 
learning. pattern recognition. vol. 4. doi:10.1117/1.2819119. 

bruni,  elia,  nam  khanh  tran,  and  marco  baroni.  2014. 
   multimodal  distributional  semantics.     journal  of  artificial 
intelligence research 49: 1   47. doi:10.1613/jair.4135. 

chelba,  ciprian,  tomas  mikolov,  mike  schuster,  qi  ge, 
thorsten brants, phillipp koehn, and tony robinson. 2014.    one 
billion  word  benchmark  for  measuring  progress  in  statistical 
language  modeling.     in  proceedings  of  the  annual  conference 
of 
international  speech  communication  association, 
interspeech, 2635   39. 

the 

collobert,  ronan,  and  jason  weston.  2008.     a  unified 
architecture  for  natural  language  processing:  deep  neural 
networks  with  multitask  learning.     in  proceedings  of  the  25th 
international  conference  on  machine  learning,  160   67. 
doi:10.1145/1390156.1390177. 

finkelstein,  lev,  evgeniy  gabrilovich,  yossi  matias,  ehud 
rivlin,  zach  solan,  gadi  wolfman,  and  eytan  ruppin.  2002. 
   placing  search  in  context:  the  concept  revisited.     acm 
(1):  116   31. 
transactions  on 
doi:10.1145/503104.503110. 

information  systems  20 

frome, andrea, gs corrado, and jonathon shlens. 2013.    devise: 
a  deep  visual-semantic  embedding  model.     advances  in 
neural information processing systems, 1   11. 

garten,  justin,  kenji  sagae,  volkan  ustun,  and  morteza 
dehghani. 2015.    combining distributed vector representations 
for  words.     inproceedings.  in  proceedings  of  naacl-hlt, 95   
101. 

hill,  felix,  roi  reichart,  and  anna  korhonen.  2015.     siid113x-
999:  evaluating  semantic  models  with  (genuine)  similarity 
estimation.     computational  linguistics  41 
(4):  665   95. 
doi:10.1162/coli. 

huang,  eric  h,  richard  socher,  christopher  d  manning,  and 
andrew ng. 2012.    improving word representations via global 

context and multiple word prototypes.    proceedings of the 50th 
annual meeting of the association for computational linguistics: 
long papers-volume 1, 873   82. 

jaakkola,  tommi  s,  and  michael  i  jordan. 1996.     a  variational 
approach  to  bayesian  logistic  regression  models  and  their 
extensions.    aistats, no. august 2001. doi:10.1.1.49.5049. 

lazaridou, angeliki, the pham  nghia, and marco baroni. 2015. 
   combining language and vision with a multimodal skip-gram 
model.     proceedings  of  human  language  technologies:  the 
2015  annual  conference  of  the  north  american  chapter  of  the 
acl, denver, colorado, may 31     june 5, 2015, 153   63. 

luong,  minh-thang,  richard  socher,  and  christopher  d. 
manning.  2013.     better  word  representations  with  recursive 
neural networks for morphology.    conll-2013, 104   13. 

mackay,  david  j.  c.  1992.     the  evidence  framework  applied 
to  classification  networks.     neural  computation  4  (5): 720   36. 
doi:10.1162/neco.1992.4.5.720. 

mikolov,  tomas,  kai  chen,  greg  corrado,  and  jeffrey  dean. 
2013.     distributed  representations  of  words  and  phrases  and 
their  compositionality.     nips,  1   9.  doi:10.1162/jmlr.2003.3.4-
5.951. 

mnih,  andriy,  and  geoffrey  e.  hinton.  2008.     a  scalable 
hierarchical  distributed  language  model.     advances  in  neural 
information processing systems, 1   8. 

paquet,  ulrich,  and  noam  koenigstein.  2013.     one-class 
id185 with random graphs.    proceedings of the 
22nd international conference on world wide web, 999   1008. 

2014. 

pennington,  jeffrey,  richard  socher,  and  christopher  d 
manning. 
for  word 
representation.     proceedings  of 
the  2014  conference  on 
empirical  methods  in  natural  language  processing,  1532   43. 
doi:10.3115/v1/d14-1162. 

   glove:  global  vectors 

robbins,  herbert,  and  sutton  monro.  1951.     a  stochastic 
approximation  method.     the  annals  of  mathematical  statistics 
22 (3): 400   407. doi:10.1214/aoms/1177729586. 

spearman, c. 1987.    the proof and measurement of association 
between  two  things.  by  c.  spearman,  1904.     the  american 
441   71. 
journal 
doi:10.1037/h0065390. 

psychology 

(3   4): 

100 

of 

vilnis, luke, and andrew mccallum. 2015.    word represenation 
via  guassian  embedding.     in  in  proceedings  of  the  iclr  2015, 
1   12. 

zhang,  j,  j  salwen,  m  glass,  and  a  gliozzo.  2014.     word 
semantic  representations  using  bayesian  probabilistic  tensor 
factorization.     in  proceedings  of  the  2014  conference  on 
empirical methods in natural language processing (emnlp). 

 

