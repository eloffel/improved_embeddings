id56s can learn logical semantics

samuel r. bowman      

christopher potts   

sbowman@stanford.edu

cgpotts@stanford.edu

   stanford linguistics

   stanford nlp group

christopher d. manning         
manning@stanford.edu
   stanford computer science

abstract

tree-structured id56s
(treeid56s) for sentence meaning have
been successful for many applications, but
it remains an open question whether the
   xed-length representations that they learn
can support tasks as demanding as logi-
cal deduction. we pursue this question
by evaluating whether two such models   
plain treeid56s and tree-structured neural
tensor networks (treerntns)   can cor-
rectly learn to identify logical relation-
ships such as entailment and contradiction
using these representations. in our    rst set
of experiments, we generate arti   cial data
from a logical grammar and use it to eval-
uate the models    ability to learn to handle
basic relational reasoning, recursive struc-
tures, and quanti   cation. we then evaluate
the models on the more natural sick chal-
lenge data. both models perform compet-
itively on the sick data and generalize
well in all three experiments on simulated
data, suggesting that they can learn suit-
able representations for logical id136
in natural language.

1

introduction

tree-structured id56 models
(treeid56s; goller and kuchler 1996) for sen-
tence meaning have been successful in an array of
sophisticated language tasks, including sentiment
analysis (socher et al., 2011b; irsoy and cardie,
2014), image description (socher et al., 2014),
and paraphrase detection (socher et al., 2011a).
these results are encouraging for the ability of
these models to learn to produce and use strong
semantic representations for sentences. however,
it remains an open question whether any such fully
learned model can achieve the kind of high-   delity

distributed representations proposed in recent al-
gebraic work on vector space modeling (coecke
et al., 2011; grefenstette, 2013; hermann et al.,
2013; rockt  aschel et al., 2014), and whether any
such model can match the performance of gram-
mars based in logical forms in their ability to
model core semantic phenomena like quanti   ca-
tion, entailment, and contradiction (warren and
pereira, 1982; zelle and mooney, 1996; zettle-
moyer and collins, 2005; liang et al., 2013).

recent work on the algebraic approach of co-
ecke et al. (2011) has yielded rich frameworks for
computing the meanings of fragments of natural
language compositionally from vector or tensor
representations, but has not yet yielded effective
methods for learning these representations from
data in typical machine learning settings. past ex-
perimental work on reasoning with distributed rep-
resentations have been largely con   ned to short
phrases (mitchell and lapata, 2010; grefenstette
et al., 2011; baroni et al., 2012). however, for ro-
bust natural language understanding, it is essential
to model these phenomena in their full generality
on complex linguistic structures.

this paper describes four machine learning ex-
periments that directly evaluate the abilities of
these models to learn representations that sup-
port speci   c semantic behaviors. these tasks fol-
low the format of natural language id136 (also
known as recognizing id123; dagan
et al. 2006), in which the goal is to determine
the core inferential relationship between two sen-
tences. we introduce a novel nn architecture for
natural language id136 which independently
computes vector representations for each of two
sentences using standard treeid56 or treerntn
(socher et al., 2013) models, and produces a judg-
ment for the pair using only those representations.
this allows us to gauge the abilities of these two
models to represent all of the necessary semantic
information in the sentence vectors.

5
1
0
2

 

y
a
m
4
1

 

 
 
]
l
c
.
s
c
[
 
 

4
v
7
2
8
1

.

6
0
4
1
:
v
i
x
r
a

much of the theoretical work on natural lan-
guage id136 (and some successful
imple-
mented models; maccartney and manning 2009;
watanabe et al. 2012) involves natural logics,
which are formal systems that de   ne rules of in-
ference between natural language words, phrases,
and sentences without the need of intermediate
representations in an arti   cial logical language.
in our    rst three experiments, we test our mod-
els    ability to learn the foundations of natural lan-
guage id136 by training them to reproduce the
behavior of the natural logic of maccartney and
manning (2009) on arti   cial data. this logic de-
   nes seven mutually-exclusive relations of syn-
onymy, entailment, contradiction, and mutual con-
sistency, as summarized in table 1, and it pro-
vides rules of semantic combination for project-
ing these relations from the lexicon up to com-
plex phrases. the formal properties of this sys-
tem are now well-understood (icard and moss,
2013a; icard and moss, 2013b). the    rst exper-
iment using this logic covers reasoning with the
bare logical relations (  3), the second extends this
to reasoning with statements constructed compo-
sitionally from recursive functions (  4), and the
third covers the additional complexity that results
from quanti   cation (  5). though the performance
of the plain treeid56 model is somewhat poor
in our    rst experiment, we    nd that the stronger
treerntn model generalizes well in every case,
suggesting that it has learned to simulate our target
logical concepts.

the experiments with simulated data provide a
convincing demonstration of the ability of neural
networks to learn to build and use semantic repre-
sentations for complex natural language sentences
from reasonably-sized training sets. however, we
are also interested in the more practical question of
whether they can learn these representations from
naturalistic text. to address this question, we ap-
ply our models to the sick entailment challenge
data in   6. the small size of this corpus puts data-
hungry nn models like ours at a disadvantage,
but we are nonetheless able to achieve competi-
tive performance on it, surpassing several submit-
ted models with signi   cant hand-engineered task-
speci   c features and our own nn baseline. this
suggests that the representational abilities that we
observe in the previous sections are not limited to
carefully circumscribed tasks. we conclude that
treerntn models are adequate for typical cases

softmax classi   er

p ((cid:64)) = 0.8

comparison
n(t)n layer

all reptiles walk vs. some turtles move

composition
rn(t)n
layers

all reptiles walk

some turtles move

all reptiles

walk

some turtles move

all

reptiles

some

turtles

pre-trained or randomly initialized learned word vectors

in our model,

figure 1:
two separate tree-
structured networks build up vector representa-
tions for each of two sentences using either nn
or ntn layer functions. a comparison layer then
uses the resulting vectors to produce features for a
classi   er.

of natural language id136, and that there is not
yet any clear level of inferential complexity for
which other approaches work and nn models fail.

2 tree-structured neural networks

we limit the scope of our experiments in this paper
to neural network models that adhere to the lin-
guistic principle of compositionality, which says
that the meanings for complex expressions are de-
rived from the meanings of their parts via speci   c
composition functions (partee, 1984;
janssen,
1997). in our distributed setting, word meanings
are embedding vectors of dimension n. a learned
composition function maps pairs of them to single
phrase vectors of dimension n, which can then be
merged again to represent more complex phrases,
forming a tree structure. once the entire sentence-
level representation has been derived at the top of
the tree, it serves as a    xed-dimensional input for
some subsequent layer function.

to apply these recursive models to our task, we
propose the tree pair model architecture depicted
in fig. 1. in it, the two phrases being compared are
processed separately using a pair of tree-structured
networks that share a single set of parameters. the
resulting vectors are fed into a separate compari-
son layer that is meant to generate a feature vec-
tor capturing the relation between the two phrases.
the output of this layer is then given to a softmax
classi   er, which produces a distribution over the
seven relations represented in table 1.

for the sentence embedding portions of the net-
work, we evaluate both treeid56 models with the
standard nn layer function (1) and those with the

name
(strict) entailment
(strict) reverse entailment
equivalence
alternation
negation
cover
independence

symbol
x (cid:64) y
x (cid:65) y
x     y
x | y
x     y
x (cid:96) y
x # y

set-theoretic de   nition
x     y
x     y
x = y
x     y =         x     y (cid:54)= d
x     y =         x     y = d
x     y (cid:54)=         x     y = d
(else)

example
turtle, reptile
reptile, turtle
couch, sofa
turtle, warthog
able, unable
animal, non-turtle
turtle, pet

table 1: the seven relations of maccartney and manning (2009)   s logic are de   ned abstractly on pairs
of sets drawing from the universe d, but can be straightforwardly applied to any pair of natural language
words, phrases, or sentences. the relations are de   ned so as to be mutually exclusive.

(cid:20)(cid:126)x(l)

(cid:21)

(cid:126)x(r)

more powerful neural tensor network layer func-
tion (2) proposed in chen et al. (2013). the non-
linearity f (x) = tanh(x) is applied elementwise
to the output of either layer function.

(1)

(cid:126)ytreeid56 = f (m

+ (cid:126)b )

(2)

(cid:126)ytreerntn = (cid:126)ytreeid56 + f ((cid:126)x(l)t t[1...n](cid:126)x(r))

here, (cid:126)x(l) and (cid:126)x(r) are the column vector represen-
tations for the left and right children of the node,
and (cid:126)y is the node   s output. the treeid56 con-
catenates them, multiplies them by an n    2n ma-
trix of learned weights, and adds a bias (cid:126)b. the
treerntn adds a learned full rank third-order
tensor t, of dimension n    n    n, modeling
multiplicative interactions between the child vec-
tors. the comparison layer uses the same layer
function as the composition layers (either an nn
layer or an ntn layer) with independently learned
parameters and a separate nonlinearity function.
rather than use a tanh nonlinearity here, we found
better results with the leaky recti   ed linear func-
tion (maas et al., 2013): f (x) = max(x, 0) +
0.01 min(x, 0).

other strong tree-structured models have been
proposed in past work (socher et al., 2014; irsoy
and cardie, 2014; tai et al., 2015), but we believe
that these two provide a valuable case study, and
that positive results on here are likely to generalize
well to stronger models.

to run the model forward, we assemble the two
tree-structured networks so as to match the struc-
tures provided for each phrase, which are either
included in the source data or given by a parser.
the word vectors are then looked up from the vo-
cabulary embedding matrix v (one of the learned
model parameters), and the composition and com-
parison functions are used to pass information up
the tree and into the classi   er. for an objective

   
       
(cid:64) (cid:64)
(cid:65) (cid:65)
   
   
|
|
(cid:96) (cid:96)
# #

(cid:64)
(cid:64)
(cid:64)
  
(cid:96)
  
(cid:96)
  

(cid:65)
(cid:65)
  
(cid:65)
|
|
  
  

   
   
|
(cid:96)
   
(cid:64)
(cid:65)
#

|
|
|
  
(cid:65)
  
(cid:65)
  

(cid:96)
(cid:96)
  
(cid:96)
(cid:64)
(cid:64)
  
  

#
#
  
  
#
  
  
  

table 2: in   3, we assess our models    ability to
learn to do id136 over pairs of relations using
the rules represented here, which are derived from
the de   nitions of the relations in table 1. as an ex-
    p3, the entry in
ample, given that p1 (cid:64) p2 and p2
the (cid:64) row and the     column lets us conclude that
p1 | p3. cells containing a dot correspond to situa-
tions for which no valid id136 can be drawn.

function, we use the negative log likelihood of the
correct label with tuned l2 id173.
we initialize parameters uniformly, using the
range (   0.05, 0.05) for layer parameters and
(   0.01, 0.01) for embeddings, and train the model
using stochastic id119 (sgd) with
learning rates computed using adadelta (zeiler,
2012). the classi   er feature vector is    xed at
75 dimensions and the dimensions of the recur-
sive layers are tuned manually. training times
on cpus vary from hours to days across exper-
iments. on the experiments which use arti   cial
data, we report mean results over    ve fold cross-
validation, where variance across runs is typically
no more than two percentage points. in addition,
because the classes are not necessarily balanced,
we report both accuracy and macroaveraged f1.1
source code and generated data will be released
after the review period.

1we compute macroaveraged f1 as the harmonic mean
of average precision and average recall, both computed for
all classes for which there is test data, setting precision to 0
where it is not de   ned.

3 reasoning about semantic relations

the simplest kinds of deduction in natural logic
involve atomic statements using the relations in
table 1. for instance, from the relation p1 (cid:65) p2
between two propositions, one can infer the rela-
tion p2 (cid:64) p1 by applying the de   nitions of the
relations directly. if one is also given the relation
p2 (cid:65) p3 one can conclude that p1 (cid:65) p3, by basic
set-theoretic reasoning (transitivity of (cid:65)). the full
set of sound such id136s on pairs of premise
relations is depicted in table 2. though these ba-
sic id136s do not involve compositional sen-
tence representations, any successful reasoning
using compositional representations will rely on
the ability to perform sound id136s of this
kind, so our    rst experiment studies how well each
model can learn to perform them them in isolation.

experiments we begin by creating a world
model on which we will base the statements in
the train and test sets. this takes the form of a
small boolean structure in which terms denote sets
of entities from a small domain. fig. 2a depicts
a structure of this form with three entities (a, b,
and c) and eight proposition terms (p1   p8). we
then generate a relational statement for each pair
of terms in the model, as shown in fig. 2b. we
divide these statements evenly into train and test
sets, and delete the test set examples which can-
not be proven from the train examples, for which
there is not enough information for even an ideal
system to choose a correct label. in each experi-
mental run, we create a model with 80 terms over
a domain of 7 elements, yielding a training set of
3200 examples and a test set of 2960 examples.

we trained models with both the nn and ntn
comparison functions on these data sets.2 in both
cases, the models are implemented as described in
  2, but since the items being compared are single
terms rather than full tree structures, the composi-
tion layer is not used, and the two models are not
recursive. we simply present the models with the
(randomly initialized) embedding vectors for each
of two terms, ensuring that the model has no infor-
mation about the terms being compared except for
the relations between them that appear in training.

results the resuts (table 3) show that ntn is
able to accurately encode the relations between the

2since this task relies crucially on the learning of a pair of
vectors, no simpler version of our model is a viable baseline.

p1, p2
{a, b}

p5, p6{a}

{a, b, c}

p3{a, c}

{b}

{}

p4{b, c}

p7, p8{c}

(a) example boolean structure. the terms p1   p8 name
the sets. not all sets have names, and some sets have
multiple names, so that learning     is non-trivial.

train
p1     p2
p1 (cid:65) p5
p4 (cid:65) p8
p5 | p7
    p1
p7

test
    p7
p2
p2 (cid:65) p5
p5     p6
p7 (cid:64) p4
p8 (cid:64) p4

(b) a few examples of atomic statements about the
model. test statements that are not provable from the
training data shown are crossed out.

figure 2: small example structure and data for
learning relation composition.

train

test

# only
15d nn
15d ntn

53.8 (10.5)
99.8 (99.0)
100 (100)

53.8 (10.5)
94.0 (87.0)
99.6 (95.5)

table 3: performance on the semantic relation ex-
periments. these results and all other results on
arti   cial data are reported as mean accuracy scores
over    ve runs followed by mean macroaveraged
f1 scores in parentheses. the    # only    entries
re   ect the frequency of the most frequent class.

terms in the geometric relations between their vec-
tors, and is able to then use that information to re-
cover relations that are not overtly included in the
training data. the nn also generalizes fairly well,
but makes enough errors that it remains an open
question whether it is capable of learning repre-
sentations with these properties.
it is not possi-
ble for us to rule out the possibility that different
optimization techniques or further hyperparameter
tuning could lead an nn model to succeed here.

as an example from our test data, both mod-
els correctly labeled p1 (cid:64) p3, potentially learning
from the training examples {p1 (cid:64) p51, p3 (cid:65) p51}
or {p1 (cid:64) p65, p3 (cid:65) p65}. on another example
involving comparably frequent relations, the ntn

correctly labeled p6 (cid:65) p24, likely on the basis of
    p24}, while
the training examples {p6 (cid:96) p28, p28
the nn incorrectly assigned it #.
4 recursive structure
a successful natural language id136 system
must reason about relations not just over famil-
iar atomic symbols, but also over novel structures
built up recursively from these symbols. this sec-
tion shows that our models can learn a composi-
tional semantics over such structures. in our evalu-
ations, we exploit the fact that our logical language
is in   nite by testing on strings that are longer and
more complex than any seen in training.
experiments as in   3, we generate arti   cial
data from a formal system, but we now replace
the unanalyzed symbols from that experiment
with complex formulae. these formulae repre-
sent a complete classical id118: each
atomic symbol is a variable over the domain {t,
f}, and the only operators are truth-functional
ones. table 4a de   nes this logic, and table 4b
gives some short examples of relational statements
from our data. to compute these relations between
statements, we exhaustively enumerate the sets of
assignments of truth values to propositional vari-
ables that would satisfy each of the statements, and
then we convert the set-theoretic relation between
those assignments into one of the seven relations
in table 1. as a result, each relational statement
represents a valid theorem of the propositional
logic, and to succeed, the models must learn to re-
produce the behavior of a theorem prover.3

in our experiments, we randomly generate
unique pairs of formulae containing up to 12 in-
stances of logical operators each and compute the
relation that holds for each pair. we discard pairs
in which either statement is either a tautology or a
contradiction, for which the seven relations in ta-
ble 1 are unde   ned. the resulting set of formula
pairs is then partitioned into 12 bins according the
number of operators in the larger of the two formu-
lae. we then sample 20% of each bin for a held-
out test set. if we do not implement any constraint
3 socher et al. (2012) show that a matrix-vector treeid56
model somewhat similar to our treerntn can learn boolean
logic, a logic where the atomic symbols are simply the values
t and f. while learning the operators of that logic is not triv-
ial, the outputs of each operator can be represented accurately
by a single bit. in the much more demanding task presented
here, the atomic symbols are variables over these values, and
the sentence vectors must thus be able to distinguish up to 264
distinct conditions on valuations.

formula
p1, p2, p3, p4, p5, p6
not   
(   and   )
(   or   )

interpretation

(cid:74)x(cid:75)     {t, f}
t iff(cid:74)  (cid:75) = f
t iff f /    {(cid:74)  (cid:75),(cid:74)  (cid:75)}
t iff t     {(cid:74)  (cid:75),(cid:74)  (cid:75)}

formed formulae, and (cid:74)  (cid:75) is the interpretation function

(a) well-formed formulae.    and    range over all well-
mapping formulae into {t, f}.

   

not p3

p3
not not p6     p6

p3 (cid:64) (p3 or p2)
not (not p1 and not p2)     (p1 or p2)

(p1 or (p2 or p4)) (cid:65) (p2 and not p4)

(b) examples of the type of statements used for training
and testing. these are relations between well-formed for-
mulae, computed in terms of sets of satisfying interpreta-

tion functions(cid:74)  (cid:75).

table 4: natural logic relations over sentences of
id118.

that the two statements being compared are similar
in any way, then the generated data are dominated
by statements in which the two formulae refer to
largely separate subsets of the six variables, which
means that the # relation is almost always cor-
rect. in an effort to balance the distribution of re-
lation labels without departing from the basic task
of modeling id118, we disallow indi-
vidual pairs of statements from referring to more
than four of the six propositional variables.

in order to test the model   s generalization to un-
seen structures, we discard training examples with
more than 4 logical operators, yielding 60k short
training examples, and 21k test examples across
all 12 bins. in addition to the two tree models, we
also train a summing nn baseline which is largely
identical to the treeid56, except that instead of
using a learned composition function, it simply
sums the term vectors in each expression to com-
pose them before passing them to the comparison
layer. unlike the two tree models, this baseline
does not use word order, and is as such guaranteed
to ignore some information that it would need in
order to succeed perfectly.

results fig. 3 shows the relationship between
test accuracy and statement size. while the sum-
ming baseline model performed poorly across the
board, we found that both recursive models were
able to perform well on unseen small test ex-
amples, with treeid56 accuracy above 98% and
treerntn accuracy above 99% on formulae be-

key test of whether our models can capture this
complexity, we now study the degree to which
they are able to develop suitable representations
for the semantics of natural language quanti   ers
like most and all as they interact with negation
and lexical entailments. quanti   cation and nega-
tion are far from the only place in natural language
where complex functional meanings are found, but
they are natural focus, since they have formed a
standard case study in prior formal work on natu-
ral language id136 (icard and moss, 2013b).
experiments our data consist of pairs of sen-
tences generated from a grammar for a sim-
ple english-like arti   cial language. each sen-
tence contains a quanti   er, a noun which may be
negated, and an intransitive verb which may be
negated. we use the quanti   ers some, most, all,
two, and three, and their negations no, not-all,
not-most, less-than-two, and less-than-three, and
also include    ve nouns, four intransitive verbs,
and the negation symbol not. in order to be able
to de   ne relations between sentences with differ-
ing lexical items, we de   ne the lexical relations
for each noun   noun pair, each verb   verb pair, and
each quanti   er   quanti   er pair. the grammar then
generates pairs of sentences and calculates the re-
lations between them. for instance, our models
might then see pairs like (3) and (4) in training
and be required to then label (5).
(3)
(4)
(5)

(most turtle) swim | (no turtle) move
(all lizard) reptile (cid:64) (some lizard) animal
(most turtle) reptile | (all turtle) (not animal)
in each run, we randomly partition the set of
valid single sentences under the grammar into
training and test, and then label all of the pairs
from within each set to generate a training set of
27k pairs and a test set of 7k pairs. because the
model doesn   t see the test sentences at training
time, it cannot directly use the kind of reasoning
described in   3, and must instead both infer the
word-level relations and learn a complete reason-
ing system over them for our logic.
we use the same summing baseline as in   4.
the highly consistent sentence structure in this ex-
periment means that this model is not as disadvan-
taged by the lack of word order information as it is
in the previous experiment, but the variable place-
ment of not nonetheless introduces potential un-
certainty in the 58.8% of examples that contain a
sentence with a single token of it.

figure 3: results on recursive structure. the ver-
tical dotted line marks the size of the longest train-
ing examples.

low length    ve, indicating that they learned correct
approximations of the underlying logic. training
accuracy was 66.6% for the sumnn, 99.4% for
the treeid56, and 99.8% for the treerntn.

after the size four training cutoff, performance
gradually decays with expression size for both
tree models, suggesting that the learned approx-
imations were accurate but lossy. despite the
treerntn   s stronger performance on short sen-
tences, its performance decayed more quickly than
the treeid56   s. this suggests to us that it learned
to interpret many speci   c    xed-size tree structures
directly, allowing it to get away without learning
as robust generalizations about how to compose
terms in the general case. two factors may have
contributed to the learning of these narrower gen-
eralizations: even with the lower dimension, the
treerntn composition function has about eight
times as many parameters as the treeid56, and
the treerntn worked best with weaker l2 reg-
ularization than the treeid56 (   = 0.0003 vs.
0.001). however, even in the most complex set
of test examples, the treerntn classi   es true ex-
amples of every class but     (which is rare in long
examples, and occurs only once here) correctly the
majority of the time, and the performance of both
models on those examples indicates that both have
learned reasonable approximations of the underly-
ing theorem proving task over recursive structure.

5 reasoning with quanti   ers and

negation

we have seen that recursive models can learn an
approximation of id118. however,
natural languages can express functional meanings
of considerably greater complexity than this. as a

25d$treerntn45d$treeid5645d$sumnn#$only10.99743610.9128220.556410262110.7690720.5437089430.9981560.992080.6736820.5597368440.9925480.984160.5698180.5224450850.9447220.9584180.5326280.5337018360.9173220.94460.5083920.5302063270.8845280.9306920.4841280.5143487980.8473340.918810.4685660.5193576890.8156920.8811880.447670.48873109100.7849360.891090.4402520.50021901110.7849420.8574280.4592780.5156038120.7572920.8475280.4200360.5045984140%50%60%70%80%90%100%123456789101112accuracysize of longer expression25d treerntn45d treeid5645d sumnn# onlytrain

test

# only
25d sumnn
25d treeid56
25d treerntn

35.4
(7.5)
96.9 (97.7)
99.6 (99.6)
100 (100)

35.4 (7.5)
93.9 (95.0)
99.2 (99.3)
99.7 (99.5)

table 5: performance on the quanti   er experi-
ments, given as % correct and macroaveraged f1.

results the results (table 5) show that both tree
models are able to learn to generalize the underly-
ing logic almost perfectly. the baseline summing
model can largely memorize the training data, but
does not generalize as well. we do not    nd any
consistent pattern in the handful of errors made by
either tree model, and no errors were consistent
across model restarts, suggesting that there is no
fundamental obstacle to learning a perfect model
for this problem.

6 the sick id123 challenge

the speci   c model architecture that we use is
novel, and though the underlying tree structure ap-
proach has been validated elsewhere, our experi-
ments so far do not guarantee that it viable model
for handling id136 over real natural language
data. to investigate our models    ability to handle
the noisy labels and the diverse range of linguis-
tic structures seen in typical natural language data,
we use the sick id123 challenge cor-
pus (marelli et al., 2014b). the corpus consists
of about 10k natural language sentence pairs, la-
beled with entailment, contradiction, or neutral.
at only a few thousand distinct sentences (many
of them variants on an even smaller set of tem-
plate sentences), the corpus is not large enough to
train a high quality learned model of general nat-
ural language, but it is the largest human-labeled
entailment corpus that we are aware of, and our
results nonetheless show that tree-structured nn
models can learn to do id136 in the real world.
adapting to this task requires us to make a few
additions to the techniques discussed in   2. in or-
der to better handle rare words, we initialized our
id27s using 200 dimensional vectors
trained with glove (pennington et al., 2014) on
data from wikipedia. since 200 dimensional vec-
tors are too large to be practical in an treerntn
on a small dataset, a new embedding transforma-
tion layer is needed. before any embedding is
used as an input to a recursive layer, it is passed

neutral

30d

30d

50d
only sumnn trid56 trrntn
74.0
50.0
97.8
56.7
76.9
56.7
88
0
100
0
72
28
68
64
96
96
77.3
50.0

67.0
95.4
74.9
68
100
64
66
79
73.5

68.0
96.6
73.4
76
96
72
61
68
73.9

dg train
sick train
sick test
passive (4%)
neg (7%)
subst (24%)
multied (39%)
diff (26%)
short (47%)

table 6: classi   cation accuracy, including a cat-
egory breakdown for sick test data. categories
are shown with their frequencies.

through an additional tanh neural network layer
with the same output dimension as the recursive
layer. this new layer aggregates any usable infor-
mation from the embedding vectors into a more
compact working representation. an identical
layer is added to the sumnn between the word
vectors and the comparison layer.

we also supplemented the sick training data4
with 600k examples of entailment data from the
denotation graph project (dg, hodosh et al.
2014, also used by the winning sick submis-
sion), a corpus of noisy automatically labeled en-
tailment examples over image captions, the same
genre of text from which sick was drawn. we
trained a single model on data from both sources,
but used a separate set of softmax parameters for
classifying into the labels from each source. we
parsed the data from both sources with the stan-
ford pid18 parser v. 3.3.1 (klein and manning,
2003). we also found that we were able to train
a working model much more quickly with an ad-
ditional technique: we collapse subtrees that were
identical across both sentences in a pair by replac-
ing them with a single head word. the training
and test data on which we report performance are
collapsed in this way, and both collapsed and un-
collapsed copies of the training data are used in
training. finally, in order to improve regulariza-
tion on the noisier data, we used dropout (srivas-
tava et al., 2014) at the input to the comparison
layer (10%) and at the output from the embedding
transform layer (25%).

4we tuned the model using performance on a held out de-
velopment set, but report performance here for a version of
the model trained on both the training and development data
and tested on the 4,928 example sick test set. we also report
training accuracy on a small sample from each data source.

entailment
the patient is being helped by the doctor
contradiction
a little girl is playing the violin on a beach
the yellow dog is drinking water from a bottle contradiction
a woman is breaking two eggs in a bowl
dough is being spread by a man

neutral
neutral

the doctor is helping the patient (passive)
there is no girl playing the violin on a beach (neg)
the yellow dog is drinking water from a pot (subst)
a man is mixing a few ingredients in a bowl (multied)
a woman is slicing meat with a knife (diff)

table 7: examples of each category used in error analysis from the sick test data.

results despite the small amount of high qual-
ity training data available and the lack of resources
for learning lexical relationships, the results (ta-
ble 6) show that our tree-structured models per-
form competitively on id123, beating
a strong baseline. neither model reached the per-
formance of the winning system (84.6%), but the
treerntn did exceed that of eight out of 18 sub-
mitted systems, including several which used so-
phisticated hand-engineered features and lexical
resources speci   c to the version of the entailment
task at hand.

to better understand our results, we manually
annotated a fraction of the sick test set, using
mutually exclusive categories for passive/active
alternation pairs (passive), pairs differing only
by the presence of negation (neg), pairs differing
by a single word or phrase substitution (subst),
pairs differing by multiple edits (multied), and
pairs with little or no content word overlap (diff).
examples of each are in table 7. we annotated
100 random examples to judge the frequency of
each category, and continued selectively annotat-
ing until each category contained at least 25. we
also use the category short for pairs in which
neither sentence contains more than ten words.

the results (table 7) show that the treerntn
performs especially strongly in the two categories
which pick out speci   c syntactic con   gurations,
passive and neg, suggesting that that model
has learned to encode the relevant structures well.
it also performs fairly on subst, which most
closely parallels the lexical entailment id136s
addressed in   5. in addition, none of the models
perform dramatically better on the short pairs
than on the rest of the data, suggesting that the
performance decay observed in   4 may not impact
models trained on typical natural language text.

it is known that a model can perform well on
sick (like other natural language id136 cor-
pora) without taking advantage of compositional
syntactic or semantic structure (marelli et al.,
2014a), and our summing baseline model is pow-
erful enough to do this. our tree models nonethe-

less perform substantially better, and we remain
con   dent that given suf   cient data, it should be
possible for the tree models, and not the summing
model, to learn a truly high-quality solution.

7 discussion and conclusion

this paper    rst evaluates two recursive models on
three natural language id136 tasks over clean
arti   cial data, covering the core relational alge-
bra of natural logic with entailment and exclu-
sion, recursive structure, and quanti   cation. we
then show that the same models can learn to per-
form an entailment task on natural language. the
results suggest that treerntns, and potentially
also treeid56s, can learn to faithfully reproduce
logical id136 behaviors from reasonably-sized
training sets. these positive results are promising
for the future of learned representation models in
the applied modeling of id152.
some questions about the abilities of these mod-
els remain open. even the treerntn falls short
of perfection in the recursion experiment, with
performance falling off steadily as the size of the
expressions grows. it remains to be seen whether
these de   ciencies are limiting in practice, and
whether they can be overcome with stronger mod-
els or learning techniques. in addition, interesting
analytical questions remain about how these mod-
els encode the underlying logics. neither the un-
derlying logical theories, nor any straightforward
parameter inspection technique provides much in-
sight on this point, but we hope that further exper-
iments may reveal structure in the learned param-
eters or the representations they produce.

our sick experiments similarly only begin to
reveal the potential of these models to learn to per-
form complex semantic id136s from corpora,
and there is ample room to develop our under-
standing using new and larger sources of natural
language data. nonetheless, the rapid progress the
   eld has made with these models in recent years
provides ample reason to be optimistic that learned
representation models can be trained to meet all
the challenges of natural language semantics.

acknowledgments
we thank jeffrey pennington, richard socher, and
audiences at csli, nuance, and baylearn, as well
as neha nayak for developing the sick collaps-
ing technique.

t.m.v. janssen. 1997. compositionality.

in j. van
benthem and a. ter meulen, editors, handbook of
logic and language. mit press and north-holland.

d. klein and c.d. manning. 2003. accurate unlexi-

calized parsing. in proc. acl.

references
m. baroni, r. bernardi, n.q. do, and c.c. shan. 2012.
entailment above the word level in distributional se-
mantics. in proc. eacl.

d. chen, r. socher, c.d. manning, and a.y. ng. 2013.
learning new facts from knowledge bases with neu-
ral tensor networks and semantic word vectors. in
proc. iclr.

b. coecke, m. sadrzadeh, and s. clark. 2011. math-
ematical foundations for a compositional distributed
model of meaning. linguistic analysis, 36(1   4).

i. dagan, o. glickman, and b. magnini. 2006. the
pascal recognising id123 chal-
lenge. in machine learning challenges. evaluating
predictive uncertainty, visual object classi   cation,
and recognising tectual entailment. springer.

c. goller and a. kuchler.

1996. learning task-
dependent distributed representations by backprop-
in proc. ieee interna-
agation through structure.
tional conference on neural networks, volume 1.
ieee.

e. grefenstette, m. sadrzadeh, s. clark, b. coecke,
and s. pulman. 2011. concrete sentence spaces for
compositional distributional models of meaning. in
proc. iwcs.

e. grefenstette. 2013. towards a formal distributional
semantics: simulating logical calculi with tensors.
arxiv:1304.5823.

k.m. hermann, e. grefenstette, and p. blunsom. 2013.
   not not bad    is not    bad   : a distributional account
of negation. in proc. of the 2013 workshop on con-
tinuous vector space models and their composition-
ality.

m. hodosh, p. young, a. lai, and j. hockenmaier.
2014. from image descriptions to visual denota-
tions: new similarity metrics for semantic id136
over event descriptions. tacl.

t.f. icard and l.s. moss. 2013a. a complete calculus
of monotone and antitone higher-order functions. in
n. galatos, a. kurz, and c. tsinakis, editors, proc.
topology, algebra, and categories in logic.

t.f. icard and l.s. moss. 2013b. recent progress on

monotonicity. lilt, 9(7).

o. irsoy and c. cardie. 2014. deep recursive neural
networks for compositionality in language. in proc.
nips.

p. liang, m.i. jordan, and d. klein. 2013. learning
dependency-based id152. com-
putational linguistics, 39(2).

a.l. maas, a.y. hannun, and a.y. ng. 2013. recti-
   er nonlinearities improve neural network acoustic
models. in proc. icml.

b. maccartney and c.d. manning. 2009. an extended

model of natural logic. in proc. iwcs.

m. marelli, l. bentivogli, m. baroni, r. bernardi,
s. menini, and r. zamparelli. 2014a. semeval-
2014 task 1: evaluation of compositional distribu-
tional semantic models on full sentences through se-
mantic relatedness and id123. semeval-
2014.

m. marelli, s. menini, m. baroni, l. bentivogli,
r. bernardi, and r. zamparelli. 2014b. a sick
cure for the evaluation of compositional distribu-
tional semantic models. in proc. lrec.

j. mitchell and m. lapata. 2010. composition in dis-
tributional models of semantics. cognitive science,
34(8).

b.h. partee. 1984. compositionality. in fred land-
man and frank veltman, editors, varieties of formal
semantics. foris.

j. pennington, r. socher, and c.d. manning. 2014.
glove: global vectors for word representation. in
proc. emnlp.

t. rockt  aschel, m. bosnjak, s. singh, and s. riedel.
2014. low-dimensional embeddings of logic.
in
proc. the acl 2014 workshop on id29.

r. socher, e.h. huang, j. pennington, c.d. manning,
and a.y. ng. 2011a. dynamic pooling and unfold-
ing recursive autoencoders for paraphrase detection.
in proc. nips.

r. socher, j. pennington, e.h. huang, a.y. ng, and
c.d. manning. 2011b. semi-supervised recursive
autoencoders for predicting sentiment distributions.
in proc. emnlp.

r. socher, b. huval, c.d. manning, and a.y. ng.
2012. semantic compositionality through recursive
matrix-vector spaces. in proc. emnlp.

r. socher, a. perelygin, j. wu, j. chuang, c.d. man-
ning, a.y. ng, and c. potts. 2013. recursive deep
models for semantic compositionality over a senti-
ment treebank. in proc. emnlp.

r. socher, a. karpathy, q.v. le, c.d. manning, and
a.y. ng. 2014. grounded id152
for    nding and describing images with sentences.
tacl.

n. srivastava, g. hinton, a. krizhevsky, i. sutskever,
and r. salakhutdinov.
2014. dropout: a sim-
ple way to prevent neural networks from over   tting.
jmlr, 15(1).

k.s. tai, r. socher, and c.d. manning.

improved semantic
structured id137.
proc. acl.

representations

2015.
from tree-
in

d.h.d. warren and f.c.n. pereira. 1982. an ef   cient
easily adaptable system for interpreting natural lan-
guage queries. american journal of computational
linguistics.

y. watanabe, j. mizuno, e. nichols, n. okazaki, and
k. inui. 2012. a latent discriminative model for
compositional entailment relation recognition using
natural logic. in proc. coling.

m.d. zeiler. 2012. adadelta: an adaptive learning

rate method. arxiv:1212.5701.

j.m. zelle and r.j. mooney.

1996. learning to
parse database queries using inductive logic pro-
gramming. in proc. aaai.

l.s. zettlemoyer and m. collins. 2005. learning to
map sentences to logical form: structured classi   ca-
tion with probabilistic categorial grammars. in proc.
of the 21st conference on uncertainty in arti   cial
intelligence.

