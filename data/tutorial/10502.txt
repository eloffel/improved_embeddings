6
1
0
2

 

v
o
n
7

 

 
 
]

g
l
.
s
c
[
 
 

4
v
9
1
5
0
0

.

9
0
5
1
:
v
i
x
r
a

under review as a conference paper at iclr 2016

importance weighted autoencoders

yuri burda, roger grosse & ruslan salakhutdinov
department of computer science
university of toronto
toronto, on, canada
{yburda,rgrosse,rsalakhu}@cs.toronto.edu

abstract

the variational autoencoder (vae; kingma & welling (2014)) is a recently pro-
posed generative model pairing a top-down generative network with a bottom-up
recognition network which approximates posterior id136. it typically makes
strong assumptions about posterior id136, for instance that the posterior dis-
tribution is approximately factorial, and that its parameters can be approximated
with nonid75 from the observations. as we show empirically, the
vae objective can lead to overly simpli   ed representations which fail to use the
network   s entire modeling capacity. we present the importance weighted autoen-
coder (iwae), a generative model with the same architecture as the vae, but
which uses a strictly tighter log-likelihood lower bound derived from importance
weighting. in the iwae, the recognition network uses multiple samples to ap-
proximate the posterior, giving it increased    exibility to model complex posteri-
ors which do not    t the vae modeling assumptions. we show empirically that
iwaes learn richer latent space representations than vaes, leading to improved
test log-likelihood on density estimation benchmarks.

1

introduciton

in recent years, there has been a renewed focus on learning deep generative models (hinton et al.,
2006; salakhutdinov & e., 2009; gregor et al., 2014; kingma & welling, 2014; rezende et al.,
2014). a common dif   culty faced by most approaches is the need to perform posterior id136
during training: the log-likelihood gradients for most latent variable models are de   ned in terms
of posterior statistics (e.g. salakhutdinov & e. (2009); neal (1992); gregor et al. (2014)). one
approach for dealing with this problem is to train a recognition network alongside the generative
model (dayan et al., 1995). the recognition network aims to predict the posterior distribution over
latent variables given the observations, and can often generate a rough approximation much more
quickly than generic id136 algorithms such as mcmc.
the variational autoencoder (vae; kingma & welling (2014); rezende et al. (2014)) is a recently
proposed generative model which pairs a top-down generative network with a bottom-up recognition
network. both networks are jointly trained to maximize a variational lower bound on the data log-
likelihood. vaes have recently been successful at separating style and content (kingma et al., 2014;
kulkarni et al., 2015) and at learning to    draw    images in a realistic manner (gregor et al., 2015).
vaes make strong assumptions about the posterior distribution. typically vae models assume that
the posterior is approximately factorial, and that its parameters can be predicted from the observables
through a nonid75. because they are trained to maximize a variational lower bound
on the log-likelihood, they are encouraged to learn representations where these assumptions are
satis   ed, i.e. where the posterior is approximately factorial and predictable with a neural network.
while this effect is bene   cial, it comes at a cost: constraining the form of the posterior limits the
expressive power of the model. this is especially true of the vae objective, which harshly penalizes
approximate posterior samples which are unlikely to explain the data, even if the recognition network
puts much of its id203 mass on good explanations.
in this paper, we introduce the importance weighted autoencoder (iwae), a generative model which
shares the vae architecture, but which is trained with a tighter log-likelihood lower bound de-

1

under review as a conference paper at iclr 2016

rived from importance weighting. the recognition network generates multiple approximate pos-
terior samples, and their weights are averaged. as the number of samples is increased, the lower
bound approaches the true log-likelihood. the use of multiple samples gives the iwae additional
   exibility to learn generative models whose posterior distributions do not    t the vae modeling as-
sumptions. this approach is related to reweighted wake sleep (bornschein & bengio, 2015), but
the iwae is trained using a single uni   ed objective. compared with the vae, our iwae is able to
learn richer representations with more latent dimensions, which translates into signi   cantly higher
log-likelihoods on density estimation benchmarks.

2 background

in this section, we review the variational autoencoder (vae) model of kingma & welling (2014). in
particular, we describe a generalization of the architecture to multiple stochastic hidden layers. we
note, however, that kingma & welling (2014) used a single stochastic hidden layer, and there are
other sensible generalizations to multiple layers, such as the one presented by rezende et al. (2014).
the vae de   nes a generative process in terms of ancestral sampling through a cascade of hidden
layers:

p(x|  ) =

p(hl|  )p(hl   1|hl,   )       p(x|h1,   ).

(cid:88)

h1,...,hl

here,    is a vector of parameters of the variational autoencoder, and h = {h1, . . . , hl} denotes the
stochastic hidden units, or latent variables. the dependence on    is often suppressed for clarity. for
convenience, we de   ne h0 = x. each of the terms p(h(cid:96)|h(cid:96)+1) may denote a complicated nonlinear
relationship, for instance one computed by a multilayer neural network. however, it is assumed
that sampling and id203 evaluation are tractable for each p(h(cid:96)|h(cid:96)+1). note that l denotes
the number of stochastic hidden layers; the deterministic layers are not shown explicitly here. we
assume the recognition model q(h|x) is de   ned in terms of an analogous factorization:

q(h|x) = q(h1|x)q(h2|h1)       q(hl|hl   1),

(1)

(2)

(cid:20) p(x, h)

(cid:21)

(cid:20)

(cid:21)

where sampling and id203 evaluation are tractable for each of the terms in the product.
in this work, we assume the same families of id155 distributions as kingma &
welling (2014). in particular, the prior p(hl) is    xed to be a zero-mean, unit-variance gaussian.
in general, each of the conditional distributions p(h(cid:96)| h(cid:96)+1) and q(h(cid:96)|h(cid:96)   1) is a gaussian with
diagonal covariance, where the mean and covariance parameters are computed by a deterministic
feed-forward neural network. for real-valued observations, p(x|h1) is also de   ned to be such a
gaussian; for binary observations, it is de   ned to be a bernoulli distribution whose mean parameters
are computed by a neural network.
the vae is trained to maximize a variational lower bound on the log-likelihood, as derived from
jensen   s inequality:

= l(x).

    eq(h|x)

p(x, h)
q(h|x)

log p(x) = log eq(h|x)

log

q(h|x)

(3)
since l(x) = log p(x)     dkl(q(h|x)||p(h|x)), the training procedure is forced to trade off the
data log-likelihood log p(x) and the kl divergence from the true posterior. this is bene   cial, in that
it encourages the model to learn a representation where posterior id136 is easy to approximate.
if one computes the log-likelihood gradient for the recognition network directly from eqn. 3, the re-
sult is a reinforce-like update rule which trains slowly because it does not use the log-likelihood
gradients with respect to latent variables (dayan et al., 1995; mnih & gregor, 2014).
instead,
kingma & welling (2014) proposed a reparameterization of the recognition distribution in terms
of auxiliary variables with    xed distributions, such that the samples from the recognition model are
a deterministic function of the inputs and auxiliary variables. while they presented the reparameter-
ization trick for a variety of distributions, for convenience we discuss the special case of gaussians,
since that is all we require in this work. (the general reparameterization trick can be used with our
iwae as well.)
in this paper, the recognition distribution q(h(cid:96)|h(cid:96)   1,   ) always takes the form of a gaussian
n (h(cid:96)|  (h(cid:96)   1,   ),   (h(cid:96)   1,   )), whose mean and covariance are computed from the the states of

2

under review as a conference paper at iclr 2016

the hidden units at the previous layer and the model parameters. this can be alternatively expressed
by    rst sampling an auxiliary variable  (cid:96)     n (0, i), and then applying the deterministic mapping
(4)
the joint recognition distribution q(h|x,   ) over all latent variables can be expressed in terms of
a deterministic mapping h( , x,   ), with   = ( 1, . . . ,  l), by applying eqn. 4 for each layer in
sequence. since the distribution of   does not depend on   , we can reformulate the gradient of the
bound l(x) from eqn. 3 by pushing the gradient operator inside the expectation:

h(cid:96)( (cid:96), h(cid:96)   1,   ) =   (h(cid:96)   1,   )1/2 (cid:96) +   (h(cid:96)   1,   ).

      log eh   q(h|x,  )

(cid:21)

(cid:20) p(x, h|  )

q(h|x,   )

(cid:20)
=      e 1,..., l   n (0,i)

= e 1,..., l   n (0,i)

log

      log

p(x, h( , x,   )|  )
q(h( , x,   )|x,   )
p(x, h( , x,   )|  )
q(h( , x,   )|x,   )

(5)

(6)

.

(cid:21)
(cid:21)

(cid:20)

assuming the mapping h is represented as a deterministic feed-forward neural network, for a    xed
 , the gradient inside the expectation can be computed using standard id26. in practice,
one approximates the expectation in eqn. 6 by generating k samples of   and applying the monte
carlo estimator

1
k

      log w (x, h( i, x,   ),   )

(7)
with w(x, h,   ) = p(x, h|  )/q(h|x,   ). this is an unbiased estimate of      l(x). we note that
the vae update and the basic reinforce-like update are both unbiased estimators of the same
gradient, but the vae update tends to have lower variance in practice because it makes use of the
log-likelihood gradients with respect to the latent variables.

i=1

k(cid:88)

3

importance weighted autoencoder

the vae objective of eqn. 3 heavily penalizes approximate posterior samples which fail to explain
the observations. this places a strong constraint on the model, since the variational assumptions
must be approximately satis   ed in order to achieve a good lower bound. in particular, the posterior
distribution must be approximately factorial and predictable with a feed-forward neural network.
this vae criterion may be too strict; a recognition network which places only a small fraction
(e.g. 20%) of its samples in the region of high posterior id203 region may still be suf   cient for
performing accurate id136. if we lower our standards in this way, this may give us additional
   exibility to train a generative network whose posterior distributions do not    t the vae assump-
tions. this is the motivation behind our proposed algorithm, the importance weighted autoencoder
(iwae).
our iwae uses the same architecture as the vae, with both a generative network and a recognition
network. the difference is that it is trained to maximize a different lower bound on log p(x). in
particular, we use the following lower bound, corresponding to the k-sample importance weighting
estimate of the log-likelihood:

lk(x) = eh1,...,hk   q(h|x)

log

1
k

p(x, hi)
q(hi|x)

.

(8)

here, h1, . . . , hk are sampled independently from the recognition model. the term inside the sum
corresponds to the unnormalized importance weights for the joint distribution, which we will denote
as wi = p(x, hi)/q(hi|x).
this is a lower bound on the marginal log-likelihood, as follows from jensen   s inequality and the
fact that the average importance weights are an unbiased estimator of p(x):

(cid:34)

(cid:35)

k(cid:88)

i=1

lk = e

log

1
k

    log e

wi

(cid:35)

(cid:34)

k(cid:88)

i=1

1
k

wi

= log p(x),

(9)

where the expectations are with respect to q(h|x).
it is perhaps unintuitive that importance weighting would be a reasonable estimator in high dimen-
sions. observe, however, that the special case of k = 1 is equivalent to the standard vae objective
shown in eqn. 3. using more samples can only improve the tightness of the bound:

3

(cid:34)

k(cid:88)

i=1

(cid:35)

under review as a conference paper at iclr 2016

theorem 1. for all k, the lower bounds satisfy

moreover, if p(h, x)/q(h|x) is bounded, then lk approaches log p(x) as k goes to in   nity.

log p(x)     lk+1     lk.

(10)

proof. see appendix a.
the bound lk can be estimated using the straightforward monte carlo estimator, where we generate
samples from the recognition network and average the importance weights. one might worry about
the variance of this estimator, since importance weighting famously suffers from extremely high
variance in cases where the proposal and target distributions are not a good match. however, as
our estimator is based on the log of the average importance weights, it does not suffer from high
variance. this argument is made more precise in appendix b.

3.1 training procedure

to train an iwae with a stochastic gradient based optimizer, we use an unbiased estimate of the
gradient of lk, de   ned in eqn. 8. as with the vae, we use the reparameterization trick to derive a
low-variance upate rule:

     lk(x) =      eh1,...,hk

log

1
k

(cid:34)

(cid:35)

wi

k(cid:88)

i=1

(cid:34)

log

i=1

1
k

w(x, h(x,  i,   ),   )

k(cid:88)
k(cid:88)

(cid:35)
(cid:35)
(cid:35)
(cid:102)wi      log w(x, h(x,  i,   ),   )

w(x, h(x,  i,   ),   )

1
k

i=1

      log

=      e 1,..., k
(cid:34)
(cid:34) k(cid:88)

= e 1,..., k

= e 1,..., k

(11)

(12)

,

(13)

where  1, . . . ,  k are the same auxiliary variables as de   ned in section 2 for the vae, wi =

w(x, h(x,  i,   ),   ) are the importance weights expressed as a deterministic function, and (cid:102)wi =
wi/(cid:80)k

i=1 wi are the normalized importance weights.

i=1

in the context of a gradient-based learning algorithm, we draw k samples from the recognition
network (or, equivalently, k sets of auxiliary variables), and use the monte carlo estimate of eqn. 13:

k(cid:88)

(cid:102)wi      log w (x, h( i, x,   ),   ) .

(14)

i=1

in the special case of k = 1, the single normalized weight(cid:102)w1 takes the value 1, and one obtains the

vae update rule.
we unpack this update because it does not quite parallel that of the standard vae.1 the gradient of
the log weights decomposes as:

      log w(x, h(x,  i,   ),   ) =       log p(x, h(x,  i,   )|  )           log q(h(x,  i,   )|x,   ).

(15)

the    rst term encourages the generative model to assign high id203 to each h(cid:96) given h(cid:96)+1
(following the convention that x = h0). it also encourages the recognition network to adjust the
hidden representations so that the generative network makes better predictions. in the case of a single
stochastic layer (i.e. l = 1), the combination of these two effects is equivalent to id26
in a stochastic autoencoder. the second term of this update encourages the recognition network to
have a spread-out distribution over predictions. this update is averaged over the samples with weight
proportional to the importance weights, motivating the name    importance weighted autoencoder.   

1kingma & welling (2014) separated out the kl divergence in the bound of eqn. 3 in order to achieve a
simpler and lower-variance update. unfortunately, no analogous trick applies for k > 1. in principle, the iwae
updates may be higher variance for this reason. however, in our experiments, we observed that the performance
of the two update rules was indistinguishable in the case of k = 1.

4

under review as a conference paper at iclr 2016

the dominant computational cost in iwae training is computing the activations and parameter gra-
dients needed for       log w(x, h(x,  i,   ),   ). this corresponds to the forward and backward passes
in id26. in the basic iwae implementation, both passes must be done independently for
each of the k samples. therefore, the number of operations scales linearly with k. in our gpu-based
implementation, the samples are processed in parallel by replicating each training example k times
within a mini-batch.
one can greatly reduce the computational cost by adding another form of stochasticity. speci   cally,
only the forward pass is needed to compute the importance weights. the sum in eqn. 14 can be
and then computing       log w(x, h(x,  i,   ),   ). this method requires k forward passes and one
backward pass per training example. since the backward pass requires roughly twice as many add-
multiply operations as the forward pass, for large k, this trick reduces the number of add-multiply
operations by roughly a factor of 3. this comes at the cost of increased variance in the updates, but
empirically we have found the tradeoff to be favorable.

stochastically approximated by choosing a single sample  i proprtional to its normalized weight(cid:102)wi

4 related work

there are several broad families of approaches to training deep generative models. some models
are de   ned in terms of boltzmann distributions (smolensky, 1986; salakhutdinov & e., 2009). this
has the advantage that many of the conditional distributions are tractable, but the inability to sample
from the model or compute the partition function has been a major roadblock (salakhutdinov &
murray, 2008). other models are de   ned in terms of belief networks (neal, 1992; gregor et al.,
2014). these models are tractable to sample from, but the conditional distributions become tangled
due to the explaining away effect.
one strategy for dealing with intractable posterior id136 is to train a recognition network
which approximates the posterior. a classic approach was the wake-sleep algorithm, used to train
helmholtz machines (dayan et al., 1995). the generative model was trained to model the condi-
tionals inferred by the recognition net, and the recognition net was trained to explain synthetic data
generated by the generative net. unfortunately, wake-sleep trained the two networks on different ob-
jective functions. deep autoregressive networks (gregor et al., 2014) consisted of deep generative
and recognition networks trained using a single variational lower bound. neural variational infer-
ence and learning (mnih & gregor, 2014) is another algorithm for training recognition networks
which reduces stochasticity in the updates by training a third network to predict reward baselines
in the context of the reinforce algorithm (williams, 1992). salakhutdinov & larochelle (2010)
used a recognition network to approximate the posterior distribution in deep id82s.
id5 (kingma & welling, 2014; rezende et al., 2014), as described in detail in
section 2, are another combination of generative and recognition networks, trained with the same
variational objective as darn and nvil. however, in place of reinforce, they reduce the
variance of the updates through a clever reparameterization of the random choices. the reparame-
terization trick is also known as    backprop through a random number generator    (williams, 1992).
one factor distinguishing vaes from the other models described above is that the model is described
in terms of a simple distribution followed by a deterministic mapping, rather than a sequence of
stochastic choices. similar architectures have been proposed which use different training objectives.
id3 (goodfellow et al., 2014) train a generative network and a recog-
nition network which act in opposition: the recognition network attempts to distinguish between
training examples and generated samples, and the generative model tries to generate samples which
fool the recognition network. maximum mean discrepancy (mmd) networks (li et al., 2015; dziu-
gaite et al., 2015) attempt to generate samples which match a certain set of statistics of the training
data. they can be viewed as a kind of adversarial net where the adversary simply looks at the set of
pre-chosen statistics (dziugaite et al., 2015). in contrast to vaes, the training criteria for adversarial
nets and mmd nets are not based on the data log-likelihood.
other researchers have derived log-id203 lower bounds by way of importance sampling. tang
& salakhutdinov (2013) and ba et al. (2015) avoided recognition networks entirely, instead perform-
ing id136 using importance sampling from the prior. gogate et al. (2007) presented a variety
of graphical model id136 algorithms based on importance weighting. reweighted wake-sleep

5

under review as a conference paper at iclr 2016

(rws) of bornschein & bengio (2015) is another recognition network approach which combines
the original wake-sleep algorithm with updates to the generative network equivalent to gradient as-
cent on our bound lk. however, bornschein & bengio (2015) interpret this update as following a
biased estimate of       log p(x), whereas we interpret it as following an unbiased estimate of      lk.
the iwae also differs from rws in that the generative and recognition networks are trained to
maximize a single objective, lk. by contrast, the q-wake and sleep steps of rws do not appear to
be related to lk. finally, the iwae differs from rws in that it makes use of the reparameterization
trick.
apart from our approach of using multiple approximate posterior samples, another way to improve
the    exibility of posterior id136 is to use a more sophisticated algorithm than importance sam-
pling. examples of this approach include normalizing    ows (rezende & mohamed, 2015) and the
hamiltonian variational approximation of salimans et al. (2015).
after the publication of this paper the authors learned that the idea of using an importance weighted
lower bound for training id5 has been independently explored by laurent dinh
and vincent dumoulin, and preliminary results of their work were presented at the 2014 cifar
ncap deep learning summer school.

5 experimental results

we have compared the generative performance of the vae and iwae in terms of their held-out log-
likelihoods on two density estimation benchmark datasets. we have further investigated a particular
issue we have observed with vaes and iwaes, namely that they learn latent spaces of signi   cantly
lower dimensionality than the modeling capacity they are allowed. we tested whether the iwae
training method ameliorates this effect.

5.1 evaluation on density estimation

we evaluated the models on two benchmark datasets: mnist, a dataset of images of handwritten
digits (lecun et al., 1998), and omniglot, a dataset of handwritten characters in a variety of world
alphabets (lake et al., 2013). in both cases, the observations were binarized 28    28 images.2 we
used the standard splits of mnist into 60,000 training and 10,000 test examples, and of omniglot
into 24,345 training and 8,070 test examples.
we trained models with two architectures:

1. an architecture with a single stochastic layer h1 with 50 units. in between the observations

and the stochastic layer were two deterministic layers, each with 200 units.

2. an architecture with two stochastic layers h1 and h2, with 100 and 50 units, respectively.
in between x and h1 were two deterministic layers with 200 units each. in between h1 and
h2 were two deterministic layers with 100 units each.

all deterministic hidden units used the tanh nonlinearity. all stochastic layers used gaussian dis-
tributions with diagonal covariance, with the exception of the visible layer, which used bernoulli
distributions. an exp nonlinearity was applied to the predicted variances of the gaussian distribu-
tions. the network architectures are summarized in appendix c.
all models were initialized with the heuristic of glorot & bengio (2010). for optimization, we used
adam (kingma & ba, 2015) with parameters   1 = 0.9,   2 = 0.999,   = 10   4 and minibaches of
size 20. the training proceeded for 3i passes over the data with learning rate of 0.001    10   i/7 for
i=0 3i = 3280 passes over the data). this learning rate schedule was

i = 0 . . . 7 (for a total of(cid:80)7

chosen based on preliminary experiments training a vae with one stochastic layer on mnist.

2unfortunately, the generative modeling literature is inconsistent about the method of binarization, and
different choices can lead to considerably different log-likelihood values. we follow the procedure of salakhut-
dinov & murray (2008): the binary-valued observations are sampled with expectations equal to the real values
in the training set. see appendix d for an alternative binarization scheme.

6

under review as a conference paper at iclr 2016

mnist

omniglot

vae

iwae

vae

iwae

# stoch.
layers

1

2

k

1
5
50
1
5
50

nll

86.76
86.47
86.35
85.33
85.01
84.78

active
units

19
20
20
16+5
17+5
17+5

nll

86.76
85.54
84.78
85.33
83.89
82.90

active
units

19
22
25
16+5
21+5
26+7

nll

108.11
107.62
107.80
107.58
106.31
106.30

active
units

28
28
28
28+4
30+5
30+5

nll

108.11
106.12
104.67
107.56
104.79
103.38

active
units

28
34
41
30+5
38+6
44+7

table 1: results on density estimation and the number of active latent dimensions. for models with two latent
layers,    k1+k2    denotes k1 active units in the    rst layer and k2 in the second layer. the generative performance
of iwaes improved with increasing k, while that of vaes bene   tted only slightly. two-layer models achieved
better generative performance than one-layer models.

for each number of samples k     {1, 5, 50} we trained a vae with the gradient of l(x) estimted
as in eqn. 7 and an iwae with the gradient estimated as in eqn. 14. for each k, the vae and the
iwae were trained for approximately the same length of time.
all log-likelihood values were estimated as the mean of l5000 on the test set. hence, the reported
values are stochastic lower bounds on the true value, but are likely to be more accurate than the
lower bounds used for training.
the log-likelihood results are reported in table 1. our vae results are comparable to those previ-
ously reported in the literature. we observe that training a vae with k > 1 helped only slightly. by
contrast, using multiple samples improved the iwae results considerably on both datasets. note that
the two algorithms are identical for k = 1, so the results ought to match up to random variability.
on mnist, iwae with two stochastic layers and k = 50 achieves a log-likelihood of -82.90 on
the permutation-invariant model on this dataset. by comparison, id50 achieved log-
likelihood of approximately -84.55 nats (murray & salakhutdinov, 2009), and deep autoregressive
networks achieved log-likelihood of -84.13 nats (gregor et al., 2014). gregor et al. (2015), who
exploited spatial structure, achieved a log-likelihood of -80.97. we did not    nd over   tting to be a
serious issue for either the vae or the iwae: in both cases, the training log-likelihood was 0.62 to
0.79 nats higher than the test log-likelihood. we present samples from our models in appendix e.
for the omniglot dataset, the best performing iwae has log-likelihood of -103.38 nats, which is
slightly worse than the log-likelihood of -100.46 nats achieved by a restricted id82
with 500 hidden units trained with persistent contrastive divergence (burda et al., 2015). rbms
trained with centering or fang methods achieve a similar performance of around -100 nats (grosse
& salakhudinov, 2015). the training log-likelihood for the models we trained was 2.39 to 2.65 nats
higher than the test log-likelihood.

5.2 latent space representation

we have observed that both vaes and iwaes tend to learn latent representations with effective
dimensions far below their capacity. our next set of experiments aimed to quantify this effect and
determine whether the iwae objective ameliorates this effect.
if a latent dimension encodes useful information about the data, we would expect its distribution
to change depending on the observations. based on this intuition, we measured activity of a latent
dimension u using the statistic au = covx
if au > 10   2. we have observed two pieces of evidence that this criterion is both well-de   ned and
meaningful:

(cid:0)eu   q(u|x)[u](cid:1). we de   ned the dimension u to be active

1. the distribution of au for a trained model consisted of two widely separated modes, as

shown in appendix c.

7

under review as a conference paper at iclr 2016

experiment 1
experiment 2

trained as

vae

iwae, k = 50

first stage
nll

active units

trained as

nll

active units

second stage

86.76
84.78

19
25

iwae, k = 50

vae

84.88
86.02

22
23

table 2: results of continuing to train a vae model with the iwae objective, and vice versa. training the
vae with the iwae objective increased the latent dimension and test log-likelihood, while training the iwae
with the vae objective had the opposite effect.

2. to con   rm that the inactive dimensions were indeed insigni   cant to the predictions, we
evaluated all models with the inactive dimensions removed. in all cases, this changed the
test log-likelihood by less than 0.06 nats.

in table 1, we report the numbers of active units for all conditions. in all conditions, the number of
active dimensions was far less than the total number of dimensions. adding more latent dimensions
did not increase the number of active dimensions. interestingly, in the two-layer models, the second
layer used very little of its modeling capacity: the number of active dimensions was always less
than 10. in all cases with k > 1, the iwae learned more latent dimensions than the vae. since this
coincided with higher log-likelihood values, we speculate that a larger number of active dimensions
re   ects a richer latent representation.
super   cially, the phenomenon of inactive dimensions appears similar to the problem of    units dying
out    in neural networks and latent variable models, an effect which is often ascribed to dif   culties
in optimization. for example, if a unit is inactive, it may never receive a meaningful gradient signal
because of a plateau in the optimization landscape. in such cases, the problem may be avoided
through a better initialization. to determine whether the inactive units resulted from an optimization
issue or a modeling issue, we took the best-performing vae and iwae models from table 1, and
continued training the vae model using the iwae objective and vice versa. in both cases, the model
was trained for an additional 37 passes over the data with a learning rate of 10   4.
the results are shown in table 2. we found that continuing to train the vae with the iwae objective
increased the number of active dimensions and the test log-likelihood, while continuing to train
the iwae with the vae objective did the opposite. the fact that training with the vae objective
actively reduces both the number of active dimensions and the log-likelihood strongly suggests that
inactivation of the latent dimensions is driven by the objective functions rather than by optimization
issues. on the other hand, optimization also appears to play a role, as the results in table 2 are not
quite identical to those in table 1.

6 conclusion

in this paper, we presented the importance weighted autoencoder, a variant on the vae trained by
maximizing a tighter log-likelihood lower bound derived from importance weighting. we showed
empirically that iwaes learn richer latent representations and achieve better generative performance
than vaes with equivalent architectures and training time. we believe this method may improve the
   exibility of other generative models currently trained with the vae objective.

7 acknowledgements

this research was supported by nserc, the fields institute, and samsung.

references
ba, j. l., mnih, v., and kavukcuoglu, k. multiple object recognition with visual attention. in international

conference on learning representations, 2015.

bornschein, j. and bengio, y. reweighted wake-sleep. international conference on learning representations,

2015.

burda, y., grosse, r. b., and salakhutdinov, r. accurate and conservative estimates of mrf log-likelihood

using reverse annealing. arti   cial intelligence and statistics, pp. 102   110, 2015.

8

under review as a conference paper at iclr 2016

dayan, p., hinton, g. e., neal, r. m., and zemel, r. s. the helmholtz machine. neural computation, 7:

889   904, 1995.

dziugaite, k. g., roy, d. m., and ghahramani, z. training generative neural networks via maximum mean

discrepancy optimization. in uncertainty in arti   cial intelligence, 2015.

glorot, x. and bengio, y. understanding the dif   culty of training deep feedforward neural networks.

arti   cial intelligence and statistics, pp. 249   256, 2010.

in

gogate, v., bidyuk, b., and dechter, r. studies in lower bounding id203 of evidence using the markov

inequality. in uncertainty in arti   cial intelligence, 2007.

goodfellow, i., pouget-abadie, j., mirza, m., xu, b., warde-farley, d., ozair, s., courville, a., and bengio,

y. generative adversarial nets. in neural information processing systems, 2014.

gregor, k., danihelka, i., mnih, a., blundell, c., and wierstra, d. deep autoregressive networks. international

conference on machine learning, 2014.

gregor, k., danihelka, i., graves, a., rezende, d. j., and wierstra, d. draw: a recurrent neural network for

image generation. in international conference on machine learning, pp. 1462   1471, 2015.

grosse, r. and salakhudinov, r. scaling up natural gradient by sparsely factorizing the inverse    sher matrix.

in international conference on machine learning, 2015.

hinton, g. e., osindero, s., and teh, y. a fast learning algorithm for deep belief nets. neural computation,

2006.

kingma, d. and ba, j. l. adam: a method for stochastic optimization.

learning representations, 2015.

in international conference on

kingma, d. p. and welling, m. auto-encoding id58.

representations, 2014.

international conference on learning

kingma, d. p., mohamed, s., rezende, d. j., and welling, m. semi-supervised learning with deep generative

models. in neural information processing systems, 2014.

kulkarni, t. d., whitney, w., kohli, p., and tenenbaum, j. b. deep convolutional inverse graphics network.

arxiv:1503.03167, 2015.

lake, b. m., salakhutdinov, r., and tenenbaum, j. b. id62 by inverting a compositional causal

process. in neural information processing systems, 2013.

larochelle, h., murray i. the neural autoregressive distribution estimator. in arti   cial intelligence and statis-

tics, pp. 29   37, 2011.

lecun, y., bottou, l., bengio, y., and haffner, p. gradient-based learning applied to document recognition.

proceedings of the ieee, 86(11):2278   2324, 1998.

li, y., swersky, k., and zemel, r. generative moment matching networks. in international conference on

machine learning, pp. 1718   1727, 2015.

mnih, a. and gregor, k. neural variational id136 and learning in belief networks. in international confer-

ence on machine learning, pp. 1791   1799, 2014.

murray, i. and salakhutdinov, r. evaluating probabilities under high-dimensional latent variable models. in

neural information processing systems, pp. 1137   1144, 2009.

neal, r. m. connectionist learning of belief networks. arti   cial intelligence, 1992.

rezende, d. j. and mohamed, s. variational id136 with normalizing    ows. in international conference on

machine learning, pp. 1530   1538, 2015.

rezende, d. j., mohamed, s., and wierstra, d. stochastic id26 and approximate id136 in deep

generative models. international conference on machine learning, pp. 1278   1286, 2014.

salakhutdinov, r. and e., hinton g. deep id82s. in neural information processing systems,

2009.

salakhutdinov, r. and larochelle, h. ef   cient learning of deep id82s. in arti   cial intelligence

and statistics, 2010.

9

under review as a conference paper at iclr 2016

salakhutdinov, r. and murray, i. on the quantitative analysis of id50. in international con-

ference on machine learning, 2008.

salimans, t., kingma, d. p., and welling, m. id115 and variational id136: bridging

the gap. in international conference on machine learning, pp. 1218   1226, 2015.

smolensky, p. information processing in dynamical systems: foundations of harmony theory. in rumelhart,
d. e. and mcclelland, j. l. (eds.), parallel distributed processing: explorations in the microstructure of
cognition. mit press, 1986.

tang, y. and salakhutdinov, r. learning stochastic feedforward neural networks.

processing systems, 2013.

in neural information

williams, r. j. simple statistical gradient-following algorithms for connectionist id23. ma-

chine learning, 8:229   256, 1992.

appendix a
proof of theorem 1. we need to show the following facts about the log-likelihood lower bound lk:

1. log p(x)     lk,
2. lk     lm for k     m,
3. log p(x) = limk       lk, assuming p(h, x)/q(h|x) is bounded.

we prove each in turn:

1. it follows from jensen   s inequality that

(cid:34)

k(cid:88)

i=1

lk = e

log

1
k

p(x, hi)
q(hi|x)

(cid:35)

    log e

(cid:34)

k(cid:88)

i=1

1
k

(cid:35)

p(x, hi)
q(hi|x)

2. let i     {1, . . . , k} with |i| = m be a uniformly distributed subset of distinct indices from
=

{1, . . . , k}. we will use the following simple observation: ei={i1,...,im}

m

a1+...+ak

k

for any sequence of numbers a1, . . . , ak.

using this observation and jensen   s inequality, we get

= log p(x)

(16)

(cid:104) ai1 +...+aim

(cid:105)

lk = eh1,...,hk

= eh1,...,hk

    eh1,...,hk

= eh1,...,hm

i=1

1
k

log

k(cid:88)

p(x, hi)
q(hi|x)

(cid:35)
(cid:34)
      log ei={i1,...,im}
       1
      log
      ei={i1,...,im}
(cid:35)
(cid:34)
m(cid:88)
(cid:80)k

p(x, hi)
q(hi|x)

1
m

p(x,hi)

1
m

log

i=1

m

            
            

m(cid:88)
m(cid:88)

j=1

j=1

p(x, hij )
q(hij|x)

p(x, hij )
q(hij|x)

= lm

(17)

(18)

(19)

(20)

3. consider the random variable mk = 1
k

i=1

it follows from the strong law of large numbers that mk converges to eq(hi|x)
p(x) almost surely. hence lk = e log[mk] converges to log p(x) as k        .

(cid:105)
(cid:104) p(x,hi)
q(hi|x) . if p(h, x)/q(h|x) is bounded, then

q(hi|x)

=

10

under review as a conference paper at iclr 2016

appendix b

it is well known that the variance of an unnormalized importance sampling based estimator can
be extremely large, or even in   nite, if the proposal distribution is not well matched to the target
distribution. here we argue that the monte carlo estimator of lk, described in section 3, does not
suffer from large variance. more precisely, we bound the mean absolute deviation (mad). while
this does not directly bound the variance, it would be surprising if an estimator had small mad yet
extremely large variance.
suppose we have a strictly positive unbiased estimator   z of a positive quantity z, and we wish to
use log   z as an estimator of log z. by jensen   s inequality, this is a biased estimator, i.e. e[log   z]    
log z. denote the bias as    = log z     e[log   z]. we start with the observation that log   z is unlikely
to overestimate log z by very much, as can be shown with markov   s inequality:

pr(log   z > log z + b)     e   b.

let (x)+ denote max(x, 0). we now use the above facts to bound the mad:

e(cid:104)(cid:12)(cid:12)(cid:12)log   z     e[log   z]

(cid:12)(cid:12)(cid:12)(cid:105)

= 2e

log   z     e[log   z]

(cid:20)(cid:16)
(cid:20)(cid:16)
(cid:20)(cid:16)
(cid:20)(cid:16)
(cid:90)    
(cid:90)    

0

(cid:17)

+

(cid:21)

(cid:16)

(cid:17)
(cid:17)

+

(cid:21)

+

+

+ 2  

(cid:17)

= 2e

log   z     log z + log z     e[log   z]

    2e

log   z     log z

log z     e[log   z]

= 2e

log   z     log z

(cid:16)

= 2

    2

e   tdt + 2  

log   z     log z > t

pr

dt + 2  

(cid:21)

(cid:17)

+

(cid:17)

(cid:21)

+

(21)

(22)

(23)

(24)

(25)

(26)

(27)

here, (22) is a general formula for the mad, (26) uses the formula e[y ] =(cid:82)    

(28)
0 pr(y > t) dt for
a nonnegative random variable y , and (27) applies the bound (21). hence, the mad is bounded by
2 + 2  . in the context of iwae,    corresponds to the gap between lk and log p(x).

= 2 + 2  

0

appendix c

network architectures

here is a summary of the network architectures used in the experiments:
q(h1|x) = n (h1|  q,1, diag(  q,1))

lin+tanh

lin+tanh

200d

x

200d

lin

lin+exp

q(h2|h1) = n (h2|  q,2, diag(  q,2))

p(h1|h2) = n (h1|  p,1, diag(  p,1))

h1

h2

lin+tanh

100d

lin+tanh

100d

lin

lin+exp

lin+tanh

100d

lin+tanh

100d

lin

lin+exp

  q,1
  q,1

  q,2
  q,2

  p,1
  p,1

p(x|h1) = bernoulli(x|  p,0)

lin+tanh

h1

200d

lin+tanh

200d

lin+sigm

  p,0

11

under review as a conference paper at iclr 2016

distribution of activity statistic

in section 5.2, we de   ned the activity statistic au = covx
of 10   2 for determining if a unit is active. one justi   cation for this is that the distribution of this
statistic consisted of two widely separated modes in every case we looked at. here is the histogram
of log au for a vae with one stochastic layer:

(cid:0)eu   q(u|x)[u](cid:1), and chose a threshold

visualization of posterior distributions

we show some examples of true and approximate posteriors for vae and iwae models trained with
two latent dimensions. heat maps show true posterior distributions for 6 training examples, and the
pictures in the bottom row show the examples and their reconstruction from samples from q(h|x).
left: vae. middle: iwae, with k = 5. right: iwae, with k = 50. the iwae prefers less regular
posteriors and more spread out posterior predictions.

12

8765432101log variance of   024681012number of unitsunder review as a conference paper at iclr 2016

appendix d

results for a fixed mnist binarization

several previous works have used a    xed binarization of the mnist dataset de   ned by larochelle
(2011). we repeated our experiments training the models on the 50000 examples from the training
dataset, and evaluating them on the 10000 examples from the test dataset. otherwise we used the
same training procedure and hyperparameters as in the experiments in the main part of the paper.
the results in table 3 indicate that the conclusions about the relative merits of vaes and iwaes are
unchanged in the new experimental setup. in this setup we noticed signi   cantly larger amounts of
over   tting.

vae

iwae

# stoch.
layers

1

2

k

1
5
50
1
5
50

nll

88.71
88.83
89.05
88.08
87.63
87.86

active
units

19
19
20
16+5
17+5
17+6

nll

88.71
87.63
87.10
88.08
86.17
85.32

active
units

19
22
24
16+5
21+5
24+7

table 3: results on density estimation and the number of active latent dimensions on the    xed binarization
mnist dataset. for models with two latent layers,    k1 + k2    denotes k1 active units in the    rst layer and k2
in the second layer. the generative performance of iwaes improved with increasing k, while that of vaes
bene   tted only slightly. two-layer models achieved better generative performance than one-layer models.

appendix e

samples

13

under review as a conference paper at iclr 2016

table 4: random samples from vae (left column) and iwae with k = 50 (right column) models. row 1:
models with one stochastic layer. row 2: models with two stochastic layers. samples are represented as the
means of the corresponding bernoulli distributions.

14

