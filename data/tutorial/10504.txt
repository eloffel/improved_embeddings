learning phrase representations using id56 encoder   decoder

for id151

kyunghyun cho

bart van merri  enboer caglar gulcehre

universit  e de montr  eal

dzmitry bahdanau

jacobs university, germany

4
1
0
2

 

p
e
s
3

 

 
 
]
l
c
.
s
c
[
 
 

3
v
8
7
0
1

.

6
0
4
1
:
v
i
x
r
a

firstname.lastname@umontreal.ca

d.bahdanau@jacobs-university.de

fethi bougares holger schwenk

universit  e du maine, france

yoshua bengio

universit  e de montr  eal, cifar senior fellow

firstname.lastname@lium.univ-lemans.fr

find.me@on.the.web

abstract

in this paper, we propose a novel neu-
ral network model called id56 encoder   
decoder that consists of two recurrent
neural networks (id56). one id56 en-
codes a sequence of symbols into a    xed-
length vector representation, and the other
decodes the representation into another se-
quence of symbols. the encoder and de-
coder of the proposed model are jointly
trained to maximize the conditional prob-
ability of a target sequence given a source
sequence. the performance of a statisti-
cal machine translation system is empiri-
cally found to improve by using the con-
ditional probabilities of phrase pairs com-
puted by the id56 encoder   decoder as an
additional feature in the existing log-linear
model. qualitatively, we show that the
proposed model learns a semantically and
syntactically meaningful representation of
linguistic phrases.

introduction

1
deep neural networks have shown great success in
various applications such as objection recognition
(see, e.g., (krizhevsky et al., 2012)) and speech
recognition (see, e.g., (dahl et al., 2012)). fur-
thermore, many recent works showed that neu-
ral networks can be successfully used in a num-
ber of tasks in natural language processing (nlp).
these include, but are not limited to, language
modeling (bengio et al., 2003), paraphrase detec-
tion (socher et al., 2011) and id27 ex-
traction (mikolov et al., 2013). in the    eld of sta-
tistical machine translation (smt), deep neural
networks have begun to show promising results.
(schwenk, 2012) summarizes a successful usage
of feedforward neural networks in the framework
of phrase-based smt system.

along this line of research on using neural net-
works for smt, this paper focuses on a novel neu-
ral network architecture that can be used as a part
of the conventional phrase-based smt system.
the proposed neural network architecture, which
we will refer to as an id56 encoder   decoder, con-
sists of two recurrent neural networks (id56) that
act as an encoder and a decoder pair. the en-
coder maps a variable-length source sequence to a
   xed-length vector, and the decoder maps the vec-
tor representation back to a variable-length target
sequence. the two networks are trained jointly to
maximize the id155 of the target
sequence given a source sequence. additionally,
we propose to use a rather sophisticated hidden
unit in order to improve both the memory capacity
and the ease of training.

the proposed id56 encoder   decoder with a
novel hidden unit is empirically evaluated on the
task of translating from english to french. we
train the model to learn the translation probabil-
ity of an english phrase to a corresponding french
phrase. the model is then used as a part of a stan-
dard phrase-based smt system by scoring each
phrase pair in the phrase table. the empirical eval-
uation reveals that this approach of scoring phrase
pairs with an id56 encoder   decoder improves
the translation performance.

we qualitatively analyze the trained id56
encoder   decoder by comparing its phrase scores
with those given by the existing translation model.
the qualitative analysis shows that
the id56
encoder   decoder is better at capturing the lin-
guistic regularities in the phrase table, indirectly
explaining the quantitative improvements in the
overall translation performance. the further anal-
ysis of the model reveals that the id56 encoder   
decoder learns a continuous space representation
of a phrase that preserves both the semantic and
syntactic structure of the phrase.

2 id56 encoder   decoder
2.1 preliminary: recurrent neural networks
a recurrent neural network (id56) is a neural net-
work that consists of a hidden state h and an
optional output y which operates on a variable-
length sequence x = (x1, . . . , xt ). at each time
step t, the hidden state h(cid:104)t(cid:105) of the id56 is updated
by

h(cid:104)t(cid:105) = f(cid:0)h(cid:104)t   1(cid:105), xt

(cid:1) ,

(1)

is

a non-linear

activation func-
where f
tion.
f may be as simple as an element-
wise logistic sigmoid function and as com-
plex as a long short-term memory (lstm)
unit (hochreiter and schmidhuber, 1997).

an id56 can learn a id203 distribution
over a sequence by being trained to predict the
next symbol in a sequence. in that case, the output
at each timestep t is the conditional distribution
p(xt | xt   1, . . . , x1). for example, a multinomial
distribution (1-of-k coding) can be output using a
softmax activation function

p(xt,j = 1 | xt   1, . . . , x1) =

exp(cid:0)wjh(cid:104)t(cid:105)(cid:1)
j(cid:48)=1 exp(cid:0)wj(cid:48)h(cid:104)t(cid:105)(cid:1) ,
(cid:80)k

(2)

t(cid:89)

for all possible symbols j = 1, . . . , k, where wj
are the rows of a weight matrix w. by combining
these probabilities, we can compute the probabil-
ity of the sequence x using

p(x) =

p(xt | xt   1, . . . , x1).

(3)

t=1

from this learned distribution, it is straightfor-
ward to sample a new sequence by iteratively sam-
pling a symbol at each time step.

2.2 id56 encoder   decoder
in this paper, we propose a novel neural network
architecture that learns to encode a variable-length
sequence into a    xed-length vector representation
and to decode a given    xed-length vector rep-
resentation back into a variable-length sequence.
from a probabilistic perspective, this new model
is a general method to learn the conditional dis-
tribution over a variable-length sequence condi-
tioned on yet another variable-length sequence,
| x1, . . . , xt ), where one
e.g.

p(y1, . . . , yt (cid:48)

figure 1: an illustration of the proposed id56
encoder   decoder.

should note that the input and output sequence
lengths t and t (cid:48) may differ.

the encoder is an id56 that reads each symbol
of an input sequence x sequentially. as it reads
each symbol, the hidden state of the id56 changes
according to eq. (1). after reading the end of
the sequence (marked by an end-of-sequence sym-
bol), the hidden state of the id56 is a summary c
of the whole input sequence.

the decoder of the proposed model is another
id56 which is trained to generate the output se-
quence by predicting the next symbol yt given the
hidden state h(cid:104)t(cid:105). however, unlike the id56 de-
scribed in sec. 2.1, both yt and h(cid:104)t(cid:105) are also con-
ditioned on yt   1 and on the summary c of the input
sequence. hence, the hidden state of the decoder
at time t is computed by,

h(cid:104)t(cid:105) = f(cid:0)h(cid:104)t   1(cid:105), yt   1, c(cid:1) ,

and similarly, the conditional distribution of the
next symbol is

p (yt|yt   1, yt   2, . . . , y1, c) = g(cid:0)h(cid:104)t(cid:105), yt   1, c(cid:1) .

for given id180 f and g (the latter
must produce valid probabilities, e.g. with a soft-
max).

see fig. 1 for a graphical depiction of the pro-

posed model architecture.

the two components of the proposed id56
encoder   decoder are jointly trained to maximize
the conditional log-likelihood

n(cid:88)

n=1

max

  

1
n

log p  (yn | xn),

(4)

x1x2xtyt'y2y1cdecoderencoderwhere    is the set of the model parameters and
each (xn, yn) is an (input sequence, output se-
quence) pair from the training set.
in our case,
as the output of the decoder, starting from the in-
put, is differentiable, we can use a gradient-based
algorithm to estimate the model parameters.

once the id56 encoder   decoder is trained, the
model can be used in two ways. one way is to use
the model to generate a target sequence given an
input sequence. on the other hand, the model can
be used to score a given pair of input and output
sequences, where the score is simply a id203
p  (y | x) from eqs. (3) and (4).
2.3 hidden unit that adaptively remembers

and forgets

in addition to a novel model architecture, we also
propose a new type of hidden unit (f in eq. (1))
that has been motivated by the lstm unit but is
much simpler to compute and implement.1 fig. 2
shows the graphical depiction of the proposed hid-
den unit.

let us describe how the activation of the j-th
hidden unit is computed. first, the reset gate rj is
computed by

(cid:16)

[wrx]j +(cid:2)urh(cid:104)t   1(cid:105)(cid:3)

(cid:17)

j

,

(5)

rj =   

where    is the logistic sigmoid function, and [.]j
denotes the j-th element of a vector. x and ht   1
are the input and the previous hidden state, respec-
tively. wr and ur are weight matrices which are
learned.

similarly, the update gate zj is computed by

(cid:16)

[wzx]j +(cid:2)uzh(cid:104)t   1(cid:105)(cid:3)

(cid:17)

j

.

(6)

zj =   

the actual activation of the proposed unit hj is

then computed by

(cid:104)t   1(cid:105)
(cid:104)t(cid:105)
j = zjh
j

h

+ (1     zj)  h

(cid:104)t(cid:105)
j ,

[wx]j +(cid:2)u(cid:0)r (cid:12) h(cid:104)t   1(cid:105)(cid:1)(cid:3)

(cid:16)

where
(cid:104)t(cid:105)
  h
j =   

(7)

.

(8)

(cid:17)

j

in this formulation, when the reset gate is close
to 0, the hidden state is forced to ignore the pre-
vious hidden state and reset with the current input
1 the lstm unit, which has shown impressive results in
several applications such as id103, has a mem-
ory cell and four gating units that adaptively control the in-
formation    ow inside the unit, compared to only two gating
units in the proposed hidden unit. for details on lstm net-
works, see, e.g., (graves, 2012).

figure 2: an illustration of the proposed hidden
activation function. the update gate z selects
whether the hidden state is to be updated with
a new hidden state   h. the reset gate r decides
whether the previous hidden state is ignored. see
eqs. (5)   (8) for the detailed equations of r, z, h
and   h.

only. this effectively allows the hidden state to
drop any information that is found to be irrelevant
later in the future, thus, allowing a more compact
representation.

on the other hand, the update gate controls how
much information from the previous hidden state
will carry over to the current hidden state. this
acts similarly to the memory cell in the lstm
network and helps the id56 to remember long-
term information. furthermore, this may be con-
sidered an adaptive variant of a leaky-integration
unit (bengio et al., 2013).

as each hidden unit has separate reset and up-
date gates, each hidden unit will learn to capture
dependencies over different time scales. those
units that learn to capture short-term dependencies
will tend to have reset gates that are frequently ac-
tive, but those that capture longer-term dependen-
cies will have update gates that are mostly active.
in our preliminary experiments, we found that
it is crucial to use this new unit with gating units.
we were not able to get meaningful result with an
oft-used tanh unit without any gating.

3 id151

in a commonly used id151
system (smt), the goal of the system (decoder,
speci   cally) is to    nd a translation f given a source
sentence e, which maximizes

p(f | e)     p(e | f )p(f ),

where the    rst term at the right hand side is called
translation model and the latter language model
(see, e.g., (koehn, 2005)).
in practice, however,
most smt systems model log p(f | e) as a log-
linear model with additional features and corre-

zrhh~xsponding weights:

n(cid:88)

log p(f | e) =

wnfn(f , e) + log z(e),

(9)

n=1

where fn and wn are the n-th feature and weight,
respectively. z(e) is a id172 constant that
does not depend on the weights. the weights are
often optimized to maximize the id7 score on a
development set.

in

smt

(koehn et al., 2003)

phrase-based
in

the
framework
introduced
and
(marcu and wong, 2002),
the translation model
log p(e | f ) is factorized into the translation
probabilities of matching phrases in the source
and target sentences.2 these probabilities are
once again considered additional features in the
log-linear model (see eq. (9)) and are weighted
accordingly to maximize the id7 score.

since the neural net language model was pro-
posed in (bengio et al., 2003), neural networks
have been used widely in smt systems.
in
many cases, neural networks have been used to
rescore translation hypotheses (n-best lists) (see,
e.g., (schwenk et al., 2006)). recently, however,
there has been interest in training neural networks
to score the translated sentence (or phrase pairs)
using a representation of the source sentence as
an additional input. see, e.g., (schwenk, 2012),
(son et al., 2012) and (zou et al., 2013).

3.1 scoring phrase pairs with id56

encoder   decoder

here we propose to train the id56 encoder   
decoder (see sec. 2.2) on a table of phrase pairs
and use its scores as additional features in the log-
linear model in eq. (9) when tuning the smt de-
coder.

when we train the id56 encoder   decoder, we
ignore the (normalized) frequencies of each phrase
pair in the original corpora. this measure was
taken in order (1) to reduce the computational ex-
pense of randomly selecting phrase pairs from a
large phrase table according to the normalized fre-
quencies and (2) to ensure that the id56 encoder   
decoder does not simply learn to rank the phrase
pairs according to their numbers of occurrences.
one underlying reason for this choice was that the
existing translation id203 in the phrase ta-
ble already re   ects the frequencies of the phrase

2 without loss of generality, from here on, we refer to

p(e | f ) for each phrase pair as a translation model as well

pairs in the original corpus. with a    xed capacity
of the id56 encoder   decoder, we try to ensure
that most of the capacity of the model is focused
toward learning linguistic regularities, i.e., distin-
guishing between plausible and implausible trans-
lations, or learning the    manifold    (region of prob-
ability concentration) of plausible translations.

once the id56 encoder   decoder is trained, we
add a new score for each phrase pair to the exist-
ing phrase table. this allows the new scores to en-
ter into the existing tuning algorithm with minimal
additional overhead in computation.

as schwenk pointed out in (schwenk, 2012),
it is possible to completely replace the existing
phrase table with the proposed id56 encoder   
decoder. in that case, for a given source phrase,
the id56 encoder   decoder will need to generate
a list of (good) target phrases. this requires, how-
ever, an expensive sampling procedure to be per-
formed repeatedly.
in this paper, thus, we only
consider rescoring the phrase pairs in the phrase
table.

3.2 related approaches: neural networks in

machine translation

before presenting the empirical results, we discuss
a number of recent works that have proposed to
use neural networks in the context of smt.

schwenk in (schwenk, 2012) proposed a simi-
lar approach of scoring phrase pairs. instead of the
id56-based neural network, he used a feedforward
neural network that has    xed-size inputs (7 words
in his case, with zero-padding for shorter phrases)
and    xed-size outputs (7 words in the target lan-
guage). when it is used speci   cally for scoring
phrases for the smt system, the maximum phrase
length is often chosen to be small. however, as the
length of phrases increases or as we apply neural
networks to other variable-length sequence data,
it is important that the neural network can han-
dle variable-length input and output. the pro-
posed id56 encoder   decoder is well-suited for
these applications.

similar

to (schwenk, 2012), devlin et al.
(devlin et al., 2014) proposed to use a feedfor-
ward neural network to model a translation model,
however, by predicting one word in a target phrase
at a time. they reported an impressive improve-
ment, but their approach still requires the maxi-
mum length of the input phrase (or context words)
to be    xed a priori.

although it is not exactly a neural network they
train, the authors of (zou et al., 2013) proposed
to learn a bilingual embedding of words/phrases.
they use the learned embedding to compute the
distance between a pair of phrases which is used
as an additional score of the phrase pair in an smt
system.

in (chandar et al., 2014), a feedforward neural
network was trained to learn a mapping from a
bag-of-words representation of an input phrase to
an output phrase. this is closely related to both the
proposed id56 encoder   decoder and the model
proposed in (schwenk, 2012), except that their in-
put representation of a phrase is a bag-of-words.
a similar approach of using bag-of-words repre-
sentations was proposed in (gao et al., 2013) as
well. earlier, a similar encoder   decoder model us-
ing two id56s was proposed
in (socher et al., 2011), but their model was re-
stricted to a monolingual setting, i.e. the model
reconstructs an input sentence. more recently, an-
other encoder   decoder model using an id56 was
proposed in (auli et al., 2013), where the de-
coder is conditioned on a representation of either
a source sentence or a source context.

one important difference between the pro-
posed id56 encoder   decoder and the approaches
in (zou et al., 2013) and (chandar et al., 2014) is
that the order of the words in source and tar-
get phrases is taken into account. the id56
encoder   decoder naturally distinguishes between
sequences that have the same words but in a differ-
ent order, whereas the aforementioned approaches
effectively ignore order information.

the closest approach related to the proposed
id56 encoder   decoder is the recurrent contin-
uous translation model (model 2) proposed in
(kalchbrenner and blunsom, 2013).
in their pa-
per, they proposed a similar model that consists
of an encoder and decoder. the difference with
our model is that they used a convolutional id165
model (cgm) for the encoder and the hybrid of
an inverse cgm and a recurrent neural network
for the decoder. they, however, evaluated their
model on rescoring the n-best list proposed by the
conventional smt system and computing the per-
plexity of the gold standard translations.

4 experiments

we evaluate our approach on the english/french
translation task of the wmt   14 workshop.

4.1 data and baseline system
large amounts of resources are available to build
an english/french smt system in the framework
of the wmt   14 translation task. the bilingual
corpora include europarl (61m words), news com-
mentary (5.5m), un (421m), and two crawled
corpora of 90m and 780m words respectively.
the last two corpora are quite noisy. to train
the french language model, about 712m words of
crawled newspaper material is available in addi-
tion to the target side of the bitexts. all the word
counts refer to french words after id121.

it is commonly acknowledged that training sta-
tistical models on the concatenation of all this
data does not necessarily lead to optimal per-
formance, and results in extremely large mod-
els which are dif   cult to handle.
instead, one
should focus on the most relevant subset of the
data for a given task. we have done so by
applying the data selection method proposed in
(moore and lewis, 2010), and its extension to bi-
texts (axelrod et al., 2011). by these means we
selected a subset of 418m words out of more
than 2g words for id38 and a
subset of 348m out of 850m words for train-
ing the id56 encoder   decoder. we used the
test set newstest2012 and 2013 for data
selection and weight
tuning with mert, and
newstest2014 as our test set. each set has
more than 70 thousand words and a single refer-
ence translation.

for training the neural networks, including the
proposed id56 encoder   decoder, we limited the
source and target vocabulary to the most frequent
15,000 words for both english and french. this
covers approximately 93% of the dataset. all the
out-of-vocabulary words were mapped to a special
token ([unk]).

the baseline phrase-based smt system was
built using moses with default settings. this sys-
tem achieves a id7 score of 30.64 and 33.3 on
the development and test sets, respectively (see ta-
ble 1).

4.1.1 id56 encoder   decoder
the id56 encoder   decoder used in the experi-
ment had 1000 hidden units with the proposed
gates at the encoder and at the decoder. the in-
put matrix between each input symbol x(cid:104)t(cid:105) and the
hidden unit is approximated with two lower-rank
matrices, and the output matrix is approximated

id7

models

dev
30.64
baseline
31.20
id56
31.48
cslm + id56
cslm + id56 + wp 31.50

test
33.30
33.87
34.64
34.54

table 1: id7 scores computed on the develop-
ment and test sets using different combinations of
approaches. wp denotes a word penalty, where
we penalizes the number of unknown words to
neural networks.

similarly. we used rank-100 matrices, equivalent
to learning an embedding of dimension 100 for
each word. the activation function used for   h in
eq. (8) is a hyperbolic tangent function. the com-
putation from the hidden state in the decoder to
the output is implemented as a deep neural net-
work (pascanu et al., 2014) with a single interme-
diate layer having 500 maxout units each pooling
2 inputs (goodfellow et al., 2013).

all the weight parameters in the id56 encoder   
decoder were initialized by sampling from an
isotropic zero-mean (white) gaussian distribution
with its standard deviation    xed to 0.01, except
for the recurrent weight parameters. for the re-
current weight matrices, we    rst sampled from a
white gaussian distribution and used its left singu-
lar vectors matrix, following (saxe et al., 2014).

we used adadelta and stochastic gradient
descent
to train the id56 encoder   decoder
with hyperparameters   = 10   6 and    =
0.95 (zeiler, 2012). at each update, we used 64
randomly selected phrase pairs from a phrase ta-
ble (which was created from 348m words). the
model was trained for approximately three days.

details of the architecture used in the experi-
ments are explained in more depth in the supple-
mentary material.

4.1.2 neural language model
in order to assess the effectiveness of scoring
phrase pairs with the proposed id56 encoder   
decoder, we also tried a more traditional approach
of using a neural network for learning a target
language model (cslm) (schwenk, 2007). espe-
cially, the comparison between the smt system
using cslm and that using the proposed approach
of phrase scoring by id56 encoder   decoder will
clarify whether the contributions from multiple
neural networks in different parts of the smt sys-

tem add up or are redundant.

we trained the cslm model on 7-grams
from the target corpus.
each input word
was projected into the embedding space r512,
and they were concatenated to form a 3072-
dimensional vector. the concatenated vector was
fed through two recti   ed layers (of size 1536 and
1024) (glorot et al., 2011). the output layer was
a simple softmax layer (see eq. (2)). all the
weight parameters were initialized uniformly be-
tween    0.01 and 0.01, and the model was trained
until the validation perplexity did not improve for
10 epochs. after training, the language model
achieved a perplexity of 45.80. the validation set
was a random selection of 0.1% of the corpus. the
model was used to score partial translations dur-
ing the decoding process, which generally leads to
higher gains in id7 score than n-best list rescor-
ing (vaswani et al., 2013).

to address the computational complexity of
using a cslm in the decoder a buffer was
used to aggregate id165s during the stack-
search performed by the decoder. only when
is full, or a stack is about
the buffer
to
be pruned,
the id165s are scored by the
cslm. this allows us to perform fast matrix-
id127 on gpu using theano
(bergstra et al., 2010; bastien et al., 2012).

figure 3: the visualization of phrase pairs accord-
ing to their scores (log-probabilities) by the id56
encoder   decoder and the translation model.

4.2 quantitative analysis
we tried the following combinations:

1. baseline con   guration
2. baseline + id56
3. baseline + cslm + id56
4. baseline + cslm + id56 + word penalty

   60   50   40   30   20   100   14   12   10   8   6   4   20id56 scores (log)tm scores (log)source
at the end of the

for the    rst time

in the united states
and
, as well as
one of the most

source
, minister of commu-
nications and trans-
port
did not comply with
the

parts of the world .

the past few days .

on friday and satur-
day

translation model
[a la    n de la] [  r la    n des ann  ees] [  etre sup-
prim  es `a la    n de la]
[r c(cid:13) pour la premir  ere fois] [  et  e donn  es pour
la premi`ere fois] [  et  e comm  emor  ee pour la
premi`ere fois]
[? aux ?tats-unis et] [  et  e ouvertes aux   etats-
unis et] [  et  e constat  ees aux   etats-unis et]
[?s , qu   ] [?s , ainsi que] [?re aussi bien que]
[?t ?l    un des plus] [?l    un des plus] [  etre retenue
comme un de ses plus]

(a) long, frequent source phrases

id56 encoder   decoder
[`a la    n du] [`a la    n des] [`a la    n de la]

[pour la premi`ere fois] [pour la premi`ere fois ,]
[pour la premi`ere fois que]

[aux etats-unis et] [des etats-unis et] [des
  etats-unis et]
[, ainsi qu   ] [, ainsi que] [, ainsi que les]
[l    un des] [le] [un des]

translation model
[secr  etaire aux communications et aux trans-
ports :] [secr  etaire aux communications et aux
transports]
[vestimentaire , ne correspondaient pas `a des]
[susmentionn  ee n   
  etait pas conforme aux]
[pr  esent  ees n      etaient pas conformes `a la]
[ c(cid:13) gions du monde .] [r  egions du monde con-
sid  er  ees .] [r  egion du monde consid  er  ee .]
[le petit texte .] [cours des tout derniers jours .]
[les tout derniers jours .]
[vendredi et samedi `a la] [vendredi et samedi `a]
[se d  eroulera vendredi et samedi ,]

(b) long, rare source phrases

id56 encoder   decoder
[secr  etaire aux communications et aux trans-
ports] [secr  etaire aux communications et aux
transports :]
[n    ont pas respect  e les] [n      etait pas conforme
aux] [n    ont pas respect  e la]

[les parties du monde .]

[parties du monde .]
[des parties du monde .]
[ces derniers jours .] [les derniers jours .] [cours
des derniers jours .]
[le vendredi et le samedi] [le vendredi et samedi]
[vendredi et samedi]

table 2: the top scoring target phrases for a small set of source phrases according to the translation
model (direct translation id203) and by the id56 encoder   decoder. source phrases were randomly
selected from phrases with 4 or more words. ? denotes an incomplete (partial) character. r is a cyrillic
letter ghe.

the results are presented in table 1. as ex-
pected, adding features computed by neural net-
works consistently improves the performance over
the baseline performance.

the best performance was achieved when we
used both cslm and the phrase scores from the
id56 encoder   decoder. this suggests that the
contributions of the cslm and the id56 encoder   
decoder are not too correlated and that one can
expect better results by improving each method in-
dependently. furthermore, we tried penalizing the
number of words that are unknown to the neural
networks (i.e. words which are not in the short-
list). we do so by simply adding the number of
unknown words as an additional feature the log-
linear model in eq. (9).3 however, in this case we
3 to understand the effect of the penalty, consider the set
of all words in the 15,000 large shortlist, sl. all words xi /   
sl are replaced by a special token [unk] before being scored
by the neural networks. hence, the id155 of
any xi

t /    sl is actually given by the model as
p (xt = [unk] | x<t) = p (xt /    sl | x<t)
t | x<t
xi

(cid:17)     p

(cid:88)

t | x<t
xj

(cid:16)

(cid:16)

(cid:17)

=

p

,

t /   sl
xj

where x<t is a shorthand notation for xt   1, . . . , x1.

were not able to achieve better performance on the
test set, but only on the development set.

4.3 qualitative analysis
in order to understand where the performance im-
provement comes from, we analyze the phrase pair
scores computed by the id56 encoder   decoder
against the corresponding p(f | e) from the trans-
lation model. since the existing translation model
relies solely on the statistics of the phrase pairs in
the corpus, we expect its scores to be better esti-
mated for the frequent phrases but badly estimated
for rare phrases. also, as we mentioned earlier
in sec. 3.1, we further expect the id56 encoder   
decoder which was trained without any frequency
information to score the phrase pairs based rather
on the linguistic regularities than on the statistics
of their occurrences in the corpus.

we focus on those pairs whose source phrase is
long (more than 3 words per source phrase) and

as a result, the id203 of words not in the shortlist is
always overestimated. it is possible to address this issue by
backing off to an existing model that contain non-shortlisted
words (see (schwenk, 2007)) in this paper, however, we opt
for introducing a word penalty instead, which counteracts the
word id203 overestimation.

source
at the end of the
for the    rst time
in the united states and
, as well as
one of the most

samples from id56 encoder   decoder
[`a la    n de la] (  11)
[pour la premi`ere fois] (  24) [pour la premi`ere fois que] (  2)
[aux   etats-unis et] (  6) [dans les   etats-unis et] (  4)
[, ainsi que] [,] [ainsi que] [, ainsi qu   ] [et unk]
[l    un des plus] (  9) [l    un des] (  5) [l    une des plus] (  2)

(a) long, frequent source phrases

source
, minister of communica-
tions and transport
did not comply with the
parts of the world .
the past few days .
on friday and saturday

samples from id56 encoder   decoder
[ , ministre des communications et le transport] (  13)
[n    tait pas conforme aux] [n    a pas respect l   ] (  2) [n    a pas respect la] (  3)
[arts du monde .] (  11) [des arts du monde .] (  7)
[quelques jours .] (  5) [les derniers jours .] (  5) [ces derniers jours .] (  2)
[vendredi et samedi] (  5) [le vendredi et samedi] (  7) [le vendredi et le samedi] (  4)

(b) long, rare source phrases

table 3: samples generated from the id56 encoder   decoder for each source phrase used in table 2. we
show the top-5 target phrases out of 50 samples. they are sorted by the id56 encoder   decoder scores.

figure 4: 2   d embedding of the learned word representation. the left one shows the full embedding
space, while the right one shows a zoomed-in view of one region (color   coded). for more plots, see the
supplementary material.

frequent. for each such source phrase, we look
at the target phrases that have been scored high
either by the translation id203 p(f | e) or
by the id56 encoder   decoder. similarly, we per-
form the same procedure with those pairs whose
source phrase is long but rare in the corpus.

table 2 lists the top-3 target phrases per source
phrase favored either by the translation model
or by the id56 encoder   decoder. the source
phrases were randomly chosen among long ones
having more than 4 or 5 words.

in most cases, the choices of the target phrases
by the id56 encoder   decoder are closer to ac-
tual or literal translations. we can observe that the
id56 encoder   decoder prefers shorter phrases in
general.

interestingly, many phrase pairs were scored
similarly by both the translation model and the
id56 encoder   decoder, but there were as many

other phrase pairs that were scored radically dif-
ferent (see fig. 3). this could arise from the
proposed approach of training the id56 encoder   
decoder on a set of unique phrase pairs, discour-
aging the id56 encoder   decoder from learning
simply the frequencies of the phrase pairs from the
corpus, as explained earlier.

furthermore, in table 3, we show for each of
the source phrases in table 2, the generated sam-
ples from the id56 encoder   decoder. for each
source phrase, we generated 50 samples and show
the top-   ve phrases accordingly to their scores.
we can see that the id56 encoder   decoder is
able to propose well-formed target phrases with-
out looking at the actual phrase table. importantly,
the generated phrases do not overlap completely
with the target phrases from the phrase table. this
encourages us to further investigate the possibility
of replacing the whole or a part of the phrase table

figure 5: 2   d embedding of the learned phrase representation. the top left one shows the full represen-
tation space (5000 randomly selected points), while the other three    gures show the zoomed-in view of
speci   c regions (color   coded).

with the proposed id56 encoder   decoder in the
future.

4.4 word and phrase representations
since the proposed id56 encoder   decoder is not
speci   cally designed only for the task of machine
translation, here we brie   y look at the properties
of the trained model.

(see,

space

embeddings

language models

it has been known for

some time that
continuous
using
neural networks are able to learn seman-
tically meaningful
e.g.,
(bengio et al., 2003; mikolov et al., 2013)). since
the proposed id56 encoder   decoder also projects
to and maps back from a sequence of words into
a continuous space vector, we expect to see a
similar property with the proposed model as well.
the left plot in fig. 4 shows the 2   d embedding
of the words using the id27 matrix
learned by the id56 encoder   decoder. the pro-
jection was done by the recently proposed barnes-
huid167 (van der maaten, 2013). we can clearly
see that semantically similar words are clustered

with each other (see the zoomed-in plots in fig. 4).
the proposed id56 encoder   decoder naturally
generates a continuous-space representation of a
phrase. the representation (c in fig. 1) in this
case is a 1000-dimensional vector. similarly to the
word representations, we visualize the representa-
tions of the phrases that consists of four or more
words using the barnes-huid167 in fig. 5.

from the visualization, it is clear that the id56
encoder   decoder captures both semantic and syn-
tactic structures of the phrases. for instance, in
the bottom-left plot, most of the phrases are about
the duration of time, while those phrases that are
syntactically similar are clustered together. the
bottom-right plot shows the cluster of phrases that
are semantically similar (countries or regions). on
the other hand, the top-right plot shows the phrases
that are syntactically similar.

5 conclusion

in this paper, we proposed a new neural network
architecture, called an id56 encoder   decoder
that is able to learn the mapping from a sequence

of an arbitrary length to another sequence, possi-
bly from a different set, of an arbitrary length. the
proposed id56 encoder   decoder is able to either
score a pair of sequences (in terms of a conditional
id203) or generate a target sequence given a
source sequence. along with the new architecture,
we proposed a novel hidden unit that includes a re-
set gate and an update gate that adaptively control
how much each hidden unit remembers or forgets
while reading/generating a sequence.

we evaluated the proposed model with the task
of id151, where we used
the id56 encoder   decoder to score each phrase
pair in the phrase table. qualitatively, we were
able to show that the new model is able to cap-
ture linguistic regularities in the phrase pairs well
and also that the id56 encoder   decoder is able to
propose well-formed target phrases.

the scores by the id56 encoder   decoder were
found to improve the overall translation perfor-
mance in terms of id7 scores. also, we
found that the contribution by the id56 encoder   
decoder is rather orthogonal to the existing ap-
proach of using neural networks in the smt sys-
tem, so that we can improve further the perfor-
mance by using, for instance, the id56 encoder   
decoder and the neural net language model to-
gether.

our qualitative analysis of the trained model
shows that it indeed captures the linguistic regu-
larities in multiple levels i.e. at the word level as
well as phrase level. this suggests that there may
be more natural language related applications that
may bene   t from the proposed id56 encoder   
decoder.

the proposed architecture has large potential
for further improvement and analysis. one ap-
proach that was not investigated here is to re-
place the whole, or a part of the phrase table by
letting the id56 encoder   decoder propose target
phrases. also, noting that the proposed model is
not limited to being used with written language,
it will be an important future research to apply the
proposed architecture to other applications such as
speech transcription.

acknowledgments

kc, bm, cg, db and yb would like to thank
nserc, calcul qu  ebec, compute canada, the
canada research chairs and cifar. fb and hs
were partially funded by the european commis-

sion under the project matecat, and by darpa
under the bolt project.

references
[auli et al.2013] michael auli, michel galley, chris
quirk, and geoffrey zweig. 2013. joint language
and translation modeling with recurrent neural net-
in proceedings of the acl conference on
works.
empirical methods in natural language processing
(emnlp), pages 1044   1054.

[axelrod et al.2011] amittai axelrod, xiaodong he,
and jianfeng gao. 2011. id20 via
pseudo in-domain data selection. in proceedings of
the acl conference on empirical methods in natu-
ral language processing (emnlp), pages 355   362.

[bastien et al.2012] fr  ed  eric bastien, pascal lamblin,
razvan pascanu, james bergstra, ian j. goodfellow,
arnaud bergeron, nicolas bouchard, and yoshua
bengio. 2012. theano: new features and speed im-
provements. deep learning and unsupervised fea-
ture learning nips 2012 workshop.

[bengio et al.2003] yoshua bengio, r  ejean ducharme,
pascal vincent, and christian janvin. 2003. a neu-
ral probabilistic language model. j. mach. learn.
res., 3:1137   1155, march.

[bengio et al.2013] y.

n.

bengio,

boulanger-
lewandowski, and r. pascanu. 2013. advances
in proceedings
in optimizing recurrent networks.
of the 38th international conference on acoustics,
speech, and signal processing (icassp 2013),
may.

[bergstra et al.2010] james bergstra, olivier breuleux,
fr  ed  eric bastien, pascal lamblin, razvan pascanu,
guillaume desjardins, joseph turian, david warde-
farley, and yoshua bengio. 2010. theano: a cpu
and gpu math expression compiler. in proceedings
of the python for scienti   c computing conference
(scipy), june. oral presentation.

[chandar et al.2014] sarath chandar, stanislas lauly,
hugo larochelle, mitesh khapra, balaraman ravin-
dran, vikas raykar, and amrita saha. 2014. an au-
toencoder approach to learning bilingual word repre-
sentations. arxiv:1402.1454 [cs.cl], febru-
ary.

[dahl et al.2012] george e. dahl, dong yu, li deng,
and alex acero. 2012. context-dependent pre-
trained deep neural networks for large vocabulary
ieee transactions on audio,
id103.
speech, and language processing, 20(1):33   42.

rabih

[devlin et al.2014] jacob devlin,

zbib,
zhongqiang huang, thomas lamar, richard
schwartz, , and john makhoul. 2014. fast and
robust neural network joint models for statistical
in proceedings of the acl
machine translation.
2014 conference, acl    14, pages 1370   1380.

[gao et al.2013] jianfeng gao, xiaodong he, wen tau
yih, and li deng. 2013. learning semantic repre-
sentations for the phrase translation model. techni-
cal report, microsoft research.

[glorot et al.2011] x. glorot, a. bordes, and y. ben-
gio. 2011. deep sparse recti   er neural networks. in
aistats   2011.

[goodfellow et al.2013] ian j. goodfellow, david
warde-farley, mehdi mirza, aaron courville, and
yoshua bengio.
in
icml   2013.

2013. maxout networks.

[graves2012] alex graves.

supervised se-
quence labelling with recurrent neural networks.
studies in computational intelligence. springer.

2012.

[hochreiter and schmidhuber1997] s. hochreiter and
j. schmidhuber. 1997. long short-term memory.
neural computation, 9(8):1735   1780.

[kalchbrenner and blunsom2013] nal kalchbrenner
and phil blunsom. 2013. two recurrent continuous
translation models. in proceedings of the acl con-
ference on empirical methods in natural language
processing (emnlp), pages 1700   1709.

[koehn et al.2003] philipp koehn, franz josef och,
and daniel marcu. 2003. statistical phrase-based
translation. in proceedings of the 2003 conference
of the north american chapter of the association
for computational linguistics on human language
technology - volume 1, naacl    03, pages 48   54.

[koehn2005] p. koehn. 2005. europarl: a parallel cor-
pus for id151. in machine
translation summit x, pages 79   86, phuket, thai-
land.

[krizhevsky et al.2012] alex

krizhevsky,

ilya
sutskever, and geoffrey hinton.
ima-
genet classi   cation with deep convolutional neural
in advances in neural information
networks.
processing systems 25 (nips   2012).

2012.

[marcu and wong2002] daniel marcu and william
2002. a phrase-based, joint id203
wong.
in pro-
model for id151.
ceedings of the acl-02 conference on empirical
methods in natural language processing - volume
10, emnlp    02, pages 133   139.

[mikolov et al.2013] tomas mikolov, ilya sutskever,
kai chen, greg corrado, and jeff dean. 2013. dis-
tributed representations of words and phrases and
their compositionality. in advances in neural infor-
mation processing systems 26, pages 3111   3119.

2010.

[moore and lewis2010] robert c. moore and william
intelligent selection of language
lewis.
in proceedings of the acl
model training data.
2010 conference short papers, aclshort    10,
pages 220   224, stroudsburg, pa, usa.

[pascanu et al.2014] r. pascanu, c. gulcehre, k. cho,
and y. bengio. 2014. how to construct deep recur-
rent neural networks. in proceedings of the second
international conference on learning representa-
tions (iclr 2014), april.

[saxe et al.2014] andrew m. saxe, james l. mcclel-
land, and surya ganguli. 2014. exact solutions
to the nonlinear dynamics of learning in deep lin-
ear neural networks. in proceedings of the second
international conference on learning representa-
tions (iclr 2014), april.

[schwenk et al.2006] holger schwenk, marta r. costa-
juss`a, and jos  e a. r. fonollosa. 2006. continuous
space language models for the iwslt 2006 task. in
iwslt, pages 166   173.

[schwenk2007] holger schwenk. 2007. continuous
space language models. comput. speech lang.,
21(3):492   518, july.

[schwenk2012] holger schwenk. 2012. continuous
space translation models for phrase-based statisti-
cal machine translation. in martin kay and chris-
tian boitet, editors, proceedings of the 24th inter-
national conference on computational linguistics
(colin), pages 1071   1080.

[socher et al.2011] richard socher, eric h. huang, jef-
frey pennington, andrew y. ng, and christopher d.
manning. 2011. dynamic pooling and unfolding
recursive autoencoders for paraphrase detection. in
advances in neural information processing systems
24.

[son et al.2012] le hai son, alexandre allauzen, and
franc  ois yvon. 2012. continuous space transla-
tion models with neural networks. in proceedings of
the 2012 conference of the north american chap-
ter of the association for computational linguistics:
human language technologies, naacl hlt    12,
pages 39   48, stroudsburg, pa, usa.

[van der maaten2013] laurens van der maaten. 2013.
in proceedings of the first inter-
barnes-huid167.
national conference on learning representations
(iclr 2013), may.

[vaswani et al.2013] ashish vaswani, yinggong zhao,
victoria fossum, and david chiang. 2013. de-
coding with large-scale neural language models im-
proves translation. proceedings of the conference
on empirical methods in natural language pro-
cessing, pages 1387   1392.

[zeiler2012] matthew d. zeiler. 2012. adadelta:
an adaptive learning rate method. technical report,
arxiv 1212.5701.

richard

[zou et al.2013] will y. zou,

socher,
daniel m. cer, and christopher d. manning.
2013. bilingual id27s for phrase-based
in proceedings of the acl
machine translation.
conference on empirical methods
in natural
language processing (emnlp), pages 1393   1398.

a id56 encoder   decoder
in this document, we describe in detail the architecture of the id56 encoder   decoder used in the exper-
iments.

let us denote an source phrase by x = (x1, x2, . . . , xn ) and a target phrase by y =
(y1, y2, . . . , ym ). each phrase is a sequence of k-dimensional one-hot vectors, such that only one
element of the vector is 1 and all the others are 0. the index of the active (1) element indicates the word
represented by the vector.

a.1 encoder
each word of the source phrase is embedded in a 500-dimensional vector space: e(xi)     r500. e(x) is
used in sec. 4.4 to visualize the words.

the hidden state of an encoder consists of 1000 hidden units, and each one of them at time t is

computed by

where

(cid:104)t   1(cid:105)
(cid:104)t(cid:105)
h
j = zjh
j

(cid:104)t(cid:105)
+ (1     zj)  h
j ,

(cid:16)
[we(xt)]j +(cid:2)u(cid:0)r (cid:12) h(cid:104)t   1(cid:105)(cid:1)(cid:3)
[wze(xt)]j +(cid:2)uzh(cid:104)t   1(cid:105)(cid:3)
[wre(xt)]j +(cid:2)urh(cid:104)t   1(cid:105)(cid:3)

(cid:17)
(cid:17)

.

,

j

(cid:16)
(cid:16)

j

(cid:17)

,

j

(cid:104)t(cid:105)
  h
j = tanh

zj =  

rj =  

   and (cid:12) are a logistic sigmoid function and an element-wise multiplication, respectively. to make the
(cid:104)0(cid:105)
equations uncluttered, we omit biases. the initial hidden state h
j

once the hidden state at the n step (the end of the source phrase) is computed, the representation of

is    xed to 0.

the source phrase c is

a.1.1 decoder
the decoder starts by initializing the hidden state with

.

(cid:16)

vh(cid:104)n(cid:105)(cid:17)
= tanh(cid:0)v(cid:48)c(cid:1) ,

c = tanh

h(cid:48)(cid:104)0(cid:105)

where we will use   (cid:48) to distinguish parameters of the decoder from those of the encoder.

the hidden state at time t of the decoder is computed by

where

h(cid:48)(cid:104)t(cid:105)
j = z(cid:48)

jh(cid:48)(cid:104)t   1(cid:105)

+ (1     z(cid:48)

j)   h(cid:48)(cid:104)t(cid:105)
j ,

j

(cid:16)(cid:2)w(cid:48)e(yt   1)(cid:3)
(cid:16)(cid:2)w(cid:48)
j +(cid:2)u(cid:48)
ze(yt   1)(cid:3)
(cid:16)(cid:2)w(cid:48)
j +(cid:2)u(cid:48)
re(yt   1)(cid:3)

j + r(cid:48)

(cid:2)u(cid:48)h(cid:48)(cid:104)t   1(cid:105) + cc(cid:3)(cid:17)
(cid:17)
zh(cid:48)(cid:104)t   1(cid:105)(cid:3)
(cid:17)
rh(cid:48)(cid:104)t   1(cid:105)(cid:3)

+ [czc]j

+ [crc]j

j

j

j

  h(cid:48)(cid:104)t(cid:105)
j = tanh
z(cid:48)
r(cid:48)

j =  

j =  

,

,

,

and e(y0) is an all-zero vector. similarly to the case of the encoder, e(y) is an embedding of a target
word.

unlike the encoder which simply encodes the source phrase, the decoder is learned to generate a target

phrase. at each time t, the decoder computes the id203 of generating j-th word by

p(yt,j = 1 | yt   1, . . . , y1, x) =

exp(cid:0)gjs(cid:104)t(cid:105)(cid:1)
j(cid:48)=1 exp(cid:0)gj(cid:48)s(cid:104)t(cid:105)(cid:1) ,
(cid:80)k

where the i-element of s(cid:104)t(cid:105) is

and

(cid:104)t(cid:105)
i = max

s

(cid:110)

2i   1, s(cid:48)(cid:104)t(cid:105)
s(cid:48)(cid:104)t(cid:105)

2i

(cid:111)

s(cid:48)(cid:104)t(cid:105)

= ohh(cid:48)(cid:104)t(cid:105)

+ oyyt   1 + occ.

is a so-called maxout unit.

(cid:104)t(cid:105)
in short, the s
i

for the computational ef   ciency, instead of a single-matrix output weight g, we use a product of two

matrices such that

g = glgr,

where gl     rk  500 and gr     r500  1000.
b word and phrase representations
here, we show enlarged plots of the word and phrase representations in figs. 4   5.

figure 6: 2   d embedding of the learned word representation. the top left one shows the full embedding space, while the other three    gures show the zoomed-in
view of speci   c regions (color   coded).

figure 7: 2   d embedding of the learned phrase representation. the top left one shows the full representation space (1000 randomly selected points), while the
other three    gures show the zoomed-in view of speci   c regions (color   coded).

