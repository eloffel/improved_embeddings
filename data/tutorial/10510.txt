capturing semantic similarity for entity linking

with convolutional neural networks

matthew francis-landau, greg durrett and dan klein

computer science division

university of california, berkeley

6
1
0
2

 
r
p
a
4

 

 
 
]
l
c
.
s
c
[
 
 

1
v
4
3
7
0
0

.

4
0
6
1
:
v
i
x
r
a

{mfl,gdurrett,klein}@cs.berkeley.edu

abstract

a key challenge in entity linking is making ef-
fective use of contextual information to dis-
ambiguate mentions that might refer to differ-
ent entities in different contexts. we present
a model that uses convolutional neural net-
works to capture semantic correspondence be-
tween a mention   s context and a proposed tar-
get entity. these convolutional networks oper-
ate at multiple granularities to exploit various
kinds of topic information, and their rich pa-
rameterization gives them the capacity to learn
which id165s characterize different topics.
we combine these networks with a sparse lin-
ear model to achieve state-of-the-art perfor-
mance on multiple entity linking datasets, out-
performing the prior systems of durrett and
klein (2014) and nguyen et al. (2014).1

introduction

1
one of the major challenges of entity linking is re-
solving contextually polysemous mentions. for ex-
ample, germany may refer to a nation, to that na-
tion   s government, or even to a soccer team. past
approaches to such cases have often focused on col-
lective entity linking: nearby mentions in a docu-
ment might be expected to link to topically-similar
entities, which can give us clues about the identity of
the mention currently being resolved (ratinov et al.,
2011; hoffart et al., 2011; he et al., 2013; cheng
and roth, 2013; durrett and klein, 2014). but an
even simpler approach is to use context information
from just the words in the source document itself to
make sure the entity is being resolved sensibly in
context. in past work, these approaches have typi-
cally relied on heuristics such as tf-idf (ratinov et

1source available at

github.com/matthewfl/nlp-entity-convnet

al., 2011), but such heuristics are hard to calibrate
and they capture structure in a coarser way than
learning-based methods.

in this work, we model semantic similarity be-
tween a mention   s source document context and its
potential entity targets using convolutional neural
networks (id98s). id98s have been shown to be ef-
fective for sentence classi   cation tasks (kalchbren-
ner et al., 2014; kim, 2014; iyyer et al., 2015) and
for capturing similarity in models for entity linking
(sun et al., 2015) and other related tasks (dong et
al., 2015; shen et al., 2014), so we expect them to be
effective at isolating the relevant topic semantics for
entity linking. we show that convolutions over mul-
tiple granularities of the input document are useful
for providing different notions of semantic context.
finally, we show how to integrate these networks
with a preexisting entity linking system (durrett and
klein, 2014). through a combination of these two
distinct methods into a single system that leverages
their complementary strengths, we achieve state-of-
the-art performance across several datasets.
2 model
our model focuses on two core ideas:    rst, that topic
semantics at different granularities in a document
are helpful in determining the genres of entities for
entity linking, and second, that id98s can distill a
block of text into a meaningful topic vector.

our entity linking model is a log-linear model
that places distributions over target entities t given
a mention x and its containing source document.
for now, we take p (t|x)     exp w(cid:62)fc(x, t;   ),
where fc produces a vector of features based on
id98s with parameters    as discussed in section 2.1.
section 2.2 describes how we combine this simple
model with a full-   edged entity linking system. as
shown in the middle of figure 1, each feature in fc

figure 1: extraction of convolutional vector space features fc (x, te). three types of information from the input document and two
types of information from the proposed title are fed through convolutional networks to produce vectors, which are systematically
compared with cosine similarity to derive real-valued semantic similarity features.

is a cosine similarity between a topic vector asso-
ciated with the source document and a topic vector
associated with the target entity. these vectors are
computed by distinct id98s operating over different
subsets of relevant text.

figure 1 shows an example of why different kinds
of context are important for entity linking. in this
case, we are considering whether pink floyd might
link to the article gavin floyd on wikipedia
(imagine that pink floyd might be a person   s nick-
name). if we look at the source document, we see
that the immediate source document context around
the mention pink floyd is referring to rock groups
(led zeppelin, van halen) and the target entity   s
wikipedia page is primarily about sports (baseball
starting pitcher). distilling these texts into succinct
topic descriptors and then comparing those helps tell
us that this is an improbable entity link pair.
in
this case, the broader source document context actu-
ally does not help very much, since it contains other
generic last names like campbell and savage that
might not necessarily indicate the document to be
in the music genre. however, in general, the whole
document might provide a more robust topic esti-
mate than a small context window does.

2.1 convolutional semantic similarity

figure 1 shows our method for computing topic vec-
tors and using those to extract features for a potential
wikipedia link. for each of three text granularities

in the source document (the mention, that mention   s
immediate context, and the entire document) and
two text granularities on the target entity side (title
and wikipedia article text), we produce vector rep-
resentations with id98s as follows. we    rst embed
each word into a d-dimensional vector space using
standard embedding techniques (discussed in sec-
tion 3.2), yielding a sequence of vectors w1, . . . , wn.
we then map those words into a    xed-size vector
using a convolutional network parameterized with a

   lter bank m     rk  d(cid:96). we put the result through a

recti   ed linear unit (relu) and combine the results
with sum pooling, giving the following formulation:

convg(w1:n) =

n   (cid:96)(cid:88)j=1

max{0, mgwj:j+(cid:96)}

(1)

where wj:j+(cid:96) is a concatenation of the given word
vectors and the max is element-wise.2 each con-
volution granularity (mention, context, etc.) has a
distinct set of    lter parameters mg.

this process produces multiple representative
topic vectors sment, scontext, and sdoc for the source
document and ttitle and tdoc for the target entity, as
shown in figure 1. all pairs of these vectors be-
tween the source and the target are then compared
using cosine similarity, as shown in the middle of
figure 1. this yields the vector of features fc(s, te)

2for all experiments, we set (cid:96) = 5 and k = 150.

mdcsource documentdocumentcontextmentionpink floydthe others are the beatles, led zeppelin, pink floyd and van halen.this includes the band members: cambell, savage, elliott, collen, allen.  as of 1992, the band consisted of  elliott (vocals), collen (guitar) campbell (guitar), savage (bass guitar), and allen (drums).  the band has sold over 65 million albums wordwide, and have two albums with riaa diamond certification, pyromania and hysteria.  history early years =sdoc=scontext=smentgw1w2w3...wn:=n   (cid:30)(cid:31)j=1max{0,mwj:j+(cid:30)}:=cosim(s,t)sdocsmentscontextttitle,etdoc,efc(s,te)ettarget entity linksentity titleentity articlegavin floydgavin christopher floyd (born january 27, 1983) is a professional baseball starting pitcher for the atlanta braves of major league baseball (mlb). he previously pitched in mlb for the philadelphia phillies and chicago white sox. floyd stands 6' 5" tall, weighs 220 pounds, and throws and bats right-handed.  professional career draft and minor leagues the philadelphia phillies selected floyd with the fourth overall selection of the 2001 draft.  in his first professional season (2002), floyd pitched for the class a lakewood ttitle,e=tdoc,e=which indicate the different types of similarity; this
vector can then be combined with other sparse fea-
tures and fed into a    nal id28 layer
(maintaining end-to-end id136 and learning of
the    lters). when trained with id26, the
convolutional networks should learn to map text into
vector spaces that are informative about whether the
document and entity are related or not.

integrating with a sparse model

2.2
the dense model presented in section 2.1 is effec-
tive at capturing semantic topic similarity, but it is
most effective when combined with other signals
for entity linking. an important cue for resolving
a mention is the use of link counts from hyperlinks
in wikipedia (cucerzan, 2007; milne and witten,
2008; ji and grishman, 2011), which tell us how
often a given mention was linked to each article on
wikipedia. this information can serve as a useful
prior, but only if we can leverage it effectively by tar-
geting the most salient part of a mention. for exam-
ple, we may have never observed president barack
obama as a linked string on wikipedia, even though
we have seen the substring barack obama and it un-
ambiguously indicates the correct answer.

following durrett and klein (2014), we introduce
a latent variable q to capture which subset of a men-
tion (known as a query) we resolve. query gen-
eration includes potentially removing stop words,
plural suf   xes, punctuation, and leading or tail-
ing words. this processes generates on average 9
queries for each mention. conveniently, this set of
queries also de   nes the set of candidate entities that
we consider linking a mention to: each query gener-
ates a set of potential entities based on link counts,
whose unions are then taken to give on the possible
entity targets for each mention (including the null
link). in the example shown in figure 1, the query
phrases are pink floyd and floyd, which generate
pink floyd and gavin floyd as potential link
targets (among other options that might be derived
from the floyd query).

linear way with three separate components:

our    nal model has the form p (t|x) =
(cid:80)q p (t, q|x). we parameterize p (t, q|x) in a log-
p (t, q|x)     exp(cid:0)w(cid:62)(fq(x, q) + fe(x, q, t) + fc(x, t;   ))(cid:1)

fq and fe are both sparse features vectors and are

taken from previous work (durrett and klein, 2014).
fc is as discussed in section 2.1. note that fc has
its own internal parameters    because it relies on
id98s with learned    lters; however, we can compute
gradients for these parameters with standard back-
propagation. the whole model is trained to maxi-
mize the log likelihood of a labeled training corpus
using adadelta (zeiler, 2012).

the indicator features fq and fe are described in
more detail in durrett and klein (2014). fq only
impacts which query is selected and not the disam-
biguation to a title. it is designed to roughly cap-
ture the basic shape of a query to measure its de-
sirability, indicating whether suf   xes were removed
and whether the query captures the capitalized sub-
sequence of a mention, as well as standard lexical,
pos, and named entity type features. fe mostly
captures how likely the selected query is to corre-
spond to a given entity based on factors like an-
chor text counts from wikipedia, string match with
proposed wikipedia titles, and discretized cosine
similarities of tf-idf vectors (ratinov et al., 2011).
adding tf-idf indicators is the only modi   cation we
made to the features of durrett and klein (2014).

3 experimental results
we performed experiments on 4 different entity link-
ing datasets.

    ace (nist, 2005; bentivogli et al., 2010):
this corpus was used in fahrni and strube
(2014) and durrett and klein (2014).
    conll-yago (hoffart et al., 2011): this cor-
pus is based on the conll 2003 dataset; the
test set consists of 231 news articles and con-
tains a number of rarer entities.
    wp (heath and bizer, 2011): this dataset con-
sists of short snippets from wikipedia.
this
    wikipedia (ratinov et al., 2011):
dataset consists of 10,000 randomly sampled
wikipedia articles, with the task being to re-
solve the links in each article.3

3we do not compare to ratinov et al. (2011) on this dataset
because we do not have access to the original wikipedia dump
they used for their work and as a result could not duplicate their
results or conduct comparable experiments, a problem which
was also noted by nguyen et al. (2014).

ace conll wp wiki4
previous work
79.6
   
   
84.8
this work
83.6
84.5
89.9

74.9
81.2
85.5

81.1
87.7
90.7

81.5
75.7
82.2

   
   

   
   

dk2014
aida-light

sparse features
id98 features
full

table 2: performance of the system in this work (full) com-
pared to two baselines from prior work and two ablations.
our results outperform those of durrett and klein (2014) and
nguyen et al. (2014). in general, we also see that the convolu-
tional networks by themselves can outperform the system using
only sparse features, and in all cases these stack to give substan-
tial bene   t.

we use standard train-test splits for all datasets ex-
cept for wp, where no standard split is available.
in this case, we randomly sample a test set. for
all experiments, we use word vectors computed by
running id97 (mikolov et al., 2013) on all
wikipedia, as described in section 3.2.

table 2 shows results for two baselines and three
variants of our system. our main contribution is
the combination of indicator features and id98 fea-
tures (full). we see that this system outperforms the
results of durrett and klein (2014) and the aida-
light system of nguyen et al. (2014). we can
also compare to two ablations: using just the sparse
features (a system which is a direct extension of
durrett and klein (2014)) or using just the id98-
derived features.5 our id98 features generally out-
perform the sparse features and improve even further
when stacked with them. this re   ects that they cap-
ture orthogonal sources of information: for example,
the sparse features can capture how frequently the
target document was linked to, whereas the id98s
can capture document context in a more nuanced
way. these id98 features also clearly supersede
the sparse features based on tf-idf (taken from (rati-
nov et al., 2011)), showing that indeed that id98s
are better at learning semantic topic similarity than
heuristics like tf-idf.

4the test set for this dataset is only 40 out of 10,000 docu-

ments and subject to wide variation in performance.

5in this model, the set of possible link targets for each
mention is still populated using anchor text information from
wikipedia (section 2.2), but note that link counts are not used
as a feature here.

cosim(sdoc, tdoc)
cosim(sment, ttitle)
all id98 pairs

ace conll wp
72.93
77.43
70.25
80.19
84.85
82.02

79.76
80.86
86.91

table 3: comparison of using only topic information derived
from the document and target article, only information derived
from the mention itself and the target entity title, and the full
set of information (six features, as shown in figure 1). nei-
ther the    nest nor coarsest convolutional context can give the
performance of the complete set. numbers are reported on a
development set.

in the sparse feature system, the highest weighted
features are typically those indicating the frequency
that a page was linked to and those indicating spe-
ci   c lexical items in the choice of the latent query
variable q. this suggests that the system of dur-
rett and klein (2014) has the power to pick the right
span of a mention to resolve, but then is left to gener-
ally pick the most common link target in wikipedia,
which is not always correct. by contrast, the full
system has a greater ability to pick less common
link targets if the topic indicators distilled from the
id98s indicate that it should do so.

3.1 multiple granularities of convolution
one question we might ask is how much we gain by
having multiple convolutions on the source and tar-
get side. table 3 compares our full suite of id98
features, i.e. the six features speci   ed in figure 1,
with two speci   c convolutional features in isola-
tion. using convolutions over just the source doc-
ument (sdoc) and target article text (tdoc) gives a
system6 that performs, in aggregate, comparably to
using convolutions over just the mention (sment)
and the entity title (ttitle). these represent two
extremes of the system: consuming the maximum
amount of context, which might give the most ro-
bust representation of topic semantics, and consum-
ing the minimum amount of context, which gives
the most focused representation of topics seman-
tics (and which more generally might allow the sys-
tem to directly memorize train-test pairs observed
in training). however, neither performs as well as
the combination of all id98 features, showing that
the different granularities capture complementary

6this model is roughly comparable to model 2 as presented

in sun et al. (2015).

destroying missiles . spy planes
and destroying missiles . spy
by u.n. weapons inspectors .
inspectors are discovering and destroying
are discovering and destroying missiles
an attack using chemical weapons
discovering and destroying missiles .
attack munitions or j-dam weapons
sanctions targeting iraq civilians ,
its nuclear weapons and missile

has died of his wounds
vittorio sacerdoti has told his
his bail hearing , his
bail hearing , his lawyer
died of his wounds after
from scott peterson    s attorney
   s murder trial . she
has told his remarkable tale
murder trial . she asking
trial lawyers are driving doctors

him which was more depressing
a trip and you see
    i can see why
i think so many americans
his life from the depression
trip and you see him
, but dumb liberal could
i can see why he
one passage . you cite
think so many americans are

table 1: five-grams representing the maximal activations from different    lters in the convolution over the source document (mdoc,
producing sdoc in figure 1). some    lters tend towards singular topics as shown in the    rst and second columns, which focus on
weapons and trials, respectively. others may have a mix of seemingly unrelated topics, as shown in the third column, which does
not have a coherent theme. however, such    lters might represent a superposition of    lters for various topics which never cooccur
and thus never need to be disambiguated between.

aspects of the entity linking task.

google news
wikipedia

ace conll wp
83.8
87.5
89.5
85.5

89.6
90.6

table 4: results of the full model (sparse and convolutional
features) comparing word vectors derived from google news
vs. wikipedia on development sets for each corpus.

3.2 embedding vectors
we also explored two different sources of embed-
ding vectors for the convolutions. table 4 shows that
word vectors trained on wikipedia outperformed
google news word vectors trained on a larger cor-
pus. further investigation revealed that the google
news vectors had much higher out-of-vocabulary
rates. for learning the vectors, we use the standard
id97 toolkit (mikolov et al., 2013) with vector
length set to 300, window set to 21 (larger windows
produce more semantically-focused vectors (levy
and goldberg, 2014)), 10 negative samples and 10
iterations through wikipedia. we do not    ne-tune
word vectors during training of our model, as that
was not found to improve performance.

3.3 analysis of learned convolutions
one downside of our system compared to its purely
indicator-based variant is that its operation is less in-
terpretable. however, one way we can inspect the
learned system is by examining what causes high ac-
tivations of the various convolutional    lters (rows of
the matrices mg from equation 1). table 1 shows
the id165s in the ace dataset leading to maximal

activations of three of the    lters from mdoc. some
   lters tend to learn to pick up on id165s character-
istic of a particular topic. in other cases, a single    l-
ter might be somewhat inscrutable, as with the third
column of table 1. there are a few possible explana-
tions for this. first, the    lter may generally have low
activations and therefore have little impact in the    -
nal feature computation. second, the extreme points
of the    lter may not be characteristic of its overall
behavior, since the bulk of id165s will lead to more
moderate activations. finally, such a    lter may rep-
resent the superposition of a few topics that we are
unlikely to ever need to disambiguate between; in
a particular context, this    lter will then play a clear
role, but one which is hard to determine from the
overall shape of the parameters.

4 conclusion

in this work, we investigated using convolutional
networks to capture semantic similarity between
source documents and potential entity link targets.
using multiple granularities of convolutions to eval-
uate the compatibility of a mention in context and
several potential link targets gives strong perfor-
mance on its own; moreover, such features also im-
prove a pre-existing entity linking system based on
sparse indicator features, showing that these sources
of information are complementary.

acknowledgments

this work was partially supported by nsf grant
cns-1237265 and a google faculty research

award. thanks to the anonymous reviewers for their
helpful comments.

references
[bentivogli et al.2010] luisa bentivogli, pamela forner,
claudio giuliano, alessandro marchetti, emanuele
pianta, and kateryna tymoshenko. 2010. extending
english ace 2005 corpus annotation with ground-
truth links to wikipedia. in proceedings of the work-
shop on the people   s web meets nlp: collaboratively
constructed semantic resources.

[cheng and roth2013] xiao cheng and dan roth. 2013.
relational id136 for wiki   cation. in proceedings
of the conference on empirical methods in natural
language processing (emnlp).

[cucerzan2007] silviu cucerzan.

2007. large-scale
named entity disambiguation based on wikipedia
data. in proceedings of the joint conference on em-
pirical methods in natural language processing and
computational natural language learning (emnlp-
conll).

[dong et al.2015] li dong, furu wei, ming zhou, and
ke xu. 2015. id53 over freebase
with multi-column convolutional neural networks.
in proceedings of the 53rd annual meeting of the as-
sociation for computational linguistics (acl) and the
7th international joint conference on natural lan-
guage processing (volume 1: long papers).

[durrett and klein2014] greg durrett and dan klein.
2014. a joint model for entity analysis: coreference,
typing, and linking. in transactions of the associa-
tion for computational linguistics (tacl).

[fahrni and strube2014] angela fahrni

and michael
strube. 2014. a latent variable model for discourse-
in gosse
aware concept and entity disambiguation.
bouma and yannick parmentier 0001,
editors,
proceedings of
the euro-
pean chapter of the association for computational
linguistics, eacl 2014, april 26-30, 2014, gothen-
burg, sweden, pages 491   500. the association for
computer linguistics.

the 14th conference of

[he et al.2013] zhengyan he, shujie liu, yang song,
mu li, ming zhou, and houfeng wang. 2013. ef-
   cient collective entity linking with stacking.
in
emnlp, pages 426   435. acl.

[heath and bizer2011] tom heath and christian bizer.
2011. linked data: evolving the web into a global
data space. morgan & claypool, 1st edition.

[hoffart et al.2011] johannes hoffart, mohamed amir
ilaria bordino, hagen f  urstenau, manfred
yosef,
pinkal, marc spaniol, bilyana taneva, stefan thater,
and gerhard weikum. 2011. robust disambiguation

of named entities in text. in proceedings of the con-
ference on empirical methods in natural language
processing (emnlp).

[iyyer et al.2015] mohit iyyer, varun manjunatha, jor-
dan boyd-graber, and hal daum  e iii. 2015. deep
unordered composition rivals syntactic methods for
text classi   cation. in proceedings of the association
for computational linguistics (acl).

[ji and grishman2011] heng ji and ralph grishman.
2011. knowledge base population: successful ap-
proaches and challenges. in proceedings of the asso-
ciation for computational linguistics (acl).

[kalchbrenner et al.2014] nal kalchbrenner,

edward
grefenstette, and phil blunsom.
2014. a convo-
lutional neural network for modelling sentences.
in proceedings of the 52nd annual meeting of the
association for computational linguistics (volume 1:
long papers), pages 655   665, baltimore, maryland,
june. association for computational linguistics.

[kim2014] yoon kim. 2014. convolutional neural net-
works for sentence classi   cation. in proceedings of
the conference on empirical methods in natural lan-
guage processing (emnlp).

[levy and goldberg2014] omer levy and yoav gold-
berg. 2014. dependency-based id27s.
in proceedings of the 52nd annual meeting of the as-
sociation for computational linguistics (acl).

[mikolov et al.2013] tomas mikolov, ilya sutskever, kai
chen, greg s corrado, and jeff dean. 2013. dis-
tributed representations of words and phrases and
their compositionality. in advances in neural infor-
mation processing systems (nips) 26, pages 3111   
3119.

[milne and witten2008] david milne and ian h. witten.
2008. learning to link with wikipedia. in proceed-
ings of the conference on information and knowledge
management (cikm).

[nguyen et al.2014] dat ba nguyen, johannes hoffart,
martin theobald, and gerhard weikum. 2014. aida-
light: high-throughput named-entity disambigua-
tion. in proceedings of the workshop on linked data
on the web co-located with the 23rd international
world wide web conference (www).

[nist2005] nist. 2005. the ace 2005 evaluation

plan. in nist.

[ratinov et al.2011] lev ratinov, dan roth, doug
downey, and mike anderson.
2011. local and
global algorithms for disambiguation to wikipedia.
in proceedings of the 49th annual meeting of the as-
sociation for computational linguistics (acl), pages
1375   1384.

[shen et al.2014] yelong shen, xiaodong he, jianfeng
gao, li deng, and gr  egoire mesnil. 2014. learning

semantic representations using convolutional neural
networks for web search. in proceedings of the 23rd
international conference on world wide web (www).
[sun et al.2015] yaming sun, lei lin, duyu tang, nan
yang, zhenzhou ji, and xiaolong wang. 2015. mod-
eling mention, context and entity with neural net-
in proceedings of
works for entity disambiguation.
the international joint conference on arti   cial intelli-
gence (ijcai), pages 1333   1339.

[zeiler2012] matthew d. zeiler.

2012.

an adaptive learning rate method.
abs/1212.5701.

adadelta:
corr,

