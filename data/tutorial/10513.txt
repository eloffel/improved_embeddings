4
1
0
2

 

b
e
f
5
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
2
2
7
3

.

2
0
4
1
:
v
i
x
r
a

id97 explained: deriving mikolov et al.   s
negative-sampling word-embedding method

yoav goldberg and omer levy

{yoav.goldberg,omerlevy}@gmail.com

february 14, 2014

the id97 software of tomas mikolov and colleagues1 has gained a lot
of traction lately, and provides state-of-the-art id27s. the learning
models behind the software are described in two research papers [1, 2]. we
found the description of the models in these papers to be somewhat cryptic
and hard to follow. while the motivations and presentation may be obvious to
the neural-networks language-modeling crowd, we had to struggle quite a bit to
   gure out the rationale behind the equations.

this note is an attempt to explain equation (4) (negative sampling) in    dis-
tributed representations of words and phrases and their compositionality    by
tomas mikolov, ilya sutskever, kai chen, greg corrado and je   rey dean [2].

1 the skip-gram model

the departure point of the paper is the skip-gram model. in this model we are
given a corpus of words w and their contexts c. we consider the conditional
probabilities p(c|w), and given a corpus t ext, the goal is to set the parameters
   of p(c|w;   ) so as to maximize the corpus id203:

arg max

   y

w   t ext

   
    y

c   c(w)

   
p(c|w;   )
   

in this equation, c(w) is the set of contexts of word w. alternatively:

arg max

   y

(w,c)   d

p(c|w;   )

here d is the set of all word and context pairs we extract from the text.

1https://code.google.com/p/id97/

1

(1)

(2)

1.1 parameterization of the skip-gram model

one approach for parameterizing the skip-gram model follows the neural-network
language models literature, and models the id155 p(c|w;   ) us-
ing soft-max:

p(c|w;   ) =

evc  vw

pc      c evc      vw

(3)

where vc and vw     rd are vector representations for c and w respectively, and
c is the set of all available contexts.2 the parameters    are vci , vwi for w     v ,
c     c, i     1,          , d (a total of |c|    |v |    d parameters). we would like to set
the parameters such that the product (2) is maximized.

now will be a good time to take the log and switch from product to sum:

arg max

   x

(w,c)   d

log p(c|w) = x

(w,c)   d

(log evc  vw     logx

c   

evc      vw )

(4)

an assumption underlying the embedding process is the following:

assumption maximizing objective 4 will result in good embeddings vw     w     v ,

in the sense that similar words will have similar vectors.

it is not clear to us at this point why this assumption holds.

while objective (4) can be computed, it is computationally expensive to do
so, because the term p(c|w;   ) is very expensive to compute due to the summa-
tion pc      c evc      vw over all the contexts c    (there can be hundreds of thousands
of them). one way of making the computation more tractable is to replace the
softmax with an hierarchical softmax. we will not elaborate on this direction.

2 negative sampling

mikolov et al.
[2] present the negative-sampling approach as a more e   cient
way of deriving id27s. while negative-sampling is based on the
skip-gram model, it is in fact optimizing a di   erent objective. what follows is
the derivation of the negative-sampling objective.

consider a pair (w, c) of word and context. did this pair come from the
training data? let   s denote by p(d = 1|w, c) the id203 that (w, c) came
from the corpus data. correspondingly, p(d = 0|w, c) = 1     p(d = 1|w, c) will
be the id203 that (w, c) did not come from the corpus data. as before,
assume there are parameters    controlling the distribution: p(d = 1|w, c;   ).

2throughout this note, we assume that the words and the contexts come from distinct
vocabularies, so that, for example, the vector associated with the word dog will be di   erent
from the vector associated with the context dog. this assumption follows the literature, where
it is not motivated. one motivation for making this assumption is the following: consider the
case where both the word dog and the context dog share the same vector v. words hardly
appear in the contexts of themselves, and so the model should assign a low id203 to
p(dog|dog), which entails assigning a low value to v    v which is impossible.

2

our goal is now to    nd parameters to maximize the probabilities that all of the
observations indeed came from the data:

arg max

   y

(w,c)   d

p(d = 1|w, c;   )

= arg max

  

log y

(w,c)   d

p(d = 1|w, c;   )

= arg max

   x

(w,c)   d

log p(d = 1|w, c;   )

the quantity p(d = 1|c, w;   ) can be de   ned using softmax:

p(d = 1|w, c;   ) =

1

1 + e   vc  vw

leading to the objective:

arg max

   x

(w,c)   d

log

1

1 + e   vc  vw

this objective has a trivial solution if we set    such that p(d = 1|w, c;   ) = 1
for every pair (w, c). this can be easily achieved by setting    such that vc = vw
and vc    vw = k for all vc, vw, where k is large enough number (practically, we
get a id203 of 1 as soon as k     40).

we need a mechanism that prevents all the vectors from having the same
value, by disallowing some (w, c) combinations. one way to do so, is to present
the model with some (w, c) pairs for which p(d = 1|w, c;   ) must be low, i.e.
pairs which are not in the data. this is achieved by generating the set d   
of random (w, c) pairs, assuming they are all incorrect (the name    negative-
sampling    stems from the set d    of randomly sampled negative examples). the
optimization objective now becomes:

arg max

   y

(w,c)   d

= arg max

   y

(w,c)   d

= arg max

   x

(w,c)   d

= arg max

   x

(w,c)   d

= arg max

   x

(w,c)   d

p(d = 1|c, w;   ) y

(w,c)   d   

p(d = 1|c, w;   ) y

(w,c)   d   

p(d = 0|c, w;   )

(1     p(d = 1|c, w;   ))

log p(d = 1|c, w;   ) + x

(w,c)   d   

log(1     p(d = 1|w, c;   ))

log

log

1

1 + e   vc  vw

+ x

(w,c)   d   

log(1    

1

1 + e   vc  vw

)

1

1 + e   vc  vw

+ x

(w,c)   d   

log(

1

1 + evc  vw

)

3

if we let   (x) = 1

1+e   x we get:

arg max

   x

(w,c)   d

= arg max

   x

(w,c)   d

log

1

1 + e   vc  vw

+ x

(w,c)   d   

log(

1

1 + evc  vw

)

log   (vc    vw) + x

(w,c)   d   

log   (   vc    vw)

which is almost equation (4) in mikolov et al ([2]).

the di   erence from mikolov et al. is that here we present the objective for
the entire corpus d     d   , while they present it for one example (w, c)     d and
k examples (w, cj )     d   , following a particular way of constructing d   .

speci   cally, with negative sampling of k, mikolov et al.   s constructed d   
is k times larger than d, and for each (w, c)     d we construct k samples
(w, c1), . . . , (w, ck), where each cj is drawn according to its unigram distribution
raised to the 3/4 power. this is equivalent to drawing the samples (w, c) in d   
from the distribution (w, c)     pwords(w) pcontexts (c)3/4
, where pwords(w) and
pcontexts(c) are the unigram distributions of words and contexts respectively,
and z is a id172 constant. in the work of mikolov et al. each context
is a word (and all words appear as contexts), and so pcontext(x) = pwords(x) =
count(x)

z

|t ext|

2.1 remarks

    unlike the skip-gram model described above, the formulation in this sec-
tion does not model p(c|w) but instead models a quantity related to the
joint distribution of w and c.

    if we    x the words representation and learn only the contexts representa-
tion, or    x the contexts representation and learn only the word represen-
tations, the model reduces to id28, and is convex. however,
in this model the words and contexts representations are learned jointly,
making the model non-convex.

3 context de   nitions

this section lists some peculiarities of the contexts used in the id97 soft-
ware, as re   ected in the code. generally speaking, for a sentence of n words
w1, . . . , wn, contexts of a word wi comes from a window of size k around the
word: c(w) = wi   k, . . . , wi   1, wi+1, . . . , wi+k, where k is a parameter. how-
ever, there are two subtleties:

dynamic window size the window size that is being used is dynamic     the
parameter k denotes the maximal window size. for each word in the
corpus, a window size k    is sampled uniformly from 1, . . . , k.

4

e   ect of subsampling and rare-word pruning id97 has two additional

parameters for discarding some of the input words: words appearing less
than min-count times are not considered as either words or contexts,
an in addition frequent words (as de   ned by the sample parameter) are
down-sampled. importantly, these words are removed from the text before
generating the contexts. this has the e   ect of increasing the e   ective win-
dow size for certain words. according to mikolov et al. [2], sub-sampling
of frequent words improves the quality of the resulting embedding on some
benchmarks. the original motivation for sub-sampling was that frequent
words are less informative. here we see another explanation for its e   ec-
tiveness: the e   ective window size grows, including context-words which
are both content-full and linearly far away from the focus word, thus mak-
ing the similarities more topical.

4 why does this produce good word represen-

tations?

good question. we don   t really know.

the distributional hypothesis states that words in similar contexts have sim-
ilar meanings. the objective above clearly tries to increase the quantity vw    vc
for good word-context pairs, and decrease it for bad ones.
intuitively, this
means that words that share many contexts will be similar to each other (note
also that contexts sharing many words will also be similar to each other). this
is, however, very hand-wavy.

can we make this intuition more precise? we   d really like to see something

more formal.

references

[1] tomas mikolov, kai chen, greg corrado, and je   rey dean. e   cient es-
timation of word representations in vector space. corr, abs/1301.3781,
2013.

[2] tomas mikolov, ilya sutskever, kai chen, gregory s. corrado, and je   rey
dean. distributed representations of words and phrases and their composi-
tionality. in advances in neural information processing systems 26: 27th
annual conference on neural information processing systems 2013. pro-
ceedings of a meeting held december 5-8, 2013, lake tahoe, nevada, united
states, pages 3111   3119, 2013.

5

