character-level id53 with attention

david golub

university of washington

xiaodong he

microsoft research

golubd@cs.washington.edu

xiaohe@microsoft.com

6
1
0
2

 

n
u
j
 

5

 
 
]
l
c
.
s
c
[
 
 

4
v
7
2
7
0
0

.

4
0
6
1
:
v
i
x
r
a

abstract

we show that a character-level encoder-
decoder framework can be successfully ap-
plied to id53 with a structured
knowledge base. we use our model for single-
relation id53 and demonstrate
the effectiveness of our approach on the sim-
plequestions dataset (bordes et al., 2015),
where we improve state-of-the-art accuracy
from 63.9% to 70.9%, without use of ensem-
bles.
importantly, our character-level model
has 16x fewer parameters than an equivalent
word-level model, can be learned with signif-
icantly less data compared to previous work,
which relies on data augmentation, and is ro-
bust to new entities in testing.

1

introduction

single-relation factoid questions are the most com-
mon form of questions found in search query logs
and community id53 websites (yih et
al., 2014; fader et al., 2013). a knowledge-base
(kb) such as freebase, dbpedia, or wikidata can
help answer such questions after users reformulate
them as queries. for instance, the question    where
was barack obama born?    can be answered by is-
suing the following kb query:

  (x).place of birth(barack obama, x)

however, automatically mapping a natural language
question such as    where was barack obama born?   
to its corresponding kb query remains a challenging
task.

there are three key issues that make learning this
mapping non-trivial. first, there are many para-
phrases of the same question. second, many of the
kb entries are unseen during training time; however,
we still need to correctly predict them at test time.
third, a kb such as freebase typically contains mil-
lions of entities and thousands of predicates, mak-
ing it dif   cult for a system to predict these entities at
scale (yih et al., 2014; fader et al., 2014; bordes et
al., 2015). in this paper, we address all three of these
issues with a character-level encoder-decoder frame-
work that signi   cantly improves performance over
state-of-the-art word-level neural models, while also
providing a much more compact model that can be
learned from less data.

first, we use a long short-term memory (lstm)
(hochreiter and schmidhuber, 1997) encoder to em-
bed the question. second, to make our model ro-
bust to unseen kb entries, we extract embeddings
for questions, predicates and entities purely from
their character-level representations.
character-
level modeling has been previously shown to gen-
eralize well to new words not seen during training
(ljube  si  c et al., 2014; chung et al., 2016), which
makes it ideal for this task. third, to scale our model
to handle the millions of entities and thousands of
predicates in the kb, instead of using a large out-
put layer in the decoder to directly predict the entity
and predicate, we use a general interaction function
between the question embeddings and kb embed-
dings that measures their semantic relevance to de-
termine the output. the combined use of character-
level modeling and a semantic relevance function
allows us to successfully produce likelihood scores

figure 1: our encoder-decoder architecture that generates a query against a structured knowledge base.
we encode our question via a long short-term memory (lstm) network and an attention mechanism to
produce our context vector. during decoding, at each time step, we feed the current context vector and an
embedding of the english alias of the previously generated knowledge base entry into an attention-based
decoding lstm to generate the new candidate entity or predicate.

for the kb entries that are not present in our vo-
cabulary, a challenging task for standard encoder-
decoder frameworks.

our

novel,

character-level

encoder-decoder
model is compact, requires signi   cantly less data to
train than previous work, and is able to generalize
well to unseen entities in test time.
in particular,
without use of ensembles, we achieve 70.9% accu-
racy in the freebase2m setting and 70.3% accuracy
in the freebase5m setting on the simplequestions
dataset, outperforming the previous state-of-arts of
62.7% and 63.9% (bordes et al., 2015) by 8.2%
and 6.4% respectively. moreover, we only use the
training questions provided in simplequestions to
train our model, which cover about 24% of words in
entity aliases on the test set. this demonstrates the
robustness of the character-level model to unseen
entities.
in contrast, data augmentation is usually
necessary to provide more coverage for unseen
entities and predicates, as done in previous work
(bordes et al., 2015; yih et al., 2014).

2 related work
our work is motivated by three major threads
of research in machine learning and natural lan-

guage processing:
domain id53, character-level
guage modeling, and encoder-decoder methods.

semantic-parsing for open-
lan-

id29 for open-domain question an-
swering, which translates a question into a struc-
tured kb query, is a key component in question an-
swering with a kb. while early approaches relied on
building high-quality lexicons for domain-speci   c
databases such as geoquery (tang and mooney,
2001), recent work has focused on building seman-
tic parsing frameworks for general knowledge bases
such as freebase (yih et al., 2014; bordes et al.,
2014a; bordes et al., 2015; berant and liang, 2014;
fader et al., 2013).

id29 frameworks for

large-scale
knowledge bases have to be able to successfully gen-
erate queries for the millions of entities and thou-
sands of predicates in the kb, many of which are
unseen during training. to address this issue, recent
work relies on producing embeddings for predicates
and entities in a kb based on their textual descrip-
tions (bordes et al., 2014a; bordes et al., 2015; yih
et al., 2014; yih et al., 2015). a general interaction
function can then be used to measure the semantic
relevance of these embedded kb entries to the ques-

wh e r e ...    b o r n ?       <    >q: where was barack obama born?o b a m ab a r a c k  o b a m aobama 0.18barack obama 0.60... id98eid98eid98e                                                   a) question encoder(character-level lstm)c) entity & predicate encoder(character-level id98s)b) kb query decoder(lstm with an attention mechanism)    people/   /spouse 0.2people/   /   birth 0.58...id98pid98pid98p   h0</s>entity attentions predicate attentionsp e o p l e/   / s p o u s e h1h2h3p e o p l e/   / p l a c e _ o f _ b ir t h s1       s4                          sn{  1}{  2}                                                                        1    2   tion and determine the most likely kb query.

most of these approaches use word-level embed-
dings to encode entities and predicates, and there-
fore might suffer from the out-of-vocabulary (oov)
problem when they encounter unseen words during
test time. consequently, they often rely on signif-
icant data augmentation from sources such as par-
alex (fader et al., 2013), which contains 18 million
question-paraphrase pairs scraped from wikian-
swers, to have suf   cient examples for each word
they encounter (bordes et al., 2014b; yih et al.,
2014; bordes et al., 2015).

as opposed to word-level modeling, character-
level modeling can be used to handle the oov is-
sue. while character-level modeling has not been
applied to factoid id53 before, it has
been successfully applied to information retrieval,
machine translation, id31, classi   ca-
tion, and id39 (huang et al.,
2013; shen et al., 2014; chung et al., 2016; zhang
et al., 2015; santos and zadrozny, 2014; dos santos
and gatti, 2014; klein et al., 2003; dos santos, 2014;
dos santos et al., 2015). moreover, chung et al.
(2015) demonstrate that gated-feedback lstms on
top of character-level embeddings can capture long-
term dependencies in id38.

lastly, encoder-decoder networks have been ap-
plied to many structured machine learning tasks.
first introduced in sutskever et al. (2014), in an
encoder-decoder network, a source sequence is    rst
encoded with a recurrent neural network (id56) into
a    xed-length vector which intuitively captures its
   meaning   , and then decoded into a desired tar-
get sequence. this approach and related memory-
based or attention-based approaches have been suc-
cessfully applied in diverse domains such as speech
recognition, machine translation, image captioning,
parsing, executing programs, and conversational di-
alogues (amodei et al., 2015; venugopalan et al.,
2015; bahdanau et al., 2015; vinyals et al., 2015;
zaremba and sutskever, 2014; xu et al., 2015;
sukhbaatar et al., 2015).

unlike previous work, we formulate question an-
swering as a problem of decoding the kb query
given the question and kb entries which are en-
coded in embedding spaces. we therefore inte-
grate the learning of question and kb embeddings
in a uni   ed encoder-decoder framework, where the

whole system is optimized end-to-end.

3 model
since we focus on single-relation question answer-
ing in this work, our model decodes every ques-
tion into a kb query that consists of exactly two
elements   the topic entity, and the predicate. more
formally, our model is a function f (q,{e},{p}) that
takes as input a question q, a set of candidate enti-
ties {e} = e1, ..., en, a set of candidate predicates
{p} = p1, ..., pm, and produces a likelihood score
p(ei, pj|q) of generating entity ei and predicate pj
given question q for all i     1...n, j     1...m.

as illustrated in figure 1, our model consists of

three components:

1. a character-level lstm-based encoder for the
question which produces a sequence of embed-
ding vectors, one for each character (figure
1a).

2. a character-level convolutional neural net-
work (id98)-based encoder
the predi-
cates/entities in a knowledge base which pro-
duces a single embedding vector for each pred-
icate or entity (figure 1c).

for

3. an lstm-based decoder with an attention
mechanism and a relevance function for gener-
ating the topic entity and predicate to form the
kb query (figure 1b).

the details of each component are described in the
following sections.

3.1 encoding the question
to encode the question, we take two steps:

1. we    rst extract one-hot encoding vectors for
characters in the question, x1, ..., xn, where xi
represents the one-hot encoding vector for the
ith character in the question. we keep the
space, punctuation and original cases without
id121. 1

1we notice that some entity mentions in the question are
capitalized and some are not. also, some questions end with a
   ?    and some do not.

2. we feed x1, ..., xn from left to right into a two-
layer gated-feedback lstm, and keep the out-
puts at all time steps as the embeddings for the
question, i.e., these are the vectors s1, ..., sn.

3.2 encoding entities and predicates in the kb
to encode an entity or predicate in the kb, we take
two steps:

1. we    rst extract one-hot encoding vectors for
characters in its english alias, x1, ..., xn, where
xi represents the one-hot encoding vector for
the ith character in the alias.

2. we then feed x1, ..., xn into a temporal id98
with two alternating convolutional and fully-
connected layers,
followed by one fully-
connected layer:
f (x1, ..., xn) = tanh(w3    max(tanh(w2  
conv(tanh(w1    conv(x1, ..., xn))))))
where f (x1...n) is an embedding vector of size
n, w3 has size rn  h, conv represents a tem-
poral convolutional neural network, and max
represents a max pooling layer in the temporal
direction.

we use a id98 as opposed to an lstm to embed
kb entries primarily for computational ef   ciency.
also, we use two different id98s to encode enti-
ties and predicates because they typically have sig-
ni   cantly different styles (e.g.,    barack obama    vs.
   /people/person/place of birth   ).

3.3 decoding the kb query
to generate the single topic entity and predicate to
form the kb query, we use a decoder with two key
components:

1. an lstm-based decoder with attention.

its
hidden states at each time step i, hi, have the
same dimensionality n as the embeddings of
entities/predicates. the initial hidden state h0
is set to the zero vector: (cid:126)0.

2. a pairwise semantic relevance function that
measures the similarity between the hidden
units of the lstm and the embedding of an en-
tity or predicate candidate. it then returns the
mostly likely entity or predicate based on the
similarity score.

in the following two sections, we will    rst de-
scribe the lstm decoder with attention, followed
by the semantic relevance function.
3.3.1 lstm-based decoder with attention

the attention-based lstm decoder uses a similar
architecture as the one described in bahdanau et al.
(2015). at each time step i, we feed in a context
vector ci and an input vector vi into the lstm. at
time i = 1 we feed a special input vector v<s> =
(cid:126)0 into the lstm. at time i = 2, during training,
the input vector is the embedding of the true entity,
while during testing, it is the embedding of the most
likely entity as determined at the previous time step.
we now describe how we produce the context
vector ci. let hi   1 be the hidden state of the lstm
at time i   1, sj be the jth question character embed-
ding, n be the number of characters in the question,
r be the size of sj, and m be a hyperparameter. then
the context vector ci, which represents the attention-
weighted content of the question, is recomputed at
each time step i as follows:

n(cid:88)

ci =

  ijsj,

j=1

(cid:80)tx

exp (eij)
k=1 exp (eik)

  ij =

eij =v(cid:62)

a tanh (wahi   1 + uasj) ,

where {  } is the attention distribution that is ap-
plied over each hidden unit sj, wa     rm  n , ua    
rm  r, and va     r1  m.
3.3.2 semantic relevance function

unlike machine translation and language model-
ing where the vocabulary is relatively small, there
are millions of entries in the kb. if we try to di-
rectly predict the kb entries, the decoder will need
an output layer with millions of nodes, which is
computationally prohibitive. therefore, we resort
to a relevance function that measures the semantic
similarity between the decoder   s hidden state and
the embeddings of kb entries. our semantic rel-
evance function takes two vectors x1, x2 and re-
turns a distance measure of how similar they are to

results on simplequestions dataset
train sources
autogen.
kb
wq siq prp questions

ensemble

sq accuracy

ours
ours

embed model
type
char
word
word memnn
char
word
word memnn
word memnn
word memnn

no
no
yes
no
no
yes
yes
yes

yes
yes
yes
yes
yes
yes
yes
yes

fb2m no
fb2m no
fb2m yes
fb5m no
fb5m no
fb5m yes
fb5m yes
fb5m yes
table 1: experimental results on the simplequestions dataset. memnn results are from bordes et al.
(2015). wq, siq and prp stand for webquestions, simplequestions and extracted paraphrases from
wikianswers, respectively.

1 model
1 model
1 model
1 model
1 model
5 models
subgraph
1 model

76k
76k
26m
76k
76k
27m
27m
27m

no
no
yes
no
no
yes
yes
yes

70.9
53.9
62.7
70.3
53.1
63.9
62.9
62.2

ours
ours

# train
examples

each other. in current experiments we use a simple
cosine-similarity metric: cos(x1, x2).

using this similarity metric, the likelihoods of

generating entity ej and predicate pk are:

(cid:80)n
(cid:80)m

exp(  cos(h1, ej))
i=1 exp(  cos(h1, ei))
exp(  cos(h2, pk))
i=1 exp(  cos(h2, pi))

p (ej) =

p (pk) =

where    is a constant, h1, h2 are the hidden states of
the lstm at times t = 1 and t = 2, e1, ..., en are the
entity embeddings, and p1, ..., pm are the predicate
embeddings. a similar likelihood function was used
to train the semantic similarity modules proposed in
yih et al. (2014) and yih et al. (2015).

during id136, e1, ..., en and p1, ..., pm are the
embeddings of candidate entities and predicates.
during training e1, ..., en, p1, ..., pm are the embed-
dings of the true entity and 50 randomly-sampled
entities, and the true predicate and 50 randomly-
sampled predicates, respectively.

id136

3.4
for each question q, we generate a candidate set
of entities and predicates, {e} and {p}, and feed it
through the model f (q,{e},{p}). we then decode
the most likely (entity, predicate) pair:

(e   , p   ) = argmaxei,pj (p (ei)     p (pj))

which becomes our semantic parse.

we use a similar procedure as the one described
in bordes et al. (2015) to generate candidate entities

{e} and predicates {p}. namely, we take all entities
whose english alias is a substring of the question,
and remove all entities whose alias is a substring of
another entity. for each english alias, we sort each
entity with this alias by the number of facts that it
has in the kb, and append the top 10 entities from
this list to our set of candidate entities. all predi-
cates pj for each entity in our candidate entity set
become the set of candidate predicates.

3.5 learning
our goal in learning is to maximize the joint likeli-
hood p (ec)  p (pc) of predicting the correct entity ec
and predicate pc pair from a set of randomly sampled
entities and predicates. we use back-propagation to
learn all of the weights in our model.

all

the parameters of our model are learned
jointly without pre-training. these parameters in-
clude the weights of the character-level embeddings,
id98s, and lstms. weights are randomly initial-
ized before training. for the ith layer in our network,
each weight is sampled from a uniform distribution
between     1|li| and 1|li|, where |li| is the number of
weights in layer i.

4 dataset and experimental settings
we evaluate the proposed model on the simpleques-
tions dataset (bordes et al., 2015). the dataset con-
sists of 108,442 single-relation questions and their
corresponding (topic entity, predicate, answer en-
tity) triples from freebase.
it is split into 75,910
train, 10,845 validation, and 21,687 test questions.
only 10,843 of the 45,335 unique words in entity

aliases and 886 out of 1,034 unique predicates in
the test set were present in the train set. for the
proposed dataset, there are two evaluation settings,
called fb2m and fb5m, respectively. the former
uses a kb for candidate generation which is a sub-
set of freebase and contains 2m entities, while the
latter uses subset of freebase with 5m entities.

in our experiments, the memory neural networks
(memnns) proposed in bordes et al. (2015) serve
as the baselines. for training, in addition to the 76k
questions in the training set, the memnns use 3k
training questions from webquestions (berant et al.,
2013), 15m paraphrases from wikianswers (fader
et al., 2013), and 11m and 12m automatically gener-
ated questions from the kb for the fb2m and fb5m
settings, respectively.
in contrast, our models are
trained only on the 76k questions in the training set.
for our model, both layers of the lstm-based
question encoder have size 200. the hidden layers
of the lstm-based decoder have size 100, and the
id98s for entity and predicate embeddings have a
hidden layer of size 200 and an output layer of size
100. the id98s for entity and predicate embeddings
use a receptive    eld of size 4,    = 5, and m = 100.
we train the models using rmsprop with a learning
rate of 1e   4.

in order to make the input character sequence
long enough to    ll up the receptive    elds of mul-
tiple id98 layers, we pad each predicate or entity
using three padding symbols p , a special start sym-
bol, and a special end symbol. for instance, obama
would become sstartp p p obamap p p send. for
consistency, we apply the same padding to the ques-
tions.

5 results
5.1 end-to-end results on simplequestions
following bordes et al. (2015), we report results
on the simplequestions dataset in terms of sq ac-
curacy, for both fb2m and fb5m settings in ta-
ble 1. sq accuracy is de   ned as the percentage
of questions for which the model generates a cor-
rect kb query (i.e., both the topic entity and predi-
cate are correct). our single character-level model
achieves sq accuracies of 70.9% and 70.3% on
the fb2m and fb5m settings, outperforming the
previous state-of-art results by 8.2% and 6.4%, re-

spectively. compared to the character-level model,
which only has 1.2m parameters, our word-level
model has 19.9m parameters, and only achieves a
best sq accuracy of 53.9%. in addition, in contrast
to previous work, the oov issue is much more se-
vere for our word-level model, since we use no data
augmentation to cover entities unseen in the train
set.

5.2 ablation and embedding experiments
we carry out ablation studies in sections 5.2.1 and
5.2.2 through a set of random-sampling experi-
ments. in these experiments, for each question, we
randomly sample 200 entities and predicates from
the test set as noise samples. we then mix the gold
entity and predicate into these negative samples, and
evaluate the accuracy of our model in predicting the
gold predicate or entity from this mixed set.
5.2.1 character-level vs. word-level models

we    rst explore using word-level models as an al-
ternative to character-level models to construct em-
beddings for questions, entities and predicates.

both word-level and character-level models per-
form comparably well when predicting the predi-
cate, reaching an accuracy of around 80% (table
3). however, the word-level model has considerable
dif   culty generalizing to unseen entities, and is only
able to predict 45% of the entities accurately from
the mixed set. these results clearly demonstrate that
the oov issue is much more severe for entities than
predicates, and the dif   culty word-level models have
when generalizing to new entities.

in contrast, character-level models have no such
issues, and achieve a 96.6% accuracy in predicting
the correct entity on the mixed set. this demon-
strates that character-level models encode the se-
mantic representation of entities and can match en-
tity aliases in a kb with their mentions in natural
language questions.
5.2.2 depth ablation study

we also study the impact of the depth of neural
networks in our model. the results are presented
in table 2. in the ablation experiments we compare
the performance of a single-layer lstm to a two-
layer lstm to encode the question, and a single-
layer vs. two-layer id98 to encode the kb entries.

# of lstm layers
2
2
1
1

# of id98 layers
2
1
2
1

joint accuracy predicate accuracy entity accuracy
78.3
77.7
71.5
72.2

80.0
79.4
73.9
74.7

96.6
96.8
95.0
94.9

table 2: results for a random sampling experiment where we varied the number of layers used for convolutions and
the question-encoding lstm. we terminated training models after 14 epochs and 3 days on a gpu.

embedding type
character
word

joint accuracy predicate accuracy entity accuracy
78.3
37.6

80.0
78.8

96.6
45.5

table 3: results for a random sampling experiment where we varied the embedding type (word vs. character-level).
we used 2 layered-lstms and id98s for all our experiments. our models were trained for 14 epochs and 3 days on a
gpu.

we    nd that a two-layer lstm boosts joint accu-
racy by over 6%. the majority of accuracy gains are
a result of improved predicate predictions, possibly
because entity accuracy is already saturated in this
experimental setup.

5.3 attention mechanisms
in order to further understand how the model per-
forms id53, we visualize the attention
distribution over question characters in the decoding
process. in each sub-   gure of figure 2, the x-axis
is the character sequence of the question, and the y-
axis is the attention weight distribution {  i}. the
blue curve is the attention distribution when gener-
ating the entity, and green curve is the attention dis-
tribution when generating the predicate.

interestingly, as the examples show, the attention
distribution typically peaks at empty spaces. this
indicates that the character-level model learns that a
space de   nes an ending point of a complete linguis-
tic unit. that is, the hidden state of the lstm en-
coder at a space likely summarizes content about the
character sequence before that space, and therefore
contains important semantic information that the de-
coder needs to attend to.

also, we observe that entity attention distribu-
tions are usually less sharp and span longer portions
of words, such as    john    or    rutters   , than predicate
attention distributions (e.g., figure 2a). for enti-
ties, semantic information may accumulate gradu-
ally when seeing more and more characters, while
for predicates, semantic information will become

clear only after seeing the complete word. for ex-
ample, it may only be clear that characters such as
   song by    refer to a predicate after a space, as op-
posed to the name of a song such as    song bye bye
love    (figures 2a, 2b).
in contrast, a sequence of
characters starts to become a likely entity after see-
ing an incomplete name such as    joh    or    rutt   .

in addition, a character-level model can identify
entities whose english aliases were never seen in
training, such as    phrenology    (figure 2d). the
model apparently learns that words ending with the
suf   x    nology    are likely entity mentions, which is
interesting because it reads in the input one character
at a time.

furthermore, as observed in figure 2d, the atten-
tion model is capable of attending disjoint regions of
the question and capture the mention of a predicate
that is interrupted by entity mentions. we also note
that predicate attention often peaks at the padding
symbols after the last character of the question, pos-
sibly because sentence endings carry extra informa-
tion that further help disambiguate predicate men-
tions.
in certain scenarios, the network may only
have suf   cient information to build a semantic rep-
resentation of the predicate after being ensured that
it reached the end of a sentence.

finally, certain words in the question help identify
both the entity and the predicate. for example, con-
sider the word    university    in the question    what
type of educational institution is eastern new mexico
university    (figure 2c). although it is a part of the
entity mention, it also helps disambiguate the predi-

cate. however, previous id29-based qa
approaches (yih et al., 2015; yih et al., 2014) as-
sume that there is a clear separation between the
predicate and entity mentions in the question.
in
contrast, the proposed model does not need to make
this hard categorization, and attends the word    uni-
versity    when predicting both the entity and predi-
cate.

6 error analysis
we randomly sampled 50 questions where the
best-performing model generated the wrong kb
query and categorized the errors. for 46 out of the
50 examples, the model predicted a predicate with
a very similar alias to the true predicate, i.e.    /mu-
sic/release/track    vs.
   /music/release/track list   .
for 21 out of the 50 examples, the model predicted
the wrong entity, e.g.,    album    vs.    still here    for
the question    what type of album is still here?   .
finally, for 18 of the 50 examples, the model pre-
dicted the wrong entity and predicate, i.e. (   play   ,
   /freebase/equivalent topic/equivalent type   )
for
the question    which instrument does amapola
cabase play?    training on more data, augment-
ing the negative sample set with words from the
question that are not an entity mention, and having
more examples that disambiguate between similar
predicates may ameliorate many of these errors.

7 conclusion
in this paper, we proposed a new character-level,
attention-based encoder-decoder model for question
answering. in our approach, embeddings of ques-
tions, entities, and predicates are all jointly learned
to directly optimize the likelihood of generating the
correct kb query. our approach improved the state-
of-the-art accuracy on the simplequestions bench-
mark signi   cantly, using much less data than pre-
vious work. furthermore, thanks to character-level
modeling, we have a compact model that is robust
to unseen entities. visualizations of the attention
distribution reveal that our model, although built
on character-level inputs, can learn higher-level se-
mantic concepts required to answer a natural lan-
guage question with a structured kb. in the future
we would like to extend our system to handle multi-
relation questions.

a)

b)

c)

d)

figure 2: attention distribution over outputs of a
left-to-right lstm on question characters.

references
[amodei et al.2015] dario amodei, rishita anubhai,
eric battenberg, carl case, jared casper, bryan c.
catanzaro, jingdong chen, mike chrzanowski, adam
coates, greg diamos, erich elsen, jesse engel, linxi
fan, christopher fougner, tony han, awni y. han-
nun, billy jun, patrick legresley, libby lin, sharan
narang, andrew y. ng, sherjil ozair, ryan prenger,
jonathan raiman, sanjeev satheesh, david seetapun,
shubho sengupta, yi wang, zhiqian wang, chong
wang, bo xiao, dani yogatama, jun zhan, and
zhenyao zhu. 2015. deep speech 2: end-to-end
id103 in english and mandarin. corr,
abs/1512.02595.

[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio.
2015. neural machine
translation by jointly learning to align and translate.
in iclr.

[berant and liang2014] j. berant and p. liang. 2014. se-
in association for

mantic parsing via id141.
computational linguistics (acl).

[berant et al.2013] jonathan berant, andrew chou, roy
frostig, and percy liang. 2013. id29
on freebase from question-answer pairs. in empirical
methods in natural language processing (emnlp).
[bordes et al.2014a] antoine bordes, sumit chopra, and
jason weston. 2014a. id53 with sub-
graph embeddings. in proceedings of the 2014 con-
ference on empirical methods in natural language
processing (emnlp), pages 615   620, doha, qatar,
october. association for computational linguistics.

[bordes et al.2014b] antoine bordes, jason weston, and
nicolas usunier.
2014b. open question answer-
ing with weakly supervised embedding models.
in
proceedings of the european conference on machine
learning and knowledge discovery in databases -
volume 8724, ecml pkdd 2014, pages 165   180,
new york, ny, usa. springer-verlag new york, inc.
[bordes et al.2015] antoine bordes, nicolas usunier,
sumit chopra, and jason weston. 2015. large-scale
simple id53 with memory networks. in
proc. nips.

[chung et al.2015] junyoung chung, c   aglar g  ulc  ehre,
kyunghyun cho, and yoshua bengio. 2015. gated
feedback recurrent neural networks. in proceedings of
the 32nd international conference on machine learn-
ing, icml 2015, lille, france, 6-11 july 2015, pages
2067   2075.

[chung et al.2016] junyoung chung, kyunghyun cho,
2016. a character-level de-
and yoshua bengio.
coder without explicit segmentation for neural ma-
chine translation. arxiv preprint arxiv:1603.06147.

[dos santos and gatti2014] cicero dos santos and maira
gatti. 2014. deep convolutional neural networks for
in proceedings of
id31 of short texts.
coling 2014, the 25th international conference on
computational linguistics: technical papers, pages
69   78, dublin, ireland, august. dublin city univer-
sity and association for computational linguistics.

dos

[dos santos et al.2015] c  cero

victor
guimaraes, rj niter  oi, and rio de janeiro. 2015.
boosting id39 with neural char-
acter embeddings. in proceedings of news 2015 the
fifth named entities workshop, page 25.

santos,

[dos santos2014] cicero dos santos. 2014. think posi-
tive: towards twitter id31 from scratch.
in proceedings of the 8th international workshop on
semantic evaluation (semeval 2014), pages 647   651,
dublin, ireland, august. association for computa-
tional linguistics and dublin city university.

[fader et al.2013] anthony fader, luke zettlemoyer, and
oren etzioni. 2013. paraphrase-driven learning for
open id53. in proceedings of the 51st
annual meeting of the association for computational
linguistics (volume 1: long papers), pages 1608   
1618, so   a, bulgaria, august. association for com-
putational linguistics.

[fader et al.2014] anthony fader, luke zettlemoyer, and
oren etzioni. 2014. open id53 over
in the 20th
curated and extracted knowledge bases.
acm sigkdd international conference on knowl-
edge discovery and data mining, kdd    14, new york,
ny, usa - august 24 - 27, 2014, pages 1156   1165.

[hochreiter and schmidhuber1997] sepp hochreiter and
j  urgen schmidhuber. 1997. long short-term memory.
neural comput., 9(8):1735   1780, november.

[huang et al.2013] po-sen huang, xiaodong he, jian-
feng gao, li deng, alex acero, and larry heck.
2013. learning deep structured semantic models for
in proceedings
web search using clickthrough data.
of the 22nd acm international conference on confer-
ence on information & knowledge management, pages
2333   2338. acm.

[klein et al.2003] dan klein, joseph smarr, huy nguyen,
and christopher d manning. 2003. named entity
recognition with character-level models. in proceed-
ings of the seventh conference on natural language
learning at hlt-naacl 2003-volume 4, pages 180   
183. association for computational linguistics.

[ljube  si  c et al.2014] nikola ljube  si  c, toma  z erjavec,
and darja fi  ser. 2014. standardizing tweets with
character-level machine translation. in proceedings of
the 15th international conference on computational
linguistics and intelligent text processing - volume
8404, cicling 2014, pages 164   175, new york, ny,
usa. springer-verlag new york, inc.

joint conference of the 53rd annual meeting of the
acl and the 7th international joint conference on
natural language processing of the afnlp. acl
association for computational linguistics, july.
[zaremba and sutskever2014] wojciech zaremba

and
ilya sutskever. 2014. learning to execute. arxiv
preprint arxiv:1410.4615.

[zhang et al.2015] xiang zhang, junbo zhao, and yann
lecun. 2015. character-level convolutional networks
for text classi   cation. in c. cortes, n. d. lawrence,
d. d. lee, m. sugiyama, and r. garnett, editors, ad-
vances in neural information processing systems 28,
pages 649   657. curran associates, inc.

[santos and zadrozny2014] cicero d santos and bianca
zadrozny. 2014. learning character-level represen-
tations for part-of-speech tagging. in proceedings of
the 31st international conference on machine learn-
ing (icml-14), pages 1818   1826.

[shen et al.2014] yelong shen, xiaodong he, jianfeng
gao, li deng, and gregoire mesnil. 2014. a latent
semantic model with convolutional-pooling structure
for information retrieval. cikm, november.

[sukhbaatar et al.2015] sainbayar sukhbaatar, jason we-
ston, rob fergus, et al. 2015. end-to-end memory
networks. in advances in neural information process-
ing systems, pages 2431   2439.

[sutskever et al.2014] ilya sutskever, oriol vinyals, and
quoc v le. 2014. sequence to sequence learning with
in advances in neural information
neural networks.
processing systems, pages 3104   3112.

[tang and mooney2001] lappoon r tang and ray-
mond j mooney. 2001. using multiple clause con-
structors in inductive logic programming for semantic
in machine learning: ecml 2001, pages
parsing.
466   477. springer.

[venugopalan et al.2015] subhashini venugopalan, hui-
juan xu, jeff donahue, marcus rohrbach, raymond
mooney, and kate saenko. 2015. translating videos
to natural language using deep recurrent neural net-
works. in proceedings of the 2015 conference of the
north american chapter of the association for com-
putational linguistics: human language technolo-
gies, pages 1494   1504, denver, colorado, may   june.
association for computational linguistics.

[vinyals et al.2015] oriol vinyals,    ukasz kaiser, terry
koo, slav petrov, ilya sutskever, and geoffrey hinton.
2015. grammar as a foreign language. in c. cortes,
n. d. lawrence, d. d. lee, m. sugiyama, and r. gar-
nett, editors, advances in neural information process-
ing systems 28, pages 2773   2781. curran associates,
inc.

[xu et al.2015] kelvin xu,

jimmy ba, ryan kiros,
kyunghyun cho, aaron courville, ruslan salakhudi-
nov, rich zemel, and yoshua bengio. 2015. show,
attend and tell: neural image id134 with
visual attention. in david blei and francis bach, ed-
itors, proceedings of the 32nd international confer-
ence on machine learning (icml-15), pages 2048   
2057. jmlr workshop and conference proceedings.
[yih et al.2014] wen-tau yih, xiaodong he, and christo-
pher meek. 2014. id29 for single-relation
id53. in proceedings of acl. associa-
tion for computational linguistics, june.

[yih et al.2015] wen-tau yih, ming-wei chang, xi-
aodong he, and jianfeng gao. 2015. semantic pars-
ing via staged query graph generation: question an-
swering with knowledge base. in proceedings of the

