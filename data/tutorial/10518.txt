5
1
0
2

 
r
p
a
3

 

 
 
]
l
c
.
s
c
[
 
 

4
v
8
4
4
6

.

2
1
4
1
:
v
i
x
r
a

accepted as a workshop contribution at iclr 2015

embedding word similarity with
id4

felix hill
university of cambridge
felix.hill@cl.cam.ac.uk

kyunghyun cho
universit  e de montr  eal

s  ebastien jean
universit  e de montr  eal

coline devin
harvey mudd college

yoshua bengio
universit  e de montr  eal, cifar senior fellow

abstract

neural language models learn word representations, or embeddings, that capture
rich linguistic and conceptual information. here we investigate the embeddings
learned by id4 models, a recently-developed class of neu-
ral language model. we show that embeddings from translation models outper-
form those learned by monolingual models at tasks that require knowledge of
both conceptual similarity and lexical-syntactic role. we further show that these
effects hold when translating from both english to french and english to german,
and argue that the desirable properties of translation embeddings should emerge
largely independently of the source and target languages. finally, we apply a
new method for training neural translation models with very large vocabularies,
and show that this vocabulary expansion algorithm results in minimal degrada-
tion of embedding quality. our embedding spaces can be queried in an online
demo and downloaded from our web page. overall, our analyses indicate that
translation-based embeddings should be used in applications that require concepts
to be organised according to similarity and/or lexical function, while monolingual
embeddings are better suited to modelling (nonspeci   c) inter-word relatedness.

1

introduction

it is well known that word representations can be learned from the distributional patterns in corpora.
originally, such representations were constructed by counting word co-occurrences, so that the fea-
tures in one word   s representation corresponded to other words (landauer & dumais, 1997; turney
& pantel, 2010). neural language models, an alternative method for learning word representations,
use language data to optimise (latent) features with respect to a language modelling objective. the
objective can be to predict either the next word given the initial words of a sentence (bengio et al.,
2003; mnih & hinton, 2009; collobert & weston, 2008), or simply a nearby word given a single cue
word (mikolov et al., 2013b; pennington et al., 2014). the representations learned by neural models
(sometimes called embeddings) perform very effectively when applied as pre-trained features in a
range of nlp applications and tasks (baroni et al., 2014).

1

accepted as a workshop contribution at iclr 2015

despite these clear results, it is not well understood how the architecture of neural models affects
the information encoded in their embeddings. here we contribute to this understanding by con-
sidering the embeddings learned by architectures with a very different objective function: neural
machine translation (id4) models. id4 models have recently emerged as an alternative to statis-
tical, phrase-based translation models, and are beginning to achieve impressive translation perfor-
mance (kalchbrenner & blunsom, 2013; devlin et al., 2014; sutskever et al., 2014).
we show that id4 models are not only a potential new direction for machine translation, but are
also an effective means of learning id27s. speci   cally, translation-based embeddings
encode information relating to conceptual similarity (rather than non-speci   c relatedness or associa-
tion) and lexical syntactic role more effectively than embeddings from monolingual neural language
models. we demonstrate that these properties persist when translating between different language
pairs (english-french and english-german). further, based on the observation of subtle language-
speci   c effects in the embedding spaces, we conjecture as to why similarity dominates over other
semantic relations in translation embedding spaces. finally, we discuss a potential limitation of the
application of id4 models for embedding learning - the computational cost of training large vo-
cabularies of embeddings - and show that a novel method for overcoming this issue preserves the
aforementioned properties of translation-based embeddings.

2 learning embeddings with neural language models

all neural language models, including id4 models, learn real-valued embeddings (of speci   ed di-
mension) for words in some pre-speci   ed vocabulary, v , covering many or all words in their training
corpus. at each training step, a    score    for the current training example (or batch) is computed based
on the embeddings in their current state. this score is compared to the model   s objective function,
and the error is backpropagated to update both the model weights (affecting how the score is com-
puted from the embeddings) and the embedding features themselves. at the end of this process, the
embeddings should encode information that enables the model to optimally satisfy its objective.

2.1 monolingual models

in the original neural language model (bengio et al., 2003) and subsequent variants (collobert &
weston, 2008), training examples consist of an ordered sequence of n words, with the model trained
to predict the n-th word given the    rst n   1 words. the model    rst represents the input as an ordered
sequence of embeddings, which it transforms into a single    xed length    hidden    representation,
generally by concatenation and non-linear projection. based on this representation, a id203
distribution is computed over the vocabulary, from which the model can sample a guess for the
next word. the model weights and embeddings are updated to maximise the id203 of correct
guesses for all sentences in the training corpus.
more recent work has shown that high quality id27s can be learned via simpler models
with no nonlinear hidden layer (mikolov et al., 2013b; pennington et al., 2014). given a single word
or unordered window of words in the corpus, these models predict which words will occur nearby.
for each word w in v , a list of training cases (w, c) : c     v is extracted from the training corpus
according to some algorithm. for instance, in the skipgram approach (mikolov et al., 2013b), for
each    cue word    w the    context words    c are sampled from windows either side of tokens of w
in the corpus (with c more likely to be sampled if it occurs closer to w).1 for each w in v , the
model initialises both a cue-embedding, representing the w when it occurs as a cue-word, and a
context-embedding, used when w occurs as a context-word. for a cue word w, the model uses the
corresponding cue-embedding and all context-embeddings to compute a id203 distribution over
v that re   ects the id203 of a word occurring in the context of w. when a training example
(w, c) is observed, the model updates both the cue-id27 of w and the context-word
embeddings in order to increase the id155 of c.

1 subsequent variants use different algorithms for selecting the (w, c) from the training corpus (hill &

korhonen, 2014; levy & goldberg, 2014)

2

accepted as a workshop contribution at iclr 2015

2.2 bilingual representation-learning models

various studies have demonstrated that word representations can also be effectively learned from
bilingual corpora, aligned at the document, paragraph or word level (haghighi et al., 2008; vuli  c
et al., 2011; mikolov et al., 2013a; hermann & blunsom, 2014; chandar et al., 2014). these
approaches aim to represent the words from two (or more) languages in a common vector space
so that words in one language are close to words with similar or related meanings in the other.
the resulting multilingual embedding spaces have been effectively applied to bilingual lexicon ex-
traction (haghighi et al., 2008; vuli  c et al., 2011; mikolov et al., 2013a) and document classi   ca-
tion (klementiev et al.; hermann & blunsom, 2014; chandar et al., 2014; ko  cisk  y et al., 2014).
we focus our analysis on two representatives of this class of (non-id4) bilingual model. the    rst is
that of hermann & blunsom (2014), whose embeddings improve on the performance of a. klemen-
tiev & bhattarai (2012) in document classi   cation applications. as with the id4 models introduced
in the next section, this model can be trained directly on bitexts aligned only at the sentence rather
than word level. when training, for aligned sentences se and sf in different languages, the model
computes representations re and rf by summing the embeddings of the words in se and sf re-
spectively. the embeddings are then updated to minimise the divergence between re and rf (since
they convey a common meaning). a noise-contrastive id168 ensures that the model does not
arrive at trivial (e.g. all zero) solutions to this objective. hermann & blunsom (2014) show that,
despite the lack of prespeci   ed word alignments, words in the two languages with similar meanings
converge in the bilingual embedding space.2
the second model we examine is that of faruqui & dyer (2014). unlike the models described above,
faruqui & dyer (2014) showed explicitly that projecting id27s from two languages
(learned independently) into a common vector space can favourably in   uence the orientation of
id27s when considered in their monolingual subspace; i.e relative to other words in
their own language. in contrast to the other models considered in this paper, the approach of faruqui
& dyer (2014) requires bilingual data to be aligned at the word level.

2.3 id4 models

the objective of id4 is to generate an appropriate sentence in a target language st given a sentence
ss in the source language (see, e.g., kalchbrenner & blunsom, 2013; sutskever et al., 2014). as a
by-product of learning to meet this objective, id4 models learn distinct sets of embeddings for the
vocabularies vs and vt in the source and target languages respectively.
observing a training case (ss, st), these models represent ss as an ordered sequence of embeddings
of words from vs. the sequence for ss is then encoded into a single representation rs.3 finally,
by referencing the embeddings in vt, rs and a representation of what has been generated thus far,
the model decodes a sentence in the target language word by word. if at any stage the decoded word
does not match the corresponding word in the training target st, the error is recorded. the weights
and embeddings in the model, which together parameterise the encoding and decoding process, are
updated based on the accumulated error once the sentence decoding is complete.
although id4 models can differ in their low-level architecture (kalchbrenner & blunsom, 2013;
cho et al., 2014; bahdanau et al., 2014), the translation objective exerts similar pressure on the em-
beddings in all cases. the source language embeddings must be such that the model can combine
them to form single representations for ordered sequences of multiple words (which in turn must en-
able the decoding process). the target language embeddings must facilitate the process of decoding
these representations into correct target-language sentences.

2the models of chandar et al. (2014) and hermann & blunsom (2014) both aim to minimise the divergence
between source and target language sentences represented as sums of id27s. because of these
similarities, we do not compare with both in this paper.

3alternatively, subsequences (phrases) of ss may be encoded at this stage in place of the whole sen-

tence (bahdanau et al., 2014).

3

accepted as a workshop contribution at iclr 2015

3 experiments

to learn translation-based embeddings, we trained two different id4 models. the    rst is the id56
encoder-decoder (id56enc, cho et al., 2014), which uses a recurrent-neural-network to encode all of
the source sentence into a single vector on which the decoding process is conditioned. the second is
the id56 search architecture (bahdanau et al., 2014), which was designed to overcome limitations
exhibited by the id56 encoder-decoder when translating very long sentences. id56 search includes
a attention mechanism, an additional feed-forward network that learns to attend to different parts
of the source sentence when decoding each word in the target sentence.4 both models were trained
on a 348m word corpus of english-french sentence pairs or a 91m word corpus of english-german
sentence pairs.5
to explore the properties of bilingual embeddings learned via objectives other than direct trans-
lation, we trained the bicvm model of hermann & blunsom (2014) on the same data, and also
downloaded the projected embeddings of faruqui & dyer (2014), fd, trained on a bilingual cor-
pus of comparable size (    300 million words per language).6 finally, for an initial comparison
with monolingual models, we trained a conventional skipgram model (mikolov et al., 2013b) and
its glove variant (pennington et al., 2014) for the same number of epochs on the english half of the
bilingual corpus.
to analyse the effect on embedding quality of increasing the quantity of training data, we then
trained the monolingual models on increasingly large random subsamples of wikipedia text (up to a
total of 1.1bn words). lastly, we extracted embeddings from a full-sentence language model (cw,
collobert & weston, 2008), which was trained for several months on the same wikipedia 1bn word
corpus. note that increasing the volume of training data for the bilingual (and id4) models was
not possible because of the limited size of available sentence-aligned bitexts.

3.1 similarity and relatedness modelling

as in previous studies (agirre et al., 2009; bruni et al., 2014; baroni et al., 2014), our initial eval-
uations involved calculating pairwise (cosine) distances between embeddings and correlating these
distances with (gold-standard) human judgements of the strength of relationships between concepts.
for this we used three different gold standards: wordsim-353 (agirre et al., 2009), men (bruni
et al., 2014) and siid113x-999 (hill et al., 2014). importantly, there is a clear distinction between
wordsim-353 and men, on the one hand, and siid113x-999, on the other, in terms of the semantic
relationship that they quantify. for both wordsim-353 and men, annotators were asked to rate
how related or associated two concepts are. consequently, pairs such as [clothes-closet], which are
clearly related but ontologically dissimilar, have high ratings in wordsim-353 and men. in con-
trast, such pairs receive a low rating in siid113x-999, where only genuinely similar concepts, such as
[coast- shore], receive high ratings.
to reproduce the scores in siid113x-999, models must thus distinguish pairs that are similar from
those that are merely related. in particular, this requires models to develop sensitivity to the distinc-
tion between synonyms (similar) and antonyms (often strongly related, but highly dissimilar).7
table 1 shows the correlations of id4 (english-french) embeddings, other bilingually-trained
embeddings and monolingual embeddings with these three lexical gold-standards. id4 outper-
form monolingual embeddings, and, to a lesser extent, the other bilingually trained embeddings,
on siid113x-999. however, this clear advantage is not observed on men and wordsim-353, where
the projected embeddings of faruqui & dyer (2014), which were tuned for high performance on
wordsim-353, perform best. given the aforementioned differences between the evaluations, this

4access to source code and limited gpu time prevent us from training and evaluating the embeddings
from other id4 models such as that of (kalchbrenner & blunsom, 2013), (devlin et al., 2014) and sutskever
et al. (2014). the underlying principles of encoding-decoding also apply to these models, and we expect the
embeddings would exhibit similar properties to those analysed here.

5these corpora were produced from the wmt    14 parallel data after conducting the data-selection proce-

dure described by cho et al. (2014).

6available from http://www.cs.cmu.edu/  mfaruqui/soft.html. the available embeddings

were trained on english-german aligned data, but the authors report similar to for english-french.
7for a more detailed discussion of the similarity/relatedness distinction, see (hill et al., 2014).

4

accepted as a workshop contribution at iclr 2015

wordsim-353

siid113x-999
siid113x-333

  
men   
  
  
toefl %
syn/antonym %

monolingual models

biling. models

id4 models

skipgram glove cw fd bicvm id56enc id56search
0.58
0.62
0.49
0.45
0.93
0.74

0.69
0.78
0.39
0.24
0.84
0.76

0.52
0.44
0.29
018
0.75
0.69

0.51
0.60
0.28
0.07
0.64
0.75

0.50
0.45
0.36
0.34
0.87
0.70

0.55
0.71
0.32
0.18
0.78
0.72

0.57
0.63
0.52
0.49
0.93
0.79

table 1: id4 embeddings (id56enc and id56search) clearly outperform alternative embedding-
learning architectures on tasks that require modelling similarity (below the dashed line), but not on
tasks that re   ect relatedness. bilingual embedding spaces learned without the translation objective
are somewhere between these two extremes.

teacher

eaten

britain

skipgram
vocational
in-service
college
spoiled
squeezed
cooked
northern
great
ireland

glove
student
pupil
university
cooked
eat
eating
ireland
kingdom
great

cw
student
tutor
mentor
baked
peeled
cooked
luxembourg
belgium
madrid

fd
elementary
school
classroom
ate
meal
salads
uk
british
london

bicvm id56enc id56search
instructor
professor
educator
ate
consumed
eat
england
uk
syria

professor
instructor
trainer
ate
consumed
tasted
uk
british
america

faculty
professors
teach
eating
eat
baking
uk
british
england

table 2: nearest neighbours (excluding plurals) in the embedding spaces of different models. all
models were trained for 6 epochs on the translation corpus except cw and fd (as noted previ-
ously). id4 embedding spaces are oriented according to similarity, whereas embeddings learned
by monolingual models are organized according to relatedness. the other bilingual model bicvm
also exhibits a notable focus on similarity.

suggests that bilingually-trained embeddings, and id4 based embeddings in particular, better cap-
ture similarity, whereas monolingual embedding spaces are orientated more towards relatedness.
to test this hypothesis further, we ran three more evaluations designed to probe the sensitivity of
models to similarity as distinct from relatedness or association. in the    rst, we measured perfor-
mance on siid113x-assoc-333 (hill et al., 2014). this evaluation comprises the 333 most related
pairs in siid113x-999, according to an independent empirical measure of relatedness (free associate
generation (nelson et al., 2004)). importantly, the pairs in siid113x-assoc-333, while all strongly
related, still span the full range of similarity scores.8 therefore, the extent to which embeddings can
model this data re   ects their sensitivity to the similarity (or dissimilarity) of two concepts, even in
the face of a strong signal in the training data that those concepts are related.
the toefl synonym test is another similarity-focused evaluation of embedding spaces. this test
contains 80 cue words, each with four possible answers, of which one is a correct synonym (lan-
dauer & dumais, 1997). we computed the proportion of questions answered correctly by each
model, where a model   s answer was the nearest (cosine) neighbour to the cue word in its vocabu-
lary.9 note that, since toefl is a test of synonym recognition, it necessarily requires models to
recognise similarity as opposed to relatedness.
finally, we tested how well different embeddings enabled a supervised classi   er to distinguish
between synonyms and antonyms, since synonyms are necessarily similar and people often    nd
antonyms, which are necessarily dissimilar, to be strongly associated. for 744 word pairs hand-
selected as either synonyms or antonyms,10 we presented a gaussian id166 with the concatenation
of the two id27s. we evaluated accuracy using 10-fold cross-validation.

8the most dissimilar pair in siid113x-assoc-333 is [shrink,grow] with a score of 0.23. the highest is [van-

ish,disappear] with 9.80.

9to control for different vocabularies, we restricted the effective vocabulary of each model to the intersection

of all model vocabularies, and excluded all questions that contained an answer outside of this intersection.

10available online at http://www.cl.cam.ac.uk/  fh295/.

5

accepted as a workshop contribution at iclr 2015

as shown in table 1, with these three additional similarity-focused tasks we again see the same
pattern of results. id4 embeddings outperform other bilingually-trained embeddings which in
turn outperform monolingual models. the difference is particularly striking on siid113x-assoc-333,
which suggests that the ability to discern similarity from relatedness (when relatedness is high) is
perhaps the most clear distinction between the bilingual spaces and those of monolingual models.
these conclusions are also supported by qualitative analysis of the various embedding spaces. as
shown in table 2, in the id4 embedding spaces the nearest neighbours (by cosine distance) to
concepts such as teacher are genuine synonyms such as professor or instructor. the bilingual ob-
jective also seems to orientate the non-id4 embeddings towards semantic similarity, although some
purely related neighbours are also oberved. in contrast, in the monolingual embedding spaces the
neighbours of teacher include highly related but dissimilar concepts such as student or college.

3.2

importance of training data quantity

in previous work, monolingual models were trained on corpora many times larger than the english
half of our parallel translation corpus. indeed, the ability to scale to large quantities of training data
was one of the principal motivations behind the skipgram architecture (mikolov et al., 2013b). to
check if monolingual models simply need more training data to capture similarity as effectively as
bilingual models, we therefore trained them on increasingly large subsets of wikipedia.11 as shown
in figure 1, this is not in fact the case. the performance of monolingual embeddings on similarity
tasks remains well below the level of the id4 embeddings and somewhat lower than the non-mt
bilingual embeddings as the amount of training data increases.

figure 1: the effect of increasing the amount of training data on the quality of monolingual embed-
dings, based on similarity-based evaluations (siid113x-999) and two relatedness-based evaluations
(men and wordsim-353). et in the legend indicates models trained on the english half of the
translation corpus. wiki indicates models trained on wikipedia.

3.3 analogy resolution

lexical analogy questions have been used as an alternative way of evaluating word representations.
in this task, models must identify the correct answer (girl) when presented with analogy questions
such as    man is to boy as woman is to ?   . it has been shown that skipgram-style models are surpris-
ingly effective at answering such questions (mikolov et al., 2013b). this is because, if m, b and w
are skigram-style embeddings for man, boy and woman respectively, the correct answer is often the
nearest neighbour in the vocabulary (by cosine distance) to the vector v = w + b     m.
we evaluated embeddings on analogy questions using the same vector-algebra method as mikolov
et al. (2013b). as in the previous section, for fair comparison we excluded questions containing a

11we did not do the same for our translation models because sentence-aligned bilingual corpora of compa-

rable size do not exist.

6

lllllllllll0.30.40.50.65001000corpus size (million words)correlationsiid113x   999lllllllllll0.40.50.60.70.85001000corpus size (million words)menlllllllllll0.40.50.60.70.85001000corpus size (million words)correlationwordsim   353llid56encid56searchfdbicvmskipgrametgloveetskipgramwikiglovewikicwaccepted as a workshop contribution at iclr 2015

word outside the intersection of all model vocabularies, and restricted all answer searches to this
reduced vocabulary. this left 11,166 analogies. of these, 7219 are classed as    syntactic   , in that
they exemplify mappings between parts-of-speech or syntactic roles (e.g. fast is to fastest as heavy
is to heaviest), and 3947 are classed as    semantic    (ottawa is to canada as paris is to france), since
successful answering seems to rely on some (world) knowledge of the concepts themselves.
as shown in fig. 2, id4 embeddings yield relatively poor answers to semantic analogy questions
compared with monolingual embeddings and the bilingual embeddings fd (which are projections of
similar monolingual embeddings).12 it appears that the translation objective prevents the embedding
space from developing the same linear, geometric regularities as skipgram-style models with respect
to semantic organisation. this also seems to be true of the embeddings from the full-sentence
language model cw. further, in the case of the glove and fd models this advantage seems to be
independent of both the domain and size of the training data, since embeddings from these models
trained on only the english half of the translation corpus still outperform the translation embeddings.
on the other hand, id4 embeddings are effective for answering syntactic analogies using the vec-
tor algebra method. they perform comparably to or even better than monolingual embeddings when
trained on less data (albeit bilingual data). it is perhaps unsurprising that the translation objective
incentivises the encoding of a high degree of lexical syntactic information, since coherent target-
language sentences could not be generated without knowledge of the parts-of-speech, tense or case
of its vocabulary items. the connection between the translation objective and the embedding of lex-
ical syntactic information is further supported by the fact that embeddings learned by the bilingual
model bicvm do not perform comparably on the syntactic analogy task. in this model, senten-
tial semantics is transferred via a bag-of-words representation, presumably rendering the precise
syntactic information less important.
when considering the two properties of id4 embeddings highlighted by these experiments, namely
the encoding of semantic similarity and lexical syntax, it is worth noting that items in the similarity-
focused evaluations of the previous section (siid113x-999 and toefl) consist of word groups or
pairs that have identical syntactic role. thus, even though lexical semantic information is in general
pertinent to conceptual similarity (levy & goldberg, 2014), the lexical syntactic and conceptual
properties of translation embeddings are in some sense independent of one another.

figure 2: translation-based embeddings perform best on syntactic analogies (run,ran: hide, hid).
monolingual skipgram/glove models are better at semantic analogies (father, man; mother, woman)

4 effect of target language

to better understand why a translation objective yields embedding spaces with particular properties,
we trained the id56 search architecture to translate from english to german.

12the performance of the fd embeddings on this task is higher than that reported by faruqui & dyer (2014)

because we search for answers over a smaller total candidate vocabulary.

7

lllllllllll0.000.250.500.755001000corpus size (million words)accuracy (%)semanticlllllllllll0.000.250.500.755001000corpus size (million words)llid56encid56searchfdbicvmskipgrametgloveetskipgramwikiglovewikicwsyntacticaccepted as a workshop contribution at iclr 2015

wordsim-353

siid113x-999
siid113x-assoc-333

  
men   
  
  

en-
fr
0.60
0.61
0.49
0.45
toefl % 0.90
syn/antonym % 0.72
syntactic analogies % 0.73
semantic analogies % 0.10

en-
de
0.61
0.62
0.50
0.47
0.93
0.70
0.62
0.11

   earned   
gained
won

acquired

gained
deserved

accummulated

   castle   
chateau
palace
fortress

   money   
silver
funds
cash

chateau
palace
padlock

funds
cash

resources

en-fr

en-de

table 3: comparison of embeddings learned by id56 search models translating between english-
french (en-fr) and english-german (en-de) on all semantic evaluations (left) and nearest neigh-
bours of selected cue words (right). bold italics indicate target-language-speci   c effects. evaluation
items and vocabulary searches were restricted to words common to both models.

as shown in table 3 (left side), the performance of the source (english) embeddings learned by this
model was comparable to that of those learned by the english-to-french model on all evaluations,
even though the english-german training corpus (91 million words) was notably smaller than the
english-french corpus (348m words). this evidence shows that the desirable properties of transla-
tion embeddings highlighted thus far are not particular to english-french translation, and can also
emerge when translating to a different language family, with different word ordering conventions.

5 overcoming the vocabulary size problem

a potential drawback to using id4 models for learning id27s is the computational cost
of training such a model on large vocabularies. to generate a target language sentence, id4 models
repeatedly compute a softmax distribution over the target vocabulary. this computation scales with
vocabulary size and must be repeated for each word in the output sentence, so that training models
with large output vocabularies is challenging. moreover, while the same computational bottleneck
does not apply to the encoding process or source vocabulary, there is no way in which a translation
model could learn a high quality source embedding for a word if the plausible translations were out-
side its vocabulary. thus, limitations on the size of the target vocabulary effectively limit the scope
of id4 models as representation-learning tools. this contrasts with the shallower monolingual and
bilingual representation-learning models considered in this paper, which ef   ciently compute a dis-
tribution over a large target vocabulary using either a hierarchical softmax (morin & bengio, 2005)
or approximate methods such as negative sampling (mikolov et al., 2013b; hermann & blunsom,
2014), and thus can learn large vocabularies of both source and target embeddings.
a recently proposed solution to this problem enables id4 models to be trained with larger target
vocabularies (and hence larger meaningful source vocabularies) at comparable computational cost to
training with a small target vocabulary (jean et al., 2014). the algorithm uses (biased) importance
sampling (bengio & s  en  ecal, 2003) to approximate the id203 distribution of words over a
large target vocabulary with a    nite set of distributions over subsets of that vocabulary. despite
this element of approximation in the decoder, extending the effective target vocabulary in this way
signi   cantly improves translation performance, since the model can make sense of more sentences
in the training data and encounters fewer unknown words at test time. in terms of representation
learning, the method provides a means to scale up the id4 approach to vocabularies as large as
those learned by monolingual models. however, given that the method replaces an exact calculation
with an approximate one, we tested how the quality of source embeddings is affected by scaling up
the target language vocabulary in this way.
as shown in table 4, there is no signi   cant degradation of embedding quality when scaling to large
vocabularies with using the approximate decoder. note that for a fair comparison we    ltered these
evaluations to only include items that are present in the smaller vocabulary. thus, the numbers do
not directly re   ect the quality of the additional 470k embeddings learned by the extended vocabulary
models, which one would expect to be lower since they are words of lower frequency. all embed-
dings can be downloaded from http://www.cl.cam.ac.uk/  fh295/, and the embeddings

8

accepted as a workshop contribution at iclr 2015

id56 search id56 search id56 search-lv id56 search-lv

wordsim-353

siid113x-999
siid113x-assoc-333

  
men   
  
  
toefl %
syn/antonym %
syntactic analogies %
semantic analogies %

en-fr
0.60
0.61
0.49
0.45
0.90
0.72
0.73
0.10

en-de
0.61
0.62
0.50
0.47
0.93
0.70
0.62
0.11

en-fr
0.59
0.62
0.51
0.47
0.93
0.74
0.71
0.08

en-de
0.57
0.61
0.50
0.46
0.98
0.71
0.62
0.13

table 4: comparison of embeddings learned by the original (id56 search - 30k french words,
50k german words) and extended-vocabulary (id56 search-lv -500k words) models translating
from english to french (en-fr) and from english to german (en-de). for fair comparisons, all
evaluations were restricted to the intersection of all model vocabularies.

from the smaller vocabulary models can be interrogated at http://lisa.iro.umontreal.
ca/mt-demo/embs/.13

6 how similarity emerges

although id4 models appear to encode both conceptual similarity and syntactic information for
any source and target languages, it is not the case that embedding spaces will always be identical.
interrogating the nearest neighbours of the source embedding spaces of the english-french and
english-german models reveals occasional language-speci   c effects. as shown in table 3 (right
side), the neighbours for the word earned in the english-german model are as one might expect,
whereas the neighbours from the english-french model contain the somewhat unlikely candidate
won. in a similar vein, while the neighbours of the word castle from the english-french model are
unarguably similar, the neighbours from the english-german model contain the word padlock.
these infrequent but striking differences between the english-german and english-french source
embedding spaces indicate how similarity might emerge effectively in id4 models. tokens of
the french verb gagner have (at least) two possible english translations (win and earn). since the
translation model, which has limited encoding capacity, is trained to map tokens of win and earn
to the same place in the target embedding space, it is ef   cient to move these concepts closer in
the source space. since win and earn map directly to two different verbs in german, this effect is
not observed. on the other hand, the english nouns castle and padlock translate to a single noun
(schloss) in german, but different nouns in french. thus, padlock and castle are only close in the
source embeddings from the english-german model.
based on these considerations, we can conjecture that the following condition on the semantic con-
   guration between two language is crucial to the effective induction of lexical similarity.

(1)

for s1 and s2 in the source language, there is some t in the target language such that
there are sentences in the training data in which s1 translates to t and sentences in
which s2 translates to t.

(2)

s1 and s2 are semantically similar.

if and only if

of course, this condition is not true in general. however, we propose that the extent to which it
holds over all possible word pairs corresponds to the quality of similarity induction in the translation
embedding space. note that strong polysemy in the target language, such as gagner = win, earn,

13a different solution to the rare-word problem was proposed by (luong et al., 2014). we do not evaluate

the effects on the resulting embeddings of this method because we lack access to the source code.

9

accepted as a workshop contribution at iclr 2015

can lead to cases in which 1 is satis   ed but 2 is not. the conjecture claims that these cases are
detrimental to the quality of the embedding space (at least with regards to similarity). in practice,
qualitative analyses of the embedding spaces and native speaker intuitions suggest that such cases are
comparatively rare. moreover, when such cases are observed, s1 and s2, while perhaps not similar,
are not strongly dissimilar. this could explain why related but strongly dissimilar concepts such
as antonym pairs do not converge in the translation embedding space. this is also consistent with
qualitative evidence presented by (faruqui & dyer, 2014) that projecting monolingual embeddings
into a bilingual space orientates them to better re   ect the synonymy/antonymy distinction.

7 conclusion

in this work, we have shown that the embedding spaces from id4 models are
orientated more towards conceptual similarity than those of monolingual models, and that transla-
tion embedding spaces also re   ect richer lexical syntactic information. to perform well on similarity
evaluations such as siid113x-999, embeddings must distinguish information pertinent to what con-
cepts are (their function or ontology) from information re   ecting other non-speci   c inter-concept
relationships. concepts that are strongly related but dissimilar, such as antonyms, are particularly
challenging in this regard (hill et al., 2014). consistent with the qualitative observation made by
faruqui & dyer (2014), we suggested how the nature of the semantic correspondence between the
words in languages enables id4 embeddings to distinguish synonyms and antonyms and, more
generally, to encode the information needed to re   ect human intuitions of similarity.
the language-speci   c effects we observed in section 4 suggest a potential avenue for improving
translation and multi-lingual embeddings in future work. first, as the availability of fast gpus for
training grows, we would like to explore the embeddings learned by id4 models that translate
between much more distant language pairs such as english-chinese or english-arabic. for these
language pairs, the word alignment will less monotonic and may result in even more important se-
mantic and syntactic information being encoded in the lexical representation. further, as observed by
both hermann & blunsom (2014) and faruqui & dyer (2014), the bilingual representation learning
paradigm can be naturally extended to update representations based on correspondences between
multiple languages (for instance by interleaving english-french and english-german training ex-
amples). such an approach should smooth out language-speci   c effects, leaving embeddings that
encode only language-agnostic conceptual semantics and are thus more generally applicable. an-
other related challenge is to develop smaller or less complex representation-learning tools that en-
code similarity with as much    delity as id4 models but without the computational overhead. one
promising approach for this is to learn word alignments and id27s jointly (ko  cisk  y et al.,
2014). this approach is effective for cross-lingual document classi   cation, although the authors do
evaluate the monolingual subspace induced by the model.14
not all id27s learned from text are born equal. depending on the application, those
learned by id4 models may have particularly desirable properties. for decades, distributional
semantic models have aimed to exploit firth   s famous distributional hypothesis to induce word
meanings from (monolingual) text. however, the hypothesis also betrays the weakness of the mono-
lingual distributional approach when it comes to learning humah-quality concept representations.
for while it is undeniable that    words which are similar in meaning appear in similar distributional
contexts    (firth, 1957), the converse assertion, which is what really matters, is only sometimes true.

acknowledgments

the authors would like to thank the developers of theano bergstra et al. (2010); bastien et al. (2012).
we acknowledge the support of the following agencies for research funding and computing support:
st john   s college cambridge, nserc, calcul qu  ebec, compute canada, the canada research
chairs and cifar.

14these embeddings are not publicly available and we were unable to re-train them using the source code.

10

accepted as a workshop contribution at iclr 2015

references
a. klementiev, i. titov and bhattarai, b. inducing crosslingual distributed representations of words.

in coling, 2012.

agirre, eneko, alfonseca, enrique, hall, keith, kravalova, jana, pasca, marius, and soroa, aitor.
in

a study on similarity and relatedness using distributional and id138-based approaches.
proceedings of naacl-hlt 2009, 2009.

bahdanau, dzmitry, cho, kyunghyun, and bengio, yoshua. id4 by jointly

learning to align and translate. arxiv:1409.0473 [cs.cl], september 2014.

baroni, marco, dinu, georgiana, and kruszewski, germ  an. don   t count, predict! a systematic
comparison of context-counting vs. context-predicting semantic vectors. in proceedings of the
52nd annual meeting of the association for computational linguistics, volume 1, 2014.

bastien, fr  ed  eric, lamblin, pascal, pascanu, razvan, bergstra, james, goodfellow, ian j., bergeron,
arnaud, bouchard, nicolas, and bengio, yoshua. theano: new features and speed improvements.
deep learning and unsupervised id171 nips 2012 workshop, 2012.

bengio, yoshua and s  en  ecal, jean-s  ebastien. quick training of probabilistic neural nets by impor-

tance sampling. in proceedings of aistats 2003, 2003.

bengio, yoshua, ducharme, r  ejean, vincent, pascal, and janvin, christian. a neural probabilistic

language model. j. mach. learn. res., 3:1137   1155, march 2003.

bergstra, james, breuleux, olivier, bastien, fr  ed  eric, lamblin, pascal, pascanu, razvan, des-
jardins, guillaume, turian, joseph, warde-farley, david, and bengio, yoshua. theano: a cpu
and gpu math expression compiler. in proceedings of the python for scienti   c computing con-
ference (scipy), june 2010. oral presentation.

bruni, elia, tran, nam-khanh, and baroni, marco. multimodal id65. j. artif.

intell. res.(jair), 49:1   47, 2014.

chandar, sarath, lauly, stanislas, larochelle, hugo, khapra, mitesh m., ravindran, balaraman,
raykar, vikas, and saha, amrita. an autoencoder approach to learning bilingual word repre-
sentations. in nips, 2014.

cho, kyunghyun, van merrienboer, bart, gulcehre, caglar, bougares, fethi, schwenk, holger,
and bengio, yoshua. learning phrase representations using id56 encoder-decoder for statistical
machine translation. in proceedings of the empirical methods in natural language processing
(emnlp 2014), october 2014. to appear.

collobert, ronan and weston, jason. a uni   ed architecture for natural language processing: deep
neural networks with multitask learning. in proceedings of the 25th international conference on
machine learning, pp. 160   167. acm, 2008.

devlin, jacob, zbib, rabih, huang, zhongqiang, lamar, thomas, schwartz, richard, and makhoul,
john. fast and robust neural network joint models for id151. in 52nd
annual meeting of the association for computational linguistics, baltimore, md, usa, june,
2014.

faruqui, manaal and dyer, chris. improving vector space word representations using multilingual

correlation. in proceedings of eacl, volume 2014, 2014.

firth, j, r. a synopsis of linguistic theory 1930-1955, pp. 1   32. oxford: philological society, 1957.

haghighi, aria, liang, percy, berg-kirkpatrick, taylor, and klein, dan. learning bilingual lexicons

from monolingual corpora. in acl, volume 2008, pp. 771   779, 2008.

hermann, karl moritz and blunsom, phil. multilingual distributed representations without word
alignment. in proceedings of iclr, april 2014. url http://arxiv.org/abs/1312.
6173.

11

accepted as a workshop contribution at iclr 2015

hill, felix and korhonen, anna. learning abstract concepts from multi-modal data: since you
probably can   t see what i mean. in proceedings of the empirical methods in natural language
processing (emnlp 2014), october 2014.

hill, felix, reichart, roi, and korhonen, anna. siid113x-999: evaluating semantic models with

(genuine) similarity estimation. arxiv preprint arxiv:1408.3456, 2014.

jean, s  ebastien, cho, kyunghyun, memisevic, roland, and bengio, yoshua. on using very large

target vocabulary for id4. arxiv preprint arxiv:1412.2007, 2014.

kalchbrenner, nal and blunsom, phil. recurrent continuous translation models. seattle, october

2013. association for computational linguistics.

klementiev, alexandre, titov, ivan, and bhattarai, binod. inducing crosslingual distributed repre-

sentations of words. coling.

ko  cisk  y, tom  a  s, hermann, karl moritz, and blunsom, phil. learning bilingual word repre-
in proceedings of acl, june 2014. url http:

sentations by marginalizing alignments.
//arxiv.org/abs/1405.0947.

landauer, thomas k and dumais, susan t. a solution to plato   s problem: the latent semantic
analysis theory of acquisition, induction, and representation of knowledge. psychological review,
104(2):211, 1997.

levy, omer and goldberg, yoav. dependency-based id27s. in proceedings of the 52nd

annual meeting of the association for computational linguistics, volume 2, 2014.

luong, thang, sutskever, ilya, le, quoc v, vinyals, oriol, and zaremba, wojciech. addressing the

rare word problem in id4. arxiv preprint arxiv:1410.8206, 2014.

mikolov, tomas, le, quoc v, and sutskever, ilya. exploiting similarities among languages for

machine translation. corr, 2013a.

mikolov, tomas, sutskever, ilya, chen, kai, corrado, greg s, and dean, jeff. distributed repre-
sentations of words and phrases and their compositionality. in advances in neural information
processing systems, pp. 3111   3119, 2013b.

mnih, andriy and hinton, geoffrey e. a scalable hierarchical distributed language model.

advances in neural information processing systems, pp. 1081   1088, 2009.

in

morin, frederic and bengio, yoshua. hierarchical probabilistic neural network language model. in

aistats, volume 5, pp. 246   252. citeseer, 2005.

nelson, douglas l, mcevoy, cathy l, and schreiber, thomas a. the university of south    orida
free association, rhyme, and word fragment norms. behavior research methods, instruments, &
computers, 36(3):402   407, 2004.

pennington, jeffrey, socher, richard, and manning, christopher. glove: global vectors for word
in proceedings of the empirical methods in natural language processing

representation.
(emnlp 2014), october 2014.

sutskever, ilya, vinyals, oriol, and le, quoc v. sequence to sequence learning with neural net-

works. arxiv preprint arxiv:1409.3215, 2014.

turney, peter d and pantel, patrick. from frequency to meaning: vector space models of semantics.

journal of arti   cial intelligence research, 37(1):141   188, 2010.

vuli  c, ivan, de smet, wim, and moens, marie-francine. identifying word translations from com-
parable corpora using latent topic models. in proceedings of the 49th annual meeting of the as-
sociation for computational linguistics: human language technologies: short papers-volume
2, pp. 479   484. association for computational linguistics, 2011.

12

