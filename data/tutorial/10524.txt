structural-id56: deep learning on spatio-temporal graphs

ashesh jain1,2, amir r. zamir2, silvio savarese2, and ashutosh saxena3

cornell university1, stanford university2, brain of things inc.3
ashesh@cs.cornell.edu, {zamir,ssilvio,asaxena}@cs.stanford.edu

6
1
0
2

 
r
p
a
1
1

 

 
 
]

v
c
.
s
c
[
 
 

3
v
8
9
2
5
0

.

1
1
5
1
:
v
i
x
r
a

abstract

deep recurrent neural network architectures, though
remarkably capable at modeling sequences, lack an intu-
itive high-level spatio-temporal structure. that is while
many problems in id161 inherently have an un-
derlying high-level structure and can bene   t from it. spatio-
temporal graphs are a popular tool for imposing such high-
level intuitions in the formulation of real world problems.
in this paper, we propose an approach for combining the
power of high-level spatio-temporal graphs and sequence
learning success of recurrent neural networks (id56s). we
develop a scalable method for casting an arbitrary spatio-
temporal graph as a rich id56 mixture that is feedforward,
fully differentiable, and jointly trainable. the proposed
method is generic and principled as it can be used for trans-
forming any spatio-temporal graph through employing a
certain set of well de   ned steps. the evaluations of the pro-
posed approach on a diverse set of problems, ranging from
modeling human motion to object interactions, shows im-
provement over the state-of-the-art with a large margin. we
expect this method to empower new approaches to problem
formulation through high-level spatio-temporal graphs and
recurrent neural networks.

links: (cid:109)web

1. introduction

the world we live in is inherently structured. it is com-
prised of components that interact with each other in space
and time, leading to a spatio-temporal composition. utiliz-
ing such structures in problem formulation allows domain-
experts to inject their high-level knowledge in learning
frameworks. this has been the incentive for many ef-
forts in id161 and machine learning, such as
logic networks [46], id114 [28], and struc-
tured id166s [26]. structures that span over both space and
time (spatio-temporal) are of particular interest to computer
vision and robotics communities. primarily, interactions be-
tween humans and environment in real world are inherently

figure 1: from st-graph to s-id56 for an example problem. (bot-
tom) shows an example activity (human microwaving food). modeling
such problems requires both spatial and temporal reasoning. (middle) st-
graph capturing spatial and temporal interactions between the human and
the objects. (top) schematic representation of our structural-id56 archi-
tecture automatically derived from st-graph. it captures the structure and
interactions of st-graph in a rich yet scalable manner.

spatio-temporal in nature. for example, during a cooking
activity, humans interact with multiple objects both in space
and through time. similarly, parts of human body (arms,
legs, etc.) have individual functions but work with each
other in concert to generate physically sensible motions.
hence, bringing high-level spatio-temporal structures and
rich sequence modeling capabilities together is of particular
importance for many applications.

the notable success of id56s has proven their capability
on many end-to-end learning tasks [19, 14, 10, 66]. how-
ever, they lack a high-level and intuitive spatio-temporal
structure though they have been shown to be successful
at modeling long sequences [49, 43, 52]. therefore, aug-
menting a high-level structure with learning capability of
id56s leads to a powerful tool that has the best of both
worlds. spatio-temporal graphs (st-graphs) are a popu-
lar [39, 37, 4, 11, 32, 65, 22] general tool for representing
such high-level spatio-temporal structures. the nodes of the
graph typically represent the problem components, and the
edges capture their spatio-temporal interactions. to achieve

t-1tt+1correspondingstructural-id56spatio-temporal graph representationproblem(e.g. activity)activityaffordanceid56activityaffordanceactivityaffordancethe above goal, we develop a generic tool for transforming
an arbitrary st-graph into a feedforward mixture of id56s,
named structural-id56 (s-id56). figure 1 schematically il-
lustrates this process, where a sample spatio-temporal prob-
lem is shown at the bottom, the corresponding st-graph rep-
resentation is shown in the middle, and our id56 mixture
counterpart of the st-graph is shown at the top.

in high-level steps, given an arbitrary st-graph, we    rst
roll it out in time and decompose it into a set of contribut-
ing factor components. the factors identify the indepen-
dent components that collectively determine one decision
and are derived from both edges and nodes of the st-graph.
we then semantically group the factor components and rep-
resent each group using one id56, which results in the de-
sired id56 mixture. the main challenges of this transfor-
mation problem are: 1) making the id56 mixture as rich as
possible to enable learning complex functions, yet 2) keep-
ing the id56 mixture scalable with respect to size of the
input st-graph. in order to make the resulting id56 mix-
ture rich, we liberally represent each spatio-temporal factor
(including node factors, temporal edge factors, and spatio-
temporal edge factors) using one id56. on the other hand,
to keep the overall mixture scalable but not lose the essen-
tial learning capacity, we utilize    factor sharing    (aka clique
templates [54, 42, 53]) and allow the factors with similar se-
mantic functions to share an id56. this results in a rich and
scalable feedforward mixture of id56s that is equivalent to
the provided st-graph in terms of input, output, and spatio-
temporal relationships. the mixture is also fully differen-
tiable, and therefore, can be trained jointly as one entity.

the proposed method is principled and generic as the
transformation is based on a set of well de   ned steps and
it is applicable to any problem that can be formulated as
st-graphs (as de   ned in section 3). several previous works
have attempted solving speci   c problems using a collection
of id56s [49, 12, 61, 10, 5], but they are almost unani-
mously task-speci   c. they also do not utilize mechanisms
similar to factorization or factor sharing in devising their
architecture to ensure richness and scalability.

s-id56 is also modular, as it is enjoying an underlying
high-level structure. this enables easy high-level manip-
ulations which are basically not possible in unstructured
(plain-vanilla) id56s (e.g., we will experimentally show
forming a feasible hybrid human motion by mixing parts
of different motion styles - sec 4.2 ). we evaluate the pro-
posed approach on a diverse set of spatio-temporal prob-
lems (human pose modeling and forecasting, human-object
interaction, and driver decision making), and show signif-
icant improvements over the state of the art on each prob-
lem. we also study complexity and convergence proper-
ties of s-id56 and provide further experimental insights by
visualizing its memory cells that reveals some cells inter-
estingly represent certain semantic operations. the code
of the entire framework that accepts a st-graph as the in-
put and yields the output id56 mixture is available at the

http://asheshjain.org/sid56.

the contribution of this paper are: 1) a generic method
for casting an arbitrary st-graph as a rich, scalable, and
jointly trainable id56 mixture, 2) in defence of structured
approaches, we show s-id56 signi   cantly outperforms its
unstructured (plain-vanilla) id56 counterparts, 3) in de-
fence of id56s, we show on several diverse spatio-temporal
problems that modeling structure with s-id56 outperforms
the non-deep learning based structured counterparts.

2. related work

we give a categorized overview of the related literature.
in general, three main characteristics differentiate our work
from existing techniques: being generic and not restricted to
a speci   c problem, providing a principled method for trans-
forming a st-graph into a scalable rich feedforward id56
mixture, and being jointly trainable.

spatio-temporal problems. problems that require spa-
tial and temporal reasoning are very common in robotics
and id161. examples include human activity
recognition and segmentation from videos [50, 47, 62, 60,
8, 25, 37, 36], context-rich human-object interactions [39,
33, 29, 20, 30], modeling human motion [14, 57, 56]
etc. spatio-temporal reasoning also    nds application in
assistive robots, driver understanding, and object recogni-
tion [65, 22, 44, 23, 11]. in fact most of our daily activities
are spatio-temporal in nature. with growing interests in rich
interactions and robotics, this form of reasoning will be-
come even more important. we evaluate our generic method
on three context-rich spatio-temporal problems: (i) human
motion modeling [14]; (ii) human-object interaction under-
standing [33]; and (iii) driver maneuver anticipation [22].
mixtures of deep architectures. several previous works
build multiple networks and wire them together in order
to capture some complex structure (or interactions) in the
problem with promising results on applications such as ac-
tivity detection, scene labeling, image captioning, and ob-
ject detection [12, 5, 9, 16, 49, 61]. however, such archi-
tectures are mostly hand designed for speci   c problems,
though they demonstrate the bene   t in using a modular deep
architecture. id56s [17] are, on the
other hand, generic feedforward architectures, but for prob-
lems with recursive structure such as parsing of natural sen-
tences and scenes [48]. our work is a remedy for prob-
lems expressed as spatio-temporal graphs. for a new spatio-
temporal problem in hand, all a practitioner needs to do is
to express their intuition about the problem as an st-graph.
deep learning with id114. many works have
addressed deep networks with id114 for struc-
tured prediction tasks. bengio et al. [1] combined id98s
with id48 for hand writing recognition. tompson et
al. [58] jointly train id98 and mrf for human pose esti-
mation. chen et al. [7] use a similar approach for image
classi   cation with general mrf. recently several works

figure 2: an example spatio-temporal graph (st-graph) of a human activity. (a) st-graph capturing human-object interaction. (b) unrolling the st-graph
through edges et . the nodes and edges are labelled with the feature vectors associated with them. (c) our factor graph parameterization of the st-graph.
each node and edge in the st-graph has a corresponding factor.

have addressed end-to-end image segmentation with fully
connected crf [66, 41, 15, 40]. several works follow a
two-stage approach and decouple the deep network from
crf. they have been applied to multiple problems includ-
ing image segmentation, pose estimation, document pro-
cessing [64, 6, 38, 3] etc. all of these works advocate and
well demonstrate the bene   t in exploiting the structure in
the problem together with rich deep architectures. how-
ever, they largely do not address spatio-temporal problems
and the proposed architectures are task-speci   c.

id49 (crf) model dependencies
between the outputs by learning a joint distribution over
them. they have been applied to many applications [34,
13, 45] including st-graphs which are commonly modeled
as spatio-temporal crf [39, 33, 65, 11]. in our approach,
we adopt st-graphs as a general graph representation and
embody it using an id56 mixture architecture. unlike crf,
our approach is not probabilistic and is not meant to model
the joint distribution over the outputs. s-id56 instead learns
the dependencies between the outputs via structural sharing
of id56s between the outputs.

3. structural-id56 architectures

in this section we describe our approach for building
structural-id56 (s-id56) architectures. we start with a
st-graph, decompose it into a set of factor components, then
represent each factor using a id56. the id56s are intercon-
nected in a way that the resulting architecture captures the
structure and interactions of the st-graph.
3.1. representation of spatio-temporal graphs

many applications that require spatial and temporal rea-
soning are modeled using st-graphs [4, 11, 33, 65, 22]. we
represent a st-graph with g = (v,es,et ), whose struc-
ture (v,es) unrolls over time through edges et . figure 2a
shows an example st-graph capturing human-object inter-
actions during an activity. the nodes v     v and edges
e     es     et of the st-graph repeats over time. in partic-
ular, figure 2b shows the same st-graph unrolled through
time.
in the unrolled st-graph, the nodes at a given time
step t are connected with undirected spatio-temporal edge
e = (u, v)     es, and the nodes at adjacent time steps (say
the node u at time t and the node v at time t + 1) are con-

nected with undirected temporal edge iff (u, v)     et .1

v and edges xt

given a st-graph and the feature vectors associated with
e, as shown in figure 2b, the goal
the nodes xt
is to predict the node labels (or real value vectors) yt
v at
each time step t. for instance, in human-object interaction,
the node features can represent the human and object poses,
and edge features can their relative orientation; the node la-
bels represent the human activity and object affordance. la-
v is affected by both its node and its interactions with
bel yt
other nodes (edges), leading to an overall complex system.
such interactions are commonly parameterized with a fac-
tor graph that conveys how a (complicated) function over
the st-graph factorizes into simpler functions [35]. we de-
rive our s-id56 architecture from the factor graph represen-
tation of the st-graph. our factor graph representation has
a factor function   v(yv, xv) for each node and a pairwise
factor   e(ye(1), ye(2), xe) for each edge. figure 2c shows
the factor graph corresponding to the st-graph in 2a. 2

sharing factors between nodes. each factor in the st-
graph has parameters that needs to be learned. instead of
learning a distinct factor for each node, semantically similar
nodes can optionally share factors. for example, all    object
nodes    {u,w} in the st-graph can share the same node fac-
tor and parameters. this modeling choice allows enforcing
parameter sharing between similar nodes. it further gives
the    exibility to handle st-graphs with more nodes without
increasing the number of parameters. for this purpose, we
partition the nodes as cv = {v1, .., vp} where vp is a set of
semantically similar nodes, and they all use the same node
factor   vp. in figure 3a we re-draw the st-graph and assign
same color to the nodes sharing node factors.
partitioning nodes on their semantic meanings leads to a
natural semantic partition of the edges, ce = {e1, .., em},
where em is a set of edges whose nodes form a seman-
tic pair. therefore, all edges in the set em share the
same edge factor   em. for example all    human-object

1for simplicity, the example st-graph in figure 2a considers temporal

edges of the form (v, v)     et .

2note that we adopted factor graph as a tool for capturing interactions
and not modeling the overall function. factor graphs are commonly used
in probabilistic id114 for factorizing joint id203 distribu-
tions. we consider them for general st-graphs and do not establish relations
to its probabilistic and function decomposition properties.

humanobjectobject            unrollhumanobjectobject        +1x    ,        x               e                            x    ,        +1factorgraph(a) spatio-temporal graph representing an activity(b) unrolled through time(c) factor graph parameterizationx    ,        x    ,        x    ,        +1x    ,        +1x        x        e    e    g=(v,e    ,e    )spatio-temporal edgetemporal edge                        +1                  ,          ,                ,          ,                  ,          ,    humanobjectobjectnode factorspatio-temporaledge factortemporaledge factoralgorithm 1 from spatio-temporal graph to s-id56

input g = (v,es,et ), cv = {v1, ..., vp}
output s-id56 graph gr = ({rem},{rvp},er)
1: semantically partition edges ce = {e1, ..., em}
2: find factor components {  vp ,   em} of g
3: represent each   vp with a nodeid56 rvp
4: represent each   em with an edgeid56 rem
5: connect {rem} and {rvp} to form a bipartite graph.
(rem, rvp )     er iff    v     vp, u     v s.t. (u, v)     em
return gr = ({rem},{rvp},er)

figure 3: an example of st-graph to s-id56. (a) the st-graph from figure 2 is redrawn with colors to indicate sharing of nodes and edge factors. nodes
and edges with same color share factors. overall there are six distinct factors: 2 node factors and 4 edge factors. (b) s-id56 architecture has one id56 for
each factor. edgeid56s and nodeid56s are connected to form a bipartite graph. parameter sharing between the human and object nodes happen through
edgeid56 re1 . (c) the forward-pass for human node v involve id56s re1 , re3 and rv1 . in figure 4 we show the detailed layout of this forward-pass.
input features into re1 is sum of human-object edge features xu,v + xv,w. (d) the forward-pass for object node w involve id56s re1 , re2 , re4 and
rv2 . in this forward-pass, the edgeid56 re1 only processes the edge feature xv,w. (best viewed in color)
edges    {(v, u), (v, w)} are modeled with the same edge fac-
tor. sharing factors based on semantic meaning makes the
overall parametrization compact.
in fact, sharing param-
eters is necessary to address applications where the num-
ber of nodes depends on the context. for example, in
human-object interaction the number of object nodes vary
with the environment. therefore, without sharing param-
eters between the object nodes, the model cannot gener-
alize to new environments with more objects. for mod-
eling    exibility, the edge factors are not shared across the
edges in es and et . hence, in figure 3a, object-object
(w, w)     et temporal edge is colored differently from
object-object (u, w)     es spatio-temporal edge.
in order to predict the label of node v     vp, we consider
its node factor   vp, and the edge factors connected to v in
the factor graph. we de   ne a node factor and an edge factor
as neighbors if they jointly affect the label of some node in
the st-graph. more formally, the node factor   vp and edge
factor   em are neighbors, if there exist a node v     vp such
that it connects to both   vp and   em in the factor graph.
we will use this de   nition in building s-id56 architecture
such that it captures the interactions in the st-graph.
3.2. structural-id56 from spatio-temporal graphs
we derive our s-id56 architecture from the factor graph
representation of the st-graph. the factors in the st-graph
operate in a temporal manner, where at each time step the
factors observe (node & edge) features and perform some
computation on those features.
in s-id56, we represent
each factor with an id56. we refer the id56s obtained from
the node factors as nodeid56s and the id56s obtained from
the edge factors as edgeid56s. the interactions represented
by the st-graph are captured through connections between
the nodeid56s and the edgeid56s.

fect the label of some node in the st-graph. to summarize,
in algorithm 1 we show the steps for constructing s-id56
architecture. figure 3b shows the s-id56 for the human
activity represented in figure 3a. the nodeid56s combine
the outputs of the edgeid56s they are connected to (i.e. its
neighbors in the factor graph), and predict the node labels.
the predictions of nodeid56s (eg. rv1 and rv2) interact
through the edgeid56s (eg. re1). each edgeid56 handles
a speci   c semantic interaction between the nodes connected
in the st-grap and models how the interactions evolve over
time. in the next section, we explain the inputs, outputs, and
the training procedure of s-id56.
3.3. training structural-id56 architecture

in order to train the s-id56 architecture, for each node
of the st-graph the features associated with the node are fed
into the architecture. in the forward-pass for node v     vp,
the input into edgeid56 rem is the temporal sequence of
e on the edge e     em, where edge e is inci-
edge features xt
dent to node v in the st-graph. the nodeid56 rvp at each
time step concatenates the node feature xt
v and the outputs
of edgeid56s it is connected to, and predicts the node label.
at the time of training, the errors in prediction are back-
propagated through the nodeid56 and edgeid56s involved
during the forward-pass. that way, s-id56 non-linearly
combines the node and edge features associated with the
nodes in order to predict the node labels.

we denote the id56s corresponding to the node factor
  vp and the edge factor   em as rvp and rem respec-
tively. in order to obtain a feedforward network, we con-
nect the edgeid56s and nodeid56s to form a bipartite graph
gr = ({rem},{rvp},er). in particular, the edgeid56
rem is connected to the nodeid56 rvp iff the factors   em
and   vp are neighbors in the st-graph, i.e. they jointly af-

figure 3c shows the forward-pass through s-id56 for
the human node. figure 4 shows a detailed architecture lay-

    2    1    1    2    3    4        1        2        3        4h-oo-oh-ho-o        1        2humanobjectx    ,    +x    ,    x    ,    x    ,    x    ,    x    ,                    (b) corresponding s-id56(c) forward-pass for human node     (d) forward-pass for object node     nodeid56sedgeid56s(a) spatio-temporal graph with colors indicating sharing of factors        1        2        3        4        1        2humanobject        1        2        3        4        1        2humanobjecthuman(h)object(o)object (o)            id56        =        =figure 4: forward-pass for human node v. shows the architecture
layout corresponding to the figure 3c on unrolled st-graph. (view in color)
out of the same forward-pass. the forward-pass involves
the edgeid56s re1 (human-object edge) and re3 (human-
human edge). since the human node v interacts with two
object nodes {u,w}, we pass the summation of the two edge
features as input to re1. the summation of features, as
opposed to concatenation, is important to handle variable
number of object nodes with a    xed architecture. since the
object count varies with environment, it is challenging to
represent variable context with a    xed length feature vector.
empirically, adding features works better than mean pool-
ing. we conjecture that addition retains the object count
and the structure of the st-graph, while mean pooling av-
erages out the number of edges. the nodeid56 rv1 con-
catenates the (human) node features with the outputs from
edgeid56s, and predicts the activity at each time step.

parameter sharing and structured feature space. an
important aspect of s-id56 is sharing of parameters across
the node labels. parameter sharing between node labels
happen when an id56 is common in their forward-pass. for
example in figure 3c and 3d, the edgeid56 re1 is com-
mon in the forward-pass for the human node and the object
nodes. furthermore, the parameters of re1 gets updated
through back-propagated gradients from both the object and
human nodeid56s. in this way, re1 affects both the human
and object node labels.

since the human node is connected to multiple object
nodes, the input into edgeid56 re1 is always a linear com-
bination of human-object edge features. this imposes an
structure on the features processed by re1. more formally,
the input into re1 is the inner product st f, where f is the
feature matrix storing the edge features xe s.t. e     e1. vec-
tor s captures the structured feature space. its entries are in
{0,1} depending on the node being forward-passed. in the
example above f = [xv,u xv,w]t . for the human node v,
s = [1 1]t , while for the object node u, s = [1 0]t .

4. experiment

we present results on three diverse spatio-temporal prob-
lems to ensure generic applicability of s-id56, shown in

figure 5: diverse spatio-temporal tasks. we apply s-id56 to the fol-
lowing three diverse spatio-temporal problems. (view in color)

figure 5. the applications include: (i) modeling human
motion [14] from motion capture data [21]; (ii) human ac-
tivity detection and anticipation [29, 31]; and (iii) maneuver
anticipation from real-world driving data [22].

4.1. human motion modeling and forecasting

human body is a good example of separate but well
related components.
its motion involves complex spatio-
temporal interactions between the components (arms, legs,
spine), resulting in sensible motion styles like walking, eat-
ing etc. in this experiment, we represent the complex mo-
tion of humans over st-graphs and learn to model them with
s-id56. we show that our structured approach outperforms
the state-of-the-art unstructured deep architecture [14] on
motion forecasting from motion capture (mocap) data. sev-
eral approaches based on gaussian processes [59, 63], re-
stricted id82s (rbms) [57, 56, 51], and
id56s [14] have been proposed to model human motion.
recently, fragkiadaki et al. [14] proposed an encoder-
id56-decoder (erd) which gets state-of-the-art forecasting
results on h3.6m mocap data set [21].
s-id56 architecture for human motion. our s-id56 ar-
chitecture follows the st-graph shown in figure 5a. ac-
cording to the st-graph, the spine interacts with all the body
parts, and the arms and legs interact with each other. the
st-graph is automatically transformed to s-id56 following
section 3.2. the resulting s-id56 have three nodeid56s,
one for each type of body part (spine, arm, and leg), four
edgeid56s for modeling the spatio-temporal interactions
between them, and three edgeid56s for their temporal con-
nections. for edgeid56s and nodeid56s we use fc(256)-
fc(256)-lstm(512) and lstm(512)-fc(256)-fc(100)-
fc(  ) architectures, respectively, with skip input and output
connections [18]. the outputs of nodeid56s are skeleton
joints of different body parts, which are concatenated to re-
construct the complete skeleton. in order to model human
motion, we train s-id56 to predict the mocap frame at time
t + 1 given the frame at time t. similar to [14], we grad-
ually add noise to the mocap frames during training. this
simulates curriculum learning [2] and helps in keeping the
forecasted motion close to the manifold of human motion.
as node features we use the raw joint values expressed as
exponential map [14], and edge features are concatenation

human                        +x        x        x    ,                +1                   sumfeaturesconcatenatefeatures                        +1h  nodeid56h-o edgeid56h-h edgeid56human activity labelx    ,        x    ,        x    ,        +1x        objectobject[        ][        ]+        1        1        1        1        3        3humanobjectobjectobjectspineright arid113ft armright legleft legdriveroutside contextinside context(a) human motion modeling(b) activity detection and anticipation(c) maneuver anticipationtable 1: motion forecasting angle error. {80, 160, 320, 560, 1000}
msecs after the seed motion. the results are averaged over 8 seed motion
sequences for each activity on the test subject.

methods

short-term forecast

80ms

160ms

320ms

long-term forecast
560ms
1000ms

erd [14]
lstm-3lr

1.30
1.18
s-id56 1.08

erd [14]
lstm-3lr

1.66
1.36
s-id56 1.35

erd [14]
lstm-3lr

2.34
2.05
s-id56 1.90

erd [14]
lstm-3lr

2.67
2.25
s-id56 1.67

1.56
1.50
1.34

1.93
1.79
1.71

2.74
2.34
2.30

2.97
2.33
2.03

walking activity

1.84
1.67
1.60

2.00
1.81
1.90

eating activity

2.28
2.29
2.12

2.36
2.49
2.28
smoking activity
3.68
3.24
3.21
discussion activity
3.47
2.48
2.39

3.73
3.10
2.90

3.23
2.45
2.20

2.38
2.20
2.13

2.41
2.82
2.58

3.82
3.42
3.23

2.92
2.93
2.43

figure 6: forecasting eating activity on test subject. on aperiodic
activities, erd and lstm-3lr struggle to model human motion. s-id56,
on the other hand, mimics the ground truth in the short-term and generates
human like motion in the long term. without (w/o) edgeid56s the motion
freezes to some mean standing position. see the video [24].

of the node features. we train all id56s jointly to minimize
the euclidean loss between the predicted mocap frame and
the ground truth. see supplementary material on the project
web page [24] for training details.
evaluation setup. we compare s-id56 with the state-
of-the-art erd architecture [14] on h3.6m mocap data
set [21]. we also compare with a 3 layer lstm architecture
(lstm-3lr) which [14] use as a baseline.3 for motion
forecasting we follow the experimental setup of [14]. we
downsample h3.6m by two and train on 6 subjects and test
on subject s5. to forecast, we    rst feed the architectures
with (50) seed mocap frames, and then forecast the future
(100) frames. following [14], we consider walking, eating,
and smoking activities. in addition to these three, we also
consider discussion activity.

forecasting is specially challenging on activities with
complex aperiodic human motion. in h3.6m data set, sig-
ni   cant parts of eating, smoking, and discussion activities
are aperiodic, while walking activity is mostly periodic. our
evaluation demonstrates the bene   ts of having an underly-
ing structure in three important ways: (i) we present vi-
sualizations and quantitative results on complex aperiodic
activities ([14] evaluates only on periodic walking motion);
(ii) we forecast human motion for almost twice longer than
state-of-the-art [14]. this is very challenging for aperiodic
activities; and    nally (iii) we show s-id56 interestingly

3we reproduce erd and lstm-3lr architectures following [14]. the

authors implementation were not available at the time of submission.

learns semantic concepts, and demonstrate its modularity
by generating hybrid human motion. unstructured deep ar-
chitectures like [14] does not offer such modularity.
qualitative results on motion forecasting. figure 6 shows
forecasting 1000ms of human motion on    eating    activity
    the subject drinks while walking. s-id56 stays close
to the ground-truth in the short-term and generates human
like motion in the long-term. on removing edgeid56s, the
parts of human body become independent and stops inter-
acting through parameters. hence without edgeid56s the
skeleton freezes to some mean position. lstm-3lr suf-
fers with a drifting problem. on many test examples it drifts
to the mean position of walking human ([14] made similar
observations about lstm-3lr). the motion generated by
erd [14] stays human-like in the short-term but it drifts
away to non-human like motion in the long-term. this was
a common outcome of erd on complex aperiodic activ-
ities, unlike s-id56. furthermore, erd produced human
motion was non-smooth on many test examples. see the
video on the project web page for more examples [24].
quantitative evaluation. we follow the evaluation metric
of fragkiadaki et al. [14] and present the 3d angle error
between the forecasted mocap frame and the ground truth
in table 1. qualitatively, erd models human motion bet-
ter than lstm-3lr. however, in the short-term, it does
not mimic the ground-truth as well as lstm-3lr. fragki-
adaki et al. [14] also note this trade-off between erd and
lstm-3lr. on the other hand, s-id56 outperforms both
lstm-3lr and erd on short-term motion forecasting on
all activities. s-id56 therefore mimics the ground truth in
the short-term and generates human like motion in the long
term. in this way it well handles both short and long term
forecasting. due to stochasticity in human motion, long-
term forecasts (> 500ms) can signi   cantly differ from the
ground truth but still depict human-like motion. for this
reason, the long-term forecast numbers in table 1 are not a
fair representative of algorithms modeling capabilities. we
also observe that discussion is one of the most challenging

ground truthlstm-3lrerdw/o edgeid56s-id56100ms200ms300ms500ms1000msseed motionshort-term forecastlong-term forecastfigure 7: s-id56 memory cell visualization. (left) a cell of the leg
nodeid56    res (red) when    putting the leg forward   . (right) a cell of the
arm nodeid56    res for    moving the hand close to the face   . we visualize
the same cell for eating and smoking activities. (see the video [24])
aperiodic activity for all algorithms.
user study. we asked users to rate the motions on a likert
scale of 1 to 3. s-id56 performed best according to the user
study. see supplementary material for the results.
to summarize, unstructured approaches like lstm-3lr
and erd struggles to model long-term human motion on
complex activities. s-id56   s good performance is attributed
to its structural modeling of human motion through the un-
derlying st-graph. s-id56 models each body part sepa-
rately with nodeid56s and captures interactions between
them with edgeid56s in order to produce coherent motions.
4.2. going deeper into structural-id56

we now present several insights into s-id56 architecture
and demonstrate the modularity of the architecture which
enables it to generate hybrid human motions.

visualization of memory cells. we investigated if s-
id56 memory cells represent meaningful semantic sub-
motions. semantic cells were earlier studied on other prob-
lems [27], we are the    rst to present it on a vision task
and human motion.
in figure 7 (left) we show a cell in
the leg nodeid56 learns the semantic motion of moving the
leg forward. the cell    res positive (red color) on the for-
ward movement of the leg and negative (blue color) on its
backward movement. as the subject walks, the cell alter-
natively    res for the right and the left leg. longer activa-
tions in the right leg corresponds to the longer steps taken
by the subject with the right leg. similarly, a cell in the
arm nodeid56 learns the concept of moving hand close to
the face, as shown in figure 7 (right). the same cell    res
whenever the subject moves the hand closer to the face dur-
ing eating or smoking. the cell remains active as long as
the hand stays close to the face. see the video [24].

generating hybrid human motion. we now demon-
strate the    exibility of our modular architecture by gener-
ating novel yet meaningful motions which are not in the
data set. such modularity is of interest and has been ex-
plored to generate diverse motion styles [55]. as a result
of having an underlying high-level structure, our approach
allows us to exchange id56s between the s-id56 architec-
tures trained on different motion styles. we leverage this to
create a novel s-id56 architecture which generates a hybrid

figure 8: (left) generating hybrid motions (see the video [24]). we
demonstrate    exibility of s-id56 by generating a hybrid motion of a    hu-
man jumping forward on one leg   . (right) train and test error. s-id56
generalizes better than erd with a smaller test error.

motion of a human jumping forward on one leg, as shown
in figure 8 (left). for this experiment we modeled the left
and right leg with different nodeid56s. we trained two in-
dependent s-id56 models     a slower human and a faster
human (by down sampling data)     and swapped the left leg
nodeid56 of the trained models. the resulting faster hu-
man, with a slower left leg, jumps forward on the left leg to
keep up with its twice faster right leg.4 unstructured archi-
tectures like erd [14] does not offer this kind of    exibility.
figure 8 (right) examines the test and train error with it-
erations. both s-id56 and erd converge to similar training
error, however s-id56 generalizes better with a smaller test
error for next step prediction. discussion in supplementary.
4.3. human activity detection and anticipation

in this section we present s-id56 for modeling human
activities. we consider the cad-120 [29] data set where
the activities involve rich human-object interactions. each
activity consist of a sequence of sub-activities (e.g. mov-
ing, drinking etc.) and objects affordance (e.g., reachable,
drinkable etc.), which evolves as the activity progresses.
detecting and anticipating the sub-activities and affordance
enables personal robots to assist humans. however, the
problem is challenging as it involves complex interactions
    humans interact with multiple objects during an activity,
and objects also interact with each other (e.g. pouring wa-
ter from    glass    into a    container   ), which makes it a par-
ticularly good    t for evaluating our method. koppula et
al. [31, 29] represents such rich spatio-temporal interactions
with the st-graph shown in figure 5b, and models it with
a spatio-temporal crf. in this experiment, we show that
modeling the same st-graph with s-id56 yields superior re-
sults. we use the node and edges features from [29].

figure 3b shows our s-id56 architecture to model the
st-graph. since the number of objects varies with envi-
ronment, factor sharing between the object nodes and the
human-object edges becomes crucial. in s-id56, rv2 and
re1 handles all the object nodes and the human-object
edges respectively. this allows our    xed s-id56 architec-
ture to handle varying size st-graphs. for edgeid56s we
use a single layer lstm of size 128, and for nodeid56s we
use lstm(256)-softmax(  ). at each time step, the human
nodeid56 outputs the sub-activity label (10 classes), and
the object nodeid56 outputs the affordance (12 classes).

4imagine your motion forward if someone holds your right leg and

runs!

(a) cell #496 fires in response to    moving the leg forward   left legforwardright legforwardleft leg cell activationstimecellseatingwithright armtwo puffsof smokewith left armtime(b) cell #419 fires in response to    moving arm close to the face   one quickpuff of smokewith right armiterationstable 2: maneuver anticipation on 1100 miles of real-world driving data. s-id56 is derived from the st-graph shown in figure 5c. jain et al. [22] use
the same st-graph but models it in a probabilistic frame with aio-id48. the table shows average precision, recall and time-to-maneuver. time-to-maneuver
is the interval between the time of algorithm   s prediction and the start of the maneuver. algorithms are compared on the features from [22].

turns

method

p r (%) re (%)

id166
aio-id48 [22]
s-id56 w/o edgeid56
s-id56

64.7
80.8
75.2
81.2

47.2
75.2
75.3
78.6

time-to-

maneuver (s)

2.40
4.16
3.68
3.94

lane change

all maneuvers

p r (%) re (%)

73.7
83.8
85.4
92.7

57.8
79.2
86.0
84.4

time-to-

maneuver (s)

2.40
3.80
3.53
3.46

p r (%) re (%)

43.7
77.4
78.0
82.2

37.7
71.2
71.1
75.9

time-to-

maneuver (s)

1.20
3.53
3.15
3.75

table 3: results on cad-120 [29]. s-id56 architecture derived from the
st-graph in figure 5b outperforms koppula et al. [31, 29] which models the
same st-graph in a probabilistic framework. s-id56 in multi-task setting
(joint detection and anticipation) further improves the performance.
anticipation f1-score
sub-
object

detection f1-score
object

sub-

activity (%) affordance (%)

activity (%) affordance (%)

method
koppula et al. [31, 29]
s-id56 w/o edgeid56
s-id56
s-id56 (multi-task)

80.4
82.2
83.2
82.4

81.5
82.1
88.7
91.1

37.9
64.8
62.3
65.6

36.7
72.4
80.7
80.9

having observed the st-graph upto time t, the goal is to
detect the sub-activity and affordance labels at the current
time t, and also anticipate their future labels of the time step
t+1. for detection we train s-id56 on the labels of the cur-
rent time step. for anticipation we train the architecture to
predict the labels of the next time step, given the observa-
tions upto the current time. we also train a multi-task ver-
sion of s-id56, where we add two softmax layers to each
nodeid56 and jointly train for anticipation and detection.

table 3 shows the detection and anticipation f1-scores
averaged over all the classes. s-id56 signi   cantly im-
proves over koppula et al. on both anticipation [31] and
detection [29]. on anticipating object affordance s-id56
f1-score is 44% more than [31], and 7% more on detec-
tion. s-id56 does not have any markov assumptions like
spatio-temporal crf, and therefore, it better models the
long-time dependencies needed for anticipation. the ta-
ble also shows the importance of edgeid56s in handling
spatio-temporal components. edgeid56 transfers the infor-
mation from the human to objects, which helps is predicting
the object labels. therefore, s-id56 without the edgeid56s
poorly models the objects. this signi   es the importance of
edgeid56s and also validates our design. finally, training
s-id56 in a multi-task manner works best in majority of
the cases. in figure 9 we show the visualization of an eat-
ing activity. we show one representative frame from each
sub-activity and our corresponding predictions.
s-id56 complexity.
in terms of complexity, we discuss
two aspects as a function of the underlying st-graph: (i) the
number of id56s in the mixture; and (ii) the complexity of
forward-pass. the number of id56s depends on the num-
ber of semantically similar nodes in the st-graph. the over-
all s-id56 architecture is compact because the edgeid56s
are shared between the nodeid56s, and the number of se-
mantic categories are usually few in context-rich applica-
tions. furthermore, because of factor sharing the number of
id56s does not increase if more semantically similar nodes
are added to the st-graph. the forward-pass complexity

figure 9: qualitative result on eating activity on cad-120. shows
multi-task s-id56 detection and anticipation results. for the sub-activity
at time t, the labels are anticipated at time t   1. (zoom in to see the image)
depends on the number of id56s. since the forward-pass
through all edgeid56s and nodeid56s can happen in paral-
lel, in practice, the complexity only depends on the cascade
of two neural networks (edgeid56 followed by nodeid56).
4.4. driver maneuver anticipation

we    nally present s-id56 for another application which
involves anticipating maneuvers several seconds before they
happen. jain et al. [22] represent this problem with the st-
graph shown in figure 5c. they model the st-graph as a
probabilistic id110 (aio-id48 [22]). the st-
graph represents the interactions between the observations
outside the vehicle (eg. the road features), the driver   s ma-
neuvers, and the observations inside the vehicle (eg.
the
driver   s facial features). we model the same st-graph with
s-id56 architecture using the node and edge features from
jain et al. [22]. table 2 shows the performance of differ-
ent algorithms on this task. s-id56 performs better than the
state-of-the-art aio-id48 [22] in every setting. see sup-
plementary material for the discussion and details [24].
5. conclusion

we proposed a generic and principled approach for
combining high-level spatio-temporal graphs with sequence
modeling success of id56s. we make use of factor graph,
and factor sharing in order to obtain an id56 mixture that
is scalable and applicable to any problem expressed over
st-graphs. our id56 mixture captures the rich interac-
tions in the underlying st-graph. we demonstrated signif-
icant improvements with s-id56 on three diverse spatio-
temporal problems including: (i) human motion modeling;
(ii) human-object interaction; and (iii) driver maneuver an-
ticipation. by visualizing the memory cells we showed that
s-id56 learns certain semantic sub-motions, and demon-
strated its modularity by generating novel human motion.5

5we acknowledge nri #1426452, onr-n00014-14-1-0156, muri-

wf911nf-15-1-0479 and panasonic center grant #122282.

nullreachingmovingdrinkingmovingplacingnullstationaryreachablemovabledrinkablemoveableground-truthplaceablestationaryanticipationdetectionobjectaffordanceground-truthanticipationdetectionhumanactivity(t-1)(t-1)time (t)references
[1] y. bengio, y. lecun, and d. henderson. globally trained
handwritten word recognizer using spatial representation,
convolutional neural networks, and id48.
nips, 1994.

[2] y. bengio, j. louradour, r. collobert, and j. weston. cur-

riculum learning. in icml, 2009.

[3] l. bottou, y. bengio, and y. lecun. global training of docu-
ment processing systems using graph transformer networks.
in cvpr, 1997.

[4] w. brendel and s. todorovic. learning spatiotemporal

graphs of human activities. in iccv, 2011.

[5] w. byeon, t. breuel, f. raue, and m. liwicki. scene label-

ing with lstm recurrent neural networks. in cvpr, 2015.

[6] l. c. chen, g. papandreou, i. kokkinos, and k. m. a. l.
yuille. semantic image segmentation with deep convolu-
tional nets and fully connected crfs. arxiv:1412.7062, 2014.
[7] l. c. chen, a. schwing, a. l. yuille, and r. urtasun. learn-

ing deep structured models. in icml, 2015.

[8] m. chen and a. hauptmann. mosift: recognizing human

actions in surveillance videos. 2009.

[9] x. chen and c. l. zitnick. mind   s eye: a recurrent visual
representation for image id134. in cvpr, 2015.
[10] j. donahue, l. a. hendricks, s. guadarrama, m. rohrbach,
s. venugopalan, k. saenko, and t. darrell. long-term recur-
rent convolutional networks for visual recognition and de-
scription. in cvpr, 2015.

[11] b. douillard, d. fox, and f. ramos. a spatio-temporal prob-
abilistic model for multi-sensor multi-class object recogni-
tion. in robotics research. 2011.

[12] y. du, w. wang, and l. wang. hierarchical recurrent neu-
ral network for skeleton based action recognition. in cvpr,
2015.

[13] p. felzenszwalb, d. mcallester, and d. ramanan. a dis-
criminatively trained, multiscale, deformable part model. in
cvpr, 2008.

[14] k. fragkiadaki, s. levine, p. felsen, and j. malik. recurrent

network models for human dynamics. in iccv, 2015.

[15] a. g. s. g and r. urtasun. fully connected deep structured

networks. arxiv:1503.02351, 2015.

[16] r. girshick. fast r-id98. in iccv, 2015.
[17] c. goller and a. kuchler. learning task-dependent dis-
tributed representations by id26 through struc-
ture. in neural networks, ieee, volume 1, 1996.

[18] a. graves. generating sequences with recurrent neural net-

works. arxiv:1308.0850, 2013.

[19] a. graves and n. jaitly. towards end-to-end speech recog-

nition with recurrent neural networks. in icml, 2014.

[20] a. gupta, a. kembhavi, and l. s. davis. observing human-
object interactions: using spatial and functional compatibil-
ity for recognition. ieee pami, 31(10), 2009.

[21] c. ionescu, d. papava, v. olaru, and c. sminchisescu. hu-
man3.6m: large scale datasets and predictive methods for 3d
human sensing in natural environments. ieee pami, 36(7),
2014.

[22] a. jain, h. s. koppula, b. raghavan, s. soh, and a. saxena.
car that knows before you do: anticipating maneuvers via
learning temporal driving models. in iccv, 2015.

[23] a. jain, s. sharma, and a. saxena. beyond geometric path
planning: learning context-driven user preferences via sub-
optimal feedback. in isrr, 2013.

[24] a. jain, a. r. zamir, s. savarese, and a. saxena. s-id56 sup-

plementary video and material. http://asheshjain.
org/sid56.

[25] m. jain, j. c. van gemert, t. mensink, and c. snoek. ob-
jects2action: classifying and localizing actions without any
video example. in iccv, 2015.

[26] t. joachims, t. finley, and c.-n. j. yu. cutting-plane train-
ing of structural id166s. machine learning, 77(1):27   59,
2009.

[27] a. karpathy, j. johnson, and f. f. li. visualizing and under-

standing recurrent networks. arxiv:1506.02078, 2015.

[28] d. koller and n. friedman. probabilistic id114:

principles and techniques. mit press, 2009.

[29] h. koppula, r. gupta, and a. saxena. learning human
ijrr,

activities and object affordances from rgb-d videos.
32(8), 2013.

[30] h. koppula, a. jain, and a. saxena. anticipatory planning

for humanrobot teams. in iser, 2014.

[31] h. koppula and a. saxena. anticipating human activities
in

using object affordances for reactive robotic response.
rss, 2013.

[32] h. koppula and a. saxena. learning spatio-temporal struc-
ture from rgb-d videos for human activity detection and an-
ticipation. in icml, 2013.

[33] h. koppula and a. saxena. anticipating human activities
using object affordances for reactive robotic response. ieee
pami, 2015.

[34] p. kr  ahenb  uhl and v. koltun. ef   cient id136 in fully con-
nected crfs with gaussian edge potentials. arxiv:1210.5644,
2012.

[35] f. r. kschischang, b. j. frey, and h.-a. loeliger. factor
graphs and the sum-product algorithm. id205,
ieee trans., 47(2), 2001.

[36] i. laptev, m. marsza  ek, c. schmid, and b. rozenfeld.
in cvpr,

learning realistic human actions from movies.
2008.

[37] j. lezama, k. alahari, j. sivic, and i. laptev. track to the
future: spatio-temporal video segmentation with long-range
motion cues. in cvpr, 2011.

[38] s. li, w. zhang, and a. b. chan. maximum-margin struc-
tured learning with deep networks for 3d human pose esti-
mation. in iccv, 2015.

[39] y. li and r. nevatia. key object driven multi-category object
recognition, localization and tracking using spatio-temporal
context. in proc. eccv, 2008.

[40] g. lin, c. shen, i. reid, et al. ef   cient piecewise train-
ing of deep structured models for semantic segmentation.
arxiv:1504.01013, 2015.

[41] z. liu, x. li, p. luo, c. c. loy, and x. tang. semantic im-
age segmentation via deep parsing network. in iccv, 2015.
[42] a. mccallum, k. schultz, and s. singh. factorie: proba-
bilistic programming via imperatively de   ned factor graphs.
in nips, 2009.

[43] t. mikolov, a. joulin, s. chopra, m. mathieu, and m. a.
ranzato. learning longer memory in recurrent neural net-
works. arxiv:1412.7753, 2014.

[44] a. pieropan, c. h. ek, and h. kjellstr  om. recognizing ob-
ject affordances in terms of spatio-temporal object-object re-
lationships. in ieee-ras intl. conf. on humanoid robots,
2014.

[45] a. quattoni, m. collins, and t. darrell. conditional random

   elds for object recognition. in nips, 2004.

[46] m. richardson and p. domingos. markov logic networks.

ml, 62(1-2), 2006.

[47] q. shi, l. cheng, l. wang, and a. smola. human action seg-
mentation and recognition using discriminative semi-markov
models. ijcv, 93(1), 2011.

[48] r. socher, c. c. lin, a. y. ng, and c. d. manning. parsing
natural scenes and natural language with recursive neural
networks. in icml, 2011.

[49] n. srivastava, e. mansimov, and r. salakhutdinov. unsu-
in

pervised learning of video representations using lstms.
icml, 2015.

[50] c. sun and r. nevatia. active: activity concept transitions

in video event classi   cation. in iccv, 2013.

[51] i. sutskever, g. hinton, and g. taylor. the recurrent tempo-

ral restricted id82. in nips, 2009.

[52] i. sutskever, o. vinyals, and q. v. le. sequence to sequence

learning with neural networks. in nips, 2014.

[53] c. sutton and a. mccallum. an introduction to conditional

random    elds. machine learning, 4(4), 2011.

[54] b. taskar, p. abbeel, and d. koller. discriminative proba-

bilistic models for relational data. in uai, 2002.

[55] g. taylor and g. e. hinton. factored conditional restricted
id82s for modeling motion style. in icml,
2009.

[56] g. taylor, l. sigal, d. j. fleet, and g. e. hinton. dynamical
binary latent variable models for 3d human pose tracking. in
cvpr, 2010.

[57] g. w. taylor, g. e. hinton, and s. t. roweis. modeling
human motion using binary latent variables. in nips, 2006.
[58] j. tompson, m. stein, y. lecun, and k. perlin. real-time
continuous pose recovery of human hands using convolu-
tional networks. acm tog, 33(5), 2014.

[59] r. urtasun, d. j. fleet, a. geiger, j. popovi  c, t. j. darrell,
and n. d. lawrence. topologically-constrained latent vari-
able models. in icml, 2008.

[60] d. l. vail, m. m. veloso, and j. d. lafferty. conditional

random    elds for activity recognition. in aamas, 2007.

[61] s. venugopalan, h. xu,

j. donahue, m. rohrbach,
translating videos to
language using deep recurrent neural networks.

r. mooney, and k. saenko.
natural
arxiv:1412.4729, 2014.

[62] h. wang and c. schmid. action recognition with improved

trajectories. in cvpr, 2013.

[63] j. m. wang, d. j. fleet, and a. hertzmann. gaussian process
ieee pami, 30(2),

dynamical models for human motion.
2008.

[64] n. zhang, m. paluri, m. a. ranzato, t. darrell, and l. bour-
dev. panda: pose aligned networks for deep attribute model-
ing. in cvpr, 2014.

[65] x. zhang, p. jiang, and f. wang. overtaking vehicle detec-

tion using a spatio-temporal crf. in ivs, ieee, 2014.

[66] s. zheng, s. jayasumana, b. romera-paredes, v. vineet,
z. su, d. du, c. huang, and p. torr. conditional random
   elds as recurrent neural networks. in iccv, 2015.

