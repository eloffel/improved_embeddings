densecap: fully convolutional localization networks for dense captioning

justin johnson   

andrej karpathy   

li fei-fei

department of computer science, stanford university

{jcjohns, karpathy, feifeili}@cs.stanford.edu

5
1
0
2

 

v
o
n
4
2

 

 
 
]

v
c
.
s
c
[
 
 

1
v
1
7
5
7
0

.

1
1
5
1
:
v
i
x
r
a

abstract

we introduce the dense captioning task, which requires a
id161 system to both localize and describe salient
regions in images in natural language. the dense caption-
ing task generalizes id164 when the descriptions
consist of a single word, and image captioning when one
predicted region covers the full image. to address the local-
ization and description task jointly we propose a fully con-
volutional localization network (fcln) architecture that
processes an image with a single, ef   cient forward pass, re-
quires no external regions proposals, and can be trained
end-to-end with a single round of optimization. the archi-
tecture is composed of a convolutional network, a novel
dense localization layer, and recurrent neural network
language model that generates the label sequences. we
evaluate our network on the visual genome dataset, which
comprises 94,000 images and 4,100,000 region-grounded
captions. we observe both speed and accuracy improve-
ments over baselines based on current state of the art ap-
proaches in both generation and retrieval settings.

1. introduction
our ability to effortlessly point out and describe all aspects
of an image relies on a strong semantic understanding of a
visual scene and all of its elements. however, despite nu-
merous potential applications, this ability remains a chal-
lenge for our state of the art visual recognition systems.
in the last few years there has been signi   cant progress
in image classi   cation [38, 26, 52, 44], where the task is
to assign one label to an image. further work has pushed
these advances along two orthogonal directions: first, rapid
progress in id164 [39, 15, 45] has identi   ed mod-
els that ef   ciently identify and label multiple salient regions
of an image. second, recent advances in image captioning
[4, 32, 22, 48, 50, 9, 5] have expanded the complexity of
the label space from a    xed set of categories to sequence of
words able to express signi   cantly richer concepts.

however, despite encouraging progress along the label
density and label complexity axes, these two directions have

   indicates equal contribution.

figure 1. we address the dense captioning task (bottom right) by
generating dense, rich annotations with a single forward pass.

remained separate. in this work we take a step towards uni-
fying these two inter-connected tasks into one joint frame-
work. first, we introduce the dense captioning task (see
figure 1), which requires a model to predict a set of descrip-
tions across regions of an image. id164 is hence
recovered as a special case when the target labels consist
of one word, and image captioning is recovered when all
images consist of one region that spans the full image.

additionally, we develop a fully convolutional local-
ization network architecture (fcln) to address the dense
captioning task. our model is inspired by recent work in
image captioning [48, 22, 32, 9, 5] in that it is composed
of a convolutional neural network followed by a recur-
rent neural network language model. however, drawing on
work in id164 [37], our second core contribution
is to introduce a new dense localization layer. this layer is
fully differentiable and can be inserted into any neural net-
work that processes images to enable region-level training
and predictions. internally, the localization layer predicts a
set of regions of interest in the image and then uses bilin-
ear interpolation [20, 17] to smoothly extract the activations
inside each region.

we evaluate the model on the large-scale visual genome
dataset, which contains 94,000 images and 4,100,000 region
captions. our results show both performance and speed im-
provements over approaches based on previous state of the
art. we make our code and data publicly available to sup-
port further progress on the dense captioning task.

1

classificationcatcaptioninga cat riding a skateboarddetectioncatskateboarddense captioningorange spotted catskateboard with red wheelscat riding a skateboardbrown hardwood flooringlabel densitywhole imageimage regionslabel complexitysinglelabelsequence2. related work
our work draws on recent work in id164, im-
age captioning, and soft spatial attention that allows down-
stream processing of particular regions in the image.
id164. our core visual processing module is a
convolutional neural network (id98) [29, 26], which has
emerged as a powerful model for visual recognition tasks
[38]. the    rst application of these models to dense predic-
tion tasks was introduced in r-id98 [15], where each re-
gion of interest was processed independently. further work
has focused on processing all regions with only single for-
ward pass of the id98 [18, 14], and on eliminating explicit
region proposal methods by directly predicting the bound-
ing boxes either in the image coordinate system [45, 10],
or in a fully convolutional [31] and hence position-invariant
settings [39, 37, 36]. most related to our approach is the
work of ren et al. [37] who develop a region proposal net-
work (rpn) that regresses from anchors to regions of in-
terest. however, they adopt a 4-step optimization process,
while our approach does not require training pipelines. ad-
ditionally, we replace their roi pooling mechanism with a
differentiable, spatial soft attention mechanism [20, 17]. in
particular, this change allows us to backpropagate through
the region proposal network and train the whole model
jointly.
image captioning. several pioneering approaches have
explored the task of describing images with natural lan-
guage [1, 27, 13, 34, 41, 42, 28, 21]. more recent ap-
proaches based on neural networks have adopted recurrent
neural networks (id56s) [49, 19] as the core architectural
element for generating captions. these models have pre-
viously been used in id38 [2, 16, 33, 43],
where they are known to learn powerful long-term inter-
actions [23]. several recent approaches to image caption-
ing [32, 22, 48, 9, 5, 25, 12] rely on a combination of id56
language model conditioned on image information. a re-
cent related approach is the work of xu et al. [50] who use
a soft attention mechanism [6] over regions of the input im-
age with every generated word. our approach to spatial at-
tention is more general in that the network can process ar-
bitrary af   ne regions in the image instead of only discrete
grid positions in an intermediate conv volume. however, for
simplicity, during generation we follow vinyals et al. [48],
where the visual information is only passed to the language
model once on the    rst time step.

finally, the metrics we develop for the dense captioning
task are inspired by metrics developed for image captioning
[47, 8, 4].

3. model
overview. our goal is to design an architecture that jointly
localizes regions of interest and then describes each with
natural language. the primary challenge is to develop a

model that supports end-to-end training with a single step of
optimization, and both ef   cient and effective id136. our
proposed architecture (see figure 2) draws on architectural
elements present in recent work on id164, image
captioning and soft spatial attention to simultaneously ad-
dress these design constraints.
in section 3.1 we    rst describe the components of our
model. then in sections 3.2 and 3.3 we address the loss
function and the details of training and id136.
3.1. model architecture
3.1.1 convolutional network
we use the vgg-16 architecture [40] for its state-of-the-art
it consists of 13 layers of 3    3 con-
performance [38].
volutions interspersed with 5 layers of 2    2 max pooling.
we remove the    nal pooling layer, so an input image of
shape 3    w    h gives rise to a tensor of features of shape

c  w (cid:48)  h(cid:48) where c = 512, w (cid:48) =(cid:4) w

(cid:5), and h(cid:48) =(cid:4) h

(cid:5).

the output of this network encodes the appearance of the
image at a set of uniformly sampled image locations, and
forms the input to the localization layer.

16

16

3.1.2 fully convolutional localization layer
the localization layer receives an input tensor of activa-
tions, identi   es spatial regions of interest and smoothly ex-
tracts a    xed-sized representation from each region. our
approach is based on that of faster r-id98 [37], but we
replace their roi pooling mechanism [14] with bilinear
interpolation [20], allowing our model to propagate gra-
dients backward through the coordinates of predicted re-
gions. this modi   cation opens up the possibility of predict-
ing af   ne or morphed region proposals instead of bounding
boxes [20], but we leave these extensions to future work.
inputs/outputs. the localization layer accepts a tensor of
activations of size c    w (cid:48)    h(cid:48). it then internally selects
b regions of interest and returns three output tensors giving
information about these regions:
1. region coordinates: a matrix of shape b    4 giving

bounding box coordinates for each output region.

2. region scores: a vector of length b giving a con-
   dence score for each output region. regions with
high con   dence scores are more likely to correspond
to ground-truth regions of interest.

3. region features: a tensor of shape b    c    x    y
giving features for output regions; is represented by an
x    y grid of c-dimensional features.

convolutional anchors. similar to faster r-id98 [37],
our localization layer predicts region proposals by regress-
ing offsets from a set of translation-invariant anchors. in
particular, we project each point in the w (cid:48)    h(cid:48) grid of

bp     b/2 positive regions and bn = b     bp negative
regions, sampled uniformly without replacement from the
set of all positive and all negative regions respectively.

figure 2. model overview. an input image is    rst processed a id98. the localization layer proposes regions and smoothly extracts a
batch of corresponding activations using bilinear interpolation. these regions are processed with a fully-connected recognition network
and described with an id56 language model. the model is trained end-to-end with id119.
input features back into the w    h image plane, and con-
sider k anchor boxes of different aspect ratios centered at
this projected point. for each of these k anchor boxes,
the localization layer predicts a con   dence score and four
scalars regressing from the anchor to the predicted box co-
ordinates. these are computed by passing the input feature
map through a 3   3 convolution with 256    lters, a recti   ed
linear nonlinearity, and a 1    1 convolution with 5k    lters.
this results in a tensor of shape 5k    w (cid:48)    h(cid:48) containing
scores and offsets for all anchors.
box regression. we adopt the parameterization of [14]
to regress from anchors to the region proposals. given an
anchor box with center (xa, ya), width wa, and height ha,
our model predicts scalars (tx, ty, tw, th) giving normalized
offsets and log-space scaling transforms, so that the output
region has center (x, y) and shape (w, h) given by

at test time we subsample using greedy non-maximum
suppression (nms) based on the predicted proposal con   -
dences to select the b = 300 most con   dent propoals.
the coordinates and con   dences of the sampled propos-
als are collected into tensors of shape b    4 and b respec-
tively, and are output from the localization layer.
bilinear interpolation. after sampling, we are left with
region proposals of varying sizes and aspect ratios. in order
to interface with the full-connected recognition network and
the id56 language model, we must extract a    xed-size fea-
ture representation for each variably sized region proposal.
to solve this problem, fast r-id98 [14] proposes an roi
pooling layer where each region proposal is projected onto
the w (cid:48)    h(cid:48) grid of convolutional features and divided into
a coarse x    y grid aligned to pixel boundaries by round-
ing. features are max-pooled within each grid cell, result-
ing in an x    y grid of output features.

y = ya + tyha
h = ha exp(hw)

x = xa + txwa
w = wa exp(tw)

(1)
(2)
box sampling. processing a typical image of size w =
720, h = 540 with k = 12 anchor boxes gives rise to
17,280 region proposals. since running the recognition net-
work and the language model for all proposals would be
prohibitively expensive, it is necessary to subsample them.
at training time, we follow the approach of [37] and
sample a minibatch containing b = 256 boxes with at most
b/2 positive regions and the rest negatives. a region is pos-
itive if it has an intersection over union (iou) of at least 0.7
with some ground-truth region; in addition, the predicted
region of maximal iou with each ground-truth region is
positive. a region is negative if it has iou < 0.3 with
all ground-truth regions. our sampled minibatch contains

the roi pooling layer is a function of two inputs: convo-
lutional features and region proposal coordinates. gradients
can be propagated backward from the output features to the
input features, but not to the input proposal coordinates. to
overcome this limitation, we replace the roi pooling layer
with with bilinear interpolation [17, 20].
concretely, given an input feature map u of shape c   
w (cid:48)    h(cid:48) and a region proposal, we interpolate the features
of u to produce an output feature map v of shape c    x   
y . after projecting the region proposal onto u we follow
[20] and compute a sampling grid g of shape x    y    2
associating each element of v with real-valued coordinates

id98image: 3 x w x hconv features:  c x w    x h   region features:b x c x x x yregion codes:b x dlstmstriped gray catcats watching tvlocalization layerconvregion proposals:4k x w    x h   region scores:k x w    x h   conv features: c x w    x h   bilinear samplerregion features:b x 512 x 7 x 7sampling grid:b x x x y x 2samplinggrid generatorbest proposals:b x 4recognition networkinto u. if gi,j = (xi,j, yi,j) then vc,i,j should be equal to u
at (c, xi,j, yi,j); however since (xi,j, yi,j) are real-valued,
we convolve with a sampling kernel k and set

vc,i,j =

uc,i(cid:48),j(cid:48)k(i(cid:48)     xi,j)k(j(cid:48)     yi,j).

(3)

w(cid:88)

h(cid:88)

i(cid:48)=1

j(cid:48)=1

we use bilinear sampling, corresponding to the kernel
k(d) = max(0, 1     |d|). the sampling grid is a linear
function of the proposal coordinates, so gradients can be
propagated backward into predicted region proposal coordi-
nates. running bilinear interpolation to extract features for
all sampled regions gives a tensor of shape b   c    x    y ,
forming the    nal output from the localization layer.

3.1.3 recognition network
the recognition network is a fully-connected neural net-
work that processes region features from the localization
layer. the features from each region are    attened into a vec-
tor and passed through two full-connected layers, each us-
ing recti   ed linear units and regularized using dropout. for
each region this produces a code of dimension d = 4096
that compactly encodes its visual appearance. the codes
for all positive regions are collected into a matrix of shape
b    d and passed to the id56 language model.

in addition, we allow the recognition network one more
chance to re   ne the con   dence and position of each pro-
posal region. it outputs a    nal scalar con   dence of each pro-
posed region and four scalars encoding a    nal spatial off-
set to be applied to the region proposal. these two outputs
are computed as a linear transform from the d-dimensional
code for each region. the    nal box regression uses the same
parameterization as section 3.1.2.

3.1.4 id56 language model
following previous work [32, 22, 48, 9, 5], we use the
region codes to condition an id56 language model [16,
33, 43]. concretely, given a training sequence of to-
kens s1, . . . , st , we feed the id56 t + 2 word vectors
x   1, x0, x1, . . . , xt , where x   1 = id98(i) is the region
code encoded with a linear layer and followed by a relu
non-linearity, x0 corresponds to a special start token, and
xt encode each of the tokens st, t = 1, . . . , t . the id56
computes a sequence of hidden states ht and output vectors
yt using a recurrence formula ht, yt = f (ht   1, xt) (we use
the lstm [19] recurrence). the vectors yt have size |v |+1
where v is the token vocabulary, and where the additional
one is for a special end token. the id168 on the
vectors yt is the average cross id178, where the targets at
times t = 0, . . . , t     1 are the token indices for st+1, and
the target at t = t is the end token. the vector y   1 is
ignored. our tokens and hidden layers have size 512.

at test time we feed the visual information x   1 to the
id56. at each time step we sample the most likely next

token and feed it to the id56 in the next time step, repeating
the process until the special end token is sampled.
3.2. id168
during training our ground truth consists of positive boxes
and descriptions. our model predicts positions and con   -
dences of sampled regions twice: in the localization layer
and again in the recognition network. we use binary logis-
tic lossses for the con   dences trained on sampled positive
and negative regions. for box regression, we use a smooth
l1 loss in transform coordinate space similar to [37]. the
   fth term in our id168 is a cross-id178 term at ev-
ery time-step of the language model.

we normalize all id168s by the batch size and
sequence length in the id56. we searched over an effec-
tive setting of the weights between these contributions and
found that a reasonable setting is to use a weight of 0.1 for
the    rst four criterions, and a weight of 1.0 for captioning.
3.3. training and optimization
we train the full model end-to-end in a single step of opti-
mization. we initialize the id98 with weights pretrained on
id163 [38] and all other weights from a gaussian with
standard deviation of 0.01. we use stochastic gradient de-
scent with momentum 0.9 to train the weights of the con-
volutional network, and adam [24] to train the other com-
ponents of the model. we use a learning rate of 1    10   6
and set   1 = 0.9,   2 = 0.99. we begin    ne-tuning the lay-
ers of the id98 after 1 epoch, and for ef   ciency we do not
   ne-tune the    rst four convolutional layers of the network.
our training batches consist of a single image that has
been resized so that the longer side has 720 pixels. our
implementation uses torch7 [7] and [35]. one mini-batch
runs in approximately 300ms on a titan x gpu and it takes
about three days of training for the model to converge.
4. experiments
dataset. existing datasets that relate images and natural
language either only include full image captions [4, 51],
or ground words of image captions in regions but do not
provide individual region captions [3]. we perform our ex-
periments using the visual genome (vg) region captions
dataset 1 this dataset contains 94,313 images and 4,100,413
snippets of text (43.5 per image), each grounded to a re-
gion of an image. images were taken from the intersection
of ms coco and yfcc100m [46], and annotations were
collected on amazon mechanical turk by asking workers
to draw a bounding box on the image and describe its con-
tent in text. example captions from the dataset include    cats
play with toys hanging from a perch   ,    newspapers are scat-
tered across a table   ,    woman pouring wine into a glass   ,
   mane of a zebra   , and    red light   .

1dataset in submission, obtained via personal communication. we

commit to releasing the relevant parts upon publication.

figure 3. example captions generated and localized by our model on test images. we render the top few most con   dent predictions. on
the bottom row we additionally contrast the amount of information our model generates compared to the full image id56.

preprocessing. we collapse words that appear less than
15 times into a special <unk> token, giving a vocabulary
of 10,497 words. we strip referring phrases such as    there
is...   , or    this seems to be a   . for ef   ciency we discard all
annotations with more than 10 words (7% of annotations).
we also discard all images that have fewer than 20 or more
than 50 annotations to reduce the variation in the number
of regions per image. we are left with 87,398 images; we
assign 5,000 each to val/test splits and the rest to train.

for test time evaluation we also preprocess the ground
truth regions in the validation/test images by merging heav-
ily overlapping boxes into single boxes with several refer-
ence captions. for each image we iteratively select the box
with the highest number of overlapping boxes (based on
iou with threshold of 0.7), and merge these together (by
taking the mean) into a single box with multiple reference
captions. we then exclude this group and repeat the process.
4.1. dense captioning
in the dense captioning task the model receives a single im-
age and produces a set of regions, each annotated with a
con   dence and a caption.
id74. intuitively, we would like our model
to produce both well-localized predictions (as in object de-
tection) and accurate descriptions (as in image captioning).
inspired by id74 in id164 [11,

30] and image captioning [47], we propose to measure the
mean average precision (ap) across a range of thresholds
for both localization and language accuracy. for localiza-
tion we use intersection over union (iou) thresholds .3, .4,
.5, .6, .7. for language we use meteor score thresholds
0, .05, .1, .15, .2, .25. we adopt meteor since this metric
was found to be most highly correlated with human judg-
ments in settings with a low number of references [47]. we
measure the average precision across all pairwise settings
of these thresholds and report the mean ap.

to isolate the accuracy of language in the predicted cap-
tions without localization we also merge ground truth cap-
tions across each test image into a bag of references sen-
tences and evaluate predicted captions with respect to these
references without taking into account their spatial position.
baseline models. following karpathy and fei-fei [22], we
train only the image captioning model (excluding the local-
ization layer) on individual, resized regions. we refer to this
approach as a region id56 model. to investigate the differ-
ences between captioning trained on full images or regions
we also train the same model on full images and captions
from ms coco (full image id56 model).

at test time we consider three sources of region propos-
als. first, to establish an upper bound we evaluate the model
on ground truth boxes (gt). second, similar to [22] we use

a man and a woman sitting at a table with a cake.a train is traveling down the tracks near a forest.a large jetliner flying through a blue sky.a teddy bear with a red bow on it.our model:full image id56:region source
full image id56 [22]
region id56 [22]
fcln on eb [14]
our model (fcln)

language (meteor)
gt
eb
0.209
0.173
0.272
0.221
0.264
0.293
0.305
0.264

rpn
0.197
0.244
0.296
0.273

dense captioning (ap)
eb
2.42
1.07
4.88
5.24

gt
14.11
21.90
26.84
27.03

rpn
4.27
4.26
3.21
5.39

test runtime (ms)

proposals id98+recog

210ms
210ms
210ms
90ms

2950ms
2950ms
140ms
140ms

id56
10ms
10ms
10ms
10ms

total
3170ms
3170ms
360ms
240ms

table 1. dense captioning evaluation on the test set of 5,000 images. the language metric is meteor (high is good), our dense captioning
metric is average precision (ap, high is good), and the test runtime performance for a 720    600 image with 300 proposals is given in
milliseconds on a titan x gpu (ms, low is good). eb, rpn, and gt correspond to edgeboxes [53], region proposal network [37], and
ground truth boxes respectively, used at test time. numbers in gt columns (italic) serve as upper bounds assuming perfect localization.
an external region proposal method to extract 300 boxes for
each test image. we use edgeboxes [53] (eb) due to their
strong performance and speed. finally, edgeboxes have
been tuned to obtain high recall for objects, but our regions
data contains a wide variety of annotations around groups of
objects, stuff, etc. therefore, as a third source of test time
regions we follow faster r-id98 [37] and train a separate
region proposal network (rpn) on the vg regions data.
this corresponds to training our full model except without
the id56 language model.

and that the rpn boxes are likely out of sample for the
fcln on eb model.
our model outperforms individual region description.
our    nal model performance is listed under the rpn col-
umn as 5.39 ap. in particular, note that in this one cell of
table 1 we report the performance of our full joint model
instead of our model evaluated on the boxes from the inde-
pendently trained rpn network. our performance is quite
a bit higher than that of the region id56 model, even when
the region model is evaluated on the rpn proposals (5.93
vs. 4.26). we attribute this improvement to the fact that our
model can take advantage of visual information from the
context outside of the test regions.
qualitative results. we show example predictions of the
dense captioning model in figure 3. the model generates
rich snippet descriptions of regions and accurately grounds
the captions in the images. for instance, note that several
parts of the elephant are correctly grounded and described
(   trunk of an elephant   ,    elephant is standing   , and both
   leg of an elephant   ). the same is true for the airplane ex-
ample, where the tail, engine, nose and windows are cor-
rectly localized. common failure cases include repeated
detections (e.g. the elephant is described as standing twice).
runtime evaluation. our model is ef   cient at test time:
a 720    600 image is processed in 240ms. this includes
running the id98, computing b = 300 region proposals,
and sampling from the language model for each region.

table 1 (right) compares the test-time runtime perfor-
mance of our model with baselines that rely on edgeboxes.
regions id56 is slowest since it processes each region with
an independent forward pass of the id98; with a runtime of
3170ms it is more than 13   slower than our method.

fcln on eb extracts features for all regions after a sin-
gle forward pass of the id98. its runtime is dominated by
edgeboxes, and it is     1.5   slower than our method.

our method takes 88ms to compute region proposals, of
which nearly 80ms is spent running nms to subsample re-
gions in the localization layer. this time can be drastically
reduced by using fewer proposals: using 100 region propos-
als reduces our total runtime to 166ms.

as the last baseline we reproduce the approach of fast
r-id98 [14], where the region proposals during training are
   xed to edgeboxes instead of being predicted by the model
(fcln on eb). the results of this experiment can be found
in table 1. we now highlight the main takeaways.
discrepancy between region and image level statistics.
focusing on the    rst two rows of table 1, the region id56
model obtains consistently stronger results on meteor
alone, supporting the difference in the language statistics
present on the level of regions and images. note that these
models were trained on nearly the same images, but one on
full image captions and the other on region captions. how-
ever, despite the differences in the language, the two models
reach comparable performance on the    nal metric.
rpn outperforms external region proposals. in all cases
we obtain performance improvements when using the rpn
network instead of eb regions. the only exception is the
fcln model that was only trained on eb boxes. our hy-
pothesis is that this re   ects people   s tendency of annotating
regions more general than those containing objects. the
rpn network can learn these distributions from the raw
data, while the edgeboxes method was designed for high
recall on objects. in particular, note that this also allows our
model (fcln) to outperform the fcln on eb baseline,
which is constrained to edgeboxes during training (5.24
vs. 4.88 and 5.39 vs. 3.21). this is despite the fact that
their localization-independent language scores are compa-
rable, which suggests that our model achieves improve-
ments speci   cally due to better localization. finally, the
noticeable drop in performance of the fcln on eb model
when evaluating on rpn boxes (5.39 down to 3.21) also
suggests that the eb boxes have particular visual statistics,

full image id56 [22]
eb + full image id56 [22]
region id56 [14]
our model (fcln)

ranking

r@1 r@5
0.30
0.10
0.40
0.11
0.18
0.43
0.53
0.27

r@10 med. rank
0.43
0.55
0.59
0.67

13
9
7
5

localization

iou@0.1

iou@0.3

iou@0.5 med. iou

-

0.348
0.460
0.560

-

0.156
0.273
0.345

-

0.053
0.108
0.153

-

0.020
0.077
0.137

table 2. results for id162 experiments. we evaluate ranking using recall at k (r@k, higher is better) and median rank of the
target image (med.rank, lower is better). we evaluate localization using ground-truth region recall at different iou thresholds (iou@t,
higher is better) and median iou (med. iou, higher is better). our method outperforms baselines at both ranking and localization.

gt image

query phrases

retrieved images

figure 4. example id162 results using our dense captioning model. from left to right, each row shows a grund-truth test image,
ground-truth region captions describing the image, and the top images retrieved by our model using the text of the captions as a query. our
model is able to correctly retrieve and localize people, animals, and parts of both natural and man-made objects.

4.2. id162 using regions and captions
in addition to generating novel descriptions, our dense cap-
tioning model can support id162 using natural-
language queries, and can localize these queries in retrieved
images. we evaluate our model   s ability to correctly retrieve
images and accurately localize textual queries.
experiment setup. we use 1000 random images from
the vg test set for this experiment. we generate 100 test
queries by repeatedly sampling four random captions from
some image and then expect the model to correct retrieve
the source image for each query.
evaluation. to evaluate ranking, we report the fraction of
queries for which the correct source image appears in the
top k positions for k     {1, 5, 10} (recall at k) and the me-
dian rank of the correct image across all queries.

to evaluate localization, for each query caption we
examine the image and ground-truth bounding box from
which the caption was sampled. we compute iou between
this ground-truth box and the model   s predicted grounding

for the caption. we then report the fraction of query cap-
tion for which this overlap is greater than a threshold t for
t     {0.1, 0.3, 0.5} (recall at t) and the median iou across
all query captions.
models. we compare the ranking and localization perfor-
mance of full model with baseline models from section 4.1.
for the full image id56 model trained on ms coco,
we compute the id203 of generating each query cap-
tion from the entire image and rank test images by mean
id203 across query captions. since this does not local-
ize captions we only evaluate its ranking performance.

the full image id56 and region id56 methods are
trained on full ms coco images and ground-truth vg re-
gions respectively. in either case, for each query and test
image we generate 100 region proposals using edgeboxes
and for each query caption and region proposal we compute
the id203 of generating the query caption from the re-
gion. query captions are aligned to the proposal of maximal
id203, and images are ranked by the mean id203

head of a giraffe

legs of a zebra

red and white sign

white tennis shoes

hands holding a phone

front wheel of a bus

figure 5. example results for open world detection. we use our dense captioning model to localize arbitrary pieces of text in images, and
display the top detections on the test set for several queries.

of aligned caption / region pairs.

the process for the full fcln model is similar, but uses
the top 100 proposals from the localization layer rather than
edgeboxes proposals.
discussion.
figure 4 shows examples of ground-truth
images, query phrases describing those images, and im-
ages retrieved from these queries using our model. our
model is able to localize small objects (   hand of the clock   ,
   logo with red letters   ), object parts, (   black seat on bike   ,
   chrome exhaust pipe   ), people (   man is wet   ) and some
actions (   man playing tennis outside   ).

quantitative results comparing our model against the
baseline methods is shown in table 2. the relatively poor
performance of the full image id56 model (med. rank 13
vs. 9,7,5) may be due to mismatched statistics between its
train and test distributions: the model was trained on full
images, but in this experiment it must match region-level
captions to whole images (full image id56) or process im-
age regions rather than full images (eb + full image id56).
the region id56 model does not suffer from a mismatch
between train and test data, and outperforms the full image
id56 model on both ranking and localization. compared to
full image id56, it reduces the median rank from 9 to 7 and
improves localization recall at 0.5 iou from 0.053 to 0.108.
our model outperforms the region id56 baseline for
both ranking and localization under all metrics, further re-
ducing the median rank from 7 to 5 and increasing localiza-
tion recall at 0.5 iou from 0.108 to 0.153.

the baseline uses edgeboxes which was tuned to local-
ize objects, but not all query phrases refer to objects. our
model achieves superior results since it learns to propose
regions from the training data.

open-world id164 using the retrieval setup de-
scribed above, our dense captioning model can also be used
to localize arbitrary pieces of text in images. this enables
   open-world    id164, where instead of commit-
ting to a    xed set of object classes at training time we can
specify object classes using natural language at test-time.
we show example results for this task in figure 5, where we
display the top detections on the test set for several phrases.
our model can detect animal parts (   head of a giraffe   ,
   legs of a zebra   ) and also understands some object at-
tributes (   red and white sign   ,    white tennis shoes   ) and in-
teractions between objects (   hands holding a phone   ). the
phrase    front wheel of a bus    is a failure case: the model
correctly identi   es wheels of buses, but cannot distinguish
between the front and back wheel.

5. conclusion
we introduced the dense captioning task, which requires a
model to simultaneously localize and describe regions of an
image. to address this task we developed the fcln ar-
chitecture, which supports end-to-end training and ef   cient
test-time performance. our fcln architecture is based on
recent id98-id56 models developed for image captioning
but includes a novel, differentiable localization layer that
can be inserted into any neural network to enable spatially-
localized predictions. our experiments in both generation
and retrieval settings demonstrate the power and ef   ciency
of our model with respect to baselines related tp previous
work, and qualitative experiments show visually pleasing
results. in future work we would like to relax the assump-
tion of rectangular proposal regions and to discard test-time
nms in favor of a trainable spatial suppression layer.

6. acknowledgments

our work is partially funded by an onr muri grant
and an intel research grant. we thank vignesh ramanathan,
yuke zhu, ranjay krishna, and joseph lim for helpful
comments and discussion. we gratefully acknowledge the
support of nvidia corporation with the donation of the
gpus used for this research.
references
[1] k. barnard, p. duygulu, d. forsyth, n. de freitas, d. m.
blei, and m. i. jordan. matching words and pictures. jmlr,
2003. 2

[2] y. bengio, r. ducharme, p. vincent, and c. janvin. a neu-
ral probabilistic language model. the journal of machine
learning research, 3:1137   1155, 2003. 2

[3] c. m. c. j. c. c. j. h. bryan a. plummer, liwei wang and
s. lazebni. flickr30k entities: collecting region-to-phrase
correspondences for richer image-to-sentence models. 2015.
4

[4] x. chen, h. fang, t.-y. lin, r. vedantam, s. gupta, p. dol-
lar, and c. l. zitnick. microsoft coco captions: data collec-
tion and evaluation server. arxiv preprint arxiv:1504.00325,
2015. 1, 2, 4

[5] x. chen and c. l. zitnick. learning a recurrent visual rep-
arxiv preprint

resentation for image id134.
arxiv:1411.5654, 2014. 1, 2, 4

[6] k. cho, a. c. courville, and y. bengio. describing multime-
dia content using attention-based encoder-decoder networks.
corr, abs/1507.01053, 2015. 2

[7] r. collobert, k. kavukcuoglu, and c. farabet. torch7: a
matlab-like environment for machine learning. in biglearn,
nips workshop, number epfl-conf-192376, 2011. 4

[8] m. denkowski and a. lavie. meteor universal: language
speci   c translation evaluation for any target language.
in
proceedings of the eacl 2014 workshop on statistical ma-
chine translation, 2014. 2

[9] j. donahue, l. a. hendricks, s. guadarrama, m. rohrbach,
s. venugopalan, k. saenko, and t. darrell. long-term recur-
rent convolutional networks for visual recognition and de-
scription. arxiv preprint arxiv:1411.4389, 2014. 1, 2, 4

[10] d. erhan, c. szegedy, a. toshev, and d. anguelov. scalable
id164 using deep neural networks. in computer
vision and pattern recognition (cvpr), 2014 ieee confer-
ence on, pages 2155   2162. ieee, 2014. 2

[11] m. everingham, l. van gool, c. k. williams, j. winn, and
a. zisserman. the pascal visual object classes (voc) chal-
lenge. international journal of id161, 88(2):303   
338, 2010. 5

[12] h. fang, s. gupta, f. iandola, r. srivastava, l. deng,
p. doll  ar, j. gao, x. he, m. mitchell, j. platt, et al.
from captions to visual concepts and back. arxiv preprint
arxiv:1411.4952, 2014. 2

[13] a. farhadi, m. hejrati, m. a. sadeghi, p. young,
c. rashtchian, j. hockenmaier, and d. forsyth. every pic-
ture tells a story: generating sentences from images.
in
eccv. 2010. 2

[14] r. girshick. fast r-id98. arxiv preprint arxiv:1504.08083,

2015. 2, 3, 6, 7

[15] r. girshick, j. donahue, t. darrell, and j. malik. rich fea-
ture hierarchies for accurate id164 and semantic
segmentation. in cvpr, 2014. 1, 2

[16] a. graves. generating sequences with recurrent neural net-

works. arxiv preprint arxiv:1308.0850, 2013. 2, 4

[17] k. gregor, i. danihelka, a. graves, and d. wierstra. draw:
arxiv

a recurrent neural network for image generation.
preprint arxiv:1502.04623, 2015. 1, 2, 3

[18] k. he, x. zhang, s. ren, and j. sun. spatial pyramid pooling
in deep convolutional networks for visual recognition. 2015.
2

[19] s. hochreiter and j. schmidhuber. long short-term memory.

neural computation, 9(8):1735   1780, 1997. 2, 4

[20] m.

jaderberg, k. simonyan, a. zisserman,

k. kavukcuoglu.
preprint arxiv:1506.02025, 2015. 1, 2, 3

spatial transformer networks.

and
arxiv

[21] y. jia, m. salzmann, and t. darrell. learning cross-modality
similarity for multinomial data. in id161 (iccv),
2011 ieee international conference on, pages 2407   2414.
ieee, 2011. 2

[22] a. karpathy and l. fei-fei. deep visual-semantic align-
arxiv preprint

ments for generating image descriptions.
arxiv:1412.2306, 2014. 1, 2, 4, 5, 6, 7

[23] a. karpathy, j. johnson, and l. fei-fei. visualizing
arxiv preprint

and understanding recurrent networks.
arxiv:1506.02078, 2015. 2

[24] d. kingma and j. ba. adam: a method for stochastic opti-

mization. arxiv preprint arxiv:1412.6980, 2014. 4

[25] r. kiros, r. salakhutdinov, and r. s. zemel. unifying
visual-semantic embeddings with multimodal neural lan-
guage models. arxiv preprint arxiv:1411.2539, 2014. 2

[26] a. krizhevsky, i. sutskever, and g. e. hinton.

classi   cation with deep convolutional neural networks.
nips, 2012. 1, 2

id163
in

[27] g. kulkarni, v. premraj, s. dhar, s. li, y. choi, a. c. berg,
and t. l. berg. baby talk: understanding and generating
simple image descriptions. in cvpr, 2011. 2

[28] p. kuznetsova, v. ordonez, a. c. berg, t. l. berg, and
y. choi. generalizing image captions for image-text parallel
corpus. in acl (2), pages 790   796. citeseer, 2013. 2

[29] y. lecun, l. bottou, y. bengio, and p. haffner. gradient-
based learning applied to document recognition. proceed-
ings of the ieee, 86(11):2278   2324, 1998. 2

[30] t.-y. lin, m. maire, s. belongie, j. hays, p. perona, d. ra-
manan, p. doll  ar, and c. l. zitnick. microsoft coco: com-
in id161   eccv 2014,
mon objects in context.
pages 740   755. springer, 2014. 5

[31] j. long, e. shelhamer, and t. darrell. fully convolutional

networks for semantic segmentation. cvpr, 2015. 2

[32] j. mao, w. xu, y. yang, j. wang, and a. l. yuille. explain
images with multimodal recurrent neural networks. arxiv
preprint arxiv:1410.1090, 2014. 1, 2, 4

[33] t. mikolov, m. kara     at, l. burget, j. cernock`y, and s. khu-
danpur. recurrent neural network based language model. in
interspeech, 2010. 2, 4

[51] p. young, a. lai, m. hodosh, and j. hockenmaier. from im-
age descriptions to visual denotations: new similarity met-
rics for semantic id136 over event descriptions. tacl,
2014. 4

[52] m. d. zeiler and r. fergus.

standing convolutional neural networks.
arxiv:1311.2901, 2013. 1

visualizing and under-
arxiv preprint

[53] c. l. zitnick and p. doll  ar. edge boxes: locating object

proposals from edges. in eccv, 2014. 6

[34] v. ordonez, x. han, p. kuznetsova, g. kulkarni,
m. mitchell, k. yamaguchi, k. stratos, a. goyal, j. dodge,
a. mensch, et al. large scale retrieval and generation of im-
age descriptions. international journal of id161
(ijcv), 2015. 2

[35] qassemoquab.

stnbhwd.

https://github.com/

qassemoquab/stnbhwd, 2015. 4

[36] j. redmon, s. divvala, r. girshick, and a. farhadi. you
only look once: uni   ed, real-time id164. arxiv
preprint arxiv:1506.02640, 2015. 2

[37] s. ren, k. he, r. girshick, and j. sun. faster r-id98: to-
wards real-time id164 with region proposal net-
works. arxiv preprint arxiv:1506.01497, 2015. 1, 2, 3, 4,
6

[38] o. russakovsky, j. deng, h. su, j. krause, s. satheesh,
s. ma, z. huang, a. karpathy, a. khosla, m. bernstein,
a. c. berg, and l. fei-fei.
id163 large scale visual
recognition challenge. international journal of computer
vision (ijcv), pages 1   42, april 2015. 1, 2, 4

[39] p. sermanet, d. eigen, x. zhang, m. mathieu, r. fergus,
and y. lecun. overfeat: integrated recognition, localization
and detection using convolutional networks. in iclr, 2014.
1, 2

[40] k. simonyan and a. zisserman. very deep convolutional
networks for large-scale image recognition. arxiv preprint
arxiv:1409.1556, 2014. 2

[41] r. socher and l. fei-fei. connecting modalities: semi-
supervised segmentation and annotation of images using un-
aligned text corpora. in cvpr, 2010. 2

[42] r. socher, a. karpathy, q. v. le, c. d. manning, and a. y.
ng. grounded id152 for    nding and de-
scribing images with sentences. tacl, 2014. 2

[43] i. sutskever, j. martens, and g. e. hinton. generating text

with recurrent neural networks. in icml, 2011. 2, 4

[44] c. szegedy, w. liu, y. jia, p. sermanet, s. reed,
d. anguelov, d. erhan, v. vanhoucke, and a. rabi-
novich. going deeper with convolutions. arxiv preprint
arxiv:1409.4842, 2014. 1

[45] c. szegedy, s. reed, d. erhan, and d. anguelov.
arxiv preprint

scalable, high-quality id164.
arxiv:1412.1441, 2014. 1, 2

[46] b. thomee, d. a. shamma, g. friedland, b. elizalde,
k. ni, d. poland, d. borth, and l.-j. li. the new data
and new challenges in multimedia research. arxiv preprint
arxiv:1503.01817, 2015. 4

[47] r. vedantam, c. l. zitnick, and d. parikh.
consensus-based image description evaluation.
preprint arxiv:1411.5726, 2014. 2, 5

cider:
arxiv

[48] o. vinyals, a. toshev, s. bengio, and d. erhan. show
and tell: a neural image caption generator. arxiv preprint
arxiv:1411.4555, 2014. 1, 2, 4

[49] p. j. werbos. generalization of id26 with appli-
cation to a recurrent gas market model. neural networks,
1(4):339   356, 1988. 2

[50] k. xu, j. ba, r. kiros, a. courville, r. salakhutdinov,
r. zemel, and y. bengio. show, attend and tell: neural im-
age id134 with visual attention. arxiv preprint
arxiv:1502.03044, 2015. 1, 2

