id33 with lstms: an empirical evaluation

adhiguna kuncoro    yuichiro sawai    kevin duh    yuji matsumoto   

   school of computer science, carnegie mellon university, pittsburgh, pa, usa

   graduate school of information science, nara institute of science and technology, japan

   hltcoe, johns hopkins university, baltimore, md, usa

akuncoro@cs.cmu.edu, {sawai.yuichiro.sn0,matsu }@is.naist.jp, kevinduh@cs.jhu.edu

6
1
0
2

 

n
u
j
 

0
3

 
 
]
l
c
.
s
c
[
 
 

2
v
9
2
5
6
0

.

4
0
6
1
:
v
i
x
r
a

abstract

we propose a transition-based dependency
parser using recurrent neural networks
with long short-term memory (lstm)
units. this extends the feedforward neu-
ral network parser of chen and manning
(2014) and enables modelling of entire
sequences of shift/reduce transition deci-
sions. on the google web treebank,
our lstm parser is competitive with the
best feedforward parser on overall ac-
curacy and notably achieves more than
3% improvement for long-range depen-
dencies, which has proved dif   cult for pre-
vious transition-based parsers due to er-
ror propagation and limited context infor-
mation. our    ndings additionally sug-
gest that dropout regularisation on the em-
bedding layer is crucial to improve the
lstm   s generalisation.

1 introduction

two complementary approaches to transition-
based dependency parsers have emerged recently.
the feature engineering approach relies on hand-
crafted feature templates to model interactions be-
tween sparse lexical features. while manually
crafting these feature templates requires substan-
tial expertise and extensive trial-and-error, this ap-
proach has led to state-of-the-art parsers in many
languages (buchholz and marsi, 2006; zhang and
nivre, 2011).

in contrast, the neural network approach en-
ables automatic learning of feature combinations
through non-linear hidden layers and mitigates
sparsity issues by sharing similar low-dimensional
distributed representations for related words (ben-
gio et al., 2003).

in this work, we explore new model architec-
tures under the neural network approach. in par-
ticular, we address the issue that the feedforward

architecture of the chen and manning parser per-
forms training on each oracle con   guration inde-
pendently of one another, disregarding the fact that
the oracles for each training sentence represent a
whole sequence of intertwined decisions. our pro-
posed extension uses a recurrent neural network
(id56) with long short-term memory (lstm)
units (hochreiter and schmidhuber, 1997). at
each time step of the transition system, the lstm
has theoretical access to the entire history of past
decisions (i.e. shift or reduce). lstms are nat-
urally suited for modelling sequences and have
shown promising results in e.g. machine transla-
tion (sutskever et al., 2014) and text-vision mod-
elling (venugopalan et al., 2014).

we particularly focus on the lstm   s perfor-
mance in identifying long-range dependencies.
such dependencies have proved dif   cult for most
greedy transition-based parsers (mcdonald and
nivre, 2007), including our feedforward baselines,
that train on each oracle independently. this
dif   culty can be attributed to two main reasons:
1) most long-range dependencies are ambiguous,
while the classi   ers only have access to a limited
context window, and 2) longer arcs are constructed
after shorter arcs in transition-based parsing, thus
increasing the chance of error propagation. in con-
trast, our lstm has the key abilities of modelling
whole sequences of training oracles and memo-
rise all past context information, both of which are
likely bene   cial for longer dependencies.

despite the lstm   s theoretical advantages,
in practice it is more prone to over   tting than
the feedforward architecture, even with the same
number of parameters. an additional contribution
of this work is an empirical investigation that sug-
gests that dropout (srivastava et al., 2014), par-
ticularly when applied to the embedding layer,
substantially improves the lstm   s generalisation
ability regardless of hidden layer size.

!"#$"#%&   ()*!

!
*"   (

"!

*"!

"""!

!
*"+(

"!

*"!

"""!

)"   (!

"""!

#"!

+,--).%&   ()*!

!"   (!

)"!

!"!

&"!

%"!

)"+(!

!"+(!

"""!

)"!

/.$"#%&   ()*!

!
$"   (

"!

$"!

"""!

!
$"+(

"!

$"!

"""!

figure 1: left: our lstm architecture. right: feedforward architecture in chen and manning (2014).

connections with dropout are denoted by dashed lines.

2 lstm parsing model

2.1 baseline model
our model is an extension to chen and manning
(2014), which uses a feedforward neural network
to predict the next transition of an arc-standard
system. in arc-standard, a con   guration consists
of a buffer b (holding the input words), a stack s
(holding the partial parse trees), and a set of de-
pendency arcs a. the parse tree is built by suc-
cessively making one of these transitions:

    shift: move the next word on b to s
    left/right-arc(l): add left/right arc

with label l between top two words on s

the features xt is a concatenation of embed-
dings from the top 3 words in s and b,    rst
and second left-/right-most children of the top two
words on s, and the leftmost of leftmost / right-
most of rightmost children of the top two words on
s. at each con   guration at time t, the neural net-
work    rst computes the hidden layer ht from the
input xt (applying a non-linear function f ), then
calculates the id203 of each transition in the
output vector yt :1

ht = f (wxhd(xt))
yt = softmax(whyd(ht))

d(  ) is a dropout operator, which randomly sets
elements to 0 with id203 pdrop.
2.2 our lstm model
our lstm model (shown in figure 1) uses the
same xt features as chen and manning (2014), but

1the dimension of xt is 2400 in experiments, and dimen-
sion of yt is 2|l|+1, the number of possible transitions where
|l| = number of dependency label types.

importantly adds new inputs based on past infor-
mation (such as ht   1). the addition of previous
state leads to recurrence and enables modelling
and training of the entire sequence of transitions.
while recurrence may cause the    vanishing gra-
dient    problem (bengio et al., 1994), the lstm
architecture solves this by introducing memory
cells ct that could store information over long time
intervals and keep gradients from diminishing. in-
put gates it control what is stored in a memory cell
ct, and output gates ot control whether the stored
information is used in further computations. this
allows information from the beginning of the sen-
tence to in   uence transition actions at the end of
the sentence. forget gates ft are used to erase the
information in the current memory cell.

the following equations describe our lstm
model with peephole connections (gers et al.,
2002), as set forth by graves (2013), and apply
dropout similar to zaremba et al. (2014).

it =   (wxid(xt) + wcict   1 + whiht   1)
ft =   (wxf d(xt) + wcf ct   1 + whf ht   1)
ct = ftct   1 +it tanh(wxcd(xt)+whcht   1)
ot =   (wxod(xt) + wcoct + whoht   1)
ht = ot tanh(ct)
yt = softmax(whyd(ht))
crucially, the lstm not only uses input xt in
its predictions for yt, but also exploits values in the
previous memory cell ct   1 and hidden layer ht   1
through the gates it, ft, and ot. the values of these
gates are bounded between [0, 1] due to the sig-
moid   , so multiplication with other components
modulates what information is passed through.

given training sentences {si}m

i=1 with gold
parse trees, our training data is a set of sequences

of con   gurations cit and oracle transition actions
ait at each time t for each sentence si. we max-
imise the log-likelihood of the oracle transition ac-
tions ait given by equation (1), where    is the
set of parameters including word, pos, and label
embeddings, and yt(a) is the id203 that the
parser takes transition action a at time t.

l(  ) =

m
x
i=1

x
t

log yt(ait)    

  
2

||  ||2

(1)

we optimise by gradient id26
through time (bptt) for each sentence si, feeding
the parser with gold sequence of con   gurations
{cit}|si|
t=1. when the parser reaches the    nal con-
   guration, the gradients are backpropagated from
each prediction yit at time t down to time 1.

3 experiment

3.1 experimental settings
we conducted the experiments on the google web
treebank (petrov and mcdonald, 2012), consist-
ing of the wsj portion of the ontonotes corpus
and    ve additional web domains, with 48 depen-
dency types. the models were trained only on the
training set of the wsj corpus, while the parame-
ters were optimised using the wsj dev set (i.e. no
tuning using any of the web domains    dev set).

as baselines, we re-implemented the chen and
manning parser with the same setting,
includ-
ing results from both the feedforward model with
tanh activation function (same activation as the
lstm) and its better-performing cubic counter-
part. training was done for a maximum of 400
epochs, stopped early if no better dev uas was
found after 30 consecutive epochs.2

3.2 main result and analysis
the las result on the google web treebank is
summarised on table 1, where f-t and f-c repre-
sent the feedforward baselines with tanh and cu-
bic activations, respectively. our lstm model
outperforms the feedforward baseline with the
same tanh activation function (87.5 vs 86.4 on
wsj test), while achieving competitive accuracy
with the cubic baseline.

2the lstm was trained with the adadelta optimiser
(zeiler, 2012), using a decay rate of 0.95 and    = 10   6.
the embeddings were similarly initialised as the feedforward
baselines, while the weight connections were initialised using
the same mechanism as glorot and bengio (2010). we used
automatic pos tags from the stanford bi-directional tagger
(toutanova et al., 2003), with tagging accuracies of 97% for
the wsj and 87-92% for the web domains.

wsj
dev
test

web test
answers
emails

newsgroups

reviews
weblogs

87.2
87.5

f - t f - c lstm
87.8
86.0
87.5
86.4
f - t f - c lstm
74.6
74.1
74.4
74.6
80.2
79.3
77.0
76.5
81.2
80.7

74.9
75.6
79.9
77.2
81.1

table 1: google web treebank las result

dep.
length

f - t f - c lstm

2

1

recall

recall

precision

precision

91.7
93.0
89.3
90.1
82.6
81.4
73.5
69.5
table 2: long-range arcs precision & recall

91.4
93.1
87.9
90.3
81.7
79.2
68.1
62.6

92.2
93.6
88.9
91.0
83.4
81.3
70.3
65.6

precision

precision

recall

recall

7-49

3-6

we furthermore investigate the models    perfor-
mance on long-range dependencies, reporting the
result in terms of labelled precision and recall
breakdown by dependency lengths on the wsj test
set in table 2. this result is also plotted in fig-
ures 2 and 3. despite the models    similar overall
accuracy, our lstm model outperforms the cubic
baseline by more than 3% in both precision and re-
call for dependency lengths greater than 7, and that
the lstm   s performance degrades more slowly as
dependency length increases.

3.3 regularisation experiments
we discover that regularisation is important for the
lstm parser, more so than feedforward architec-
tures. table 3 compares the relative improvement
due to dropout for feedforward vs. lstm by con-
straining both models to have the same number
of 500,000 parameters, corresponding to 50 hid-
den units for lstm. observe that lstm becomes
competitive only with dropout.

89.1
87.4

f-cubic
lstm

no dropout with dropout    
0.4
2.1
table 3: effect of dropout on uas accuracy
to investigate what kind of dropout is bene   -
cial, we conducted further experiments on a subset
of the training data (the    rst 80,000 tokens of the

89.5
89.5

figure 2: precision by dependency length

figure 3: recall by dependency length

regsettings

dev

test

epoch

l2   

0

10   8
10   7
10   6
10   5
10   4
10   3

dropout pdrop

e-h

h-o

both

0.2
0.4
0.6
0.2
0.4
0.6
0.2
0.4
0.6

80.2
80.7
79.9
79.8
80.5
83.4
81.6

84.4
85.8
86.2
81.8
82.3
81.9
85.4
86.1
85.3

80.0
80.8
80.3
80.1
80.4
82.9
81.6

84.3
85.7
85.5
81.6
82.1
81.7
85.0
85.9
85.3

42
25
43
43
46
206
159

97
257
273
52
93
69
122
315
500

table 4: uas accuracy of various regularisation

wsj training set).3 the results of dropout and l-2
regularisation are in table 4, along with the epoch
where the best dev uas is found. e-h and h-o in-
dicate dropout between the embedding-hidden and
hidden-output connections, respectively.

while dropout generally results in slower con-
vergence,
the technique outperforms l-2 and
signi   cantly improves the model   s accuracy by
more than 6%. most
importantly, we found
input dropout to be more crucial than hidden-
output dropout and achieves the same accuracy
as dropout on both input and hidden layers, sug-
gesting that our model can achieve good accu-
racy with input dropout alone. we found dropout
rates between 0.4 and 0.6 to be effective. further,
we found that dropout generally improves lstms

3we used the same experimental settings as in subsection
3.1 and evaluate uas on the full wsj dev and test set, with
hidden layer size    xed at 60.

figure 4: uas accuracy vs hidden layer size

regardless of model size. figure 4 shows how
dropout of 0.5 on e-h and e-o layers improve re-
sults for various hidden layer sizes.

4 related work

recently, various neural network models have
achieved state of the art results in many parsing
tasks and languages, including the google web
treebank dataset used in this paper. vinyals et
al. (2014) used lstms for sequence-to-sequence
constituency parsing that makes no prior assump-
tion of the parsing problem. for dependency pars-
ing, stenetorp (2013) presented an id56 compo-
sitional model, similar to the id56 constituency
parser of socher et al. (2013).

more recently, the works of dyer et al. (2015)
and kiperwasser and goldberg (2016) proposed
transition-based lstm models to automatically
extract real-valued feature vectors from the parser
con   guration. the transition-based parser of dyer
et al. (2015) used a    id200    architecture
and composition functions to obtain a continu-
ous, low-dimensional representation of the stack
to represent partial trees, along with the buffer and
history of actions. both our work and the stack
lstm model similarly used greedy decoding, al-
though one primary difference is that we used the
lstm to form temporal recurrence over the hid-
den states4. we used the same feature extrac-

4we de   ne the hidden states as the penultimate layer right

tion template as chen and manning (2014) and
replaced the feedforward connections with lstm
network, while dyer et al. (2015) instead used the
id200 as a means to extract dense features
from the parser con   guration without explicit tem-
poral recurrence.

neural network models have also been used
for structured training in transition-based parsing,
achieving state of the art results on various dataset.
weiss et al. (2015) used a structured id88
model on top of a feedforward transition-based
dependency parser. when augmented with tri-
training method on unlabelled data, their model
achieved an impressive 87% las on the web do-
main data of the google web treebank similarly
used in this work. zhou et al. (2015) used beam
search and contrastive learning to maximise the
id203 of the entire gold sequence with re-
spect to all other sequences in the beam. andor
et al. (2016) similarly proposed a globally nor-
malised model using id125 and conditional
random fields (crf) loss (lafferty, 2001) that
achieved state of the art results on the benchmark
english ptb dataset.

our id56 parsing model is most similar with
xu et al. (2016) that used temporal recurrence over
the hidden states for id35 parsing, although we
use lstms instead of elman id56s. our work
additionally investigates the effect of dropout on
model performance, and demonstrate the ef   cacy
of temporal recurrence to better capture long-
range dependencies.

5 conclusions

we present a transition-based dependency parser
using recurrent lstm units. the motivation is to
exploit the entire history of shift/reduce transitions
when making predictions. this lstm parser is
competitive with the feedforward neural network
parser of chen and manning (2014) on overall
las, and notably improves the accuracy of long-
range dependencies. we also show the importance
of dropout, particularly on the embedding layer, in
improving the model   s accuracy.

acknowledgments

we thank graham neubig and hiroyuki shindo
for the useful feedback and comments.

before the softmax.

references

daniel andor, chris alberti, david weiss, ali-
aksei severyn, alessandro presta, kuzman
ganchev, slav petrov, and michael collins.
2016. globally normalized transition-based
neural networks. corr, abs/1603.06042.

mohit bansal, kevin gimpel, and karen livescu.
2014. tailoring continuous word representa-
tions for id33.
in proceedings
of acl.

fr  ed  eric bastien, pascal lamblin, razvan pas-
canu, james bergstra, ian j. goodfellow, ar-
naud bergeron, nicolas bouchard, and yoshua
bengio. 2012. theano: new features and speed
improvements. deep learning and unsuper-
vised id171 nips 2012 workshop.

yoshua bengio, patrice simard, and paolo fras-
coni. 1994. learning long-term dependencies
with id119 is dif   cult. trans. neur.
netw., 5(2):157   166, march.

yoshua bengio, rjean ducharme, pascal vin-
cent, and christian jauvin. 2003. a neural
probabilistic language model. journal of
machine learning research, 3:1137   
1155.

james bergstra, olivier breuleux, fr  ed  eric
bastien, pascal lamblin, razvan pascanu,
guillaume desjardins, joseph turian, david
warde-farley, and yoshua bengio.
2010.
theano: a cpu and gpu math expression com-
piler. in proceedings of the python for scienti   c
computing conference (scipy), june. oral pre-
sentation.

sabine buchholz and erwin marsi. 2006. conll-
x shared task on multilingual dependency pars-
ing. in proc. of conll, pages 149   164.

danqi chen and christopher d. manning. 2014.
a fast and accurate dependency parser using
neural networks. in empirical methods in nat-
ural language processing (emnlp).

ronan collobert, jason weston, l  eon bottou,
and
language
corr,

michael karlen, koray kavukcuoglu,
pavel p. kuksa.
processing (almost)
abs/1103.0398.

2011. natural
from scratch.

john duchi, elad hazan, and yoram singer. 2011.
adaptive subgradient methods for online learn-
ing and stochastic optimization.
j. mach.
learn. res., 12:2121   2159, july.

chris dyer, miguel ballesteros, wang ling,
austin matthews, and noah a. smith. 2015.
transition-based id33 with stack
long short-term memory.
in proceedings of
the 53rd annual meeting of the association for
computational linguistics and the 7th interna-
tional joint conference on natural language
processing (volume 1: long papers), pages
334   343, beijing, china, july. association for
computational linguistics.

erick r fonseca, avenida trabalhador s  ao-
carlense, and sandra m alu    sio.
2015. a
deep architecture for non-projective depen-
dency parsing. proceedings of naacl-hlt,
pages 56   61.

felix a. gers, nicol n. schraudolph, and j  urgen
schmidhuber. 2002. learning precise timing
with lstm recurrent networks. jmlr, 3:115   143.

xavier glorot and yoshua bengio. 2010. un-
derstanding the dif   culty of training deep feed-
forward neural networks.
in proceedings of
the international conference on arti   cial intel-
ligence and statistics (aistats10). society for
arti   cial intelligence and statistics.

alex graves.

2013.

with recurrent neural networks.
abs/1308.0850.

generating sequences
corr,

klaus greff, rupesh kumar srivastava,

jan
koutn    k, bas r. steunebrink,
and j  urgen
schmidhuber. 2015. lstm: a search space
odyssey. corr, abs/1503.04069.

sepp hochreiter and j  urgen schmidhuber. 1997.
long short-term memory. neural comput.,
9(8):1735   1780, november.

eliyahu kiperwasser and yoav goldberg. 2016.
simple and accurate id33 us-
ing bidirectional lstm feature representations.
corr, abs/1603.04351.

john lafferty. 2001. conditional random    elds:
probabilistic models for segmenting and label-
ing sequence data.
pages 282   289. morgan
kaufmann.

ryan mcdonald and joakim nivre. 2007. char-
acterizing the errors of data-driven depen-
dency parsing models.
in proceedings of the
conference on empirical methods in natural
language processing and natural language
learning.

slav petrov and ryan mcdonald.

2012.
overview of the 2012 shared task on pars-
ing the web. notes of the first workshop on
syntactic analysis of non-canonical language
(sancl).

richard socher, john bauer, christopher d. man-
ning, and andrew y. ng. 2013. parsing with
compositional vector grammars.
in proceed-
ings of the acl conference.

ilya sutskever,

nitish srivastava, geoffrey hinton, alex
krizhevsky,
and ruslan
salakhutdinov.
2014. dropout: a simple
way to prevent neural networks from over   t-
ting. journal of machine learning research,
15:1929   1958.

pontus stenetorp. 2013. transition-based depen-
dency parsing using id56s.
in deep learning workshop at the 2013 confer-
ence on neural information processing systems
(nips), lake tahoe, nevada, usa, december.

ilya sutskever, oriol vinyals, and quoc v. le.
2014. sequence to sequence learning with neu-
ral networks. corr, abs/1409.3215.

kristina toutanova, dan klein, christopher d.
manning, and yoram singer. 2003. feature-
rich part-of-speech tagging with a cyclic depen-
dency network.
in proceedings of the 2003
conference of the north american chapter of
the association for computational linguistics
on human language technology - volume 1,
naacl    03, pages 173   180, stroudsburg, pa,
usa. association for computational linguis-
tics.

subhashini venugopalan, huijuan xu, jeff don-
ahue, marcus rohrbach, raymond j. mooney,
and kate saenko. 2014. translating videos
to natural language using deep recurrent neural
networks. corr, abs/1412.4729.

oriol vinyals, lukasz kaiser, terry koo, slav
petrov, ilya sutskever, and geoffrey e. hinton.

2014. grammar as a foreign language. corr,
abs/1412.7449.

1222, beijing, china, july. association for
computational linguistics.

david weiss, chris alberti, michael collins, and
slav petrov. 2015. structured training for neu-
ral network transition-based parsing.
in pro-
ceedings of the 53rd annual meeting of the as-
sociation for computational linguistics and the
7th international joint conference on natural
language processing (volume 1: long papers),
pages 323   333, beijing, china, july. associa-
tion for computational linguistics.

wenduan xu, michael auli, and stephen clark.
2016. expected f-measure training for shift-
reduce parsing with recurrent neural networks.
in proceedings of the 2016 conference of the
north american chapter of the association for
computational linguistics: human language
technologies, pages 210   220, san diego, cal-
ifornia, june. association for computational
linguistics.

hiroyasu yamada and yuji matsumoto.

2003.
statistical dependency analysis with support
vector machines.
in proceedings of the in-
ternational workshop on parsing technologies
(iwpt).

wojciech zaremba,

ilya sutskever, and oriol
vinyals. 2014. recurrent neural network regu-
larization. corr, abs/1409.2329.

matthew d. zeiler.

2012.

an adaptive learning rate method.
abs/1212.5701.

adadelta:
corr,

yue zhang and joakim nivre. 2011. transition-
based id33 with rich non-local
features.
in proceedings of the 49th annual
meeting of the association for computational
linguistics: human language technologies:
short papers - volume 2, hlt    11, pages 188   
193, stroudsburg, pa, usa. association for
computational linguistics.

hao zhou, yue zhang, shujian huang, and jiajun
chen. 2015. a neural probabilistic structured-
prediction model for transition-based depen-
dency parsing. in proceedings of the 53rd an-
nual meeting of the association for compu-
tational linguistics and the 7th international
joint conference on natural language pro-
cessing (volume 1: long papers), pages 1213   

