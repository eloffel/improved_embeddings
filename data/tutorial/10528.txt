5
1
0
2

 
r
p
a
0
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
7
7
2
6

.

2
1
4
1
:
v
i
x
r
a

accepted as a workshop contribution at iclr 2015

id165-based low-dimensional representa-
tion for document classification

r  emi lebret
idiap research institute, martigny, switzerland
ecole polytechnique f  ed  erale de lausanne (epfl), lausanne, switzerland
remi@lebret.ch

ronan collobert   
facebook ai research, menlo park, ca, usa
idiap research institute, martigny, switzerland
ronan@collobert.com

abstract

the bag-of-words (bow) model is the common approach for classifying docu-
ments, where words are used as feature for training a classi   er. this generally
involves a huge number of features. some techniques, such as latent semantic
analysis (lsa) or id44 (lda), have been designed to sum-
marize documents in a lower dimension with the least semantic information loss.
some semantic information is nevertheless always lost, since only words are con-
sidered. instead, we aim at using information coming from id165s to overcome
this limitation, while remaining in a low-dimension space. many approaches, such
as the skip-gram model, provide good word vector representations very quickly.
we propose to average these representations to obtain representations of id165s.
all id165s are thus embedded in a same semantic space. a id116 id91
can then group them into semantic concepts. the number of features is there-
fore dramatically reduced and documents are then represented as bag of semantic
concepts. we show that this model outperforms lsa and lda on a sentiment
classi   cation task, and yields similar results than a traditional bow-model with
far less features.

1

introduction

text document classi   cation aims at assigning a text document to one or more classes. successful
methods are traditionally based on bag-of-words (bow). finding discriminative keywords is, in
general, good enough for text classi   cation. given a dictionary of words d to consider, documents
are represented by a |d|-dimensional vector (the bag of its words). each dimension is either a binary
value (present or not in the document) or a word occurrence frequency. some term weightings (e.g.
the popular td-idf) have also been de   ned to re   ect how discriminative a word is for a document.
these are considered as features for training a classi   er. naive bayes (nb) and support vector
machine (id166) models are often the    rst choices. one limitation of the bag-of-words model is that
the discriminative words are usually not the most frequent ones. a large dictionary of words needs
to be de   ned to obtain a robust model. classi   ers then have to deal with a huge number of features,
and thus become time-consuming and memory-hungry.

some techniques have been proposed to reduce the dimensionality and represent documents in a
low-dimensional semantic space. latent semantic analysis (lsa) (deerwester et al., 1990) uses
the term-document matrix and a singular value decomposition (svd) to represent terms and doc-
uments in a new low-dimensional space. id44 (lda) (blei et al., 2003) is
a generative probabilistic model of a corpus. each document is represented as a mixture of la-
tent topics, where each topic is characterized by a distribution over words. by de   ning k topics,

   all research was conducted at the idiap research institute, before ronan collobert joined facebook ai

research.

1

accepted as a workshop contribution at iclr 2015

documents can then be represented as k-dimensional vectors. pessiot et al. (2010) also proposed
probabilistic models for unsupervised id84 in the context of document cluster-
ing. they make the hypothesis that words occuring with the same frequencies in the same document
are semantically related. based on this assumption, words are partioned into word topics. docu-
ment are then represented by a vector where each feature corresponds to a word-topic representing
the number of occurrences of words from that word-topic in the document. other techniques have
tried to improve text document id91 by taking into account relationships between important
terms. some have enriched id194s by integrating core ontologies as background
knowledge (staab & hotho, 2003), or with wikipedia concepts and category information (hu et al.,
2009). part-of-speech tags have also been used to disambiguate words (sedding & kazakov, 2004).

all these techniques are based on words alone, which raises another limitation. a collection of words
cannot capture phrases or multi-word expressions, while id165s have shown to be helpful features
in several natural language processing tasks (tan et al., 2002; lin & wu, 2009; wang & manning,
2012). n -gram features are not commonly used in text classi   cation, probably because the dictio-
nary dn tends to grow exponentially with n. phrase structure extraction can be used to identify only
id165s which are phrase patterns, and thus limit the dictionary size. however, this adds another
step to the model, making it more complex. to overcome these barriers, we propose that documents
be represented as a bag of semantic concepts, where id165s are considered instead of only words.
good word vector representations are obtained very quickly with many different recent approaches
(mikolov et al., 2013b; mnih & kavukcuoglu, 2013; lebret & collobert, 2014; pennington et al.,
2014). mikolov et al. (2013a) also showed that simple vector addition can often produce meaning-
ful results, such as king - man + woman     queen. by leveraging the ability of these word vector
representations to compose, representations for id165s are easily computed with an element-wise
addition. using a id91 algorithm such as id116, those representations are grouped into k
clusters which can be viewed as semantic concepts. text documents are now represented as bag
of semantic concepts, with each feature corresponding to the presence or not of id165s from the
resulting clusters. therefore, more information is captured while remaining in a low-dimensional
space. as mikolov et al   s skip-gram model and id116 are highly parallelizable, this model is
much faster to compute than lsa or lda. the same classi   ers as with bow-based models are then
applied on these bag of semantic concepts. we show that such model is a good alternative to lsa
or lda to represent documents and yields even better results on movie review tasks.

2 a bag of semantic concepts model

the model is divided into three steps: (1) vector representations of id165s are obtained by aver-
aging pre-trained representations of its individual words; (2) id165s are gouped into k semantic
concepts by performing id116 id91 on all id165 representations; (3) documents are rep-
resented by a bag of k semantic concepts, where each entry depends on the presence of id165s
from the concepts de   ned in the previous step.

2.1 n -gram representation

the    rst step of the model is to generate continuous vector representations xw for each word w
within the dictionary d. leveraging recent models, such as the skip-gram (mikolov et al., 2013b)
or glove (pennington et al., 2014) models, they are trained over a large corpus of unlabeled data
in an ef   cient manner. these models are indeed highly parallelizable, which helps to obtain these
representations very quickly. word representations are then summed to generate id165 represen-
tations:

1
n

n

xi=1

xwi .

(1)

these representations are vectors which keep the semantic information of id165s with different
n in the same dimensionality. distances between them are thus computable. it allows the use of a
id116 id91 for grouping all id165s into k classes.

2

accepted as a workshop contribution at iclr 2015

2.2 id116 id91

id116 is an unsupervised learning algorithm commonly used to automatically partition a data set
into k clusters. considering a set of id165 representations xi     rm, the algorithm will determine
a set of k centroids   k     rm, so as to minimize the average distance from each representation to
its nearest centroid:

xi

||xi         i||2 , where   i = argmin

||xi       k||2 .

(2)

k

the limitation due to the size of the dictionary is therefore overcomed. by setting k to a low value,
documents can also be represented by more compact vectors than with a bag-of-words model, while
keeping all the meaningful information.

2.3 id194

denoting d = (d1, d2, . . . , dl) a set of text documents, where each document di contains a set
of id165s. first, each id165 is embedded into a common vector space by averaging its word
vector representations. the resulting id165s representations are assigned to clusters using the
centroids   k de   ned by the id116 id91. documents di are then represented by a vector of
i usually corresponds to the frequency of id165s from the kth
k features, fi     rk. each entry f k
cluster within the document di. the set of text documents is then de   ned as   d = {(fi, yi)| fi    
rk, yi     {   1, 1}}l

i=1.

with nb features. for certain type of document, such as movie reviews, the use of naive bayes
features can improve the general performance (wang & manning, 2012). success in sentiment anal-
ysis relies mostly on the capability of the models to detect negative and positive id165s in a doc-
ument. a proper id172 is then calculated to determine how important each id165 is for a
given class y. we denote ngm = (ngm1, . . . , ngmn ) a set of count vectors for all id165s con-
t represents the number of occurence of the id165 t in the training
tained in d, ngmi     rl. ngmi
document di. de   ning count vectors as p = 1 + pi:yi=1 ngmi and q = 1 + pi:yi=   1 ngmi, a

log-count ratio is calculated to determine how important id165s are for the classes y:

r = log(cid:18) p/||p||1

q/||q||1(cid:19) , with r     rn .

(3)

because id165s are in clusters, we extract the maximum absolute log-count ratio for every cluster:

  f k
i = argmax

rt

|rt| ,    t     k, ngmi

t > 0

(4)

these id194s can then be used for several nlp tasks such as classi   cation or
information retrieval. as for bow-based models, this model is particulary suitable for linear id166.

3 experiments with id31

sentiments can have a completely different meaning if id165s are considered instead of words.
a classi   er might leverage a bigram such as    not good    to classify a document as negative, while
this would probably fail if only unigrams (words) were considered. we thus benchmark the bag of
semantic concepts model on id31.

3.1

imdb movie reviews datasets

datasets from imdb have the nice property of containing long documents. it is thus valuable to
considerer id165s in such a framework. we did experiments with small and large collections of
reviews. we can thus analyse how well our model compares against classical models, for different
dataset sizes.

3

accepted as a workshop contribution at iclr 2015

3.1.1 pang & lee (2004)

the collection consists of 1,000 positive and 1,000 negative processed reviews1. so a random guess
yields 50% accuracy. the authors selected only reviews where rating was expressed either with stars
or some numerical value. to avoid domination of the corpus by a small number of proli   c reviewers,
they imposed a limit of fewer than 20 reviews per author per sentiment category. as there is no test
set, we used 10-fold cross-validation.

3.1.2 maas et al. (2011)

the collection consists of 100,000 reviews2. it has been divided into three datasets: training and test
sets (25,000 labeled reviews each), and 50,000 unlabeled training reviews. it allows no more than
30 reviews per movie. it contains an even number of positive and negative reviews, so randomly
guessing yields 50% accuracy. only highly polarized reviews have been considered. a negative
review has a score     4 out of 10, and a positive review has a score     7 out of 10.

3.2 experimental setup

we    rst learn word vector representations over a large corpus of unlabeled text. this step could
however be skipped by taking existing pre-trained word representations3 instead of learning them
from scratch. by following the three steps described in section 2, movie reviews are then represented
as bags of semantic concepts. these representations are    nally used for training a linear id166 to
classify sentiment.

3.2.1 learning word representation over large corpora

our english corpus is composed of the entire english wikipedia4, the reuters corpus and the wall
street journal (wsj) corpus. we consider lower case words and replace digits with a special token.
the resulting text is tokenized using the stanford tokenizer. the    nal data set contains about 2
billion words. our dictionary d consists of all the words appearing at least one hundred times. this
results in a 202,255 words dictionary. we then train a skip-gram model to get word representation
in a 100-dimensional vector. this dimension is intentionally quite low to speed up the id91
afterwards. as other hyperparameters, we use a    xed learning rate of 0.01, a context size of 5
phrases, negative sampling with 5 negative samples for each positive sample, and a subsampling
approach with a threshold of 10   5

3.2.2 bag of semantic concepts for movie reviews

computing id165 representations. we consider id165s up to n = 3. only id165s with
words from our dictionary are considered for both datasets.5 this results in a set of 34,360 1-
gram representations, 419,918 2-gram representations, and 921,837 3-gram representations for the
pang and lee   s dataset. and 67,847 1-gram representations, 1,842,461 2-gram representations, and
5,724,871 3-gram representations for the maas et al.   s dataset. because id165 representations
are computed by averaging representations of its word, all id165s are also represented in a 100-
dimensional vector.

partitioning id165s into semantic concepts. because id165s are represented in a common
vector space, similarities between id165s of different length can be computed. to evaluate the
bene   t of adding id165s for id31, we de   ne semantic concepts with different com-
binations of id165s: (1) only 1-grams (i.e. clusters of words), (2) only 2-grams, (3) only 3-grams,
(4) with 1-grams and 2-grams, and (5) with 1-grams, 2-grams and 3-grams. each of these    ve sets

1available at http://www.cs.cornell.edu/people/pabo/movie-review-data/.
2available at http://www.andrew-maas.net/data/sentiment.
3different

pre-trained

vector

word

representations

available
http://stanford.edu/  jpennin/

are

at
or

https://code.google.com/p/id97/,
http://lebret.ch/words/.

4we took the january 2014 version.
5our english corpus is not large enough to cover all the words present in the imdb datasets. we thus use

the same 1-gram dictionary with the other methods.

4

accepted as a workshop contribution at iclr 2015

of id165 representations are then partitioned in k = {100, 200, 300} clusters with the id116
id91. the centroids   k     r100 are obtained after 10 iterations of the algorithm.

movie review representations. movie reviews are then represented as bags of semantic concepts
with naive bayes features as described in section 2.3. the log-count ratio for each id165 is calcu-
lated on the training set for both datasets.

3.2.3 comparison with other methods

we compare our models with two classical techniques for representing text documents in a low-
dimensional vector space: lsa and lda. both methods use the same 1-gram dictionaries than with
the bag of semantic concepts model with k = {100, 200, 300}. in the framework of maas et al.   s
dataset, lsa and lda bene   t from the large set of unlabeled reviews.

latent id31 (lsa) (deerwester et al., 1990). let x     r|d|  l be a matrix where
each element xi,j describes the log count ratio of words i in document j, with l the number of
training documents and d the dictionary of words (i.e. 34,360 for pang and lee   s dataset, 67,847
for maas et al   s dataset). by applying truncated svd to the log-count ratio matrix x, we thus obtain
semantic representations in a k-dimensional space for movie reviews.

id44 (lda) (blei et al., 2003). we train the k-topics lda model using
the code released by blei et al. (2003)6. we leave the lda hyperparameters at their default values.
like our model, lda extracts k topics (i.e. semantic concepts) and assigns words to these topics.
considering only the words in documents, we thus apply the method described in section 2.3 to get
id194s. a movie review di is then represented in a k-dimensional vector, where
each feature   f k

i is the maximum absolute log-count ratio for the kth topic.

3.2.4 classification using id166

having representations of movie reviews in a k-dimensional vector, a classi   er is trained to de-
termine whether a given review is positive or negative. given the set of training documents
  d = {(  fi, yi)|   fi     rk, yi     {   1, 1}}l
i=1, we picked a linear id166 as a classi   er, trained using the
liblinear library (fan et al., 2008):

min

w

1
2

wt w + cxi

max(0, 1     yiwt   fi)2 ,

(5)

with w the weight vector, and c a penalty parameter.

3.3 results

the overall results summarized in table 1 show that the bag of semantic concepts approach out-
performs the traditionnal lda and lsa approaches to represent documents in a low-dimensional
space. good performance is achieved even with only 100 clusters, where lsa needs more clus-
ters to improve. we also denote that our approach performs well on a small dataset, where lda
fails. a signi   cant increase is observed when using 2-grams instead of 1-grams. however, using
only 3-grams hurts the performance. the best results are obtained using a combination of id165s,
which con   rms the bene   t of the method. that also means that word vector representations can be
combined while keeping relevant semantic information. this is illustrated in table 3 where semanti-
cally close id165s are in the same cluster. we can see that the model is furthermore able to clearly
separate antonyms, which is a good asset for sentiment classi   cation. the results are also very
competitive with a traditional bow-model. using the same 1-gram dictionary and a linear id166
classi   er with the naive bayes features, bow-model achieves 83% accuracy for pang and lee   s
dataset, and 88.58% for maas et al   s dataset. our model therefore performs better with about 344
times less features for the    rst dataset, and yields similar result with about 678 times less features
for the second one.

6available at http://www.cs.princeton.edu/  blei/lda-c/.

5

accepted as a workshop contribution at iclr 2015

pang and lee, 2004

maas et al., 2011

k =

100

200

300

100

200

300

lda
lsa
1-grams
2-grams
3-grams
1+2-grams
1+2+3-grams

76.20
81.60
81.60
82.30
73.85
83.85
82.45

77.10
82.55
82.60
82.25
73.05
84.00
83.05

76.80
83.75
82.70
83.15
72.65
84.00
83.05

85.43
85.82
84.51
88.02
87.41
88.10
88.39

85.45
86.63
84.76
88.06
87.46
88.19
88.46

84.40
86.88
85.54
87.87
87.22
88.18
88.55

table 1: classi   cation accuracy on both movie review tasks with k = {100, 200, 300} number of
features.

3.4 computation time

the bag of semantic concepts model can leverage information coming from id165s to improve
sentiment classi   cation of documents. this model has also the nice property to build document rep-
resentations in an ef   cient and timely manner. the most time-consuming and costly process step in
the model is the id116 id91, especially when dealing with millions of id165 representa-
tions. however, this step can be done very quickly with low memory by using mini-batch id116
method. computation times for generating 300-dimensional representations are reported in table 2.
all experiments have been run on single cpu core intel i7 2600k 3.4 ghz. despite the fact that
single cpu has been used for this benchmark, the three steps of the model are highly parallelizable.
the recorded times could thus be divided by the number of cpus available. we see that represen-
tations can be computed in less than one minute with only 1-gram dictionary. about 10 minutes are
necessary when adding 2-grams, and about 40 minutes by adding 3-grams. in comparison, lda
needs six hours for extracting 100 topics and three days for 300 topics. our model is also very com-
petitive with lsa which takes 540 seconds to generate 300-dimensional id194s.
however, adding 2-grams and 3-grams to perform a lsa would be extremely time-consuming and
memory-hungry while our model can handle it.

1-grams

2-grams

3-grams

1+2-grams

1+2+3-grams

n -gram representations
id116
id194s

total

0
14.18
36.45

50.63

43.00
291.62
173.48

164.34
747.90
494.06

508.10

1406.30

43.00
302.34
343.29

688.63

207.34
1203.99
949.01

2360.34

table 2: computation time for building movie review representations with k = 300 semantic
concepts. time is reported in seconds.

3.5

inferring semantic concepts for unseen id165s

another drawback of classical models is that they cannot deal with unseen words. only words
present in the training documents are used to infer representation for a new text document. unlike
these models, our model can easily assign semantic concepts for new id165s. because id165
representations are based on its word vector representations, a new id165 vector representation
can be calculated if a representation is available for each of its words. this new representations
is then assigned to the nearest centroid   k, which determines its semantic concept. with a small
training set, this is a valuable asset when compared to other models.

6

accepted as a workshop contribution at iclr 2015

good

k=269

not good

k=297

enjoy

k=160

did n   t enjoy

k=108

nice one
liked here
is pretty nice
the greatest thing

suf   ciently bad
not liked
is far worse
not that greatest

entertain
adored them
enjoying
watched and enjoy

sceptics
did n   t like
n   t enjoy any
valueless

table 3: selected pairs of antonyms and their cluster number. here, id165s from maas et al   s
dataset have been partitioned into 300 clusters. each id165 is accompagnied with a selection of
others from its cluster.

4 conclusion

word vector representations can be quickly obtained with recent techniques such as the skip-gram
model. n -grams with different length n can then be embedded in a same dimensional vector space
with a simple element-wise addition. this makes it possible to compute distances between id165s,
which can have many applications in natural language processing. we therefore proposed a bag
of semantic concepts model to represent documents in a low-dimensional space. these semantic
concepts are obtained by performing a id116 id91 which partition all id165s into k clus-
ters. this model has several advantages over classical approaches for representing documents in a
low-dimensional space: it leverages semantic information coming from id165s; it builds document
representations with low resource consumption (time and memory); it can infer semantic concepts
for unseen id165s. furthermore, we have shown that such model is suitable for document classi-
   cation. competitive performance has been reached on binary sentiment classi   cation tasks, where
this model outperforms traditional approaches. it also attained similar results to traditional bag-of-
words with considerably less features.

acknowledgments

this work was supported by the hasler foundation through the grant    information and commu-
nication technology for a better world 2020    (smartworld).

references

blei, d. m., ng, a. y., and jordan, m. i. id44. journal of machine learning

research, 2003.

deerwester, s., dumais, s. t., furnas, g. w., landauer, t. k., and harshman, r. indexing by latent

semantic analysis. journal of the american society for information science, 1990.

fan, r., chang, k., hsieh, c., wang, x., and lin, c. liblinear: a library for large linear

classi   cation. journal of machine learning research, 2008.

hu, x., zhang, x., lu, c., park, e. k., and zhou, x. exploiting wikipedia as external knowledge
for document id91. in proceedings of the 15th acm sigkdd international conference
on knowledge discovery and data mining, kdd    09, pp. 389   396, new york, ny, usa, 2009.
acm.

lebret, r. and collobert, r. id27s through hellinger pca.

eacl, 2014.

in proceedings of the

lin, d. and wu, x. phrase id91 for discriminative learning. in proceedings of acl, 2009.

7

accepted as a workshop contribution at iclr 2015

maas, a. l., daly, r. e., pham, p. t., huang, d., ng, a. y., and potts, c. learning word vectors

for id31. in proceedings of acl, 2011.

mikolov, t., chen, k., corrado, g., and dean, j. ef   cient estimation of word representations in

vector space. iclr workshp, 2013a.

mikolov, t., sutskever, i., chen, k., corrado, g., and dean, j. distributed representations of words

and phrases and their compositionality. in nips. 2013b.

mnih, a. and kavukcuoglu, k. learning id27s ef   ciently with noise-contrastive esti-

mation. in nips. 2013.

pang, b. and lee, l. a sentimental education: id31 using subjectivity. in proceedings

of acl, 2004.

pennington, j., socher, r., and manning, c. d. glove: global vectors for word representation. in

proceedings of emnlp, 2014.

pessiot, j.-f., kim, y.-m., amini, m.-r., and gallinari, p. improving document id91 in a

learned concept space. information processing & management, 46(2):180   192, 2010.

sedding, j. and kazakov, d. id138-based text document id91. in proceedings of the 3rd
workshop on robust methods in analysis of natural language data, pp. 104   113, stroudsburg,
pa, usa, 2004. association for computational linguistics.

staab, s. and hotho, a. ontology-based text document id91.

in intelligent information
processing and web mining, proceedings of the international iis: iipwm   03 conference held in
zakopane, pp. 451   452, 2003.

tan, c., wang, y., and lee, c. the use of bigrams to enhance text categorization. journal of

information processing and management, 2002.

wang, s. i. and manning, c. d. baselines and bigrams: simple, good sentiment and topic classi-

   cation. in proceedings of acl, 2012.

8

