5
1
0
2

 
r
p
a
1
1

 

 
 
]
l
c
.
s
c
[
 
 

3
v
9
1
4
8

.

2
1
4
1
:
v
i
x
r
a

accepted as a workshop contribution at iclr 2015

simple image description generator via
a linear phrase-based model

r  emi lebret    & pedro o. pinheiro   
idiap research institute, martigny, switzerland
  ecole polytechnique f  ed  erale de lausanne (epfl), lausanne, switzerland
remi@lebret.ch, pedro@opinheiro.com

ronan collobert   
facebook ai research, menlo park, ca, usa
idiap research institute, martigny, switzerland
ronan@collobert.com

abstract

generating a novel textual description of an image is an interesting problem that
connects id161 and natural language processing.
in this paper, we
present a simple model that is able to generate descriptive sentences given a sam-
ple image. this model has a strong focus on the syntax of the descriptions. we
train a purely bilinear model that learns a metric between an image representation
(generated from a previously trained convolutional neural network) and phrases
that are used to described them. the system is then able to infer phrases from a
given image sample. based on caption syntax statistics, we propose a simple lan-
guage model that can produce relevant descriptions for a given test image using the
phrases inferred. our approach, which is considerably simpler than state-of-the-
art models, achieves comparable results on the recently release microsoft coco
dataset.

1

introduction

being able to automatically generate a description from an image is a fundamental problem in ar-
ti   cial intelligent, connecting id161 and natural language processing. the problem is
particularly challenging because it requires to correctly recognize different objects in images and
also how they interact.
convolutional neural networks (id98) have achieved state of the art results in different computer
vision tasks in the last few years. more recently, different authors proposed automatic image sen-
tence description approaches based on deep neural networks. all the solutions use the representation
of images generated by id98 that was previously trained for object recognition tasks as start point.
vinyals et al. (2014) consider the problem in a similar way as a machine translation problem. the
authors propose a encoder/decoder (id98/id137) system that is trained to maximize the
likelihood of the target description sentence given a training image. kiros et al. (2014) also consider
a encoder/decoder pipeline, but uses a combination of id98 and id137 for encoding and a
language model for decoding. karpathy & fei-fei (2014) propose an approach that is a combination
of id98, id182 over sentences and a structured objective respon-
sible for a multimodal embedding. they propose a second recurrent neural network architecture to
generate new sentences. similar to the previous works, mao et al. (2014) and donahue et al. (2014)
propose a system that uses a id98 to extract image features and a deep recurrent neural network for
sentences. the two networks interact with each other in a multimodal common layer.

   these two authors contributed equally to this work.
   all research was conducted at the idiap research institute, before ronan collobert joined facebook ai

research.

1

accepted as a workshop contribution at iclr 2015

figure 1: schematic illustration of our phrase-based model for image descriptions.

fang et al. (2014) propose a different approach to the problem that does not rely on recurrent neural
networks. their solution can be divided into three steps: (i) visual detector for words that com-
monly occur are trained using multiple instance learning, (ii) a set of sentences are generated using
a maximum-id178 language-model and (iii) the sentences are re-ranked using sentence-level fea-
tures and a proposed deep multimodal similarity model.
this paper proposes a different approach to the problem. we propose a system that at the same time:
(i) automatically generates a sentence describing a given scene and (ii) is relatively simpler than the
recently proposed approaches. our model shares some similarities with previously proposed deep
approaches. for instance, we also use a pre-trained id98 to extract image features and we also
consider a multimodal embedding. however, thanks to the phrase-based approach, we do not use
any complex recurrent network for sentence generation.
we represent the ground-truth sentences as a collection of noun, verb and prepositional phrases.
each phrase is represented by the mean of the vector representation of the words that compose
it. we then train a simple linear embedding model that transform an image representation into
a multimodal space that is common to the image and the phrases that are used to describe them.
to automatically generate sentences in id136 time, we (i) infer the phrases that correspond to
the sample image and (ii) use a simple language model based on the statistics of the ground-truth
sentences present in the corpus.

2 phrase-based model for image descriptions

2.1 understanding structures of image descriptions

the art of writing sentences can vary a lot according to the domain it is being applied. when
reporting news or reviewing an item, not only the choice of the words might vary, but also the
general structure of the sentence. sentence structures used for describing images can therefore be
identi   ed.
they possess a very distinct structure, usually describing the different objects present on the scene
and how they interact between each other. this interaction among objects is described as actions
or relative position between different objects. the sentence can be short or long, but it generally
respects this process. this statement is illustrated with the ground-truth sentence descriptions of the
image in figure 1.

chunking-based approach all the key elements in a given image are usually described with a
noun phrase (np). interactions between these elements can then be explained using prepositional
phrases (pp) or verb phrases (vp). describing an image is therefore just a matter of identifying
these constituents to describe images. we propose to train a model which can predict the phrases
which are likely to be in a given image.

2

ua man in a helment skateboarding before an audience.man riding on edge of an oval ramp with a skate board.a man riding a skateboard up the side of a wooden ramp.a man on a skateboard is doing a trick.a man is grinding a ramp on a skateboard.va mana wooden rampridingona skate boardis grindingwithnpvpppaccepted as a workshop contribution at iclr 2015

phrase representations noun phrases or verb phrases are often a combination of several words.
good word vector representations can be obtained very quickly with many different recent ap-
proaches (mikolov et al., 2013b; mnih & kavukcuoglu, 2013; pennington et al., 2014; lebret &
collobert, 2014). mikolov et al. (2013a) also showed that simple vector addition can often pro-
duce meaningful results, such as king - man + woman     queen. by leveraging the ability of these
word vector representations to compose, representations for phrases are easily computed with an
element-wise addition.

from phrases to sentence after identifying
the most likely constituents of the image, we
propose to use a statistical language model to
combine them and generate a proper descrip-
tion. a general framework is de   ned to reduce
the total number of combination and thus speed
up the process for generating sentences. the
constrained language model used is illustrated
in figure 2. in general, a noun phrase is always
followed by a verb phrase or a prepositional
phrase, and both are then followed by another
noun phrase. this process is repeated n times
until reaching the end of a sentence (character-
ized by a period). this heuristic is based on
the analysis of syntax if the sentences (see sec-
tion 3.1).

2.2 a multimodal representation

start

np

c

vp

pp

.

c

n

figure 2: the constrained language model for
generating description given the predicted phrases
for an image.

image representations for the representa-
tion of images, we choose to use a convolu-
tional neural network. id98s have been widely used for many different vision domains and are
currently the state-of-the-art in many object recognition tasks. we consider a id98 that has been
pre-trained for the task of object classi   cation. we use a id98 solely to the purpose of feature
extraction, that is, no learning is done in the id98 layers.
learning of a common space for image and phrase representations let i be the set of training
images, d the set of all sentence descriptions for i, c the set of all phrases occuring in d, and    the
trainable parameters of the model. di is the set of sentences describing a given image i     i, and cd
is the set of phrases which compose a sentence description d     di. the training objective is to    nd
the phrases c that describe the images i by maximizing the log id203:

(cid:88)

(cid:88)

(cid:88)

i   i

d   di

c   cd

log p(c|i)

(1)

each image i     i is represented by a vector xi     rn thanks to a pre-trained id98. each phrase c is
composed of k words w which are represented by a vector xw     rm thanks to another pre-trained
model for word representations. a vector representation zc for a phrase c = {w1, . . . , wk} is then
calculated by averaging its word vector representations:

k=1

zc =

xwk .

(cid:3)     rm  |c| .

(cid:2)zc1 , . . . , zc|c|

(2)
vector representations for all phrases c     c can thus be obtained to build a matrix v =
in general, m (cid:28) n. an encoding function is therefore de   ned to
map image representations xi     rn in the same vector space than phrase representations zc     rm:
(3)
where u     rn  m is initialized randomly and trained to encode images in the same vectorial space
than the phrases used for their descriptions. because representations of images and phrases are in a
common vector space, similarities between a given image i and all phrases can be calculated:

g  (i) = xiu ,

k(cid:88)

1
k

f  (i) = g  (i)v ,

3

(4)

accepted as a workshop contribution at iclr 2015

(a) the number of phrases per sentence.

(b) the 20 most frequent sentence syntactic structures.

figure 3: sentence structure statistics of coco datasets.

where v is    ne-tuned to incorporate other features coming from the images. by denoting [f  (i)]j
the score for the jth phrase, this score can be interpreted as the id155 p(c = cj|i,   )
by applying a softmax operation over all the phrases:

p(c = cj|i,   ) =

(cid:80)|c|

e[f  (i)]j
k=1 e[f  (i)]k

.

(5)

in practice, this formulation is often impractical due to the large set of possible phrases c.
training with negative sampling with    = {u, v } and a negative sampling approach, we instead
minimize the following logistic id168 with respect to   :

   (cid:55)   (cid:88)

(cid:88)

(cid:88)

(cid:16)

i   i

d   di

cj   cd

(cid:16)

(cid:16)

(cid:17)

n(cid:88)

k=1

log

1 + e[f  (i)]j

+

log

1 + e   [f  (i)]k

.

(6)

(cid:17)(cid:17)

thus the task is to distinguish the target phrase from draws from the noise distribution, where there
are n negative samples for each data sample. the model is trained using stochastic id119.

3 experiments

3.1 experimental setup

dataset we validate our model on the recently proposed coco dataset (lin et al., 2014), which
contains complex images with multiple objects. the dataset contains a total of 123,000 images,
each of them with 5 human annotated sentences. the testing images has not yet been released. we
thus use two sets of 5,000 images from the validation images for validation and test, as in karpathy
& fei-fei (2014)1. we measure the quality of of the generated sentences using the popular, yet
controversial, id7 score (papineni et al., 2002).

feature selection following karpathy & fei-fei (2014), the image features are extracted using
vgg id98 (chat   eld et al., 2014). this model generates image representations of dimension 4096
form rgb input images. for sentence features, we extract phrases from the 576,737 training sen-
tences with the senna software2. statistics reported in figure 3 con   rm the hypothesis that image

1available at http://cs.stanford.edu/people/karpathy/deepimagesent/
2available at http://ml.nec-labs.com/senna/

4

01234567npvpppappareance frequencies (%)020406080llllllllllllllllllllappareance frequencies (%)np vp np pp np onp vp np onp pp np vp np onp pp np pp np onp vp np pp np pp np onp vp np vp np onp pp np vp np pp np onp pp np onp pp np pp np pp np onp np vp np onp pp np o np onp np vp np pp np onp vp np vp np pp np onp vp np pp np vp np onp pp np pp np vp np onp vp np pp np pp np pp np onp vp np o np onp o np vp np onp pp np np vp np onp vp np sbar vp np o051015accepted as a workshop contribution at iclr 2015

descriptions have a simple syntactic structure. a large majority of sentences contain from two to four
noun phrases. two noun phrases then interact using a verb or prepositional phrase. only phrases
occuring at least ten times in the training set are considered. this results in 11,688 noun phrases,
3,969 verb phrases3 and 219 prepositional phrases. phrase representations are then computed by
averaging vector representations of their words. we obtained word vector representations from the
hellinger pca of a word co-occurence matrix, following the method described in lebret & col-
lobert (2014). the word co-occurence matrix is built over the entire english wikipedia4, with a
symmetric context window of ten words coming from the 10,000 most frequent words. words, and
therefore also phrases, are represented in 400-dimensional vectors.
learning multimodal representation the parameters    are u     r4096  400 and v    
r400  15876. the latter is initialized with the phrase representations. they are trained with n = 15
negative samples and a learning rate set to 0.00025.

generating sentences from the predicted phrases according to the statistics of ground-truth
sentence structures, we set n = {2, 3, 4}. as nodes, we consider only the top twenty predicted
noun phrases, the top ten predicted verb phrases and the top    ve predicted prepositional phrases. a
trigram language model is used for the transition probabilities between two nodes. the id203
of each lexical phrase is calculated using the previous phrases, p(cj|cj   2, cj   1), and the constraint
described in figure 2.
in order to reduce the number of sentences generated, we just consider
the transitions which are likely to happen (we discard any sentence which would have a trigram
transition id203 inferior to 0.01). this thresholding also helps to discard sentences that are
semantically incorrect.

ranking generated sentences our    nal step consists on ranking the sentences generated and
choosing the one with the highest score as the    nal output. for each test image i, we generate a set of
m sentence candidates using the proposed language model. for each sentence sm (m     {1, ..., m}),
we compute its vector representation zsm by averaging the representation of the phrases zc     v that
make the sentence. the    nal score for each sentence sm is computed by doing a dot product between
the sentence vector representation and the encoded representation of the sample image i:

f  (i, m) = g  (i)zsm .

(7)

the output of the system is the sentence which has the highest score. this ranking helps the system
to chose the sentence which is closer to the sample image.

3.2 experimental results

table 1 show our sentence generation results on the coco dataset. id7 scores are reported up to
4-grams. human agreement scores are computed by comparing one of the ground-truth description
against the others. for comparison, we include results from recently proposed models. although we
use the same test set as in karpathy & fei-fei (2014), there are slight variations between the test sets
chosen in other papers. our model gives competitive results at all id165 levels. it is interesting
to note that our results are very close to the human agreement scores. examples of full automatic
generated sentences can be found in figure 4.

4 conclusion and future works

in this paper, we propose a simple model that is able to automatically generate sentences from an im-
age sample. our model is considerably simpler than the current state of the art, which uses complex
recurrent neural networks. we predict phrase components that are likely to describe a given image
and use a simple statistical language model to generate sentences. our model achieves promising
   rst results. future works include apply the model to different datasets (flickr8k, flickr30k and
   nal coco version for benchmarking), do image-sentence ranking experiments and improve the
language model used.

3pre-verbal and post-verbal adverb phrases are merged with verb phrases.
4available at http://download.wikimedia.org. we took the january 2014 version.

5

accepted as a workshop contribution at iclr 2015

captioning method

human agreement

karpathy & fei-fei (2014)
vinyals et al. (2014)
donahue et al. (2014)
fang et al. (2014)
our model

b-1

b-2

b-3

b-4

0.68

0.45

0.30

0.20

0.57
0.67
0.63

-

0.37

0.19

-

-

0.44

0.30

-

-

0.70

0.46

0.30

-

0.21
0.21
0.20

table 1: comparison between human agreement scores, state of the art models and our model on
the coco dataset. note that there are slight variations between the test sets chosen in each paper.

figure 4: quantitative results for images on the coco dataset. ground-truth annotation (in blue),
the np, vp and pp predicted from the model and generated annotation (in black) are shown for each
image. the two last are failure samples.

acknowledgements

this work was supported by the hasler foundation through the grant    information and commu-
nication technology for a better world 2020    (smartworld).

references
chat   eld, k., simonyan, k., vedaldi, a., and zisserman, a. return of the devil in the details:

delving deep into convolutional nets. in british machine vision conference, 2014.

donahue, jeff, hendricks, lisa anne, guadarrama, sergio, rohrbach, marcus, venugopalan, sub-
hashini, saenko, kate, and darrell, trevor. long-term recurrent convolutional networks for visual
recognition and description. corr, abs/1411.4389, 2014.

fang, hao, gupta, saurabh, iandola, forrest n., srivastava, rupesh, deng, li, doll  ar, piotr, gao,
jianfeng, he, xiaodong, mitchell, margaret, platt, john c., zitnick, c. lawrence, and zweig,
geoffrey. from captions to visual concepts and back. corr, abs/1411.4952, 2014.

6

a man riding skis on a snow covered ski slope. np: a man, skis, the snow, a person, a woman, a snow covered slope,  a slope, a snowboard, a skier, man. vp: wearing, riding, holding, standing on, skiing down. pp: on, in, of, with, down. a man wearing skis on the snow.a man is doing skateboard tricks on a ramp. np: a skateboard, a man, a trick, his skateboard, the air, a skateboarder, a ramp, a skate board, a person, a woman. vp: doing, riding, is doing, performing,    ying through. pp: on, of, in, at, with. a man riding a skateboard on a ramp.the girl with blue hair stands under the umbrella. np: a woman, an umbrella, a man, a person, a girl, umbrellas, that, a little girl, a cell phone. vp: holding, wearing, is holding, holds, carrying. pp: with, on, of, in, under. a woman is holding an umbrella.a slice of pizza sitting on top of a white plate. np:  a plate, a white plate, a table, pizza, it, a pizza, food, a sandwich, top, a close. vp: topped with, has, is, sitting on, is on. pp: of, on, with, in, up. a table with a plate of pizza on a white plate.a person on a surf board in the ocean. np: a dog, a wave, a person, the water, a man, the ocean, top, that, the snow, a surfboard. vp: riding, standing on, wearing, laying on, sitting on. pp: on, of, in, with, near. a dog standing on top of a wave on the ocean.a cat sitting in a chair staring at a plate on a table. np: a table, top, a desk, a cat, front, it, that, a laptop, a laptop computer, the table. vp: sitting on, is, sitting in, sitting next to, has. pp: of, on, with, in, next to. a cat sitting on top of a desk with a laptop.accepted as a workshop contribution at iclr 2015

karpathy, andrej and fei-fei, li. deep visual-semantic alignments for generating image descrip-

tions. corr, abs/1412.2306, 2014.

kiros, r., salakhutdinov, r., and zemel, r. s. unifying visual-semantic embeddings with multi-

modal neural language models. volume abs/1411.2539, 2014.

lebret, remi and collobert, ronan. rehabilitation of count-based models for word vector repre-

sentations. corr, abs/1412.4930, 2014.

lin, tsung-yi, maire, michael, belongie, serge, hays, james, perona, pietro, ramanan, deva,
doll  ar, piotr, and zitnick, c. lawrence. microsoft coco: common objects in context. in european
conference on id161 (eccv), 2014.

mao, junhua, xu, wei, yang, yi, wang, jiang, and yuille, alan l. explain images with multimodal

recurrent neural networks. corr, abs/1410.1090, 2014.

mikolov, t., chen, k., corrado, g., and dean, jeff. ef   cient estimation of word representations

in vector space. iclr workshp, 2013a.

mikolov, t., sutskever, i., chen, k., corrado, g., and dean, j. distributed representations of words

and phrases and their compositionality. in nips. 2013b.

mnih, a. and kavukcuoglu, koray. learning id27s ef   ciently with noise-contrastive

estimation. in nips. 2013.

papineni, kishore, roukos, salim, ward, todd, and zhu, wei-jing. id7: a method for automatic
evaluation of machine translation. in proceedings of the 40th annual meeting on association for
computational linguistics, 2002.

pennington, j., socher, richard, and manning, c. d. glove: global vectors for word representa-

tion. in proceedings of emnlp, 2014.

vinyals, oriol, toshev, alexander, bengio, samy, and erhan, dumitru. show and tell: a neural

image caption generator. corr, abs/1411.4555, 2014.

7

