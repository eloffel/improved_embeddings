5
1
0
2

 

y
a
m
0
3

 

 
 
]
l
c
.
s
c
[
 
 

4
v
6
0
2
8

.

0
1
4
1
:
v
i
x
r
a

addressing the rare word problem in

id4

minh-thang luong       

stanford

lmthang@stanford.edu

ilya sutskever   

google

quoc v. le   

google

oriol vinyals

google

wojciech zaremba   
new york university

{ilyasu,qvl,vinyals}@google.com

woj.zaremba@gmail.com

abstract

id4 (id4) is a
new approach to machine translation that
has shown promising results that are com-
parable to traditional approaches. a sig-
ni   cant weakness in conventional id4
systems is their inability to correctly trans-
late very rare words: end-to-end id4s
tend to have relatively small vocabularies
with a single unk symbol that represents
every possible out-of-vocabulary (oov)
word. in this paper, we propose and im-
plement an effective technique to address
this problem. we train an id4 system
on data that is augmented by the output
of a word alignment algorithm, allowing
the id4 system to emit, for each oov
word in the target sentence, the position of
its corresponding word in the source sen-
tence. this information is later utilized in
a post-processing step that translates every
oov word using a dictionary. our exper-
iments on the wmt   14 english to french
translation task show that this method pro-
vides a substantial improvement of up to
2.8 id7 points over an equivalent id4
system that does not use this technique.
with 37.5 id7 points, our id4 sys-
tem is the    rst to surpass the best result
achieved on a wmt   14 contest task.

1 introduction

id4 (id4) is a novel
approach to mt that has achieved promising
(kalchbrenner and blunsom, 2013;
results
cho et al., 2014;
sutskever et al., 2014;
bahdanau et al., 2015;
an
id4 system is a conceptually simple large neural

jean et al., 2015).

   work done while the authors were in google.     indicates

equal contribution.

network that reads the entire source sentence
and produces an output translation one word at a
time. id4 systems are appealing because they
use minimal domain knowledge which makes
them well-suited to any problem that can be
formulated as mapping an input sequence to
an output sequence (sutskever et al., 2014).
in
addition, the natural ability of neural networks to
generalize implies that id4 systems will also
generalize to novel word phrases and sentences
that do not occur in the training set. in addition,
id4 systems potentially remove the need to store
explicit phrase tables and language models which
are used in conventional systems. finally, the
decoder of an id4 system is easy to implement,
unlike the highly intricate decoders used by
phrase-based systems (koehn et al., 2003).

to represent

despite these advantages, conventional id4
systems are incapable of translating rare words
because they have a    xed modest-sized vocab-
ulary1 which forces them to use the unk sym-
bol
the large number of out-of-
vocabulary (oov) words, as illustrated in fig-
ure 1. unsurprisingly, both sutskever et al. (2014)
and bahdanau et al. (2015) have observed that
sentences with many rare words tend to be trans-
lated much more poorly than sentences contain-
ing mainly frequent words.
standard phrase-
based systems (koehn et al., 2007; chiang, 2007;
cer et al., 2010; dyer et al., 2010), on the other
hand, do not suffer from the rare word problem to
the same extent because they can support a much
larger vocabulary, and because their use of ex-
plicit alignments and phrase tables allows them to
memorize the translations of even extremely rare

1 due to the computationally intensive nature of the soft-
max, id4 systems often limit their vocabularies to be the
top 30k-80k most frequent words in each language. how-
ever, jean et al. (2015) has very recently proposed an ef   cient
approximation to the softmax that allows for training ntms
with very large vocabularies. as discussed in section 2, this
technique is complementary to ours.

en: the ecotax portico in pont-de-buis , . . . [truncated] . . . , was taken down on thursday morning

            
fr: le portique   ecotaxe de pont-de-buis , . . . [truncated] . . . , a   et  e d  emont  e jeudi matin

            
            

   
   
   

      

   
   

   

   

nn: le unk de unk `a unk , . . . [truncated] . . . , a   et  e pris le jeudi matin

figure 1: example of the rare word problem     an english source sentence (en), a human translation to
french (fr), and a translation produced by one of our neural network systems (nn) before handling oov
words. we highlight words that are unknown to our model. the token unk indicates an oov word. we
also show a few important alignments between the pair of sentences.

words.

motivated by the strengths of standard phrase-
based system, we propose and implement a novel
approach to address the rare word problem of
id4s. our approach annotates the training cor-
pus with explicit alignment information that en-
ables the id4 system to emit, for each oov
word, a    pointer    to its corresponding word in the
source sentence. this information is later utilized
in a post-processing step that translates the oov
words using a dictionary or with the identity trans-
lation, if no translation is found.

our experiments con   rm that this approach is
effective. on the english to french wmt   14
translation task,
this approach provides an im-
provement of up to 2.8 (if the vocabulary is rel-
atively small) id7 points over an equivalent
id4 system that does not use this technique.
moreover, our system is the    rst id4 that out-
performs the winner of a wmt   14 task.

2 id4

a id4 system is any neural
network that maps a source sentence, s1, . . . , sn,
to a target sentence, t1, . . . , tm, where all sen-
tences are assumed to terminate with a special
   end-of-sentence    token <eos>. more con-
cretely, an id4 system uses a neural network to
parameterize the conditional distributions

p(tj|t<j, s   n)

(1)

for

distributions.

there are many ways to parameterize these
conditional
example,
kalchbrenner and blunsom (2013) used a com-
bination of a convolutional neural network and a
recurrent neural network, sutskever et al. (2014)
used a deep long short-term memory (lstm)
model, cho et al. (2014) used an architecture
similar to the lstm, and bahdanau et al. (2015)
used a more elaborate neural network architecture
that uses an attentional mechanism over
the
input sequence,
similar to graves (2013) and
graves et al. (2014).

in

use

the model

this work, we

of
sutskever et al. (2014), which uses a deep lstm
to encode the input sequence and a separate deep
lstm to output the translation. the encoder
reads the source sentence, one word at a time,
and produces a large vector that represents the
entire source sentence. the decoder is initialized
with this vector and generates a translation, one
word at a time, until it emits the end-of-sentence
symbol <eos>.

none the early work in neural machine transla-
tion systems has addressed the rare word problem,
but the recent work of jean et al. (2015) has tack-
led it with an ef   cient approximation to the soft-
max to accommodate for a very large vocabulary
(500k words). however, even with a large vocab-
ulary, the problem with rare words, e.g., names,
numbers, etc., still persists, and jean et al. (2015)
found that using techniques similar to ours are
bene   cial and complementary to their approach.

for 1     j     m. by doing so, it becomes pos-
sible to compute and therefore maximize the log
id203 of the target sentence given the source
sentence

log p(t|s) =

m

x

j=1

log p (tj|t<j, s   n)

(2)

3 rare word models

despite the relatively large amount of work done
on pure id4 systems, there
has been no work addressing the oov problem
in id4 systems, with the notable exception of
jean et al. (2015)   s work mentioned earlier.

en: the unk1 portico in unk2

. . .

en: the unk portico in unk . . .

fr: le unk    unk1 de unk2

. . .

figure 2: copyable model     an annotated exam-
ple with two types of unknown tokens:    copyable   
unkn and null unk   .

fr: le p0 unk p   1 unk p1 de p    unk p   1 . . .
figure 3: positional all model     an example of
the posall model. each word is followed by the
relative positional tokens pd or the null token p   .

we propose to address the rare word problem
by training the id4 system to track the origins
of the unknown words in the target sentences. if
we knew the source word responsible for each un-
known target word, we could introduce a post-
processing step that would replace each unk in
the system   s output with a translation of its source
word, using either a dictionary or the identity
translation.
if the
model knows that the second unknown token in
the id4 (line nn) originates from the source word
ecotax, it can perform a word dictionary lookup
to replace that unknown token by   ecotaxe. sim-
ilarly, an identity translation of the source word
pont-de-buis can be applied to the third un-
known token.

for example,

in figure 1,

we

three

present

annotation

can easily be

strategies
applied to any id4
that
system
(kalchbrenner and blunsom, 2013;
sutskever et al., 2014; cho et al., 2014). we treat
the id4 system as a black box and train it on
a corpus annotated by one of the models below.
first, the alignments are produced with an unsu-
pervised aligner. next, we use the alignment links
to construct a word dictionary that will be used
for the word translations in the post-processing
step.2 if a word does not appear in our dictionary,
then we apply the identity translation.

the    rst few words of the sentence pair in fig-

ure 1 (lines en and fr) illustrate our models.

3.1 copyable model

in this approach, we introduce multiple tokens
to represent the various unknown words in the
source and in the target language, as opposed to
using only one unk token. we annotate the oov
words in the source sentence with unk1, unk2,
unk3, in that order, while assigning repeating un-
known words identical tokens. the annotation

2when a source word has multiple translations, we use
the translation with the highest id203. these translation
probabilities are estimated from the unsupervised alignment
links. when constructing the dictionary from these alignment
links, we add a word pair to the dictionary only if its align-
ment count exceeds 100.

of the unknown words in the target language is
slightly more elaborate: (a) each unknown target
word that is aligned to an unknown source word
is assigned the same unknown token (hence, the
   copy    model) and (b) an unknown target word
that has no alignment or that is aligned with a
known word uses the special null token unk   . see
figure 2 for an example. this annotation enables
us to translate every non-null unknown token.

3.2 positional all model (posall)

the copyable model is limited by its inability to
translate unknown target words that are aligned
to known words in the source sentence, such as
the pair of words,    portico    and    portique   , in our
running example. the former word is known on
the source sentence; whereas latter is not, so it
is labelled with unk   . this happens often since
the source vocabularies of our models tend to be
much larger than the target vocabulary since a
large source vocabulary is cheap. this limita-
tion motivated us to develop an annotation model
that includes the complete alignments between the
source and the target sentences, which is straight-
forward to obtain since the complete alignments
are available at training time.

speci   cally, we return to using only a single
universal unk token. however, on the target
side, we insert a positional token pd after ev-
ery word. here, d indicates a relative position
(d =    7, . . . ,    1, 0, 1, . . . , 7) to denote that a tar-
get word at position j is aligned to a source word
at position i = j     d. aligned words that are too
far apart are considered unaligned, and unaligned
words rae annotated with a null token pn. our an-
notation is illustrated in figure 3.

3.3 positional unknown model (posunk)

the main weakness of the posall model is that
it doubles the length of the target sentence. this
makes learning more dif   cult and slows the speed
of parameter updates by a factor of two. how-
ever, given that our post-processing step is con-
cerned only with the alignments of the unknown

words, so it is more sensible to only annotate the
unknown words. this motivates our positional un-
known model which uses unkposd tokens (for d
in    7, . . . , 7 or    ) to simultaneously denote (a)
the fact that a word is unknown and (b) its rela-
tive position d with respect to its aligned source
word. like the posall model, we use the symbol
unkpos    for unknown target words that do not
have an alignment. we use the universal unk for
all unknown tokens in the source language. see
figure 4 for an annotated example.

en: the unk portico in unk . . .

fr: le unkpos1 unkpos   1 de unkpos1 . . .
figure 4: positional unknown model     an exam-
ple of the posunk model: only aligned unknown
words are annotated with the unkposd tokens.

it is possible that despite its slower speed, the
posall model will learn better alignments because
it is trained on many more examples of words and
their alignments. however, we show that this is
not the case (see   5.2).

4 experiments

we evaluate the effectiveness of our oov mod-
els on the wmt   14 english-to-french transla-
tion task.translation quality is measured with the
id7 metric (papineni et al., 2002) on the new-
stest2014 test set (which has 3003 sentences).

4.1 training data
to be comparable with the results reported by
previous work on id4
systems (sutskever et al., 2014; cho et al., 2014;
bahdanau et al., 2015), we train our models on
the same training data of 12m parallel sentences
(348m french and 304m english words), obtained
from (schwenk, 2014). the 12m subset was se-
lected from the full wmt   14 parallel corpora us-
ing the method proposed in axelrod et al. (2011).
due to the computationally intensive nature of
the naive softmax, we limit the french vocabulary
(the target language) to the either the 40k or the
80k most frequent french words. on the source
side, we can afford a much larger vocabulary, so
we use the 200k most frequent english words.
the model treats all other words as unknowns.3

we annotate our training data using the three
schemes described in the previous section. the
alignment is computed with the berkeley aligner
(liang et al., 2006) using its default settings. we
discard sentence pairs in which the source or the
target sentence exceed 100 tokens.

4.2 training details

are

training

procedure
similar

and
hyperparame-
our
to those used by
ter choices
sutskever et al. (2014).
in more details, we
train multi-layer deep lstms, each of which has
1000 cells, with 1000 dimensional embeddings.
like sutskever et al. (2014), we reverse the words
in the source sentences which has been shown to
improve lstm memory utilization and results
in better translations of long sentences. our hy-
perparameters can be summarized as follows: (a)
the parameters are initialized uniformly in [-0.08,
0.08] for 4-layer models and [-0.06, 0.06] for 6-
layer models, (b) sgd has a    xed learning rate of
0.7, (c) we train for 8 epochs (after 5 epochs, we
begin to halve the learning rate every 0.5 epoch),
(d) the size of the mini-batch is 128, and (e) we
rescale the normalized gradient to ensure that its
norm does not exceed 5 (pascanu et al., 2012).

we also follow the gpu parallelization scheme
proposed in (sutskever et al., 2014), allowing us
to reach a training speed of 5.4k words per
second to train a depth-6 model with 200k
source and 80k target vocabularies ; whereas
sutskever et al. (2014) achieved 6.3k words per
second for a depth-4 models with 80k source and
target vocabularies. training takes about 10-14
days on an 8-gpu machine.

4.3 a note on id7 scores

(a)
we report id7 scores based on both:
detokenized translations, i.e., wmt   14 style, to
be comparable with results reported on the wmt
website4 and (b) tokenized translations, so as to be
consistent with previous work (cho et al., 2014;
bahdanau et al., 2015;
schwenk, 2014;
sutskever et al., 2014; jean et al., 2015).5

the existing wmt   14 state-of-the-art

sys-
tem (durrani et al., 2014) achieves a detokenized
id7 score of 35.8 on the newstest2014 test set
for english to french language pair (see table 2).

3when the french vocabulary has 40k words, there are
on average 1.33 unknown words per sentence on the target
side of the test set.

4http://matrix.statmt.org/matrix
5the tokenizer.perl and multi-id7.pl

scripts are used to tokenize and score translations.

system
state of the art in wmt   14 (durrani et al., 2014)

standard mt + neural components

schwenk (2014)     neural language model
cho et al. (2014)    phrase table neural features
sutskever et al. (2014)     5 lstms, reranking 1000-best lists

existing end-to-end id4 systems

vocab corpus id7

all

all
all
all

36m 37.0

12m 33.3
12m 34.5
12m 36.5

bahdanau et al. (2015)     single gated id56 with search
sutskever et al. (2014)     5 lstms
jean et al. (2015)     8 gated id56s with search + unk replacement

30k
80k
500k

12m 28.5
12m 34.8
12m 37.2

our end-to-end id4 systems

single lstm with 4 layers
single lstm with 4 layers + posunk
single lstm with 6 layers
single lstm with 6 layers + posunk
ensemble of 8 lstms
ensemble of 8 lstms + posunk
single lstm with 6 layers
single lstm with 6 layers + posunk
ensemble of 8 lstms
ensemble of 8 lstms + posunk

40k
40k
40k
40k
40k
40k
80k
80k
80k
80k

12m 29.5
12m 31.8 (+2.3)
12m 30.4
12m 32.7 (+2.3)
12m 34.1
12m 36.9 (+2.8)
36m 31.5
36m 33.1 (+1.6)
36m 35.6
36m 37.5 (+1.9)

table 1: tokenized id7 on newstest2014     translation results of various systems which differ in
terms of: (a) the architecture, (b) the size of the vocabulary used, and (c) the training corpus, either
using the full wmt   14 corpus of 36m sentence pairs or a subset of it with 12m pairs. we highlight
the performance of our best system in bolded text and state the improvements obtained by our technique
of handling rare words (namely, the posunk model). notice that, for a given vocabulary size, the more
accurate systems achieve a greater improvement from the post-processing step. this is the case because
the more accurate models are able to pin-point the origin of an unknown word with greater accuracy,
making the post-processing more useful.

in terms of the tokenized id7, its performance
is 37.0 points (see table 1).

system
existing sota (durrani et al., 2014)
ensemble of 8 lstms + posunk

id7
35.8
36.6

table 2: detokenized id7 on newstest2014    
translation results of the existing state-of-the-art
system and our best system.

4.4 main results
we compare our systems to others,including
the
system
(durrani et al., 2014),
recent end-to-end neu-
ral systems, as well as phrase-based baselines
with neural components.

state-of-the-art mt

current

the results shown in table 1 demonstrate that
our unknown word translation technique (in par-
ticular, the posunk model) signi   cantly improves

the translation quality for both the individual (non-
ensemble) lstm models and the ensemble mod-
els.6 for 40k-word vocabularies, the performance
gains are in the range of 2.3-2.8 id7 points.
with larger vocabularies (80k), the performance
gains are diminished, but our technique can still
provide a nontrivial gains of 1.6-1.9 id7 points.
it is interesting to observe that our approach is
more useful for ensemble models as compared to
the individual ones. this is because the useful-
ness of the posunk model directly depends on the
ability of the id4 to correctly locate, for a given
oov target word, its corresponding word in the
source sentence. an ensemble of large models
identi   es these source words with greater accu-

6 for the 40k-vocabulary ensemble, we combine 5 mod-
els with 4 layers and 3 models with 6 layers. for the 80k-
vocabulary ensemble, we combine 3 models with 4 layers and
5 models with 6 layers. two of the depth-6 models are reg-
ularized with dropout, similar to zaremba et al. (2015) with
the dropout id203 set to 0.2.

racy. this is why for the same vocabulary size,
better models obtain a greater performance gain
our post-processing step. e except for the very re-
cent work of jean et al. (2015) that employs a sim-
ilar unknown treatment strategy7 as ours, our best
result of 37.5 id7 outperforms all other id4
systems by a arge margin, and more importanly,
our system has established a new record on the
wmt   14 english to french translation.

5 analysis

we analyze and quantify the improvement ob-
tained by our rare word translation approach and
provide a detailed comparison of the different
rare word techniques proposed in section 3. we
also examine the effect of depth on the lstm
architectures and demonstrate a strong correla-
tion between perplexities and id7 scores. we
also highlight a few translation examples where
our models succeed in correctly translating oov
words, and present several failures.

5.1 rare word analysis

the

effect of

to analyze
rare words on
translation quality, we follow sutskever et
al. (sutskever et al., 2014) and sort sentences in
newstest2014 by the average inverse frequency
of their words. we split the test sentences into
groups where the sentences within each group
have a comparable number of rare words and
evaluate each group independently. we evaluate
our systems before and after translating the oov
words and compare with the standard mt systems
    we use the best system from the wmt   14
contest
(durrani et al., 2014), and neural mt
systems     we use the ensemble systems described
in (sutskever et al., 2014) and section 4.

rare word translation is challenging for neu-
ral machine translation systems as shown in fig-
ure 5. speci   cally, the translation quality of our
model before applying the postprocessing step is
shown by the green curve, and the current best

7their unknown replacement method and ours both track
the locations of target unknown words and use a word dic-
tionary to post-process the translation. however, the mech-
anism used to achieve the    tracking    behavior is different.
jean et al. (2015)   s uses the attentional mechanism to track
the origins of all target words, not just the unknown ones. in
contrast, we only focus on tracking unknown words using un-
supervised alignments. our method can be easily applied to
any sequence-to-sequence models since we treat any model
as a blackbox and manipulate only at the input and output
levels.

42

40

38

36

34

32

30

 

28
0

u
e
l
b

sota durrani et al. (37.0)
sutskever et al. (34.8)
ours (35.6)
ours + posunk (37.5)

500

1000

1500

2000

sents

2500

3000

figure 5: rare word translation     on the x-axis,
we order newstest2014 sentences by their aver-
age frequency rank and divide the sentences into
groups of sentences with a comparable prevalence
of rare words. we compute the id7 score of
each group independently.

id4 system (sutskever et al., 2014) is the pur-
ple curve. while (sutskever et al., 2014) pro-
duces better translations for sentences with fre-
quent words (the left part of the graph),
they
are worse than best system (red curve) on sen-
tences with many rare words (the right side of
the graph). when applying our unknown word
translation technique (purple curve), we signi   -
cantly improve the translation quality of our id4:
for the last group of 500 sentences which have
the greatest proportion of oov words in the test
set, we increase the id7 score of our system
by 4.8 id7 points. overall, our rare word
translation model interpolates between the sota
system and the system of sutskever et al. (2014),
which allows us to outperform the winning en-
try of wmt   14 on sentences that consist predom-
inantly of frequent words and approach its perfor-
mance on sentences with many oov words.

5.2 rare word models

we examine the effect of the different rare word
models presented in section 3, namely: (a) copy-
able     which aligns the unknown words on both
the input and the target side by learning to copy in-
dices, (b) the positional all (posall)     which pre-
dicts the aligned source positions for every target
word, and (c) the positional unknown (posunk)
    which predicts the aligned source positions for
only the unknown target words.8 it is also interest-

8in this section and in section 5.3, all models are trained
on the unreversed sentences, and we use the following hyper-

(cid:9)
(cid:9)
(cid:9)
(cid:9)
32

30

28

26

24

22

20

u
e
l
b

+2.4

+2.2

+1.0

+0.8

noalign (5.31)

copyable (5.38)

posall (5.30, 1.37)

posunk (5.32)

figure 6: rare word models     translation perfor-
mance of 6-layer lstms: a model that uses no
alignment (noalign) and the other rare word mod-
els (copyable, posall, posunk). for each model,
we show results before (left) and after (right) the
rare word translation as well as the perplexity (in
parentheses). for posall, we report the perplexi-
ties of predicting the words and the positions.

ing to measure the improvement obtained when no
alignment information is used during training. as
such, we include a baseline model with no align-
ment knowledge (noalign) in which we simply as-
sume that the ith unknown word on the target sen-
tence is aligned to the ith unknown word in the
source sentence.

from the results in figure 6, a simple mono-
tone alignment assumption for the noalign model
yields a modest gain of 0.8 id7 points. if we
train the model to predict the alignment, then the
copyable model offers a slightly better gain of 1.0
id7. note, however, that english and french
have similar word order structure, so it would
be interesting to experiment with other language
pairs, such as english and chinese, in which the
word order is not as monotonic. these harder lan-
guage pairs potentially imply a smaller gain for the
noalign model and a larger gain for the copyable
model. we leave it for future work.

the positional models (posall and posunk) im-
prove translation performance by more than 2
id7 points. this proves that the limitation of the
copyable model, which forces it to align each un-
known output word with an unknown input word,
in contrast, the positional mod-
is considerable.

parameters: we initialize the parameters uniformly in [-0.1,
0.1], the learning rate is 1, the maximal gradient norm is 1,
with a source vocabulary of 90k words, and a target vocab-
ulary of 40k (see section 4.2 for more details). while these
lstms do not achieve the best possible performance, it is
still useful to analyze them.

32

30

28

26

24

22

20

u
e
l
b

+2.0

+1.9

+2.2

depth 3 (6.01)

depth 4 (5.71)

depth 6 (5.46)

figure 7: effect of depths     id7 scores
achieved by posunk models of various depths (3,
4, and 6) before and after the rare word transla-
tion. notice that the posunk model is more useful
on more accurate models.

els can align the unknown target words with any
source word, and as a result, post-processing has a
much stronger effect. the posunk model achieves
better translation results than the posall model
which suggests that it is easier to train the lstm
on shorter sequences.

5.3 other effects
deep lstm architecture     we compare posunk
models trained with different number of layers (3,
4, and 6). we observe that the gain obtained by
the posunk model increases in tandem with the
overall accuracy of the model, which is consistent
with the idea that larger models can point to the ap-
propriate source word more accurately. addition-
ally, we observe that on average, each extra lstm
layer provides roughly 1.0 id7 point improve-
ment as demonstrated in figure 7.

26.5

26

25.5

25

24.5

24

23.5

u
e
l
b

23
5.6

5.8

6

6.2

perplexity

6.4

6.6

6.8

figure 8: perplexity vs. id7     we show the
correlation by evaluating an lstm model with 4
layers at various stages of training.

perplexity and id7     lastly, we    nd it inter-
esting to observe a strong correlation between the
perplexity (our training objective) and the transla-
tion quality as measured by id7. figure 8 shows

(cid:9)
(cid:9)
(cid:9)
(cid:9)
(cid:9)
src

sentences
an additional 2600 operations including orthopedic and cataract surgery will
help clear a backlog .

trans en outre , unkpos1 op  erations suppl  ementaires , dont la chirurgie unkpos5

et la unkpos6 , permettront de r  esorber l    arri  er  e .

+unk en outre , 2600 op  erations suppl  ementaires , dont la chirurgie orthop  ediques

tgt

src

et la cataracte , permettront de r  esorber l    arri  er  e .
2600 op  erations suppl  ementaires , notamment dans le domaine de la chirurgie
orthop  edique et de la cataracte , aideront `a rattraper le retard .
this trader , richard usher , left rbs in 2010 and is understand to have be
given leave from his current position as european head of forex spot trading at
jpmorgan .

trans ce unkpos0 , richard unkpos0 , a quitt  e unkpos1 en 2010 et a compris qu   
il est autoris  e `a quitter son poste actuel en tant que leader europ  een du march  e
des points de vente au unkpos5 .

tgt

+unk ce n  egociateur , richard usher , a quitt  e rbs en 2010 et a compris qu    il est
autoris  e `a quitter son poste actuel en tant que leader europ  een du march  e des
points de vente au jpmorgan .
ce trader , richard usher , a quitt  e rbs en 2010 et aurait   et  e mis suspendu
de son poste de responsable europ  een du trading au comptant pour les devises
chez jpmorgan
but concerns have grown after mr mazanga was quoted as saying renamo was
abandoning the 1992 peace accord .

src

trans mais les inqui  etudes se sont accrues apr`es que m. unkpos3 a d  eclar  e que la

unkpos3 unkpos3 l    accord de paix de 1992 .

+unk mais les inqui  etudes se sont accrues apr`es que m. mazanga a d  eclar  e que la

renamo   etait l    accord de paix de 1992 .

tgt mais l    inqui  etude a grandi apr`es que m. mazanga a d  eclar  e que la renamo

abandonnait l    accord de paix de 1992 .

table 3: sample translations     the table shows the source (src) and the translations of our best model
before (trans) and after (+unk) unknown word translations. we also show the human translations (tgt)
and italicize words that are involved in the unknown word translation process.

the performance of a 4-layer lstm, in which we
compute both perplexity and id7 scores at dif-
ferent points during training. we    nd that on aver-
age, a reduction of 0.5 perplexity gives us roughly
1.0 id7 point improvement.

5.4 sample translations

we present three sample translations of our best
system (with 37.5 id7) in table 3.
in our
   rst example,
the model translates all the un-
known words correctly: 2600, orthop  ediques, and
cataracte.
it is interesting to observe that the
model can accurately predict an alignment of dis-
tances of 5 and 6 words. the second example
highlights the fact that our model can translate
long sentences reasonably well and that it was able
to correctly translate the unknown word for jp-

morgan at the very far end of the source sentence.
lastly, our examples also reveal several penalties
incurred by our model: (a) incorrect entries in the
word dictionary, as with n  egociateur vs. trader in
the second example, and (b) incorrect alignment
prediction, such as when unkpos3 is incorrectly
aligned with the source word was and not with
abandoning, which resulted in an incorrect trans-
lation in the third sentence.

6 conclusion

we have shown that a simple alignment-based
technique can mitigate and even overcome one
of the main weaknesses of current id4 systems,
which is their inability to translate words that are
not in their vocabulary. a key advantage of our
technique is the fact that it is applicable to any

[graves et al.2014] a. graves, g. wayne, and i. dani-
arxiv

2014. id63s.

helka.
preprint arxiv:1410.5401.

[graves2013] a. graves. 2013. generating sequences
in arxiv preprint

with recurrent neural networks.
arxiv:1308.0850.

[jean et al.2015] s  ebastien jean, kyunghyun cho,
roland memisevic, and yoshua bengio. 2015. on
using very large target vocabulary for neural ma-
chine translation. in acl.

[kalchbrenner and blunsom2013] n. kalchbrenner and
p. blunsom. 2013. recurrent continuous translation
models. in emnlp.

[koehn et al.2003] philipp koehn, franz josef och,
and daniel marcu. 2003. statistical phrase-based
translation. in naacl.

[koehn et al.2007] philipp koehn, hieu hoang,
alexandra birch, chris callison-burch, marcello
federico, nicola bertoldi, brooke cowan, wade
shen, christine moran, richard zens, et al. 2007.
moses: open source toolkit for statistical machine
translation. in acl, demonstration session.

[liang et al.2006] p. liang, b. taskar, and d. klein.

2006. alignment by agreement. in naacl.

[papineni et al.2002] kishore papineni, salim roukos,
todd ward, and wei jing zhu. 2002. id7: a
method for automatic evaluation of machine trans-
lation. in acl.

[pascanu et al.2012] r. pascanu, t. mikolov,

and
2012. on the dif   culty of train-
arxiv preprint

y. bengio.
ing recurrent neural networks.
arxiv:1211.5063.

[schwenk2014] h.

2014.
http://www-lium.univ-lemans.fr/  schwenk/cslm_joint_paper/
[online; accessed 03-september-2014].

university

le

schwenk.
mans.

[sutskever et al.2014] i. sutskever, o. vinyals, and
q. v. le. 2014. sequence to sequence learning with
neural networks. in nips.

[zaremba et al.2015] wojciech

sutskever, and oriol vinyals.
neural network id173. in iclr.

zaremba,
ilya
2015. recurrent

id4 system and not only to the deep lstm
model of sutskever et al. (2014). a technique like
ours is likely necessary if an id4 system is to
achieve state-of-the-art performance on machine
translation.

we have demonstrated empirically that on the
wmt   14 english-french translation task, our
technique yields a consistent and substantial im-
provement of up to 2.8 id7 points over various
id4 systems of different architectures. most im-
portantly, with 37.5 id7 points, we have estab-
lished the    rst id4 system that outperformed the
best mt system on a wmt   14 contest dataset.

acknowledgments

we thank members of the google brain team
for thoughtful discussions and insights. the    rst
author especially thanks chris manning and the
stanford nlp group for helpful comments on the
early drafts of the paper. lastly, we thank the an-
nonymous reviewers for their valuable feedback.

references
[axelrod et al.2011] amittai axelrod, xiaodong he,
and jianfeng gao. 2011. id20 via
pseudo in-domain data selection. in emnlp.

[bahdanau et al.2015] d. bahdanau, k. cho,

and
y. bengio. 2015. id4 by
jointly learning to align and translate. in iclr.

[cer et al.2010] d. cer, m. galley, d. jurafsky, and
c. d. manning. 2010. phrasal: a statistical ma-
chine translation toolkit for exploring new model
features. in acl, demonstration session.

[chiang2007] david chiang.

hierarchical
phrase-based translation. computational linguis-
tics, 33(2):201   228.

2007.

[cho et al.2014] kyunghyun cho, bart van merrien-
boer, caglar gulcehre, fethi bougares, holger
schwenk, and yoshua bengio.
2014. learning
phrase representations using id56 encoder-decoder
for id151. in emnlp.

[durrani et al.2014] nadir durrani, barry haddow,
philipp koehn, and kenneth hea   eld. 2014. ed-
inburgh   s phrase-based machine translation systems
for wmt-14. in wmt.

[dyer et al.2010] chris dyer, jonathan weese, hendra
setiawan, adam lopez, ferhan ture, vladimir ei-
delman, juri ganitkevitch, phil blunsom, and philip
resnik. 2010. cdec: a decoder, alignment, and
learning framework for    nite-state and context-free
translation models. in acl, demonstration session.

