achieving open vocabulary id4

with hybrid word-character models

minh-thang luong and christopher d. manning

computer science department, stanford university, stanford, ca 94305

{lmthang,manning}@stanford.edu

6
1
0
2

 

n
u
j
 

3
2

 
 
]
l
c
.
s
c
[
 
 

2
v
8
8
7
0
0

.

4
0
6
1
:
v
i
x
r
a

abstract

translate mostly at

nearly all previous work on neural ma-
chine translation (id4) has used quite
restricted vocabularies, perhaps with a
subsequent method to patch in unknown
words. this paper presents a novel word-
character solution to achieving open vo-
cabulary id4. we build hybrid systems
that
the word level
and consult the character components for
rare words. our character-level recur-
rent neural networks compute source word
representations and recover unknown tar-
get words when needed. the twofold
advantage of such a hybrid approach is
that it is much faster and easier to train
than character-based ones; at the same
time, it never produces unknown words
as in the case of word-based models. on
the wmt   15 english to czech translation
task, this hybrid approach offers an ad-
dition boost of +2.1   11.4 id7 points
over models that already handle unknown
words. our best system achieves a new
state-of-the-art
result with 20.7 id7
score. we demonstrate that our character
models can successfully learn to not only
generate well-formed words for czech,
a highly-in   ected language with a very
complex vocabulary, but also build correct
representations for english source words.

1 introduction

id4 (id4) is a sim-
ple new architecture for getting machines to
translate.
its core, id4 is a single
deep neural network that is trained end-to-end
with several advantages such as simplicity and
generalization.
despite being relatively new,

at

figure 1: hybrid id4     example of a word-
character model for translating    a cute cat    into
   un joli chat   . hybrid id4 translates at the word
level. for rare tokens, the character-level compo-
nents build source representations and recover tar-
get <unk>.    _    marks sequence boundaries.

id4 has already achieved state-of-the-art trans-
lation results for several
language pairs such
as english-french (luong et al., 2015b), english-
german (jean et al., 2015a; luong et al., 2015a;
luong and manning, 2015), and english-czech
(jean et al., 2015b).

while id4 offers many advantages over tra-
ditional phrase-based approaches, such as small
memory footprint and simple decoder implemen-
tation, nearly all previous work in id4 has used
quite restricted vocabularies, crudely treating all
other words the same with an <unk> symbol.
sometimes, a post-processing step that patches
in unknown words is introduced to alleviate this

problem. luong et al. (2015b) propose to annotate
occurrences of target <unk> with positional infor-
mation to track their alignments, after which sim-
ple word dictionary lookup or identity copy can
be performed to replace <unk> in the translation.
jean et al. (2015a) approach the problem similarly
but obtain the alignments for unknown words from
the attention mechanism. we refer to these as the
unk replacement technique.

important properties of languages.

though simple, these approaches ignore sev-
eral
first,
monolingually, words are morphologically re-
they are currently treated as
lated; however,
independent entities.
this is problematic as
pointed out by luong et al. (2013): neural net-
works can learn good representations for fre-
quent words such as    distinct   , but fail for rare-
but-related words like    distinctiveness   .
sec-
ond, crosslingually, languages have different al-
phabets, so one cannot na  vely memorize all
possible surface word translations such as name
id68 between    christopher    (english)
and    kry  stof    (czech). see more on this problem
in (sennrich et al., 2016).

to overcome these shortcomings, we propose a
novel hybrid architecture for id4 that translates
mostly at the word level and consults the char-
acter components for rare words when necessary.
as illustrated in figure 1, our hybrid model con-
sists of a word-based id4 that performs most of
the translation job, except for the two (hypotheti-
cally) rare words,    cute    and    joli   , that are han-
dled separately. on the source side, representa-
tions for rare words,    cute   , are computed on-the-
   y using a deep recurrent neural network that op-
erates at the character level. on the target side,
we have a separate model that recovers the sur-
face forms,    joli   , of <unk> tokens character-by-
character. these components are learned jointly
end-to-end, removing the need for a separate unk
replacement step as in current id4 practice.

our hybrid id4 offers a twofold advantage: it
is much faster and easier to train than character-
based models; at the same time, it never produces
unknown words as in the case of word-based ones.
we demonstrate at scale that on the wmt   15 en-
glish to czech translation task, such a hybrid ap-
proach provides an additional boost of +2.1   11.4
id7 points over models that already handle un-
known words. we achieve a new state-of-the-
art result with 20.7 id7 score. our analysis

demonstrates that our character models can suc-
cessfully learn to not only generate well-formed
words for czech, a highly-in   ected language with
a very complex vocabulary, but also build correct
representations for english source words.

we provide

code,

data,

and models at

http://nlp.stanford.edu/projects/id4.

2 related work

text

dependency

line of work on
there has been a recent
end-to-end
neural models
character-based
which achieve good results for part-of-speech
(dos santos and zadrozny, 2014;
tagging
parsing
ling et al., 2015a),
(ballesteros et al., 2015),
classi   ca-
id103
tion (zhang et al., 2015),
bahdanau et al., 2016),
(chan et al., 2016;
and
(kim et al., 2016;
jozefowicz et al., 2016). however, success has
not been shown for cross-lingual
tasks such
as machine translation.1
sennrich et al. (2016)
propose to segment words into smaller units and
translate just like at the word level, which does not
learn to understand relationships among words.

id38

our

takes

work

inspiration

from
(luong et al., 2013) and (li et al., 2015). similar
to the former, we build representations for rare
words on-the-   y from subword units. however,
we utilize recurrent neural networks with charac-
ters as the basic units; whereas luong et al. (2013)
use id56s with morphemes as
units, which requires existence of a morphological
analyzer.
in comparison with (li et al., 2015),
our hybrid architecture is also a hierarchical
sequence-to-sequence model, but operates at a
different granularity level, word-character.
in
contrast, li et al. (2015) build id187
at
the sentence-word level for paragraphs and
documents.

3 background & our models

id4 aims
model
translating
sentence,
to a target sentence, y1, . . . , ym.
plishes this goal

to directly
the id155 p(y|x) of
x1, . . . , xn,
it accom-
through an encoder-decoder

source

a

1recently, ling et al. (2015b) attempt character-level
id4; however, the experimental evidence is weak. the au-
thors demonstrate only small improvements over word-level
baselines and acknowledge that there are no differences of
signi   cance. furthermore, only small datasets were used
without comparable results from past id4 work.

framework
(kalchbrenner and blunsom, 2013;
sutskever et al., 2014; cho et al., 2014). the en-
coder computes a representation s for each source
sentence. based on that source representation, the
decoder generates a translation, one target word at
a time, and hence, decomposes the log conditional
id203 as:

log p(y|x) = xm

t=1

log p (yt|y<t, s)

(1)

a natural model for sequential data is the re-
current neural network (id56), used by most of
the recent id4 work. papers, however, differ in
terms of: (a) architecture     from unidirectional,
to bidirectional, and deep multi-layer id56s; and
(b) id56 type     which are long short-term mem-
ory (lstm) (hochreiter and schmidhuber, 1997)
and the gated recurrent unit (cho et al., 2014). all
our models utilize the deep multi-layer architec-
ture with lstm as the recurrent unit; detailed for-
mulations are in (zaremba et al., 2014).

considering the top recurrent layer in a deep
lstm, with ht being the current target hidden
state as in figure 2, one can compute the proba-
bility of decoding each target word yt as:

p (yt|y<t, s) = softmax (ht)

(2)

for a parallel corpus d, we train our model by

minimizing the below cross-id178 loss:

   

(3)

the

approaches

    log p(y|x)

j = x(x,y)   d
attention mechanism
early
id4
(sutskever et al., 2014;
cho et al., 2014), which we have described above,
use only the last encoder state to initialize the
decoder, i.e., setting the input representation s in
eq. (1) to [  hn]. recently, bahdanau et al. (2015)
propose an attention mechanism, a form of
random access memory for id4 to cope with
long input sequences. luong et al. (2015a) further
extend the attention mechanism to different
scoring functions, used to compare source and
target hidden states, as well as different strategies
to place the attention. in all our models, we utilize
the global attention mechanism and the bilinear
form for the attention scoring function similar to
(luong et al., 2015a).

speci   cally, we set s in eq. (1) to the set of
source hidden states at the top layer, [  h1, . . . ,   hn].
as illustrated in figure 2, the attention mechanism
consists of two stages:
(a) context vector     the

yt

  ht

ct

  h1

  hn

ht

figure 2: attention mechanism.

current hidden state ht is compared with individ-
ual source hidden states in s to learn an alignment
vector, which is then used to compute the context
vector ct as a weighted average of s; and (b) atten-
tional hidden state     the context vector ct is then
used to derive a new attentional hidden state:

  ht = tanh(w[ct; ht])

the attentional vector   ht
eq. (2) in predicting the next word.

then replaces ht

(4)

in

4 hybrid id4

our hybrid architecture, illustrated in figure 1,
leverages the power of both words and characters
to achieve the goal of open vocabulary id4. the
core of the design is a word-level id4 with the
advantage of being fast and easy to train. the
character components empower the word-level
system with the abilities to compute any source
word representation on the    y from characters and
to recover character-by-character unknown target
words originally produced as <unk>.

4.1 word-based translation as a backbone
the core of our hybrid id4 is a deep lstm
encoder-decoder that translates at the word level as
described in section 3. we maintain a vocabulary
of |v | frequent words for each language. other
words not inside these lists are represented by a
universal symbol <unk>, one per language. we
translate just like a word-based id4 system with
respect to these source and target vocabularies, ex-
cept for cases that involve <unk> in the source in-

put or the target output. these correspond to the
character-level components illustrated in figure 1.
a nice property of our hybrid approach is that
by varying the vocabulary size, one can control
how much to blend the word- and character-based
models; hence, taking the best of both worlds.

current word-level state. we train our system such
that whenever the word-level id4 produces an
<unk>, we can consult this character-level de-
coder to recover the correct surface form of the un-
known target word. this is illustrated in figure 1.
the training objective in eq. (3) now becomes:

4.2 source character-based representation
in regular word-based id4, for all rare words out-
side the source vocabulary, one feeds the univer-
sal embedding representing <unk> as input to the
encoder. this is problematic because it discards
valuable information about the source word. to
   x that, we learn a deep lstm model over char-
acters of source words. for example, in figure 1,
we run our deep character-based lstm over    c   ,
   u   ,    t   ,    e   , and    _    (the boundary symbol). the    -
nal hidden state at the top layer will be used as the
on-the-   y representation for the current rare word.
the layers of the deep character-based lstm
are always initialized with zero states. one might
propose to connect hidden states of the word-
based lstm to the character-based model; how-
ever, we chose this design for various reasons.
first, it simpli   es the architecture. second, it al-
lows for ef   ciency through precomputation: be-
fore each mini-batch, we can compute represen-
tations for rare source words all at once. all in-
stances of the same word share the same embed-
ding, so the computation is per type.2

4.3 target character-level generation
general word-based id4 allows generation of
<unk> in the target output. afterwards, there
is usually a post-processing step that handles
these unknown tokens by utilizing the alignment
information derived from the attention mecha-
nism and then performing simple word dictio-
nary lookup or identity copy (luong et al., 2015a;
jean et al., 2015a). while this approach works, it
suffers from various problems such as alphabet
mismatches between the source and target vocab-
ularies and multi-word alignments. our goal is
to address all these issues and create a coherent
framework that handles an unlimited output vo-
cabulary.

our solution is to have a separate deep lstm
that    translates    at the character level given the

2while ling et al. (2015b) found that it is slow and dif   -
cult to train source character-level models and had to resort to
pretraining, we demonstrate later that we can train our deep
character-level lstm perfectly    ne in an end-to-end fashion.

j = jw +   jc

(5)

it

in our example,

here, jw refers to the usual loss of the word-
level id4;
is the sum
of
the negative log likelihood of generating
{   un   ,    <unk>   ,    chat   ,    _   }.
the remaining
component jc corresponds to the loss incurred by
the character-level decoder when predicting char-
acters, e.g., {   j   ,    o   ,    l   ,    i   ,    _   }, of those rare
words not in the target vocabulary.

which

representations,

hidden-state initialization unlike the source
character-based
are
context-independent,
the target character-level
generation requires the current word-level context
to produce meaningful translation. this brings
up an important question about what can best
represent the current context so as to initialize the
character-level decoder. we answer this question
in the context of the attention mechanism (  3).

the    nal vector   ht, just before the softmax as
shown in figure 2, seems to be a good candidate
to initialize the character-level decoder. the rea-
son is that   ht combines information from both the
context vector ct and the top-level recurrent state
ht. we refer to it later in our experiments as the
same-path target generation approach.

on the other hand, the same-path approach wor-
ries us because all vectors   ht used to seed the
character-level decoder might have similar values,
leading to the same character sequence being pro-
duced. the reason is because   ht is directly used in
the softmax, eq. (2), to predict the same <unk>.
that might pose some challenges for the model to
learn useful representations that can be used to ac-
complish two tasks at the same time, that is to pre-
dict <unk> and to generate character sequences.
to address that concern, we propose another ap-
proach called the separate-path target generation.
our separate-path target generation approach
works as follows. we mimic the process described
in eq. (4) to create a counterpart vector   ht that will
be used to seed the character-level decoder:

  ht = tanh(   w [ct; ht])

(6)

here,   w is a new learnable parameter matrix,
with which we hope to release w from the pres-
sure of having to extract
information relevant
to both the word- and character-generation pro-
cesses. only the hidden state of the    rst layer
is initialized as discussed above. the other com-
ponents in the character-level decoder such as the
lstm cells of all layers and the hidden states of
higher layers, all start with zero values.

implementation-wise,

the computation in the
character-level decoder is done per word token in-
stead of per type as in the source character com-
ponent (  4.2). this is because of the context-
dependent nature of the decoder.

word-character generation strategy with
the character-level decoder, we can view the    -
nal hidden states as representations for the surface
forms of unknown tokens and could have fed these
to the next time step. however, we chose not to
do so for the ef   ciency reason explained next; in-
stead, <unk> is fed to the word-level decoder    as
is    using its corresponding id27.

during training,

this design choice decou-
ples all executions over <unk> instances of the
character-level decoder as soon the word-level
id4 completes. as such, the forward and back-
ward passes of the character-level decoder over
rare words can be invoked in batch mode. at test
time, our strategy is to    rst run a id125 de-
coder at the word level to    nd the best transla-
tions given by the word-level id4. such trans-
lations contains <unk> tokens, so we utilize our
character-level decoder with id125 to gen-
erate actual words for these <unk>.

5 experiments

we evaluate the effectiveness of our mod-
els on the publicly available wmt   15 transla-
tion task from english into czech with new-
stest2013 (3000 sentences) as a development
set and newstest2015 (2656 sentences) as a test
set.
case-sensitive
nist id7 (papineni et al., 2002) and chrf3
(popovi  c, 2015).3
the
amounts of overlapping character id165s and has
been argued to be a better metric for translation
tasks out of english.

two metrics are used:

the latter measures

3for nist id7, we    rst run detokenizer.pl and
then use mteval-v13a to compute the scores as per wmt
guideline. for chrf3, we utilize the implementation here
https://github.com/rsennrich/subword-id4.

english

czech

word

char

word

char

15.8m

254m 1,269m 224m 1,347m
1,172k

1,760k

2003

2053

98.1%

98.8%

# sents
# tokens
# types
200-char

table 1: wmt   15 english-czech data     shown
are various statistics of our training data such as
sentence, token (word and character counts), as
well as type (sizes of the word and character vo-
cabularies). we show in addition the amount of
words in a vocabulary expressed by a list of 200
characters found in frequent words.

5.1 data
among the available language pairs in wmt   15,
all involving english, we choose czech as a target
language for several reasons. first and foremost,
czech is a slavic language with not only rich and
complex in   ection, but also fusional morphology
in which a single morpheme can encode multiple
grammatical, syntactic, or semantic meanings. as
a result, czech possesses an enormously large vo-
cabulary (about 1.5 to 2 times bigger than that of
english according to statistics in table 1) and is
a challenging language to translate into. further-
more, this language pair has a large amount of
training data, so we can evaluate at scale. lastly,
though our techniques are language independent,
it is easier for us to work with czech since czech
uses the latin alphabet with some diacritics.

in terms of preprocessing, we apply only the
standard id121 practice.4 we choose for
each language a list of 200 characters found in
frequent words, which, as shown in table 1, can
represent more than 98% of the vocabulary.

5.2 training details
we train three types of systems, purely word-
based, purely character-based, and hybrid. com-
mon to these architectures is a word-based id4
since the character-based systems are essentially
word-based ones with longer sequences and the
core of hybrid models is also a word-based id4.
in training word-based id4, we follow
luong et al. (2015a) to use the global attention
mechanism together with similar hyperparame-
ters: (a) deep lstm models, 4 layers, 1024 cells,
and 1024-dimensional embeddings, (b) uniform

4use tokenizer.perl in moses with default settings.

system

(a) best wmt   15, big data (bojar and tamchyna, 2015)
existing id4

vocab

-

perplexity
w
-

c
-

(b) id56search + unk replace (jean et al., 2015b)
(c) ensemble 4 models + unk replace (jean et al., 2015b)

200k
200k

-
-

our word-based id4

(d) base + attention + unk replace
(e) ensemble 4 models + unk replace

50k
50k

5.9
-

our character-based id4

(f) base-512 (600-step backprop)
(g) base-512 + attention (600-step backprop)
(h) base-1024 + attention (300-step backprop)

our hybrid id4

base + attention + same-path
base + attention + separate-path

(i)
(j)
(k) base + attention + separate-path + 2-layer char
(l)
base + attention + separate-path + 2-layer char
(m) ensemble 4 models

200
200
200

10k
10k
10k
50k
50k

-
-
-

4.9
4.9
4.7
5.7
-

-
-

-
-

2.4
1.6
1.9

1.7
1.7
1.6
1.6
-

id7 chrf3

18.8

15.7
18.3

17.5
18.4

3.8
17.5
15.7

14.1
15.6
17.7
19.6
20.7

-

-
-

42.4
43.9

25.9
46.6
41.1

37.2
39.6
44.1
46.5
47.5

table 2: wmt   15 english-czech results     shown are the vocabulary sizes, perplexities, id7, and
chrf3 scores of various systems on newstest2015. perplexities are listed under two categories, word (w)
and character (c). best and important results per metric are highlighed.

initialization of parameters in [   0.1, 0.1], (c) 6-
epoch training with plain sgd and a simple learn-
ing rate schedule     start with a learning rate of 1.0;
after 4 epochs, halve the learning rate every 0.5
epoch, (d) mini-batches are of size 128 and shuf-
   ed, (e) the gradient is rescaled whenever its norm
exceeds 5, and (f) dropout is used with probabil-
ity 0.2 according to (pham et al., 2014). we now
detail differences across the three architectures.

word-based id4     we constrain our source
and target sequences to have a maximum length
of 50 each; words that go past
the boundary
are ignored. the vocabularies are limited to the
top |v | most frequent words in both languages.
words not in these vocabularies are converted into
<unk>. after translating, we will perform dictio-
nary5 lookup or identity copy for <unk> using the
alignment information from the id12.
such procedure is referred as the unk replace tech-
nique (luong et al., 2015b; jean et al., 2015a).

character-based id4     the source and target
sequences at the character level are often about 5
times longer than their counterparts in the word-
based models as we can infer from the statistics in
table 1. due to memory constraint in gpus, we

5obtained from the alignment

links produced by the
berkeley aligner (liang et al., 2006) over the training corpus.

limit our source and target sequences to a maxi-
mum length of 150 each, i.e., we backpropagate
through at most 300 timesteps from the decoder to
the encoder. with smaller 512-dimensional mod-
els, we can afford to have longer sequences with
up to 600-step id26.

hybrid id4     the word-level component
uses the same settings as the purely word-based
id4. for the character-level source and target
components, we experiment with both shallow and
deep 1024-dimensional models of 1 and 2 lstm
layers. we set the weight    in eq. (5) for our
character-level loss to 1.0.

training time     it takes about 3 weeks to train
a word-based model with |v | = 50k and about
3 months to train a character-based model. train-
ing and testing for the hybrid models are about 10-
20% slower than those of the word-based models
with the same vocabulary size.

5.3 results

we compare our models with several strong
systems. these include the winning entry in
wmt   15, which was trained on a much larger
amount of data, 52.6m parallel and 393.0m mono-

lingual sentences (bojar and tamchyna, 2015).6
in contrast, we merely use the provided parallel
corpus of 15.8m sentences. for id4, to the best
of our knowledge, (jean et al., 2015b) has the best
published performance on english-czech.

as shown in table 2, for a purely word-based
approach, our single id4 model outperforms the
best single model in (jean et al., 2015b) by +1.8
points despite using a smaller vocabulary of only
50k words versus 200k words. our ensemble
system (e) slightly outperforms the best previous
id4 system with 18.4 id7.

to our surprise, purely character-based models,
though extremely slow to train and test, perform
quite well. the 512-dimensional attention-based
model (g) is best, surpassing the single word-
based model in (jean et al., 2015b) despite hav-
ing much fewer parameters. it even outperforms
most id4 systems on chrf3 with 46.6 points.
this indicates that this model translate words that
closely but not exactly match the reference ones
as evidenced in section 6.3. we notice two in-
teresting observations. first, attention is critical
for character-based models to work as is obvious
from the poor performance of the non-attentional
model; this has also been shown in speech recog-
nition (chan et al., 2016). second, long time-step
id26 is more important as re   ected by
the fact that the larger 1024-dimensional model (h)
with shorter backprogration is inferior to (g).

our hybrid models achieve the best results. at
10k words, we demonstrate that our separate-
path strategy for the character-level target gener-
ation (  4.3) is effective, yielding an improvement
of +1.5 id7 points when comparing systems (j)
vs. (i). a deeper character-level architecture of 2
lstm layers provides another signi   cant boost of
+2.1 id7. with 17.7 id7 points, our hybrid
system (k) has surpassed word-level id4 models.
when extending to 50k words, we further im-
prove the translation quality. our best single
model, system (l) with 19.6 id7, is already
better than all existing systems. our ensemble
model (m) further advances the sota result to
20.7 id7, outperforming the winning entry in
the wmt   15 english-czech translation task by a
large margin of +1.9 points. our ensemble model

6this entry combines two independent systems, a phrase-
based moses model and a deep-syntactic transfer-based
model. additionally, there is an automatic post-editing sys-
tem with hand-crafted rules to correct errors in morphological
agreement and semantic meanings, e.g., loss of negation.

20

15

10

5

 

0
0

u
e
l
b

+5.0

+3.5

+11.4

 

+2.1

word
word + unk replace
hybrid

10

20

30

vocabulary size (x1000)

40

50

figure 3: vocabulary size effect     shown are the
performances of different systems as we vary their
vocabulary sizes. we highlight the improvements
obtained by our hybrid models over word-based
systems which already handle unknown words.

is also best in terms of chrf3 with 47.5 points.

6 analysis

this section    rst studies the effects of vocabulary
sizes towards translation quality. we then analyze
more carefully our character-level components by
visualizing and evaluating rare id27s
as well as examining sample translations.

6.1 effects of vocabulary sizes

as shown in figure 3, our hybrid models of-
fer large gains of +2.1-11.4 id7 points over
strong word-based systems which already handle
unknown words. with only a small vocabulary,
e.g., 1000 words, our hybrid approach can pro-
duce systems that are better than word-based mod-
els that possess much larger vocabularies. while
it appears from the plot that gains diminish as we
increase the vocabulary size, we argue that our hy-
brid models are still preferable since they under-
stand word structures and can handle new complex
words at test time as illustrated in section 6.3.

6.2 rare id27s

we evaluate the source character-level model by
building representations for rare words and mea-
suring how good these embeddings are.

quantitatively, we follow luong et al. (2013) in
using the word similarity task, speci   cally on the
rare word dataset, to judge the learned represen-
tations for complex words. the evaluation metric
is the spearman   s correlation    between similarity
scores assigned by a model and by human anno-
tators. from the results in table 3, we can see
that source representations produced by our hy-

(cid:9)
(cid:9)
(cid:9)
loveless
spiritless

heartlessly
heartlessness

narrow   mindedness

narrow   minded

wholeheartedness

nonconscious
uncontroversial

unattainableness

inabilities

impossibilities

untrustworthy

unrealizable

disrespectful
ungraceful
regretful

illiberal

unconcern

possible

necessary

impossible

acceptable
satisfactory

obvious

advance

unacceptable

unsatisfactory

explicit

evidently

acknowledgement

developments

develop

uncomfortable

insufficiency

unsuitable

insensitive

unaffected

noticeable
perceptible

immobile

immoveable

admittance

admitting

admission
admit

founder

chooses
nominated

sponsor

decide

choose

antagonist

antagonize

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

unsighted
unfeathered
unfledged

practice

0

0

0.1

0.2

0.3

0.4

0.5

governance

management

0.6

connect

link
0.8

0.7

cofounders

companionships

0.9

1

figure 4: barnes-huid167 visualization of source word representations     shown are sample words
from the rare word dataset. we differentiate two types of embeddings: frequent words in which encoder
embeddings are looked up directly and rare words where we build representations from characters. boxes
highlight examples that we will discuss in the text. we use the hybrid model (l) in this visualization.

br  models are signi   cantly better than those of
the word-based one. it is noteworthy that our deep
recurrent character-level models can outperform
the model of (luong et al., 2013), which uses re-
cursive neural networks and requires a complex
morphological analyzer, by a large margin. our
performance is also competitive to the best glove
embeddings (pennington et al., 2014) which were
trained on a much larger dataset.

system
(luong et al., 2013)

glove (pennington et al., 2014)

our id4 models

(d) word-based
(k) hybrid
(l) hybrid

  

size
|v |
138k 34.4
1b
6b
400k 38.1
42b 400k 47.8

0.3b
0.3b
0.3b

50k 20.4
10k 42.4
50k 47.1

table 3: word similarity task     shown are spear-
man   s correlation    on the rare word dataset of
various models (with different vocab sizes |v |).

qualitatively, we visualize embeddings pro-
for selected
duced by the hybrid model
(l)
figure 4
words in the rare word dataset.
shows the two-dimensional
representations of
words computed by the barnes-huid167 algo-

rithm (van der maaten, 2013).8 it is extremely in-
teresting to observe that words are clustered to-
gether not only by the word structures but also by
the meanings. for example, in the top-left box,
the character-based representations for    loveless   ,
   spiritless   ,    heartlessly   , and    heartlessness    are
nearby, but clearly separated into two groups.
similarly, in the center boxes, word-based embed-
dings of    acceptable   ,    satisfactory   ,    unaccept-
able   , and    unsatisfactory   , are close by but sep-
arated by meanings. lastly, the remaining boxes
demonstrate that our character-level models are
able to build representations comparable to the
word-based ones, e.g.,    impossibilities    vs.    im-
possible    and    antagonize    vs.    antagonist   . all
of this evidence strongly supports that the source
character-level models are useful and effective.

6.3 sample translations

we show in table 4 sample translations between
various systems. in the    rst example, our hybrid
model translates perfectly. the word-based model
fails to translate    diagnosis    because the second
<unk> was incorrectly aligned to the word    af-
ter   . the character-based model, on the other
hand, makes a mistake in translating names.

7we look up the encoder embeddings for frequent words

and build representations for rare word from characters.

8we run barnes-huid167 algorithm over a set of 91

words, but    lter out 27 words for displaying clarity.

1

2

3

word

source the author stephen jay gould died 20 years after diagnosis .
human autor stephen jay gould zem  rel 20 let po diagn  ze .
autor stephen jay <unk> zem  rel 20 let po <unk> .
autor stephen jay gould zem  rel 20 let po po .
autor stepher stepher zem  rel 20 let po diagn  ze .
autor <unk> <unk> <unk> zem  rel 20 let po <unk>.
autor stephen jay gould zem  rel 20 let po diagn  ze .

hybrid

char

word

source as the reverend martin luther king jr. said    fty years ago :
jak p  red pades  ti lety   rekl reverend martin luther king jr . :
human
jak   rekl reverend martin <unk> king <unk> p  red pades  ti lety :
jak   rekl reverend martin luther king   rekl p  red pades  ti lety :
jako reverend martin luther kr  l   r  kal p  red pades  ti lety :
jak p  red <unk> lety   rekl <unk> martin <unk> <unk> <unk> :
jak p  red pades  ti lety   rekl reverend martin luther king jr. :

hybrid

char

source her 11-year-old daughter , shani bart , said it felt a " little bit weird " [..] back to school .
human

jej   jeden  ctilet   dcera shani bartov   prozradila ,   ze " je to trochu zvl    stn   " [..] znova do   skoly .
jej   <unk> dcera <unk> <unk>   rekla ,   ze je to " trochu divn   " , [..] vrac   do   skoly .
jej   11-year-old dcera shani ,   rekla ,   ze je to " trochu divn   " , [..] vrac   do   skoly .
jej   jeden  ctilet   dcera , shani bartov   ,   r  kala ,   ze c  t   trochu divn  e , [..] vr  tila do   skoly .
jej   <unk> dcera , <unk> <unk> ,   rekla ,   ze c  t   " trochu <unk> " , [..] vr  tila do   skoly .
jej   jeden  ctilet   dcera , graham bart ,   rekla ,   ze c  t   " trochu divn   " , [..] vr  tila do   skoly .

word

char

hybrid

table 4: sample translations on newstest2015     for each example, we show the source, human transla-
tion, and translations of the following id4 systems: word model (d), char model (g), and hybrid model
(k). we show the translations before replacing <unk> tokens (if any) for the word-based and hybrid
models. the following formats are used to highlight correct, wrong, and close translation segments.

for the second example, the hybrid model sur-
prises us when it can capture the long-distance re-
ordering of       fty years ago    and    p  red pades  ti
lety    while the other two models do not. the
word-based model translates    jr.   
inaccurately
due to the incorrect alignment between the sec-
ond <unk> and the word    said   . the character-
based model literally translates the name    king   
into    kr  l    which means    king   .

lastly, both the character-based and hybrid
models impress us by their ability to translate
compound words exactly, e.g.,    11-year-old    and
   jeden  ctilet     ; whereas the identity copy strategy
of the word-based model fails. of course, our hy-
brid model does make mistakes, e.g., it fails to
translate the name    shani bart   . overall, these ex-
amples highlight how challenging translating into
czech is and that being able to translate at the
character level helps improve the quality.

7 conclusion

we have proposed a novel hybrid architecture
that combines the strength of both word- and
character-based models. word-level models are
fast to train and offer high-quality translation;
whereas, character-level models help achieve the
goal of open vocabulary id4. we have demon-

strated these two aspects through our experimental
results and translation examples.

our best hybrid model has surpassed the perfor-
mance of both the best word-based id4 system
and the best non-neural model to establish a new
state-of-the-art result for english-czech transla-
tion in wmt   15 with 20.7 id7. moreover, we
have succeeded in replacing the standard unk re-
placement technique in id4 with our character-
level components, yielding an improvement of
+2.1   11.4 id7 points. our analysis has shown
that our model has the ability to not only generate
well-formed words for czech, a highly in   ected
language with an enormous and complex vocab-
ulary, but also build accurate representations for
english source words.

additionally, we have demonstrated the poten-
tial of purely character-based models in produc-
ing good translations; they have outperformed past
word-level id4 models. for future work, we
hope to be able to improve the memory usage and
speed of purely character-based models.

acknowledgments

this work was partially supported by nsf award
iis-1514268 and by a gift from bloomberg l.p.
we thank dan jurafsky, andrew ng, and quoc

le for earlier feedback on the work, as well as
sam bowman, ziang xie, and jiwei li for their
valuable comments on the paper draft. lastly, we
thank nvidia corporation for the donation of
tesla k40 gpus as well as andrew ng and his
group for letting us use their computing resources.

references
[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2015. neural machine
translation by jointly learning to align and translate.
in iclr.

[bahdanau et al.2016] dzmitry

jan
chorowski, dmitriy serdyuk, philemon brakel, and
yoshua bengio. 2016. end-to-end attention-based
large vocabulary id103. in icassp.

bahdanau,

[ballesteros et al.2015] miguel ballesteros, chris dyer,
and noah a. smith. 2015.
improved transition-
based parsing by modeling characters instead of
words with lstms. in emnlp.

[bojar and tamchyna2015] ond  rej bojar and ale  s
2015. cuni in wmt15: chimera

tamchyna.
strikes again. in wmt.

[chan et al.2016] william chan, navdeep

jaitly,
quoc v. le, and oriol vinyals. 2016. listen, attend
and spell. in icassp.

[cho et al.2014] kyunghyun cho, bart van merrien-
boer, caglar gulcehre, fethi bougares, holger
schwenk, and yoshua bengio.
2014. learning
phrase representations using id56 encoder-decoder
for id151. in emnlp.

[dos santos and zadrozny2014] c  cero nogueira dos
santos and bianca zadrozny.
2014. learning
character-level representations for part-of-speech
tagging. in icml.

[hochreiter and schmidhuber1997] sepp hochreiter
and j  rgen schmidhuber. 1997. long short-term
memory. 9(8):1735   1780.

[jean et al.2015a] s  bastien jean, kyunghyun cho,
roland memisevic, and yoshua bengio. 2015a. on
using very large target vocabulary for neural ma-
chine translation. in acl.

[jean et al.2015b] s  bastien

firat,
kyunghyun cho, roland memisevic, and yoshua
bengio. 2015b. montreal neural machine transla-
tion systems for wmt   15. in wmt.

orhan

jean,

[jozefowicz et al.2016] rafal

vinyals, mike schuster, noam shazeer,
yonghui wu.
id38.

jozefowicz,

oriol
and
exploring the limits of

2016.

[kim et al.2016] yoon kim, yacine jernite, david son-
2016. character-

tag, and alexander m. rush.
aware neural language models. in aaai.

[li et al.2015] jiwei li, minh-thang luong, and dan
jurafsky. 2015. a hierarchical neural autoencoder
for paragraphs and documents. in acl.

[liang et al.2006] percy liang, ben taskar, and dan
klein. 2006. alignment by agreement. in naacl.

[ling et al.2015a] wang ling, chris dyer, alan w.
black, isabel trancoso, ramon fermandez, silvio
amir, lu  s marujo, and tiago lu  s. 2015a. find-
ing function in form: compositional character mod-
els for open vocabulary word representation.
in
emnlp.

[ling et al.2015b] wang ling, isabel trancoso, chris
dyer, and alan black. 2015b. character-based neu-
ral machine translation.

[luong and manning2015] minh-thang luong and
christopher d. manning. 2015. stanford neural
machine translation systems for spoken language
domain. in iwslt.

[luong et al.2013] minh-thang

richard
socher, and christopher d. manning. 2013. better
word representations with id56s
for morphology. in conll.

luong,

[luong et al.2015a] minh-thang luong, hieu pham,
and christopher d. manning. 2015a. effective ap-
proaches to attention-based neural machine transla-
tion. in emnlp.

[luong et al.2015b] minh-thang

ilya
sutskever, quoc v. le, oriol vinyals, and wo-
jciech zaremba. 2015b. addressing the rare word
problem in id4. in acl.

luong,

[papineni et al.2002] kishore papineni, salim roukos,
todd ward, and wei jing zhu.
2002. id7: a
method for automatic evaluation of machine trans-
lation. in acl.

[pennington et al.2014] jeffrey pennington, richard
socher, and christopher d. manning. 2014. glove:
global vectors for word representation. in emnlp.

[pham et al.2014] vu pham, th  odore bluche, christo-
pher kermorvant, and j  r  me louradour.
2014.
dropout improves recurrent neural networks for
handwriting recognition. in icfhr.

[popovi  c2015] maja popovi  c. 2015. chrf: character
in

id165 f-score for automatic mt evaluation.
wmt.

[sennrich et al.2016] rico sennrich, barry haddow,
and alexandra birch. 2016. neural machine trans-
lation of rare words with subword units. in acl.

[kalchbrenner and blunsom2013] nal kalchbrenner
and phil blunsom. 2013. recurrent continuous
translation models. in emnlp.

[sutskever et al.2014] ilya sutskever, oriol vinyals,
and quoc v. le. 2014. sequence to sequence learn-
ing with neural networks. in nips.

[van der maaten2013] laurens van der maaten. 2013.

barnes-huid167. in iclr.

[zaremba et al.2014] wojciech

sutskever, and oriol vinyals.
neural network id173. abs/1409.2329.

zaremba,
ilya
2014. recurrent

[zhang et al.2015] xiang zhang, junbo zhao, and yann
lecun. 2015. character-level convolutional net-
works for text classi   cation. in nips.

