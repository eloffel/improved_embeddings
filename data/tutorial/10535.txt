5
1
0
2

 

n
u
j
 

1
1

 
 
]

v
c
.
s
c
[
 
 

5
v
2
3
6
6

.

2
1
4
1
:
v
i
x
r
a

published as a conference paper at iclr 2015

deep captioning with multimodal recurrent
neural networks (m-id56)

junhua mao
university of california, los angeles; baidu research
mjhustc@ucla.edu

wei xu & yi yang & jiang wang & zhiheng huang
baidu research
{wei.xu,yangyi05,wangjiang03,huangzhiheng}@baidu.com

alan yuille
university of california, los angeles
yuille@stat.ucla.edu

abstract

in this paper, we present a multimodal recurrent neural network (m-id56) model
for generating novel image captions. it directly models the id203 distribution
of generating a word given previous words and an image. image captions are gen-
erated according to this distribution. the model consists of two sub-networks: a
deep recurrent neural network for sentences and a deep convolutional network for
images. these two sub-networks interact with each other in a multimodal layer
to form the whole m-id56 model. the effectiveness of our model is validated on
four benchmark datasets (iapr tc-12, flickr 8k, flickr 30k and ms coco).
our model outperforms the state-of-the-art methods. in addition, we apply the
m-id56 model to retrieval tasks for retrieving images or sentences, and achieves
signi   cant performance improvement over the state-of-the-art methods which di-
rectly optimize the ranking objective function for retrieval. the project page of
this work is: www.stat.ucla.edu/  junhua.mao/m-id56.html. 1

1

introduction

obtaining sentence level descriptions for images is becoming an important task and it has many ap-
plications, such as early childhood education, id162, and navigation for the blind. thanks
to the rapid development of id161 and natural language processing technologies, recent
work has made signi   cant progress on this task (see a brief review in section 2). many previous
methods treat it as a retrieval task. they learn a joint embedding to map the features of both sen-
tences and images to the same semantic space. these methods generate image captions by retrieving
them from a sentence database. thus, they lack the ability of generating novel sentences or describ-
ing images that contain novel combinations of objects and scenes.
in this work, we propose a multimodal recurrent neural networks (m-id56) model 2 to address
both the task of generating novel sentences descriptions for images, and the task of image and
sentence retrieval. the whole m-id56 model contains a language model part, a vision part and a
multimodal part. the language model part learns a dense feature embedding for each word in the

1most recently, we adopt a simple strategy to boost the performance of image captioning task signi   cantly.
more details are shown in section 8. the code and related data (e.g. re   ned image features and hypotheses
sentences generated by the m-id56 model) are available at https://github.com/mjhucla/mid56-cr.
2a previous version of this work appears in the nips 2014 deep learning workshop with the title    explain
images with multimodal recurrent neural networks    http://arxiv.org/abs/1410.1090 (mao et al.
(2014)). we observed subsequent arxiv papers which also use recurrent neural networks in this topic and cite
our work. we gratefully acknowledge them.

1

published as a conference paper at iclr 2015

figure 1: examples of the generated and two top-ranked retrieved sentences given the query image
from iapr tc-12 dataset. the sentences can well describe the content of the images. we show a
failure case in the fourth image, where the model mistakenly treats the lake as the sky and misses
all the people. more examples from the ms coco dataset can be found on the project page:
www.stat.ucla.edu/  junhua.mao/m-id56.html.

dictionary and stores the semantic temporal context in recurrent layers. the vision part contains a
deep convolutional neural network (id98) which generates the image representation. the multi-
modal part connects the language model and the deep id98 together by a one-layer representation.
our m-id56 model is learned using a log-likelihood cost function (see details in section 4). the
errors can be backpropagated to the three parts of the m-id56 model to update the model parameters
simultaneously.
in the experiments, we validate our model on four benchmark datasets: iapr tc-12 (grubinger
et al. (2006)), flickr 8k (rashtchian et al. (2010)), flickr 30k (young et al. (2014)) and ms coco
(lin et al. (2014)). we show that our method achieves state-of-the-art performance, signi   cantly
outperforming all the other methods for the three tasks: generating novel sentences, retrieving im-
ages given a sentence and retrieving sentences given an image. our framework is general and can
be further improved by incorporating more powerful deep representations for images and sentences.

2 related work

deep model for id161 and natural language. the methods based on the deep neural
network developed rapidly in recent years in both the    eld of id161 and natural lan-
guage. for id161, krizhevsky et al. (2012) propose a deep convolutional neural net-
works (id98) with 8 layers (denoted as alexnet) and outperform previous methods by a large
margin in the image classi   cation task of id163 challenge (russakovsky et al. (2014)). this
network structure is widely used in id161, e.g. girshick et al. (2014) design a object de-
tection framework (rid98) based on this work. recently, simonyan & zisserman (2014) propose a
id98 with over 16 layers (denoted as vggnet) and performs substantially better than the alexnet.
for natural language, the recurrent neural network (id56) shows the state-of-the-art performance
in many tasks, such as id103 and id27 learning (mikolov et al. (2010; 2011;
2013)). recently, id56s have been successfully applied to machine translation to extract semantic
information from the source sentence and generate target sentences (e.g. kalchbrenner & blunsom
(2013), cho et al. (2014) and sutskever et al. (2014)).
image-sentence retrieval. many previous methods treat the task of describing images as a retrieval
task and formulate the problem as a ranking or embedding learning problem (hodosh et al. (2013);
frome et al. (2013); socher et al. (2014)). they    rst extract the word and sentence features (e.g.
socher et al. (2014) uses dependency tree id56 to extract sentence features)
as well as the image features. then they optimize a ranking cost to learn an embedding model that
maps both the sentence feature and the image feature to a common semantic feature space. in this
way, they can directly calculate the distance between images and sentences. recently, karpathy
et al. (2014) show that object level image features based on id164 results can generate
better results than image features extracted at the global level.

2

retr.gen.1. tourists are sitting at a long table with beer bottles on it in a rather dark restaurant and are raising their bierglaeser; 2. tourists are sitting at a long table with a white table-cloth in a somewhat dark restaurant;tourists are sitting at a long table with a white table cloth and are eating;1. top view of the lights of a city at night, with a well-illuminated square in front of a church in the foreground;2. people on the stairs in front of an illuminated cathedral with two towers at night;a square with burning street lamps and a street in the foreground;1. a dry landscape with light brown grass and green shrubs and trees in the foreground and large reddish-brown rocks and a blue sky in the background;2. a few bushes at the bottom and a clear sky in the background; a dry landscape with green trees and bushes and light brown grass in the foreground and reddish-brown round rock domes and a blue sky in the background;1. group picture of nine tourists and one local on a grey rock with a lake in the background;2. five people are standing and four are squatting on a brown rock in the foreground;a blue sky in the background;published as a conference paper at iclr 2015

figure 2: illustration of the simple recurrent neural network (id56) and our multimodal recurrent
neural network (m-id56) architecture. (a). the simple id56. (b). our m-id56 model. the inputs
of our model are an image and its corresponding sentence descriptions. w1, w2, ..., wl represents the
words in a sentence. we add a start sign wstart and an end sign wend to all the training sentences. the
model estimates the id203 distribution of the next word given previous words and the image.
it consists of    ve layers (i.e. two id27 layers, a recurrent layer, a multimodal layer and
a softmax layer) and a deep id98 in each time frame. the number above each layer indicates the
dimension of the layer. the weights are shared among all the time frames. (best viewed in color)

generating novel sentence descriptions for images. there are generally three categories of meth-
ods for this task. the    rst category assumes a speci   c rule of the language grammar. they parse
the sentence and divide it into several parts (mitchell et al. (2012); gupta & mannem (2012)). each
part is associated with an object or an attribute in the image (e.g. kulkarni et al. (2011) uses a con-
ditional random field model and farhadi et al. (2010) uses a markov random field model). this
kind of method generates sentences that are syntactically correct. the second category retrieves
similar captioned images, and generates new descriptions by generalizing and re-composing the re-
trieved captions (kuznetsova et al. (2014)). the third category of methods, which is more related
to our method, learns a id203 density over the space of multimodal inputs (i.e. sentences and
images), using for example, deep id82s (srivastava & salakhutdinov (2012)), and
topic models (barnard et al. (2003); jia et al. (2011)). they generate sentences with richer and more
   exible structure than the    rst group. the id203 of generating sentences using the model can
serve as the af   nity metric for retrieval. our method falls into this category. more closely related
to our tasks and method is the work of kiros et al. (2014b), which is built on a log-bilinear model
(mnih & hinton (2007)) and use alexnet to extract visual features. it needs a    xed length of context
(i.e.    ve words), whereas in our model, the temporal context is stored in a recurrent architecture,
which allows arbitrary context length.
shortly after mao et al. (2014), several papers appear with record breaking results (e.g. kiros et al.
(2014a); karpathy & fei-fei (2014); vinyals et al. (2014); donahue et al. (2014); fang et al. (2014);
chen & zitnick (2014)). many of them are built on recurrent neural networks. it demonstrates the
effectiveness of storing context information in a recurrent layer. our work has two major difference
from these methods. firstly, we incorporate a two-layer id27 system in the m-id56
network structure which learns the word representation more ef   ciently than the single-layer word
embedding. secondly, we do not use the recurrent layer to store the visual information. the image
representation is inputted to the m-id56 model along with every word in the sentence description.
it utilizes of the capacity of the recurrent layer more ef   ciently, and allows us to achieve state-of-
the-art performance using a relatively small dimensional recurrent layer. in the experiments, we
show that these two strategies lead to better performance. our method is still the best-performing
approach for almost all the id74.

3 model architecture
3.1 simple recurrent neural network

we brie   y introduce the simple recurrent neural network (id56) or elman network (elman
(1990)). its architecture is shown in figure 2(a). it has three types of layers in each time frame:

3

(b). the m-id56 modelembedding iembedding iirecurrentmultimodalsoftmaxwstartimageid98w1imageid98wlimageid98...predictw1predictw2predictwend(a). the simple id56 modelw(t)r(t-1)r(t)y(t)w(t-1)...y(t-1)input word layer wrecurrentlayer routputlayer yunfoldthe m-id56 model for one time frame128256256512published as a conference paper at iclr 2015

the input word layer w, the recurrent layer r and the output layer y. the activation of input, re-
current and output layers at time t is denoted as w(t), r(t), and y(t) respectively. w(t) denotes
the current word vector, which can be a simple 1-of-n coding representation h(t) (i.e. the one-hot
representation, which is binary and has the same dimension as the vocabulary size with only one
non-zero element) mikolov et al. (2010). y(t) can be calculated as follows:

x(t) = [w(t) r(t     1)]; r(t) = f1(u    x(t)); y(t) = g1(v    r(t));

(1)
where x(t) is a vector that concatenates w(t) and r(t    1), f1(.) and g1(.) are element-wise sigmoid
and softmax function respectively, and u, v are weights which will be learned.
the size of the id56 is adaptive to the length of the input sequence. the recurrent layers connect
the sub-networks in different time frames. accordingly, when we do id26, we need to
propagate the error through recurrent connections back in time (rumelhart et al. (1988)).

3.2 our m-id56 model

the structure of our multimodal recurrent neural network (m-id56) is shown in figure 2(b). it
has    ve layers in each time frame: two id27 layers, the recurrent layer, the multimodal
layer, and the softmax layer).
the two id27 layers embed the one-hot input into a dense word representation. it en-
codes both the syntactic and semantic meaning of the words. the semantically relevant words can be
found by calculating the euclidean distance between two dense word vectors in embedding layers.
most of the sentence-image multimodal models (karpathy et al. (2014); frome et al. (2013); socher
et al. (2014); kiros et al. (2014b)) use pre-computed id27 vectors as the initialization of
their model. in contrast, we randomly initialize our id27 layers and learn them from the
training data. we show that this random initialization is suf   cient for our architecture to generate
the state-of-the-art result. we treat the activation of the id27 layer ii (see figure 2(b))
as the    nal word representation, which is one of the three direct inputs of the multimodal layer.
after the two id27 layers, we have a recurrent layer with 256 dimensions. the calcula-
tion of the recurrent layer is slightly different from the calculation for the simple id56. instead of
concatenating the word representation at time t (denoted as w(t)) and the recurrent layer activation
at time t    1 (denoted as r(t    1)), we    rst map r(t    1) into the same vector space as w(t) and add
them together:

r(t) = f2(ur    r(t     1) + w(t));

(2)
where    +    represents element-wise addition. we set f2(.) to be the recti   ed linear unit (relu),
inspired by its the recent success when training very deep structure in id161    eld (nair
& hinton (2010); krizhevsky et al. (2012)). this differs from the simple id56 where the sigmoid
function is adopted (see section 3.1). relu is faster, and harder to saturate or over   t the data than
non-linear functions like the sigmoid. when the id26 through time (bptt) is conducted
for the id56 with sigmoid function, the vanishing or exploding gradient problem appears since even
the simplest id56 model can have a large temporal depth 3. previous work (mikolov et al. (2010;
2011)) use heuristics, such as the truncated bptt, to avoid this problem. the truncated bptt
stops the bptt after k time steps, where k is a hand-de   ned hyperparameter. because of the good
properties of relu, we do not need to stop the bptt at an early stage, which leads to better and
more ef   cient utilization of the data than the truncated bptt.
after the recurrent layer, we set up a 512 dimensional multimodal layer that connects the language
model part and the vision part of the m-id56 model (see figure 2(b)). this layer has three inputs:
the word-embedding layer ii, the recurrent layer and the image representation. for the image rep-
resentation, here we use the activation of the 7th layer of alexnet (krizhevsky et al. (2012)) or 15th
layer of vggnet (simonyan & zisserman (2014)), though our framework can use any image fea-
tures. we map the activation of the three layers to the same multimodal feature space and add them
together to obtain the activation of the multimodal layer:

m(t) = g2(vw    w(t) + vr    r(t) + vi    i);

(3)

3we tried sigmoid and scaled hyperbolic tangent function as the non-linear functions for id56 in the

experiments but they lead to the gradient explosion problem easily.

4

published as a conference paper at iclr 2015

where    +    denotes element-wise addition, m denotes the multimodal layer feature vector, i denotes
the image feature. g2(.) is the element-wise scaled hyperbolic tangent function (lecun et al. (2012)):

g2(x) = 1.7159    tanh(

2
3

x)

(4)

this function forces the gradients into the most non-linear value range and leads to a faster training
process than the basic hyperbolic tangent function.
both the simple id56 and m-id56 models have a softmax layer that generates the id203 dis-
tribution of the next word. the dimension of this layer is the vocabulary size m, which is different
for different datasets.

4 training the m-id56

to train our m-id56 model we adopt a log-likelihood cost function. it is related to the perplexity of
the sentences in the training set given their corresponding images. perplexity is a standard measure
for evaluating language model. the perplexity for one word sequence (i.e. a sentence) w1:l is
calculated as follows:

log2 ppl(w1:l|i) =     1
l

log2 p (wn|w1:n   1, i)

(5)

where l is the length of the word sequence, ppl(w1:l|i) denotes the perplexity of the sentence
w1:l given the image i. p (wn|w1:n   1, i) is the id203 of generating the word wn given i and
previous words w1:n   1. it corresponds to the activation of the softmax layer of our model.
the cost function of our model is the average log-likelihood of the words given their context words
and corresponding images in the training sentences plus a id173 term. it can be calculated
by the perplexity:

l(cid:88)

n=1

li    log2 ppl(w(i)

1:li

|i(i)) +         (cid:107)  (cid:107)2

2

(6)

ns(cid:88)

i=1

c =

1
n

where ns and n denotes the number of sentences and the number of words in the training set
receptively, li denotes the length of ith sentences, and    represents the model parameters.
our training objective is to minimize this cost function, which is equivalent to maximize the proba-
bility of generating the sentences in the training set using the model. the cost function is differen-
tiable and we use id26 to learn the model parameters.

5 sentence generation, id162 and sentence retrieval

we use the trained m-id56 model for three tasks: 1) sentences generation, 2) id162 (re-
trieving most relevant images to the given sentence), 3) sentence retrieval (retrieving most relevant
sentences to the given image).
the sentence generation process is straightforward. starting from the start sign wstart or arbitrary
number of reference words (e.g. we can input the    rst k words in the reference sentence to the
model and then start to generate new words), our model can calculate the id203 distribution
of the next word: p (wn|w1:n   1, i). then we can sample from this id203 distribution to pick
the next word. in practice, we    nd that selecting the word with the maximum id203 performs
slightly better than sampling. after that, we input the picked word to the model and continue the
process until the model outputs the end sign wend.
for the retrieval tasks, we use our model to calculate the id203 of generating a sentence w1:l
n p (wn|w1:n   1, i). the id203 can be treated as an af   nity

given an image i: p (w1:l|i) =(cid:81)

measurement between sentences and images.
for the id162 task, given the query sentence wq
ing to the id203 p (wq
perplexity-based id162 in kiros et al. (2014b).

1:l, we rank the dataset images id accord-
1:l|id) and retrieved the top ranked images. this is equivalent to the

5

published as a conference paper at iclr 2015

the sentence retrieval task is trickier because there might be some sentences that have high proba-
bility or perplexity for any image query (e.g. sentences consist of many frequently appeared words).
to solve this problem, kiros et al. (2014b) uses the perplexity of a sentence conditioned on the
averaged image feature across the training set as the reference perplexity to normalize the original
perplexity. different from them, we use the normalized id203 where the id172 factor
is the marginal id203 of wd

1:l);

p (wd

p (wd

1:l denotes the sentence in the dataset, iq denotes the query image, and i

(7)
are images
where wd
sampled from the training set. we approximate p (i
) by a constant and ignore this term. this
strategy leads to a much better performance than that in kiros et al. (2014b) in the experiments.
the normalized id203 is equivalent to the id203 p (iq|wd
1:l), which is symmetric to the
id203 p (wq

1:l|id) used in the id162 task.

(cid:48) p (wd
i

)

(cid:48)

(cid:48)

(cid:48)

1:l|i

)    p (i

(cid:48)

1:l:

1:l|iq)/p (wd

1:l) =(cid:80)

6 learning of sentence and image features

the architecture of our model allows the gradients from the id168 to be backpropagated to
both the id38 part (i.e. the id27 layers and the recurrent layer) and the
vision part (e.g. the alexnet or vggnet).
for the language part, as mentioned above, we randomly initialize the id38 layers and
learn their parameters. for the vision part, we use the pre-trained alexnet (krizhevsky et al. (2012))
or the vggnet (simonyan & zisserman (2014)) on id163 dataset (russakovsky et al. (2014)).
recently, karpathy et al. (2014) show that using the rid98 id164 results (girshick et al.
(2014)) combined with the alexnet features performs better than simply treating the image as a
whole frame. in the experiments, we show that our method performs much better than karpathy
et al. (2014) when the same image features are used, and is better than or comparable to their results
even when they use more sophisticated features based on id164.
we can update the id98 in the vision part of our model according to the gradient backpropagated
from the multimodal layer. in this paper, we    x the image features and the deep id98 network in the
training stage due to a shortage of data. in future work, we will apply our method on large datasets
(e.g. the complete ms coco dataset, which has not yet been released) and    netune the parameters
of the deep id98 network in the training stage.
the m-id56 model is trained using baidu   s internal deep learning platform paddle, which allows
us to explore many different model architectures in a short period. the hyperparameters, such as
layer dimensions and the choice of the non-linear id180, are tuned via cross-validation
on flickr8k dataset and are then    xed across all the experiments. it takes 25 ms on average to
generate a sentence (excluding image feature extraction stage) on a single core cpu.

7 experiments

7.1 datasets

we test our method on four benchmark datasets with sentence level annotations: iapr tc-12 (grub-
inger et al. (2006)), flickr 8k (rashtchian et al. (2010)), flickr 30k (young et al. (2014)) and ms
coco (lin et al. (2014)).
iapr tc-12. this dataset consists of around 20,000 images taken from different locations around
the world. it contains images of different sports and actions, people, animals, cities, landscapes,
etc. for each image, it provides at least one sentence annotation. on average, there are about 1.7
sentence annotations for one image. we adopt the standard separation of training and testing set as
previous works (guillaumin et al. (2010); kiros et al. (2014b)) with 17,665 images for training and
1962 images for testing.
flickr8k. this dataset consists of 8,000 images extracted from flickr. for each image, it provides
   ve sentence annotations. we adopt the standard separation of training, validation and testing set
provided by the dataset. there are 6,000 images for training, 1,000 images for validation and 1,000
images for testing.

6

published as a conference paper at iclr 2015

flickr30k. this dataset is a recent extension of flickr8k. for each image, it also provides    ve
sentences annotations. it consists of 158,915 crowd-sourced captions describing 31,783 images.
the grammar and style for the annotations of this dataset is similar to flickr8k. we follow the
previous work (karpathy et al. (2014)) which used 1,000 images for testing. this dataset, as well as
the flick8k dataset, were originally used for the image-sentence retrieval tasks.
ms coco. the current release of this recently proposed dataset contains 82,783 training images
and 40,504 validation images. for each image, it provides    ve sentences annotations. we randomly
sampled 4,000 images for validation and 1,000 images for testing from their currently released
validation set. the dataset partition of ms coco and flickr30k is available in the project page 4.

7.2 id74

sentence generation. following previous works, we use the sentence perplexity (see equ. 5) and
id7 scores (i.e. b-1, b-2, b-3, and b-4) (papineni et al. (2002)) as the id74. id7
scores were originally designed for automatic machine translation where they rate the quality of a
translated sentences given several reference sentences. similarly, we can treat the sentence gener-
ation task as the    translation    of the content of images to sentences. id7 remains the standard
evaluation metric for sentence generation methods for images, though it has drawbacks. for some
images, the reference sentences might not contain all the possible descriptions in the image and
id7 might penalize some correctly generated sentences. please see more details of the calcula-
tion of id7 scores for this task in the supplementary material section 10.3 5.
sentence retrieval and id162. we adopt the same id74 as previous works
(socher et al. (2014); frome et al. (2013); karpathy et al. (2014)) for both the tasks of sentences
retrieval and id162. we use r@k (k = 1, 5, 10) as the measurement. r@k is the recall
rate of a correctly retrieved groundtruth given top k candidates. higher r@k usually means better
retrieval performance. since we care most about the top-ranked retrieved results, the r@k scores
with smaller k are more important.
the med r is another metric we use, which is the median rank of the    rst retrieved groundtruth
sentence or image. lower med r usually means better performance. for iapr tc-12 datasets,
we use additional id74 to conduct a fair comparison with previous work (kiros et al.
(2014b)). please see the details in the supplementary material section 10.3.

7.3 results on iapr tc-12

the results of the sentence generation task6 are shown in table 1. ours-id56-base serves as a
baseline method for our m-id56 model. it has the same architecture as m-id56 except that it does
not have the image representation input.
to conduct a fair comparison, we follow the same experimental settings of kiros et al. (2014b)
to calculate the id7 scores and perplexity. these two id74 are not necessarily
correlated to each other for the following reasons. as mentioned in section 4, perplexity is calculated
according to the id155 of the word in a sentence given all of its previous reference
words. therefore, a strong language model that successfully captures the distributions of words in
sentences can have a low perplexity without the image content. but the content of the generated
sentences might be uncorrelated to images. from table 1, we can see that although our baseline
method of id56 generates a low perplexity, its id7 score is low, indicating that it fails to generate
sentences that are consistent with the content of images.
table 1 shows that our m-id56 model performs much better than our baseline id56 model and the
state-of-the-art methods both in terms of the perplexity and id7 score.

4www.stat.ucla.edu/  junhua.mao/m-id56.html
5the id7 outputted by our implementation is slightly lower than the recently released ms coco caption
evaluation toolbox (chen et al. (2015)) because of different id121 methods of the sentences. we re-
evaluate our method using the toolbox in the current version of the paper.

6kiros et al. (2014b) further improved their results after the publication. we compare our results with their

updated ones here.

7

published as a conference paper at iclr 2015

lbl, mnih & hinton (2007)
mlblb-alexnet, kiros et al. (2014b)
mlblf-alexnet, kiros et al. (2014b)
gupta et al. (2012)
gupta & mannem (2012)
ours-id56-base
ours-m-id56-alexnet

ppl
9.29
9.86
9.90

-
-

7.77
6.92

b-1
0.321
0.393
0.387
0.15
0.33
0.307
0.482

b-2
0.145
0.211
0.209
0.06
0.18
0.177
0.357

b-3
0.064
0.112
0.115
0.01
0.07
0.096
0.269

b-4
-
-
-
-
-

0.043
0.208

table 1: results of the sentence generation task on the iapr tc-12 dataset.    b    is short for id7.

sentence retrival (image to text)
r@1 r@5 r@10 med r

ours-m-id56 20.9

43.8

54.4

8

image retrival (text to image)
r@1 r@5 r@10 med r
13.2

31.2

40.8

21

table 2: r@k and median rank (med r) for iapr tc-12 dataset.

random
sdt-id56-alexnet
socher-avg-rid98
devise-avg-rid98
deepfe-alexnet
deepfe-rid98
ours-m-id56-alexnet

sentence retrival (image to text)
r@1 r@5 r@10 med r
631
0.1
4.5
32
23
6.0
28
4.8
34
5.9
14
12.6
14.5
11

0.5
18.0
22.7
16.5
19.2
32.9
37.2

1.0
28.6
34.0
27.3
27.3
44.0
48.5

image retrival (text to image)
r@1 r@5 r@10 med r
500
0.1
6.1
29
25
6.6
29
5.9
32
5.2
15
9.7
11.5
15

0.5
18.5
21.6
20.1
17.6
29.6
31.0

1.0
29.0
31.7
29.6
26.5
42.5
42.4

table 3: results of r@k and median rank (med r) for flickr8k dataset.    -alexnet    denotes the
image representation based on alexnet extracted from the whole image frame.    -rid98    denotes
the image representation extracted from possible objects detected by the rid98 algorithm.

for the retrieval tasks, since there are no publicly available results of r@k and med r in this dataset,
we report r@k scores of our method in table 2 for future comparisons. the result shows that
20.9% top-ranked retrieved sentences and 13.2% top-ranked retrieved images are groundtruth. we
also adopt additional id74 to compare our method with kiros et al. (2014b), see sup-
plementary material section 10.2.

7.4 results on flickr8k

this dataset was widely used as a benchmark dataset for image and sentence retrieval. the r@k
and med r of different methods are shown in table 3. we compare our model with several state-of-
the-art methods: sdt-id56 (socher et al. (2014)), devise (frome et al. (2013)), deepfe (karpathy
et al. (2014)) with various image representations. our model outperforms these methods by a large
margin when using the same image representation (e.g. alexnet). we also list the performance of
methods using more sophisticated features in table 3.    -avg-rid98    denotes methods with features
of the average id98 activation of all objects above a detection con   dence threshold. deepfe-rid98
karpathy et al. (2014) uses a fragment mapping strategy to better exploit the id164 results.
the results show that using these features improves the performance. even without the help from
the id164 methods, however, our method performs better than these methods in almost all
the id74. we will develop our framework using better image features based on object
detection in the future work.
the ppl, b-1, b-2, b-3 and b-4 of the generated sentences using our m-id56-alexnet model in
this dataset are 24.39, 0.565, 0.386, 0.256, and 0.170 respectively.

8

published as a conference paper at iclr 2015

sentence retrival (image to text)
r@1 r@5 r@10 med r

image retrival (text to image)
r@1 r@5 r@10 med r

flickr30k

random
devise-avg-rid98
deepfe-rid98
rvr
mnlm-alexnet
mnlm-vggnet
nic
lrcn
deepvs
ours-m-id56-alexnet
ours-m-id56-vggnet

random
deepvs-rid98
ours-m-id56-vggnet

0.1
4.8
16.4
12.1
14.8
23.0
17.0
14.0
22.2
18.4
35.4

0.1
29.4
41.0

0.6
16.5
40.2
27.8
39.2
50.7
56.0
34.9
48.2
40.2
63.8

0.6
62.0
73.0

1.1
27.3
54.7
47.8
50.9
62.9

-

47.0
61.4
50.9
73.7

1.1
75.9
83.5

ms coco

631
28
8
11
10
5
7
11
4.8
10
3

631
2.5
2

0.1
5.9
10.3
12.7
11.8
16.8
17.0

-

15.2
12.6
22.8

0.5
20.1
31.4
33.1
34.0
42.0
57.0

-

37.7
31.2
50.7

1.0
29.6
44.5
44.9
46.3
56.5

-
-

50.5
41.5
63.1

0.1
20.9
29.0

0.5
52.8
42.2

1.0
69.2
77.0

500
29
13
12.5
13
8
7
-
9.2
16
5

500
4
3

table 4: results of r@k and median rank (med r) for flickr30k dataset and ms coco dataset.

flickr30k

ppl b-1 b-2 b-3 b-4 ppl b-1 b-2 b-3 b-4
0.19

0.13

-

-

-

-

-

ms coco

-

-
-

0.47 0.21 0.09
21.20 0.50 0.30 0.15

rvr
deepvs-alexnet
deepvs-vggnet
nic
lrcn
dmsm
ours-m-id56-alexnet 35.11 0.54 0.36 0.23 0.15
ours-m-id56-vggnet

0.66
0.59 0.39 0.25 0.16

-
-
-

-
-
-

-

-

-

-

-

-

-
-

-
-
-
-

0.53 0.28 0.15
19.64 0.57 0.37 0.19

-

-

0.67
0.63 0.44 0.31 0.21
0.21

-
-

-
-

-
-

-
-
-

-

20.72 0.60 0.41 0.28 0.19 13.60 0.67 0.49 0.35 0.25

table 5: results of generated sentences on the flickr 30k dataset and ms coco dataset.

id56 dim.
lstm

our m-id56 mnlm nic
512
yes

256
no

300
yes

lrcn
1000 (  4)

yes

rvr deepvs
100
300-600
no

no

table 6: properties of the recurrent layers for the    ve very recent methods. lrcn has a stack of
four 1000 dimensional lstm layers. we achieves state-of-the-art performance using a relatively
small dimensional recurrent layer. lstm (hochreiter & schmidhuber (1997)) can be treated as a
sophisticated version of the id56.

7.5 results on flickr30k and ms coco

we compare our method with several state-of-the-art methods in these two recently released dataset
(note that the last six methods appear very recently, we use the results reported in their papers):
devise (frome et al. (2013)), deepfe (karpathy et al. (2014)), mnlm (kiros et al. (2014a)),
dmsm (fang et al. (2014)), nic (vinyals et al. (2014)), lrcn (donahue et al. (2014)), rvr
(chen & zitnick (2014)), and deepvs (karpathy & fei-fei (2014)). the results of the retrieval
tasks and the sentence generation task 7 are shown in table 4 and table 5 respectively. we also
summarize some of the properties of the recurrent layers adopted in the    ve very recent methods in
table 6.

7we only select the word with maximum id203 each time in the sentence generation process in table
5 while many comparing methods (e.g. dmsm, nic, lrcn) uses a id125 scheme that keeps the best k
candidates. the id125 scheme will lead to better performance in practice using the same model.

9

published as a conference paper at iclr 2015

m-id56-greedy-c5
m-id56-greedy-c40
m-id56-beam-c5
m-id56-beam-c40

b1
0.668
0.845
0.680
0.865

b2
0.488
0.730
0.506
0.760

b3
0.342
0.598
0.369
0.641

b4
0.239
0.473
0.272
0.529

cider id8 l meteor
0.729
0.740
0.791
0.789

0.489
0.616
0.499
0.640

0.221
0.291
0.225
0.304

table 7: results of the ms coco test set evaluated by ms coco evaluation server

our method with vggnet image representation (simonyan & zisserman (2014)) outperforms the
state-of-the-art methods, including the very recently released methods, in almost all the evaluation
metrics. note that the dimension of the recurrent layer of our model is relatively small compared
to the competing methods. it shows the advantage and ef   ciency of our method that directly inputs
the visual information to the multimodal layer instead of storing it in the recurrent layer. the m-
id56 model with vggnet performs better than that with alexnet, which indicates the importance
of strong image representations in this task. 71% of the generated sentences for ms coco datasets
are novel (i.e. different from training sentences).
we also validate our method on the test set of ms coco by their evaluation server (chen et al.
(2015)). the results are shown in table 7. we evaluate our model with greedy id136 (select
the word with the maximum id203 each time) as well as with the id125 id136.    -
c5    represents results using 5 reference sentences and    -c40    represents results using 40 reference
sentences.
to further validate the importance of different components of the m-id56 model, we train sev-
eral variants of the original m-id56 model and compare their performance. in particular, we show
that the two-layer id27 system outperforms the single-layer version and the strategy of
directly inputting the visual information to the multimodal layer substantially improves the perfor-
mance (about 5% for b-1). due to the limited space, we put the details of these experiments in
section 10.1 in the supplementary material after the main paper.

8 nearest neighbor as reference

recently, devlin et al. (2015b) proposed a nearest neighbor approach that retrieves the captions
of the k nearest images in the training set, ranks these captions according to the consensus of the
caption w.r.t. to the rest of the captions, and output the top ranked one.
inspired by this method, we    rst adopt the m-id56 model with the transposed weight sharing strat-
egy (mao et al. (2015), denoted as m-id56-shared) to generate n hypotheses using a id125
scheme. speci   cally, we keep the n best candidates in the sentence generation process until the
model generates the end sign wend. these n best candidates are approximately the n most probable
sentences generated by the model, and can be treated as the n hypotheses. in our experiments, we set
n = 10 since it gives us a diversi   ed set of hypotheses without too much outliers on our validation
set. 8
after generating the hypotheses of a target image, we retrieve its nearest neighbors in the image
feature space on the training set (see details in section 8.1). then we calculate the    consensus   
scores (devlin et al. (2015a)) of the hypotheses w.r.t.
to the groundtruth captions of the nearest
neighbor images, and rerank the hypotheses according to these scores (see details in section 8.2).

8.1

image features for the nearest neighbor image search

we try two types of image features for the nearest neighbor image search 9. the    rst one is the
original image features extracted by the vggnet (simonyan & zisserman (2014)). we    rst resize
the image so that its short side is 256 pixels. then we extract features on ten 224    224 windows

8if we directly output the top hypotheses generated by the model, then n = 5 gives us the best performance.

but if we want to rerank the hypotheses, then n = 10 gives us a better result on the validation set.

9we release both types of the features on ms coco 2014 train, val and test sets. please refer to the readme

   le at https://github.com/mjhucla/mid56-cr to see how to download and use them.

10

published as a conference paper at iclr 2015

figure 3: the sample images and their nearest neighbors retrieved by two types of features. com-
pared to the original vggnet features, the features re   ned by the m-id56 model are better for cap-
turing richer and more accurate visual information.

(the four corners, the center and their mirrored versions) on the resized image. finally, we average
pool the ten features to make it a 4,096 dimensional feature.
the second type is the feature re   ned by our m-id56 model. it can be calculated as: ir = g2(vi  i),
where vi is the weight matrix between the image representation and the multimodal layer (see
equation 3), and g2(.) is the scaled hyperbolic tangent function.
we show the sample images and their nearest neighbors in figure 3. we    nd that compared to the
original vggnet features, the features re   ned by the m-id56 model capture richer and more accurate
visual information. e.g., the target image in the second row contains an old woman with a bunch of
bananas. the original vggnet features do not retrieve images with bananas in them.

8.2 consensus reranking

suppose we have get the k nearest neighbor images in the training set as the reference. we follow
devlin et al. (2015a) to calculate the consensus score of a hypotheses. the difference is that devlin
et al. (2015a) treat the captions of the k nearest neighbor images as the hypotheses while our hy-
potheses are generated by the m-id56 model. more speci   cally, for each hypothesis, we calculate
the mean similarity between this hypothesis and all the captions of the k nearest neighbor images.

ms coco val for consensus reranking

b4 cider id8 l meteor

b2

b1
m-id56-shared
0.686 0.511 0.375 0.280 0.842
m-id56-shared-nnref-id7
0.718 0.550 0.409 0.305 0.909
m-id56-shared-nnref-cider
0.714 0.543 0.406 0.304 0.938
m-id56-shared-nnref-id7-orcale 0.792 0.663 0.543 0.443 1.235
m-id56-shared-nnref-cider-oracle 0.784 0.648 0.529 0.430 1.272

b3

m-id56-shared
m-id56-shared-nnref-id7
m-id56-shared-nnref-cider

ms coco 2014 test server

b3

b2

b1
0.685 0.512 0.376 0.279 0.819
0.720 0.553 0.410 0.302 0.886
0.716 0.545 0.404 0.299 0.917

b4 cider id8 l meteor

0.500
0.519
0.519
0.602
0.593

0.228
0.235
0.239
0.287
0.287

0.504
0.524
0.521

0.229
0.238
0.242

table 8: results of m-id56-shared model after applying consensus reranking using nearest neigh-
bors as references (m-id56-shared-nnref), compared with those of the original m-id56 model on
our validation set and ms coco test server.

11

target imagenearest five neighbors in terms of m-id56 refined featurenearest five neighbors in terms of original vgg featurepublished as a conference paper at iclr 2015

the consensus score of this hypothesis is the mean similarity score of the m nearest captions. the
similarity between a hypothesis and one of its nearest neighbor reference captions is de   ned by
a sentence-level id7 score (papineni et al. (2002)) or a sentence-level cider (vedantam et al.
(2014)). we cross-validate the hyperparamters k and m. for the id7-based similarity, the opti-
mal k and m are 60 and 175 respectively. for the cider-based similarity, the optimal k and m are
60 and 125 respectively.

8.3 experiments

we show the results of our model on our validation set and the ms coco testing server in table
8. for id7-based consensus reranking, we get an improvement of 3.5 points on our validation
set and 3.3 points on the ms coco test 2014 set in terms of id74 score. for the cider-based
consensus reranking, we get an improvement of 9.4 points on our validation set and 9.8 points on
the ms coco test 2014 set in terms of cider.

8.4 discussion

we show the rank of the ten hypotheses before and after reranking in figure 4. although the hy-
potheses are similar to each other, there are some variances among them (e.g., some of them capture
more details of the images. some of them might be partially wrong). the reranking process is able
to improve the rank of good captions.
we also show the oracle performance of the ten hypotheses, which is the upper bound of the con-
sensus reranking. more speci   cally, for each image in our validation set, we rerank the hypotheses
according to the scores (id7 or cider) w.r.t to the groundtruth captions. the results of this oracle
reranking are shown in table 8 (see rows with    -oracle   ). the oracle performance is surprisingly
high, indicating that there is still room for improvement, both for the m-id56 model itself and the
reranking strategy.

9 conclusion

we propose a multimodal recurrent neural network (m-id56) framework that performs at the
state-of-the-art in three tasks: sentence generation, sentence retrieval given query image and image

figure 4: the original rank of the hypotheses and the rank after consensus reranking (cider).

12

original after reranking (c(der) 1. a piece of cake on a plate on a table 2. a piece of cake on a white plate 3. a piece of cake sitting on top of a white plate 4. a piece of cake sitting on top of a plate 5. a piece of cake on a plate with a fork 6. a close up of a piece of cake on a plate 7. a piece of chocolate cake on a plate 8. a piece of cake sitting on a plate 9. a slice of cake on a white plate 10. a slice of cake on a plate with a fork 1. a piece of cake on a plate with a fork 2. a slice of cake on a plate with a fork 3. a close up of a piece of cake on a plate 4. a piece of cake on a plate on a table 5. a piece of cake on a white plate 6. a piece of cake sitting on top of a plate 7. a piece of cake sitting on top of a white plate 8. a piece of chocolate cake on a plate 9. a piece of cake sitting on a plate 10. a slice of cake on a white plate 1. a black and white photo of a black bear 2. a black and white photo of a bear 3. a black bear laying on top of a rock 4. a black bear sitting on top of a wooden bench 5. a black bear sitting on top of a rock 6. a black bear laying on top of a wooden bench 7. a black and white photo of a dog 8. a black bear laying on top of a wooden floor 9. a close up of a black and white dog 10. a close up of a black and white cat 1. a black bear sitting on top of a rock 2. a black bear laying on top of a rock 3. a black bear sitting on top of a wooden bench 4. a black bear laying on top of a wooden bench 5. a black bear laying on top of a wooden floor 6. a black and white photo of a black bear 7. a black and white photo of a bear 8. a close up of a black and white dog 9. a black and white photo of a dog 10. a close up of a black and white cat 1. a group of people standing next to each other 2. a group of people standing around a train 3. a group of people standing in a room 4. a group of people in a room with luggage 5. a group of people that are standing in a room 6. a group of people standing next to a train 7. a group of people standing in front of a train 8. a group of people sitting on a bench 9. a group of people standing in a room with luggage 10. a group of people standing next to each other on a train 1. a group of people standing in a room with luggage 2. a group of people in a room with luggage 3. a group of people standing next to a train 4. a group of people standing in front of a train 5. a group of people standing around a train 6. a group of people standing next to each other on a train 7. a group of people standing in a room 8. a group of people standing next to each other 9. a group of people that are standing in a room 10. a group of people sitting on a bench published as a conference paper at iclr 2015

retrieval given query sentence. the model consists of a deep id56, a deep id98 and these two
sub-networks interact with each other in a multimodal layer. our m-id56 is powerful of connecting
images and sentences and is    exible to incorporate more complex image representations and more
sophisticated language models.

acknowledgments

we thank andrew ng, kai yu, chang huang, duohao qin, haoyuan gao, jason eisner for useful
discussions and technical support. we also thank the comments and suggestions of the anonymous
reviewers from iclr 2015 and nips 2014 deep learning workshop. we acknowledge the center
for minds, brains and machines (cbmm), partially funded by nsf stc award ccf-1231216, and
aro 62250-cs.

references
barnard, kobus, duygulu, pinar, forsyth, david, de freitas, nando, blei, david m, and jordan,

michael i. matching words and pictures. jmlr, 3:1107   1135, 2003.

chen, x., fang, h., lin, ty, vedantam, r., gupta, s., dollr, p., and zitnick, c. l. microsoft coco

captions: data collection and evaluation server. arxiv preprint arxiv:1504.00325, 2015.

chen, xinlei and zitnick, c lawrence. learning a recurrent visual representation for image caption

generation. arxiv preprint arxiv:1411.5654, 2014.

cho, kyunghyun, van merrienboer, bart, gulcehre, caglar, bougares, fethi, schwenk, holger,
and bengio, yoshua. learning phrase representations using id56 encoder-decoder for statistical
machine translation. arxiv preprint arxiv:1406.1078, 2014.

devlin, jacob, cheng, hao, fang, hao, gupta, saurabh, deng, li, he, xiaodong, zweig, geoffrey,
and mitchell, margaret. language models for image captioning: the quirks and what works.
arxiv preprint arxiv:1505.01809, 2015a.

devlin, jacob, gupta, saurabh, girshick, ross, mitchell, margaret, and zitnick, c lawrence. ex-
ploring nearest neighbor approaches for image captioning. arxiv preprint arxiv:1505.04467,
2015b.

donahue, jeff, hendricks, lisa anne, guadarrama, sergio, rohrbach, marcus, venugopalan, sub-
hashini, saenko, kate, and darrell, trevor. long-term recurrent convolutional networks for visual
recognition and description. arxiv preprint arxiv:1411.4389, 2014.

elman, jeffrey l. finding structure in time. cognitive science, 14(2):179   211, 1990.

fang, hao, gupta, saurabh, iandola, forrest, srivastava, rupesh, deng, li, doll  ar, piotr, gao,
jianfeng, he, xiaodong, mitchell, margaret, platt, john, et al. from captions to visual concepts
and back. arxiv preprint arxiv:1411.4952, 2014.

farhadi, ali, hejrati, mohsen, sadeghi, mohammad amin, young, peter, rashtchian, cyrus, hock-
enmaier, julia, and forsyth, david. every picture tells a story: generating sentences from images.
in eccv, pp. 15   29. 2010.

frome, andrea, corrado, greg s, shlens, jon, bengio, samy, dean, jeff, mikolov, tomas, et al.

devise: a deep visual-semantic embedding model. in nips, pp. 2121   2129, 2013.

girshick, r., donahue, j., darrell, t., and malik, j. rich feature hierarchies for accurate object

detection and semantic segmentation. in cvpr, 2014.

grubinger, michael, clough, paul, m  uller, henning, and deselaers, thomas. the iapr tc-12 bench-
in international workshop

mark: a new evaluation resource for visual information systems.
ontoimage, pp. 13   23, 2006.

guillaumin, matthieu, verbeek, jakob, and schmid, cordelia. multiple instance metric learning

from automatically labeled bags of faces. in eccv, pp. 634   647, 2010.

13

published as a conference paper at iclr 2015

gupta, ankush and mannem, prashanth. from image annotation to image description. in iconip,

2012.

gupta, ankush, verma, yashaswi, and jawahar, cv. choosing linguistics over vision to describe

images. in aaai, 2012.

hochreiter, sepp and schmidhuber, j  urgen. long short-term memory. neural computation, 9(8):

1735   1780, 1997.

hodosh, micah, young, peter, and hockenmaier, julia. framing image description as a ranking

task: data, models and id74. jair, 47:853   899, 2013.

jia, yangqing, salzmann, mathieu, and darrell, trevor. learning cross-modality similarity for

multinomial data. in iccv, pp. 2407   2414, 2011.

kalchbrenner, nal and blunsom, phil. recurrent continuous translation models. in emnlp, pp.

1700   1709, 2013.

karpathy, andrej and fei-fei, li. deep visual-semantic alignments for generating image descrip-

tions. arxiv preprint arxiv:1412.2306, 2014.

karpathy, andrej, joulin, armand, and fei-fei, li. deep fragment embeddings for bidirectional

image sentence mapping. in arxiv:1406.5679, 2014.

kiros, ryan, salakhutdinov, ruslan, and zemel, richard s. unifying visual-semantic embeddings

with multimodal neural language models. arxiv preprint arxiv:1411.2539, 2014a.

kiros, ryan, zemel, r, and salakhutdinov, ruslan. multimodal neural language models. in icml,

2014b.

krizhevsky, alex, sutskever, ilya, and hinton, geoffrey e. id163 classi   cation with deep con-

volutional neural networks. in nips, pp. 1097   1105, 2012.

kulkarni, girish, premraj, visruth, dhar, sagnik, li, siming, choi, yejin, berg, alexander c, and

berg, tamara l. baby talk: understanding and generating image descriptions. in cvpr, 2011.

kuznetsova, polina, ordonez, vicente, berg, tamara l, and choi, yejin. treetalk: composition and
compression of trees for image descriptions. transactions of the association for computational
linguistics, 2(10):351   362, 2014.

lecun, yann a, bottou, l  eon, orr, genevieve b, and m  uller, klaus-robert. ef   cient backprop. in

neural networks: tricks of the trade, pp. 9   48. springer, 2012.

lin, tsung-yi, maire, michael, belongie, serge, hays, james, perona, pietro, ramanan, deva,
doll  ar, piotr, and zitnick, c lawrence. microsoft coco: common objects in context. arxiv
preprint arxiv:1405.0312, 2014.

mao, junhua, xu, wei, yang, yi, wang, jiang, and yuille, alan l. explain images with multimodal

recurrent neural networks. nips deeplearning workshop, 2014.

mao, junhua, xu, wei, yang, yi, wang, jiang, huang, zhiheng, and yuille, alan. learning like a
child: fast novel visual concept learning from sentence descriptions of images. arxiv preprint
arxiv:1504.06692, 2015.

mikolov, tomas, kara     at, martin, burget, lukas, cernock`y, jan, and khudanpur, sanjeev. recur-

rent neural network based language model. in interspeech, pp. 1045   1048, 2010.

mikolov, tomas, kombrink, stefan, burget, lukas, cernocky, jh, and khudanpur, sanjeev. exten-

sions of recurrent neural network language model. in icassp, pp. 5528   5531, 2011.

mikolov, tomas, sutskever, ilya, chen, kai, corrado, greg s, and dean, jeff. distributed represen-

tations of words and phrases and their compositionality. in nips, pp. 3111   3119, 2013.

14

published as a conference paper at iclr 2015

mitchell, margaret, han, xufeng, dodge, jesse, mensch, alyssa, goyal, amit, berg, alex, ya-
maguchi, kota, berg, tamara, stratos, karl, and daum  e iii, hal. midge: generating image
descriptions from id161 detections. in eacl, 2012.

mnih, andriy and hinton, geoffrey. three new id114 for statistical language modelling.

in icml, pp. 641   648. acm, 2007.

nair, vinod and hinton, geoffrey e. recti   ed linear units improve restricted id82s.

in icml, pp. 807   814, 2010.

papineni, kishore, roukos, salim, ward, todd, and zhu, wei-jing. id7: a method for automatic

evaluation of machine translation. in acl, pp. 311   318, 2002.

rashtchian, cyrus, young, peter, hodosh, micah, and hockenmaier, julia. collecting image anno-

tations using amazon   s mechanical turk. in naacl-hlt workshop 2010, pp. 139   147, 2010.

rumelhart, david e, hinton, geoffrey e, and williams, ronald j. learning representations by

back-propagating errors. cognitive modeling, 1988.

russakovsky, olga, deng, jia, su, hao, krause, jonathan, satheesh, sanjeev, ma, sean, huang,
zhiheng, karpathy, andrej, khosla, aditya, bernstein, michael, berg, alexander c., and fei-fei,
li. id163 large scale visual recognition challenge, 2014.

simonyan, karen and zisserman, andrew. very deep convolutional networks for large-scale image

recognition. arxiv preprint arxiv:1409.1556, 2014.

socher, richard, le, q, manning, c, and ng, a. grounded id152 for    nding and

describing images with sentences. in tacl, 2014.

srivastava, nitish and salakhutdinov, ruslan. multimodal learning with deep id82s.

in nips, pp. 2222   2230, 2012.

sutskever, ilya, vinyals, oriol, and le, quoc vv. sequence to sequence learning with neural net-

works. in nips, pp. 3104   3112, 2014.

vedantam, ramakrishna, zitnick, c lawrence, and parikh, devi. cider: consensus-based image

description evaluation. arxiv preprint arxiv:1411.5726, 2014.

vinyals, oriol, toshev, alexander, bengio, samy, and erhan, dumitru. show and tell: a neural

image caption generator. arxiv preprint arxiv:1411.4555, 2014.

young, peter, lai, alice, hodosh, micah, and hockenmaier, julia. from image descriptions to visual
denotations: new similarity metrics for semantic id136 over event descriptions. in acl, pp.
479   488, 2014.

10 supplementary material

10.1 effectiveness of the different components of the m-id56 model

m-id56
m-id56-noembinput
m-id56-onelayeremb
m-id56-emboneinput
m-id56-visinid56
m-id56-visinid56-both
m-id56-visinid56-both-shared

b-1
0.600
0.592
0.594
0.590
0.466
0.546
0.478

b-2
0.412
0.408
0.406
0.406
0.267
0.333
0.279

b-3
0.278
0.277
0.274
0.274
0.157
0.191
0.171

b-4
0.187
0.188
0.184
0.185
0.101
0.120
0.110

table 9: performance comparison of different versions of m-id56 models on the flickr30k dataset.
all the models adopt vggnet as the image representation. see figure 5 for details of the models.

15

published as a conference paper at iclr 2015

figure 5: illustration of the seven variants of the m-id56 models.

in this section, we compare different variants of our m-id56 model to show the effectiveness of the
two-layer id27 and the strategy to input the visual information to the multimodal layer.
the id27 system. intuitively, the two id27 layers capture high-level se-
mantic meanings of words more ef   ciently than the single layer id27. as an input to
the multimodal layer, it offers useful information for predicting the next word distribution.
to validate its ef   ciency, we train three different m-id56 networks: m-id56-noembinput, m-id56-
onelayeremb, m-id56-emboneinput. they are illustrated in figure 5.    m-id56-noembinput   
denotes the m-id56 model whose connection between the id27 layer ii and the mul-
timodal layer is cut off. thus the multimodal layer has only two inputs: the recurrent layer and
the image representation.    m-id56-onelayeremb    denotes the m-id56 model whose two word
embedding layers are replaced by a single 256 dimensional word-embedding layer. there are much
more parameters of the word-embedding layers in the m-id56-onelayeremb than those in the
original m-id56 (256    m v.s. 128    m + 128    256) if the dictionary size m is large.    m-id56-
emboneinput    denotes the m-id56 model whose connection between the id27 layer ii
and the multimodal layer is replaced by the connection between the id27 layer i and the
multimodal layer. the performance comparisons are shown in table 9.
table 9 shows that the original m-id56 model with the two id27 layers and the con-
nection between id27 layer ii and multimodal layer performs the best. it veri   es the
effectiveness of the two id27 layers.
how to connect the vision and the language part of the model. we train three variants of m-id56
models where the image representation is inputted into the recurrent layer: m-id56-visualinid56,
m-id56-visualinid56-both, and m-id56-visualinid56-both-shared. for m-id56-visualinid56,
we only input the image representation to the id27 layer ii while for the later two mod-
els, we input the image representation to both the multimodal layer and id27 layer ii.

16

embedding iembedding iirecurrentmultimodalsoftmaxwstartimageid98predictw1the original m-id56 model for one time frame128256256512wstartimageid98predictw1128256256512m-id56-noembinputwstartimageid98predictw1256256512m-id56-onelayerembwstartimageid98predictw1128256256512m-id56-emboneinputwstartimageid98predictw1128256256512wstartimageid98predictw1128256256512vi(1)vi(2)wstartimageid98predictw1128256256512vivim-id56-visualinid56m-id56-visualinid56-bothm-id56-visualinid56-both-sharedpublished as a conference paper at iclr 2015

i

i

, v (2)

the weights of the two connections v (1)
are shared for m-id56-visualinid56-both-shared.
please see details of these models in figure 5. table 9 shows that the original m-id56 model
performs much better than these models, indicating that it is effective to directly input the visual
information to the multimodal layer.
in practice, we    nd that it is harder to train these variants than to train the original m-id56 model
and we have to keep the learning rate very small to avoid the exploding gradient problem. increasing
the dimension of the recurrent layer or replacing id56 with lstm (a sophisticated version of id56
hochreiter & schmidhuber (1997)) might solve the problem. we will explore this issue in future
work.

10.2 additional retrieval performance comparisons on iapr tc-12

for the retrieval results in this dataset, in addition to the r@k and med r, we also adopt exactly
the same id74 as kiros et al. (2014b) and plot the mean number of matches of the
retrieved groundtruth sentences or images with respect to the percentage of the retrieved sentences
or images for the testing set. for the sentence retrieval task, kiros et al. (2014b) uses a shortlist of
100 images which are the nearest neighbors of the query image in the feature space. this shortlist
strategy makes the task harder because similar images might have similar descriptions and it is often
harder to    nd subtle differences among the sentences and pick the most suitable one.
the recall accuracy curves with respect to the percentage of retrieved images (sentence retrieval
task) or sentences (sentence retrieval task) are shown in figure 6. the    rst method, bowdecaf, is a
strong image based bag-of-words baseline (kiros et al. (2014b)). the second and the third models
(kiros et al. (2014b)) are all multimodal deep models. our m-id56 model signi   cantly outperforms
these three methods in this task.

10.3 the calculation of id7 score

the id7 score was proposed by papineni et al. (2002) and was originally used as a evaluation
metric for machine translation. to calculate id7-n (i.e. b-n in the paper where n=1,2,3,4) score,
we    rst compute the modi   ed id165 precision (papineni et al. (2002)), pn. then we compute the
geometric mean of pn up to length n and multiply it by a brevity penalty bp:

(8)

(9)
where r is the length of the reference sentence and c is the length of the generated sentence. we
use the same strategy as fang et al. (2014) where pn, r, and c are computed over the whole testing
corpus. when there are multiple reference sentences, the length of the reference that is closest
(longer or shorter) to the length of the candidate is used to compute r.

n=1 log pn

bp = min(1, e1    r
c )
(cid:80)n

b-n = bp    e

1
n

(a) image to text curve

(b) text to image curve

figure 6: retrieval recall curve for (a). sentence retrieval task (b). id162 task on iapr
tc-12 dataset. the behavior on the far left (i.e. top few retrievals) is most important.

17

0.010.020.050.1 0.250.5 1   00.10.20.30.40.50.60.70.80.91  ours   mid56bow   decafmlbl   f   decafmlbl   b   decaf0.00050.001 0.002 0.005 0.01  0.02  0.05  0.1   0.25  0.5   1     00.10.20.30.40.50.60.70.80.91  ours   mid56bow   decafmlbl   f   decafmlbl   b   decaf