explain images with multimodal recurrent neural networks

junhua mao1,2

wei xu1
1baidu research

yi yang1
2university of california, los angeles

jiang wang1

alan l. yuille2

mjhustc@ucla.edu, {wei.xu,yangyi05,wangjiang03}@baidu.com, yuille@stat.ucla.edu

4
1
0
2

 
t
c
o
4

 

 
 
]

v
c
.
s
c
[
 
 

1
v
0
9
0
1

.

0
1
4
1
:
v
i
x
r
a

abstract

in this paper, we present a multimodal recurrent neural network (m-id56) model
for generating novel sentence descriptions to explain the content of images. it
directly models the id203 distribution of generating a word given previous
words and the image. image descriptions are generated by sampling from this
distribution. the model consists of two sub-networks: a deep recurrent neural
network for sentences and a deep convolutional network for images. these two
sub-networks interact with each other in a multimodal layer to form the whole
m-id56 model. the effectiveness of our model is validated on three benchmark
datasets (iapr tc-12 [8], flickr 8k [28], and flickr 30k [13]). our model out-
performs the state-of-the-art generative method. in addition, the m-id56 model
can be applied to retrieval tasks for retrieving images or sentences, and achieves
signi   cant performance improvement over the state-of-the-art methods which di-
rectly optimize the ranking objective function for retrieval.

1

introduction

obtaining sentence level descriptions for images is becoming an important task and has many ap-
plications, such as early childhood education, id162, and navigation for the blind. thanks
to the rapid development of id161 and natural language processing technologies, recent
works have made signi   cant progress for this task (see a brief review in section 2). many of these
works treat it as a retrieval task. they extract features for both sentences and images, and map them
to the same semantic embedding space. these methods address the tasks of retrieving the sentences
given the query image or retrieving the images given the query sentences. but they can only label
the query image with the sentence annotations of the images already existing in the datasets, thus
lack the ability to describe new images that contain previously unseen combinations of objects and
scenes.
in this work, we propose a multimodal recurrent neural networks (m-id56) model to address both
the task of generating novel sentences descriptions for images, and the task of image and sentence
retrieval. the whole m-id56 architecture contains a language model part, an image part and a
multimodal part. the language model part learns the dense feature embedding for each word in the
dictionary and stores the semantic temporal context in recurrent layers. the image part contains a
deep convulutional neural network (id98) [17] which extracts image features. the multimodal
part connects the language model and the deep id98 together by a one-layer representation. our
m-id56 model is learned using a perplexity based cost function (see details in section 4). the
errors are backpropagated to the three parts of the m-id56 model to update the model parameters
simultaneously. to the best of our knowledge, this is the    rst work that incorporates the recurrent
neural network in a deep multimodal architecture.
in the experiments, we validate our model on three benchmark datasets: iapr tc-12 [8], flickr 8k
[28], and flickr 30k [13]. we show that our method signi   cantly outperforms the state-of-the-art
methods in both the task of generating sentences and the task of image and sentence retrieval when
using the same image feature extraction networks. our model is extendable and has the potential to
be further improved by incorporating more powerful deep networks for the image and the sentence.

1

figure 1: examples of the generated and two top-ranked retrieved sentences given the query image
from iapr tc-12 dataset. the sentences can well describe the content of the images. we show a
failure case in the fourth image, where the model mistakenly treats the lake as the sky.

2 related work

deep model for id161 and natural language. the deep neural network structure devel-
ops rapidly in recent years in both the    eld of id161 and natural language. for computer
vision, krizhevsky et. al [17] proposed a deep convolutional neural networks with 8 layers (denoted
as alexnet) for image classi   cation tasks and outperformed previous methods by a large margin.
recently, girshick et. al [7] proposed a id164 framework based on alexnet. for natural
language, the recurrent neural network shows the state-of-the-art performance in many tasks, such
as id103 and id27 learning [22, 23, 24].
image-sentence retrieval. many works treat the task of describe images as a retrieval task and
formulate the problem as a ranking or embedding learning problem [12, 6, 30]. they will    rst
extract the word and sentence features (e.g. socher et.al [30] uses dependency tree recursive neural
network to extract sentence features) as well as the image features. then they optimize a ranking
cost to learn an embedding model that maps both the language feature and the image feature to a
common semantic feature space. in this way, they can directly calculate the distance between images
and sentences. most recently, karpathy et.al [15] showed that object level image features based on
id164 results will generate better results than image features extracted at the global level.
generating novel sentence descriptions for images. there are generally two categories of meth-
ods for this task. the    rst category assumes a speci   c rule of the language grammar. they parse
the sentence and divide it into several parts [25, 10]. then each part is associated to a object or
an attribute in the image (e.g. [18] uses a conditional random field model and [5] uses a markov
random field model). this kind of method generates sentences that are syntactically correct. an-
other category of methods, which is more related to our method, learns a id203 density over
the space of multimodal inputs (i.e. sentences and images), using for example, deep boltzmann
machines [31], and topic models [1, 14]. they can generate sentences with richer and more    exible
structure than the    rst group. the id203 of generating sentences given the corresponding image
can serves as the af   nity metric for retrieval. our method falls into this category. more close related
to our tasks and method is the work of kiros et al. [16], which is built on a log-bilinear model
[26]. it needs a    xed length of context (i.e.    ve words), whereas in our model, the temporal context
is stored in a recurrent architecture, which allows arbitrary context length.

3 model architecture

3.1 simple recurrent neural network

we brie   y introduce the simple recurrent neural network (id56) or elman network [4] that is
widely used for many natural language processing tasks, such as id103 [22, 23]. its
architecture is shown in figure 2(a). it has three types of layers in each time frame: the input word

2

retr.gen.1. tourists are sitting at a long table with beer bottles on it in a rather dark restaurant and are raising their bierglaeser; 2. tourists are sitting at a long table with a white table-cloth in a somewhat dark restaurant;tourists are sitting at a long table with a white table cloth and are eating;1. top view of the lights of a city at night, with a well-illuminated square in front of a church in the foreground;2. people on the stairs in front of an illuminated cathedral with two towers at night;a square with burning street lamps and a street in the foreground;1. a dry landscape with light brown grass and green shrubs and trees in the foreground and large reddish-brown rocks and a blue sky in the background;2. a few bushes at the bottom and a clear sky in the background; a dry landscape with green trees and bushes and light brown grass in the foreground and reddish-brown round rock domes and a blue sky in the background;1. group picture of nine tourists and one local on a grey rock with a lake in the background;2. five people are standing and four are squatting on a brown rock in the foreground;a blue sky in the background;figure 2: illustration of the simple recurrent neural network (id56) and our multimodal recurrent
neural network (m-id56) architecture. (a). the simple id56. (b). our m-id56 model. the input
of our model is an image and its corresponding sentences (e.g. the sentence for the shown image is:
a man at a giant tree in the jungle). the model will estimate the id203 distribution of the next
word given previous words and the image. this architecture is much deeper than the simple id56.
(c). the illustration the unfolded m-id56. the model parameters are shared for each temporal frame
of the m-id56 model.

layer w, the recurrent layer r and the output layer y. the activation of input, recurrent and output
layers at time t is denoted as w(t), r(t), and y(t) respectively. w(t) is the one-hot representation of
the current word. this representation is binary, and has the same dimension of the vocabulary size
with only one non-zero element. y(t) can be calculated as follows:

x(t) = [w(t) r(t     1)]; r(t) = f1(u    x(t)); y(t) = g1(v    r(t));

(1)
where x(t) as a vector that concatenates w(t) and r(t     1), f1(.) and g1(.) are element-wised
sigmoid and softmax function respectively, and u, v are weights which will be learned.
the size of id56 is adaptive to the length of the input sequence and the recurrent layers connect the
sub-networks in different time frames. accordingly, when we do the id26, we need to
propagate the error through recurrent connections back in time [29].

3.2 our m-id56 model

the structure of our multimodal recurrent neural network (m-id56) is shown in figure 2(b). the
m-id56 model is much deeper than the simple id56 model. it has six layers in each time frame:
the input word layer, two id27 layers, the recurrent layer, the multimodal layer, and the
softmax layer).
the two id27 layers embed the one-hot input into a dense word representation. it has
several advantages. firstly, it will signi   cantly lower the number of parameters in the networks
because the dense word vector (128 dimension) is much smaller than the one-hot word vector. sec-
ondly, the dense id27 encodes the semantic meanings of the words [21]. the seman-
tically relevant words can be found by calculating the euclidean distance between two dense word
vectors in embedding layers.
most of the sentence-image multimodal models [15, 6, 30, 16] use pre-computed id27
vectors as the initialization of their model. in contrast, we randomly initialize our id27
layers and learn them from the training data. we show that this random initialization is suf   cient for
our architecture to generate the state-of-the-art results. we treat the activation of the id27
layer 2 (see figure 2(b)) as the    nal word representation, which directly inputs in the multimodal
layer.

3

.........table projection128256embedding 1256recurrent...512...softmaxmsample/maxinput wordnext wordfully connectedimagedeep id98image feature......deep image feature extractionfully connected(c)(b)(a)w(t)r(t-1)r(t)y(t)w(t-1)...y(t-1)multimodalembedding 2m-id56 model for one time frame##start##ajungleinput wordrecurrentmultimodal   aman##end##next word                           input word layer wrecurrentlayer routputlayer yunfoldr(t) = f2(ur    r(t     1) + w(t));

after the two id27 layers, we have a recurrent layer with 256 dimensions. the calcula-
tion of the recurrent layer is slightly different from the calculation for the simple id56. instead of
concatenating the word representation at time t (denoted as w(t)) and the recurrent layer activation
at time t    1 (denoted as r(t    1)), we    rst map r(t    1) into the same vector space as w(t) and add
them together:

(2)
we set f2(.) as the recti   ed linear unit (relu), inspired by its the recent success when training
very deep structure in id161    eld [17]. this differs from the simple id56 where the
sigmoid function is adopted (see section 3.1). relu is faster, and harder to saturate or over   t the
data than non-linear functions like the sigmoid. when id26 through time (bptt) [29]
is conducted for id56 with sigmoid function, the vanishing gradient problem appears since even the
simplest id56 model can have a large temporal depth. previous methods [22, 23] used heuristics,
such as truncated bptt, to avoid this problem. truncated bptt stops the bptt after k time steps,
where k is a hand-de   ned hyperparameter. because of the good properties of relu, we do not need
to stop the bptt at an early stage, which leads to a better and more ef   cient utilization of the data
than truncated bptt.
after the recurrent layer, we set up a 512 dimensional multimodal layer that connect the language
model part and the image part of the m-id56 model (see figure 2(b)). the language model part
includes the id27 layer 2 (the    nal word representation) and the recurrent layer (the
sentence context). the image part contains the image feature extraction network. here we connect
the seventh layer of alexnet [17] to the multimodal layer (please refer to section 5 for more details).
but our framework can use any image features. we map the feature vector for each layer to the same
feature space and add them together to obtain the feature vector for the multimodal layer:

(3)
where m denotes the multimodal layer feature vector, i denotes the image feature, g2(.) is the
element-wised scaled hyperbolic tangent function [19]:

m(t) = g2(vw    w(t) + vr    r(t) + vi    i);

g2(x) = 1.7159    tanh(

(4)
this function forces the gradients into the most non-linear value range and accelerates the training
process than the basic hyperbolic tangent function.
as the simple id56, our m-id56 model has a softmax layer that will generate the id203 distri-
bution of the next word. the dimension of this layer is the vocabulary size m, which is different for
different datasets.

x)

2
3

4 training the m-id56

l(cid:88)

for training our m-id56 model we adopt a cost function based on the perplexity of the sentences in
the training set given their corresponding images. perplexity is a standard measure for evaluating
language model. the perplexity for one word sequence (i.e. a sentences) w1:l is calculated as
follows:

log2 p (wn|w1:n   1, i)

log2 ppl(w1:l|i) =     1
l

(5)
where l is the length of the word sequences, ppl(w1:l|i) denotes the perplexity of the sentence
w1:l given the image i. p (wn|w1:n   1, i) is the id203 of generating the word wn given i and
previous words w1:n   1. it corresponds to the feature vector of the softmax layer of our model.
the cost function of our model is the average log-likelihood of the words given their context words
and corresponding images in the training sentences plus a id173 term. it can be calculated
by the perplexity:

n=1

l    log2 ppl(w(i)

1:l|i(i)) + (cid:107)  (cid:107)2

2

(6)

n(cid:88)

i=1

c =

1
n

where n is the number of words in the training set and    is the model parameters.
our training objective is to minimize this cost function, which is equivalent to maximize the prob-
ability of the model to generate the sentences in the training set given their corresponding images.
the cost function is differentiable and we use id26 to learn the model parameters.

4

5 learning of sentence and image features

the architecture of our model allows the gradients from the id168 to be backpropagated to
both the id38 part (i.e. the id27 layers and the recurrent layer) as well as
the image part (e.g. the alexnet [17]).
for the id38 part, as mentioned above, we randomly initialize the id38
layers and learn their parameters. for the image part, we connect the seventh layer of a pre-trained
convolutional neural network [17, 3] (denoted as alexnet). the same features extracted from
the seventh layer of alexnet (also denoted as decaf features [3]) are widely used by previous mul-
timodal methods [16, 6, 15, 30]. a recent multimodal retrieval work [15] showed that using the
rid98 id164 framework [7] combined with the decaf features signi   cantly improves the
performance. in the experiments, we show that our method performs much better than [15] when
the same image features are used, and is better than or comparable to their results even when they
use more sophisticated features based on id164.
we can update the alexnet according to the gradient backpropagated from the multimodal layer.
in this paper, we    x the image features and the deep id98 network in the training stage due to a
shortage of data (the datasets we used in the experiment have less than 30k images). in future
work, we will apply our method on large datasets and    netune the parameters of the deep id98
network in the training stage.

6 sentence generation, image and sentence retrieval

we can use the trained m-id56 model for three tasks: 1) sentences generation; 2) sentence retrieval
(retrieving most relevant sentences to the given image); 3) id162 (retrieving most relevant
images to the given sentence);
the sentence generation process is straightforward. start from the start sign    ##start##    or arbi-
trary number of reference words (e.g. we can input the    rst k words in the reference sentence to the
model and then start to generate new words), our model can calculate the id203 distribution
of the next word: p (w|w1:n   1, i). then we can sample from this id203 distribution to pick
the next word. in practice, we    nd that selecting the word with the maximum id203 performs
slightly better than sampling. after that, we input the picked word to the model and continue the
process until the model outputs the end sign    ##end##   .
for the retrieval tasks, we use our model to calculate the perplexity of generating a sentence given
an image. the perplexity can be treated as an af   nity measurement between sentences and images.
for the id162 task, we rank the images based on their perplexity with the query sentence
and output the top ranked ones.
the sentence retrieval task is trickier because there might be some sentences that have high prob-
ability for any image query (e.g.
in-
stead of looking at the perplexity or the id203 of generating the sentences given the query
image, we use the normalized id203 for each sentence: p (w1:l|i)/p (w1:l). p (w1:l) =
are images sampled from the training set. we approximate p (i
)
by a constant and ignore this term. p (w1:l|i) = ppl(w1:l|i)   l.

sentences consists of many frequently appeared words).

(cid:48) p (w1:l|i

), where i

)    p (i

(cid:80)

i

(cid:48)

(cid:48)

(cid:48)

(cid:48)

7 experiments

7.1 datasets

we test our method on three benchmark datasets with sentence level annotations: iapr tc-12 [8],
flickr 8k [28], and flickr 30k [13].
iapr tc-12 benchmark. this dataset consists of around 20,000 images taken from locations
around the world. this includes images of different sports and actions, people, animals, cities,
landscapes, and so on. for each image, they provide at least one sentence annotation. on average,
there are about 1.7 sentences annotations for one image. we adopt the publicly available separation

5

of training and testing set as previous works [9, 16]. there are 17,665 images for training and 1962
images for testing.
flickr8k benchmark. this dataset consists of 8,000 images extracted from flickr. for each image,
it provides    ve sentences annotations. the grammar of the annotations for this dataset is simpler
than that for the iapr tc-12 dataset. we adopt the standard separation of training, validation and
testing set which is provided by the dataset. there are 6,000 images for training, 1,000 images for
validation and 1,000 images for testing.
flickr30k benchmark. this dataset is a recent extension of flickr8k. for each image, it also pro-
vides    ve sentences annotations. it consists of 158,915 crowd-sourced captions describing 31,783
images. the grammar and style for the annotations of this dataset is similar to flickr8k. we follow
the previous work [15] which used 1,000 images for testing. this dataset, as well as the flick8k
dataset, are commonly used for the image-sentence retrieval tasks.

7.2 id74

sentence generation. following previous works, we adopt sentence perplexity and id7 scores
(i.e. b-1, b-2, and b-3) [27, 20] as the id74. id7 scores were originally designed
for automatic machine translation where they rate the quality of a translated sentences given several
references sentences. we can treat the sentence generation task as the    translation    of the content of
images to sentences. id7 remains the standard evaluation metric for sentence generation methods
for images, though it has drawbacks. for some images, the reference sentences might not contains all
the possible descriptions in the image and id7 might penalize some correctly generated sentences.
to conduct a fair comparison, we adopt the same sentence generation steps and experiment settings
as [16], and generate as many words as there are in the reference sentences to calculate id7. note
that our model does not need to know the length of the reference sentence because we add a end sign
   ##end##    at the end of every training sentences and we can stop the generation process when our
model outputs the end sign.
sentence retrieval and id162 for flickr8k and flickr30k datasets, we adopted the
same id74 as previous works [30, 6, 15] for both the tasks of sentences retrieval and
id162. they used r@k (k = 1, 5, 10) as the measurements, which are the recall rates of
the    rst retrieved groundtruth sentences (sentence retrieval task) or images (id162 task).
higher r@k usually mean better retrieval performance. since we care most about the top-ranked
retrieved results, the r@k with small k are more important. the med r is another score we used,
which is the median rank of the    rst retrieved groundtruth sentences or images. lower med r usually
means better performance.
for iapr tc-12 datasets, we adopt exactly the same id74 as [16] and plot the mean
number of matches of the retrieved groundtruth sentences or images with respect to the percentage
of the retrieved sentences or images for the testing set. for sentences retrieval task, [16] used a
shortlist of 100 images which are the nearest neighbors of the query image in the feature space.
this shortlist strategy makes the task harder because similar images might have similar descriptions
and it is often harder to    nd subtle differences among the sentences and pick the most suitable one.
although there are no published r@k scores and med r score for this dataset available for the best
of our knowledge, we also report these scores of our method for future comparison.

7.3 results on iapr tc-12

the results of the sentence generation task are shown in table 1. back-off gt2 and gt3 are
id165s methods with katz backoff and good-turing discounting [2, 16]. ours-id56-base serves
as a baseline method for our m-id56 model. it has the same architecture with m-id56 except that
we will not input the image features to the network.
to conduct a fair comparison, we followed the same experimental settings of [16], include the con-
text length to calculate the id7 scores and perplexity. these two id74 are not nec-
essarily correlated to each other for the following reasons. as mentioned in section 4, perplexity is
calculated according to the id155 of the word in a sentence given all of its previous
reference words. therefore, a strong language model that successfully captures the distributions of
words in sentences can have a low perplexity without the image content. but the content of the gen-

6

back-off gt2
back-off gt3
lbl [26]
mlbl-b-decaf [16]
mlbl-f-decaf [16]
gupta et al. [11]
gupta & mannem [10]
ours-id56-base
ours-m-id56

ppl
54.5
55.6
20.1
24.7
21.8

/
/

7.77
6.92

b-1
0.323
0.312
0.327
0.373
0.361
0.15
0.33
0.3134
0.3951

b-2
0.145
0.131
0.144
0.187
0.176
0.06
0.18
0.1168
0.1828

b-3
0.059
0.059
0.068
0.098
0.092
0.01
0.07
0.0803
0.1311

table 1: results of the sentence generation task on the iapr tc-12 dataset.    b    is short for blue

erated sentences might be unrelated to images. from table 1, we can see that although our baseline
method of id56 generates a very low perplexity, its id7 score is not very high, indicating that it
failed to generate sentences with high quality.
we show that our m-id56 model performs much better than our baseline id56 model in terms
of both perplexity and id7 score. it also outperforms the state-of-the-art methods in terms of
perplexity, b-1, b-3, and a comparable result for b-2 1.
for retrieval tasks, as mentioned in section 7.2, we draw a recall accuracy curve with respect to
the percentage of retrieved images (sentence retrieval task) or sentences (sentence retrieval task)
in figure 3. for sentence retrieval task, we used a shortlist of 100 images as the three comparing
methods shown in [16]. the    rst method, bowdecaf, is a strong image based bag-of-words baseline.
the second and the third models are all multimodal deep models. our m-id56 model signi   cantly
outperforms these three methods in this task.
since there are no publicly available results of r@k and median rank in this dataset, we report
r@k scores of our method in table 2 for future comparisons. the result shows that 20.9% top-
ranked retrieved images and 13.2% top-ranked retrieved sentences are groundtruth.

(a) image to text curve

(b) text to image curve

figure 3: retrieval recall curve for (a). sentence retrieval task (b). id162 task on iapr
tc-12 dataset. the behavior on the far left (i.e. top few retrievals) is most important.

sentence retrival (image to text)
r@1 r@5 r@10 med r

ours-m-id56 20.9

43.8

54.4

8

image retrival (text to image)
r@1 r@5 r@10 med r
13.2

31.2

40.8

21

table 2: r@k and median rank (med r) for iaprtc-12 dataset.

1[16] further improve their results after the publication. the perplexity of mlbl-f and lbl now are 9.90

and 9.29 respectively.

7

0.010.020.050.1 0.250.5 1   00.10.20.30.40.50.60.70.80.91  ours   mid56bow   decafmlbl   f   decafmlbl   b   decaf0.00050.001 0.002 0.005 0.01  0.02  0.05  0.1   0.25  0.5   1     00.10.20.30.40.50.60.70.80.91  ours   mid56bow   decafmlbl   f   decafmlbl   b   decaf7.4 results on    ickr8k

this dataset was widely used as a benchmark dataset of image and sentence retrieval. the r@k
and med r of different methods are shown in table 3. our model outperforms the state-of-the-
art methods (i.e socher-decaf, devise-decaf, deepfe-decaf) by a large margin when using the
same image features (i.e. decaf features). we also list the performance of methods using more
sophisticated features in table 3.    -avg-rid98    denotes methods with features of the average id98
activation of all objects above a detection con   dence threshold. deepfe-rid98 [15] uses a fragment
mapping strategy to better exploit the id164 results. the results show that using these
features will improve the performance. even without the help from the id164 methods,
however, our method performs better than these methods in most of the id74. we will
develop our framework using better image features in the future work.

random
socher-decaf [30]
socher-avg-rid98 [30]
devise-avg-rid98 [6]
deepfe-decaf [15]
deepfe-rid98 [15]
ours-m-id56-decaf

sentence retrival (image to text)
r@1 r@5 r@10 med r
631
0.1
32
4.5
23
6.0
4.8
28
34
5.9
14
12.6
14.5
11

1.0
28.6
34.0
27.3
27.3
44.0
48.5

0.5
18.0
22.7
16.5
19.2
32.9
37.2

image retrival (text to image)
r@1 r@5 r@10 med r
500
0.1
29
6.1
25
6.6
5.9
29
32
5.2
15
9.7
11.5
15

0.5
18.5
21.6
20.1
17.6
29.6
31.0

1.0
29.0
31.7
29.6
26.5
42.5
42.4

table 3: results of r@k and median rank (med r) for flickr8k dataset. note deepfe-rid98 uses
more sophisticated image features than we do

we report the results of generated sentences in table 5. there is no publicly available algorithm
that reported results on this dataset. so we compared our m-id56 model with the ours-id56-base
model. the m-id56 model performs much better than this baseline both in terms of the perplexity
and id7 scores.

7.5 results on    ickr30k

this dataset is a new dataset and there are only a few methods report their retrieval results on it so
far. we    rst show the r@k evaluation metric in table 4. our method outperforms the state-of-the-
art methods in most of the id74. the results of the sentence generation task with a
comparison of our id56 baseline are shown in table 5.

random
devise-avg-rid98 [6]
deepfe-rid98 [15]
ours-m-id56-decaf

sentence retrival (image to text)
r@1 r@5 r@10 med r
631
0.1
4.8
28
8
16.4
18.4
10

1.1
27.3
54.7
50.9

0.6
16.5
40.2
40.2

image retrival (text to image)
r@1 r@5 r@10 med r
500
0.1
5.9
29
13
10.3
12.6
16

0.5
20.1
31.4
31.2

1.0
29.6
44.5
41.5

table 4: results of r@k and median rank (med r) for flickr30k dataset.

ours-id56-base
ours-m-id56

ppl
30.39
24.39

flickr 8k
b-1
0.4383
0.5778

b-2
0.1849
0.2751

b-3 ppl
43.96
35.11

0.1339
0.2307

flickr 30k
b-1
0.4699
0.5479

b-2
0.1964
0.2392

b-3
0.1252
0.1952

table 5: results of generated sentences in the flickr8k and flickr 30k dataset.

8 conclusion

we propose a multimodal recurrent neural network (m-id56) framework that performs at the
state-of-the-art in three tasks: sentence generation, sentence retrieval given query image and image

8

retrieval given query sentence. our m-id56 can be extended to use more complex image features
(e.g. id164 features) and more sophisticated language models.

references
[1] k. barnard, p. duygulu, d. forsyth, n. de freitas, d. m. blei, and m. i. jordan. matching words and

pictures. jmlr, 3:1107   1135, 2003.

[2] s. f. chen and r. rosenfeld. a survey of smoothing techniques for me models. tsap, 8(1):37   50, 2000.
[3] j. donahue, y. jia, o. vinyals, j. hoffman, n. zhang, e. tzeng, and t. darrell. decaf: a deep convolu-

tional activation feature for generic visual recognition. arxiv preprint arxiv:1310.1531, 2013.

[4] j. l. elman. finding structure in time. cognitive science, 14(2):179   211, 1990.
[5] a. farhadi, m. hejrati, m. a. sadeghi, p. young, c. rashtchian, j. hockenmaier, and d. forsyth. every

picture tells a story: generating sentences from images. in eccv, pages 15   29. 2010.

[6] a. frome, g. s. corrado, j. shlens, s. bengio, j. dean, t. mikolov, et al. devise: a deep visual-semantic

embedding model. in nips, pages 2121   2129, 2013.

[7] r. girshick, j. donahue, t. darrell, and j. malik. rich feature hierarchies for accurate id164

and semantic segmentation. in cvpr, 2014.

[8] m. grubinger, p. clough, h. m  uller, and t. deselaers. the iapr tc-12 benchmark: a new evaluation

resource for visual information systems. in international workshop ontoimage, pages 13   23, 2006.

[9] m. guillaumin, j. verbeek, and c. schmid. multiple instance metric learning from automatically labeled

bags of faces. in eccv, pages 634   647, 2010.

[10] a. gupta and p. mannem. from image annotation to image description. in iconip, 2012.
[11] a. gupta, y. verma, and c. jawahar. choosing linguistics over vision to describe images. in aaai, 2012.
[12] m. hodosh, p. young, and j. hockenmaier. framing image description as a ranking task: data, models

and id74. jair, 47:853   899, 2013.

[13] p. y. a. l. m. hodosh and j. hockenmaier. from image descriptions to visual denotations: new similarity

metrics for semantic id136 over event descriptions.

[14] y. jia, m. salzmann, and t. darrell. learning cross-modality similarity for multinomial data. in iccv,

pages 2407   2414, 2011.

mapping. in arxiv:1406.5679, 2014.

[15] a. karpathy, a. joulin, and l. fei-fei. deep fragment embeddings for bidirectional image sentence

[16] r. kiros, r. zemel, and r. salakhutdinov. multimodal neural language models. in icml, 2014.
[17] a. krizhevsky, i. sutskever, and g. e. hinton. id163 classi   cation with deep convolutional neural

networks. in nips, pages 1097   1105, 2012.

[18] g. kulkarni, v. premraj, s. dhar, s. li, y. choi, a. c. berg, and t. l. berg. baby talk: understanding

and generating image descriptions. in cvpr, 2011.

[19] y. a. lecun, l. bottou, g. b. orr, and k.-r. m  uller. ef   cient backprop. in neural networks: tricks of

the trade, pages 9   48. springer, 2012.

[20] c.-y. lin and f. j. och. automatic evaluation of machine translation quality using longest common

subsequence and skip-bigram statistics. in acl, page 605, 2004.

[21] t. mikolov, k. chen, g. corrado, and j. dean. ef   cient estimation of word representations in vector

space. arxiv preprint arxiv:1301.3781, 2013.

[22] t. mikolov, m. kara     at, l. burget, j. cernock`y, and s. khudanpur. recurrent neural network based

language model. in interspeech, pages 1045   1048, 2010.

[23] t. mikolov, s. kombrink, l. burget, j. cernocky, and s. khudanpur. extensions of recurrent neural

network language model. in icassp, pages 5528   5531, 2011.

[24] t. mikolov, i. sutskever, k. chen, g. s. corrado, and j. dean. distributed representations of words and

phrases and their compositionality. in nips, pages 3111   3119, 2013.

[25] m. mitchell, x. han, j. dodge, a. mensch, a. goyal, a. berg, k. yamaguchi, t. berg, k. stratos, and
h. daum  e iii. midge: generating image descriptions from id161 detections. in eacl, 2012.
[26] a. mnih and g. hinton. three new id114 for statistical language modelling. in icml, pages

[27] k. papineni, s. roukos, t. ward, and w.-j. zhu. id7: a method for automatic evaluation of machine

641   648. acm, 2007.

translation. in acl, pages 311   318, 2002.

[28] c. rashtchian, p. young, m. hodosh, and j. hockenmaier. collecting image annotations using amazon   s

mechanical turk. in naacl-hlt workshop 2010, pages 139   147, 2010.

[29] d. e. rumelhart, g. e. hinton, and r. j. williams. learning representations by back-propagating errors.

[30] r. socher, q. le, c. manning, and a. ng. grounded id152 for    nding and describing

[31] n. srivastava and r. salakhutdinov. multimodal learning with deep id82s. in nips, pages

cognitive modeling, 1988.

images with sentences. in tacl, 2014.

2222   2230, 2012.

9

