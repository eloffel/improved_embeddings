5
1
0
2

 
r
p
a
6
1

 

 
 
]

g
l
.
s
c
[
 
 

3
v
1
8
8
6

.

2
1
4
1
:
v
i
x
r
a

accepted as a workshop contribution at iclr 2015

on learning vector representations
in hierarchical label spaces

jinseok nam1,2, johannes f  urnkranz1
1 knowledge engineering group
department of computer science
technische universit  at darmstadt
2 knowledge discovery in scienti   c literature
german institute for educational research
{nam@cs, juffi@ke}.tu-darmstadt.de

abstract

an important problem in multi-label classi   cation is to capture label patterns or
underlying structures that have an impact on such patterns. this paper addresses
one such problem, namely how to exploit hierarchical structures over labels. we
present a novel method to learn vector representations of a label space given a
hierarchy of labels and label co-occurrence patterns. our experimental results
demonstrate qualitatively that the proposed method is able to learn regularities
among labels by exploiting a label hierarchy as well as label co-occurrences. it
highlights the importance of the hierarchical information in order to obtain reg-
ularities which facilitate analogical reasoning over a label space. we also ex-
perimentally illustrate the dependency of the learned representations on the label
hierarchy.

1

introduction

multi-label classi   cation is an area of machine learning which aims to learn a function that maps
instances to label spaces. in contrast to multi-class classi   cation, each instance is assumed to be
associated with more than one label. one of the goals in multi-label classi   cation is to model
underlying structures over a label space because in many such problems, the occurrence of labels is
not independent of each other. many attempts have been made to capture and exploit label structures
(f  urnkranz et al., 2008; hsu et al., 2009; read et al., 2011; dembczy  nski et al., 2012). as the number
of possible con   gurations of labels grows exponentially with respect to the number of labels, it is
required for multi-label classi   er to handle many labels ef   ciently (bi & kwok, 2013). not only
multi-label classi   ers but also human annotators need a way to handle a large number of labels
ef   ciently. thus, human experts put a lot of effort into maintaining and updating hierarchies of
labels and such hierarchies are used to generate the ground truth for training classi   ers, and many
methods have been developed to use hierarchical output structures in machine learning (silla jr. &
freitas, 2011). in particular, several researchers have looked into utilizing the hierarchical structure
of the label space for improved predictions in multi-label classi   cation (rousu et al., 2006; vens
et al., 2008; zimek et al., 2010; bi & kwok, 2011).
in this work, we present a novel method to ef   ciently learn from hierarchical structures over labels
as well as occurrences of labels, and investigate the importance of hierarchical structures to identify
internal structures of labels.
the rest of the paper is organized as follows. in section 2, we de   ne multi-label classi   cation and
hierarchical structures of labels in a graph. we then introduce our proposed method that learns
label spaces while taking into account label hierarchies. in section 4, we set up our experiments,
and present empirical analysis in section 5. finally, we discuss    ndings from our experiments and
provide directions for future work in section 6.

1

accepted as a workshop contribution at iclr 2015

2 multi-label classification with label hierarchies

throughout this work we will present a method to learn representations of labels in multi-label
classi   cation. firstly, in this section we de   ne multi-label classi   cation and notation for label hier-
archies.
notation. multi-label classi   cation refers to the task of learning a function that maps instances x to
label sets y     {1, 2,       , l} given a training dataset d = {(xn,yn)}n
n=1, where n is the number
of instances and l is the number of labels. consider multi-label problems where label hierarchies
exist. label graphs are a natural way to represent such hierarchical structures. because it is possible
for a label to have more than one parent node, we represent a hierarchy of labels in a directed
acyclic graph (dag). consider a graph g = {v, e} where v denotes a node and e represent a
connection between two nodes. each node corresponds a label and an edge represents a parent-child
relationship between the two labels in the hierarchy. a node u     v corresponds to a label. if there
exists a directed path from u to v, u is an ancestor of v and v is a descendant of u. the set of all
ancestors of v is denoted as sa(v), and the set of all descendants is denoted as sd(u).
for large-scale multi-label classi   cation problems, high dimensionality of label spaces makes learn-
ing classi   ers dif   cult. in the literature, several work (hsu et al., 2009; tai & lin, 2012; chen &
lin, 2012; yu et al., 2014) have been proposed to tackle the problem by reducing the dimensionality
of label spaces in which label co-occurrence patterns play a crucial role. in id38,
another area of research where high dimensionality issues arise more severely, one can deal with
the problem by representing words as dense vectors in rd and then to learn these representations
in a way of maximizing a id155 of a word given the context of the word (bengio
et al., 2003). recently, for learning word representations, mikolov et al. (2013) have proposed a
very ef   cient log-linear model relying on co-occurrence patterns of words given the    xed-length
contexts and have shown interesting properties of word representations by analogical reasoning. in
the following section, we introduce a novel approach to reduce the dimensionality of label spaces
by adapting the id148 capable of handling the large-scale problems ef   ciently.

3 the log-linear model

as stated in section 1, there are several work making use of hierarchical structures in label spaces
for improving predictive performance of multi-label classi   ers. this is because label hierarchies are
often built and maintained by human experts, and can be used as an additional source of information
to identify internal structures. thus, we propose a log-linear model to reduce the dimensionality of
label spaces by exploiting label co-occurrences and hierarchical relations between labels.
formally, the basic idea is to maximize a id203 of predicting an ancestor given a particular
label in a hierarchy as well as predicting co-occurring labels with it. given the labels of n train-
ing instances dy = {y1,y2,       ,yn}, the objective function l is to maximize the average log
id203 given by

             1

za

n(cid:88)

n=1

(cid:88)

(cid:88)

i   yn

j   sa(i)

(cid:88)

(cid:88)

i   yn

k   yn
k(cid:54)=i

            

l (  ;dy ) =

log p (yj|yi) +

1
zn

log p (yk|yi)

(1)

where za = |yn||sa(  )| and zn = |yn|(|yn|     1), and    is a set of parameters (to be introduced
shortly). the id203 of predicting an ancestor label j of a label i, i.e., p(yj|yi) in equation 1,
can be computed by softmax as follows

p(yj|yi) =

(cid:80)

exp(u(cid:48)t
j ui)
l   l exp(u(cid:48)t

l ui)

(2)

j is a j-th column vector of u(cid:48)     rd  l and ui is a i-th column vector of u     rd  l.
where u(cid:48)
here, u and u(cid:48) correspond to continuous vector representations for input labels and output labels,
respectively, and they are used as parameters of equation 1, that is,    = {u, u(cid:48)}. this enables
for labels that share the same ancestors or co-occurring labels to have similar representations. the
softmax function in equation 2 can be viewed as an objective function of a neural network consisting

2

accepted as a workshop contribution at iclr 2015

of a linear activation function in the hidden layer and two weights {u, u(cid:48)}, where u connects the
input layer to the hidden layer while u(cid:48) is used to convey the hidden activations to the output layer.
thus, the log of a id203 of predicting a label j given a label i in eq 2, i.e., ej|i (cid:44) log p(yj|yi),
can be obtained in a neural network framework:

hi = uyi
oj|i = u(cid:48)t  jhi
ej|i = oj|i     log

(cid:88)

k

exp(ok|i)

(3)
(4)

(5)

(t+1) = u(cid:48)

(t) +      u(cid:48)

where yi is a l dimensional vector whose i-th element is set to one and zero elsewhere and hi is
hidden activation of a label i. note that since only i-th element of yi is one, hi is equal to ui in eq
2. the id203 of predicting a label k, which is assigned together with a label i to a label set yn,
p(yk|yi) is computed in the same way meaning that it is parameterized by u and u(cid:48) as well. the
parameters are updated as follows u(t+1) = u(t) +      u(t)l, u(cid:48)
(t)l where
   denotes a learning rate.
due to computational cost for    u(cid:48)
(t)l, we use hierarchical softmax (morin & bengio, 2005; mnih
& hinton, 2009) which reduces the gradient computing cost from o(l) to o(log l). similar to
mikolov et al. (2013), in order to make use of the hierarchical softmax, a binary tree is constructed
by huffman coding giving binary codes with variable length to each label according to |sd(  )|.
in other words, the more descendants a label has in a hierarchy, the shorter codes it is assigned.
note that by de   nition of the huffman coding all labels correspond to leaf nodes in the binary tree,
namely huffman tree, and there are l     1 internal nodes in the huffman tree. instead of computing
l outputs, the hierarchical softmax computes a id203 of (cid:100)log l(cid:101) binary decisions over a path
from the root node of the tree to the leaf node corresponding to a target label, say, yj in equation 2.
more speci   cally, let c(y) be a codeword of a label y by the huffman coding, where each bit can be
either 0 or 1, and i(c(y)) be the number of bits in the codeword for that label. cl(y) is the l-th bit
in y   s codeword. unlike the softmax, in computing the hierarchical softmax we use the output label
representations u(cid:48) as vector representations for inner nodes in the huffman tree. the hierarchical
softmax is given by

i(c(yj ))(cid:89)

p(yj|yi) =

  ((cid:74)cl(yj) = 0(cid:75) u(cid:48)t

n(l,yj )ui)

l=1

where   (  ) is the logistic function,(cid:74)  (cid:75) denotes a function taking 1 if its argument is true, otherwise

-1, and u(cid:48)
n(l,yj ) is a vector representation for the l-th node in the path from the root node to the
node corresponding to the label yj in the huffman tree. while l inner products are required to
compute the id172 term in equation 2, the hierarchical softmax needs i(c(  )) times com-
putations. hence, the hierarchical softmax allows substantial improvements in computing gradients
if e [i(c(  ))] (cid:28) l.

(6)

4 experimental setup

we carried out all experiments on the bioasq task 2a dataset in which 12 millions of training
documents and approximately 27,000 index terms are given with parent-child pairs de   ned over
index terms.1 the index terms are known as medical subject headings (mesh), which is controlled
and updated continually by national library of medicine (nlm), and used to index articles for the
medline and pubmed databases.
representing parent-child pairs in a dag. it is often the case that if we represent parent-child
pairs of labels as a graph, in real-world problems, it contains cycles. sometimes, it is reasonable to
assume that these cycles result from wrong annotations due to complex relationship between labels
with different abstract levels. additionally, cycles in a graph also introduce other dif   culties to
learning algorithms. for instance, if we want to visit all ancestors given a node, where the node
has a direct edge to one of its ancestors, such graph traversal never ends unless stopping criteria are
used. hence, we remove edges resulting in cycles as follows:

1http://www.bioasq.org

3

accepted as a workshop contribution at iclr 2015

figure 1: learned representations of 16 major categories in mesh vocabulary. projection of label
representations into 2d is done by tsne (van der maaten & hinton, 2008).

    pick a node that has no parent as a starting node.
    run depth-first search (dfs) from the starting node in order to detect edges pointing to
    repeat the above steps until all nodes having no parent are visited.

nodes visited already, then remove such edges.

the above preprocessing steps left us 46,455 parent-child pairs by removing 3,461 ones, from which
we obtain a dag-structured label hierarchy.
training details. we set the dimension of vector representations d to 128 and the learning rate   
is    xed to 0.1 during training. it took about 3 hours to iterate 50 times over 12 millions of training
instances on intel xeon e5-2620 equipped with 24 threads.

5 experimental results

the hierarchy of mesh vocabulary consists of 16 major categories. each index term belongs to at
least one major category. after training the log-linear model on the bioasq dataset, we selected
labels corresponding terminals and belonging to a single major category in order to visualize learned
representations. figure 1 shows that using a hierarchy and co-occurrences of labels the model sepa-
rates reasonably well 16 major categories de   ned in mesh vocabulary. please note that whereas we
use tsne for better visualization in figure 1, principal component analysis is used to project label
representations for the rest of    gures in this paper. in the following section, we investigate what
label representations learn from a hierarchy and co-occurrences.

5.1 encoding hierarchical structures

as an illustration of our results, we will focus on a subgraph related to health care, shown in the top
of figure 2. consider the leaf nodes in the    gure. according to our objective in equation 1, urban
health, suburban health and rural health are trained to have representations for predicting their
common ancestors, i.e., health, population characteristics and health care category. the child
nodes of population are also trained similarly. although urban, suburban and rural health are
separated from urban, suburban and rural population, their representations tend to be similar since
they share the same ancestors. in other words, urban health and rural population, for example,
should have somewhat similar representations in part so as to increase a id203 of predicting
their common ancestors even though they are rarely assigned to the same instance.

4

accepted as a workshop contribution at iclr 2015

(a)

(b)

(c)

figure 2: (a) a part of the hierarchy related to health care. (b) learned vector representations for
the index terms at leaf without the hierarchy (manually rotated). (c) learned vector representations
for the same index terms with the hierarchy.

we did perform an experiment to see whether learning from only the co-occurrences yields mean-
ingful structures.
in this case, our objective function is limited to the right term in equation 1.
co-occurrence information enables to learn internal structures (see figure 2b). if we consider hier-
archical relationship between labels as well as co-occurrences of them, a more interesting property
of our proposed method can be observed. a major difference between figure 2b and 2c are the inter-
and intra-group relationships among labels at the leaves. speci   cally, all child nodes of health are
located in the left side and those of population appear in the opposite side (figure 2c). in addition
to such relationship between two groups, relations between labels belonging to health (on the left)
resemble those found among the children of population (on the right).
figure 3 shows how important it is to use hierarchical information for capturing regularities of the
labels. it is likely that label pairs that co-occur frequently are close to each other (see figure 3a and
3c). in particular, figure 3a illustrates that urban population is close to urban health and rural
population because urban population is often assigned together with urban population and rural
population to a document. likewise, as shown in figure 3c, each therapy is close by a disorder
for which the therapy is an effective treatment. if we make use of hierarchical information during
training the model, it reveals more interesting relationship which is not observed from the model
trained on only co-occurrences. figure 3b shows that there is a strong directed relation from urban
to rural which can be represented by a direction vector pointing upper-left. we can say the learned
vector representations has identi   ed therapy-disorders/diseases relationship (figure 3d).

5.2 analogical reasoning

one way to evaluate representation quality is analogical reasoning as shown in (mikolov et al.,
2013). for example, one could represent the analogy that a king is to a queen like a man to a woman
with a qualitative equation such as    king - man + woman     queen.    upon the observations in section
5.1, we performed analogical reasoning on both the representations trained with the hierarchy and
ones without the hierarchy, speci   cally, regarding therapy-disorders/diseases relationship (table
1). as expected, it seems like the label representations trained with the hierarchy are clearly advan-

5

population   characteristicshealthpopulationdemographysocioeconomic   factorsurban    healthsuburban   healthrural   health   urbanpopulationsuburban   populationrural   populationhealth care   category0.40.20.00.20.40.30.20.10.00.10.20.30.4rural healthsuburban healthurban healthrural populationsuburban populationurban populationhealthpopulation0.40.20.00.20.40.30.20.10.00.10.20.3rural healthsuburban healthurban healthrural populationsuburban populationurban populationhealthpopulationaccepted as a workshop contribution at iclr 2015

(a)

(b)

(c)

(d)

figure 3: (a) & (c) labels    representations learned from only co-occurrences (b) & (d) from a hier-
archy as well as co-occurrences

tageous to ones trained without the hierarchy on analogical reasoning. to be more speci   c, consider
the    rst example, where we want to know what kinds of therapies are effective on    respiration
disorders    as the relationship between    diet therapy    and    cardiovascular diseases.    when we
perform such analogical reasoning using learned representations with the hierarchy, the most prob-
able answers to this analogy question are therapies that can used to treat    respiration disorders   
including nutritional therapy. unlike the learned representations with the hierarchy, it is likely that
the label representations learned without the hierarchy perform poorly on this type of tasks.
however, we could not observe such regularities from analogical reasoning questions in terms of
parent-child relationships. for instance, it was expected that either    urban health - health + popu-
lation    or    urban health - health -population characteristics + population    results in representa-
tions close to urban population, but the answers of such questions did not end up with something
close to urban population. we conjecture that this is because our proposed method only encodes
one-way hierarchical dependence of labels. in other words, each index term is trained to predict its
ancestors, but not to predict its descendants.

5.3 different hierarchies, different representations

in the previous section, we have shown our proposed method is capable of capturing hierarchical
structures over labels and co-occurrences of them. in this case, it can be expected that the learned
representations change when some part of the hierarchy is changed while the label co-occurrences
remains same. to answer this question,    rstly, we modi   ed the original hierarchy (left in figure 4a)
so as to obtain the new hierarchy (right in figure 4a). originally, population characteristics has two
child nodes, health and population. contrary to population that has only three child nodes, health
has dozens of child nodes. instead of removing the other nodes from the hierarchy, we kept them as
they are in the hierarchy. however, population was removed from the hierarchy. we then created
three new internal nodes, urban, suburban and rural representing types of developed environments.
finally, all nodes of interest (in blue and red) were grouped according to the developed types.

6

0.80.60.40.20.00.20.40.60.81.00.40.20.00.20.4rural healthurban healthrural populationurban populationrural hospitalsurban hospitals0.80.60.40.20.00.20.40.60.81.00.40.20.00.20.4rural healthurban healthrural populationurban populationrural hospitalsurban hospitals0.80.60.40.20.00.20.40.60.80.80.60.40.20.00.20.40.60.8respiratory_therapyspeech_therapyart_therapyacupuncture_therapybehavior_therapydiet_therapyrespiration_disordersspeech_disorderspost_traumaticstress_disorderssleep_disordersmental_disorderscardiovascular_diseasestherapydisorders/diseases0.60.40.20.00.20.40.60.80.60.40.20.00.20.40.6respiratory_therapyspeech_therapyart_therapyacupuncture_therapybehavior_therapydiet_therapyrespiration_disordersspeech_disorderspost_traumaticstress_disorderssleep_disordersmental_disorderscardiovascular_diseasestherapydisorders/diseasesaccepted as a workshop contribution at iclr 2015

table 1: analogical reasoning on learned vector representations of mesh vocabulary

analogy questions

most probable answers

on learned representations using the hierarchy

cardiovascular diseases : diet therapy
    respiration disorders : ?

mental disorders : behavior therapy
    post traumatic stress disorders : ?

cardiovascular diseases : diet therapy
    respiration disorders : ?

mental disorders : behavior therapy
    post traumatic stress disorders : ?

diet therapy

enteral nutrition

gastrointestinal intubation
total parenteral nutrition

parenteral nutrition
respiratory therapy
behavior therapy
cognitive therapy

rational-emotive psychotherapy

brief psychotherapy

psychologic desensitization

implosive therapy

respiration disorders

respiratory tract diseases

respiratory sounds
airway obstruction

hypoventilation

croup

behavior therapy

psychologic desensitization
internal-external control

post traumatic stress disorders

phobic disorders

anger

on learned representations without using the hierarchy

figure 4d shows learned vector representations with modi   ed hierarchy and compares the learned
representations with the previous results.2 unlike the previous result in figure 4c, where health-
related nodes and population-related nodes form clusters, urban health and urban population are
clustered together since they share the common parent node, urban. we can also observe similar
patterns for suburban and rural. besides, relative distance between each population and health
within a cluster is identical, and the same direction vector from health to population or the other way
around can be de   ned. please note that, in this case, learning the model only from co-occurrences
yields always similar label representations since we only modi   ed the hierarchy (figure 4b).

6 discussion and future work

we have presented a method that learns vector representation of labels taking hierarchical structures
into account. the empirical results demonstrate that using hierarchical structures of labels with
label co-occurrences have more impact on identifying regularities of labels than using label co-
occurrences only.
we evaluated label representations qualitatively by observing label representations in 2d. even
though inter- and intra-group relations can be found in the learned label space, it is still desired
to evaluate our method using quantitative measures because we have compared and analyzed both
learned representations with the very limited number of examples and chosen arbitrarily. hence,
we are currently attempting to evaluate the model quantitatively to check whether the model brings
on better prediction results in multi-label classi   cation as a means of pre-training a joint embedding
space (weston et al., 2010). we are also interested in extending the model to be capable of analogical
reasoning on parent-child relationship.

2for the sake of readability, we used repeated results, the left graph in (a), (b) and (c) taken from figure 2,
in order to clearly show the difference between representations learned from the original hierarchy and from
the modi   ed one as well as learned from only co-occurrences.

7

accepted as a workshop contribution at iclr 2015

(a)

(b)

(c)

(d)

figure 4: (a) a modi   ed hierarchy (right) from the original one (left) obtained by grouping the same
types of developed environments. see text for further explanation. (b) learned vector representa-
tions without using a hierarchy. (c) learned vector representations using the original hierarchy. (d)
learned vector representations using the modi   ed hierarchy.

references
bengio, yoshua, ducharme, r  ejean, vincent, pascal, and jauvin, christian. a neural probabilistic

language model. journal of machine learning research, 3:1137   1155, 2003.

bi, wei and kwok, james t. multilabel classi   cation on tree- and dag-structured hierarchies. in

proceedings of the 28th international conference on machine learning, pp. 17   24, 2011.

bi, wei and kwok, james t. ef   cient multi-label classi   cation with many labels. in proceedings of

the 30th international conference on machine learning, pp. 405   413, 2013.

chen, yao-nan and lin, hsuan-tien. feature-aware label space dimension reduction for multi-label

classi   cation. in advances in neural information processing systems, pp. 1529   1537, 2012.

dembczy  nski, krzysztof, waegeman, willem, cheng, weiwei, and h  ullermeier, eyke. on label
dependence and loss minimization in multi-label classi   cation. machine learning, 88(1-2):5   45,
2012.

f  urnkranz, johannes, h  ullermeier, eyke, loza menc    a, eneldo, and brinker, klaus. multilabel

classi   cation via calibrated label ranking. machine learning, 73(2):133   153, 2008.

hsu, daniel, kakade, sham, langford, john, and zhang, tong. multi-label prediction via com-
in advances in neural information processing systems 22, volume 22, pp.

pressed sensing.
772   780, 2009.

mikolov, tomas, sutskever, ilya, chen, kai, corrado, greg s, and dean, jeff. distributed repre-
sentations of words and phrases and their compositionality. in advances in neural information
processing systems 26, pp. 3111   3119. 2013.

mnih, andriy and hinton, geoffrey e. a scalable hierarchical distributed language model.

advances in neural information processing systems 22, pp. 1081   1088, 2009.

in

morin, frederic and bengio, yoshua. hierarchical probabilistic neural network language model.
in proceedings of the 10th international workshop on arti   cial intelligence and statistics, pp.
246   252, 2005.

8

population   characteristicsurbansuburbanurban    healthsuburban   healthrural   healthurbanpopulationsuburban   populationrural   populationhealth care   categoryruralhealth   population   characteristicshealthpopulationdemographysocioeconomic   factorsurban    healthsuburban   healthrural   health   urbanpopulationsuburban   populationrural   populationhealth care   category      0.40.20.00.20.40.30.20.10.00.10.20.30.4rural healthsuburban healthurban healthrural populationsuburban populationurban populationhealthpopulation0.40.20.00.20.40.30.20.10.00.10.20.3rural healthsuburban healthurban healthrural populationsuburban populationurban populationhealthpopulation0.60.40.20.00.20.40.30.20.10.00.10.20.3urban healthurban populationsuburban healthsuburban populationrural healthrural populationaccepted as a workshop contribution at iclr 2015

read, jesse, pfahringer, bernhard, holmes, geoff, and frank, eibe. classi   er chains for multi-label

classi   cation. machine learning, 85(3):333   359, 2011.

rousu, juho, saunders, craig, szedm  ak, s  andor, and shawe-taylor, john. kernel-based learning
of hierarchical multilabel classi   cation models. journal of machine learning research, 7:1601   
1626, 2006.

silla jr., carlos nascimento and freitas, alex alves. a survey of hierarchical classi   cation across

different application domains. data mining and knowledge discovery, 22(1-2):31   72, 2011.

tai, farbound and lin, hsuan-tien. multilabel classi   cation with principal label space transforma-

tion. neural computation, 24(9):2508   2542, 2012.

van der maaten, laurens and hinton, geoffrey. visualizing data using id167. journal of machine

learning research, 9(2579-2605):85, 2008.

vens, celine, struyf, jan, schietgat, leander, d  zeroski, saso, and blockeel, hendrik. id90

for hierarchical multi-label classi   cation. machine learning, 73(2):185   214, 2008.

weston, jason, bengio, samy, and usunier, nicolas. large scale image annotation: learning to

rank with joint word-image embeddings. machine learning, 81(1):21   35, 2010.

yu, hsiang-fu, jain, prateek, kar, purushottam, and dhillon, inderjit. large-scale multi-label learn-
ing with missing labels. in proceedings of the 31st international conference on machine learn-
ing, pp. 593   601, 2014.

zimek, arthur, buchwald, fabian, frank, eibe, and kramer, stefan. a study of hierarchical and    at
classi   cation of proteins. ieee/acm transactions on computational biology and bioinformat-
ics, 7:563   571, 2010.

9

