multilingual part-of-speech tagging with

bidirectional long short-term memory models and auxiliary loss

barbara plank

anders s  gaard

university of groningen

university of copenhagen

the netherlands

denmark

yoav goldberg

bar-ilan university

israel

6
1
0
2

 
l
u
j
 

1
2

 
 
]
l
c
.
s
c
[
 
 

3
v
9
2
5
5
0

.

4
0
6
1
:
v
i
x
r
a

b.plank@rug.nl

soegaard@hum.ku.dk

yoav.goldberg@gmail.com

abstract

bidirectional long short-term memory (bi-
lstm) networks have recently proven
successful for various nlp sequence mod-
eling tasks, but
little is known about
their reliance to input representations, tar-
get languages, data set size, and label
noise. we address these issues and eval-
uate bi-lstms with word, character, and
unicode byte embeddings for pos tag-
ging. we compare bi-lstms to tradi-
tional pos taggers across languages and
data sizes. we also present a novel bi-
lstm model, which combines the pos
tagging id168 with an auxiliary
id168 that accounts for rare words.
the model obtains state-of-the-art perfor-
mance across 22 languages, and works es-
pecially well for morphologically complex
languages. our analysis suggests that bi-
lstms are less sensitive to training data
size and label corruptions (at small noise
levels) than previously assumed.

introduction

1
recently, bidirectional long short-term memory
networks (bi-lstm) (graves and schmidhuber,
2005; hochreiter and schmidhuber, 1997) have
been used for language modelling (ling et al.,
2015), id52 (ling et al., 2015; wang
et al., 2015), transition-based dependency pars-
ing (ballesteros et al., 2015; kiperwasser and
goldberg, 2016),    ne-grained id31
(liu et al., 2015), syntactic chunking (huang et
al., 2015), and id14 (zhou and
xu, 2015). lstms are recurrent neural networks
(id56s) in which layers are designed to prevent
vanishing gradients. bidirectional lstms make a
backward and forward pass through the sequence

before passing on to the next layer. for further de-
tails, see (goldberg, 2015; cho, 2015).

we consider using bi-lstms for id52.
previous work on using deep learning-based meth-
ods for id52 has focused either on a sin-
gle language (collobert et al., 2011; wang et al.,
2015) or a small set of languages (ling et al.,
2015; santos and zadrozny, 2014).
instead we
evaluate our models across 22 languages. in ad-
dition, we compare performance with represen-
tations at different levels of granularity (words,
characters, and bytes). these levels of represen-
tation were previously introduced in different ef-
forts (chrupa  a, 2013; zhang et al., 2015; ling
et al., 2015; santos and zadrozny, 2014; gillick
et al., 2016; kim et al., 2015), but a comparative
evaluation was missing.

moreover, deep networks are often said to re-
quire large volumes of training data. we investi-
gate to what extent bi-lstms are more sensitive
to the amount of training data and label noise than
standard pos taggers.

finally, we introduce a novel model, a bi-lstm
trained with auxiliary loss. the model jointly pre-
dicts the pos and the log frequency of the word.
the intuition behind this model is that the aux-
iliary loss, being predictive of word frequency,
helps to differentiate the representations of rare
and common words. we indeed observe perfor-
mance gains on rare and out-of-vocabulary words.
these performance gains transfer into general im-
provements for morphologically rich languages.

contributions
in this paper, we a) evaluate the
effectiveness of different representations in bi-
lstms, b) compare these models across a large
set of languages and under varying conditions
(data size, label noise) and c) propose a novel bi-
lstm model with auxiliary loss (logfreq).

2 tagging with bi-lstms

recurrent neural networks (id56s) (elman, 1990)
allow the computation of    xed-size vector repre-
sentations for word sequences of arbitrary length.
an id56 is a function that reads in n vectors
x1, ..., xn and produces an output vector hn, that
depends on the entire sequence x1, ..., xn. the
vector hn is then fed as an input to some classi-
   er, or higher-level id56s in stacked/hierarchical
models. the entire network is trained jointly such
that the hidden representation captures the impor-
tant information from the sequence for the predic-
tion task.

a bidirectional recurrent neural network (bi-
id56) (graves and schmidhuber, 2005) is an ex-
tension of an id56 that reads the input sequence
twice, from left to right and right to left, and the
encodings are concatenated. the literature uses
the term bi-id56 to refer to two related architec-
tures, which we refer to here as    context bi-id56   
and    sequence bi-id56   . in a sequence bi-id56
(bi-id56seq), the input is a sequence of vectors
x1:n and the output is a concatenation (   ) of a for-
ward (f) and reverse (r) id56 each reading the
sequence in a different directions:
v = bi-id56seq(x1:n) = id56f (x1:n)     id56r(xn:1)

in a context bi-id56 (bi-id56ctx), we get an addi-
tional input i indicating a sequence position, and
the resulting vectors vi result from concatenating
the id56 encodings up to i:
vi = bi-id56ctx(x1:n, i) = id56f (x1:i)     id56r(xn:i)

thus, the state vector vi in this bi-id56 encodes
information at position i and its entire sequential
context. another view of the context bi-id56 is
of taking a sequence x1:n and returning the corre-
sponding sequence of state vectors v1:n.

lstms (hochreiter and schmidhuber, 1997)
are a variant of id56s that replace the cells of
id56s with lstm cells that were designed to pre-
vent vanishing gradients. bidirectional lstms
are the bi-id56 counterpart based on lstms.

our basic bi-lstm tagging model is a context
bi-lstm taking as input id27s (cid:126)w. we
incorporate subtoken information using an hierar-
chical bi-lstm architecture (ling et al., 2015;
ballesteros et al., 2015). we compute subtoken-
level (either characters (cid:126)c or unicode byte(cid:126)b) embed-
dings of words using a sequence bi-lstm at the

lower level. this representation is then concate-
nated with the (learned) id27s vector
(cid:126)w which forms the input to the context bi-lstm at
the next layer. this model, illustrated in figure 1
(lower part in left    gure), is inspired by balles-
teros et al. (2015). we also test models in which
we only keep sub-token information, e.g., either
both byte and character embeddings (figure 1,
right) or a single (sub-)token representation alone.

figure 1: right: bi-lstm, illustrated with (cid:126)b + (cid:126)c
(bytes and characters), for (cid:126)w + (cid:126)c replace (cid:126)b with
words (cid:126)w. left: freqbin, our multi-task bi-
lstm that predicts at every time step the tag and
the frequency class for the token.

in our novel model, cf. figure 1 left, we train
the bi-lstm tagger to predict both the tags of the
sequence, as well as a label that represents the
log frequency of the token as estimated from the
training data. our combined cross-id178 loss
is now: l(   yt, yt) + l(   ya, ya), where t stands for
a pos tag and a is the log frequency label, i.e.,
a = int(log(f reqtrain(w)). combining this log
frequency objective with the tagging task can be
seen as an instance of id72 in which
the labels are predicted jointly. the idea behind
this model is to make the representation predictive
for frequency, which encourages the model to not
share representations between common and rare
words, thus bene   ting the handling of rare tokens.

3 experiments
all bi-lstm models were implemented in
id98/pyid98,1 a    exible neural network library.
for all models we use the same hyperparameters,
which were set on english dev, i.e., sgd train-
ing with cross-id178 loss, no mini-batches, 20

1https://github.com/clab/id98

epochs, default learning rate (0.1), 128 dimensions
for id27s, 100 for character and byte
embeddings, 100 hidden states and gaussian noise
with   =0.2. as training is stochastic in nature, we
use a    xed seed throughout. embeddings are not
initialized with pre-trained embeddings, except
when reported otherwise. in that case we use off-
the-shelf polyglot embeddings (al-rfou et al.,
2013).2 no further unlabeled data is considered
in this paper. the code is released at: https:
//github.com/bplank/bilstm-aux
taggers we want to compare pos taggers un-
der varying conditions. we hence use three dif-
ferent types of taggers: our implementation of a
bi-lstm; tnt (brants, 2000)   a second order
id48 with suf   x trie handling for oovs. we use
tnt as it was among the best performing taggers
evaluated in horsmann et al. (2015).3 we comple-
ment the nn-based and id48-based tagger with
a crf tagger, using a freely available implemen-
tation (plank et al., 2014) based on crfsuite.

3.1 datasets
for the multilingual experiments, we use the
data from the universal dependencies project
v1.2 (nivre et al., 2015) (17 pos) with the canon-
ical data splits. for languages with token segmen-
tation ambiguity we use the provided gold seg-
mentation.
if there is more than one treebank
per language, we use the treebank that has the
canonical language name (e.g., finnish instead of
finnish-ftb). we consider all languages that have
at least 60k tokens and are distributed with word
forms, resulting in 22 languages. we also re-
port accuracies on wsj (45 pos) using the stan-
dard splits (collins, 2002; manning, 2011). the
overview of languages is provided in table 1.

3.2 results
our results are given in table 2. first of all, no-
tice that tnt performs remarkably well across the
22 languages, closely followed by crf. the bi-
lstm tagger ( (cid:126)w) without lower-level bi-lstm
for subtokens falls short, outperforms the tradi-
tional taggers only on 3 languages. the bi-lstm
2https://sites.google.com/site/rmyeid/

projects/polyglot

3they found treetagger was closely followed by hun-
pos, a re-implementation of tnt, and stanford and clearnlp
were lower ranked. in an initial investigation, we compared
tnt, hunpos and treetagger and found tnt to be consistently
better than treetagger, hunpos followed closely but crashed
on some languages (e.g., arabic).

coarse
non-ie
indoeuropean
indoeuropean
indoeuropean
indoeuropean
indoeuropean
indoeuropean

ar
bg
cs
da
de
en
es
eu language isolate
fa
   
fr

indoeuropean
non-ie
indoeuropean

fine
semitic
slavic
slavic
germanic
germanic
germanic
romance

indo-iranian
uralic
romance

he
hi
hr
id
it
nl
no
pl
pt
sl
sv

austronesian

fine
semitic
indo-iranian

coarse
non-ie
indoeuropean
indoeuropean slavic
non-ie
indoeuropean romance
indoeuropean germanic
indoeuropean germanic
indoeuropean slavic
indoeuropean romance
indoeuropean slavic
indoeuropean germanic

table 1: grouping of languages.

model clearly bene   ts from character representa-
tions. the model using characters alone ((cid:126)c) works
remarkably well, it improves over tnt on 9 lan-
guages (incl. slavic and nordic languages). the
combined word+character representation model is
the best representation, outperforming the baseline
on all except one language (indonesian), provid-
ing strong results already without pre-trained em-
beddings. this model ( (cid:126)w + (cid:126)c) reaches the biggest
improvement (more than +2% accuracy) on he-
brew and slovene. initializing the word embed-
dings (+polyglot) with off-the-shelf language-
speci   c embeddings further improves accuracy.
the only system we are aware of that evaluates on
ud is gillick et al. (2016) (last column). however,
note that these results are not strictly comparable
as they use the earlier ud v1.1 version.

the overall best system is the multi-task bi-
lstm freqbin (it uses (cid:126)w + (cid:126)c and polyglot
initialization for (cid:126)w). while on macro average it
is on par with bi-lstm (cid:126)w + (cid:126)c, it obtains the best
results on 12/22 languages, and it is successful in
predicting pos for oov tokens (cf. table 2 oov
acc columns), especially for languages like ara-
bic, farsi, hebrew, finnish.

we examined simple id56s and con   rm the
   nding of ling et al. (2015) that they performed
worse than their lstm counterparts. finally, the
bi-lstm tagger is competitive on wsj, cf. ta-
ble 3.

rare words
in order to evaluate the effect of
modeling sub-token information, we examine ac-
curacy rates at different frequency rates. figure 2
shows absolute improvements in accuracy of bi-
lstm (cid:126)w + (cid:126)c over mean log frequency, for dif-
ferent language families. we see that especially
for slavic and non-indoeuropean languages, hav-
ing high morphologic complexity, most of the im-
provement is obtained in the zip   an tail. rare to-
kens bene   t from the sub-token representations.

(cid:126)c

(cid:126)w

95.70

   
   
   
   
   
   

(cid:126)w + (cid:126)c +polyglot
bi-lstm freqbin bi-lstm freqbin

oov acc

bts

baselines
tnt
crf
94.27
94.61
94.58
94.70
94.57
93.62
93.21
93.27
95.53
95.37
94.96
95.64
97.82
97.56
96.36
96.84
96.56
96.82
93.83
94.29
91.38
92.64
92.66
93.35
94.23
94.55
91.63
93.35
95.65
95.98
93.59
90.32
95.14
94.51
93.63
93.71
96.00
94.53
94.06
93.16
92.96
93.16
96.43
96.16
90.03
88.54
96.21
96.31
95.57
93.96
96.32
96.27
94.77
94.92
95.19
94.45

bi-lstm using:
(cid:126)c + (cid:126)b
94.01
94.28
93.16
92.59
94.49
96.26
98.43
97.78
96.08
94.88
90.11
91.57
92.29
92.72
95.03
89.15
93.69
93.58
92.99
94.47
91.46
95.77
87.74
95.75
96.19
96.20
96.77
95.50

avg
indoeur.
non-indo.
germanic
romance
slavic
ar
bg
cs
da
de
en
es
eu
fa
   
fr
he
hi
hr
id
it
nl
no
pl
pt
sl
sv
95.57
table 2: tagging accuracies on ud 1.2 test sets. (cid:126)w: words, (cid:126)c: characters, (cid:126)b: bytes. bold/   : best
accuracy/representation; +polyglot: using pre-trained embeddings. freqbin: our multi-task model.
oov acc: accuracies on oovs. bts: best results in gillick et al. (2016) (not strictly comparable).

96.00   
96.15   
95.67   
95.09   
96.51   
96.91   
98.91
98.02
97.80
96.19
92.64
94.46
95.12
94.70
97.19
94.85
95.80
95.79
96.23
94.76
93.11
97.59
93.32
97.57
96.41
97.53
97.55
96.36

(cid:126)w + (cid:126)c
92.37
92.72
91.97
91.18
94.71
91.79
95.48
95.12
93.77
91.96
90.33
92.10
93.60
88.00
95.31
87.95
94.44
93.97
95.99
89.24
90.48
96.57
84.96
94.39
89.73
94.24
91.09
93.32

96.50
96.63
96.21
95.55
96.93
97.42
98.87
98.23
98.02
96.16
93.51
95.17
95.67
95.38
97.60
95.74
96.20
96.92
96.97
96.27
93.32
97.90
93.82
98.06
97.63
97.94
96.97
96.60

96.52
96.63
96.28
95.49
96.93
97.50
98.91
97.97
98.24
96.35
93.38
95.16
95.74
95.51
97.49
95.85
96.11
96.96
97.10
96.82
93.41
97.95
93.30
98.03
97.62
97.90
96.84
96.69

94.29
94.58
93.51
92.89
94.76
96.45
98.68
97.89
96.38
95.12
90.02
91.62
93.06
92.48
95.82
90.25
94.39
93.74
93.40
95.32
91.37
95.62
89.11
95.87
95.80
95.96
96.87
95.57

87.98
87.63
90.39
85.45
86.07
91.69
96.21
90.56
91.30
86.35
86.77
80.11
79.27
84.30
89.05
88.85
83.54
88.83
85.27
92.71
87.67
89.15
75.95
93.72
94.94
92.33
88.94
89.80

83.48
82.77
87.44
81.22
81.31
86.66
95.04
87.40
89.02
77.09
81.95
71.23
71.38
79.87
80.00
86.34
78.09
80.11
81.19
84.62
88.25
83.59
76.62
92.05
91.77
92.16
80.48
88.37

97.84
98.50
95.52
92.87
93.87
95.80

96.82
95.48
95.75

92.85
97.56

   
   
   
   
   

   
   
   

   

wsj
convnet (santos and zadrozny, 2014)
convnet reimplementation (ling et al., 2015)
bi-id56 (ling et al., 2015)
bi-lstm (ling et al., 2015)
our bi-lstm (cid:126)w+(cid:126)c

accuracy

97.32
96.80
95.93
97.36
97.22

table 3: comparison pos accuracy on wsj; bi-
lstm: 30 epochs,   =0.3, no polyglot.

ing amounts of training instances (number of sen-
tences). the learning curves in figure 3 show
similar trends across language families.4 tnt
is better with little data, bi-lstm is better with
more data, and bi-lstm always wins over crf.
the bi-lstm model performs already surpris-
ingly well after only 500 training sentences. for
non-indoeuropean languages it is on par and above

4we observe the same pattern with more, 40, iterations.

figure 2: absolute improvements of bi-lstm
( (cid:126)w + (cid:126)c) over tnt vs mean log frequency.

data set size prior work mostly used large
data sets when applying neural network based
approaches (zhang et al., 2015). we evaluate
how brittle such models are with respect to their
more traditional counterparts by training bi-lstm
( (cid:126)w + (cid:126)c without polyglot embeddings) for increas-

state-of-the-art performance close to taggers us-
ing carefully designed feature templates. ling et
al. (2015) extend this line and compare a novel
bi-lstm model,
learning word representations
through character embeddings. they evaluate
their model on a id38 and pos tag-
ging setup, and show that bi-lstms outperform
the id98 approach of santos and zadrozny (2014).
similarly, labeau et al. (2015) evaluate character
embeddings for german. bi-lstms for pos tag-
ging are also reported in wang et al. (2015), how-
ever, they only explore id27s, ortho-
graphic information and evaluate on wsj only. a
related study is cheng et al. (2015) who propose a
multi-task id56 for id39 by
jointly predicting the next token and current to-
ken   s name label. our model is simpler, it uses
a very coarse set of labels rather then integrating
an entire id38 task which is compu-
tationally more expensive. an interesting recent
study is gillick et al. (2016), they build a single
byte-to-span model for multiple languages based
on a sequence-to-sequence id56 (sutskever et al.,
2014) achieving impressive results. we would like
to extend this work in their direction.

5 conclusions

we evaluated token and subtoken-level representa-
tions for neural network-based part-of-speech tag-
ging across 22 languages and proposed a novel
multi-task bi-lstm with auxiliary loss. the aux-
iliary loss is effective at improving the accuracy of
rare words.

subtoken representations are necessary to ob-
tain a state-of-the-art pos tagger, and charac-
ter embeddings are particularly helpful for non-
indoeuropean and slavic languages.

combining them with id27s in a hi-
erarchical network provides the best representa-
tion. the bi-lstm tagger is as effective as the
crf and id48 taggers with already as little as
500 training sentences, but is less robust to label
noise (at higher noise rates).

acknowledgments

we thank the anonymous reviewers for their feed-
back. as is funded by the erc starting grant
lowlands no. 313695. yg is supported by
the israeli science foundation (grant number
1555/15) and a google research award.

figure 3: amount of training data (number of sen-
tences) vs tagging accuracy.

the other taggers with even less data (100 sen-
tences). this shows that the bi-lstms often needs
more data than the generative markovian model,
but this is de   nitely less than what we expected.

label noise we investigated the susceptibility
of the models to noise, by arti   cially corrupting
training labels. our initial results show that at low
noise rates, bi-lstms and tnt are affected sim-
ilarly, their accuracies drop to a similar degree.
only at higher noise levels (more than 30% cor-
rupted labels), bi-lstms are less robust, showing
higher drops in accuracy compared to tnt. this
is the case for all investigated language families.

4 related work

introduced
character embeddings were    rst
by sutskever et al. (2011) for language model-
ing. early applications include text classi   ca-
tion (chrupa  a, 2013; zhang et al., 2015). re-
cently, these representations were successfully ap-
plied to a range of id170 tasks. for
id52, santos and zadrozny (2014) were
the    rst to propose character-based models. they
use a convolutional neural network (id98; or con-
vnet) and evaluated their model on english (ptb)
and portuguese, showing that the model achieves

references
rami al-rfou, bryan perozzi, and steven skiena.
2013. polyglot: distributed word representations
for multilingual nlp. in conll.

miguel ballesteros, chris dyer, and noah a. smith.
2015. improved transition-based parsing by mod-
eling characters instead of words with lstms. in
emnlp.

thorsten brants.
speech tagger.
ence on applied natural language processing.

2000. tnt: a statistical part-of-
in proceedings of the sixth confer-

hao cheng, hao fang, and mari ostendorf. 2015.
open-domain name error detection using a multi-
task id56. in emnlp.

kyunghyun cho.

standing with distributed representation.
abs/1511.07916.

2015. natural language under-
arxiv,

grzegorz chrupa  a. 2013. text segmentation with
in workshop on
character-level text embeddings.
deep learning for audio, speech and language
processing, icml.

michael collins. 2002. discriminative training meth-

ods for id48. in emnlp.

ronan collobert, jason weston, l  eon bottou, michael
karlen, koray kavukcuoglu, and pavel kuksa.
2011. natural language processing (almost) from
the journal of machine learning re-
scratch.
search, 12:2493   2537.

jeffrey l elman. 1990. finding structure in time.

cognitive science, 14(2):179   211.

dan gillick, cliff brunk, oriol vinyals, and amarnag
subramanya. 2016. multilingual language process-
ing from bytes. in naacl.

yoav goldberg. 2015. a primer on neural network
models for natural language processing. arxiv,
abs/1510.00726.

alex graves and j  urgen schmidhuber. 2005. frame-
wise phoneme classi   cation with bidirectional lstm
and other neural network architectures. neural net-
works, 18(5):602   610.

sepp hochreiter and j  urgen schmidhuber.

1997.
neural computation,

long short-term memory.
9(8):1735   1780.

tobias horsmann, nicolai erbs, and torsten zesch.
2015. fast or accurate?   a comparative evaluation of
id52 models. in proceedings of the interna-
tional conference of the german society for com-
putational linguistics and language technology.

yoon kim, yacine jernite, david sontag, and alexan-
der m rush. 2015. character-aware neural lan-
guage models. arxiv preprint arxiv:1508.06615.

e. kiperwasser and y. goldberg. 2016. simple and
accurate id33 using bidirectional
lstm feature representations. arxiv e-prints.

matthieu labeau, kevin l  oser, and alexandre al-
lauzen. 2015. non-lexical neural architecture for
   ne-grained id52. in emnlp.

wang ling, chris dyer, alan w black, isabel tran-
coso, ramon fermandez, silvio amir, luis marujo,
and tiago luis. 2015. finding function in form:
compositional character models for open vocabu-
lary word representation. in emnlp.

pengfei liu, sha   q joty, and helen meng. 2015. fine-
grained opinion mining with recurrent neural net-
works and id27s. in emnlp.

christopher d manning. 2011. part-of-speech tag-
ging from 97% to 100%: is it time for some linguis-
tics? in computational linguistics and intelligent
text processing. springer.

joakim nivre,

  zeljko agi  c, maria jesus aranzabe,
masayuki asahara, aitziber atutxa, miguel balles-
teros, john bauer, kepa bengoetxea, riyaz ah-
mad bhat, cristina bosco, sam bowman, giuseppe
g. a. celano, miriam connor, marie-catherine
de marneffe, arantza diaz de ilarraza, kaja do-
brovoljc, timothy dozat, toma  z erjavec, rich  ard
farkas, jennifer foster, daniel galbraith, filip gin-
ter, iakes goenaga, koldo gojenola, yoav gold-
berg, berta gonzales, bruno guillaume, jan haji  c,
dag haug, radu ion, elena irimia, anders jo-
hannsen, hiroshi kanayama, jenna kanerva, simon
krek, veronika laippala, alessandro lenci, nikola
ljube  si  c, teresa lynn, christopher manning, ctlina
mrnduc, david mare  cek, h  ector mart    nez alonso,
jan ma  sek, yuji matsumoto, ryan mcdonald,
anna missil  a, verginica mititelu, yusuke miyao,
simonetta montemagni, shunsuke mori, hanna
nurmi, petya osenova, lilja   vrelid, elena pascual,
marco passarotti, cenel-augusto perez, slav petrov,
jussi piitulainen, barbara plank, martin popel,
prokopis prokopidis, sampo pyysalo, loganathan
ramasamy, rudolf rosa, shadi saleh, sebastian
schuster, wolfgang seeker, mojgan seraji, natalia
silveira, maria simi, radu simionescu, katalin
simk  o, kiril simov, aaron smith, jan   st  ep  anek,
alane suhr, zsolt sz  ant  o, takaaki tanaka, reut
tsarfaty, sumire uematsu, larraitz uria, viktor
varga, veronika vincze, zden  ek   zabokrtsk  y, daniel
zeman, and hanzhi zhu. 2015. universal depen-
dencies 1.2. lindat/clarin digital library at in-
stitute of formal and applied linguistics, charles
university in prague.

zhiheng huang, wei xu, and kai yu. 2015. bidi-
rectional lstm-crf models for sequence tagging.
arxiv preprint arxiv:1508.01991.

barbara plank, dirk hovy, ryan mcdonald, and an-
ders s  gaard. 2014. adapting taggers to twitter us-
ing not-so-distant supervision. in coling.

cicero d santos and bianca zadrozny. 2014. learning
character-level representations for part-of-speech
tagging. in icml.

ilya sutskever, james martens, and geoffrey e hin-
ton. 2011. generating text with recurrent neural
networks. in icml.

ilya sutskever, oriol vinyals, and quoc v le. 2014.
sequence to sequence learning with neural net-
works. in nips.

peilu wang, yao qian, frank k. soong, lei he, and
hai zhao. 2015. part-of-speech tagging with bidi-
rectional long short-term memory recurrent neural
network. pre-print, abs/1510.06168.

xiang zhang, junbo zhao, and yann lecun. 2015.
character-level convolutional networks for text clas-
si   cation. in advances in neural information pro-
cessing systems, pages 649   657.

jie zhou and wei xu. 2015. end-to-end learning of
id14 using recurrent neural net-
works. in acl.

