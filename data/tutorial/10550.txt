modeling relational information in question-answer pairs with

convolutional neural networks

aliaksei severyn   
google research
zurich, switzerland

alessandro moschitti

qatar computing research institute, hbku

5825 doha, qatar

6
1
0
2

 
r
p
a
5

 

 
 
]
l
c
.
s
c
[
 
 

1
v
8
7
1
1
0

.

4
0
6
1
:
v
i
x
r
a

severyn@google.com

amoschitti@qf.org.qa

abstract

in this paper, we propose convolutional neural
networks for learning an optimal representa-
tion of question and answer sentences. their
main aspect is the use of relational information
given by the matches between words from the
two members of the pair. the matches are en-
coded as embeddings with additional parame-
ters (dimensions), which are tuned by the net-
work. these allows for better capturing in-
teractions between questions and answers, re-
sulting in a signi   cant boost in accuracy. we
test our models on two widely used answer
sentence selection benchmarks. the results
clearly show the effectiveness of our relational
information, which allows our relatively sim-
ple network to approach the state of the art.

introduction

1
modeling text pairs to compute their semantic simi-
larity is at the core of many nlp tasks. the most
common approach is to encode them with many
complex lexical, syntactic and semantic features and
then compute various similarity measures between
the obtained representations. recently, it has been
shown that the problem of semantic text match-
ing can be tackled using distributional word match-
ing, e.g., for matching questions with candidate an-
swers (yih et al., 2013).

deep learning approaches generalize the distri-
butional word matching problem to matching sen-
tences and take it one step further by learning the op-
timal sentence representations for a given task. deep
neural networks are able to effectively capture the
compositional process of mapping the meaning of
individual words in a sentence to a continuous rep-
resentation of the sentence. in particular, it has been
recently shown that convolutional neural networks
are able to ef   ciently learn to embed input sentences
    this work was carried out during his phd in the univer-

sity of trento.

into low-dimensional vector space preserving im-
portant syntactic and semantic aspects of the input
sentence, which leads to state-of-the-art results in
many nlp tasks (kalchbrenner et al., 2014; kim,
2014; yu et al., 2014).

in this paper, we capitalize on our previous work
(severyn and moschitti, 2015) extending it with
a novel deep learning architecture for modelling
question-answer pairs for answer sentence rerank-
ing. the main building blocks of our architecture
are two distributional sentence models based on con-
volutional neural networks (convnets). these un-
derlying sentence models work in parallel, map-
ping questions and answer sentences to their dis-
tributional vectors, which are then used to learn
the semantic similarity between them. to compute
question-answer similarity score we adopt an ap-
proach used in the deep learning model of (yu et al.,
2014), which produces excellent results on the an-
swer sentence selection task. however, their model
only operates on unigram or bigrams, while our ar-
chitecture learns to extract and compose id165s of
higher degrees, thus allowing for capturing longer
range dependencies. additionally, our architecture
uses not only the intermediate representations of
questions and answers to compute their similarity
but also includes them in the    nal representation,
which constitutes a much richer representation of the
question-answer pairs.

the main novelty of our architecture is the way
we choose to model relational information in a pair.
yu et al. (2014) combine the output of their deep
learning model with additional features in the    nal
id28 model. such features count the
number word overlaps between the two pair mem-
bers. this provides a sort of relational information,
which signi   cantly improves the network accuracy.
in contrast, our model uses a completely different
approach, which injects relational information about

matching words directly into the id27s
as additional dimensions. the augmented word em-
beddings are thus passed through the layers of the
convolutional feature extractors: this enables the au-
tomatic encoding of the relations between question-
answer pairs in a more structured manner. more-
over, our embedding dimensions encoding matches
are parameters of the network and are tuned during
the training.

in summary,

the distinctive properties of our
model are:
(i) we use a state-of-the-art distribu-
tional sentence model for learning to map input
sentences to vectors, which are then used to mea-
sure the similarity between them; (ii) our model en-
codes question-answer pairs in a richer representa-
tion using not only their similarity score but also
their intermediate representations; (iii) we augment
the id27s with additional dimensions
to encode the fact that certain words overlap in a
given question-answer pair and let the network tune
these parameters; (iv) the architecture of our net-
work makes it straightforward to include any addi-
tional features encoding question-answer similari-
ties; and    nally (v) our model is trained end-to-end
starting from the input sentences to producing a    nal
score that is used to rerank answers. we only require
to initialize id27s trained on some large
unsupervised corpora. however, given a large train-
ing set the network can also optimize the embed-
dings directly for the task, thus omitting the need for
pre-training of the id27s.

we test our model on a popular answer sentence
selection benchmark trec13 (wang et al., 2007)
and on the more recent dataset wikiqa (yang et
al., 2015). the results show the importance of using
relational information and on wikiqa our network
reaches the state of the art, i.e., an mrr of 71.07
and an map of 69.51.
2 our deep learning model
this section explains the architecture of our deep
learning model for modelling question-answer pairs
to rerank answer sentences. we treat the answer sen-
tence selection problem as a simple binary classi   -
cation where answer candidates with higher predic-
tion scores are ranked above the ones with lower
scores. more formally, each question qi     q is
associated with a list of labelled candidate answer

figure 1: our sentence model for mapping input sentences to
their intermediate representations.
sentences {(yi1, ai1), . . . , (yin, ain)}, where labels
yij     {0, 1} with 1 corresponding to answers that
contain a correct answer and 0 otherwise. our
goal is to learn a decision function that maps each
question-answer pair to a score re   ecting their sim-
ilarity: f (  ,   (qi, aij)), where   (  ) is a function
encoding question-answer pairs into a joint feature
space, and    are model parameters.

given that we choose to model the answer sen-
tence selection task as a binary classi   cation, our
main effort lies in designing a deep learning ar-
chitecture for learning an optimal representation of
question-answer pairs. its main building blocks are
two distributional sentence models based on convo-
lutional neural networks. these underlying sentence
models work in parallel mapping question and an-
swer sentences to their distributional vectors, which
are then used to learn the similarity between them.
in the following, we    rst describe our sentence
model for mapping queries and documents to their
intermediate representations and then describe how
they can be used for learning semantic matching be-
tween input query-document pairs.
2.1 distributional sentence model
the architecture of our network for mapping sen-
tences to feature vectors is shown on fig. 1.
it
is mainly inspired by the convolutional architec-
tures used in (kalchbrenner et al., 2014; kim, 2014)
for performing various sentence classi   cation tasks.
however, different from previous work the goal of
our distributional sentence model is to learn interme-
diate representations of questions and answers used
to compute their semantic matching.

our sentence model is composed of a single wide

figure 2: our deep learning architecture for reranking question-answer pairs. the relational information in a pair is modelled by
augmenting id27s with additional dimensions to encode overlapping words, e.g., we feed the network with additional
word overlap indicator features whose values equal to 1 correspond to words that overlap in a pair, e.g., a non-stop word cat.

convolutional layer followed by a non-linearity and
simple max pooling. in the following we give a brief
explanation of its main components: sentence ma-
trix, activations, convolutional and pooling layers.

2.1.1 sentence matrix

the input to the network are raw words that need
to be translated into real-valued feature vectors to be
processed by subsequent layers of the network.

the input is a sentence s treated as a sequence
of words: [w1, . . . , w|s|], where each word is drawn
from a    nite-sized vocabulary v . the architecture
of neural networks is not well suited for dealing
with discrete words, hence, they are represented by
low-dimensional, real-valued, dense vectors w    
rdw looked up in a matrix w     rdw  |v | (whose
columns correspond to words in v ). the mapping
from words to their id27s is performed
by a lookup table operation ltw(wi) = wi. hence,
for each input sentence s we build a sentence matrix
s where each i-th column corresponds to a word em-
bedding wi.

to learn to capture and compose features of in-
dividual words in a given sentence from low-level
id27s into higher level semantic con-
cepts, the neural network applies a series of transfor-
mations to the input sentence matrix s using convo-
lution, non-linearity and pooling operations, which

we describe next.
2.1.2 convolutional feature maps

(cid:88)

(s[:,i   m+1:i]     f)kj

the aim of the convolutional layer is to extract
patterns, i.e., discriminative word sequences that are
common throughout the training instances.
more formally, the convolution operation     be-
tween an input matrix s     rd  |s| and a    lter (or
a convolution kernel) f     rd  m of width m results
in a vector c     r|s|+m   1 where each component is
computed as follows:
ci = (s     f)i =

(1)
where     is the element-wise multiplication and
s[:,i   m+1:i] is a matrix slice of size m along the
columns. note that the convolution    lter is of the
same dimensionality d as the input sentence matrix.
as shown in fig. 1, it slides along the column di-
mension of s producing a vector c     r|s|   m+1 in
output. each component ci is the result of comput-
ing an element-wise product between a column slice
of s and a    lter matrix f, which is then summed to
a single value.

k,j

so far we have described a way to compute a con-
volution between the input sentence matrix and a
single    lter. to form a richer representation of the
data, deep learning models apply a set of    lters that
work in parallel generating multiple feature maps

(also shown on fig. 1). a set of    lters form a    l-
ter bank f     rn  d  m sequentially convolved with
the sentence matrix s and producing a feature map
matrix c     rn  (|s|   m+1).
in practice, we also need to add a bias vector b    
rn to the result of a convolution     a single bi value
for each feature map ci. this allows the network to
learn an appropriate threshold.
activation units. to enable the learning of non-
linear decision boundaries, each convolutional layer
is typically followed by a non-linear activation func-
tion   () applied element-wise. we use a recti   ed
linear (relu) function de   ned as simply max(0, x)
in our model since, as shown in (nair and hinton,
2010), it speeds up the training and sometimes pro-
duces more accurate results.
pooling. the output from the convolutional layer
(passed through the activation function) are then
passed to the pooling layer, whose goal is to aggre-
gate the information and reduce the representation.
we use max pooling in our model which simply re-
turns the maximum value. it operates on columns of
the feature map matrix c returning the largest value:
pool(ci) : r|s|+m   1     r (also shown schemati-
cally in fig. 1).

convolutional layer passed through the activation
function together with pooling layer acts as a non-
linear feature extractor. given that multiple feature
maps are used in parallel to process the input, deep
learning networks are able to build rich feature rep-
resentations of the input.

this ends the description of our sentence model.
in the following we present our deep learning archi-
tecture for learning to match short text pairs.
2.2 our relational model
when learning to match text pairs, modelling the
relational connections between sentences has been
shown to greatly improve the accuracy of the se-
mantic similarity models. for example, top perform-
ing systems on semantic textual similarity bench-
marks (agirre et al., 2015) rely on similarity scores
obtained by aligning the words and phrases between
sentences in a pair. yih et al. (2013) also uses la-
tent word-alignment structure in their semantic sim-
ilarity model to compute similarity between ques-
tion and answer sentences. yu et al. (2014) achieves
large improvements by combining the output of their

deep learning model with word count features in a
id28 model.

to allow our convolutional neural network cap-
ture the connections between related words in a pair
we feed it with an additional binary-like input about
overlapping words. in particular, for each word w in
the input sentence we associate an additional word
overlap indicator feature o     {0, 1}, where 1 corre-
sponds to words that overlap in a given pair and 0
otherwise (see fig. 2). to decide if the words over-
lap, we perform string matching.
hence, we require an additional lookup table
layer for the word overlap features ltwo(  ) with
parameters wo     rdo  2, where do     n is the num-
ber of dimensions to encode word overlap features
and is a hyper-parameter of the model. effectively,
we are augmenting id27s with addi-
tional dimensions that encode the fact that a given
word in a pair is overlapping or semantically similar
and let the network learn its optimal representation.
given a word wi its,    nal id27 wi     rd
(where d = dw + do) is obtained by concatenating
the output of two lookup table operations ltw(wi)
and ltwo(wi) (also see fig. 2).
2.3 the matching model
for matching
the architecture of our model
question-answer pairs is presented in fig. 2. our
sentence models (described in sec. 2.1) learn to map
input sentences to vectors, which can then be used
to compute their similarity. these are then used
to compute a similarity score, which together with
the distributional vector of question and answer sen-
tences are used in a single joint representation.

in the following we describe how the intermedi-
ate representations produced by the sentence model
can be used to compute question-answer similarity
scores and give a brief explanation of the remaining
layers, e.g. hidden and softmax.
similarity model. given the output of our sen-
tence models, their resulting vector representations
xq and xa, can be used to compute a question-
answer similarity score. we follow the approach
of (bordes et al., 2014) that de   nes the similarity
between xq and xa vectors as follows:
q mxa,

(2)
where m     rd  d is a similarity matrix. eq. 2 can
be viewed as a model of the noisy channel approach

sim(xq, xa) = xt

from machine translation, which has been widely
used as a scoring model in information retrieval and
qa (echihabi and marcu, 2003).
in this model,
we seek a transformation of the candidate document
x(cid:48)
a = mxa that is the closest to the input query xq.
the similarity matrix m is a parameter of the net-
work and is optimized during the training.
hidden and classi   cation layers. our model in-
cludes an additional hidden layer right before the
softmax layer (described next) to allow for model-
ing interactions between the components of the in-
termediate representation. it computes the follow-
ing transformation:   (wh    x + b), where wh is the
weight vector of the hidden layer and   () is non-
linearity.finally, to transform the output of the net-
work to the id203 distribution over the labels
we apply a softmax function.
2.4 the information    ow
the output of our sentence models (sec. 2.1) are dis-
tributional representations of a question xq and an
answer xa, which are then matched using a simi-
larity matrix m according to eq. 2. this produces
a single score xsim capturing various aspects of
similarity (syntactic and semantic) of the question-
answer pair. note that it is also straight-forward to
add additional features xfeat to the model.

a ; xt

q ; xsim; xt

the join layer concatenates all intermediate vec-
tors, the similarity score and any additional features
into a single vector: xjoin = [xt
feat]
this vector is then passed through a fully connected
hidden layer, which allows for modelling interac-
tions between the components of the joined rep-
resentation vector. finally, the output of the hid-
den layer is further fed to the softmax classi   cation
layer, which generates a distribution over the class
labels.
2.5 training
the model is trained to minimise the negative con-
ditional log-likelihood of the training set:
i=1 p(yi|qi, ai;   )

c =    log(cid:81)n

where    contains all the network parameters:
   = {w; wo; fq; bq; fa; ba; m; wh; bh; ws; bs},
namely the id27s matrix w and word
overlap feature matrix wo,    lter weights and biases
of the convolutional layers, similarity matrix m, pa-
rameters of the hidden and softmax layers.

the parameters of the network are optimized by
stochastic id119 (sgd) using backpro-
pogation algorithm to compute the gradients.

3 experiments and evaluation

this section describes the dataset and our experi-
mental setup, also giving details about how we ob-
tain the id27s matrix w and train our
network.
3.1 data and setup
we test our model on the manually curated trec
qa dataset1 from wang et al. (2007), which ap-
pears to be one of the most widely used benchmarks
for answer sentence reranking. the set of questions
are collected from trec qa tracks 8-13. the man-
ual judgement of candidate answer sentences is pro-
vided for the entire trec 13 set and for the    rst 100
questions from trec 8-12.

to enable a direct comparison with the previous
work, we use the same train, dev and test sets.
table 1 summarizes the datasets used in our experi-
ments. an additional training set train-all pro-
vided by wang et al. (2007) contains 1,229 questions
from the entire trec 8-12 collection and comes
with automatic judgements. this set represents a
more noisy setting, nevertheless, it provides many
more qa pairs for learning.
wikiqa. this is an the open domain qa dataset
(yang et al., 2015).
its questions were derived
from the bing query logs and the candidate answers
were extracted from paragraphs of the associated
wikipedia pages. the training, test, and develop-
ment set contain 2,118, 633 and 296 questions, re-
spectively. consistently with (yin et al., 2015), we
remove the questions without answers for our eval-
uations.
evaluation. the two metrics used to evaluate the
quality of our model are mean average precision
(map) and mean reciprocal rank (mrr). we use
the of   cial trec eval scorer to compute the above
metrics.
word vectors. while our model allows for learn-
ing the id27s directly, we keep the word
matrix parameter w static. this is due to a com-
mon experience that a minimal size of the dataset

1http://cs.stanford.edu/people/mengqiu/

data/qg-emnlp07-data.tgz

table 1: trec qa datasets for answer reranking.

data
train-all
train
dev
test

# questions
1,229
94
82
100

# qa pairs % correct
12.0%
7.4%
19.3%
18.7%

53,417
4,718
1,148
1,517

required for tuning the id27s for a given
task should be at least in the order of hundred thou-
sands, while in our case the number of question-
answer pairs is one order of magnitude smaller.
hence, similar to (denil et al., 2014; kim, 2014; yu
et al., 2014) we keep the id27s    xed and
initialize the word matrix w from an unsupervised
neural language model.

we run id97 tool (mikolov et al., 2013)
on the english wikipedia dump and the aquaint
corpus2 containing roughly 375 million words. we
opt for a skipgram model with window size 5 and
   ltering words with frequency less than 5. we set
the dimensionality of our id27s to 50 to
be on the line with (yu et al., 2014). the resulting
model contains 50-dimensional vectors for about 3.5
million words. embeddings for words not present in
the id97 model are randomly initialized with
each component uniformly sampled.

we minimally preprocess the data only perform-
ing id121 and lowercasing all words. to re-
duce the size of the resulting vocabulary v , we also
replace all digits with 0. the size of the word vocab-
ulary v for experiments using train set is 17,023
with approximately 95% of words initialized using
wor2vec embeddings and the remaining 5% words
are initialized at random. for the train-all set-
ting the |v | = 56, 953 with about 90% words found
in the id97 model.
word matching features.
in contrast to the word
embeddings matrix, the size of the vocabulary vo to
encode word overlap features is tiny. given such a
small parameter space it is possible to tune the vec-
tors even on small sized datasets. hence, we keep
this as a parameter optimized by our network. we
set the size of the space, do, to 5 and randomly ini-
tialize the entries of the matrix wo by sampling
from the uniform distribution.

2https://catalog.ldc.upenn.edu/

ldc2002t31

3.2 training and hyperparameters
the parameters of our deep learning model were
chosen on a dev set: the width m of the convolution
   lters is 5 and the number of convolutional feature
maps is 100. we use relu activation function and
a simple max-pooling. the size of the hidden layer
is equal to the size of the xjoin vector obtained after
concatenating question and answer sentence vectors
from the distributional models, similarity score and
additional features.

to train the network we use stochastic gradi-
ent descent with shuf   ed mini-batches. we elimi-
nate the need to tune the learning rate by using the
adadelta update rule (zeiler, 2012). the batch size
is set to 50 examples. the network is trained for 25
epochs with early stopping, i.e., we stop the training
if no update to the best accuracy on the dev set has
been made for the last 5 epochs. the accuracy com-
puted on the dev set is the map score. at test time
we use the parameters of the network obtained with
the best map score on the dev set: we compute
the map score after each 10 mini-batch updates and
save the network parameters if a new best dev score
is obtained. in practice, the training converges after
a few epochs.
3.3 results and discussion
our goal is to evaluate the impact of using our: (i)
more powerful convolutional network for sentence
modeling; (ii) distributional representations of ques-
tions and answers in addition to the similarity score;
and (iii) approach to model matching words by aug-
menting id27s with additional dimen-
sions vs. providing the network with a pre-computed
feature vector of overlapping word counts as in (yu
et al., 2014).
3.3.1 distributional sentence models

table 2 summarises the results for

the set-
ting when the network is trained using only input
question-answer pairs without using any additional
features, i.e., we omit the word overlap features.

first, we report the results of our model when us-
ing only a similarity score xsim. it should be noted
that the network by yu et al. (2014), similarly to
ours, relies on a convolutional neural network to
learn intermediate representations. however, their
convolutional neural network operates only on uni-
gram or bigrams, while in our architecture we use a

table 2: results on trec qa using only similarity score
(sim) and also including the distributional representation (dist)
of question and answer sentences.

table 3: results on trec qa when augmenting the deep
learning model with relational information about overlapping
words.

train

sim
map
.5884
mrr .6036

dist
.6258
.6591

train-all
sim
dist
.6709
.6521
.7010
.7280

train

fvec
map
.7275
mrr .7796

emb.
.7325
.8018

train-all
fvec
emb.
.7654
.7459
.8078
.8186

larger width of the convolution    lter, thus allowing
for capturing longer range dependencies.

additionally, along with the question-answer sim-
ilarity score from eq. 2, our architecture includes
intermediate representations of the question and the
answer xq and xa into the    nal vector representation
xjoin, which together constitute a much richer repre-
sentation for computing the    nal score. we call this
network simply id98.
3.3.2 relational models

yu et al. (2014) shows that combining the output
of their deep learning system with a simple feature
vector that includes word overlap counts in a logis-
tic regression model, provides a signi   cant boost in
accuracy and yields new state-of-the-art results.

table 3 provides the results when we include
the information about overlapping words in two
modes: (i) feature vector (fvec) mode     when we in-
clude overlapping word counts replicating (yu et al.,
2014), which is represented by a feature vector xfeat
that is plugged into the    nal representation xjoin (see
fig. 2); and (ii) embeddings mode     when we aug-
ment the representation of input words with addi-
tional word overlap indicator features (as described
in sec. 2.1.1)3. first, we note that the results are
signi   cantly better than in table 2 when no overlap
information is used. adding word overlap informa-
tion in the form of a feature vector xfeat results in a
considerable generalization improvement of the net-
work. as argued by yu et al. (2014), distributional
id27s have certain shortcomings espe-
cially when dealing with proper nouns and cardinal
numbers, which are frequent in factoid questions.

in contrast, our approach to encode the rela-
tional information about overlapping words in a pair
(embeddings) directly into id27s shows
even larger improvement on train, achieving the

3a combined model using both fvec and embeddings modes
yielded the same performance as using the embeddings model.

table 4: survey of the results on the trec qa answer selec-
tion task (after score rescaling).

model
wang et al. (2007)
heilman and smith (2010)
wang and manning (2010)
yao et al. (2013)
severyn & moschitti (2013)
yih et al. (2013)
yu et al. (2014)
wang and ittycheriah (2015)
yin et al. (2015)
miao et al. (2015)
id98r on (train)
id98r on (train-all)

map mrr
.6852
.6029
.6917
.6091
.6951
.5951
.7477
.6307
.6781
.7358
.7700
.7092
.7846
.7113
.7740
.7063
.6951
.7633
.8117
.7339
.7660
.6857
.7186
.7828

best results on train-all with a map score of
76.54% and an mrr of 81.86%. we call this net-
work using relational information, id98r.

3.4 comparing with the state of the art
it should be noted that, to be consistent with the re-
sults of previous work on trec13, it is required to
evaluate our models in the same setting as (wang
et al., 2007; yih et al., 2014), i.e., we need to (i)
remove the questions having only correct or only in-
correct answer sentence candidates and (ii) use the
same evaluation script and the gold judgment    le as
they used. as pointed out by footnote 7 in (yih et
al., 2014), the evaluation script always considers 4
questions to be answered incorrectly thus penalizing
the overall score of the systems. this basically low-
ered the performance of id98r from an map and
an mrr of .7654 and .8186 to .7186 and .7828, re-
spectively. we used these numbers in table 4 for ex-
actly comparing with the results of previously pub-
lished systems. we note that our model is almost on
par with previous models. however, trec13 is too
small to assess the rank of our approach. therefore,

map mrr

p@1

state of the art

id98c (yang et al., 2015)
abid98 (yin et al., 2015)
lstma,c (miao et al., 2015)
nasmc (miao et al., 2015)

.6520
.6914
.6855
.6886

.6652
.7127
.7041
.7069

our models

id98
id98r

.6851
.7107
table 5: performance on the wikiqa dataset

.6661
.6951

n/a
n/a
n/a
n/a

.5401
.5720

we evaluated our best systems on wikiqa, which
being larger, enables a more reliable system com-
parison. tab. 5 reports the system performance on
wikiqa. it shows that our model reaches the accu-
racy of the best system, abid98, and outperforms
nasmc by miao et al. (2015), which was superior
to our models on trec13. finally, the difference
between id98 and id98r is again remarkable con-
   rming the bene   t of relational information.
4 related work
most of the previous work to tackle the answer sen-
tence selection task use various approaches to model
transformations of syntactic trees between a ques-
tion and its candidate answer sentence, e.g., wang
et al. (2007) use quasi-synchronous grammar, heil-
man & smith (2010) develop an improved tree edit
distance (ted) model, wang & manning (2010)
develop a probabilistic model to learn tree-edit op-
erations on dependency parse trees, while yao et
al. (2013) applies linear chain crfs with features
derived from ted. severyn and moschitti (2013)
applied id166 with tree kernels to shallow syntactic
representations. yih et al. (2013) use distributional
models based on lexical semantics to match seman-
tic relations of aligned words in qa pairs.

recently, deep learning approaches have been
successfully applied to various sentence classi   ca-
tion tasks, e.g., (kalchbrenner et al., 2014; kim,
2014), and for modelling text pairs, e.g. (lu and li,
2013; hu et al., 2014), where in the latter model
they use up to 3 convolution-pooling layers, while in
our experiments deeper architectures were severely
over   tting and we compensate our more shallow
sentence convnets by using a more powerful rela-
tional model.

additionally, a number of deep learning models
have been recently applied to id53,
e.g., yih et al. (2014) applied convolutional neural

networks to open-domain id53; bor-
des et al. (2014) propose a neural embedding model
combined with the knowledge base for open-domain
qa; iyyer et al. (2014) applied recursive neural net-
works to the factoid qa over paragraphs.

the work closest to ours is (yu et al., 2014),
where they present a deep learning architecture for
answer sentence selection. however, their sentence
model to map questions and answers to vectors op-
erates only on unigrams or bigrams. our sentence
model is based on a convolutional neural network
that uses a larger width of the convolution    lter, thus
allowing the network to capture longer range depen-
dencies. moreover, our architecture along with the
similarity score also encodes vector representations
of questions and answers used to compute the    -
nal score. hence, our model constructs and learns
a richer representation of the question-answer pairs,
which results in superior results on the answer sen-
tence selection dataset. moreover, we use a com-
pletely different way to encode relational informa-
tion about words that overlap in a pair. finally,
our deep learning model is trained end-to-end, while
in (yu et al., 2014) they use the output of their neural
network in a separate logistic scoring model.
5 conclusions and future work
in this paper, we propose a novel deep learning ar-
chitecture for answer sentence selection. our exper-
imental    ndings show that our model can achieve
the accuracy of state-of-the-art networks, which are
much more complex. this is largely due to our use
of more expressive models for the input question and
answer sentences, and our approach to inject rela-
tional information directly in the id27s.
however, our word overlap indicator features are
based on simple string matching, which is clearly a
very coarse way to model relatedness between words
in a question-answer pair.

recently, deep learning architectures have been
successfully applied to learn word alignments in ma-
chine translation, e.g., (yang et al., 2013). it sounds
promising to allow the network to learn to dynam-
ically align the related words in a question and its
answer. this in turn requires to maximize over the
latent alignment con   gurations, thus making the op-
timization problem highly non-convex. our prelim-
inary experiments show that a far larger number of

text pairs are required to train such architectures. we
leave it for the future work.

references
[agirre et al.2015] eneko agirre, carmen banea, claire
cardie, daniel cer, mona diab, aitor gonzalez-
agirre, weiwei guo, iigo lopez-gazpio, montse mar-
itxalar, rada mihalcea, german rigau, larraitz uria,
and janyce wiebe. 2015. semeval-2015 task 2: se-
mantic textual similarity, english, spanish and pilot
in proceedings of the 9th inter-
on interpretability.
national workshop on semantic evaluation (semeval
2015).

[bordes et al.2014] antoine bordes, jason weston, and
nicolas usunier. 2014. open id53 with
weakly supervised embedding models. in ecml.

[denil et al.2014] misha denil, alban demiraj, nal
kalchbrenner, phil blunsom, and nando de freitas.
2014. modelling, visualising and summarising doc-
uments with a single convolutional neural network.
technical report, university of oxford.

[echihabi and marcu2003] abdessamad echihabi

and
daniel marcu. 2003. a noisy-channel approach to
id53. in acl.

[heilman and smith2010] michael heilman and noah a.
smith. 2010. tree edit models for recognizing textual
entailments, paraphrases, and answers to questions. in
naacl.

[hu et al.2014] baotian hu, zhengdong lu, hang li, and
qingcai chen. 2014. convolutional neural network
architectures for matching natural language sentences.
in nips.

[iyyer et al.2014] mohit

jordan boyd-graber,
leonardo claudino, richard socher, and hal daum  e
iii.
2014. a neural network for factoid question
answering over paragraphs. in emnlp.

iyyer,

[kalchbrenner et al.2014] nal kalchbrenner,

edward
grefenstette, and phil blunsom.
2014. a convo-
lutional neural network for modelling sentences.
acl.

[kim2014] yoon kim. 2014. convolutional neural net-
in emnlp, pages

works for sentence classi   cation.
1746   1751, doha, qatar, october.

[lu and li2013] zhengdong lu and hang li. 2013. a

deep architecture for matching short texts. in nips.

[miao et al.2015] yishu miao, lei yu, and phil blunsom.
2015. neural variational id136 for text processing.
arxiv preprint arxiv:1511.06038.

[mikolov et al.2013] tomas mikolov, ilya sutskever, kai
chen, greg s corrado, and jeff dean. 2013. dis-
tributed representations of words and phrases and their

compositionality. in advances in neural information
processing systems 26, pages 3111   3119.

[nair and hinton2010] vinod nair and geoffrey e. hin-
ton. 2010. recti   ed linear units improve restricted
id82s. in icml.

[severyn and moschitti2013] aliaksei

alessandro moschitti.
engineering for answer selection and extraction.
emnlp.

and
2013. automatic feature
in

severyn

severyn

[severyn and moschitti2015] aliaksei

and
alessandro moschitti. 2015. learning to rank short
text pairs with convolutional deep neural networks.
in proceedings of the 38th international acm si-
gir conference on research and development in
information retrieval, pages 373   382. acm.

[wang and ittycheriah2015] zhiguo wang and abraham
ittycheriah. 2015. faq-based id53 via
word alignment. arxiv preprint arxiv:1507.02628.

[wang and manning2010] mengqiu wang and christo-
pher d. manning. 2010. probabilistic tree-edit models
with structured latent variables for id123
and question answer- ing. in acl.

[wang et al.2007] mengqiu wang, noah a. smith, and
teruko mitaura. 2007. what is the jeopardy model? a
quasi-synchronous grammar for qa. in emnlp.

[xuchen yao and callison-burch2013] peter

clark
xuchen yao, benjamin van durme and chris
callison-burch. 2013. answer extraction as sequence
tagging with tree id153. in naacl.

[yang et al.2013] nan yang, shujie liu, mu li, ming
zhou, and nenghai yu. 2013. word alignment mod-
eling with context dependent deep neural network. in
acl.

[yang et al.2015] yi yang, wen-tau yih, and christopher
meek. 2015. wikiqa: a challenge dataset for open-
in proceedings of the
domain id53.
2015 conference on empirical methods in natural
language processing, pages 2013   2018, lisbon, por-
tugal, september. association for computational lin-
guistics.

[yih et al.2013] wen-tau yih, ming-wei chang, christo-
pher meek, and andrzej pastusiak. 2013. question
answering using enhanced lexical semantic models. in
acl, august.

[yih et al.2014] wen-tau yih, xiaodong he, and christo-
pher meek. 2014. id29 for single-relation
id53. in acl.

[yin et al.2015] wenpeng yin, hinrich sch  utze, bing xi-
ang, and bowen zhou. 2015. abid98: attention-based
convolutional neural network for modeling sentence
pairs. arxiv preprint arxiv:1512.05193.

[yu et al.2014] lei yu, karl moritz hermann, phil blun-
som, and stephen pulman. 2014. deep learning for
answer sentence selection. corr.

[zeiler2012] matthew d. zeiler. 2012. adadelta: an

adaptive learning rate method. corr.

