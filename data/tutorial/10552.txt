neural id12 for sequence classi   cation:

analysis and application to

key term extraction and dialogue act detection

sheng-syun shen hung-yi lee

graduate institute of communication engineering

national taiwan university

r03942071@ntu.edu.tw, hungyilee@ntu.edu.tw

6
1
0
2

 
r
a

 

m
1
3

 
 
]
l
c
.
s
c
[
 
 

1
v
7
7
0
0
0

.

4
0
6
1
:
v
i
x
r
a

abstract

recurrent neural network architectures combining with atten-
tion mechanism, or neural attention model, have shown promis-
ing performance recently for the tasks including speech recog-
nition, image id134, visual id53 and
machine translation. in this paper, neural attention model is ap-
plied on two sequence labeling tasks, dialogue act detection and
key term extraction. in the sequence labeling tasks, the model
input is a sequence, and the output is the label of the input se-
quence. the major dif   culty of sequence labeling is that when
the input sequence is long, it can include many noisy or irrel-
evant part. if the information in the whole sequence is treated
equally, the noisy or irrelevant part may degrade the classi   -
cation performance. the attention mechanism is helpful for
sequence classi   cation task because it is capable of highlight-
ing important part among the entire sequence for the classi   ca-
tion task. the experimental results show that with the attention
mechanism, discernible improvements were achieved in the se-
quence labeling task considered here. the roles of the attention
mechanism in the tasks are further analyzed and visualized in
this paper.
index terms: attention model, key term extraction, dialogue
act detection, long short-term memory (lstm)

1. introduction

recently, attention-mechanism has been incorporated with re-
current neural networks, and has shown signi   cant improve-
ment on a great variety of tasks. attention mechanism is
   rst introduced by bahdanau et al. [1] in the task of machine
translation. they proposed an recurrent neural network (id56)
[2,3] encoder-decoder model for end-to-end translation, and this
mechanism is intuitively designed in order to take care about
the positions of input elements according to previous output
result.
inspired by this work, chorowski et al. [4] then pro-
posed attention-based models for id103, which are
claimed to be robust to long inputs. kelvin xu et al. [5] and
huijuan xu et al. [6] also demonstrated how attention mech-
anism works while reading a picture. the above works iter-
atively process their input by selecting relevant content at ev-
ery step. attention-mechanism are also useful for tasks other
than sequence to sequence learning. memory neural net-
works (memnn) which are developed by weston et al. [7] and
sukhbaatar et al.
[8] can deal with id53 (qa)
task [7   9], and the attention-mechanism plays an important role
in the model.

in this paper, neural attention model is applied on se-
in a sequence classi   cation task,

quence classi   cation tasks.

the input of the model is a sequence, and the model output is
the class of the sequence. many common tasks can be for-
mulated as sequence classi   cation including speaker recogni-
tion [10], audio emotion classi   cation [11], spoken term detec-
tion (std) [12   14], dialogue act detection [15   17], key term
extraction [18   21], etc. one of the major dif   culties for se-
quence classi   cation is that when the input sequence is long, it
can include many noisy or irrelevant parts, and without tech-
niques to ignore these parts, they may degrade the classi   cation
performance. attention-mechanism shows the potential of au-
tomatically ignoring the unimportant parts in the entire input
sequence and highlighting the important parts [7   9]. this in-
spires us to explore the use of attention mechanism on sequence
classi   cation.

in this paper, we present a novel attention-mechanism long
short-term memory (lstm) [22, 23] network architecture for
sequence classi   cation, in which the lstm network reads the
entire input, attention-mechanism highlights the important ele-
ments, and the sequence classes are predicted by the highlighted
parts. this model is    rst tested on dialogue act detection in
which the model input is the transcriptions of one to several ut-
terances, and the output is the dialogue acts. it is shown that
the attention-mechanism is especially helpful with longer input.
we further formulate the key term extraction as sequence classi-
   cation task [18], and apply the proposed model. this method-
ology shows promising results on key term extraction. finally,
visualization and analysis are also performed to understand how
the attention process works.

2. neural attention model for

sequence classi   cation

the overall structure of the proposed method is in figure 1.
the inputs of model would be represented as a dense sequence
vector ot , which will be described in section 2.1. with the
sequence vector, attention mechanism is then applied to extract
related information from input sequence in section 2.2. in sec-
tion 2.3, the model will predict target according to the selected
feature vectors.

2.1. sequence representation
we use recurrent neural networks (id56) for encoding. id56s
are capable of handling sequence information over time, so
they have demonstrated outstanding performance on natural
language understanding tasks [24   26] in recent years. we select
long short-term memory (lstm) networks, a type of recurrent
neural networks with a more complex computational unit, to

smoothing: the sharpening id172 method prefers
to mostly focus on only a single feature vector vi, and might
negatively affects the model   s performance. we then apply a
new way for the model to aggregate selections from multiple
top-scored frames. in this way, more input locations are con-
sidered for bringing more diversity to the model. we replace
the exponential function in equation (2) with logistic sigmoid
function    :

  (ei)(cid:80)t

i=1   (ei)

  i =

(3)

figure 1: architecture of the proposed neural attention model.

processes inputs sequentially. a brief introduction of lstms
can be found in [22, 23].

in the upper part of figure 1, we demonstrate the encoding
procedure to transform input sequences into    xed-length vector
representation ot . the set x = (x1, x2, . . . , xt ) denotes the
input sequence, where t is the sequence length. each element
in x represents a    xed-length feature vector. for example, it
might be a high dimensional 1-of-n encoding unigram vector
for the task of text classi   cation. in order to reduce the model
complexity, we set an embedding layer, a linear transformation
matrix, to turn the inputs into low dimensional dense vectors
v = (v1, v2, . . . , vt ), and then they will be sent to the lstm
encoder.
in each time step, the lstm takes one element vi
from feature vector set, and after processing the last element, it
then generates an output vector ot , which can be regarded as
the summaries of the preceding feature vectors.

2.2. attention mechanism
when input sequence x is long, the summaries vector ot is
likely to contain noisy information from many irrelevant fea-
ture vectors vi, we thus apply attention mechanism to select
only relevant frames among the entire sequence. the proce-
dures are shown in the lower part of figure 1. there is also an
embedding layer to transform input sequences into dense vec-
tors, and all the parameters in the embedding layer are shared
with the previous one. we then calculate the cosine similarity
between the sequence vector ot and id27 set v :

ei = ot (cid:12) vi,

(1)
where (cid:12) denotes cosine similarity between two vectors. as a
result, we have a list of score e = (e1, e2, . . . , et ). the at-
tention weights    = (  1,   2, . . . ,   t ) come from the normal-
ized score list e. due to some considerations, we normalize the
scores in two ways, which is inspired by chorowski et al. in [4]:
sharpening: the score list is normalized using sof tmax

activation function:

(cid:80)t

exp(ei)
i=1 exp(ei)

,

  i =

(2)

it has been widely used in many existing neural attention frame-
works [1, 5   7, 9], and is capable of solving the data noisy issue.

visualization and analysis of the both id172 functions
are provided in the experiment section.

2.3. target selection
the right part of figure 1 illustrates the target selection proce-

dures. we weighted sum all the feature vectors as(cid:80)   ivi, and

sending it to a fully connected layer. usually, the neurons in
this layer are activated by nonlinear functions. the last layer is
for target prediction, and the dimension is set to be candidate
target numbers.

3. experiments

we conducted two sequence classi   cation tasks in this section.
in section 3.1, we describe the de   nition of dialogue act detec-
tion, and also demonstrate the experimental results. in section
3.2, we introduce how to apply the proposed methodology on
key term extraction task. the role of attention mechanism dur-
ing classi   cation procedure will be discussed in section 3.3, and
we also show the visualization results.

3.1. dialogue act detection
dialogue act (da) detection [15   17] is about categorizing
the intention behind the speaker   s move in conversations, and
recognition of a speaker   s act may help reason the entire dia-
logue. this prediction task is still challenging because there are
various distinct ways of formulating an intention. in this work,
das are labeled with one of a number of tags. for example,
the tag <offer> is related to the situation that someone com-
mands partner to carry out actions, e.g.,    you need to give me
your ideas, and then i need to see whether that would sell in the
market place.   

3.1.1. experimental setup

we conducted experiments on switchboard dialog act (swda)
corpus [27], which is a corpus of telephone conversations on
selected topics. it consists of about 2,500 conversations by 500
speakers from the u.s. the conversations in the corpus are la-
beled with 43 unique dialogue act tags and split to 1,115 train
and 19 test conversations. the training and testing corpus re-
spectively contain 213,543 and 4,514 utterances, having aver-
age length of about 8 words.

3.1.2. baselines

we compared the proposed model with the following baselines.
support vector machines: id166 is the most common way
to be adopted for text classi   cation. silva et al. [28] chose sen-
tence unigrams as input feature vector, and trained the id166
model. we extracted one-of-n encoding unigram features for
every word in the dataset, aggregating them together for each
training example. to reduce the number of dimensions, we set

minimum word counts to 5. the radial basis function (rbf)
[29] kernel was also applied.

multiple layer id88: the work introduced by ries
et. al [30] is the    rst approach that importing arti   cial neu-
ral networks (ann) for dialogue act detection. we also ex-
tracted unigram features as the model input for experiments. we
trained an mlp model with 3 hidden layers. each hidden layer
has 512 neurons. the relu activation function was applied on
every hidden layer, and we set rmsprop as the optimizer. the
training epoch was set to be 20.

long short-term memory: in order to examine the use of
attention mechanism, we also implemented the original lstm
network. the lstm model takes one word from the input
sequence in each time step. we applied id27 for
unigram features, thus the high dimensional sparse vectors are
transformed into dense vectors. the embedding size was 400,
and we set the dimension of recurrent layers as 128 and the fully
connected layer before output as 500, respectively. to avoid
over   tting, we only trained the lstm network for 10 epochs.

3.1.3. experimental results
we implemented both sharpening-attend and smoothing-attend
neural attention model in the experiments. the lstm part of
the proposed model is the same as the original lstm brie   y
illustrated in the previous subsection, and the hyper-parameters
for model training was also the same. as the previous work
stated [31], context information from previous utterances may
help for the dialogue act prediction. therefore, we also ap-
pended n previous utterances to the the utterance being clas-
si   ed, and n was set to be 3 in the experiments.

the results are reported in table 1. rows (a) to (d) are the
baseline results, and the results of the proposed approaches are
in rows (e) to (h). it is clear that the id137 already
outperformed the other baselines (rows (c) vs (a), (b)) because
the id137 have better capability of handling sequence
information than multiple layer id88s and support vec-
tors. moreover, with context information the lstm can have
higher accuracy than the one without it (rows (d) vs (c)).

considering the case without context information, the pro-
posed approaches show improvements comparing to all the
baselines no matter the attention is sharpening or smoothing
(rows (e), (f) vs (a), (b), (c)). the neural attention model with
sharpening attention is only slightly better than the original
lstm (rows (e) v.s. (c)), but the smoothing attention shows sig-
ni   cant improvement (rows (f) v.s. (c)). besides, we also know
that the prediction of sequence classi   cation cannot just rely on
the most relevant element, the rest of the relevant part should
also be considered. neural attention model with sharpening at-
tention does not show any improvement after adding context
information into the prediction procedure (rows (g) v.s.
(e)).
this is because the sharpening-attend mechanism only focuses
on the most relevant part of the input sequence, adding more
candidates would not be helpful. on the other hand, when us-
ing smoothing attention, context information became very help-
ful (rows (h) v.s. (f)). this shows that smoothing attention can
better exploit the context information than sharpening attention.

3.2. key term extraction
the goal of key term extraction [18   21] is to automatically ex-
tract relevant terms from a given document. key terms may
possibly describe the core concept or summary of a document,
which can help users understand, organize, and extract impor-
tant information ef   ciently from documents. these terms are

table 1: swda dialogue act detection accuracies.

model
(a) support vector machine
(b) multiple layer id88
(c) long short-term memory
(d) lstm with context information
neural attention model
neural attention model
with context information

(e) sharpening
(f) smoothing
(g) sharpening
(h) smoothing

accuracy (%)

65.8
67.3
69.7
71.7
69.9
70.4
69.8
72.6

usually manually labeled by humans according to cognition and
domain knowledge, so automatic key term extraction is not an
easy task.

key term extraction can be regarded as a sequence classi-
   cation problem [18]. the model input is a document, while
the model selects some terms as key terms from a set of candi-
dates. each term in the set of candidate terms is considered as a
class, and the documents containing the same key terms belong
to the same class. in our task, chances are that some terms do
not exist in the document, but they represent the core concepts
of the document. these terms are also regarded as key terms
here, which makes this task even more dif   cult. it is possible
that a document has more than one key term, or a document can
belong to multiple classes. however, the number of key terms
in each testing document is unknown, as a result we consider
this task to be a ranking problem. that is, the model assigns
a score to each candidate term. then, the candidate terms are
ranked according to the scores. the target of the system is to
rank the key terms above the non key terms.

in training procedures, each document with n labeled key
terms would be mapped into a sparse vector, which is the proba-
bility training target. the dimension of this sparse vector is the
number of candidate terms. most of the values are zero, only
the indexes corresponding to labeled terms would be assigned
to a value 1
n , and the summation of this vector is 1. for exam-
ple, assuming we have 1,000 term candidates and the number
of labeled key terms is 4 in a document, we then have an 1,000-
dimension sparse target vector with only 4 elements all assigned
with 1
4 .

3.2.1. experimental setup
we collected the data from stack over   ow1 website where
serves as a platform for users to ask and answer questions.
while users of stack over   ow post questions on the forum, they
are asked to label 2~6 key terms for each post. the dataset we
collected includes 290,000 examples in total (250,000 for train-
ing and 40,000 for testing), and there are about 24,000 kinds of
labeled key term. each example contains a post and 2~6 key
term labels, and the average length of the article is about 120
words. the collected dataset is available for download. 2

in practice, to reduce the training complexity, we only se-
lected the 1,000 most frequent key terms in the training set as
candidates. these top 1,000 candidates cover over 76% of the
key term labels in the training set, so we can still expect to get
reasonable results.

1 http://stackoverflow.com/
2http://speech.ee.ntu.edu.tw/  sense/

stackoverflow_pack.zip

figure 2: attention-mechanism visualization for sequence classi   cation tasks. the darker color a word get, the higher attention weight
it has. figures (a) and (b) illustrate how attention weights work in dialogue act detection task. figures (c) and (d) represent the key
term extraction results. the texts in red represent the ground truth and texts in blue are our prediction results.

3.2.2. baselines
we implemented multiple layer id88s (mlp) and long
short-term memory (lstm) networks as the baseline models,
which have already been described in section 3.1.2.

tf-idf sorting is the baseline we also applied.    tf-idf    is
the abbreviation of term frequency-inverse document frequency.
it is a numerical statistic that is intended to re   ect how impor-
tant a word is to a document in a collection or corpus. a brief
introduction about how tf-idf sorting extracts key terms can be
found in [32]. we calculated the tf-idf values of a set of can-
didate key terms according to the dataset, and these candidates
were sorted by their values. we then reported the ranking list
for evaluation.

3.2.3. experimental results
to examine the prediction result, we chose map and p@r as
the evaluation methods. the map score for a set of documents
is the mean of the average precision scores for each document.
p@r is de   ned as the precision after r elements have been
selected by the system, where r is also the total number of
judged relevant results for the given inputs. precision is de   ned
as the portion of returned results that are truly belong to the
ground truth set.

the experimental results are demonstrated in table 2. row
(a) is the oracle score, which is for reference. since we only
selected 1,000 most frequent key terms as candidates from the
training set, we can   t achieve 100% accurate performance. the
score of baseline approaches we applied are in rows (b) to (d),
and rows (e), (f) are the performance of the proposed neural at-

table 2: the evaluation result for key term extraction.

model
(a) oracle
(b) tf-idf sorting
(c) multiple layer id88
(d) long short-term memory

neural attention model

(e) sharpening
(f) smoothing

map (%)

p@r (%)

77.2

9.9
33.1
43.1
39.3
50.5

8.9
29.7
40.2
36.2
46.4

tention model. the supervised learning baselines outperformed
the tf-idf sorting baseline (rows (c), (d) vs (b)). that is because
without supervised learning, we may not    t the dataset, and we
also can   t predict the key terms which do not exist in the doc-
ument. besides, like the experiment we previously conducted,
lstm shows better ability of handling sequence information
in comparison to original neural networks (rows (d) vs (c)), so
the lstm network performs better while using both map and
p@r as evaluation methods. we found that the performance
of our neural attention model with sharpening-attend mecha-
nism degraded while comparing to the original lstm (rows (e)
vs (d), but the one with smoothing attention outperformed all
the other approaches (rows (f) vs (b), (c), (d), (e)). this result
proved that adding more relevant elements into consideration
can help solving sequence classi   cation problems.

3.3. visualization and analysis
figure 2 demonstrates the visualization of how attention-
mechanism works in the sequence classi   cation tasks. the up-
per row is for dialogue act detection and the lower row is for key
term extraction. the darker the color, the higher the weights.
we only chose the smoothing-attend mechanism for visualiza-
tion due to its better performance. according to this    gure, we
found that attention weights are capable of reducing sentence
dis   uency problems and    ltering out most of the unimportant
elements such as function words.

4. conclusions

in this paper, we proposed a neural attention model for sequence
classi   cation.
in such kinds of task, the input of model is a
sequence, and the output is the class of sequence. the major
dif   culty is that when the input sequence is long, the noisy or
irrelevant part may degrade the classi   cation performance. the
proposed model can reduce the in   uences because it is able to
highlight important part among the entire sequence. in the ex-
periments, the neural attention model can achieve 72.6% ac-
curacy for dialogue act detection task and 50.5% map score
for key term extraction task, which shows discernible improve-
ments comparing to the other approaches.

[20] hiroshi nakagawa and tatsunori mori. a simple but powerful
automatic term extraction method. in coling-02 on comput-
erm 2002: second international workshop on computational
terminology-volume 14, pages 1   7. association for computa-
tional linguistics, 2002.

[21] yun-nung chen, wei yu wang, and alexander i rudnicky. an
empirical investigation of sparse id148 for improved
dialogue act classi   cation. in acoustics, speech and signal pro-
cessing (icassp), 2013 ieee international conference on, pages
8317   8321. ieee, 2013.

[22] felix a gers and j  urgen schmidhuber. lstm recurrent networks
learn simple context-free and context-sensitive languages. neural
networks, ieee transactions on, 12(6):1333   1340, 2001.

[23] sepp hochreiter and j  urgen schmidhuber. long short-term mem-

ory. neural computation, 9(8):1735   1780, 1997.

[24] kaisheng yao, baolin peng, yu zhang, dong yu, geoffrey zweig,
and yangyang shi. spoken language understanding using long
short-term memory neural networks. in spoken language tech-
nology workshop (slt), 2014 ieee, pages 189   194. ieee, 2014.
[25] kaisheng yao, geoffrey zweig, mei-yuh hwang, yangyang shi,
and dong yu. recurrent neural networks for language under-
standing. in interspeech, pages 2524   2528, 2013.

[26] gr  egoire mesnil, yann dauphin, kaisheng yao, yoshua bengio,
li deng, dilek hakkani-tur, xiaodong he, larry heck, gokhan
tur, dong yu, et al. using recurrent neural networks for slot    ll-
ing in spoken language understanding. audio, speech, and lan-
guage processing, ieee/acm transactions on, 23(3):530   539,
2015.

[27] dan jurafsky, elizabeth shriberg, and debra biasca. switchboard
swbd-damsl shallow-discourse-function annotation coders man-
institute of cognitive science technical report, pages 97   
ual.
102, 1997.

[28] joao silva, lu    sa coheur, ana cristina mendes, and andreas
wichert. from symbolic to sub-symbolic information in ques-
tion classi   cation. arti   cial intelligence review, 35(2):137   154,
2011.

[29] yin-wen chang, cho-jui hsieh, kai-wei chang, michael ring-
gaard, and chih-jen lin. training and testing low-degree poly-
nomial data mappings via linear id166. the journal of machine
learning research, 11:1471   1490, 2010.

[30] klaus ries. id48 and neural network based speech act detection.
in acoustics, speech, and signal processing, 1999. proceedings.,
1999 ieee international conference on, volume 1, pages 497   
500. ieee, 1999.

[31] eug  enio ribeiro, ricardo ribeiro, and david martins de matos.
arxiv

the in   uence of context on dialogue act recognition.
preprint arxiv:1506.00839, 2015.
[32] http://stevenloria.com/

finding-important-words-in-a-document-using-tf-idf/.

5. references

[1] dzmitry bahdanau, kyunghyun cho, and yoshua bengio. neu-
ral machine translation by jointly learning to align and translate.
arxiv preprint arxiv:1409.0473, 2014.

[2] jeffrey l elman. finding structure in time. cognitive science,

14(2):179   211, 1990.

[3] michael i jordan. serial order: a parallel distributed processing

approach. advances in psychology, 121:471   495, 1997.

[4] jan k chorowski, dzmitry bahdanau, dmitriy serdyuk,
kyunghyun cho, and yoshua bengio. attention-based models
for id103. in advances in neural information pro-
cessing systems, pages 577   585, 2015.

[5] kelvin xu, jimmy ba, ryan kiros, aaron courville, ruslan
salakhutdinov, richard zemel, and yoshua bengio. show, attend
and tell: neural image id134 with visual attention.
arxiv preprint arxiv:1502.03044, 2015.

[6] huijuan xu and kate saenko. ask, attend and answer: exploring
question-guided spatial attention for visual id53.
arxiv preprint arxiv:1511.05234, 2015.

[7] jason weston, sumit chopra, and antoine bordes. memory net-

works. arxiv preprint arxiv:1410.3916, 2014.

[8] sainbayar sukhbaatar, jason weston, rob fergus, et al. end-to-
end memory networks. in advances in neural information pro-
cessing systems, pages 2431   2439, 2015.

[9] wei-ning hsu, yu zhang, and james glass. recurrent neural net-
work encoder with attention for community id53,
2016.

[10] najim dehak, reda dehak, patrick kenny, niko brummer, pierre
ouellet, and pierre dumouchel. support vector machines ver-
sus fast scoring in the low-dimensional total variability space for
speaker veri   cation. in interspeech, 2009.

[11] bjorn schuller, stefan steidl, and anton batliner. the inter-

speech 2009 emotion challenge. in interspeech, 2009.

[12] hung-yi lee and lin-shan lee. enhanced spoken term detection
using support vector machines and weighted pseudo examples.
audio, speech, and language processing, ieee transactions on,
21(6):1272   1284, 2013.

[13] i.-f. chen and c.-h. lee. a hybrid id48/dnn approach to key-

word spotting of short words. in interspeech, 2013.

[14] a. norouzian, a. jansen, r. rose, and s. thomas. exploiting
discriminative point process models for spoken term detection. in
interspeech, 2012.

[15] max m louwerse and scott a crossley. dialog act classi   cation
using id165 algorithms. in flairs conference, pages 758   763,
2006.

[16] kristy elizabeth boyer, joseph f grafsgaard, eun young ha,
robert phillips, and james c lester. an affect-enriched dialogue
act classi   cation model for task-oriented dialogue. in proceed-
ings of the 49th annual meeting of the association for com-
putational linguistics: human language technologies-volume
1, pages 1190   1199. association for computational linguistics,
2011.

[17] dinoj surendran and gina-anne levow. dialog act tagging with
support vector machines and id48. in inter-
speech, 2006.

[18] kamal sarkar, mita nasipuri, and suranjan ghose. a new ap-
arxiv

proach to keyphrase extraction using neural networks.
preprint arxiv:1004.3274, 2010.

[19] yun-nung chen, yu huang, sheng-yi kong, and lin-shan lee.
automatic key term extraction from spoken course lectures using
branching id178 and prosodic/semantic features. in spoken lan-
guage technology workshop (slt), 2010 ieee, pages 265   270.
ieee, 2010.

