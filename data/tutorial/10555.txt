5
1
0
2

 

v
o
n
3

 

 
 
]

g
l
.
s
c
[
 
 

2
v
7
8
3
0
0

.

5
0
5
1
:
v
i
x
r
a

id199

rupesh kumar srivastava
klaus greff
j  urgen schmidhuber

rupesh@idsia.ch
klaus@idsia.ch
juergen@idsia.ch

the swiss ai lab idsia

istituto dalle molle di studi sull   intelligenza arti   ciale

universit`a della svizzera italiana (usi)

scuola universitaria professionale della svizzera italiana (supsi)

galleria 2, 6928 manno-lugano, switzerland

abstract

there is plenty of theoretical and empirical evi-
dence that depth of neural networks is a crucial
ingredient for their success. however, network
training becomes more dif   cult with increasing
depth and training of very deep networks remains
an open problem. in this extended abstract, we
introduce a new architecture designed to ease
gradient-based training of very deep networks.
we refer to networks with this architecture as
id199, since they allow unimpeded
information    ow across several layers on infor-
mation highways. the architecture is character-
ized by the use of gating units which learn to reg-
ulate the    ow of information through a network.
id199 with hundreds of layers can
be trained directly using stochastic gradient de-
scent and with a variety of id180,
opening up the possibility of studying extremely
deep and ef   cient architectures.

note: a full paper extending this study is available at
http://arxiv.org/abs/1507.06228, with addi-
tional references, experiments and analysis.

1. introduction
many recent empirical breakthroughs in supervised ma-
chine learning have been achieved through the applica-
tion of deep neural networks. network depth (referring to
the number of successive computation layers) has played
perhaps the most important role in these successes. for

presented at the deep learning workshop, international confer-
ence on machine learning, lille, france, 2015. copyright 2015
by the author(s).

instance, the top-5 image classi   cation accuracy on the
1000-class id163 dataset has increased from    84%
(krizhevsky et al., 2012) to    95% (szegedy et al., 2014;
simonyan & zisserman, 2014) through the use of ensem-
bles of deeper architectures and smaller receptive    elds
(ciresan et al., 2011a;b; 2012) in just a few years.
on the theoretical side, it is well known that deep net-
works can represent certain function classes exponentially
more ef   ciently than shallow ones (e.g. the work of h  astad
(1987); h  astad & goldmann (1991) and recently of mont-
ufar et al. (2014)). as argued by bengio et al. (2013), the
use of deep networks can offer both computational and sta-
tistical ef   ciency for complex tasks.
however, training deeper networks is not as straightfor-
ward as simply adding layers. optimization of deep net-
works has proven to be considerably more dif   cult, lead-
ing to research on initialization schemes (glorot & ben-
gio, 2010; saxe et al., 2013; he et al., 2015), techniques
of training networks in multiple stages (simonyan & zis-
serman, 2014; romero et al., 2014) or with temporary
companion id168s attached to some of the layers
(szegedy et al., 2014; lee et al., 2015).
in this extended abstract, we present a novel architecture
that enables the optimization of networks with virtually ar-
bitrary depth. this is accomplished through the use of a
learned gating mechanism for regulating information    ow
which is inspired by long short term memory recurrent
neural networks (hochreiter & schmidhuber, 1995). due
to this gating mechanism, a neural network can have paths
along which information can    ow across several layers
without attenuation. we call such paths information high-
ways, and such networks id199.
in preliminary experiments, we found that highway net-
works as deep as 900 layers can be optimized using simple
stochastic id119 (sgd) with momentum. for

id199

up to 100 layers we compare their training behavior to that
of traditional networks with normalized initialization (glo-
rot & bengio, 2010; he et al., 2015). we show that opti-
mization of id199 is virtually independent of
depth, while for traditional networks it suffers signi   cantly
as the number of layers increases. we also show that archi-
tectures comparable to those recently presented by romero
et al. (2014) can be directly trained to obtain similar test
set accuracy on the cifar-10 dataset without the need for
a pre-trained teacher network.

1.1. notation

we use boldface letters for vectors and matrices, and ital-
icized capital letters to denote transformation functions. 0
and 1 denote vectors of zeros and ones respectively, and i
denotes an identity matrix. the function   (x) is de   ned as
  (x) = 1

1+e   x , x     r.

2. id199
a plain feedforward neural network typically consists of l
layers where the lth layer (l     {1, 2, ..., l}) applies a non-
linear transform h (parameterized by wh,l) on its input
xl to produce its output yl. thus, x1 is the input to the
network and yl is the network   s output. omitting the layer
index and biases for clarity,

y = h(x, wh).

(1)

h is usually an af   ne transform followed by a non-linear
activation function, but in general it may take other forms.
for a highway network, we additionally de   ne two non-
linear transforms t (x, wt) and c(x, wc) such that

y = h(x, wh)   t (x, wt) + x    c(x, wc).

(2)

we refer to t as the transform gate and c as the carry gate,
since they express how much of the output is produced by
transforming the input and carrying it, respectively. for
simplicity, in this paper we set c = 1     t , giving

y = h(x, wh)   t (x, wt) + x    (1     t (x, wt)). (3)

the dimensionality of x, y, h(x, wh) and t (x, wt)
must be the same for equation (3) to be valid. note that
this re-parametrization of the layer transformation is much
more    exible than equation (1). in particular, observe that

y =

x,
h(x, wh),

if t (x, wt) = 0,
if t (x, wt) = 1.

(4)

(cid:40)

(cid:40)

similarly, for the jacobian of the layer transform,

dy
dx

=

i,
h(cid:48)(x, wh),

if t (x, wt) = 0,
if t (x, wt) = 1.

(5)

thus, depending on the output of the transform gates, a
highway layer can smoothly vary its behavior between that
of a plain layer and that of a layer which simply passes
its inputs through. just as a plain layer consists of multi-
ple computing units such that the ith unit computes yi =
hi(x), a highway network consists of multiple blocks such
that the ith block computes a block state hi(x) and trans-
form gate output ti(x). finally, it produces the block out-
put yi = hi(x)     ti(x) + xi     (1     ti(x)), which is con-
nected to the next layer.

2.1. constructing id199

as mentioned earlier, equation (3) requires that the dimen-
sionality of x, y, h(x, wh) and t (x, wt) be the same.
in cases when it is desirable to change the size of the rep-
resentation, one can replace x with   x obtained by suitably
sub-sampling or zero-padding x. another alternative is to
use a plain layer (without highways) to change dimension-
ality and then continue with stacking highway layers. this
is the alternative we use in this study.
convolutional highway layers are constructed similar to
fully connected layers. weight-sharing and local receptive
   elds are utilized for both h and t transforms. we use
zero-padding to ensure that the block state and transform
gate feature maps are the same size as the input.

2.2. training deep id199

for plain deep networks, training with sgd stalls at the
beginning unless a speci   c weight initialization scheme is
used such that the variance of the signals during forward
and backward propagation is preserved initially (glorot &
bengio, 2010; he et al., 2015). this initialization depends
on the exact functional form of h.
for highway layers, we use the transform gate de   ned as
t x + bt), where wt is the weight matrix
t (x) =   (wt
and bt the bias vector for the transform gates. this sug-
gests a simple initialization scheme which is independent
of the nature of h: bt can be initialized with a negative
value (e.g. -1, -3 etc.) such that the network is initially
biased towards carry behavior. this scheme is strongly in-
spired by the proposal of gers et al. (1999) to initially bias
the gates in a long short-term memory recurrent network
to help bridge long-term temporal dependencies early in
learning. note that   (x)     (0, 1),   x     r, so the condi-
tions in equation (4) can never be exactly true.
in our experiments, we found that a negative bias initial-

id199

ization was suf   cient for learning to proceed in very deep
networks for various zero-mean initial distributions of wh
and different id180 used by h. this is sig-
ni   cant property since in general it may not be possible to
   nd effective initialization schemes for many choices of h.

3. experiments
3.1. optimization

very deep plain networks become dif   cult to optimize even
if using the variance-preserving initialization scheme form
(he et al., 2015). to show that id199 do not
suffer from depth in the same way we train run a series
of experiments on the mnist digit classi   cation dataset.
we measure the cross id178 error on the training set, to
investigate optimization, without con   ating them with gen-
eralization issues.
we train both plain networks and id199 with
the same architecture and varying depth. the    rst layer is
always a regular fully-connected layer followed by 9, 19,
49, or 99 fully-connected plain or highway layers and a
single softmax output layer. the number of units in each
layer is kept constant and it is 50 for highways and 71
for plain networks. that way the number of parameters
is roughly the same for both. to make the comparison fair
we run a random search of 40 runs for both plain and high-
way networks to    nd good settings for the hyperparame-
ters. we optimized the initial learning rate, momentum,
learning rate decay rate, activation function for h (either
relu or tanh) and, for id199, the value for
the transform gate bias (between -1 and -10). all other
weights were initialized following the scheme introduced
by (he et al., 2015).
the convergence plots for the best performing networks for
each depth can be seen in figure 1. while for 10 layers
plain network show very good performance, their perfor-
mance signi   cantly degrades as depth increases. highway
networks on the other hand do not seem to suffer from an
increase in depth at all. the    nal result of the 100 layer
highway network is about 1 order of magnitude better than
the 10 layer one, and is on par with the 10 layer plain net-
work. in fact, we started training a similar 900 layer high-
way network on cifar-100 which is only at 80 epochs
as of now, but so far has shown no signs of optimization
dif   culties. it is also worth pointing out that the highway
networks always converge signi   cantly faster than the plain
ones.

3.2. comparison to fitnets

deep id199 are easy to optimize, but are they
also bene   cial for supervised learning where we are in-
terested in generalization performance on a test set? to

address this question, we compared id199 to
the thin and deep architectures termed fitnets proposed re-
cently by romero et al. (2014) on the cifar-10 dataset
augmented with random translations. results are summa-
rized in table 1.
romero et al. (2014) reported that training using plain
backpropogation was only possible for maxout networks
with depth up to 5 layers when number of parameters was
limited to    250k and number of multiplications to    30m.
training of deeper networks was only possible through the
use of a two-stage training procedure and addition of soft
targets produced from a pre-trained shallow teacher net-
work (hint-based training). similarly it was only possible
to train 19-layer networks with a budget of 2.5m parame-
ters using hint-based training.
we found that it was easy to train id199 with
number of parameters and operations comparable to    t-
nets directly using id26. as shown in table 1,
highway 1 and highway 4, which are based on the archi-
tecture of fitnet 1 and fitnet 4 respectively obtain similar
or higher accuracy on the test set. we were also able to
train thinner and deeper networks: a 19-layer highway net-
work with    1.4m parameters and a 32-layer highway net-
work with    1.25m parameter both perform similar to the
teacher network of romero et al. (2014).

4. analysis
in figure 2 we show some inspections on the inner work-
ings of the best1 50 hidden layer fully-connected high-
way networks trained on mnist (top row) and cifar-
100 (bottom row). the    rst three columns show, for each
transform gate, the bias, the mean activity over 10k ran-
dom samples, and the activity for a single random sample
respectively. the block outputs for the same single sample
are displayed in the last column.
the transform gate biases of the two networks were initial-
ized to -2 and -4 respectively. it is interesting to note that
contrary to our expectations most biases actually decreased
further during training. for the cifar-100 network the bi-
ases increase with depth forming a gradient. curiously this
gradient is inversely correlated with the average activity of
the transform gates as seen in the second column. this in-
dicates that the strong negative biases at low depths are not
used to shut down the gates, but to make them more selec-
tive. this behavior is also suggested by the fact that the
transform gate activity for a single example (column 3) is
very sparse. this effect is more pronounced for the cifar-
100 network, but can also be observed to a lesser extent in
the mnist network.

1obtained via random search over hyperparameters to mini-
mize the best training set error achieved using each con   guration

id199

figure 1. comparison of optimization of plain networks and id199 of various depths. all networks were optimized using
sgd with momentum. the curves shown are for the best hyperparameter settings obtained for each con   guration using a random
search. plain networks become much harder to optimize with increasing depth, while id199 with up to 100 layers can still
be optimized well.

network
fitnet results reported by romero et al. (2014)

teacher
fitnet 1
fitnet 2
fitnet 3
fitnet 4

id199

highway 1 (fitnet 1)
highway 2 (fitnet 4)
highway 3*
highway 4*

number of layers number of parameters accuracy

5
11
11
13
19

11
19
19
32

   9m
   250k
   862k
   1.6m
   2.5m
   236k
   2.3m
   1.4m
   1.25m

90.18%
89.01%
91.06%
91.10%
91.61%

89.18%
92.24%
90.68%
90.34%

table 1. cifar-10 test set accuracy of convolutional id199 with recti   ed linear activation and sigmoid gates. for compar-
ison, results reported by romero et al. (2014) using maxout networks are also shown. fitnets were trained using a two step training
procedure using soft targets from the trained teacher network, which was trained using id26. we trained all highway net-
works directly using id26. * indicates networks which were trained only on a set of 40k out of 50k examples in the training
set.

the last column of figure 2 displays the block outputs and
clearly visualizes the concept of    information highways   .
most of the outputs stay constant over many layers form-
ing a pattern of stripes. most of the change in outputs hap-
pens in the early layers (    10 for mnist and     30 for
cifar-100). we hypothesize that this difference is due to
the higher complexity of the cifar-100 dataset.
in summary it is clear that id199 actually uti-
lize the gating mechanism to pass information almost un-
changed through many layers. this mechanism serves not
just as a means for easier training, but is also heavily used
to route information in a trained network. we observe very
selective activity of the transform gates, varying strongly in
reaction to the current input patterns.

5. conclusion
learning to route information through neural networks has
helped to scale up their application to challenging prob-
lems by improving credit assignment and making training
easier (srivastava et al., 2015). even so, training very deep
networks has remained dif   cult, especially without consid-
erably increasing total network size.
id199 are novel neural network architectures
which enable the training of extremely deep networks us-
ing simple sgd. while the traditional plain neural archi-
tectures become increasingly dif   cult to train with increas-
ing network depth (even with variance-preserving initial-
ization), our experiments show that optimization of high-
way networks is not hampered even as network depth in-
creases to a hundred layers.
the ability to train extremely deep networks opens up the
possibility of studying the impact of depth on complex

050100150200250300350400number of epochs10-510-410-310-210-1100mean cross id178 errordepth 10050100150200250300350400number of epochsdepth 20050100150200250300350400number of epochsdepth 50050100150200250300350400number of epochsdepth 100plainhighwayid199

figure 2. visualization of certain internals of the blocks in the best 50 hidden layer id199 trained on mnist (top row) and
cifar-100 (bottom row). the    rst hidden layer is a plain layer which changes the dimensionality of the representation to 50. each of
the 49 highway layers (y-axis) consists of 50 blocks (x-axis). the    rst column shows the transform gate biases, which were initialized
to -2 and -4 respectively. in the second column the mean output of the transform gate over 10,000 training examples is depicted. the
third and forth columns show the output of the transform gates and the block outputs for a single random training sample.

problems without restrictions. various id180
which may be more suitable for particular problems but for
which robust initialization schemes are unavailable can be
used in deep id199. future work will also at-
tempt to improve the understanding of learning in highway
networks.

acknowledgments
this research was supported by the by eu project
   nascence    (fp7-ict-317662). we gratefully ac-
knowledge the support of nvidia corporation with the
donation of the tesla k40 gpus used for this research.

references
bengio, yoshua, courville, aaron, and vincent, pas-
representation learning: a review and new
pattern analysis and machine in-
ieee transactions on, 35(8):1798   1828,
url http://ieeexplore.ieee.org/

cal.
perspectives.
telligence,
2013.
xpls/abs_all.jsp?arnumber=6472238.

   c sign classi   cation. in neural networks (ijid98), the
2011 international joint conference on, pp. 1918   1921.
ieee, 2011a. url http://ieeexplore.ieee.
org/xpls/abs_all.jsp?arnumber=6033458.

ciresan, dan, meier, ueli, and schmidhuber, j  urgen.
multi-column deep neural networks for image classi   -
in ieee conference on id161 and
cation.
pattern recognition, 2012.

ciresan, dc, meier, ueli, masci, jonathan, gambardella,
luca m, and schmidhuber, j  urgen.
flexible, high
performance convolutional neural networks for image
classi   cation. in ijcai, 2011b. url http://www.
aaai.org/ocs/index.php/ijcai/ijcai11/
paper/download/3098/3425%0020http:
//dl.acm.org/citation.cfm?id=2283603.

gers, felix a., schmidhuber, j  urgen, and cummins,
learning to forget: continual prediction
in icann, volume 2, pp. 850   855,
url http://ieeexplore.ieee.org/

fred.
with lstm.
1999.
xpls/abs_all.jsp?arnumber=818041.

ciresan, dan, meier, ueli, masci, jonathan, and schmid-
huber, j  urgen. a committee of neural networks for traf-

glorot, xavier and bengio, yoshua. understanding the
dif   culty of training deep feedforward neural networks.

010203040mnistdepthtransform gate biasesmean transform gate outputstransform gate outputsblock outputs010203040block010203040cifar-100depth010203040block010203040block010203040block2.62.52.42.32.22.12.01.90.000.040.080.120.160.200.240.280.320.360.400.00.10.20.30.40.50.60.70.80.91.01.00.80.60.40.20.00.20.40.60.81.06.05.65.24.84.44.03.63.22.80.000.040.080.120.160.200.240.280.320.360.400.00.10.20.30.40.50.60.70.80.91.01.00.80.60.40.20.00.20.40.60.81.0id199

in international conference on arti   cial intelligence
and statistics, pp. 249   256, 2010.
url http:
//machinelearning.wustl.edu/mlpapers/
paper_files/aistats2010_glorotb10.pdf.

simonyan, karen and zisserman, andrew. very deep con-
volutional networks for large-scale image recognition.
arxiv:1409.1556 [cs], september 2014. url http:
//arxiv.org/abs/1409.1556.

srivastava, rupesh kumar, masci, jonathan, gomez,
faustino, and schmidhuber, j  urgen. understanding lo-
in international confer-
cally competitive networks.
ence on learning representations, 2015. url http:
//arxiv.org/abs/1410.1165.

szegedy, christian, liu, wei, jia, yangqing, sermanet,
pierre, reed, scott, anguelov, dragomir, erhan, du-
mitru, vanhoucke, vincent, and rabinovich, andrew.
going deeper with convolutions. arxiv:1409.4842 [cs],
september 2014. url http://arxiv.org/abs/
1409.4842.

h  astad, johan. computational limitations of small-depth
circuits. mit press, 1987. url http://dl.acm.
org/citation.cfm?id=series9056.27031.

h  astad,

johan and goldmann, mikael.

power of small-depth threshold circuits.
tational complexity, 1(2):113   129, 1991.
http://link.springer.com/article/10.
1007/bf01272517.

on the
compu-
url

he, kaiming, zhang, xiangyu, ren, shaoqing, and
sun, jian. delving deep into recti   ers: surpassing
human-level performance on id163 classi   cation.
arxiv:1502.01852 [cs], february 2015. url http:
//arxiv.org/abs/1502.01852.

hochreiter, sepp and schmidhuber,

long
short term memory.
technical report fki-207-95,
technische universit  at m  unchen, m  unchen, august
1995. url http://citeseerx.ist.psu.edu/
viewdoc/summary?doi=10.1.1.51.3117.

j  urgen.

krizhevsky, alex, sutskever, ilya, and hinton, geoffrey e.
id163 classi   cation with deep convolutional neural
networks. in advances in neural information process-
ing systems, 2012. url http://books.nips.cc/
papers/files/nips25/nips2012_0534.pdf.

lee, chen-yu, xie, saining, gallagher, patrick, zhang,
zhengyou, and tu, zhuowen. deeply-supervised nets.
pp. 562   570, 2015.
url http://jmlr.org/
proceedings/papers/v38/lee15a.html.

montufar, guido f, pascanu, razvan, cho, kyunghyun,
and bengio, yoshua. on the number of linear re-
in advances in
gions of deep neural networks.
neural information processing systems. 2014. url
http://papers.nips.cc/paper/5422-
on-the-number-of-linear-regions-of-
deep-neural-networks.pdf.

romero,

ballas,

nicolas,

adriana,

kahou,
samira ebrahimi, chassang, antoine, gatta, carlo,
and bengio, yoshua.
fitnets: hints for thin deep
arxiv:1412.6550 [cs], december 2014. url
nets.
http://arxiv.org/abs/1412.6550.

saxe, andrew m., mcclelland, james l., and gan-
guli, surya.
exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks.
arxiv:1312.6120 [cond-mat, q-bio, stat], december
2013.
url http://arxiv.org/abs/1312.
6120.

