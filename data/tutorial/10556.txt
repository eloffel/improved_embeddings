5
1
0
2

 
r
p
a
6
1

 

 
 
]
l
c
.
s
c
[
 
 

3
v
8
1
4
6

.

2
1
4
1
:
v
i
x
r
a

accepted as a workshop contribution at iclr 2015

inducing semantic representation from text
by jointly predicting and factorizing rela-
tions

ivan titov, ehsan khoddam
universiteit van amsterdam
amsterdam, the netherlands
{titov,e.khoddammohammadi}@uva.nl

abstract

in this work, we propose a new method to integrate two recent lines of work: un-
supervised induction of shallow semantics (e.g., semantic roles) and factorization
of relations in text and knowledge bases. our model consists of two components:
(1) an encoding component: a id14 model which predicts roles
given a rich set of syntactic and lexical features; (2) a reconstruction component: a
tensor factorization model which relies on roles to predict argument    llers. when
the components are estimated jointly to minimize errors in argument reconstruc-
tion, the induced roles largely correspond to roles de   ned in annotated resources.
our method performs on par with most accurate role induction methods on en-
glish, even though, unlike these previous approaches, we do not incorporate any
prior linguistic knowledge about the language.

1

introduction

shallow representations of meaning, and semantic role labels in particular, have a long history in
linguistics (fillmore, 1968). more recently, with an emergence of large annotated resources such as
propbank (palmer et al., 2005) and framenet (baker et al., 1998), automatic id14
(srl) has attracted a lot of attention (surdeanu et al., 2008; haji  c et al., 2009; das et al., 2010).
semantic role representations encode the underlying predicate-argument structure of sentences, or,
more speci   cally, for every predicate in a sentence they identify a set of arguments and associate
each argument with an underlying semantic role, such as an agent (an initiator or doer of the action)
or a patient (an affected entity). consider the following sentence:

[agent the police] charged [p atient the demonstrators] [instrument with batons].

here, the police, the demonstrators and with batons are assigned to roles agent, patient and
instrument, respectively. semantic roles have many potential applications in nlp and have been
shown to bene   t, for example, id53 (shen and lapata, 2007; berant et al., 2014) and
id123 (sammons et al., 2009), among others.
the scarcity of annotated data has motivated the research into unsupervised learning of seman-
tic representations (lang and lapata, 2010; 2011a;b; titov and klementiev, 2012; f  urstenau and
rambow, 2012; garg and henderson, 2012). the existing methods have a number of serious short-
comings. first, they make very strong assumptions, for example, assuming that arguments are con-
ditionally independent of each other given the predicate. second, unlike state-of-the-art supervised
parsers, they rely on a very simplistic set of features of a sentence. these factors lead to models be-
ing insuf   ciently expressive to capture syntax-semantics interface, inadequate handling of language
ambiguity and, overall, introduces an upper bound on their performance.
in this work, we propose a method for effective unsupervised estimation of feature-rich models of
semantic roles. we demonstrate that reconstruction-error objectives, which have been shown to be
effective primarily for training neural networks, are well suited for inducing feature-rich log-linear
models of semantics. our model consists of two components: a log-linear feature rich semantic role

1

accepted as a workshop contribution at iclr 2015

figure 1: (a) an autoencoder from rm to rp (typically p < m).
reconstruction-error minimization framework.

(b) modeling roles within the

labeler and a tensor-factorization model which captures interaction between semantic roles and argu-
ment    llers. our method rivals the most accurate semantic role induction methods on english (titov
and klementiev, 2012; lang and lapata, 2011a). importantly, no prior knowledge about any spe-
ci   c language was incorporated in our feature-rich model, whereas the id91 counterparts relied
on language-speci   c argument signatures.

2 approach

at the core of our approach is a statistical model encoding an interdependence between a seman-
tic role structure and its realization in a sentence. in the unsupervised learning setting, sentences,
their syntactic representations and argument positions (denoted by x) are observable whereas the
associated semantic roles r are latent and need to be induced by the model. crucially, the good r
should encode roles rather than some other form of abstraction. in what follows, we will refer to
roles using their names, though, in the unsupervised setting, our method, as any other latent variable
model, will not yield human-interpretable labels for them. we also focus only on the labeling stage
of id14. identi   cation, though an important problem, can be tackled with heuris-
tics (lang and lapata, 2011a), with unsupervised techniques (abend et al., 2009) or potentially by
using a supervised classi   er trained on a small amount of data.
the model consists of two components. the    rst component is responsible for prediction of argu-
ment tuples based on roles and the predicate. in our experiments, in this component, we represent
arguments as lemmas of their lexical heads (e.g., baton instead of with batons), and we also restrict
ourselves to only verbal predicates. intuitively, we can think of predicting one argument at a time
(see figure 1(b)): an argument (e.g., demonstrator in our example) is predicted based on the pred-
icate lemma (charge), the role assigned to this argument (i.e. patient) and other role-argument
pairs ((agent, police) and (instrument, baton)). while learning to predict arguments, the in-
ference algorithm will search for role assignments which simplify this prediction task as much as
possible. our hypothesis is that these assignments will correspond to roles accepted in linguistic
theories (or, more importantly, useful in practical applications). why is this hypothesis plausible?
primarily because these semantic representations were introduced as an abstraction capturing cru-
cial properties of a relation (or an event). thus, these representations, rather than surface linguistic
details like argument order or syntactic functions, should be crucial for modeling sets of potential
argument tuples. the reconstruction component is not the only part of the model. crucially, what we
referred to above as    searching for role assignments to simplify argument prediction    would actually
correspond to learning another component: a semantic role labeler which predicts roles relying on
a rich set of sentence features. these two components will be estimated jointly in such a way as to
minimize errors in recovering arguments. the role labeler will be the end-product of learning: it
will be used to process new sentences, and it will be compared to existing methods in our evaluation.
generative modeling is not the only way to learn latent representations. one alternative, popular
in the neural network community, is to use autoencoders instead and optimize the reconstruction
error (hinton, 1989; vincent et al., 2008). the encoding model will be a feature-rich classi   er

2

 reconstructed inputencodingreconstructioninputy2rplatent representationfeature representation of "the police charged...  " (    )semantic role prediction ( = encoding)charge(agent: police,   patient:  demonstrator,   instrument: baton)demonstratorargument prediction( = reconstruction)hidden p(r|x,w)feature-rich model"argument prediction" model(a)(b)x2rm  x2rmp(ai|a i,r,v,   )xaccepted as a workshop contribution at iclr 2015

which predicts semantic roles for a sentence, and the reconstruction model is the model which
predicts an argument given its role, and given the rest of the arguments and their roles. the idea of
training linear models with reconstruction error was previously explored by daum  e iii (2009) and
very recently by ammar et al. (2014). however, they do not consider learning factorization models,
and they also do not deal with semantics. tensor and factorization methods used in the context of
modeling knoweldge bases (e.g., (bordes et al., 2011)) are also close in spirit. however, they do not
deal with inducing semantics but rather factorize existing relations (i.e. rely on semantics).

2.1 modeling semantics within the reconstruction-error framework

as we mentioned above, we focus on argument labeling: we assume that arguments a =
(a1, . . . , an ), ai     a, are known, and only their roles r = (r1, . . . , rn ), ri     r need to be
induced. for the encoder (i.e. the semantic role labeler), we use a log-linear model:

p(r|x, w)     exp(wt g(x, r)),

where g(x, r) is a feature vector encoding interactions between sentence x and the semantic role
representation r. any model can be used here as long as the posterior distributions of roles ri can be
ef   ciently computed or approximated. in our experiments, we used a model which factorizes over
individual arguments (i.e. independent id28 classi   ers).
the reconstruction component predicts an argument (e.g., the ith argument ai) given the semantic
roles r, the predicate v and other arguments a   i = (a1, . . . , ai   1, ai+1, . . . , an ) with a bilinear
softmax model:

exp(ut

aic t

j(cid:54)=i cv,rj uaj )

v,ri
z(r, v, i)

p(ai|a   i, r, v, c, u) =

(1)
ua     rd (for every a     a) and cv,r     rd  k (for every verb v and every role r     r) are model
parameters, z(r, v, i) is the partition function ensuring that the probabilities sum to one. intuitively,
embeddings ua encode semantic properties of an argument: for example, embeddings for the words
demonstrator and protestor should be somewhere near each other in rd space, and further away from
that for the word cat. the product cp,rua is a k-dimensional vector encoding beliefs about other
arguments based on the argument-role pair (a, r). in turn, the dot product (cv,riuai)t cv,rj uaj is
large if the argument pair (ai, aj) is semantically compatible with the predicate, and small otherwise.
intuitively, this objective corresponds to scoring argument tuples according to

,

(cid:80)

h(a, r, v, c, u) =

ut
aic t

v,ricv,rj uaj ,

i(cid:54)=j

hinting at connections to (coupled) tensor and factorization methods (y  lmaz et al., 2011; bordes
et al., 2011) and id65 (mikolov et al., 2013; pennington et al., 2014). note also
that the reconstruction model does not have access to any features of the sentence (e.g., argument
order or syntax), forcing the roles to convey all the necessary information.
in practice, we smooth the model by using a sum of predicate-speci   c and cross-predicate projection
matrices (cv,r + cr) instead of just cv,r.

2.2 learning

parameters of both model components (w, u and c) are learned jointly: the natural objective asso-
ciated with every sentence would be the following:

(cid:88)

p(ai|a   i, r, v, c, u)p(r|x, w).

(2)

however optimizing this objective is not practical in its exact form for two reasons: (1) the marginal-
ization over r is exponential in the number of arguments; (2) the partition function z(r, v, i) re-
quires summation over the entire set of potential argument lemmas. we use existing techniques
to address both challenges.
in order to deal with the    rst challenge, we use a basic mean-   eld
approximation: instead of marginalization over r we substitute r with their posterior distributions
  is = p(ri = s|x, w). to tackle the second problem, the computation of z(  , v, i), we use a
negative sampling technique (see, e.g., mikolov et al. (2013)). at test time, only the linear semantic
role labeler is used, so the id136 is straightforward.

3

n(cid:88)

(cid:88)

log

i=1

r

accepted as a workshop contribution at iclr 2015

table 1: purity (pu), collocation (co) and f1 on english (propbank / conll 2008).

our model
bayes
agglom+
roleordering
agglom
graphpart
llogistic
syntf

pu
79.7
89.3
87.9
83.5
88.7
88.6
79.5
81.6

co
86.2
76.6
75.6
78.5
73.0
70.7
76.5
77.5

f1
82.8
82.5
81.3
80.9
80.1
78.6
78.0
79.5

3 experiments

we followed lang and lapata (2010) and used the conll 2008 shared task data (surdeanu et al.,
2008). as in most previous work on unsupervised srl, we evaluate our model using id91
metrics: purity, collocation and their harmonic mean f1. for the id14 (encoding)
component, we relied on 14 feature patterns used for argument labeling in one of the popular su-
pervised role labelers (johansson and nugues, 2008), which resulted in a quite large feature space
(49,474 feature instantiations for our english dataset).
for the reconstruction component, we set the dimensionality of embeddings d, the projection di-
mensionality k and the number of negative samples n to 30, 15 and 20, respectively. these hyper-
parameters were tuned on held-out data (the conll 2008 development set), d and k were chosen
among {10, 15, 20, 30, 50} with constraining d to be always greater than k, n was    xed to 10 and
20. the model was not sensitive to the parameter de   ning the number of roles as long it was large
enough. for training, we used uniform random initialization and adagrad (duchi et al., 2011).
following (lang and lapata, 2010), we use a baseline (syntf) which simply clusters predicate
arguments according to the dependency relation to their head. a separate cluster is allocated for each
of 20 most frequent relations in the dataset and an additional cluster is used for all other relations.
as observed in the previous work (lang and lapata, 2011a), this is a hard baseline to beat.
we also compare against previous approaches: the latent logistic classi   cation model (lang and
lapata, 2010) (labeled llogistic), the agglomerative id91 method (lang and lapata, 2011a)
(agglom), the graph partitioning approach (lang and lapata, 2011b) (graphpart), the global role
ordering model (garg and henderson, 2012) (roleordering). we also report results of an improved
version of agglom, recently reported by lang and lapata (2014) (agglom+). the strongest previous
model is bayes: bayes is the most accurate (   coupled   ) version of the bayesian model of titov and
klementiev (2012), estimated from the conll data without relying on any external data.
our model outperforms or performs on par with best previous models in terms of f1 (see table 1).
interestingly, the purity and collocation balance is very different for our model and for the rest of
the systems. in fact, our model induces at most 4-6 roles. on the contrary, bayes predicts more
than 30 roles for the majority of frequent predicates (e.g., 43 roles for the predicate include or 35 for
say). though this tendency reduces the purity scores for our model, this also means that our roles
are more human interpretable. for example, agents and patients are clearly identi   able in the model
predictions.

4 conclusions

we introduced a method for inducing feature-rich semantic role labelers from unannoated text. in
our approach, we view a semantic role representation as an encoding of a latent relation between a
predicate and a tuple of its arguments. we capture this relation with a probabilistic tensor factor-
ization model. our estimation method yields a semantic role labeler which achieves state-of-the-art
results on english.

4

accepted as a workshop contribution at iclr 2015

references
abend, o., reichart, r., and rappoport, a. (2009). unsupervised argument identi   cation for semantic role

labeling. in acl-ijcnlp.

ammar, w., dyer, c., and smith, n. (2014). conditional random    eld autoencoders for unsupervised structured

prediction. in nips.

baker, c. f., fillmore, c. j., and lowe, j. b. (1998). the berkeley framenet project. in acl-coling.
berant, j., srikumar, v., chen, p., huang, b., manning, c. d., vander linden, a., harding, b., and clark, p.

(2014). modeling biological processes for reading comprehension. in emnlp.

bordes, a., weston, j., collobert, r., and bengio, y. (2011). learning structured embeddings of knowledge

bases. in aaai.

das, d., schneider, n., chen, d., and smith, n. a. (2010). probabilistic frame-id29. in naacl.
daum  e iii, h. (2009). unsupervised search-based id170. in icml.
duchi, j., hazan, e., and singer, y. (2011). adaptive subgradient methods for online learning and stochastic

optimization. jmlr, 12:2121   2159.

fillmore, c. j. (1968). the case for case. in e., b. and r.t., h., editors, universals in linguistic theory, pages

1   88. holt, rinehart, and winston, new york.

f  urstenau, h. and rambow, o. (2012). unsupervised induction of a syntax-semantics lexicon using iterative
in the first joint conference on lexical and computational semantics-volume 1: the main

re   nement.
conference and the shared task, and volume 2: the sixth international workshop on semantic evaluation.

garg, n. and henderson, j. (2012). unsupervised semantic role induction with global role ordering. in acl:

short papers-volume 2.

haji  c, j., ciaramita, m., johansson, r., kawahara, d., mart    , m. a., m`arquez, l., meyers, a., nivre, j., pad  o,
s.,   st  ep  anek, j., stra  n  ak, p., surdeanu, m., xue, n., and zhang, y. (2009). the conll-2009 shared task:
syntactic and semantic dependencies in multiple languages. in conll.

hinton, g. e. (1989). connectionist learning procedures. arti   cial intelligence, 40(1):185   234.
johansson, r. and nugues, p. (2008). dependency-based syntactic-semantic analysis with propbank and nom-

bank. in conll.

lang, j. and lapata, m. (2010). unsupervised induction of semantic roles. in acl.
lang, j. and lapata, m. (2011a). unsupervised semantic role induction via split-merge id91. in acl.
lang, j. and lapata, m. (2011b). unsupervised semantic role induction with graph partitioning. in emnlp.
lang, j. and lapata, m. (2014). similarity-driven semantic role induction via graph partitioning. computational

linguistics, 40(3):633   669.

mikolov, t., chen, k., corrado, g., and dean, j. (2013). ef   cient estimation of word representations in vector

space. arxiv preprint arxiv:1301.3781.

palmer, m., gildea, d., and kingsbury, p. (2005). the proposition bank: an annotated corpus of semantic

roles. computational linguistics, 31(1):71   106.

pennington, j., socher, r., and manning, c. d. (2014). glove: global vectors for word representation. in

emnlp.

sammons, m., vydiswaran, v., vieira, t., johri, n., chang, m., goldwasser, d., srikumar, v., kundu, g., tu,
y., small, k., rule, j., do, q., and roth, d. (2009). relation alignment for id123 recognition.
in text analysis conference (tac).

shen, d. and lapata, m. (2007). using semantic roles to improve id53. in emnlp.
surdeanu, m., johansson, a. m. r., m`arquez, l., and nivre, j. (2008). the conll-2008 shared task on joint

parsing of syntactic and semantic dependencies. in conll.

titov, i. and klementiev, a. (2012). a bayesian approach to semantic role induction. in eacl.
vincent, p., larochelle, h., bengio, y., and manzagol, p.-a. (2008). extracting and composing robust features

with denoising autoencoders. in icml.

y  lmaz, k. y., cemgil, a. t., and simsekli, u. (2011). generalised coupled tensor factorisation. in nips.

5

