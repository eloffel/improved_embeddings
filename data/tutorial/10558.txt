cross-lingual models of id27s: an empirical comparison

shyam upadhyay1 manaal faruqui2 chris dyer2 dan roth1

1 department of computer science, university of illinois, urbana-champaign, il, usa

2 school of computer science, carnegie mellon university, pittsburgh, pa, usa

6
1
0
2

 

n
u
j
 

8

 
 
]
l
c
.
s
c
[
 
 

2
v
5
2
4
0
0

.

4
0
6
1
:
v
i
x
r
a

upadhya3@illinois.edu, mfaruqui@cs.cmu.edu

cdyer@cs.cmu.edu, danr@illinois.edu

abstract

despite interest
in using cross-lingual
knowledge to learn id27s for
various tasks, a systematic comparison of
the possible approaches is lacking in the
literature. we perform an extensive eval-
uation of four popular approaches of in-
ducing cross-lingual embeddings, each re-
quiring a different form of supervision,
on four typologically different language
pairs. our evaluation setup spans four dif-
ferent tasks, including intrinsic evaluation
on mono-lingual and cross-lingual simi-
larity, and extrinsic evaluation on down-
stream semantic and syntactic applica-
tions. we show that models which require
expensive cross-lingual knowledge almost
always perform better, but cheaply super-
vised models often prove competitive on
certain tasks.
introduction

1
learning word vector representations using mono-
lingual distributional information is now a ubiqui-
tous technique in nlp. the quality of these word
vectors can be signi   cantly improved by incor-
porating cross-lingual distributional information
(klementiev et al., 2012; zou et al., 2013; vuli  c
and moens, 2013b; mikolov et al., 2013b; faruqui
and dyer, 2014; hermann and blunsom, 2014;
chandar et al., 2014, inter alia), with improve-
ments observed both on monolingual (faruqui and
dyer, 2014; rastogi et al., 2015) and cross-lingual
tasks (guo et al., 2015; s  gaard et al., 2015; guo
et al., 2016).

several models for inducing cross-lingual em-
beddings have been proposed, each requiring a dif-
ferent form of cross-lingual supervision     some
can use document-level alignments (vuli  c and
moens, 2015), others need alignments at the sen-
tence (hermann and blunsom, 2014; gouws et

al., 2015) or word level (faruqui and dyer, 2014;
gouws and s  gaard, 2015), while some require
both sentence and word alignments (luong et al.,
2015). however, a systematic comparison of these
models is missing from the literature, making it
dif   cult to analyze which approach is suitable for a
particular nlp task. in this paper, we    ll this void
by empirically comparing four cross-lingual word
embedding models each of which require different
form of alignment(s) as supervision, across several
dimensions. to this end, we train these models on
four different language pairs, and evaluate them on
both monolingual and cross-lingual tasks.1

first, we show that different models can be
viewed as instances of a more general frame-
work for inducing cross-lingual id27s.
then, we evaluate these models on both extrin-
sic and intrinsic tasks. our intrinsic evaluation
assesses the quality of the vectors on monolin-
gual (  4.2) and cross-lingual (  4.3) word simi-
larity tasks, while our extrinsic evaluation spans
semantic (cross-lingual document classi   cation
  4.4) and syntactic tasks (cross-lingual depen-
dency parsing   4.5).

our experiments show that word vectors trained
using expensive cross-lingual supervision (word
alignments or sentence alignments) perform the
best on semantic tasks. on the other hand, for syn-
tactic tasks like cross-lingual id33,
models requiring weaker form of cross-lingual su-
pervision (such as context agnostic translation dic-
tionary) are competitive to models requiring ex-
pensive supervision. we also show qualitatively
how the nature of cross-lingual supervision used
to train word vectors affects the proximity of
translation pairs across languages, and of words
with similar meaning in the same language in the
vector-space.

1instructions and code to reproduce the experiments
available at http://cogcomp.cs.illinois.edu/
page/publication_view/794

ticular, given a word alignment link from word
v     v in language l2 to w     w in language l1,
the model predicts the context words of w using v
and vice-versa. formally, the cross lingual part of
the objective is,

d12(w, v) =     (cid:88)

log p (wc | v)

(cid:88)

(v,w)   q

wc   nbr1(w)

(1)
where nbr1(w) is the context of w in language l1,
q is the set of word alignments, and p (wc | v)    
c v). another similar term d21 models the
exp(wt
objective for v and nbr2(v). the objective can be
cast into algorithm 1 as,

c(w, v) = d12(w, v) + d21(w, v)

a(w) =     (cid:88)
b(v) =    (cid:88)

w   w

(cid:88)
(cid:88)

wc   nbr1(w)

v   v

vc   nbr2(v)

(2)
log p (wc | w) (3)

log p (vc | v)

(4)

where a(w) and b(v) are the familiar skip-
gram formulation of the monolingual part of the
objective.    and    are chosen hyper-parameters
which set the relative importance of the monolin-
gual terms.

2.2 bilingual compositional model (bicvm)
hermann and blunsom (2014) present a method
that learns bilingual word vectors from a sentence
aligned corpus. their model leverages the fact that
aligned sentences have equivalent meaning, thus
their sentence representations should be similar.
we denote two aligned sentences, (cid:126)v =
(cid:104)x1, . . . ,(cid:105) and (cid:126)w = (cid:104)y1, . . .(cid:105) , where xi    
v, yi     w, are vectors corresponding to the
words in the sentences. let functions f : (cid:126)v     rn
and g : (cid:126)w     rn, map sentences to their seman-
tic representations in rn. bicvm generates word
vectors by minimizing the squared (cid:96)2 norm be-
tween the sentence representations of aligned sen-
tences.
in order to prevent the degeneracy aris-
ing from directly minimizing the (cid:96)2 norm, they
use a noise-contrastive large-margin update, with
randomly drawn sentence pairs ((cid:126)v, (cid:126)wn) as negative
samples. the loss for the sentence pairs ((cid:126)v, (cid:126)w) and
((cid:126)v, (cid:126)wn) can be written as,

e((cid:126)v, (cid:126)w, (cid:126)wn) = max (   +    e((cid:126)v, (cid:126)w, (cid:126)wn), 0) (5)

where,

e((cid:126)v, (cid:126)w) = (cid:107)f ((cid:126)v)     g( (cid:126)w)(cid:107)2

(6)

algorithm 1 general algorithm
1: initialize w     w0,v     v0
2: (w   , v   )     arg min   a(w) +   b(v) + c(w, v)

figure 1: (above) a general schema for induction of cross-
lingual word vector representations. the word vector model
generates embeddings which incorporates distributional in-
formation cross-lingually. (below) a general algorithm for
inducing bilingual id27s, where   ,   , w0, v0
are parameters and a, b, c are suitably de   ned losses.

2 bilingual embeddings
a general schema for inducing bilingual embed-
dings is shown in figure 1. our comparison fo-
cuses on dense,    xed-length distributed embed-
dings which are obtained using some form of
cross-lingual supervision. we brie   y describe the
embedding induction procedure for each of the se-
lected bilingual word vector models, with the aim
to provide a uni   ed algorithmic perspective for all
methods, and to facilitate better understanding and
comparison. our choice of models spans across
different forms of supervision required for induc-
ing the embeddings, illustrated in figure 2.
notation. let w = {w1, w2, . . . , w|w|} be the
vocabulary of a language l1 with |w| words, and
w     r|w|  l be the corresponding word embed-
dings of length l. let v = {v1, v2, . . . , v|v |} be
the vocabulary of another language l2 with |v |
words, and v     r|v |  m the corresponding word
embeddings of length m. we denote the word vec-
tor for a word w by w.

2.1 bilingual skip-gram model (biskip)
luong et al. (2015) proposed bilingual skip-
gram, a simple extension of the monolingual skip-
gram model, which learns bilingual embeddings
by using a parallel corpus along with word align-
ments (both sentence and word level alignments).
the learning objective is a simple extension
of the skip-gram model, where the context of a
word is expanded to include bilingual links ob-
tained from word alignments, so that the model is
trained to predict words cross-lingually.
in par-

je

i

t   

aime

je t    aime

love you

(a) biskip

i love you

(b) bicvm

(you, t   )

(love, aime)

(i, je)

(c) bicca

bonjour! je t    aime.

hello! how are
you? i love you.

(d) bivcd

figure 2: forms of supervision required by the four models compared in this paper. from left to right, the cost of the supervision
required varies from expensive (biskip) to cheap (bivcd). biskip requires a parallel corpus annotated with word alignments
(fig. 2a), bicvm requires a sentence-aligned corpus (fig. 2b), bicca only requires a bilingual lexicon (fig. 2c) and bivcd
requires comparable documents (fig. 2d).

and,

   e((cid:126)v, (cid:126)w, (cid:126)wn) = e((cid:126)v, (cid:126)w)     e((cid:126)v, (cid:126)wn)

(7)

(cid:88)

this can be cast into algorithm 1 by,

c(w, v) =

e((cid:126)v, (cid:126)w, (cid:126)wn)

(8)

aligned ((cid:126)v, (cid:126)w)
random (cid:126)wn

a(w) = (cid:107)w(cid:107)2 b(v) = (cid:107)v(cid:107)2

(9)
with a(w) and b(v) being regularizers, with
   =   .

2.3 bilingual correlation based embeddings

(bicca)

the bicca model, proposed by faruqui and dyer
(2014), showed that when (independently trained)
monolingual vector matrices w, v are projected
using cca (hotelling, 1936) to respect a transla-
tion lexicon, their performance improves on word
similarity and word analogy tasks. they    rst con-
struct w(cid:48)     w, v(cid:48)     v such that |w(cid:48)|= |v(cid:48)|
and the corresponding words (wi, vi) in the matri-
ces are translations of each other. the projection
is then computed as:

(10)

pw , pv = cca(w(cid:48), v(cid:48))
w    = wpw v    = vpv

(11)
where, pv     rl  d, pw     rm  d are the projec-
tion matrices with d     min(l, m) and the v       
r|v |  d, w        r|w|  d are the word vectors that
have been    enriched    using bilingual knowledge.
the bicca objective can be viewed2 as the fol-

(12)

lowing instantiation of algorithm 1:
w0 = w(cid:48), v0 = v(cid:48)

c(w, v) = (cid:107)w     v(cid:107)2+  (cid:0)vt w(cid:1)

a(w) = (cid:107)w(cid:107)2   1 b(v) = (cid:107)v(cid:107)2   1

(13)
(14)
where w = w0pw and v = v0pv , where we
set    =    =    =     to set hard constraints.
2described in section 6.5 of (hardoon et al., 2004)

2.4 bilingual vectors from comparable data

(bivcd)

another approach of inducing bilingual word vec-
tors, which we refer to as bivcd, was proposed
by vuli  c and moens (2015). their approach is
designed to use comparable corpus between the
source and target language pair to induce cross-
lingual vectors.

let de and df denote a pair of comparable
documents with length in words p and q respec-
tively (assume p > q). bivcd    rst merges these
two comparable documents into a single pseudo-
bilingual document using a deterministic strategy
q(cid:99).
based on length ratio of two documents r = (cid:98) p
every rth word of the merged pseudo-bilingual
document is picked sequentially from df . finally,
a skip-gram model is trained on the corpus of
pseudo-bilingual documents, to generate vectors
for all words in w        v   . the vectors consti-
tuting w    and v    can then be easily identi   ed.

instantiating bivcd in the general algorithm
is obvious: c(w, v) assumes the familiar
id97 skip-gram objective over the pseudo-
bilingual document,

c(w, v) =     (cid:88)

(cid:88)

log p (t | s)

s   w   v

t   nbr(s)

(15)
where nbr(s) is de   ned by the pseudo-bilingual
document and p (t | s)     exp(tt s). note that
t, s     w     v .

although bivcd is designed to use comparable
corpus, we provide it with parallel data in our ex-
periments (to ensure comparability), and treat two
aligned sentences as comparable.

3 data

we train cross-lingual embeddings for 4 language
pairs: english-german (en-de), english-french
(en-fr), english-swedish (en-sv) and english-
chinese (en-zh). for en-de and en-sv we use the

l1

en

l2
de
fr
sv
zh

#sent
1.9
2.0
1.7
2.0

#l1-words

#l2-words

53
55
46
58

51
61
42
50

table 1: the size of parallel corpora (in millions) of different
language pairs used for training cross-lingual word vectors.

europarl v7 parallel corpus3 (koehn, 2005). for
en-fr, we use europarl combined with the news-
commentary and un-corpus dataset from wmt
2015.4 for en-zh, we use the fbis parallel cor-
pus from the news domain (ldc2003e14). we
use the stanford chinese segmenter (tseng et al.,
2005) to preprocess the en-zh parallel corpus. cor-
pus statistics for all languages is shown in table 1.

4 evaluation
we measure the quality of the induced cross-
lingual id27s in terms of their per-
formance, when used as features in the following
tasks:

    monolingual word similarity for english
    cross-lingual dictionary induction
    cross-lingual document classi   cation
    cross-lingual syntactic id33
the    rst two tasks intrinsically measure how
much can monolingual and cross-lingual similar-
ity bene   t from cross-lingual training. the last
two tasks measure the ability of cross-lingually
trained vectors to extrinsically facilitate model
transfer across languages, for semantic and syn-
tactic applications respectively. these tasks have
been used in previous works (klementiev et al.,
2012; luong et al., 2015; vuli  c and moens, 2013a;
guo et al., 2015) for evaluating cross-lingual em-
beddings, but no comparison exists which uses
them in conjunction.

to ensure fair comparison, all models are
trained with embeddings of size 200. we provide
all models with parallel corpora, irrespective of
their requirements. whenever possible, we also
report statistical signi   cance of our results.

3www.statmt.org/europarl/v7/{de,

sv}-en.tgz

4www.statmt.org/wmt15/

translation-task.html

4.1 parameter selection
we follow the bestavg parameter selection strat-
egy from lu et al. (2015): we selected the param-
eters for all models by tuning on a set of values
(described below) and picking the parameter set-
ting which did best on an average across all tasks.

biskip. all models were trained using a win-
dow size of 10 (tuned over {5, 10, 20}), and
30 negative samples (tuned over {10, 20, 30}).
the cross-lingual weight was set
to 4 (tuned
over {1, 2, 4, 8}).
the word alignments for
training the model
(available at github.
com/lmthang/bivec) were generated using
fast_align (dyer et al., 2013). the number
of training iterations was set to 5 (no tuning) and
we set    = 1 and    = 1 (no tuning).

g((cid:126)x) = (cid:80)

bicvm. we use the tool (available at github.
com/karlmoritz/bicvm) released by her-
mann and blunsom (2014) to train all embed-
dings. we train an additive model (that is, f ((cid:126)x) =
i xi) with hinge loss margin set to
200 (no tuning), batch size of 50 (tuned over
50, 100, 1000) and noise parameter of 10 (tuned
over {10, 20, 30}). all models are trained for 100
iterations (no tuning).

bicca. first, monolingual word vectors are
trained using the skip-gram model5 with negative
sampling (mikolov et al., 2013a) with window
of size 5 (tuned over {5, 10, 20}). to generate a
cross-lingual dictionary, word alignments are gen-
erated using cdec from the parallel corpus. then,
word pairs (a, b), a     l1, b     l2 are selected such
that a is aligned to b the most number of times and
vice versa. this way, we obtained dictionaries of
approximately 36k, 35k, 30k and 28k word pairs
for en-de, en-fr, en-sv and en-zh respectively.

the monolingual vectors are aligned using the
above dictionaries with the tool (available at
github.com/mfaruqui/eacl14-cca) re-
leased by faruqui and dyer (2014) to generate the
cross-lingual id27s. we use k = 0.5
as the number of canonical components (tuned
over {0.2, 0.3, 0.5, 1.0}). note that this results in
a embedding of size 100 after performing cca.

bivcd. we use id97   s skip gram model
for training our embeddings, with a window size
of 5 (tuned on {5, 10, 20, 30}) and negative sam-
pling parameter set to 5 (tuned on {5, 10, 25}).
every pair of parallel sentences is treated as a

5code.google.com/p/id97

pair of comparable documents, and merging is per-
formed using the sentence length ratio strategy de-
scribed earlier.6

4.2 monolingual evaluation
we    rst evaluate if the inclusion of cross-lingual
knowledge improves the quality of english em-
beddings.
word similarity. word similarity datasets con-
tain word pairs which are assigned similarity rat-
ings by humans. the task evaluates how well
the notion of word similarity according to humans
is emulated in the vector space. evaluation is
based on the spearman   s rank correlation coef-
   cient (myers and well, 1995) between human
rankings and rankings produced by computing co-
sine similarity between the vectors of two words.
we use the siid113x dataset for english (hill
et al., 2014) which contains 999 pairs of en-
glish words, with a balanced set of noun, adjec-
tive and verb pairs. siid113x is claimed to capture
word similarity exclusively instead of wordsim-
353 (finkelstein et al., 2001) which captures both
word similarity and relatedness. we declare sig-
ni   cant improvement if p < 0.1 according to
steiger   s method (steiger, 1980) for calculating
the statistical signi   cant differences between two
dependent correlation coef   cients.

table 2 shows the performance of english em-
beddings induced by all the models by training on
different language pairs on the siid113x word sim-
ilarity task. the score obtained by monolingual
english embeddings trained on the respective en-
glish side of each language is shown in column
marked mono. in all cases (except bicca on en-
sv), the bilingually trained vectors achieve better
scores than the mono-lingually trained vectors.

overall, across all language pairs, bicvm is
the best performing model in terms of spearman   s
correlation, but its improvement over biskip and
bivcd is often insigni   cant. it is notable that 2 of
the 3 top performing models, bicvm and bivcd,
need sentence aligned and document-aligned cor-
pus only, which are easier to obtain than parallel
data with word alignments required by biskip.
qvec. tsvetkov et al. (2015) proposed an in-
trinsic evaluation metric for estimating the qual-
ity of english word vectors. the score produced
by qvec measures how well a given set of word
vectors is able to quantify linguistic properties

6we implemented the code for performing the merging as

we could not    nd a tool provided by the authors.

pair mono biskip bicvm bicca bivcd
en-de 0.29
en-fr 0.30
en-sv 0.28
en-zh 0.28
0.29
avg.

0.37
0.39
0.34
0.39
0.37

0.32
0.36
0.32
0.31
0.33

0.34
0.35
0.32
0.34
0.34

0.30
0.31
0.27
0.30
0.30

table 2: word similarity score measured in spearman   s cor-
relation ratio for english on siid113x-999. the best score for
each language pair is shown in bold. scores which are sig-
ni   cantly better (per steiger   s method with p < 0.1) than
the next lower score are underlined. for example, for en-zh,
bicvm is signi   cantly better than biskip, which in turn is
signi   cantly better than bivcd.

pair mono biskip bicvm bicca bivcd
en-de 0.39
en-fr 0.39
en-sv 0.39
en-zh 0.40
0.39
avg.

0.40
0.40
0.39
0.40
0.40

0.37
0.38
0.37
0.38
0.38

0.31
0.31
0.31
0.32
0.31

0.33
0.33
0.32
0.33
0.33

table 3: intrinsic evaluation of english word vectors mea-
sured in terms of qvec score across models. best scores for
each language pair is shown in bold.

of words, with higher being better. the metric
is shown to have strong correlation with perfor-
mance on downstream semantic applications. as
it can be currently only used for english, we use
it to evaluate the english vectors obtained using
cross-lingual training of different models. ta-
ble 3 shows that on average across language pairs,
biskip achieves the best score, followed by mono
(mono-lingually trained english vectors), bivcd
and bicca. a possible explanation for why mono
scores are better than those obtained by some of
the cross-lingual models is that qvec measures
monolingual semantic content based on a linguis-
tic oracle made for english. cross-lingual training
might affect these semantic properties arbitrarily.

interestingly, bicvm which was the best model
according to siid113x,
ranks last according to
qvec. the fact that the best models according
to qvec and word similarities are different re-
inforces observations made in previous work that
performance on word similarity tasks alone does
not re   ect quanti   cation of linguistic properties
of words (tsvetkov et al., 2015; schnabel et al.,
2015).

l1

en

l2
de
fr
sv
zh

avg.

biskip bicvm bicca bivcd
79.7
78.9
77.1
69.4
76.3

72.4
70.1
74.2
59.6
69.1

74.5
72.9
76.7
66.0
72.5

62.5
68.8
56.9
53.2
60.4

table 4: cross-lingual dictionary induction results (top-10
accuracy). the same trend was also observed across models
when computing mrr (mean reciprocal rank).

4.3 cross-lingual dictionary induction

the task of cross-lingual dictionary induc-
tion (vuli  c and moens, 2013a; gouws et al., 2015;
mikolov et al., 2013b) judges how good cross-
lingual embeddings are at detecting word pairs
that are semantically similar across languages. we
follow the setup of vuli  c and moens (2013a), but
instead of manually creating a gold cross-lingual
dictionary, we derived our gold dictionaries using
the open id73net data released by
bond and foster (2013). the data includes synset
alignments across 26 languages with over 90% ac-
curacy. first, we prune out words from each synset
whose frequency count is less than 1000 in the vo-
cabulary of the training data from   3. then, for
each pair of aligned synsets s1 = {k1, k2,      }
s2 = {g1, g2,      }, we include all elements from
the set {(k, g) | k     s1, g     s2} into the gold dic-
tionary, where k and g are the lemmas. using this
approach we generated dictionaries of sizes 1.5k,
1.4k, 1.0k and 1.6k pairs for en-fr, en-de, en-sv
and en-zh respectively.

we report top-10 accuracy, which is the frac-
tion of the entries (e, f ) in the gold dictionary, for
which f belongs to the list of top-10 neighbors
of the word vector of e, according to the induced
cross-lingual embeddings. from the results (ta-
ble 4), it can be seen that for dictionary induction,
the performance improves with the quality of su-
pervision. as we move from cheaply supervised
methods (eg. bivcd) to more expensive supervi-
sion (eg. biskip), the accuracy improves. this
suggests that for cross lingual similarity tasks,
the more expensive the cross-lingual knowledge
available, the better. models using weak super-
vision like bivcd perform poorly in comparison
to models like biskip and bicvm, with perfor-
mance gaps upwards of 10 pts on an average.

l2
de
fr
sv
zh

en

l1

en

de
fr
sv
zh

avg.

biskip bicvm bicca bivcd
85.2
77.7
72.3
75.5
74.9
80.4
73.4
81.1
77.6

79.9
72.0
59.9
73.0
74.1
77.6
78.2
80.9
74.5

79.1
70.7
65.3
69.4
64.9
75.5
67.0
77.3
71.2

85.0
71.7
69.1
73.6
71.1
73.7
67.7
76.4
73.5

table 5: cross-lingual document classi   cation accuracy
when trained on language l1, and evaluated on language l2.
the best score for each language is shown in bold. scores
which are signi   cantly better (per mcnemar   s test with p <
0.05) than the next lower score are underlined. for example,
for sv   en, bivcd is signi   cantly better than biskip, which
in turn is signi   cantly better than bicvm.

4.4 cross-lingual document classi   cation
we follow the cross-lingual document classi   ca-
tion (cldc) setup of klementiev et al. (2012), but
extend it to cover all of our language pairs. we use
the rcv2 reuters multilingual corpus7 for our ex-
periments. in this task, for a language pair (l1, l2),
a document classi   er is trained using the docu-
ment representations derived from word embed-
dings in language l1, and then the trained model
is tested on documents from language l2 (and
vice-versa). by using supervised training data in
one language and evaluating without further su-
pervision in another, cldc assesses whether the
learned cross-lingual representations are semanti-
cally coherent across multiple languages.
all embeddings are learned on the data de-
scribed in   3, and we only use the rcv2 data to
learn document classi   cation models. following
previous work, we compute document representa-
tion by taking the tf-idf weighted average of vec-
tors of the words present in it.8 a multi-class clas-
si   er is trained using an averaged id88 (fre-
und and schapire, 1999) for 10 iterations, using
the document vectors of language l1 as features9.
majority baselines for en     l2 and l1     en are
49.7% and 46.7% respectively, for all languages.
table 5 shows the performance of different mod-
els across different language pairs. we computed
con   dence values using the mcnemar test (mcne-

7http://trec.nist.gov/data/reuters/

reuters.html

8tf-idf (salton and buckley, 1988) was computed using all

documents for that language in rcv2.

9we use the implementation of klementiev et al. (2012).

mar, 1947) and declare signi   cant improvement if
p < 0.05.

table 5 shows that in almost all cases, biskip
performs signi   cantly better than the remaining
models.
for transferring semantic knowledge
across languages via embeddings, sentence and
word level alignment proves superior to sentence
or word level alignment alone. this observation
is consistent with the trend in cross-lingual dictio-
nary induction, where too the most expensive form
of supervision performed the best.

4.5 cross-lingual id33
using cross lingual similarity for direct-transfer of
dependency parsers was    rst shown in t  ackstr  om
et al. (2012). the idea behind direct-transfer is
to train a id33 model using em-
beddings for language l1 and then test the trained
model on language l2, replacing embeddings for
language l1 with those of l2. the transfer relies
on coherence of the embeddings across languages
arising from the cross lingual training. for our ex-
periments, we use the cross lingual transfer setup
of guo et al. (2015).10 their framework trains a
transition-based dependency parser using nonlin-
ear activation function, with the source-side em-
beddings as lexical features. these embeddings
can be replaced by target-side embeddings at test
time.

all models are trained for 5000 iterations with
   xed id27s during training. since our
goal is to determine the utility of word embed-
dings in id33, we turn off other
features that can capture distributional information
like brown clusters, which were originally used in
guo et al. (2015). we use the universal depen-
dency treebank (mcdonald et al., 2013) version-
2.0 for our evaluation. for chinese, we use the
treebank released as part of the conll-x shared
task (buchholz and marsi, 2006).

we    rst evaluate how useful the word embed-
dings are in cross-lingual model transfer of depen-
dency parsers (table 6). on an average, bicca
does better than other models. biskip is a close
second, with an average performance gap of less
than 1 point. biskip outperforms bicvm on ger-
man and french (over 2 point improvement), ow-
ing to word alignment information biskip   s model
uses during training.
it is not surprising that
english-chinese transfer scores are low, due to the
signi   cant difference in syntactic structure of the

10

acl15-clnndep

github.com/jiangfeng1124/

l2
de
fr
sv
zh

en

l1

en

de
fr
sv
zh

avg.

biskip bicvm bicca bivcd
49.8
65.8
56.9
6.4
49.7
53.3
48.2
0.17
41.3

51.3
65.9
59.4
6.4
50.3
54.2
49.9
0.17
42.2

47.5
63.2
56.7
6.1
45.0
50.6
49.0
0.12
39.8

49.0
60.7
54.6
6.0
43.6
49.5
44.6
0.15
38.5

table 6: labeled attachment score (las) for cross-lingual
id33 when trained on language l1, and eval-
uated on language l2. the best score for each language is
shown in bold.

two languages. surprisingly, unlike the seman-
tic tasks considered earlier, the models with ex-
pensive supervision requirements like biskip and
bicvm could not outperform a cheaply super-
vised bicca.

we also evaluate whether using cross-lingually
trained vectors for learning dependency parsers is
better than using mono-lingually trained vectors
in table 7. we compare against parsing models
trained using mono-lingually trained word vectors
(column marked mono in table 7). these vectors
are the same used as input to the bicca model.
all other settings remain the same. on an aver-
age across language pairs, improvement over the
monolingual embeddings was obtained with the
biskip and bicca models, while bicvm and
bivcd consistently performed worse. a possible
reason for this is that bicvm and bivcd oper-
ate on sentence level contexts to learn the embed-
dings, which only captures the semantic meaning
of the sentences and ignores the internal syntac-
tic structure. as a result, embedding trained us-
ing bicvm and bivcd are not informative for
syntactic tasks. on the other hand, biskip and
bicca both utilize the word alignment informa-
tion to train their embeddings and thus do better in
capturing some notion of syntax.

5 qualitative analysis
figure 3 shows the pca projection of some of the
most frequent words in the english-french corpus.
it is clear that biskip and bicvm produce cross-
lingual vectors which are the most comparable, the
english and french words which are translations
of each other are represented by almost the same
point in the vector-space. in bicca and bivcd

(a) biskip

(b) bicvm

(c) bicca

(d) bivcd

figure 3: pca projection of id27s of some frequent words present in english-french corpus. english and french
words are shown in blue and red respectively.

l mono biskip bicvm bicca bivcd
71.1
de
78.9
fr
75.5
sv
73.8
zh
avg. 74.8

72.0
80.4
78.2
73.1
75.9

71.4
80.2
79.0
71.7
75.6

58.9
69.5
64.5
67.0
66.8

60.4
73.7
70.5
65.8
67.6

table 7: labeled attachment score (las) for dependency
parsing when trained and tested on language l. mono refers
to parser trained with mono-lingually induced embeddings.
scores in bold are better than the mono scores for each lan-
guage, showing improvement from cross-lingual training.

bivcd are better at separating antonyms. the
words peace and war, (and their french trans-
lations paix and guerre) are well separated in
bicca and bivcd. however,
in biskip and
bicvm these pairs are very close together. this
can be attributed to the fact
that biskip and
bicvm are trained on parallel sentences, and if
two antonyms are present in the same sentence in
english, they will also be present together in its
french translation. however, bicca uses bilin-
gual dictionary and bivcd use comparable sen-
tence context, which helps in pulling apart the syn-
onyms and antonyms.

the translated words are more distant than biskip
and bicvm. this is not surprising because biskip
and bicvm require more expensive supervision
at the sentence level in contrast to the other two
models.

an interesting observation is that bicca and

6 discussion

the goal of this paper was to formulate the task
of learning cross-lingual word vector representa-
tions in a uni   ed framework, and conduct exper-
iments to compare the performance of existing

0.80.60.40.20.00.20.40.60.80.40.20.00.20.40.60.8marketworldcountryenergyproblemlawmoneychildrenpeacelifewarpaysmarch  probl  memondevie  nergieenfantspaixargentguerreloi0.80.60.40.20.00.20.40.60.81.00.60.40.20.00.20.40.60.81.0marketlawcountryproblemenergymoneylifechildrenworldpeacewarpaysprobl  memarch  vie  nergieargentmondeenfantspaixloiguerre0.40.20.00.20.40.60.80.80.60.40.20.00.20.40.6marketworldcountryenergyproblemlawmoneychildrenpeacelifewarpaysmarch  probl  memondevie  nergieenfantspaixargentguerreloi0.80.60.40.20.00.20.40.60.60.40.20.00.20.40.60.8marketworldcountryenergyproblemlawmoneychildrenpeacelifewarpaysmarch  probl  memondevie  nergieenfantspaixguerreargentloimodels in a unbiased manner. we chose exist-
ing cross-lingual word vector models that can be
trained on two languages at a given time. in re-
cent work, ammar et al. (2016) train multilingual
word vectors using more than two languages; our
comparison does not cover this setting. it is also
worth noting that we compare here different cross-
lingual id27s, which are not to be con-
fused with a collection of monolingual word em-
beddings trained for different languages individu-
ally (al-rfou et al., 2013).

the paper does not cover all approaches
that generate cross-lingual id27s.
some methods do not have publicly available
code (coulmance et al., 2015; zou et al., 2013);
for others, like bilbowa (gouws et al., 2015), we
identi   ed problems in the available code, which
caused it to consistently produced results that are
inferior even to mono-lingually trained vectors.11
however, the models that we included for com-
parison in our survey are representative of other
cross-lingual models in terms of the form of cross-
lingual supervision required by them. for exam-
ple, bilbowa (gouws et al., 2015) and cross-
lingual auto-encoder (chandar et al., 2014) are
similar to bicvm in this respect. multi-view
cca (rastogi et al., 2015) and deep cca (lu et
al., 2015) can be viewed as extensions of bicca.
our choice of models was motivated to com-
pare different forms of supervision, and therefore,
adding these models, would not provide additional
insight.
7 conclusion
we presented the    rst systematic comparative
evaluation of cross-lingual embedding methods on
several downstream nlp tasks, both intrinsic and
extrinsic. we provided a uni   ed representation
for all approaches, showing them as instances of
a general algorithm. our choice of methods spans
a diverse range of approaches, in that each requires
a different form of supervision.

reveal

our experiments

interesting trends.
when evaluating on intrinsic tasks such as mono-
lingual word similarity, models relying on cheaper
forms of supervision (such as bivcd) perform al-
most on par with models requiring expensive su-
pervision. on the other hand, for cross-lingual se-
mantic tasks, like cross-lingual document classi-
   cation and dictionary induction, the model with
the most informative supervision performs best
11we contacted the authors of the papers and were unable

to resolve the issues in the toolkit.

overall. in contrast, for the syntactic task of de-
pendency parsing, models that are supervised at
a word alignment level perform slightly better.
overall this suggests that semantic tasks can ben-
e   t more from richer cross-lingual supervision, as
compared to syntactic tasks.

acknowledgement
this material is based on research sponsored by darpa
under agreement number fa8750-13-2-0008 and contract
hr0011-15-2-0025. approved for public release, distribu-
tion unlimited. the views expressed are those of the authors
and do not re   ect the of   cial policy or position of the depart-
ment of defense or the u.s. government.

references
[al-rfou et al.2013] rami al-rfou, bryan perozzi, and
steven skiena. 2013. polyglot: distributed word
in proc. of
representations for multilingual nlp.
conll.

[ammar et al.2016] waleed ammar, george mulcaire,
yulia tsvetkov, guillaume lample, chris dyer, and
noah a smith. 2016. massively id73
embeddings. arxiv preprint arxiv:1602.01925.

[bond and foster2013] francis bond and ryan foster.
2013. linking and extending an open multilingual
id138. in proc. of acl.

[buchholz and marsi2006] sabine buchholz and erwin
marsi. 2006. conll-x shared task on multilingual
id33. in proc. of conll.

[chandar et al.2014] sarath chandar, stanislas lauly,
hugo larochelle, mitesh khapra, balaraman ravin-
dran, vikas c raykar, and amrita saha.
2014.
an autoencoder approach to learning bilingual word
representations. in proc. of nips.

[coulmance et al.2015] jocelyn coulmance, jean-marc
marty, guillaume wenzek, and amine benhal-
loum. 2015. trans-gram, fast cross-lingual word-
embeddings. in proc. of emnlp.

[dyer et al.2013] chris dyer, victor chahuneau, and
noah a smith. 2013. a simple, fast, and effec-
tive reparameterization of ibm model 2. in proc. of
naacl.

[faruqui and dyer2014] manaal faruqui and chris
improving vector space word repre-
in proc.

dyer. 2014.
sentations using multilingual correlation.
of eacl.

[finkelstein et al.2001] lev

finkelstein,

evgeniy
gabrilovich, yossi matias, ehud rivlin, zach
solan, gadi wolfman, and eytan ruppin. 2001.
placing search in context: the concept revisited. in
proc. of www.

[freund and schapire1999] yoav freund and robert e
schapire. 1999. large margin classi   cation us-
ing the id88 algorithm. machine learning,
37(3):277   296.

[gouws and s  gaard2015] stephan gouws and anders
s  gaard. 2015. simple task-speci   c bilingual word
embeddings. in proc. of naacl.

[gouws et al.2015] stephan gouws, yoshua bengio,
and greg corrado. 2015. bilbowa: fast bilingual
distributed representations without word alignments.
in proc. of icml.

[guo et al.2015] jiang guo, wanxiang che, david
yarowsky, haifeng wang, and ting liu.
2015.
cross-lingual id33 based on dis-
tributed representations. in proc. of acl.

[guo et al.2016] jiang guo, wanxiang che, david
yarowsky, haifeng wang, and ting liu. 2016. a
representation learning framework for multi-source
transfer parsing. in proc. of aaai.

[hardoon et al.2004] david r hardoon, sandor szed-
mak, and john shawe-taylor.
2004. canoni-
cal correlation analysis: an overview with appli-
cation to learning methods. neural computation,
16(12):2639   2664.

[hermann and blunsom2014] karl moritz hermann
and phil blunsom. 2014. multilingual models for
compositional id65. in proc. of
acl.

[hill et al.2014] felix hill, roi reichart, and anna ko-
rhonen. 2014. siid113x-999: evaluating semantic
models with (genuine) similarity estimation. arxiv
preprint arxiv:1408.3456.

[hotelling1936] harold hotelling. 1936. relations be-
tween two sets of variates. biometrika, 28(3/4):321   
377.

[klementiev et al.2012] alexandre klementiev,

ivan
titov, and binod bhattarai.
inducing
crosslingual distributed representations of words. in
proc. of coling.

2012.

[koehn2005] philipp koehn. 2005. europarl: a par-
in

allel corpus for id151.
proc. of mt summit.

[lu et al.2015] ang lu, weiran wang, mohit bansal,
kevin gimpel, and karen livescu. 2015. deep
multilingual correlation for improved word embed-
dings. in proc. of naacl.

[luong et al.2015] thang luong, hieu pham, and
christopher d. manning. 2015. bilingual word rep-
resentations with monolingual quality in mind.
in
proc. of the workshop on vector space modeling for
nlp.

[mcdonald et al.2013] ryan mcdonald, joakim nivre,
yvonne quirmbach-brundage, yoav goldberg, di-
panjan das, kuzman ganchev, keith hall, slav
petrov, hao zhang, oscar t  ackstr  om, claudia be-
dini, n  uria bertomeu castell  o, and jungmee lee.
2013. universal dependency annotation for multi-
lingual parsing. in proc. of acl.

[mcnemar1947] quinn mcnemar.

1947. note on
the sampling error of the difference between cor-
related proportions or percentages. psychometrika,
12(2):153   157.

[mikolov et al.2013a] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013a. ef   cient estima-
tion of word representations in vector space. arxiv
preprint arxiv:1301.3781.

[mikolov et al.2013b] tomas mikolov, quoc v le, and
exploiting similarities
arxiv

ilya sutskever.
among languages for machine translation.
preprint arxiv:1309.4168.

2013b.

[myers and well1995] jerome l. myers and arnold d.
well. 1995. research design & statistical analysis.
routledge.

[rastogi et al.2015] pushpendre rastogi, benjamin
van durme, and raman arora. 2015. multiview
lsa: representation learning via generalized cca.
in proceedings of naacl.

[salton and buckley1988] gerard salton and christo-
pher buckley. 1988. term-weighting approaches
in automatic text retrieval. information processing
and management.

[schnabel et al.2015] tobias schnabel, igor labutov,
david mimno, and thorsten joachims. 2015. eval-
uation methods for unsupervised id27s.
in proc. of emnlp.

[s  gaard et al.2015] anders s  gaard,

  zeljko agi  c,
h  ector mart    nez alonso, barbara plank, bernd
bohnet, and anders johannsen. 2015. inverted in-
dexing for cross-lingual nlp. in proc. of acl.

[steiger1980] james h steiger. 1980. tests for com-
paring elements of a correlation matrix. psycholog-
ical bulletin, 87(2):245.

[t  ackstr  om et al.2012] o. t  ackstr  om, r. mcdonald,
and j. uszkoreit. 2012. cross-lingual word clusters
for direct transfer of linguistic structure. in naacl.

[tseng et al.2005] huihsin tseng, pichuan chang,
galen andrew, daniel jurafsky, and christopher
manning. 2005. a conditional random    eld word
in proc. of
segmenter for sighan bakeoff 2005.
sighan.

[tsvetkov et al.2015] yulia tsvetkov, manaal faruqui,
wang ling, guillaume lample, and chris dyer.
2015. evaluation of word vector representations by
subspace alignment. in proc. of emnlp.

[vuli  c and moens2013a] ivan vuli  c

and marie-
francine moens. 2013a. cross-lingual semantic
similarity of words as the similarity of their semantic
word responses. in proc. of naacl.

[vuli  c and moens2013b] ivan vuli  c

and marie-
francine moens. 2013b. a study on id64
bilingual vector spaces from non-parallel data (and
nothing else). in proc. of emnlp.

[vuli  c and moens2015] ivan vuli  c and marie-francine
moens. 2015. bilingual id27s from
non-parallel document-aligned data applied to bilin-
gual lexicon induction. in proc. of acl.

[zou et al.2013] will y. zou, richard socher, daniel
cer, and christopher d. manning. 2013. bilingual
id27s for phrase-based machine transla-
tion. in proc. of emnlp.

