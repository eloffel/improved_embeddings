bilingual learning of multi-sense embeddings with discrete autoencoders

university of groningen

university of amsterdam

ivan titov

netherlands

gertjan van noord

university of groningen

netherlands

simon   suster

netherlands

6
1
0
2

 
r
a

 

m
0
3

 
 
]
l
c
.
s
c
[
 
 

1
v
8
2
1
9
0

.

3
0
6
1
:
v
i
x
r
a

s.suster@rug.nl

titov@uva.nl

g.j.m.van.noord@rug.nl

abstract

we present an approach to learning multi-sense
id27s relying both on monolingual
and bilingual information. our model consists
of an encoder, which uses monolingual and
bilingual context (i.e. a parallel sentence) to
choose a sense for a given word, and a decoder
which predicts context words based on the cho-
sen sense. the two components are estimated
jointly. we observe that the word representa-
tions induced from bilingual data outperform
the monolingual counterparts across a range
of evaluation tasks, even though crosslingual
information is not available at test time.

1

introduction

approaches to learning id27s (i.e. real-
valued vectors) relying on word context have received
much attention in recent years, and the induced rep-
resentations have been shown to capture syntactic
and semantic properties of words. they have been
evaluated intrinsically (mikolov et al., 2013a; baroni
et al., 2014; levy and goldberg, 2014) and have also
been used in concrete nlp applications to deal with
word sparsity and improve generalization (turian et
al., 2010; collobert et al., 2011; bansal et al., 2014;
passos et al., 2014). while most work to date has
focused on developing embedding models which rep-
resent a word with a single vector, some researchers
have attempted to capture polysemy explicitly and
have encoded properties of each word with multi-
ple vectors (huang et al., 2012; tian et al., 2014;
neelakantan et al., 2014; chen et al., 2014; li and
jurafsky, 2015).

in parallel to this work on multi-sense word em-
beddings, another line of research has investigated in-
tegrating multilingual data, with largely two distinct
goals in mind. the    rst goal has been to obtain repre-
sentations for several languages in the same semantic
space, which then enables the transfer of a model
(e.g., a syntactic parser) trained on annotated training
data in one language to another language lacking this
annotation (klementiev et al., 2012; hermann and
blunsom, 2014; gouws et al., 2014; chandar a p
et al., 2014). secondly, information from another
language can also be leveraged to yield better    rst-
language embeddings (guo et al., 2014). our paper
falls in the latter, much less explored category. we ad-
here to the view of multilingual learning as a means
of language grounding (faruqui and dyer, 2014b;
zou et al., 2013; titov and klementiev, 2012; snyder
and barzilay, 2010; naseem et al., 2009). intuitively,
polysemy in one language can be at least partially
resolved by looking at the translation of the word and
its context in another language (kaji, 2003; ng et
al., 2003; diab and resnik, 2002; ide, 2000; dagan
and itai, 1994; brown et al., 1991). better sense as-
signment can then lead to better sense-speci   c word
embeddings.

we propose a model that uses second-language
embeddings as a supervisory signal in learning multi-
sense representations in the    rst language. this su-
pervision is easy to obtain for many language pairs
as numerous parallel corpora exist nowadays. our
model, which can be seen as an autoencoder with a
discrete hidden layer encoding word senses, lever-
ages bilingual data in its encoding part, while the de-
coder predicts the surrounding words relying on the

right

context-word
prediction

decoder

p(xj|xi, s,   )

sense(xi: turn-off):    road   ,    feeling   ... (hidden)

sense prediction

encoder
p(s|xi, ci, c(cid:48)

i,   )

the next turn-off on the right

`a droite `a la bifurcation suivante

figure 1: model schema:
signal and the context-word predictor are learned jointly.

the sense encoder with bilingual

predicted senses. we strive to remain    exible as to
the form of parallel data used in training and support
both the use of word- and sentence-level alignments.

our    ndings are:
    the second-language signal effectively im-
proves the quality of multi-sense embeddings as
seen on a variety of intrinsic tasks for english,
with the results superior to that of the baseline
skip-gram model, even though the crosslingual
information is not available at test time.

    this    nding is robust across several settings,
such as varying dimensionality, vocabulary size
and amount of data.

    in the extrinsic pos-tagging task, the second-
language signal also offers improvements over
monolingually-trained multi-sense embeddings,
however, the standard skip-gram embeddings
turn out to be the most robust in this task.

we make the implementation of all the models
as well as the evaluation scripts available at http:
//github.com/rug-compling/bimu.

2 id27s with discrete

autoencoders

our method borrows its general structure from neu-
ral autoencoders (rumelhart et al., 1986; bengio et

al., 2013). autoencoders are trained to reproduce
their input by    rst mapping their input to a (lower
dimensional) hidden layer and then predicting an ap-
proximation of the input relying on this hidden layer.
in our case, the hidden layer is not a real-valued vec-
tor, but is a categorical variable encoding the sense
of a word. discrete-state autoencoders have been
successful in several natural language processing ap-
plications, including id52 and word align-
ment (ammar et al., 2014), semantic role induction
(titov and khoddam, 2015) and relation discovery
(marcheggiani and titov, 2016).

more formally, our model consists of two com-
ponents: an encoding part which assigns a sense to
a pivot word, and a reconstruction (decoding) part
recovering context words based on the pivot word
and its sense. as predictions are probabilistic (   soft   ),
the reconstruction step involves summation over all
potential word senses. the goal is to    nd embedding
parameters which minimize the error in recovering
context words based on the pivot word and the sense
assignment. parameters of both encoding and recon-
struction are jointly optimized. intuitively, a good
sense assignment should make the reconstruction step
as easy as possible. the encoder uses not only words
in the    rst-language sentence to choose the sense but
also, at training time, is conditioning its decisions
on the words in the second-language sentence. we
hypothesize that the injection of crosslingual informa-
tion will guide learning towards inducing more infor-
mative sense-speci   c word representations. conse-
quently, using this information at training time would
bene   t the model even though crosslingual informa-
tion is not available to the encoder at test time.

we specify the encoding part as a log-linear model:
p(s|xi, ci, c(cid:48)

  j+

(cid:88)

j   ci

i,   )     exp(cid:0)  (cid:62)
(cid:88)
k)(cid:1).

i,s(

  (cid:48)

  
|c(cid:48)
i|

k   c(cid:48)

i

1       
|ci|

(1)

to choose the sense s     s for a word xi, we use
the bag of context words ci from the    rst language
l, as well as the bag of context words c(cid:48)
i from the
second language l(cid:48).1 the context ci is de   ned as a
1we have also considered a formulation which included a
sense-speci   c bias bxi,s     r to capture relative frequency of
latent senses but it did not seem to affect performance.

multiset ci = {xi   n, . . . , xi   1, xi+1, . . . , xi+n}, in-
cluding words around the pivot word in the window
of size n to each side. we set n to 5 in all our exper-
iments. the crosslingual context c(cid:48)
i is discussed in
   3, where we either rely on word alignments or use
the entire second-language sentence as the context.
we distinguish between sense-speci   c embeddings,
denoted by        rd, and generic sense-agnostic ones,
denoted {  ,   (cid:48)}     rd for    rst and second language,
respectively. the number of sense-speci   c embed-
dings is the same for all words. we use    to denote
all these embedding parameters. they are learned
jointly, with the exception of the pre-trained second-
language embeddings.
the hyperparameter        r, 0            1 weights
the contribution of each language. setting    = 0
would drop the second-language component and use
only the    rst language. our formulation allows the
addition of new languages easily, provided that the
second-language embeddings live in the same seman-
tic space.

the reconstruction part predicts a context word xj
given the pivot xi and the current estimate of its s:

p(xj|xi, s,   ) =

(cid:80)
exp(  (cid:62)
i,s  j)
k   |v| exp(  (cid:62)

i,s  k)

,

(2)

where |v| is the vocabulary size. this is effectively
a skip-gram model (mikolov et al., 2013a) extended
to rely on senses.

2.1 learning and id173
as sense assignments are not observed during train-
ing, the learning objective includes marginalization
over word senses and thus can be written as:
p(xj|xi, s,   )p(s|xi, ci, c(cid:48)

(cid:88)

(cid:88)

(cid:88)

log

i,   ),

j   cxi

i

s   s

in which index i goes over all pivot words in the    rst
language, j over all context words to predict at each
i, and s marginalizes over all possible senses of the
word xi. in practice, we avoid the costly computation
of the id172 factor in the softmax computa-
tion of eq. (2) and use negative sampling (mikolov
et al., 2013b) instead of log p(xj|xi, s,   ):
log   (     (cid:62)

log   (  (cid:62)

(cid:88)

(3)

i,s  j) +

i,s  x),

x   n

where    is the sigmoid non-linearity function and   x
is a id27 from the sample of negative
(noisy) words n. optimizing the autoencoding ob-
jective is broadly similar to the learning algorithm
de   ned for multi-sense embedding induction in some
of the previous work (neelakantan et al., 2014; li
and jurafsky, 2015). note though that this previous
work has considered only monolingual context.
we use a minibatch training regime and seek to op-
timize the objective function l(b,   ) for each mini-
batch b. we found that optimizing this objective
directly often resulted in inducing very    at poste-
rior distributions. we therefore use a form of poste-
rior id173 (ganchev et al., 2010) where we
can encode our prior expectations that the posteri-
ors should be sharp. the regularized objective for a
minibatch is de   ned as

l(b,   ) +   h

h(qi),

(4)

(cid:88)

i   b

where h is the id178 function and qi are
the posterior distributions
from the encoder
(p(s|xi, ci, c(cid:48)
i,   )). this modi   ed objective can also
be motivated from a variational approximation per-
spective, see marcheggiani and titov (2016) for de-
tails. by varying the parameter   h     r, it is easy
to control the amount of id178 id173. for
  h > 0, the objective is optimized with    atter pos-
teriors, while   h < 0 infers more peaky posteriors.
when   h           , the id203 mass needs to be
concentrated on a single sense, resulting in an algo-
rithm similar to hard em. in practice, we found that
using hard-update training2, which is closely related
to the   h            setting, led to best performance.
2.2 obtaining word representations
at test time, we construct the word representations
by averaging all sense embeddings for a word xi and
weighting them with the sense expectations (li and
jurafsky, 2015)3:

  i =

p(s|xi, ci)  i,s.

(5)

(cid:88)

s   s

2i.e. updating only that embedding   i,s    for which

s    = arg maxs p(s|xi, ci, c(cid:48)

i,   ).

3although our training objective has sparsity-inducing prop-
erties, the posteriors at test time are not entirely peaked, which
makes weighting bene   cial.

unlike in training, the sense prediction step here
does not use the crosslingual context c(cid:48)
i since it is not
available in the evaluation tasks. in this work, instead
of marginalizing out the unobservable crosslingual
context, we simply ignore it in computation.

sometimes, even the    rst-language context is miss-
ing, as is the situation in many word similarity
tasks. in that case, we just use the uniform average,

1/|s|(cid:80)

s   s   i,s.

3 word af   liation from alignments

ai   m, ..., x(cid:48)

in de   ning the crosslingual signal we draw on a
heuristic inspired by devlin et al. (2014). the second-
language context words are taken to be the multiset
of words around and including the pivot af   liated to
xi:

i = {x(cid:48)
c(cid:48)
where x(cid:48)
ai is the word af   liated to xi and the parame-
ter m regulates the context window size. by choosing
m = 0, only the af   liated word is used as l(cid:48) context,
and by choosing m =    , the l(cid:48) context is the entire
sentence (   uniform alignment). to obtain the index
ai, we use the following:
1) if xi aligns to exactly one second-language word,

ai, ..., x(cid:48)

ai+m},

(6)

ai is the index of the word it aligns to.

2) if xi aligns to multiple words, ai is the index
of the aligned word in the middle (and rounding
down when necessary).
3) if xi is unaligned, c(cid:48)
i is empty, therefore no l(cid:48)

context is used.

their default values can be examined in the source
code available online.

4.2 bilingual data
in a large body of work on id73 repre-
sentations, europarl (koehn, 2005) is the preferred
source of parallel data. however, the domain of eu-
roparl is rather constrained, whereas we would like
to obtain word representations of more general lan-
guage, also to carry out an effective evaluation on
semantic similarity datasets where domains are usu-
ally broader. we therefore use the following paral-
lel corpora: news commentary (bojar et al., 2013)
(nc), yandex-1m4 (ru-en), czeng 1.0 (bojar et
al., 2012) (cz-en) from which we exclude the eu
legislation texts, and gigafren (callison-burch et al.,
2009) (fr-en). the sizes of the corpora are reported
in table 1. the word representations trained on the
nc corpora are evaluated only intrinsically due to
the small sizes.

fr, ru, cz, de, es

words
sent.
3-4 m .1-.2 m
1 m
24 m
10 m
126 m
670 m
23 m

corpus language
nc
ru-en ru
cz-en cz
fr-en fr
table 1: parallel corpora used in this paper. the word sizes
reported are based on the english part of the corpus. each
language pair in nc has a different english part, hence the
varying number of sentences per target language.

we use the cdec aligner (dyer et al., 2010) to word-
align the parallel corpora.

4 parameters and set-up
4.1 learning parameters
we use the adagrad optimizer (duchi et al., 2011)
with initial learning rate set to 0.1. we set the mini-
batch size to 1000, the number of negative samples
to 1, the sampling factor to 0.001 and the window
size parameter m to 5. all the embeddings are 50-
dimensional (unless speci   ed otherwise) and initial-
ized by sampling from the uniform distribution be-
tween [   0.05, 0.05]. we include in the vocabulary
all words occurring in the corpus at least 20 times.
we set the number of senses per word to 3 (see further
discussion in    6.4 and    7). all other parameters with

5 evaluation tasks

we evaluate the quality of our word representations
on a number of tasks, both intrinsic and extrinsic.

5.1 word similarity
we are interested here in how well the semantic simi-
larity ratings obtained from embedding comparisons
correlate to human ratings. for this purpose, we use
a variety of similarity benchmarks for english and
report the spearman    correlation scores between the
human ratings and the cosine ratings obtained from
our word representations. the scws benchmark
(huang et al., 2012) is probably the most suitable

4https://translate.yandex.ru/corpus

similarity dataset for evaluating multi-sense embed-
dings, since it allows us to perform the sense predic-
tion step based on the sentential context provided for
each word in the pair.

the other benchmarks we use provide the ratings
for the word pairs without context. ws-353 contains
353 human-rated word pairs (finkelstein et al., 2001),
while agirre et al. (2009) separate this benchmark
for similarity (ws-sim) and relatedness (ws-rel).
the rg-65 (rubenstein and goodenough, 1965) and
the mc-30 (miller and charles, 1991) benchmarks
contain nouns only. the mturk-287 (radinsky et
al., 2011) and mturk-771 (halawi et al., 2012) in-
clude word pairs whose similarity was crowdsourced
from amt. similarly, men (bruni et al., 2012) is an
amt-annotated dataset of 3000 word pairs. the yp-
130 (yang and powers, 2006) and verb-143 (baker
et al., 2014) measure verb similarity. rare-word
(luong et al., 2013) contains 2034 rare-word pairs.
finally, siid113x-999 (hill et al., 2014b) is intended
to measure pure similarity as opposed to relatedness.
for these benchmarks, we prepare the word repre-
sentations by taking a uniform average of all sense
embeddings per word. the evaluation is carried out
using the tool described in faruqui and dyer (2014a).
due to space constraints, we report the results by
averaging over all benchmarks (similarity), and in-
clude the individual results in the online repository.

5.2 supersense similarity
we also evaluate on a task measuring the similarity
between the embeddings   in our case uniformly av-
eraged in the case of multi-sense embeddings   and
a matrix of supersense features extracted from the
english semcor, using the qvec tool (tsvetkov et
al., 2015). we choose this method because it has
been shown to output scores that correlate well with
extrinsic tasks, e.g. text classi   cation and sentiment
analysis. we believe that this, in combination with
word similarity tasks from the previous section, can
give a reliable picture of the generic quality of word
embeddings studied in this work.

5.3 id52
as our downstream evaluation task, we use the
learned word representations to initialize the embed-
ding layer of a neural network tagging model. we use
the same convolutional architecture as li and juraf-

sky (2015): an input layer taking a concatenation of
neighboring embeddings as input, three hidden layers
with a recti   ed linear unit activation function and a
softmax output layer. we train for 10 epochs using
one sentence as a batch. other hyperparameters can
be examined in the source code. the multi-sense
id27s are inferred from the sentential
context (weighted average), as for the evaluation on
the scws dataset. we use the standard splits of the
wall street journal portion of the id32:
0   18 for training, 19   21 for development and 22   24
for testing.

6 results

we compare three embeddings models, skip-gram
(sg), multi-sense (mu) and bilingual multi-sense
(bimu), using our own implementation for each of
them. the    rst two can be seen as simpler variants of
the bimu model: in sg we omit the encoder entirely,
and in mu we omit the second-language (l(cid:48)) part of
the encoder in eq. (1). we train the sg and the mu
models on the english part of the parallel corpora.
those parameters common to all methods are kept
   xed during experiments. the values    and m for
controlling the second-language signal in bimu are
set on the pos-tagging development set (cf.    6.3).
the results on the scws benchmark (table 2)
show consistent improvements of the bimu model
over sg and mu across all parallel corpora, except
on the small cz-en (nc) corpus. we have also mea-
sured the 95% con   dence intervals of the difference
between the correlation coef   cients of bimu and
sg, following the method described in zou (2007).
according to these values, bimu signi   cantly out-
performs sg on ru-en, and on french, russian and
spanish nc corpora.5

next, ignoring any language-speci   c factors, we
would expect to observe a trend according to which
the larger the corpus, the higher the correlation score.
however, this is not what we    nd. among the largest
corpora, i.e. ru-en, cz-en and fr-en, the models
trained on ru-en perform surprisingly well, practi-
cally on par with the 23-times larger fr-en corpus.
similarly, the quality of the embeddings trained on
cz-en is generally lower than when trained on the

5i.e. counting those results in which the ci of the difference

does not include 0.

bimu-sg

4.79.8
0.9
4.18.8   0.6
1.75.9   2.6
7.112.0
2.2
6.712.8
0.6
4.210.3   2.0
5.511.6   0.6
7.313.3
1.1

task corpus

sg mu

s
w
c
s

y
t
i
r
a
l
i

m
i
s

ru-en
cz-en
fr-en

fr-en (nc)
ru-en (nc)
cz-en (nc)
de-en (nc)
es-en (nc)

ru-en
cz-en
fr-en

fr-en (nc)
ru-en (nc)
cz-en (nc)
de-en (nc)
es-en (nc)

c ru-en
cz-en
fr-en

e
v
q

s
o
p

ru-en
cz-en
fr-en

54.8
51.2
58.8

47.2
47.3
47.7
48.5
47.2

37.8
39.5
46.3
17.9
19.3
15.8
20.7
19.9

55.8
56.6
57.5
93.5
94.0
94.1

57.3
54.0
60.4

52.4
54.0
52.1
52.9
53.2

41.2
36.9
42.0

26.0
27.3
26.6
28.4
27.2

56.0
56.5
57.1

93.2
93.7
93.8

bimu
59.5
55.3
60.5
54.3
54.0
51.9
54.0
54.5
46.3
41.9
43.5
27.6
28.4
25.4
30.8
31.2
56.5
55.9
57.6
93.3
94.0
94.0

table 2: results, per-row best in bold. sg and mu are trained
on the english part of the parallel corpora. in bimu-sg, we report
the difference between bimu and sg, together with the 95% ci
of that difference. the similarity scores are averaged over 12
benchmarks described in    5.1. for id52, we report the
accuracy.

10 times smaller ru-en corpus. one explanation
for this might be different text composition of the
corpora, with ru-en matching the domain of the
evaluation task better than the larger two corpora.
also, fr-en is known to be noisy, containing web-
crawled sentences that are not parallel or not natural
language (denkowski et al., 2012). furthermore,
language-dependent effects might be playing a role:
for example, there are signs of czech being the least
helpful language among those studied. but while
there is evidence for that in all intrinsic tasks, the
situation in id52 does not con   rm this specu-
lation.

we relate our models to previously reported scws
scores from the literature using 300-dimensional
models in table 3. even though we train on a much
smaller corpus than the previous works,6 the bimu

6for example, li and jurafsky (2015) use the concatenation

of gigaword and wikipedia with more than 5b words.

model (300-dim.)

scws

sg
mu
bimu
chen et al. (2014)
neelakantan et al. (2014)
li and jurafsky (2015)

65.0
66.7
69.0
68.4
69.3
69.7

table 3: comparison to other works (reprinted), for the vocab-
ulary of top-6000 words. our models are trained on ru-en, a
much smaller corpus than those used in previous work.

model achieves a very competitive correlation score.
the results on similarity benchmarks and qvec
largely con   rm those on scws, despite the lack
of sentential context which would allow to weight
the contribution of different senses more accurately
for the multi-sense models. why, then, does simply
averaging the mu and bimu embeddings lead to
better results than when using the sg embeddings?
we hypothesize that the single-sense model tends to
over-represent the dominant sense with its generic,
one-vector-per-word representation, whereas the uni-
formly averaged embeddings yielded by the multi-
sense models better encode the range of potential
senses. similar observations have been made in the
context of selectional preference modeling of polyse-
mous verbs (greenberg et al., 2015).

in id52, the relationship between mu and
bimu models is similar as discussed above. overall,
however, neither of the multi-sense models outper-
forms the sg embeddings. the neural network tagger
may be able to implicitly perform disambiguation on
top of single-sense sg embeddings, similarly to what
has been argued in li and jurafsky (2015). the tag-
ging accuracies obtained with mu on cz-en and
fr-en are similar to the one obtained by li and
jurafsky with their multi-sense model (93.8), while
the accuracy of sg is more competitive in our case
(around 94.0 compared to 92.5), although they use a
larger corpus for training the word representations.

in all tasks, the addition of the bilingual compo-
nent during training increases the accuracy of the
encoder for most corpora, even though the bilingual
information is not available during evaluation.

6.1 the amount of (parallel) data
fig. 2a displays how the semantic similarity as mea-
sured on scws evolves as a function of increasingly

(a)

(b)

figure 2: (a) effect of amount of data used in learning on the scws correlation scores. (b) effect of embedding dimensionality on
the models trained on ru-en and evaluated on scws with either full vocabulary or the top-6000 words.

larger sub-samples from fr-en, our largest parallel
corpus. the bimu embeddings show relatively sta-
ble improvements over mu and especially over sg
embeddings. the same performance as that of sg at
100% is achieved by mu and bimu sooner, using
only around 40/50% of the corpus.

6.2 the dimensionality and frequent words
it is argued in li and jurafsky (2015) that often just in-
creasing the dimensionality of the sg model suf   ces
to obtain better results than that of their multi-sense
model. we look at the effect of dimensionality on
semantic similarity in    g. 2b, and see that simply in-
creasing the dimensionality of the sg model (to any
of 100, 200 or 300 dimensions) is not suf   cient to out-
perform the mu or bimu models. when constrain-
ing the vocabulary to 6,000 most frequent words,
the representations obtain higher quality. we can
see that the models, especially sg, bene   t slightly
more from the increased dimensionality when look-
ing at these most frequent words. this is according
to expectations   frequent words need more represen-
tational capacity due to their complex semantic and
syntactic behavior (atkins and rundell, 2008).

6.3 the role of bilingual signal
the degree of contribution of the second language l(cid:48)
during learning is affected by two parameters,    for
the trade-off between the importance of    rst and sec-
ond language in the sense prediction part (encoder)
and the value of m for the size of the window around
the second-language word af   liated to the pivot. fig.
3a suggests that the context from the second language

is useful in sense prediction, and that it should be
weighted relatively heavily (around 0.7 and 0.8, de-
pending on the language).

regarding the role of the context-window size in
sense disambiguation, the wsd literature has re-
ported both smaller (more local) and larger (more
topical) monolingual contexts to be useful, see e.g.
ide and v  eronis (1998) for an overview. in    g. 3b we
   nd that considering a very narrow context in the sec-
ond language   the af   liated word only or a m = 1
window around it   performs the best, and that there
is little gain in using a broader window. this is un-
derstandable since the l(cid:48) representation participating
in the sense selection is simply an average over all
generic embeddings in the window, which means that
the averaged representation probably becomes noisy
for large m, i.e. more irrelevant words are included
in the window. however, the negative effect on the
accuracy is still relatively small, up to around    0.1
for the models using french and russian as the sec-
ond languages, and    0.25 for czech when setting
m =    . the in   nite window size setting, corre-
sponding to the sentence-only alignment, performs
well also on scws, improving on the monolingual
multi-sense baseline on all corpora (table 4).

model

ru-en cz-en fr-en

mu
bimu, m =    

63.29
65.61

59.12
62.07

64.19
64.36

table 4: comparison of scws correlation scores of bimu
trained with in   nite l(cid:48) window to the mu baseline (vocabulary
of top-6000 words).

llllllllll55565758596061102030405060708090100% of fr   en (670m)correlationlbimumusgllllllll5455565758596061626364656667686950100200300dimensionalitycorrelationlbimumusg6kfull(a)

(b)

figure 3: controlling the bilingual signal. (a) effect of varying the parameter    for controlling the importance of second-language
context (0.1-least important, 0.9-most important). (b) effect of second-language window size m on the accuracy. in both (a) and (b)
the reported accuracies are measured on the pos-tagging development set.

6.4 the number of senses
in our work, the number of senses k is a model pa-
rameter, which we keep    xed to 3 throughout the
empirical study. we comment here brie   y on other
choices of k     {2, 4, 5}. we have found k = 2 to
be a good choice on the ru-en and fr-en corpora
(but not on cz-en), with an around 0.2-point im-
provement over k = 3 on scws and in id52.
with the larger values of k, the performance tends to
degrade. for example, on ru-en, the k = 5 score on
scws is about 0.6 point below our default setting.

7 additional related work

multi-sense models. one line of research has dealt
with sense induction as a separate, id91 problem
that is followed by an embedding learning compo-
nent (huang et al., 2012; reisinger and mooney,
2010). in another, the sense assignment and the em-
beddings are trained jointly (neelakantan et al., 2014;
tian et al., 2014; li and jurafsky, 2015; bartunov
et al., 2015). neelakantan et al. (2014) propose an
extension of skip-gram (mikolov et al., 2013a) by in-
troducing sense-speci   c parameters together with the
id116-inspired    centroid    vectors that keep track
of the contexts in which word senses have occurred.
they explore two model variants, one in which the
number of senses is the same for all words, and an-
other in which a threshold value determines the num-
ber of senses for each word. the results comparing
the two variants are inconclusive, with the advantage
of the dynamic variant being virtually nonexistent.

in our work, we use the static approach. whenever
there is evidence for less senses than the number of
available sense vectors, this is unlikely to be a seri-
ous issue as the learning would concentrate on some
of the senses, and these would then be the preferred
predictions also at test time. li and jurafsky (2015)
build upon the work of neelakantan et al. with a
more principled method for introducing new senses
using the chinese restaurant processes (crp). our
experiments con   rm the    ndings of neelakantan et
al. that multi-sense embeddings improve skip-gram
embeddings on intrinsic tasks, as well as those of li
and jurafsky, who    nd that multi-sense embeddings
offer little bene   t to the neural network learner on
extrinsic tasks. our discrete-autoencoding method
when viewed without the bilingual part in the encoder
has a lot in common with their methods.

multilingual models. the research on using multi-
lingual information in the learning of multi-sense em-
bedding models is scarce. guo et al. (2014) perform a
sense induction step based on id91 translations
prior to learning id27s. once the trans-
lations are clustered, they are mapped to a source
corpus using wsd heuristics, after which a recur-
rent neural network is trained to obtain sense-speci   c
representations. unlike in our work, the sense induc-
tion and embedding learning components are entirely
separated, without a possibility for one to in   uence
another. in a similar vein, bansal et al. (2012) use
bilingual corpora to perform soft word id91, ex-
tending the previous work on the monolingual case of

lllllllll92.993.193.393.593.793.90.10.20.30.40.50.60.70.80.9laccuracylcz   enfr   enru   enllllll   0.3   0.2   0.10.001234  window size in l'deviation in accuracylcz   enfr   enru   enlin and wu (2009). single-sense representations in
the multilingual context have been studied more ex-
tensively (lu et al., 2015; faruqui and dyer, 2014b;
hill et al., 2014a; zhang et al., 2014; faruqui and
dyer, 2013; zou et al., 2013), with a goal of bringing
the representations in the same semantic space. a
related line of work concerns the crosslingual setting,
where one tries to leverage training data in one lan-
guage to build models for typically lower-resource
languages (hermann and blunsom, 2014; gouws et
al., 2014; chandar a p et al., 2014; soyer et al., 2014;
klementiev et al., 2012; t  ackstr  om et al., 2012).

the recent works of kawakami and dyer (2015)
and nalisnick and ravi (2015) are also of interest.
the latter work on the in   nite skip-gram model in
which the embedding dimensionality is stochastic is
relevant since it demonstrates that their embeddings
exploit different dimensions to encode different word
meanings. just like us, kawakami and dyer (2015)
use bilingual supervision, but in a more complex
lstm network that is trained to predict word trans-
lations. although they do not represent different
word senses separately, their method produces repre-
sentations that depend on the context. in our work,
the second-language signal is introduced only in the
sense prediction component and is    exible   it can
be de   ned in various ways and can be obtained from
sentence-only alignments as a special case.

8 conclusion

we have presented a method for learning multi-sense
embeddings that performs sense estimation and con-
text prediction jointly. both mono- and bilingual in-
formation is used in the sense prediction during train-
ing. we have explored the model performance on a
variety of tasks, showing that the bilingual signal im-
proves the sense predictor, even though the crosslin-
gual information is not available at test time. in this
way, we are able to obtain word representations that
are of better quality than the monolingually-trained
multi-sense representations, and that outperform the
skip-gram embeddings on intrinsic tasks. we have
analyzed the model performance under several con-
ditions, namely varying dimensionality, vocabulary
size, amount of data, and size of the second-language
context. for the latter parameter, we    nd that bilin-
gual information is useful even when using the entire

sentence as context, suggesting that sentence-only
alignment might be suf   cient in certain situations.

acknowledgments
we would like to thank jiwei li for providing his
tagger implementation, and robert grimm, diego
marcheggiani and the anonymous reviewers for use-
ful comments. the computational work was carried
out on peregrine hpc cluster of the university of
groningen. the second author was supported by
nwo vidi grant 016.153.327.

references
eneko agirre, enrique alfonseca, keith hall, jana kraval-
ova, marius pas  ca, and aitor soroa. 2009. a study
on similarity and relatedness using distributional and
id138-based approaches. in naacl-hlt.

waleed ammar, chris dyer, and noah a. smith. 2014.
conditional random    eld autoencoders for unsuper-
vised id170. in nips.

sue b. t. atkins and michael rundell. 2008. the oxford
guide to practical id69. oxford university
press.

simon baker, roi reichart, and anna korhonen. 2014.
an unsupervised model for instance level subcatego-
rization acquisition. in emnlp.

mohit bansal, john denero, and dekang lin. 2012. unsu-
pervised translation sense id91. in naacl-hlt.
mohit bansal, kevin gimpel, and karen livescu. 2014.
tailoring continuous word representations for depen-
dency parsing. in acl.

marco baroni, georgiana dinu, and germ  an kruszewski.
2014. don   t count, predict! a systematic comparison of
context-counting vs. context-predicting semantic vec-
tors. in acl.

sergey bartunov, dmitry kondrashkin, anton osokin,
and dmitry vetrov. 2015. breaking sticks and am-
arxiv preprint
biguities with adaptive skip-gram.
arxiv:1502.07257.

yoshua bengio, aaron courville, and pierre vincent.
2013. representation learning: a review and new per-
spectives. ieee transactions on pattern analysis and
machine intelligence, 35(8):1798   1828.

ond  rej bojar, zden  ek   zabokrtsk  y, ond  rej du  sek, pe-
tra galu  s  c  akov  a, martin majli  s, david mare  cek, ji  r    
mar  s    k, michal nov  ak, martin popel, and ale  s tam-
chyna. 2012. the joy of parallelism with czeng 1.0.
in lrec.

ond  rej bojar, christian buck, chris callison-burch,
christian federmann, barry haddow, philipp koehn,
christof monz, matt post, radu soricut, and lucia

specia. 2013. findings of the 2013 workshop on
id151. in wmt.

peter f brown, stephen a della pietra, vincent j della
pietra, and robert l mercer. 1991. word-sense disam-
biguation using statistical methods. in acl.

elia bruni, gemma boleda, marco baroni, and nam-
khanh tran. 2012. id65 in techni-
color. in acl.

chris callison-burch, philipp koehn, christof monz, and
josh schroeder. 2009. findings of the 2009 workshop
on id151. in wmt.

sarath chandar a p, stanislas lauly, hugo larochelle,
mitesh m. khapra, balaraman ravindran, vikas c.
raykar, and amrita saha. 2014. an autoencoder ap-
proach to learning bilingual word representations. in
nips.

xinxiong chen, zhiyuan liu, and maosong sun. 2014. a
uni   ed model for word sense representation and disam-
biguation. in emnlp.

ronan collobert, jason weston, l  eon bottou, michael
karlen, koray kavukcuoglu, and pavel kuksa. 2011.
natural language processing (almost) from scratch. the
journal of machine learning research, 12:2493   2537.
ido dagan and alon itai. 1994. word sense disam-
biguation using a second language monolingual corpus.
computational linguistics, 20(4):563   596.

michael denkowski, greg hanneman, and alon lavie.
2012. the cmu-avenue french-english translation
system. in wmt.

jacob devlin, rabih zbib, zhongqiang huang, thomas
lamar, richard schwartz, and john makhoul. 2014.
fast and robust neural network joint models for statisti-
cal machine translation. in acl.

mona diab and philip resnik. 2002. an unsupervised
method for word sense tagging using parallel corpora.
in acl.

john duchi, elad hazan, and yoram singer.

2011.
adaptive subgradient methods for online learning and
stochastic optimization. the journal of machine learn-
ing research, 12:2121   2159.

chris dyer, adam lopez, juri ganitkevitch, johnathan
weese, ferhan ture, phil blunsom, hendra setiawan,
vladimir eidelman, and philip resnik. 2010. cdec: a
decoder, alignment, and learning framework for    nite-
state and context-free translation models. in acl.

manaal faruqui and chris dyer. 2013. an information
in

theoretic approach to bilingual word id91.
acl.

manaal faruqui and chris dyer. 2014a. community eval-
uation and exchange of word vectors at wordvectors.org.
in acl system demonstrations.

manaal faruqui and chris dyer. 2014b. improving vector
space word representations using multilingual correla-
tion. in eacl.

lev finkelstein, evgeniy gabrilovich, yossi matias, ehud
rivlin, zach solan, gadi wolfman, and eytan ruppin.
2001. placing search in context: the concept revisited.
in www.

kuzman ganchev, jo  ao grac  a, jennifer gillenwater, and
ben taskar. 2010. posterior id173 for struc-
tured latent variable models. the journal of machine
learning research, 11:2001   2049.

stephan gouws, yoshua bengio, and greg corrado.
2014. bilbowa: fast bilingual distributed repre-
sentations without word alignments. arxiv preprint
arxiv:1410.2455.

clayton greenberg, asad sayeed, and vera demberg.
2015. improving unsupervised vector-space thematic
   t evaluation via role-   ller prototype id91.
in
naacl.

jiang guo, wanxiang che, haifeng wang, and ting liu.
2014. learning sense-speci   c id27s by
exploiting bilingual resources. in coling.

guy halawi, gideon dror, evgeniy gabrilovich, and
yehuda koren. 2012. large-scale learning of word
relatedness with constraints. in kdd.

karl moritz hermann and phil blunsom. 2014. multi-
lingual models for compositional distributed semantics.
in acl.

felix hill, kyunghyun cho, s  ebastien jean, coline devin,
and yoshua bengio. 2014a. embedding word simi-
larity with id4. arxiv preprint
arxiv:1412.6448.

felix hill, roi reichart, and anna korhonen. 2014b.
evaluating semantic models with
arxiv preprint

siid113x-999:
(genuine) similarity estimation.
arxiv:1408.3456.

eric h. huang, richard socher, christopher d. manning,
and andrew y. ng. 2012. improving word representa-
tions via global context and multiple word prototypes.
in acl.

nancy ide and jean v  eronis. 1998. introduction to the
special issue on id51: the state
of the art. computational linguistics, 24(1):2   40.

nancy ide. 2000. cross-lingual sense determination: can
it work? computers and the humanities, 34(1-2):223   
234.

hiroyuki kaji. 2003. word sense acquisition from bilin-

gual comparable corpora. in naacl-hlt.

kazuya kawakami and chris dyer. 2015. learning to rep-
resent words in context with multilingual supervision.
arxiv preprint arxiv:1511.04623.

alexandre klementiev, ivan titov, and binod bhattarai.
2012. inducing crosslingual distributed representations
of words. in coling.

philipp koehn. 2005. europarl: a parallel corpus for sta-
tistical machine translation. in mt summit, volume 5.

herbert rubenstein and john b goodenough. 1965. con-
textual correlates of synonymy. communications of the
acm, 8(10):627   633.

david e. rumelhart, geoffrey e. hinton, and ronald j.
williams. 1986. learning internal representations by
error propagation. in david e. rumelhart, james l.
mcclelland, and pdp research group, editors, par-
allel distributed processing: explorations in the mi-
crostructure of cognition, vol. 1. mit press.

benjamin snyder and regina barzilay. 2010. climbing
the tower of babel: unsupervised multilingual learn-
ing. in icml.

hubert soyer, pontus stenetorp, and akiko aizawa. 2014.
leveraging monolingual data for crosslingual composi-
tional word representations. corr, abs/1412.6334.

oscar t  ackstr  om, ryan mcdonald, and jakob uszkoreit.
2012. cross-lingual word clusters for direct transfer of
linguistic structure. in naacl-hlt.

fei tian, hanjun dai, jiang bian, bin gao, rui zhang,
enhong chen, and tie-yan liu. 2014. a probabilistic
model for learning multi-prototype id27s.
in coling.

ivan titov and ehsan khoddam. 2015. unsupervised
induction of semantic roles within a reconstruction-
error minimization framework. in naacl.

ivan titov and alexandre klementiev. 2012. crosslingual

induction of semantic roles. in acl.

yulia tsvetkov, manaal faruqui, wang ling, guillaume
lample, and chris dyer. 2015. evaluation of word vec-
tor representations by subspace alignment. in emnlp.
joseph turian, lev ratinov, and yoshua bengio. 2010.
word representations: a simple and general method for
semi-supervised learning. in acl.

dongqiang yang and david m. w. powers. 2006. verb

similarity on the taxonomy of id138. in gwc.

jiajun zhang, shujie liu, mu li, ming zhou, and
2014. bilingually-constrained

chengqing zong.
phrase embeddings for machine translation. in acl.

will y. zou, richard socher, daniel cer, and christo-
pher d. manning. 2013. bilingual id27s
for phrase-based machine translation. in emnlp.

guang yong zou. 2007. toward using con   dence inter-
vals to compare correlations. psychological methods,
12(4).

omer levy and yoav goldberg. 2014. linguistic reg-
ularities in sparse and explicit word representations.
conll.

jiwei li and dan jurafsky. 2015. do multi-sense em-
beddings improve natural language understanding? in
emnlp.

dekang lin and xiaoyun wu. 2009. phrase id91 for

discriminative learning. in acl-ijcnlp of afnlp.

ang lu, weiran wang, mohit bansal, kevin gimpel, and
karen livescu. 2015. deep multilingual correlation
for improved id27s. in naacl.

minh-thang luong, richard socher, and christopher d
manning. 2013. better word representations with
id56s for morphology. in conll.
diego marcheggiani and ivan titov. 2016. discrete-state
id5 for joint discovery and factor-
ization of relations. transactions of the association for
computational linguistics, 4.

tomas mikolov, kai chen, greg corrado, and jeffrey
dean. 2013a. ef   cient estimation of word representa-
tions in vector space. in iclr workshop papers.

tomas mikolov, ilya sutskever, kai chen, greg corrado,
and jeffrey dean. 2013b. distributed representations
of words and phrases and their compositionality. in
nips.

george a miller and walter g charles. 1991. contex-
tual correlates of semantic similarity. language and
cognitive processes, 6(1):1   28.

eric nalisnick and sachin ravi.
dimensional id27s.
arxiv:1511.05392.

2015.
in   nite
arxiv preprint

tahira naseem, benjamin snyder, jacob eisenstein, and
regina barzilay. 2009. multilingual part-of-speech
tagging: two unsupervised approaches. journal of
arti   cial intelligence research, 36:1   45.

arvind neelakantan, jeevan shankar, alexandre pas-
sos, and andrew mccallum. 2014. ef   cient non-
parametric estimation of multiple embeddings per word
in vector space. in emnlp.

hwee tou ng, bin wang, and yee seng chan. 2003.
exploiting parallel texts for id51:
an empirical study. in acl.

alexandre passos, vineet kumar, and andrew mccallum.
2014. lexicon infused phrase embeddings for named
entity resolution. in conll.

kira radinsky, eugene agichtein, evgeniy gabrilovich,
and shaul markovitch. 2011. a word at a time: com-
puting word relatedness using temporal semantic anal-
ysis. in www.

joseph reisinger and j. raymond mooney. 2010. multi-
prototype vector-space models of word meaning. in
naacl-hlt.

