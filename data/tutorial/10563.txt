ssp: semantic space projection

for id13 embedding with text descriptions

han xiao   , minlie huang   , lian meng, xiaoyan zhu

state key lab. of intelligent technology and systems,
national lab. for information science and technology,

dept. of computer science and technology, tsinghua university, beijing 100084, pr china
   correspondence authors: http://www.ibookman.net, http://www.aihuang.org

bookman@vip.163.com; {aihuang,zxy-dcs}@tsinghua.edu.cn; mengl15@foxmail.com;

7
1
0
2

 

n
u
j
 

7
1

 
 
]
l
c
.
s
c
[
 
 

3
v
5
3
8
4
0

.

4
0
6
1
:
v
i
x
r
a

abstract

id13 embedding represents entities and relations
in id13 as low-dimensional, continuous vectors,
and thus enables id13 compatible with machine
learning models. though there have been a variety of mod-
els for id13 embedding, most methods merely
concentrate on the fact triples, while supplementary textual
descriptions of entities and relations have not been fully em-
ployed. to this end, this paper proposes the semantic space
projection (ssp) model which jointly learns from the sym-
bolic triples and textual descriptions. our model builds inter-
action between the two information sources, and employs tex-
tual descriptions to discover semantic relevance and offer pre-
cise semantic embedding. extensive experiments show that
our method achieves substantial improvements against base-
lines on the tasks of id13 completion and entity
classi   cation. papers, posters, slides, datasets and codes:
http://www.ibookman.net/conference.html

introduction

id13 provides an effective basis for nlp in
many tasks such as id53, web search and se-
mantic analysis. in order to provide a numerical computa-
tion framework for id13, id13 em-
bedding projects the entities and relations to a continuous
low-dimensional vector space. more speci   cally, a fact in
id13 is usually represented as a symbolic triple
(h, r, t), while id13 embedding attempts to rep-
resent the symbols with vectors, say h, r, t. to this end,
a number of embedding methods have been proposed, such
as transe (bordes et al. 2013), ptranse (lin, liu, and sun
2015), kg2e (he et al. 2015), etc.

as a key branch of embedding models, the translation-
based methods adopt the principle of translating the head
entity to the tail one by a relation-speci   c vector, or formally
as h + r = t. as fig.1 shows, in the id13, the
entities such as h, t have textual descriptions, which contain
much supplementary semantic information about knowledge
triples.

copyright c(cid:13) 2017, association for the advancement of arti   cial
intelligence (www.aaai.org). all rights reserved.

figure 1: textual descriptions for entities in a fact triple.

despite the success of conventional id13 em-
bedding models, there are still two reasons why textual de-
scriptions would be necessary in this task: discovering se-
mantic relevance and offering precise semantic expression.
firstly, the semantic relevance between entities is capa-
ble to recognize the true triples, which are dif   cult to be
inferred only with fact triples. for example, the triple (anna
roosevelt, parents, franklin roosevelt), indicates    franklin
roosevelt    is the parent of    anna roosevelt   . however,
it   s quite dif   cult to infer this fact merely from other sym-
bolic triples.
in contrast, in the textual description of the
head entity, there are many keywords such as    roosevelt   
and    daughter of the president   , which may infer the fact
triple easily. speci   cally, we measure the possibility of a
triple by projecting the loss onto a hyperplane that repre-
sents the semantic relevance between entities. thus, it is
always possible to accept a fact triple so long as the l2-norm
of the projected loss vector onto the semantic hyperplane is
suf   ciently small.

secondly, precise semantic expression could promote
the discriminative ability between two triples.
for in-
stance, when we query about the profession of    daniel
sturgeon   , there are two possible candidates:    politician   
and    lawyer   . it   s hard to distinguish if only focusing on
the symbolic triples. however, the textual description of
   daniel sturgeon    is full of politics-related keywords such
as    democratic party   ,    state legislature    etc. and even
   politician   . the textual descriptions help to re   ne the
topic of    daniel sturgeon    in a more precise way from the
social celebrities to the government of   cers, which makes
the true answer    politician    more preferable. formally,
even though the loss vectors of the two facts are almost
of equal norm, after respectively projected onto the    politi-
cian    and    lawyer    related semantic hyperplanes, the losses

are distinguished reasonably. in this way, precise semantic
expression re   nes the embedding.

figure 2: simple illustration of transe and ssp where
h + r     t is the loss vector. the loss vectors of the two
triples in (a) are length-equal, thus, it is hard to identify the
correctness.in (b), we introduce a semantic hyperplane, and
project the loss vectors to the hyperplane to consider the se-
mantics of triples.

the existing embedding methods with textual semantics
such as dkrl (xie et al. 2016) and    jointly    (zhong et al.
2015), have achieved much success. but there is still an is-
sue to be addressed, the weak-correlation modeling issue
that current models could hardly characterize the strong cor-
relations between texts and triples. in dkrl, for a triple,
the embedding vector of the head entity is translated to that
of the tail one as well as possible, and then the vectors of
texts and entities are concatenated as the    nal embedding
vectors. while in the    jointly    model the authors attempt to
generate coherent embeddings of the corresponding entity
and text. both dkrl and    jointly    apply    rst-order con-
straints which are weak in capturing the correlation of texts
and triples. it   s noteworthy that triple embedding is always
the main procedure and textual descriptions must inter-
act with triples for better embedding. only in this way, the
semantic effects could make more senses. for the above ex-
ample of    daniel sturgeon   , the textual descriptions imply
two candidate answers    banker    and    politician   . thus,
by considering both triples and texts, we can obtain the true
answer. therefore, we focus on the stronger semantic inter-
action by projecting triple embedding onto a semantic sub-
space such as hyperplane, as shown in fig.2. mathemati-
cally, the quadratic constraint is adopted to model the strong
correlation, thus the embedding topologies are suf   ciently
semantics-speci   c.

we evaluate the effectiveness of our model semantic sub-
space projection (ssp) with two tasks on three benchmark
datasets that are the subsets of id138 (miller 1995) and
freebase (bollacker et al. 2008). experimental results on
the datasets show that our model consistently outperforms
the other baselines with remarkable improvement.

contributions. we propose a id13 embed-
ding method ssp which models the strong correlations be-
tween the symbolic triples and the textual descriptions by
performing the embedding process in a semantic subspace.
besides, our method outperforms all the baselines on the
tasks of id13 completion and entity classi   ca-
tion, which justi   es the effectiveness of our proposal.

related work

we have surveyed the previous studies and categorized the
embedding methods into two branches: triple-only em-
bedding models that use only symbolic triples and    text-
aware    embedding models that employ textual descriptions.

triple-only embedding models
transe (bordes et al. 2013) is a pioneering work for this
branch, which translates the head entity to the tail one by
the relation vector, or h + r = t. naturally, the l2 norm
of the loss vector is the score function, which measures the
plausibility of a triple and a smaller score is better.

the following variants transform entities into different
subspaces. manifolde (xiao, huang, and zhu 2016a) opens
a classic branch, where a manifold is applied for the trans-
lation. transh (wang et al. 2014b) utilizes the relation-
speci   c hyperplane to lay the entities. transr (lin et al.
2015) applies the relation-related matrix to rotate the em-
bedding space. similar researches include transg (xiao,
huang, and zhu 2016b), transd (ji et al. ), (feng et al.
2016) and transm (fan et al. 2014).

further researches encode additional structural informa-
tion into embedding. ptranse (lin, liu, and sun 2015) is a
path-based model, simultaneously considering the informa-
tion and con   dence level of a path in the id13.
(wang, wang, and guo 2015) incorporate the rules to re-
strict the embeddings for the complex relation types such
as 1-n, n-1 and n-n. sse (guo et al. 2015) aims at dis-
covering the geometric structure of embedding topologies
and then based on these assumptions, designs a semantically
smoothing score function. also, kg2e (he et al. 2015) in-
volves probabilistic analysis to characterize the uncertainty
concepts of id13. there are also some other
works such as se (bordes et al. 2011), lfm (jenatton et
al. 2012), ntn (socher et al. 2013) and rescal (nickel,
tresp, and kriegel 2011), transa (xiao et al. 2015), etc.

text-aware embedding models
   text-aware    embedding, which attempts to representing
id13 with textual information, generally dates
back to ntn (socher et al. 2013). ntn makes use of en-
tity name and embeds an entity as the average word embed-
ding vectors of the name. (wang et al. 2014a) attempts to
aligning the id13 with the corpus then jointly
conducting knowledge embedding and id27.
however, the necessity of the alignment information limits
this method both in performance and practical applicabil-
ity. thus, (zhong et al. 2015) proposes the    jointly    method
that only aligns the freebase entity to the corresponding
wiki-page. dkrl (xie et al. 2016) extends the translation-
based embedding methods from the triple-speci   c one to the
   text-aware    model. more importantly, dkrl adopts a
id98-structure to represent words, which promotes the ex-
pressive ability of word semantics. generally speaking, by
jointly modeling knowledge and texts, text-aware embed-
ding models obtains the state-of-the-art performance.

methodology

in this section, we    rst introduce our model and then provide
two different perspective to address the advantages of our
model. first of all, let us introduce some notations: all the
symbols h, t indicate the head and tail entity, respectively. h
(or t) is the embedding of the entity from the triples, sh (or
st) is the semantic vector generated from the texts, and d is
the dimension of embedding.

the data involved in our model are the knowledge triples
and the textual descriptions of entities. in experiments, we
adopt the    entity descriptions    of freebase and the textual
de   nitions of id138 as textual information.

model description
previous analysis in the introduction suggests to character-
ize the strong correlation between triples and texts. for the
purpose of interacting between the symbolic triples and tex-
tual descriptions, this paper attempts to restrict the embed-
ding procedure of a speci   c triple in the semantic subspace.
speci   cally, we leverage a hyperplane with normal vector
= s(sh, st) as the subspace, where s : r2d (cid:55)    rd is
.
s
the semantic composition function which will be discussed
in the next section, and sh, st is the head-speci   c and tail-
speci   c semantic vectors, respectively.
the score function in the translation-based methods is
||h + r     t||2
2, which means the triple embedding focuses on
= h + r     t . according to our motiva-
.
the loss vector e
tion, assuming e is length-   xed, the target is to maximize the
component inside the hyperplane, which is ||e     s(cid:62)es||2
2. in
detail, the component of the loss in the normal vector direc-
tion is (s(cid:62)es), then the other orthogonal one, that is inside
the hyperplane, is (e     s(cid:62)es).

it   s natural that the norm of the loss vector should also be
constrained. to this end, we introduce a factor    to balance
the two parts, formally as:

fr(h, t) =      ||e     s(cid:62)es||2

2 + ||e||2

2

where    is a suitable hyper-parameter. moreover, a smaller
score means the triple is more plausible. for clarity, the
de   nitions of the symbols are boxed. notably, the projec-
tion part in our score function is negative, so smaller value
means less loss.

semantic vector generation
there are at least two methods that could be leveraged to
generate the semantic vectors: topic model (blei 2012) such
as lsa, lda, nmf (stevens et al. 2012) and word em-
bedding such as cbow (mikolov, yih, and zweig 2013),
skip-gram (mikolov et al. 2013). more concretely, this
paper adopts the topic model, treating each entity descrip-
tion as a document and then obtains the topic distribution
of document as the semantic vector of entity. the entities
are usually organized by the topic in knowledge base, for
example,    entity type    is used in freebase to categorize en-
tities. therefore, we conjecture that the topic model could
be more suitable. notably, the id27 would also
work well though maybe not better.

given the pre-trained semantic vectors, our model    xes
them and then optimizes the other parameters. we call this
setting standard (short as std.). the reason why we could
not adapt all the parameters, is that the training procedure
would re   ll the semantic vectors and    ush the semantics
out. for the purpose of jointly learning the semantics and
the embeddings, we conduct the topic model and the em-
bedding model, simultaneously. in this way, the symbolic
triples also impose a positive effect on the textual semantics
and we call this setting joint.

as each component of a semantic vector indicates the rel-
evant level to a topic, we suggest the semantic composition
should take the addition form:
s(sh, st) =

sh + st
||sh + st||2

2

where the id172 is applied to make a normal vector.
since the largest components represent the topics, the ad-
dition operator corresponds to the union of topics, making
the composition indicate the entire semantics. for example,
when sh = (0.1, 0.9, 0.0) and st = (0.8, 0.0, 0.2), the topic
of the head entity is #2 and that of the tail is #1, while the
composition is s = (0.45, 0.45, 0.10), corresponding to the
topic of #1,#2, which is accordant to our intuition.
correlation perspective
speci   cally, our model attempts to lay the loss h(cid:48)     t onto
the hyperplane, where h(cid:48)
.
= h + r is the translated head en-
tity. mathematically, if a line lies on a hyperplane, so do
all the points of this line. correspondingly, the loss lays on
the hyperplane, implying the head and tail entity also lay
on it, as the beginning and ending point. thus, there ex-
ists the important restriction, that the entities co-occur in
a triple should be embedded in the semantic space com-
posed by the associated textual semantics. this restriction
is implemented as a quadric form to characterize the strong
correlation between texts and triples, in other words, to in-
teract with both the information sources. a strong interac-
tion between the textual descriptions and symbolic triples
complements each other in a more semantics-speci   c form,
which guarantees the semantic effects. more concretely, the
embeddings are decided in the training procedure not only
by triples but also by textual semantics, based on which,
our embedding topologies are semantically different from
the other methods.
semantic perspective
there are two semantic effects for textual descriptions: dis-
covering semantic relevance and offering precise semantic
expression. our model characterizes the strong correla-
tions with a semantic hyperplane, which is capable of tak-
ing the advantages of both two semantic effects.

firstly, according to the correlation perspective, the enti-
ties which are semantically relevant, approximately lay on
a consistent hyperplane. therefore, the loss vector between
them (h(cid:48)     t) is also around the hyperplane. based on this
geometric insight, when a head entity matches a negative
tail, the triple is far from the hyperplane, making a large loss
to be classi   ed. conversely, even if a correct triple makes

much loss, the score function after projected onto the hyper-
plane could be relatively smaller (or better). by this mean,
the semantic relevance achieved from the texts, promotes
embedding. for instance, the fact triple (portsmouth foot-
ball club, locate, portsmouth) could hardly be inferred only
within the triple embedding. it ranks 11,549 out of 14,951
by transe in link prediction, which means a totally impos-
sible fact. but the keywords    portsmouth   ,    england   , and
   football    occur many times in both the textual descriptions,
making the two entities semantically relevant. unsurpris-
ingly, after the semantic projection, the case ranks 65 out of
14,951 in our model, which is much more plausible.

secondly, all the equal-length loss vectors 1 in transe are
equivalent in term of discrimination since the score function
of transe is ||h + r     t||2
2, which leads to the weak distinc-
tion. however, with textual semantics, the discriminative
ability could be strengthened in our model. speci   cally, the
equal-length loss vectors are measured with the projection
onto the corresponding semantic hyperplanes, which makes
a reasonable division of the losses. for an instance of the
query about which    lm    john powell    contributes to, there
are two candidate entities, that the true answer    kung fu
panda    and the negative one    terminator salvation   . with-
out textual semantics, it   s dif   cult to discriminate, thus the
losses calculated by transe are 8.1 and 8.0, respectively,
leading to a hard decision. diving into the textural seman-
tics, we discover,    john powell    is much relevant to the
topic of    animated films   , which matches that of    kung fu
panda    and does not for the other. based on this fact, both
the query and the true answer lie in the    animated films   -
directed hyperplane, whereas the query and the negative one
do not co-occur in the corresponding associated semantic
hyperplane. thus, the projected loss of the true answer could
be much less than that of the false one. concretely, the losses
in our model are 8.5 and 10.8, respectively, which are suf   -
cient for discrimination.
objectives & training
there are two parts in the objective function, which are re-
spectively embedding-speci   c and topic-speci   c. to bal-
ance the two parts, a hyper-parameter    is introduced. over-
all, the total loss is:

l = lembed +   ltopic

(1)
notably, there is only the    rst part in the standard setting
where    = 0 in fact.

in term of the embedding-related objective, the rank-
based hinge loss is applied, which means to maximize the
discriminative margin between the golden triples and the
negative ones:

[fr(cid:48)(h(cid:48), t(cid:48))     fr(h, t) +   ]+

(cid:88)

l

embed

=

(h, r, t)        
(h(cid:48), r(cid:48), t(cid:48))        (cid:48)

where     is the set of golden triples and    (cid:48) is that of the
negative ones.    is the margin, and [    ]+ = max(    , 0) is
the hinge loss. the false triples are sampled with    bernoulli

1two vectors have equal-length iff they have the same l2 norm.

sampling method    as introduced in (wang et al. 2014b) and
the method selects the negative samples from the set of

{(h(cid:48), r, t)|h(cid:48)     e}     {(h, r, t(cid:48))|t(cid:48)     e}
    {(h, r(cid:48), t)|r(cid:48)     r}

e   e, w   de

(cid:88)

ltopic =

we initialize the embedding vectors by the similar methods
used in the deep neural network (glorot and bengio 2010)
and pre-train the topic model with non-negative matrix fac-
torization (nmf) (stevens et al. 2012). the stochastic gradi-
ent descent algorithm (sgd) is adopted in the optimization.
for the topic-related objective, we take the advantage of
the nmf topic model (stevens et al. 2012), which is both
simple and effective. then we re-write the target as an inner-
product form with the l2-loss, stated as:
(ce,w     s(cid:62)
e w)2
se     0, w     0

(3)
where e is the set of entities, and de is the set of words in
the description of entity e. ce,w is the times of the word w
occurring in the description e. se is the semantic vector of
entity e and w is the topic distribution of word w. similarly,
sgd is applied in the optimization.
theoretically, our computation complexity is comparable
to transe, as o(     o(t ranse)), and the small constant   
is caused by the projection operation and topic calculation.
in practice, transe costs 0.28s for one round in link predic-
tion and our model costs 0.36s in the same setting. in gen-
eral, transe is most ef   cient among all the translation-based
methods, while our method could be comparable to transe
in running time, justifying the ef   ciency of our model.

(2)

experiments

datasets & general settings
our experiments are conducted on three public benchmark
datasets that are the subsets of id138 and freebase. about
the statistics of these datasets, we strongly suggest the read-
ers to refer to (xie et al. 2016) and (lin et al. 2015). the
entity descriptions of fb15k and fb20k are the same as
dkrl (xie et al. 2016), each of which is a small part of the
corresponding wiki-page. the textual information of wn18
is the de   nitions that we extract from the id138. notably,
for the zero-shot learning, fb20k is involved, which is also
built by the authors of dkrl.
id13 completion
evaluation protocol. the same protocol used in previous
studies, is adopted. first, for each testing triple (h, r, t), we
replace the tail t (or the head h) with every entity e in the
id13. then, a probabilistic score of this cor-
rupted triple is calculated with the score function fr(h, t).
by ranking these scores in ascending order, we then get the
rank of the original triple. the id74 are the av-
erage of the ranks as mean rank and the proportion of test-
ing triple whose rank is not larger than 10 (as hits@10).
this is called    raw    setting. when we    lter out the cor-
rupted triples that exist in the training, validation, or test

datasets, this is the   filter    setting. if a corrupted triple exists
in the id13, ranking it ahead the original triple
is also correct. to eliminate this effect, the    filter    setting
is more preferred. in both settings, a higher hits@10 and
a lower mean rank mean better performance.

table 1: mean rank and hits@10 of id13
completion (for predicting entity) on fb15k and wn18.

fb15k
transe
transh
jointly

dkrl(bow)
dkrl(all)
ssp (std.)
ssp (joint)

wn18
transe
transh

ssp (std.)
ssp (joint)

mean rank
119
210
212
87
39 1
167 1
113
200
181
93
213
113
85
188
mean rank
263
251
338
401
193
312
168
156

hits@10
66.1
48.5
45.7
64.4
77.3 1
51.7 1
57.6
44.3
67.4
49.6
52.0
73.3
53.5
77.1
hits@10
75.4
89.2
82.3
73.0
91.4
81.3
81.2
93.2

implementation. as the datasets are the same, we di-
rectly reprint the experimental results of several baselines
from the literature. we have attempted several settings on
the validation dataset to get the best con   guration. under the
   bern.    sampling strategy, the optimal con   gurations of our
model ssp are as follows. for wn18, embedding dimen-
sion d = 100, learning rate    = 0.001, margin    = 6.0, bal-
ance factor    = 0.2 and for ssp(joint)    = 0.1. for fb15k,
embedding dimension d = 100, learning rate    = 0.001,
margin    = 1.8, balance factor    = 0.2 and for ssp(joint)
   = 0.1. we train the model until convergence. usually, it
would converge until 10,000 rounds, but in current version,
instead, we report the results of standard setting with 2,000
rounds. regarding the joint setting, which is more dif   cult
to converge, we train the model until 5,000 rounds.

table 2: mean rank and hits@10 of id13
completion (for predicting relation) on fb15k.

fb15k
transe
transh

dkrl(bow)
dkrl(all)
ssp (std.)
ssp (joint)

mean rank hits@10
90.2
2.91
72.5
8.25
2.85
82.7
90.8
2.41
1.58
89.2
90.9
1.87

2.53
7.91
2.51
2.03
1.22
1.47

69.5
60.3
65.3
69.8
69.9
70.0

results evaluation results are reported in tab.1 and
tab.2. note that    jointly    refers to (zhong et al. 2015). we
observe that:

1this method involves much more extra text corpus, thus it   s

unfair to directly compare with others.

1. ssp outperforms all the baselines in all the tasks, demon-
strating the effectiveness of our models and the correct-
ness of our theoretical analysis. speci   cally, ssp(joint)
improves much more than ssp(std.) for jointly learning
the textual semantics and symbolic triples.

2. dkrl and    jointly    model only consider the    rst-order
constraints, which interact between the textual and sym-
bolic information, unsatisfactorily. by focusing on the
strong correlation, ssp outperforms them. notice that the
   jointly    model involves much more additional data to
produce the result, but ssp also has an remarkable ad-
vantage against it. though transh is also a hyperplane-
based method, ssp adopts the hyperplane in a semantics-
speci   c way rather than a simple relation-speci   c form.

3. transe could be treated as missing textual descriptions,
and dkrl(bow) could be viewed as missing symbolic
triples. ssp (joint) improves 12.4% against transe while
20.9% against dkrl(bow), illustrating the triple em-
bedding is always the key point and the interactions be-
tween the two information sources is a key factor.

entity classi   cation
this task is essentially a multi-label classi   cation, focusing
on predicting entity types, which is crucial and widely used
in many nlp & ir tasks (neelakantan and chang 2015).
the entity in freebase always has types, for instance, the
entity types of    scots    are human language, rosetta lan-
guoid.

we adopt the same datasets as dkrl, for the details of
which, we refer readers to (xie et al. 2016). overall, this
is a multi-label classi   cation task with 50 classes, which
means for each entity, the method should provide a set of
types rather than a single type.

evaluation protocol in the training, we use the concate-
nation of semantic vector and embedding vector (se, e) as
entity representation, which is the feature for the front-end
classi   er. for a fair comparison, our front-end classi   er is
also the id28 as dkrl in a one-versus-rest
setting for multi-label classi   cation. the evaluation is fol-
lowing (neelakantan and chang 2015), which applies the
mean average precision (map) that is commonly used in
multi-label classi   cation. speci   cally, for an entity, if the
methods predict a rank list of types and there are three cor-
rect types that lie in #1, #2, #4, the map is calculated as
1/1+2/2+3/4
. for fb20k, the methods could only make use
of the descriptions. obviously, fb20k is a zero-shot sce-
nario which is really hard.

implementation. as the datasets are the same, we di-
rectly report the experimental results of several baselines
from the literature. we have attempted several settings on
the validation dataset to get the best con   guration. under the
   bern.    sampling strategy, the optimal con   gurations of our
model ssp are as follows. for fb15k,embedding dimen-
sion d = 100, learning rate    = 0.001, margin    = 1.8, bal-
ance factor    = 0.2 and for ssp(joint)    = 0.1. for fb20k,
embedding dimension d = 100, learning rate    = 0.001,
margin    = 1.8, balance factor    = 0.2 and for ssp(joint)
   = 0.1. we train the model until convergence.

3

table 3: the map result of entity classi   cation

metrics
transe
bow

dkrl(bow)
dkrl(all)

nmf

ssp (std.)
ssp (joint)

-

fb15k fb20k
87.8
86.3
89.3
90.1
86.1
93.2
94.4

57.5
52.0
61.9
59.6

67.4

-

figure 3: precise semantic expression analysis for ssp
(std.). the x-axis indicates the score difference, where a
bigger value means better. the y-axis means the proportion
of the corresponding triple pairs. the gray part indicates
that both the transe and ssp make mistakes while the yel-
low colored part means that ssp succeeds but transe fails.
the statistics are similar for the ssp(j.) setting.

results the evaluation results are listed in tab. 3. no-
tably, transe is only triple-speci   c and ssp (std.) performs
as well as the nmf in the zero-shot learning. thus they are
trivial for fb20k. the observations are as follows:
1. overall, our method ssp yields the best accuracy, justify-

ing the effectiveness of our method.

2. compared to nmf and transe, the results show that the
interactions between triples and texts make a key differ-
ence and only one of them could hardly produce an ef-
fective performance. the promotion of ssp (joint) in
fb20k demonstrates the triple embedding also has a pos-
itive effect on textual semantics.

3. compared to transe, the improvements illustrate the ef-
fectiveness of semantic integration. compared to dkrl,
the promotion demonstrates the important role of model-
ing strong correlation.

semantic relevance analysis
one bene   t of modeling semantic relevance is the ability to
correctly classify the triples that could not be discriminated
by just using the information from symbolic triples. hence,
we make statistic analysis of the results in link prediction,
as reported in the tab.4. the number in each cell means

the number of triples whose rank is larger than m in transe
and less than n in our models. for instance, the number 601
means there are 601 triples whose ranks are less than 100
in ssp(s.) while their ranks are more than 500 in transe.
note that ssp(s.) indicates the standard setting, and ssp(j.)
means the joint setting. the statistic results indicate that
many triples bene   t from the semantic relevance offered by
textual descriptions. the experiments also justify the the-
oretical analysis about the semantic relevance and demon-
strate the effectiveness of our models.

table 4: rank statistics of link prediction. the number in
each cell indicates the number of triples, and r is the rank
given by the corresponding models.

ssp(s.)#r   100

ssp(j.)#r   100

transe#r   500
transe#r   1000
transe#r   2000
transe#r   3000
transe#r   5000

601
275
80
32
3

672
298
89
39
3

precise semantic expression analysis

as discussed previously, precise semantic expression leads
to better discrimination. to justify this claim, we have
collected the negative triples by link prediction, which are
scored sightly better than the golden ones by transe (that
means, these are hard examples for transe), and then plotted
the ssp score difference between each corresponding pair
of the negative and golden triples as fig.3 shows. all these
triples are predicted wrongly by transe, but with precise se-
mantic expression, our model correctly distinguishes 82.0%
(std.) and 83.2% (joint) of them. in the histogram, the right
bars indicate that ssp makes correct decision while transe
fails and the left bars mean both ssp and transe fail. the
experiments justify the theoretical analysis about the pre-
cise semantic expression and demonstrate the effectiveness
of our models.

conclusion

in this paper, we propose the id13 embedding
model ssp, which jointly learns from the symbolic triples
and textual descriptions. ssp could interact between the
triples and texts by characterizing the strong correlations be-
tween fact triples and textual descriptions. the textual de-
scriptions have much effect on discovering semantic rele-
vance and then offering precise semantic expression. exten-
sive experiments show our method achieves the substantial
improvements against the state-of-the-art baselines.

acknowledgement. this work was partly supported by
the national basic research program (973 program) under
grant no. 2013cb329403, and the national science foun-
dation of china under grant no.61272227/61332007.

references

blei, d. m. 2012. probabilistic topic models. communica-
tions of the acm 55(4):77   84.
bollacker, k.; evans, c.; paritosh, p.; sturge, t.; and taylor,
j. 2008. freebase: a collaboratively created graph database
in proceedings of the
for structuring human knowledge.
2008 acm sigmod international conference on manage-
ment of data, 1247   1250. acm.
bordes, a.; weston, j.; collobert, r.; bengio, y.; et al.
2011. learning structured embeddings of knowledge bases.
in proceedings of the twenty-   fth aaai conference on ar-
ti   cial intelligence.
bordes, a.; usunier, n.; garcia-duran, a.; weston, j.; and
yakhnenko, o. 2013. translating embeddings for modeling
in advances in neural information
multi-relational data.
processing systems, 2787   2795.
fan, m.; zhou, q.; chang, e.; and zheng, t. f. 2014.
transition-based id13 embedding with rela-
tional mapping properties. in proceedings of the 28th paci   c
asia conference on language, information, and computa-
tion, 328   337.
feng, j.; huang, m.; yang, y.; and zhu, x. 2016. gake:
graph aware knowledge embedding. in coling 2016, 26th
international conference on computational linguistics.
glorot, x., and bengio, y. 2010. understanding the dif   -
culty of training deep feedforward neural networks. in inter-
national conference on arti   cial intelligence and statistics,
249   256.
guo, s.; wang, q.; wang, b.; wang, l.; and guo, l. 2015.
semantically smooth id13 embedding. in pro-
ceedings of acl.
he, s.; liu, k.; ji, g.; and zhao, j. 2015. learning to rep-
resent id13s with gaussian embedding. in pro-
ceedings of the 24th acm international on conference on
information and knowledge management, 623   632. acm.
jenatton, r.; roux, n. l.; bordes, a.; and obozinski, g. r.
2012. a latent factor model for highly multi-relational
in advances in neural information processing sys-
data.
tems, 3167   3175.
ji, g.; he, s.; xu, l.; liu, k.; and zhao, j. knowledge
graph embedding via dynamic mapping matrix.
lin, y.; liu, z.; sun, m.; liu, y.; and zhu, x. 2015. learn-
ing entity and relation embeddings for id13
completion. in proceedings of the twenty-ninth aaai con-
ference on arti   cial intelligence.
lin, y.; liu, z.; and sun, m. 2015. modeling relation paths
for representation learning of knowledge bases. proceedings
of the 2015 conference on empirical methods in natural
language processing (emnlp). association for computa-
tional linguistics.
mikolov, t.; sutskever, i.; chen, k.; corrado, g. s.; and
dean, j. 2013. distributed representations of words and
phrases and their compositionality. in advances in neural
information processing systems, 3111   3119.

mikolov, t.; yih, w.-t.; and zweig, g. 2013. linguistic reg-
ularities in continuous space word representations. in hlt-
naacl, 746   751.
miller, g. a. 1995. id138: a lexical database for english.
communications of the acm 38(11):39   41.
neelakantan, a., and chang, m.-w. 2015. inferring missing
entity type instances for knowledge base completion: new
dataset and methods. arxiv preprint arxiv:1504.06658.
nickel, m.; tresp, v.; and kriegel, h.-p. 2011. a three-
way model for collective learning on multi-relational data.
in proceedings of the 28th international conference on ma-
chine learning (icml-11), 809   816.
socher, r.; chen, d.; manning, c. d.; and ng, a. 2013.
reasoning with neural tensor networks for knowledge base
completion. in advances in neural information processing
systems, 926   934.
stevens, k.; kegelmeyer, p.; andrzejewski, d.; and buttler,
d. 2012. exploring topic coherence over many models and
many topics. in proceedings of the 2012 joint conference
on empirical methods in natural language processing and
computational natural language learning, 952   961. as-
sociation for computational linguistics.
wang, z.; zhang, j.; feng, j.; and chen, z. 2014a. knowl-
edge graph and text jointly embedding. in emnlp, 1591   
1601. citeseer.
wang, z.; zhang, j.; feng, j.; and chen, z. 2014b. knowl-
edge graph embedding by translating on hyperplanes.
in
proceedings of the twenty-eighth aaai conference on ar-
ti   cial intelligence, 1112   1119.
wang, q.; wang, b.; and guo, l. 2015. knowledge base
completion using embeddings and rules. in proceedings of
the 24th international joint conference on arti   cial intelli-
gence.
xiao, h.; huang, m.; hao, y.; and zhu, x. 2015. transa:
an adaptive approach for id13 embedding.
arxiv preprint arxiv:1509.05490.
xiao, h.; huang, m.; and zhu, x. 2016a. from one point
to a manifold: id13 embedding for precise link
in proceedings of the 25th international joint
prediction.
conference on arti   cial intelligence.
xiao, h.; huang, m.; and zhu, x. 2016b. transg : a gen-
erative model for id13 embedding. in proceed-
ings of the 29th international conference on computational
linguistics. association for computational linguistics.
xie, r.; liu, z.; jia, j.; luan, h.; and sun, m. 2016. repre-
sentation learning of id13s with entity descrip-
tions.
zhong, h.; zhang, j.; wang, z.; wan, h.; and chen, z.
2015. aligning knowledge and text embeddings by entity
descriptions. in proceedings of emnlp, 267   272.

