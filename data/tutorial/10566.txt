attention-based convolutional neural network

for machine comprehension

wenpeng yin, sebastian ebert, hinrich sch  utze

university of munich, germany

wenpeng, ebert@cis.lmu.de

6
1
0
2

 

b
e
f
3
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
1
4
3
4
0

.

2
0
6
1
:
v
i
x
r
a

abstract

understanding open-domain text is one of the pri-
mary challenges in natural language processing
(nlp). machine comprehension benchmarks eval-
uate the system   s ability to understand text based on
the text content only. in this work, we investigate
machine comprehension on mctest, a question an-
swering (qa) benchmark. prior work is mainly
based on feature engineering approaches. we come
up with a neural network framework, named hier-
archical attention-based convolutional neural net-
work (habid98), to address this task without any
manually designed features. speci   cally, we ex-
plore habid98 for this task by two routes, one
is through traditional joint modeling of document,
question and answer, one is through textual entail-
ment. habid98 employs an attention mechanism
to detect key phrases, key sentences and key snip-
pets that are relevant to answering the question. ex-
periments show that habid98 outperforms prior
deep learning approaches by a big margin.

1 introduction
endowing machines with the ability to understand natu-
ral language is a long-standing goal in nlp and holds the
promise of revolutionizing the way in which people interact
with machines and retrieve information. richardson et al.
[2013] proposed the task of machine comprehension, along
with mctest, a id53 dataset for evaluation.
the ability of the machine to understand text is evaluated
by posing a series of questions, where the answer to each
question can be found only in the associated text. solutions
typically focus on some semantic interpretation of the text,
possibly with some form of probabilistic or logic id136,
to answer the question. despite intensive recent work [we-
ston et al., 2014; weston et al., 2015; hermann et al., 2015;
sachan et al., 2015], the problem is far from solved.

machine comprehension is an open-domain question-
answering problem which contains factoid questions, but the
answers can be derived by extraction or induction of key
clues. figure 1 shows one example in mctest. each ex-
ample consists of one document, four associated questions;
each question is followed by four answer candidates in which

figure 1: one example with 2 out of 4 questions in the
mctest.    *    marks correct answer.

only one is correct. questions in mctest have two categories:
   one    and    multiple   . the label means one or multiple sen-
tences from the document are required to answer this ques-
tion. to correctly answer the    rst question in the example,
the two blue sentences are required; for the second question
instead, only the red sentence can help. the following obser-
vations hold for the whole mctest. (i) most of the sentences
in the document are irrelavent for a given question. it hints
that we need to pay attention to just some key regions. (ii) the
answer candidates can be    exible text in length and abstrac-
tion level, and probably do not appear in the document. for
example, candidate b for the second question is    outside   ,
which is one word and does not exist in the document, while
the answer candidates for the    rst question are longer texts
with some auxiliary words like    because    in the text. this
requires our system to handle    exible texts via extraction as
well as abstraction. (iii) some questions require multiple sen-
tences to infer the answer, and those vital sentences mostly
appear close to each other (we call them snippet). hence,
our system should be able to make a choice or compromise
between potential single-sentence clue and snippet clue.

prior work on this task is mostly based on feature engi-
neering. this work, instead, takes the lead in presenting a
deep neural network based approach without any linguistic
features involved.

concretely, we propose habid98,

a hierarchical
attention-based convolutional neural network, to address this
task in two roadmaps. in the    rst one, we project the docu-

ment in two different ways, one based on question-attention,
one based on answer-attention and then compare the two
projected id194s to determine whether
the answer matches the question. in the second one, every
question-answer pair is reformatted into a statement, then the
whole task is treated through id123.

in both roadmaps, convolutional neural network (id98) is
explored to model all types of text. as human beings usu-
ally do for such a qa task, our model is expected to be able
to detect the key snippets, key sentences, and key words or
phrases in the document. in order to detect those informative
parts required by questions, we explore an attention mecha-
nism to model the document so that its representation con-
tains required information intensively.
in practice, instead
of imitating human beings in qa task top-down, our system
models the document bottom-up, through accumulating the
most relevant information from word level to snippet level.

our approach is novel in three aspects.

(i) a document
is modeled by a hierarchical id98 for different granularity,
from word to sentence level, then from sentence to snippet
level. the reason of choosing a id98 rather than other se-
quence models like recurrent neural network [mikolov et al.,
2010], long short-term memory unit (lstm [hochreiter and
schmidhuber, 1997]), gated recurrent unit (gru [cho et al.,
2014]) etc, is that we argue id98s are more suitable to detect
the key sentences within documents and key phrases within
sentences. considering again the second question in figure
1, the original sentence    they sat by the    re and talked about
he insects    has more information than required, i.e, we do not
need to know    they talked about the insects   . sequence mod-
eling neural networks usually model the sentence meaning by
accumulating the whole sequence. id98s, with convolution-
pooling steps, are supposed to detect some prominent features
no matter where the features come from. (ii) in the example
in figure 1, apparently not all sentences are required given a
question, and usually different snippets are required by dif-
ferent questions. hence, the same document should have dif-
ferent representations based on what the question is. to this
end, attentions are incorporated into the hierarchical id98
to guide the learning of dynamic id194s
which closely match the information requirements by ques-
tions. (iii) id194s at sentence and snippet
levels both are informative for the question, a highway net-
work is developed to combine them, enabling our system to
make a    exible tradeoff.

overall, we make three contributions. (i) we present a hi-
erarchical attention-based id98 system    habid98   . it is, to
our knowledge, the    rst deep learning based system for this
mctest task. (ii) prior document modeling systems based on
deep neural networks mostly generate generic representation,
this work is the    rst to incorporate attention so that docu-
ment representation is biased towards the question require-
ment.
(iii) our habid98 systems outperform other deep
learning competitors by big margins.

2 related work
existing systems for mctest task are mostly based on man-
ually engineered features. representative work includes

figure 2: illustrations of habid98-qap (top), habchh-
qp (middle) and habid98-te (bottom). q, a, s: question,
answer, statement; d: document
[narasimhan and barzilay, 2015; sachan et al., 2015; wang
and mcallester, 2015; smith et al., 2015]. in these works,
a common route is    rst to de   ne a regularized loss func-
tion based on assumed feature vectors, then the effort focuses
on designing effective features based on various rules. even
though these researches are groundbreaking for this task, their
   exibility and their capacity for generalization are limited.

deep learning based approaches appeal to increasing inter-
est in analogous tasks. weston et al., [2014] introduce mem-
ory networks for factoid qa. memory network framework
is extended in [weston et al., 2015; kumar et al., 2015] for
facebook babi dataset. peng et al. [2015]   s neural reasoner
infers over multiple supporting facts to generate an entity an-
swer for a given question and it is also tested on babi. all of
these works deal with some short texts with simple-grammar,
aiming to generate an answer which is restricted to be one
word denoting a location, a person etc.

some works also tried over other kinds of qa tasks. for
example, iyyer et al., [2014] present qanta, a recursive
neural network, to infer an entity based on its description text.
this task is basically a matching between description and en-
tity, no explicit question exist. another difference with us
lies in that all the sentences in the entity description actually
contain partial information about the entity, hence a descrip-
tion is supposed to have only one representation. however
in our task, the modeling of document should be dynami-
cally changed according to the question analysis. hermann
et al., [2015] incorporate attention mechanism into lstm for
a qa task over news text. still, their work does not handle
some complex question types like    why...   , they merely aim
to    nd the entity from the document to    ll the slot in the query
so that the completed query is true based on the document.
nevertheless, it inspires us to treat our task as a textual en-
tailment problem by    rst reformatting question-answer pairs
into statements.

some other deep learning systems are developed for an-
swer selection task [yu et al., 2014; yang et al., 2015;
severyn and moschitti, 2015; shen et al., 2015; wang et
al., 2010]. differently, this kind of id53 task
does not involve document comprehension. they only try to
match the question and answer candidate without any back-
ground information. instead, we treat machine comprehen-
sion in this work as a question-answer matching problem un-
der background guidance.

overall, for open-domain mctest machine comprehension

task, this work is the    rst to resort to deep neural networks.

pi = tanh(w    ci + b)

where bias b     rd1. d1 is called    kernel size    in id98.

vi   w+1, . . . , vi using the convolution weights w     rd1  wd:
(1)

figure 3: habid98. feature maps for phrase representations pi and the max pooling steps that create sentence representations
out of phrase representations are omitted for simpli   cation. each snippet covers three sentences in snippet-id98. symbols    
mean cosine similarity calculation.
3 model
we investigate this task by three approaches, illustrated in
figure 2. (i) we can compute two different document (d) rep-
resentations in a common space, one based on question (q)
attention, one based on answer (a) attention, and compare
them. this architecture we name habid98-qap. (ii) we
compute a representation of d based on q attention (as be-
fore), but now we compare it directly with a representation of
a. we name this architecture habid98-qp. (iii) we treat this
qa task as id123 (te),    rst reformatting q-a
pair into a statement (s), then matching s and d directly. this
architecture we name habid98-te. all three approaches
are implemented in the common framework habid98.
3.1 habid98
recall that we use the abbreviations a (answer), q (ques-
tion), s (statement), d (document). habid98 performs rep-
resentation learning for triple (q, a, d) in habid98-qp and
habid98-qap, for tuple (s, d) in habid98-te. for con-
venience, we use    query    to refer to q, a, or s uniformly.
habid98, depicted in figure 3, has the following phases.

sentence-level representation. the sentence-id98 gen-
erates a new feature map (omitted in figure 3) for each input
sentence, one column in the feature map denotes a phrase rep-
resentation (i.e., pi in equation (1)).

for the query and each sentence of d, we do element-wise
1-max-pooling (   max-pooling    for short) [collobert and we-
ston, 2008] over phrase representations to form their repre-
sentations at this level.

note that the sentence-id98s for query and all document
sentences share the same weights, so that the resulting sen-
tence representations are comparable.

input layer. the input is (query,d). query is two indi-
vidual sentences (for q, a) or one single sentence (for s),
d is a sequence of sentences. words are initialized by d-
dimensional pre-trained id27s. as a result, each
sentence is represented as a feature map with dimensionality
of d    s (s is sentence length). in figure 3, each sentence
in the input layer is depicted by a rectangle with multiple
columns.

sentence-id98. sentence-id98 is used for sentence rep-
resentation learning from word level. given a sentence of
length s with a word sequence: v1, v2, . . . , vs, let vector
ci     rwd be the concatenated embeddings of w words
vi   w+1, . . . , vi where w is the    lter width, d is the dimen-
sionality of word representations and 0 < i < s + w. em-
beddings for words vi, i < 1 and i > s, are zero padding.
we then generate the representation pi     rd1 for the phrase

we now treat d as a set of    vital    sentences and    noise   
sentences. we propose attention-pooling to learn the
sentence-level representation of d as follows:    rst identify
vital sentences by computing attention for each d   s sentence
as the cosine similarity between the its representation and the
query representation, then select the k highest-attention sen-
tences to do max-pooling over them. taking figure 3 as an
example, based on the output of sentence-id98 layer, k = 2
important sentences with blue color are combined by max-
pooling as the sentence-level representation vs of d; the other
    white-color     sentence representations are neglected as they
have low attentions. (if k = all, attention-pooling returns to
the common max-pooling in [collobert and weston, 2008].)
when the query is (q,a), this step will be repeated, once for
q, once for a, to compute representations of d at the sentence
level that are biased with respect to q and a, respectively.

snippet-id98. as the example in figure 1 shows, to an-
swer the    rst question    why did grandpa answer the door?   ,
it does not suf   ce to compare this question only to the sen-
tence    grandpa answered the door with a smile and wel-
comed jimmy inside   ; instead, the snippet    finally, jimmy
arrived at grandpa   s house and knocked. grandpa answered

the door with a smile and welcomed jimmy inside    should
be used to compare. to this end, it is necessary to stack
another id98 layer, snippet-id98, to learn representations of
snippets, i.e., units of one or more sentences. thus, the ba-
sic units input to snippet-id98 (resp. sentence-id98) are sen-
tences (resp. words) and the output is representations of snip-
pets (resp. sentences).

concretely, snippet-id98 puts all sentence representations
in column sequence as a feature map and conducts another
convolution operation over it. with    lter width w,
this
step generates representation of snippet with w consecu-
tive sentences. similarly, we use the same id98 to learn
higher-abstraction query representations (just treating query
as a document with only one sentence, so that the higher-
abstraction query representation is in the same space with
corresponding snippet representations).

snippet-level representation. for the output of snippet-
id98, each representation is more abstract and denotes big-
ger granularity. we apply the same attention-pooling process
to snippet level representations: attention values are com-
puted as cosine similarities between query and snippets and
the snippets with the k largest attentions are retained. max-
pooling over the k selected snippet representations then cre-
ates the snippet-level representation vt of d. two selected
snippets are shown as red in figure 3.

overall representation. based on convolution layers at
two different granularity, we have derived query-biased rep-
resentations of d at sentence level (i.e., vs) as well as snippet
level (i.e., vt). in order to create a    exible choice for open
q/a, we develop a highway network [srivastava et al., 2015]
to combine the two levels of representations as an overall rep-
resentation vo of d:

vo = (1     h) (cid:12) vs + h (cid:12) vt

(2)

where highway network weights h are learned by

h =   (whvs + b)

(3)
where wh     rd1  d1. with the same highway network, we
can generate the overall query representation, ri in figure 3,
by combining the two representations of the query at sentence
and snippet levels.
3.2 habid98-qp & habid98-qap
habid98-qp/qap computes the representation of d as a
projection of d, either based on attention from q or based on
attention from a. we hope that these two projections of the
document are close for a correct a and less close for an incor-
rect a. as we said in related work, machine comprehension
can be viewed as an answer selection task using the docu-
ment d as critical background information. here, habid98-
qp/qap do not compare q and a directly, but they use q and
a to    lter the document differently, extracting what is critical
for the q/a match by attention-pooling. then they match the
two id194s in the new space.

for ease of exposition, we have used the symbol vo so far,
but in habid98-qp/qap we compute two different docu-
ment representations: voq, for which attention is computed
with respect to q; and voa for which attention is computed

with respect to a. ri also has two versions, one for q: riq,
one for a: ria.

habid98-qp and habid98-qap make different use of
voq. habid98-qp compares voq with answer representation
ria. habid98-qap compares voq with voa. habid98-
qap projects d twice, once based on attention from q, once
based on attention from a and compares the two projected
representations, shown in figure 2 (top). habid98-qp only
utilizes the q-based projection of d and then compares the
projected document with the answer representation, shown in
figure 2 (middle).
3.3 habid98-te
habid98-te treats machine comprehension as textual en-
tailment. we use the statements that are provided as part of
mctest. each statement corresponds to a question-answer
pair; e.g., the q/a pair    why did grandpa answer the door?   
/    because he saw the insects    (figure 1) is reformatted into
the statement    grandpa answered the door because he saw the
insects   . the id53 task is then cast as:    does
the document entail the statement?   

for habid98-te, shown in figure 2 (bottom), the input
for figure 3 is the pair (s,d). habid98-te tries to match
the s   s representation ri with the d   s representation vo.

4 experiments
4.1 dataset
mctest1 has two subsets. mctest-160 is a set of 160 items,
each consisting of a document, four questions followed by
one correct anwer and three incorrect answers (split into 70
train, 30 dev and 60 test) and mctest-500 a set of 500 items
(split into 300 train, 50 dev and 150 test).
4.2 training setup and tricks
our training objective is to minimize the following ranking
id168:

l(d, a+, a   ) = max(0,    + s(d, a   )     s(d, a+))

(4)
where s(  ,  ) is a matching score between two representation
vectors. cosine similarity is used throughout.    is a constant.
for this common ranking loss, we also have two styles to
utilize the data in view of each positive answer is accompa-
nied with three negative answers. one is treating (d, a+, a   
1 ,
a   
2 , a   
3 ) as a training example, then our id168 can
have three    max()    terms, each for a positive-negative pair;
the other one is treating (d, a+, a   
i ) as an individual training
example. in practice, we    nd the second way works better.
we conjecture that the second way has more training exam-
ples, and positive answers are repeatedly used to balance the
amounts of positive and negative answers.

multitask learning: question typing is commonly used
and proved to be very helpful in qa tasks [sachan et al.,
2015].
inspired, we stack a id28 layer over
question representation riq, with the purpose that this sub-
task can favor the parameter tuning of the whole system, and

1http://requery.microsoft.com/mct

k

[1,3]

lr
0.05

d1

[90, 90]

bs
1

w
[2,2]

l2

0.0065

  
0.2

table 1: hyperparameters. k: top-k in attention-pooling for
both id98 layers; lr: learning rate; d1: kernel size in id98
layers; bs: mini-batch size; w:    lter width; l2: l2 normal-
ization;   : constant in id168.

   nally the question is better recognized and is able to    nd the
answer more accurately.

to be speci   c, we classify questions into 12 classes:
   how   ,    how much   ,    how many   ,    what   ,    who   ,    where   ,
   which   ,    when   ,    whose   ,    why   ,    will    and    other   . the
question label is created by querying for the label keyword in
the question. if more than one keyword appears in a question,
we adopt the one appearing earlier and the more speci   c one
(e.g.,    how much   , not    how   ). in case there is no match, the
class    other    is assigned.

we train with adagrad [duchi et al., 2011] and use 50-
dimensional glove [pennington et al., 2014] to initialize
word representations,2 kept    xed during training. table 1
gives hyperparameter values, tuned on dev.

we consider two id74: accuracy (proportion
of questions correctly answered) and ndcg4 [j  arvelin and
kek  al  ainen, 2002]. unlike accuracy which evaluates if the
question is correctly answered or not, ndcg4, being a mea-
sure of ranking quality, evaluates the position of the correct
answer in our predicted ranking.

4.3 baseline systems
this work focuses on the comparison with systems about dis-
tributed representation learning and deep learning:

addition. directly compare question and answers without
considering the d. sentence representations are computed by
element-wise addition over word representations.

addition-proj. first compute sentence representations for
q, a and all d sentences as the same way as addition, then
match the two sentences in d which have highest similarity
with q and a respectively.

nr. the neural reasoner [peng et al., 2015] has an encod-
ing layer, multiple reasoning layers and a    nal answer layer.
the input for the encoding layer is a question and the sen-
tences of the document (called facts); each sentence is en-
coded by a gru into a vector. in each reasoning layer, nr
lets the question representation interact with each fact repre-
sentation as reasoning process. finally, all temporary reason-
ing clues are pooled as answer representation.

ar. the attentive reader [hermann et al., 2015] is im-
plemented by modeling the whole d as a word sequence    
without speci   c sentence / snippet representations     using an
lstm. attention mechanism is implemented at word repre-
sentation level.

overall, baselines addition and addition-proj do not in-
volve complex composition and id136. nr and ar rep-
resent the top-performing deep neural networks in qa tasks.

2http://nlp.stanford.edu/projects/glove/

4.4 habid98 variants
in addition to the main architectures described above, we also
explore two variants of abchnn, inspired by [peng et al.,
2015] and [hermann et al., 2015], respectively.

variant-i: as id56s are widely recognized as a competi-
tor of id98s in sentence modeling, similar with [peng et al.,
2015], we replace the sentence-id98 in figure 3 by a gru
while keeping other parts unchanged.

variant-ii: how to model attention at the granularity of
words was shown in [hermann et al., 2015]; see their paper
for details. we develop their attention idea and model at-
tention at the granularity of sentence and snippet. our atten-
tion gives different weights to sentences/snippets (not words),
then computes the id194 as a weighted av-
erage of all sentence/snippet representations.
4.5 results
table 2 lists the performance of baselines, habid98-
te variants, habid98 systems in the    rst, second and
last block, respectively (we only report variants for top-
performing habid98-te). consistently, our habid98 sys-
tems outperform all baselines, especially surpass the two
competitive deep learning based systems ar and nr. the
margin between our best-performing abhid98-te and nr
is 15.6/16.5 (accuracy/ndcg) on mctest-150 and 7.3/4.6 on
mctest-500. this demonstrates the promise of our architec-
ture in this task.

as said before, both ar and nr systems aim to generate
answers in entity form. their designs might not suit this ma-
chine comprehension task, in which the answers are openly-
formed based on summarizing or abstracting the clues. to be
more speci   c, ar models d always at word level, attentions
are also paid to corresponding word representations, which
is applicable for entity-style answers, but is less suitable for
comprehension at sentence level or even snippet level. nr
contrarily models d in sentence level always, neglecting the
discovering of key phrases which however compose most of
answers.
in addition, the attention of ar system and the
question-fact interaction in nr system both bring large num-
bers of parameters, this potentially constrains their power in
a dataset of limited size.

for variant-i and variant-ii (second block of table 2),
we can see that both modi   cations do harm to the original
habid98-te performance. the    rst variant, i.e, replacing
the sentence-id98 in figure 3 as gru module is not help-
ful for this task. we suspect that this lies in the fundamen-
tal function of id98 and gru. the id98 models a sentence
without caring about the global word order information, and
max-pooling is supposed to extract the features of key phrases
in the sentence no matter where the phrases are located. this
property should be useful for answer detection, as answers
are usually formed by discovering some key phrases, not all
words in a sentence should be considered. however, a gru
models a sentence by reading the words sequentially, the im-
portance of phrases is less determined by the question re-
quirement. the second variant, using a more complicated
attention scheme to model biased d representations than sim-
ple cosine similarity based attention used in our model, is less
effective to detect truly informative sentences or snippet. we

mctest-150

mctest-500

ndcg4

e
n
i
l
e
s
a
b

s addition

method

ndcg4

addition-proj
ar
nr
variant-i
variant-ii

one mul
55.2
56.6
60.1
63.3
64.2
66.7
69.5
71.9
68.8
64.5
63.8
70.2
72.7
75.4
72.6
75.7
76.1
74.4
table 2: experimental results for one-sentence (one), multiple-sentence (mul) and all cases.

one mul
50.3
60.4
61.3
65.3
68.9
70.5
68.2
70.7
73.9
71.2
70.1
74.6
80.0
80.4
79.9
81.5
86.6
85.9

all
32.9
38.0
41.9
45.6
43.7
46.5
50.1
50.6
52.9

all
35.7
40.3
46.3
47.6
49.0
52.2
55.7
58.4
63.1

all
54.6
63.2
69.6
69.7
72.5
72.3
80.2
80.6
86.2

acc
one mul
30.2
35.7
36.7
39.4
39.5
44.4
45.6
45.7
45.4
42.1
46.0
47.1
46.7
53.7
47.2
54.0
54.2
51.7

acc
one mul
32.4
39.3
38.7
42.1
44.7
48.1
46.8
48.4
50.4
47.7
51.0
53.6
53.7
57.9
57.9
59.0
63.3
62.9

habid98-qp
habid98-qap
habid98-te

all
55.8
61.7
65.4
70.6
66.6
66.9
74.0
74.1
75.2

figure 4: attention visualization for statement    grandpa answered the door because jimmy knocked    in the example figure 1.
too long sentences are truncated with    . . .   . left is attention weights for each single sentence after the sentence-id98, right is
attention weights for each snippet (two consecutive sentences as    lter width w = 2 in table 1) after the snippet-id98.

doubt such kind of attention scheme when used in sentence
sequences of large size. in training, the attention weights after
softmax id172 have actually small difference across
sentences, this means the system can not distinguish key sen-
tences from noise sentences effectively. our cosine similarity
based attention-pooling, though pretty simple, is able to    l-
ter noise sentences more effectively, as we only pick top-k
pivotal sentences to form d representation    nally. this trick
makes the system simple while effective.
4.6 case study and error analysis
in figure 4, we visualize the attention distribution at sentence
level as well as snippet level for the statement     grandpa an-
swered the door because jimmy knocked    whose correspond-
ing question requires multiple sentences to answer. from its
left part, we can see that    grandpa answered the door with a
smile and welcomed jimmy inside    has the highest attention
weight. this meets the intuition that this sentence has seman-
tic overlap with the statement. and yet this sentence does not
contain the answer. look further the right part, in which the
id98 layer over sentence-level representations is supposed to
extract high-level features of snippets. in this level, the high-
est attention weight is cast to the best snippet    finally, jimmy
arrived...knocked. grandpa answered the door...   . and the
neighboring snippets also get relatively higher attentions than
other regions. recall that our system chooses the one sen-
tence with top attention at left part and choose top-3 snip-
pets at right part (referring to k value in table 1) to form
d representations at different granularity, then uses a high-

way network to combine both representations as an overall
d representation. this visualization hints that our architec-
ture provides a good way for a question to compromise key
information from different granularity.

we also do some preliminary error analysis. one big obsta-
cle for our systems is the    how many    questions. for exam-
ple, for question    how many rooms did i say i checked?    and
the answer candidates are four digits    5,4,3,2    which never
appear in the d, but require the counting of some locations.
however, these digital answers can not be modeled well by
distributed representations so far. in addition, digital answers
also appear for    what    questions, like    what time did...   . an-
other big limitation lies in    why    questions. this question
type requires complex id136 and long-distance dependen-
cies. we observed that all deep lerning systems, including the
two baselines, suffered somewhat from it.

5 conclusion
this work takes the lead in presenting a id98 based neu-
ral network system for open-domain machine comprehen-
sion task. our systems tried to solve this task in a docu-
ment projection way as well as a id123 way. the
latter one demonstrates slightly better performance. over-
all, our architecture, modeling dynamic document representa-
tion by attention scheme from sentence level to snippet level,
shows promising results in this task. in the future, more    ne-
grained representation learning approaches are expected to
model complex answer types and question types.

of text.
193   203, 2013.

in proceedings of emnlp, volume 1, pages

[sachan et al., 2015] mrinmaya sachan, avinava dubey,
eric p xing, and matthew richardson. learning answer-
entailing structures for machine comprehension. in pro-
ceedings of acl, pages 239   249, 2015.

[severyn and moschitti, 2015] aliaksei

and
alessandro moschitti. learning to rank short text pairs
with convolutional deep neural networks. in proceedings
of sigir, pages 373   382, 2015.

severyn

[shen et al., 2015] yikang shen, wenge rong, zhiwei sun,
yuanxin ouyang, and zhang xiong. question/answer
matching for cqa system via combining lexical and se-
quential information. in proceedings of aaai, pages 275   
281, 2015.

[smith et al., 2015] ellery smith, nicola greco, matko
bosnjak, and andreas vlachos. a strong lexical matching
method for the machine comprehension test. in proceed-
ings of emnlp, pages 1693   1698, 2015.

[srivastava et al., 2015] rupesh k srivastava, klaus greff,
and j  urgen schmidhuber. training very deep networks.
in nips, pages 2368   2376, 2015.

[wang and mcallester, 2015] hai wang and mohit bansal
kevin gimpel david mcallester. machine comprehen-
sion with syntax, frames, and semantics. in proceedings
of acl, volume 2: short papers, pages 700   706, 2015.

[wang et al., 2010] baoxun wang,

xiaolong wang,
chengjie sun, bingquan liu, and lin sun. modeling
semantic relevance for question-answer pairs in web social
communities. in proceedings of acl, pages 1230   1238,
2010.

[weston et al., 2014] jason weston, sumit chopra, and an-
toine bordes. memory networks. in proceedings of iclr,
2014.

[weston et al., 2015] jason weston, antoine bordes, sumit
chopra, and tomas mikolov. towards ai-complete ques-
arxiv
tion answering: a set of prerequisite toy tasks.
preprint arxiv:1502.05698, 2015.

[yang et al., 2015] yi yang, wen-tau yih, and christopher
meek. wikiqa: a challenge dataset for open-domain ques-
tion answering. in proceedings of emnlp, pages 2013   
2018, 2015.

[yu et al., 2014] lei yu, karl moritz hermann, phil blun-
som, and stephen pulman. deep learning for answer sen-
tence selection. in iclr workshop, 2014.

references
[cho et al., 2014] kyunghyun cho, bart van merri  enboer,
caglar gulcehre, dzmitry bahdanau, fethi bougares,
holger schwenk, and yoshua bengio. learning phrase
representations using id56 encoder-decoder for statistical
in proceedings of emnlp, pages
machine translation.
1724   1734, 2014.

[collobert and weston, 2008] ronan collobert and jason
weston. a uni   ed architecture for natural language pro-
cessing: deep neural networks with multitask learning. in
proceedings of icml, pages 160   167, 2008.

[duchi et al., 2011] john duchi, elad hazan, and yoram
singer. adaptive subgradient methods for online learn-
ing and stochastic optimization. the journal of machine
learning research, 12:2121   2159, 2011.

[hermann et al., 2015] karl moritz hermann, tomas ko-
cisky, edward grefenstette, lasse espeholt, will kay,
mustafa suleyman, and phil blunsom. teaching machines
to read and comprehend. in proceedings of nips, pages
1684   1692, 2015.

[hochreiter and schmidhuber, 1997] sepp hochreiter and
j  urgen schmidhuber. long short-term memory. neural
computation, 9(8):1735   1780, 1997.

iyyer,

[iyyer et al., 2014] mohit

jordan boyd-graber,
leonardo claudino, richard socher, and hal daum  e iii.
a neural network for factoid id53 over
in proceedings of emnlp, pages 633   644,
paragraphs.
2014.

[j  arvelin and kek  al  ainen, 2002] kalervo j  arvelin and jaana
kek  al  ainen.
cumulated gain-based evaluation of ir
techniques. acm transactions on information systems
(tois), 20(4):422   446, 2002.

[kumar et al., 2015] ankit kumar, ozan irsoy, jonathan su,
james bradbury, robert english, brian pierce, peter on-
druska, ishaan gulrajani, and richard socher. ask me
anything: dynamic memory networks for natural language
processing. arxiv preprint arxiv:1506.07285, 2015.

[mikolov et al., 2010] tomas mikolov, martin kara     at,
lukas burget, jan cernock`y, and sanjeev khudanpur. re-
current neural network based language model. in proceed-
ings of interspeech, pages 1045   1048, 2010.

[narasimhan and barzilay, 2015] karthik narasimhan and
regina barzilay. machine comprehension with discourse
relations. in proceedings of acl, pages 1253   1262, 2015.
[peng et al., 2015] baolin peng, zhengdong lu, hang li,
and kam-fai wong. towards neural network-based rea-
soning. corr, abs/1508.05508, 2015.

[pennington et al., 2014] jeffrey

richard
socher, and christopher d manning. glove: global
vectors for word representation. proceedings of emnlp,
12:1532   1543, 2014.

pennington,

[richardson et al., 2013] matthew richardson, christo-
pher jc burges, and erin renshaw. mctest: a challenge
dataset for the open-domain machine comprehension

