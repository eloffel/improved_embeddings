6
1
0
2

 
r
p
a
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
3
0
5
0
0

.

4
0
6
1
:
v
i
x
r
a

discriminative phrase embedding for paraphrase identi   cation

wenpeng yin and hinrich sch  utze

center for information and language processing

university of munich, germany

wenpeng@cis.lmu.de

abstract

this work, concerning paraphrase identi   ca-
tion task, on one hand contributes to expand-
ing deep learning embeddings to include con-
tinuous and discontinuous linguistic phrases.
on the other hand, it comes up with a new
scheme tf-kld-knn to learn the discrimi-
native weights of words and phrases speci   c
to paraphrase task, so that a weighted sum of
embeddings can represent sentences more ef-
fectively. based on these two innovations we
get competitive state-of-the-art performance
on paraphrase identi   cation.

1 introduction

this work investigates representation learning via
deep learning in paraphrase identi   cation task,
which aims to determine whether two sentences
have the same meaning. one main innovation of
deep learning is that it learns distributed word repre-
sentations (also called    id27s   ) to deal
with various natural language processing (nlp)
tasks. our goal is to use and re   ne embeddings to
get competitive performance.

we adopt a supervised classi   cation approach to
paraphrase identi   cation like most top performing
systems. our focus is representation learning of sen-
tences. following prior work (e.g., blacoe and lap-
ata (2012)), we compute the vector of a sentence as
the sum of the vectors of its components. but unlike
prior work we use single words, continuous phrases
and discontinuous phrases as the components, not
just single words. our rationale is that many seman-
tic units are formed by multiple words     e.g., the

continuous phrase    side effects    and the discontin-
uous phrase    pick . . . off   . the better we can dis-
cover and represent such components, the better the
compositional sentence vector should be. we use
the term unit to refer to single words, continuous
phrases and discontinuous phrases.

ji and eisenstein (2013) show that not all words
are equally important for paraphrase identi   cation.
they propose tf-kld, a discriminative weighting
scheme to address this problem. while they do not
represent sentences as vectors composed of other
vectors, tf-kld is promising for a vector-based
approach as well since the insight that units are of
different importance still applies. a shortcoming of
tf-kld is its failure to de   ne weights for words
that do not occur in the training set. we propose
tf-kld-knn, an extension of tf-kld that com-
putes the weight of an unknown unit as the average
of the weights of its k nearest neighbors. we de-
termine nearest neighbors by cosine measure over
embedding space. we then represent a sentence as
the sum of the vectors of its units, weighted by tf-
kld-knn.

we use (madnani et al., 2012) as our baseline
system. they used simple features     eight dif-
ferent machine translation metrics     yet got good
performance. based on above new sentence rep-
resentations, we compute three kinds of features
to describe a pair of sentences     cosine similarity,
element-wise sum and absolute element-wise differ-
ence     and show that combining them with the fea-
tures from madnani et al. (2012) gets state-of-the-art
performance on the microsoft research paraphrase
(msrp) corpus (dolan et al., 2004).

in summary, our    rst contribution lies in em-
bedding learning of continuous and discontinuous
phrases. our second contribution is the weighting
scheme tf-kld-knn.

this paper is structured as follows. section 2 re-
views related work. section 3 describes our method
for learning embeddings of units. section 4 intro-
duces a measure of unit discriminativity that can be
used for differential weighting of units. section 5
presents experimental setup and results. section 6
concludes.

2 related work

the key for good performance in paraphrase iden-
ti   cation is the design of good features. we now
discuss relevant prior work based on the linguistic
granularity of id171.

the    rst line is id152, which
learns representations for words and then composes
them to representations of sentences. blacoe and la-
pata (2012) carried out a comparative study of three
word representation methods (the simple distribu-
tional semantic space (mitchell and lapata, 2010),
distributional memory tensor (baroni and lenci,
2010) and id27 (collobert and weston,
2008)), along with three composition methods (ad-
dition, point-wise multiplication, and recursive auto-
encoder (socher et al., 2011)). they showed that ad-
dition over id27s is competitive, despite
its simplicity.

the second category directly seeks sentence-level
features.
ji and eisenstein (2013) explored uni-
grams, bigrams and dependency pairs as sentence
features. they proposed tf-kld to weight fea-
tures and used non-negative factorization to learn la-
tent sentence representations. our method tf-kld-
knn is an extension of their work.

the third line directly computes features for sen-
tence pairs. wan et al. (2006) used id165 overlap,
dependency relation overlap, dependency tree-edit
distance and difference of sentence lengths. finch
et al. (2005) and madnani et al. (2012) combined
several machine translation metrics. das and smith
(2009) presented a generative model over two sen-
tences    dependency trees, incorporating syntax, lex-
ical semantics, and hidden loose alignments between
the trees to model generating a paraphrase of a given

sentence. socher et al. (2011) used recursive autoen-
coders to learn representations for words and word
sequences on each layer of the sentence parsing tree,
and then proposed dynamic pooling layer to form
a    xed-size matrix as the representation of the two
sentences. other work representative of this line is
by kozareva and montoyo (2006), qiu et al. (2006),
ul-qayyum and altaf (2012).

our work,    rst learning unit embeddings, then
adding them to form sentence representations,    -
nally calculating pair features (cosine similarity, ab-
solute difference and mt metrics) actually is a com-
bination of above three lines.

3 embedding learning for units

as explained in section 1,    units    in this work in-
clude single words, continuous phrases and discon-
tinuous phrases. phrases have a larger linguistic
granularity than words and thus will in general con-
tain more meaning aspects for a sentence. for ex-
ample, successful detection of continuous phrase
   side effects    and discontinuous phrase    pick         
off    is helpful to understand the sentence meaning
correctly. this section focuses on how to detect
phrases and how to represent them.

3.1 phrase collection

phrases de   ned by a lexicon have not been inves-
tigated extensively before in deep learning. to
collect canonical phrase set, we extract two-word
phrases de   ned in wiktionary1 and id138 (miller
and fellbaum, 1998) to form a collection of size
95,218. this collection contains continuous phrases
    phrases whose parts always occur next to each
other (e.g.,    side effects   )     and discontinuous
phrases     phrases whose parts more often occur sep-
arated from each other (e.g.,    pick . . . off   ).

3.2

identi   cation of phrase continuity

wiktionary and id138 do not categorize phrases
as continuous or discontinuous.
so we need a
heuristic to determine this automatically.

for each phrase    a b   , we compute [c1, c2, c3,
c4, c5] where ci, 1     i     5, indicates there are ci
occurrences of a and b in that order with a distance

1http://en.wiktionary.org

of i. we compute these statistics for a corpus con-
sisting of english gigaword (graff et al., 2003) and
wikipedia. we set the maximal distance to 5 be-
cause discontinuous phrases are rarely separated by
more than 5 tokens.

if c1 is 10 times higher than (c2 + c3 + c4 + c5)/4,
we classify    a b    as continuous, otherwise as dis-
continuous. for example, [c1, . . . , c5] is [1121, 632,
337, 348, 4052] for    pick off   , so c1 is smaller than
the average 1342.25 and    pick off    is set as    discon-
tinuous   ; [c1, . . . , c5] is [14831, 16, 177, 331, 3471]
for    cornell university   , c1 is 10 times larger than
the average and this phrase is set to    continuous   .

we found that that this heuristic for distinguish-
ing between continuous and discontinuous phrases
works well and leave the development of a more
principled method for future work.

3.3 sentence reformatting
sentence    . . . a . . . b . . .     is

    reformatted as    . . . a b . . .     if a and b form a
continuous phrase and no word intervenes be-
tween them and

    reformatted as    . . . a b . . . a b . . .     if a and
b form a discontinuous phrase and are sepa-
rated by 1 to 4 words. we replace each of
the two component words with a b to make
the context of both constituents available to the
phrase in learning.

this method of phrase detection will generate
some false positives, e.g., if    pick    and    off    occur
in a context like    she picked an island off the coast
of maine   . however, our experimental results indi-
cate that it is robust enough for our purposes.

we run id97 (mikolov et al., 2013) on the
reformatted wikipedia corpus to learn embeddings
for all units. embedding size is set to 200.

4 measure of unit discriminativity

we will represent a sentence as the sum of the em-
beddings of its units. building on ji and eisenstein
(2013)   s tf-kld, we want to weight units accord-
ing to their ability to discriminate two sentences spe-
ci   c to the paraphrase task.

tf-kld assumes a training set of sentence pairs
in the form hui, vi, tii, where ui and vi denote the

binary unit occurrence vectors for the sentences in
the ith pair and ti     {0, 1} is the gold tag. then, we
de   ne pk and qk as follows.

    pk = p (uik|vik = 1, ti = 1). this is the prob-
ability that unit wk occurs in sentence ui given
that wk occurs in its counterpart vi and they are
paraphrases.

    qk = p (uik|vik = 1, ti = 0). this is the prob-
ability that unit wk occurs in sentence ui given
that wk occurs in its counterpart vi and they are
not paraphrases.

tf-kld computes the discriminativity of unit wk
as the id181 of the bernoulli
distributions (pk, 1-pk) and (qk, 1-qk)

tf-kld has a serious shortcoming for unknown
units. unfortunately, the test data of the commonly
used mspr corpus in paraphrase task has about 6%
unknown words and 62.5% of its sentences contain
unknown words.
it motivates us to design an im-
proved scheme tf-kld-knn to reweight the fea-
tures.

tf-kld-knn weights are the same as tf-kld
weights for known units. for a unit that did not oc-
cur in training, tf-kld-knn computes its weight
as the average of the weights of its k nearest neigh-
bors in embedding space, where unit similarity is
calculated by cosine measure.2

id97 learns id27s based on the
word context. the intuition of tf-kld-knn is
that words with similar context have similar discrim-
inativities. this enables us to transfer the weights
of features in training data to the unknown features
in test data, greatly helping to address problems of
sparseness.

5 experiments

5.1 data and baselines

we use the msrp corpus (dolan et al., 2004) for
evaluation. it consists of a training set of 2753 true
paraphrase pairs and 1323 false paraphrase pairs and
a test set of 1147 true and 578 false pairs.

2unknown words without embeddings (only seven cases in
our experiments) are ignored. this problem can be effectively
relieved by training embedding on larger corpora.

for our new method, it is interesting to measure
the improvement on the subset of those msrp sen-
tences that contain at least one phrase. in the stan-
dard msrp corpus, 3027 training pairs (2123 true,
904 false) and 1273 test pairs (871 true, 402 false)
contain phrases; we denote this subset as subset.
we carry out experiments on overall (all msrp sen-
tences) as well as subset cases.

we compare six methods for paraphrase identi   -

cation.

    noweight. following blacoe and lapata
(2012), we simply represent a sentence as the
unweighted sum of the embeddings of all its
units.

    mt is the method proposed by madnani et
al. (2012): the sentence pair is represented as
a vector of eight different machine translation
metrics.

    ji and eisenstein (2013). we reimplemented
their    inductive    setup which is based on ma-
trix factorization and is the top-performing sys-
tem in id141 task.3
the following three methods not only use this
vector of eight mt metrics, but use three
kinds of additional features given two sentence
representations s1 and s2: cosine similarity,
element-wise sum s1+s2 and element-wise ab-
solute difference |s1     s2|. we now describe
how each of the three methods computes the
sentence vectors.

    word. the sentence is represented as the sum
of all single-id27s, weighted by
tf-kld-knn.

    word+phrase. the sentence is repre-
sented as the sum of the embeddings of all
its units (including phrases), weighted by tf-
kld-knn.

    word+google. mikolov et al. (2013)
use a data-driven method to detect statistical
phrases which are mostly continuous bigrams.

3they report even better performance in a    transductive   
setup that makes use of test data. we only address paraphrase
identi   cation for the case that the test data are not available for
training the model in this paper.

we implement their system by    rst exploiting
word2phrase4 to reformat wikipedia, then us-
ing id97 skip-gram model to train phrase
embeddings.

that

we use the same weighting scheme tf-kld-
knn for
the three weighted sum approaches:
word, word+phrase and word+google.
note however
there is an interaction be-
tween representation space and nearest neighbor
search. we limit the neighbor range of unknown
words for word to single words; in contrast, we
search the space of all single words and linguistic
(resp. google) phrases for word+phrase (resp.
word+google).

we use liblinear (fan et al., 2008) as our lin-
ear id166 implementation. 20% training data is used
as development data. parameter k is    ne-tuned on
development set and the best value 3 is    nally used
in following reported results.

5.2 experimental results
table 1 shows performance for the six methods as
well as for the majority baseline. in the overall (resp.
subset) setup, word+phrase performs best and
outperforms (ji and eisenstein, 2013) by .009 (resp.
.052) on accuracy.
interestingly, ji and eisen-
stein (2013)   s method obtains worse performance on
subset. this can be explained by the effect of ma-
trix factorization in their work:
it works less well
for smaller datasets like subset. this is a short-
coming of their approach. word+google has a
slightly worse performance than word+phrase;
this suggests that linguistic phrases might be more
effective than statistical phrases in identifying para-
phrases.

cases overall and subset both suggest that phrase
embeddings improve sentence representations. the
accuracy of word+phrase is lower on overall
than on subset because word+phrase has no ad-
vantage over word for sentences without phrases.

5.3 effectiveness of tf-kld-knn
the key contribution of tf-kld-knn is that it
achieves full coverage of feature weights in the face
of data sparseness. we now compare four weight-
ing methods on overall corpus and with the combi-

4https://code.google.com/p/id97/

subset
acc f1

overall
method
acc f1
baseline
.665 .799 .684 .812
noweight
.708 .809 .713 .823
.774 .841 .772 .839
mt
ji and eisenstein (2013) .778 .843 .749 .827
.775 .839 .776 .843
word
.780 .843 .795 .853
word+google
.787 .848    .801 .857   
word+phrase

y
c
a
r
u
c
c
a

0.788

0.787

0.786

0.785

0.784

0.783

0.782

0.781

0.78

knn

zero

type   average context   average

reweighting methods for unseen units

table 1: results on overall and subset corpus. signi   cant
improvements over mt are marked with     (approximate
randomization test, pad  o (2006), p < .05).

acc
method
.746
noweight
.752
tf-idf
tf-kld
.774
tf-kld-knn .787

f1
.815
.821
.842
.848

table 2: effects of different reweighting methods on
overall.

nation of mt features: noweight, tf-idf, tf-
kld, tf-kld

table 2 suggests that task-speci   c reweighting ap-
proaches (including tf-kld and tf-kld-knn)
are superior to unspeci   c schemes (noweight
and tf-idf). also, it demonstrates the effectiveness
of our weight learning solution for unknown units in
paraphrase task.

5.4 reweighting schemes for unseen units
we compare our reweighting scheme knn (i.e., tf-
kld-knn) with three other reweighting schemes.
zero: zero weight, i.e., ignore unseen units; type-
average: take the average of weights of all known
unit types in test set; context-average: average of
the weights of the adjacent known units of the un-
known unit (two, one or defaulting to zero, depend-
ing on how many there are). figure 1 shows that
knn performs best.

6 conclusion

this work introduced tf-kld-knn, a new
reweighting scheme that learns the discriminativi-
ties of known as well as unknown units effectively.
we further improved paraphrase identi   cation per-

figure 1: performance of different reweighting schemes
for unseen units on overall.

formance by the utilization of continuous and dis-
continuous phrase embeddings.

in future, we plan to do experiments in a cross-
domain setup and enhance our algorithm for domain
adaptation paraphrase identi   cation.

acknowledgments

to members of cis for com-
we are grateful
ments on earlier versions of this paper.
this
work was supported by baidu (through a baidu
scholarship awarded to wenpeng yin) and by
deutsche forschungsgemeinschaft
(grant dfg
schu 2246/8-2, spp 1335).

references

marco baroni and alessandro lenci. 2010. distribu-
tional memory: a general framework for corpus-based
semantics. computational linguistics, 36(4):673   
721.

william blacoe and mirella lapata. 2012. a compari-
son of vector-based representations for semantic com-
position. in proceedings of the 2012 joint conference
on empirical methods in natural language process-
ing and computational natural language learning,
pages 546   556. association for computational lin-
guistics.

ronan collobert and jason weston. 2008. a uni   ed ar-
chitecture for natural language processing: deep neu-
ral networks with multitask learning. in proceedings
of the 25th international conference on machine learn-
ing, pages 160   167. acm.

dipanjan das and noah a smith. 2009. paraphrase iden-
ti   cation as probabilistic quasi-synchronous recogni-
tion.
in proceedings of the joint conference of the
47th annual meeting of the acl and the 4th inter-
national joint conference on natural language pro-

richard socher, eric h huang, jeffrey pennington, an-
drew y ng, and christopher d manning. 2011. dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection.
in advances in neural in-
formation processing systems, volume 24, pages 801   
809.

zia ul-qayyum and wasif altaf. 2012. paraphrase iden-
ti   cation using semantic heuristic features. research
journal of applied sciences, engineering and tech-
nology, 4(22):4894   4904.

stephen wan, mark dras, robert dale, and c  ecile paris.
2006. using dependency-based features to take the
para-farce out of paraphrase.
in proceedings of the
australasian language technology workshop, volume
2006, pages 131   138.

cessing of the afnlp: volume 1-volume 1, pages 468   
476. association for computational linguistics.

bill dolan, chris quirk, and chris brockett. 2004. un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources.
in pro-
ceedings of the 20th international conference on com-
putational linguistics, pages 350   356. association for
computational linguistics.

rong-en fan, kai-wei chang, cho-jui hsieh, xiang-rui
wang, and chih-jen lin. 2008. liblinear: a library
for large linear classi   cation. the journal of machine
learning research, 9:1871   1874.

andrew finch, young-sook hwang,

and eiichiro
sumita.
2005. using machine translation evalua-
tion techniques to determine sentence-level semantic
equivalence. in proceedings of the third international
workshop on id141 (iwp2005), pages 17   24.
david graff, junbo kong, ke chen, and kazuaki maeda.
2003. english gigaword. linguistic data consortium,
philadelphia.

yangfeng ji and jacob eisenstein. 2013. discriminative
improvements to distributional sentence similarity. in
proceedings of the conference on empirical methods
in natural language processing (emnlp).

zornitsa kozareva and andr  es montoyo. 2006. para-
phrase identi   cation on the basis of supervised ma-
chine learning techniques. in advances in natural lan-
guage processing, pages 524   533. springer.

nitin madnani, joel tetreault, and martin chodorow.
2012. re-examining machine translation metrics for
paraphrase identi   cation. in proceedings of the 2012
conference of the north american chapter of the as-
sociation for computational linguistics: human lan-
guage technologies, pages 182   190. association for
computational linguistics.

tomas mikolov, ilya sutskever, kai chen, greg s cor-
rado, and jeff dean. 2013. distributed representations
of words and phrases and their compositionality.
in
advances in neural information processing systems,
pages 3111   3119.

george miller and christiane fellbaum. 1998. id138:

an electronic lexical database.

jeff mitchell and mirella lapata. 2010. composition in
distributional models of semantics. cognitive science,
34(8):1388   1429.

sebastian pad  o, 2006. user   s guide to sigf: signi   -

cance testing by approximate randomisation.

long qiu, min-yen kan, and tat-seng chua. 2006.
paraphrase recognition via dissimilarity signi   cance
classi   cation. in proceedings of the 2006 conference
on empirical methods in natural language process-
ing, pages 18   26. association for computational lin-
guistics.

