yu, yang, song, xiang, hospedales: sketch-a-net that beats humans

1

sketch-a-net that beats humans
qian yu   
q.yu@qmul.ac.uk
yongxin yang   
yongxin.yang@qmul.ac.uk
yi-zhe song
yizhe.song@qmul.ac.uk
tao xiang
t.xiang@qmul.ac.uk
timothy m. hospedales
t.hospedales@qmul.ac.uk

5
1
0
2

 
l
u
j
 

1
2

 
 
]

v
c
.
s
c
[
 
 

school of electronic engineering and
computer science
queen mary, university of london
london, e1 4ns
united kingdom

3
v
3
7
8
7
0

.

1
0
5
1
:
v
i
x
r
a

abstract

we propose a multi-scale multi-channel deep neural network framework that, for
the    rst time, yields sketch recognition performance surpassing that of humans. our
superior performance is a result of explicitly embedding the unique characteristics of
sketches in our model: (i) a network architecture designed for sketch rather than natural
photo statistics, (ii) a multi-channel generalisation that encodes sequential ordering in the
sketching process, and (iii) a multi-scale network ensemble with joint bayesian fusion
that accounts for the different levels of abstraction exhibited in free-hand sketches. we
show that state-of-the-art deep networks speci   cally engineered for photos of natural ob-
jects fail to perform well on sketch recognition, regardless whether they are trained using
photo or sketch. our network on the other hand not only delivers the best performance
on the largest human sketch dataset to date, but also is small in size making ef   cient
training possible using just cpus.

1

introduction

sketches are very intuitive to humans and have long been used as an effective communicative
tool. with the proliferation of touchscreens, sketching has become a much easier undertaking
for many     we can sketch on phones, tablets and even watches. research on sketches has
consequently    ourished in recent years, with a wide range of applications being investigated,
including sketch recognition [6, 21], sketch-based id162 [5, 9], sketch-based 3d
model retrieval [25], and forensic sketch analysis [12, 20].

recognising free-hand sketches (e.g. asking a person to draw a car without any instance
of car as reference) is an extremely challenging task. this is due to a number of reasons:
(i) sketches are highly iconic and abstract, e.g., human    gures can be depicted as stickmen;
(ii) due to the free-hand nature, the same object can be drawn with hugely varied levels of
detail/abstraction, e.g., a human    gure sketch can be either a stickman or a portrait with    ne
c(cid:13) 2015. the copyright of this document resides with its authors.
it may be distributed unchanged freely in print or electronic forms.
    these authors contributed equally to this work

2

yu, yang, song, xiang, hospedales: sketch-a-net that beats humans

details depending on the drawer; (iii) sketches lack visual cues, i.e., they consist of black
and white lines instead of coloured pixels. a recent large-scale study on 20,000 free-hand
sketches across 250 categories of daily objects puts human sketch recognition accuracy at
73.1% [6], suggesting that the task is challenging even for humans.

prior work on sketch recognition generally follows the conventional image classi   cation
paradigm, that is, extracting hand-crafted features from sketch images followed by feeding
them to a classi   er. most hand-crafted features traditionally used for photos (such as hog,
sift and shape context) have been employed, which are often coupled with bag-of-words
(bow) to yield a    nal feature representations that can then be classi   ed. however, existing
hand-crafted features designed for photos do not account for the unique abstract and sparse
nature of sketches. furthermore, they ignore a key unique characteristics of sketches, that
is, a sketch is essentially an ordered list of strokes; they are thus sequential in nature. in
contrast with photos that consist of pixels sampled all at once, a sketch is the result of an
online drawing process. it had long been recognised in psychology [11] that such sequential
ordering is a strong cue in human sketch recognition, a phenomenon that is also con   rmed
by recent studies in the id161 literature [21]. however, none of the existing ap-
proaches attempted to embed sequential ordering of strokes in the recognition pipeline even
though that information is readily available.

in this paper, we propose a novel deep neural network (dnn), sketch-a-net, for free-
hand sketch recognition, which is speci   cally designed to accommodate the unique charac-
teristics of sketches including multiple levels of abstraction and being sequential in nature.
dnns, especially deep convolutional neural networks (id98s) have achieved tremendous
successes recently in replacing representation hand-crafting with representation learning for
a variety of vision problems [13, 22]. however, existing dnns are primarily designed for
photos; we demonstrate experimentally that directly employing them for the sketch mod-
elling problem produces little improvement over hand-crafted features, indicating special
model architecture is required for sketches. to this end, our sketch-a-net has three key
features that distinguish it from the existing dnns: (i) a number of model architecture
and learning parameter choices speci   cally for addressing the iconic and abstract nature
of sketches; (ii) a multi-channel architecture designed to model the sequential ordering of
strokes in each sketch; and (iii) a multi-scale network ensemble to address the variability in
abstraction and sparsity, followed by a joint bayesian fusion scheme to exploit the comple-
mentarity of different scales. the overall model is small in size, being 7 times smaller than
the classic alexnet [13] in terms of the number of parameters, therefore making it ef   cient
to train independently of special hardware, i.e. gpus.

our contributions are summarised as follows: (i) for the    rst time, a representation learn-
ing model based on dnn is presented for sketch recognition in place of the conventional
hand-crafted feature based sketch representations; (ii) we demonstrate how sequential order-
ing information in sketches can be embedded into the dnn architecture and in turn improve
sketch recognition performance; (iii) we propose a multi-scale network ensemble that fuses
networks learned at different scales together via joint bayesian fusion to address the variabil-
ity of levels of abstraction in sketches. extensive experiments on the largest hand-free sketch
benchmark dataset, the tu-berlin sketch dataset [6], show that our model signi   cantly out-
performs existing approaches and can even beat humans at sketch recognition.

3

yu, yang, song, xiang, hospedales: sketch-a-net that beats humans
2 related work
free-hand sketch recognition: early studies on sketch recognition worked with profes-
sional cad or artistic drawings as input [10, 18, 23, 28]. free-hand sketch recognition had
not attracted much attention until very recently when a large crowd-sourced dataset was pub-
lished in [6]. free-hand sketches are drawn by non-artists using touch sensitive devices rather
than purpose-made equipments; they are thus often highly abstract and exhibit large intra-
class deformations. most existing works [6, 17, 21] use id166 as the classi   er and differ only
in what hand-crafted features borrowed from photos are used as representation. li et al. [17]
demonstrated that fusing different local features using multiple kernel learning helps improve
the recognition performance. they also examined the performance of many features indi-
vidually and found that hog generally outperformed others. very recently, schneider and
tuytelaars [21] demonstrated that fisher vectors, an advanced feature representation scheme
successfully applied to image recognition, can be adapted to sketch recognition and achieve
near-human accuracy (68.9% vs. 73.1% for humans on the tu-berlin sketch dataset).

despite these great efforts, no attempt was made thus far for either designing or learning
feature representations speci   cally for sketches. moreover, the role of sequential ordering in
sketch recognition remains unaddressed. in this paper, we turn to dnns which have shown
great promise in many areas of id161 [13, 22] for representation learning. our
learned representation uniquely exploits the sequential ordering information of strokes in a
sketch and is able to cope with multiple levels of abstraction in the same sketch category.
note that the id42 (ocr) community has exploited stroke ordering
with some success [26], yet the problem of encoding sequential information is harder on
sketches     handwriting characters have relatively    xed structural ordering therefore simple
heuristics often suf   ce; sketches on the the other hand exhibit a much higher degree of intra-
class variation in stroke ordering, which motivates us to resort to the powerful dnns to learn
the most suitable sketch representation.
dnns for visual recognition: deep neural networks (dnns) have recently achieved im-
pressive performance for many recognition tasks across different disciplines. in particular,
convolutional neural networks (id98s) have dominated top benchmark results on visual
recognition challenges such as ilsvrc [3]. when    rst introduced in the 1980s, id98s were
the preferable solution for small problems only (e.g. lenet [14] for handwritten digit recog-
nition). their practical applications were severely bottlenecked by the high computational
cost when the number of classes and training data are large. however with the recent prolif-
eration of modern gpus, this bottleneck has been largely alleviated. nonetheless, it was not
until the introduction of relu neurons (instead of tanh), max-pooling (instead of average
pooling) and dropout regularisation that dnns maximised their effectiveness and regained
their popularity [13]. an important advantage of dnns, particularly id98s, compared with
conventional classi   ers such as id166s, lies with the closely coupled nature of presentation
learning and classi   cation (i.e., from raw pixels to class labels in a single network), which
makes the learned feature representation maximally discriminative. more recently, it was
shown that even deeper networks with smaller    lters [22] are preferable for photo image
recognition. despite these great strides, to the best of our knowledge, all existing image
recognition dnns are optimised for photos, ultimately making them perform sub-optimally
on sketches. in this paper, we show that directly applying successful photo-oriented dnns to
sketches leads to little improvement over hand-crafted feature based methods. in contrast, by
embedding the unique characteristics of sketches into the network design, our sketch-a-net
advances sketch recognition to the over-human level.

4

yu, yang, song, xiang, hospedales: sketch-a-net that beats humans

figure 1: illustration of our overall framework.

3 methodology

in this section we introduce the three key technical components of our framework. we    rst
detail our basic id98 architecture and outline the important considerations for sketch-a-
net compared to the conventional photo-oriented dnns (sec. 3.1). we next explain our
simple but novel generalisation that gives a dnn the ability to exploit the stroke ordering
information that is unique to sketches (sec. 3.2). we then introduce a multi-scale ensemble
of networks to address the variability in the levels of abstraction with a joint bayesian fusion
method for exploiting the complementarity of different scales (sec. 3.3). fig. 1 illustrates
our overall framework.

3.1 a id98 for sketch recognition
our sketch-a-net is a deep id98. despite all the efforts so far, it remains an open question
how to design the architecture of id98s given a speci   c visual recognition task; but most
recent recognition networks [1, 22] now follow a design pattern of multiple convolutional
layers followed by fully connected layers, as popularised by the work of [13].

our speci   c architecture is as follows:    rst we use    ve convolutional layers, each with
recti   er (relu) [15] units, while the    rst, second and    fth layers are followed by max
pooling (maxpool). the    lter size of the sixth convolutional layer (index 14 in table 1) is
7  7, which is the same as the output from previous pooling layer, thus it is precisely a fully-
connected layer. then two more fully connected layers are appended. dropout regularisation
[8] is applied on the    rst two fully connected layers. the    nal layer has 250 output units
corresponding to 250 categories (that is the number of unique classes in the tu-berlin sketch
dataset), upon which we place a softmax loss. the details of our id98 are summarised
in table 1. note that for simplicity of presentation, we do not explicitly distinguish fully
connected layers from their convolutional equivalents.

most id98s are proposed without explaining why parameters, such as    lter size, stride,
   lter number, padding and pooling size, are chosen. although it is impossible to exhaustively
verify the effect of every free (hyper-)parameter, we discuss some points that are consistent
with classic designs, as well as those that are speci   cally designed for sketches, thus consid-
erably different from the id98s targeting photos, such as alexnet [13] and decaf [4].

multi-channel multi-scale   bayesian fusion channel 1:  part1 channel 2:  part2 channel 3:  part3 channel 4:  part1+part2 channel 5:  part2+part3 channel 6:  whole sketch 64*64 192*192 128*128 224*224 256*256 yu, yang, song, xiang, hospedales: sketch-a-net that beats humans

5

filter num

stride

index

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

layer

l1

l2

l3

l4

l5

l6

l7

l8

type
input
conv
relu
maxpool

conv
relu
maxpool

conv
relu
conv
relu
conv
relu
maxpool
conv(=fc)

relu

dropout (0.50)

conv(=fc)

relu

dropout (0.50)

-

filter size
15   15
-
3   3
5   5
-
3   3
3   3
-
3   3
-
3   3
-
3   3
7   7
-
-
1   1
-
-
1   1

-
64
-
-
128
-
-
256
-
256
-
256
-
-
512
-
-
512
-
-
250

pad
-
0
-
0
0
-
0
1
-
1
-
1
-
0
0
-
-
0
-
-
0

output size
225   225
71   71
71   71
35   35
31   31
31   31
15   15
15   15
15   15
15   15
15   15
15   15
15   15
7   7
1   1
1   1
1   1
1   1
1   1
1   1
1   1

-
3
-
2
1
-
2
1
-
1
-
1
-
2
1
-
-
1
-
-
1

conv(=fc)
table 1: the architecture of sketch-a-net.

commonalities between sketch-a-net and photo-oriented id98 architectures
filter number:
in both our sketch-a-net and recent photo-oriented id98s [13, 22], the
number of    lters increases with depth. in our case the    rst layer is set to 64, and this is
doubled after every pooling layer (indicies: 3     4, 6     7 and 13     14) until 512.
stride: as with photo-oriented id98s, the stride of convolutional layers after the    rst is
set to one. this keeps as much information as possible.
padding: zero-padding is used only in l3-5 (indices 7, 9 and 11). this is to ensure that
the output size is an integer number, as in photo-oriented id98s [1].
unique aspects in our sketch-a-net architecture
larger first layer filters: the size of    lters in the    rst convolutional layer might be
the most sensitive parameter, as all subsequent processing depends on the    rst layer output.
while classic networks use large 11   11    lters [13], the current trend of research [27] is
moving toward ever smaller    lters: very recent [22] state of the art networks have attributed
their success in large part to use of tiny 3   3    lters. in contrast, we    nd that larger    lters are
more appropriate for sketch modelling. this is because sketches lack texture information,
e.g., a small round-shaped patch can be recognised as eye or button in a photo based on
texture, but this is infeasible for sketches. larger    lters thus help to capture more structured
context rather than textured information. to this end, we use a    lter size of 15   15.
no local response normalisation: local response normalisation (lrn) [13] imple-
ments a form of lateral inhibition, which is found in real neurons. this is used pervasively in
contemporary id98 recognition architectures [1, 13, 22]. however, in practice lrn   s ben-
e   t is due to providing    brightness normalisation   . this is not necessary in sketches since
brightness is not an issue in line-drawings. thus removing lrn layers makes learning faster
without sacri   cing performance.
larger pooling size: many recent id98s use 2   2 max pooling with stride 2 [22]. it ef   -
ciently reduces the size of the layer by 75% while bringing some spatial invariance. however,
we use the modi   cation: 3  3 pooling size with stride 2, thus generating overlapping pooling
areas [13]. we found this brings     1% improvement without much additional computation.

6

yu, yang, song, xiang, hospedales: sketch-a-net that beats humans

figure 2: illustration of stroke ordering in sketching with the alarm clock category.

higher dropout: deeper neural networks generally improve performance but risk over   t-
ting [22]. recent id98 successes [1, 13, 22] deal with this using the (very large) id163
dataset [3] for training, and dropout [8] regularisation (randomly setting units activation to
zero). since a sketch dataset is typically much smaller than id163, we compensate for
this by setting a much higher dropout rate of 50%.
lower computational cost: the total number of parameters in sketch-a-net is 8.5 mil-
lion, which is relatively small for modern id98s. for example, the classic alexnet [13] has
60 million parameters (7 times larger), and recent state-of-the-art [22] reaches 144 million.

3.2 modelling sketch stroke order with multiple channels
stroke ordering: the order of drawn strokes is key information associated with sketches
drawn on touchscreens compared to conventional photos where all pixels are captured in
parallel. although this information exists in main sketch datasets such as tu-berlin, existing
work has generally ignored it. to provide intuition about this, fig. 2 illustrates some sketches
in the alarm clock category, with strokes broken down into three parts according to stroke
order. clearly there are different sketching strategies in terms of which semantic parts to
draw    rst, but it is common to draw the main outline    rst, followed by details, as a recent
study also found [6]. modelling stroke ordering information is thus useful in distinguishing
categories that are similar in their parts but differ in their typical ordering.
modelling stroke order: we propose a simple but effective approach to modelling the
sequential order of strokes by extending sketch-a-net to a multi-channel id98: discretising
strokes into three sequential groups (fig. 2), and treating these parts as different channels in
the    rst layer. speci   cally, we use the three stroke parts to generate six images containing
combinations of the stroke parts. as illustrated in fig. 1, the    rst three images contain the
three parts alone; the next two contain pairwise combinations of two parts, and the third is the
original sketch of all parts. our sketch-a-net described in sec. 3.1 is then modi   ed to take
the six channel images as input (i.e. the    rst layer convolution    lter size is changed to 15  
15   6). this multi-channel model has a couple of advantages: (i) the relative importance of
early versus late strokes are learned automatically by back propagation training; (ii) it is a
simple and ef   cient modi   cation of the existing architecture: the number of parameters and
hence training time is only increased by 1% compared to the single channel sketch-a-net.

3.3 a multi-scale network ensemble with bayesian fusion
the next challenging aspect of sketch recognition to be addressed is the variability in sketch-
ing abstraction. to deal with this we introduce an ensemble of our multi-channel sketch-a-
nets. for each network in the ensemble we learn a model of varying coarseness by blurring
its training data to different degrees. speci   cally, we create a 5 network ensemble by blur-
ring     downsampling and then upsampling by to the original 256   256 pixel image size.

part 1 whole sketch part 2 part 3 part 1 whole sketch part 2 part 3 yu, yang, song, xiang, hospedales: sketch-a-net that beats humans

7

the downsample sizes are: 256,224,192,128,64. each network in the ensemble is indepen-
dently trained by backdrop using one of these blur levels.

the multi-scale sketch-a-net ensemble can be used for classifying a test image using
score-level fusion, i.e., averaging the softmax scores. however, this fusion strategy treats
each network thus each scale equally without discrimination. alternatively, one could con-
catenate the id98 learned representations in each network and feed them to a downstream
classi   er [4]. however, again no scale and feature selection is possible with this feature-level
fusion strategy. in this work, we propose to take the (512d) activation of the penultimate
layer of our network as a representation, and apply the recent joint bayesian (jb) fusion
method [2] to exploit the complementarity between different scales.

the jb framework models pairs of instances (in this case id98 activations), by full
covariance gaussians, under the prior assumption that each instance x is a sum of its (nor-
mally distributed) category mean and instance speci   c deviation: x =    +   . in particular
it learns two full covariance gaussians, representing pairs from the same category and dif-
ferent categories respectively, i.e., it models p(x1,x2|hi) and p(x1,x2|he ) where x1 and x2
are instances, and hi and he are the matched and mismatched pair hypotheses respectively.
jb provides an em algorithm for learning these covariances   i and   e respectively. once
learned, optimal bayesian matching can be done using a likelihood ratio test:

r(x1,x2) = log

p(x1,x2 | hi)
p(x1,x2 | he )

(1)

which turns out to be equivalent [2] to a metric learner capable of learning strong metrics
with more degrees of freedom than traditional mahalanobis metrics.
although initially designed for veri   cation, we re-purpose jb for classi   cation here. let
each x represent the 5   512 = 2560d concatenated feature vector from our network ensem-
ble. training: using this activation vector as a new representation for the training data, we
train the jb model, thus learning a good metric. testing: given the activation vectors of train
and test data, we use the likelihood-ratio test (eq. 1) to compare each test point to the full
train set. with this mechanism to match test to train points,    nal classi   cation is achieved
with k-nearest-neighbour (knn) matching12. note that in this way each feature dimen-
sion from each network is fused together, implicitly giving more weight to more important
features, as well as    nding the optimal combination of different features at different scales.

4 experiments

dataset: we evaluate our model on the tu-berlin sketch dataset [6], which is the largest
and now the most commonly used human sketch dataset. it contains 250 categories with
80 sketches per category. it was collected on amazon mechanical turk (amt) from 1,350
participants, thus providing a diversity of both categories and sketching styles within each
category. we rescaled all images to 256   256 pixels in order to make it comparable with
previous work. also following previous work we performed 3-fold cross-validation within
this dataset (2 folds for training and 1 for testing).

1we set k = 5 in this work and the regularisation parameter of jb is set to 0.1
2for robustness at test time, we also take 10 crops and re   ections of each train and test image [13]. this in   ates
the knn train and test pool by 10, and the crop-level matches are combined to image predictions by majority voting.

8

yu, yang, song, xiang, hospedales: sketch-a-net that beats humans

data augmentation: data augmentation is commonly with id98s to reduce over   tting.
we performed data augmentation by replicating the sketches with a number of transforma-
tions. speci   cally, for each input sketch, we did horizontal re   ection, rotation (in the range
[-5, +5] degrees) and systematic combinations of horizontal and vertical shifts (up to 32 pix-
els). thus, when using two thirds of the data for training, the total pool of training instances
is (20000   0.67)   (32   32   11   2) = 300m, increasing the size by a factor of 22,528.
competitors: we compared our results with a variety of alternatives. these included the
conventional hog-id166 pipeline [6], structured ensemble matching [16], multi-kernel
id166 [17], the current state-of-the-art fisher vector spatial pooling (fv-sp) [21], and dnn
based models including alexnet [13] and lenet [15]. alexnet is a large deep id98 de-
signed for classifying id163 lsvrc-2010 [3] images. it has    ve convolutional layers
and 3 fully connected layers. we used two versions of alexnet: (i) alexnet-id166: follow-
ing common practice [4], it was used as a pre-trained feature extractor, by taking the second
4096d fully-connected layer of the id163-trained model as a feature vector for id166
classi   cation. (ii) alexnet-sketch: we re-trained alexnet for the 250-category sketch clas-
si   cation task, i.e. it was trained using the same data as our sketch-a-net. finally, although
lenet is quite old, we note that it is speci   cally designed for handwritten digits rather than
photos. thus it is potentially more suited for sketches than the photo-oriented alexnet.

hog-id166 [6]

ensemble [16]

mkl-id166 [17]

fv-sp [21]

alexnet-id166 [13] alexnet-sketch [13]

61.5%

68.6%

65.8%

lenet [15]

55.2%

68.9

74.9%

sketch-a-net human [6]

73.1%

56%

67.1%

table 2: comparison with state of the art results on sketch recognition

comparative results: we    rst report the sketch recognition results of our full sketch-a-
net, compared to state-of-the-art alternatives as well as humans in table 2. the following
observations can be made: (i) sketch-a-net signi   cantly outperforms all existing methods
purpose designed for sketch [6, 16, 21], as well as the state-of-the-art photo-oriented id98
model [13] repurposed for sketch; (ii) we show for the    rst time, an automated sketch recog-
nition model can surpass human performance on sketch recognition (74.9% by our sketch-
a-net vs. 73.1% for humans based on the study in [6]); (iii) sketch-a-net is superior to
alexnet, despite being much smaller with only 14% of the total number of parameters of
alexnet. this veri   es that new network design is required for sketch images. in particular, it
is noted that either trained using the larger id163 data (67.1%) or the sketch data (68.6%),
alexnet cannot beat the best hand-crafted feature based approach (68.9% of fv-sp); (iv)
among the deep dnn based models, the performance of lenet (55.2%) is the weakest.
although designed for handwriting digit recognition, a task similar to that of sketch recog-
nition, the model is much simpler and shallow. this suggests that a deeper/more complex
model is necessary to cope with the larger intra-class variations exhibited in sketches; (v)
last but not least, upon close category-level examination, we found that sketch-a-net tends
to perform better at    ne-grained object categories. this indicates that sketch-a-net learned
a more discriminative feature representation capturing    ner details than conventional hand-
crafted features, as well as human. for example, for    seagull   ,       ying-bird   ,    standing-bird   
and    pigeon   , all of which belong to the coarse semantic category of    bird   , sketch-a-net
obtained an average accuracy of 42.5% while human only achieved 24.8%. in particular,
the category    seagull   , is the worst performing category for human with an accuracy of just

yu, yang, song, xiang, hospedales: sketch-a-net that beats humans

9

2.5%, since it was mostly confused with other types of birds.
yielded 23.9% for    seagull    which is nearly 10 times better.

in contrast, sketch-a-net

full model (m-cha+m-sca) m-cha+s-sca

s-cha+s-sca alexnet-sketch

74.9%

72.6%

72.2%

68.6%

table 3: evaluation on the contributions of individual components of sketch-a-net.

contributions of individual components: compared to conventional photo-oriented dnns
such as alexnet, our sketch-a-net has three distinct features: (i) the speci   c network archi-
tecture (see sec. 3.1), (ii) the multi-channel structure for modelling stroke ordering (see
sec. 3.2), and (iii) the multi-scale network ensemble to deal with variable levels of ab-
straction (see sec. 3.3).
in this experiment, we evaluate the contributions of each new
feature. speci   cally, we examined two stripped-down versions of our full model (multi-
channel-multi-scale (m-cha+m-sca)): multi-channel-single-scale (m-cha+s-sca) sketch-
a-net which uses only one network scale (the original scale of 256    256), and single-
channel-single-scale (s-cha+s-sca) sketch-a-net which uses only sketches at the original
scale. results in table 2 show that all three features contribute to the    nal strong perfor-
mance of sketch-a-net. in particular, (i) the improvement of s-cha+s-sca over alexnet-
sketch shows that our sketch-speci   c network architecture is effective; (ii) m-cha+s-sca
achieved better performance than s-cha+s-sca, indicating the multi-channel features worked;
(iii) the best result is achieved when all three new features are combined.

joint bayesian

feature fusion

score fusion

74.9%

72.8%

74.1%

table 4: comparison of different fusion strategies.

comparison of different fusion strategies: given an ensemble of sketch-a-net at dif-
ferent scales, various fusion strategies can be adopted for the    nal classi   cation task. table 4
compares our joint bayesian fusion method with the two most commonly adopted alterna-
tives: feature level fusion and score level fusion. for feature level fusion, we treat each single
scale network as a feature extractor, and concatenate the 512d outputs of their penultimate
layers into a single feature. we then trained a linear id166 based on this 5   512 = 2560d
feature vector. for score level fusion, we average the 250d softmax probabilities of each
network in the ensemble to make a    nal prediction. for jb fusion, we take the same 2560d
concatenated feature vector used by feature fusion, but perform knn matching with jb sim-
ilarity metric, rather than id166 classi   cation. interestingly, although score fusion is better
than vanilla id166 feature fusion, jb makes much better use of the concatenated feature vec-
tor because the full covariance model better learns how to weight the outputs of the networks
and exploit their complementarity.
qualitative results: figure 3 shows some qualitative results. some examples of surpris-
ingly tough successes are shown in green. mistakes made by the network (red) (intended
category of the sketcher in black) are very reasonable. the clear challenge level of their
ambiguity demonstrates why reliable sketch-based communication is hard even for humans.
what has been learned by sketch-a-net: as illustrated fig. 4, the    lters in the    rst
layer of sketch-a-net (fig. 4(left)) learn something very similar to the biologically plausible
gabor    lters (fig. 4(right)) [7]. this is interesting because it is not obvious that learning from
sketches should produce such    lters, as their emergence is typically attributed to learning
from the statistics of natural images [19, 24].

10

yu, yang, song, xiang, hospedales: sketch-a-net that beats humans

figure 3: qualitative illustration of recognition successes (green) and failures (red).

figure 4: visualisation of the learned    lters. left: randomly selected    lters from the    rst
layer in our model; right: the real parts of some gabor    lters.

running cost: our sketch-a-net model was implemented using matlab based on the mat-
convnet [1] toolbox. we trained our 5-network ensemble for 230 epochs each, with each
instance undergoing random data augmentation during each iteration. this took roughly
80 hours in total on a 2.60ghz cpu (without explicit parallelisation), or 10 hours using a
nvidia k40-gpu. note that this means sketch-a-net was not trained for long enough to
use the full pool of available data augmentations.
reproducibility: for reproducibility and to support future research, our training and test-
ing pipeline is made available at http://www.eecs.qmul.ac.uk/~tmh/.

5 conclusion
we have proposed a deep neural network based sketch recognition model, which we call
sketch-a-net, that beats human recognition performance by 1.8% on a large scale sketch
benchmark dataset. key to the superior performance of our method lies with the speci   cally
designed network model that accounts for unique characteristics found in sketches that were
otherwise unaddressed in prior art. the learned sketch feature representation could bene   t
other sketch-related applications such as sketch-based id162 and automatic sketch
synthesis, which could be interesting venues for future work.
acknowledgements: this project has received funding from the european union   s hori-
zon 2020 research and innovation programme under grant agreement no 640891. we grate-
fully acknowledge the support of nvidia corporation for the donation of the gpus used
for this research.

guitar'(violin)'windmill'(fan)'piano'satellite'(wristwatch)'floor'lamp'(wineglass)'panda'gira   e'bed'yu, yang, song, xiang, hospedales: sketch-a-net that beats humans
references

11

[1] k. chat   eld, k. simonyan, a. vedaldi, and a. zisserman. return of the devil in the

details: delving deep into convolutional nets. in bmvc, 2014.

[2] d. chen, x. cao, l. wang, f. wen, and j. sun. bayesian face revisited: a joint

formulation. in eccv, 2012.

[3] j. deng, w. dong, r. socher, l. li, k. li, and l. fei-fei. id163: a large-scale

hierarchical image database. in cvpr, 2009.

[4] j. donahue, y. jia, o. vinyals, j. hoffman, n. zhang, e. tzeng, and t. darrell. decaf:
a deep convolutional activation feature for generic visual recognition. in icml, 2015.

[5] m. eitz, k. hildebrand, t. boubekeur, and m. alexa. sketch-based id162:

benchmark and bag-of-features descriptors. tvcg, 2011.

[6] m. eitz, j. hays, and m. alexa. how do humans sketch objects? in siggraph, 2012.

[7] d. gabor. theory of communication. part 1: the analysis of information. journal of the
institution of electrical engineers-part iii: radio and communication engineering, 93
(26):429   441, 1946.

[8] g. e. hinton, n. srivastava, a. krizhevsky, i. sutskever, and r. r. salakhutdinov.
improving neural networks by preventing co-adaptation of feature detectors. in arxiv
preprint arxiv:1207.0580, 2012.

[9] r. hu and j. collomosse. a performance evaluation of gradient    eld hog descriptor

for sketch based id162. cviu, 2013.

[10] m. f. a. jabal, m. s. m. rahim, n. z. s. othman, and z. jupri. a comparative study
on extraction and recognition method of cad data from cad drawings. in international
conference on information management and engineering (icime), 2009.

[11] g. johnson, m. d. gross, j. hong, and e. yi-luen do. computational support for
sketching in design: a review. foundations and trends in human-computer interac-
tion, 2009.

[12] b. f. klare, z. li, and a. k. jain. matching forensic sketches to mug shot photos.

tpami, 2011.

[13] a. krizhevsky, i. sutskever, and g. e. hinton. id163 classi   cation with deep con-

volutional neural networks. in nips, 2012.

[14] y. le cun, b. boser, j. s. denker, d. henderson, r. e. howard, w. hubbard, and l. d.
jackel. handwritten digit recognition with a back-propagation network. in nips, 1990.

[15] y. lecun, l. bottou, g. b. orr, and k. m  ller. ef   cient backprop. neural networks:

tricks of the trade, pages 9   48, 2012.

[16] y. li, y. song, and s. gong. sketch recognition by ensemble matching of structured

features. in bmvc, 2013.

12

yu, yang, song, xiang, hospedales: sketch-a-net that beats humans

[17] y. li, t. m. hospedales, y. song, and s. gong. free-hand sketch recognition by multi-

kernel id171. cviu, 2015.

[18] t. lu, c. tai, f. su, and s. cai. a new recognition model for electronic architectural

drawings. computer-aided design, 2005.

[19] b. a. olshausen and field d. j. emergence of simple-cell receptive    eld properties by

learning a sparse code for natural images. nature, 1996.

[20] s. ouyang, t. hospedales, y. song, and x. li. cross-modal face matching: beyong

viewed sketches. in accv, 2014.

[21] r. g. schneider and t. tuytelaars. sketch classi   cation and classi   cation-driven anal-

ysis using    sher vectors. in siggraph asia, 2014.

[22] k. simonyan and a. zisserman. very deep convolutional networks for large-scale

image recognition. in iclr, 2015.

[23] p. sousa and m. j. fonseca. geometric matching for clip-art drawing retrieval. journal

of visual communication and image representation, 20(2):71   83, 2009.

[24] m. f. stollenga, j. masci, f. gomez, and j. schmidhuber. deep networks with internal

selective attention through feedback connections. in nips, 2014.

[25] f. wang, l. kang, and y. li. sketch-based 3d shape retrieval using convolutional

neural networks. in arxiv preprint arxiv:1504.03504, 2015.

[26] f. yin, q. wang, x. zhang, and c. liu. icdar 2013 chinese handwriting recognition
competition. in international conference on document analysis and recognition (ic-
dar), 2013.

[27] m. d. zeiler and r. fergus. visualizing and understanding convolutional networks. in

eccv, 2014.

[28] c. l. zitnick and d. parikh. bringing semantics into focus using visual abstraction. in

cvpr, 2013.

