semi-supervised id51 with neural models

dayu yuan

julian richardson
{dayuyuan,jdcr,portal   re,colinhevans,ealtendorf}@google.com

google, mountain view ca, usa

ryan doherty

colin evans

eric altendorf

6
1
0
2

 

v
o
n
5

 

 
 
]
l
c
.
s
c
[
 
 

2
v
2
1
0
7
0

.

3
0
6
1
:
v
i
x
r
a

abstract

determining the intended sense of words in text     id51 (wsd)     is a long-
standing problem in natural language processing. recently, researchers have shown promising
results using word vectors extracted from a neural network language model as features in wsd
algorithms. however, a simple average or concatenation of word vectors for each word in a text
loses the sequential and syntactic information of the text. in this paper, we study wsd with a
sequence learning neural net, lstm, to better capture the sequential and syntactic patterns of the
text. to alleviate the lack of training data in all-words wsd, we employ the same lstm in a
semi-supervised label propagation classi   er. we demonstrate state-of-the-art results, especially
on verbs.
introduction

1
id51 (wsd) is a long-standing problem in natural language processing (nlp)
with broad applications. supervised, unsupervised, and knowledge-based approaches have been studied
for wsd (navigli, 2009). however, for all-words wsd, where all words in a corpus need to be annotated
with word senses, it has proven extremely challenging to beat the strong baseline, which always assigns
the most frequent sense of a word without considering the context (pradhan et al., 2007a; navigli, 2009;
navigli et al., 2013; moro and navigli, 2015). given the good performance of published supervised
wsd systems when provided with signi   cant training data on speci   c words (zhong and ng, 2010), it
appears lack of suf   cient labeled training data for large vocabularies is the central problem.

one way to leverage unlabeled data is to train a neural network language model (nnlm) on the data.
id27s extracted from such a nnlm (often id97 (mikolov et al., 2013)) can be incorpo-
rated as features into a wsd algorithm. iacobacci et al. (2016) show that this can substantially improve
wsd performance and indeed that competitive performance can be attained using id27s
alone.

in this paper, we describe two novel wsd algorithms. the    rst is based on a long short term memory
(lstm) (hochreiter and schmidhuber, 1997). since this model is able to take into account word order
when classifying, it performs signi   cantly better than an algorithm based on a continuous bag of words
model (id97) (mikolov et al., 2013; iacobacci et al., 2016), especially on verbs.

we then present a semi-supervised algorithm which uses label propagation (talukdar and crammer,
2009; ravi and diao, 2016) to label unlabeled sentences based on their similarity to labeled ones. this
allows us to better estimate the distribution of word senses, obtaining more accurate decision boundaries
and higher classi   cation accuracy.

the best performance was achieved by using an lstm language model with label propagation. our
algorithm achieves state-of-art performance on many semeval all-words tasks. it also outperforms the
most-frequent-sense and id97 baselines by 10% (see section 5.2 for details).

organization: we review related work in section 2. we introduce our supervised wsd algorithm in
section 3, and the semi-supervised wsd algorithm in section 4. experimental results are discussed in
section 5. we provide further discussion and future work in section 6.
this work is licensed under a creative commons attribution 4.0 international license. license details: http://
creativecommons.org/licenses/by/4.0/

2 related work

the development of large lexical resources, such as id138 (fellbaum, 1998) and babelnet (navigli
and ponzetto, 2012), has enabled knowledge-based algorithms which show promising results on all-
words prediction tasks (ponzetto and navigli, 2010; navigli et al., 2013; moro and navigli, 2015).
wsd algorithms based on supervised learning are generally believed to perform better than knowledge-
based wsd algorithms, but they need large training sets to perform well (pradhan et al., 2007a; navigli
et al., 2007; navigli, 2009; zhong and ng, 2010). acquiring large training sets is costly. in this paper,
we show that a supervised wsd algorithm can perform well with     20 training examples per sense.

in the past few years, much progress has been made on using neural networks to learn word embed-
dings (mikolov et al., 2013; levy and goldberg, 2014), to construct language models (mikolov et al.,
2011), perform id31 (socher et al., 2013), machine translation (sutskever et al., 2014) and
many other nlp applications.

common elements:

a number of different ways have been studied for using id27s in wsd. there are some
    context embeddings. given a window of text wn   k, ..., wn, ..., wn+k surrounding a focus word wn
(whose label is either known in the case of example sentences or to be determined in the case of
classi   cation), an embedding for the context is computed as a concatenation or weighted sum of the
embeddings of the words wi, i (cid:54)= n. context embeddings of various kinds are used in both (chen et
al., 2014) and (iacobacci et al., 2016).
    sense embeddings. embeddings are computed for each word sense in the word sense inventory
(e.g. id138). in (rothe and sch  utze, 2015), equations are derived relating embeddings for word
senses with embeddings for undisambiguated words. the equations are solved to compute the sense
embeddings. in (chen et al., 2014), sense embeddings are computed    rst as weighted sums of the
embeddings of words in the id138 gloss for each sense. these are used in an initial id64
wsd phase, and then re   ned by a neural network which is trained on this bootstrap data.
    embeddings as id166 features. context embeddings (iacobacci et al., 2016; taghipour and ng,
2015b), or features computed by combining context embeddings with sense embeddings (rothe
and sch  utze, 2015), can be used as additional features in a supervised wsd system e.g. the id166-
based ims (zhong and ng, 2010). indeed iacobacci et al. (2016) found that using embeddings as
the only features in ims gave competitive wsd performance.
    nearest neighbor classi   er. another way to perform classi   cation is to    nd the word sense whose
sense embedding is closest, as measured by cosine similarity, to the embedding of the classi   cation
context. this is used, for example, in the id64 phase of (chen et al., 2014).
    retraining embeddings. a feedforward neural network can be used to jointly perform wsd and

adjust embeddings (chen et al., 2014; taghipour and ng, 2015b).

in our work, we start with a baseline classi   er which uses 1000-dimensional embeddings trained on
a 100 billion word news corpus using id97 (mikolov et al., 2013). the vocabulary consists of the
most frequent 1, 000, 000 words, without lemmatization or case id172. sense embeddings are
computed by averaging the context embeddings of sentences which have been labeled with that sense. to
classify a word in a context, we assign the word sense whose embedding has maximal cosine similarity
with the embedding of the context. this classi   er has similar performance to the best classi   er in (ia-
cobacci et al., 2016) when semcor is used as a source of labeled sentences. the id97 embeddings
are trained using a id159, i.e. without considering word order in the training context, and
word order is also not considered in the classi   cation context. in section 3 we show that using a more
expressive language model which takes account of word order yields signi   cant improvements.

semi-supervised learning has previously been applied successfully to id51. in
(yarowsky, 1995) id64 was used to learn a high precision wsd classi   er. a low recall classi   er
was learned from a small set of labeled examples, and the labeled set then extended with those sentences
from an unlabeled corpus which the classi   er could label with high con   dence. the classi   er was then
retrained, and this iterative training process continued to convergence. additional heuristics helped to
maintain the stability of the id64 process. the method was evaluated on a small data set.

in (niu et al., 2005), a label propagation algorithm was proposed for id51
and compared to id64 and a id166 supervised classi   er. label propagation can achieve better
performance because it assigns labels to optimize a global objective, whereas id64 propagates
labels based on local similarity of examples.

in section 4 we describe our use of label propagation to improve on nearest neighbor for classi   cation.

3 supervised wsd with lstm
neural networks with long short-term memory (lstm) units (hochreiter and schmidhuber, 1997) make
good language models which take into account word order (sundermeyer et al., 2012). we train a lstm
language model to predict the held-out word in a sentence. as shown in figure 1, we    rst replace the
held-out word with a special symbol $, and then, after consuming the remaining words in the sentence,
project the h dimensional hidden layer to a p dimensional context layer, and    nally predict the held out
word with softmax. by default, the lstm model has 2048 hidden units, 512 dimensional context layer
and 512 dimensional id27s. we also studied other settings, see section 5.2.2 for details. we
train the lstm on a news corpus of about 100 billion tokens, with a vocabulary of 1, 000, 000 words.
words in the vocabulary are neither lemmatized nor case normalized.

our lstm model is different from that of kgebck and salomonsson (k  ageb  ack and salomonsson,
2016). we train a lstm language model, which predicts a held-out word given the surrounding context,
with a large amount of unlabeled text as training data. the huge training dataset allows us to train a
high-capacity model (2048 hidden units, 512 dimensional embeddings), which achieves high precision
without over   tting. in our experiments, this directional lstm model was faster and easier to train than
a bidirectional lstm, especially given our huge training dataset. kgebck and salomonsson   s lstm
directly predicts the word senses and it is trained with a limited number of word sense-labeled exam-
ples. although id173 and dropout are used to avoid over   tting the training data, the bidirectional
lstm is small with only 74 + 74 neurons and 100 dimensional id27s (k  ageb  ack and sa-
lomonsson, 2016). because our lstm is generally applicable to any word, it achieves high performance
on all-words wsd tasks (see section 5 for details), which is the focus of this paper. kgebck and sa-
lomonsson   s lstm is only evaluated on lexical sample wsd tasks of semeval 2 and 3 (k  ageb  ack and
salomonsson, 2016).

figure 1: lstm: replace the focus word w3 with a special symbol $ and predict w3 at the end of the sentence.

the behavior of the lstm can be intuited by its predictions. table 1 shows the top 10 words predicted

by an lstm language model for the word    stock    in sentences containing various senses of    stock   .

in our initial experiments, we computed similarity between two contexts by the overlap between their
bags of predicted words. for example (table 1) the top predictions for the query overlap most with the
lstm predictions for    sense#1       we predict that    sense#1    is the correct sense. this bag of predic-
tions, while easily interpretable, is just a discrete approximation to the internal state of the lstm when
predicting the held out word. we therefore directly use the lstm   s context layer from which the bag
of predictions was computed as a representation of the context (see figure 1). given context vectors ex-
tracted from the lstm, our supervised wsd algorithms classify a word in a context by    nding the sense
vector which has maximum cosine similarity to the context vector (figure 2a). we    nd the sense vectors

(cid:7658)(w1)(cid:7658)(w5)(cid:7658)(w4)(cid:7658)(w2)w1w2w4w5(cid:7658)($)$htht+1ht+2ht+3ht+4(cid:7658)(eos)eosht+5context layerlstmw3id
1

2

3

sentence
employee compensation is offered in the form of cash
and/or stock.
the stock would be redeemed in    ve years, subject to
terms of the company   s debt.
these stores sell excess stock or factory overruns .
our soups are cooked with vegan stock and seasonal
vegetables.

4
query in addition, they will receive stock in the reorganized
company, which will be named ranger industries inc.

top 10 predictions from lstm
cash, stock, equity, shares, loans, bonus,
bene   ts, awards, equivalents, deposits
bonds, debt, notes, shares, stock, balance,
securities, rest, notes, debentures
inventory, goods, parts, sales, inventories,
capacity, products, oil, items, fuel
foods,
food, vegetables, meats,
cheese, meat, chicken, pasta, milk
shares, positions, equity, jobs, awards, rep-
resentation, stock, investments, roles, funds

recipes,

sense

sense#1

sense#2

sense#3

?

table 1: top predictions of    stock    in 5 sentences of different word senses

by averaging context vectors of all training sentences of the same sense. we observed in a few cases that
the context vector is far from the held-out word   s embedding, especially when the input sentence is not
informative. for example, the lstm language model will predict    night    for the input sentence    i fell
asleep at [work].    when we hold out    work   . currently, we treat the above cases as outliers. we would
like explore alternative solutions, e.g., forcing the model to predict words that are close to one sense
vector of the held-out word, in further work. as can be seen in semeval all-words tasks and tables 6,
this lstm model has signi   cantly better performance than the id97 models.

4 semi-supervised wsd

(a) nearest neighbor

(b) semi-supervised wsd with label propagation

figure 2: wsd classi   ers. filled nodes represent labeled sentences. un   lled nodes represent unlabeled sentences.

the non-parametric nearest neighbor algorithm described in section 3 has the following drawbacks:
    it assumes a spherical shape for each sense cluster, being unable to accurately model the decision
    it has no training data for, and does not model, the sense prior, omitting an extremely powerful

boundaries given the limited number of examples.

potential signal.

to overcome these drawbacks we present a semi-supervised method which augments the labeled ex-
ample sentences with a large number of unlabeled sentences from the web. sense labels are then propa-
gated from the labeled to the unlabeled sentences. adding a large number of unlabeled sentences allows
the decision boundary between different senses to be better approximated.

a label-propagation graph consists of (a) vertices with a number of labeled seed nodes and (b) undi-
rected weighted edges. label propagation (lp) (talukdar and crammer, 2009) iteratively computes a
distribution of labels on the graph   s vertices to minimize a weighted combination of:

    the discrepancy between seed labels and their computed labels distributions.
    the disagreement between the label distributions of connected vertices.
    a id173 term which penalizes distributions which differ from the prior (by default, a uni-

form distribution).

we construct a graph for each lemma with labeled vertices for the labeled example sentences, and
unlabeled vertices for sentences containing the lemma, drawn from some additional corpus. vertices for
suf   ciently similar sentences (based on criteria discussed below) are connected by an edge whose weight
is the cosine similarity between the respective context vectors, using the lstm language model. to

?c1c2c3?classify an occurrence of the lemma, we create an additional vertex for the new sentence and run lp to
propagate the sense labels from the seed vertices to the unlabeled vertices.

figure 2 (b) illustrates the graph con   guration. spatial proximity represents similarity of the sentences
attached to each vertex and the shape of each node represents the word sense. filled nodes represent seed
nodes with known word senses. un   lled nodes represent sentences with no word sense label, and the ?
represents the word we want to classify.

with too many edges, sense labels propagate too far, giving low precision. with too few, sense labels
do not propagate suf   ciently, giving low recall. we found that the graph has about the right density
for common senses when we ranked vertex pairs by similarity and connected the pairs above the 95
percentile. this may still leave rare senses sparsely connected, so we additionally added edges to ensure
that every vertex is connected to at least 10 other vertices. our experiments (table 9) show that this
setting achieves good performance on wsd, and the performance is stable when the percentile ranges
between 85 to 98. since it requires running lp for every classi   cation, the algorithm is slow compared
to the nearest neighbor algorithm.

5 experiments

we evaluated the lstm algorithm with and without label propagation on standard semeval all-words
tasks using id138 as the inventory. our proposed algorithms achieve state-of-art performance on
many semeval all-words wsd tasks. in order to assess the effects of training corpus size and language
model capacity we also evaluate our algorithms using the new oxford american dictionary (noad)
inventory with semcor (miller et al., 1993) or masc 1.

5.1 semeval tasks
in this section, we study the performance of our classi   ers on senseval2 (edmonds and cotton, 2001),
senseval3 (snyder and palmer, 2004), semeval-2007 (pradhan et al., 2007b), semeval-2013 task
12 (navigli et al., 2013) and semeval-2015 task 13 (moro and navigli, 2015) 2. we focus the study
on all-words wsd tasks. for a fair comparison with related works, the classi   ers are evaluated on all
words (both polysemous and monosemous).

following related works, we use semcor or omsti (taghipour and ng, 2015a) for training. in our
lp classi   ers, unlabeled data for each lemma consists either of 1000 sentences which contain the lemma,
randomly sampled from the web, or all omsti sentences (without labels) which contain the lemma.

model
ims + id97 (t:semcor)
ims + id97 (t:omsti)
taghipour and ng (2015b)
chen et al. (2014)
weissenborn et al. (2015)
id97 (t:semcor)
lstm (t:semcor)
lstm (t:omsti)
lstmlp (t:semcor, u:omsti)
lstmlp (t:semcor, u:1k)
lstmlp (t:omsti, u:1k)

senseval3
all
n.

senseval2
n.
0.742 0.653 0.701 0.578 0.686
0.777 0.682 0.741 0.591 0.715

semeval7
all
n.

all
0.634
0.683

0.682

semeval7-coarse
all

n.

semeval13
n.

0.660

0.688

0.826 0.853
0.855
0.737 0.621 0.714 0.585 0.673 0.795 0.814
0.786 0.692 0.723 0.642 0.723 0.828 0.834
0.777 0.643 0.680 0.607 0.673 0.811 0.820
0.797 0.711 0.748 0.637 0.704 0.843 0.834
0.796 0.718 0.763 0.635 0.717 0.836 0.831
0.799 0.710 0.753 0.633 0.717 0.833 0.825

0.678
0.736
0.724
0.739
0.738
0.744

0.728
0.661
0.670
0.673
0.679
0.695
0.681

table 2: f1 scores on semeval all-words tasks. t:semcor stands for models trained with semcor. u:omsti stands for using
omsti as unlabeled sentences in semi-supervised wsd. ims + id97 scores are from (iacobacci et al., 2016)

table 2 shows the sem-eval results. our proposed algorithms achieve the highest all-words f1 scores
except for sem-eval 2013. weissenborn et al.(2015) only disambiguates nouns, and it outperforms our
algorithms on sem-eval 2013 by 4%, but is ranked behind our algorithms on senseval-3 and semeval-7

1http://www.anc.org/masc/about.html/
2we mapped all senses to id1383.0 by using maps in https://id138.princeton.edu/id138/download/current-version/

and http://web.eecs.umich.edu/ mihalcea/downloads.html

tasks with an f1 score more than 6% lower than our algorithms. uni   ed wsd (chen et al., 2014) has the
highest f1 score on nouns (sem-eval-7 coarse), but our algorithms outperform uni   ed wsd on other
part-of-speech tags.

settings for a fair comparison of id97 and lstm, we do not use pre-trained word-embeddings as
in (iacobacci et al., 2016), but instead train the id97 and lstm models on a 100 billion word news
corpus 3 with a vocabulary of the most frequent 1,000,000 words. our self-trained id27s
have similar performance to the pre-trained embeddings, as shown in table 2. the id97 word
vectors are of dimension 1024. the lstm model has 2048 hidden units, and inputs are 512-dimensional
word vectors. we train the lstm model by minimizing sampled softmax loss (jean et al., 2014) with
adagrad (duchi et al., 2011). the learning rate is 0.1. we experimented with other learning rates,
and observed no signi   cant performance difference after the training converges. we also downsample
frequent terms in the same way as (mikolov et al., 2013).

id97 vectors vs. lstm to better compare lstm with word vectors we also build a near-
est neighbor classi   er using id97 id27s and semcor example sentences, id97
(t:semcor). it performs similar to ims + id97 (t:semcor), a id166-based classi   er studied in (ia-
cobacci et al., 2016). table 2 shows that the lstm classi   er outperforms the id97 classi   er across
the board.

semcor vs. omsti contrary to the results observed in (iacobacci et al., 2016), the lstm classi   er
trained with omsti performs worse than that trained with semcor. it seems that the larger size of the
omsti training data set is more than offset by noise present in its automatically generated labels. while
the id166 classi   er studied in (iacobacci et al., 2016) may be able to learn a model which copes with this
noise, our naive nearest neighbor classi   ers do not have a learned model and deal less well with noisy
labels.

label propagation we use the implementation of dist expander (ravi and diao, 2016). we test
the label propagation algorithm with semcor or omsti as labeled data sets and omsti or 1000 random
sentences from the web per lemma as unlabeled data. the algorithm performs similarly on the different
data sets.

table 3 shows the results of sem-eval 2015. the lstm lp classi   er with an lstm language model
achieves the highest scores on nouns and adverbs as well as overall f1. the lstm classi   er has the
highest f1 on verbs.

algorithm
limsi
dfki
uniba
bfs baseline
id97 (t:semcor)
lstm (t:semcor)
lstmlp (t:semcor, u:1k)

all
0.647

0.663
0.667
0.721
0.726

n.

v.

adj.

0.703

0.577

0.667
0.661
0.713
0.728

0.551
0.555
0.642
0.622

0.790
0.821
0.789
0.813
0.813

adv.
0.795

0.825
0.810
0.845
0.857

table 3: f1 scores of semeval-2015 english dataset. the bfs baseline uses babelnet    rst sense.

5.2 noad eval
many dictionary lemmas and senses have no examples in semcor or ostmi, giving rise to losses in all-
words wsd when these corpora are used as training data. the above semeval scores do not distinguish
errors caused by missing training data for certain labels or inaccurate classi   er. to better study the pro-
posed algorithms, we train the classi   ers with the new oxford american dictionary (noad) (stevenson
and lindberg, 2010), in which there are example sentences for each word sense.

3the training corpus could not be released, but we have plans to open source the well-trained models

5.2.1 word sense inventory
the noad focuses on american english and is based on the oxford dictionary of english
(ode) (stevenson, 2010). it distinguishes between coarse (core) and    ne-grained (sub) word senses
in the same manner as ode. previous investigations (navigli, 2006; navigli et al., 2007) using the
ode have shown that coarse-grained word senses induced by the ode inventory address problems with
id138   s    ne-grained inventory, and that the inventory is useful for id51.

for our experiments, we use noad   s core senses, and we also use lexicographer-curated example sen-
tences from the semantic english language database (seld)4, provided by oxford university press.
we manually annotated all words of the english language semcor corpus and masc corpora with
noad word senses in order to evaluate performance 5. table 4 shows the total number of polysemes
(more than one core sense) and average number of senses per polyseme in noad/seld (hereafter,
noad), semcor and masc. both semcor and masc individually cover around 45% of noad pol-
ysemes and 62% of senses of those polysemes.

number of polysemous
lemmas in dictionary / corpus

sense count per polyseme

noad
semcor
masc
noad
semcor
masc

noun
8097
2833
2738
2.46
1.44
1.48

verb
2124
1362
1250
2.58
1.69
1.66

adj.
2126
911
829
2.30
1.49
1.48

adv.
266
193
181
2.47
1.84
2.01

table 4: noad polysemous lemma in noad, semcor and masc

table 5 gives the number of labeled sentences of these datasets. note that although noad has more
labeled sentences than semcor or masc, the average numbers of sentences per sense of these datasets
are similar. this is because noad has labeled sentences for each word sense, whereas semcor (masc)
only covers a subset of lemmas and senses (table 4). the last column of table 5 shows that each
annotated word in semcor and masc has an average of more than 4 noad corse senses. hence, a
random guess will have a precision around 1/4.

example count (in 1000   s)

dataset
noad
semcor
masc

all
580
115
133

n.
312
38
50

v.
150
57
57

adj.
95
11.6
12.7

adv
13
8.6
13.6

example count
per sense
18.43
14.27
17.38

number of candidate
senses per example

3.1
4.1
4.2

table 5: number of examples in each dataset and the average sense count per example.

in the default setting, we use noad example sentences as labeled training data and evaluate on sem-

cor and masc. we evaluate all polysemous words in the evaluation corpus.

5.2.2 lstm classi   er
we compare our algorithms with two baseline algorithms:

    most frequent sense: compute the sense frequency (from a labeled corpus) and label word w with
    id97: a nearest-neighbor classi   er with id97 id27, which has similar per-

w   s most frequent sense.

formance to cutting-edge algorithms studied in (iacobacci et al., 2016) on semeval tasks.

table 6 compares the f1 scores of the lstm and baseline algorithms. lstm outperforms id97
by more than 10% over all words, where most of the gains are from verbs and adverbs. the results sug-
gest that syntactic information, which is well modeled by lstm but ignored by id97, is important
to distinguishing word senses of verbs and adverbs.

4http://oxfordgls.com/our-content/english-language-content/
5https://research.google.com/research-outreach.html#/research-outreach/research-datasets

eval data
train data
model
frequent sense
semcor
frequent sense masc
noad
id97
semcor
id97
id97
noad,semcor
masc
id97
noad,masc
id97
noad
lstm
lstm
semcor
noad,semcor
lstm
masc
lstm
lstm
noad,masc

semcor
all
n.

v.

adj.

adv.

0.752 0.751 0.749 0.737 0.789
0.709 0.783 0.657 0.736 0.693

0.698 0.785 0.619 0.766 0.744
0.695 0.801 0.605 0.767 0.719
0.786 0.796 0.782 0.781 0.784

0.810 0.825 0.799 0.809 0.825
0.821 0.834 0.814 0.818 0.821

masc

n.

all
adv.
0.753 0.799 0.713 0.758 0.741

adj.

v.

0.671 0.790 0.562 0.724 0.638
0.692 0.806 0.592 0.754 0.635
0.678 0.808 0.565 0.753 0.604

0.786 0.805 0.772 0.776 0.786
0.799 0.843 0.767 0.808 0.767
0.812 0.846 0.786 0.816 0.798

table 6: f1 scores of lstm algorithm in comparison with baselines

eval data
model
lstm
lstm
lstm
lstm
lstm

train data
noad
semcor
noad,semcor
masc
noad,masc

semcor
all
n.
adv.
0.769 0.791 0.759 0.751 0.672

adj.

v.

0.631 0.653 0.606 0.617 0.600
0.782 0.803 0.774 0.761 0.688

masc

v.

n.

adj.

all
adv.
0.780 0.791 0.768 0.780 0.726
0.656 0.663 0.668 0.643 0.581
0.796 0.805 0.790 0.794 0.742

table 7: macro f1 scores of lstm classi   er

change of training data by default, the wsd classi   er uses the noad example sentences as training
data. we build a larger training dataset by adding labeled sentences from semcor and masc, and study
the change of f1 scores in table 6. across all part of speech tags and datasets, f1 scores increase after
adding more training data. we further test our algorithm by using semcor (or masc) as training data
(without noad examples). the semcor (or masc) trained classi   er is on a par with the noad trained
classi   er on f1 score. however, the macro f1 score of the former is much lower than the latter, as shown
in table 7, because of the limited coverage of rare senses and words in semcor and masc.

change of language model capacity in this experiment, we change the lstm model capacity by
varying the number of hidden units h and the dimensions of the input embeddings p and measuring f1.
figure 3 shows strong positive correlation between f1 and the capacity of the language model. however,
larger models are slower to train and use more memory. to balance the accuracy and resource usage, we
use the second best lstm model (h = 2048 and p = 512) by default.

(a) eval on semcor

(b) eval on masc

figure 3: f1 scores of lstm models with different capacity: h is the number of hidden units; p is the embedding dimension.

0.7	0.74	0.78	0.82	h512	p128	h512_p256	h1024_p256	h1024_p512	h2048_p512	h2048_p1024	overall	noun	verb	adj	adv	0.7	0.74	0.78	0.82	h512_p128	h512_p256	h1024_p256	h1024_p512	h2048_p512	h2048_p1024	overall	noun	verb	adj	adv	5.2.3 semi-supervised wsd
we evaluate our semi-supervised wsd classi   er in this subsection. we construct the graph as described
in section 4 and run lp to propagate sense labels from the seed vertices to the unlabeled vertices. we
evaluate the performance of the algorithm by comparing the predicted labels and the gold labels on eval
nodes. as can be observed from table 8, lp did not yield clear bene   ts when using the id97
language model. we did see signi   cant improvements, 6.3% increase on semcor and 7.3% increase on
masc, using lp with the lstm language model. we hypothesize that this is because lp is sensitive to
the quality of the graph distance metric.

train data

eval data
model
id97 lp noad
noad
lstm lp
noad,semcor
lstm lp
lstm lp
noad,masc

semcor
all
adv.
n.
0.642 0.733 0.554 0.705 0.725
0.822 0.859 0.800 0.817 0.816

adj.

v.

0.873 0.883 0.870 0.858 0.874

table 8: f1 scores of label propagation

masc

v.

n.

adj.

all
adv.
0.643 0.752 0.524 0.726 0.664
0.831 0.865 0.806 0.825 0.821
0.872 0.897 0.852 0.865 0.868

change of seed data: as can be seen in table 8, lp substantially improves classi   er f1 when the
training datasets are semcor+noad or masc+noad. as discussed in section 4, the improvement
may come from explicitly modeling the sense prior. we did not see much performance lift by increasing
the number of unlabeled sentences per lemma.
change of graph density: by default, we construct the lp graph by connecting two nodes if their
af   nity is above 95% percentile. we also force each node to connect to at least 10 neighbors to prevent
isolated nodes. table 9 shows the performance of the lp algorithm by changing the percentile threshold.
the f1 scores are relatively stable when the percentile ranges between 85 to 98, but decrease when the
percentile drops to 80. also, it takes longer to run the lp algorithm on a denser graph. we pick the 95
percentile in our default setting to achieve both high f1 scores and short running time.

semcor
85

masc
85

pos-tag
overall
n.
v.

90

95

98
0.823 0.822 0.823 0.818 0.813 0.800 0.827 0.831 0.835 0.830 0.824 0.806
0.848 0.859 0.852 0.846 0.840 0.828 0.863 0.865 0.868 0.865 0.861 0.847
0.810 0.800 0.803 0.797 0.792 0.778 0.800 0.806 0.806 0.799 0.794 0.769

80

70

98

95

90

80

70

table 9: f1 scores of the lstm lp trained on noad with varying graph density.

6 conclusions and future work
in this paper, we have presented two wsd algorithms which combine (1) lstm neural network language
models trained on a large unlabeled text corpus, with (2) labeled data in the form of example sentences,
and, optionally, (3) unlabeled data in the form of additional sentences. using an lstm language model
gave better performance than one based on id97 embeddings. the best performance was achieved
by our semi-supervised wsd algorithm which builds a graph containing labeled example sentences
augmented with a large number of unlabeled sentences from the web, and classi   es by propagating
sense labels through this graph.

several unanswered questions suggest lines of future work. since our general approach is amenable to
incorporating any language model, further developments in nnlms may permit increased performance.
we would also like to better understand the limitations of id38 for this task: we expect
there are situations     e.g., in idiomatic phrases     where per-word predictions carry little information.

we believe our model should generalize to languages other than english, but have not yet explored
this. character-level lstms (kim et al., 2015) may provide robustness to morphology and diacritics and
may prove useful even in english for spelling errors and out of vocabulary words.

we would like to see whether our results can be improved by incorporating global (document) context

and multiple embeddings for polysemous words (huang et al., 2012).

finally, many applications of wsd systems for nominal resolution require integration with resolution
systems for named entities, since surface forms often overlap (moro et al., 2014; navigli and ponzetto,
2012). this will require inventory alignment work and model reformulation, since we currently use no
document-level, topical, or knowledge-base coherence features.

we thank our colleagues and the anonymous reviewers for their insightful comments on this paper.

references
xinxiong chen, zhiyuan liu, and maosong sun. 2014. a uni   ed model for word sense representation and

disambiguation. in emnlp, pages 1025   1035. citeseer.

john duchi, elad hazan, and yoram singer. 2011. adaptive subgradient methods for online learning and stochas-

tic optimization. j. mach. learn. res., 12:2121   2159, july.

philip edmonds and scott cotton. 2001. senseval-2: overview.

in proceedings of senseval-2 second
international workshop on evaluating id51 systems, pages 1   5, toulouse, france, july.
association for computational linguistics.

christiane fellbaum. 1998. id138. wiley online library.

sepp hochreiter and j  urgen schmidhuber. 1997. long short-term memory. neural computation, 9(8):1735   1780.

eric h huang, richard socher, christopher d manning, and andrew y ng. 2012. improving word representations
via global context and multiple word prototypes. in proceedings of the 50th annual meeting of the associa-
tion for computational linguistics: long papers-volume 1, pages 873   882. association for computational
linguistics.

ignacio iacobacci, mohammad taher pilehvar, and roberto navigli. 2016. embeddings for word sense disam-

biguation: an evaluation study. in acl. citeseer.

s. jean, k. cho, r. memisevic, and y. bengio. 2014. on using very large target vocabulary for neural machine

translation. arxiv e-prints, december.

m. k  ageb  ack and h. salomonsson. 2016. id51 using a bidirectional lstm. arxiv

e-prints.

yoon kim, yacine jernite, david sontag, and alexander m rush. 2015. character-aware neural language models.

arxiv preprint arxiv:1508.06615.

omer levy and yoav goldberg. 2014. neural id27 as implicit id105. in advances in

neural information processing systems, pages 2177   2185.

tom  a  s mikolov, stefan kombrink, luk  a  s burget, jan honza   cernock`y, and sanjeev khudanpur. 2011. extensions
of recurrent neural network language model. in acoustics, speech and signal processing (icassp), 2011 ieee
international conference on, pages 5528   5531. ieee.

tomas mikolov, ilya sutskever, kai chen, greg s corrado, and jeff dean. 2013. distributed representations of
words and phrases and their compositionality. in c. j. c. burges, l. bottou, m. welling, z. ghahramani, and
k. q. weinberger, editors, advances in neural information processing systems 26, pages 3111   3119. curran
associates, inc.

george a miller, claudia leacock, randee tengi, and ross t bunker. 1993. a semantic concordance.

in
proceedings of the workshop on human language technology, pages 303   308. association for computational
linguistics.

andrea moro and roberto navigli. 2015. semeval-2015 task 13: multilingual all-words sense disambiguation

and entity linking. proc. of semeval, pages 288   297.

andrea moro, alessandro raganato, and roberto navigli. 2014. entity linking meets id51:

a uni   ed approach. transactions of the association for computational linguistics, 2:231   244.

roberto navigli and simone paolo ponzetto. 2012. babelnet: the automatic construction, evaluation and appli-

cation of a wide-coverage multilingual semantic network. arti   cial intelligence, 193:217   250.

roberto navigli, kenneth c litkowski, and orin hargraves. 2007. semeval-2007 task 07: coarse-grained en-
glish all-words task. in proceedings of the 4th international workshop on semantic evaluations, pages 30   35.
association for computational linguistics.

roberto navigli, david jurgens, and daniele vannella. 2013. semeval-2013 task 12: id73 sense
in second joint conference on lexical and computational semantics (* sem), volume 2,

disambiguation.
pages 222   231.

roberto navigli. 2006. meaningful id91 of senses helps boost id51 performance. in
proceedings of the 21st international conference on computational linguistics and the 44th annual meeting
of the association for computational linguistics, acl-44, pages 105   112, stroudsburg, pa, usa. association
for computational linguistics.

roberto navigli. 2009. id51: a survey. acm computing surveys (csur), 41(2):10.

zheng-yu niu, dong-hong ji, and chew lim tan. 2005. id51 using label propagation
based semi-supervised learning. in proceedings of the 43rd annual meeting on association for computational
linguistics, pages 395   402. association for computational linguistics.

simone paolo ponzetto and roberto navigli. 2010. knowledge-rich id51 rivaling super-
vised systems. in proceedings of the 48th annual meeting of the association for computational linguistics,
acl    10, pages 1522   1531, stroudsburg, pa, usa. association for computational linguistics.

sameer s pradhan, edward loper, dmitriy dligach, and martha palmer. 2007a. semeval-2007 task 17: english
lexical sample, srl and all words. in proceedings of the 4th international workshop on semantic evaluations,
pages 87   92. association for computational linguistics.

sameer s pradhan, edward loper, dmitriy dligach, and martha palmer. 2007b. semeval-2007 task 17: english
lexical sample, srl and all words. in proceedings of the 4th international workshop on semantic evaluations,
pages 87   92. association for computational linguistics.

sujith ravi and qiming diao. 2016. large scale distributed semi-supervised learning using streaming approxima-

tion. in aistats.

sascha rothe and hinrich sch  utze. 2015. autoextend: extending id27s to embeddings for synsets
in proceedings of the 53rd annual meeting of the association for computational linguistics
and lexemes.
and the 7th international joint conference on natural language processing (volume 1: long papers), pages
1793   1803, beijing, china, july. association for computational linguistics.

benjamin snyder and martha palmer. 2004. the english all-words task.

in senseval-3: third international

workshop on the evaluation of systems for the semantic analysis of text, pages 41   43.

richard socher, alex perelygin, jean y wu, jason chuang, christopher d manning, andrew y ng, and christo-
pher potts. 2013. recursive deep models for semantic compositionality over a sentiment treebank. in pro-
ceedings of the conference on empirical methods in natural language processing (emnlp), volume 1631, page
1642. citeseer.

angus stevenson and christine a. lindberg. 2010. new oxford american dictionary.

angus stevenson. 2010. oxford dictionary of english.

martin sundermeyer, ralf schl  uter, and hermann ney. 2012. lstm neural networks for id38. in

interspeech, pages 194   197.

ilya sutskever, oriol vinyals, and quoc v le. 2014. sequence to sequence learning with neural networks. in
z. ghahramani, m. welling, c. cortes, n. d. lawrence, and k. q. weinberger, editors, advances in neural
information processing systems 27, pages 3104   3112. curran associates, inc.

kaveh taghipour and hwee tou ng. 2015a. one million sense-tagged instances for id51

and induction. conll 2015, page 338.

kaveh taghipour and hwee tou ng. 2015b. semi-supervised id51 using id27s
in general and speci   c domains. in proceedings of the 2015 conference of the north american chapter of the
association for computational linguistics: human language technologies, pages 314   323, denver, colorado,
may   june. association for computational linguistics.

partha pratim talukdar and koby crammer. 2009. new regularized algorithms for transductive learning.

in
proceedings of the european conference on machine learning and knowledge discovery in databases: part
ii, ecml pkdd    09, pages 442   457, berlin, heidelberg. springer-verlag.

dirk weissenborn, leonhard hennig, feiyu xu, and hans uszkoreit. 2015. multi-objective optimization for the
joint disambiguation of nouns and named entities. in proceedings of the 53rd annual meeting of the association
for computational linguistics and the 7th international joint conference on natural language processing of
the asian federation of natural language processing, pages 596   605.

david yarowsky. 1995. unsupervised id51 rivaling supervised methods. in proceedings
of the 33rd annual meeting on association for computational linguistics, pages 189   196. association for
computational linguistics.

zhi zhong and hwee tou ng. 2010. it makes sense: a wide-coverage id51 system for free

text. in acl, acldemos    10.

