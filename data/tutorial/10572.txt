id21 for low-resource id4

barret zoph1, deniz yuret2, jonathan may1, kevin knight3
1information sciences institute, university of southern california

{zoph, jonmay}@isi.edu

2computer engineering, koc   university

dyuret@ku.edu.tr

3information sciences institute &

computer science department, university of southern california

knight@isi.edu

6
1
0
2

 
r
p
a
8

 

 
 
]
l
c
.
s
c
[
 
 

1
v
1
0
2
2
0

.

4
0
6
1
:
v
i
x
r
a

abstract

the encoder-decoder framework for neu-
ral machine translation (id4) has been
shown effective in large data scenarios, but
is much less effective for low-resource lan-
guages. we present a id21
method that signi   cantly improves id7
scores across a range of low-resource lan-
guages. our key idea is to    rst train a high-
resource language pair (the parent model),
then transfer some of the learned param-
eters to the low-resource pair (the child
model) to initialize and constrain training.
using our id21 method we im-
prove baseline id4 models by an aver-
age of 5.6 id7 on four low-resource lan-
guage pairs. ensembling and unknown
word replacement add another 2 id7
which brings the id4 performance on
low-resource machine translation close to
a strong syntax based machine transla-
tion (sbmt) system, exceeding its perfor-
mance on one language pair. additionally,
using the id21 model for re-
scoring, we can improve the sbmt sys-
tem by an average of 1.3 id7, improving
the state-of-the-art on low-resource ma-
chine translation.

introduction

1
id4 (id4) (sutskever et
al., 2014) is a promising paradigm for extracting
translation knowledge from parallel text. id4
systems have achieved competitive accuracy rates
under large-data training conditions for language
pairs such as english-french. however, neural
methods are data-hungry and learn poorly from
low-count events. this behavior makes vanilla
id4 a poor choice for low-resource languages,

test sbmt id4
language train
id7 id7
size
size
16.8
1.0m 11.3k
11.4
1.4m 11.6k
1.8m 11.5k
10.7
5.2
0.2m 11.4k

hausa
turkish
uzbek
urdu

23.7
20.4
17.9
17.9

table 1: id4 models with attention are outper-
formed by standard string-to-tree statistical mt
(sbmt) when translating low-resource languages
into english. train/test bitext corpus sizes are
given in word tokens on the english side. single-
reference, case-insensitive id7 scores are given
for held-out test corpora.

where parallel data is scarce. table 1 shows that
for 4 low-resource languages, a standard string-
to-tree statistical mt system (sbmt) (galley et
al., 2004; galley et al., 2006) strongly outperforms
id4, even when id4 uses the state-of-the-art lo-
cal attention plus feed-input techniques from lu-
ong et al. (2015a).

in this paper, we give a method for substan-
tially improving id4 results on these languages.
neural models learn representations of their in-
put that are often useful across tasks. our key
idea is to    rst train a high-resource language pair,
then use the resulting trained network (the par-
ent model) to initialize and constrain training for
our low-resource language pair (the child model).
we    nd that we can optimize our results by    x-
ing certain parameters of the parent model, letting
the rest be    ne-tuned by the child model. we re-
port id4 improvements from id21 of
5.6 id7 on average, and we provide an analysis
of why the method works. the    nal id4 sys-
tem approaches strong sbmt baselines in in all
four language pairs, and exceeds sbmt perfor-
mance in one of them. furthermore, we show that

2 id4 background
in the neural encoder-decoder framework for mt
(neco and forcada, 1997; casta  no and casacu-
berta, 1997; sutskever et al., 2014; bahdanau et
al., 2014; luong et al., 2015a), we use a recurrent
neural network (encoder) to convert a source sen-
tence into a dense,    xed-length vector. we then
use another recurrent network (decoder) to con-
vert that vector to a target sentence.
in this pa-
per, we use a two-layer encoder-decoder system
(figure 1) with long short-term memory (lstm)
units (hochreiter and schmidhuber, 1997) trained
to optimize maximum likelihood (via a softmax
layer) with back-propagation through time (wer-
bos, 1990). additionally, we use an attention
mechanism that allows the target decoder to look
back at the source encoder, speci   cally the local
attention plus feed-input model from luong et al.
(2015a).

3 id21
id21 uses knowledge from a learned
task to improve the performance on a related task,
typically reducing the amount of required training
data (torrey and shavlik, 2009; pan and yang,
2010).
in natural language processing, transfer
learning methods have been successfully applied
to id103, document classi   cation and
id31 (wang and zheng, 2015). deep
learning models discover multiple levels of rep-
resentation, some of which may be useful across
tasks, which makes them particularly suited to
id21 (bengio, 2012). for example,
cires  an et al. (2012) use a convolutional neural
network to recognize handwritten characters and
show positive effects of transfer between models
for latin and chinese characters. ours is the    rst
study to apply id21 to neural machine
translation.

the id21 approach we use is simple
and effective. we    rst train an id4 model on a
dataset where there is a large amount of bilingual
data (e.g., french-english), which we call the par-
ent model. next, we initialize an id4 model with
the already-trained parent model. this new model
is then trained on a dataset with very little bilin-
gual data (e.g., uzbek-english), which we call the
child model. this means that the low-data id4
model will not start with random weights, but with
the weights from the parent model.

a justi   cation for this approach is that in sce-

figure 1: the encoder-decoder framework for
id4 (id4) (sutskever et
al., 2014). here, a source sentence c b a (pre-
sented in reverse order as a b c) is translated into
a target sentence w x y z. at each step, an evolv-
ing real-valued vector summarizes the state of the
encoder (blue) and decoder (red). not shown here
are the attention connections present in our model
used by the decoder to access encoder states.

id4 is an exceptional re-scorer of    traditional   
mt output; even id4 that on its own is worse
than sbmt is consistently able to improve upon
sbmt system output when incorporated as a re-
scoring model.

we start by a brief description of our id4
model in section 2. section 3 gives some back-
ground on id21 and explains how
we use it to improve machine translation perfor-
mance. our main experiments translating hausa,
turkish, uzbek and urdu into english with the
help of a french-english parent model are pre-
sented in section 4. section 5 explores alterna-
tives to our model to enhance understanding. we
   nd that the choice of parent language pair af-
fects performance, and provide an empirical up-
per bound on transfer performance using an arti-
   cial language. we experiment with english-only
language models, copy models, and word-sorting
models to show that what we transfer goes be-
yond monolingual information and using a trans-
lation model trained on bilingual corpora as a par-
ent is essential. we show the effects of freezing,
   ne-tuning, and smarter initialization of different
components of the attention-based id4 system
during transfer. we compare the learning curves
of transfer and no-transfer models showing that
transfer solves an over   tting problem, not a search
problem. we summarize our contributions in sec-
tion 6.

narios where we have limited training data, we
need a strong prior distribution over models. the
parent model trained on a large amount of bilin-
gual data can be considered an anchor point, the
peak of our prior distribution in model space.
when we train the child model initialized with the
parent model, we    x parameters likely to be useful
across tasks so that they will not be changed dur-
ing child-model training.
in the french-english
to uzbek-english example, as a result of the ini-
tialization, the english id27s from the
parent model are copied, but the uzbek words
are initially mapped to random french embed-
dings. the english embeddings should be kept but
the uzbek embeddings should be modi   ed dur-
ing training of the child model. freezing certain
portions of the parent model and    ne tuning oth-
ers can be considered a hard approximation to a
tight prior or strong id173 applied to some
of the parameters. we also experiment with or-
dinary l2 id173, but    nd it does not sig-
ni   cantly improve over the parameter freezing de-
scribed above.

our method results in large id7 increases for
a variety of low resource languages. in one of the
four language pairs our id4 system using trans-
fer beats a strong sbmt baseline. not only do
these transfer models do well on their own, they
also give large gains when used for rescoring n-
best lists (n = 1000) from the sbmt system. sec-
tion 4 details these results.

4 experiments

to evaluate how well our transfer method works
we apply it
to a variety of low-resource lan-
guages, both stand-alone and for re-scoring a
strong sbmt baseline. we report large id7 in-
creases across the board with our transfer method.
for all of our experiments with low-resource
languages we use french as the parent source
language and for child source languages we use
hausa, turkish, uzbek, and urdu. the target lan-
guage is always english. table 1 shows paral-
lel training data set sizes for the child languages,
where the language with the most data has only
1.8m english tokens. for comparison, our par-
ent french-english model uses a training set with
300 million english tokens and achieves 26 id7
on the development set. table 1 also shows the
sbmt system scores along with the id4 base-
lines that do not use transfer. there is a large gap

language sbmt id4 xfer final
hausa
24.0
18.7
turkish
16.8
uzbek
urdu
14.5

16.8
11.4
10.7
5.2

23.7
20.4
17.9
17.9

21.3
17.0
14.4
13.8

table 2: our method signi   cantly improves id4
results for the translation of low-resource lan-
guages into english. results show test-set id7
scores. the    id4    column shows results with-
out transfer, and the    xfer    column shows results
with transfer. the    final    column shows id7 af-
ter we ensemble 8 models and use unknown word
replacement.

between the sbmt and id4 systems without us-
ing our transfer method.

the sbmt system used in this paper is a string-
to-tree id151 system (gal-
ley et al., 2006; galley et al., 2004). in this sys-
tem there are two count-based 5-gram language
models. one is trained on the english side of the
wmt 2015 english-french dataset and the other
is trained on the english side of the low-resource
bitext. additionally, the sbmt models use thou-
sands of sparsely-occurring, lexicalized syntactic
features (chiang et al., 2009).

for our id4 system, we use development sets
for hausa, turkish, uzbek, and urdu to tune
the learning rate, parameter initialization range,
dropout rate and hidden state size for all the ex-
periments. for training we use a minibatch size
of 128, hidden state size of 1000, a target vocab-
ulary size of 15k and a source vocabulary size of
30k. the child models are trained with a dropout
id203 of 0.5 as in zaremba et al. (2014). the
common parent model is trained with a dropout
id203 of 0.2. the learning rate used for
both child and parents is 0.5 with a decay rate of
0.9 when the development perplexity does not im-
prove. the child models are all trained for 100
epochs. we re-scale the gradient when the gradi-
ent norm is greater than 5. the initial parameter
range is [-0.08, +0.08].

4.1 transfer results
the results for our id21 method ap-
plied to the four languages above are in table 2.
the parent models were trained on the wmt 2015
(bojar et al., 2015) french-english corpus for 5
epochs. our baseline id4 systems (   id4    col-

setting
hausa
turkish
uzbek
urdu

sbmt id4 xfer
24.8
21.8
19.5
19.1

23.7
20.4
17.9
17.9

24.5
21.4
19.5
18.2

lm
23.6
21.1
17.9
18.2

table 3: our transfer method applied to re-scoring
output n-best lists from the sbmt system. addi-
tionally, the    lm    column shows the results when
an id56 lm was trained on the large english cor-
pus and used to re-score the n-best list.

umn) all receive a large id7 improvement when
using the transfer method (the    xfer    column) with
an average id7 improvement of 5.6. addition-
ally, when we use unknown word replacement
from luong et al. (2015b) and ensemble together
8 models (the    final    column) we further improve
upon our id7 scores, bringing the average id7
improvement to 7.5. overall our method allows
the id4 system to reach competitive scores and
beat the sbmt system in one of the four language
pairs.

4.2 re-scoring results
we also use the id4 model with id21
to re-score output n-best lists (n = 1000) from
the sbmt system. table 3 shows the results of
re-scoring. transfer id4 models give the high-
est gains over using re-scoring with a neural lan-
guage model or an id4 system that does not use
transfer. the neural language model is an lstm
id56 with 2 layers and 1000 hidden states. it has
a target vocabulary of 100k and is trained using
noise-contrastive estimation (mnih and teh, 2012;
vaswani et al., 2013; baltescu and blunsom, 2014;
williams et al., 2015). additionally, it is trained
using dropout with a dropout id203 of 0.2 as
in (zaremba et al., 2014). from re-scoring with
the transfer model, we get an improvement of 1.1   
1.6 id7 points above the strong sbmt system.
we ran a number of additional experiments to
understand what components of our    nal transfer
model signi   cantly contribute to the overall result.
section 5 details these experiments.

5 analysis

we analyze the effects of using different parent
models, regularizing different parts of the child
model and trying different id173 tech-
niques.

language pair

spanish-english
french-english
german-english

role train dev test
size size size
59k
2.5m 58k
59k
53m 58k
53m 58k
59k

child
parent
parent

table 4: data used for a low-resource spanish-
english task. sizes are numbers of word tokens
on the english side of the bitext.

parent
none
french-english
german-english

id7 ppl
15.9
5.8
6.2

16.4
31.0
29.8

table 5: for a low-resource spanish-english
task, we experiment with several choices of par-
ent model: none, french-english, and german-
english. we hypothesize that french-english is
best because french and spanish are similar.

5.1 different parent languages
in the above experiments we use french-english
as the parent language pair. here, we experiment
with different parent languages. in this set of ex-
periments we use spanish-english as the child lan-
guage pair, rather then hausa, turkish, uzbek, or
urdu. a description of the data used in this section
is presented in table 4.

our experimental results are shown in table 5,
where we use french and german as parent lan-
guages.
if we just train a model with no trans-
fer on a small spanish-english training set we get
a id7 score of 16.4. when using our transfer
method using french and german as parent lan-
guages, we get id7 scores of 31.0 and 29.8 re-
spectively. as expected, french is a better parent
than german for spanish, which could be the re-
sult of the parent language being more similar to
the child language.

overall, we can see that the choice of parent lan-
guage can make a difference in the id7 score,
so in our hausa, turkish, uzbek, and urdu ex-
periments, a parent language more optimal than
french might improve results.

5.2 effects of having similar parent

language

next, we look at a best-case scenario in which
the parent language is as similar as possible to
the child language. here we devise a synthetic

id7 ppl

model

uzbek-english
uzbek-english transfer
french   -english
french   -english transfer

train
size
1.8m
10.7
1.8m 15.0 (+4.3)
1.8m
13.3
1.8m 20.0 (+6.7)

22.4
13.9
28.2
10.9

table 6: a better match between parent and
child languages should improve transfer results.
we devised a child language called french   ,
identical
to french except for word spellings.
we observe that french id21 helps
french    (13.3   20.0) more than it helps uzbek
(10.7   15.0).

child language (called french   ) which is exactly
like french, except its vocabulary is shuf   ed ran-
domly.
(e.g.,    internationale    is now    pomme   ,
etc). this language, which looks unintelligible to
human eyes, nevertheless has the same distribu-
tional and relational properties as actual french,
i.e.
the word that, prior to vocabulary reassign-
ment, was    roi    (king) is likely to share distribu-
tional characteristics, and hence embedding simi-
larity, to the word that, prior to reassignment, was
   reine    (queen). such a language should be the
ideal parent model.

the results of this experiment are shown in ta-
ble 6. we get a 4.3 id7 improvement with an
unrelated parent (i.e. french-parent and uzbek-
child), but we get a 6.7 id7 improvement with
a    closely related    parent (i.e. french-parent and
french   -child). we conclude that the choice of
parent model can have a strong impact on transfer
models, and choosing better parents for our low-
resource languages (if data for such parents can be
obtained) could improve the    nal results.

5.3 ablation analysis
in all the above experiments, only the target in-
put and output embeddings are    xed during train-
ing.
in this section we analyze what happens
when different parts of the model are    xed, in or-
der to see what yields optimal performance. fig-
ure 2 shows a diagram of the components of a
sequence-to-sequence model. table 7 shows how
we begin to allow more components of the child
id4 model to be trained and see the effect on
performance in the model. we see that the opti-
mal setting for transferring from french-english
to uzbek-english in terms of id7 performance

figure 2: our id4 model architecture, show-
ing six blocks of parameters,
in addition to
source/target words and predictions. during trans-
fer learning, we expect the source-language re-
lated blocks to change more than the target-
language related blocks.

setting

no retraining
retrain source embeddings
+ source id56
+ target id56
+ target attention
+ target input embeddings
+ target output embeddings

dev
id7
0.0
7.7
11.8
14.2
15.0
14.7
13.7

dev
ppl
112.6
24.7
17.0
14.5
13.9
13.8
14.4

table 7: starting with the parent french-english
model (id7 =24.4, ppl=6.2), we randomly as-
sign uzbek word types to french word embed-
dings, freeze various parameters of the neural
network model, and allow uzbek-english (child
model) training to modify other parts. the table
shows how uzbek-english id7 and perplexity
vary as we allow more parameters to be re-trained.

attentiontargetid56sourceid56sourceembeddingsourcewordtarget inputembeddingsourceid56sourceembeddingsourcewordtarget inputembeddingtargetid56targetpredtarget outputembeddingtarget outputembeddingtargetpredtargetwordtargetwordfigure 3: uzbek-english learning curves for the
id4 attention model with and without transfer
learning. the training perplexity converges to a
similar value in both cases. however, the devel-
opment perplexity for the transfer model is signif-
icantly better.

is to allow all of the components of the child model
to be trained except for the input and output target
embeddings.

even though we use this setting for our main
experiments, the optimum setting is likely to be
language- and corpus-dependent. for turkish, ex-
periments show that freezing target attention pa-
rameters as well gives slightly better results. for
parent-child models with closely related languages
we expect freezing, or strongly regularizing, more
components of the model to give better results.

5.4 learning curve
in figure 3 we plot learning curves for both a
transfer and a non-transfer model on training and
development sets. we see that the    nal training set
perplexities for both the transfer and non-transfer
model are very similar, but the development set
perplexity for the transfer model is much better.

the fact that the two models start from and con-
verge to very different points, yet have similar
training set performances, indicates that our archi-
tecture and training algorithm are able to reach a
good minimum of the training objective regardless
of the initialization. however, the training objec-
tive seems to have a large basin of models with
similar performance and not all of them generalize
well to the development set. the transfer model,
starting with and staying close to a point known
to perform well on a related task, is guided to a
   nal point in the weight space that generalizes to
the development set much better.

figure 4: uzbek-english learning curves for the
transfer model with and without dictionary-based
assignment of uzbek word types to french word
embeddings (from the parent model). dictionary-
based assignment enables faster improvement in
early epochs, reaching 25 dev perplexity at epoch
2 vs 4. however the    nal perplexities are simi-
lar, showing that the model is able to untangle the
initial random uzbek/french word-type mapping
without help.

5.5 dictionary initialization
using the transfer method, we always initialize in-
put language embeddings for the child model with
randomly-assigned embeddings from the parent
(which has a different input language). a smarter
method might be to initialize child embeddings
with similar parent embeddings, where similarity
is measured by word-to-word t-table probabilities.
to get these probabilities we compose uzbek-
english and english-french t-tables obtained from
the berkeley aligner (liang et al., 2006). we see
from figure 4 that this dictionary-based assign-
ment results in faster improvement in the early part
of the training. however the    nal performance is
similar to our standard model, indicating that the
training is able to untangle the dictionary permu-
tation introduced by randomly-assigned embed-
dings.

5.6 different parent models
in the above experiments, we use a parent model
trained on a large french/english bilingual cor-
pus. one might hypothesize that our gains come
from exploiting the english half of the corpus as
an additional language model resource. therefore,
we explore id21 for the child model
with parent models that only use the english side
of the bilingual corpus. table 8 shows the re-

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             setting
id4
french-english transfer
english-english transfer
engperm-english transfer
lm transfer

id7 ppl
22.4
14.3
55.8
20.4
16.3

10.7
14.4
5.3
10.8
12.9

table 8: transfer with parent models trained only
on english data. the child data is the uzbek-
english corpus from table 3. the english-english
parent learns to copy english sentences, and the
engperm-english learns to un-permute scrambled
english sentences. the lm is a 2-layer lstm
id56 language model trained on the english cor-
pus.

sults for these experiments where we train one
parent model to copy english sentences (english-
english) and another parent model to un-permute
scrambled english sentences (engperm-english).
additionally, we train a parent model that is just
an id56 language model. these results show that
our id21 is not simply importing an
english language model, but making use of trans-
lation parameters learned from the parent   s large
bilingual text.

6 conclusion

overall our transfer method improves id4 scores
on low-resource languages by a large margin and
allows our transfer id4 system to come close
to the performance of a very strong sbmt sys-
tem, even exceeding its performance on hausa-
english. in addition, we consistently and signif-
icantly improve state-of-the-art sbmt systems on
low-resource languages when the transfer id4
system is used for re-scoring. our experiments
suggest that there is still room for improvement
in selecting parent languages that are more similar
to child languages, provided data for such parents
can be found.

7 acknowledgments

this work was carried out with funding from
darpa (hr0011-15-c-0115) and arl/aro
(w911nf-10-1-0533).

jointly learning to align and translate.
iclr.

in proc.

[baltescu and blunsom2014] p. baltescu and p. blun-
language mod-
arxiv preprint

som.
elling in machine translation.
arxiv:1412.7119.

pragmatic neural

2014.

[bengio2012] yoshua bengio. 2012. deep learning of
representations for unsupervised and transfer learn-
ing. jmlr, 27.

[bojar et al.2015] o. bojar, r. chatterjee, c. feder-
mann, b. haddow, m. huck, c. hokamp, p. koehn,
v. logacheva, c. monz, m. negri, m. post, c. scar-
ton, l. specia, and m. turchi. 2015. findings of the
2015 workshop on id151. in
proc. wmt.

[casta  no and casacuberta1997] m. a. casta  no and
f. casacuberta. 1997. a connectionist approach to
machine translation. in proc. eurospeech.

[chiang et al.2009] d. chiang, k. knight,

and
w. wang. 2009. 11,001 new features for statistical
machine translation. in proc. naacl.

[cires  an et al.2012] d. c. cires  an, u. meier, and
j. schmidhuber. 2012. id21 for latin
and chinese characters with deep neural networks.
in proc. ijid98.

[galley et al.2004] m. galley, m. hopkins, k. knight,
and d. marcu. 2004. what   s in a translation rule?
in proc. hlt-naacl.

[galley et al.2006] m. galley, j. graehl, k. knight,
d. marcu, s. deneefe, w. wang, and i. thayer.
2006. scalable id136 and training of context-
in proc. acl-
rich syntactic translation models.
coling.

[hochreiter and schmidhuber1997] s. hochreiter and
j. schmidhuber. 1997. long short-term memory.
neural computation, 9(8).

[liang et al.2006] p. liang, b. taskar, and d. klein.

2006. alignment by agreement. in proc. naacl.

[luong et al.2015a] m. luong, h. pham, and c. man-
ning.
2015a. effective approaches to attention-
based id4. in proc. emnlp.

[luong et al.2015b] t. luong,

i. sutskever, q. le,
o. vinyals, and w. zaremba. 2015b. addressing
the rare word problem in id4.
in proc. acl.

[mnih and teh2012] a. mnih and y. w. teh. 2012.
a fast and simple algorithm for training neu-
ral probabilistic language models. arxiv preprint
arxiv:1206.6426.

references
[bahdanau et al.2014] d. bahdanau, k. cho,

and
y. bengio. 2014. id4 by

[neco and forcada1997] r. neco and m. forcada.
1997. asynchronous translations with recurrent
in proc. international conference on
neural nets.
neural networks.

[pan and yang2010] s. j. pan and q. yang. 2010. a
ieee transactions
survey on id21.
on knowledge and data engineering, 22(10):1345   
1359.

[sutskever et al.2014] i. sutskever, o. vinyals, and
q. v. le. 2014. sequence to sequence learning with
neural networks. in proc. nips.

[torrey and shavlik2009] l. torrey and j. shavlik.
in e. soria, j. martin,
2009. id21.
r. magdalena, m. martinez, and a. serrano, edi-
tors, handbook of research on machine learning
applications and trends: algorithms, methods, and
techniques. igi global.

[vaswani et al.2013] a. vaswani, y. zhao, v. fossum,
and d. chiang. 2013. decoding with large-scale
neural language models improves translation.
in
proc. emnlp.

[wang and zheng2015] d. wang and t. fang zheng.
2015. id21 for speech and language
processing. arxiv preprint arxiv:1511.06066.

[werbos1990] p. j. werbos. 1990. id26
through time: what it does and how to do it. proc.
ieee, 78(10):1550   1560.

[williams et al.2015] w. williams, n. prasad, d. mrva,
t. ash, and t. robinson.
scaling re-
current neural network language models. corr,
abs/1502.00512.

2015.

[zaremba et al.2014] w. zaremba, i. sutskever, and
o. vinyals. 2014. recurrent neural network reg-
ularization. corr, abs/1409.2329.

