7
1
0
2

 

b
e
f
9

 

 
 
]
l
c
.
s
c
[
 
 

3
v
7
0
2
4
0

.

8
0
6
1
:
v
i
x
r
a

published as a conference paper at iclr 2017

fine-grained analysis of sentence
embeddings using auxiliary prediction tasks

yossi adi1,2, einat kermany2, yonatan belinkov3, ofer lavi2, yoav goldberg1

1bar-ilan university, ramat-gan, israel
{yoav.goldberg, yossiadidrum}@gmail.com
2ibm haifa research lab, haifa, israel
{einatke, oferl}@il.ibm.com
3mit computer science and arti   cial intelligence laboratory, cambridge, ma, usa
belinkov@mit.edu

abstract

there is a lot of research interest in encoding variable length sentences into    xed
length vectors, in a way that preserves the sentence meanings. two common
methods include representations based on averaging word vectors, and represen-
tations based on the hidden states of recurrent neural networks such as lstms.
the sentence vectors are used as features for subsequent machine learning tasks
or for pre-training in the context of deep learning. however, not much is known
about the properties that are encoded in these sentence representations and about
the language information they capture.
we propose a framework that facilitates better understanding of the encoded rep-
resentations. we de   ne prediction tasks around isolated aspects of sentence struc-
ture (namely sentence length, word content, and word order), and score repre-
sentations by the ability to train a classi   er to solve each prediction task when
using the representation as input. we demonstrate the potential contribution of the
approach by analyzing different sentence representation mechanisms. the analy-
sis sheds light on the relative strengths of different sentence embedding methods
with respect to these low level prediction tasks, and on the effect of the encoded
vector   s dimensionality on the resulting representations.

1

introduction

while sentence embeddings or sentence representations play a central role in recent deep learning
approaches to nlp, little is known about the information that is captured by different sentence em-
bedding learning mechanisms. we propose a methodology facilitating    ne-grained measurement
of some of the information encoded in sentence embeddings, as well as performing    ne-grained
comparison of different sentence embedding methods.
in sentence embeddings, sentences, which are variable-length sequences of discrete symbols, are
encoded into    xed length continuous vectors that are then used for further prediction tasks. a
simple and common approach is producing word-level vectors using, e.g., id97 (mikolov et al.,
2013a;b), and summing or averaging the vectors of the words participating in the sentence. this
continuous-bag-of-words (cbow) approach disregards the word order in the sentence.1
another approach is the encoder-decoder architecture, producing models also known as sequence-
to-sequence models (sutskever et al., 2014; cho et al., 2014; bahdanau et al., 2014, inter alia). in
this architecture, an encoder network (e.g. an lstm) is used to produce a vector representation
of the sentence, which is then fed as input into a decoder network that uses it to perform some
prediction task (e.g. recreate the sentence, or produce a translation of it). the encoder and decoder
networks are trained jointly in order to perform the    nal task.

1we use the term cbow to refer to a sentence representation that is composed of an average of the vectors
of the words in the sentence, not to be confused with the training method by the same name which is used in
the id97 algorithm.

1

published as a conference paper at iclr 2017

some systems (for example in machine translation) train the system end-to-end, and use the trained
system for prediction (bahdanau et al., 2014). such systems do not generally care about the encoded
vectors, which are used merely as intermediate values. however, another common case is to train an
encoder-decoder network and then throw away the decoder and use the trained encoder as a general
mechanism for obtaining sentence representations. for example, an encoder-decoder network can
be trained as an auto-encoder, where the encoder creates a vector representation, and the decoder
attempts to recreate the original sentence (li et al., 2015). similarly, kiros et al. (2015) train a net-
work to encode a sentence such that the decoder can recreate its neighboring sentences in the text.
such networks do not require specially labeled data, and can be trained on large amounts of unanno-
tated text. as the decoder needs information about the sentence in order to perform well, it is clear
that the encoded vectors capture a non-trivial amount of information about the sentence, making
the encoder appealing to use as a general purpose, stand-alone sentence encoding mechanism. the
sentence encodings can then be used as input for other prediction tasks for which less training data
is available (dai & le, 2015). in this work we focus on these    general purpose    sentence encodings.
the resulting sentence representations are opaque, and there is currently no good way of comparing
different representations short of using them as input for different high-level semantic tasks (e.g.
sentiment classi   cation, entailment recognition, document retrieval, id53, sentence
similarity, etc.) and measuring how well they perform on these tasks. this is the approach taken
by li et al. (2015), hill et al. (2016) and kiros et al. (2015). this method of comparing sentence
embeddings leaves a lot to be desired: the comparison is at a very coarse-grained level, does not tell
us much about the kind of information that is encoded in the representation, and does not help us
form generalizable conclusions.

our contribution we take a    rst step towards opening the black box of vector embeddings for
sentences. we propose a methodology that facilitates comparing sentence embeddings on a much
   ner-grained level, and demonstrate its use by analyzing and comparing different sentence repre-
sentations. we analyze sentence representation methods that are based on lstm auto-encoders and
the simple cbow representation produced by averaging id97 id27s. for each of
cbow and lstm auto-encoder, we compare different numbers of dimensions, exploring the ef-
fect of the dimensionality on the resulting representation. we also provide some comparison to the
skip-thought embeddings of kiros et al. (2015).
in this work, we focus on what are arguably the three most basic characteristics of a sequence:
its length, the items within it, and their order. we investigate different sentence representations
based on the capacity to which they encode these aspects. our analysis of these low-level properties
leads to interesting, actionable insights, exposing relative strengths and weaknesses of the different
representations.

limitations focusing on low-level sentence properties also has limitations: the tasks focus on
measuring the preservation of surface aspects of the sentence and do not measure syntactic and
semantic generalization abilities; the tasks are not directly related to any speci   c downstream appli-
cation (although the properties we test are important factors in many tasks     knowing that a model
is good at predicting length and word order is likely advantageous for syntactic parsing, while mod-
els that excel at word content are good for text classi   cation tasks). dealing with these limitations
requires a complementary set of auxiliary tasks, which is outside the scope of this study and is left
for future work.
the study also suffers from the general limitations of empirical work: we do not prove general
theorems but rather measure behaviors on several data points and attempt to draw conclusions from
these measurements. there is always the risk that our conclusions only hold for the datasets on
which we measured, and will not generalize. however, we do consider our large sample of sentences
from wikipedia to be representative of the english language, at least in terms of the three basic
sentence properties that we study.

summary of findings our analysis reveals the following insights regarding the different sentence
embedding methods:
    sentence representations based on averaged word vectors are surprisingly effective, and encode
a non-trivial amount of information regarding sentence length. the information they contain

2

published as a conference paper at iclr 2017

can also be used to reconstruct a non-trivial amount of the original word order in a probabilistic
manner (due to regularities in the natural language data).

    lstm auto-encoders are very effective at encoding word order and word content.
    increasing the number of dimensions bene   ts some tasks more than others.
    adding more hidden units sometimes degrades the encoders    ability to encode word content. this
degradation is not correlated with the id7 scores of the decoder, suggesting that id7 over
the decoder output is sub-optimal for evaluating the encoders    quality.
    lstm encoders trained as auto-encoders do not rely on ordering patterns in the training sentences

when encoding novel sentences, while the skip-thought encoders do rely on such patterns.

2 related work

word-level distributed representations have been analyzed rather extensively, both empirically and
theoretically, for example by baroni et al. (2014), levy & goldberg (2014) and levy et al. (2015).
in contrast, the analysis of sentence-level representations has been much more limited. commonly
used approaches is to either compare the performance of the sentence embeddings on down-stream
tasks (hill et al., 2016), or to analyze models, speci   cally trained for prede   ned task (schmaltz
et al., 2016; sutskever et al., 2011).
while the resulting analysis reveals differences in performance of different models, it does not ade-
quately explain what kind of linguistic properties of the sentence they capture. other studies analyze
the hidden units learned by neural networks when training a sentence representation model (elman,
1991; karpathy et al., 2015; k  ad  ar et al., 2016). this approach often associates certain linguistic
aspects with certain hidden units. k  ad  ar et al. (2016) propose a methodology for quantifying the
contribution of each input word to a resulting gru-based encoding. these methods depend on the
speci   c learning model and cannot be applied to arbitrary representations. moreover, it is still not
clear what is captured by the    nal sentence embeddings.
our work is orthogonal and complementary to the previous efforts: we analyze the resulting sentence
embeddings by devising auxiliary prediction tasks for core sentence properties. the methodology
we purpose is general and can be applied to any sentence representation model.

3 approach

we aim to inspect and compare encoded sentence vectors in a task-independent manner. the main
idea of our method is to focus on isolated aspects of sentence structure, and design experiments to
measure to what extent each aspect is captured in a given representation.
in each experiment, we formulate a prediction task. given a sentence representation method, we
create training data and train a classi   er to predict a speci   c sentence property (e.g. their length)
based on their vector representations. we then measure how well we can train a model to perform the
task. the basic premise is that if we cannot train a classi   er to predict some property of a sentence
based on its vector representation, then this property is not encoded in the representation (or rather,
not encoded in a useful way, considering how the representation is likely to be used).
the experiments in this work focus on low-level properties of sentences     the sentence length, the
identities of words in a sentence, and the order of the words. we consider these to be the core
elements of sentence structure. generalizing the approach to higher-level semantic and syntactic
properties holds great potential, which we hope will be explored in future work, by us or by others.

3.1 the prediction tasks

we now turn to describe the speci   c prediction tasks. we use lower case italics (s, w) to refer
to sentences and words, and boldface to refer to their corresponding vector representations (s, w).
when more than one element is considered, they are distinguished by indices (w1, w2, w1, w2).
our underlying corpus for generating the classi   cation instances consists of 200,000 wikipedia
sentences, where 150,000 sentences are used to generate training examples, and 25,000 sentences

3

published as a conference paper at iclr 2017

are used for each of the test and development examples. these sentences are a subset of the training
set that was used to train the original sentence encoders. the idea behind this setup is to test the
models on what are presumably their best embeddings.
length task this task measures to what extent the sentence representation encodes its length.
given a sentence representation s     rk, the goal of the classi   er is to predict the length (number
of words) in the original sentence s. the task is formulated as multiclass classi   cation, with eight
output classes corresponding to binned lengths.2 the resulting dataset is reasonably balanced, with
a majority class (lengths 5-8 words) of 5,182 test instances and a minority class (34-70) of 1,084 test
instances. predicting the majority class results in classi   cation accuracy of 20.1%.
word-content task this task measures to what extent the sentence representation encodes the
identities of words within it. given a sentence representation s     rk and a word representation
w     rd, the goal of the classi   er is to determine whether w appears in the s, with access to neither
w nor s. this is formulated as a binary classi   cation task, where the input is the concatenation of s
and w.
to create a dataset for this task, we need to provide positive and negative examples. obtaining
positive examples is straightforward: we simply pick a random word from each sentence. for
negative examples, we could pick a random word from the entire corpus. however, we found that
such a dataset tends to push models to memorize words as either positive or negative words, instead
of    nding their relation to the sentence representation. therefore, for each sentence we pick as a
negative example a word that appears as a positive example somewhere in our dataset, but does
not appear in the given sentence. this forces the models to learn a relationship between word and
sentence representations. we generate one positive and one negative example from each sentence.
the dataset is balanced, with a baseline accuracy of 50%.
word-order task this task measures to what extent the sentence representation encodes word
order. given a sentence representation s     rk and the representations of two words that appear in
the sentence, w1, w2     rd, the goal of the classi   er is to predict whether w1 appears before or after
w2 in the original sentence s. again, the model has no access to the original sentence and the two
words. this is formulated as a binary classi   cation task, where the input is a concatenation of the
three vectors s, w1 and w2.
for each sentence in the corpus, we simply pick two random words from the sentence as a positive
example. for negative examples, we    ip the order of the words. we generate one positive and one
negative example from each sentence. the dataset is balanced, with a baseline accuracy of 50%.

4 sentence representation models
given a sentence s = {w1, w2, ..., wn} we aim to    nd a sentence representation s using an encoder:

enc : s = {w1, w2, ..., wn} (cid:55)    s     rk

the encoding process usually assumes a vector representation wi     rd for each word in the vo-
cabulary. in general, the word and sentence embedding dimensions, d and k, need not be the same.
the word vectors can be learned together with other encoder parameters or pre-trained. below we
describe different instantiations of enc.

continuous bag-of-words (cbow) this simple yet effective text representation consists of per-
forming element-wise averaging of word vectors that are obtained using a word-embedding method
such as id97.
despite its obliviousness to word order, cbow has proven useful in different tasks (hill et al., 2016)
and is easy to compute, making it an important model class to consider.

encoder-decoder (ed) the encoder-decoder framework has been successfully used in a number
of sequence-to-sequence learning tasks (sutskever et al., 2014; bahdanau et al., 2014; dai & le,
2015; li et al., 2015). after the encoding phase, a decoder maps the sentence representation back to
the sequence of words:

dec : s     rk (cid:55)    s = {w1, w2, ..., wn}

2we use the bins (5-8), (9-12), (13-16), (17-20), (21-25), (26-29), (30-33), (34-70).

4

published as a conference paper at iclr 2017

(a) length test.

(b) content test.

(c) order test.

figure 1: task accuracy vs. embedding size for different models; ed id7 scores given for reference.

here we investigate the speci   c case of an auto-encoder, where the entire encoding-decoding process
can be trained end-to-end from a corpus of raw texts. the sentence representation is the    nal output
vector of the encoder. we use a long short-term memory (lstm) recurrent neural network (hochre-
iter & schmidhuber, 1997; graves et al., 2013) for both encoder and decoder. the lstm decoder
is similar to the lstm encoder but with different weights.

5 experimental setup

the bag-of-words (cbow) and encoder-decoder models are trained on 1 million sentences from a
2012 wikipedia dump with vocabulary size of 50,000 tokens. we use nltk (bird, 2006) for tok-
enization, and constrain sentence lengths to be between 5 and 70 words. for both models we control
the embedding size k and train word and sentence vectors of sizes k     {100, 300, 500, 750, 1000}.
more details about the experimental setup are available in the appendix.

6 results

in this section we provide a detailed description of our experimental results along with their analysis.
for each of the three main tests     length, content and order     we investigate the performance of
different sentence representation models across embedding size.

6.1 length experiments

we begin by investigating how well the different representations encode sentence length. figure 1a
shows the performance of the different models on the length task, as well as the id7 obtained by
the lstm encoder-decoder (ed).
with enough dimensions, the lstm embeddings are very good at capturing sentence length, ob-
taining accuracies between 82% and 87%. length prediction ability is not perfectly correlated with
id7 scores: from 300 dimensions onward the length prediction accuracies of the lstm remain
relatively stable, while the id7 score of the encoder-decoder model increases as more dimensions
are added.
somewhat surprisingly, the cbow model also encodes a fair amount of length information, with
length prediction accuracies of 45% to 65%, way above the 20% baseline. this is remarkable, as the
cbow representation consists of averaged word vectors, and we did not expect it to encode length
at all. we return to cbow   s exceptional performance in section 7.

6.2 word content experiments

to what extent do the different sentence representations encode the identities of the words in the
sentence? figure 1b visualizes the performance of our models on the word content test.
all the representations encode some amount of word information, and clearly outperform the ran-
dom baseline of 50%. some trends are worth noting. while the capacity of the lstm encoder
to preserve word identities generally increases when adding dimensions, the performance peaks at
750 dimensions and drops afterwards. this stands in contrast to the id7 score of the respective

5

1003005007501000representation dimensions102030405060708090length prediction accuracy05101520253035id7edcbowed id71003005007501000representation dimensions505560657075808590content prediction accuracy05101520253035id7edcbowed id71003005007501000representation dimensions5060708090order prediction accuracy05101520253035id7edcbowed id7published as a conference paper at iclr 2017

encoder-decoder models. we hypothesize that this occurs because a sizable part of the auto-encoder
performance comes from the decoder, which also improves as we add more dimensions. at 1000 di-
mensions, the decoder   s language model may be strong enough to allow the representation produced
by the encoder to be less informative with regard to word content.
cbow representations with low dimensional vectors (100 and 300 dimensions) perform exception-
ally well, outperforming the more complex, sequence-aware models by a wide margin. if your task
requires access to word identities, it is worth considering this simple representation. interestingly,
cbow scores drop at higher dimensions.

6.3 word order experiments

figure 1c shows the performance of the different models on the order test. the lstm encoders are
very capable of encoding word order, with lstm-1000 allowing the recovery of word order in 91%
of the cases. similar to the length test, lstm order prediction accuracy is only loosely correlated
with id7 scores. it is worth noting that increasing the representation size helps the lstm-encoder
to better encode order information.
surprisingly, the cbow encodings manage to reach an accuracy of 70% on the word order task,
20% above the baseline. this is remarkable as, by de   nition, the cbow encoder does not attempt
to preserve word order information. one way to explain this is by considering distribution patterns
of words in natural language sentences: some words tend to appear before others. in the next section
we analyze the effect of natural language on the different models.

7

importance of    natural languageness   

natural language imposes many constraints on sentence structure. to what extent do the differ-
ent encoders rely on speci   c properties of word distributions in natural language sentences when
encoding sentences?
to account for this, we perform additional experiments in which we attempt to control for the effect
of natural language.
how can cbow encode sentence length? is the ability of cbow embeddings to encode length
related to speci   c words being indicative of longer or shorter sentences? to control for this, we
created a synthetic dataset where each word in each sentence is replaced by a random word from
the dictionary and re-ran the length test for the cbow embeddings using this dataset. as figure 2a
shows, this only leads to a slight decrease in accuracy, indicating that the identity of the words is not
the main component in cbow   s success at predicting length.

(a) length accuracy for different
cbow sizes on natural and synthetic
(random words) sentences.

(b) average embedding norm vs. sen-
tence length for cbow with an em-
bedding size of 300.

an alternative explanation for cbow   s ability to encode sentence length is given by considering the
norms of the sentence embeddings. indeed, figure 2b shows that the embedding norm decreases as
sentences grow longer. we believe this is one of the main reasons for the strong cbow results.
while the correlation between the number of averaged vectors and the resulting norm surprised us,
in retrospect it is an expected behavior that has sound mathematical foundations. to understand
the behavior, consider the different word vectors to be random variables, with the values in each

6

1003005007501000representation dimensions35404550556065length prediction accuracycbowcbow syn sent5101520253035sentence length0.350.400.450.500.55normpublished as a conference paper at iclr 2017

dimension centered roughly around zero. both central limit theorem and hoeffding   s inequality tell
us that as we add more samples, the expected average of the values will better approximate the true
mean, causing the norm of the average vector to decrease. we expect the correlation between the
sentence length and its norm to be more pronounced with shorter sentences (above some number of
samples we will already be very close to the true mean, and the norm will not decrease further), a
behavior which we indeed observe in practice.
how does cbow encode word order? the surprisingly strong performance of the cbow model
on the order task made us hypothesize that much of the word order information is captured in general
natural language word order statistics.
to investigate this, we re-run the word order tests, but this time drop the sentence embedding in
training and testing time, learning from the word-pairs alone. in other words, we feed the network as
input two id27s and ask which word comes    rst in the sentence. this test isolates general
word order statistics of language from information that is contained in the sentence embedding (fig.
2).
the difference between including and remov-
ing the sentence embeddings when using the
cbow model is minor, while the lstm-ed
suffers a signi   cant drop. clearly, the lstm-
ed model encodes word order, while the pre-
diction ability of cbow is mostly explained by
general language statistics. however, cbow
does bene   t from the sentence to some extent:
we observe a gain of    3% accuracy points
when the cbow tests are allowed access to the
sentence representation. this may be explained
by higher order statistics of correlation between
word order patterns and the occurrences of spe-
ci   c words.
how important is english word order for en-
coding sentences? to what extent are the models trained to rely on natural language word order
when encoding sentences? to control for this, we create a synthetic dataset, permuted, in which
the word order in each sentence is randomly permuted. then, we repeat the length, content and
order experiments using the permuted dataset (we still use the original sentence encoders that are
trained on non-permuted sentences). while the permuted sentence representation is the same for
cbow, it is completely different when generated by the encoder-decoder.
results are presented in fig. 3. when considering cbow embeddings, word order accuracy drops
to chance level, as expected, while results on the other tests remain the same. moving to the lstm
encoder-decoder, the results on all three tests are comparable to the ones using non-permuted sen-
tences. these results are somewhat surprising since the models were originally trained on    real   ,
non-permuted sentences. this indicates that the lstm encoder-decoder is a general-purpose se-
quence encoder that for the most part does not rely on word ordering properties of natural language
when encoding sentences. the small and consistent drop in word order accuracy on the permuted
sentences can be attributed to the encoder relying on natural language word order to some extent,
but can also be explained by the word order prediction task becoming harder due to the inability to

figure 2: order accuracy w/ and w/o sentence repre-
sentation for ed and cbow models.

(a) length test.

(b) content test.

(c) order test.

figure 3: results for length, content and order tests on natural and permuted sentences.

7

1003005007501000representation dimensions657075808590order prediction accuracyeded no sentcbowcbow no sent1003005007501000representation dimensions405060708090100length prediction accuracycbowperm cbowencoder-decoderperm ed1003005007501000representation dimensions5560657075808590content prediction accuracy1003005007501000representation dimensions5060708090order prediction accuracypublished as a conference paper at iclr 2017

use general word order statistics. the results suggest that a trained encoder will transfer well across
different natural language domains, as long as the vocabularies remain stable. when considering
the decoder   s id7 score on the permuted dataset (not shown), we do see a dramatic decrease
in accuracy. for example, lstm encoder-decoder with 1000 dimensions drops from 32.5 to 8.2
id7 score. these results suggest that the decoder, which is thrown away, contains most of the
language-speci   c information.

8 skip-thought vectors

in addition to the experiments on cbow and lstm-encoders, we also experiment with the skip-
thought vectors model (kiros et al., 2015). this model extends the idea of the auto-encoder to
neighboring sentences.
given a sentence si, it    rst encodes it using an id56, similar to the auto-encoder model. however,
instead of predicting the original sentence, skip-thought predicts the preceding and following sen-
tences, si   1 and si+1. the encoder and decoder are implemented with id149 (cho
et al., 2014).
here, we deviate from the controlled environment and use the author   s provided model3 with the
recommended embeddings size of 4800. this makes the direct comparison of the models    unfair   .
however, our aim is not to decide which is the    best    model but rather to show how our method can
be used to measure the kinds of information captured by different representations.
table 1 summarizes the performance of the skip-thought embeddings in each of the prediction tasks
on both the permuted and original dataset.

original
permuted

length word content word order
82.1%
68.2%

79.7%
76.4%

81.1%
76.5%

table 1: classi   cation accuracy for the prediction tasks using skip-thought embeddings.

the performance of the skip-thought embeddings is well above the baselines and roughly similar
for all tasks. its performance is similar to the higher-dimensional encoder-decoder models, except
in the order task where it lags somewhat behind. however, we note that the results are not directly
comparable as skip-thought was trained on a different corpus.
the more interesting    nding is its performance on the permuted sentences. in this setting we see
a large drop. in contrast to the lstm encoder-decoder, skip-thought   s ability to predict length and
word content does degrade signi   cantly on the permuted sentences, suggesting that the encoding
process of the skip-thought model is indeed specialized towards natural language texts.

9 conclusion
we presented a methodology for performing    ne-grained analysis of sentence embeddings using
auxiliary prediction tasks. our analysis reveals some properties of sentence embedding methods:
    cbow is surprisingly effective     in addition to being very strong at content, it is also predictive
of length, and can be used to reconstruct a non-trivial amount of the original word order. 300
dimensions perform best, with greatly degraded word-content prediction performance on higher
dimensions.
    with enough dimensions, lstm auto-encoders are very effective at encoding word order and
word content information. increasing the dimensionality of the lstm encoder does not signif-
icantly improve its ability to encode length, but does increase its ability to encode content and
order information. 500 dimensional embeddings are already quite effective for encoding word
order, with little gains beyond that. word content accuracy peaks at 750 dimensions and drops at
1000, suggesting that larger is not always better.

3https://github.com/ryankiros/skip-thoughts

8

published as a conference paper at iclr 2017

    the trained lstm encoder (when trained with an auto-encoder objective) does not rely on order-

ing patterns in the training sentences when encoding novel sequences.
in contrast, the skip-thought encoder does rely on such patterns. its performance on the other
tasks is similar to the higher-dimensional lstm encoder, which is impressive considering it was
trained on a different corpus.
    finally, the encoder-decoder   s ability to recreate sentences (id7) is not entirely indicative of
the quality of the encoder at representing aspects such as word identity and order. this suggests
that id7 is sub-optimal for model selection.

references
dzmitry bahdanau, kyunghyun cho, and yoshua bengio. id4 by jointly

learning to align and translate. arxiv preprint arxiv:1409.0473, 2014.

marco baroni, georgiana dinu, and germ  an kruszewski. don   t count, predict! a systematic
comparison of context-counting vs. context-predicting semantic vectors. in proceedings of the
52nd annual meeting of the association for computational linguistics (volume 1: long papers),
pp. 238   247, baltimore, maryland, june 2014. association for computational linguistics. url
http://www.aclweb.org/anthology/p14-1023.

steven bird. nltk: the natural language toolkit. in proceedings of the coling/acl on interactive

presentation sessions, pp. 69   72. association for computational linguistics, 2006.

kyunghyun cho, bart van merri  enboer, caglar gulcehre, dzmitry bahdanau, fethi bougares, hol-
ger schwenk, and yoshua bengio. learning phrase representations using id56 encoder-decoder
for id151. arxiv preprint arxiv:1406.1078, 2014.

ronan collobert, koray kavukcuoglu, and cl  ement farabet. torch7: a matlab-like environment

for machine learning. in biglearn, nips workshop, number epfl-conf-192376, 2011.

andrew m dai and quoc v le. semi-supervised sequence learning. in advances in neural infor-

mation processing systems, pp. 3061   3069, 2015.

john duchi, elad hazan, and yoram singer. adaptive subgradient methods for online learning and

stochastic optimization. the journal of machine learning research, 12:2121   2159, 2011.

jeffrey l elman. distributed representations, simple recurrent networks, and grammatical structure.

machine learning, 7(2-3):195   225, 1991.

xavier glorot, antoine bordes, and yoshua bengio. deep sparse recti   er neural networks.

international conference on arti   cial intelligence and statistics, pp. 315   323, 2011.

in

alex graves, abdel-rahman mohamed, and geoffrey hinton. id103 with deep recur-

rent neural networks. in proceedings of icassp, 2013.

felix hill, kyunghyun cho, and anna korhonen. learning distributed representations of sen-
in proceedings of the 2016 conference of the north american
tences from unlabelled data.
chapter of the association for computational linguistics: human language technologies, pp.
1367   1377, san diego, california, june 2016. association for computational linguistics. url
http://www.aclweb.org/anthology/n16-1162.

geoffrey e. hinton, nitish srivastava, alex krizhevsky, ilya sutskever, and ruslan salakhutdinov.

improving neural networks by preventing co-adaptation of feature detectors. corr, 2012.

sepp hochreiter and j  urgen schmidhuber. long short-term memory. neural computation, 9(8):

1735   1780, 1997.

  akos k  ad  ar, grzegorz chrupa  a, and afra alishahi. representation of linguistic form and function

in recurrent neural networks. arxiv preprint arxiv:1602.08952, 2016.

andrej karpathy, justin johnson, and fei-fei li. visualizing and understanding recurrent networks.

arxiv preprint arxiv:1506.02078, 2015.

9

published as a conference paper at iclr 2017

diederik kingma and jimmy ba. adam: a method for stochastic optimization. arxiv preprint

arxiv:1412.6980, 2014.

ryan kiros, yukun zhu, ruslan r salakhutdinov, richard zemel, raquel urtasun, antonio tor-
in advances in neural information processing

ralba, and sanja fidler. skip-thought vectors.
systems, pp. 3276   3284, 2015.

nicholas l  eonard, sagar waghmare, and yang wang.

preprint arxiv:1511.07889, 2015.

id56: recurrent library for torch. arxiv

omer levy and yoav goldberg. linguistic regularities in sparse and explicit word representations.

in proc. of conll, pp. 171   180, baltimore, maryland, 2014.

omer levy, yoav goldberg, and ido dagan.

improving distributional similarity with lessons
learned from id27s. transactions of the association for computational linguistics, 3:
211   225, 2015. issn 2307-387x. url https://tacl2013.cs.columbia.edu/ojs/
index.php/tacl/article/view/570.

jiwei li, minh-thang luong, and dan jurafsky. a hierarchical neural autoencoder for paragraphs

and documents. arxiv preprint arxiv:1506.01057, 2015.

tomas mikolov, kai chen, greg corrado, and jeffrey dean. ef   cient estimation of word represen-

tations in vector space. arxiv preprint arxiv:1301.3781, 2013a.

tomas mikolov, ilya sutskever, kai chen, greg s corrado, and jeff dean. distributed represen-
tations of words and phrases and their compositionality. in advances in neural information pro-
cessing systems, pp. 3111   3119, 2013b.

vinod nair and geoffrey e hinton. recti   ed linear units improve restricted id82s. in
proceedings of the 27th international conference on machine learning (icml-10), pp. 807   814,
2010.

kishore papineni, salim roukos, todd ward, and wei-jing zhu. id7: a method for automatic
evaluation of machine translation. in proceedings of the 40th annual meeting on association for
computational linguistics, pp. 311   318. association for computational linguistics, 2002.

donald b rubin. matching to remove bias in observational studies. biometrics, pp. 159   183, 1973.

allen schmaltz, alexander m rush, and stuart m shieber. word ordering without syntax. arxiv

preprint arxiv:1604.08633, 2016.

ilya sutskever, james martens, and geoffrey e hinton. generating text with recurrent neural net-
works. in proceedings of the 28th international conference on machine learning (icml-11),
pp. 1017   1024, 2011.

ilya sutskever, oriol vinyals, and quoc vv le. sequence to sequence learning with neural net-

works. in advances in neural information processing systems, pp. 3104   3112, 2014.

tijmen tieleman and geoffrey hinton. lecture 6.5-rmsprop. coursera: neural networks for

machine learning, 2012.

matthew d zeiler. adadelta: an adaptive learning rate method. arxiv preprint arxiv:1212.5701,

2012.

10

published as a conference paper at iclr 2017

appendix i: experimental setup

sentence encoders the bag-of-words (cbow) and encoder-decoder models are trained on 1
million sentences from a 2012 wikipedia dump with vocabulary size of 50,000 tokens. we use
nltk (bird, 2006) for id121, and constrain sentence lengths to be between 5 and 70 words.
for the cbow model, we train skip-gram word vectors (mikolov et al., 2013a), with hierarchical-
softmax and a window size of 5 words, using the gensim implementation.4 we control for the
embedding size k and train word vectors of sizes k     {100, 300, 500, 750, 1000}.
for the encoder-decoder models, we use an in-house implementation using the torch7 toolkit (col-
lobert et al., 2011). the decoder is trained as a language model, attempting to predict the correct
word at each time step using a negative-log-likelihood objective (cross-id178 loss over the softmax
layer). we use one layer of lstm cells for the encoder and decoder using the implementation in
l  eonard et al. (2015).
we use the same size for word and sentence representations (i.e. d = k), and train models of
sizes k     {100, 300, 500, 750, 1000}. we follow previous work on sequence-to-sequence learn-
ing (sutskever et al., 2014; li et al., 2015) in reversing the input sentences and clipping gradients.
word vectors are initialized to random values.
we evaluate the encoder-decoder models using id7 scores (papineni et al., 2002), a popular ma-
chine translation evaluation metric that is also used to evaluate auto-encoder models (li et al., 2015).
id7 score measures how well the original sentence is recreated, and can be thought of as a proxy
for the quality of the encoded representation. we compare it with the performance of the models
on the three prediction tasks. the results of the higher-dimensional models are comparable to those
found in the literature, which serves as a sanity check for the quality of the learned models.

auxiliary task classi   er for the auxiliary task predictors, we use multi-layer id88s with
a single hidden layer and relu activation, which were carefully tuned for each of the tasks. we
experimented with several network architectures prior to arriving at this con   guration.
further details regarding the training and architectures of both the sentence encoders and auxiliary
task classi   ers are available in the appendix.

appendix ii: technical details

encoder decoder

parameters of the encoder-decoder were tuned on a dedicated validation set. we experienced with
different learning rates (0.1, 0.01, 0.001), dropout-rates (0.1, 0.2, 0.3, 0.5) (hinton et al., 2012) and
optimization techniques (adagrad (duchi et al., 2011), adadelta (zeiler, 2012), adam (kingma &
ba, 2014) and rmsprop (tieleman & hinton, 2012)). we also experimented with different batch
sizes (8, 16, 32), and found improvement in runtime but no signi   cant improvement in performance.
based on the tuned parameters, we trained the encoder-decoder models on a single gpu (nvidia
tesla k40), with mini-batches of 32 sentences, learning rate of 0.01, dropout rate of 0.1, and the
adagrad optimizer; training takes approximately 10 days and is stopped after 5 epochs with no loss
improvement on a validation set.

prediction tasks

parameters for the predictions tasks as well as classi   er architecture were tuned on a dedicated vali-
dation set. we experimented with one, two and three layer feed-forward networks using relu (nair
& hinton, 2010; glorot et al., 2011), tanh and sigmoid id180. we tried different hid-
den layer sizes: the same as the input size, twice the input size and one and a half times the input
size. we tried different learning rates (0.1, 0.01, 0.001), dropout rates (0.1, 0.3, 0.5, 0.8) and differ-
ent optimization techniques (adagrad, adadelta and adam).

4https://radimrehurek.com/gensim

11

published as a conference paper at iclr 2017

our best tuned classi   er, which we use for all experiments, is a feed-forward network with one
hidden layer and a relu activation function. we set the size of the hidden layer to be the same size
as the input vector. we place a softmax layer on top whose size varies according to the speci   c task,
and apply dropout before the softmax layer. we optimize the log-likelihood using adagrad. we
use a dropout rate of 0.8 and a learning rate of 0.01. training is stopped after 5 epochs with no loss
improvement on the development set. training was done on a single gpu (nvidia tesla k40).

10 additional experiments - content task

how well do the models preserve content when we increase the sentence length? in fig. 4 we plot
content prediction accuracy vs. sentence length for different models.

figure 4: content accuracy vs. sentence length for selected models.

as expected, all models suffer a drop in content accuracy on longer sentences. the degradation is
roughly linear in the sentence length. for the encoder-decoder, models with fewer dimensions seem
to degrade slower.

appendix iii: significance tests

in this section we report the signi   cance tests we conduct in order to evaluate our    ndings. in order
to do so, we use the paired t-test (rubin, 1973).
all the results reported in the summery of    ndings are highly signi   cant (p-value (cid:28) 0.0001). the
ones we found to be not signi   cant (p-value (cid:29) 0.03) are the ones which their accuracy does not
have much of a difference, i.e ed with size 500 and ed with size 750 tested on the word order task
(p-value=0.11), or cbow with dimensions 750 and 1000 (p-value=0.3).

dim.
100
300
500
750
1000

0.0
0.0
0.0
0.0

length word content word order
1.77e-147
1.83e-296

0.0
0.0
0.0
0.0
0.0

0.0
0.0
0.0
0.0

table 2: p-values for ed vs. cbow over the different dimensions and tasks. for example, in the row where
dim equals 100, we compute the p-value of ed compared to cbow with embed size of 100 on all three tasks.

dim.

100 vs. 300
300 vs. 500
500 vs. 750
750 vs. 1000

length word content word order

0.0

7.3e-71
3.64e-175
1.37e-111

8.56e-190
4.20e-05
4.46e-65
2.35e-243

0.0

5.48e-56

0.11

4.32e-61

table 3: p-values for ed models over the different dimensions and tasks.

12

51015202530sentence length0.650.700.750.800.850.900.951.00content prediction accuracycbow 300cbow 100ed 750ed 500ed 1000published as a conference paper at iclr 2017

length word content word order

dim.

100 vs. 300
300 vs. 500
500 vs. 750
750 vs. 1000

0.0

1.47e-215

0.68

4.44e-32

0.0
0.0
0.032
0.3

1.5e-33
3.06e-64

0.05
0.08

table 4: p-values for cbow models over the different dimensions and tasks.

13

