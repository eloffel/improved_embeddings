neural versus phrase-based machine translation quality: a case study

luisa bentivogli

fbk, trento

italy

arianna bisazza

university of amsterdam

the netherlands

mauro cettolo
fbk, trento

italy

marcello federico

fbk, trento

italy

6
1
0
2

 
t
c
o
9

 

 
 
]
l
c
.
s
c
[
 
 

2
v
1
3
6
4
0

.

8
0
6
1
:
v
i
x
r
a

abstract

within the    eld of statistical machine trans-
lation (smt), the neural approach (id4) has
recently emerged as the    rst technology able
to challenge the long-standing dominance of
phrase-based approaches (pbmt). in particu-
lar, at the iwslt 2015 evaluation campaign,
id4 outperformed well established state-of-
the-art pbmt systems on english-german, a
language pair known to be particularly hard
because of morphology and syntactic differ-
ences. to understand in what respects id4
provides better translation quality than pbmt,
we perform a detailed analysis of neural vs.
phrase-based smt outputs, leveraging high
quality post-edits performed by professional
translators on the iwslt data. for the    rst
time, our analysis provides useful insights on
what linguistic phenomena are best modeled
by neural models     such as the reordering of
verbs     while pointing out other aspects that
remain to be improved.

1

introduction

the wave of neural models has eventually reached
the    eld of id151 (smt).
after a period in which neural mt (id4) was
too computationally costly and resource demanding
to compete with state-of-the-art phrase-based mt
(pbmt)1, the situation changed in 2015. for the
   rst time, in the latest edition of iwslt2 (cettolo et

1we use the generic term phrase-based mt to cover standard
phrase-based, hierarchical and syntax-based smt approaches.
2international workshop on spoken language translation

(http://workshop2015.iwslt.org/)

al., 2015), the system described in (luong and man-
ning, 2015) overtook a variety of pbmt approaches
with a large margin (+5.3 id7 points) on a dif   -
cult language pair like english-german     anticipat-
ing what, most likely, will be the new id4 era.

this impressive improvement follows the dis-
tance reduction previously observed in the wmt
2015 shared translation task (bojar et al., 2015).
just few months earlier,
the id4 systems de-
scribed in (jean et al., 2015b) ranked on par with
the best phrase-based models on a couple of lan-
guage pairs. such rapid progress stems from the im-
provement of the recurrent neural network encoder-
decoder model, originally proposed in (sutskever et
al., 2014; cho et al., 2014b), with the use of the at-
tention mechanism (bahdanau et al., 2015). this
evolution has several implications. on one side,
id4 represents a simpli   cation with respect to pre-
vious paradigms. from a management point of view,
similar to pbmt, it allows for a more ef   cient use
of human and data resources with respect to rule-
based mt. from the architectural point of view, a
large recurrent network trained for end-to-end trans-
lation is considerably simpler than traditional mt
systems that integrate multiple components and pro-
cessing steps. on the other side, the id4 pro-
cess is less transparent than previous paradigms. in-
deed, it represents a further step in the evolution
from rule-based approaches that explicitly manipu-
late knowledge, to the statistical/data-driven frame-
work, still comprehensible in its inner workings, to
a sub-symbolic framework in which the translation
process is totally opaque to the analysis.

what do we know about the strengths of id4

and the weaknesses of pbmt? what are the linguis-
tic phenomena that deep learning translation models
can handle with such greater effectiveness? to an-
swer these questions and go beyond poorly informa-
tive id7 scores, we perform the very    rst compar-
ative analysis of the two paradigms in order to shed
light on the factors that differentiate them and deter-
mine their large quality differences.

we build on evaluation data available for the
iwslt 2015 mt english-german task, and com-
pare the results of the    rst four top-ranked partic-
ipants. we choose to focus on one language pair
and one task because of the following advantages:
(i) three state-of-the art pbmt systems compared
against the id4 system on the same data and in
the very same period (that of the evaluation cam-
paign); (ii) a challenging language pair in terms of
morphology and word order differences; (iii) avail-
ability of mt outputs    post-editing done by pro-
fessional translators, which is very costly and thus
rarely available. in general, post-edits have the ad-
vantage of allowing for informative and detailed
analyses since they directly point to translation er-
rors.
in this speci   c framework, the high quality
data created by professional translators guarantees
reliable evaluations. for all these reasons we present
our study as a solid contribution to the better under-
standing of this new paradigm shift in mt.

after reviewing previous work (section 2), we in-
troduce the analyzed data and the systems that pro-
duced them (section 3). we then present three in-
creasingly    ne levels of mt quality analysis. we
   rst investigate how mt systems    quality varies with
speci   c characteristics of the input, i.e.
sentence
length and type of content of each talk (section 4).
then, we focus on differences among mt systems
with respect to morphology, lexical, and word or-
der errors (section 5). finally, based on the    nding
that word reordering is the strongest aspect of id4
compared to the other systems, we carry out a    ne-
grained analysis of word order errors (section 6).

2 previous work

to date, id4 systems have only been evaluated by
id7 in single-reference setups (bahdanau et al.,
2015; sutskever et al., 2014; luong et al., 2015;
jean et al., 2015a; g  ulc  ehre et al., 2015). ad-

ditionally, the montreal id4 system submitted to
wmt 2015 (jean et al., 2015b) was part of a man-
ual evaluation experiment where a large number of
non-professional annotators were asked to rank the
outputs of multiple mt systems (bojar et al., 2015).
results for the montreal system were very positive
    ranked    rst in english-german, third in german-
english, english-czech and czech-english     which
con   rmed and strengthened the id7 results pub-
lished so far. unfortunately neither id7 nor man-
ual ranking judgements tell us which translation as-
pects are better modeled by different mt frame-
works. to this end, a detailed and systematic error
analysis of id4 vs. pbmt output is required.

existing tools for

translation error analysis, as a way to identify
systems    weaknesses and de   ne priorities for their
improvement, has received a fair amount of atten-
tion in the mt community. in this work we opt for
the automatic detection and classi   cation of transla-
tion errors based on manual post-edits of the mt
output. we believe this choice provides an opti-
mal trade-off between fully manual error analysis
(farr  us cabeceran et al., 2010; popovi  c et al., 2013;
daems et al., 2014; federico et al., 2014; neubig
et al., 2015), which is very costly and complex,
and fully automatic error analysis (popovi  c and ney,
2011; irvine et al., 2013), which is noisy and biased
towards one or few arbitrary reference translations.
translation error detection
are either based on word error rate (wer)
and position-independent word error rate (per)
(popovi  c, 2011) or on output-reference alignment
(zeman et al., 2011). regarding error classi   -
cation, hjerson (popovi  c, 2011) detects    ve main
types of word-level errors as de   ned in (vilar et al.,
2006): morphological, reordering, missing words,
extra words, and lexical choice errors. we follow
a similar but simpler error classi   cation (morpho-
logical, lexical, and word order errors), but detect
the errors differently using ter as this is the most
natural choice in our evaluation framework based on
post-edits (see also section 3.4). irvine et al. (2013)
propose another word-level error analysis technique
speci   cally focused on lexical choice and aimed at
understanding the effects of domain differences on
mt. their error classi   cation is strictly related to
model coverage and insensitive to word order dif-
ferences. the technique requires access to the sys-

tem   s phrase table and is thus not applicable to id4,
which does not rely on a    xed inventory of transla-
tion units extracted from the parallel data.

previous error analyses based on manually post-
edited translations were presented in (bojar, 2011;
koponen, 2012; popovi  c et al., 2013). we are the
   rst to conduct this kind of study on the output of a
neural mt system.

3 experimental setting

we perform a number of analyses on data and re-
sults of the iwslt 2015 mt en-de task, which
consists in translating manual transcripts of english
ted talks into german. evaluation data are pub-
licly available through the wit3 repository (cettolo
et al., 2012).3

3.1 task data
ted talks4 are a collection of rather short speeches
(max 18 minutes each, roughly equivalent to 2,500
words) covering a wide variety of topics. all talks
have captions, which are translated into many lan-
guages by volunteers worldwide. besides represent-
ing a popular benchmark for spoken language tech-
nology, ted talks embed interesting research chal-
lenges. translating ted talks implies dealing with
spoken rather than written language, which is hence
expected to be structurally less complex, formal and
   uent (ruiz and federico, 2014). moreover, as hu-
man translations of the talks are required to follow
the structure and rhythm of the english captions, a
lower amount of rephrasing and reordering is ex-
pected than in the translation of written documents.
as regards the english-german language pair, the
two languages are interesting since, while belonging
to the same language family, they have marked dif-
ferences in levels of in   ection, morphological varia-
tion, and word order, especially long-range reorder-
ing of verbs.

3.2 evaluation data
five systems participated in the mt en-de task and
were manually evaluated on a representative subset
of the of   cial 2015 test set. the human evaluation
(he) set includes the    rst half of each of the 12 test

3wit3.fbk.eu
4http://www.ted.com/

system
pbsy

approach

data

hpb

(jehl et al.,

(huck and
birch, 2015)

combination: phrase+syntax-based 175m/
ghkm string-to-tree; hierarchical + 3.1b
sparse lexicalized reordering models
hierarchical phrase-based
166m/
source pre-ordering (dependency tree 854m
-based); re-scoring with neural lm
standard phrase-based
source pre-ordering (pos- and tree-
based); re-scoring with neural lms
recurrent neural network (lstm)
(luong & man- attention-based; source reversing;

120m/
   

117m/
2.4b

2015)
id4

2015)
spb

(ha et al.,

ning, 2015)

rare words handling

table 1: mt systems    overview. data column: size of paral-
lel/monolingual training data for each system in terms of en-
glish and german tokens.

talks, for a total of 600 sentences and around 10k
words. five professional translators were asked to
post-edit the mt output by applying the minimal ed-
its required to transform it into a    uent sentence with
the same meaning as the source sentence. data were
prepared so that all translators equally post-edited
the    ve mt outputs, i.e. 120 sentences for each eval-
uated system.

the resulting evaluation data consist of    ve new
reference translations for each of the sentences in
the he set. each one of these references represents
the targeted translation of the system output from
which it was derived, but the other four additional
translations can also be used to evaluate each mt
system. we will see in the next sections how we ex-
ploited the available post-edits in the more suitable
way depending on the kind of analysis carried out.

3.3 mt systems
our analysis focuses on the    rst four top-ranking
systems, which include id4 (luong and manning,
2015) and three different phrase-based approaches:
standard phrase-based (ha et al., 2015), hierarchi-
cal (jehl et al., 2015) and a combination of phrase-
based and syntax-based (huck and birch, 2015). ta-
ble 1 presents an overview of each system, as well
as    gures about the training data used.5

the phrase+syntax-based (pbsy) system com-
bines the outputs of a string-to-tree decoder, trained
with the ghkm algorithm, with those of two stan-

5detailed information about training data was kindly made

available by participating teams.

dard phrase-based systems featuring, among others,
adapted phrase tables and language models enriched
with morphological information, hierarchical lexi-
calized reordering models and different variations of
the operational sequence model.

the hierarchical phrase-based mt (hpb) system
leverages thousands of lexicalised features, data-
driven source pre-ordering (dependency tree-based),
word-based and class-based language models, and
n-best re-scoring models based on syntactic and neu-
ral language models.

the standard phrase-based mt (spb) system fea-
tures an adapted phrase-table combining in-domain
and out-domain data, discriminative word lexicon
models, multiple language models (word-, pos- and
class-based), data-driven source pre-ordering (pos-
and constituency syntax-based), n-best re-scoring
models based on neural lexicons and neural lan-
guage models.

finally, the neural mt (id4) system is an en-
semble of 8 long short-term memory (lstm) net-
works of 4 layers featuring 1,000-dimension word
embeddings, attention mechanism, source revers-
ing, 50k source and target vocabularies, and out-of-
vocabulary word handling. training with ted data
was performed on top of models trained with large
out-domain parallel data.

with respect to the use of training data, it is worth
noticing that id4 is the only system not employ-
ing monolingual data in addition to parallel data.
moreover, id4 and spb were trained with smaller
amounts of parallel data with respect to pbsy and
hpb (see table 1).

3.4 translation edit rate measures
the translation edit rate (ter) (snover et al.,
2006)
naturally    ts our evaluation framework,
where it traces the edits done by post-editors. also,
ter shift operations are reliable indicators of re-
ordering errors, in which we are particularly inter-
ested. we exploit the available post-edits in two dif-
ferent ways: (i) for human-targeted ter (hter)
we compute ter between the machine translation
and its manually post-edited version (targeted ref-
erence), (ii) for multi-reference ter (mter), we
compute ter against the closest translation among
all available post-edits (i.e. targeted and additional
references) for each sentence.

system id7 hter mter
pbsy 25.3
24.6
hpb
25.8
spb
31.1   
id4

28.0
29.9
29.0
21.1   

21.8
23.4
22.7
16.2   

table 2: overall results on the he set: id7, computed
against the original reference translation, and ter, computed
with respect to the targeted post-edit (hter) and multiple post-
edits (mter).

throughout sections 4 and 5, we mark a score
achieved by id4 with the symbol * if this is bet-
ter than the score of its best competitor at statistical
signi   cance level 0.01. signi   cance tests for hter
and mter are computed by bootstrap re-sampling,
while differences among proportions are assessed
via one-tailed z-score tests.

4 overall translation quality
table 2 presents overall system results according
to hter and mter, as well as id7 computed
against the original ted talks reference translation.
we can see that id4 clearly outperforms all other
approaches both in terms of id7 and ter scores.
focusing on mter results, the gain obtained by
id4 over the second best system (pbsy) amounts
to 26%. it is also worth noticing that mter is con-
siderably lower than hter for each system. this re-
duction shows that exploiting all the available post-
edits as references for ter is a viable way to control
and overcome post-editors variability, thus ensuring
a more reliable and informative evaluation about the
real overall performance of mt systems. for this
reason, the two following analyses rely on mter.
in particular, we investigate how speci   c character-
istics of input documents affect the system   s overall
translation quality, focusing on (i) sentence length
and (ii) the different talks composing the dataset.

4.1 translation quality by sentence length
long sentences are known to be dif   cult to trans-
late by the id4 approach. following previous work
(cho et al., 2014a; pouget-abadie et al., 2014; bah-
danau et al., 2015; luong et al., 2015), we investi-
gate how sentence length affects overall translation
quality. figure 1 plots mter scores against source
sentence length. id4 clearly outperforms every
pbmt system in any length bin, with statistically

figure 1: mter scores on bins of sentences of different length.
points represent the average mter of the mt outputs for the
sentences in each given bin.

signi   cant differences. as a general tendency, the
performance of all approaches worsens as sentence
length increases. however, for sentences longer than
35 words we see that id4 quality degrades more
markedly than in pbmt systems. considering the
percentage decrease with respect to the preceding
length bin (26-35), we see that the %    for id4
(-15.4) is much larger than the average %    for the
three pbmt systems (-7.9). hence, this still seems
an issue to be addressed for further improving id4.

4.2 translation quality by talk

as we saw in section 3.1, the ted dataset is very
heterogeneous since it consists of talks covering dif-
ferent topics and given by speakers with different
styles. it is therefore interesting to evaluate trans-
lation quality also at the talk level.

figure 2 plots the mter scores for each of the
twelve talks included in the he set, sorted in ascend-
ing order of id4 scores. in all talks, the id4 sys-
tem outperforms the pbmt systems in a statistically
signi   cant way.

we analysed different factors which could impact
translation quality in order to understand if they cor-
relate with such performance differences. we stud-
ied three features which are typically considered as
indicators of complexity (see (franc  ois and fairon,
2012) for an overview), namely (i) the length of the
talk, (ii) its average sentence length, and (iii) the

figure 2: mter scores per talk, sorted in ascending order of
id4 scores.

type-token ratio6 (ttr) which     measuring lexical
diversity     re   ects the size of a speaker   s vocabulary
and the variety of subject matter in a text.

for the    rst two features we did not    nd any cor-
relation; on the contrary, we found a moderate pear-
son correlation (r=0.7332) between ttr and the
mter gains of id4 over its closest competitor in
each talk. this result suggests that id4 is able to
cope with lexical diversity better than any other con-
sidered approach.

5 analysis of translation errors

we now turn to analyze which types of linguistic er-
rors characterize id4 vs. pbmt. in the literature,
various error taxonomies covering different levels of
granularity have been developed (flanagan, 1994;
vilar et al., 2006; farr  us cabeceran et al., 2010;
stymne and ahrenberg, 2012; lommel et al., 2014).
we focus on three error categories, namely (i) mor-
phology errors, (ii) lexical errors, and (iii) word or-
der errors. as for lexical errors, a number of existing
taxonomies further distinguish among translation er-
rors due to missing words, extra words, or incor-
rect lexical choice. however, given the proven dif-
   culty of disambiguating between these three sub-
classes (popovi  c and ney, 2011; fishel et al., 2012),
we prefer to rely on a more coarse-grained linguistic
error classi   cation where lexical errors include all of
them (farr  us cabeceran et al., 2010).

6the type-token-ratio of a text is calculated dividing the
number of word types (vocabulary) by the total number of word
tokens (occurrences).

for error analysis we rely on hter results under
the assumption that, since the targeted translation is
generated by post-editing the given mt output, this
method is particularly informative to spot mt er-
rors. we are aware that translator subjectivity is still
an issue (see section 4), however in this more    ne-
grained analysis we prefer to focus on what a hu-
man implicitly annotated as a translation error. this
particularly holds in our speci   c evaluation frame-
work, where the goal is not to measure the absolute
number of errors made by each system, but to com-
pare systems with each other. moreover, the post-
edits collected for each mt output within iwslt
allow for a fair and reliable comparison since sys-
tems were equally post-edited by all translators (see
section 3.2), making all analyses uniformly affected
by such variability.

5.1 morphology errors
a morphology error occurs when a generated word
form is wrong but
its corresponding base form
(lemma) is correct. thus, we assess the ability of
systems to deal with morphology by comparing the
hter score computed on the surface forms (i.e.
morphologically in   ected words) with the hter
score obtained on the corresponding lemmas. the
additional matches counted on lemmas with respect
to word forms indicate morphology errors. thus, the
closer the two hter scores, the more accurate the
system in handling morphology.

to carry out this analysis, the lemmatized (and
pos tagged) version of both mt outputs and cor-
responding post-edits was produced with the ger-
man parser parzu (sennrich et al., 2013). then, the
hter-based evaluation was slightly adapted in or-
der to be better suited to an accurate detection of
morphology errors. first, punctuation was removed
since     not being subject to morphological in   ection
    it could smooth the results. second, shift errors
were not considered. a word form or a lemma that
matches a corresponding word or lemma in the post-
edit, but is in the wrong position with respect to it,
is counted as a shift error in ter. instead     when
focusing on morphology     exact matches are not er-
rors, regardless their position in the text.7

7note that the ter score calculated by setting to 0 the
cost of shifts approximates the position-independent error
rate (tillmann et al., 1997).

system

hternoshft
word
pbsy 27.1
28.7
hpb
spb
28.3
21.7   
id4

lemma %   
-16.9
22.5
-18.4
23.5
23.2
-18.0
18.7   
-13.7

table 3: hter ignoring shift operations computed on words
and corresponding lemmas, and their % difference.

table 3 presents hter scores on word forms and
lemmas, as well as their percentage difference which
gives an indication of morphology errors. we can
see that id4 generates translations which are mor-
phologically more correct than the other systems. in
particular, the %    for id4 (-13.7) is lower than
that of the second best system (pbsy, -16.9) by
3.2% absolute points, leading to a percentage gain
of around 19%. we can thus say that id4 makes
at least 19% less morphology errors than any other
pbmt system.

5.2 lexical errors
another important feature of mt systems is their
ability to choose lexically appropriate words. in or-
der to compare systems under this aspect, we con-
sider hter results at the lemma level as a way
to abstract from morphology errors and focus only
on actual lexical choice problems. the evaluation
on the lemmatised version of the data performed to
identify morphology errors    ts this purpose, since
its driving assumptions (i.e. punctuation can be ex-
cluded and lemmas in the wrong order are not errors)
hold for lexical errors too.

the lemma column of table 3 shows that id4
outperforms the other systems. more precisely, the
id4 score (18.7) is better than the second best
(pbsy, 22.5) by 3.8% absolute points. this corre-
sponds to a relative gain of about 17%, meaning that
id4 makes at least 17% less lexical errors than any
pbmt system. similarly to what observed for mor-
phology errors, this can be considered a remarkable
improvement over the state of the art.

5.3 word order errors
to analyse reordering errors, we start by focusing on
shift operations identi   ed by the hter metrics. the
   rst three columns of table 4 show, respectively:
(i) the number of words generated by each system

system #words
pbsy 11,517
11,417
hpb
11,420
spb
id4
11,284

#shifts %shifts
354
415
398
173

3.1
3.6
3.5
1.5   

krs
84.6
84.3
84.5
88.3   

table 4: word reordering evaluation in terms of shift opera-
tions in hter calculation and of krs. for each system, the
number of generated words, the number of shift errors and their
corresponding percentages are reported.

(ii) the number of shifts required to align each sys-
tem output to the corresponding post-edit; and (iii)
the corresponding percentage of shift errors. notice
that the shift error percentages are incorporated in
the hter scores reported in table 2. we can see
in table 4 that shift errors in id4 translations are
de   nitely less than in the other systems. the error
reduction of id4 with respect to the second best
system (pbsy) is about 50% (173 vs. 354).

it should be recalled that these numbers only re-
fer to shifts detected by hter, that is (groups of)
words of the mt output and corresponding post-edit
that are identical but occurring in different positions.
words that had to be moved and modi   ed at the
same time (for instance replaced by a synonym or
a morphological variant) are not counted in hter
shift    gures, but are detected as substitution, inser-
tion or deletion operations. to ensure that our re-
ordering evaluation is not biased towards the align-
ment between the mt output and the post-edit per-
formed by hter, we run an additional assessment
using krs     kendall reordering score (birch et
al., 2010)     which measures the similarity between
the source-reference reorderings and the source-mt
output reorderings.8 being based on bilingual word
alignment via the source sentence, krs detects re-
ordering errors also when post-edit and mt words
are not identical. also unlike ter, krs is sensitive
to the distance between the position of a word in the
mt output and that in the reference.

looking at the last column of table 4, we can say
that our observations on hter are con   rmed by the
krs results: the reorderings performed by id4 are
much more accurate than those performed by any
pbmt system.9 moreover, according to the approx-

imate randomization test, krs differences are statis-
tically signi   cant between id4 and all other sys-
tems, but not among the three pbmt systems.

given the concordant results of our two quanti-
tative analyses, we conclude that one of the ma-
jor strengths of the id4 approach is its ability to
place german words in the right position even when
this requires considerable reordering. this outcome
calls for a deeper investigation, which is carried out
in the following section.

6 fine-grained word order error

analysis

we have observed that word reordering is a very
strong aspect of id4 compared to pbmt, accord-
ing to both hter and krs. to better understand
this    nding, we investigate whether reordering er-
rors concentrate on speci   c linguistic constructions
across our systems. using the id52 and
id33 of the post-edits produced by
parzu, we classify the shift operations detected by
hter and count how often a word with a given pos
label was misplaced by each of the systems (alone or
as part of a shifted block). for each word class, we
also compute the percentage order error reduction
of id4 with respect to the pbmt system that has
highest reordering accuracy overall, that is pbsy.
results are presented in table 5, ranked by id4-
vs-pbsy gain. punctuation is omitted as well as
word classes that were shifted less than 10 times by
all systems. examples of salient word order error
types are presented in table 6.

the upper part of table 5 shows that verbs are
by far the most often misplaced word category in all
pbmt systems     an issue already known to affect
standard phrase-based smt between german and
english (bisazza and federico, 2013). reordering is
particularly dif   cult when translating into german,
since the position of verbs in this language varies
according to the clause type (e.g. main vs. subor-
dinate). our results show that even syntax-informed
pbmt does not solve this issue. using syntax at
decoding time, as done by one of the systems com-
bined within pbsy, appears to be a better strategy

8to compute the word alignments required by krs, we used

the fastalign tool (dyer et al., 2013).

9to put our results into perspective, note that birch (2011)

reports a difference of 5 krs points between the translations of
a pbmt system and those produced by four human translators
tested against each other, in a chinese-english experiment.

class

v

pro

ptkzu

adv

n

kon
prep

ptkneg

art
aux:v
neb:v
objc:v
subj:pro
root:v
adv:adv
obja:n
cj:v

part:ptkzu
obja:pro
mroot:v

pn:n
subj:n
pp:prep

adv:ptkneg

det:art

all

id4- id4 pbsy hpb spb
vs-pbsy
133 155
-70%
62
53
-57%
-54%
4
11
36
44
-50%
56
99
-47%
12
8
-33%
28
27
-18%
-17%
10
7
35
38
-4%
18
17
-87%
19
7
-83%
-79%
21
24
46
34
-70%
27
28
-68%
28
33
-67%
-65%
28
12
22
21
-59%
11
4
-54%
7
14
-38%
-36%
26
20
19
33
-36%
7
10
-33%
23
19
-30%
-17%
10
7
38
34
-4%
-48%
493 488

116
51
13
28
70
9
22
12
27
23
12
14
40
19
24
17
17
13
8
11
25
9
20
12
27
429

35
22
6
14
37
6
18
10
26
3
2
3
12
6
8
6
7
6
5
7
16
6
14
10
26
222

table 5: main pos tags and dependency labels of words oc-
curring in shifted blocks detected by hter. id4-vs-pbsy
denotes the reduction of reordering errors in id4 vs. pbsy
system. only word classes that were shifted 10 or more times
in at least one system output are shown.

than using it for source pre-ordering, as done by the
hpb and spb systems. however this only results
in a moderate reduction of verb reordering errors (-
12% and -25% vs. hpb and spb respectively). on
the contrary, id4 reduces verb order errors by an
impressive -70% with respect to pbsy (-74% and
-77% vs. hpb and spb respectively) despite being
trained on raw parallel data without any syntactic
annotation, nor explicit modeling of word reorder-
ing. this result shows that the recurrent neural lan-
guage model at the core of the id4 architecture is
very successful at generating well-formed sentences
even in languages with less predictable word order,
like german (see examples in table 6(a,b)). id4,
though, gains notably less on nouns (-47%), which
is the second most often misplaced word category

in pbsy. more insight on this is provided by the
lower part of the table, where reordering errors are
divided by their dependency label as well as pos
tag. here we see that order errors on nouns are
notably reduced by id4 when they act as syntac-
tic objects (-65% obja:n) but less when they act as
preposition complements (-36% pn:n) or subjects (-
33% subj:n).

the smallest id4-vs-pbsy gains are observed
on prepositions (-18% prep), negation particles
(-17% ptkneg) and articles (-4% art). manual
inspection of a data sample reveals that misplaced
prepositions are often part of misplaced preposi-
tional phrases acting, for instance, as temporal or
   in my life   ,    with this
instrumental adjuncts (e.g.
video   ).
in these cases, the original mt output is
overall understandable and grammatical, but does
not conform to the order of german semantic argu-
ments that is consistently preferred by post-editors
(see example in table 6(c)). articles, due to their
commonness, are often misaligned by hter and
marked as shift errors instead of being marked as
two unrelated substitutions. finally, negation parti-
cles account for less than 1% of the target tokens but
play a key role in determining the sentence meaning.
looking closely at some error examples, we found
that the correct placement of the german particle
nicht was determined by the focus of negation in the
source sentence, which is dif   cult to detect in en-
glish. for instance in table 6(d) two interpretations
are possible (   that did not work    or    that worked, but
not for systematic reasons   ), each resulting in a dif-
ferent, but equally grammatical, location of nicht. in
fact, negation-focus detection calls for a deep un-
derstanding of the sentence semantics, often requir-
ing extra-sentential context (blanco and moldovan,
2011). when faced with this kind of translation de-
cisions, id4 performs as poorly as its competitors.
in summary, our    ne-grained analysis con   rms
that id4 concentrates its word order improvements
on important linguistic constituents and, speci   cally
in english-german, is very close to solving the infa-
mous problem of long-range verb reordering which
so many pbmt approaches have only poorly man-
aged to handle. on the other hand, id4 still strug-
gles with more subtle translation decisions depend-
ing, for instance, on the semantic ordering of adjunct
prepositional phrases or on the focus of negation.

auxiliary-main verb construction [aux:v]:

src in this experiment , individuals were shown hundreds of hours of youtube videos
hpb in diesem experiment , individuen gezeigt wurden hunderte von stunden youtube-videos

in diesem experiment wurden individuen hunderte von stunden youtube-videos gezeigt

(a) pe

id4 in diesem experiment wurden individuen hunderte stunden youtube videos gezeigt
in diesem experiment wurden individuen hunderte stunden youtube videos gezeigt
pe

verb in subordinate (adjunct) clause [neb:v]:

src ... when coaches and managers and owners look at this information streaming ...
pbsy ... wenn trainer und manager und eigent  umer betrachten diese information streaming ...

... wenn trainer und manager und eigent  umer dieses informations-streaming betrachten ...

(b) pe

id4 ... wenn trainer und manager und besitzer sich diese informationen anschauen ...
... wenn trainer und manager und besitzer sich diese informationen anschauen ...
pe

prepositional phrase [pp:prep det:art pn:n] acting as temporal adjunct:

src so like many of us , i    ve lived in a few closets in my life
spb
(c) pe

so wie viele von uns , ich habe in ein paar schr  anke in meinem leben gelebt
so habe ich wie viele von uns w  ahrend meines lebens in einigen verstecken gelebt

id4 wie viele von uns habe ich in ein paar schr  anke in meinem leben gelebt
wie viele von uns habe ich in meinem leben in ein paar schr  anken gelebt
pe

negation particle [adv:ptkneg]:

src but i eventually came to the conclusion that that just did not work for systematic reasons
hpb aber ich kam schlielich zu dem schluss , dass nur aus systematischen gr  unden nicht funktionieren

aber ich kam schlielich zu dem schluss , dass es einfach aus systematischen gr  unden nicht funktioniert

(d) pe

%

!

%

!

%

%

!

id4 aber letztendlich kam ich zu dem schluss , dass das einfach nicht aus systematischen gr  unden funktionierte %
pe

ich musste aber einsehen , dass das aus systematischen gr  unden nicht funktioniert

table 6: mt output and post-edit examples showing common types of reordering errors.

7 conclusions

we analysed the output of four state-of-the-art mt
systems that participated in the english-to-german
task of the iwslt 2015 evaluation campaign. our
selected runs were produced by three phrase-based
mt systems and a neural mt system. the analysis
leveraged high quality post-edits of the mt outputs,
which allowed us to pro   le systems with respect to
reliable measures of post-editing effort and transla-
tion error types.

the outcomes of the analysis con   rm that id4
has signi   cantly pushed ahead the state of the art,
especially in a language pair involving rich morphol-
ogy prediction and signi   cant word reordering. to
summarize our    ndings: (i) id4 generates outputs
that considerably lower the overall post-edit effort
with respect to the best pbmt system (-26%); (ii)
id4 outperforms pbmt systems on all sentence
lengths, although its performance degrades faster
with the input length than its competitors; (iii) id4
seems to have an edge especially on lexically rich
texts; (iv) id4 output contains less morphology er-

rors (-19%), less lexical errors (-17%), and substan-
tially less word order errors (-50%) than its closest
competitor for each error type; (v) concerning word
order, id4 shows an impressive improvement in
the placement of verbs (-70% errors).

while id4 proved superior to pbmt with re-
spect to all error types that were investigated, our
analysis also pointed out some aspects of id4 that
deserve further work, such as the handling of long
sentences and the reordering of particular linguistic
constituents requiring a deep semantic understand-
ing of text. machine translation is de   nitely not a
solved problem, but the time is    nally ripe to tackle
its most intricate aspects.

acknowledgments

fbk authors were supported by the cracker,
qt21 and moderid4 projects, which received
funding from the european union   s horizon 2020
programme under grants no. 645357, 645452 and
645487. ab was funded in part by the nwo under
projects 639.022.213 and 612.001.218.

references

[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio.
2015. neural machine
translation by jointly learning to align and translate.
in proc. of iclr, san diego, us-ca.

[birch et al.2010] alexandra birch, miles osborne, and
phil blunsom. 2010. metrics for mt evaluation: eval-
uating reordering. machine translation, 24(1):15   26.
[birch2011] alexandra birch. 2011. reordering met-
rics for id151. ph.d. thesis,
school of informatics, university of edinburgh, uk.
[bisazza and federico2013] arianna bisazza and mar-
cello federico. 2013. ef   cient solutions for word
reordering in german-english phrase-based statistical
machine translation. in proc. of wmt, so   a, bulgaria.
[blanco and moldovan2011] eduardo blanco and dan
moldovan. 2011. semantic representation of negation
using focus detection. in proc. of acl-hlt, portland,
us-or.

[bojar et al.2015] ond  rej bojar, rajen chatterjee, chris-
tian federmann, barry haddow, matthias huck, chris
hokamp, philipp koehn, varvara logacheva, christof
monz, matteo negri, matt post, carolina scarton, lu-
cia specia, and marco turchi. 2015. findings of the
2015 workshop on id151. in
proc. of wmt, lisbon, portugal.

[bojar2011] ondrej bojar. 2011. analyzing error types
in english-czech machine translation. the prague
bulletin of mathematical linguistic, (95):63   76.

[cettolo et al.2012] mauro cettolo, christian girardi, and
marcello federico. 2012. wit3: web inventory of
transcribed and translated talks. in proc. of eamt,
trento, italy.

[cettolo et al.2015] mauro cettolo, jan niehues, sebas-
tian st  uker, luisa bentivogli, roldano cattoni, and
marcello federico. 2015. the iwslt 2015 evalua-
tion campaign. in proc. of iwslt, da nang, vietnam.
[cho et al.2014a] kyunghyun cho, bart van merri  enboer,
dzmitry bahdanau, and yoshua bengio. 2014a. on
the properties of id4: encoder   
decoder approaches. in proc. of ssst-8, doha, qatar.
[cho et al.2014b] kyunghyun cho, bart van merrien-
boer, caglar gulcehre, dzmitry bahdanau, fethi
bougares, holger schwenk, and yoshua bengio.
2014b. learning phrase representations using id56
encoder   decoder for id151. in
proc. of emnlp, doha, qatar.

[daems et al.2014] joke daems, lieve macken, and so-
nia vandepitte. 2014. on the origin of errors: a    ne-
grained analysis of mt and pe errors and their rela-
tionship. in proc. of lrec, reykjavik, iceland.

[dyer et al.2013] chris dyer, victor chahuneau, and
noah a. smith. 2013. a simple, fast, and effec-
tive reparameterization of ibm model 2. in proc. of
nacl-hlt, atlanta, us-ga.

[farr  us cabeceran et al.2010] mireia farr  us cabeceran,
marta ruiz costa-juss`a, jos  e bernardo mari  no ace-
bal, and jos  e adri  an rodr    guez fonollosa.
2010.
linguistic-based evaluation criteria to identify statis-
in proc. of eamt,
tical machine translation errors.
saint-rapha  el, france.

[federico et al.2014] marcello federico, matteo negri,
luisa bentivogli, and marco turchi. 2014. assess-
ing the impact of translation errors on machine trans-
lation quality with mixed-effects models. in proc. of
emnlp, doha, qatar.

[fishel et al.2012] mark fishel, ondrej bojar,

and
maja popovi  c. 2012. terra: a collection of translation
error-annotated corpora. in proc. of lrec, istanbul,
turkey.

[flanagan1994] mary flanagan. 1994. error classi   ca-
tion for mt evaluation. in proc. of amta, columbia,
us-md.

[franc  ois and fairon2012] thomas franc  ois and c  edrick
fairon. 2012. an    ai readability    formula for french
in proc. of emnlp-conll,
as a foreign language.
jeju island, korea.

[g  ulc  ehre et al.2015] c   aglar g  ulc  ehre, orhan firat,
kelvin xu, kyunghyun cho, lo    c barrault, huei-chi
lin, fethi bougares, holger schwenk, and yoshua
bengio.
2015. on using monolingual corpora in
id4. corr, abs/1503.03535.

[ha et al.2015] thanh-le ha, jan niehues, eunah cho,
mohammed mediani, and alex waibel. 2015. the
kit translation systems for iwslt 2015. in proc. of
iwslt, da nang, vietnam.

[huck and birch2015] matthias huck and alexandra
birch. 2015. the edinburgh machine translation sys-
tems for iwslt 2015. in proc. of iwslt, da nang,
vietnam.

[irvine et al.2013] ann irvine,

john morgan, marine
carpuat, hal daum  e iii, and dragos munteanu. 2013.
measuring machine translation errors in new domains.
transactions of the association for computational
linguistics, 1:429   440.
[jean et al.2015a] s  ebastien

jean, kyunghyun cho,
roland memisevic, and yoshua bengio. 2015a. on
using very large target vocabulary for neural machine
translation. in proc. of acl-ijcnlp, beijing, china.
firat,
kyunghyun cho, roland memisevic, and yoshua
bengio. 2015b. montreal id4
in proc. of wmt, lisbon,
systems for wmt15.
portugal.

[jean et al.2015b] s  ebastien

orhan

jean,

pos-tagging, and morphological analysis. in proc. of
ranlp, hissar, bulgaria.

[snover et al.2006] matthew snover, bonnie dorr, rich
schwartz, linnea micciulla, and john makhoul. 2006.
a study of translation edit rate with targeted human
annotation. in proc. of amta, boston, us-ma.

[stymne and ahrenberg2012] sara stymne

and lars
ahrenberg. 2012. on the practice of error analysis
for machine translation evaluation. in proc. of lrec,
istanbul, turkey.

[sutskever et al.2014] ilya sutskever, oriol vinyals, and
quoc v. le. 2014. sequence to sequence learning
in proc. of nips, montr  eal,
with neural networks.
canada.

[tillmann et al.1997] christoph tillmann, stephan vogel,
hermann ney, alexander zubiaga, and hassan sawaf.
1997. accelerated dp based search for statistical
translation. in proc. of eurospeech, rhodes, greece.
[vilar et al.2006] david vilar, jia xu, luis fernando
2006. error analysis
in proc. of

d   haro, and hermann ney.
of id151 output.
lrec, genoa, italy.

[zeman et al.2011] daniel zeman, mark fishel,

jan
berka, and ondrej bojar. 2011. addicter: what is
wrong with my translations? the prague bulletin of
mathematical linguistic, (96):79   88.

[jehl et al.2015] laura jehl, patrick simianer,

julian
hitschler, and stefan riezler.
2015. the heidel-
berg university english-german translation system for
iwslt 2015. in proc. of iwslt, da nang, vietnam.
2012. comparing
human perceptions of post-editing effort with post-
in proc. of wmt, montr  eal,
editing operations.
canada.

[koponen2012] maarit koponen.

[lommel et al.2014] arle lommel, aljoscha burchardt,
maja popovi  c, kim harris, eleftherios avramidis, and
hans uszkoreit. 2014. using a new analytic measure
for the annotation and analysis of mt errors on real
data. in proc. of eamt, dubrovnik, croatia.

2015.

luong

[luong and manning2015] minh-thang

and
christopher d manning.
stanford neural
machine translation systems for spoken language
domains. in proc. of iwslt, da nang, vietnam.
[luong et al.2015] thang luong, hieu pham,

and
christopher d. manning. 2015. effective approaches
to attention-based id4.
in
proc. of emnlp, lisbon, portugal.

[neubig et al.2015] graham neubig, makoto morishita,
and satoshi nakamura.
2015. neural reranking
improves subjective quality of machine translation:
in proc. of wat2015, kyoto,
naist at wat2015.
japan.

[popovi  c and ney2011] maja popovi  c and hermann ney.
2011.
towards automatic error analysis of ma-
chine translation output. computational linguistics,
37(4):657   688.

[popovi  c et al.2013] maja

popovi  c,

eleftherios
avramidis, aljoscha burchardt, sabine hunsicker,
sven schmeier, cindy tscherwinka, david vilar,
and hans uszkoreit. 2013. learning from human
judgments of machine translation output. in proc. of
mt summit, nice, france.

[popovi  c2011] maja popovi  c. 2011. hjerson: an open
source tool for automatic error classi   cation of ma-
chine translation output. the prague bulletin of math-
ematical linguistic, (96):59   68.

[pouget-abadie et al.2014] jean pouget-abadie, dzmitry
bahdanau, bart van merrienboer, kyunghyun cho,
and yoshua bengio. 2014. overcoming the curse of
sentence length for id4 using
in proc. of ssst-8, doha,
automatic segmentation.
qatar.

[ruiz and federico2014] nicholas ruiz and marcello
federico. 2014. complexity of spoken versus written
language for machine translation. in proc. of eamt,
dubrovnik, croatia.

[sennrich et al.2013] rico sennrich, martin volk, and
gerold schneider. 2013. exploiting synergies be-
tween open resources for german id33,

