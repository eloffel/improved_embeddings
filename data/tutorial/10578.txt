man is to computer programmer as woman is to homemaker?

debiasing id27s

tolga bolukbasi1, kai-wei chang2, james zou2, venkatesh saligrama1,2, adam kalai2

1boston university, 8 saint mary   s street, boston, ma

2microsoft research new england, 1 memorial drive, cambridge, ma

6
1
0
2

 
l
u
j
 

1
2

 
 
]
l
c
.
s
c
[
 
 

1
v
0
2
5
6
0

.

7
0
6
1
:
v
i
x
r
a

tolgab@bu.edu, kw@kwchang.net, jamesyzou@gmail.com, srv@bu.edu, adam.kalai@microsoft.com

abstract

the blind application of machine learning runs the risk of amplifying biases present in data. such a
danger is facing us with id27, a popular framework to represent text data as vectors which
has been used in many machine learning and natural language processing tasks. we show that even
id27s trained on google news articles exhibit female/male gender stereotypes to a disturbing
extent. this raises concerns because their widespread use, as we describe, often tends to amplify these
biases. geometrically, gender bias is    rst shown to be captured by a direction in the id27.
second, gender neutral words are shown to be linearly separable from gender de   nition words in the word
embedding. using these properties, we provide a methodology for modifying an embedding to remove
gender stereotypes, such as the association between between the words receptionist and female, while
maintaining desired associations such as between the words queen and female. we de   ne metrics to
quantify both direct and indirect gender biases in embeddings, and develop algorithms to    debias    the
embedding. using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate
that our algorithms signi   cantly reduce gender bias in embeddings while preserving the its useful properties
such as the ability to cluster related concepts and to solve analogy tasks. the resulting embeddings can
be used in applications without amplifying gender bias.

introduction

1
there have been hundreds or thousands of papers written about id27s and their applications,
from web search [27] to parsing curriculum vitae [16]. however, none of these papers have recognized how
blatantly sexist the embeddings are and hence risk introducing biases of various types into real-world systems.
a id27 that represent each word (or common phrase) w as a d-dimensional word vector
(cid:126)w     rd. id27s, trained only on word co-occurrence in text corpora, serve as a dictionary of sorts
for computer programs that would like to use word meaning. first, words with similar semantic meanings
tend to have vectors that are close together. second, the vector di   erences between words in embeddings
have been shown to represent relationships between words [32, 26]. for example given an analogy puzzle,
   man is to king as woman is to x    (denoted as man:king :: woman:x), simple arithmetic of the embedding
vectors    nds that x=queen is the best answer because:

         man                    woman              

king                 queen

similarly, x=japan is returned for paris:france :: tokyo:x. it is surprising that a simple vector arithmetic
can simultaneously capture a variety of relationships. it has also excited practitioners because such a tool
could be useful across applications involving natural language. indeed, they are being studied and used
in a variety of downstream applications (e.g., document ranking [27], id31 [18], and question
retrieval [22]).

however, the embeddings also pinpoint sexism implicit in text. for instance, it is also the case that:

         man                    woman                                                        

computer programmer                                
homemaker.

1

1. homemaker
4. librarian
7. nanny
10. housekeeper

extreme she occupations
2. nurse
5. socialite
8. bookkeeper
11. interior designer

3. receptionist
6. hairdresser
9. stylist
12. guidance counselor

1. maestro
4. philosopher
7.    nancier
10. magician

extreme he occupations
2. skipper
5. captain
8. warrior
11.    gher pilot

3. protege
6. architect
9. broadcaster
12. boss

figure 1: the most extreme occupations as projected on to the she   he gender direction on g2vnews.
occupations such as businesswoman, where gender is suggested by the orthography, were excluded.

sewing-carpentry
nurse-surgeon
blond-burly
giggle-chuckle
sassy-snappy
volleyball-football

gender stereotype she-he analogies.
register-nurse-physician
interior designer-architect
feminism-conservatism
vocalist-guitarist
diva-superstar
cupcakes-pizzas

housewife-shopkeeper
softball-baseball
cosmetics-pharmaceuticals
petite-lanky
charming-a   able
hairdresser-barber

queen-king
waitress-waiter

gender appropriate she-he analogies.

sister-brother
ovarian cancer-prostate cancer

mother-father
convent-monastery

figure 2: analogy examples. examples of automatically generated analogies for the pair she-he using the
procedure described in text. for example, the    rst analogy is interpreted as she:sewing :: he:carpentry in the
original w2vnews embedding. each automatically generated analogy is evaluated by 10 crowd-workers are
to whether or not it re   ects gender stereotype. top: illustrative gender stereotypic analogies automatically
generated from w2vnews, as rated by at least 5 of the 10 crowd-workers. bottom: illustrative generated
gender-appropriate analogies.

softball extreme
1. pitcher
2. bookkeeper
3. receptionist
4. registered nurse
5. waitress

football extreme
1. footballer
2. businessman
3. pundit
4. maestro
5. cleric

gender portion after debiasing
-1%
20%
67%
29%
35%

1. pitcher
2. in   elder
3. major leaguer
4. bookkeeper
5. investigator

gender portion after debiasing
2%
31%
10%
42%
2%

1. footballer
2. cleric
3. vice chancellor
4. lecturer
5. mid   elder

figure 3: example of indirect bias. the    ve most extreme occupations on the softball-football axis, which
indirectly captures gender bias. for each occupation, the degree to which the association represents a gender
bias is shown, as described in section 5.3.

2

in other words, the same system that solved the above reasonable analogies will o   ensively answer    man is to
computer programmer as woman is to x    with x=homemaker. similarly, it outputs that a father is to a doctor
as a mother is to a nurse. the primary embedding studied in this paper is the popular publicly-available
id97 [24, 25] embedding trained on a corpus of google news texts consisting of 3 million english words
and terms into 300 dimensions, which we refer to here as the w2vnews. one might have hoped that
the google news embedding would exhibit little gender bias because many of its authors are professional
journalists. we also analyze other publicly available embeddings trained via other algorithms and    nd similar
biases.

in this paper, we will quantitatively demonstrate that word-embeddings contain biases in their geometry
that re   ect gender stereotypes present in broader society. due to their wide-spread usage as basic features,
id27s not only re   ect such stereotypes but can also amplify them. this poses a signi   cant risk
and challenge for machine learning and its applications.

to illustrate bias ampli   cation, consider bias present in the task of retrieving relevant web pages for
a given query. in web search, one recent project has shown that, when carefully combined with existing
approaches, word vectors have the potential to improve web page relevance results [27]. as an example,
suppose the search query is cmu computer science phd student for a computer science ph.d. student at
carnegie mellon university. now, the directory1 o   ers 127 nearly identical web pages for students     these
pages di   er only in the names of the students. a id27   s semantic knowledge can improve relevance
by identifying, for examples, that the terms graduate research assistant and phd student are related. however,
id27s also rank terms related to computer science closer to male names than female names (e.g.,
the embeddings give john:computer programmer :: mary:homemaker). the consequence is that, between two
pages that di   er only in the names mary and john, the id27 would in   uence the search engine to
rank john   s web page higher than mary. in this hypothetical example, the usage of id27 makes it
even harder for women to be recognized as computer scientists and would contribute to widening the existing
gender gap in computer science. while we focus on gender bias, speci   cally female-male (f-m) bias, the
approach may be applied to other types of bias.

uncovering gender stereotypes from text may seem like a trivial matter of counting pairs of words that
occur together. however, such counts are often misleading [14]. for instance, the term male nurse is several
times more frequent than female nurse (similarly female quarterback is many times more frequent than male
quarterback). hence, extracting associations from text, f-m or otherwise, is not simple, and       rst-order   
approaches would predict that the word nurse is more male than quarterback. more generally, gordon and
van durme show how reporting bias [14], including the fact that common assumptions are often left unsaid,
         
poses a challenge to extracting knowledge from raw text. nonetheless,             nurse is closer to
male,
suggesting that id27s may be capable of circumventing reporting bias in some cases. this happens
because id27s are trained using second-order methods which require large amounts of data to
extract associations and relationships about words.

               
female than

the analogies generated from these embeddings spell out the bias implicit in the data on which they were
trained. hence, id27s may serve as a means to extract implicit gender associations from a large
text corpus similar to how implicit association tests [15] detect automatic gender associations possessed by
people, which often do not align with self reports.
to quantify bias, we compare a id27 to the embeddings of a pair of gender-speci   c words. for
instance, the fact that             nurse is close to                woman is not in itself necessarily biased (it is also somewhat close to
         man     all are humans), but the fact that these distances are unequal suggests bias. to make this rigorous,
consider the distinction between gender speci   c words that are associated with a gender by de   nition, and
the remaining gender neutral words. standard examples of gender speci   c words include brother, sister,
                  
brother is closer to          man than to                woman is expected since
businessman and businesswoman. the fact that
they share the de   nitive feature of relating to males. we will use the gender speci   c words to learn a gender
subspace in the embedding, and our debiasing algorithm removes the bias only from the gender neutral words
while respecting the de   nitions of these gender speci   c words.

we refer to this type of bias, where there is an association between a gender neutral word and a clear

1graduate research assistants listed at http://cs.cmu.edu/directory/csd.

3

gender pair as direct bias. we also consider a notion of indirect bias,2 which manifests as associations between
gender neutral words that are clearly arising from gender. for instance, the fact that the word receptionist is
much closer to softball than football may arise from female associations with both receptionist and softball.
note that many pairs of male-biased (or female-biased) words have legitimate associations having nothing to
do with gender. for instance, while the words mathematician and geometry both have a strong male bias,
their similarity is justi   ed by factors other than gender. more often than not, associations are combinations
of gender and other factors that can be di   cult to disentangle. nonetheless, we can use the geometry of the
id27 to determine the degree to which those associations are based on gender.
aligning biases with stereotypes. stereotypes are biases that are widely held among a group of people.
we show that the biases in the id27 are in fact closely aligned with social conception of gender
stereotype, as evaluated by u.s.-based crowd workers on amazon   s mechanical turk.3 the crowd agreed
               
doctor closer to          man than to                woman) as well as
that the biases re   ected both in the location of vectors (e.g.
in analogies (e.g., he:coward :: she:whore) exhibit common gender stereotypes.
debiasing. our goal is to reduce gender biases in the id27 while preserving the useful properties
of the embedding. surprisingly, not only does the embedding capture bias, but it also contains su   cient
information to reduce this bias, as illustrated in 7. we will leverage the fact that there exists a low dimensional
subspace in the embedding that empirically captures much of the gender bias. the goals of debiasing are:

1. reduce bias:

(a) ensure that gender neutral words such as nurse are equidistant between gender pairs such as he

and she.

(b) reduce gender associations that pervade the embedding even among gender neutral words.

2. maintain embedding utility:

(a) maintain meaningful non-gender-related associations between gender neutral words, including
associations within stereotypical categories of words such as fashion-related words or words
associated with football.

(b) correctly maintain de   nitional gender associations such as between man and father.

paper outline. after discussing related literature, we give preliminaries necessary for understanding the
paper in section 3. next we propose methods to identify the gender bias of an embedding and show that
w2vnews exhibits bias which is aligned with common gender stereotypes (section 4). in section 5, we de   ne
several simple geometric properties associated with bias, and in particular discuss how to identify the gender
subspace. using these geometric properties, we introduce debiasing algorithms (section 6) and demonstrate
their performance (section 8). finally we conclude with additional discussions of related literature, other
types of biases in the embedding and future works.

2 related work
related work can be divided into relevant literature on bias in language and bias in algorithms.

2the terminology indirect bias follows pedreshi et al. [29] who distinguish direct versus indirect discrimination in rules of fair
classi   ers. direct discrimination involves directly using sensitive features such as gender or race, whereas indirect discrimination
involves using correlates that are not inherently based on sensitive features but that, intentionally or unintentionally, lead to
disproportionate treatment nonetheless.

3http://mturk.com

4

2.1 gender bias and stereotype in english
it is important to quantify and understand bias in languages as such biases can reinforce the psychological
status of di   erent groups [33]. gender bias in language has been studied over a number of decades in a
variety of contexts (see, e.g., [17]) and we only highlight some of the    ndings here. biases di   er across people
though commonalities can be detected. implicit association tests [15] have uncovered gender-word biases
that people do not self-report and may not even be aware of. common biases link female terms with liberal
arts and family and male terms with science and careers [28]. bias is seen in word morphology, i.e., the fact
that words such as actor are, by default, associated with the dominant class [19], and female versions of these
words, e.g., actress, are marked. there is also an imbalance in the number of words with f-m with various
associations. for instance, while there are more words referring to males, there are many more words that
sexualize females than males [35].

glick and fiske [13] introduce the notion of benevolent sexism in which women are perceived with positive
traits such as helpful or intimacy-seeking. despite its seemingly positive nature, benevolent sexism can be
harmful, insulting, and discriminatory. in terms of words, female gender associations with any word, even
a subjectively positive word such as attractive, can cause discrimination against women if it reduces their
association with other words, such as professional.

stereotypes, as mentioned, are biases that are widely held within a group. while gender bias of any
kind is concerning, stereotypes are often easier to study due to their consistent nature. stereotypes have
commonalities across cultures, though there is some variation between cultures [5]. complimentary stereotypes
are common between females and males, in which each gender is associated with strengths that are perceived
to o   set its own weaknesses and compliment the strengths of the other gender [20]. these and compensatory
stereotypes are used by people to justify the status quo.

consistent biases have been studied within online contexts and speci   cally related to the contexts we
study such as online news (e.g., [31]), web search (e.g., [21]), and wikipedia (e.g., [39]). in wikipedia, wager
et al. [39] found that, as suggested by prior work on gender bias in language [2], articles about women more
often emphasize their gender, their husbands and their husbands    jobs, and other topics discussed consistently
less often than in articles about men. regarding individual words, they    nd that certain words are predictive
of gender, e.g., husband appears signi   cantly more often in articles about women while baseball occurs more
often in articles about men.

2.2 bias within algorithms
a number of online systems have been shown to exhibit various biases, such as racial discrimination and
gender bias in the ads presented to users [36, 6]. a recent study found that algorithms used to predict repeat
o   enders exhibit indirect racial biases [1]. di   erent demographic and geographic groups also use di   erent
dialects and word-choices in social media [8]. an implication of this e   ect is that language used by minority
group might not be able to be processed by natural language tools that are trained on    standard    data-sets.
biases in the curation of machine learning data-sets have explored in [37, 4].

independent from our work, schmidt [34] identi   ed the bias present in id27s and proposed
debiasing by entirely removing multiple gender dimensions, one for each gender pair. his goal and approach,
similar but simpler than ours, was to entirely remove gender from the embedding. there is also an intense
research agenda focused on improving the quality of id27s from di   erent angles (e.g., [23, 30, 40, 9]),
and the di   culty of evaluating embedding quality (as compared to supervised learning) parallels the di   culty
of de   ning bias in an embedding.

within machine learning, a body of notable work has focused on    fair    binary classi   cation in particular.
a de   nition of fairness based on legal traditions is presented by barocas and selbst [3]. approaches to modify
classi   cation algorithms to de   ne and achieve various notions of fairness have been described in a number of
works, see, e.g., [3, 7, 10] and a recent survey [41].

feldman et al. [10] distinguish classi   cation algorithms that achieve fairness by modifying the underlying
data from those that achieve fairness by modifying the classi   cation algorithm. our approach is more
similar to the former. however, it is unclear how to apply any of these previous approaches without a clear

5

classi   cation task in hand, and the problem is exacerbated by indirect bias.

this prior work on algorithmic fairness is largely for supervised learning. fair classi   cation is de   ned based
on the fact that algorithms were classifying a set of individuals using a set of features with a distinguished
sensitive feature. in id27s, there are no clear individuals and no a priori de   ned classi   cation
problem. however, similar issues arise, such as direct and indirect bias [29].

3 preliminaries
we    rst very brie   y de   ne an embedding and some terminology. an embedding consists of a unit vector
(cid:126)w     rd, with (cid:107) (cid:126)w(cid:107) = 1, for each word (or term) w     w . we assume there is a set of gender neutral words
n     w , such as    ight attendant or shoes, which, by de   nition, are not speci   c to any gender. we denote the
size of a set s by |s|. we also assume we are given a set of f-m gender pairs p     w    w , such as she-he
or mother-father whose de   nitions di   er mainly in gender. section 7 discusses how n and p can be found
within the embedding itself, but until then we take them as given.
as is common, similarity between words w1 and w2 is measured by their inner product, (cid:126)w1    (cid:126)w2. finally,
we will abuse terminology and refer to the embedding of a word and the word interchangeably. for example,
the statement cat is more similar to dog than to cow means       cat          
dog           cat             cow. for arbitrary vectors u and
v, de   ne:

cos(u, v) =

u    v
(cid:107)u(cid:107)(cid:107)v(cid:107) .

this normalized similarity between vectors u and v is written as cos because it is the cosine of the angle
between the two vectors. since words are normalized cos( (cid:126)w1, (cid:126)w2) = (cid:126)w1    (cid:126)w2.
embedding. unless otherwise stated, the embedding we refer to in this paper is the aforementioned
w2vnews embedding, a d = 300-dimensional id97 [24, 25] embedding, which has proven to be
immensely useful since it is high quality, publicly available, and easy to incorporate into any application. in
particular, we downloaded the pre-trained embedding on the google news corpus,4 and normalized each
word to unit length as is common. starting with the 50,000 most frequent words, we selected only lower-case
words and phrases consisting of fewer than 20 lower-case characters (words with upper-case letters, digits, or
punctuation were discarded). after this    ltering, 26,377 words remained. while we focus on w2vnews, we
show later that gender stereotypes are also present in other embedding data-sets.
crowd experiments. all human experiments were performed on the amazon mechanical turk crowdsourc-
ing platform. we selected for u.s.-based workers to maintain homogeneity and reproducibility to the extent
possible with id104. two types of experiments were performed: ones where we solicited words from
the crowd (to see if the embedding biases contain those of the crowd) and ones where we solicited ratings
on words or analogies generated from our embedding (to see if the crowd   s biases contain those from the
embedding). these two types of experiments are analogous to experiments performed in rating results in
information retrieval to evaluate precision and recall. when we speak of the majority of 10 crowd judgments,
we mean those annotations made by 5 or more independent workers.

since gender associations vary by culture and person, we ask for ratings of stereotypes rather than bias. in
addition to possessing greater consistency than biases, people may feel more comfortable rating the stereotypes
of their culture than discussing their own gender biases. the appendix contains the questionnaires that were
given to the crowd-workers to perform these tasks.

4 gender stereotypes in id27s
our    rst task is to understand the biases present in the word-embedding (i.e. which words are closer to
she than to he, etc.) and the extent to which these geometric biases agree with human notion of gender
stereotypes. we use two simple methods to approach this problem: 1) evaluate whether the embedding has

4https://code.google.com/archive/p/id97/

6

figure 4: comparing the bias of two di   erent embeddings   the w2vnews and the glove web-crawl embedding.
in each embedding, the occupation words are projected onto the she-he direction. each dot corresponds
to one occupation word; the gender bias of occupations is highly consistent across embeddings (spearman
   = 0.81).

stereotypes on occupation words and 2) evaluate whether the embedding produces analogies that are judged
to re   ect stereotypes by humans. the exploratory analysis of this section will motivate the more rigorous
metrics used in the next two sections.

occupational stereotypes. figure 1 lists the occupations that are closest to she and to he in the
w2vnews embeddings. we asked the crowdworkers to evaluate whether an occupation is considered female-
stereotypic, male-stereotypic, or neutral. each occupation word was evaluated by ten crowd-workers as to
whether or not it re   ects gender stereotype. hence, for each word we had a integer rating, on a scale of 0-10,
of stereotypicality. the projection of the occupation words onto the she-he axis is strongly correlated with
the stereotypicality estimates of these words (spearman    = 0.51), suggesting that the geometric biases of
embedding vectors is aligned with crowd judgment of gender stereotypes. we used occupation words here
because they are easily interpretable by humans and often capture common gender stereotypes. other word
sets could be used for this task. also note that we could have used other words, e.g. woman and man, as the
gender-pair in the task. we chose she and he because they are frequent and do not have fewer alternative
word senses (e.g., man can also refer to mankind).

we projected each of the occupations onto the she-he direction in the w2vnews embedding as well as a
di   erent embedding generated by the glove algorithm on a web-crawl corpus [30]. the results are highly
consistent (figure 4), suggesting that gender stereotypes is prevalent across di   erent embeddings and is not
an artifact of the particular training corpus or methodology of id97.

analogies exhibiting stereotypes. analogies are a useful way to both evaluate the quality of a word
embedding and also its stereotypes. we    rst brie   y describe how the embedding generate analogies and then
discuss how we use analogies to quantify gender stereotype in the embedding. a more detailed discussion of
our algorithm and prior analogy solvers is given in appendix a.

in the standard analogy tasks, we are given three words, for example he, she, king, and look for the 4th
word to solve he to king is as she to x. here we modify the analogy task so that given two words, e.g. he,
she, we want to generate a pair of words, x and y, such that he to x as she to y is a good analogy. this
modi   cation allows us to systematically generate pairs of words that the embedding believes it analogous to
he, she (or any other pair of seed words).
the input into our analogy generator is a seed pair of words (a, b) determining a seed direction (cid:126)a     (cid:126)b
corresponding to the normalized di   erence between the two seed words. in the task below, we use (a, b) =

7

(she, he). we then score all pairs of words x, y by the following metric:

(cid:40)

(cid:16)

cos

0

(cid:17)

s(a,b)(x, y) =

(cid:126)a     (cid:126)b, (cid:126)x     (cid:126)y

if (cid:107)(cid:126)x     (cid:126)y(cid:107)       
otherwise

(1)

where    is a threshold for similarity. the intuition of the scoring metric is that we want a good analogy
pair to be close to parallel to the seed direction while the two words are not too far apart in order to be
semantically coherent. the parameter    sets the threshold for semantic similarity. in all the experiments,
we take    = 1 as we    nd that this choice often works well in practice. since all embeddings are normalized,
this threshold corresponds to an angle       /3, indicating that the two words are closer to each other than
they are to the origin. in practice, it means that the two words forming the analogy are signi   cantly closer
together than two random embedding vectors. given the embedding and seed words, we output the top
analogous pairs with the largest positive s(a,b) scores. to reduce redundancy, we do not output multiple
analogies sharing the same word x.

since analogies, stereotypes, and biases are heavily in   uenced by culture, we employed u.s. based crowd-
workers to evaluate the analogies output by the analogy generating algorithm described above. for each
analogy, we asked the workers two yes/no questions: (a) whether the pairing makes sense as an analogy, and
(b) whether it re   ects a gender stereotype. every analogy is judged by 10 workers, and we used the number
of workers that rated this pair as stereotyped to quantify the degree of bias of this analogy. overall, 72 out of
150 analogies were rated as gender-appropriate by    ve or more crowd-workers, and 29 analogies were rated
as exhibiting gender stereotype by    ve or more crowd-workers (figure 8). examples of analogies generated
from w2vnews that were rated as stereotypical are shown at the top of figure 2, and examples of analogies
that make sense and are rated as gender-appropriate are shown at the bottom of figure 2. the full list of
analogies and crowd ratings are in appendix g.

indirect gender bias. the direct bias analyzed above manifests in the relative similarities between gender-
speci   c words and gender neutral words. gender bias could also a   ect the relative geometry between gender
neutral words themselves. to test this indirect gender bias, we take pairs of words that are gender-neutral, for
example softball and football. we project all the occupation words onto the
football direction and
looked at the extremes words, which are listed in figure 3. for instance, the fact that the words bookkeeper
and receptionist are much closer to softball than football may result indirectly from female associations with
bookkeeper, receptionist and softball. it   s important to point out that that many pairs of male-biased (or
female-biased) words have legitimate associations having nothing to do with gender. for example, while both
footballer and football have strong male biases, their similarity is justi   ed by factors other than gender. in
section 5, we de   ne a metric to more rigorously quantify these indirect e   ects of gender bias.

               
softball                       

5 geometry of gender and bias
in this section, we study the bias present in the embedding geometrically, identifying the gender direction
and quantifying the bias independent of the extent to which it is aligned with the crowd bias. we develop
metrics of direct and indirect bias that more rigorously quantify the observations of the previous section.

identifying the gender subspace

5.1
language use is    messy    and therefore individual word pairs do not always behave as expected. for instance,
the word man has several di   erent usages: it may be used as an exclamation as in oh man! or to refer to
people of either gender or as a verb, e.g., man the station. to more robustly estimate bias, we shall aggregate
he and                woman              man, we
across multiple paired comparisons. by combining several directions, such as
identify a gender direction g     rd that largely captures gender in the embedding. this direction helps us
to quantify direct and indirect biases in words and associations.

      
she           

8

she         
      
he
      
her         
his
               woman            man
            
mary               
john
               
herself                  
himself

stereo.

def.
92% 89%
84% 87%
90% 83%
75% 87%
93% 89%

                     
daughter         son
               
mother               
father
      
gal            guy
girl            
      
boy
               
female            
male

stereo.

def.
93% 91%
91% 85%
85% 85%
90% 86%
84% 75%

figure 5: ten possible word pairs to de   ne gender, ordered by word frequency, along with agreement with
two sets of 100 words solicited from the crowd, one with de   nitional and and one with stereotypical gender
associations. for each set of words, comprised of the most frequent 50 female and 50 male crowd suggestions,
the accuracy is shown for the corresponding gender classi   er based on which word is closer to a target word,
e.g., the she-he classi   er predicts a word is female if it is closer to she than he. with roughly 80-90% accuracy,
the gender pairs predict the gender of both stereotypes and de   nitionally gendered words solicited from the
crowd.

in english as in many languages, there are numerous gender pair terms, and for each we can consider
the di   erence between their embeddings. before looking at the data, one might imagine that they all had
roughly the same vector di   erences, as in the following caricature:
      
         
gal
wise +
         
wise +          guy
      
gal              guy = g

                              
grandmother =
                           
grandfather =
                              
grandmother                                
grandfather =

in any    nite sample will also lead to di   erences. figure 5 illustrates ten possible gender pairs,(cid:8)(xi, yi)(cid:9)10

however, gender pair di   erences are not parallel in practice, for multiple reasons. first, there are di   erent
biases associated with with di   erent gender pairs. second is polysemy, as mentioned, which in this case occurs
due to the other use of grandfather as in to grandfather a regulation. finally, randomness in the word counts
i=1.
we experimentally veri   ed that the pairs of vectors corresponding to these words do agree with the
crowd concept of gender. on amazon mechanical turk, we asked crowdworkers to generate two lists of
words: one list corresponding to words that they think are gendered by de   nition (waitress, menswear) and a
separate list corresponding to words that they believe captures gender stereotypes (e.g., sewing, football).
from this we generated the most frequently suggested 50 male and 50 female words for each list to be used
      
he, we say that it accurately classi   es a
for a classi   cation task. for each candidate pair, for example
      
crowd suggested female de   nition (or stereotype) word if that word vector is closer to
he. table 5
reports the classi   cation accuracy for de   nition and stereotype words for each gender pair. the accuracies
are high, indicating that these pairs capture the intuitive notion of gender.

      
she than to

      
she,

to identify the gender subspace, we took the ten gender pair di   erence vectors and computed its principal
components (pcs). as figure 6 shows, there is a single direction that explains the majority of variance
in these vectors. the    rst eigenvalue is signi   cantly larger than the rest. note that, from the randomness
in a    nite sample of ten noisy vectors, one expects a decrease in eigenvalues. however, as also illustrated
in 6, the decrease one observes due to random sampling is much more gradual and uniform. therefore we
hypothesize that the top pc, denoted by the unit vector g, captures the gender subspace. in general, the
gender subspace could be higher dimensional and all of our analysis and algorithms (described below) work
with general subspaces.

5.2 direct bias
to measure direct bias, we    rst identify words that should be gender-neutral for the application in question.
how to generate this set of gender-neutral words is described in section 7. given the gender neutral words,
denoted by n, and the gender direction learned from above, g, we de   ne the direct gender bias of an

9

figure 6: left: the percentage of variance explained in the pca of these vector di   erences (each di   erence
normalized to be a unit vector). the top component explains signi   cantly more variance than any other.
right: for comparison, the corresponding percentages for random unit vectors (   gure created by averaging
over 1,000 draws of ten random unit vectors in 300 dimensions).

embedding to be

directbiasc =

(cid:88)

w   n

1
|n|

|cos( (cid:126)w, g)|c

where c is a parameter that determines how strict do we want to in measuring bias.
if c is 0, then
|cos( (cid:126)w     g)|c = 0 only if (cid:126)w has no overlap with g and otherwise it is 1. such strict measurement of bias might
be desirable in settings such as the college admissions example from the introduction, where it would be
unacceptable for the embedding to introduce a slight preference for one candidate over another by gender. a
more gradual bias would be setting c = 1. the presentation we have chosen favors simplicity     it would be
natural to extend our de   nitions to weight words by frequency. for example, in w2vnews, if we take n to
be the set of 327 occupations, then directbias1 = 0.08, which con   rms that many occupation words have
substantial component along the gender direction.

indirect bias

5.3
unfortunately, the above de   nitions still do not capture indirect bias. to see this, imagine completely
removing from the embedding both words in gender pairs (as well as words such as beard or uterus that are
arguably gender-speci   c but which cannot be paired). there would still be indirect gender association in that
a word that should be gender neutral, such as receptionist, is closer to softball than football (see figure 3).
as discussed in the introduction, it can be subtle to obtain the ground truth of the extent to which such
similarities is due to gender.
the gender subspace g that we have identi   ed allows us to quantify the contribution of g to the similarities
between any pair of words. we can decompose a given word vector w     rd as w = wg + w   , where
wg = (w   g)g is the contribution from gender and w    = w    wg. note that all the word vectors are normalized
to have unit length. we de   ne the gender component to the similarity between two word vectors w and v as

(cid:18)

  (w, v) =

(cid:19)(cid:30)

w    v.

w    v     w       v   
(cid:107)w   (cid:107)2(cid:107)v   (cid:107)2
w     v   

the intuition behind this metric is as follow:

is the inner product between the two vectors if
we project out the gender subspace and renormalize the vectors to be of unit length. the metric quanti   es
how much this inner product changes (as a fraction of the original inner product value) due to this operation
of removing the gender subspace. because of noise in the data, every vector has some non-zero component
w    and    is well-de   ned. note that   (w, w) = 0, which is reasonable since the similarity of a word to itself
should not depend on gender contribution. if wg = 0 = vg, then   (w, v) = 0; and if w    = 0 = v   , then
  (w, v) = 1.

(cid:107)w   (cid:107)2(cid:107)v   (cid:107)2

in figure 3, as a case study, we examine the most extreme words on the

football direction.
the    ve most extreme words (i.e. words with the highest positive or the lowest negative projections onto

               
softball                       

10

figure 7: selected words projected along two axes: x is a projection onto the di   erence between the
embeddings of the words he and she, and y is a direction learned in the embedding that captures gender
neutrality, with gender neutral words above the line and gender speci   c words below the line. our hard
debiasing algorithm removes the gender pair associations for gender neutral words. in this    gure, the words
above the horizontal line would all be collapsed to the vertical line.
               
softball                       
football) are shown in the table. words such as receptionist, waitress and homemaker are closer to
softball than football, and the      s between these words and softball is substantial (67%, 35%, 38%, respectively).
               
this suggests that the apparent similarity in the embeddings of these words to
softball can be largely explained
by gender biases in the embedding. similarly, businessman and maestro are closer to football and this can
also be attributed largely to indirect gender bias, with      s of 31% and 42%, respectively.

6 debiasing algorithms
the debiasing algorithms are de   ned in terms of sets of words rather than just pairs, for generality, so that
we can consider other biases such as racial or religious biases. we also assume that we have a set of words to
neutralize, which can come from a list or from the embedding as described in section 7. (in many cases it
may be easier to list the gender speci   c words not to neutralize as this set can be much smaller.)

the    rst step, called identify gender subspace, is to identify a direction (or, more generally, a subspace)
of the embedding that captures the bias. for the second step, we de   ne two options: neutralize and
equalize or soften. neutralize ensures that gender neutral words are zero in the gender subspace.
equalize perfectly equalizes sets of words outside the subspace and thereby enforces the property that any
neutral word is equidistant to all words in each equality set. for instance, if {grandmother, grandfather} and
{guy, gal} were two equality sets, then after equalization babysit would be equidistant to grandmother and
grandfather and also equidistant to gal and guy, but presumably closer to the grandparents and further from
the gal and guy. this is suitable for applications where one does not want any such pair to display any bias
with respect to neutral words.

the disadvantage of equalize is that it removes certain distinctions that are valuable in certain applications.
for instance, one may wish a language model to assign a higher id203 to the phrase to grandfather a
regulation) than to grandmother a regulation since grandfather has a meaning that grandmother does not    
equalizing the two removes this distinction. the soften algorithm reduces the di   erences between these sets

11

heshegeniusbrilliantpriesthomemakerfeministdivorcedraftedearringsbeautifuldressdancersmodelingcraftsdancerbuddiesgurusewingcockypearlsdancesalonfirepowerultrasoundwitchwitchessassybuildertacticalbuddyburlytanningtrimestermatescrimmagepageantbabecommandtotevasesrulecommitthighsjourneymanbrotherssistersqueenbeardbreastschaparrivalbrowsingactressesfiancenuclearsecondscauseddropseekinglookssubjectvotersfriendpartssitesyardhousingvictimsgovernorboysheavyslowuserfirmsbusyhopedtieslettersidentityfolksquitsharplysonsgirlfrienddaughterslobbygrandmothercousinladiescaketreatswivesnephewbrassrosesdaddyfianceedadsreelgirlfriendsfrostlustboyhoodgalsgamewifeladilliipalhayvirdsonwhile maintaining as much similarity to the original embedding as possible, with a parameter that controls
this trade-o   .
to de   ne the algorithms, it will be convenient to introduce some further notation. a subspace b is de   ned
by k orthogonal unit vectors b = {b1, . . . , bk}     rd. in the case k = 1, the subspace is simply a direction.
we denote the projection of a vector v onto b by,

k(cid:88)

j=1

vb =

(v    bj)bj.

this also means that v     vb is the projection onto the orthogonal subspace.
step 1: identify gender subspace. inputs: word sets w , de   ning sets d1, d2, . . . , dn     w as well as

embedding(cid:8) (cid:126)w     rd(cid:9)

w   w and integer parameter k     1. let
(cid:126)w/|di|

(cid:88)

  i :=

w   di

be the means of the de   ning sets. let the bias subspace b be the    rst k rows of svd(c) where

n(cid:88)

(cid:88)

i=1

w   di

c :=

( (cid:126)w       i)t ( (cid:126)w       i)(cid:14)|di|.

step 2a: hard de-biasing (neutralize and equalize). additional inputs: words to neutralize n     w ,
family of equality sets e = {e1, e2, . . . , em} where each ei     w . for each word w     n, let (cid:126)w be re-embedded
to

(cid:126)w := ( (cid:126)w     (cid:126)wb)(cid:14)(cid:107) (cid:126)w     (cid:126)wb(cid:107).

for each set e     e, let

(cid:88)

   :=

w/|e|

for each w     e, (cid:126)w :=    +(cid:112)1     (cid:107)  (cid:107)2 (cid:126)wb       b

:=          b

  

w   e

(cid:107) (cid:126)wb       b(cid:107)

finally, output the subspace b and the new embedding(cid:8) (cid:126)w     rd(cid:9)

w   w .

equalize equates each set of words outside of b to their simple average    and then adjusts vectors so that
they are unit length. it is perhaps easiest to understand by thinking separately of the two components (cid:126)wb
and (cid:126)w   b = (cid:126)w     (cid:126)wb. the latter (cid:126)w   b are all simply equated to their average. within b, they are centered
(moved to mean 0) and then scaled so that each (cid:126)w is unit length. to motivate why we center, beyond the
fact that it is common in machine learning, consider the bias direction being the gender direction (k = 1) and
a gender pair such as e = {male, female}. as discussed, it so happens that both words are positive (female)
in the gender direction, though female has a greater projection. one can only speculate as to why this is
the case, e.g., perhaps the frequency of text such as male nurse or male escort or she was assaulted by the
male. however, because female has a greater gender component, after centering the two will be symmetrically
balanced across the origin. if instead, we simply scaled each vector   s component in the bias direciton without
centering, male and female would have exactly the same embedding and we would lose analogies such as
father:male :: mother:female.

before de   ning the soften alternative step, we note that neutralizing and equalizing completely remove

pair bias.

e1, e2     e, (cid:126)w    (cid:126)e1 = w    (cid:126)e2 and (cid:107) (cid:126)w     (cid:126)e1(cid:107) = (cid:107) (cid:126)w     (cid:126)e2(cid:107). furthermore, if e =(cid:8){x, y}|(x, y)     p(cid:9) are the sets of

observation 1. after steps 1 and 2a, for any gender neutral word w any equality set e, and any two words

pairs de   ning pairbias, then pairbias = 0.

12

proof. step 1 ensures that (cid:126)wb = 0, while step 2a ensures that (cid:126)e1     vece2 lies entirely in b. hence, their inner
product is 0 and (cid:126)w  (cid:126)e1 = w  (cid:126)e2. lastly, (cid:107) (cid:126)w   (cid:126)e1(cid:107) = (cid:107) (cid:126)w   (cid:126)e2(cid:107) follows from the fact that (cid:107)u1    u2(cid:107)2 = 2    2u1   u2
for unit vectors u1, u2 and pairbias being 0 follows trivially from the de   nition of pairbias.

step 2b: soft bias correction. overloading the notation, we let w     rd  |vocab| denote the matrix of
all embedding vectors and n denote the matrix of the embedding vectors corresponding to gender neutral
words. w and n are learned from some corpus and are inputs to the algorithm. the desired debiasing
transformation t     rd  d is a linear transformation that seeks to preserve pairwise inner products between
all the word vectors while minimizing the projection of the gender neutral words onto the gender subspace.
this can be formalized as the following optimization problem

(cid:107)(t w )t (t w )     w t w(cid:107)2

f +   (cid:107)(t n )t (t b)(cid:107)2

f

min

t

where b is the gender subspace learned in step 1 and    is a tuning parameter that balances the objective
of preserving the original embedding inner products with the goal of reducing gender bias. for    large, t
would remove the projection onto b from all the vectors in n, which corresponds exactly to step 2a. in
the experiment, we use    = 0.2. the optimization problem is a semi-de   nite program and can be solved
e   ciently. the output embedding is normalized to have unit length,   w = {t w/(cid:107)t w(cid:107)2, w     w}.

7 determining gender neutral words
for practical purposes, since there are many fewer gender speci   c words, it is more e   cient to enumerate
the set of gender speci   c words s and take the gender neutral words to be the compliment, n = w \ s.
using dictionary de   nitions, we derive a subset s0 of 218 words out of the words in w2vnews. recall that
this embedding is a subset of 26,377 words out of the full 3 million words in the embedding, as described in
section 3. this base list s0 is given in appendix c. note that the choice of words is subjective and ideally
should be customized to the application at hand.

we generalize this list to the entire 3 million words in the google news embedding using a linear classi   er,
resulting in the set s of 6,449 gender-speci   c words. more speci   cally, we trained a linear support vector
machine (id166) with the default id173 parameter of c = 1.0. we then ran this classi   er on the
remaining words, taking s = s0     s1, where s1 were the words labeled as gender speci   c by our classi   er
among the words in the entire embedding that were not in the 26,377 words of w2vnews.
using 10-fold cross-validation to evaluate the accuracy of this process, we    nd an f -score of .627    .102
based on strati   ed 10-fold cross-validation. the binary accuracy is well over 99% due to the imbalanced
nature of the classes. for another test of how accurately the embedding agrees with our base set of 218
words, we evaluate the class-balanced error by re-weighting the examples so that the positive and negative
examples have equal weights, i.e., weighting each class inverse proportionally to the number of samples from
that class. here again, we use strati   ed 10-fold cross validation to evaluate the error. within each fold,
the id173 parameter was also chosen by 10-fold (nested) cross validation. the average (balanced)
accuracy of the linear classi   ers, across folds, was 95.12%    1.46% with 95% con   dence.

figure 7 illustrates the results of the classi   er for separating gender-speci   c words from gender-neutral
words. to make the    gure legible, we show a subset of the words. the x-axis correspond to projection of
      
she           
words onto the
he direction and the y-axis corresponds to the distance from the decision boundary of
the trained id166.

8 debiasing results
we evaluated our debiasing algorithms to ensure that they preserve the desirable properties of the original
embedding while reducing both direct and indirect gender biases.

13

rg ws

analogy

before
hard-debiased
soft-debiased

62.3
62.4
62.4

54.5
54.1
54.2

57.0
57.0
56.8

table 1: the columns show the performance of the original w2vnews embedding (   before   ) and the debiased
w2vnews on the standard id74 measuring coherence and analogy-solving abilities: rg [32],
ws [12], msr-analogy [26]. higher is better. the results show that the performance does not degrade after
debiasing. note that we use a subset of vocabulary in the experiments. therefore, the performances are lower
than the previously published results.

direct bias. first we used the same analogy generation task as before: for both the hard-debiased and the
soft-debiased embeddings, we automatically generated pairs of words that are analogous to she-he and asked
crowd-workers to evaluate whether these pairs re   ect gender stereotypes. figure 8 shows the results. on the
initial w2vnews embedding, 19% of the top 150 analogies were judged as showing gender stereotypes by a
majority of the ten workers. after applying our hard debiasing algorithm, only 6% of the new embedding
were judged as stereotypical. as an example, consider the analogy puzzle, he to doctor is as she to x. the
original embedding returns x = nurse while the hard-debiased embedding    nds x = physician. moreover
the hard-debiasing algorithm preserved gender appropriate analogies such as she to ovarian cancer is as he
to prostate cancer. this demonstrates that the hard-debiasing has e   ectively reduced the gender stereotypes
in the id27. figure 8 also shows that the number of appropriate analogies remains similar as
in the original embedding after executing hard-debiasing. this demonstrates that that the quality of the
embeddings is preserved. the details results are in appendix g. soft-debiasing was less e   ective in removing
gender bias.

to further con   rms the quality of embeddings after debiasing, we tested the debiased embedding on
several standard benchmarks that measure whether related words have similar embeddings as well as how well
the embedding performs in analogy tasks. table 1 shows the results on the original and the new embeddings
and the transformation does not negatively impact the performance.

indirect bias. we also investigated how the strict debiasing algorithm a   ects indirect gender bias. because
we do not have the ground truth on the indirect e   ects of gender bias, it is challenging to quantify the
performance of the algorithm in this regard. however we do see promising qualitative improvements, as
shown in figure 3 in the softball, football example. after applying the strict debias algorithm, we repeated the
experiment and show the most extreme words in the
football direction. the most extreme words
closer to softball are now in   elder and major leaguer in addition to pitcher, which are more relevant and do
not exhibit gender bias. gender stereotypic associations such are receptionist, waitress and homemaker are
moved down the list. similarly, words that clearly show male bias, e.g. businessman, are also no longer at the
football direction are pitcher and
top of the list. note that the two most extreme words in the
footballer. the similarities between pitcher and softball and between footballer and football comes from the
actual functions of these words and hence have little gender contribution. these two words are essentially
unchanged by the debiasing algorithm.

               
softball                       

               
softball                       

9 discussion
id27s help us further our understanding of bias in language. we    nd a single direction that
largely captures gender, that helps us capture associations between gender neutral words and gender as well
as indirect inequality.the projection of gender neutral words on this direction enables us to quantify their
degree of female- or male-bias.

to reduce the bias in an embedding, we change the embeddings of gender neutral words, by removing

14

figure 8: number of stereotypical (left) and appropriate (right) analogies generated by wordembeddings
before and after debiasing.

their gender associations. for instance, nurse is moved to to be equally male and female in the direction g. in
addition, we    nd that gender-speci   c words have additional biases beyond g. for instance, grandmother and
grandfather are both closer to wisdom than gal and guy are, which does not re   ect a gender di   erence. on
the other hand, the fact that babysit is so much closer to grandmother than grandfather (more than for other
gender pairs) is a gender bias speci   c to grandmother. by equating grandmother and grandfather outside of
gender, and since we   ve removed g from babysit, both grandmother and grandfather and equally close to babysit
after debiasing. by retaining the gender component for gender-speci   c words, we maintain analogies such as
she:grandmother :: he:grandfather. through empirical evaluations, we show that our hard-debiasing algorithm
signi   cantly reduces both direct and indirect gender bias while preserving the utility of the embedding. we
have also developed a soft-embedding algorithm which balances reducing bias with preserving the original
distances, and could be appropriate in speci   c settings.

one perspective on bias in id27s is that it merely re   ects bias in society, and therefore one
should attempt to debias society rather than id27s. however, by reducing the bias in today   s
computer systems (or at least not amplifying the bias), which is increasingly reliant on id27s, in
a small way debiased id27s can hopefully contribute to reducing gender bias in society. at the
very least, machine learning should not be used to inadvertently amplify these biases, as we have seen can
naturally happen.

in speci   c applications, one might argue that gender biases in the embedding (e.g. computer programmer
is closer to he) could capture useful statistics and that, in these special cases, the original biased embeddings
could be used. however given the potential risk of having machine learning algorithms that amplify gender
stereotypes and discriminations, we recommend that we should err on the side of neutrality and use the
debiased embeddings provided here as much as possible.

minorities                    
                        

in this paper, we focus on quantifying and reducing gender bias in id27s. corpus of documents
often contain other undesirable stereotypes and these can also be re   ected in the embedding vectors. the same
w2vnews also exhibits strong racial stereotype. for example, projecting all the occupation words onto the
direction
whites, we    nd that the most extreme occupations closer to whites are parliamentarian,
advocate, deputy, chancellor, legislator, and lawyer. in contrast, the most extreme occupations at the minorites
end are butler, footballer, socialite, and crooner. it is a subtle issue to understand the direct and indirect bias
due to racial, ethnic and cultural stereotypes. an important direction of future work would be to quantify
and remove these biases.

while we focus on english id27s, it is also an interesting direction to consider how the
approach and    ndings here would apply to other languages, especially languages with grammatical gender
where the de   nitions of most nouns carry a gender marker.

15

references
[1] j. angwin, j. larson, s. mattu, and l. kirchner. machine bias: there   s software used across the country

to predict future criminals. and it   s biased against blacks., 2016.

[2] c. aschwanden. the    nkbeiner test: what matters in stories about women scientists? doublexscience,

2013.

[3] s. barocas and a. d. selbst. big data   s disparate impact. available at ssrn 2477899, 2014.

[4] e. beigman and b. b. klebanov. learning with annotation noise. in acl, 2009.

[5] a. j. cuddy, e. b. wolf, p. glick, s. crotty, j. chong, and m. i. norton. men as cultural ideals: cultural
values moderate gender stereotype content. journal of personality and social psychology, 109(4):622,
2015.

[6] a. datta, m. c. tschantz, and a. datta. automated experiments on ad privacy settings. proceedings on

privacy enhancing technologies, 2015.

[7] c. dwork, m. hardt, t. pitassi, o. reingold, and r. zemel. fairness through awareness. in innovations

in theoretical computer science conference, 2012.

[8] j. eisenstein, b. o   connor, n. a. smith, and e. p. xing. di   usion of lexical change in social media.

plos one, pages 1   13, 2014.

[9] m. faruqui, j. dodge, s. k. jauhar, c. dyer, e. hovy, and n. a. smith. retro   tting word vectors to

semantic lexicons. in naacl, 2015.

[10] m. feldman, s. a. friedler, j. moeller, c. scheidegger, and s. venkatasubramanian. certifying and

removing disparate impact. in kdd, 2015.

[11] c. fellbaum, editor. id138: an electronic lexical database. the mit press, cambridge, ma, 1998.

[12] l. finkelstein, e. gabrilovich, y. matias, e. rivlin, z. solan, g. wolfman, and e. ruppin. placing

search in context: the concept revisited. in www. acm, 2001.

[13] p. glick and s. t. fiske. the ambivalent sexism inventory: di   erentiating hostile and benevolent sexism.

journal of personality and social psychology, 70(3):491, 1996.

[14] j. gordon and b. van durme. reporting bias and knowledge extraction. automated knowledge base

construction (akbc), 2013.

[15] a. g. greenwald, d. e. mcghee, and j. l. schwartz. measuring individual di   erences in implicit
cognition: the implicit association test. journal of personality and social psychology, 74(6):1464, 1998.

[16] c. hansen, m. tosik, g. goossen, c. li, l. bayeva, f. berbain, and m. rotaru. how to get the best
word vectors for resume parsing. in snn adaptive intelligence / symposium: machine learning 2015,
nijmegen.

[17] j. holmes and m. meyerho   . the handbook of language and gender, volume 25. john wiley & sons,

2008.

[18] o.   rsoy and c. cardie. deep id56s for compositionality in language. in nips. 2014.

[19] r. jakobson, l. r. waugh, and m. monville-burston. on language. harvard univ pr, 1990.

[20] j. t. jost and a. c. kay. exposure to benevolent sexism and complementary gender stereotypes:
consequences for speci   c and di   use forms of system justi   cation. journal of personality and social
psychology, 88(3):498, 2005.

16

[21] m. kay, c. matuszek, and s. a. munson. unequal representation and gender stereotypes in image search

results for occupations. in human factors in computing systems. acm, 2015.

[22] t. lei, h. joshi, r. barzilay, t. jaakkola, a. m. katerina tymoshenko, and l. marquez. semi-supervised

question retrieval with gated convolutions. in naacl. 2016.

[23] o. levy and y. goldberg. linguistic regularities in sparse and explicit word representations. in conll,

2014.

[24] t. mikolov, k. chen, g. corrado, and j. dean. e   cient estimation of word representations in vector

space. in iclr, 2013.

[25] t. mikolov, i. sutskever, k. chen, g. s. corrado, and j. dean. distributed representations of words

and phrases and their compositionality. in nips.

[26] t. mikolov, w.-t. yih, and g. zweig. linguistic regularities in continuous space word representations.

in hlt-naacl, pages 746   751, 2013.

[27] e. nalisnick, b. mitra, n. craswell, and r. caruana. improving document ranking with dual word

embeddings. in www, april 2016.

[28] b. a. nosek, m. banaji, and a. g. greenwald. harvesting implicit group attitudes and beliefs from a

demonstration web site. group dynamics: theory, research, and practice, 6(1):101, 2002.

[29] d. pedreshi, s. ruggieri, and f. turini. discrimination-aware data mining. in proceedings of the 14th
acm sigkdd international conference on knowledge discovery and data mining, pages 560   568. acm,
2008.

[30] j. pennington, r. socher, and c. d. manning. glove: global vectors for word representation. in emnlp,

2014.

[31] k. ross and c. carter. women and news: a long and winding road. media, culture & society,

33(8):1148   1165, 2011.

[32] h. rubenstein and j. b. goodenough. contextual correlates of synonymy. communications of the acm,

8(10):627   633, 1965.

[33] e. sapir. selected writings of edward sapir in language, culture and personality, volume 342. univ of

california press, 1985.

[34] b. schmidt. rejecting the gender binary: a vector-space operation. http://bookworm.benschmidt.

org/posts/2015-10-30-rejecting-the-gender-binary.html, 2015.

[35] j. p. stanley. paradigmatic woman: the prostitute. papers in language variation, pages 303   321, 1977.

[36] l. sweeney. discrimination in online ad delivery. queue, 11(3):10, 2013.

[37] a. torralba and a. efros. unbiased look at dataset bias. in cvpr, 2012.

[38] p. d. turney. domain and function: a dual-space model of semantic relations and compositions. journal

of arti   cial intelligence research, pages 533   585, 2012.

[39] c. wagner, d. garcia, m. jadidi, and m. strohmaier. it   s a man   s wikipedia? assessing gender inequality

in an online encyclopedia. in ninth international aaai conference on web and social media, 2015.

[40] d. yogatama, m. faruqui, c. dyer, and n. a. smith. learning word representations with hierarchical

sparse coding. in icml, 2015.

[41] i. zliobaite. a survey on measuring indirect discrimination in machine learning. arxiv preprint

arxiv:1511.00148, 2015.

17

a generating analogies
we now expand on di   erent possible methods for generating (x, y) pairs, given (a, b) for generating analogies
a:x :: b:y. the    rst and simplest metric is to consider scoring an analogy by (cid:107)((cid:126)a     (cid:126)b)     ((cid:126)x     (cid:126)y). this may
be called the parallelogram approach and, for the purpose of    nding the best single y given a, b, x, it is
equivalent to the most common approach to    nding single word analogies, namely maximizing cos((cid:126)y, (cid:126)x +(cid:126)b   (cid:126)a)
called cosadd in earlier work [26] since we assume all vectors are unit length. this works well in some
cases, but a weakness can be seen that, for many triples (a, b, x), the closest word to x is y = x, i.e.,
x = arg miny (cid:107)((cid:126)a     (cid:126)b)     ((cid:126)x     (cid:126)y)(cid:107). as a result, the de   nition explicitly excludes the possibility of returning
x itself. in these cases, y is often a word very similar to x, and in most of these cases such an algorithm
produces two opposing analogies: a:x :: b:y as well as a:y :: b:x, which violates a desideratum of analogies
(see [38], section 2.2).

related issues are discussed in [38, 23], the latter of which proposes the 3cosmul objective to    nding y

given (a, b, x):

(1 + cos((cid:126)x, (cid:126)y))(1 + cos((cid:126)x,(cid:126)b)

1 + cos((cid:126)y, (cid:126)a) +  

.

max

y

the additional   is necessary so that the denominator is positive. this approach is designed for    nding a
single word y and not directly applicable for the problem of generating both x and y as the objective is not
symmetric in x and y.
in the spirit of their work, we note that a desired property is that the direction (cid:126)a    (cid:126)b should be similar (in
angle) to the direction (cid:126)x     (cid:126)y even if the magnitudes di   er. interestingly, given (a, b, x), the y that maximizes
cos((cid:126)a     (cid:126)b, (cid:126)x     (cid:126)y) is generally an extreme. for instance, for a =he and b =she, for the vast majority of words
x, the word her maximizes the expression for y. this is due to the fact that the most signi   cant di   erence
between a random word x and the word her is that her is likely much more feminine than x. since, from
a perceptual point of view it is easier to compare and contrast similar items than very di   erent items, we
instead seek x and y that are not semantically similar, which is why our de   nition is restricted to (cid:107)(cid:126)x    (cid:126)y(cid:107)       .
as    varies from small to large, the analogies vary from generating very similar x and y to very loosely

related x and y where their relationship is vague and more    creative   .

finally, figure 9 highlights di   erences between analogies generated from our approach and the corre-

sponding analogies generated by the    rst approach mentioned above, namely minimizing:

min

x,y:x(cid:54)=a,y(cid:54)=b,x(cid:54)=y

(cid:107)((cid:126)a     (cid:126)b)     ((cid:126)x     (cid:126)y)(cid:107),

(2)

to compare, we took the    rst 100 analogies generated using the two approaches that did not have any
gender-speci   c words. we then display the    rst 10 analogies from each list which do not occur in the other
list of 100.

b learning the linear transform
in the soft debiasing algorithm, we need to solve the following optimization problem.

(cid:107)(t w )t (t w )     w t w(cid:107)2

f +   (cid:107)(t n )t (t b)(cid:107)2
f .

min

t

let x = t t t , then this is equivalent to the following semi-de   nite programming problem

(cid:107)w t xw     w t w(cid:107)2

f +   (cid:107)n t xb(cid:107)2

f

s.t.x (cid:23) 0.

(3)

min
x

the    rst term ensures that the pairwise inner products are preserved and the second term induces the biases
of gender neutral words onto the gender subspace to be small. the user-speci   ed parameter    balances the
two terms.

18

analogies generated using eq. (2) analogies generated using our approach, eq. (1)
petite-diminutive
seventh inning-eighth inning
seventh-sixth
east-west
tripled-doubled
breast cancer-cancer
meter hurdles-meter dash
thousands-tens
eight-seven
unemployment rate-jobless rate

petite-lanky
volleyball-football
interior designer-architect
bitch-bastard
bra-pants
nurse-surgeon
feminine-manly
glamorous-   ashy
registered nurse-physician
cupcakes-pizzas

figure 9: first 10 di   erent she-he analogies generated using the parallelogram approach and our approach,
from the top 100 she-he analogies not containing gender speci   c words. most of the analogies on the left
seem to have little connection to gender.

directly solving this sdp optimization problem is challenging. in practice, the dimension of matrix w is
in the scale of 300    400, 000. the dimensions of the matrices w t xw and w t w are 400, 000    400, 000,
causing computational and memory issues. we perform singular value decomposition on w , such that
w = u   v t , where u and v are orthogonal matrices and    is a diagonal matrix.

(cid:107)w t xw     w t w(cid:107)2

f = (cid:107)w t (x     i)w(cid:107)2
= (cid:107)v   u t (x     i)u   v t(cid:107)2
= (cid:107)  u t (x     i)u   (cid:107)2
f .

f

f

(4)

the last equality follows the fact that v is an orthogonal matrix and ((cid:107)v y v t(cid:107)2
tr(v y t y v t ) = tr(y t y v t v ) = tr(y t y ) = (cid:107)y (cid:107)2

f .)

substituting eq. (4) to eq. (3) gives

f = tr(v y t v t v y v t ) =

(5)
here   u t (x     i)u    is a 300    300 matrix and can be solved e   ciently. the solution t is the debiasing
transformation of the id27.

min
x

f

(cid:107)  u t (x     i)u   (cid:107)2

f +   (cid:107)p xst(cid:107)2

s.t. x (cid:23) 0.

c details of gender speci   c words base set
this section gives precise details of how we derived our list of gender neutral words. note that the choice of
gender neutral words is partly subjective. some words are most often associated with females or males but
have exceptions, such as beard (bearded women), estrogen (men have small amounts of the hormone estrogen),
and rabbi (reformed jewish congregations recognize female rabbis). there are also many words that have
multiple senses, some of which are gender neutral and others of which are gender speci   c. for instance, the
profession of nursing is gender neutral while nursing a baby (i.e., breastfeeding) is only performed by women.
to derive the base subset of words from w2vnews, for each of the 26,377 words in the    ltered embedding,
we selected words whose de   nitions include any of the following words in their singular or plural forms:
female, male, woman, man, girl, boy, sister, brother, daughter, son, grandmother, grandfather, wife, husband.
de   nitions were taken from id138 [11] (in the case where a word had multiple senses/synsets, we chose
the de   nition whose corresponding lemma had greatest frequency in terms of its count). this list of hundreds
of words contains most gender speci   c words of interest but also contains some gender neutral words, e.g.,
the de   nition of mating is    the act of pairing a male and female for reproductive purposes.    even though
the word female is in the de   nition, mating is not gender speci   c. we went through this list and manually
selected those words that were clearly gender speci   c. motivated by the application of improving web search,

19

we used a strict de   nition of gender speci   city, so that when in doubt a word was de   ned to be gender neutral.
for instance, clothing words (e.g., the de   nition of vest is    a collarless men   s undergarment for the upper
part of the body   ) were classi   ed as gender neutral since there are undoubtedly people of every gender that
wear any given type of clothing. after this    ltering, we were left with the following list of 218 gender-speci   c
words (sorted by word frequency):

he, his, her, she, him, man, women, men, woman, spokesman, wife, himself, son, mother, father, chairman,
daughter, husband, guy, girls, girl, boy, boys, brother, spokeswoman, female, sister, male, herself, brothers, dad,
actress, mom, sons, girlfriend, daughters, lady, boyfriend, sisters, mothers, king, businessman, grandmother,
grandfather, deer, ladies, uncle, males, congressman, grandson, bull, queen, businessmen, wives, widow,
nephew, bride, females, aunt, prostate cancer, lesbian, chairwoman, fathers, moms, maiden, granddaughter,
younger brother, lads, lion, gentleman, fraternity, bachelor, niece, bulls, husbands, prince, colt, salesman, hers,
dude, beard,    lly, princess, lesbians, councilman, actresses, gentlemen, stepfather, monks, ex girlfriend, lad,
sperm, testosterone, nephews, maid, daddy, mare,    ance,    ancee, kings, dads, waitress, maternal, heroine,
nieces, girlfriends, sir, stud, mistress, lions, estranged wife, womb, grandma, maternity, estrogen, ex boyfriend,
widows, gelding, diva, teenage girls, nuns, czar, ovarian cancer, countrymen, teenage girl, penis, bloke, nun,
brides, housewife, spokesmen, suitors, menopause, monastery, motherhood, brethren, stepmother, prostate,
hostess, twin brother, schoolboy, brotherhood,    llies, stepson, congresswoman, uncles, witch, monk, viagra,
paternity, suitor, sorority, macho, businesswoman, eldest son, gal, statesman, schoolgirl, fathered, goddess,
hubby, stepdaughter, blokes, dudes, strongman, uterus, grandsons, studs, mama, godfather, hens, hen, mommy,
estranged husband, elder brother, boyhood, baritone, grandmothers, grandpa, boyfriends, feminism, countryman,
stallion, heiress, queens, witches, aunts, semen, fella, granddaughters, chap, widower, salesmen, convent,
vagina, beau, beards, handyman, twin sister, maids, gals, housewives, horsemen, obstetrics, fatherhood,
councilwoman, princes, matriarch, colts, ma, fraternities, pa, fellas, councilmen, dowry, barbershop, fraternal,
ballerina

d questionnaire for generating gender stereotypical words
task: for each category, please enter 10 or more words, separated by commas. we are looking
for a variety of creative answers     this is a mentally challenging hit that will make you think.

    10 or more comma-separated words de   nitionally associated with males.

examples: dude, menswear, king, penis, ...

    10 or more comma-separated words de   nitionally associated with females.

examples: queen, jane, girl, ...

    10 or more comma-separated words stereotypically associated with males

examples: football, janitor, cocky, ...

    10 or more comma-separated words stereotypically associated with females

examples: pink, sewing, caring, sassy, nurse, ...

thank you for your help in making arti   cially intelligent systems that aren   t prejudiced. :-)

e questionnaire for generating gender stereotypical analogies
an analogy describes two pairs of words where the relationship between the two words in each pair is
the same. an example of an analogy is apple is to fruit as asparagus is to vegetable (denoted as ap-
ple:fruit::asparagus:vegetable). we need your help to improve our analogy generating system.

20

task: please enter 10 or more analogies re   ecting gender stereotypes, separated by commas.
we are looking for a variety of creative answers     this is a mentally challenging hit that will make you think.

examples of stereotypes

    tall : man :: short : woman re   ects a cultural stereotype that men are tall and women are short.
    doctor : man :: nurse : woman re   ects a stereotype that doctors are typically men and nurses are

typically women.

f questionnaire for rating stereotypical analogies
an analogy describes two pairs of words where the relationship between the two words in each pair is
the same. an example of an analogy is apple is to fruit as asparagus is to vegetable (denoted as ap-
ple:fruit::asparagus:vegetable). we need your help to improve our analogy generating system.

task: which analogies are stereotypes? which ones are appropriate analogies?
    examples of stereotype analogies

tall : man :: short : woman
doctor : man :: nurse : woman

    examples of appropriate analogies

king: man :: queen : woman
brother : man :: sister : woman
john : man :: mary : woman
his : man :: hers : woman
salesman : man :: saleswoman : woman
penis : man :: vagina : woman

warning: this hit may contain adult content. worker discretion is advised.
check the analogies that are stereotypes
...
check the analogies that are nonsensical
...
check the analogies that are nonsensical
...
any suggestions or comments on the hit? optional feedback

g analogies generated by id27s

analogy
hostess:bartender
ballerina:dancer
colts:mares
ma:na
salesperson:salesman

after executing hard debiasing

before executing debiasing

appropriate biased analogy

appropriate biased

midwife:doctor
sewing:carpentry
pediatrician:orthopedic_surgeon
registered_nurse:physician
housewife:shopkeeper

1
2
0
1
1

10
9
9
9
9

1
0
6
8
1

8
7
7
7
7

21

diva:superstar
witches:vampires
hair_salon:barbershop
maid:housekeeper
soprano:baritone
footy:blokes
maids:servants
dictator:strongman
bachelor:bachelor_degree
witch:witchcraft
ga   er:lads
convent:monastery
hen:cock
aldermen:councilmen
girlfriend:friend
housewife:homemaker
maternal:infant_mortality
beau:lover
mistress:prostitute
heroine:protagonist
heiress:socialite
teenage_girl:teenager
estrogen:testosterone
actresses:actors
blokes:bloke
girlfriends:buddies
compatriot:countryman
compatriots:countrymen
gals:dudes
eldest:elder_brother
sperm:embryos
mother:father
wedlock:fathered
mama:fella
lesbian:gay
kid:guy
carpenter:handyman
she:he
herself:himself
her:his
uterus:intestine
queens:kings
female:male
women:men
pa:mo
nun:monk
matriarch:patriarch
nuns:priests
menopause:puberty
   ance:roommate
daughter:son

4
1
4
3
4
0
4
0
7
0
1
8
8
0
0
2
1
1
0
2
2
3
9
10
1
6
3
2
10
1
2
10
0
7
8
1
5
9
10
10
1
10
9
10
9
7
9
9
2
0
9

skirts:shorts
nurse:surgeon
interior_designer:architect
softball:baseball
blond:burly
nanny:chau   eur
feminism:conservatism
adorable:goofy
vocalists:guitarists
cosmetics:pharmaceuticals
whore:coward
vocalist:guitarist
petite:lanky
salesperson:salesman
sassy:snappy
diva:superstar
charming:a   able
giggle:chuckle
witch:demon
volleyball:football
feisty:mild_mannered
cupcakes:pizzas
dolls:replicas
netball:rugby
hairdresser:barber
soprano:baritone
gown:blazer
glamorous:   ashy
sweater:jersey
feminist:liberal
bra:pants
rebounder:playmaker
nude:shirtless
judgmental:arrogant
boobs:ass
salon:barbershop
lovely:brilliant
practicality:durability
singer:frontman
gorgeous:magni   cent
ponytail:mustache
feminists:socialists
bras:trousers
wedding_dress:tuxedo
violinist:virtuoso
handbag:briefcase
giggling:grinning
kids:guys
beautiful:majestic
feminine:manly
convent:monastery

7
7
6
6
5
5
5
5
4
4
3
3
2
2
2
2
2
2
2
2
2
2
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

22

0
1
1
4
2
1
2
0
0
1
0
1
1
1
2
4
2
1
2
1
0
1
0
0
6
4
6
2
0
0
2
0
0
1
1
7
0
0
0
2
2
0
5
6
0
8
0
3
1
8
8

9
9
8
8
8
8
8
8
8
8
7
7
7
7
7
7
6
6
6
6
6
6
6
6
5
5
5
5
5
5
5
5
5
4
4
4
4
4
4
4
4
4
4
4
4
3
3
3
3
3
3

daughters:sons
spokeswoman:spokesman
politician:statesman
stallion:stud
suitor:takeover_bid
waitress:waiter
lady:waitress
bride:wedding
widower:widowed
husband:younger_brother
actress:actor
mustache:beard
facial_hair:beards
suitors:bidders
girl:boy
childhood:boyhood
girls:boys
counterparts:brethren
brides:bridal
sister:brother
friendship:brotherhood
sisters:brothers
businesswoman:businessman
businesspeople:businessmen
chairwoman:chairman
bastard:chap
hens:chickens
viagra:cialis
   lly:colt
   llies:colts
congresswoman:congressman
councilwoman:councilman
wife:cousin
mom:dad
mommy:daddy
moms:dads
widow:deceased
gal:dude
stepmother:eldest_son
deer:elk
estranged_husband:estranged
ex_boyfriend:ex_girlfriend
widows:families
motherhood:fatherhood
mothers:fathers
guys:fellas
feminism:feminist
womb:fetus
sorority:fraternity
lesbians:gays
mare:gelding

10
10
1
7
8
10
0
0
3
3
9
0
0
6
9
1
10
4
1
10
3
9
9
1
10
0
3
1
9
8
9
9
0
10
10
9
0
9
3
1
0
7
4
10
10
1
1
0
9
9
7

sexism:racism
pink:red
blouse:shirt
bitch:bastard
wig:beard
hysterical:comical
male_counterparts:counterparts
beauty:grandeur
cheerful:jovial
breast_cancer:lymphoma
heiress:magnate
estrogen:testosterone
starlet:youngster
mary:john
actresses:actors
middle_aged:bearded
mums:blokes
girlfriends:buddies
mammogram:colonoscopy
compatriot:countryman
luscious:crisp
gals:dudes
siblings:elder_brother
mother:father
babe:fella
lesbian:gay
breasts:genitals
wonderful:great
she:he
herself:himself
her:his
mommy:kid
queens:kings
female:male
women:men
boyfriend:pal
matriarch:patriarch
nun:priest
breast:prostate
daughter:son
daughters:sons
spokeswoman:spokesman
fabulous:terri   c
headscarf:turban
waitress:waiter
husband:younger_brother
hers:yours
teenage_girls:youths
actress:actor
blonde:blond
girl:boy

1
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

23

0
0
6
8
4
0
1
0
0
3
6
9
2
9
10
0
5
6
0
3
0
10
1
10
9
8
0
0
9
10
10
0
10
9
10
0
9
10
9
9
10
10
3
6
10
3
2
0
9
4
9

3
3
3
2
2
2
2
2
2
2
2
2
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0
0
0

fella:gentleman
ladies:gentlemen
boyfriends:girlfriend
goddess:god
grandmother:grandfather
grandma:grandpa
grandmothers:grandparents
granddaughter:grandson
granddaughters:grandsons
me:him
queen:king
youngster:lad
elephant:lion
elephants:lions
manly:macho
females:males
woman:man
   ancee:married
maternity:midwives
monks:monasteries
niece:nephew
nieces:nephews
hubby:pal
obstetrics:pediatrics
vagina:penis
princess:prince
colon:prostate
ovarian_cancer:prostate_cancer
salespeople:salesmen
semen:saliva
schoolgirl:schoolboy
replied:sir
spokespeople:spokesmen
boyfriend:stepfather
stepdaughter:stepson
teenage_girls:teenagers
hers:theirs
twin_sister:twin_brother
aunt:uncle
aunts:uncles
husbands:wives

1
10
3
9
10
9
5
10
9
2
10
1
0
0
4
10
8
4
1
0
9
9
1
3
10
9
6
10
2
7
8
0
0
1
9
1
0
9
9
10
7

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

childhood:boyhood
girls:boys
sister:brother
sisters:brothers
businesswoman:businessman
chairwoman:chairman
   lly:colt
   llies:colts
congresswoman:congressman
councilwoman:councilman
mom:dad
moms:dads
gal:dude
motherhood:fatherhood
mothers:fathers
sorority:fraternity
mare:gelding
lady:gentleman
ladies:gentlemen
goddess:god
grandmother:grandfather
grandma:grandpa
granddaughter:grandson
granddaughters:grandsons
kinda:guy
heroine:hero
me:him
queen:king
females:males
woman:man
niece:nephew
nieces:nephews
vagina:penis
princess:prince
ovarian_cancer:prostate_cancer
schoolgirl:schoolboy
spokespeople:spokesmen
stepdaughter:stepson
twin_sister:twin_brother
aunt:uncle
aunts:uncles

1
10
10
9
9
10
9
8
9
9
10
9
9
10
10
9
7
9
10
9
10
9
10
9
1
9
2
10
10
8
9
9
10
9
10
8
0
9
9
9
10

h debiasing the full w2vnews embedding.
in the main text, we focused on the results from a cleaned version of w2vnews consisting of 26,377 lower-case
words. we have also applied our hard debiasing algorithm to the full w2vnews dataset. evalution based on
the standard metrics shows that the debiasing does not degrade the utility of the embedding (table 3).

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

24

rg ws

analogy

before
hard-debiased
soft-debiased

76.1
76.5
76.9

70.0
69.7
69.7

71.2
71.2
71.2

table 3: the columns show the performance of the original, complete w2vnews embedding (   before   ) and
the debiased w2vnews on the standard id74 measuring coherence and analogy-solving abilities:
rg [32], ws [12], msr-analogy [26]. higher is better. the results show that the performance does not
degrade after debiasing.

25

