node2vec: scalable id171 for networks

aditya grover
stanford university

adityag@cs.stanford.edu

jure leskovec
stanford university

jure@cs.stanford.edu

6
1
0
2

 
l
u
j
 

3

 
 
]
i
s
.
s
c
[
 
 

1
v
3
5
6
0
0

.

7
0
6
1
:
v
i
x
r
a

abstract
prediction tasks over nodes and edges in networks require careful
effort in engineering features used by learning algorithms. recent
research in the broader    eld of representation learning has led to
signi   cant progress in automating prediction by learning the fea-
tures themselves. however, present id171 approaches
are not expressive enough to capture the diversity of connectivity
patterns observed in networks.

here we propose node2vec, an algorithmic framework for learn-
ing continuous feature representations for nodes in networks. in
node2vec, we learn a mapping of nodes to a low-dimensional space
of features that maximizes the likelihood of preserving network
neighborhoods of nodes. we de   ne a    exible notion of a node   s
network neighborhood and design a biased random walk procedure,
which ef   ciently explores diverse neighborhoods. our algorithm
generalizes prior work which is based on rigid notions of network
neighborhoods, and we argue that the added    exibility in exploring
neighborhoods is the key to learning richer representations.

we demonstrate the ef   cacy of node2vec over existing state-of-
the-art techniques on multi-label classi   cation and link prediction
in several real-world networks from diverse domains. taken to-
gether, our work represents a new way for ef   ciently learning state-
of-the-art task-independent representations in complex networks.
categories and subject descriptors: h.2.8 [database manage-
ment]: database applications   data mining; i.2.6 [arti   cial in-
telligence]: learning
general terms: algorithms; experimentation.
keywords: information networks, id171, node embed-
dings, graph representations.

1.

introduction

many important tasks in network analysis involve predictions
over nodes and edges.
in a typical node classi   cation task, we
are interested in predicting the most probable labels of nodes in
a network [33]. for example, in a social network, we might be
interested in predicting interests of users, or in a protein-protein in-
teraction network we might be interested in predicting functional
labels of proteins [25, 37]. similarly, in link prediction, we wish to

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro   t or commercial advantage and that copies bear this notice and the full citation
on the    rst page. copyrights for components of this work owned by others than the
author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speci   c permission
and/or a fee. request permissions from permissions@acm.org.
kdd    16, august 13 - 17, 2016, san francisco, ca, usa
c(cid:13) 2016 copyright held by the owner/author(s). publication rights licensed to acm.
isbn 978-1-4503-4232-2/16/08. . . $15.00
doi: http://dx.doi.org/10.1145/2939672.2939754

predict whether a pair of nodes in a network should have an edge
connecting them [18]. link prediction is useful in a wide variety
of domains; for instance, in genomics, it helps us discover novel
interactions between genes, and in social networks, it can identify
real-world friends [2, 34].

any supervised machine learning algorithm requires a set of in-
formative, discriminating, and independent features. in prediction
problems on networks this means that one has to construct a feature
vector representation for the nodes and edges. a typical solution in-
volves hand-engineering domain-speci   c features based on expert
knowledge. even if one discounts the tedious effort required for
feature engineering, such features are usually designed for speci   c
tasks and do not generalize across different prediction tasks.

an alternative approach is to learn feature representations by
solving an optimization problem [4]. the challenge in feature learn-
ing is de   ning an objective function, which involves a trade-off
in balancing computational ef   ciency and predictive accuracy. on
one side of the spectrum, one could directly aim to    nd a feature
representation that optimizes performance of a downstream predic-
tion task. while this supervised procedure results in good accu-
racy, it comes at the cost of high training time complexity due to a
blowup in the number of parameters that need to be estimated. at
the other extreme, the objective function can be de   ned to be inde-
pendent of the downstream prediction task and the representations
can be learned in a purely unsupervised way. this makes the op-
timization computationally ef   cient and with a carefully designed
objective, it results in task-independent features that closely match
task-speci   c approaches in predictive accuracy [21, 23].

however, current techniques fail to satisfactorily de   ne and opti-
mize a reasonable objective required for scalable unsupervised fea-
ture learning in networks. classic approaches based on linear and
non-linear id84 techniques such as principal
component analysis, multi-dimensional scaling and their exten-
sions [3, 27, 30, 35] optimize an objective that transforms a repre-
sentative data matrix of the network such that it maximizes the vari-
ance of the data representation. consequently, these approaches in-
variably involve eigendecomposition of the appropriate data matrix
which is expensive for large real-world networks. moreover, the
resulting latent representations give poor performance on various
prediction tasks over networks.

alternatively, we can design an objective that seeks to preserve
local neighborhoods of nodes. the objective can be ef   ciently op-
timized using stochastic id119 (sgd) akin to backpro-
pogation on just single hidden-layer feedforward neural networks.
recent attempts in this direction [24, 28] propose ef   cient algo-
rithms but rely on a rigid notion of a network neighborhood, which
results in these approaches being largely insensitive to connectiv-
ity patterns unique to networks. speci   cally, nodes in networks

could be organized based on communities they belong to (i.e., ho-
mophily); in other cases, the organization could be based on the
structural roles of nodes in the network (i.e., structural equiva-
lence) [7, 10, 36]. for instance, in figure 1, we observe nodes
u and s1 belonging to the same tightly knit community of nodes,
while the nodes u and s6 in the two distinct communities share the
same structural role of a hub node. real-world networks commonly
exhibit a mixture of such equivalences. thus, it is essential to allow
for a    exible algorithm that can learn node representations obeying
both principles: ability to learn representations that embed nodes
from the same network community closely together, as well as to
learn representations where nodes that share similar roles have sim-
ilar embeddings. this would allow id171 algorithms to
generalize across a wide variety of domains and prediction tasks.
present work. we propose node2vec, a semi-supervised algorithm
for scalable id171 in networks. we optimize a custom
graph-based objective function using sgd motivated by prior work
on natural language processing [21]. intuitively, our approach re-
turns feature representations that maximize the likelihood of pre-
serving network neighborhoods of nodes in a d-dimensional fea-
ture space. we use a 2nd order random walk approach to generate
(sample) network neighborhoods for nodes.

our key contribution is in de   ning a    exible notion of a node   s
network neighborhood. by choosing an appropriate notion of a
neighborhood, node2vec can learn representations that organize
nodes based on their network roles and/or communities they be-
long to. we achieve this by developing a family of biased random
walks, which ef   ciently explore diverse neighborhoods of a given
node. the resulting algorithm is    exible, giving us control over the
search space through tunable parameters, in contrast to rigid search
procedures in prior work [24, 28]. consequently, our method gen-
eralizes prior work and can model the full spectrum of equivalences
observed in networks. the parameters governing our search strat-
egy have an intuitive interpretation and bias the walk towards dif-
ferent network exploration strategies. these parameters can also
be learned directly using a tiny fraction of labeled data in a semi-
supervised fashion.

we also show how feature representations of individual nodes
can be extended to pairs of nodes (i.e., edges). in order to generate
feature representations of edges, we compose the learned feature
representations of the individual nodes using simple binary oper-
ators. this compositionality lends node2vec to prediction tasks
involving nodes as well as edges.

our experiments focus on two common prediction tasks in net-
works: a multi-label classi   cation task, where every node is as-
signed one or more class labels and a link prediction task, where we
predict the existence of an edge given a pair of nodes. we contrast
the performance of node2vec with state-of-the-art id171
algorithms [24, 28]. we experiment with several real-world net-
works from diverse domains, such as social networks, information
networks, as well as networks from systems biology. experiments
demonstrate that node2vec outperforms state-of-the-art methods by
up to 26.7% on multi-label classi   cation and up to 12.6% on link
prediction. the algorithm shows competitive performance with
even 10% labeled data and is also robust to perturbations in the
form of noisy or missing edges. computationally, the major phases
of node2vec are trivially parallelizable, and it can scale to large
networks with millions of nodes in a few hours.

overall our paper makes the following contributions:
1. we propose node2vec, an ef   cient scalable algorithm for
id171 in networks that ef   ciently optimizes a novel
network-aware, neighborhood preserving objective using sgd.

2. we show how node2vec is in accordance with established

figure 1: bfs and dfs search strategies from node u (k = 3).

principles in network science, providing    exibility in discov-
ering representations conforming to different equivalences.

3. we extend node2vec and other id171 methods based
on neighborhood preserving objectives, from nodes to pairs
of nodes for edge-based prediction tasks.

4. we empirically evaluate node2vec for multi-label classi   ca-

tion and link prediction on several real-world datasets.

the rest of the paper is structured as follows. in section 2, we
brie   y survey related work in id171 for networks. we
present the technical details for id171 using node2vec
in section 3. in section 4, we empirically evaluate node2vec on
prediction tasks over nodes and edges on various real-world net-
works and assess the parameter sensitivity, perturbation analysis,
and scalability aspects of our algorithm. we conclude with a dis-
cussion of the node2vec framework and highlight some promis-
ing directions for future work in section 5. datasets and a refer-
ence implementation of node2vec are available on the project page:
http://snap.stanford.edu/node2vec.

2. related work

feature engineering has been extensively studied by the machine
learning community under various headings. in networks, the con-
ventional paradigm for generating features for nodes is based on
feature extraction techniques which typically involve some seed
hand-crafted features based on network properties [8, 11]. in con-
trast, our goal is to automate the whole process by casting feature
extraction as a representation learning problem in which case we
do not require any hand-engineered features.

unsupervised id171 approaches typically exploit the
spectral properties of various matrix representations of graphs, es-
pecially the laplacian and the adjacency matrices. under this linear
algebra perspective, these methods can be viewed as dimensional-
ity reduction techniques. several linear (e.g., pca) and non-linear
(e.g., isomap) id84 techniques have been pro-
posed [3, 27, 30, 35]. these methods suffer from both computa-
tional and statistical performance drawbacks. in terms of computa-
tional ef   ciency, eigendecomposition of a data matrix is expensive
unless the solution quality is signi   cantly compromised with ap-
proximations, and hence, these methods are hard to scale to large
networks. secondly, these methods optimize for objectives that are
not robust to the diverse patterns observed in networks (such as ho-
mophily and structural equivalence) and make assumptions about
the relationship between the underlying network structure and the
prediction task. for instance, spectral id91 makes a strong
homophily assumption that graph cuts will be useful for classi   ca-
tion [29]. such assumptions are reasonable in many scenarios, but
unsatisfactory in effectively generalizing across diverse networks.
recent advancements in representational learning for natural lan-
guage processing opened new ways for id171 of discrete
objects such as words. in particular, the skip-gram model [21] aims
to learn continuous feature representations for words by optimiz-
ing a neighborhood preserving likelihood objective. the algorithm

u s3 s2 s1 s4 s8 s9 s6 s7 s5 bfs dfs proceeds as follows: it scans over the words of a document, and
for every word it aims to embed it such that the word   s features
can predict nearby words (i.e., words inside some context win-
dow). the word feature representations are learned by optmizing
the likelihood objective using sgd with negative sampling [22].
the skip-gram objective is based on the distributional hypothe-
sis which states that words in similar contexts tend to have similar
meanings [9]. that is, similar words tend to appear in similar word
neighborhoods.

inspired by the skip-gram model, recent research established
an analogy for networks by representing a network as a    docu-
ment    [24, 28]. the same way as a document is an ordered se-
quence of words, one could sample sequences of nodes from the
underlying network and turn a network into a ordered sequence of
nodes. however, there are many possible sampling strategies for
nodes, resulting in different learned feature representations. in fact,
as we shall show, there is no clear winning sampling strategy that
works across all networks and all prediction tasks. this is a major
shortcoming of prior work which fail to offer any    exibility in sam-
pling of nodes from a network [24, 28]. our algorithm node2vec
overcomes this limitation by designing a    exible objective that is
not tied to a particular sampling strategy and provides parameters
to tune the explored search space (see section 3).

finally, for both node and edge based prediction tasks, there is
a body of recent work for supervised id171 based on ex-
isting and novel graph-speci   c deep network architectures [15, 16,
17, 31, 39]. these architectures directly minimize the id168
for a downstream prediction task using several layers of non-linear
transformations which results in high accuracy, but at the cost of
scalability due to high training time requirements.

3. id171 framework

we formulate id171 in networks as a maximum like-
lihood optimization problem. let g = (v, e) be a given net-
work. our analysis is general and applies to any (un)directed,
(un)weighted network. let f : v     rd be the mapping func-
tion from nodes to feature representaions we aim to learn for a
downstream prediction task. here d is a parameter specifying the
number of dimensions of our feature representation. equivalently,
f is a matrix of size |v |    d parameters. for every source node
u     v , we de   ne ns(u)     v as a network neighborhood of node
u generated through a neighborhood sampling strategy s.

we proceed by extending the skip-gram architecture to networks
[21, 24]. we seek to optimize the following objective function,
which maximizes the log-id203 of observing a network neigh-
borhood ns(u) for a node u conditioned on its feature representa-
tion, given by f:

max

f

log p r(ns(u)|f (u)).

(1)

(cid:88)

u   v

two standard assumptions:

in order to make the optimization problem tractable, we make
    conditional independence. we factorize the likelihood by as-
suming that the likelihood of observing a neighborhood node
is independent of observing any other neighborhood node
given the feature representation of the source:

p r(ns(u)|f (u)) =

p r(ni|f (u)).

(cid:89)

ni   ns (u)

    symmetry in feature space. a source node and neighbor-
hood node have a symmetric effect over each other in fea-
ture space. accordingly, we model the conditional likeli-

hood of every source-neighborhood node pair as a softmax
unit parametrized by a dot product of their features:

(cid:80)

.

(cid:20)

max

(cid:88)

with the above assumptions, the objective in eq. 1 simpli   es to:

p r(ni|f (u)) =

exp(f (ni)    f (u))
v   v exp(f (v)    f (u))
(cid:21)
(cid:88)
the per-node partition function, zu = (cid:80)

f (ni)    f (u)

v   v exp(f (u)    f (v)),
is expensive to compute for large networks and we approximate it
using negative sampling [22]. we optimize eq. 2 using stochastic
gradient ascent over the model parameters de   ning the features f.
id171 methods based on the skip-gram architecture

    log zu +

.

(2)

ni   ns (u)

u   v

f

have been originally developed in the context of natural language [21].
given the linear nature of text, the notion of a neighborhood can be
naturally de   ned using a sliding window over consecutive words.
networks, however, are not linear, and thus a richer notion of a
neighborhood is needed. to resolve this issue, we propose a ran-
domized procedure that samples many different neighborhoods of a
given source node u. the neighborhoods ns(u) are not restricted
to just immediate neighbors but can have vastly different structures
depending on the sampling strategy s.
3.1 classic search strategies

we view the problem of sampling neighborhoods of a source
node as a form of local search. figure 1 shows a graph, where
given a source node u we aim to generate (sample) its neighbor-
hood ns(u). importantly, to be able to fairly compare different
sampling strategies s, we shall constrain the size of the neighbor-
hood set ns to k nodes and then sample multiple sets for a single
node u. generally, there are two extreme sampling strategies for
generating neighborhood set(s) ns of k nodes:
    breadth-   rst sampling (bfs) the neighborhood ns is re-
stricted to nodes which are immediate neighbors of the source.
for example, in figure 1 for a neighborhood of size k = 3,
bfs samples nodes s1, s2, s3.
    depth-   rst sampling (dfs) the neighborhood consists of
nodes sequentially sampled at increasing distances from the
source node. in figure 1, dfs samples s4, s5, s6.

the breadth-   rst and depth-   rst sampling represent extreme sce-
narios in terms of the search space they explore leading to interest-
ing implications on the learned representations.

in particular, prediction tasks on nodes in networks often shut-
tle between two kinds of similarities: homophily and structural
equivalence [12]. under the homophily hypothesis [7, 36] nodes
that are highly interconnected and belong to similar network clus-
ters or communities should be embedded closely together (e.g.,
nodes s1 and u in figure 1 belong to the same network commu-
nity). in contrast, under the structural equivalence hypothesis [10]
nodes that have similar structural roles in networks should be em-
bedded closely together (e.g., nodes u and s6 in figure 1 act as
hubs of their corresponding communities). importantly, unlike ho-
mophily, structural equivalence does not emphasize connectivity;
nodes could be far apart in the network and still have the same
structural role. in real-world, these equivalence notions are not ex-
clusive; networks commonly exhibit both behaviors where some
nodes exhibit homophily while others re   ect structural equivalence.
we observe that bfs and dfs strategies play a key role in pro-
ducing representations that re   ect either of the above equivalences.
in particular, the neighborhoods sampled by bfs lead to embed-
dings that correspond closely to structural equivalence. intuitively,

we note that in order to ascertain structural equivalence, it is of-
ten suf   cient to characterize the local neighborhoods accurately.
for example, structural equivalence based on network roles such as
bridges and hubs can be inferred just by observing the immediate
neighborhoods of each node. by restricting search to nearby nodes,
bfs achieves this characterization and obtains a microscopic view
of the neighborhood of every node. additionally, in bfs, nodes in
the sampled neighborhoods tend to repeat many times. this is also
important as it reduces the variance in characterizing the distribu-
tion of 1-hop nodes with respect the source node. however, a very
small portion of the graph is explored for any given k.

the opposite is true for dfs which can explore larger parts of
the network as it can move further away from the source node u
(with sample size k being    xed). in dfs, the sampled nodes more
accurately re   ect a macro-view of the neighborhood which is es-
sential in inferring communities based on homophily. however,
the issue with dfs is that it is important to not only infer which
node-to-node dependencies exist in a network, but also to charac-
terize the exact nature of these dependencies. this is hard given
we have a constrain on the sample size and a large neighborhood
to explore, resulting in high variance. secondly, moving to much
greater depths leads to complex dependencies since a sampled node
may be far from the source and potentially less representative.
3.2 node2vec

building on the above observations, we design a    exible neigh-
borhood sampling strategy which allows us to smoothly interpolate
between bfs and dfs. we achieve this by developing a    exible
biased random walk procedure that can explore neighborhoods in a
bfs as well as dfs fashion.
3.2.1 id93
formally, given a source node u, we simulate a random walk of
   xed length l. let ci denote the ith node in the walk, starting with
c0 = u. nodes ci are generated by the following distribution:

p (ci = x | ci   1 = v) =

(cid:40)   vx

z

0

if (v, x)     e
otherwise

where   vx is the unnormalized transition id203 between nodes
v and x, and z is the normalizing constant.

search bias   

3.2.2
the simplest way to bias our id93 would be to sample
the next node based on the static edge weights wvx i.e.,   vx = wvx.
(in case of unweighted graphs wvx = 1.) however, this does
not allow us to account for the network structure and guide our
search procedure to explore different types of network neighbor-
hoods. additionally, unlike bfs and dfs which are extreme sam-
pling paradigms suited for structural equivalence and homophily
respectively, our id93 should accommodate for the fact
that these notions of equivalence are not competing or exclusive,
and real-world networks commonly exhibit a mixture of both.

we de   ne a 2nd order random walk with two parameters p and q
which guide the walk: consider a random walk that just traversed
edge (t, v) and now resides at node v (figure 2). the walk now
needs to decide on the next step so it evaluates the transition prob-
abilities   vx on edges (v, x) leading from v. we set the unnormal-
ized transition id203 to   vx =   pq(t, x)    wvx, where

                1

p
1
1
q

if dtx = 0
if dtx = 1
if dtx = 2

  pq(t, x) =

figure 2: illustration of the random walk procedure in node2vec.
the walk just transitioned from t to v and is now evaluating its next
step out of node v. edge labels indicate search biases   .

and dtx denotes the shortest path distance between nodes t and x.
note that dtx must be one of {0, 1, 2}, and hence, the two parame-
ters are necessary and suf   cient to guide the walk.

intuitively, parameters p and q control how fast the walk explores
and leaves the neighborhood of starting node u. in particular, the
parameters allow our search procedure to (approximately) interpo-
late between bfs and dfs and thereby re   ect an af   nity for dif-
ferent notions of node equivalences.
return parameter, p. parameter p controls the likelihood of im-
mediately revisiting a node in the walk. setting it to a high value
(> max(q, 1)) ensures that we are less likely to sample an already-
visited node in the following two steps (unless the next node in
the walk had no other neighbor). this strategy encourages moder-
ate exploration and avoids 2-hop redundancy in sampling. on the
other hand, if p is low (< min(q, 1)), it would lead the walk to
backtrack a step (figure 2) and this would keep the walk    local   
close to the starting node u.
in-out parameter, q. parameter q allows the search to differentiate
between    inward    and    outward    nodes. going back to figure 2,
if q > 1, the random walk is biased towards nodes close to node t.
such walks obtain a local view of the underlying graph with respect
to the start node in the walk and approximate bfs behavior in the
sense that our samples comprise of nodes within a small locality.

in contrast, if q < 1, the walk is more inclined to visit nodes
which are further away from the node t. such behavior is re   ec-
tive of dfs which encourages outward exploration. however, an
essential difference here is that we achieve dfs-like exploration
within the random walk framework. hence, the sampled nodes are
not at strictly increasing distances from a given source node u, but
in turn, we bene   t from tractable preprocessing and superior sam-
pling ef   ciency of id93. note that by setting   v,x to be
a function of the preceeding node in the walk t, the id93
are 2nd order markovian.
bene   ts of id93. there are several bene   ts of random
walks over pure bfs/dfs approaches. id93 are compu-
tationally ef   cient in terms of both space and time requirements.
the space complexity to store the immediate neighbors of every
node in the graph is o(|e|). for 2nd order id93, it is
helpful to store the interconnections between the neighbors of ev-
ery node, which incurs a space complexity of o(a2|v |) where a
is the average degree of the graph and is usually small for real-
world networks. the other key advantage of id93 over
classic search-based sampling strategies is its time complexity. in
particular, by imposing graph connectivity in the sample genera-
tion process, id93 provide a convenient mechanism to in-
crease the effective sampling rate by reusing samples across differ-
ent source nodes. by simulating a random walk of length l > k we
can generate k samples for l     k nodes at once due to the marko-

t x2 x1 v x3   =1   =1/q   =1/q   =1/p u s3 s2 s1 s4 s7 s6 s5 bfs dfs v   =1   =1/q   =1/q   =1/p x2 x3 t x1 2

symbol

de   nition

(cid:1)
(cid:0)
(cid:107)    (cid:107)  1
(cid:107)    (cid:107)  2

operator
[f (u) (cid:1) f (v)]i = fi(u)+fi(v)
average
[f (u) (cid:0) f (v)]i = fi(u)     fi(v)
hadamard
(cid:107)f (u)    f (v)(cid:107)  1i = |fi(u)     fi(v)|
weighted-l1
(cid:107)f (u)    f (v)(cid:107)  2i = |fi(u)     fi(v)|2
weighted-l2
table 1: choice of binary operators     for learning edge features.
the de   nitions correspond to the ith component of g(u, v).

(cid:1) per sample. for example, in figure 1 we sample a

vian nature of the random walk. hence, our effective complexity
random walk {u, s4, s5, s6, s8, s9} of length l = 6, which results
in ns(u) = {s4, s5, s6}, ns(s4) = {s5, s6, s8} and ns(s5) =
{s6, s8, s9}. note that sample reuse can introduce some bias in the
overall procedure. however, we observe that it greatly improves
the ef   ciency.
3.2.3 the node2vec algorithm

is o(cid:0)

k(l   k)

l

algorithm 1 the node2vec algorithm.
learnfeatures (graph g = (v, e, w ), dimensions d, walks per

node r, walk length l, context size k, return p, in-out q)
   = preprocessmodi   edweights(g, p, q)
g(cid:48) = (v, e,   )
initialize walks to empty
for iter = 1 to r do
for all nodes u     v do

walk = node2vecwalk(g(cid:48), u, l)
append walk to walks

f = stochasticgradientdescent(k, d, walks)
return f

node2vecwalk (graph g(cid:48) = (v, e,   ), start node u, length l)

inititalize walk to [u]
for walk_iter = 1 to l do
curr = walk[   1]
vcurr = getneighbors(curr, g(cid:48))
s = aliassample(vcurr,   )
append s to walk

return walk

the pseudocode for node2vec, is given in algorithm 1. in any
random walk, there is an implicit bias due to the choice of the start
node u. since we learn representations for all nodes, we offset this
bias by simulating r id93 of    xed length l starting from
every node. at every step of the walk, sampling is done based on
the transition probabilities   vx. the transition probabilities   vx for
the 2nd order markov chain can be precomputed and hence, sam-
pling of nodes while simulating the random walk can be done ef-
   ciently in o(1) time using alias sampling. the three phases of
node2vec, i.e., preprocessing to compute transition probabilities,
random walk simulations and optimization using sgd, are exe-
cuted sequentially. each phase is parallelizable and executed asyn-
chronously, contributing to the overall scalability of node2vec.
node2vec is available at: http://snap.stanford.edu/node2vec.

3.3 learning edge features

the node2vec algorithm provides a semi-supervised method to
learn rich feature representations for nodes in a network. however,
we are often interested in prediction tasks involving pairs of nodes
instead of individual nodes. for instance, in link prediction, we pre-
dict whether a link exists between two nodes in a network. since
our id93 are naturally based on the connectivity structure

between nodes in the underlying network, we extend them to pairs
of nodes using a id64 approach over the feature represen-
tations of the individual nodes.
given two nodes u and v, we de   ne a binary operator     over the
corresponding feature vectors f (u) and f (v) in order to generate
a representation g(u, v) such that g : v    v     rd(cid:48)
where d(cid:48) is
the representation size for the pair (u, v). we want our operators
to be generally de   ned for any pair of nodes, even if an edge does
not exist between the pair since doing so makes the representations
useful for link prediction where our test set contains both true and
false edges (i.e., do not exist). we consider several choices for the
operator     such that d(cid:48) = d which are summarized in table 1.
4. experiments

the objective in eq. 2 is independent of any downstream task and
the    exibility in exploration offered by node2vec lends the learned
feature representations to a wide variety of network analysis set-
tings discussed below.
4.1 case study: les mis  rables network

in section 3.1 we observed that bfs and dfs strategies repre-
sent extreme ends on the spectrum of embedding nodes based on
the principles of homophily (i.e., network communities) and struc-
tural equivalence (i.e., structural roles of nodes). we now aim to
empirically demonstrate this fact and show that node2vec in fact
can discover embeddings that obey both principles.

we use a network where nodes correspond to characters in the
novel les mis  rables [13] and edges connect coappearing charac-
ters. the network has 77 nodes and 254 edges. we set d = 16
and run node2vec to learn feature representation for every node
in the network. the feature representations are clustered using k-
means. we then visualize the original network in two dimensions
with nodes now assigned colors based on their clusters.

figure 3(top) shows the example when we set p = 1, q = 0.5.
notice how regions of the network (i.e., network communities) are
colored using the same color.
in this setting node2vec discov-
ers clusters/communities of characters that frequently interact with
each other in the major sub-plots of the novel. since the edges be-
tween characters are based on coappearances, we can conclude this
characterization closely relates with homophily.

in order to discover which nodes have the same structural roles
we use the same network but set p = 1, q = 2, use node2vec to get
node features and then cluster the nodes based on the obtained fea-
tures. here node2vec obtains a complementary assignment of node
to clusters such that the colors correspond to structural equivalence
as illustrated in figure 3(bottom). for instance, node2vec embeds
blue-colored nodes close together. these nodes represent charac-
ters that act as bridges between different sub-plots of the novel.
similarly, the yellow nodes mostly represent characters that are at
the periphery and have limited interactions. one could assign al-
ternate semantic interpretations to these clusters of nodes, but the
key takeaway is that node2vec is not tied to a particular notion of
equivalence. as we show through our experiments, these equiva-
lence notions are commonly exhibited in most real-world networks
and have a signi   cant impact on the performance of the learned
representations for prediction tasks.
4.2 experimental setup

our experiments evaluate the feature representations obtained
through node2vec on standard supervised learning tasks: multi-
label classi   cation for nodes and link prediction for edges. for
both tasks, we evaluate the performance of node2vec against the
following id171 algorithms:

algorithm

spectral id91
deepwalk
line
node2vec
node2vec settings (p,q)
gain of node2vec [%]

blogcatalog

0.0405
0.2110
0.0784
0.2581
0.25, 0.25

22.3

dataset

ppi wikipedia

0.0681
0.1768
0.1447
0.1791
4, 1
1.3

0.0395
0.1274
0.1164
0.1552
4, 0.5
21.8

table 2: macro-f1 scores for multilabel classi   cation on blogcat-
alog, ppi (homo sapiens) and wikipedia word cooccurrence net-
works with 50% of the nodes labeled for training.

a parameter for the number of context neighborhood nodes to opti-
mize for and the greater the number, the more rounds of optimiza-
tion are required. this parameter is set to unity for line, but since
line completes a single epoch quicker than other approaches, we
let it run for k epochs.

the parameter settings used for node2vec are in line with typical
values used for deepwalk and line. speci   cally, we set d = 128,
r = 10, l = 80, k = 10, and the optimization is run for a sin-
gle epoch. we repeat our experiments for 10 random seed initial-
izations, and our results are statistically signi   cant with a p-value
of less than 0.01.the best in-out and return hyperparameters were
learned using 10-fold cross-validation on 10% labeled data with a
grid search over p, q     {0.25, 0.50, 1, 2, 4}.
4.3 multi-label classi   cation
in the multi-label classi   cation setting, every node is assigned
one or more labels from a    nite set l. during the training phase, we
observe a certain fraction of nodes and all their labels. the task is
to predict the labels for the remaining nodes. this is a challenging
task especially if l is large. we utilize the following datasets:
    blogcatalog [38]: this is a network of social relationships
of the bloggers listed on the blogcatalog website. the la-
bels represent blogger interests inferred through the meta-
data provided by the bloggers. the network has 10,312 nodes,
333,983 edges, and 39 different labels.
    protein-protein interactions (ppi) [5]: we use a subgraph of
the ppi network for homo sapiens. the subgraph corre-
sponds to the graph induced by nodes for which we could
obtain labels from the hallmark gene sets [19] and repre-
sent biological states. the network has 3,890 nodes, 76,584
edges, and 50 different labels.
    wikipedia [20]: this is a cooccurrence network of words
appearing in the    rst million bytes of the wikipedia dump.
the labels represent the part-of-speech (pos) tags inferred
using the stanford pos-tagger [32]. the network has 4,777
nodes, 184,812 edges, and 40 different labels.

all these networks exhibit a fair mix of homophilic and struc-
tural equivalences. for example, we expect the social network of
bloggers to exhibit strong homophily-based relationships; however,
there might also be some    familiar strangers   , i.e., bloggers that do
not interact but share interests and hence are structurally equivalent
nodes. the biological states of proteins in a protein-protein interac-
tion network also exhibit both types of equivalences. for example,
they exhibit structural equivalence when proteins perform functions
complementary to those of neighboring proteins, and at other times,
they organize based on homophily in assisting neighboring proteins
in performing similar functions. the word cooccurence network is
fairly dense, since edges exist between words cooccuring in a 2-
length window in the wikipedia corpus. hence, words having the
same pos tags are not hard to    nd, lending a high degree of ho-
mophily. at the same time, we expect some structural equivalence

figure 3: complementary visualizations of les mis  rables coap-
pearance network generated by node2vec with label colors re   ect-
ing homophily (top) and structural equivalence (bottom).

    spectral id91 [29]: this is a id105 ap-
proach in which we take the top d eigenvectors of the nor-
malized laplacian matrix of graph g as the feature vector
representations for nodes.
    deepwalk [24]: this approach learns d-dimensional feature
representations by simulating uniform id93. the
sampling strategy in deepwalk can be seen as a special case
of node2vec with p = 1 and q = 1.
    line [28]: this approach learns d-dimensional feature rep-
resentations in two separate phases.
in the    rst phase, it
learns d/2 dimensions by bfs-style simulations over imme-
diate neighbors of nodes. in the second phase, it learns the
next d/2 dimensions by sampling nodes strictly at a 2-hop
distance from the source nodes.

we exclude other id105 approaches which have al-
ready been shown to be inferior to deepwalk [24]. we also exclude
a recent approach, grarep [6], that generalizes line to incorpo-
rate information from network neighborhoods beyond 2-hops, but
is unable to ef   ciently scale to large networks.

in contrast to the setup used in prior work for evaluating sampling-
based id171 algorithms, we generate an equal number of
samples for each method and then evaluate the quality of the ob-
tained features on the prediction task. in doing so, we discount for
performance gain observed purely because of the implementation
language (c/c++/python) since it is secondary to the algorithm.
thus, in the sampling phase, the parameters for deepwalk, line
and node2vec are set such that they generate equal number of sam-
ples at runtime. as an example, if k is the overall sampling budget,
then the node2vec parameters satisfy k = r    l    |v |. in the opti-
mization phase, all these benchmarks optimize using sgd with two
key differences that we correct for. first, deepwalk uses hierarchi-
cal sampling to approximate the softmax probabilities with an ob-
jective similar to the one use by node2vec. however, hierarchical
softmax is inef   cient when compared with negative sampling [22].
hence, keeping everything else the same, we switch to negative
sampling in deepwalk which is also the de facto approximation in
node2vec and line. second, both node2vec and deepwalk have

figure 4: performance evaluation of different benchmarks on varying the amount of labeled data used for training. the x axis denotes the
fraction of labeled data, whereas the y axis in the top and bottom rows denote the micro-f1 and macro-f1 scores respectively. deepwalk
and node2vec give comparable performance on ppi. in all other networks, across all fractions of labeled data node2vec performs best.

in the pos tags due to syntactic grammar patterns such as nouns
following determiners, punctuations succeeding nouns etc.
experimental results. the node feature representations are input
to a one-vs-rest id28 classi   er with l2 id173.
the train and test data is split equally over 10 random instances. we
use the macro-f1 scores for comparing performance in table 2 and
the relative performance gain is over the closest benchmark. the
trends are similar for micro-f1 and accuracy and are not shown.

from the results, it is evident we can see how the added    exi-
bility in exploring neighborhoods allows node2vec to outperform
the other benchmark algorithms. in blogcatalog, we can discover
the right mix of homophily and structural equivalence by setting
parameters p and q to low values, giving us 22.3% gain over deep-
walk and 229.2% gain over line in macro-f1 scores. line showed
worse performance than expected, which can be explained by its
inability to reuse samples, a feat that can be easily done using the
random walk methods. even in our other two networks, where we
have a mix of equivalences present, the semi-supervised nature of
node2vec can help us infer the appropriate degree of exploration
necessary for id171. in the case of ppi network, the best
exploration strategy (p = 4, q = 1) turns out to be virtually indis-
tinguishable from deepwalk   s uniform (p = 1, q = 1) exploration
giving us only a slight edge over deepwalk by avoiding redudancy
in already visited nodes through a high p value, but a convincing
23.8% gain over line in macro-f1 scores. however, in general,
the uniform id93 can be much worse than the exploration
strategy learned by node2vec. as we can see in the wikipedia word
cooccurrence network, uniform walks cannot guide the search pro-
cedure towards the best samples and hence, we achieve a gain of
21.8% over deepwalk and 33.2% over line.

for a more    ne-grained analysis, we also compare performance
while varying the train-test split from 10% to 90%, while learn-
ing parameters p and q on 10% of the data as before. for brevity,

we summarize the results for the micro-f1 and macro-f1 scores
graphically in figure 4. here we make similar observations. all
methods signi   cantly outperform spectral id91, deepwalk
outperforms line, node2vec consistently outperforms line and
achieves large improvement over deepwalk across domains. for
example, we achieve the biggest improvement over deepwalk of
26.7% on blogcatalog at 70% labeled data. in the worst case, the
search phase has little bearing on learned representations in which
case node2vec is equivalent to deepwalk. similarly, the improve-
ments are even more striking when compared to line, where in
addition to drastic gain (over 200%) on blogcatalog, we observe
high magnitude improvements upto 41.1% on other datasets such
as ppi while training on just 10% labeled data.
4.4 parameter sensitivity

the node2vec algorithm involves a number of parameters and
in figure 5a, we examine how the different choices of parameters
affect the performance of node2vec on the blogcatalog dataset us-
ing a 50-50 split between labeled and unlabeled data. except for the
parameter being tested, all other parameters assume default values.
the default values for p and q are set to unity.

we measure the macro-f1 score as a function of parameters p
and q. the performance of node2vec improves as the in-out pa-
rameter p and the return parameter q decrease. this increase in
performance can be based on the homophilic and structural equiva-
lences we expect to see in blogcatalog. while a low q encourages
outward exploration, it is balanced by a low p which ensures that
the walk does not go too far from the start node.

we also examine how the number of features d and the node   s
neighborhood parameters (number of walks r, walk length l, and
neighborhood size k) affect the performance. we observe that per-
formance tends to saturate once the dimensions of the representa-
tions reaches around 100. similarly, we observe that increasing
the number and length of walks per source improves performance,

0.00.20.40.60.81.00.150.200.250.300.350.400.45micro-f1scoreblogcatalog0.00.20.40.60.81.00.000.050.100.150.200.250.30macro-f1score0.00.20.40.60.81.00.000.050.100.150.200.250.30ppi(homosapiens)0.00.20.40.60.81.00.000.050.100.150.200.250.300.00.20.40.60.81.00.350.400.450.500.550.60wikipedia0.00.20.40.60.81.00.000.050.100.150.200.250.30spectralid91deepwalklinenode2vecfigure 5: (a). parameter sensitivity (b). perturbation analysis for multilabel classi   cation on the blogcatalog network.

(a)

(b)

which is not surprising since we have a greater overall sampling
budget k to learn representations. both these parameters have a
relatively high impact on the performance of the method. interest-
ingly, the context size, k also improves performance at the cost of
increased optimization time. however, the performance differences
are not that large in this case.
4.5 perturbation analysis

for many real-world networks, we do not have access to accurate
information about the network structure. we performed a pertur-
bation study where we analyzed the performance of node2vec for
two imperfect information scenarios related to the edge structure
in the blogcatalog network. in the    rst scenario, we measure per-
formace as a function of the fraction of missing edges (relative to
the full network). the missing edges are chosen randomly, subject
to the constraint that the number of connected components in the
network remains    xed. as we can see in figure 5b(top), the de-
crease in macro-f1 score as the fraction of missing edges increases
is roughly linear with a small slope. robustness to missing edges
in the network is especially important in cases where the graphs
are evolving over time (e.g., id191), or where network
construction is expensive (e.g., biological networks).

in the second perturbation setting, we have noisy edges between
randomly selected pairs of nodes in the network. as shown in
figure 5b(bottom), the performance of node2vec declines slightly
faster initially when compared with the setting of missing edges,
however, the rate of decrease in macro-f1 score gradually slows
down over time. again, the robustness of node2vec to false edges
is useful in several situations such as sensor networks where the
measurements used for constructing the network are noisy.
4.6 scalability

to test for scalability, we learn node representations using node2vec

with default parameter values for erdos-renyi graphs with increas-
ing sizes from 100 to 1,000,000 nodes and constant average degree

figure 6: scalability of node2vec on erdos-renyi graphs with an
average degree of 10.

of 10. in figure 6, we empirically observe that node2vec scales lin-
early with increase in number of nodes generating representations
for one million nodes in less than four hours. the sampling pro-
cedure comprises of preprocessing for computing transition proba-
bilities for our walk (negligibly small) and simulation of random
walks. the optimization phase is made ef   cient using negative
sampling [22] and asynchronous sgd [26].

many ideas from prior work serve as useful pointers in mak-
ing the sampling procedure computationally ef   cient. we showed
how id93, also used in deepwalk [24], allow the sampled
nodes to be reused as neighborhoods for different source nodes ap-
pearing in the walk. alias sampling allows our walks to general-
ize to weighted networks, with little preprocessing [28]. though
we are free to set the search parameters based on the underlying
task and domain at no additional cost, learning the best settings of

 3 2 10123log2p0.160.180.200.220.240.260.28macro-f1score 3 2 10123log2q0.160.180.200.220.240.260.283456789log2d0.160.180.200.220.240.260.2868101214161820numberofwalkspernode,r0.160.180.200.220.240.260.28macro-f1score30405060708090100110lengthofwalk,l0.160.180.200.220.240.260.288101214161820contextsize,k0.160.180.200.220.240.260.28 3 2 10123log2p0.160.180.200.220.240.260.28macro-f1score 3 2 10123log2q0.160.180.200.220.240.260.280.00.10.20.30.40.50.6fractionofmissingedges0.000.050.100.150.200.250.30macro-f1score68101214161820numberofwalkspernode,r0.160.180.200.220.240.260.28macro-f1score30405060708090100110lengthofwalk,l0.160.180.200.220.240.260.280.00.10.20.30.40.50.6fractionofadditionaledges0.000.050.100.150.200.250.30macro-f1score1234567log10nodes01234log10time(inseconds)sampling+optimizationtimesamplingtimescore
common neighbors
jaccard   s coef   cient
adamic-adar score
preferential attachment

(cid:80)

de   nition

| n (u)     n (v) |

|n (u)   n (v)|
|n (u)   n (v)|

t   n (u)   n (v)
| n (u) |    | n (v) |

log|n (t)|

1

table 3: link prediction heuristic scores for node pair (u, v) with
immediate neighbor sets n (u) and n (v) respectively.

our search parameters adds an overhead. however, as our exper-
iments con   rm, this overhead is minimal since node2vec is semi-
supervised and hence, can learn these parameters ef   ciently with
very little labeled data.
4.7 link prediction

in link prediction, we are given a network with a certain frac-
tion of edges removed, and we would like to predict these missing
edges. we generate the labeled dataset of edges as follows: to ob-
tain positive examples, we remove 50% of edges chosen randomly
from the network while ensuring that the residual network obtained
after the edge removals is connected, and to generate negative ex-
amples, we randomly sample an equal number of node pairs from
the network which have no edge connecting them.

since none of id171 algorithms have been previously
used for link prediction, we additionally evaluate node2vec against
some popular heuristic scores that achieve good performance in
link prediction. the scores we consider are de   ned in terms of the
neighborhood sets of the nodes constituting the pair (see table 3).
we test our benchmarks on the following datasets:
    facebook [14]: in the facebook network, nodes represent
users, and edges represent a friendship relation between any
two users. the network has 4,039 nodes and 88,234 edges.
    protein-protein interactions (ppi) [5]: in the ppi network for
homo sapiens, nodes represent proteins, and an edge indi-
cates a biological interaction between a pair of proteins. the
network has 19,706 nodes and 390,633 edges.
    arxiv astro-ph [14]: this is a collaboration network gen-
erated from papers submitted to the e-print arxiv where nodes
represent scientists, and an edge is present between two sci-
entists if they have collaborated in a paper. the network has
18,722 nodes and 198,110 edges.

experimental results. we summarize our results for link pre-
diction in table 4. the best p and q parameter settings for each
node2vec entry are omitted for ease of presentation. a general ob-
servation we can draw from the results is that the learned feature
representations for node pairs signi   cantly outperform the heuris-
tic benchmark scores with node2vec achieving the best auc im-
provement on 12.6% on the arxiv dataset over the best performing
baseline (adamic-adar [1]).

amongst the id171 algorithms, node2vec outperforms
both deepwalk and line in all networks with gain up to 3.8% and
6.5% respectively in the auc scores for the best possible choices
of the binary operator for each algorithm. when we look at opera-
tors individually (table 1), node2vec outperforms deepwalk and
line barring a couple of cases involving the weighted-l1 and
weighted-l2 operators in which line performs better. overall,
the hadamard operator when used with node2vec is highly stable
and gives the best performance on average across all networks.

5. discussion and conclusion

in this paper, we studied id171 in networks as a search-
based optimization problem. this perspective gives us multiple ad-
vantages. it can explain classic search strategies on the basis of

op

algorithm

(a) deepwalk

(b) deepwalk

common neighbors
jaccard   s coef   cient
adamic-adar
pref. attachment
spectral id91

line
node2vec
spectral id91

line
node2vec
spectral id91

line
node2vec
spectral id91

(c) deepwalk

(d) deepwalk

line
node2vec

facebook
0.8100
0.8880
0.8289
0.7137
0.5960
0.7238
0.7029
0.7266
0.6192
0.9680
0.9490
0.9680
0.7200
0.9574
0.9483
0.9602
0.7107
0.9584
0.9460
0.9606

dataset

ppi
0.7142
0.7018
0.7126
0.6670
0.6588
0.6923
0.6330
0.7543
0.4920
0.7441
0.7249
0.7719
0.6356
0.6026
0.7024
0.6292
0.6026
0.6118
0.7106
0.6236

arxiv
0.8153
0.8067
0.8315
0.6996
0.5812
0.7066
0.6516
0.7221
0.5740
0.9340
0.8902
0.9366
0.7099
0.8282
0.8809
0.8468
0.6765
0.8305
0.8862
0.8477

table 4: area under curve (auc) scores for link prediction. com-
parison with popular baselines and embedding based methods boot-
stapped using binary operators: (a) average, (b) hadamard, (c)
weighted-l1, and (d) weighted-l2 (see table 1 for de   nitions).

the exploration-exploitation trade-off. additionally, it provides a
degree of interpretability to the learned representations when ap-
plied for a prediction task. for instance, we observed that bfs can
explore only limited neighborhoods. this makes bfs suitable for
characterizing structural equivalences in network that rely on the
immediate local structure of nodes. on the other hand, dfs can
freely explore network neighborhoods which is important in dis-
covering homophilous communities at the cost of high variance.

both deepwalk and line can be seen as rigid search strategies
over networks. deepwalk [24] proposes search using uniform ran-
dom walks. the obvious limitation with such a strategy is that it
gives us no control over the explored neighborhoods. line [28]
proposes primarily a breadth-   rst strategy, sampling nodes and op-
timizing the likelihood independently over only 1-hop and 2-hop
neighbors. the effect of such an exploration is easier to charac-
terize, but it is restrictive and provides no    exibility in exploring
nodes at further depths. in contrast, the search strategy in node2vec
is both    exible and controllable exploring network neighborhoods
through parameters p and q. while these search parameters have in-
tuitive interpretations, we obtain best results on complex networks
when we can learn them directly from data. from a practical stand-
point, node2vec is scalable and robust to perturbations.

we showed how extensions of node embeddings to link predic-
tion outperform popular heuristic scores designed speci   cally for
this task. our method permits additional binary operators beyond
those listed in table 1. as a future work, we would like to explore
the reasons behind the success of hadamard operator over oth-
ers, as well as establish interpretable equivalence notions for edges
based on the search parameters. future extensions of node2vec
could involve networks with special structure such as heteroge-
neous information networks, networks with explicit domain fea-
tures for nodes and edges and signed-edge networks. continuous
feature representations are the backbone of many deep learning al-
gorithms, and it would be interesting to use node2vec representa-
tions as building blocks for end-to-end deep learning on graphs.

acknowledgements. we are thankful to austin benson, will hamil-
ton, rok sosi  c, marinka   itnik as well as the anonymous review-
ers for their helpful comments. this research has been supported
in part by nsf cns-1010921, iis-1149837, nih bd2k, aro
muri, darpa xdata, darpa simplex, stanford data sci-
ence initiative, boeing, lightspeed, sap, and volkswagen.

6. references
[1] l. a. adamic and e. adar. friends and neighbors on the

web. social networks, 25(3):211   230, 2003.

[2] l. backstrom and j. leskovec. supervised id93:
predicting and recommending links in social networks. in
wsdm, 2011.

[3] m. belkin and p. niyogi. laplacian eigenmaps and spectral

techniques for embedding and id91. in nips, 2001.
[4] y. bengio, a. courville, and p. vincent. representation
learning: a review and new perspectives. ieee tpami,
35(8):1798   1828, 2013.

[5] b.-j. breitkreutz, c. stark, t. reguly, l. boucher,

a. breitkreutz, m. livstone, r. oughtred, d. h. lackner,
j. b  hler, v. wood, et al. the biogrid interaction database.
nucleic acids research, 36:d637   d640, 2008.

[6] s. cao, w. lu, and q. xu. grarep: learning graph

representations with global structural information. in cikm,
2015.

[7] s. fortunato. community detection in graphs. physics

reports, 486(3-5):75     174, 2010.

[8] b. gallagher and t. eliassi-rad. leveraging

label-independent features for classi   cation in sparsely
labeled networks: an empirical study. in lecture notes in
computer science: advances in social network mining and
analysis. springer, 2009.

[9] z. s. harris. word. distributional structure,

10(23):146   162, 1954.

[10] k. henderson, b. gallagher, t. eliassi-rad, h. tong,

s. basu, l. akoglu, d. koutra, c. faloutsos, and l. li.
rolx: structural role extraction & mining in large graphs. in
kdd, 2012.

[11] k. henderson, b. gallagher, l. li, l. akoglu, t. eliassi-rad,
h. tong, and c. faloutsos. it   s who you know: graph mining
using recursive structural features. in kdd, 2011.

[12] p. d. hoff, a. e. raftery, and m. s. handcock. latent space

approaches to social network analysis. j. of the american
statistical association, 2002.

[13] d. e. knuth. the stanford graphbase: a platform for
combinatorial computing, volume 37. addison-wesley
reading, 1993.

[14] j. leskovec and a. krevl. snap datasets: stanford large
network dataset collection. http://snap.stanford.edu/data,
june 2014.

[15] k. li, j. gao, s. guo, n. du, x. li, and a. zhang. lrbm: a

restricted id82 based approach for
representation learning on linked data. in icdm, 2014.

[16] x. li, n. du, h. li, k. li, j. gao, and a. zhang. a deep

learning approach to link prediction in dynamic networks. in
icdm, 2014.

[17] y. li, d. tarlow, m. brockschmidt, and r. zemel. gated

graph sequence neural networks. in iclr, 2016.

[18] d. liben-nowell and j. kleinberg. the link-prediction

problem for social networks. j. of the american society for
information science and technology, 58(7):1019   1031, 2007.

[19] a. liberzon, a. subramanian, r. pinchback,

h. thorvaldsd  ttir, p. tamayo, and j. p. mesirov. molecular
signatures database (msigdb) 3.0. bioinformatics,
27(12):1739   1740, 2011.

[20] m. mahoney. large text compression benchmark.

www.mattmahoney.net/dc/textdata, 2011.

[21] t. mikolov, k. chen, g. corrado, and j. dean. ef   cient

estimation of word representations in vector space. in iclr,
2013.

[22] t. mikolov, i. sutskever, k. chen, g. s. corrado, and

j. dean. distributed representations of words and phrases
and their compositionality. in nips, 2013.

[23] j. pennington, r. socher, and c. d. manning. glove: global

vectors for word representation. in emnlp, 2014.

[24] b. perozzi, r. al-rfou, and s. skiena. deepwalk: online

learning of social representations. in kdd, 2014.

[25] p. radivojac, w. t. clark, t. r. oron, a. m. schnoes,

t. wittkop, a. sokolov, k. graim, c. funk, verspoor, et al.
a large-scale evaluation of computational protein function
prediction. nature methods, 10(3):221   227, 2013.

[26] b. recht, c. re, s. wright, and f. niu. hogwild!: a
lock-free approach to parallelizing stochastic gradient
descent. in nips, 2011.

[27] s. t. roweis and l. k. saul. nonlinear dimensionality

reduction by locally linear embedding. science,
290(5500):2323   2326, 2000.

[28] j. tang, m. qu, m. wang, m. zhang, j. yan, and q. mei.
line: large-scale information network embedding. in
www, 2015.

[29] l. tang and h. liu. leveraging social media networks for

classi   cation. data mining and knowledge discovery,
23(3):447   478, 2011.

[30] j. b. tenenbaum, v. de silva, and j. c. langford. a global

geometric framework for nonlinear id84.
science, 290(5500):2319   2323, 2000.

[31] f. tian, b. gao, q. cui, e. chen, and t.-y. liu. learning
deep representations for graph id91. in aaai, 2014.

[32] k. toutanova, d. klein, c. d. manning, and y. singer.

feature-rich part-of-speech tagging with a cyclic dependency
network. in naacl, 2003.

[33] g. tsoumakas and i. katakis. multi-label classi   cation: an

overview. dept. of informatics, aristotle university of
thessaloniki, greece, 2006.

[34] a. vazquez, a. flammini, a. maritan, and a. vespignani.

global protein function prediction from protein-protein
interaction networks. nature biotechnology, 21(6):697   700,
2003.

[35] s. yan, d. xu, b. zhang, h.-j. zhang, q. yang, and s. lin.
graph embedding and extensions: a general framework for
id84. ieee tpami, 29(1):40   51, 2007.
[36] j. yang and j. leskovec. overlapping communities explain
core-periphery organization of networks. proceedings of the
ieee, 102(12):1892   1902, 2014.

[37] s.-h. yang, b. long, a. smola, n. sadagopan, z. zheng,

and h. zha. like like alike: joint friendship and interest
propagation in social networks. in www, 2011.

[38] r. zafarani and h. liu. social computing data repository at

asu, 2009.

[39] s. zhai and z. zhang. dropout training of matrix

factorization and autoencoder for link prediction in sparse
graphs. in sdm, 2015.

