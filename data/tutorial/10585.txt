reading scene text in deep convolutional sequences

pan he   1, 2, weilin huang   1, 2, yu qiao1, chen change loy2, 1, and xiaoou tang2, 1

1shenzhen key lab of comp. vis and pat. rec.,

shenzhen institutes of advanced technology, chinese academy of sciences, china

2department of information engineering, the chinese university of hong kong

{pan.he,wl.huang,yu.qiao}@siat.ac.cn, {ccloy,xtang}@ie.cuhk.edu.hk

5
1
0
2
 
c
e
d
0
2

 

 
 
]

v
c
.
s
c
[
 
 

2
v
5
9
3
4
0

.

6
0
5
1
:
v
i
x
r
a

abstract

we develop a deep-text recurrent network (dtrn)
that regards scene text reading as a sequence labelling
problem. we leverage recent advances of deep convo-
lutional neural networks to generate an ordered high-
level sequence from a whole word image, avoiding the
dif   cult character segmentation problem. then a deep
recurrent model, building on long short-term memory
(lstm), is developed to robustly recognize the gener-
ated id98 sequences, departing from most existing ap-
proaches recognising each character independently. our
model has a number of appealing properties in com-
parison to existing scene text recognition methods: (i)
it can recognise highly ambiguous words by leverag-
ing meaningful context information, allowing it to work
reliably without either pre- or post-processing; (ii) the
deep id98 feature is robust to various image distortions;
(iii) it retains the explicit order information in word im-
age, which is essential to discriminate word strings; (iv)
the model does not depend on pre-de   ned dictionary,
and it can process unknown words and arbitrary strings.
it achieves impressive results on several benchmarks,
advancing the-state-of-the-art substantially.

text recognition in natural image has received increas-
ing attention in id161 and machine intelligence,
due to its numerous practical applications. this problem in-
cludes two sub tasks, namely text detection (huang, qiao,
and tang 2014; yin et al. 2014; he et al. 2015; zhang et
al. 2015; huang et al. 2013; neumann and matas 2013)
and text-line/word recognition (jaderberg, vedaldi, and zis-
serman 2014; almaz  an et al. 2014; jaderberg et al. 2014;
bissacco et al. 2013; yao et al. 2014). this work focuses on
the latter that aims to retrieve a text string from a cropped
word image. though huge efforts have been devoted to
this task, reading text in unconstrained environment is still
extremely challenging, and remains an open problem, as
substantiated in recent literature (jaderberg et al. 2015b;
almaz  an et al. 2014). the main dif   culty arises from the
large diversity of text patterns (e.g. low resolution, low
contrast, and blurring), and highly complicated background
clutters. consequently, individual character segmentation or
separation is extremely challenging.

most previous studies focus on developing powerful char-
   authors contributed equally

figure 1: the word image recognition pipeline of the proposed
deep-text recurrent networks (dtrn) model.

acter classi   ers, some of which are incorporated with a lan-
guage model, leading to the state-of-the-art performance
(jaderberg, vedaldi, and zisserman 2014; bissacco et al.
2013; yao et al. 2014; lee et al. 2014). these approaches
mainly follow the pipeline of conventional ocr techniques
by    rst involving a character-level segmentation, then fol-
lowed by an isolated character classi   er and post-processing
for recognition. they also adopt deep neural networks for
representation learning, but the recognition is still con   ned
to character-level classi   cation. thus their performance are
severely harmed by the dif   culty of character segmentation
or separation. importantly, recognizing each character inde-
pendently discards meaningful context information of the
words, signi   cantly reducing its reliability and robustness.
first, we wish to address the issue of context information
learning. the main inspiration for approaching this issue
comes from the recent success of recurrent neural networks
(id56) for handwriting recognition (graves, liwicki, and
fernandez 2009; graves and schmidhuber 2008), speech
recognition (graves and jaitly 2014), and language transla-

tion (sutskever, vinyals, and le 2014). we found the strong
capability of id56 in learning continuous sequential features
particularly well-suited for text recognition task to retain
the meaningful interdependencies of the continuous text se-
quence. we note that id56s have been formulated for rec-
ognizing handwritten or documented images (graves, li-
wicki, and fernandez 2009; graves and schmidhuber 2008;
breuel et al. 2013), nevertheless, the background in these
tasks is relatively plain, and the raw image feature can be
directly input to id56 for recognition, or the text stroke in-
formation can be easily extracted or binarized at pixel level,
making it possible to manually design a sequential heuristic
feature for the input to id56. in contrast, the scene text im-
age is much more complicated where pixel-level segmenta-
tion is extremely dif   cult, especially for highly ambiguous
images (fig. 1). thus it is non-trivial to directly apply the
sequence labelling models to scene text.

consequently, the second challenge we need to resolve is
the issue of character segmentation. we argue that individ-
ual character segmentation is not a    must    in text recogni-
tion. the key is to acquire strong representation from the
image, with explicit order information. the strong repre-
sentation ensures robustness to various distortions and back-
ground clutters, whilst the explicit order information is cru-
cial to discriminate a meaningful word. the ordered strong
feature sequence computed from the sequential regions of
word image allows each frame region to locate the part of a
character, which can be stored sequentially by the recurrent
model. this makes it possible to recognize the character ro-
bustly by using its continuous parts, and thus successfully
avoid the character segmentation.

to this end, we develop a deep recurrent model that reads
word images in deep convolutional sequences. the new
model is referred as deep-text recurrent network (dtrn),
of which the pipeline is shown in fig. 1. it takes both the ad-
vantages of the deep id98 for image representation learning
and the id56 model for sequence labelling, with the follow-
ing appealing properties:
1) strong and high-level representation without charac-
ter segmentation     the dtrn generates a convolutional
image sequence, which is explicitly ordered by scanning a
sliding window through a word image. the id98 sequence
captures meaningful high-level representation that is robust
to various image distortions. it differs signi   cantly from
manually-designed sequential features used by most prior
studies based on sequence labelling (breuel et al. 2013;
graves, liwicki, and fernandez 2009; su and lu 2014).
the sequence is generated without any low-level operation
or challenging character segmentation.
2) exploiting context information in contrast to existing
systems (bissacco et al. 2013; jaderberg, vedaldi, and zis-
serman 2014; wang et al. 2012) that read each character in-
dependently, we formulate this task as a sequence labelling
problem. speci   cally, we build our system on the lstm, so
as to capture the interdependencies inherent in the deep se-
quences. such consideration allows our system to recognize
highly ambiguous words, and work reliably without either
pre- or post-processing. in addition, the recurrence allows it
to process sequences of various lengths, going beyond tradi-

tional neural networks of    xed-length input and output.
3) process unknown words and arbitrary strings with
properly learned deep id98s and id56s, our model does not
depend on any pre-de   ned dictionary, unlike exiting stud-
ies (jaderberg et al. 2015b; jaderberg, vedaldi, and zisser-
man 2014; wang et al. 2012), and it can process unknown
words, and arbitrary strings, including multiple words.

we note that id98 and id56 have been independently ex-
ploited in the domain of text recognition. our main con-
tribution in this study is to develop a uni   ed deep recur-
rent system that leverages both the advantages of id98 and
id56 for the dif   cult scene text recognition problem, which
has been solved based on analyzing character independently.
this is the    rst attempt to show the effectiveness of exploit-
ing convolutional sequence with sequence labeling model
for this challenging task. we highlight the considerations re-
quired to make this system reliable and discuss the unique
advantages offered by it. the proposed dtrn demonstrate
promising results on a number of benchmarks, improving
recent results of (jaderberg, vedaldi, and zisserman 2014;
almaz  an et al. 2014) considerably.

related work

previous work mainly focuses on developing a powerful
character classi   er with manually-designed image features.
a hog feature with random ferns was developed for char-
acter classi   cation in (wang, babenko, and belongie 2011).
neumann and matas proposed new oriented strokes for
character detection and classi   cation (neumann and matas
2013). their performance is limited by the low-level fea-
tures. in (lee et al. 2014), a mid-level representation of
characters was developed by proposing a discriminative fea-
ture pooling. similarly, yao et al. proposed the mid-level
strokelets to describe the parts of characters (yao et al.
2014).

recent advances of dnn for image representation en-
courage the development of more powerful character classi-
   ers, leading to the state-of-the-art performance on this task.
the pioneer work was done by lecun et al., who designed
a id98 for isolated handwriting digit recognition (lecun et
al. 1998). a two-layer id98 system was proposed for both
character detection and classi   cation in (wang et al. 2012).
photoocr system employs a    ve-layer dnn for character
recognition (bissacco et al. 2013). similarly, jaderberg et
al. (jaderberg, vedaldi, and zisserman 2014) proposed novel
deep features by employing a maxout id98 model for learn-
ing common features, which were subsequently used for a
number of different tasks, such as character classi   cation,
location optimization and language model learning.

these approaches treat isolated character classi   cation
and subsequent word recognition separately. they do not
unleash the full potential of word context information in the
recognition. they often design complicated optimization al-
gorithm to infer word string by incorporating multiple ad-
ditional visual cues, or require a number of post-processing
steps to re   ne the results (jaderberg, vedaldi, and zisserman
2014; bissacco et al. 2013). our model differs signi   cantly
from them by exploring the recurrence of deep features, al-
lowing it to leverage the underlying context information to

directly recognise the whole word image in a deep sequence,
without a language model and any kind of post-processing.
there is another group of studies that recognise text
strings from the whole word images. almazan et al. (al-
maz  an et al. 2014) proposed a subspace regression method
to jointly embed both word image and its string into a com-
mon subspace. a powerful id98 model was developed to
compute a deep feature from a whole word image in (jader-
berg et al. 2015b). again, our model differs from these stud-
ies in the deep recurrent nature. our sequential feature in-
cludes explicit spatial order information, which is crucial to
discriminate the order-sensitive word string. while the other
global representation would lost such strict order, leading
to poorer discrimination power. furthermore, the model of
(jaderberg et al. 2015b) is strictly constrained by the pre-
de   ned dictionary, making it unable to recognise a novel
word. by contrast, our model can process an unknown word.
for unconstrained recognition, jaderberg et al. proposed
another id98 model, which incorporates a conditional ran-
dom field (jaderberg et al. 2015a). this model recognizes
word strings in character sequences, allowing it for process-
ing a single unknown word. but the model is highly sensitive
to the non-character space, making it dif   cult to recognize
multiple words. our recurrent model can process arbitrary
strings, including multiple words, and thus generalizes bet-
ter. our method also relates to (su and lu 2014), where a
id56 is built upon hog features. however, its performance
is signi   cantly limited by the hog. while the strong deep
id98 feature is crucial to the success of our model.

our approach is partially motivated by the recent success
of deep models for image captioning, where the combination
of the id98 and id56 has been applied (andrej and li 2015;
donahue et al. 2015; alsharif and pineau 2013). they ex-
plored the id98 for computing a deep feature from a whole
image, followed by a id56 to decode it into a sequence of
words. renet (visin et al. 2015) was proposed to directly
compute the deep image feature by using four id56 to sweep
across the image. generally, these models do not explicitly
store the strict spatial information by using the global im-
age representation. by contrast, our word images include ex-
plicit order information of its string, which is a crucial cue
to discriminate a word. our goal here is to derive a set of
robust sequential features from the word image, and design
an new model that bridges the image representation learning
and sequence labelling task.

deep-text recurrent networks

the pipeline of deep-text recurrent network (dtrn) is
shown in fig. 1. it starts by encoding a given word image
into an ordered sequence with a specially designed id98.
then a id56 is employed to decode (recognise) the id98
sequence into a word string. the system is end-to-end, i.e. it
takes a word image as input and directly outputs the corre-
sponding word string, without any pre- and post-processing
steps. both the input word image and output string can be
of varying lengths. this section revisits some important de-
tails of id98 and id56 and highlight the considerations that
make their combination reliable for scene text recognition.

formally, we formulate the word image recognition as a
sequence labeling problem. we maximize the id203 of
the correct word strings (sw), given an input image (i),

     = arg max

  

log p (sw|i;   ),

(1)

(cid:88)

(i,sw)

w, s2

where    are the parameters of
the recurrent system.
(i, sw)         is a sample pair from a training set,    , where
w } is the ground truth word string
sw = {s1
(containing k characters) of the image i. commonly, the
k(cid:88)
chain rule is applied to model the joint id203 over sw,

w, ..., sk

w|i, s0

w, ..., si   1

w ;   )

log p (sw|i;   ) =

log p (si

(2)

i=1

thus we optimize the sum of the log probabilities over all
sample pairs in the training set (   ) to learn the model pa-
rameters. we develop a id56 to model the sequential prob-
abilities p (si
w ), where the variable number
of the sequentially conditioned characters can be expressed
by an internal state of the id56 in hidden layer, ht. this in-
ternal state is updated when the next sequential input xt is
presented by computing a non-linear function h,

w, ..., si   1

w|i, s0

ht+1 = h(ht, xt)

(3)
where the non-linear function h de   nes exact form of the
proposed recurrent system. x = {x1, x2, x3, ..., xt} is the
sequential id98 features computed from the word image,

{x1, x2, x3, ..., xt} =   (i)

(4)
designs of the    and h play crucial roles in the proposed
system. we develop a id98 model to generate the sequential
xt, and de   ne h with a long short-term memory (lstm)
architecture (hochreiter and schmidhuber 1997).
sequence generation with maxout id98
the main challenge of obtaining low-level sequential repre-
sentation from the word images arises from the dif   culties
of correct segmentation at either pixel or character level. we
argue that it is not necessary to perform such low-level fea-
ture extraction. on the contrary, it is more natural to describe
word strings in sequences where their explicit order infor-
mation is retained. this information is extremely important
to discriminate a word string. furthermore, the variations be-
tween continuous examples in a sequence should encode ad-
ditional information, which could be useful in making more
reliable prediction. by considering these factors, we propose
to generate an explicitly ordered deep sequence with a id98
model, by sliding a sub window through the word image.

to this end, we develop a maxout network (goodfel-
low et al. 2013) for computing the deep feature. it has
been shown that the maxout id98 is powerful for charac-
ter classi   cation (jaderberg, vedaldi, and zisserman 2014;
alsharif and pineau 2013). the basic pipeline is to compute
point-wise maximum through a number of grouped feature
maps or channels. our networks is shown in fig 2 (a), the
input image is of size 32    32, corresponding to the size
of sliding-window. it has    ve convolutional layers, each of

figure 2: the structures of our maxout id98 model.

which is followed by a two- or four-group maxout operation,
with various numbers of feature maps, i.e. 48, 64, 128,128
and 36, respectively. similar to the id98 used in (jaderberg,
vedaldi, and zisserman 2014), our networks does not in-
volve any pooling operation, and the output of last two con-
volutional layers are just one pixel. this allows our id98 to
convolute the whole word images at once, leading to a sig-
ni   cant computational ef   ciency. for each word image, we
resize it into the height of 32, and keep its original aspect ra-
tio unchanged. we apply the learned    lters to the resized im-
age, and get a 128d id98 sequence directly from the output
of last second convolutional layer. this operation is similar
to computing deep feature independently from the sliding-
window by moving it densely through the image, but with
much computational ef   ciency. our maxout id98 is trained
on 36-class case insensitive character images.

sequence labeling with id56
we believe that the interdependencies between the convo-
lutional sequence include meaningful context information
which would be greatly helpful to identify an ambitious
character. id56 has shown strong capability for learning
meaningful structure from an ordered sequence. another im-
portant property of the id56 is that the rate of changes of
the internal state can be    nely modulated by the recurrent
weights, which contributes to its robustness against localised
distortions of the input data (graves, liwicki, and fernandez
2009). thus we propose the use of id56 in our framework
to model the generated id98 sequence {x1, x2, x3, ..., xt}.
the structure of our id56 model is shown in fig. 3.

the main shortcoming of the standard id56 is the vanish-
ing gradient problem, making it hard to transmit the gradi-
ent information consistently over long time (hochreiter and
schmidhuber 1997). this is a crucial issue in designing a
id56 model, and the long short-term memory (lstm) was
proposed specially to address this problem (hochreiter and
schmidhuber 1997). the lstm de   nes a new neuron or
cell structure in the hidden layer with three additional mul-
tiplicative gates: the input gate, forget gate and output gate.
these new cells are referred as memory cells, which allow
the lstm to learn meaningful long-range interdependen-
cies. we skip standard descriptions of the lstm memory
cells and its formulation, by leaving them in the supplemen-
tary material.

the sequence labelling of varying lengths is processed by

figure 3: the structure of our recurrent neural networks.

recurrently implementing the lstm memory for each se-
quential input xt, such that all lstms share the same pa-
rameters. the output of the lstm ht is fed to the lstm
at next input xt+1. it is also used to compute the current
output, which is transformed to the estimated probabilities
over all possible characters. it    nally generates a sequence
of the estimations with the same length of input sequence,
p = {p1, p2, p3, ..., pt}.

due to the unsegmented nature of the word image at the
character level, the length of the lstm outputs (t ) is not
consistent with the length of a target word string, |sw| = k.
this makes it dif   cult to train our recurrent system directly
with the target strings. to this end, we follow the recurrent
system developed for the handwriting recognition (graves,
liwicki, and fernandez 2009) by applying a connection-
ist temporal classi   cation (ctc) (graves and schmidhuber
2005) to approximately map the lstm sequential output (p)
into its target string as follow,
w     b(arg max
s(cid:63)

(5)
where the projection b removes the repeated labels and the
non-character labels (graves and schmidhuber 2005). for
example, b(   gg   o   oo   dd   ) = good. the ctc looks for
an approximately optimized path (  ) with maximum proba-
bility through the lstms output sequence, which aligns the
different lengths of lstm sequence and the word string.

p (  |p))

the ctc is speci   cally designed for the sequence la-
belling tasks where it is hard to pre-segment the input se-
quence to the segments that exactly match a target sequence.

  

(a) the lstm output

(b) the ctc path

(c) p and   

(d) dtrn vs deepfeatures

figure 4: (a-c)id56s training process recorded at epoch 0 (row 1), 5 (row 2) and 50 (row 3) with a same word image (row 4). (a) the
lstm output (p); (b) the ctc path (  ) mapped from ground truth word string (b   1(sw)); (c) maximum probabilities of the character and
segmentation line with p and   ; (d) output con   dent maps of the deepfeatures (middle) and the lstm layer of the dtrn (bottom).

in our id56 model, the ctc layer is directly connected to
the outputs of lstms, and works as the output layer of the
whole id56. it not only allows our model to avoid a num-
ber of complicated post-processing (e.g. transforming the
lstm output sequence into a word string), but also makes
it possible to be trained in an end-to-end fashion by mini-
mizing an overall id168 over (x, sw)        . the loss
for each sample pair is computed as sum of the negative log
likelihood of the true word string,

l(x, sw) =     k(cid:88)

log p (si

w|x)

(6)

i=1

finally, our id56s model follows a bidirectional lstm
architecture, as shown in fig. 3 (b). it has two separate
lstm hidden layers that process the input sequence forward
and backward, respectively. both hidden layers are con-
nected to the same output layers, allowing it to access both
past and future information. in several sequence labelling
tasks, such as handwriting recognition (graves, liwicki,
and fernandez 2009) and phoneme recognition (graves and
schmidhuber 2005), the bidirectional id56s have shown
stronger capability than the standard id56s. our id56s
model is trained with the forward-backward algorithm that
jointly optimizes the bidirectional lstm and ctc. details
are presented in the supplementary material.

implementation details
our id98 model is trained on about 1.8    105 character im-
ages cropped from the training sets of a number of bench-
marks by (jaderberg, vedaldi, and zisserman 2014). we
generate the id98 sequence by applying the trained id98
with a sliding-window, followed by a column-wise normal-
ization. our recurrent model contains a bidirectional lstm.
each lstm layer has 128 cell memory blocks. the input
layer has 128 neurons (corresponding to 128d id98 se-
quence), which are fully connected to both hidden layers.
the outputs of two hidden layers are concatenated, and then
fully connected to the output layer of lstm with 37 out-
put classes (including the non-character), by using a softmax
function. our id56 model has 273k parameters in total. in
our experiments, we found that adding more layers lstm
does not lead to better results in our task. we conjecture that

lstm needs not be deep, given the deep id98 which has
provided strong representations.

the recurrent model is trained with steepest descent. the
parameters are updated per training sequence by using a
learning rate of 10   4 and a momentum of 0.9. we per-
form forward-backward algorithm (graves et al. 2006) to
jointly optimize the lstm and ctc parameters, where
a forward propagation is implemented through whole net-
work, followed by a forward-backward algorithm that aligns
the ground truth word strings to the lstm outputs,       
b   1(sw),   , p     r37  t . the loss of e.q.(6) is computed
approximately as:

l(x, sw)         t(cid:88)

log p (  t|x)

(7)

t=1

finally, the approximated error is propagated backward
to update the parameters. the id56 is trained on about
3000 word images (all characters of them are included in
previously-used 1.8   105 character images), taken from the
training sets of three benchmarks used bellow. the training
process is shown in fig. 4.

experiments and results

the experiments were conducted on three standard bench-
marks for cropped word image recognition: the street view
text (sv t ) (wang, babenko, and belongie 2011), ic-
dar 2003 (ic03) (lucas et al. 2003) and iiit 5k-word
(iiit 5k) (mishra., alahari, and jawahar 2012). the sv t
has 647 word images collected from google street view of
road-side scenes. it provides a lexicon of 50 words per im-
age for recognition (svt-50). the ic03 contains 860 word
images cropped from 251 natural images. lexicons with 50
words per image (ic03-50) and all words of the test set
(ic03-full) are provided. the iiit 5k is comprised of
5000 cropped word images from both scene and born-digital
images. the dataset is split into subsets of 2000 and 3000
images for training and test. each image is associated with
lexicons of 50 (iiit5k-50) and 1k words (iiit5k-1k) for test.
dtrn vs deepfeatures
the recurrence property of the dtrn makes it distinct
against the current deep id98 models, such as deepfeatures

figure 5: (left) correct recognitions; (right) incorrect samples.

(jaderberg, vedaldi, and zisserman 2014)) and the system
of (wang et al. 2012). the advantage is shown clearly in
fig. 4 (d), where the output maps of the lstm layer and
the maxout id98 of deepfeatures are compared. as can
be observed, our maps are much clearer than those of the
deepfeatures in a number of highly ambiguous word im-
ages. the character id203 distribution and segmenta-
tion are shown accurately on our maps, indicating the excel-
lent capability of our model for correctly identifying word
texts from challenging images. the    nal word recognition
is straightforward by simply applying the b projection (e.q.
5) on these maps. however, the maps of deepfeatures are
highly confused, making it extremely dif   cult to infer the
correct word strings from their maps. essentially, the recur-
rent property of dtrn allows it to identify a character ro-
bustly from a number of continuous regions or sided win-
dows, while the deepfeatures classi   es each isolated region
independently so that it is confused when a located region
just includes a part of the character or multiple characters.

comparisons with state-of-the-art
the evaluation is conducted by following the standard pro-
tocol, where each word image is associated with a lexicon,
and id153 is computed to    nd the optimized word.
the recognition results by the dtrn are presented in fig. 5,
including both the correct and incorrect recognitions. as
can been seen, the dtrn demonstrates excellent capabil-
ity on recognising extremely ambiguous word images, some
of which are even hard to human. this is mainly bene   cial
from its strong ability to leverage explicit order and mean-
ingful word context information. the results on three bench-
marks are compared with the state-of-the-art in table 1.

mid-level representation: strokelet (yao et al. 2014) and
lee et al.   s method (lee et al. 2014) achieved leading perfor-
mance based on the mid-level features. though they show
large improvements over conventional low-level features,
their performance are not comparable to ours, with signif-

icant reductions in accuracies in all the three datasets.

deep neural networks: as shown in table 1, the dnn
methods largely outperform the mid-level approaches, with
close to 10% of improvement in all cases. the considerable
performance gains mainly come from its ability to learn a
deep high-level feature from the word image. su and lu   s
method obtained accuracy of 83% on svt by building a
id56 model upon the hog features. deepfeatures achieved
leading results on both the svt and ic03 datasets. however,
the deepfeatures are still built on isolate character classi-
   er. by training a similar id98 model with the same training
data, the dtrn achieved signi   cant improvements over the
deepfeatures in all datasets. the results agree with our anal-
ysis conducted above. on the widely-used svt, our model
outperforms the deepfeatures considerably from 86.1% to
93.5%, indicating the superiority of our recurrent model in
connecting the isolated deep features sequentially for recog-
nition. furthermore, our system does not need to learn the
additional language model and character location informa-
tion, all of which are optimized jointly and automatically by
our id56 in an end-to-end fashion.

whole image representation: almazan et al.   s approach,
based on the whole word image representation, achieved
87.0% accuracy on the svt (almaz  an et al. 2014), slightly
over that of deepfeatures. in the iiit5k, it yielded 88.6%
and 75.6% on small and large lexicons, surpassing previous
results with a large margin. our dtrn strives for a further
step by reaching the accuracies of 94% and 91.5% on the
iiit5k. the large improvements may bene   t from the ex-
plicit order information included in our id98 sequence. it
is the key to increase discriminative power of our model for
word representation, which is highly sensitive to the order of
characters. the strong discriminative power can be further
veri   ed by the consistent high-performance of our system
along with the increase of lexicon sizes, where the accuracy
of almazan et al.   s approach drops signi   cantly.

training on additional large datasets: the photoocr

method

wang et al. 2011
mishra et al. 2012
novikova et al. 2012
tsm+crf(shi et al. 2013)
lee et al. 2014
strokelets(yao et al. 2014)
wang et al. 2012
alsharif and pineau 2013
su and lu 2014
deepfeatures
goel et al. 2013
almaz  an et al. 2014
dtrn
photoocr
jaderberg2015a
jaderberg2015b

ic03-50

cropped word recognition accuracy(%)
ic03-full

iiit5k-50

svt-50

iiit5k-1k

64.1

57.5

76.0
81.8
82.8
87.4
88.0
88.5
90.0
93.1
92.0
96.2
89.7

-

97.0

62.0
67.8

-

79.3
76.0
80.3
84.0
88.6
82.0
91.5

-
-

93.8

-

97.8
98.7

-

97.0
98.6

57.0
73.2
72.9
73.5
80.0
75.9
70.0
74.3
83.0
86.1
77.3
87.0
93.5
90.4
93.2
95.4

-
-
-
-

-
-
-
-
-

-
-
-
-

-
-
-
-
-

80.2

69.3

88.6
94.0

75.6
91.5

-

95.5
97.1

-

89.6
92.7

table 1: cropped word recognition results on the svt, icdar 2003, and iiit 5k-word. the bottom    gure shows unconstrained recognitions
of the dtrn and the publicly available model (jaderberg et al. 2014), which is similar to jaderberg2015a. obviously, it seems to be
sensitive to non-character spaces.

(bissacco et al. 2013) sets a strong baseline on the svt
(90.4%) by using large additional training data. it employed
about 107 character examples to learn a powerful dnn clas-
si   er, and also trained a strong language model with a cor-
pus of more than a trillion tokens. however, it involves a
number of low-level techniques to over-segment characters,
and jointly optimizes the segmentation, character classi   ca-
tion and language model with id125. furthermore, it
also includes a number of post-processing steps to further
improve the performance, making the system highly com-
plicated. the dtrn achieved 3.1% improvement over the
photoocr, which is also signi   cant by considering only a
fraction of the training data (two orders of magnitude less
data) we used. while our model works without a language
model, and does not need any post-processing step.

jaderberg et al. proposed several powerful deep id98
models by computing a deep feature from the whole word
image (jaderberg et al. 2014; 2015a; 2015b). however, di-
rectly comparing our dtrn to these models may be dif-
   cult. first, these models was trained on 7.2    106 word
images, comparing to ours 3    103 word images (with
1.8    105 characters). nevertheless, our model achieves
comparable results against jaderberg2015a with higher ac-
curacies on the svt and iiit5k-1k. importantly, the dtrn
also provides unique capability for unconstrained recogni-
tion of any number of characters and/or word strings in a
text-line. several examples are presented in the    gure of
table 1. jaderberg2015b model achieves the best results
in all databases. it casts the word recognition problem as a
large-scale classi   cation task by considering the images of a
same word as a class. thus the output layer should include a
large number of classes, e.g. 90,000, imposing a huge num-
ber of model parameters which are dif   cult to be trained.
furthermore, it is not    exible to recognize a new word not
trained. while the scene texts often include many irregular
word strings (the number could be unlimited) which are im-

possible to be known in advanced, such as    ab00d   . thus
our dtrn can process unknown words and arbitrary strings,
providing a more    exible approach for this task.

conclusion

we have presented a deep-text recurrent network (dtrn)
for scene text recognition. it models the task as a deep se-
quence labelling problem that overcomes a number of main
limitations. it computes a set of explicitly-ordered deep fea-
tures from the word image, which is not only robust to
low-level image distortions, but also highly discriminative
to word strings. the recurrence property makes it capa-
ble of recognising highly ambiguous images by leveraging
meaningful word context information, and also allows it to
process unknown words and arbitrary strings, providing a
more principled approach for this task. experimental results
show that our model has achieved the state-of-the-art perfor-
mance.

acknowledgments

this work is partly supported by national natural
science foundation of china (61503367, 91320101,
61472410), guangdong natural science foundation
(2015a030310289), guangdong innovative research pro-
gram (201001d0104648280, 2014b050505017) and shen-
zhen basic research program (kqcx2015033117354153).
yu qiao is the corresponding author.
references

almaz  an, j.; gordo, a.; forn  es, a.; and valveny, e. 2014.
word spotting and recognition with embedded attributes.
ieee trans. pattern analysis and machine intelligence
(tpami) 36:2552   2566.
alsharif, o., and pineau, j. 2013. end-to-end text recogni-
tion with hybrid id48 maxout models. arxiv:1310.1811v1.

2013.

andrej, k., and li, f. 2015. deep visual-semantic align-
ments for generating image descriptions.
ieee computer
vision and pattern recognition (cvpr).
bissacco, a.; cummins, m.; netzer, y.; and neven, h. 2013.
photoocr: reading text in uncontrolled conditions.
ieee
international conference on id161 (iccv).
breuel, t.; ui-hasan, a.; azawi, m.; and shafait, f. 2013.
high-performance ocr for printed english and fraktur us-
ing id137. international conference on document
analysis and recognition (icdar).
donahue, j.; hendricks, l.; guadarrama, s.; and rohrbach,
m. 2015. long-term recurrent covolutional networks for
visual recognition and description. ieee id161
and pattern recognition (cvpr).
goodfellow, i.; warde-farley, d.; mirza, m.; courville,
a.; and bengio, y.
maxout networks.
arxiv:1302.4389v4.
graves, a., and jaitly, n. 2014. towards end-to-end speech
recognition with recurrent neural networks. ieee interna-
tional conference on machine learning (icml).
graves, a., and schmidhuber, j. 2005. framewise phoneme
classi   cation with bidirectional lstm and other neural net-
work architectures. neural networks 18:602   610.
graves, a., and schmidhuber, j. 2008. of   ine handwrit-
ing recognition with multidimensional recurrent neural net-
works. neural information processing systems (nips).
graves, a.; fernandez, s.; gomez, f.; and schmidhu-
ber, j. 2006. connectionist temporal classi   cation: la-
belling unsegmented sequence data with recurrent neu-
ral networks.
ieee international conference on machine
learning (icml).
graves, a.; liwicki, m.; and fernandez, s. 2009. a novel
connectionist system for unconstrained handwriting recog-
nition. ieee trans. pattern analysis and machine intelli-
gence (tpami) 31:855   868.
he, t.; huang, w.; qiao, y.; and yao, j.
2015. text-
attentional convolutional neural networks for scene text de-
tection. arxiv:1510.03283.
hochreiter, s., and schmidhuber, j. 1997. long short-term
memory. neural computation 9:1735   1780.
huang, w.; lin, z.; yang, j.; and wang, j. 2013. text local-
ization in natural images using stroke feature transform and
text covariance descriptors. ieee international conference
on id161 (iccv).
huang, w.; qiao, y.; and tang, x. 2014. robust scene
text detection with convolution neural network induced mser
trees. european conference on id161 (eccv).
jaderberg, m.; simonyan, k.; vedaldi, a.; and zisserman,
a. 2014. synthetic data and arti   cial neural networks for
natural scene text recognition. workshop in neural infor-
mation processing systems (nips).
jaderberg, m.; simonyan, k.; vedaldi, a.; and zisserman,
a. 2015a. deep structured output learning for unconstrained
text recognition. international conference on learning rep-
resentation (iclr).

jaderberg, m.; simonyan, k.; vedaldi, a.; and zisserman,
a. 2015b. reading text in the wild with convolutional neural
networks. international jounal of computver vision (ijcv).
jaderberg, m.; vedaldi, a.; and zisserman, a. 2014. deep
features for text spotting. european conference on com-
puter vision (eccv).
lecun, y.; bottou, l.; bengio, y.; and haffner, p. 1998.
gradient-based learning applied to document recognition.
proceedings of the ieee.
lee, c.; bhardwaj, a.; di, w.; and andr. piramuthu, v. j.
2014. region-based discriminative feature pooling for scene
text recognition. ieee id161 and pattern recog-
nition (cvpr).
lucas, s. m.; panaretos, a.; sosa, l.; tang, a.; wong, s.;
and young, r. 2003. icdar 2003 robust reading competi-
tions. international conference on document analysis and
recognition (icdar).
mishra., a.; alahari, k.; and jawahar, c. 2012. scene text
recognition using higher order language priors. british ma-
chine vision conference (bmvc).
neumann, l., and matas, j. 2013. scene text localization
and recognition with oriented stroke detection. ieee inter-
national conference on id161 (iccv).
shi, c.; wang, c.; xiao, b.; zhang, y.; gao, s.; and zhang,
z. 2013. scene text recognition using part-based tree-
structured character detection. ieee id161 and
pattern recognition (cvpr).
su, b., and lu, s. 2014. accurate scene text recognition
based on recurrent neural network. asian conference on
id161 (iccv).
sutskever, i.; vinyals, o.; and le, q. v. 2014. sequence to
sequence learning with neural networks. neural information
processing systems (nips).
visin, f.; kastner, k.; cho, k.; matteucci, m.; courville,
a.; and bengio, y.
renet: a recurrent neu-
ral network based alternative to convolutional networks.
arxiv:1505.00393.
wang, k.; babenko, b.; and belongie, s. 2011. end-to-end
scene text recognition.
ieee international conference on
id161 (iccv).
wang, t.; wu, d.; coates, a.; and ng, a. y.
2012.
end-to-end text recognition with convolutional neural net-
works. ieee international conference on pattern recogni-
tion (icpr).
yao, c.; bai, x.; shi, b.; and liu, w. 2014. strokelets: a
learned multi-scale representation for scene text recognition.
ieee id161 and pattern recognition (cvpr).
yin, x. c.; yin, x.; huang, k.; and hao, h. w. 2014. robust
text detection in natural scene images. ieee trans. pattern
analysis and machine intelligence 36:970   983.
zhang, z.; shen, w.; yao, c.; and bai, x. 2015. symmetry-
based text line detection in natural scenes. ieee computer
vision and pattern recognition (cvpr).

2015.

