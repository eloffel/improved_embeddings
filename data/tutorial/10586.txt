5
1
0
2

 

v
o
n
9
1

 

 
 
]
l
c
.
s
c
[
 
 

3
v
0
4
3
3
0

.

6
0
5
1
:
v
i
x
r
a

teaching machines to read and comprehend

karl moritz hermann    tom  a  s ko  cisk  y       edward grefenstette   
phil blunsom      
lasse espeholt    will kay    mustafa suleyman   

   google deepmind

   university of oxford

{kmh,tkocisky,etg,lespeholt,wkay,mustafasul,pblunsom}@google.com

abstract

teaching machines to read natural language documents remains an elusive chal-
lenge. machine reading systems can be tested on their ability to answer questions
posed on the contents of documents that they have seen, but until now large scale
training and test datasets have been missing for this type of evaluation. in this
work we de   ne a new methodology that resolves this bottleneck and provides
large scale supervised reading comprehension data. this allows us to develop a
class of attention based deep neural networks that learn to read real documents and
answer complex questions with minimal prior knowledge of language structure.

1

introduction

progress on the path from shallow bag-of-words information retrieval algorithms to machines ca-
pable of reading and understanding documents has been slow. traditional approaches to machine
reading and comprehension have been based on either hand engineered grammars [1], or information
extraction methods of detecting predicate argument triples that can later be queried as a relational
database [2]. supervised machine learning approaches have largely been absent from this space due
to both the lack of large scale training datasets, and the dif   culty in structuring statistical models
   exible enough to learn to exploit document structure.
while obtaining supervised natural language reading comprehension data has proved dif   cult, some
researchers have explored generating synthetic narratives and queries [3, 4]. such approaches allow
the generation of almost unlimited amounts of supervised data and enable researchers to isolate the
performance of their algorithms on individual simulated phenomena. work on such data has shown
that neural network based models hold promise for modelling reading comprehension, something
that we will build upon here. historically, however, many similar approaches in computational
linguistics have failed to manage the transition from synthetic data to real environments, as such
closed worlds inevitably fail to capture the complexity, richness, and noise of natural language [5].
in this work we seek to directly address the lack of real natural language training data by intro-
ducing a novel approach to building a supervised reading comprehension data set. we observe that
summary and paraphrase sentences, with their associated documents, can be readily converted to
context   query   answer triples using simple entity detection and anonymisation algorithms. using
this approach we have collected two new corpora of roughly a million news stories with associated
queries from the id98 and daily mail websites.
we demonstrate the ef   cacy of our new corpora by building novel deep learning models for reading
comprehension. these models draw on recent developments for incorporating attention mechanisms
into recurrent neural network architectures [6, 7, 8, 4]. this allows a model to focus on the aspects of
a document that it believes will help it answer a question, and also allows us to visualises its id136
process. we compare these neural models to a range of baselines and heuristic benchmarks based
upon a traditional frame semantic analysis provided by a state-of-the-art natural language processing

1

daily mail
valid
1

id98
train valid
1

95

# months
# documents
# queries
max # entities
avg # entities
avg # tokens
vocab size

test
1

train
56

test
1
90,266 1,220 1,093 196,961 12,148 10,397
380,298 3,924 3,198 879,450 64,835 53,182
245
26.0
780

527
396
26.4 26.5 24.5
762
716

371
26.5
813

187

232
25.5
774
208,045

763
118,497

table 1: corpus statistics. articles were collected starting in
april 2007 for id98 and june 2010 for the daily mail, both until
the end of april 2015. validation data is from march, test data
from april 2015. articles of over 2000 tokens and queries whose
answer entity did not appear in the context were    ltered out.

top n cumulative %

id98 daily mail
25.6
30.5
47.7
42.4
53.7
58.1
68.1
70.6
85.1
85.5

1
2
3
5
10

table 2: percentage of time that
the correct answer is contained in
the top n most frequent entities
in a given document.

(nlp) pipeline. our results indicate that the neural models achieve a higher accuracy, and do so
without any speci   c encoding of the document or query structure.

2 supervised training data for reading comprehension

the reading comprehension task naturally lends itself to a formulation as a supervised learning
problem. speci   cally we seek to estimate the id155 p(a|c, q), where c is a context
document, q a query relating to that document, and a the answer to that query. for a focused
evaluation we wish to be able to exclude additional information, such as world knowledge gained
from co-occurrence statistics, in order to test a model   s core capability to detect and understand the
linguistic relationships between entities in the context document.
such an approach requires a large training corpus of document   query   answer triples and until now
such corpora have been limited to hundreds of examples and thus mostly of use only for testing [9].
this limitation has meant that most work in this area has taken the form of unsupervised approaches
which use templates or syntactic/semantic analysers to extract relation tuples from the document to
form a id13 that can be queried.
here we propose a methodology for creating real-world, large scale supervised training data for
learning reading comprehension models. inspired by work in summarisation [10, 11], we create two
machine reading corpora by exploiting online newspaper articles and their matching summaries. we
have collected 93k articles from the id981 and 220k articles from the daily mail2 websites. both
news providers supplement their articles with a number of bullet points, summarising aspects of the
information contained in the article. of key importance is that these summary points are abstractive
and do not simply copy sentences from the documents. we construct a corpus of document   query   
answer triples by turning these bullet points into cloze [12] style questions by replacing one entity
at a time with a placeholder. this results in a combined corpus of roughly 1m data points (table 1).
code to replicate our datasets   and to apply this method to other sources   is available online3.

2.1 entity replacement and permutation

note that the focus of this paper is to provide a corpus for evaluating a model   s ability to read
and comprehend a single document, not world knowledge or co-occurrence. to understand that
distinction consider for instance the following cloze form queries (created from headlines in the
daily mail validation set): a) the hi-tech bra that helps you beat breast x; b) could saccharin help
beat x ?; c) can    sh oils help    ght prostate x ? an ngram language model trained on the daily mail
would easily correctly predict that (x = cancer), regardless of the contents of the context document,
simply because this is a very frequently cured entity in the daily mail corpus.

1www.id98.com
2www.dailymail.co.uk
3http://www.github.com/deepmind/rc-data/

2

original version

anonymised version

context

the bbc producer allegedly struck by jeremy
clarkson will not press charges against the    top
gear    host, his lawyer said friday. clarkson, who
hosted one of the most-watched television shows
in the world, was dropped by the bbc wednesday
after an internal investigation by the british broad-
caster found he had subjected producer oisin tymon
   to an unprovoked physical and verbal attack.    . . .

the ent381 producer allegedly struck by ent212 will
not press charges against the     ent153     host , his
lawyer said friday . ent212 , who hosted one of the
most - watched television shows in the world , was
dropped by the ent381 wednesday after an internal
investigation by the ent180 broadcaster found he
had subjected producer ent193     to an unprovoked
physical and verbal attack .     . . .

query

producer x will not press charges against jeremy
clarkson, his lawyer says.

producer x will not press charges against ent212 ,
his lawyer says .

answer

oisin tymon

ent193

table 3: original and anonymised version of a data point from the daily mail validation set. the
anonymised entity markers are constantly permuted during training and testing.

to prevent such degenerate solutions and create a focused task we anonymise and randomise our
corpora with the following procedure, a) use a coreference system to establish coreferents in each
data point; b) replace all entities with abstract entity markers according to coreference; c) randomly
permute these entity markers whenever a data point is loaded.
compare the original and anonymised version of the example in table 3. clearly a human reader can
answer both queries correctly. however in the anonymised setup the context document is required
for answering the query, whereas the original version could also be answered by someone with the
requisite background knowledge. therefore, following this procedure, the only remaining strategy
for answering questions is to do so by exploiting the context presented with each question. thus
performance on our two corpora truly measures reading comprehension capability. naturally a
production system would bene   t from using all available information sources, such as clues through
language and co-occurrence statistics.
table 2 gives an indication of the dif   culty of the task, showing how frequent the correct answer is
contained in the top n entity markers in a given document. note that our models don   t distinguish
between entity markers and regular words. this makes the task harder and the models more general.

3 models

so far we have motivated the need for better datasets and tasks to evaluate the capabilities of machine
reading models. we proceed by describing a number of baselines, benchmarks and new models to
evaluate against this paradigm. we de   ne two simple baselines, the majority baseline (maximum
frequency) picks the entity most frequently observed in the context document, whereas the ex-
clusive majority (exclusive frequency) chooses the entity most frequently observed in the
context but not observed in the query. the idea behind this exclusion is that the placeholder is
unlikely to be mentioned twice in a single cloze form query.

3.1 symbolic matching models

traditionally, a pipeline of nlp models has been used for attempting id53, that is
models that make heavy use of linguistic annotation, structured world knowledge and semantic
parsing and similar nlp pipeline outputs. building on these approaches, we de   ne a number of
nlp-centric models for our machine reading task.

frame-id29 frame-id29 attempts to identify predicates and their argu-
ments, allowing models access to information about    who did what to whom   . naturally this kind
of annotation lends itself to being exploited for id53. we develop a benchmark that

3

makes use of frame-semantic annotations which we obtained by parsing our model with a state-of-
the-art frame-semantic parser [13, 14]. as the parser makes extensive use of linguistic information
we run these benchmarks on the unanonymised version of our corpora. there is no signi   cant advan-
tage in this as the frame-semantic approach used here does not possess the capability to generalise
through a language model beyond exploiting one during the parsing phase. thus, the key objective
of evaluating machine comprehension abilities is maintained. extracting entity-predicate triples   
denoted as (e1, v, e2)   from both the query q and context document d, we attempt to resolve queries
using a number of rules with an increasing recall/precision trade-off as follows (table 4).

strategy
exact match
1
2
be.01.v match
3 correct frame
4
permuted frame
5 matching entity
6 back-off strategy

example (cloze / context)
x loves suse / kim loves suse

pattern     q
(p, v, y)
(p, be.01.v, y)
(p, v, y)
(p, v, y)
(p, v, y)
pick the most frequent entity from the context that doesn   t appear in the query

pattern     d
(x, v, y)
(x, be.01.v, y) x is president / mike is president
(x, v, z)
(y, v, x)
(x, z, y)

x won oscar / tom won academy award
x met suse / suse met tom
x likes candy / tom loves candy

table 4: resolution strategies using propbank triples. x denotes the entity proposed as answer, v is
a fully quali   ed propbank frame (e.g. give.01.v). strategies are ordered by precedence and answers
determined accordingly. this heuristic algorithm was iteratively tuned on the validation data set.

for reasons of clarity, we pretend that all propbank triples are of the form (e1, v, e2). in practice,
we take the argument numberings of the parser into account and only compare like with like, except
in cases such as the permuted frame rule, where ordering is relaxed. in the case of multiple possible
answers from a single rule, we randomly choose one.

word distance benchmark we consider another baseline that relies on word distance measure-
ments. here, we align the placeholder of the cloze form question with each possible entity in the
context document and calculate a distance measure between the question and the context around the
aligned entity. this score is calculated by summing the distances of every word in q to their nearest
aligned word in d, where alignment is de   ned by matching words either directly or as aligned by the
coreference system. we tune the maximum penalty per word (m = 8) on the validation data.

3.2 neural network models

neural networks have successfully been applied to a range of tasks in nlp. this includes classi   ca-
tion tasks such as id31 [15] or id52 [16], as well as generative problems such
as language modelling or machine translation [17]. we propose three neural models for estimating
the id203 of word type a from document d answering query q:

p(a|d, q)     exp (w (a)g(d, q)) ,

s.t. a     v,

where v is the vocabulary4, and w (a) indexes row a of weight matrix w and through a slight
abuse of notation word types double as indexes. note that we do not privilege entities or variables,
the model must learn to differentiate these in the input sequence. the function g(d, q) returns a
vector embedding of a document and query pair.

the deep lstm reader long short-term memory (lstm, [18]) networks have recently seen
considerable success in tasks such as machine translation and language modelling [17]. when used
for translation, deep lstms [19] have shown a remarkable ability to embed long sequences into
a vector representation which contains enough information to generate a full translation in another
language. our    rst neural model for reading comprehension tests the ability of deep lstm encoders
to handle signi   cantly longer sequences. we feed our documents one word at a time into a deep
lstm encoder, after a delimiter we then also feed the query into the encoder. alternatively we also
experiment with processing the query then the document. the result is that this model processes
each document query pair as a single long sequence. given the embedded document and query the
network predicts which token in the document answers the query.

4the vocabulary includes all the word types in the documents, questions, the entity maskers, and the ques-

tion unknown entity marker.

4

(a) attentive reader.

(b) impatient reader.

(c) a two layer deep lstm reader with the question encoded before the document.

figure 1: document and query embedding models.

we employ a deep lstm cell with skip connections from each input x(t) to every hidden layer,
and from every hidden layer to the output y(t):

y(t) = y(cid:48)(t, 1)|| . . .||y(cid:48)(t, k)

x(cid:48)(t, k) = x(t)||y(cid:48)(t, k     1),
i(t, k) =    (wkxix(cid:48)(t, k) + wkhih(t     1, k) + wkcic(t     1, k) + bki)
f (t, k) =    (wkxf x(t) + wkhf h(t     1, k) + wkcf c(t     1, k) + bkf )
c(t, k) = f (t, k)c(t     1, k) + i(t, k) tanh (wkxcx(cid:48)(t, k) + wkhch(t     1, k) + bkc)
o(t, k) =    (wkxox(cid:48)(t, k) + wkhoh(t     1, k) + wkcoc(t, k) + bko)
h(t, k) = o(t, k) tanh (c(t, k))
y(cid:48)(t, k) = wkyh(t, k) + bky

where || indicates vector concatenation h(t, k) is the hidden state for layer k at time t, and i, f,
o are the input, forget, and output gates respectively. thus our deep lstm reader is de   ned by
glstm(d, q) = y(|d| +|q|) with input x(t) the concatenation of d and q separated by the delimiter |||.

the attentive reader the deep lstm reader must propagate dependencies over long distances
in order to connect queries to their answers. the    xed width hidden vector forms a bottleneck for
this information    ow that we propose to circumvent using an attention mechanism inspired by recent
results in translation and image recognition [6, 7]. this attention model    rst encodes the document
and the query using separate bidirectional single layer lstms [19].
we denote the outputs of the forward and backward lstms as       y (t) and       y (t) respectively. the
encoding u of a query of length |q| is formed by the concatenation of the    nal forward and backward
outputs, u =       yq (|q|) ||       yq (1).
for the document the composite output for each token at position t is, yd(t) =       yd(t) ||       yd(t). the
representation r of the document d is formed by a weighted sum of these output vectors. these
weights are interpreted as the degree to which the network attends to a particular token in the docu-
ment when answering the query:

m(t) = tanh (wymyd(t) + wumu) ,
s(t)     exp (w

(cid:124)
msm(t)) ,

r = yds,

where we are interpreting yd as a matrix with each column being the composite representation yd(t)
of document token t. the variable s(t) is the normalised attention at token t. given this attention

5

rs(1)y(1)s(3)y(3)s(2)y(2)ugs(4)y(4)marywenttoxvisitedenglandenglandrurmarywenttoxvisitedenglandenglandrgmarywenttoxvisitedenglandengland|||gscore the embedding of the document r is computed as the weighted sum of the token embeddings.
the model is completed with the de   nition of the joint document and query embedding via a non-
linear combination:

gar(d, q) = tanh (wrgr + wugu) .

the attentive reader can be viewed as a generalisation of the application of memory networks to
id53 [3]. that model employs an attention mechanism at the sentence level where
each sentence is represented by a bag of embeddings. the attentive reader employs a    ner grained
token level attention mechanism where the tokens are embedded given their entire future and past
context in the input document.

the impatient reader the attentive reader is able to focus on the passages of a context doc-
ument that are most likely to inform the answer to the query. we can go further by equipping the
model with the ability to reread from the document as each query token is read. at each token i
of the query q the model computes a id194 vector r(i) using the bidirectional
embedding yq(i) =       yq (i) ||       yq (i):

m(i, t) = tanh (wdmyd(t) + wrmr(i     1) + wqmyq(i)) ,
s(i, t)     exp (w
r(0) = r0,

(cid:124)
d s(i) + tanh (wrrr(i     1))

(cid:124)
msm(i, t)) ,
r(i) = y

1     i     |q|.

1     i     |q|,

the result is an attention mechanism that allows the model to recurrently accumulate information
from the document as it sees each query token, ultimately outputting a    nal joint document query
representation for the answer prediction,

gir(d, q) = tanh (wrgr(|q|) + wqgu) .

4 empirical evaluation

having described a number of models in the previous section, we next evaluate these models on our
reading comprehension corpora. our hypothesis is that neural models should in principle be well
suited for this task. however, we argued that simple recurrent models such as the lstm probably
have insuf   cient expressive power for solving tasks that require complex id136. we expect that
the attention-based models would therefore outperform the pure lstm-based approaches.
considering the second dimension of our investigation, the comparison of traditional versus neural
approaches to nlp, we do not have a strong prior favouring one approach over the other. while nu-
merous publications in the past few years have demonstrated neural models outperforming classical
methods, it remains unclear how much of that is a side-effect of the language modelling capabilities
intrinsic to any neural model for nlp. the entity anonymisation and permutation aspect of the task
presented here may end up levelling the playing    eld in that regard, favouring models capable of
dealing with syntax rather than just semantics.
with these considerations in mind, the experimental part of this paper is designed with a three-
fold aim. first, we want to establish the dif   culty of our machine reading task by applying a wide
range of models to it. second, we compare the performance of parse-based methods versus that of
neural models. third, within the group of neural models examined, we want to determine what each
component contributes to the end performance; that is, we want to analyse the extent to which an
lstm can solve this task, and to what extent various attention mechanisms impact performance.
all model hyperparameters were tuned on the respective validation sets of the two corpora.5 our
experimental results are in table 5, with the attentive and impatient readers performing best across
both datasets.

5for the deep lstm reader, we consider hidden layer sizes [64, 128, 256], depths [1, 2, 4], initial learning
rates [1e   3, 5e   4, 1e   4, 5e   5], batch sizes [16, 32] and dropout [0.0, 0.1, 0.2]. we evaluate two types of
feeds. in the cqa setup we feed    rst the context document and subsequently the question into the encoder,
while the qca model starts by feeding in the question followed by the context document. we report results on
the best model (underlined hyperparameters, qca setup). for the id12 we consider hidden layer
sizes [64, 128, 256], single layer, initial learning rates [1e   4, 5e   5, 2.5e   5, 1e   5], batch sizes [8, 16, 32]
and dropout [0, 0.1, 0.2, 0.5]. for all models we used asynchronous rmsprop [20] with a momentum of 0.9
and a decay of 0.95. see appendix a for more details of the experimental setup.

6

id98

daily mail

valid
30.5
maximum frequency
36.6
exclusive frequency
frame-semantic model 36.3
50.5
word distance model
55.0
deep lstm reader
39.0
uniform reader
attentive reader
61.6
61.8
impatient reader

test valid
25.6
33.2
39.3
32.7
35.5
40.2
56.4
50.9
63.3
57.0
34.6
39.4
70.5
63.0
63.8
69.0

test
25.5
32.8
35.5
55.5
62.2
34.4
69.0
68.0

table 5: accuracy of all the models and bench-
marks on the id98 and daily mail datasets. the
uniform reader baseline sets all of the m(t) pa-
rameters to be equal.

figure 2: precision@recall for the attention
models on the id98 validation data.

frame-semantic benchmark while the one frame-semantic model proposed in this paper is
clearly a simpli   cation of what could be achieved with annotations from an nlp pipeline, it does
highlight the dif   culty of the task when approached from a symbolic nlp perspective.
two issues stand out when analysing the results in detail. first, the frame-semantic pipeline has a
poor degree of coverage with many relations not being picked up by our propbank parser as they
do not adhere to the default predicate-argument structure. this effect is exacerbated by the type
of language used in the highlights that form the basis of our datasets. the second issue is that
the frame-semantic approach does not trivially scale to situations where several sentences, and thus
frames, are required to answer a query. this was true for the majority of queries in the dataset.

word distance benchmark more surprising perhaps is the relatively strong performance of the
word distance benchmark, particularly relative to the frame-semantic benchmark, which we had
expected to perform better. here, again, the nature of the datasets used can explain aspects of this
result. where the frame-semantic model suffered due to the language used in the highlights, the word
distance model bene   ted. particularly in the case of the daily mail dataset, highlights frequently
have signi   cant lexical overlap with passages in the accompanying article, which makes it easy for
the word distance benchmark. for instance the query    tom hanks is friends with x   s manager,
scooter brown    has the phrase    ... turns out he is good friends with scooter brown, manager for
carly rae jepson    in the context. the word distance benchmark correctly aligns these two while
the frame-semantic approach fails to pickup the friendship or management relations when parsing
the query. we expect that on other types of machine reading data where questions rather than cloze
queries are used this particular model would perform signi   cantly worse.

neural models within the group of neural models explored here, the results paint a clear picture
with the impatient and the attentive readers outperforming all other models. this is consistent with
our hypothesis that attention is a key ingredient for machine reading and id53 due to
the need to propagate information over long distances. the deep lstm reader performs surpris-
ingly well, once again demonstrating that this simple sequential architecture can do a reasonable
job of learning to abstract long sequences, even when they are up to two thousand tokens in length.
however this model does fail to match the performance of the attention based models, even though
these only use single layer lstms.6
the poor results of the uniform reader support our hypothesis of the signi   cance of the attention
mechanism in the attentive model   s performance as the only difference between these models is
that the attention variables are ignored in the uniform reader. the precision@recall statistics in
figure 2 again highlight the strength of the attentive approach.
we can visualise the attention mechanism as a heatmap over a context document to gain further
insight into the models    performance. the highlighted words show which tokens in the document
were attended to by the model. in addition we must also take into account that the vectors at each

6memory constraints prevented us from experimenting with deeper attentive readers.

7

figure 3: attention heat maps from the attentive reader for two correctly answered validation set
queries (the correct answers are ent23 and ent63, respectively). both examples require signi   cant
lexical generalisation and co-reference resolution in order to be answered correctly by a given model.

token integrate long range contextual information via the bidirectional lstm encoders. figure 3
depicts heat maps for two queries that were correctly answered by the attentive reader.7 in both
cases con   dently arriving at the correct answer requires the model to perform both signi   cant lexical
generalsiation, e.g.    killed           deceased   , and co-reference or id2, e.g.    ent119 was
killed           he was identi   ed.    however it is also clear that the model is able to integrate these signals
with rough heuristic indicators such as the proximity of query words to the candidate answer.

5 conclusion

the supervised paradigm for training machine reading and comprehension models provides a
promising avenue for making progress on the path to building full natural language understanding
systems. we have demonstrated a methodology for obtaining a large number of document-query-
answer triples and shown that recurrent and attention based neural networks provide an effective
modelling framework for this task. our analysis indicates that the attentive and impatient read-
ers are able to propagate and integrate semantic information over long distances. in particular we
believe that the incorporation of an attention mechanism is the key contributor to these results.
the attention mechanism that we have employed is just one instantiation of a very general idea
which can be further exploited. however, the incorporation of world knowledge and multi-document
queries will also require the development of attention and embedding mechanisms whose complex-
ity to query does not scale linearly with the data set size. there are still many queries requiring
complex id136 and long range reference resolution that our models are not yet able to answer.
as such our data provides a scalable challenge that should support nlp research into the future. fur-
ther, signi   cantly bigger training data sets can be acquired using the techniques we have described,
undoubtedly allowing us to train more expressive and accurate models.

7note that these examples were chosen as they were short, the average id98 validation document contained

763 tokens and 27 entities, thus most instances were signi   cantly harder to answer than these examples.

8

. . .. . .references
[1] ellen riloff and michael thelen. a rule-based id53 system for reading com-
prehension tests. in proceedings of the anlp/naacl workshop on reading comprehension
tests as evaluation for computer-based language understanding sytems.

[2] hoifung poon, janara christensen, pedro domingos, oren etzioni, raphael hoffmann, chloe
kiddon, thomas lin, xiao ling, mausam, alan ritter, stefan schoenmackers, stephen soder-
land, dan weld, fei wu, and congle zhang. machine reading at the university of washing-
ton. in proceedings of the naacl hlt 2010 first international workshop on formalisms and
methodology for learning by reading.

[3] jason weston, sumit chopra, and antoine bordes. memory networks. corr, abs/1410.3916,

2014.

[4] sainbayar sukhbaatar, arthur szlam, jason weston, and rob fergus. end-to-end memory

networks. corr, abs/1503.08895, 2015.

[5] terry winograd. understanding natural language. academic press, inc., orlando, fl, usa,

1972.

[6] dzmitry bahdanau, kyunghyun cho, and yoshua bengio. id4 by

jointly learning to align and translate. corr, abs/1409.0473, 2014.

[7] volodymyr mnih, nicolas heess, alex graves, and koray kavukcuoglu. recurrent models of

visual attention. in advances in neural information processing systems 27.

[8] karol gregor, ivo danihelka, alex graves, and daan wierstra. draw: a recurrent neural

network for image generation. corr, abs/1502.04623, 2015.

[9] matthew richardson, christopher j. c. burges, and erin renshaw. mctest: a challenge dataset

for the open-domain machine comprehension of text. in proceedings of emnlp.

[10] krysta svore, lucy vanderwende, and christopher burges. enhancing single-document sum-
marization by combining ranknet and third-party sources. in proceedings of emnlp/conll.
[11] kristian woodsend and mirella lapata. automatic generation of story highlights. in proceed-

ings of acl, 2010.

[12] wilson l taylor.    cloze procedure   : a new tool for measuring readability. journalism quar-

terly, 30:415   433, 1953.

[13] dipanjan das, desai chen, andr  e f. t. martins, nathan schneider, and noah a. smith. frame-

id29. computational linguistics, 40(1):9   56, 2013.

[14] karl moritz hermann, dipanjan das, jason weston, and kuzman ganchev. semantic frame

identi   cation with distributed word representations. in proceedings of acl, june 2014.

[15] nal kalchbrenner, edward grefenstette, and phil blunsom. a convolutional neural network

for modelling sentences. in proceedings of acl, 2014.

[16] ronan collobert, jason weston, l  eon bottou, michael karlen, koray kavukcuoglu, and pavel
kuksa. natural language processing (almost) from scratch. journal of machine learning
research, 12:2493   2537, november 2011.

[17] ilya sutskever, oriol vinyals, and quoc v. v le. sequence to sequence learning with neural

networks. in advances in neural information processing systems 27.

[18] sepp hochreiter and j  urgen schmidhuber. long short-term memory. neural computation,

9(8):1735   1780, november 1997.

[19] alex graves. supervised sequence labelling with recurrent neural networks, volume 385 of

studies in computational intelligence. springer, 2012.

[20] t. tieleman and g. hinton. lecture 6.5   rmsprop: divide the gradient by a running average

of its recent magnitude. coursera: neural networks for machine learning, 2012.

9

a model hyperparameters

the precise hyperparameters used for the various attentive models are as in table 6. all models
were trained using asynchronous rmsprop [20] with a momentum of 0.9 and a decay of 0.95.

model
uniform, id98
attentive, id98
impatient, id98
uniform, daily mail
attentive, daily mail
impatient, daily mail

hidden size learning rate batch size dropout
0.2
0.2
0.3
0.2
0.1
0.1

5e-5
5e-5
5e-5
5e-5
2.5e-5
5e-5

256
256
256
256
256
256

32
32
32
32
32
32

table 6: model hyperparameters

b performance across document length

to understand how the model performance depends on the size of the context, we plot performance
versus document lengths in figures 4 and 5. the    rst    gure (fig. 4) plots a sliding window of perfor-
mance across document length, showing that performance of the attentive models degrades slightly
as documents increase in length. the second    gure (fig. 5) shows the cumulative performance with
documents up to length n, showing that while the length does impact the models    performance, that
effect becomes negligible after reaching a length of ~500 tokens.

figure 4: precision@document length for the
id12 on the id98 validation data.
the chart shows the precision for each decile in
document lengths across the corpus as well as the
precision for the 5% longest articles.

c additional heatmap analysis

figure 5: aggregated precision for documents
up to a certain lengths. the points mark the ith
decile in document lengths across the corpus.

we expand on the analysis of the attention mechanism presented in the paper by including visuali-
sations for additional queries from the id98 validation dataset below. we consider examples from
the attentive reader as well as the impatient reader in this appendix.

c.1 attentive reader

positive instances figure 6 shows two positive examples from the id98 validation set that re-
quire reasonable levels of lexical generalisation and co-reference in order to be answered. the    rst
query in figure 7 contains strong lexical cues through the quote, but requires identifying the entity

10

quoted, which is non-trivial in the context document. the    nal positive example (also in figure 7)
demonstrates the fearlessness of our model.

figure 6: attention heat maps from the attentive reader for two more correctly answered validation
set queries. both examples require signi   cant lexical generalisation and co-reference resolution to
   nd the correct answers ent201 and ent214, respectively.

negative instances figures 8 and 9 show examples of queries where the attentive reader fails
to select the correct answer. the two examples in figure 8 highlight a fairly common phenomenon
in the data, namely ambiguous queries, where   at least following the anonymisation process   
multiple entities are plausible answers even when evaluated manually. note that in both cases the
query searches for an entity marker that describes a geographic location, preceded by the word    in   .
here it is unclear whether the placeholder refers to a part of town, town, region or country.
figure 9 contains two additional negative cases. the    rst failure is caused by the co-reference entity
selection process. the correct entity, ent15, and the predicted one, ent81, both refer to the same
person, but not being clustered together. arguably this is a dif   cult id91 as one entity refers
to    kate middleton    and the other to    the duchess of cambridge   . the right example shows a
situation in which the model fails as it perhaps gets too little information from the short query and
then selects the wrong cue with the term    claims    near the wrongly identi   ed entity ent1 (correct:
ent74).

c.2

impatient reader

to give a better intuition for the behaviour of the impatient reader, we use a similar visualisation
technique as before. however, this time around we highlight the attention at every time step as
the model updates its focus while moving through a given query. figures 10   13 shows how the
attention of the impatient reader changes and becomes increasingly more accurate as the model

11

figure 7: two more correctly answered validation set queries. the left example (entity ent315) re-
quires correctly attributing the quote, which does not appear trivial with a number of other candidate
entities in the vicinity. the right hand side shows our model is not afraid of chuck norris (ent164).

figure 8: attention heat maps from the attentive reader for two wrongly answered validation set
queries. in the left case the model returns ent85 (correct: ent67), in the right example it gives ent24
(correct: ent64). in both cases the query is unanswerable due to its ambiguous nature and the model
selects a plausible answer.

considers larger parts of the query. note how the attention is distributed fairly arbitraty at    rst,
slowly focussing on the correct entity ent5 only once the question has suf   ciently been parsed.

12

figure 9: additional heat maps for negative results. here the left query selected ent81 instead of
ent15 and the right query ent1 instead of ent74.

figure 10: attention of the impatient reader at time steps 1, 2 and 3.

figure 11: attention of the impatient reader at time steps 4, 5 and 6.

13

figure 12: attention of the impatient reader at time steps 7, 8 and 9.

figure 13: attention of the impatient reader at time steps 10, 11 and 12.

14

