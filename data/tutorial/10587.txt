university of california,

irvine

discovery of latent factors in high-dimensional data using tensor methods

dissertation

submitted in partial satisfaction of the requirements

for the degree of

doctor of philosophy

in electrical and computer engineering

by

furong huang

6
1
0
2

 

n
u
j
 

0
1

 
 
]

g
l
.
s
c
[
 
 

1
v
2
1
2
3
0

.

6
0
6
1
:
v
i
x
r
a

dissertation committee:
assistant professor animashree anandkumar, chair
professor carter butts
associate professor athina markopoulou

2016

all materials c(cid:13) 2016 furong huang

dedication

to jinsong huang and shaoyun liu

ii

table of contents

list of figures

list of tables

list of algorithms

acknowledgments

curriculum vitae

abstract of the dissertation

1 introduction

1.1 summary of contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.1 globally guaranteed online tensor decomposition . . . . . . . . . .
1.1.2 deployment of scalable tensor decomposition framework . . . . . .
1.1.3 learning invariant models using convolutional tensor decomposition
1.1.4 learning latent tree models using hierarchical tensor decomposition
1.1.5 discovering neuronal cell types using id106 . . . . . . .
1.2 tensor preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 background and related works . . . . . . . . . . . . . . . . . . . . . . . . .
1.3.1 online stochastic gradient for tensor decomposition . . . . . . . . .
1.3.2 applying online tensor methods for learning latent variable models
1.3.3 dictionary learning through convolutional tensor decomposition . .
1.3.4 latent tree model learning via hierarchical tensor decomposition .
1.4 thesis structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 online stochastic gradient for tensor decomposition

2.1 preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 stochastic id119 for strict saddle function . . . . . . . . . . . .
2.2.1
strict saddle property . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.2 proof sketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.3 constrained problems
. . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 online tensor decomposition . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.1 optimization problem for tensor decomposition . . . . . . . . . . . .

iii

page

vii

ix

x

xi

xiii

xvi

1
3
3
4
6
7
8
9
12
12
15
17
22
24

25
27
29
29
32
34
36
36

2.3.2

implementing stochastic gradient oracle . . . . . . . . . . . . . . . .
2.4 experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5 conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3

3.2 learning using third order moment

3 applying online tensor methods for learning latent variable models
3.1 tensor forms for topic and community models . . . . . . . . . . . . . . . .
3.1.1 id96 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.2 mixed membership model
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
3.2.1 id84 and whitening . . . . . . . . . . . . . . . .
3.2.2
stochastic tensor id119 . . . . . . . . . . . . . . . . . . .
3.2.3 post-processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
implementation details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
symmetrization step to compute m2 . . . . . . . . . . . . . . . . . .
3.3.1
3.3.2 e   cient randomized svd computations
. . . . . . . . . . . . . . .
3.3.3
stochastic updates . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3.4 computational complexity . . . . . . . . . . . . . . . . . . . . . . . .
3.4 validation methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.1 p -value testing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.2 id74 . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5 experimental results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.6 conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

38
40
42

43
45
45
48
50
51
52
53
54
54
55
59
61
66
66
67
70
76

4 dictionary learning through convolutional tensor decomposition

4.1.1 convolutional dictionary learning/ica model

78
80
81
82
84
87
88
89
91
92
92
97
4.7 conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102

4.1 model and formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
4.2 form of cumulant moment tensors . . . . . . . . . . . . . . . . . . . . . . .
4.3 alternating least squares for convolutional tensor decomposition . . . . .
4.4 algorithm optimization to reduce memory and computational costs . . . .
. . . . . . . . . . . . . . .
. . . . .
4.5 experiments: comparison with alternating minimization . . . . . . . . . . .
4.6 application: learning word-sequence embeddings . . . . . . . . . . . . . . .
4.6.1 word-sequence modeling and formulation . . . . . . . . . . . . . . .
4.6.2 evaluating embeddings through downstream tasks . . . . . . . . . .

4.4.1 challenge: computing ((h   h).     (g   g))   
4.4.2 challenge: computing m = c3(h     g)    ((h   h).     (g   g))   

5 latent tree model learning through hierarchical tensor decomposition 103
5.1 latent tree graphical model preliminaries . . . . . . . . . . . . . . . . . . . 105
5.2 overview of approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
5.3 structure learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
5.4 parameter estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
5.5
integrated structure and parameter estimation . . . . . . . . . . . . . . . . 111
5.5.1 local recursive grouping with tensor decomposition . . . . . . . . . 111
5.5.2 merging and alignment correction . . . . . . . . . . . . . . . . . . . 113

iv

5.6 theoretical gaurantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
5.7 experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
5.7.1 validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
5.8 conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122

6.1

6 discovering cell types with spatial point process mixture model

123
introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
6.1.1 motivations and goals . . . . . . . . . . . . . . . . . . . . . . . . . . 124
6.1.2 previous work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
6.2 modeling cell-types using spatial point process features . . . . . . . . . . . 129
6.2.1 the marked spatial point process representation of ish images . . . 129
130
6.2.2 representing spatial point processes using joint feature histograms
6.3 un-mixing spatial point processes to discover cell-types . . . . . . . . . . . 131
6.3.1 generative model: a variation of id44 . . . . 131
6.3.2 estimating the cell-type dependent gene expression pro   le    . . . . 132
6.3.3 estimating the cell-type dependent spatial point process histogram h 133
6.4 results and evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
implementation details . . . . . . . . . . . . . . . . . . . . . . . . . . 134
6.4.1
6.4.2 evaluating cell-type gene expression pro   le predictions . . . . . . . 135
6.4.3 comparison to standard average gene expression features
. . . . . 136
6.4.4 a brief analysis of recovered cell types in somatosensory cortex . 138
6.5 conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139

7 conclusion and outlook

141
7.1 conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
7.2 outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142

bibliography

143

a appendix for online stochastic gradient for tensor decomposition

156
a.1 detailed analysis for section 2.2 in unconstrained case . . . . . . . . . . . . 156
a.2 detailed analysis for section 2.2 in constrained case . . . . . . . . . . . . . 172
a.2.1 preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
a.2.2 geometrical lemmas regarding constraint manifold . . . . . . . . . 178
a.2.3 main theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
a.3 detailed proofs for section 2.3 . . . . . . . . . . . . . . . . . . . . . . . . . . 195
a.3.1 warm up: maximum eigenvalue formulation . . . . . . . . . . . . . 195
a.3.2 new formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
a.3.3 extending to tensors of di   erent order
. . . . . . . . . . . . . . . . 209

b appendix for applying online tensor methods for learning lvms

212
b.1 stochastic updates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
b.2 proof of algorithm correctness
. . . . . . . . . . . . . . . . . . . . . . . . . 214
b.3 gpu architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
b.4 results on synthetic datasets . . . . . . . . . . . . . . . . . . . . . . . . . . 219

v

b.5 comparison of error scores

. . . . . . . . . . . . . . . . . . . . . . . . . . . 220

c appendix for dictionary learning via convolutional tensor method

224
c.1 cumulant form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
c.2 proof for main theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
c.3 parallel inversion of    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226

d appendix for latent tree learning via hierarchical tensor method

228
d.1 additivity of the multivariate information distance . . . . . . . . . . . . . . 228
d.2 local recursive grouping . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
d.3 proof sketch for theorem 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . 231
d.4 proof of correctness for lrg . . . . . . . . . . . . . . . . . . . . . . . . . . 233
d.5 cross group alignment correction . . . . . . . . . . . . . . . . . . . . . . . 235
d.6 computational complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
d.7 sample complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
d.8 e   cient svd using sparsity and id84 . . . . . . . . . 238

e appendix for spatial point process mixture model learning

240
e.1 morphological basis extraction . . . . . . . . . . . . . . . . . . . . . . . . . 240
e.1.1 gaussian prior convolutional sparse coding . . . . . . . . . . . . . . 241
e.1.2 image registration/alignment . . . . . . . . . . . . . . . . . . . . . . 242

vi

list of figures

1.1 unsupervised learning general framework . . . . . . . . . . . . . . . . . . . .
1.2 tensor decomposition framework is versatile . . . . . . . . . . . . . . . . . .
1.3 tensor decomposition vs variational id136 on pubmed . . . . . . . . . . .
1.4 tensor decomposition vs variational id136 on social networks . . . . . . .
1.5 id27 and sentence embedding . . . . . . . . . . . . . . . . . . .
1.6 hierarchical tensor decomposition. . . . . . . . . . . . . . . . . . . . . . . . .
1.7 examples of brain slices.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.8 orthogonal matrix decomposition is not unique without eigenvalue gap. . . .
1.9 orthogonal tensor decomposition is unique with or without eigenvalue gap.
.
. . . . . . . .
1.10 flat multi-view vs hierarchical latent variable graphical model

2.1 comparison of di   erent objective functions . . . . . . . . . . . . . . . . . . .
2.2 comparison of di   erent objective functions . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . .
3.1 e   cient computation in smart order
3.2 data transfer between cpu and gpu . . . . . . . . . . . . . . . . . . . . . .
3.3 stgd running time comparison . . . . . . . . . . . . . . . . . . . . . . . . .
3.4 p -value matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5 yelp result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.6 facebook result tunning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.1 block structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 error and running time comparison . . . . . . . . . . . . . . . . . . . . . . .
4.3 principal component projection . . . . . . . . . . . . . . . . . . . . . . . . .
4.4 overview of our convdic+deconvdec framework . . . . . . . . . . . . . . . .
4.5 tensor decomposition for learning convolutional ica models . . . . . . . . .
4.6 third order cumulant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

page

3
5
5
5
6
7
8
10
11
23

41
42

55
58
60
66
73
76

84
92
93
94
95
95

5.1 latent tree and hierarchical tensor decomposition . . . . . . . . . . . . . . . 104
5.2 overall approach illustrated in a toy example
. . . . . . . . . . . . . . . . . 107
5.3 running time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
5.4 subtree 1 of estimated human disease hierarchy . . . . . . . . . . . . . . . . 120
5.5 subtree 1 of estimated human disease hierarchy . . . . . . . . . . . . . . . . 121

6.1 overview of the proposed framework . . . . . . . . . . . . . . . . . . . . . . 128
6.2 synthetic results and comparison with average gene expression level baseline. 135

vii

6.3 estimated    on marker genes for 8 cell types . . . . . . . . . . . . . . . . . . 137
6.4 detected 8 cell type feature visualization . . . . . . . . . . . . . . . . . . . . 138

viii

list of tables

3.1 id202ic operation counts . . . . . . . . . . . . . . . . . . . . . . . .
3.2 time and space complexity . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
3.3 new york times results: topics
3.4 new york times results: words
. . . . . . . . . . . . . . . . . . . . . . . . .
3.5 datasets summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.6 compare community detection results against variational method . . . . . .
3.7 membership recovery in yelp review data . . . . . . . . . . . . . . . . . . . .

page

59
61
72
72
73
74
75

97
4.1 summary statistics of the datasets used.
. . . . . . . . . . . . . . . . . . . .
99
4.2 classi   cation tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 paraphrase detection tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
4.4 sts task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101

5.1 worst-case computational complexity of our algorithm . . . . . . . . . . . . 117
5.2 robinson foulds (rf) metric
. . . . . . . . . . . . . . . . . . . . . . . . . . 119

ix

list of algorithms

noisy stochastic gradient
projected noisy stochastic gradient

page
31
1
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
2
. . . . . . . . . . . . . . . . . . . . . .
51
3 moment-based spectral learning of latent variable models . . . . . . . . . . .
57
randomized tall-thin svd . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
57
5
randomized pseudoinverse . . . . . . . . . . . . . . . . . . . . . . . . . . . .
lrg with parameter estimation . . . . . . . . . . . . . . . . . . . . . . . . 112
6
7
. . . . . . . . . . . . . . . . . . . . 113
merging and alignment correction (mac)
8
parameter alignment correction . . . . . . . . . . . . . . . . . . . . . . . . 115

x

acknowledgments

first and foremost i want to thank my advisor animashree anandkumar, who has been my
role model as a successful female professor in machine learning. it has been an honor to be
her    rst ph.d. student. i appreciate all the e   orts she put to help build my con   dence, guide
me through my early research career, and make my graduate study experience productive
and stimulating. her endless enthusiasm for research has been contagious and a source of
motivation. she has also continually and convincingly conveyed a spirit of adventure with
regard to research and scholarship. anima is not only a role model, a career guide but also a
friend who shares life experience and o   ers excellent advice. i couldn   t have fought through
the tough times in my ph.d. pursuit without her inspiration or support.

during my graduate studies, i have been lucky to have collaborated with some smart and
innovative minds who inspired me profoundly. my collaborator rong ge has impressed
me by his enthusiasm, intensity and incredible ability to disentangle complicated research
problems. i would also like to acknowledge chi jin and yang yuan for always being available
for discussions and brainstorming. i am especially grateful for working with srini turaga and
ernest fraenkel. they provided comments and advice from fresh angles and stimulated me to
think di   erently. i appreciate insightful and sparkling discussions with sham kakade, daniel
hsu, david mimno, david blei, qirong ho, alex smola, paul mineiro, nikos karampatziakis
and others. during my internship in microsoft research new england, i have met the most
wonderful mentors jennifer chayes and christian borgs, whose support has powered me to
chase my academic dreams.

i would like to thank my committee members, professor athina markopoulou, and professor
carter butts, who are always there for me whenever i need advice. in addition, a thank
you to professor max welling and professor alexander ihler, who introduced me to machine
learning, and stimulated my long lasting enthusiasm for machine learning. i also appreciate
the e   orts of professor padhraic smyth, who started the data science initiative, a growing
interdisciplinary machine learning community, in uc irvine.

the members of the megadata group, majid janzamin, hanie sedghi, niranjan un,
forough arabshahi, yang shi, kamyar azizzade, and saeed karimi bidhendi, have brought
immense amount of joy to my personal and professional time at uc irvine. i am grateful
for the nights we spent working on paper deadlines, as well as the fun times we had wearing
bean sprout hair clips in the lab and posing for group pro   le pictures. the group has been
a source of friendships and collaborations.

i thank mit press for permission to include chapter 2 of my thesis, which was originally
published in conference of learning theory. and i thank mit press for permission to
include chapter 3 and 4 of my thesis, which was originally published in journal of machine
learning. i gratefully acknowledge the funding sources that made my ph.d. work possible.
i was funded by the eecs department fellowship. my work was also supported by the
national science foundation bigdata award.

xi

lastly, i would like to thank my family for all their unconditional love and faithful support.
thank my parents, jinsong huang and shaoyun liu, for raising me with hard-working spirit
and a love of science. wenchao xi, thank you for always being by my side, sharing joy and
sorrow, in the years of adventure.

xii

curriculum vitae

furong huang

education

doctor of philosophy in ece
university of california irvine

master of science in ece
university of california irvine

bachelor of science in eecs
zhejiang university

research experience

graduate research assistant
university of california irvine

research intern
microsoft research

2016
irvine, ca, usa

2012
irvine, ca, usa

2010
hangzhou, zhejiang, china

2010   2016
irvine, california

2014.3   2014.5
redmond, washington

research intern
microsoft research new england

2014.6   2014.12
cambridge, massachusetts

refereed journal publications

f. huang, u.n. niranjan, m.u. hakeem and a. anandkumar,    on-
line tensor methods for learning latent variable models   
journal of machine learning

a. anandkumar, v.y.f tan, f. huang and a.s. willsky,    high-
dimensional structure learning of ising models: local separation
criterion   
annals of statistics

a. anandkumar, v.y.f tan, f. huang and a.s. willsky,    high-
dimensional gaussian graphical model selection: walk-summability
and local separation criterion   
journal of machine learning

2014

2012

2012

xiii

refereed conference publications

f. huang, a. anandkumar, c. borgs, j. chayes, e. fraenkel, m.
hawrylycz, e. lein, a. ingrosso, s. turaga,    discovering neuronal
cell types and their gene expression pro   les using a spatial point
process mixture model   
nips bigneuro workshop 2015

f. huang, u.n. niranjan, j. perros, r. chen, j. sun, a. anand-
kumar,   scalable latent tree model and its application to health
analytics   
nips 2015 workshop on machine learning in healthcare

f. huang, a. anandkumar,    convolutional dictionary learning
through tensor factorization   
jmlr conference and workshop proceedings

f. arabshahi, f. huang, a. anandkumar, c. butts,    are you going
to the party: depends, who else is coming?    learning hidden group
dynamics via conditional latent tree models   
2015 ieee international conference on data mining (icdm)

f. huang, s. matusevych, a.anandkumar, n. karampatziakism and
p. mineiro,    distributed id44 via tensor fac-
torization   
nips optimization for machine learning workshop

a. anandkumar, d. hsu, f. huang and s.m. kakade,    learning
high-dimensional mixtures of id114   
proc. of nips 2012

f. huang and a. anandkumar,    fcd: fast-concurrent-distributed
load balancing under switching costs and imperfect observations   
in proc. of the 32nd ieee infocom

f. huang, w. wang and z. zhang,    prediction-based spectrum ag-
gregation with hardware limitation in cognitive radio networks   
ieee vehicular technology conference

2015

2015

2015

2015

2014

2012

2013

2010

software

xiv

tensordecom4topicmodeling
c++ algorithm that solves id96 lda using tensor decomposition on single
node workstations.

link to github repository

onlinetensorcommunity
c++ and cuda algorithms that solves community detection problem using tensor de-
composition on single node cpu and gpu.

link to github repository

spectrallda-tensorspark
spark spectral lda algorithms in scala that solves large scale tensor decomposition.

link to github repository

convdiclearntensorfactor
tensor decomposition algorithms that learns convolutional dictionary models.

link to github repository

awards

mlconf industry impact student research winner
google

2015
san francisco, california

travel grant
nips

travel grant
wiml

fellowship
university of california irvine

2015
montreal, canada

2013
lake tahoe, nevada

2010
irvine, california

xv

abstract of the dissertation

discovery of latent factors in high-dimensional data using tensor methods

by

furong huang

doctor of philosophy in electrical and computer engineering

university of california, irvine, 2016

assistant professor animashree anandkumar, chair

unsupervised learning aims at the discovery of hidden structure that drives the observations

in the real world. it is essential for success in modern machine learning and arti   cial intel-

ligence. latent variable models are versatile in unsupervised learning and have applications

in almost every domain, e.g., social network analysis, natural language processing, computer

vision and computational biology. training latent variable models is challenging due to the

non-convexity of the likelihood objective function. an alternative method is based on the

spectral decomposition of low order moment matrices and tensors. this versatile framework

is guaranteed to estimate the correct model consistently. my thesis spans both theoretical

analysis of tensor decomposition framework and practical implementation of various appli-

cations.

this thesis presents theoretical results on convergence to globally optimal solution of tensor

decomposition using the stochastic id119, despite non-convexity of the objective.

this is the    rst work that gives global convergence guarantees for the stochastic gradient

descent on non-convex functions with exponentially many local minima and saddle points.

this thesis also presents large-scale deployment of id106 (matrix and tensor

decomposition) carried out on cpu, gpu and spark platforms. id84

techniques such as random projection are incorporated for a highly parallel and scalable

xvi

tensor decomposition algorithm. we obtain a gain in both accuracies and in running times

by several orders of magnitude compared to the state-of-art variational methods.

to solve real world problems, more advanced models and learning algorithms are proposed.

after introducing tensor decomposition framework under id44 (lda)

model, this thesis discusses generalization of lda model to mixed membership stochastic

block model for learning hidden user commonalities or communities in social network, con-

volutional dictionary model for learning phrase templates and word-sequence embeddings,

hierarchical tensor decomposition and latent tree structure model for learning disease hierar-

chy in healthcare analytics, and spatial point process mixture model for detecting cell types

in neuroscience.

xvii

chapter 1

introduction

there has been tremendous excitement about machine learning and arti   cial intelligence over

the last few years. we are now able to do automated classi   cation of images, where there

are a prede   ned set of image categories. due to the enormous amount of available labeled

data, and powerful computation resources, we can train massive neural networks and obtain

features for classi   cation in domains such as image classi   cation, id103, and

text understanding. however, all these tasks fall under what we call supervised learning,

where the training data provides label information. what if such labeled information about

the categories is absent? can we have automated discovery of the features and categories?

this problem is known as unsupervised learning, and experts agree that it is one of the

hardest problems in machine learning. unsupervised learning is usually the foundation for

the success of supervised learning in many real world problems, and it aims at summarizing

key features in the data. human beings are known to be good at unsupervised learning,

as we accumulate    general knowledge    or    common sense.    but can we have    intelligent   

machines that mimic such capabilities?

1

we live in a world with explosively growing data; as we receive more data, not only do we

get more information but also are we confronted with more variables or    unknowns   .

in

other words, as the data grows, the number of variables also grows, and this is known as

the high-dimensional regime. learning the data patterns or the model in high dimensions is

extremely challenging due to curse of dimensionality. however, the useful information that

we need to gain an insightful understanding of the data usually hides in a low dimensional

space. finding these hidden structures is computationally challenging since it is akin to

   nding    a needle in a haystack   .

the hidden structures in data can be e   ciently expressed with the use of probabilistic

latent variable models. the computational task of searching for hidden structures is then

expressed as learning a probabilistic latent variable model. once the model is learned, the

hidden variables can be inferred based on the model parameters, as depicted in figure 1.1.

there exit numerous popular approaches for probabilistic latent variable model learning

algorithms, among which two families of approaches are particularly successful: randomized

algorithms (such as mcmc) and deterministic algorithms (such as maximum likelihood

based variational id136). however, randomized algorithms are typically slow due to the

exponential mixing time. the deterministic maximum likelihood based estimators tend to

be faster than randomized algorithms, but the likelihood function is often intractable. one

solution is to substitute the likelihood objective with its approximation and search for the

optima. however, local search methods are susceptible to spurious local optima as the

surrogate likelihoods are usually non-convex.

in this thesis, we analyze and deploy an alternative tensor decomposition framework for

learning latent variable models. the basic paradigm of tensor decomposition framework

dates back to 1894 when pearson [135] proposed the method of moments, a classical pa-

rameter estimation technique using data statistics. the method of moments identi   es the

model whose parameters give rise to the observed aggregated statistics of the data (such

2

h

choice variable

k1

k2

k3

k4

k5

topics

a

a

a

a

a

life

gene data dna rna

words

unlabeled data probabilistic latent variable model

learning algorithm

id136

figure 1.1: a general framework of unsupervised learning framework.

as empirical moments) [12]. although matching the model parameters to the observed mo-

ments may involve solving computationally intractable systems of multivariate polynomial

equations, low-order moments (typically third or fourth order) completely characterize the

distribution for many classes of latent variable models [37, 36, 38, 128, 81, 15, 80], and

decomposition of the low-order statistics of the data (tensors) reveals the consistent model

parameters asymptotically. therefore, the inverse method of moments is solved e   ciently

with consistency guarantees (both in terms of computational and sample complexity), in

contrast to the computationally prohibitive maximum likelihood estimators which require

non-id76 and are subject to local optimality.

1.1 summary of contributions

1.1.1 globally guaranteed online tensor decomposition

learning latent variable models via method of moments involves a challenging non-convex

optimization problem in the high-dimensional regime as tensor decomposition is np-hard

in general. we identify strict saddle property for non-convex problem that allows for ef-

   cient optimization. using this property, we show that from an arbitrary starting point,

noisy stochastic id119 converges to a local minimum in a polynomial number of

iterations. to the best of our knowledge, this is the    rst work that gives global convergence

3

guarantees for stochastic id119 on non-convex functions with exponentially many

local minima and saddle points. our analysis is applied to orthogonal tensor decomposition,

and we propose a new optimization formulation for the tensor decomposition problem that

has strict saddle property. as a result, we get the    rst online algorithm for orthogonal ten-

sor decomposition with global convergence guarantee [64]. by employing this algorithm, we

obtain an e   cient unsupervised learning algorithm for a wide class of latent variable models.

1.1.2 deployment of scalable tensor decomposition framework

tensor decomposition framework is tailored for automated categorization of documents (that

is    nding the hidden topics of articles) and prediction of social actors    common interests or

communities (using the connectivity graph) in social networks e   ciently, see figure 1.2.

compared to the state of the art variational id136, which optimizes the lower bound on

the likelihood, our results are surprisingly accurate and much faster [84, 86]. for instance,

we implemented our tensor decomposition on spark to learn topics in the pubmed data,

which consists of 8 million documents and 700 million words. tensor method achieves much

more accurate results (better likelihood) compared to variational id136 although we never

compute or optimize over the likelihood function. furthermore, tensor method requires much

less computation time and is at least an order of magnitude faster.

another comparison is carried out on graph data to evaluate the performance of discovering

hidden communities. on the facebook friendship network, yelp bipartite review graph and

dblp co-authorship system, tensor decomposition framework continues to be both accuracy

and fast compared to the state-of-the-art variational methods [86].

4

topics

education

crime

sports

figure 1.2: tensor decomposition framework is versatile. (a) automated hidden topic dis-
covery. (b) scalable community membership detection via connectivity graph.

tensor
variational

105

y
t
i
x
e
l
p
r
e
p

104

103

10   104

)
s
(

i

e
m
t
g
n
i
n
n
u
r

8

6

4

2

0

figure 1.3: tensor decomposition framework vs variational id136 on pubmed.

p
u
o
r
g
/

r
o
r
r
e

101

100

10-1

10-2

fb

yp

dblpsub

dblp

106

)
s
(

105

i

s
e
m
t
g
n
i
n
n
u
r

104

103

102

fb

yp

dblpsub

dblp

figure 1.4: tensor decomposition framework vs variational id136 on facebook, yelp and
dblp.

5

1.1.3 learning invariant models using convolutional tensor de-

composition

tensor methods can also be extended to solving the problem of learning shift invariant dic-

tionary elements. the data is modeled as linear combinations of    lters/templates convolved

with activation maps. the    lters are shift invariant dictionary elements due to the con-

volution. a tensor decomposition algorithm with additional shift invariance constraints on

the factors is introduced, and it converges to models with better reconstruction error and is

much faster, compared to the popular alternating minimization heuristic, where the    lters

and activation maps are alternately updated.

this convolutional tensor decomposition framework successfully solves challenging natural

language processing tasks such as learning phrase templates and extracting word-sequence

embeddings, as in figure 1.5. convolutional tensor decomposition learns a good set of

   lters/templates [82] and discriminative features (such as word-sequence embeddings) which

yield successful automated understanding and classi   cation of word-sequences.

tree

the weather is good.

soccer

football

her life spanned years of 
incredible change for women.

mary lived through an era of
liberating reform for women. 

id27

word sequence embedding

figure 1.5: id27 and sentence embedding. id27s are vector repre-
sentations of words, such that words with similar semantic meanings are closer in the vector
space. therefore, a machine can    comprehend    the words. similarly, a more challenging task
is to extract word sequence embeddings, where sentences or arbitrary length word-sequences
that share semantic and syntactic properties are mapped to similar vector representations.

6

1.1.4 learning latent tree models using hierarchical tensor de-

composition

tensor decomposition framework is also extended to learning models with hierarchy. this

thesis presents an integrated approach to structure and parameter estimation in latent tree

models. the proposed algorithm automatically learns the latent variables and their loca-

tions and achieves consistent structure estimation with logarithmic computational complex-

ity. meanwhile, the inverse method of moments is carried out on smartly selected local

neighborhoods with linear computational complexity. a rigorous proof of the global consis-

tency of the structure and parameter estimation under the    divide-and-conquer    framework

is presented. the consistency guarantees apply to a broad class of linear multivariate la-

tent tree models including discrete distributions, continuous multivariate distributions (e.g.

gaussian), and mixed distributions such as gaussian mixtures [88]. this model class is much

more general than discrete models, prevalent in most of the previous works on latent tree

models [128, 127, 59, 17].

=

+

+

=

+

+

=

+

+

=

+

+

figure 1.6: hierarchical tensor decomposition.

this e   cient approach is shown to be useful in healthcare analytics [88], where we account

for the co-occurrence of diseases on individuals and learn a clinical meaningful human disease

hierarchy, using big electronic hospital records which involve millions of patients, hundreds

of millions diagnostic events, and tens of thousands of diseases. the learned hierarchy

7

on human diseases is clinically meaningful and can help doctors prevent potential diseases

according to partial information on patients    health condition.

1.1.5 discovering neuronal cell types using id106

the above advances in unsupervised learning have rich applications in neuroscience. using

spectral decomposition framework, we analyze challenging tasks. for instance, cataloging

neuronal cell types in the brain, which has been the number one goal of the brain initiative

and modern neuroscience. it is an extremely challenging task partly due to the petabyte-

scale size brain-wide single-cell resolution in situ hybridization imagery. previous methods

average over image intensity in local voxels for a rough estimation of gene expression levels.

the success of these methods rely on a precise neuron level image alignment across di   erent

brains, which is computationally prohibitive.

(a)

figure 1.7: examples of brain slices.

(b)

in this thesis, we resolve the above problem using a spatial point process mixture model. we

measure the spatial distribution of neurons labeled in the ish image for each gene and model

it as a spatial point process mixture, whose mixture weights are given by the cell types which

8

express that gene. by    tting a point process mixture model jointly to the ish images, we

infer both the spatial point process distribution for each cell type and their gene expression

pro   le. we validate our predictions of cell type-speci   c gene expression pro   les using single

cell rna sequencing data, recently published for the mouse somatosensory cortex. jointly

with the gene expression pro   les, cell features such as cell size, orientation, intensity and

local density level are inferred per cell type. compared with the state-of-the-art approaches,

our method [83] yields lower/better perplexity scores. in addition, 8 cell types are detected

and their cell features are estimated.

1.2 tensor preliminaries

what is a tensor? a pth order tensor is a p-dimensional array. we will use 4th order tensor
as an example. if t     rd4 is a 4th order tensor, we use ti1,i2,i3,i4(i1, ..., i4     [d]) to denote its
(i1, i2, i3, i4)th entry.

tensors can be constructed from tensor products. we use (u     v) to denote a 2nd order
tensor where (u     v)i,j = uivj. this generalizes to higher order and we use u   4 to denote
the 4th order tensor

[u   4]i1,i2,i3,i4 = ui1ui2ui3ui4.

we say a 4th order tensor t     rd4 has an orthogonal decomposition if it can be written as

t =

a   4
i

,

dxi=1

(1.1)

where ai   s are orthonormal vectors that satisfy kaik = 1 and at
i aj = 0 for i 6= j. we call
the vectors ai   s the components of this decomposition. such a decomposition is unique up

to permutation of ai   s and sign-   ips.

9

a tensor also de   nes a multilinear form (just as a matrix de   nes a bilinear form), for a pth
order tensor t     rdp and matrices mi     rd  ni, i     [p], we de   ne
tj1,j2,...,jpyt   [p]

[t (m1, m2, ..., mp)]i1,i2,...,ip = xj1,j2,...,jp   [d]

mt[jt, it].

that is, the result of the multilinear form t (m1, m2, ..., mp) is another tensor in rn1  n2          np.

we will most often use vectors or identity matrices in the multilinear form. in particular, for a
4th order tensor t     rd4 we know t (i, u, u, u) is a vector and t (i, i, u, u) is a matrix. in par-
ticular, if t has the orthogonal decomposition in (1.1), we know t (i, u, u, u) =pd
i=1(ut ai)3ai
and t (i, i, u, u) =pd

i=1(ut ai)2aiat
i .

why are tensors powerful?

let us start with the simple matrix decomposition, where

the goal is to discover the orthogonal eigenvectors of a matrix. however, it is known that

if the eigenvalues of the matrix are equal to each other, one can not uniquely identify the

eigenvectors. for instance, an identity matrix can be decomposed as the set of basis vector

e1 and e2, as well as u1 and u2, who are 45 degree rotated e1 and e2:

         

1

0

0

1

          = e1e   1 + e2e   2 = u1u   1 + u2u   2 .

e2

u2 = [

   2
2 ,

   2
2 ]

e1

u1 = [

2 ,       2
   2
2 ]

figure 1.8: orthogonal matrix decomposition is not unique without eigenvalue gap.

10

however, in tensors, there exists a unique decomposition even without eigenvalue gap. let

a third order tensor (a cube) be decomposed as a linear combination of 2 rank-1 tensors as

in red and blue, see figure 1.9a. the eigenvectors of the tensor are this red vector and this

blue vector who are orthogonal to each other, and the eigenvalues of the tensor are equal.

consider taking a slice of the tensor, which yields matrix. this matrix shares the same

eigenvectors with the tensor, but the eigenvalues of this matrix will be di   erent depending

on the direction of the slice. therefore, the slice of tensor has eigenvalue gap. and thus we

are able to identify the eigenvectors for the tensor uniquely. since higher order tensors have

additional dimensions and contains more information, it is more powerful than second-order

matrices.

+

   

=

(a)

+

   

=

(b)

+

   

=

(c)

figure 1.9: orthogonal tensor decomposition is unique with or without eigenvalue gap. (a)
a third order tensor equals to a linear combination of rank 1 tensors, where each rank 1
tensor is a third order tensor product of the tensor   s eigenvector. (b) a slice of the tensor
results in a matrix. the matrix shares the same set of eigenvectors with the original tensor,
with a di   erent scaling factor, i.e., di   erent eigenvalues. (c) tensor eigenvectors are uniquely
identi   ed when there is a eigenvalue gap in the slice.

orthogonal tensor decomposition given a tensor t with an orthogonal decomposition, the

orthogonal tensor decomposition problem asks to    nd the individual components a1, ..., ad.

this is a central problem in learning many latent variable models, including hidden markov

model, multi-view models, topic models, mixture of gaussians and independent component

analysis (ica). see the discussion and citations in [13]. orthogonal tensor decomposition
problem can be solved by many algorithms even when the input is a noisy estimation   t     t
in practice this approach has been successfully applied to ica [49], topic
[77, 105, 13].

models [171] and community detection [87].

11

1.3 background and related works

1.3.1 online stochastic gradient for tensor decomposition

stochastic id119 is one of the basic algorithms in optimization. it is often used

to solve the following stochastic optimization problem

w = arg min
w   rd

f (w), where f (w) = ex   d[  (w, x)]

(1.2)

here x is a data point that comes from some unknown distribution d, and    is a id168
that is de   ned for a pair (x, w) of sample and parameters. we hope to minimize the expected

loss e[  (w, x)].

when the function f (w) is convex, convergence of stochastic id119 is well-understood

[147, 138]. however, the stochastic id119 is not only limited to convex functions.

especially, in the context of neural networks, the stochastic id119 is known as

the    id26    algorithm [141], and has been the main algorithm that underlies the

success of deep learning [28]. however, the guarantees in the convex setting do not transfer

to the non-convex settings.

optimizing a non-convex function is np-hard in general. the di   culty comes from two

aspects. first, the function may have many local minima, and it can be hard to    nd the

best one (global minimum) among them. second, even    nding a local minimum can be hard

as there can be many saddle points which have 0-gradient but are not local minima1. in the

most general case, there is no known algorithm that guarantees to    nd a local minimum in a

polynomial number of steps. the discrete analog (   nding a local minimum in domains like
{0, 1}n) has been studied in complexity theory and is pls-complete [96].

1see section 2.2 for the de   nition of saddle points.

12

in many cases, especially in those related to deep neural networks [53]

[43], the main bottleneck in optimization is not due to local minima, but the existence

of many saddle points. gradient-based algorithms are in particular susceptible to saddle

point problems as they only rely on the gradient information. the saddle point problem is

alleviated for second-order methods that also rely on the hessian information [53].

however, using hessian information usually increases the memory requirement and compu-

tation time per iteration. as a result, many applications still use stochastic gradient and

empirically get reasonable results. in this paper we investigate why stochastic gradient meth-

ods can be e   ective even in presence of saddle point, in particular, we answer the following

question:

question: given a non-convex function f with many saddle points, what properties of f

will guarantee stochastic id119 to converge to a local minimum e   ciently?

we identify a property of non-convex functions which we call strict saddle. intuitively, it

guarantees local progress if we have access to the hessian information. surprisingly we show

that, with only    rst order (gradient) information, the stochastic gradient escape from the

saddle points e   ciently. we provide a framework for analyzing stochastic gradient in both

unconstrained and equality-constrained case using this property.

we apply our framework to orthogonal tensor decomposition, which is a core problem in

learning many latent variable models. the tensor decomposition problem is inherently sus-

ceptible to the saddle point issues, as the problem asks to    nd d di   erent components and

any permutation of the true components yields a valid solution. such symmetry creates

exponentially many local minima and saddle points in the optimization problem. using our

new analysis of stochastic gradient, we give the    rst online algorithm for orthogonal tensor

decomposition with global convergence guarantee. this is a key step towards making tensor

decomposition algorithms more scalable.

13

relaxed notions of convexity in optimization theory and economics, there are extensive works

on understanding functions that behave similarly to convex functions (and in particular can

be optimized e   ciently). such notions involve pseudo-convexity [117], quasi-convexity [104],

invexity[75] and their variants. more recently there are also works that consider classes

that admit more e   cient optimization procedures like rsc (restricted strong convexity) [3].

although these classes involve functions that are non-convex, the function (or at least the

function restricted to the region of analysis) still has a unique stationary point that is the

desired local/global minimum. therefore, these works cannot be used to prove global con-

vergence for problems like tensor decomposition, where there are exponentially many local

minima and saddle points by the symmetry of the problem.

second-order algorithms the most popular second-order method is the newton   s method.

although newton   s method converges fast near a local minimum, its global convergence

properties are less understood in the more general case. for non-convex functions, [63] gave

a concrete example where second-order method converges to the desired local minimum in

a polynomial number of steps (interestingly the function of interest is trying to    nd one

component in a 4th order orthogonal tensor, which is a simpler case of our application).

as newton   s method often converges also to saddle points, to avoid this behavior, di   erent

trusted-region algorithms are applied [53].

stochastic gradient and symmetry the tensor decomposition problem we consider in this

paper has the following symmetry: the solution is a set of d vectors v1, ..., vd. if (v1, v2, ..., vd)
is a solution, then for any permutation    and any sign    ips        {  1}d, (..,   iv  (i), ...) is also
a valid solution. in general, symmetry is known to generate saddle points, and variants of

id119 often perform reasonably in these cases (see [143], [139], [92]). the settings

in these work are di   erent from ours, and none of them give bounds on number of steps

required for convergence.

14

many other problems have the same symmetric structure as the tensor decomposition prob-

lem, including the sparse coding problem [132] and many deep learning applications [28]. in

these problems, the goal is to learn multiple    features    where the solution is invariant under

permutation. note that there are many recent papers on iterative/gradient-based algorithms

for problems related to id105 [93, 145]. these problems often have very di   er-

ent symmetry, as if y = ax then for any invertible matrix r we know y = (ar)(r   1x).

in this case, all the equivalent solutions are in a connected low dimensional manifold, and

there need not be saddle points between them.

1.3.2 applying online tensor methods for learning latent vari-

able models

the spectral or moment-based approach involves decomposition of certain empirical moment

tensors, estimated from observed data to obtain the parameters of the proposed probabilistic

model. unsupervised learning for a wide range of latent variable models can be carried out

e   ciently via tensor-based techniques with low sample and computational complexities [10].

in contrast, usual methods employed in practice such as expectation maximization (em)

and id58 do not have such consistency guarantees. while the previous works [8]

focused on theoretical guarantees, in chapter 3 of this thesis, we focus on the implementation

of the tensor methods, study its performance on several datasets.

we introduce an online tensor decomposition based approach for two latent variable mod-

eling problems namely, (1) community detection, in which we learn the latent communities

that the social actors in social networks belong to, and (2) id96, in which we

infer hidden topics of text articles. we consider decomposition of moment tensors using

stochastic id119. we conduct optimization of multilinear operations in sgd and

avoid directly forming the tensors, to save computational and storage costs. we present opti-

15

mized algorithm in two platforms. our gpu-based implementation exploits the parallelism

of simd architectures to allow for maximum speed-up by a careful optimization of storage

and data transfer, whereas our cpu-based implementation uses e   cient sparse matrix com-

putations and is suitable for large sparse data sets. for the community detection problem,

we demonstrate accuracy and computational e   ciency on facebook, yelp, and dblp data

sets, and for the id96 problem, we also demonstrate good performance on the new

york times data set. we compare our results to the state-of-the-art algorithms such as the

variational method and report a gain of accuracy and a gain of several orders of magnitude

in the execution time.

chapter 3 builds on the recent works of anandkumar et al [10, 8] which establishes the cor-

rectness of tensor-based approaches for learning mmsb [5] models and other latent variable

models. while, the earlier works provided a theoretical analysis of the method, the current

paper considers a careful implementation of the method. moreover, there are a number of

algorithmic improvements in this thesis. for instance, while [10, 8] consider tensor power

iterations, based on batch data and de   ations performed serially, here, we adopt a stochastic

id119 approach for tensor decomposition, which provides the    exibility to trade-

o    sub-sampling with accuracy. moreover, we use randomized methods for dimensionality

reduction in the preprocessing stage of our method which enables us to scale our method to

graphs with millions of nodes.

there are other known methods for learning the stochastic block model based on techniques

such as spectral id91 [120] and id76 [39]. however, these methods

are not applicable for learning overlapping communities. we note that learning the mixed

membership model can be reduced to a id105 problem [169]. while collabo-

rative    ltering techniques such as [126, 144] focus on id105 and the prediction

accuracy of recommendations on an unseen test set, we recover the underlying latent com-

16

munities, which helps with the interpretability, and the statistical model can be employed

for other tasks.

although there have been other fast implementations for community detection before [152,

112], these methods are not statistical and do not yield descriptive statistics such as bridging

nodes [129], and cannot perform predictive tasks such as link classi   cation which are the main

strengths of the mmsb model. with the implementation of our tensor-based approach, we

record huge speed-ups compared to existing approaches for learning the mmsb model.

to the best of our knowledge, while stochastic methods for matrix decomposition have been

considered earlier [130, 18], this is the    rst work incorporating stochastic optimization for

tensor decomposition, and paves the way for further investigation on many theoretical and

practical issues. we also note that we never explicitly form or store the subgraph count

tensor, of size o(n3) where n is the number of nodes, in our implementation, but directly

manipulate the neighborhood vectors to obtain tensor decompositions through stochastic

updates. this is a crucial departure from other works on tensor decompositions on gpus [25,

146], where the tensor needs to be stored and manipulated directly.

1.3.3 dictionary learning through convolutional tensor decom-

position

feature or representation learning forms a cornerstone of modern machine learning. repre-

senting the data in the relevant feature space is critical to obtaining good performance in

challenging machine learning tasks in speech, id161 and natural language process-

ing. a popular representation learning framework is based on dictionary learning. here, the

input data is modeled as a linear combination of dictionary elements. however, this model

fails to incorporate natural domain-speci   c invariances such as shift invariance and results

in highly redundant dictionary elements, which makes id136 in these models expensive.

17

these shortcomings can be remedied by incorporating invariances into the dictionary model,

and such models are known as convolutional models. convolutional models are ubiquitous

in machine learning for image, speech and sentence representations [167, 101, 33], and in

neuroscience for modeling neural spike trains [131, 58]. deep convolutional neural networks

are a multi-layer extension of these models with non-linear activations. such models have

revolutionized performance in image, speech and natural language processing [167, 97]. the

convolutional dictionary learning model posits that the input signal x is generated as a linear

combination of convolutions of unknown dictionary elements or    lters f   1 , . . . f   l and unknown

activation maps w   1, . . . w   l:

x =xi   [l]

f   i     w   i ,

(1.3)

where [l] := 1, . . . , l. the vector w   i denotes the activations at locations, where the corre-

sponding    lter f   i

is active.

in order to learn the model in (1.3), usually a square loss reconstruction criterion is employed:

min

fi,wi:kfik=1kx    xi   [l]

fi    wik2.

(1.4)

the constraints (kfik = 1) are enforced, since otherwise, the scaling can be exchanged
between the    lters fi and the activation maps wi. also, an additional id173 term

(for example an    1 term on the w   is) is usually added to the above objective to promote

sparsity on wi.

a popular heuristic for solving (1.4) is based on alternating minimization [34], where the

   lters fi are optimized, while keeping the activations wi    xed, and vice versa. each alter-

nating update can be solved e   ciently (since it is linear in each of the variables). however,

the method is computationally expensive in the large sample setting since each iteration re-

18

quires a pass over all the samples, and in modern machine learning applications, the number

of samples can run into billions. moreover, alternating minimization has multiple spurious

local optima, and reaching the global optimum of (1.4) is np-hard in general. this problem

is severely ampli   ed in the convolutional setting due to additional symmetries, compared

to the usual dictionary learning setting (without the convolutional operation). due to shift

invariance of the convolutional operator, shifting a    lter fi by some amount, and applying

a corresponding negative shift on the activation wi leaves the objective in (1.4) unchanged.

can we design alternative methods for convolutional dictionary learning that are scalable to

huge datasets?

the special case of (1.3) with one    lter (l = 1) is a well studied problem, and is referred to as

blind deconvolution [90]. in general, this problem is not identi   able, i.e. multiple equivalent

solutions can exist [44]. it has been documented that in many cases alternating minimization

produces trivial solutions, where the    lter f = x is the signal itself and the activation is the

identity function [116]. therefore, alternative techniques have been proposed, such as convex

programs, based on nuclear norm minimization [4] and imposing hierarchical bayesian priors

for activation maps [163]. however, there is no analysis for settings with more than one

   lter. incorporating bayesian priors has shown to reduce the number of local optima, but

not eliminate them [163, 109]. moreover, bayesian techniques are in general more expensive

than alternating minimization.

the extension of blind deconvolution to multiple    lters is known as convolutive blind source

separation or convolutive independent component analysis (ica) [90]. previous methods

directly reformulate convolutive ica as an ica model, without incorporating the shift con-

straints. moreover, reformulation leads to an increase in the number of hidden sources from

l to nl in the new model, where n is the input dimension, which is harder to separate

and computationally more expensive. other methods are based on performing ica in the

fourier domain, but the downside is that the new mixing matrix depends on the angular fre-

19

quency, and leads to permutation and sign indeterminacies of the sources across frequencies.

complicated interpolation methods [90] overcome these indeterminacies.

in contrast, our

method avoids all these issues. we do not perform fourier transform on the input. instead,

we employ ffts at di   erent iterations of our method to estimate the    lters e   ciently.

the dictionary learning problem without convolution has received much attention. recent

results show that simple iterative methods can learn the globally optimal solution [2, 19].

also, tensor decomposition methods provably learn the model, when the activations are

independently drawn (the ica model) [12] or are sparse (the sparse coding model) [14]. in

this work, we extend the tensor decomposition methods to e   ciently incorporate the shift

invariance constraints imposed by the convolution operator. this framework is applied to

word-sequence embedding learning in natural language processing.

we have recently witnessed the tremendous success of id27s or word vector

representations in natural language processing. this involves mapping words to vector rep-

resentations such that words which share similar semantic or syntactic meanings are close

to one another in the vector space [29, 47, 48, 124, 136]. id27s have attained

state-of-the-art performance in tasks such as part-of-speech (pos) tagging, chunking, named

entity recognition (ner), and id14. despite this impressive performance,

id27s do not su   ce for more advanced tasks which require context-aware infor-

mation or word orders, e.g. paraphrase detection, id31, plagiarism detection,

information retrieval and machine translation. therefore, extracting word-sequence vector

representations is crucial for expanding the realm of automated text understanding.

previous works on word-sequence embeddings are based on a variety of mechanisms. a

popular method is to learn the composition operators in sequences [125, 166]. the complexity

of the compositionality varies widely: from simple operations such as addition [125, 166] to

complicated id56s [149, 150, 27], convolutional neural networks [97,

97], long short-term memory (lstm) recurrent neural networks [154], or combinations of

20

these architectures [161]. all these methods produce sentence representations that depend

on a supervised task, and the class labels are back-propagated to update the composition

weights [98].

since the above methods rely heavily on the downstream task and the domain of the training

samples, they can hardly be used as universal embeddings across domains, and require inten-

sive pre-training and hyper-parameter tuning. the state-of-the-art unsupervised framework

is skip-thought [103], based on an objective function that abstracts the skip-gram model to

the sentence level, and encodes a sentence to predict the sentences around it. however, the

skip-thought model requires a large corpus of contiguous text, such as the book corpus with

more than 74 million sentences. can we instead e   ciently learn sentence embeddings using

small amounts of samples without supervision/labels or annotated features(such as parse

trees)? also, can the sentence embeddings be context-aware, can handle variable lengths,

and is not limited to speci   c domains?

we propose an unsupervised convdic+deconvdec framework that satis   es all the above

constraints. it is composed of two phases, a comprehension phase which summarizes template

phrases using convolutional dictionary elements, followed by a feature-extraction phase which

extracts activations using deconvolutional decoding. we propose a novel learning algorithm

for the comprehension phase based on convolutional tensor decomposition. note that in the

comprehension phase, phrase templates are learned over    xed length small patches (patch

length is equal to phrase template length), whereas entire word-sequence is decoded to get

the    nal word-sequence embedding in the feature-extraction phase.

we employ our sentence embeddings in the tasks of sentiment classi   cation, semantic textual

similarity estimation, and paraphrase detection over eight datasets from various domains.

these are challenging tasks since they require a contextual understanding of text relation-

ships rather than bags of words. we learn the embeddings from scratch without using any

auxiliary information. while previous works use information such as parse trees, id138

21

or pre-train on a much larger corpus, we train from scratch on small amounts of text and

obtain competitive results, which are close or even better than the state-of-the-art.

this is due to the combination of e   cient modeling and learning approaches in our work.

the convolutional model incorporates word orders and phrase representations, and our tensor

decomposition algorithm can e   ciently learn a set of parameters (phrase templates) for the

convolutional model.

1.3.4 latent tree model learning through hierarchical tensor

decomposition

latent variable id114 span    at models and id187, see figure 1.10

for a    at multi-view model and a hierarchical model. latent tree id114 are a

popular class of latent variable models, where a id203 distribution involving observed

and hidden variables are markovian on a tree. due to the fact that structure of (observable

and hidden) variable interactions are approximated as a tree, id136 on latent trees can be

carried out exactly through a simple belief propagation [134]. therefore, latent tree graphical

models present a good trade-o    between model accuracy and computational complexity.

they are applicable in many domains, where it is natural to expect hierarchical or sequential

relationships among the variables (through a hidden-markov model). for instance, latent

tree models have been employed for phylogenetic reconstruction [56], object recognition [40],

[42] and human pose estimation [157].

the task of learning a latent tree model consists of two parts: learning the tree structure,

and learning the parameters of the tree. there exist many challenges which prohibit e   cient

or guaranteed learning of the latent tree graphical model, which will be addressed in this

thesis:

22

(a) multi-view

(b) hierarchical tree

figure 1.10: flat multi-view latent variable graphical model vs hierarchical latent variable
graphical model.

1. the location and the number of latent variables are hidden, and the marginalized graph

over the observable variables no longer conforms to a tree structure.

2. structure learning algorithms are typically of computational complexity polynomial

with p (number of variables) as discussed in [6, 41]. these methods are serial in nature

and therefore are not scalable for large p.

3. parameter estimation in latent tree models is typically carried out through expecta-

tion maximization (em) or other local search heuristics [41]. these methods have

no consistency guarantees, su   er from the problem of local optima and are not easily

parallelizable.

4. typically structure learning and parameter estimation are carried out one after an-

other.

there has been widespread interest in developing distributed learning techniques, e.g., the

recent works of [148] and [160]. these works consider parameter estimation via likelihood-

based optimizations such as id150, while our method involves more challenging

tasks where both the structure and the parameters are estimated. simple methods such as

local neighborhood selection through    1-id173 [121] or local conditional independence

testing [16] can be parallelized, but these methods do not incorporate hidden variables.

finally, note that the latent tree models provide a statistical description, in addition to

23

revealing the hierarchy. in contrast, hierarchical id91 techniques are not based on a

statistical model [108] and cannot provide valuable information such as the level of correlation

between observed and hidden variables.

1.4 thesis structure

in my thesis, i will    rst prove that simple noisy id119 on a carefully selected ob-

jective function yields global convergence guarantee in chapter 2. based on the theoretical

guarantees, i will show how to make tensor decomposition highly scalable, highly parallel

in chapter 3. furthermore, i extend the framework to learn dictionary or templates with

additional constraints such as shift invariance in image or text dictionary learning using

convolutional dictionary tensor decomposition in chapter 4. i do not limit myself to shallow

models where observations are conditional independent on the hidden dimension. on the

contrary, i extend the multi-view tensor decomposition framework to a hierarchical tensor

decomposition framework to analyze data with complicated hierarchical structure. a latent

tree model is therefore proposed in chapter 5, where latent variable graphical model struc-

ture learning technique is combined with hierarchical tensor decomposition for a consistent

learning of the hierarchical model structure and parameter. finally, i conclude my thesis

with a challenging but important task in chapter 6, discovering cell types in the brain. this

work brings together the techniques used in all previous chapters, such as image processing

to extract cells and cell features from brain slices, learning a point process admixture model.

24

chapter 2

online stochastic gradient for tensor

decomposition

it is established in the previous work [13] that a wide class of latent variable id114

can be learned through tensor decomposition, and model parameters are obtained by decom-

posing higher order data aggregates or modi   ed data moments. therefore, learning latent

variable graphical model is reduced to tensor decomposition problem. tensor decomposition

is a non-id76 problem, and it is known that non-id76 problem

is np hard in general. now the question is: could we use e   cient methods such as stochastic

id119 to reach local optima for a class of function under mild conditions? could

we    t tensor decomposition problem into the class of function?

we analyze stochastic id119 for optimizing non-convex functions. in many cases

for non-convex functions the goal is to    nd a reasonable local minimum, and the main concern

is that gradient updates are trapped in saddle points. in this chapter we identify strict saddle

property for non-convex problem that allows for e   cient optimization. using this property

we show that from an arbitrary starting point, stochastic id119 converges to a

25

local minimum in a polynomial number of iterations. to the best of our knowledge this

is the    rst work that gives global convergence guarantees for stochastic id119 on

non-convex functions with exponentially many local minima and saddle points.

our analysis can be applied to orthogonal tensor decomposition, which is widely used in

learning a rich class of latent variable models. we propose a new optimization formulation

for the tensor decomposition problem that has strict saddle property. as a result we get the

   rst online algorithm for orthogonal tensor decomposition with global convergence guarantee.

strict saddle functions

given a function f (w) that is twice di   erentiable, we call w a stationary point if    f (w) = 0.
a stationary point can either be a local minimum, a local maximum or a saddle point. we

identify an interesting class of non-convex functions which we call strict saddle. for these

functions the hessian of every saddle point has a negative eigenvalue.

in particular, this

means that local second-order algorithms which are similar to the ones in [53] can always

make some progress.

it may seem counter-intuitive why stochastic gradient can work in these cases: in particular

if we run the basic id119 starting from a stationary point then it will not move.

however, we show that the saddle points are not stable and that the randomness in stochastic

gradient helps the algorithm to escape from the saddle points.

theorem 2.1 (informal). suppose f (w) is strict saddle (see de   nition 2.3), noisy gradient

descent (algorithm 1) outputs a point that is close to a local minimum in polynomial number

of steps.

online tensor decomposition requiring all saddle points to have a negative eigenvalue may

seem strong, but it already allows non-trivial applications to natural non-id76

26

problems. as an example, we consider the orthogonal tensor decomposition problem. this

problem is the key step in spectral learning for many latent variable models.

we design a new objective function for tensor decomposition that is strict saddle.

theorem 2.2. given random variables x such that t = e[g(x)]     rd4 is an orthogonal
4-th order tensor, there is an objective function f (w) = e[  (w, x)] w     rd  d such that every
local minimum of f (w) corresponds to a valid decomposition of t . further, function f is

strict saddle.

combining this new objective with our framework for optimizing strict saddlefunctions, we

get the    rst online algorithm for orthogonal tensor decomposition with global convergence

guarantee.

2.1 preliminaries

the stochastic gradient aims to solve the stochastic optimization problem (1.2), which we

restate here:

w = arg min
w   rd

f (w), where f (w) = ex   d[  (w, x)].

recall   (w, x) denotes the id168 evaluated for sample x at point w. the algorithm

follows a stochastic gradient

wt+1 = wt          wt  (wt, xt),

(2.1)

where xt is a random sample drawn from distribution d and    is the learning rate.

in the more general setting, stochastic id119 can be viewed as optimizing an

arbitrary function f (w) given a stochastic gradient oracle.

27

de   nition 2.1. for a function f (w) : rd     r, a function sg(w) that maps a variable to
a random vector in rd is a stochastic gradient oracle if e[sg(w)] =    f (w) and ksg(w)    
   f (w)k     q.

in this case the update step of the algorithm becomes wt+1 = wt       sg(wt).

smoothness and strong convexity traditional analysis for stochastic gradient often assumes

the function is smooth and strongly convex. a function is   -smooth if for any two points

w1, w2,

k   f (w1)        f (w2)k       kw1     w2k.

(2.2)

when f is twice di   erentiable this is equivalent to assuming that the spectral norm of the

hessian matrix is bounded by   . we say a function is   -strongly convex if the hessian at
any point has smallest eigenvalue at least    (  min(   2f (w))       ).

using these two properties, previous work [138] shows that stochastic gradient converges at

a rate of 1/t. in this thesis we consider non-convex functions, which can still be   -smooth

but cannot be strongly convex.

smoothness of hessians it is common to assume the hessian of the function f to be smooth.

we say a function f (w) has   -lipschitz hessian if for any two points w1, w2 we have

k   2f (w1)        2f (w2)k       kw1     w2k.

(2.3)

this is a third order condition that is true if the third order derivative exists and is bounded.

28

2.2 stochastic id119 for strict saddle func-

tion

in this section we discuss the properties of saddle points, and show if all the saddle points

are well-behaved then stochastic id119    nds a local minimum for a non-convex

function in polynomial time.

notation throughout the chapter we use [d] to denote set {1, 2, ..., d}. we use k  k to denote
the    2 norm of vectors and spectral norm of matrices. for a matrix we use   min to denote
its smallest eigenvalue. for a function f : rd     r,    f and    2f denote its gradient vector
and hessian matrix.

2.2.1 strict saddle property

for a twice di   erentiable function f (w), we call a point stationary point if its gradient is

equal to 0. stationary points could be local minima, local maxima or saddle points. by local

optimality conditions [164], in many cases we can tell what type a point w is by looking at
its hessian: if    2f (w) is positive de   nite then w is a local minimum; if    2f (w) is negative
de   nite then w is a local maximum; if    2f (w) has both positive and negative eigenvalues
then w is a saddle point. these criteria do not cover all the cases as there could be degenerate
scenarios:    2f (w) can be positive semide   nite with an eigenvalue equal to 0, in which case
the point could be a local minimum or a saddle point.

if a function does not have these degenerate cases, then we say the function is strict saddle:

de   nition 2.2. a twice di   erentiable function f (w) is strict saddle, if all its local minima
have    2f (w)     0 and all its other stationary points satisfy   min(   2f (w)) < 0.

29

intuitively, if we are not at a stationary point, then we can always follow the gradient and

reduce the value of the function. if we are at a saddle point, we need to consider a second

order taylor expansion:

f (w +    w)     w + (   w)t   2f (w)(   w) + o(k   wk3).

since the strict saddle property guarantees    2f (w) to have a negative eigenvalue, there is
always a point that is near w and has strictly smaller function value. it is possible to make

local improvements as long as we have access to second order information. however it is not

clear whether the more e   cient stochastic gradient updates can work in this setting.

to make sure the local improvements are signi   cant, we use a robust version of the strict

saddle property:

de   nition 2.3. a twice di   erentiable function f (w) is (  ,   ,   ,   )-strict saddle, if for any

point w at least one of the following is true

1. k   f (w)k       .

2.   min(   2f (w))          .

3. there is a local minimum w    such that kw    w   k       , and the function f (w   ) restricted

to 2   neighborhood of w    (kw        w   k     2  ) is   -strongly convex.

intuitively, this condition says for any point whose gradient is small, it is either close to a

robust local minimum, or is a saddle point (or local maximum) with a signi   cant negative

eigenvalue.

we purpose a simple variant of stochastic gradient algorithm, where the only di   erence to the

traditional algorithm is we add an extra noise term to the updates. the main bene   t of this

additional noise is that we can guarantee there is noise in every direction, which allows the

30

procedure 1 noisy stochastic gradient
input: stochastic gradient oracle sg(w), initial point w0, desired accuracy   .
output: wt that is close to some local minimum w   .
1: choose    = min{   o(  2/ log(1/  )),   max}
2: for t = 0 to   o(1/  2) do
3:
4:

sample noise n uniformly from unit sphere.
wt+1     wt       (sg(w) + n)

algorithm to e   ectively explore the local neighborhood around saddle points. if the noise

from stochastic gradient oracle already has nonnegligible variance in every direction, our

analysis also applies without adding additional noise. we show noise can help the algorithm

escape from saddle points and optimize strict saddle functions.

theorem 2.3 (main theorem). suppose a function f (w) : rd     r that is (  ,   ,   ,   )-strict
saddle, and has a stochastic gradient oracle with radius at most q. further, suppose the

function is bounded by |f (w)|     b, is   -smooth and has   -lipschitz hessian. then there ex-
ists a threshold   max =     (1), so that for any    > 0, and for any          max/ max{1, log(1/  )},
with id203 at least 1       in t =   o(     2 log(1/  )) iterations, algorithm 1 (noisy gradient
descent) outputs a point wt that is   o(p   log(1/    ))-close to some local minimum w   .
here (and throughout the rest of the chapter)   o(  ) (      ,     ) hides the factor that is polynomially
dependent on all other parameters (including q, 1/  , 1/  , 1/  , 1/  , b,   ,   , and d), but

independent of    and   . so it focuses on the dependency on    and   . our proof technique

can give explicit dependencies on these parameters however we hide these dependencies for

simplicity of presentation. 1

remark (decreasing learning rate). often analysis of stochastic id119 uses de-

creasing learning rates and the algorithm converges to a local (or global) minimum. since the

function is strongly convex in the small region close to local minimum, we can use theorem

2.3 to    rst    nd a point that is close to a local minimum, and then apply standard analysis

1 currently, our number of iteration is a large polynomial in the dimension d. we have not tried to
optimize the degree of this polynomial. empirically the dependency on d is much better, whether the
dependency on d can be improved to poly log d is left as an open problem.

31

of sgd in the strongly convex case (where we decrease the learning rate by 1/t and get 1/   t
convergence in kw     w   k).

in the next part we sketch the proof of the main theorem. details are deferred to ap-

pendix a.1.

2.2.2 proof sketch

in order to prove theorem 2.3, we analyze the three cases in de   nition 2.3. when the

gradient is large, we show the function value decreases in one step (see lemma 2.1); when

the point is close to a local minimum, we show with high id203 it cannot escape in the

next polynomial number of iterations (see lemma 2.2).

lemma 2.1 (gradient). under the assumptions of theorem 2.3, for any point with k   f (wt)k
    c      (where c =     (1)) and c            , after one iteration we have e[f (wt+1)]    
f (wt)          (  2).

the proof of this lemma is a simple application of the smoothness property.

lemma 2.2 (local minimum). under the assumptions of theorem 2.3, for any point wt
that is   o(     ) <    close to local minimum w   , in   o(     2 log(1/  )) number of steps all future

wt+i   s are   o(p   log(1/    ))-close with id203 at least 1       /2.

the proof of this lemma is similar to the standard analysis [138] of stochastic id119

in the smooth and strongly convex setting, except we only have local strong convexity. the

proof appears in appendix a.1.

the hardest case is when the point is    close    to a saddle point: it has gradient smaller than

   and smallest eigenvalue of the hessian bounded by      . in this case we show the noise in
our algorithm helps the algorithm to escape:

32

lemma 2.3 (saddle point). under the assumptions of theorem 2.3, for any point wt where

k   f (wt)k     c      (for the same c as in lemma 2.1), and   min(   2f (wt))          , there is a
number of steps t that depends on wt such that e[f (wt+t )]     f (wt)          (  ). the number of
steps t has a    xed upper bound tmax that is independent of wt where t     tmax =   o(1/  ).

intuitively, at point wt there is a good direction that is hiding in the hessian. the hope of

the algorithm is that the additional (or inherent) noise in the update step makes a small

step towards the correct direction, and then the gradient information will reinforce this small

perturbation and the future updates will    slide    down the correct direction.

to make this more formal, we consider a coupled sequence of updates   w such that the

function to minimize is just the local second order approximation

  f (w) = f (wt) +    f (wt)t (w     wt) +

1
2

(w     wt)t   2f (wt)(w     wt).

the dynamics of stochastic id119 for this quadratic function is easy to analyze as

  wt+i can be calculated analytically. indeed, we show the expectation of   f (   w) will decrease.

more concretely we show the point   wt+i will move substantially in the negative curvature di-

rections and remain close to wt in positive curvature directions. we then use the smoothness

of the function to show that as long as the points did not go very far from wt, the two update
sequences   w and w will remain close to each other, and thus   f (   wt+i)     f (wt+i). finally we
prove the future wt+i   s (in the next t steps) will remain close to wt with high id203 by

martingale bounds. the detailed proof appears in appendix a.1.

with these three lemmas it is easy to prove the main theorem.
intuitively, as long as
there is a small id203 of being   o(     )-close to a local minimum, we can always apply
lemma 2.1 or lemma 2.3 to make the expected function value decrease by      (  ) in at most

  o(1/  ) iterations, this cannot go on for more than   o(1/  2) iterations because in that case

33

the expected function value will decrease by more than 2b, but max f (x)     min f (x)     2b
by our assumption. therefore in   o(1/  2) steps with at least constant id203 wt will
become   o(     )-close to a local minimum. by lemma 2.2 we know once it is close it will
almost always stay close, so after q epochs of   o(1/  2) iterations each, the id203 of

success will be 1     exp(      (q)). taking q = o(log(1/  )) gives the result. more details
appear in appendix a.1.

2.2.3 constrained problems

in many cases, the problem we are facing are constrained optimization problems. in this

part we brie   y describe how to adapt the analysis to problems with equality constraints

(which su   ces for the tensor application). dealing with general inequality constraint is left

as future work.

for a constrained optimization problem:

min
w   rd
s.t.

f (w)

ci(w) = 0,

i     [m]

(2.4)

in general we need to consider the set of points in a low dimensional manifold that is de   ned

by the constraints. in particular, in the algorithm after every step we need to project back

to this manifold (see algorithm 2 where   w is the projection to this manifold).

procedure 2 projected noisy stochastic gradient
input: stochastic gradient oracle sg(w), initial point w0, desired accuracy   .
output: wt that is close to some local minimum w   .
1: choose    = min{   o(  2/ log(1/  )),   max}
2: for t = 0 to   o(1/  2) do
3:
4:
5:

sample noise n uniformly from unit sphere.
vt+1     wt       (sg(w) + n)
wt+1 =   w (vt+1)

34

for constrained optimization it is common to consider the lagrangian:

l(w,   ) = f (w)    

  ici(w).

mxi=1

(2.5)

under common regularity conditions, it is possible to compute the value of the lagrangian

multipliers:

     (w) = arg min

   k   wl(w,   )k.

we can also de   ne the tangent space, which contains all directions that are orthogonal to all
the gradients of the constraints: t (w) = {v :    ci(w)t v = 0; i = 1,         , m}. in this case the
corresponding gradient and hessian we consider are the    rst-order and second-order partial
derivative of lagrangian l at point (w,      (w)):

  (w) =    wl(w,   )|(w,     (w)) =    f (w)    

     i (w)   ci(w)

m(w) =    2

wwl(w,   )|(w,     (w)) =    2f (w)    

     i (w)   2ci(w)

mxi=1
mxi=1

(2.6)

(2.7)

we replace the gradient and hessian with   (w) and m(w), and when computing eigenvectors

of m(w) we focus on its projection on the tangent space. in this way, we can get a similar

de   nition for strict saddle (see appendix a.2), and the following theorem.

theorem 2.4. (informal) under regularity conditions and smoothness conditions, if a con-

strained optimization problem satis   es strict saddle property, then for a small enough   , in

  o(     2 log 1/  ) iterations projected noisy id119 (algorithm 2) outputs a point w
that is   o(      log(1/    )) close to a local minimum with id203 at least 1       .

detailed discussions and formal version of this theorem are deferred to appendix a.2.

35

2.3 online tensor decomposition

in this section we describe how to apply our stochastic id119 analysis to tensor

decomposition problems. we    rst give a new formulation of tensor decomposition as an

optimization problem, and show that it satis   es the strict saddle property. then we explain

how to compute stochastic gradient in a simple example of independent component analysis

(ica) [91].

2.3.1 optimization problem for tensor decomposition

given a tensor t     rd4 that has an orthogonal decomposition

t =

a   4
i

,

dxi=1

(2.8)

where the components ai   s are orthonormal vectors (kaik = 1, at
i aj = 0 for i 6= j), the goal of
orthogonal tensor decomposition is to    nd the components ai   s. this problem has inherent

symmetry: for any permutation    and any set of   i     {  1}, i     [d], we know ui =   ia  (i)
is also a valid solution. this symmetry property makes the natural optimization problems

non-convex.

in this section we will give a new formulation of orthogonal tensor decomposition as an

optimization problem, and show that this new problem satis   es the strict saddle property.

previously, [63] solves the problem of    nding one component, with the following objective

function

max
kuk2=1

t (u, u, u, u).

(2.9)

36

in appendix a.3.1, as a warm-up example we show this function is indeed strict saddle,

and we can apply theorem 2.4 to prove global convergence of stochastic id119

algorithm.

it is possible to    nd all components of a tensor by iteratively    nding one component, and

do careful de   ation, as described in [13] or [20]. however, in practice the most popular

approaches like alternating least squares [50] or fastica [89] try to use a single optimization

problem to    nd all the components. empirically these algorithms are often more robust to

noise and model misspeci   cation.

the most straight-forward formulation of the problem aims to minimize the reconstruction

error

min

   i,kuik2=1

kt    

dxi=1

u   4
i k2
f .

(2.10)

here k    kf is the frobenius norm of the tensor which is equal to the    2 norm when we view
the tensor as a d4 dimensional vector. however, it is not clear whether this function satis   es

the strict saddle property, and empirically stochastic id119 is unstable for this

objective.

we propose a new objective that aims to minimize the correlation between di   erent compo-

nents:

min

   i,kuik2=1 xi6=j

t (ui, ui, uj, uj),

(2.11)

to understand this objective intuitively, we    rst expand vectors uk in the orthogonal basis

formed by {ai}   s. that is, we can write uk =pd

i=1 zk(i)ai, where zk(i) are scalars that cor-
respond to the coordinates in the {ai} basis. in this way we can rewrite t (uk, uk, ul, ul) =

37

pd

i=1(zk(i))2(zl(i))2. from this form it is clear that the t (uk, uk, ul, ul) is always nonnega-

tive, and is equal to 0 only when the support of zk and zl do not intersect. for the objective

function, we know in order for it to be equal to 0 the z   s must have disjoint support. there-

fore, we claim that {uk},   k     [d] is equivalent to {ai},   i     [d] up to permutation and sign
   ips when the global minimum (which is 0) is achieved.

we further show that this optimization program satis   es the strict saddle property and all

its local minima in fact achieves global minimum value. the proof is deferred to appendix

a.3.2.

theorem 2.5. the optimization problem (2.11) is (  ,   ,   ,   )-strict saddle, for    = 1 and

  ,   ,    = 1/poly(d). moreover, all its local minima have the form ui =   ia  (i) for some

  i =   1 and permutation   (i).

note that we can also generalize this to handle 4th order tensors with di   erent positive

weights on the components, or other order tensors, see appendix a.3.3.

2.3.2

implementing stochastic gradient oracle

to design an online algorithm based on objective function (2.11), we need to give an imple-

mentation for the stochastic gradient oracle.

in applications, the tensor t is oftentimes the expectation of multilinear operations of

samples g(x) over x where x is generated from some distribution d.
any x     d, the tensor is t = e[g(x)]. using the linearity of the multilinear map, we
know e[g(x)](ui, ui, uj, uj) = e[g(x)(ui, ui, uj, uj)]. therefore we can de   ne the id168

in other words, for

  (u, x) =pi6=j g(x)(ui, ui, uj, uj), and the stochastic gradient oracle sg(u) =    u  (u, x).

38

for concreteness, we look at a simple ica example. in the simple setting we consider an
unknown signal x that is uniform2 in {  1}d, and an unknown orthonormal linear transfor-
mation3 a (aat = i). the sample we observe is y := ax     rd. using standard techniques
(see [35]), we know the 4-th order cumulant of the observed sample is a tensor that has

orthogonal decomposition. here for simplicity we don   t de   ne 4-th order cumulant, instead

we give the result directly.

de   ne tensor z     rd4 as follows:

z(i, i, i, i) = 3,

   i     [d]

z(i, i, j, j) = z(i, j, i, j) = z(i, j, j, i) = 1,    i 6= j     [d]

where all other entries of z are equal to 0. the tensor t can be written as a function of the

auxiliary tensor z and multilinear form of the sample y.

lemma 2.4. the expectation e[ 1

i=1 a   4

i = t , where ai   s are columns of the

unknown orthonormal matrix a.

2(z     y   4)] =pd

this lemma is easy to verify, and is closely related to cumulants [35]. recall that   (u, y)

denotes the loss (objective) function evaluated at sample y for point u. let   (u, y) =
2(z     y   4)(ui, ui, uj, uj). by lemma 2.4, we know that e[  (u, y)] is equal to the ob-
jective function as in equation (2.11). therefore we rewrite objective (2.11) as the following

pi6=j

1

stochastic optimization problem

min

   i,kuik2=1

e[  (u, y)], where   (u, y) =xi6=j

1
2

(z     y   4)(ui, ui, uj, uj)

2in general ica the entries of x are independent, non-gaussian variables.
3in general (under-complete) ica this could be an arbitrary linear transformation, however usually after

the    whitening    step (see [35]) the linear transformation becomes orthonormal.

39

the stochastic gradient oracle is then

   ui  (u, y) =xj6=i(cid:0)huj, uji ui + 2hui, uji uj     huj, yi2 hui, yi y(cid:1) .

(2.12)

notice that computing this stochastic gradient does not require constructing the 4-th order
tensor t     y   4. in particular, this stochastic gradient can be computed very e   ciently:

remark. the stochastic gradient (2.12) can be computed for all ui   s in o(d3) time for one

sample or o(d3 + d2k) for average of k samples.

proof. the proof is straight forward as the    rst two terms on the right hand side take o(d3)

and is shared by all samples. the third term can be e   ciently computed once the inner-

products between all the y   s and all the ui   s are computed (which takes o(kd2) time).

2.4 experiments

we run simulations for projected noisy id119 (algorithm 2) applied to orthogo-

nal tensor decomposition. the results show that the algorithm converges from random initial

points e   ciently (as predicted by the theorems), and our new formulation (2.11) performs

better than reconstruction error (2.10) based formulation.

settings we set dimension d = 10, the input tensor t is a random tensor in r104 that

has orthogonal decomposition (1.1). the step size is chosen carefully for respective ob-

jective functions. the performance is measured by normalized reconstruction error e =

(cid:16)kt    pd

i=1 u   4
i k2

f(cid:17) /ktk2

f .

samples and stochastic gradients we use two ways to generate samples and compute stochas-

tic gradients. in the    rst case we generate sample x by setting it equivalent to d

1

4 ai with

40

id203 1/d. it is easy to see that e[x   4] = t . this is a very simple way of generating

samples, and we use it as a sanity check for the objective functions.

in the second case we consider the ica example introduced in section 2.3.2, and use equation

(2.12) to compute a stochastic gradient.

in this case the stochastic gradient has a large

variance, so we use mini-batch of size 100 to reduce the variance.

comparison of objective functions we use the simple way of generating samples for our new

objective function (2.11) and reconstruction error objective (2.10). the result is shown in

figure 2.1. our new objective function is empirically more stable (always converges within

10000 iterations); the reconstruction error do not always converge within the same number

of iterations and often exhibits long periods with small improvement (which is likely to be

caused by saddle points that do not have a signi   cant negative eigenvalue).

simple ica example as shown in figure 2.2, our new algorithm also works in the ica

setting. when the learning rate is constant the error stays at a    xed small value. when we

decrease the learning rate the error converges to 0.

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0
0

r
o
r
r
e

n
o
i
t
c
u
r
t
s
n
o
c
e
r

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0
0

r
o
r
r
e

n
o
i
t
c
u
r
t
s
n
o
c
e
r

2000

4000

6000

8000

10000

iteration

2000

4000

6000

8000

10000

iteration

(a) new objective (2.11)

(b) reconstruction error objective (2.10)

figure 2.1: comparison of di   erent objective functions

41

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

r
o
r
r
e

n
o
i
t
c
u
r
t
s
n
o
c
e
r

0
0

200

400
iteration

600

800

1000

101

100

10   1

10   2

r
o
r
r
e

n
o
i
t
c
u
r
t
s
n
o
c
e
r

10   3

100

101

102

iteration

103

104

(a) constant learning rate   

(b) learning rate   /t (in log scale)

figure 2.2: comparison of di   erent objective functions

2.5 conclusion

in this chapter we identify the strict saddle property and show stochastic id119

converges to a local minimum under this assumption. this leads to new online algorithm

for orthogonal tensor decomposition. we hope this is a    rst step towards understanding

stochastic gradient for more classes of non-convex functions. we believe strict saddle prop-

erty can be extended to handle more functions, especially those functions that have similar

symmetry properties.

42

chapter 3

applying online tensor methods for

learning latent variable models

in chapter 2, we have established a guaranteed online stochastic id119 algorithm

for tensor decomposition. theoretically, it is solid and well justi   ed. we will now    ll in the

gap of theoretical    ndings and practical applications by applying the algorithm to real world

problems.

we consider two problems: (1) community detection (wherein we compute the decomposition

of a tensor which relates to the count of 3-stars in a graph) and (2) id96 (wherein we

consider the tensor related to co-occurrence of triplets of words in documents); decomposition

of the these tensors allows us to learn the hidden communities and topics from observed data.

community detection: we recover hidden communities in several real datasets with high

accuracy. when ground-truth communities are available, we propose a new error score based

on the hypothesis testing methodology involving p-values and false discovery rates [153] to

validate our results. the use of p-values eliminates the need to carefully tune the number of

communities output by our algorithm, and hence, we obtain a    exible trade-o    between the

43

fraction of communities recovered and their estimation accuracy. we    nd that our method

has very good accuracy on a range of network datasets: facebook, yelp and dblp. we

summarize the datasets used in this chapter in table 3.5. to get an idea of our running

times, let us consider the larger dblp collaborative data set for a moment. it consists of 16

million edges, one million nodes and 250 communities. we obtain an error of 10% and the

method runs in about two minutes, excluding the 80 minutes taken to read the edge data

from    les stored on the hard disk and converting it to sparse matrix format.

compared to the state-of-the-art method for learning mmsb models using the stochastic

variational id136 algorithm of [70], we obtain several orders of magnitude speed-up in

the running time on multiple real datasets. this is because our method consists of e   cient

matrix operations which are embarrassingly parallel. matrix operations are carried out in the

sparse format which is e   cient especially for social network settings involving large sparse

graphs. moreover, our code is    exible to run on a range of graphs such as directed, undi-

rected and bipartite graphs, while the code of [70] is designed for homophilic networks, and

cannot handle bipartite graphs in its present format. note that bipartite networks occur in

the recommendation setting such as the yelp data set. additionally, the variational imple-

mentation in [70] assumes a homogeneous connectivity model, where any pair of communities

connect with the same id203 and the id203 of intra-community connectivity is

also    xed. our framework does not su   er from this restriction. we also provide arguments

to show that the normalized mutual information (nmi) and other scores, previously used

for evaluating the recovery of overlapping community, can underestimate the errors.

id96: we also employ the tensor method for topic-modeling, and there are many

similarities between the topic and community settings. for instance, each document has

multiple topics, while in the network setting, each node has membership in multiple commu-

nities. the words in a document are generated based on the latent topics in the document,

and similarly, edges are generated based on the community memberships of the node pairs.

44

the tensor method is even faster for id96, since the word vocabulary size is typi-

cally much smaller than the size of real-world networks. we learn interesting hidden topics

in new york times corpus from uci bag-of-words data set1 with around 100, 000 words and

300, 000 documents in about two minutes. we present the important words for recovered

topics, as well as interpret    bridging    words, which occur in many topics.

implementations: we present two implementations, viz., a gpu-based implementation which

exploits the parallelism of simd architectures and a cpu-based implementation for larger

datasets, where the gpu memory does not su   ce. we discuss various aspects involved such

as implicit manipulation of tensors since explicitly forming tensors would be unwieldy for

large networks, optimizing for communication bottlenecks in a parallel deployment, the need

for sparse matrix and vector operations since real world networks tend to be sparse, and a

careful statistical approach to validating the results, when ground truth is available.

3.1 tensor forms for topic and community models

in this section, we brie   y recap the topic and community models, as well as the tensor forms

for their exact moments, derived in [10, 8].

3.1.1 id96

in id96, a document is viewed as a bag of words. each document has a latent set

of topics, and h = (h1, h2, . . . , hk) represents the proportions of k topics in a given document.

given the topics h, the words are independently drawn and are exchangeable, and hence,

the term    bag of words    model. we represent the words in the document by d-dimensional
random vectors x1, x2, . . . xl     rd, where xi are coordinate basis vectors in rd and d is the

1https://archive.ics.uci.edu/ml/datasets/bag+of+words

45

size of the word vocabulary. conditioned on h, the words in a document satisfy e[xi|h] =   h,
where    := [  1, . . . ,   k] is the topic-word matrix. and thus   j is the topic vector satisfying

  j = pr (xi|hj),    j     [k]. under the id44 (lda) topic model [31], h
is drawn from a dirichlet distribution with concentration parameter vector    = [  1, . . . ,   k].
iid    dir(  ),    u     [n] with parameter vector        rk
+.

in other words, for each document u, hu

we de   ne the dirichlet concentration (mixing) parameter

  0 :=xi   [k]

  i.

the dirichlet distribution allows us to specify the extent of overlap among the topics by

controlling for sparsity in topic density function. a larger   0 results in more overlapped

(mixed) topics. a special case of   0 = 0 is the single topic model.

due to exchangeability, the order of the words does not matter, and it su   ces to consider

the frequency vector for each document, which counts the number of occurrences of each
word in a document. let ct := (c1,t, c2,t, . . . , cd,t)     rd denote the frequency vector for tth
document, and let n be the number of documents.

46

we consider the    rst three order empirical moments, given by

ct

nxt=1

  0 + 1

n

m top

1

:=

1
n

m top

2

:=

m top

3

:=

nxt=1

2n

(  0 + 1)(  0 + 2)

(ct     ct     diag (ct))       0m top
1     m top
dxj=1
dxi=1

nxt=1

1

      ct     ct     ct    
dxi=1
dxj=1
ci,t(ei     ei     m top

1

nxt=1" dxi=1
1     ei     ei)# +   2

) +

dxi=1
1     m top
1     m top

1

0m top

ci,tcj,t(ei     ei     ej)

dxi=1
ci,t(ei     m top

1     ei)

ci,tcj,t(ei     ej     ei)    

ci,tcj,t(ei     ej     ej) + 2

   

   

+

dxi=1

dxj=1

  0(  0 + 1)

2n

ci,t(m top

dxi=1

(3.1)

(3.2)

ci,t(ei     ei     ei)      

we recall theorem 3.5 of [10]:

lemma 3.1. the exact moments can be factorized as

e[m top

1

] =

e[m top

2

] =

e[m top

3

] =

  i
  0

  i
  0

  i
  0

kxi=1
kxi=1
kxi=1

  i

  i       i

  i       i       i.

.

(3.3)

(3.4)

(3.5)

(3.6)

where    = [  1, . . . ,   k] and   i = pr (xt|h = i),    t     [l]. in other words,    is the topic-word
matrix.

from the lemma 3.1, we observe that the    rst three moments of a lda topic model have

a simple form involving the topic-word matrix    and dirichlet parameters   i. in [10], it is

shown that these parameters can be recovered under a weak non-degeneracy assumption.

we will employ tensor decomposition techniques to learn the parameters.

47

3.1.2 mixed membership model

in the mixed membership stochastic block model (mmsb), introduced by [5], the edges

in a social network are related to the hidden communities of the nodes. a batch tensor

decomposition technique for learning mmsb was derived in [8].

let n denote the number of nodes, k the number of communities and g     rn  n the adjacency
matrix of the graph. each node i     [n] has an associated community membership vector
  i     rk, which is a latent variable, and the vectors are contained in a simplex, i.e.,

xi   [k]

  u(i) = 1,    u     [n]

where the notation [n] denotes the set {1, . . . , n}. membership vectors are sampled from
the dirichlet distribution   u
+ where

iid    dir(  ),    u     [n] with parameter vector        rk

  0 := pi   [k]   i. as in the id96 setting, the dirichlet distribution allows us to

specify the extent of overlap among the communities by controlling for sparsity in community

membership vectors. a larger   0 results in more overlapped (mixed) memberships. a special

case of   0 = 0 is the stochastic block model [8].

the community connectivity matrix is denoted by p     [0, 1]k  k where p (a, b) measures the
connectivity between communities a and b,    a, b     [k]. we model the adjacency matrix
entries as either of the two settings given below:

bernoulli model: this models a network with unweighted edges. it is used for facebook and

dblp datasets in section 3.5 in our experiments.

gij

iid    ber(     i p   j),    i, j     [n].

48

poisson model [100]: this models a network with weighted edges. it is used for the yelp

data set in section 3.5 to incorporate the review ratings.

gij

iid    poi(     i p   j),    i, j     [n].

the tensor decomposition approach involves up to third order moments, computed from

the observed network. in order to compute the moments, we partition the nodes randomly

into sets x, a, b, c. let fa :=      ap    , fb :=      bp    , fc :=      cp     (where p is the

community connectivity matrix and    is the membership matrix) and      := (cid:16)   1

denote the normalized dirichlet concentration parameter. we de   ne pairs over y1 and y2 as
pairs(y1, y2) := g   x,y1     g   x,y2. de   ne the following matrices

  0(cid:17)

, . . . ,   k

  0

zb := pairs (a, c) (pairs (b, c))    ,

zc := pairs (a, b) (pairs (c, b))    .

we consider the    rst three empirical moments, given by

g   x,a

1

nxxx   x
nx xx   x

  0 + 1

m1

com :=

m2

com :=

m3

com :=

zcg   x,cgx,bz   b       0m1

comm1

com   

(  0 + 1)(  0 + 2)

2nx

g   x,a     zbg   x,b     zcg   x,c

xx   x
com     m1

com

+   2

com     m1

0m1
  0(  0 + 1)

   

+m1

2nx xx   x(cid:0)g   x,a     zbg   x,b     m1
com     zbg   x,b     zcg   x,c(cid:1)

com + g   x,a     m1

com     zcg   x,c

49

(3.7)

(3.8)

(3.9)

(3.10)

(3.11)

we now recap proposition 2.2 of [9] which provides the form of these moments under expec-

tation.

lemma 3.2. the exact moments can be factorized as

e[m1

e[m2

e[m3

com|  a,   b,   c] :=xi   [k]
com|  a,   b,   c] :=xi   [k]
com|  a,   b,   c] :=xi   [k]

    i(fa)i

    i(fa)i     (fa)i

    i(fa)i     (fa)i     (fa)i

(3.12)

(3.13)

(3.14)

where     denotes the kronecker product and (fa)i corresponds to the ith column of fa.

we observe that the moment forms above for the mmsb model have a similar form as

the moments of the topic model in the previous section. thus, we can employ a uni   ed

framework for both topic and community modeling involving decomposition of the third

order moment tensors m top

3

and m com

3

. second order moments m top

2

and m com

2

are used

for preprocessing of the data (i.e., whitening, which is introduced in detail in section 3.2.1).

for the sake of the simplicity of the notation, in the rest of the chapter, we will use m2 to

denote empirical second order moments for both m top

2

in id96 setting, and m com

2

in the mixed membership model setting. similarly, we will use m3 to denote empirical third

order moments for both m top

3

and m com

3

.

3.2 learning using third order moment

our learning algorithm uses up to the third-order moment to estimate the topic word matrix

   or the community membership matrix   . first, we obtain co-occurrence of triplet words

or subgraph counts (implicitly). then, we perform preprocessing using second order moment

50

m2. then we perform tensor decomposition e   ciently using stochastic id119 [111]

on m3. we note that, in our implementation of the algorithm on the graphics processing

unit (gpu), id202ic operations are extremely fast. we also implement our algorithm

on the cpu for large datasets which exceed the memory capacity of gpu and use sparse

matrix operations which results in large gains in terms of both the memory and the running

time requirements. the overall approach is summarized in algorithm 3.

procedure 3 overall approach for learning latent variable models via a moment-based
approach.
input: observed data: social network graph or document samples.
output: learned latent variable model and infer hidden attributes.
1: estimate the third order moments tensor m3 (implicitly). the tensor is not formed

explicitly as we break down the tensor operations into vector and matrix operations.

2: whiten the data, via svd of m2, to reduce dimensionality via symmetrization and

orthogonalization. the third order moments m3 are whitened as t .

3: use stochastic id119 to estimate spectrum of whitened (implicit) tensor t .
4: apply post-processing to obtain the topic-word matrix or the community memberships.

5: if ground truth is known, validate the results using various evaluation measures.

3.2.1 id84 and whitening

whitening step utilizes id202ic manipulations to make the tensor symmetric and

orthogonal (in expectation). moreover, it leads to id84 since it (im-

plicitly) reduces tensor m3 of size o(n3) to a tensor of size k3, where k is the number of

communities. typically we have k     n. the whitening step also converts the tensor m3 to
a symmetric orthogonal tensor. the whitening matrix w     rna  k satis   es w    m2w = i.
the idea is that if the bilinear projection of the second order moment onto w results in

the identity matrix, then a trilinear projection of the third order moment onto w would

result in an orthogonal tensor. we use multilinear operations to get an orthogonal tensor

t := m3(w, w, w ).

51

the whitening matrix w is computed via truncated k   svd of the second order moments.

w = um2     1/2
m2

,

where um2 and   m2 = diag(  m2,1, . . . ,   m2,k) are the top k singular vectors and singular

values of m2 respectively. we then perform multilinear transformations on the triplet data

using the whitening matrix. the whitened data is thus

yt

yt

yt

a :=(cid:10)w, ct(cid:11) ,
b :=(cid:10)w, ct(cid:11) ,
c :=(cid:10)w, ct(cid:11) ,

for the id96, where t denotes the index of the documents. note that yt
c     rk. implicitly, the whitened tensor is t = 1
yt
dimension tensor. since k     n, the id84 is crucial for our speedup.

b and
c and is a k    k    k

nx pt   x

a     yt
yt

b     yt

a, yt

3.2.2 stochastic tensor id119

in [8] and [10], the power method with de   ation is used for tensor decomposition where the

eigenvectors are recovered by iterating over multiple loops in a serial manner. furthermore,

batch data is used in their iterative power method which makes that algorithm slower than

its stochastic counterpart. in addition to implementing a stochastic spectral optimization

algorithm, we achieve further speed-up by e   ciently parallelizing the stochastic updates.

let v = [v1|v2| . . .|vk] be the true eigenvectors. denote the cardinality of the sample set
as nx, i.e., nx := |x|. now that we have the whitened tensor, we propose the stochastic
tensor id119 (stgd) algorithm for tensor decomposition. consider the tensor

52

t     rk  k  k using whitened samples, i.e.,

t =

   

1

t t =
  0(  0 + 1)

nxxt   x
2nx xt   x(cid:2)yt

(  0 + 1)(  0 + 2)

xt   x
b       yc + yt

2nx

a     yt

a     yt
yt

b     yt

c

a       yb     yt

c +   ya     yt

b     yt

0   ya       yb       yc,

c(cid:3) +   2

where t     x and denotes the index of the online data and   ya,   yb, and   yc denote the mean
of the whitened data. our goal is to    nd a symmetric cp decomposition of the whitened

tensor, and this will be extensively discussed in the next chapter.

after learning the decomposition of the third order moment, we perform post-processing to

estimate b  .

3.2.3 post-processing

eigenvalues    := [  1,   2, . . . ,   k] are estimated as the norm of the eigenvectors   i = k  ik3.

lemma 3.3. after we obtain    and   , the estimate for the topic-word matrix is given by

     = w         ,

and in the community setting, the community membership matrix is given by

    ac = diag(  )1/3 diag(  )   1        w    ga,ac.

where ac := x     b     c. similarly, we estimate     a by exchanging the roles of x and a.
next, we obtain the dirichlet distribution parameters

    i =   2     2

i

,   i     [k].

53

where   2 is chosen such that we have id172pi   [k]     i :=pi   [k]

  i
  0

= 1.

thus, we perform stgd method to estimate the eigenvectors and eigenvalues of the whitened

tensor, and then use these to estimate the topic word matrix    and community membership

matrix b   by thresholding.

3.3

implementation details

3.3.1 symmetrization step to compute m2

note that for the topic model, the second order moment m2 can be computed easily from

the word-frequency vector. on the other hand, for the community setting, computing m2

requires additional id202ic operations. it requires computation of matrices zb and

zc in equation (3.7). this requires computation of pseudo-inverses of    pairs    matrices.

now, note that pseudo-inverse of (pairs (b, c)) in equation (3.7) can be computed using

rank k-svd:

k-svd (pairs (b, c)) = ub(:, 1 : k)  bc (1 : k)vc(:, 1 : k)   .

we exploit the low rank property to have e   cient running times and storage. we    rst

implement the k-svd of pairs, given by g   x,cgx,b. then the order in which the matrix

products are carried out plays a signi   cant role in terms of both memory and speed. note

that zc involves the multiplication of a sequence of matrices of sizes rna  nb , rnb  k, rk  k,

rk  nc , g   x,cgx,b involves products of sizes rnc  k, rk  k, rk  nb, and zb involving products

of sizes rna  nc , rnc  k, rk  k, rk  nb. while performing these products, we avoid products

of sizes ro(n)  o(n) and ro(n)  o(n). this allows us to have e   cient storage requirements.

such manipulations are represented in figure 3.1.

54

|a|

=

|a|

=

=

   

   

   

      

   

   

   

   

   

figure 3.1: by performing the id127s in an e   cient order (equation (3.10)),
we avoid products involving o(n)    o(n) objects.
instead, we use objects of size
o(n)    k which improves the speed, since k     n.
equation (3.10) is equivalent
to m2 = (cid:16)pairsa,b pairs   c,b(cid:17) pairsc,b (cid:16)pairs   b,c(cid:17)   
pairs   a,c    shift, where the shift =
  0+1(cid:0)m1m1        diag(cid:0)m1m1   (cid:1)(cid:1). we do not explicitly calculate the pseudoinverse but main-

tain the low rank matrix decomposition form.

  0

we then orthogonalize the third order moments to reduce the dimension of its modes to k.

we perform linear transformations on the data corresponding to the partitions a, b and c

using the whitening matrix. the whitened data is thus yt

b :=(cid:10)w, zbg   t,b(cid:11),
c :=(cid:10)w, zcg   t,c(cid:11), where t     x and denotes the index of the online data. since k     n,

a :=(cid:10)w, g   t,a(cid:11), yt

the id84 is crucial for our speedup.

and yt

3.3.2 e   cient randomized svd computations

when we consider very large-scale data, the whitening matrix is a bottleneck to handle

when we aim for fast running times. we obtain the low rank approximation of matrices

using random projections. in the cpu implementation, we use tall-thin svd (on a sparse

matrix) via the lanczos algorithm after the projection and in the gpu implementation,

55

we use tall-thin qr. we give the overview of these methods below. again, we use graph

community membership model without loss of generality.

r

randomized low rank approximation: from [66], for the k-rank positive semi-de   nite matrix
m2     rna  na with na     k, we can perform random projection to reduce dimensionality.
more precisely, if we have a random matrix s     rna    k with unit norm (rotation matrix),
we project m2 onto this random matrix to get rn    k tall-thin matrix. note that we choose
  k = 2k in our implementation. we will obtain lower dimension approximation of m2 in
  k    k. here we emphasize that s     rn    k is a random matrix for dense m2. however for
sparse m2, s     {0, 1}n    k is a column selection matrix with random sign for each entry.
after the projection, one approach we use is svd on this tall-thin (rn    k) matrix. de   ne
o := m2s     rn    k and     := s   m2s     r  k    k. a low rank approximation of m2 is given by
o      o    [66]. recall that the de   nition of a whitening matrix w is that w    m2w = i. we
can obtain the whitening matrix of m2 without directly doing a svd on m2     rna  na.

tall-thin svd: this is used in the cpu implementation. the whitening matrix can be

obtained by

w     (o   )   (   

1

2 )   .

(3.15)

the pseudo code for computing the whitening matrix w using tall-thin svd is given in

algorithm 4. therefore, we only need to compute svd of a tall-thin matrix o     rna    k.
note that         r  k    k, its square-root is easy to compute. similarly, pseudoinverses can also
be obtained without directly doing svd. for instance, the pseudoinverse of the pairs (b, c)

matrix is given by

(pairs (b, c))    = (j   )     j   ,

56

procedure 4 randomized tall-thin svd
input: second moment matrix m2.
output: whitening matrix w .
1: generate random matrix s     rn    k if m2 is dense.
2: generate column selection matrix with random sign s     {0, 1}n    k if m2 is sparse.
3: o = m2s     rn    k
4: [uo, lo, vo] =svd(o)
  k    k
5:     = s   o     r
6: [u   , l   , v   ] =svd(   )
7: w = uol   1

o v    o v   l

   u      

1
2

where    = s    (pairs (b, c)) s and j = (pairs (b, c)) s. the pseudo code for computing

pseudoinverses is given in algorithm 5.

procedure 5 randomized pseudoinverse
input: pairs matrix pairs (b, c).
output: pseudoinverse of the pairs matrix (pairs (b, c))   .
1: generate random matrix s     rn,k if m2 is dense.
2: generate column selection matrix with random sign s     {0, 1}n  k if m2 is sparse.
3: j = (pairs (b, c)) s
4:    = s   j
5: [uj , lj , vj ] =svd(j)
6: (pairs (b, c))    = uj l   1

j v    j   vj l   1

j u   j

the sparse representation of the data allows for scalability on a single machine to datasets

having millions of nodes. although the gpu has simd architecture which makes paralleliza-

tion e   cient, it lacks advanced libraries with sparse svd operations and out-of-gpu-core

implementations. we therefore implement the sparse format on cpu for sparse datasets. we

implement our algorithm using random projection for e   cient id84 [45]

along with the sparse matrix operations available in the eigen toolkit2, and we use the

svdlibc [30] library to compute sparse svd via the lanczos algorithm. theoretically, the

lanczos algorithm [69] on a n   n matrix takes around (2d + 8)n    ops for a single step where
d is the average number of non-zero entries per row.

2

http://eigen.tuxfamily.org/index.php?title=main_page

57

cpu

b,yt
c

yt
a,yt
vt
i

cpu

yt
a,yt

b,yt
c

gpu

standard interface

gpu

device interface

vt
i

vt
i

figure 3.2: data transfers in the standard and device interfaces of the gpu implementation.

tall-thin qr: this is used in the gpu implementation due to the lack of library to do sparse

tall-thin svd. the di   erence is that we instead implement a tall-thin qr on o, therefore

the whitening matrix is obtained as

w     q(r   )   (   

1

2 )   .

the main bottleneck for our gpu implementation is device storage, since gpu memory is

highly limited and not expandable. random projections help in reducing the dimensionality

from o(n   n) to o(n    k) and hence, this    ts the data in the gpu memory better. conse-
quently, after the whitening step, we project the data into k-dimensional space. therefore,

the stgd step is dependent only on k, and hence can be    t in the gpu memory. so, the

main bottleneck is computation of large svds. in order to support larger datasets such as

the dblp data set which exceed the gpu memory capacity, we extend our implementation

with out-of-gpu-core matrix operations and the nystrom method [66] for the whitening

matrix computation and the pseudoinverse computation in the pre-processing module.

58

3.3.3 stochastic updates

stgd can potentially be the most computationally intensive task if carried out naively

since the storage and manipulation of a o(n3)-sized tensor makes the method not scalable.

however we overcome this problem since we never form the tensor explicitly; instead, we col-

lapse the tensor modes implicitly. we gain large speed up by optimizing the implementation

of stgd.to implement the tensor operations e   ciently we convert them into matrix and

vector operations so that they are implemented using blas routines. we obtain whitened

vectors ya, yb and yc and manipulate these vectors e   ciently to obtain tensor eigenvector

updates using the gradient scaled by a suitable learning rate.

e   cient stgd via stacked vector operations: we convert the blas ii into blas iii

operations by stacking the vectors to form matrices, leading to more e   cient operations.

although the updating equation for the stochastic gradient update is presented serially, we

can update the k eigenvectors simultaneously in parallel. the basic idea is to stack the k
eigenvectors   i     rk into a matrix   , then using the internal parallelism designed for blas
iii operations.

overall, the stgd step involves 1 + k + i(2 + 3k) blas ii over rk vectors, 7n blas iii

over rk  k matrices and 2 qr operations over rk  k matrices, where i denotes the number of

iterations. we provide a count of blas operations for various steps in table 3.1.

module blas i blas ii blas iii
pre
stgd
post

8
n k
0

19
7n
7

0
0
0

svd qr

3
0
0

0
2
0

table 3.1: id202ic operation counts: n denotes the number of iterations for stgd
and k, the number of communities.

reducing communication in gpu implementation:

in stgd, note that the storage needed

for the iterative part does not depend on the number of nodes in the data set, rather,

59

104

103

102

)
s
c
e
s
(
e
m

101

i
t

i

g
n
n
n
u
r

100

10   1

 

102

 

matlab tensor toolbox
cula standard interface
cula device interface
eigen sparse

number of communities k

103

figure 3.3: comparison of the running time for stgd under di   erent k for 100 iterations.

it depends on the parameter k, i.e., the number of communities to be estimated, since

whitening performed before stgd leads to id84. this makes it suitable

for storing the required bu   ers in the gpu memory, and using the cula device interface

for the blas operations. in figure 3.2, we illustrate the data transfer involved in the gpu

standard and device interface codes. while the standard interface involves data transfer

(including whitened neighborhood vectors and the eigenvectors) at each stochastic iteration

between the cpu memory and the gpu memory, the device interface involves allocating and

retaining the eigenvectors at each stochastic iteration which in turn speeds up the spectral

estimation.

60

we compare the running time of the cula device code with the matlab code (using the

tensor toolbox [23]), cula standard code and eigen sparse code in figure 3.3. as expected,

the gpu implementations of matrix operations are much faster and scale much better than

the cpu implementations. among the cpu codes, we notice that sparsity and optimization

o   ered by the eigen toolkit gives us huge gains. we obtain orders of magnitude of speed up

for the gpu device code as we place the bu   ers in the gpu memory and transfer minimal

amount of data involving the whitened vectors only once at the beginning of each iteration.

the running time for the cula standard code is more than the device code because of the

cpu-gpu data transfer overhead. for the same reason, the sparse cpu implementation, by

avoiding the data transfer overhead, performs better than the gpu standard code for very

small number of communities. we note that there is no performance degradation due to the

parallelization of the matrix operations. after whitening, the stgd requires the most code

design and optimization e   ort, and so we convert that into blas-like routines.

3.3.4 computational complexity

time

module
preprocessing (matrix multiply) o (max(nsk/c, log s))
preprocessing (cpu svd)
preprocessing (gpu qr)
preprocessing(short-thin svd)
stgd
post-processing

space
o (max(s2, sk))
o (max(nsk/c, log s) + max(k2/c, k))
o(sk)
o (max(sk2/c, log s) + max(sk2/c, log k)) o(sk)
o(k2)
o (max(k3/c, log k) + max(k2/c, k))
o (max(k3/c, log k))
o(k2)
o(nk)
o (max(nsk/c, log s))

table 3.2: the time and space complexity (number of compute cores required) of our al-
gorithm. note that k     n, s is the average degree of a node (or equivalently, the average
number of non-zeros per row/column in the adjacency sub-matrix); note that the stgd
time is per iteration time. we denote the number of cores as c - the time-space trade-o   
depends on this parameter.

we partition the execution of our algorithm into three main modules namely, pre-processing,

stgd and post-processing, whose various matrix operation counts are listed above in ta-

ble 3.1.

61

the theoretical asymptotic complexity of our method is summarized in table 3.2 and is

best addressed by considering the parallel model of computation [94], i.e., wherein a number

of processors or compute cores are operating on the data simultaneously in parallel. this

is justi   ed considering that we implement our method on gpus and matrix products are

embarrassingly parallel. note that this is di   erent from serial computational complexity.

we now break down the entries in table 3.2. first, we recall a basic lemma regarding the

lower bound on the time complexity for parallel addition along with the required number of

cores to achieve a speed-up.

lemma 3.4.

[94] addition of s numbers in serial takes o(s) time; with    (s/ log s) cores,

this can be improved to o(log s) time in the best case.

essentially, this speed-up is achieved by recursively adding pairs of numbers in parallel.

lemma 3.5.

[94] consider m     rp  q and n     rq  r with s non-zeros per row/column.
naive serial id127 requires o(psr) time; with    (psr/ log s) cores, this can be

improved to o(log s) time in the best case.

lemma 3.5 follows by simply parallelizing the sparse inner products and applying lemma 3.4

for the addition in the inner products. note that, this can be generalized to the fact that

given c cores, the multiplication can be performed in o(max(psr/c, log s)) running time.

pre-processing

random projection: in preprocessing, given c compute cores, we    rst do random projection

using id127. we multiply an o(n)    o(n) matrix m2 with an o(n)    o(k)
random matrix s. therefore, this requires o(nsk) serial operations, where s is the number of

non-zero elements per row/column of m2. using lemma 3.5, given c = nsk

log s cores, we could

62

achieve o(log s) computational complexity. however, the parallel computational complexity

is not further reduced with more than nsk

log s cores.

after the multiplication, we use tall-thin svd for cpu implementation, and tall-thin qr

for gpu implementation.

tall-thin svd: we perform lanczos svd on the tall-thin sparse o(n)   o(k) matrix, which
involves a tri-diagonalization followed with the qr on the tri-diagonal matrix. given c = nsk
log s

cores, the computational complexity of the tri-diagonalization is o(log s). we then do qr on

the tridiagonal matrix which is as cheap as o(k2) serially. each orthogonalization requires

o(k) inner products of constant entry vectors, and there are o(k) such orthogonalizations

to be done. therefore given o(k) cores, the complexity is o(k). more cores does not help

since the degree of parallelism is k.

tall-thin qr: alternatively, we perform qr in the gpu implementation which takes o(sk2).

to arrive at the complexity of obtaining q, we analyze the gram-schmidt orthoid172

procedure under sparsity and parallelism conditions. consider a serial gram-schmidt on k

columns (which are s-dense) of o(n)    o(k) matrix. for each of the columns 2 to k, we
perform projection on the previously computed components and subtract it. both inner

product and subtraction operations are on the s-dense columns and there are o(s) operations

which are done o(k2) times serially. the last step is the id172 of k s-dense vectors

with is an o(sk) operation. this leads to a serial complexity of o(sk2 + sk) = o(sk2). using

this, we may obtain the parallel complexity in di   erent regimes of the number of cores as

follows.

parallelism for inner products : for each component i, we need i     1 projections on previ-
ous components which can be parallel. each projection involves scaling and inner product

operations on a pair of s-dense vectors. using lemma 3.4, projection for component i can

63

be performed in o(max( sk

c , log s)) time. o(log s) complexity is obtained using o(sk/ log s)

cores.

parallelism for subtractions: for each component i, we need i     1 subtractions on a s-
dense vector after the projection. serially the subtraction requires o(sk) operations, and

this can be reduced to o(log k) with o(sk/ log k) cores in the best case. the complexity is

o(max( sk

c , log k)).

+ max( sk

combing the inner products and subtractions, the complexity is o(cid:0)max( sk
parallel. in total, the complexity for the parallel qr is o(cid:16)max( sk2

c , log k)(cid:1) for component i. there are k components in total, which can not be
c , log k)(cid:17).

c , log s) + max( sk2

c , log s)

short-thin svd: svd of the smaller o(rk  k) matrix time requires o(k3) computations

in serially. we note that this is the bottleneck for the computational complexity, but we

emphasize that k is su   ciently small in many applications. furthermore, this k3 complexity

can be reduced by using distributed svd algorithms e.g. [99, 62]. an analysis with respect

to lanczos parallel svd is similar with the discussion in the tall-thin svd paragraph. the

complexity is o(max(k3/c, log k)+max(k2/c, k)). in the best case, the complexity is reduced

to o(log k + k).

the serial time complexity of svd is o(n2k) but with randomized dimensionality reduc-

tion [66] and parallelization [51], this is signi   cantly reduced.

stgd

in stgd, we perform implicit stochastic updates, consisting of a constant number of matrix-

matrix and matrix-vector products, on the set of eigenvectors and whitened samples which
is of size k    k. when c     [1, k3/ log k], we obtain a running time of o(k3/c) for computing
inner products in parallel with c compute cores since each core can perform an inner product

64

to compute an element in the resulting matrix independent of other cores in linear time. for
c     (k3/ log k,   ], using lemma 3.4, we obtain a running time of o(log k). note that the
stgd time complexity is calculated per iteration.

post-processing

finally, post-processing consists of sparse matrix products as well. similar to pre-processing,

this consists of multiplications involving the sparse matrices. given s number of non-zeros

per column of an o(n)    o(k) matrix, the e   ective number of elements reduces to o(sk).
hence, given c     [1, nks/ log s] cores, we need o(nsk/c) time to perform the inner products
for each entry of the resultant matrix. for c     (nks/ log s,   ], using lemma 3.4, we obtain
a running time of o(log s).

note that nk2 is the complexity of computing the exact svd and we reduce it to o(k) when

there are su   cient cores available. this is meant for the setting where k is small. this
k3 complexity of svd on o(k    k) matrix can be reduced to o(k) using distributed svd
algorithms e.g. [99, 62]. we note that the variational id136 algorithm complexity, by

gopalan and blei [71], is o(mk) for each iteration, where m denotes the number of edges
in the graph, and n < m < n2. in the regime that n     k, our algorithm is more e   cient.
moreover, a big di   erence is in the scaling with respect to the size of the network and ease

of parallelization of our method compared to variational one.

65

figure 3.4: bipartite graph g{pval} induced by p-value testing. edges represent statistically
signi   cant relationships between ground truth and estimated communities.

3.4 validation methods

3.4.1 p -value testing

of communities speci   ed to our method. recall that the true community membership matrix

we recover the estimated community membership matrix b       rbk  n, wherebk is the number
is   , and we consider datasets where ground truth is available. let i-th row of b   be denoted
by b  i. our community detection method is unsupervised, which inevitably results in row
permutations between    and b   andbk may not be the same as k. to validate the results, we
need to    nd a good match between the rows of b   and   . we use the notion of p-values to

test for statistically signi   cant dependencies among a set of random variables. the p-value

denotes the id203 of not rejecting the null hypothesis that the random variables under

consideration are independent and we use the student   s3 t-test statistic [60] to compute the

p-value. we use multiple hypothesis testing for di   erent pairs of estimated and ground-

3note that student   s t-test is robust to the presence of unequal variances when the sample sizes of the

two are equal which is true in our setting.

66

truth communities b  i,   j and adjust the p-values to ensure a small enough false discovery

rate (fdr) [153].

the test statistic used for the p-value testing of the estimated communities is

tij :=

  (cid:16)b  i,   j(cid:17)   n     2
r1       (cid:16)b  i,   j(cid:17)2

.

the right p-value is obtained via the id203 of obtaining a value (say tij) greater than

the test statistic tij, and it is de   ned as

pval(  i,b  j) := 1     p (tij > tij) .

note that tij has student   s t-distribution with degree of freedom n     2 (i.e. tij     tn   2).
thus, we obtain the right p-value4.

in this way, we compute the pval matrix as

pval(i, j) := pvalhb  i,   ji ,   i     [k] and j     [bk].

3.4.2 id74

recovery ratio: validating the results requires a matching of the true membership    with

estimated membership b  . let pval(  i,b  j) denote the right p-value under the null hypothesis
that   i andb  j are statistically independent. we use the p-value test to    nd out pairs   i,b  j

which pass a speci   ed p-value threshold, and we denote such pairs using a bipartite graph

4the right p-value accounts for the fact that when two communities are anti-correlated they are not
paired up. hence note that in the special case of block model in which the estimated communities are just
permuted version of the ground truth communities, the pairing results in a perfect matching accurately.

67

g{pval}. thus, g{pval} is de   ned as

g{pval} :=(cid:16)nv (1)

{pval}

, v (2)

{pval}o , e{pval}(cid:17) ,

where the nodes in the two node sets are

v (1)
{pval}
v (2)
{pval}

= {  1, . . . ,   k} ,

=nb  1, . . . ,b  bko

and the edges of g{pval} satisfy

(i, j)     e{pval} s.t. pvalhb  i,   ji     0.01.

a simple example is shown in figure 3.4, in which   2 has statistically signi   cant dependence

with b  1, i.e., the id203 of not rejecting the null hypothesis is small (recall that null

hypothesis is that they are independent). if no estimated membership vector has a signi   cant

overlap with   3, then   3 is not recovered. there can also be multiple pairings such as for   1

of not rejecting the null hypothesis is small, i.e., they are independent. we use 0.01 as the

and {b  2,b  3,b  6}. the p-value test between   1 and {b  2,b  3,b  6} indicates that id203
threshold. the same holds for   2 and {b  1} and for   4 and {b  4,b  5}. there can be a perfect
one to one matching like for   2 and b  1 as well as a multiple matching such as for   1 and
{b  2,b  3,b  6}. or another multiple matching such as for {  1,   2} and b  3.

let degreei denote the degree of ground truth community i     [k] in g{pval}, we de   ne the
recovery ratio as follows.

de   nition 3.1. the recovery ratio is de   ned as

r :=

1

kxi

i {degreei > 0} ,

i     [k]

68

where i(x) is the indicator function whose value equals one if x is true.

the perfect case is that all the memberships have at least one signi   cant overlapping es-

timated membership, giving a recovery ratio of 100%. error function: for performance

analysis of our learning algorithm, we use an error function given as follows:

de   nition 3.2. the average error function is de   ned as

e :=

1

k x(i,j)   e{p

val}

         

1

n xx   |x|(cid:12)(cid:12)(cid:12)(cid:12)b  i(x)       j(x)(cid:12)(cid:12)(cid:12)(cid:12)

,

         

where e{pval} denotes the set of edges based on thresholding of the p-values.

the error function incorporates two aspects, namely the l1 norm error between each estimated

community and the corresponding paired ground truth community, and the error induced by

false pairings between the estimated and ground-truth communities through p-value testing.

for the former l1 norm error, we normalize with n which is reasonable and results in the range

of the error in [0, 1]. for the latter, we de   ne the average error function as the summation of

all paired memberships errors divided by the true number of communities k. in this way we

penalize falsely discovered pairings by summing them up. our error function can be greater

than 1 if there are too many falsely discovered pairings through p-value testing (which can

be as large as k   bk).

bridgeness: bridgeness in overlapping communities is an interesting measure to evaluate.

a bridge is de   ned as a vertex that crosses structural holes between discrete groups of

people and bridgeness analyzes the extent to which a given vertex is shared among di   erent

69

communities [129]. formally, the bridgeness of a vertex i is de   ned as

bi := 1    vuut bk
bk     1

bkxj=1(cid:18)b  i(j)    

1

bk(cid:19)2

.

(3.16)

note that centrality measures should be used in conjunction with bridge score to distinguish

outliers from genuine bridge nodes [129]. the degree-corrected bridgeness is used to evaluate

our results and is de   ned as

bi := dibi,

where di is degree of node i.

3.5 experimental results

results on synthetic datasets:

(3.17)

we perform experiments for both the stochastic block model (  0 = 0) and the mixed mem-

bership model. for the mixed membership model, we set the concentration parameter   0 = 1.

we note that the error is around 8%    14% and the running times are under a minute, when
n     10000 and n     k.

we observe that more samples result in a more accurate recovery of memberships which

matches intuition and theory. overall, our learning algorithm performs better in the stochas-

tic block model case than in the mixed membership model case although we note that the

accuracy is quite high for practical purposes. theoretically, this is expected since smaller

concentration parameter   0 is easier for our algorithm to learn [8]. also, our algorithm is

scalable to an order of magnitude larger in n as illustrated by experiments on real-world

large-scale datasets.

70

note that we threshold the estimated memberships to clean the results. there is a tradeo   

between match ratio and average error via di   erent thresholds. in synthetic experiments,

the tradeo    is not evident since a perfect matching is always present. however, we need to

carefully handle this in experiments involving real data.

results on id96: we perform experiments for the bag of words data set [22] for

the new york times. we set the concentration parameter to be   0 = 1 and observe top

recovered words in numerous topics. the results are in table 3.3. many of the results are

expected. for example, the top words in topic # 11 are all related to some bad personality.

we also present the words with most spread membership, i.e., words that belong to many

topics as in table 3.4. as expected, we see minutes, consumer, human, member and so on.

these words can appear in a lot of topics, and we expect them to connect topics.

results on real-world graph datasets: we describe the results on real datasets summarized

in table 3.5 in detail below. the simulations are summarized in table 3.6.

the results are presented in table 3.6. we note that our method, in both dense and sparse

implementations, performs very well compared to the state-of-the-art variational method.

for the yelp dataset, we have a bipartite graph where the business nodes are on one side

and user nodes on the other and use the review stars as the edge weights. in this bipartite

setting, the variational code provided by gopalan et al [70] does not work on since it is not

applicable to non-homophilic models. our approach does not have this restriction. note

that we use our dense implementation on the gpu to run experiments with large number

of communities k as the device implementation is much faster in terms of running time of

the stgd step.on the other hand, the sparse implementation on cpu is fast and memory

e   cient in the case of sparse graphs with a small number of communities while the dense

implementation on gpu is faster for denser graphs such as facebook. note that data

reading time for dblp is around 4700 seconds, which is not negligible as compared to other

71

topic #

top words

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

prompting
renegotiating
hamstrung
ennobled
scariest
mesmerize
reelection
hyperextended
believe
munching
gainfully
narrative
faithful
martialed
capable
aerodynamic
apostles
gospel
physique
belonged
smirky
thoughtful
o   setting
litigator
undertaken
multipolar
athletically
resurrect
dialog
password
recessed
redlining
sponsor
rati   cation
trespasses
ine   ectiveness

eviscerated
entity
quasi
irrelevance
knowingly
millennium
arthroscopic
precipitating
ballcarrier
unsettle
narrator
deviating
corrupted
dowdy
dashboard
system
believer
mobbed
visualizing
mauling
bad
moron
acknowledgment
revoked
idealism
multilateral
myer
backslide
diabolical
swiss
butyl

complicated
loose
airbrushed
tantalize
pest
dawned
quixotic
anus
signing
prorated
settles
rosier
betcha
winston
misdeed
airbag
oracles
apt
jumping
loo
silly
freaked
preparing
prevented
wilsonian
hegemonist
   ctitious
slug
   les
list
phased
prescription marched
televise
insinuating
buckle
coexisted

sponsorship
warhead
divestment
repentance

predetermined
legalese
outsold
noncontroversial
causing
ecological
versatility
underhand
parallel
linebacking
considerable
protagonist
inept
islamic
navigation
braking
deliberately
manipulate
hedgehog
postproduction
natured
obtuse
agree
preseason
brethren
enlargement
majorleaguebaseball
superseding
lion
coldblooded
lowlight
mischaracterization
festival
staged
schoolchild
divvying

lap
justice
fargo
untalented
   ub
ecologist
commanded
knee
anomalies
bonus
articles
deductible
retrench
corrupting
opportunistically
mph
loafer
dialogue
zeitgeist
plunk
frat
stink
misstating
entomology
writeo   
mutating
familiarizing
artistically
town
outgained
balmy
tertiary
sullied
reconstruct
refuel
overexposed

table 3.3: top recovered topic groups from the new york times dataset along with the
words present in them.

keywords
minutes, consumer, human, member, friend, program, board, cell, insurance, shot

table 3.4: the top ten words which occur in multiple contexts in the new york times
dataset.

72

statistics facebook yelp
|e|
|v |
gd
k
ab
adcb

766,800
18,163
0.004649
360
0.5379
47.01

672,515
10,010+28,588
0.000903
159
0.4281
30.75

dblp sub dblp

5,066,510
116,317
0.000749
250
0.3779
48.41

16,221,000
1,054,066
0.000029
6,003
0.2066
6.36

table 3.5: summary of real datasets used in our thesis: |v | is the number of nodes in the
graph, |e| is the number of edges, gd is the graph density given by
|v |(|v |   1), k is the
number of communities, ab is the average bridgeness and adcb is the average degree-
corrected bridgeness(explained in section 3.4).

2|e|

datasets (usually within a few seconds). e   ectively, our algorithm, excluding the    le i/o

time, executes within two minutes for k = 10 and within ten minutes for k = 100.

300

250

200

150

100

s
s
e
n
i
s
u
b
#

50

0
0

50

100

150

200

250

300

business category id

0.35

0.3

0.25

0.2

0.15

r
o
r
r
e

e
g
a
r
e
v
a

0.1

0.05

0
0

0.2

0.4

0.6

0.8

1

recovery ratio

figure 3.5: distribution of business categories (left) and result tradeo    between recovery
ratio and error for yelp (right).

interpretation on yelp dataset: the ground truth on business attributes such as location

and type of business are available (but not provided to our algorithm) and we provide the

distribution in figure 3.5 on the left side. there is also a natural trade-o    between recovery

ratio and average error or between attempting to recover all the business communities and the

accuracy of recovery. we can either recover top signi   cant communities with high accuracy

or recover more with lower accuracy. we demonstrate the trade-o    in figure 3.5 on the right

side.

73

data

method

ten(sparse)
ten(sparse)
ten(sparse)
ten(dense)
ten(dense)
variational
ten(dense)
ten(dense)
variational
ten(sparse)
ten(sparse)
ten(dense)
ten(dense)
ten(dense)
ten(dense)
ten(dense)
ten(dense)
variational
ten(dense)
ten(dense)
variational
ten(sparse)
ten(sparse)
ten(sparse)

fb

yp

db sub

db

bk

10
100
100
100
100
100
500
500
500
10
100
100
100
500
500
100
100
100
500
500
500
10
100
100

thre

0.10
0.08
0.05
0.100
0.070
   
0.020
0.015
   
0.10
0.08
0.100
0.090
0.020
0.015
0.15
0.09
   
0.10
0.04
   
0.30
0.08
0.05

e
0.063
0.024
0.118
0.012
0.019
0.070
0.014
0.018
0.031
0.271
0.046
0.023
0.061
0.064
0.336
0.072
0.260
7.453
0.010
0.139
16.38
0.103
0.003
0.105

r(%)
13
62
95
39
100
100
71
100
100
43
86
43
80
72
100
36
80
99
19
89
99
73
57
95

time(s)

35
309
309
190
190
10, 795
468
468
86, 808
10
287
1, 127
1, 127
1, 706
1, 706
7, 664
7, 664
69, 156
10, 157
10, 157
558, 723
4716
5407
5407

table 3.6: yelp, facebook and dblp main quantitative evaluation of the tensor method

versus the variational method: bk is the community number speci   ed to our algorithm, thre

is the threshold for picking signi   cant estimated membership entries. refer to table 3.5 for
statistics of the datasets.

we select the top ten categories recovered with the lowest error and report the business

with highest weights in b  . among the matched communities, we    nd the business with

the highest membership weight (table 3.7). we can see that most of the    top    recovered

businesses are rated high. many of the categories in the top ten list are restaurants as they

have a large number of reviewers. our method can recover restaurant category with high

accuracy, and the speci   c restaurant in the category is a popular result (with high number

of stars). also, our method can also recover many of the categories with low review counts

accurately like hobby shops, yoga, churches, galleries and religious organizations which are

the    niche    categories with a dedicated set of reviewers, who mostly do not review other

categories.

our algorithm can also recover the attributes of users. however, the ground truth available

about users is far more limited than businesses, and we only have information on gender,

average review counts and average stars (we infer the gender of the users through their

74

category
latin american
gluten free
hobby shops
mass media
yoga
churches
art galleries
libraries
religious
wickenburg

business
salvadoreno
p.f. chang   s
make meaning
kjzz 91.5fm
sutra midtown
st andrew church
sette lisa
cholla branch
st andrew church
taste of caribbean

star(b)
4.0
3.5
4.5
4.0
4.5
4.5
4.5
4.0
4.5
4.0

star(c) rc(b) rc(c)

3.94
3.72
4.13
3.63
4.55
4.52
4.48
4.00
4.40
3.66

36
55
14
13
31
3
4
5
3
60

93.8
50.6
7.6
5.6
12.6
4.2
6.6
11.2
4.2
6.7

table 3.7: most accurately recovered categories and businesses with highest membership
weights for the yelp dataset.    star(b)    denotes the review stars that the business receive
and    star(c)   , the average review stars that businesses in that category receive.    rc(b)   
denotes the review counts for that business and    rc(c)    , the average review counts in that
category.

names). our algorithm can recover all these attributes. we observe that gender is the hardest

to recover while review counts is the easiest. we see that the other user attributes recovered

by our algorithm correspond to valuable user information such as their interests, location,

age, lifestyle, etc. this is useful, for instance, for businesses studying the characteristics of

their users, for delivering better personalized advertisements for users, and so on.

facebook dataset: a snapshot of the facebook network of unc [155] is provided with user

attributes. the ground truth communities are based on user attributes given in the dataset

which are not exposed to the algorithm. there are 360 top communities with su   cient (at

least 20) users. our algorithm can recover these attributes with high accuracy compared

with variational id136 result [70].

we also obtain results for a range of values of   0 (figure 3.6). we observe that the recovery

ratio improves with larger   0 since a larger   0 can recover overlapping communities more

e   ciently while the error score remains relatively the same.

for the facebook dataset, the top ten communities recovered with lowest error consist of cer-

tain high schools, second majors and dorms/houses. we observe that high school attributes

are easiest to recover and second major and dorm/house are reasonably easy to recover by

looking at the friendship relations in facebook. this is reasonable: college students from

75

ts

o
i
t
a
r
 

y
r
e
v
o
c
e
r

1

0.8

0.6

0.4

0.2

0
 
0

 

  0:0.1
  0:0.5
  0:0.9

 

  0:0.1
  0:0.5
  0:0.9

0.25

0.2

0.15

r
o
r
r
e

0.1

0.05

0.05

0.1
threshold

0.15

0.2

0
 
0

0.05

0.1
threshold

0.15

0.2

figure 3.6: performance analysis of facebook dataset under di   erent settings of the concen-
tration parameter (  0) for   k = 100.

the same high school have a high id203 of being friends; so do colleges students from

the same dorm.

dblp dataset:

the dblp data contains bibliographic records5 with various publication venues, such as

journals and conferences, which we model as communities. we then consider authors who

have published at least one paper in a community (publication venue) as a member of it.

co-authorship is thus modeled as link in the graph in which authors are represented as nodes.

in this framework, we could recover the top authors in communities and bridging authors.

3.6 conclusion

in this chapter, we presented a fast and uni   ed moment-based framework for learning over-

lapping communities as well as topics in a corpus. there are several key insights involved.

firstly, our approach follows from a systematic and guaranteed learning procedure in contrast

to several heuristic approaches which may not have strong statistical recovery guarantees.

5http://dblp.uni-trier.de/xml/dblp.xml

76

secondly, though using a moment-based formulation may seem computationally expensive

at    rst sight, implementing implicit    tensor    operations leads to signi   cant speed-ups of the

algorithm. thirdly, employing randomized methods for id106 is promising in the

computational domain, since the running time can then be signi   cantly reduced.

this work paves the way for several interesting directions for further research. while our

current deployment incorporates community detection in a single graph, extensions to multi-

graphs and hypergraphs are possible in principle. a careful and e   cient implementation

for such settings will be useful in a number of applications.

it is natural to extend the

deployment to even larger datasets by having cloud-based systems. the issue of e   cient

partitioning of data and reducing communication between the machines becomes signi   cant

there. combining our approach with other simple community detection approaches to gain

even more speedups can be explored.

77

chapter 4

dictionary learning through

convolutional tensor decomposition

in this chapter, we extend tensor decomposition framework to models with invariances, such

as convolutional dictionary models. learning invariant dictionary elements is crucial to

remove unnecessary model redundancy in a lot of settings. for instance, in image    lter

bank learning where image    lters    activation locations in the image are ignored, in natural

language process where the phrase templates are not distinguished by their location in the

sentence, and in neural science where neural spikes consist of template spikes activated at

di   erent time.

we propose a tensor decomposition algorithm to solve this problem of learning shift invariant

dictionary elements. our tensor decomposition algorithm is based on the popular alternating

least squares (als) method, but with additional shift invariance constraints on the factors.

we demonstrate that each als update can be computed e   ciently using simple operations

such as fast fourier transforms and id127s. our algorithm converges to mod-

78

els with better reconstruction error and is much faster, compared to the popular alternating

minimization heuristic, where the    lters and activation maps are alternately updated.

we propose a novel framework for learning convolutional models through tensor decom-

position. we consider inverse method of moments to estimate the model parameters via

decomposition of higher order (third or fourth order) moment tensors. when the inputs x

are generated from a convolutional model in (1.3), with independent activation maps w   i , i.e.

a convolutional ica model, we show that the cumulant tensors have a cp decomposition,

whose components correspond to    lters and their circulant shifts. we propose a novel method

for tensor decomposition when such circulant constraints are imposed on the components of

the tensor decomposition.

our tensor decomposition method is a constrained form of the popular alternating least

squares (als) method1. we show that the resulting optimization problem in each tensor

als iteration can be solved in closed form, and uses simple operations such as fast fourier

transforms (fft) and id127s. these operations have a high degree of par-

allelism:

for estimating l    lters, each of length n, we require o(log n + log l) time and

o(l2n3) processors. note that there is no dependence on the number of data samples n,

since the empirical moment tensor can be computed in one data pass, and the als iterations

only updates the    lters. this is a huge saving in running time, compared to the alternate

minimization method which requires a pass over data in each step to decode all the activation

per iteration with o(max( nn l

maps wi. the running time of alternating minimization is o(max(log n log l, log n log n))
log l )) processors, and when n     ln2, which is the typical
scenario, our method is hugely advantageous. our method avoids decoding the activation

log n , nn l

maps in each iteration since they are averaged out in the input moment tensor, on which

the als method operates and we only estimate the    lters fi in the learning step. in other

1the als method for tensor decomposition is not to be confused with the alternating minimization
method for solving (1.4). while (1.4) acts on data samples and alternates between updating    lters and
activation maps, tensor als operates on averaged moment tensors and alternates between di   erent modes
of the tensor decomposition.

79

words, the activation maps wi   s are easily estimated using (1.4) in one data pass after    lter

estimation. thus, our method is highly parallel and scalable to huge datasets.

we carefully optimize computation and memory costs by exploiting tensor algebra and cir-

culant structure, due to the shift invariance of the convolutional model. we implicitly carry

out many of the operations and do not form large (circulant) matrices and minimize stor-

age requirements. preliminary experiments further demonstrate superiority of our method

compared to alternating minimization. our algorithm converges accurately and much faster

to the true underlying    lters compared to alternating minimization. moreover, it results in

much lower reconstruction error, while alternating minimization tends to get stuck in spu-

rious local optima. our algorithm is also orders of magnitude faster than the alternating

minimization.

4.1 model and formulation

notation let [n] := {1, 2, . . . , n}. for a vector v, denote the ith element as v(i). for a
matrix m, denote the ith row as m i and j th column as mj. for a tensor t     rn  n  n, its
(i1, i2, i3)th entry is denoted by [t ]i1,i2,i3. a column-stacked matrix m consisting of m   i s (with

same number of rows) is m := [m1, m2, . . . , ml]. similarly, a row-stacked matrix m from

m   i s (with same number of columns) is m := [m1; m2; . . . ; ml].

cyclic convolution the 1-dimensional (1-d) n-cyclic convolution f     w between vectors
f and w is de   ned as v = f    n w, v(i) =pj   [n] f (j)w((i     j + 1) mod n). note that the

linear convolution is the combination without the modulo operation (i.e. cyclic shifts) above.

n-cyclic convolution is equivalent to linear convolution, when n is at least twice the support

length of both f and w [133], which will be assumed. we drop the notation n in     for

80

convenience. cyclic convolution in (4.1) is equivalent to f     w = cir(f )    w, and
cir(f ) :=xp

j :=    {((i     j) mod n) = p     1} ,

f (p)gp     rn  n,

(gp)i

   p     [n]. (4.1)

de   nes a circulant matrix. a circulant matrix cir(f ) is characterized by the vector f , and

each column corresponds to a cyclic shift of f .

properties of circulant matrices let f be the discrete fourier transform matrix whose

k =   (m   1)(k   1)

(m, k)-th entry is f m

n ). if u :=    nf    1,
u is the set of eigenvectors for all n    n circulant matrices [73]. let the discrete fourier
transform of a vector f be fft(f ), we express the circulant matrix cir(f ) as

,    m, k     [n] where   n = exp(    2  i

n

cir(f ) = u diag(f    f )u h = u diag(fft(f ))u h.

(4.2)

this is an important property we use in algorithm optimization to improve computational

e   ciency.

column stacked circulant matrices we will extensively use column stacked circulant matrices

f := [cir(f1), . . . , cir(fl)], where cir(fj) is the circulant matrix corresponding to    lter fj.

4.1.1 convolutional dictionary learning/ica model

we assume that the input x     rn is generated as

x = xj   [l]

f   j     w   j = xj   [l]

cir(f   j )w   j = f       w   ,

(4.3)

where f    := [cir(f   1 ), cir(f   2 ), . . . , cir(f   l)] is the concatenation or column stacked version of
circulant matrices and w    is the row-stacked vector w    := [w   1; w   2; . . . w   l]     rnl. recall that
cir(f   l ) is circulant matrix corresponding to    lter f   l , as given by (4.2). note that although

81

f    is a n by nl matrix, there are only nl free parameters. we never explicitly form the
estimates f of f   , but instead use    lter estimates fl   s to characterize f . in addition, we can
handle additive gaussian noise in (4.17), but do not incorporate it for simplicity. activation

maps:for each observed sample x, the activation map w   i

in (4.17) indicates the locations

where each    lter f   i

is active and w    is the row-stacked vector w    := [w   1; w   2; . . . w   l]. we

assume that the coordinates of w    are drawn from some product distribution, i.e. di   erent

entries are independent of one another and we have the independent component analysis

(ica) model in (4.17). when the distribution encourages sparsity, e.g. bernoulli-gaussian,

only a small subset of locations are active, and we have the sparse coding model in that case.

we can also extend to dependent distributions such as dirichlet for w   , along the lines of [32],

but limit ourselves to ica model for simplicity. learning problem:given access to n i.i.d.
samples, x := [x1, x2, . . . , xn ]     rn  n , generated according to the above model, we aim to
estimate the true    lters f   i , for i     [l]. once the    lters are estimated, we can use standard
decoding techniques, such as the square loss criterion in (1.4) to learn the activation maps

for the individual maps. we focus on developing a novel method for    lter estimation in this

chapter.

4.2 form of cumulant moment tensors

tensor preliminaries we consider 3rd order tensors in this chapter but the analysis is easily
extended to higher order tensors. for tensor t     rn  n  n, its (i1, i2, i3)th entry is denoted by
[t ]i1,i2,i3,   i1     [n], i2     [n], i3     [n]. a    attening or unfolding of tensor t     r is the column-
stacked matrix of all its slices, given by unf old(t ) := [[t ]:,:,1, [t ]:,:,2, . . . , [t ]:,:,n]     rn  n2.
de   ne the khatri-rao product for vectors u     ra and v     rb as a row-stacked vector
[u     v] := [u(1)v; u(2)v; . . . ; u(a)v]     rab. khatri-rao product is also de   ned for matrices
with same columns. for m     ra  c and m        rb  c, m     m    := [m1     m   1, . . . , mc     m   c, ]    

82

rab  c, where mi denotes the ith column of m. cumulantthe third order cumulant of a

multivariate distribution is a third order tensor, which uses (raw) moments up to third
order. let c3     rn  n2 denote the unfolded version of third order cumulant tensor, it is given
by

c3 := e[x(x     x)   ]     unf old(z)

(4.4)

where [z]a,b,c := e[xa]e[xbxc] + e[xb]e[xaxc] + e[xc]e[xaxb]     2e[xa]e[xb]e[xc],    a, b, c     [n].

under the convolution ica model in section 4.1.1, we show that the third order cumulant

has a nice tensor form, as given below.

lemma 4.1 (form of cumulants). the unfolded third order cumulant c3 in (4.4) has the

following decomposition form

c3 = xj   [nl]

     jf   j (f   j    f   j )    = f         (f        f   )    , where       := diag(     1,      2, . . . ,      nl) (4.5)

where f   j denotes the j th column of the column-stacked circulant matrix f    and      j is the
third order cumulant corresponding to the (univariate) distribution of w   (j).

for example, if the lth activation is drawn from a poisson distribution with mean     , we have

that      l =     . note that if the third order cumulants of the activations, i.e.      j    s, are zero, we

need to consider higher order cumulants. this holds for zero-mean activations and we need

to use fourth order cumulant instead. our method extends in a straightforward manner for

higher order cumulants.

the decomposition form in (4.5) is known as the candecomp/parafac (cp) decom-

position form [12] (the usual form has the decomposition of the tensor and not its unfolding,

as above). we now attempt to recover the unknown    lters f   i through decomposition of the

third order cumulants c3. this is formally stated below.

83

f = blk1(f )

. . .

blkl(f )

   =

blk1

1(  )
. . .

blkl

1 (  )

. . .

. . .

. . .

blk1

l(  )
. . .

blkl

l(  )

figure 4.1: (a) blocks of the column-stacked circulant matrix f . (b) blocks of the row-and-
column-stacked diagonal matrices   . blki

j(  ) is diagonal.

objective function: our goal is to obtain    lter estimates fi   s which minimize the frobenius

norm k    kf of reconstruction of the cumulant tensor c3,

kc3     f    (f     f )   k2
f ,

min
f
s.t. blkl(f ) = u diag(fft(fl))u h, kflk2 = 1,

   l     [l],    = diag(  ).

(4.6)

where blkl(f ) denotes the lth circulant matrix in f . the conditions in (4.6) enforce blkl(f )
to be circulant and for the    lters to be normalized. recall that u denotes the eigenvectors

for circulant matrices. the rest of the chapter is devoted to devising e   cient methods to

solve (4.6).

throughout the chapter, we will use fj to denote the j th column of f , and blkl(f ) to denote
the lth circulant matrix block in f . note that f     rn  nl, fj     rn and blkl(f )     rn  n.

4.3 alternating least squares for convolutional ten-

sor decomposition

to solve the non-id76 problem in (4.6), we consider the alternating least

squares (als) method with column stacked circulant constraint. we    rst consider the

asymmetric relaxation of (4.6) and introduce separate variables f ,g and h for    lter es-

84

timates along each of the modes to    t the third order cumulant tensor c3. we then perform

alternating updates by    xing two of the modes and updating the third one.

min
f

kc3   f    (h     g)   k2

f s.t. blkl(f ) = u   diag(fft(fl))  u h, kflk2

2 = 1,   l     [l] (4.7)

similarly, g and h have the same column-stacked circulant matrix constraint and are up-
dated similarly in alternating steps. the diagonal matrix    is updated through normaliza-

tion.

we now introduce the convolutional tensor

(ct) decomposition algorithm to e   ciently

solve (4.7) in closed form, using simple operations such as id127s and fast
fourier transform (fft). we do not form matrices f ,g and h     rn  nl, which are large,
but only update them using    lter estimates f1, . . . , fl, g1, . . . , gl, h1, . . . hl. denote

m := c3((h     g)   )   ,

(4.8)

where     denotes pseudoinverse. let blkl(m) and blkl(  ) denote the lth blocks of m and   .
we have a closed form solution for    lter update, once we have computed m, and we present

the main result as follows.

theorem 4.1. [closed form updates] the optimal solution f opt

l

for (c.9) is given by

(p) = pi,j   [n]kblkl(m)jk   1    blkl(m)i
j    i q
p   1

f opt
l

i q
p   1

pi,j   [n]

,

   p     [n], q := (i     j) mod n.

(4.9)

further    = diag(  ) is updated as   (i) = kmik, for all i     [nl]. note that i q
(q, (p     1))th element of the identity matrix.

p   1 denotes the

85

proof sketch: using the property of least squares, the optimization problem in (4.7) is

equivalent to

min

f kc3((h     g)   )           fk2

f s.t. blkl(f ) = u  diag(fft(fl))  u h, kflk2

2 = 1,   l     [l] (4.10)

when (h     g) and    are full column rank. the full rank condition requires nl < n2 or
l < n, and it is a reasonable assumption since otherwise the    lter estimates are redundant.

in practice, we can additionally regularize the update to ensure full rank condition is met.

since (c.8) has block constraints, it can be broken down in to solving l independent sub-

problems

min

fl (cid:13)(cid:13)blkl(m)    blkl(  )        u    diag(fft(fl))    u h(cid:13)(cid:13)2

f

s.t. kflk2

2 = 1,   l     [l]

(4.11)

our proof for the closed form solution is similar to the analysis in [57], where they proposed

a closed form solution for    nding the closest circulant/toeplitz matrix. for a detailed proof

of theorem 4.1, see appendix c.2.

thus, the reformulated problem in (c.9) can be solved in closed form e   ciently. a bulk

of the computational e   ort will go into computing m in (4.8). computation of m requires

2l fast fourier transforms of length n    lters and simple id127s without

explicitly forming g or h. we make this concrete in the next section. the closed form
update after getting m is highly parallel. with o(n2l/ log n) processors, it takes o(log n)

time.

86

4.4 algorithm optimization to reduce memory and

computational costs

we now focus on estimating m := c3((h     g)   )    in (4.8). if done naively, this requires
inverting n2    nl matrix and multiplication of n    n2 and n2    nl matrices with o(n6)
time. however, forming and computing with these matrices is very expensive when n (and

l) are large.

instead, we utilize the properties of circulant matrices and the khatri-rao

product     to e   ciently carry out these computations implicitly. we present our    nal result
on computational complexity of the proposed method. recall that n is the    lter size and l

is the number of    lters.

lemma 4.2. [computational complexity] with multi-threading, the running time of

our algorithm for n dimensional input and l number of    lters is o(log n+log l) per iteration

using o(l2n3) processors.

note that before the iterative updates, we compute the third order cumulant2 c3 once

whose computational complexity is o(log n) with n

log n processors, where n is the number

of samples. however, this operation is not iterative. in contrast, alternating minimization

(am) requires pass over all the data samples in each iteration, while our algorithm requires

only one pass of the data.

the parallel computational complexity of am is as follows. in each iteration of am, com-

puting the derivative with respect to either    lters or activation maps requires nl number

of ffts (requires o(nln log n) serial time), and the degrees of parallelism are o(nn log l)

and o(nn log n) respectively. therefore with multi-threading, the running time of am is

o(max(log n log l, log n log n)) per iteration using o(max( nn l

log n , nn l

log l )) processors. compar-

2instead of computing the cumulant tensor c3, a randomized sketch can be computed e   ciently, following
the recent work of [159], and the als updates can be performed e   ciently without forming the cumulant
tensor c3.

87

ing with lemma 4.2, we    nd that our algorithm is advantageous in the regime of n     ln2,
which is the typical regime in applications.

let us describe how we utilize various algebraic structures to obtain e   cient computation.
property 1 (khatri-rao product): ((h   g)   )    = (h   g)((h   h).   (g   g))   , where .    denotes
element-wise product.

computational goals: find ((h   h).     (g   g))       rst and multiply the result with c3(h     g)
to    nd m.

we now describe in detail how to carry out each of these steps.

4.4.1 challenge: computing ((h   h).     (g   g))   

a naive implementation to    nd the matrix inversion ((h   h).     (g   g))    is very expensive.
however, we incorporate the stacked circulant structure of g and h to reduce computation.
note that this is not completely straightforward since although g and h are column stacked
circulant matrices, the resulting product whose inverse is required, is not circulant. below,

we show that however, it is partially circulant along di   erent rows and columns.

property 2

(block circulant matrix): the matrix (h   h).     (g   g) consists of row and

column stacked circulant matrices.

we now make the above property precise by introducing some new notations. de   ne column
stacked identity matrix i := [i, . . . , i]     rn  nl, where i is n    n identity matrix. let
u := blkdiag(u, u, . . . u)     rnl  nl be the block diagonal matrix with u along the diagonal.
the    rst thing to note is that g and h, which are column stacked circulant matrices, can

88

be written as

g = i    u    diag(v)    u

h

,

v := [fft(g1); fft(g2); . . . ; fft(gl)],

(4.12)

where g1, . . . , gl are the    lters corresponding to g, and similarly for h, where the diagonal
matrix consists of fft coe   cients of the respective    lters h1, . . . , hl.

by appealing to the above form, we have the following result. we use the notation blki
for a matrix        rnl  nl to denote (i, j)th block of size n    n.
lemma 4.3 (form of (h   h).     (g   g) ). we have

j(  )

((h   h).     (g   g))    = u             uh,

(4.13)

where        rnl  nl has l by l blocks, each block of size n    n. its (j, l)th block is given by

blkj

l (  ) = diag(fft(  (gj, gl).       (hj, hl)))     rn  n

(4.14)

where   (gj, gl) := reverse(reverse(gj)    gl) and   (hj, hl) := reverse(reverse(hj)    hl).
therefore, the inversion of (h   h).   (g   g) can be reduced to the inversion of row-and-column
stacked set of diagonal matrices which form   . computing    simply requires fft on all

2l    lters g1, . . . , gl and h1, . . . , hl, i.e. 2l ffts, each on length n vector. we propose

an e   cient iterative algorithm to compute       via block matrix inversion theorem[68] in

appendix c.3.

4.4.2 challenge: computing m = c3(h     g)    ((h   h).     (g   g))   

now that we have computed ((h   h).     (g   g))    e   ciently, we need to compute the resulting
matrix with c3(h     g) to obtain m. we observe that the mth row of the result m is given

89

by

m m = xj   [nl]

uj diagh (z)   (m) diag (v) (uj)huj     uh,

   m     [nl],

(4.15)

where v := [fft(g1); . . . ; fft(gl)], z := [fft(h1); . . . ; fft(hl)] are concatenated fft co-

e   cients of the    lters, and

  (m) := uhi     (m)iu,

[  (m)]i

j := [c3]m

i+(j   1)n,

   i, j, m     [n]

(4.16)

note that   (m) and   (m) are    xed for all iterations and need to be computed only once.

note that   (m) is the result of taking mth row of the cumulant unfolding c3 and matricizing

it. equation (4.15) uses the property that c m
h     (m)g.

3 (h     g) is equal to the diagonal elments of

we now bound the cost for computing (4.15). (1) inverting    takes o(log l+log n) time with

o(n2l2/(log n+log l)) processors according to appendix c.3. (2) since diag(v) and diag(z)

are diagonal and    is a matrix with diagonal blocks, the overall id127 in

equation (4.15) takes o(l2n2) time serially with o(l2n2) degree of parallelism for each row.

therefore the overall serial computation cost is o(l2n3) with o(l2n3) degree of parallelism.

with multi-threading, the running time is o(1) per iteration using o(l2n3) processes. (3)

fft requires o(n log n) serial time, with o(n) degree of parallelism. therefore computing

2l fft   s takes o(log n) time with o(ln) processors.

combining the above discussion, it takes o(log l + log n) time with o(l2n3) processors.

90

4.5 experiments: comparison with alternating mini-

mization

we compare our convolutional tensor decomposition framework with solving equation (1.4)

using alternating (between    lters and activation map) minimization method where gradient

descent is employed to update fi and wi alternatively. the error comparison between our

proposed convolutional tensor algorithm and the alternating minimization algorithm is in

   gure 4.2a. we evaluate the errors for both algorithms by comparing the reconstruction of

error and    lter recovery error3. our algorithm converges much faster to the solution than

the alternating minimization algorithm. in fact, alternating minimization leads to spurious

solution where the reconstruction error is signi   cantly larger compared to the error achieved

by the tensor method. the error bump in the reconstruction error curve in    gure 4.2a for

tensor method is due to the random initialization following de   ation of one    lter, and esti-

mation of the second one. the running time is also reported in    gure 4.2b and 4.2c between

our proposed convolutional tensor algorithm and the alternating minimization. our algo-

rithm is orders of magnitude faster than the alternating minimization. both our algorithm

and alternating minimization scale linearly with number of    lters. however convolutional

tensor algorithm is almost constant time with respect to the number of samples, whereas

the alternating minimization scales linearly. this results in huge savings in running time for

large datasets.

3note that circulant shifts of the    lters result in the same reconstruction error, and we report the lowest

error between the estimated    lters and all circulant shifts of the ground-truth.

91

 

proposed ct: f1
baseline am: f1
proposed ct: f2
baseline am: f2
proposed ct: reconst
baseline am: reconst

101

r
o
r
r
e

100

proposed ct
baseline am

400

350

300

250

200

150

100

50

s
d
n
o
c
e
s

proposed ct
baseline am

 

70

60

50

40

30

20

10

s
d
n
o
c
e
s

 

10   1
100

101

iteration

102

0
 
0

2

4

6

8

number of filters l

10

0
 
0

200

400

600

800

number of samples n

 

1000

figure 4.2:
(a) error comparison between our convolutional tensor method (proposed ct)
and the baseline alternate minimization method (baseline am). (b) running time compar-
ison between our proposed ct and the baseline am method under varying l. (c) running
time comparison between ct and am method under varying n.

4.6 application: learning word-sequence embeddings

4.6.1 word-sequence modeling and formulation

our convdic+deconvdec framework focuses on a convolutional dictionary model to summa-

rize phrase templates, and then decode word-sequence signals to obtain the word-sequence

embeddings. the    rst question is how to encode the word sequence into a signal, to be input

to the convolutional model and we discuss that below.

from raw text to signals

word encoding: a word is represented as a one-hot encoding vector, i.e. with vector ei     rd
whose ith entry is 1 and other entries are 0, where i is the index of the word in the dictionary.

alternatively, one could use the id97 embeddings instead of one-hot encodings. we

then stack the one-hot encoding vectors of each sentence together to form a encoding matrix.

the stacking order conforms the word-sequence order.

92

n1

sseq1

y1y1y1y1y1y1

d

k

=y

y

y

(1)
1
(2)
1

n2

n3

k

sseq2

sseq3

svd=

u

  

v    

y2y2y2y2y2y2

y

(1)
2

y

(2)
2

y3y3y3y3y3y3

=

u   

sseq1

sseq2

sseq3

y

(1)
3

y

(2)
3

figure 4.3: principal component projection to obtain [y1,y2, . . . ,ym ] = u   s =
u   [sseq1,sseq2, . . . ,sseqm ] using s. note that u is the top k left eigenvectors of s.

to be precise, let us consider sentenc with n words. the encoding matrix of this word-
sequence sseq is sseq := [sword1, sword2, . . . , swordn ]     rd  n .

principal components: now that we have encoded words in each sentence, we want to    nd

a compact representation of them in terms of a dictionary model. however, the encoding

matrices are too sparse to    t a convolutional model in the word space. instead, we perform

id84 through pca and carry out dictionary modeling in the projected

space.

concretely, we stack the encoding matrices side by side as s := [sseq1,sseq2, . . . ,sseqm ]    
rd  (pm
i=1 ni), assuming there are m number of sentences in the collection of varying lengths
n1, n2 and so on. let u     rd  k denote the top k left eigenvectors of s. we consider
yi := u   sseq1     rk  ni, for each sentence i. we treat the rows of yi independently in
parallel and    t convolutional model to each row. denote j th row of yi as y(j)
, and thus

i

yi =

                  

y(1)
i
...
y(k)
i

.

                  

93

yi

y(1)
i

y(2)
i

y(k)
i

=

=

=

* +

*

* +

*

* +

*

activation maps

word-sequence embedding

+

+

+

=

=

=

m
a
x
   
k
p
o
o

 

l
i

n
g
m
a
x
   
k
p
o
o

 

l
i

n
g
m
a
x
   
k
p
o
o

 

l
i

n
g

stack

coordinate 1

coordinate 2

coordinate k

comprehension phase

feature-extraction phase

figure 4.4: overview of our convdic+deconvdec framework for the ith word-sequence over k
coordinates. the comprehension phase learns phrase templates using tensor decomposition
algorithm. the feature-extraction phase decodes activation maps using deconvolutional
decoding algorithm. the activation maps are max-k pooled and stacked as the    nal word-
sequence embedding.

each y(j)

i

is generated through a convolutional dictionary model over phrase templates and

activation maps. our goal in the learning phase is to learn template phrases for the collection

of [y(j)

i

] over all word-sequences    i     [m] across all parallel directions    j     [k]. we will state
the learning problem formally in the next section. since all the coordinates are independent

and the phrase templates are learned in parallel over all the coordinates, we drop the index

j to denote a coordinate of the ith word sequence y(j)

i

. in the following subsection, a patch

from y(j)

i will be denoted as x.

comprehension phase     learning phrase templates

a word sequence is composed of superposition of overlapping patches, therefore we are

interested in learning a generative model over overlapping patches. we can also view these

patches as phrases. a length n patch x is generated as the superposition of l phrase
embeddings f   l convolved at l activation maps w   l ,    l     [l]. due to the property of the
convolution, the convolution is reformulated as the multiplication of f    and w   , where
f    := [cir(f   1 ), cir(f   2 ), . . . , cir(f   l)] is the concatenation of circulant matrices and w    is the

94

=

   

   

=

x

f   1

w   1

f   l

w   l

x

f   

w   

(a) convolutional model

(b) reformulated model

figure 4.5: convolutional tensor decomposition for learning convolutional ica mod-
els [82].(a) the convolutional generative model with template phrases. (b) reformulated
multiplicative model where f    is column-stacked circulant matrix.

=

+...+

+

+...+

c3

  1(f   1 )   3

. . .

+  2(f   2 )   3

figure 4.6: the third order cumulant is decomposed superposition of third order outer
product of template phrases and third order outer product of shifted template phrases.

row-stacked vector w    :=

                           

w   1

w   2
...

w   l

                           

    rnl. to be precise, a patch

x =xl   [l]

f   l     wl    = f       w   ,

(4.17)

this is illustrated in fig 4.6(a). cir(f   l ) is circulant matrix corresponding to phrase template
f   l , whose columns are shifted versions of f   l as shown in fig 4.6(a). note that although f   
is a n by nl matrix, there are only nl free parameters. given access to the collection of

word-sequence sample patches, x := [x1, x2, . . .], generated according to the above model,
we aim to estimate the true template phrases f   i , for i     [l].

95

if the patches are in the same coordinate of the word sequence, these patches share a common

set of phase templates, but their activation maps are di   erent. the activation maps are the

discriminative features that distinguish di   erent patches. once the template phrases are

estimated, we can use standard decoding techniques, such as the square loss criterion in

(1.4) to learn the activation maps for the individual maps.

feature-extraction phase     word-sequence embeddings

activation maps in a coordination: after learning a good set of phrase templates {f1, . . . , fl}
and thus f , we use the deconvolutional decoding (deconvdec) to obtain the activation maps
for the j th coordinate. for each observed coordinate of the word-sequence y(j)
, the activation

i

in (4.17) indicates the locations where ith template phrase f   l

map w   l
is the row-stacked vector w    := [w   1; w   2; . . . w   l]. an estimation of w   , w(j)

i

is activated and w   

, is achieved as

(4.18)

follows

i = f   y(j)
w(j)

i

   .

note that the estimated phrase templates are zero padded to match the length of the word-

sequence.

we assume that the elements of w    are drawn from some product distribution, i.e. di   erent

entries are independent of one another, and we have the independent component analysis

(ica) model in (4.17). when the distribution encourages sparsity, e.g. bernoulli-gaussian,

only a small subset of locations are active, and we have the sparse coding model in that

of [32], but limit ourselves to ica model for simplicity. this activation map w(j)

case. we can also extend to dependent distributions such as dirichlet for w   , along the lines
i     rni  l
contains sequence embeddings from coordinate j only, and will be used as one coordinate of

our    nal word-sequence embeddings.

96

varying sentence length: one di   culty in learning the template phrases using our convo-

lutional tensor decomposition model is that di   erent word-sequence has a di   erent length

ni, therefore the activation maps are of varying length as well. we resolved this problem

by max-k pooling. in other words, we extract most informative global discriminative fea-

tures from the activation maps, as illustrated in figure 4.4. finally, we concatenate all the

max-k pooled coordinate sequence embeddings as a long vector as the    nal word-sequence

embedding.

the overall framework    ow is depicted in fig 4.4.

4.6.2 evaluating embeddings through downstream tasks

we evaluate the quality of our word sequence embeddings using three challenging natural

language process tasks: sentiment classi   cation, paraphrase detection, and semantic textual

similarity estimation. eight datasets which cover various domains are used as shown in

table 4.1.

dataset
review
subj
msrpara
sts-msrpar
sts-msrvid
sts-onwn
sts-smteuroparl machine translation
sts-smtnews
machine translation

domain
moview reviews
obj/subj comments
news sources
newswire
video caption
glosses

label
{-1,1}
{-1,1}
{-1,1}
[0,5]
[0,5]
[0,5]
[0,5]
[0,5]

label distribution

[0.49,0.51]
[0.50,0.50]
[0.33,0.67]

[0.00,0.02,0.10,0.24,0.47,0.17]
[0.13,0.21,0.14,0.16,0.21,0.14]
[0.01,0.02,0.04,0.12,0.35,0.47]
[0.01,0.00,0.00,0.02,0.19,0.78]
[0.00,0.01,0.01,0.06,0.19,0.73]

m
64720
1000
5801  2
1500  2
1500  2
750  2
1193  2
399  2

table 4.1: summary statistics of the datasets used.

for all the datasets, we train a simple id28 model on the training samples and

report test classi   cation accuracy using a 10-fold cross validation. id31 and

paraphrase detection belong to binary classi   cation tasks. in a binary classi   cation task,

either accuracy or f score is used as evaluate metric. recall that f-score is the harmonic

97

mean of precision and recall, i.e., f = 2    (precision    recall)/precision + recall. precision is
the number of true positives divided by the total number of elements labeled as belonging

to the positive class, and recall is the number of true positives divided by the total number

of elements that belong to the positive class.

our convdic+deconvdec learns word-sequence embeddings from scratch and requires no

pre-training. when working on a new dataset from a new domain, we train fresh set of

phrase templates as called domain phrase templates. using these domain phrase templates,

we decode activation maps and then form phrase-embeddings. our approach is di   erent

from skip thoughts, where universal phrase embeddings are generated [103].

evaluation task: sentiment classi   cation

id31 is an important task in natural language process as automated labeling of

word sequences into positive and negative opinions is used in various settings. we evaluate

our sentence embeddings on two datasets from di   erent domains, such as movie review

and subjective and objective comments, as in table 4.1. using word-sequence embeddings

combined with nb features, we obtain the state-of-the-art classi   cation results for both these

datasets as in table 4.2.

evaluation task: paraphrase detection

we consider the paraphrase detection task on the microsoft paraphrase corpus [137, 55]. we

employ 4076 sentence pairs as training data to learn the sentence embeddings and regress on

the ground truth binary labels with our learned sentence embeddings. the remaining test

data is used to calculate classi   cation error.

4the word similarities information they use are either trained in wikipedia (4.4 million articles in contrast

to the 4076 sentences of paraphrase dataset we use) or from id138 with expert knowledge.

98

method
nb-id166 [158]
mnb [158]
cbow [170]
grconv [170]
id56 [170]
bid56 [170]
id98 [102]
adasent [170]
paragraph-vector [114]
skip-thought [103]
convdic+deconvdec

subj
mr
93.2
79.4
93.6
79.0
91.3
77.2
89.5
76.3
93.7
77.2
94.2
82.3
93.4
81.5
95.5
83.1
90.5
74.8
75.5
92.1
78.9 92.4

table 4.2: binary classi   cation tasks: id31 task of cataloging a word-sequence
into two di   erent categories. classi   cation accuracies in percentage on standard benchmarks
(movie review and subject dataset) are displayed. the    rst group contains results using bag-of-
words models; the second group exhibits some supervised compositional models; the third group is
paragraph vector; the fourth is the skip-thought result.

method
vector similarity [123]
esa [78]
lsa [78]
rmlmg [142]
convdic+deconvdec
skip-thought [103]

outside information 4
word similarity
word semantic pro   les
word semantic pro   les
syntacticinfo
none
train large book corpus

f score
0.75
0.79
0.80
0.81
0.81
0.82

table 4.3: binary classi   cation tasks: paraphrase detection task, which operates on pairs of
word-sequences and decides on whether they are a paraphrase of each other or not. com-
parison of f-score with other unsupervised sentence paraphrase approaches. other methods
use auxiliary information such as word similarities trained on wikipedia or from id138.
in contrast, our algorithm learns sentence embeddings from scratch.

99

as discussed in [154], we combine the pair of sentence embeddings produced earlier wl

and wr, i.e., the embedding for the right and the left sentences. we generate features for

classi   cation using both the distance (absolute di   erence) and the product between the pair

(wl, wr): [wl     wr,kwl     wrk], where     denotes the element-wise multiplication.

in contrast to other unsupervised methods which are trained using outside information such

as id138 and parse trees, our unsupervised approach use no extra information, and still

achieves comparable results with the state of art [162] as in table 4.3. we show some examples

of paraphrase and non-paraphrase we identi   ed.

paraphrase detected:

(1) amrozi accused his brother, whom he called    the witness   ,

of deliberately distorting his evidence. (2) referring to him as only    the witness   , amrozi

accused his brother of deliberately distorting his evidence. the two sentences are the    di   cult

sentence    to show how our algorithm detect paraphrases since they are not simple switching

of clauses, and the sentence structures di   er quite signi   cantly in the two sentences.

non-paraphrase detected :

(1) i never organised a youth camp for the diocese of

bendigo.

(2) i never attended a youth camp organised by that diocese. similarly with

non-paraphrase detection, the two sentences share common words such as youth camp and

organized, but our method is able to successfully detect them as non-paraphrase.

evaluation task: semantic textual similarity estimation

for the semantic textual similarity (sts) task, the goal is to predict a real-valued similarity

score in a range [1, k] given a sentence pair. we include datasets from sts task in various

domains including news, image and video description, glosses from id138/ontonotes, the

output of machine translation systems with reference translation.

100

to frame semantic test similarity estimation task into the multi-class classi   cation frame-
work, the gold rating        [k1, k2] is discretized as p        k2   k1 in the follow manner [154],
pi =                 + 1 if i =          + 1     k1, pi =                 if i =          + 2     k1, and pi = 0 otherwise.
this reduces to    nding a predicted   p          k2   k1 given model parameters    to be closest to
p in terms of kl divergence [154]. we use a id28 classi   er to predict   p   and

estimate        = [k1, . . . , k2]  p.

results on sts task datasets are illustrated in table 4.4. as in [161], pearson   s r of the me-

dian, 75th percentile, and highest score from the o   cial task rankings are showed. we then

compare our method against the performance of supervised models in [161]: paragram-

phrase (pp), projection (proj.), deep-averaging network (dan), recurrent neural net-

work (id56) and lstm; as well as the state-of-the-art unsupervised model skip-thought

vectors [103].

as we can see from the table, lst is performing poorly even though a back-propagation after

seeing the training labelings is carried out for sequence embedding learning. our method

is an unsupervised approach as in skip-thought vectors. however, our algorithm doesn   t

output universal word-sequence embeddings across domains. we train a fresh model and a

new set of domain phrase templates from scratch. therefore our algorithm is performing

better for these individual datasets on the sts task.

supervised

+

unsupervised

supervised methods

unsupervised

methods

dataset

msrpar
msrvid
smt-eur
onwn
smt-news

50%

51.5
75.5
44.4
60.8
40.1

75%

57.6
80.3
48.1
65.9
45.4

max

73.4
88.0
56.7
72.7
60.9

dan

id56

lstm

skip-thought

convdic+deconvdec

40.3
70.0
43.8
65.9
60.0

18.6
66.5
40.9
63.1
51.3

9.3
71.3
44.3
56.4
51.0

16.8
41.7
35.2
29.7
30.8

36.0

61.8

37.5

33.1

72.1

table 4.4: sts task results: pearson   s r    100 on msrpar, msrvid, onwn, smteuroparl and
smtnews dataset. the    rst three columns are o   cial rankings reported in the sts2012
o   cial website, so it combines both supervised and unsupervised methods. the second three
columns are reported by [161]. our comparison against the state-of-the-art unsupervised
word-sequence embedding method is in the last two columns.

101

4.7 conclusion

in this chapter, we proposed a novel tensor decomposition framework for learning convolu-

tional dictionary models. unlike the popular alternating minimization, our method avoids

expensive decoding of activation maps in each step and can reach better solutions with

faster run times. we derived e   cient updates for tensor decomposition based on modi   ed

alternating least squares, and it consists of simple operations such as ffts and matrix mul-

tiplications. our framework easily extends to convolutional models for higher dimensional

signals (such as images), where the circulant matrix is replaced with block circulant matri-

ces [73]. more generally, our framework can handle general group structure, by replacing

the fft operation with the appropriate group fft [106]. by combining the advantages

of tensor methods with a general class of invariant representations, we thus have a pow-

erful paradigm for learning e   cient latent variable models and embeddings in a variety of

domains.

102

chapter 5

latent tree model learning through

hierarchical tensor decomposition

in previous chapters, we introduced id44 and its variations to model

data with    shallow    structure, for instance, multi-view model. however, real world data

is usually generated through more complicated models such as a latent (hierarchical) tree

graphical model. latent tree id114 characterize a id203 distribution involv-

ing observed and hidden variables which are markovian on a tree. learning is challenging

as the number of latent variables and the location of them are not observed. we present an

integrated approach to structure and parameter estimation in latent tree id114,

where some nodes are hidden.

we present an integrated approach to structure and parameter estimation in latent tree

models. our method overcomes all the above shortcomings simultaneously. first, it au-

tomatically learns the latent variables and their locations. second, our method achieves

consistent structure estimation with log(p) computational complexity with enough compu-

tational resources via    divide-and-conquer    manner. we also present a rigorous proof on the

103

replacements

=

+

+

(a) latent tree

(b) hierarchical tensor decomposition

=

+

+

=

+

+

=

+

+

figure 5.1: learning hierarchical latent variable graphical model parameter using hierarchical
tensor decomposition.

global consistency of the structure and parameter estimation under the    divide-and-conquer   

framework. our consistency guarantees are applicable to a broad class of linear multivari-

ate latent tree models including discrete distributions, continuous multivariate distributions

(e.g. gaussian), and mixed distributions such as gaussian mixtures. this model class is

much more general than discrete models, prevalent in most of the previous works on latent

tree models [128, 127, 59, 17]. third, our algorithm considers the inverse method of mo-

ments, and estimates the model parameters via tensor decomposition with low perturbation

guarantees. moreover, we carefully integrate structure learning with parameter estimation,

based on tensor spectral decompositions [11]. finally, our approach has a high degree of

parallelism, and is bulk asynchronous parallel [65].

in addition to the aforementioned technical contributions, we showcase the impact of our

work by applying it to two real datasets originating from the healthcare domain. the

algorithm was used to discover hidden patterns, or concepts re   ecting co-occurrences of

particular diagnoses in patients in outpatient and intensive care settings. while such a task

is currently done through manual analysis of the data, our method provides an automated

method for the discovery of novel clinical concepts from high dimensional, multi-modal data.

104

our overall approach follows a    divide-and-conquer    strategy that learns models over small

groups of variables and iteratively merges into a global solution. the structure learning

involves combinatorial operations such as minimum spanning tree construction and local re-

cursive grouping; the parameter learning is based on the method of moments and on tensor

decompositions. our method is guaranteed to correctly recover the unknown tree structure

and the model parameters with low sample complexity for the class of linear multivari-

ate latent tree models which includes discrete and gaussian distributions, and gaussian

mixtures. our bulk asynchronous parallel algorithm is implemented in parallel using the

openmp framework and scales logarithmically with the number of variables and linearly

with dimensionality of each variable.

our experiments con   rm a high degree of e   ciency and accuracy on large datasets of elec-

tronic health records. we use latent tree model for discovering a hierarchy among diseases

based on comorbidities exhibited in patients    health records, i.e. co-occurrences of diseases

in patients. in particular, two large healthcare datasets of 30k and 1.6m patients are used

to build the latent disease trees, where clinically meaningful disease clusters are identi   ed

as shown in    g 5.4 and 5.5. the proposed algorithm also generates intuitive and clinically

meaningful disease hierarchies.

5.1 latent tree graphical model preliminaries

we denote [n] := {1, . . . , n}. let t := (v,e) denote an undirected tree with vertex set v
and edge set e. the neighborhood of a node vi, nbd(vi), is the set of nodes to which vi is
directly connected on the tree. leaves which have a common neighboring node are known

as siblings, and the common node is referred to as their parent. let n denote the number

of samples. an example of latent tree is depicted in figure 5.2(a).

105

there are two types of variables on the nodes, namely, the observable variables, denoted

by x := {x1, . . . , xp} (p := |x|), and hidden variables, denoted by h := {h1, . . . , hm}
(m := |h|). let y := x     h denote the complete set of variables and let yi denote the
random variable at node vi     v, and similarly let ya denote the set of random variables in
set a.

a graphical model is de   ned as follows: given the neighborhood nbd(vi) of any node vi     v,
the variable yi is conditionally independent of the rest of the variables in v, i.e., yi    
yj|ynbd(vi),    vj     v\{vi     nbd(vi)}.

linear models we consider the class of linear latent tree models. the observed variables
xi are random vectors of length di, i.e., xi     rdi,    i     [p] while the latent nodes are k-state
categorical variables, i.e., hi     {e1, . . . , ek}, where ej     rk is the j th standard basis vector.
although di can vary across variables, we use d for notation simplicity. in other words, for
notation simplicity, xi     rd,    i     [p] is equivalent to xi     rdi,    i     [p]. for any variable yi
with neighboring hidden variable hj, we assume a linear relationship:

e[yi|hj] = ayi| hj

hj,

(5.1)

where transition matrix ayi| hj     rd  k is assumed to have full column rank,    yi, hj     v. this
implies that k     d, which is natural if we want to enforce a parsimonious model for    tting
the observed data.

for a pair of (observed or hidden) variables ya and yb, consider the pairwise correlation

matrix e(cid:2)yay   b(cid:3) where the expectation is over samples. since our model assumes that two

observable variables interact through at least a hidden variable, we have

e[yay   b ] :=xei

e[hj = ei]aya| hj=ei

a   
yb| hj =ei

(5.2)

106

we see that e[yay   b ] is of rank k since aya| hj=ei

or ayb| hj=ei

is of rank k.

5.2 overview of approach

figure 5.2: (a) ground truth latent tree to be estimated, numbers on edges are multivariate
information distances. (b) mst constructed using the multivariate information distances. v3 and
v5 are internal nodes (leaders). note that multivariate information distances are additive on latent
tree, not on mst. (c1) lcr on nbd[v3, mst] to get local structure n3. pink shadow denotes the
active set. local parameter estimation is carried out over triplets with joint node, such as (v2, v3,
v5) with joint node h1. (c2) lcr on nbd[v5, mst] to get local structure n5. cyan shadow denotes
the active set. (d1)(d2) merging local sub-trees. path(v3,v5; n3) and path(v3,v5; n5) con   ict.
(e) final recovery.

the overall approach is depicted in figure 5.2, where (a) and (b) show the id174

step, (c) - (e) illustrate the divide-and-conquer step for structure and parameter learning.

more speci   cally, we start with the parallel computation of pairwise multivariate information

distances. information distance roughly measures the extent of correlation between di   erent

pairs of observed variables and requires svd computations in step (a). then in step (b) a

minimum spanning tree (mst) is constructed over observable variables in parallel [24] using

the multivariate information distance. the local groups are also obtained through mst so

that they are available for the structure and parameter learning step that follows.

the structure and parameter learning is done jointly through a divide-and-conquer strategy.

step-(c) illustrates the divide step (or local learning), where local structure and parameter

estimation is performed. it also performs the local merge to obtain group level structure and

parameter estimates. after the local structure and parameter learning is    nished within the

107

groups, we perform merge operations among groups, again guided by the minimum spanning

tree structure. for the structure estimation it consists of a union operation of sub-trees;

for the parameter estimation, it consists of id202ic operations. since our method is

unsupervised, an alignment procedure of the hidden states is carried out which    nalizes the

global estimates of the tree structure and the parameters.

5.3 structure learning

structure learning in id114 involves    nding the underlying markov graph, given

the observed samples. for latent tree models, structure can be estimated via distance based

methods. this involves computing certain information distances between any pair of ob-

served variables, and then    nding a tree which    ts the computed distances.

multivariate information distances: we propose an additive distance for multivariate

linear latent tree models. for a pair of (observed or hidden) variables ya and yb, consider the

pairwise correlation matrix e(cid:2)yay   b(cid:3) (the expectation is over samples). note that its rank is

k, dimension of the hidden variables.

de   nition 5.1. the multivariate information distance between nodes i and j is de   ned as

dist(va, vb) :=     log

kqi=1

  i(cid:0)e(yay   b )(cid:1)

pdet(e(yay   a )) det(e(yby   b ))

where {  1(  ), . . . ,   k(  )} are the top k singular values.

(5.3)

note that de   nition 5.1 suggests that this multivariate information distance allows hetero-

geneous settings where the dimensions of ya and yb are di   erent (and     k).

108

for latent tree models, we can    nd information distances which are provably additive on the

underlying tree in expectation, i.e. the expected distance between any two nodes in the tree

is the sum of distances along the path between them.

lemma 5.1. the multivariate information distance is additive on the tree t , i.e., dist(va, vc)
= dist(va, vb) + dist(vb, vc), where vb is a node in the path from va to vc and va,vb,vc     v.

refer to appendix d.1 for proof. the empirical distances can be computed via rank-k svd

of the empirical pairwise moment matrix   e[yay   b ] note that the distances for all the pairs

can be computed in parallel.

formation of local groups via mst: once the empirical distances are computed, we

construct a minimum spanning tree (mst), based on those distances. note that the mst

can be computed e   ciently in parallel [156, 122]. we now form groups of observed variables

over which we carry out learning independently, without any coordination. these groups

are obtained by the (closed) neigborhoods in the mst, i.e. an internal node and its one-hop

neighbors form a group. the corresponding internal node is referred to as the group leader.

see figure 5.2(b).

local recursive grouping (lrg): once the groups are constructed via neighborhoods

of mst, we construct a sub-tree with hidden variables in each group (in parallel) using

the recursive grouping introduced in [41]. the recursive grouping uses the multivariate

information distances and decides the locations and numbers of hidden nodes.

it pro-

ceeds by deciding which nodes are siblings, which proceeds as follows: consider two ob-

served nodes vi, vj which are siblings on the tree with a common parent vl, and consider

any other observed node va. from additivity of the (expected) information distances, we

have dist(vi, va) = dist(vi, vl) + dist(vl, va) and similarly for dist(vj, va). thus, we have

  (vi, vj; va) := dist(vi, va)     dist(vj, va) = dist(vi, vl)     dist(vj, vl), which is independent
of node va. thus, comparing the quantity   (vi, vj; va) for di   erent nodes va allows us to

109

conclude that vi and vj are siblings. once the siblings are inferred, the hidden nodes are

introduced, and the same procedure repeats to construct the higher layers. note that when-

ever we introduce a new hidden node hnew as a parent, we need to estimate multivariate

information distance between hnew and nodes in active set    . this is discussed in [41] with

details.

we will describe the lrg in details with integrated parameters estimation in procudure 6

in section 5.5. in the end, we obtain a sub-tree over the local group of variables. after this

local recursive grouping test, we store the neighborhood relationship for the leader vi using

an adjacency list n i. we call the resultant local structure as latent sub-tree.

5.4 parameter estimation

along with the structure learning, we adopt a moment-based spectral learning technique for

parameter estimation. this is a guaranteed and fast approach to recover parameters via

moment matching for third order moments of the observed data. in contrast, traditional

approaches such as expectation maximization (em) su   er from spurious local optima and

cannot provably recover the parameters.

a latent tree with three leaves: we    rst consider an example of three observable leaves

x1, x2, x3 (i.e., a triplet) with a common hidden parent h. we then clarify how this can be

generalized to learn the parameters of the latent tree model. let     denote for the tensor
product. for example, if x1, x2, x3     rd, we have x1     x2     x3     rd  d  d.

property 5.1 (tensor decomposition for triplets). for a linear latent tree model with three

observed nodes v1, v2, v3 with joint hidden node h, we have

e(x1     x2     x3) =

kxr=1

p[h = er]ar

x1|h     ar

x2|h     ar

x3|h,

(5.4)

110

where ar

xi|h = e(xi|h = er), i.e., rth column of the transition matrices from h to xi. the
tensor decomposition method of [11] provably recovers the parameters axi|h,    i     [3], and
p[h].

tensor decomposition for learning latent tree models: we employ the above approach

for learning latent tree model parameters as follows: for every triplet of variables ya, yb, and

yc (hidden or observed), we consider the hidden variable hi which is the joining point of ya, yb

and yc on the tree. they form a triplet model, for which we employ the tensor decomposition

procedure. however, it is wasteful to do it over all the triplets in the latent tree.

in the next section, we demonstrate how we e   ciently estimate the parameters as we learn

the structure, and minimize the tensor decompositions required for estimation. issues such

as alignment of hidden labels across di   erent decompositions will also be addressed.

5.5

integrated structure and parameter estimation

so far, we described high-level procedures of structure estimation through local recursive

grouping (lrg) and parameter estimation through tensor decomposition over triplets of

variables, respectively. we now describe an integrated and e   cient approach which brings

all these ingredients together.

in addition, we provide merging steps to obtain a global

model, using the sub-trees and parameters learnt over local groups.

5.5.1 local recursive grouping with tensor decomposition

next we present an integrated procedure where the parameter estimation goes hand-in-hand

with structure estimation. intuitively, we    nd e   cient groups of triplets to carry out tensor

decomposition simultaneously, as we estimate the structure through recursive grouping. in

111

recursive grouping, pairs of nodes are recursively grouped as siblings or as parent-child. as

this process continues, we carry out tensor decompositions whenever there are siblings present

as triplets. if there are only a pair of siblings, we    nd an observed node with closest distance

to the pair. once the tensor decompositions are carried out on the observed nodes, we

proceed to structure and parameter estimation of the added hidden variables. the samples

of the hidden variables can be obtained via the posterior distribution, which is learnt earlier

through tensor decomposition. this allows us to predict information distances and third

order moments among the hidden variables as process continues. the full algorithm is given

in procedure 6.

procedure 6 lrg with parameter estimation
input: for each vi     xint, active set     := nbd[vi; mst].
output: for each vi     xint, local sub-tree adjacency matrix n i, and e[ya|yb] for all (va, vb)    
1: active set         nbd[vi; mst]
2: while |   | > 2 do

n i.

for all va, vb         do

if   (va, vb; vc) = dist(va, vb),     vc        \{va, vb} then

va is a leaf node and vb is its parent,
eliminate va from    .

if    dist(va, vb) <   (va, vb; vc) =   (va, vb; v   c) < dist(va, vb),   vc, v   c        \{va, vb}
then

va and vb are siblings,eliminate va and vb from    , add hnew to    .
introduce new hidden node hnew as parent of va and vb.
if more than 3 siblings under hnew then

   nd vc in siblings,

else

   nd vc = arg minvc       dist(va, vc).

estimate empirical third order moments be(ya     yb     yc)
decompose be(ya     yb     yc) to get pr[hnew] and e(yr|hnew),    r = {a, b, c}.

3:
4:
5:
6:
7:

8:
9:
10:
11:
12:
13:

14:

15:

the divide-and-conquer local spectral parameter estimation is superior compared to pop-

ular em-based method [41], which is slow and prone to local optima. more importantly,

em can only be applied on a stable structure since it is a global update procedure. our

proposed spectral learning method, in contrast, is applied locally over small groups of vari-

ables, and is a guaranteed learning with su   cient number of samples [11]. moreover, since

112

we integrate structure and parameter learning, we avoid recomputing the same quantities,

e.g. svd computations are required both for structure estimation (for computing distances)

and parameter estimation (for whitening the tensor). combining these operations results in

huge computational savings (see section 5.6 for the exact computational complexity of our

method).

procedure 7 merging and alignment correction (mac)
input: latent sub-trees n i for all internal nodes i.
output: global latent tree t structure and parameters.
1: for n i and n j in all the sub-trees do

if there are common nodes between n i and n j then

2:
3:

4:

5:
6:

find the shortest path path(vi, vj;n i) between vi and vj on n i and path(vi, vj;n j)
in n j;
union the only con   icting path(vi, vj;n i) and path(vi, vj;n j) according to equa-
tion (5.7) ;
attach other nodes in n i and n j to the union path;
perform alignment correction as described in procedure 8.

5.5.2 merging and alignment correction

we have so far learnt sub-trees and parameters over local groups of variables, where the

groups are determined by the neighborhoods of the mst. the challenge now is to combine

them to obtain a globally consistent estimate. there are non-trivial obstacles to achieving

this:    rst, the constructed local sub-trees span overlapping groups of observed nodes, and

possess con   icting paths. second, local parameters need to be re-aligned as we merge the

subtrees to obtain globally consistent estimates due to the nature of unsupervised learning.

to be precise, di   erent tensor decompositions lead to permutation of the hidden labels (i.e.

columns of the transition matrices) across triplets. thus, we need to    nd the permutation

matrix correcting the alignment of hidden states of the transition matrices, so as to guarantee

global consistency.

113

structure union: we now describe the procedure to merge the local structures. we merge

them in pairs to obtain the    nal global latent tree. recall that n i denotes a sub-tree
constructed locally over a group, whose leader is node vi. consider a pair of subtrees n i
and n j, whose group leaders vi and vj are neighbors on the mst. since vi and vj are
neighbors, both the sub-trees contain them, and have di   erent paths between them (with

hidden variables added). moreover, note that this is the only con   icting path in the two
subtrees. we now describe how we can resolve this: in n i, let hi
node for vi and hi
2. similarly, in n i, let hj
hi
between vi and vj in the two sub-trees are given as follows:

1 and
2 be the corresponding nodes in n j. the shortest path

2 be the neighbor of vj. there could be more hidden nodes between hi

1 be the neighboring hidden

1 and hj

path(vi, vj;n i) := [vi     hi
path(vi, vj;n j) := [vi     hj

1     . . .     hi
1     . . .     hj

2     vj]
2     vj]

then the union path is formed as follows:

merge(path(vi, vj;n i), path(vi, vj;n j))
1 . . . hj

:= [vi     hi

1     . . .     hi

2     hj

2     vj]

(5.5)

(5.6)

(5.7)

in other words, we retain the immediate hidden neighbor of each group leader, and break

the paths on the other end. for example in figure 5.2(d1,d2), we have the path v3     h1     v5
in n 3 and path v3     h3     h2     v5 in n 5. the resulting path is v3     h1     h3     h2     v5, as
see in figure 5.2(e). after the union of the con   icting paths, the other nodes are attached

to the resultant latent tree. we present the pseudo code in procedure 7 in appendix d.5.

parameter alignment correction: as mentioned before, our parameter estimation is

unsupervised, and therefore, columns of the estimated transition matrices may be permuted

for di   erent triplets over which tensor decomposition is carried out. note that the parameter

114

procedure 8 parameter alignment correction
(gr denotes reference group, go denotes the list of other groups, each group has a reference
node denoted as rl, and the reference node in gr is rg. the details on alignment at line 8
is in appendix d.5.)

input: triplets and unaligned parameters estimated for these triplets, denoted as

trip(yi, yj, yk).

output: aligned parameters for the entire latent tree t .
1: select gr which has su   cient children;
2: select refer node rg in gr;
3: for all a, b in gr do
4: align tripin(ya, yb,rg);
5: for all ig in go do
6:

7: align tripout(rg, ya,rl) and tripout(rl, yi,rg);

select refer node rl in go[ig];
for all i, j in go[ig] do
align trip(yi, yj,rl);

8:
9:

estimation within the triplet is automatically acquired through the tensor decomposition

technique, so that the alignment issue only arises across triplets. we refer to this as the

alignment issue and it is required at various levels.

there are two types of triplets, namely, in-group and out-group triplets. a triplet of nodes

trip(yi, yj, yl) is said to be in-group (denoted by tripin(yi, yj, yl) ) if its containing nodes

share a joint node hk and there are no other hidden nodes in path(yi, hk), path(yj, hk) or

path(yl, hk). otherwise, this triplet is out-group denoted by tripout(yi, yj, yl). we de   ne a

group as su   cient children group if it contains at least three in-group nodes.

designing an in-group alignment correction with su   cient children is relatively simple: we

achieve this by including a local reference node for all the in-group triplets. thus, all the

triplets are aligned with the reference node. the alignment correction is more challenging if

lacking su   cient children. we propose out-group alignment to solve this problem. we    rst

assign one group as a reference group, and the local reference node in that reference group

becomes the global reference node. in this way, we align all recovered transition matrices

115

in the same order of hidden states as in the reference node. overall, we merge the local

structures and align the parameters from lrg local sub-trees using procedure 7 and 8.

5.6 theoretical gaurantees

correctness of proposed parallel algorithm: we now provide the main result of this

chapter on global consistency for our method, despite the high degree of parallelism.

theorem 5.1. given samples from an identi   able latent tree model, the proposed method

consistently recovers the structure with o(log p) sample complexity and parameters with

o(poly p) sample complexity.

the proof sketch is in appendix d.3.

computational complexity: we recall some notations here: d is the observable node

dimension, k is the hidden node dimension (k     d), n is the number of samples, p is the
number of observable nodes, and z is the number of non-zero elements in each sample.

let    denote the maximum size of the groups, over which we operate the local recursive

grouping procedure. thus,    a   ects the degree of parallelism for our method. recall that

it is given by the neighborhoods on mst, i.e.,    := maxi|nbd[i; mst]|. below, we provide a
bound on   .

lemma 5.2. the maximum size of neighborhoods on mst, denoted as   , satis   es

          1+

ud
ld

  ,

(5.8)

where    := maxi{minj{path(vi, vj;t )}} is the e   ective depth,     is the maximum degree of t ,
and the ud and ld are the upper and lower bound of information distances between neighbors

on t .

116

thus, we see that for many natural cases, where the degree and the depth in the latent tree

are bounded (e.g. the hidden markov model), and the parameters are mostly homogeneous

(i.e., ud/ld is small), the group sizes are bounded, leading to a high degree of parallelism.

we summarize the computational complexity in table 5.1. details can be found in ap-

pendix d.6.

algorithm steps time per worker degree of parallelism
distance est.
mst
lrg
tensor decomp. o(  k3 +   dk2)
merging step

o(nz + d + k3) o(p2)
o(p2)
o(log p)
o(  3)
o(p/  )
o(p/  )
o(p/  )

o(dk2)

table 5.1: worst-case computational complexity of our algorithm. the total complexity is
the product of the time per work and degree of parallelism.

5.7 experiments

setup experiments are conducted on a server running the red hat enterprise 6.6 with 64

amd opteron processors and 265 gbram. the program is written in c++, coupled with

the multi-threading capabilities of the openmp environment [52] (version 1.8.1). we use

the eigen toolkit1 where blas operations are incorporated. for svds of large matrices, we

use randomized projection methods [66] as described in appendix d.8.

healthcare data analysis the goal of our analysis is to discover a disease hierarchy based

on their co-occurring relationships in the patient records. in general, longitudinal patient

records store the diagnosed diseases on patients over time, where the diseases are encoded

with international classi   cation of diseases (icd) code.

1

http://eigen.tuxfamily.org/index.php?title=main_page

117

data description we used two large patient datasets of di   erent sizes with respect to the

number of samples, variables and dimensionality.

(1) mimic2: the mimic2 dataset record disease history of 29,862 patients where a overall

of 314,647 diagnostic events over time representing 5675 diseases are logged. we consider

patients as samples and groups of diseases as variables. we analyze and compare the results

by varying the group size (therefore varying d and p).

(2) cms: the cms dataset includes 1.6 million patients, for whom 15.8 million medical

encounter events are logged. across all events, 11,434 distinct diseases (represented by icd

codes) are logged. we consider patients as samples and groups of diseases as variables. we

consider speci   c diseases within each group as dimensions. we analyze and compare the

results by varying the group size (therefore varying d and p). while the mimic2 dataset

and cms dataset both contain logged diagnostic events, the larger volume of data in cms

provides an opportunity for testing the algorithm   s scalability. we qualitatively evaluate

biological implications on mimic2 and quantitatively evaluate algorithm performance and

scalability on cms.

to learn the disease hierarchy from data, we also leverage some existing domain knowledge

about diseases. in particular, we use an existing mapping between icd codes and higher-

level phenome-wide association study (phewas) codes [54]. we use (about 200) phewas

codes as observed nodes and the observed node dimension is set to be binary (d = 2) or the

maximum number of icd codes within a phewas code (d = 31). the goal is to learn the

latent nodes and the disease hierarchy and associated parameters from data.

5.7.1 validation

we conduct both quantitative and qualitative validation of the resulting disease hierarchy.

118

(a) running time vs number of samples

(b) running time vs number of nodes

(c) speed-up vs available threads

  104

)
s
d
n
o
c
e
s
(

e
m

i
t

g
n

i

n
n
u
r

8,000

6,000

4,000

2,000

0

0

0.5
number of samples

1

1.5
  106

)
s
d
n
o
c
e
s
(

e
m

i
t

g
n

i

n
n
u
r

1.5

1

0.5

0

r
o
t
c
a
f

p
u
-
d
e
e
p
s

60

40

20

0

0

method speed-up
ideal speed-up

20

40

60

number of threads

0 200 400 600 8001,000
number of observed nodes

figure 5.3: (a) cms dataset sub-sampling w.r.t. varying number of samples. (b) mimic2
dataset sub-sampling w.r.t. varying number of observed nodes. each one of the observed
nodes is binary (d = 2). (c) mimic2 dataset: scaling w.r.t. varying computational power,
establishing the scalability of our method even in the large p regime. the number of observed
nodes is 1083 and each one of them is binary (p = 1083, d = 2).

quantitative analysis we    rst compare our resulting hierarchy with a ground truth tree

based on medical knowledge2. the standard robinson foulds (rf) metric [140](between

our estimated latent tree and the ground truth tree) is computed to evaluate the structure

recovery in table 5.2. the smaller the metric is, the better the recovered tree is. we also

compare our results with a baseline: the agglomerative id91. the proposed method are

slightly better than the baseline and the advantage is increased with more nodes. however,

the proposed method provides an e   cient probabilistic graphical model that can support

general id136 which is beyond the baseline.

data

mimic2

cms

mimic2

p
163
168
952

rf(agglo.) rf(proposed)

0.0061
0.0060
0.0060

0.0061
0.0059
0.0011

table 5.2: robinson foulds (rf) metric compared with the    ground-truth    tree for both
mimic2 and cms dataset. our proposed results are better as we increase the number of
nodes.

qualitative analysis the qualitative analysis is done by a senior md-phd student in our

team.

2the ground truth tree is the phewas hierarchy provided in the clinical study [54]

119

(a) case d=2: here we report the results from the 2-dimensional case (i.e., observed variable

is binary). in    gure 5.4, we show a portion of the learned tree using the mimic2 healthcare

figure 5.4: an example of two subtrees which represent groups of similar diseases which
may commonly co-occur. nodes colored yellow are latent nodes from learned subtrees.

data. the yellow nodes are latent nodes from the learned subtrees while the blue nodes

represent observed nodes(diagnosis codes) in the original dataset. diagnoses that are similar

were generally grouped together. for example, many neoplastic diseases were grouped under

the same latent node (node 1135). while some dissimilar diseases were grouped together,

there usually exists a known or plausible association of the diseases in the clinical setting.

for example, in    gure 5.4, clotting-related diseases and altered mental status were grouped

under the same latent node as several neoplasms. this may re   ect the fact that altered

mental status and clotting conditions such as thrombophlebitis can occur as complications

of neoplastic diseases [61]. the association of malignant neoplasms of prostate and colon

polyps, two common cancers in males, is captured under latent node 1136 [74].

120

figure 5.5: an example of four subtrees which represent groups of similar diseases which
may commonly co-occur. most variables in this subtree are related to trauma.

(b) case d =31: we also learn a tree from the mimic2 dataset, in which we grouped

diseases into 163 phewas codes and up to 31 dimensions per variable. figure 5.5 shows

a portion of the learned tree of four subtrees which all re   ect similar diseases relating to

trauma. a majority of the learned subtrees re   ected clinically meaningful concepts, in that

related and commonly co-occurring diseases tended to group together in the same subtrees

or in nearby subtrees. we also learn the disease tree from the larger cms dataset, in which

we group diseases into 168 variables and up to 31 dimensions per variable. similar to the

case from the mimic2 dataset, a majority of learned subtrees re   ected clinically meaningful

concepts.

for both the mimic2 and cms datasets, we performed a qualitative comparison of the

resulting trees while varying the hidden dimension k for the algorithm. the resulting trees

for di   erent values of k did not exhibit signi   cant di   erences. this implies that our algorithm

is robust with di   erent choices of hidden dimensions. the estimated model parameters are

also robust for di   erent values of k based on the results.

121

scalability our algorithm is scalable w.r.t. varying characteristics of the input data. first,

it can handle a large number of patients e   ciently, as shown in figure 5.3(a). it has also

a linear scaling behavior as we vary the number observed nodes, as shown in figure 5.3(b).

furthermore, even in cases where the number of observed variables is large, our method

maintains an almost linear scale-up as we vary the computational power available, as shown

in figure 5.3(c). as such, by providing the respective resources, our algorithm is practical

under any variation of the input data characteristics.

5.8 conclusion

we present an integrated approach to structure and parameter estimation in latent tree

models. our method overcomes challenges such as uncertainty of location and number

of hidden variables, problem of local optima with no consistency guarantees, di   culty in

scalability with respect to number of variables. the proposed algorithm is ideal for parallel

computing and highly scalable. we successfully applied the algorithm to a real application

for disease hierarchy discovery using large patient data for 1.6m patients.

122

chapter 6

discovering cell types with spatial

point process mixture model

cataloging the neuronal cell types that comprise circuitry of individual brain regions is a

major goal of modern neuroscience and the brain initiative. single-cell rna sequencing

can now be used to measure the gene expression pro   les of individual neurons and to cate-

gorize neurons based on their gene expression pro   les. while the single-cell techniques are

extremely powerful and hold great promise, they are currently still labor intensive, have a

high cost per cell, and, most importantly, do not provide information on spatial distribution

of cell types in speci   c regions of the brain. we propose a complementary approach that

uses computational methods to infer the cell types and their gene expression pro   les through

analysis of brain-wide single-cell resolution in situ hybridization (ish) imagery contained in

the allen brain atlas (aba). we measure the spatial distribution of neurons labeled in the

ish image for each gene and model it as a spatial point process mixture, whose mixture

weights are given by the cell types which express that gene. by    tting a point process mix-

ture model jointly to the ish images, we infer both the spatial point process distribution

for each cell type and their gene expression pro   le. we validate our predictions of cell type-

123

speci   c gene expression pro   les using single cell rna sequencing data, recently published

for the mouse somatosensory cortex. jointly with the gene expression pro   les, cell features

such as cell size, orientation, intensity and local density level are inferred per cell type. this

work brings together the techniques used in all previous chapters, such as image processing

to extract cells and cell features from brain slices, learning a point process admixture model.

6.1

introduction

6.1.1 motivations and goals

the human brain comprises about one hundred billion neurons and one trillion supporting

glial cells. these cells are specialized into a surprising diversity of cell types. the retina

alone boasts well over 50 cell types, and it is an active area of research to perform a census of

the various neuronal cell types that comprise the central nervous system. many criteria have

been used to categorize neuronal cell types, from neuronal morphology and connectivity to

their functional response properties. neurons can also be categorized based on the proteins

they make.

immunohistochemistry has been used with great success for many decades

to di   erentiate excitatory neurons from inhibitory neurons by labeling for known proteins

involved in the synthesis and regulation of glutamate and gaba, the primary excitatory

and inhibitory neurotransmitters respectively.

more recently, there has been an e   ort to systematically measure the complete transcriptome

of single neurons. single-cell rna sequencing (rna-seq) is an extremely powerful technique

that can quantitatively determine the expression level of every gene that is expressed in in-

dividual neurons. this so-called transcriptome or gene expression / transcription pro   le can

then be used to de   ne cell types by id91. a recent study produced the most compre-

hensive census of cell types to date in the mouse somatosensory cortex and hippocampus

124

by performing single-cell rna-seq on over 3000 neurons [168]. while this study is quite

exciting, tyring to replicate it for all brain regions might well require the equivalent of a

thousand such experiments. thus, it is likely that the unprecedented insights that rna-seq

can provide will be slow to arrive. more importantly, single cell sequencing methods are not

currently able to capture the precise three-dimensional location of the individual neurons.

here we propose a complementary approach that uses computational strategies to identify

cell types and their spatial distribution by re-analysing data published by the allen institute

for brain research. the allen brain atlas (aba) contains cellular resolution brain-wide in-

situ hybridization (ish) images for 20,000 genes1. ish is a histological technique that labels

the mrna in all cells expressing the corresponding gene in a manner roughly proportion to

the gene expression level. an example of an ish image can be seen in    gure 6.1(a).

the aba contains genome-wide and brain-wide ish images of the adult mouse brain. these

images were generated by slicing the brain into a series of 25   m thin sections and performing

ish. image series of ish performed for di   erent genes come from di   erent mouse brains,

since ish can only be performed for one gene at a time. the ish image series for di   erent

genes were then computational aligned into a common reference brain coordinate system.

such data have been productively used to infer the average transcriptomes corresponding to

di   erent brain regions.

it is commonly thought that the aba cannot be used to infer the transcriptomes of individual

cells in a given brain region since mouse brains cannot be aligned to the precision of a single

cell. this is because there is individual variation in the precise number and location of

neurons from brain to brain. however, we expect that the average number and spatial

distribution of neurons from each cell type to be conserved from brain to brain, for a given

brain area. more concretely, we might expect that parvalbumin-expressing (pv) inhibitory

1 although the atlas contains ish data for approximately 20,000 distinct mouse genes, we focus on the

top 1743 reliable genes whose sagittal and coronal experiments are highly correlated.

125

interneurons in layer 2/3 of the mouse somatosensory cortex comprise approximately 7% of

all neurons and have a conserved spatial and size distribution from brain to brain. we use

this fact to derive a method for simultaneously inferring the cell types in a given brain region

and their gene expression pro   les from the aba.

we propose to model the spatial distribution of neurons in a brain as being generated by

sampling from an unknown but consistent brain-region and cell-type dependent spatial point

process distribution. and since each gene might only be expressed in a subset of cell types,

an ish image for a single gene can be thought of as a mixture of spatial point processes where

the mixture weights represent the individual cell types expressing that gene. we infer cell

types, their gene expression pro   les and their spatial distribution by unmixing the spatial

point processes corresponding to the ish images for 1743 genes. this is in notable contrast

to the information provided by single-cell rna sequencing which can only measure the

gene expression pro   le of individual cells to high accuracy but where, due to the destructive

measurement process, all information about the spatial position and distribution of cell types

is lost.

6.1.2 previous work

allen brain atlas (aba) [115] is a landmark study which mapped the gene expression of

about 20,000 genes across the entire mouse brain. the aba dataset consists of cellular

high-resolution 2d imagery of in-situ hybridized series of brain sections, digitally aligned

to a common reference atlas. however, since the in-situ images for each gene come from

di   erent mouse brains and since there is signi   cant variability in the individual locations of

labeled cells, it is not possible to register brain-wide gene expression at a resolution higher

than about 250  m. therefore, the cellular resolution detail was down-sampled to construct

126

a coarser 3d representation of the average gene expression level in 250  m   250  m   250  m
voxels.

the coarse-resolution averaged gene expression representation has been widely used and an-

alyzed to understand di   erences in gene expression at the level of brain region. hawrylycz

et al [79] analyzed the correlational structure of gene expression at this scale, across the

entire mouse brain. however, due to the poor resolution of the average gene expression rep-

resentation, it has proven challenging to use the aba to discover the microstructure of gene

expression within a brain region. to address this issue from a complementary perspective,

grange et al [72] used the gene expression pro   les of 64 known cell-types, combined with

linear unmixing to determine the spatial distribution of these known cell-types. however,

such an approach can be confounded by the presence of cell-types whose expression pro   les

have yet to be characterized, and limited by the resolution of the averaged gene expression

representation.

in contrast to previous approaches, we aim to solve the di   cult problem of automatically

discovering the gene expression pro   les of cell-types within a brain region by analyzing the

original cellular resolution ish imagery. we propose to use the spatial distributions of

labeled cells, and their shapes and sizes, which are a far richer representation than simply

the average expression level in 250  m    250  m    250  m voxels. this spatial point process
is then un-mixed to determine the gene expression pro   le of cell types.

most previous work on unmixing point process mixtures adopted parametric generative

models where the point process is limited to some distribution family such as poisson or

gaussian [95, 107]. however, since we are not interested in building a generative model of a

point process, but rather care more about inferring the mixing proportions (gene expression

pro   le), we take a simpler parameter-free approach. this approach models only the statistics

of the point process, but is not a generative model, and so cannot be use to model individual

points/cells.

127

extract point process:

 

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

 

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

(a) patch from gene pvalb slice

(b) cell detection and extraction of

spatial point process features

joint histogram:

pvalb
rasgrf2

(c1) size

  /6 2  /6 3  /6 4  /6 5  /6

(c2) orientation

(c3) expression level

(c4) cell counts in 100   m radius

discover cell types:

(d) point process histogram

representation: [xm

n ]     r+

ng  nf

(f) lda model for inferring cell types

figure 6.1: overview of the proposed framework - discovering neuronal cell types via un-
mixing of spatial point process mixtures. (a) & (b) an in situ hybridization image for gene
pvalb along with detected cells. (c) marginalized point process feature histograms for genes
pvalb and rasgrf2. note that size denotes the principal axis diameter. we have ng genes
and 4d joint histogram with nf bins.

128

6.2 modeling the spatial distribution of cell-types us-

ing spatial point process features

most analyses of the aba in situ hybridization dataset have utilized a simple measure of

average expression level in relatively large 250  m    250  m    250  m voxels of brain tissue.
due to the large volume over which the expression level is averaged, such a representation

cannot distinguish between large numbers of cells expressing small amounts of rna vs.

small numbers of cells expressing large amounts of rna. all information about the spatial

organization of labeled cells, their shapes, sizes and spatial density are lost and summarized

by a single scalar number. here, we describe a more sophisticated representation of the

labeled cells in an ish image based on marked spatial point processes.

6.2.1 the marked spatial point process representation of ish

images

our approach requires processing the high-resolution ish images to detect individual labeled

cells and their visual characteristics. we developed a cell detection algorithm described in

the supplementary section. our algorithm additionally also estimates the expression level of

each detected cell, its shape, size and orientation. figure 6.1(a) and figure 6.1(b) illustrate

the results of our cell detection algorithm.

since cell-types di   er not only in terms of gene expression pattern, but also display a di-

versity of shapes, sizes and spatial densities, we sought to characterize these properties. we

measured: (1) cell size s = [r1, r2]: the radius in two principal directions of an ellipse    t to

each cell; (2) cell orientation o: the orientation of the    rst principle axis of the ellipse; (3)

gene intensity level p: intensity of labeling of a cell relative to the image background; (4)

129

spatial distribution c: the number of cells within a local area centered around the cell,

which can be regarded as a measure of the local cell density.

the collection of detected cells within an atlas-de   ned brain region, along with their features,

constitutes a marked spatial point process. this point process is considered    marked   ,

because each point is characterized by the shape, size, expression level and local density

features, in addition to just their location in space.

6.2.2 a model-free approach to representing spatial point pro-

cesses using joint feature histograms

the statistical modeling of repulsive spatial point processes such as those that arise in biology

is non-trivial, and many generative models such as determinantal point processes [110]and

matern point processes have high computational complexity. but since we are not interested

in directly modeling the individual labeled cells, but instead in modeling only their aggre-

gate spatial statistics, and in inferring their gene expression pro   les, we can take a simpler

approach.

we use a joint histogram simple statistics of the collection of detected cells to characterize the

underlying point process from which they are drawn. this is an empirical moment approach

which side-steps the need to carefully de   ne a generative point process distribution.

as we describe in the next section, we propose to model the point process measured from the

ish image for each gene as a mixture of point processes belonging to individual cell-types.

for this, we use a linear mixing model, the id44 model. the use of

this model is greatly simpli   ed if we carefully choose our feature representation such that

the linear mixture of point processes results in a linear mixture of histogram statistics. this

is clearly the case for the features we have chosen. for instance, if we sample equally from

130

two point process distributions p1 and p2 with average densities of d1 and d2, the addition of

these two point processes p = p1 + p2 results in the addition of the two densities d = d1 + d2.

this is not the case for second order features, such as the distances to the nearest neighbors,

which would have a more nonlinear relationship.

in    gure 6.1(c), we display marginal histograms corresponding to the joint histogram for two

genes, pvalb and rasgrf2, which are well-known markers for a speci   c class of inhibitory and

excitatory cortical neuronal cell-types respectively.

6.3 un-mixing spatial point processes to discover cell-

types

6.3.1 generative model: a variation of latent dirichlet alloca-

tion

the spatial point process histogram representation of the aba ish dataset results, for each
brain region, is an nf    ng matrix [xm
(henceforward called the number of histogram features) 2, ng is the number of genes, and

n ], where nf is the total number of histogram bins

xm
n is the number of cells expressing gene n in histogram bin m.

we model the gene-spatial histogram matrix [xm

n ] by assuming it is generated by a variation

of id44 (vlda) [32] model of cell types. this id105

based latent variable model assumes that the ish histograms are generated from a small

number of cell-types, k, and each cell-type i is associated with a type-dependent spatial

point process histogram hi and a gene expression pro   le   i.

2note that there are two types of features     the features characterizing each detected cell, and the features

characterizing the collection of detected cells that constitute a single sample from a spatial point process

131

our generative model for each histogram bin m (characterizing a particular bin in the size/

orientation/ gene pro   le/ spatial distribution) is as follows: let lm = png

detected number of cells in the joint histogram bin m. for each cell l in this bin, its cell-type

n be the

n xm

t is sampled from the multinomial distribution hm. and given the cell-type t of cell l, the

genes n expressed by this cell are sampled from a multinomial distribution given by the

type-dependent gene expression pro   le/distribution   t. for a given gene n and histogram

bin m, this generative process determines the number of cells that would be detected xm
n .

we further place a dirichlet prior over hm     dir(  ), with the concentration parameter
   which determines the prior id203 over the number of cell-types present in a given

histogram bin m. this prior represents our prior knowledge of how many cell-types express

each gene, and also how well our feature representation separates cells of di   erent types into

di   erent histogram bins. in principle, we could generalize this to be a gene-speci   c prior, if

we had such information available. we could also use    to incorporate information about our

prior knowledge over the distribution of cells from each cell-type, for instance that excitatory

neurons greatly outnumber inhibitory neurons in a roughly 5 : 1 ratio.

we now describe how we estimate the model parameters     the cell-type speci   c multinomial

gene expression pro   le    and the cell-type speci   c spatial point process histogram h from

the gene-speci   c spatial point process histograms measured from the ish images.

6.3.2 estimating the cell-type dependent gene expression pro-

   le   

after testing several estimation methods for the parameters of our model, we found that

non-negative id105 (nmf) performed well in estimating the cell-type speci   c

132

gene expression pro   les   , see figure 6.2a. we solve the following optimization problem:

min
  ,h

nfxm

ngxn

(xm

n    

kxt

hm
t   t

nlm)2,

s.t.   t

n     0,

ngxn

  t
n = 1, hm

t     0,

kxt

hm
t = 1 (6.1)

here, the non-negativity and sum-to-one constraints on hm

t and   t

n ensure that h and    result

in properly normalized multinomial distributions. while this estimation procedure results

in joint estimates for h and   , it does not enforce the dirichlet prior over h. so we re   ne

our nmf-derived estimates for h using variational id136 [32].

6.3.3 estimating the cell-type dependent spatial point process

histogram h

we use a standard id113 procedure for h [32]. iteratively, we re   ne
the id136 of the cell type membership hm        k under each joint histogram feature m.
we update hm

i until convergence [148].

hm
i    

1

lm +pk

t   t

xm
n

ngxn=1

hm
i   i
n

hm
l   l
n

kpl=1

+   i,    i     [k], m     [nf ]

(6.2)

recall that the dirichlet prior    encodes the number of cell-types that we expect on average

to express each gene. we set    to be a symmetric dirichlet with   1 =   2 = . . . =   k, and

pt   t = 0.01 for all cell-types t. in practice, we observe that our estimates of h are fairly
insensitive to the speci   c choice for    as long aspt   t is small enough. the smaller    is,

the fewer cell-types expressing a given gene we expect to observe in a single histogram bin.

133

6.4 results and evaluation

6.4.1

implementation details

we tested our proposed cell-type discovery algorithm using the high-resolution in situ hy-

bridization image series for 1743 of the most reliably imaged and annotated genes in the

aba. individual cells were detected in the cellular resolution ish images using custom al-

gorithms (detailed in supplementary information). for each detected cell, we    t ellipses

and extract several local features: (a) size and shape represented as the diameters along the

principle axes of the ellipse, (b) orientation of the    rst principle axis, (c) gene intensity level

as measured by the intensity of labeling of the cell body, and (d) the number of cells detected

with-in a 100   m radius around the cell, which is a measure of the local cell density. we

aligned the ish images to the aba reference atlas and, for this paper, focused our attention

on cells in the somatosensory cortex, since independent rna-seq data exist for this region

the can be used to evaluate our approach. we computed joint histograms for the collection

of cells found with-in the somatosensory cortex, resulting in a spatial point process feature

vector of nf = 10010 histogram bins per gene.

synthetic experiment: the vlda model we proposed is then    t to ng    nf gene point
process histogram matrix to estimate the cell-type gene expression pro   le matrix    using the

non-negative id105 (nnmf) algorithm. the reason why we choose nnmf

over variational id136 (which is a popular approach for lda) for    estimation is that

nnmf produces more accurate    estimation in simulated data, illustrated in fig 6.2a. in

the synthetic experiment, we simulate point process data ( with some prede   ned golden

standard   ) and use the data to estimate b  . the errors were computed after pairing the

estimated columns of    with a closest golden standard    column via hypothesis testing. note

that the columns of    are normalized to 1, so the errors are bounded.

134

permute   
vi estimated   
nnmf estimated   
nnmf robust

e
p
y
t
r
e
p

r
o
r
r
e

0.2

0.15

0.1

0.05

0

3 10

20

30

40

50

60

70

80

90 100

number of cell types

(a) validate nnmf method

k

2  

2.0

1  

1.0

0  

spatial point process  (ours)

(

permuted

)

average expression level (baseline)

(

permuted

)

(b) validate point process data

figure 6.2: (a) synthetic experiment : comparison of non-negative id105
(nnmf) with variational id136 (vi) on simulated point process cell data using known
gene expression pro   le   . an additional robustness test of nnmf is done to see how good
the algorithm is when a wrong number of cell types k is input. a permutation test (shu   ing
the gene expression levels between cell) is done to access statistical signi   cance. comparing
with permute test shows that our cell-types are signi   cantly di   erent from chance. error per

type is computed by pairing the columns of estimatedb   with the columns of the ground-truth

  .
(b) comparison of gene expression pro   les recovered for cell-types in the somatosensory
cortex by    tting an lda model using spatial point process features (ours) vs the standard
average gene expression level feature (baseline). our features provide a signi   cantly better
match, with lower perplexity, to ground truth single-cell rna sequencing derived transcrip-
tomes. a permutation test is done to access statistical signi   cance. perplexity is computed
by matching to surrogate single-cell rna transcriptomes by shu   ing the gene expression
levels between cells. comparing with permute test shows that our cell-types are signi   cantly
di   erent from chance.

6.4.2 evaluating cell-type gene expression pro   le predictions

a recent study performed single-cell rna sequencing on 1691 neurons isolated from mouse

somatosensory cortex. we use this dataset to evaluate the quality of the cell-types we

discover.

the single cell rna-seq data, g := [g1|g2| . . .|gnc ]     rng  nc , contains the gene expression
pro   les for nc = 1691 cells. we infer the cell types hi for these cells using equation (6.2), and

then compute the likelihood li of observing each for each cell under our estimated cell-type

dependent gene expression pro   le matrix    using equation (6.4). we can then evaluate the

135

perplexity, a commonly used measure of goodness of    t under the vlda model, of single cell

rna-seq data on the model we learned from our spatial point process data.

the perplexity score is a standard metric, which is de   ned as the geometric mean per-cell

likelihood. it is a monotonically decreasing function of the log-likelihood l(g) of test data
g.

perplexity(g) = exp(   pnc
pnc

i=1 log p(gi)

)

i=1 li

where the likelihood is evaluated as

p(gm|hm,   ,   ) =

   (pi   i)
qi    (  i)

kyi=1

(hm

i )  i   1

lmyj=1  kxi=1

ngxn=1

  gm

j ,enhm

i   i

n! .

(6.3)

(6.4)

where   i,j is the kronecker delta,   i,j = 1 when i = j and 0 otherwise. en is the nth basis

vector.

6.4.3 comparison to standard average gene expression features

baseline and a permutation test for signi   cance

here we demonstrate the superiority of our method and its statistical signi   cance in two

ways. first we compared the perplexity of the single-cell rna seq dataset g under our

model (   gure 6.2b, solid blue) against the perplexity of a surrogate dataset with the same

marginal statistics, but whose gene-cell correlations were destroyed (   gure 6.2b, dashed blue).

we generated this surrogate dataset by randomly permuting the gene expression levels for

each gene across cells. this permuted dataset had a signi   cantly higher (worse) perplexity

than the true single-cell dataset. this demonstrates that our model trained to un-mix the

ish-derived spatial point processes discovered cell-types whose gene expression pro   les are

signi   cantly better match to single-cells than by chance.

136

gad1

sp8

tox3

nkx2-1

lhx6

pax6

dlx5

arx

dlx2

dlx1

elavl2

sp9

tbr1

foxp2

tshz2

stat4

ascl1

cux2

neurod1

mef2c

ptrf

cldn5

maf

hcls1

spi1

myb

fhl1

aldoc

sall3

sox21

mbp

etv6

sox10

st18

olig2

m ural

interneurons
oligoden drocytes

s1

p ara m idal
a strocytes
e pen dy m al

m icroglia
e n dothelial

m ural

interneurons
oligoden drocytes

s1 p yra m idal
a strocytes
e pen dy m al

m icroglia
e n dothelial

figure 6.3: estimated memberships    on marker genes for 8 cell types. these marker genes
are used to label the columns of the membership matrix.

we also compared the predictions of cell-type gene expression pro   les derived by un-mixing

our spatial point process features against gene expression pro   les derived by un-mixing the

more standard 250  m    250  m    250  m averaged gene expression level features. we see
a very large improvement in perplexity by switching from the standard simple averaging

of gene expression, to extracting spatial point process features (   gure 6.2b). the single-

cell rna seq dataset analysis from    gure 6.2b shows that the perplexity of our recovered

cell-types rapidly    attens after we recover approximately 10 clusters (k = 10).

137

9

8

7

6

5

4

3

2

e

n

r

e

u

s

1

t

n

i

axis 1

axis 2

g l i a
d
n

s
e
m i c

e l i a l
t
a s

s

e

t

y m a l

s

n

a m d i a l

r

c

c

e

o

o

o

o

y

y

r

t

r

t

r

h

d

n

d

o
y
p
o l i g
(a) cell diameter in principal axes

m u

e

e

p

n

d

r

o

e

a l

r

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

n

r

e

t

n

i

u

e

1

s

s

r

n

r

o
y
p
o l i g

a m d i a l

r

d

n

e

d

o

t

y

c

o

s
e
m i c

g l i a
d
n

o

r

e

h

t

o

e l i a l
t
a s

r

o

t

p

y

c

e

s

n

e

e

d

y m a l

a l

r

m u

(b) orientation

150

100

50

0

n

r

e

t

n

i

u

1

e

s

s

r

r

n
o
y
p
o l i g

50

40

30

20

10

0

a m d i a l

r

d

n

e

d

o

t

y

c

o

s
e
m i c

g l i a
d
n

o

r

e

h

t

o

e l i a l
t
a s

r

o

t

p

y

c

e

s

n

e

e

d

y m a l

a l

r

m u

n

r

e

t

n

i

u

e

1

s

s

r

n

r

o
y
p
o l i g

a m d i a l

r

d

n

e

d

o

t

y

c

o

s
e
m i c

g l i a
d
n

o

r

e

h

t

o

e l i a l
t
a s

r

o

t

p

c

y

e

s

n

e

e

d

y m a l

a l

r

m u

(c) intensity

(d) cells in 100   m radius

figure 6.4: figure of 5% and 95% percentile estimated cell features for 8 cell types we
detected. id136 is performed on the spatial point process histograms data we estimated.

6.4.4 a brief analysis of recovered cell types in somatosensory

cortex

in this section we describe the representative spatial point process statistics and gene ex-

pressions for 8 cell-types we recovered. we attempted to align our 8 clusters to cell-types

de   ned by [168] in the single-cell rna sequencing paper. we found high overlap in the gene

expression pro   les for all 8 clusters with known cell-types de   ned in [168], interneurons, s1

138

pyramidal, mural, endothelial, microglia, ependymal, astrocytes and oligodendrocytes, in

figure 6.3.

the estimate of    was combined with id113 to infer the cell-type speci   c spatial point process

representation hm

l . in examining the spatial point process distributions that we predict for

each of these cell types, we discover that while the distribution of cell body orientations is

quite broad and similar across cell types, the cell count distribution, which is a measure

of cell density, varies in a systematic way from one cell type to another. fig 6.4d shows

that inhibitory interneurons are less dense than s1pyramidal neurons. this is consistent

with their known prevalence, roughly 20% of all neurons are gabaergic interneurons [118],

while the remaining 80% are excitatory glutamatergic pyramidal neurons. as expected, this

excitatory neuronal category of s1pyramidal is the most common and hence most dense

class of neuronal cells. they also have slightly larger cell bodies, compared to interneurons,

as can be seen in fig 6.4a. the remaining 6 cell types correspond to various glial sub-types.

6.5 conclusion

we developed a computational method for discovering cell types in a brain region by an-

alyzing the high-resolution in situ hybridization image series from the allen brain atlas.

under the assumption that cell types have unique spatial distributions and gene expression

pro   les, we used a varied id44 (vlda) based on spatial point process

process mixture model to simultaneously infer the cell feature spatial distribution and gene

expression pro   les of cell types. by comparing our gene expression pro   le predictions to a

single-cell rna sequencing dataset, we demonstrated that our model improves signi   cantly

on state of the art.

139

the accuracy of our method relies heavily on the assumption that cell-types di   er in their

spatial distribution, and that our point process features perform a good job of distinguishing

these di   erences. thus the performance of our method can be improved by better estimates

of better features. we would expect our method to perform better for large brain areas,

which can be more accurately aligned, and which have more cells to estimate point process

features.

there are several modi   cations to our vlda model which might improve the faithfulness

of our generative model to the biology. we place a symmetric dirichlet prior over cell-type

multinomial distribution hm for a given histogram bin m. this assumes that the number of

cell-types expressing each gene is the same for all genes. but since some genes are expressed

more commonly and non-speci   cally than others, we might expect a gene-speci   c prior to

be a better model. further, the symmetric dirichlet assumes that all cell-types have equal

proportions of cells. but evidence suggests that excitatory neurons are more common than

inhibitory neurons in cortex [76], and using a non-uniform dirichlet prior could account for

this.

140

chapter 7

conclusion and outlook

7.1 conclusion

now that we are at the end of the dissertation, we are convinced that id106

including tensor decomposition are good candidates for unsupervised learning. they re-

veal hidden structure using transformations and extract useful and clean information to

characterize the complicated data. id106 are proved to be potential in various

application. for instance, text and image processing, social networks, healthcare analytics

and neuroscience.

id106 especially matrix/tensor decomposition framework is versatile. they are

straightforward to apply to    at models, such as exchangeable model, multi-view model,

and hidden markov model, but they are also amendable to learn models with a hierarchy

such as a mixture of trees and latent tree model. id106 not only perform well

on traditional multiplicative sparse coding models but also outperforms the state-of-the-art

on models with group invariance. the tensor decomposition framework is e   cient and is

guaranteed to converge to global optima.

141

7.2 outlook

now the question is what is beyond? could we further push the boundaries of spectral meth-

ods? can we have a tensor library with optimal hardware support for tensor operations? in

the region of high dimensional hidden space, could we develop approximated algorithms that

are computational more e   cient? could we have tensor sketching where the decomposition

happens in a sketching vector space, and the tensor is never explicitly formed? furthermore,

could we use tensor decomposition to train models with other invariances (such as rotation

invariance and scaling invariance) or general invariance constraints?

in the real world, we could push our framework further for more challenging tasks.

in

neuroscience, we would like to understand the brain; that is to systematically model and

learn brain neural system and sort out its relationship to body functions. we know that

deep neural network system inspired by the architecture of neural circuits have been hugely

successful empirically. could we utilize the neural network techniques to foster understanding

of the brain neural circuits? or could we use our knowledge of the brain neural circuits to

understand fundamental reasons for a certain structure of a deep neural network system in

machine learning? even in healthcare analytics, simple usage of the co-occurrence of diseases

is not as informative as considering other factors such as symptoms. with more information,

the model gets more complicated, but we hope to achieve personalized identi   cation of

diseases or curing plans.

overall, there are numerous exciting open problems ahead. graduation is not an end; rather

it is a fresh start. i am looking forward to the uncertainty of the future career. keep curious

and continue exploring. may the world be more intelligent!

142

bibliography

[1] website: 2014 allen institute for brain science. allen mouse brain atlas [internet]. avail-

able from: http://mouse.brain-map.org/. accessed: 2014-11-06.

[2] a. agarwal, a. anandkumar, p. jain, p. netrapalli, and r. tandon. learning sparsely
in conference on learning theory (colt), june

used overcomplete dictionaries.
2014.

[3] a. agarwal, s. negahban, and m. j. wainwright. fast global convergence rates of
in advances in neural

gradient methods for high-dimensional statistical recovery.
information processing systems, pages 37   45, 2010.

[4] a. ahmed, b. recht, and j. romberg. blind deconvolution using convex programming.

id205, ieee transactions on, 60(3):1711   1732, 2014.

[5] e. m. airoldi, d. m. blei, s. e. fienberg, and e. p. xing. mixed membership stochastic

blockmodels. journal of machine learning research, 9:1981   2014, june 2008.

[6] a. anandkumar, k. chaudhuri, d. hsu, s. m. kakade, l. song, and t. zhang.
arxiv preprint

id106 for learning multivariate latent tree structure.
arxiv:1107.1283, 2011.

[7] a. anandkumar, d. p. foster, d. hsu, s. m. kakade, and y.-k. liu. two svds su   ce:
spectral decompositions for probabilistic id96 and id44.
corr, abs/1204.6703, 1, 2012.

[8] a. anandkumar, r. ge, d. hsu, and s. m. kakade. a tensor spectral approach to
learning mixed membership community models. in conference on learning theory
(colt), june 2013.

[9] a. anandkumar, r. ge, d. hsu, and s. m. kakade. a tensor spectral approach to

learning mixed membership community models. arxiv 1302.2684, feb. 2013.

[10] a. anandkumar, r. ge, d. hsu, s. m. kakade, and m. telgarsky. tensor decomposi-

tions for latent variable models, 2012.

[11] a. anandkumar, r. ge, d. hsu, s. m. kakade, and m. telgarsky. tensor decomposi-

tions for learning latent variable models. arxiv preprint arxiv:1210.7559, 2012.

143

[12] a. anandkumar, r. ge, d. hsu, s. m. kakade, and m. telgarsky. tensor decomposi-
tions for learning latent variable models. the journal of machine learning research,
15(1):2773   2832, 2014.

[13] a. anandkumar, r. ge, d. hsu, s. m. kakade, and m. telgarsky. tensor decom-
positions for learning latent variable models. journal of machine learning research,
15:2773   2832, 2014.

[14] a. anandkumar, r. ge, and m. janzamin. learning overcomplete latent variable
in conference on learning theory (colt), june

models through tensor methods.
2015.

[15] a. anandkumar, d. hsu, and s. m. kakade. a method of moments for mixture models

and id48. arxiv preprint arxiv:1203.0683, 2012.

[16] a. anandkumar, v. y. f. tan, f. huang, and a. s. willsky. high-dimensional struc-
local separation criterion. the annals of statistics,

ture learning of ising models:
40(3):1346   1375, 2012.

[17] a. anandkumar, r. valluvan, et al. learning loopy id114 with latent
variables: e   cient methods and guarantees. the annals of statistics, 41(2):401   435,
2013.

[18] r. arora, a. cotter, k. livescu, and n. srebro. stochastic optimization for pca
and pls. in communication, control, and computing (allerton), 2012 50th annual
allerton conference on, pages 861   868, 2012.

[19] s. arora, r. ge, and a. moitra. new algorithms for learning incoherent and overcom-

plete dictionaries. in conference on learning theory (colt), june 2014.

[20] s. arora, r. ge, a. moitra, and s. sachdeva. provable ica with unknown gaussian
noise, with implications for gaussian mixtures and autoencoders. in advances in neural
information processing systems, pages 2375   2383, 2012.

[21] k. azuma. weighted sums of certain dependent random variables. tohoku mathemat-

ical journal, second series, 19(3):357   367, 1967.

[22] k. bache and m. lichman. uci machine learning repository, 2013.

[23] b. w. bader, t. g. kolda, et al. matlab tensor toolbox version 2.5. available online,

january 2012.

[24] d. a. bader and g. cong. fast shared-memory algorithms for computing the mini-
mum spanning forest of sparse graphs. journal of parallel and distributed computing,
66(11):1366   1378, 2006.

[25] g. ballard, t. kolda, and t. plantenga. e   ciently computing tensor eigenvalues on
a gpu. in parallel and distributed processing workshops and phd forum (ipdpsw),
2011 ieee international symposium on, pages 1340   1348. ieee, 2011.

144

[26] a. banerjee and j. langford. an objective evaluation criterion for id91. in pro-
ceedings of the tenth acm sigkdd international conference on knowledge discovery
and data mining, pages 515   520. acm, 2004.

[27] d. belanger and s. kakade. a linear dynamical system model for text. arxiv preprint

arxiv:1502.04081, 2015.

[28] y. bengio. learning deep architectures for ai. foundations and trends r(cid:13) in machine

learning, 2(1):1   127, 2009.

[29] y. bengio, h. schwenk, j.-s. sen  ecal, f. morin, and j.-l. gauvain. neural probabilistic
language models. in innovations in machine learning, pages 137   186. springer, 2006.

[30] m. berry, t. do, g. o   brien, v. krishna, and s. varadhan. svdlibc version 1.4.

available online, 2002.

[31] d. m. blei. probabilistic topic models. communications of the acm, 55(4):77   84,

2012.

[32] d. m. blei, a. y. ng, and m. i. jordan. id44. the journal of

machine learning research, 3:993   1022, 2003.

[33] h. bristow, a. eriksson, and s. lucey. fast convolutional sparse coding. in computer
vision and pattern recognition (cvpr), 2013 ieee conference on, pages 391   398.
ieee, 2013.

[34] h. bristow and s. lucey. optimization methods for convolutional sparse coding. arxiv

preprint arxiv:1406.2407, 2014.

[35] j.-f. cardoso. source separation using higher order moments. in acoustics, speech,

and signal processing, pages 2109   2112. ieee, 1989.

[36] j.-f. cardoso. super-symmetric decomposition of the fourth-order cumulant tensor.
blind identi   cation of more sources than sensors.
in acoustics, speech, and signal
processing, 1991. icassp-91., 1991 international conference on, pages 3109   3112.
ieee, 1991.

[37] r. b. cattell. parallel proportional pro   les and other principles for determining the

choice of factors by rotation. psychometrika, 9(4):267   283, 1944.

[38] j. t. chang. full reconstruction of markov models on evolutionary trees: identi   ability

and consistency. mathematical biosciences, 137(1):51   73, 1996.

[39] y. chen, s. sanghavi, and h. xu. id91 sparse graphs.

arxiv preprint

arxiv:1210.3335, 2012.

[40] m. choi, a. torralba, and a. willsky. context models and out-of-context objects.

pattern recognition letters, 2012.

145

[41] m. j. choi, v. y. tan, a. anandkumar, and a. s. willsky. learning latent tree

id114. the journal of machine learning research, 12:1771   1812, 2011.

[42] m. j. choi, a. torralba, and a. s. willsky. context models and out-of-context objects.

pattern recognition letters, 33(7):853   862, 2012.

[43] a. choromanska, m. hena   , m. mathieu, g. b. arous, and y. lecun. the loss surface

of multilayer networks. arxiv:1412.0233, 2014.

[44] s. choudhary and u. mitra. sparse blind deconvolution: what cannot be done. in
id205 (isit), 2014 ieee international symposium on, pages 3002   3006.
ieee, 2014.

[45] k. l. clarkson and d. p. woodru   . low rank approximation and regression in input

sparsity time. corr, abs/1207.6365, 2012.

[46] k. l. clarkson and d. p. woodru   . low rank approximation and regression in input
sparsity time. in proceedings of the 45th annual acm symposium on symposium on
theory of computing, pages 81   90. acm, 2013.

[47] r. collobert and j. weston. a uni   ed architecture for natural language processing:
deep neural networks with multitask learning. in proceedings of the 25th international
conference on machine learning, pages 160   167. acm, 2008.

[48] r. collobert, j. weston, l. bottou, m. karlen, k. kavukcuoglu, and p. kuksa. natural
language processing (almost) from scratch. the journal of machine learning research,
12:2493   2537, 2011.

[49] p. comon. tensor decompositions. mathematics in signal processing v, pages 1   24,

2002.

[50] p. comon, x. luciani, and a. l. de almeida. tensor decompositions, alternating least

squares and other tales. journal of chemometrics, 23(7-8):393   405, 2009.

[51] p. g. constantine and d. f. gleich. tall and skinny qr factorizations in mapreduce
architectures. in proceedings of the second international workshop on mapreduce and
its applications, pages 43   50. acm, 2011.

[52] l. dagum and r. menon. openmp: an industry standard api for shared-memory

programming. computational science & engineering, ieee, 5(1):46   55, 1998.

[53] y. n. dauphin, r. pascanu, c. gulcehre, k. cho, s. ganguli, and y. bengio. iden-
tifying and attacking the saddle point problem in high-dimensional non-convex opti-
mization. in advances in neural information processing systems, pages 2933   2941,
2014.

[54] j. denny, m. ritchie, m. basford, j. pulley, l. bastarache, k. brown-gentry, d. wang,
d. masys, r. dm, and d. crawford. phewas: demonstrating the feasibility of a
phenome-wide scan to discover genedisease associations. bioinformatics, 26(9):1205   
1210, 2010.

146

[55] b. dolan, c. quirk, and c. brockett. unsupervised construction of large paraphrase
corpora: exploiting massively parallel news sources. in proceedings of the 20th inter-
national conference on computational linguistics, page 350. association for compu-
tational linguistics, 2004.

[56] r. durbin, s. r. eddy, a. krogh, and g. mitchison. biological sequence analysis:

probabilistic models of proteins and nucleic acids. cambridge univ. press, 1999.

[57] m. g. eberle and m. c. maciel. finding the closest toeplitz matrix. computational &

applied mathematics, 22(1):1   18, 2003.

[58] c. ekanadham, d. tranchina, and e. p. simoncelli. a blind sparse deconvolution
method for neural spike identi   cation. in advances in neural information processing
systems, pages 1440   1448, 2011.

[59] p. l. erdos, m. a. steel, l. a. sz  ekely, and t. j. warnow. a few logs su   ce to build

(almost) all trees (i). random structures and algorithms, 14(2):153   184, 1999.

[60] b. fadem. high-yield behavioral science. lww, 2012.

[61] a. falanga, m. marchetti, a. vignoli, and d. balducci. clotting mechanisms and
cancer: implications in thrombus formation and tumor progression. clinical advances
in hematology & oncology: h&o, 1(11):673   678, 2003.

[62] d. feldman, m. schmidt, and c. sohler. turning big data into tiny data: constant-size
coresets for id116, pca and projective id91.
in proceedings of the twenty-
fourth annual acm-siam symposium on discrete algorithms, pages 1434   1453.
siam, 2013.

[63] a. frieze, m. jerrum, and r. kannan. learning linear transformations. in 2013 ieee

54th annual symposium on foundations of computer science, pages 359   359, 1996.

[64] r. ge, f. huang, c. jin, and y. yuan. escaping from saddle points     online stochastic
gradient for tensor decomposition. in proc. of conf. on learning theory, june 2015.

[65] a. v. gerbessiotis and l. g. valiant. direct bulk-synchronous parallel algorithms.

journal of parallel and distributed computing, 22(2):251   267, 1994.

[66] a. gittens and m. w. mahoney. revisiting the nystrom method for improved large-

scale machine learning. arxiv preprint arxiv:1303.1849, 2013.

[67] a. gittens and m. w. mahoney. revisiting the nystrom method for improved large-

scale machine learning. corr, abs/1303.1849, 2013.

[68] g. h. golub and c. f. van loan. matrix computations, volume 3. jhu press, 2012.

[69] g. h. golub and c. f. van loan. matrix computations. 4th ed. baltimore, md: the

johns hopkins university press, 4th ed. edition, 2013.

147

[70] p. gopalan, d. mimno, s. gerrish, m. freedman, and d. blei. scalable id136 of
overlapping communities. in advances in neural information processing systems 25,
pages 2258   2266, 2012.

[71] p. k. gopalan and d. m. blei. e   cient discovery of overlapping communities in
massive networks. proceedings of the national academy of sciences, 110(36):14534   
14539, 2013.

[72] p. grange, j. w. bohland, b. w. okaty, k. sugino, h. bokil, s. b. nelson, l. ng,
m. hawrylycz, and p. p. mitra. cell-type   based model explaining coexpression
patterns of genes in the brain. proceedings of the national academy of sciences,
111(14):5397   5402, 2014.

[73] r. m. gray. toeplitz and circulant matrices: a review. communications and infor-

mation theory, 2(3):155   239, 2005.

[74] u. c. s. w. group et al. united states cancer statistics: 1999   2010 incidence and
mortality web-based report. atlanta (ga): department of health and human services,
centers for disease control and prevention, and national cancer institute, 2014.

[75] m. a. hanson. invexity and the kuhn   tucker theorem. journal of mathematical anal-

ysis and applications, 236(2):594   604, 1999.

[76] k. d. harris and t. d. mrsic-flogel. cortical connectivity and sensory coding. nature,

503(7474):51   58, 2013.

[77] r. a. harshman. foundations of the parafac procedure: models and conditions for
an    explanatory    multi-modal factor analysis. ucla working papers in id102,
16(1):84, 1970.

[78] s. hassan. measuring semantic relatedness using salient encyclopedic concepts. uni-

versity of north texas, 2011.

[79] m. hawrylycz, l. ng, d. page, j. morris, c. lau, s. faber, v. faber, s. sunkin,
v. menon, e. lein, et al. multi-scale correlation structure of gene expression in the
brain. neural networks, 24(9):933   942, 2011.

[80] d. hsu and s. m. kakade. learning mixtures of spherical gaussians: moment methods
and spectral decompositions. in proceedings of the 4th conference on innovations in
theoretical computer science, pages 11   20. acm, 2013.

[81] d. hsu, s. m. kakade, and t. zhang. a spectral algorithm for learning hidden markov

models. journal of computer and system sciences, 78(5):1460   1480, 2012.

[82] f. huang and a. anandkumar. convolutional dictionary learning through tensor
factorization. in proceedings of the 1st international workshop on feature extraction:
modern questions and challenges nips, pages 116   129, 2015.

148

[83] f. huang, a. anandkumar, c. borgs, j. chayes, e. fraenkel, m. hawrylycz, e. lein,
a. ingrosso, and s. turaga. discovering neuronal cell types and their gene expression
pro   les using a spatial point process mixture model. arxiv preprint arxiv:1602.01889,
2016.

[84] f. huang, s. matusevych, a. anandkumar, n. karampatziakis, and p. mineiro. dis-
in nips optimization

tributed id44 via tensor factorization.
workshop, 2014.

[85] f. huang, n. u. n, m. u. hakeem, p. verma, and a. anandkumar. fast detection of
overlapping communities via online tensor methods on gpus. corr, abs/1309.0787,
2013.

[86] f. huang, u. niranjan, m. hakeem, and a. anandkumar. online tensor methods for

learning latent variable models, 2014.

[87] f. huang, u. niranjan, m. u. hakeem, and a. anandkumar. fast detection of over-

lapping communities via online tensor methods. arxiv:1309.0787, 2013.

[88] f. huang, i. perros, r. chen, j. sun, a. anandkumar, et al. scalable latent tree model

and its application to health analytics. arxiv preprint arxiv:1406.4566, 2014.

[89] a. hyvarinen. fast ica for noisy data using gaussian moments.

in circuits and

systems, volume 5, pages 57   61, 1999.

[90] a. hyv  arinen, j. karhunen, and e. oja. independent component analysis, volume 46.

john wiley & sons, 2004.

[91] a. hyv  arinen, j. karhunen, and e. oja. independent component analysis, volume 46.

john wiley & sons, 2004.

[92] m. inoue, h. park, and m. okada. on-line learning theory of soft committee machines
with correlated hidden units   steepest id119 and natural id119   .
journal of the physical society of japan, 72(4):805   810, 2003.

[93] p. jain, p. netrapalli, and s. sanghavi. low-rank matrix completion using alternating
minimization. in proceedings of the forty-   fth annual acm symposium on theory of
computing, pages 665   674, 2013.

[94] j. j  aj  a. an introduction to parallel algorithms. addison wesley longman publishing

co., inc., 1992.

[95] c. ji, d. merl, t. b. kepler, and m. west. spatial mixture modelling for unobserved
point processes: examples in immuno   uorescence histology. bayesian analysis (on-
line), 4(2):297, 2009.

[96] d. s. johnson, c. h. papadimitriou, and m. yannakakis. how easy is local search?

journal of computer and system sciences, 37(1):79   100, 1988.

149

[97] n. kalchbrenner, e. grefenstette, and p. blunsom. a convolutional neural network

for modelling sentences. arxiv preprint arxiv:1404.2188, 2014.

[98] n. kalchbrenner, e. grefenstette, and p. blunsom. a convolutional neural network
for modelling sentences. in proceedings of the 52nd annual meeting of the association
for computational linguistics, acl 2014, june 22-27, 2014, baltimore, md, usa,
volume 1: long papers, pages 655   665. the association for computer linguistics,
2014.

[99] r. kannan, s. s. vempala, and d. p. woodru   . principal component analysis and
in proceedings of the 27th conference on

higher correlations for distributed data.
learning theory, pages 1040   1057, 2014.

[100] b. karrer and m. e. newman. stochastic blockmodels and community structure in

networks. physical review e, 83(1):016107, 2011.

[101] k. kavukcuoglu, p. sermanet, y.-l. boureau, k. gregor, m. mathieu, and y. l. cun.
learning convolutional feature hierarchies for visual recognition. in advances in neural
information processing systems, pages 1090   1098, 2010.

[102] y. kim. convolutional neural networks for sentence classi   cation. arxiv preprint

arxiv:1408.5882, 2014.

[103] r. kiros, y. zhu, r. r. salakhutdinov, r. zemel, r. urtasun, a. torralba, and s. fi-
dler. skip-thought vectors. in advances in neural information processing systems,
pages 3276   3284, 2015.

[104] k. c. kiwiel. convergence and e   ciency of subgradient methods for quasiconvex

minimization. mathematical programming, 90(1):1   25, 2001.

[105] t. g. kolda. orthogonal tensor decompositions. siam journal on matrix analysis

and applications, 23(1):243   255, 2001.

[106] r. kondor. group theoretical methods in machine learning. columbia university,

2008.

[107] a. kottas and b. sans  o. bayesian mixture modeling for spatial poisson process inten-
sities, with applications to extreme value analysis. journal of statistical planning and
id136, 137(10):3151   3163, 2007.

[108] a. krishnamurthy, s. balakrishnan, m. xu, and a. singh. e   cient active algorithms

for hierarchical id91. arxiv preprint arxiv:1206.4672, 2012.

[109] d. krishnan, j. bruna, and r. fergus. blind deconvolution with non-local sparsity

reweighting. arxiv preprint arxiv:1311.4029, 2013.

[110] a. kulesza and b. taskar. determinantal point processes for machine learning. ma-

chine learning, 5(2-3):123   286, 2012.

150

[111] h. kushner and g. yin. stochastic approximation and recursive algorithms and

applications. applications of mathematics series. springer, 2003.

[112] a. lancichinetti and s. fortunato. community detection algorithms: a comparative

analysis. physical review e, 80(5):056117, 2009.

[113] a. lancichinetti, s. fortunato, and j. kert  esz. detecting the overlapping and hierarchi-
cal community structure in complex networks. new journal of physics, 11(3):033015,
2009.

[114] q. v. le and t. mikolov. distributed representations of sentences and documents.

arxiv preprint arxiv:1405.4053, 2014.

[115] e. s. lein, m. j. hawrylycz, n. ao, m. ayres, a. bensinger, a. bernard, a. f.
boe, m. s. boguski, k. s. brockway, e. j. byrnes, et al. genome-wide atlas of gene
expression in the adult mouse brain. nature, 445(7124):168   176, 2007.

[116] a. levin, y. weiss, f. durand, and w. t. freeman. understanding and evaluating
blind deconvolution algorithms. in id161 and pattern recognition, 2009.
cvpr 2009. ieee conference on, pages 1964   1971. ieee, 2009.

[117] o. l. mangasarian. pseudo-convex functions. journal of the society for industrial &

applied mathematics, series a: control, 3(2):281   290, 1965.

[118] h. markram, m. toledo-rodriguez, y. wang, a. gupta, g. silberberg, and c. wu.
interneurons of the neocortical inhibitory system. nat rev neurosci, 5(10):793   807,
oct. 2004.

[119] m. mcpherson, l. smith-lovin, and j. cook. birds of a feather: homophily in social

networks. annual review of sociology, pages 415   444, 2001.

[120] f. mcsherry. spectral partitioning of random graphs. in focs, 2001.

[121] n. meinshausen and p. b  uhlmann. high dimensional graphs and variable selection

with the lasso. annals of statistics, 34(3):1436   1462, 2006.

[122] michael. boruvka algorithm parallel implementation cuda, december 2012.

[123] r. mihalcea, c. corley, and c. strapparava. corpus-based and knowledge-based mea-

sures of text semantic similarity. in aaai, volume 6, pages 775   780, 2006.

[124] t. mikolov, k. chen, g. corrado, and j. dean. e   cient estimation of word represen-

tations in vector space. arxiv preprint arxiv:1301.3781, 2013.

[125] j. mitchell and m. lapata. composition in distributional models of semantics. cog-

nitive science, 34(8):1388   1429, 2010.

[126] a. mnih and r. salakhutdinov. probabilistic id105.

in advances in

neural information processing systems, pages 1257   1264, 2007.

151

[127] e. mossel. distorted metrics on trees and phylogenetic forests. ieee/acm transac-

tions on computational biology and bioinformatics (tcbb), 4(1):108   116, 2007.

[128] e. mossel and s. roch. learning nonsingular phylogenies and id48.
in proceedings of the thirty-seventh annual acm symposium on theory of computing,
pages 366   375. acm, 2005.

[129] t. nepusz, a. petr  oczi, l. n  egyessy, and f. bazs  o. fuzzy communities and the concept

of bridgeness in complex networks. physical review e, 77(1):016107, 2008.

[130] e. oja and j. karhunen. on stochastic approximation of the eigenvectors and eigen-
values of the expectation of a random matrix. journal of mathematical analysis and
applications, 106(1):69   84, 1985.

[131] b. a. olshausen. sparse codes and spikes. probabilistic models of the brain: perception

and neural function, pages 257   272, 2002.

[132] b. a. olshausen and d. j. field. sparse coding with an overcomplete basis set: a

strategy employed by v1? vision research, 37(23):3311   3325, 1997.

[133] a. v. oppenheim and a. s. willsky. signals and systems. prentice-hall, 1997.

[134] j. pearl. probabilistic reasoning in intelligent systems: networks of plausible id136.

morgan kaufmann, 1988.

[135] k. pearson. contributions to the mathematical theory of evolution. philosophical

transactions of the royal society of london. a, 185:71   110, 1894.

[136] j. pennington, r. socher, and c. d. manning. glove: global vectors for word repre-

sentation. in emnlp, volume 14, pages 1532   1543, 2014.

[137] c. quirk, c. brockett, and w. b. dolan. monolingual machine translation for para-

phrase generation. in emnlp, pages 142   149, 2004.

[138] a. rakhlin, o. shamir, and k. sridharan. making id119 optimal for strongly

convex stochastic optimization. in icml, pages 449   456, 2012.

[139] m. rattray, d. saad, and s.-i. amari. natural id119 for on-line learning.

physical review letters, 81(24):5461, 1998.

[140] d. robinson and l. r. foulds. comparison of phylogenetic trees. mathematical bio-

sciences, 53(1):131   147, 1981.

[141] d. e. rumelhart, g. e. hinton, and r. j. williams. learning representations by

back-propagating errors. cognitive modeling, 5, 1988.

[142] v. rus, p. m. mccarthy, m. c. lintean, d. s. mcnamara, and a. c. graesser. para-
phrase identi   cation with lexico-syntactic graph subsumption. in flairs conference,
pages 201   206, 2008.

152

[143] d. saad and s. a. solla. on-line learning in soft committee machines. physical review

e, 52(4):4225, 1995.

[144] r. salakhutdinov and a. mnih. bayesian probabilistic id105 using
in proceedings of the 25th international conference on

id115.
machine learning, pages 880   887. acm, 2008.

[145] a. m. saxe, j. l. mcclelland, and s. ganguli. exact solutions to the nonlinear dy-

namics of learning in deep linear neural networks. arxiv:1312.6120, 2013.

[146] m. d. schatz, t. m. low, r. a. van de geijn, and t. g. kolda. exploiting symmetry

in tensors for high performance. arxiv preprint arxiv:1301.7744, 2013.

[147] s. shalev-shwartz, o. shamir, k. sridharan, and n. srebro. stochastic convex opti-

mization. in proceedings of the 22nd conference on learning theory, 2009.

[148] a. smola and s. narayanamurthy. an architecture for parallel topic models. proceed-

ings of the vldb endowment, 3(1-2):703   710, 2010.

[149] r. socher, c. c. lin, c. manning, and a. y. ng. parsing natural scenes and natural
in proceedings of the 28th international

language with id56s.
conference on machine learning (icml-11), pages 129   136, 2011.

[150] r. socher, a. perelygin, j. y. wu, j. chuang, c. d. manning, a. y. ng, and c. potts.
recursive deep models for semantic compositionality over a sentiment treebank. in
proceedings of the conference on empirical methods in natural
language processing
(emnlp), volume 1631, page 1642. citeseer, 2013.

[151] r. r. sokal and f. j. rohlf. the comparison of dendrograms by objective methods.

taxon, 11(2):33   40, 1962.

[152] j. soman and a. narang. fast community detection algorithm with gpus and multicore
architectures. in parallel & distributed processing symposium (ipdps), 2011 ieee
international, pages 568   579. ieee, 2011.

[153] k. strimmer.

fdrtool: a versatile r package for estimating local and tail area-based

false discovery rates. bioinformatics, 24(12):1461   1462, 2008.

[154] k. s. tai, r. socher, and c. d. manning. improved semantic representations from
tree-structured id137. arxiv preprint arxiv:1503.00075,
2015.

[155] a. l. traud, e. d. kelsic, p. j. mucha, and m. a. porter. comparing community
structure to characteristics in online collegiate social networks. siam review, in press
(arxiv:0809.0960), 2010.

[156] v. vineet, p. harish, s. patidar, and p. narayanan. fast minimum spanning tree
for large graphs on the gpu. in proceedings of the conference on high performance
graphics 2009, pages 167   171. acm, 2009.

153

[157] f. wang and y. li. beyond physical connections: tree models in human pose estima-

tion. in proc. of cvpr, 2013.

[158] s. wang and c. d. manning. baselines and bigrams: simple, good sentiment and
topic classi   cation.
in proceedings of the 50th annual meeting of the association
for computational linguistics: short papers-volume 2, pages 90   94. association for
computational linguistics, 2012.

[159] y. wang, h.-y. tung, a. smola, and a. anandkumar. fast and guaranteed tensor

decomposition via sketching. in proc. of nips, 2015.

[160] j. wei, w. dai, a. kumar, x. zheng, q. ho, and e. p. xing. consistent bounded-

asynchronous parameter servers for distributed ml. arxiv e-prints, dec. 2013.

[161] j. wieting, m. bansal, k. gimpel, and k. livescu. towards universal paraphrastic

sentence embeddings. arxiv preprint arxiv:1511.08198, 2015.

[162] a. wiki. paraphrase identi   cation (state of the art), 2014.

[163] d. wipf and h. zhang. revisiting bayesian blind deconvolution. arxiv preprint

arxiv:1305.2362, 2013.

[164] s. j. wright and j. nocedal. numerical optimization, volume 2. springer new york,

1999.

[165] j. yang and j. leskovec. de   ning and evaluating network communities based on
in proceedings of the acm sigkdd workshop on mining data se-

ground-truth.
mantics, page 3. acm, 2012.

[166] m. yu and m. dredze. learning composition models for phrase embeddings. trans-

actions of the association for computational linguistics, 3:227   242, 2015.

[167] m. d. zeiler, d. krishnan, g. w. taylor, and r. fergus. deconvolutional networks. in
id161 and pattern recognition (cvpr), 2010 ieee conference on, pages
2528   2535. ieee, 2010.

[168] a. zeisel, a. b. mu  noz-manchado, s. codeluppi, p. l  onnerberg, g. la manno,
a. jur  eus, s. marques, h. munguba, l. he, c. betsholtz, et al. cell types in the mouse
cortex and hippocampus revealed by single-cell rna-seq. science, 347(6226):1138   1142,
2015.

[169] y. zhang and d.-y. yeung. overlapping community detection via bounded nonnega-
tive matrix tri-factorization. in proceedings of the 18th acm sigkdd international
conference on knowledge discovery and data mining, kdd    12, pages 606   614, new
york, ny, usa, 2012. acm.

[170] h. zhao, z. lu, and p. poupart. self-adaptive hierarchical sentence model. arxiv

preprint arxiv:1504.05070, 2015.

154

[171] j. y. zou, d. hsu, d. c. parkes, and r. p. adams. contrastive learning using spectral
methods. in advances in neural information processing systems, pages 2238   2246,
2013.

155

appendix a

appendix for online stochastic

gradient for tensor decomposition

a.1 detailed analysis for section 2.2 in unconstrained

case

in this section we give detailed analysis for noisy id119, under the assumption

that the unconstrained problem satis   es (  ,   ,   ,   )-strict saddle property.

the algorithm we investigate in algorithm 1, we can combine the randomness in the stochas-

tic gradient oracle and the arti   cial noise, and rewrite the update equation in form:

wt = wt   1       (   f (wt   1) +   t   1)

(a.1)

where    is step size,    = sg(wt   1)        f (wt   1) + n (recall n is a random vector on unit
sphere) is the combination of two source of noise.

156

by assumption, we know      s are independent and they satisfying e   = 0, k  k     q + 1. due
to the explicitly added noise in algorithm 1, we further have e    t     1
di. for simplicity,
we assume e    t =   2i, for some constant    =     (1), then the algorithm we are running

is exactly the same as stochastic id119 (sgd). our proof can be very easily

extended to the case when 1

di (cid:22) e[    t ] (cid:22) (q + 1

d)i because both the upper and lower

bounds are     (1).

we    rst restate the main theorem in the context of stochastic id119.

theorem a.1 (main theorem). suppose a function f (w) : rd     r that is (  ,   ,   ,   )-
strict saddle, and has a stochastic gradient oracle where the noise satisfy e    t =   2i.

further, suppose the function is bounded by |f (w)|     b, is   -smooth and has   -lipschitz
hessian. then there exists a threshold   max =     (1), so that for any    > 0, and for any
         max/ max{1, log(1/  )}, with id203 at least 1        in t =   o(     2 log(1/  )) iterations,
sgd outputs a point wt that is   o(p   log(1/    ))-close to some local minimum w   .
recall that   o(  ) (      ,     ) hides the factor that has polynomial dependence on all other parame-
ters, but is independent of    and   . so it focuses on the dependency on    and   . throughout
the proof, we interchangeably use both h(w) and    2f (w) to represent the hessian matrix
of f (w).

as we discussed in the proof sketch in section 2.2, we analyze the behavior of the algorithm

in three di   erent cases. the    rst case is when the gradient is large.

lemma a.1. under the assumptions of theorem a.1, for any point with k   f (w0)k    

p2    2  d wherep2    2  d <   , after one iteration we have:

ef (w1)     f (w0)              (  2)

(a.2)

157

proof. our assumption can guarantee   max < 1

   , then by update equation eq.(a.1), we have:

ef (w1)     f (w0)        f (w0)t e(w1     w0) +

  
2

ekw1     w0k2

  
2

ek     (   f (w0) +   0)k2

=    f (w0)t e (     (   f (w0) +   0)) +
  2  2  d
=    (      
       

)k   f (w0)k2 +
  2  2  d

  
2k   f (w0)k2 +

2
       

    2
2

2

2

  2  2  d

(a.3)

which    nishes the proof.

lemma a.2. under the assumptions of theorem a.1, for any initial point w0 that is

  o(     ) <    close to a local minimum w   , with id203 at least 1       /2, we have fol-

lowing holds simultaneously:

   t       o(

1
  2 log

1
  

),

kwt     w   k       o(r   log

1
    

) <   

where w    is the locally optimal point.

(a.4)

proof. we shall construct a supermartingale and use azuma   s inequality [21] to prove this

result.

let    ltration ft =   {  0,           t   1}, and note   {   0,         ,    t}     ft, where   {  } denotes the
     <   }, where    is independent
of (  ,   ), and will be speci   ed later. to ensure the correctness of proof,   o notation in this

sigma    eld. let event et = {          t,kw       w   k       q   log 1

proof will never hide any dependence on   . clearly there   s always a small enough choice of

     <    holds as long as          max/ max{1, log(1/  )}. also note

  max =     (1) to make   q   log 1

et     et   1, that is 1et     1et   1.

158

by de   nition 2.3 of (  ,   ,   ,   )-strict saddle, we know f is locally   -strongly convex in the
2  -neighborhood of w   . since    f (w   ) = 0, we have

   f (wt)t (wt     w   )1et       kwt     w   k21et

(a.5)

furthermore, with   max <   

  2 , using   -smoothness, we have:

e[kwt     w   k21et   1|ft   1] =e[kwt   1       (   f (wt   1) +   t   1)     w   k2|ft   1]1et   1

=(cid:2)kwt   1     w   k2     2     f (wt   1)t (wt   1     w   )
+  2k   f (wt   1)k2 +   2d  2(cid:3) 1et   1
   [(1     2     +   2  2)kwt   1     w   k2 +   2d  2]1et   1
   [(1         )kwt   1     w   k2 +   2d  2]1et   1

therefore, we have:

(cid:20)e[kwt     w   k2|ft   1]    

  d  2

   (cid:21) 1et   1     (1         )(cid:20)kwt   1     w   k2    

  d  2

   (cid:21) 1et   1

then, let gt = max{(1         )   t(kwt     w   k2       d  2

   ), 0}, we have:

e[gt1et   1|ft   1]     gt   11et   1     gt   11et   2

which means gt1et   1 is a supermartingale.

(a.6)

(a.7)

(a.8)

159

therefore, with id203 1, we have:

|gt1et   1     e[gt1et   1|ft   1]|
   (1         )   t[ kwt   1          f (wt   1)     w   k      k  t   1k +   2k  t   1k2 +   2d  2 ]1et   1
   (1         )   t      o(    1.5 log

) = dt

1
    

1
2

let

ct =vuut
tx   =1

   =   o(    1.5 log
d2

1
2

1
    

)vuut
tx   =1

(1         )   2  

by azuma   s inequality, with id203 less than   o(  3  ), we have:

gt1et   1 >   o(1)ct log

1

2 (

1
    

) + g0

we know gt >   o(1)ct log

1

2 ( 1

     ) + g0 is equivalent to:

kwt     w   k2 >   o(  ) +   o(1)(1         )tct log

1

2 (

1
    

)

we know:

(1         )tct log

1

2 (

=        o(  1.5 log

1
    

=         o(   log

1
    

)

1
    

1
    

)vuut
tx   =1
(1         )2               o(  1.5 log

) =         o(  1.5 log
)vuut
t   1x   =0

(1         )2(t      )

)s

1
    

1

1     (1         )2

(a.9)

(a.10)

(a.11)

(a.12)

(a.13)

160

this means azuma   s inequality implies, there exist some   c =   o(1) so that:

p(cid:18)et   1    (cid:26)kwt     w   k2 >         c   log

1
    

)(cid:27)(cid:19)       o(  3  )

by choosing    >   c, this is equivalent to:

p(cid:18)et   1    (cid:26)kwt     w   k2 >   2   log

1

    (cid:27)(cid:19)       o(  3  )

then we have:

(a.14)

(a.15)

p (et) = p(cid:18)et   1    (cid:26)kwt     w   k >   r   log

1

    (cid:27)(cid:19) + p (et   1)       o(  3  ) + p (et   1)

(a.16)

by initialization conditions, we know p (e0) = 0, and thus p (et)     t   o(  3  ). take t =
   ). when   max =   o(1) is chosen small enough, and
  o( 1

  2 log 1

   ), we have p (et)       o(     log 1
         max/ log(1/  ), this    nishes the proof.

lemma a.3. under the assumptions of theorem a.1, for any initial point w0 where k   f (w0)k

   p2    2  d <   , and   min(h(w0))          , then there is a number of steps t that depends on

w0 such that:

ef (wt )     f (w0)              (  )

(a.17)

the number of steps t has a    xed upper bound tmax that is independent of w0 where t    
tmax = o((log d)/    ).

remark. in general, if we relax the assumption e    t =   2i to   2

mini (cid:22) e    t (cid:22)   2

maxi,

the upper bound tmax of number of steps required in lemma a.3 would be increased to

tmax = o( 1

     (log d + log   max
  min

))

161

as we described in the proof sketch, the main idea is to consider a coupled update sequence

that correspond to the local second-order approximation of f (x) around w0. we characterize

this sequence of update in the next lemma.

lemma a.4. under the assumptions of theorem a.1. let   f de   ned as local second-order

approximation of f (x) around w0:

  f (w)

.
= f (w0) +    f (w0)t (w     w0) +

1
2

(w     w0)th(w0)(w     w0)

(a.18)

{   wt} be the corresponding sequence generated by running sgd on function   f , with   w0 = w0.
for simplicity, denote h = h(w0) =    2f (w0), then we have analytically:

      f (   wt) = (1       h)t   f (w0)       h

(1       h)t        1    

  wt     w0 =      

t   1x   =0

(1       h)     f (w0)       

t   1x   =0

(1       h)t        1    

t   1x   =0

(a.19)

(a.20)

furthermore, for any initial point w0 where k   f (w0)k       o(  ) <   , and   min(h(w0)) =      0.
then, there exist a t     n satisfying:

d
    0    

t   1x   =0

(1 +     0)2   <

3d
    0

(a.21)

with id203 at least 1       o(  3), we have following holds simultaneously for all t     t :

k   wt     w0k       o(  

1

2 log

1
  

);

k      f (   wt)k       o(  

1

2 log

1
  

)

proof. denote h = h(w0), since   f is quadratic, clearly we have:

      f (   wt) =       f (   wt   1) + h(   wt       wt   1)

(a.22)

(a.23)

162

substitute the update equation of sgd in eq.(a.23), we have:

      f (   wt) =       f (   wt   1)       h(      f (   wt   1) +   t   1)

= (1       h)      f (   wt   1)       h  t   1
= (1       h)2      f (   wt   2)       h  t   1       h(1       h)  t   2 =         
= (1       h)t   f (w0)       h

(1       h)t        1    

t   1x   =0

therefore, we have:

  wt     w0 =      

=      

=      

(      f (   w   ) +      )

t   1x   =0
t   1x   =0 (1       h)     f (w0)       h
t   1x   =0
t   1x   =0

(1       h)     f (w0)       

(1       h)              1         +     !
     1x      =0

(1       h)t        1    

(a.24)

(a.25)

next, we prove the existence of t in eq.(a.21). since pt

   =0(1 +     0)2   is monotonically
increasing w.r.t t, and diverge to in   nity as t        . we know there is always some t     n
   =0 (1 +     0)2   . let t be the smallest integer satisfying above equation. by
gives

d

    0    pt   1
t+1x   =0

assumption, we know          0     l, and

(1 +     0)2   = 1 + (1 +     0)2

(1 +     0)2  

tx   =0

we can choose   max < min{(   2     1)/l, 2d/  } so that

d
    0    

t   1x   =0

(1 +     0)2       1 +

2d
    0    

3d
    0

163

(a.26)

(a.27)

finally, by eq.(a.21), we know t = o(log d/  0  ), and (1 +     0)t       o(1). also because
e   = 0 and k  k     q =   o(1) with id203 1, then by hoe   ding inequality, we have for
each dimension i and time t     t :

p |  

t   1x   =0

(1       h)t        1    ,i| >   o(  

1

2 log

1
  

)!     e         (log2 1

   )       o(  4)

(a.28)

then by summing over dimension d and taking union bound over all t     t , we directly have:

p    t     t,k  

t   1x   =0

(1       h)t        1    k >   o(  

1

2 log

)!       o(  3).

1
  

(a.29)

combine this fact with eq.(a.24) and eq.(a.25), we    nish the proof.

next we need to prove that the two sequences of updates are always close.

lemma a.5. under the assumptions of theorem a.1. and let {wt} be the corresponding se-
quence generated by running sgd on function f . also let   f and {   wt} be de   ned as in lemma
a.4. then, for any initial point w0 where k   f (w0)k       o(  ) <   , and   min(   2f (w0)) =      0.
given the choice of t as in eq.(a.21), with id203 at least 1      o(  2), we have following
holds simultaneously for all t     t :

kwt       wtk       o(   log2 1

  

);

k   f (wt)           f (   wt)k       o(   log2 1

  

)

(a.30)

proof. first, we have update function of gradient by:

   f (wt) =   f (wt   1) +z 1

0 h(wt   1 + t(wt     wt   1))dt    (wt     wt   1)

=   f (wt   1) + h(wt   1)(wt     wt   1) +   t   1

(a.31)

164

where the remainder:

  t   1    z 1

0

[h(wt   1 + t(wt     wt   1))     h(wt   1)] dt    (wt     wt   1)

(a.32)

denote h = h(w0), and h   t   1 = h(wt   1)   h(w0). by hessian smoothness, we immediately
have:

kh   t   1k = kh(wt   1)     h(w0)k       kwt   1     w0k       (kwt       wtk + k   wt     w0k)
k  t   1k    

  
2kwt     wt   1k2

(a.33)

(a.34)

substitute the update equation of sgd (eq.(a.1)) into eq.(a.31), we have:

   f (wt) =    f (wt   1)       (h + h   t   1)(   f (wt   1) +   t   1) +   t   1

= (1       h)   f (wt   1)       h  t   1       h   t   1(   f (wt   1) +   t   1) +   t   1

(a.35)

let    t =    f (wt)         f (   wt) denote the di   erence in gradient, then from eq.(a.24), eq.(a.35),
and eq.(a.1), we have:

   t = (1       h)   t   1       h   t   1[   t   1 +       f (   wt   1) +   t   1] +   t   1
wt       wt =      

     

t   1x   =0

(a.36)

(a.37)

let    ltration ft =   {  0,           t   1}, and note   {   0,         ,    t}     ft, where   {  } denotes the
   ), k   w       w0k    
sigma    eld. also,
  }, where    is independent of (  ,   ),
and will be speci   ed later. again,   o notation in this proof will never hide any dependence

let event kt = {          t, k      f (   w   )k       o(  

   )}, and et = {          t, k     k          log2 1

2 log 1

2 log 1

  o(  

1

1

165

on   . clearly, we have kt     kt   1 (et     et   1), thus 1kt     1kt   1 (1et     1et   1), where 1k is
the indicator function of event k.

we    rst need to carefully bounded all terms in eq.(a.36), conditioned on event kt   1     et   1,
by eq.(a.33), eq.(a.34)), and eq.(a.37), with id203 1, for all t     t     o(log d/  0  ),
we have:

k(1       h)   t   1k       o(     log2 1
k  h   t   1  t   1k       o(  1.5 log

  
1
  

)

)

k  h   t   1(   t   1 +       f (   wt   1))k       o(  2 log2 1
k  t   1k       o(  2)

  

)

(a.38)

since event kt   1     ft   1, et   1     ft   1 thus independent of   t   1, we also have:

e[((1       h)   t   1)t   h   t   1  t   11kt   1   et   1 | ft   1]
=1kt   1   et   1((1       h)   t   1)t   h   t   1

e[  t   1 | ft   1] = 0

therefore, from eq.(a.36) and eq.(a.38):

21kt   1   et   1 | ft   1]

e[k   tk2
   (cid:20)(1 +     0)2k   t   1k2 + (1 +     0)k   t   1k   o(  2 log2 1
)(cid:21) 1kt   1   et   1
   (cid:20)(1 +     0)2k   t   1k2 +   o(    3 log4 1

  

  

) +   o(  3 log2 1
  

)(cid:21) 1kt   1   et   1

de   ne

gt = (1 +     0)   2t[ k   tk2 +     2 log4 1

  

]

166

(a.39)

(a.40)

(a.41)

then, when   max is small enough, we have:

e[gt1kt   1   et   1 | ft   1] = (1 +     0)   2t(cid:20)e[k   tk2
    (1 +     0)   2t(cid:20)(1 +     0)2k   t   1k2 +   o(    3 log4 1
    (1 +     0)   2t(cid:20)(1 +     0)2k   t   1k2 + (1 +     0)2    2 log4 1

21kt   1   et   1 | ft   1] +     2 log3 1
  (cid:21) 1kt   1   et   1

) +     2 log4 1

  (cid:21) 1kt   1   et   1

  

  (cid:21) 1kt   1   et   1

= gt   11kt   1   et   1     gt   11kt   2   et   2

(a.42)

therefore, we have e[gt1kt   1   et   1 | ft   1]     gt   11kt   2   et   2 which means gt1kt   1   et   1 is a
supermartingale.

on the other hand, we have:

   t = (1       h)   t   1       h   t   1(   t   1 +       f (   wt   1))       h   t   1  t   1 +   t   1

(a.43)

once conditional on    ltration ft   1, the    rst two terms are deterministic, and only the third
and fourth term are random. therefore, we know, with id203 1:

| k   tk2

2     e[k   tk2

2|ft   1] |1kt   1   et   1       o(    2.5 log3 1

  

)

(a.44)

where the main contribution comes from the product of the    rst term and third term. then,

with id203 1, we have:

|gt1kt   1   et   1     e[gt1kt   1   et   1 | ft   1]|
=(1 + 2    0)   2t    | k   tk2

2     e[k   tk2

2|ft   1] |    1kt   1   et   1       o(    2.5 log3 1

  

) = ct   1

(a.45)

167

by azuma-hoe   ding inequality, with id203 less than   o(  3), for t     t     o(log d/  0  ):

gt1kt   1   et   1     g0    1 >   o(1)vuut
t   1x   =0

c2
   log(

1
  

) =   o(    2 log4 1
  

)

this means there exist some   c =   o(1) so that:

p(cid:18)gt1kt   1   et   1       c    2 log4 1

  (cid:19)       o(  3)

by choosing    >   c, this is equivalent to:

p(cid:18)kt   1     et   1    (cid:26)k   tk2       2  2 log4 1

  (cid:27)(cid:19)       o(  3)

therefore, combined with lemma a.4, we have:

(a.46)

(a.47)

(a.48)

  (cid:27)(cid:19)
p(cid:18)et   1    (cid:26)k   tk          log2 1
=p(cid:18)kt   1     et   1    (cid:26)k   tk          log2 1
      o(  3) + p (kt   1)       o(  3)

  (cid:27)(cid:19) + p(cid:18)kt   1     et   1    (cid:26)k   tk          log2 1
  (cid:27)(cid:19)

finally, we know:

p (et) = p(cid:18)et   1    (cid:26)k   tk          log2 1

  (cid:27)(cid:19) + p (et   1)       o(  3) + p (et   1)

because p (e0) = 0, and t       o( 1
kwt       wtk       pt   1

   ), we have p (et )       o(  2). due to eq.(a.37), we have

   =0 k     k, then by the de   nition of et , we    nish the proof.

using the two lemmas above we are ready to prove lemma a.3

168

(a.49)

(a.50)

proof of lemma a.3. let   f and {   wt} be de   ned as in lemma a.4. and also let   min(h(w0))
=      0. since h(w) is   -lipschitz, for any w, w0, we have:

f (w)     f (w0) +    f (w0)t (w     w0) +

1
2

(w     w0)th(w0)(w     w0) +

  
6kw     w0k3 (a.51)

denote      =   wt     w0 and    = wt       wt , we have:
f (wt )     f (w0)    (cid:20)   f (w0)t (wt     w0) +
=(cid:20)   f (w0)t (     +   ) +
=(cid:20)   f (w0)t      +

1
2

1
2

1
2
(     +   )th(     +   ) +

(wt     w0)th(w0)(wt     w0) +
6k     +   k3(cid:21)

  

    th    (cid:21) +(cid:20)   f (w0)t    +     th   +

1
2

  th   +

  

6kwt     w0k3(cid:21)
6k     +   k3(cid:21)

  

where h = h(w0). denote      =    f (w0)t      + 1
    th   + 1

2  th   +   

2

6k     +   k3 be the second term. we have f (wt )     f (w0)          +   .

(a.52)

    th     be the    rst term, and    =    f (w0)t    +

let et = {          t,k   w       w0k       o(  
lemma a.4 and lemma a.5, we know p (et )     1       o(  2). then, clearly, we have:

   ), kwt       wtk       o(   log2 1

2 log 1

   )}, by the result of

1

ef (wt )     f (w0) =e[f (wt )     f (w0)]1et + e[f (wt )     f (w0)]1et

   e    1et + e  1et + e[f (wt )     f (w0)]1et
=e     + e  1et + e[f (wt )     f (w0)]1et     e    1et

(a.53)

we will carefully caculate e     term    rst, and then bound remaining term as    perturbation   

to    rst term.

169

let   1,         ,   d be the eigenvalues of h. by the result of lemma a.4 and simple id202,
we have:

(1         i)  |   if (w0)|2 +

1
2

dxi=1

  i

t   1x   =0

(1         i)2     2  2

e     =    

  
2

dxi=1
2t   1x   =0
t   1x   =0
dxi=1
2 "d     1

  i

  2  2

1
2

   

   

(1         i)2     2  2

         0

(1 +     0)2  #        

t   1x   =0

    2
2

(a.54)

the last inequality is directly implied by the choice of t as in eq.(a.21). also, by eq.(a.21),

we also immediately have that t = o(log d/  0  )     o(log d/    ). therefore, by choose
tmax = o(log d/    ) with large enough constant, we have t     tmax = o(log d/    ).

for bounding the second term, by de   nition of et, we have:

e  1et = e(cid:20)   f (w0)t    +     th   +

1
2

  th   +

  

6k     +   k3(cid:21) 1et       o(  1.5 log3 1

  

)

(a.55)

on the other hand, since noise is bounded as k  k       o(1), from the results of lemma a.4,
it   s easy to show k   w     w0k = k    k       o(1) is also bounded with id203 1. recall the
assumption that function f is also bounded, then we have:

e[f (wt )     f (w0)]1et     e    1et
=e[f (wt )     f (w0)]1et     e(cid:20)   f (w0)t      +

1
2

    th    (cid:21) 1et       o(1)p (et )       o(  2)

(a.56)

finally, substitute eq.(a.54), eq.(a.55) and eq.(a.56) into eq.(a.53), we    nish the proof.

finally, we combine three cases to prove the main theorem.

170

proof of theorem a.1. let   s set l1 = {w | k   f (w)k    p2    2  d}, l2 = {w | k   f (w)k    
p2    2  d and   min(h(w))          }, and l3 = lc
we could makep2    2  d < min{  ,     }. under this choice, we know from de   nition 2.3 of
(  ,   ,   ,   )-strict saddlethat l3 is the locally   -strongly convex region which is   o(     )-close

2. by choosing small enough   max,

1     lc

to some local minimum.

we shall    rst prove that within   o( 1

   ) steps with id203 at least 1       /2 one of wt
is in l3. then by lemma a.2 we know with id203 at most   /2 there exists a wt that
is in l3 but the last point is not. by union bound we will get the main result.

  2 log 1

to prove within   o( 1

  2 log 1

show starting from any point, in   o( 1

   ) steps with id203 at least 1      /2 one of wt is in l3, we    rst
  2 ) steps with id203 at least 1/2 one of wt is in l3.

then we can repeat this log 1/   times to get the high id203 result.

de   ne stochastic process {  i} s.t.   0 = 0, and

  i+1 =                     

  i + 1

  i + t (w  i)

if w  i     l1     l3
if w  i     l2

(a.57)

where t (w  i) is de   ned by eq.(a.21) with   0 =   min(h(w  i))and we know t     tmax =   o( 1
   ).

by lemma a.1 and lemma a.3, we know:

e[f (w  i+1)     f (w  i)|w  i     l1, f  i   1] = e[f (w  i+1)     f (w  i)|w  i     l1]           o(  2) (a.58)
e[f (w  i+1)     f (w  i)|w  i     l2, f  i   1] = e[f (w  i+1)     f (w  i)|w  i     l2]           o(  )
(a.59)

therefore, combine above equation, we have:

e[f (w  i+1)    f (w  i)|w  i 6    l3, f  i   1] = e[f (w  i+1)    f (w  i)|w  i 6    l3]        (  i+1       i)   o(  2)
(a.60)

171

de   ne event ei = {   j     i, w  j     l3}, clearly ei     ei+1, thus p (ei)     p (ei+1). finally,
consider f (w  i+1)1ei, we have:

ef (w  i+1)1ei     ef (w  i)1ei   1     b    p (ei     ei   1) + e[f (w  i+1)     f (w  i)|ei]    p (ei)

    b    p (ei     ei   1)     (  i+1       i)   o(  2)p (ei)

(a.61)

therefore, by summing up over i, we have:

ef (w  i)1ei     f (w0)     bp (ei)       i   o(  2)p (ei)     b       i   o(  2)p (ei)

(a.62)

since |f (w  i)1ei| < b is bounded, as   i grows to as large as 6b
that is, after   o( 1

  2 , we must have p (ei) < 1
2.
  2 ) steps, with at least id203 1/2, {wt} have at least enter l3 once.
since this argument holds for any starting point, we can repeat this log 1/   times and we

know after   o( 1

  2 log 1/  ) steps, with id203 at least 1      /2, {wt} have at least enter l3

once.

combining with lemma a.2, and by union bound we know after   o( 1

  2 log 1/  ) steps, with

id203 at least 1     , wt will be in the   o(q   log 1

     ) neigborhood of some local minimum.

a.2 detailed analysis for section 2.2 in constrained

case

so far, we have been discussed all about unconstrained problem. in this section we extend

our result to equality constraint problems under some mild conditions.

172

consider the equality constrained optimization problem:

min

w

f (w)

s.t.

ci(w) = 0,

i = 1,         , m

(a.63)

de   ne the feasible set as the set of points that satisfy all the constraints w = {w | ci(w) =
0; i = 1,         , m}.

in this case, the algorithm we are running is projected noisy id119. let function

  w (v) to be the projection to the feasible set where the projection is de   ned as the global
solution of minw   w kv     wk2.

with same argument as in the unconstrained case, we could slightly simplify and convert it

to standard projected stochastic id119 (psgd) with update equation:

vt = wt   1          f (wt   1) +   t   1
wt =   w (vt)

(a.64)

(a.65)

as in unconstrained case, we are interested in noise    is i.i.d satisfying e   = 0, e    t =   2i
and k  k     q almost surely. our proof can be easily extended to algorithm 2 with 1
e    t (cid:22) (q + 1
optimization problems (most these materials can be found in [164]), then we prove some

di (cid:22)
in this section we    rst introduce basic tools for handling constrained

d)i.

technical lemmas that are useful for dealing with the projection step in psgd,    nally we

point out how to modify the previous analysis.

173

a.2.1 preliminaries

often for constrained optimization problems we want the constraints to satisfy some regular-

ity conditions. licq (linear independent constraint quanti   cation) is a common assumption

in this context.

de   nition a.1 (licq). in equality-constraint problem eq.(a.63), given a point w, we say

that the linear independence constraint quali   cation (licq) holds if the set of constraint

gradients {   ci(x), i = 1,         , m} is linearly independent.

in constrained optimization, we can locally transform it to an unconstrained problem by

introducing lagrangian multipliers. the langrangian l can be written as

l(w,   ) = f (w)    

  ici(w)

mxi=1

(a.66)

then, if licq holds for all w     w, we can properly de   ne function      (  ) to be:

     (w) = arg min

   k   f (w)    

mxi=1

  i   ci(w)k = arg min

   k   wl(w,   )k

(a.67)

where      (  ) can be calculated analytically: let matrix c(w) = (   c1(w),         ,   cm(w)), then
we have:

     (w) = c(w)      f (w) = (c(w)t c(w))   1c(w)t   f (w)

(a.68)

where (  )    is moore-penrose pseudo-inverse.

in our setting we need a stronger regularity condition which we call robust licq (rlicq).

de   nition a.2 (   c-rlicq ). in equality-constraint problem eq.(a.63), given a point w,

we say that   c-robust linear independence constraint quali   cation (   c-rlicq ) holds if the

174

minimum singular value of matrix c(w) = (   c1(w),         ,   cm(w)) is greater or equal to   c,
that is   min(c(w))       c.

remark. given a point w     w,   c-rlicq implies licq. while licq holds for all w     w
is a necessary condition for      (w) to be well-de   ned; it   s easy to check that   c-rlicq holds
for all w     w is a necessary condition for      (w) to be bounded. later, we will also see
  c-rlicq combined with the smoothness of {ci(w)}m
i=1 guarantee the curvature of constraint
manifold to be bounded everywhere.

note that we require this condition in order to provide a quantitative bound, without this

assumption there can be cases that are exponentially close to a function that does not satisfy

licq.

we can also write down the    rst-order and second-order partial derivative of lagrangian l
at point (w,      (w)):

  (w) =    wl(w,   )|(w,     (w)) =    f (w)    

m(w) =    2

wwl(w,   )|(w,     (w)) =    2f (w)    

mxi=1
     i (w)   ci(w)
mxi=1

     i (w)   2ci(w)

(a.69)

(a.70)

de   nition a.3 (tangent space and normal space). given a feasible point w     w, de   ne
its corresponding tangent space to be t (w) = {v |    ci(w)t v = 0; i = 1,         , m}, and
normal space to be t c(w) = span{   c1(w)         ,   cm(w)}

if w     rd, and we have m constraint satisfying   c-rlicq , the tangent space would be
a linear subspace with dimension d     m; and the normal space would be a linear subspace
with dimension m. we also know immediately that   (w) de   ned in eq.(a.69) has another

interpretation: it   s the component of gradient    f (w) in tangent space.

175

also, it   s easy to see the normal space t c(w) is the orthogonal complement of t . we can
also de   ne the projection matrix of any vector onto tangent space (or normal space) to be

pt (w) (or pt c(w)). then, clearly, both pt (w) and pt c(w) are orthoprojector, thus symmetric.
also by pythagorean theorem, we have:

kvk2 = kpt (w)vk2 + kpt c(w)vk2,

   v     rd

(a.71)

taylor expansion let w, w0     w, and    x       =      (w0) independent of w, assume    2
is   l-lipschitz, that is k   2
we have:

wwl(w,      )
wwl(w2,      )k       lkw1   w2k by taylor expansion,

wwl(w1,      )      2

l(w,      )    l(w0,      ) +    wl(w0,      )t (w     w0)

+

1
2

(w     w0)t   2

wwl(w0,      )(w     w0) +

  l
6 kw     w0k3

(a.72)

since w, w0 are feasible, we know: l(w,      ) = f (w) and l(w0,      ) = f (w0), this gives:

f (w)     f (w0) +   (w0)t (w     w0) +

1
2

(w     w0)t m(w0)(w     w0) +

  l
6 kw     w0k3

(a.73)

derivative of   (w) by taking derative of   (w) again, we know the change of this tangent

gradient can be characterized by:

     (w) = h    

mxi=1

     i (w)   2ci(w)    

mxi=1

   ci(w)        i (w)t

denote

n(w) =    

mxi=1

   ci(w)        i (w)t

176

(a.74)

(a.75)

we immediately know that      (w) = m(w) + n(w).

remark. the additional term n(w) is not necessary to be even symmetric in general. this

is due to the fact that   (w) may not be the gradient of any scalar function. however, n(w)
has an important property that is: for any vector v     rd, n(w)v     t c(w).

finally, for completeness, we state here the    rst/second-order necessary (or su   cient) con-

ditions for optimality. please refer to [164] for the proof of those theorems.

theorem a.2 (first-order necessary conditions). in equality constraint problem eq.(a.63),

suppose that w    is a local solution, and that the functions f and ci are continuously di   eren-

tiable, and that the licq holds at w   . then there is a lagrange multiplier vector      , such

that:

   wl(w   ,      ) = 0
ci(w   ) = 0,

for i = 1,         , m

(a.76)

(a.77)

these conditions are also usually referred as karush-kuhn-tucker (kkt) conditions.

theorem a.3 (second-order necessary conditions). in equality constraint problem eq.(a.63),

suppose that w    is a local solution, and that the licq holds at w   . let       lagrange multiplier

vector for which the kkt conditions are satis   ed. then:

vt   2

xxl(w   ,      )v     0

for all v     t (w   )

(a.78)

theorem a.4 (second-order su   cient conditions). in equality constraint problem eq.(a.63),
suppose that for some feasible point w        rd, and there   s lagrange multiplier vector       for
which the kkt conditions are satis   ed. suppose also that:

vt   2

xxl(w   ,      )v > 0

for all v     t (w   ), v 6= 0

(a.79)

177

then w    is a strict local solution.

remark. by de   nition eq.(a.68), we know immediately      (w   ) is one of valid lagrange
multipliers       for which the kkt conditions are satis   ed. this means   (w   ) =    wl(w   ,      )
and m(w   ) = l(w   ,      ).

therefore, theorem a.2, a.3, a.4 gives strong implication that   (w) and m(w) are the right
thing to look at, which are in some sense equivalent to    f (w) and    2f (w) in unconstrained
case.

a.2.2 geometrical lemmas regarding constraint manifold

since in equality constraint problem, at each step of psgd, we are e   ectively considering

the local manifold around feasible point wt   1. in this section, we provide some technical
lemmas relating to the geometry of constraint manifold in preparsion for the proof of main

theorem in equality constraint case.

we    rst show if two points are close, then the projection in the normal space is much smaller

than the projection in the tangent space.

lemma a.6. suppose the constraints {ci}m

i=1 are   i-smooth, and   c-rlicq holds for all

w     w. then, letpm

i=1

  2
i
  2
c

= 1

r2 , for any w, w0     w, let t0 = t (w0), then

kpt c

0 (w     w0)k    

1
2rkw     w0k2

furthermore, if kw     w0k < r holds, we additionally have:

0 (w     w0)k     kpt0(w     w0)k2

r

kpt c

(a.80)

(a.81)

178

proof. first, since for any vector   v     t0, we have kc(w0)t   vk = 0, then by simple linear
algebra, it   s easy to show:

kc(w0)t (w     w0)k2 =kc(w0)t pt c

0 (w     w0)k2       2

minkpt c

0 (w     w0)k2

     2

ckpt c

0 (w     w0)k2

on the other hand, by   i-smooth, we have:

|ci(w)     ci(w0)        ci(w0)t (w     w0)|    

  i
2 kw     w0k2

since w, w0 are feasible points, we have ci(w) = ci(w0) = 0, which gives:

kc(w0)t (w     w0)k2 =

mxi=1

(   ci(w0)t (w     w0))2    

  2
i
4 kw     w0k4

mxi=1

combining eq.(a.82) and eq.(a.84), and the de   nition of r, we have:

(a.82)

(a.83)

(a.84)

kpt c

0 (w     w0)k2    

1
4r2kw     w0k4 =

1
4r2 (kpt c

0 (w     w0)k2 + kpt0(w     w0)k2)2 (a.85)

solving this second-order inequality gives two solution

0 (w     w0)k     kpt0(w     w0)k2

r

kpt c

or kpt c

0 (w     w0)k     r

(a.86)

by assumption, we know kw     w0k < r (so the second case cannot be true), which    nishes
the proof.

here, we see theqpm

i=1

  2
i
  2
c

= 1

r serves as a upper bound of the curvatures on the constraint

manifold, and equivalently, r serves as a lower bound of the radius of curvature.   c-rlicq

and smoothness guarantee that the curvature is bounded.

179

next we show the normal/tangent space of nearby points are close.

lemma a.7. suppose the constraints {ci}m

i=1 are   i-smooth, and   c-rlicq holds for all
r2 , for any w, w0     w, let t0 = t (w0). then for all   v     t (w) so

= 1

w     w. letpm

that k  vk = 1, we have

  2
i
  2
c

i=1

0      vk     kw     w0k

r

kpt c

proof. with similar calculation as eq.(a.82), we immediately have:

0      vk2     kc(w0)t   vk2

min(c(w))     kc(w0)t   vk2

  2
c

  2

kpt c

(a.87)

(a.88)

since   v     t (w) , we have c(w)t   v = 0, combined with the fact that   v is a unit vector, we
have:

kc(w0)t   vk2 =k[c(w0)     c(w)]t   vk2 =

([   ci(w0)        ci(w)]t   v)2

mxi=1

   

mxi=1

k   ci(w0)        ci(w)k2k  vk2    

mxi=1

  2
i kw0     wk2

(a.89)

combining eq.(a.88) and eq.(a.89), and the de   nition of r, we concludes the proof.

lemma a.8. suppose the constraints {ci}m

i=1 are   i-smooth, and   c-rlicq holds for all
r2 , for any w, w0     w, let t0 = t (w0). then for all   v     t c(w) so

= 1

w     w. letpm

that k  vk = 1, we have

  2
i
  2
c

i=1

kpt0      vk     kw     w0k

r

(a.90)

proof. by de   nition of projection, clearly, we have pt0      v + pt c

without loss of generality, assume   v =pm

i=1   i   ci(w). de   ne   d =pm

0      v =   v. since   v     t c(w),
i=1   i   ci(w0), clearly

180

  d     t c

0 . since projection gives the closest point in subspace, we have:

kpt0      vk =kpt c

0      v       vk     k   d       vk
mxi=1
  ik   ci(w0)        ci(w)k    

   

mxi=1

  i  ikw0     wk

on the other hand, let    = (  1,         ,   m)t , we know c(w)   =   v, thus:

   = c(w)     v = (c(w)t c(w))   1c(w)t   v

(a.91)

(a.92)

therefore, by   c-rlicq and the fact   v is unit vector, we know: k  k     1
eq.(a.91), we    nished the proof.

  c

. combined with

using the previous lemmas, we can then prove that: starting from any point w0 on constraint

manifold, the result of adding any small vector v and then projected back to feasible set, is

not very di   erent from the result of adding pt (w0)v.

lemma a.9. suppose the constraints {ci}m

i=1 are   i-smooth, and   c-rlicq holds for all
r2 , for any w0     w, let t0 = t (w0). then let w1 = w0 +     v, and

  2
i
  2
c

= 1

w     w. let pm

i=1

w2 = w0 +   pt0      v, where   v     sd   1 is a unit vector. then, we have:

k  w (w1)     w2k    

4  2
r

(a.93)

where projection   w (w) is de   ned as the closet point to w on feasible set w.

proof. first, note that kw1     w0k =   , and by de   nition of projection, there must exist a
project   w (w) inside the ball b  (w1) = {w | kw     w1k       }.

181

denote u1 =   w (w1), and clearly u1     w. we can formulate u1 as the solution to following
constrained optimization problems:

min

u

s.t.

kw1     uk2
ci(u) = 0,

i = 1,         , m

(a.94)

since function f (u) = kw1   uk2 and ci(u) are continuously di   erentiable by assumption, and
the condition   c-rlicq holds for all w     w implies that licq holds for u1. therefore, by
karush-kuhn-tucker necessary conditions, we immediately know (w1     u1)     t c(u1).

since u1     b  (w1), we know kw0     u1k     2  , by lemma a.8, we immediately have:

kpt0(w1     u1)k = kpt0(w1     u1)k
kw1     u1k

kw1     u1k    

1
rkw0     u1k    kw1     u1k    

2
r

  2 (a.95)

let v1 = w0 + pt0(u1     w0), we have:

(a.96)

(a.97)

kv1     w2k =k(v1     w0)     (w2     w0)k = kpt0(u1     w0)     pt0(w1     w0)k

=kpt0(w1     u1)k    

2
r

  2

on the other hand by lemma a.6, we have:

ku1     v1k = kpt c

0 (u1     w0)k    

1
2rku1     w0k2    

2
r

  2

combining eq.(a.96) and eq.(a.97), we    nished the proof.

182

a.2.3 main theorem

now we are ready to prove the main theorems. first we revise the de   nition of strict saddle in

the constrained case.

de   nition a.4. a twice di   erentiable function f (w) with constraints ci(w) is (  ,   ,   ,   )-

strict saddle, if for any point w one of the following is true

1. k  (w)k       .

2.   vt m(w)  v           for some   v     t (w), k  vk = 1

3. there is a local minimum w    such that kw     w   k       , and for all w    in the 2   neigh-

borhood of w   , we have   vt m(w   )  v        for all   v     t (w   ), k  vk = 1

next, we prove a equivalent formulation for psgd.

lemma a.10. suppose the constraints {ci}m
i=1 are   i-smooth, and   c-rlicq holds for all
w     w. furthermore, if function f is l-lipschitz, and the noise    is bounded, then running
psgd as in eq.(a.64) is equivalent to running:

wt = wt   1           (  (wt   1) + pt (wt   1)  t   1) +   t   1

(a.98)

where    is the correction for projection, and k  k       o(  2).

proof. lemma a.10 is a direct corollary of lemma a.9.

the intuition behind this lemma is that: when {ci}m
i=1 are smooth and   c-rlicq holds for
all w     w, then the constraint manifold has bounded curvature every where. then, if we
only care about    rst order behavior, it   s well-approximated by the local dynamic in tangent

plane, up to some second-order correction.

183

therefore, by eq.(a.98), we see locally it   s not much di   erent from the unconstrainted case

eq.(a.1) up to some negeligable correction. in the following analysis, we will always use

formula eq.(a.98) as the update equation for psgd.

since most of following proof bears a lot similarity as in unconstrained case, we only pointed

out the essential steps in our following proof.

theorem a.5 (main theorem for equality-constrained case). suppose a function f (w) :
rd     r with constraints ci(w) : rd     r is (  ,   ,   ,   )-strict saddle, and has a stochastic
gradient oracle with radius at most q, also satisfying e   = 0 and e    t =   2i. further,

suppose the function function f is b-bounded, l-lipschitz,   -smooth, and has   -lipschitz
hessian, and the constraints {ci}m
i=1 is li-lipschitz,   i-smooth, and has   i-lipschitz hes-
sian. then there exists a threshold   max =     (1), so that for any    > 0, and for any
         max/ max{1, log(1/  )}, with id203 at least 1        in t =   o(     2 log(1/  )) iterations,
psgd outputs a point wt that is   o(p   log(1/    ))-close to some local minimum w   .

first, we proof the assumptions in main theorem implies the smoothness conditions for
m(w), n(w) and    2

wwl(w,      (w   )).

lemma a.11. under the assumptions of theorem a.5, there exists   m ,   n ,   m ,   n ,   l poly-

nomial related to b, l,   ,   , 1
  c

and {li,   i,   i}m

i=1 so that:

1. km(w)k       m and kn(w)k       n for all w     w.

2. m(w) is   m -lipschitz, and n(w) is   n -lipschitz, and    2

wwl(w,      (w   )) is   l-lipschitz

for all w        w.

wwl(w,      (w   )), the above conditions will holds
proof. by de   nition of m(w), n(w) and    2
if there exists b  , l  ,      bounded by   o(1), so that      (w) is b  -bounded, l  -lipschitz, and

    -smooth.

184

by de   nition eq.(a.68), we have:

     (w) = c(w)      f (w) = (c(w)t c(w))   1c(w)t   f (w)

(a.99)

because f is b-bounded, l-lipschitz,   -smooth, and its hessian is   -lipschitz, thus, even-

tually, we only need to prove that there exists bc, lc,   c bounded by   o(1), so that the

pseudo-inverse c(w)    is bc-bounded, lc-lipschitz, and   c-smooth.

since   c-rlicq holds for all feasible points, we immediately have: kc(w)   k     1
bounded. for simplicity, in the following context we use c    to represent c   (w) without

, thus

  c

ambiguity. by some calculation of id202, we have the derivative of pseudo-inverse:

   c(w)   

   wi

=    c   

   c(w)

   wi

c    + c   [c   ]t    c(w)t
   wi

(i     cc   )

(a.100)

again,   c-rlicq holds implies that derivative of pseudo-inverse is well-de   ned for every

feasible point. let tensor e(w),   e(w) to be the derivative of c(w), c   (w), which is de   ned

as:

[e(w)]ijk =

   [c(w)]ik

   wj

[   e(w)]ijk =

   [c(w)   ]ik

   wj

(a.101)

de   ne the transpose of a 3rd order tensor et

i,j,k = ek,j,i, then we have

  e(w) =    [e(w)](c   , i, c   ) + [e(w)t ](c   [c   ]t , i, (i     cc   ))

(a.102)

where by calculation [e(w)](i, i, ei) =    2ci(w).

finally, since c(w)    and    2ci(w) are bounded by   o(1), by eq.(a.102), we know   e(w) is
bounded, that is c(w)    is lipschitz. again, since both c(w)    and    2ci(w) are bounded,
lipschitz, by eq.(a.102), we know   e(w) is also   o(1)-lipschitz. this    nishes the proof.

185

from now on, we can use the same proof strategy as unconstraint case. below we list the

corresponding lemmas and the essential steps that require modi   cations.

lemma a.12. under the assumptions of theorem a.5, with notations in lemma a.11,

for any point with k  (w0)k     p2    2  m (d     m) where p2    2  m (d     m) <   , after one

iteration we have:

ef (w1)     f (w0)              (  2)

(a.103)

proof. choose   max < 1
  m

, and also small enough, then by update equation eq.(a.98), we

have:

ef (w1)     f (w0)       (w0)t e(w1     w0) +

  m
2
)k  (w0)k2 +

ekw1     w0k2
  2  2  m (d     m)

2
)k  (w0)k2 +

  m   2

2

  2  2  m (d     m)

2

+   o(  2)k  (w0)k +   o(  3)

+   o(  3)

(a.104)

  m   2

2

       (      
       (         o(  1.5)    
       

  2  2  m d

4

which    nishes the proof.

theorem a.6. under the assumptions of theorem a.5, with notations in lemma a.11, for
any initial point w0 that is   o(     ) <    close to a local minimum w   , with id203 at least

1       /2, we have following holds simultaneously:

   t       o(

1
  2 log

1
  

),

kwt     w   k       o(r   log

1
    

) <   

where w    is the locally optimal point.

186

(a.105)

proof. by calculus, we know

  (wt) =  (w   ) +z 1

0

(m + n)(w    + t(wt     w   ))dt    (wt     w   )

(a.106)

let    ltration ft =   {  0,           t   1}, and note   {   0,         ,    t}     ft, where   {  } denotes the
     <   }, where    is independent

sigma    eld. let event et = {          t,kw       w   k       q   log 1

of (  ,   ), and will be speci   ed later.

by de   nition a.4 of (  ,   ,   ,   )-strict saddle, we know m(w) is locally   -strongly convex
restricted to its tangent space t (w). in the 2  -neighborhood of w   . if   max is chosen small
enough, by remark a.2.1 and lemma a.6, we have in addition:

  (wt)t (wt     w   )1et = (wt     w   )tz 1

0

(m + n)(w    + t(wt     w   ))dt    (wt     w   )1et
    [  kwt     w   k2       o(kwt     w   k3)]1et     0.5  kwt     w   k21et

(a.107)

then, everything else follows almost the same as the proof of lemma a.2.

lemma a.13. under the assumptions of theorem a.5, with notations in lemma a.11, for
any initial point w0 where k  (w0)k       o(  ) <   , and   vt m(w0)  v           for some   v     t (w),
k  vk = 1, then there is a number of steps t that depends on w0 such that:

ef (wt )     f (w0)              (  )

(a.108)

the number of steps t has a    xed upper bound tmax that is independent of w0 where t    
tmax = o((log(d     m))/    ).

similar to the unconstrained case, we show this by a coupling sequence. here the sequence

we construct will only walk on the tangent space, by lemmas in previous subsection, we

187

know this is not very far from the actual sequence. we    rst de   ne and characterize the

coupled sequence in the following lemma:

lemma a.14. under the assumptions of theorem a.5, with notations in lemma a.11. let
  f de   ned as local second-order approximation of f (x) around w0 in tangent space t0 = t (w0):

  f (w)

.
= f (w0) +   (w0)t (w     w0) +

1
2

(w     w0)t [p t
t0

m(w0)pt0](w     w0)

(a.109)

{   wt} be the corresponding sequence generated by running sgd on function   f , with   w0 = w0,
and noise projected to t0, (i.e.   wt =   wt   1       (     (   wt   1) + pt0  t   1). for simplicity, denote

    (w) =       f (w), andfm = p t

t0

m(w0)pt0, then we have analytically:

    (   wt) = (1       fm)t     (   w0)       fm

t   1x   =0
(1       fm)t        1pt0    
t   1x   =0
(1       fm)       (   w0)       

(1       fm)t        1pt0    

t   1x   =0

  wt     w0 =      

(a.110)

(a.111)

further, for any initial point w0 where k  (w0)k       o(  ) <   , and min  v   t (w),k  vk=1   vt m(w0)  v
=      0. there exist a t     n satisfying:

d     m
    0    

t   1x   =0

(1 +     0)2   <

3(d     m)

    0

(a.112)

with id203 at least 1       o(  3), we have following holds simultaneously for all t     t :

k   wt     w0k       o(  

1

2 log

1
  

);

k     (   wt)k       o(  

1

2 log

1
  

)

proof. clearly we have:

    (   wt) =     (   wt   1) +fm(   wt       wt   1)

188

(a.113)

(a.114)

and

  wt =   wt   1       (     (   wt   1) + pt0  t   1)

(a.115)

this lemma is then proved by a direct application of lemma a.4.

then we show the sequence constructed is very close to the actual sequence.

lemma a.15. under the assumptions of theorem a.5, with notations in lemma a.11. let
{wt} be the corresponding sequence generated by running psgd on function f . also let   f
and {   wt} be de   ned as in lemma a.14. then, for any initial point w0 where k  (w0)k2    
  o(  ) <   , and min  v   t (w),k  vk=1   vt m(w0)  v =      0. given the choice of t as in eq.(a.112),
with id203 at least 1       o(  2), we have following holds simultaneously for all t     t :

kwt       wtk       o(   log2 1

  

);

(a.116)

proof. first, we have update function of tangent gradient by:

  (wt) =  (wt   1) +z 1

0      (wt   1 + t(wt     wt   1))dt    (wt     wt   1)

=  (wt   1) + m(wt   1)(wt     wt   1) + n(wt   1)(wt     wt   1) +   t   1

(a.117)

where the remainder:

  t   1    z 1

0

[     (wt   1 + t(wt     wt   1))          (wt   1)] dt    (wt     wt   1)

(a.118)

189

project it to tangent space t0 = t (w0). denotefm = p t

m(w0) ]pt0. then, we have:

t0

m(w0)pt0, andfm   t   1 = p t

t0[ m(wt1)   

pt0      (wt) =pt0      (wt   1) + pt0(m(wt   1) + n(wt   1))(wt     wt   1) + pt0  t   1

=pt0      (wt   1) + pt0m(wt   1)pt0(wt     wt   1)

+ pt0m(wt   1)pt c

0 (wt     wt   1) + pt0n(wt   1)(wt     wt   1) + pt0  t   1

=pt0      (wt   1) +fm(wt     wt   1) +   t   1

where

  t   1 = [fm   t   1 + pt0m(wt   1)pt c

0 + pt0n(wt   1) ](wt     wt   1) + pt0  t   1

by hessian smoothness, we immediately have:

(a.119)

(a.120)

kfm   t   1k = km(wt1)     m(w0)k       mkwt   1     w0k       m (kwt       wtk + k   wt     w0k)

(a.121)

k  t   1k    

  m +   n

2

kwt     wt   1k2

(a.122)

substitute the update equation of psgd (eq.(a.98)) into eq.(a.119), we have:

pt0      (wt) = pt0      (wt   1)       fm(pt0      (wt   1) + pt0    pt (wt   1)  t   1) +fm      t   1 +   t   1
= (1       fm)pt0      (wt   1)       fmpt0  t   1 +   fmpt0    pt c(wt   1)  t   1 +fm      t   1 +   t   1

(a.123)

190

let    t = pt0      (wt)         (   wt) denote the di   erence of tangent gradient in t (w0), then from
eq.(a.114), eq.(a.115), and eq.(a.123) we have:

   t = (1       h)   t   1 +   fmpt0    pt c(wt   1)  t   1 +fm      t   1 +   t   1

pt0    (wt     w0)     (   wt     w0) =      

pt0    pt c(w   )     +

      +   

t   1x   =0

t   1x   =0

by lemma a.6, we know ifpm
0 (wt     w0)k     kwt     w0k2

kpt c

2r

i=1

  2
i
  2
c

= 1

r2 , then we have:

    

t   1x   =0

(a.124)

(a.125)

(a.126)

let    ltration ft =   {  0,           t   1}, and note   {   0,         ,    t}     ft, where   {  } denotes the
sigma    eld. also, let event kt = {          t, k     (   w   )k       o(  
   )},
and denote   t =   pt   1
   ,k    k    
  } where (  1,   2,   3) are is independent of (  ,   ), and will be
determined later. to prevent ambiguity in the proof,   o notation will not hide any dependence

   ), k   w      w0k       o(  
   =0 pt0    pt c(w   )     , let et = {          t, k     k       1   log2 1

   ,kw         w  k       3   log2 1

  2   log2 1

2 log 1

2 log 1

1

1

on   . clearly event kt   1     ft   1, et   1     ft   1 thus independent of   t   1.

1

2 log 1

then, conditioned on event kt   1     et   1, by triangle inequality, we have kw       w0k    
  o(  
   ), for all        t     1     t     1. we then need to carefully bound the following
bound each term in eq.(a.124). we know wt     wt   1 =          (  (wt   1) + pt (wt   1)  t   1) +   t   1,

191

and then by lemma a.8 and lemma a.7, we have:

1
  

)

k  fmpt0    pt c(wt   1)  t   1k       o(  1.5 log
kfm      t   1k       o(  2)
0 + pt0n(wt   1) ](           (wt   1))k       o(  2 log2 1
0 + pt0n(wt   1) ](     pt (wt   1)  t   1)k       o(  1.5 log

  
1
  

)

k[fm   t   1 + pt0m(wt   1)pt c

0 + pt0n(wt   1) ]  t   1k       o(  2)
kpt0  t   1k       o(  2)

)

(a.127)

k[fm   t   1 + pt0m(wt   1)pt c
k[fm   t   1 + pt0m(wt   1)pt c

therefore, abstractly, conditioned on event kt   1     et   1, we could write down the recursive
equation as:

   t = (1       h)   t   1 + a + b

(a.128)

where kak       o(  1.5 log 1
   ), and in addition, by independence, easy
to check we also have e[(1       h)   t   1a|ft   1] = 0. this is exactly the same case as in the
proof of lemma a.5. by the same argument of martingale and azuma-hoe   ding, and by

   ) and kbk       o(  2 log2 1

choosing   1 large enough, we can prove

  (cid:27)(cid:19)       o(  3)

p(cid:18)et   1    (cid:26)k   tk       1   log2 1
on the other hand, for   t =   pt   1

   =0 pt0    pt c(w   )     , we have:

e[  t1kt   1   et   1|ft   1] =(cid:2)  t   1 +   e[pt0    pt c(wt   1)  t   1|ft   1](cid:3) 1kt   1   et   1

=   t   11kt   1   et   1       t   11kt   2   et   2

(a.129)

(a.130)

192

therefore, we have e[  t1kt   1   et   1 | ft   1]       t   11kt   2   et   2 which means   t1kt   1   et   1 is a
supermartingale.

we also know by lemma a.8, with id203 1:

|  t1kt   1   et   1     e[  t1kt   1   et   1 | ft   1]| = |  pt0    pt c(wt   1)  t   1|    1kt   1   et   1
      o(  )kwt   1     w0k1kt   1   et   1       o(  1.5 log

) = ct   1

1
  

(a.131)

by azuma-hoe   ding inequality, with id203 less than   o(  3), for t     t     o(log(d    
m)/  0  ):

  t1kt   1   et   1       0    1 >   o(1)vuut
t   1x   =0

c2
   log(

1
  

) =   o(   log2 1
  

)

this means there exists some   c2 =   o(1) so that:

p(cid:18)kt   1     et   1    (cid:26)k  tk       c2   log2 1

  (cid:27)(cid:19)       o(  3)

by choosing   2 >   c2, we have:

p(cid:18)kt   1     et   1    (cid:26)k  tk       2   log2 1

  (cid:27)(cid:19)       o(  3)

therefore, combined with lemma a.14, we have:

p(cid:18)et   1    (cid:26)k  tk       2   log2 1

  (cid:27)(cid:19)       o(  3) + p (kt   1)       o(  3)

(a.132)

(a.133)

(a.134)

(a.135)

finally, conditioned on event kt   1     et   1, if we have k  tk       2   log2 1

   , then by eq.(a.125):

  (cid:19)
kpt0    (wt     w0)     (   wt     w0)k       o(cid:18)(  1 +   2)   log2 1

(a.136)

193

since kwt   1     w0k       o(  

1

2 log 1

   ), and kwt     wt   1k       o(  ), by eq.(a.126):

0 (wt     w0)k     kwt     w0k2

2r

kpt c

      o(   log2 1

  

)

(a.137)

thus:

kwt       wtk2 =kpt0    (wt       wt)k2 + kpt c

0    (wt       wt)k2
=kpt0    (wt     w0)     (   wt     w0)k2 + kpt c

0 (wt     w0)k2       o((  1 +   2)2  2 log4 1

  

)

(a.138)

that is there exist some   c3 =   o(1) so that kwt       wtk       c3(  1 +   2)   log2 1
   therefore,
conditioned on event kt   1    et   1, we have proved that if choose   3 >   c3(  1 +   2), then event
{kwt       wtk       3   log2 1
  }. then, combined this fact with eq.(a.129),
eq.(a.135), we have proved:

  }     {k  tk       2   log2 1

p(cid:0)et   1     et(cid:1)       o(  3)

(a.139)

because p (e0) = 0, and t       o( 1

   ), we have p (et )       o(  2), which concludes the proof.

these two lemmas allow us to prove the result when the initial point is very close to a saddle

point.

proof of lemma a.13. combine talyor expansion eq.a.73 with lemma a.14, lemma a.15,

we prove this lemma by the same argument as in the proof of lemma a.3.

finally the main theorem follows.

194

proof of theorem a.5. by lemma a.12, lemma a.13, and lemma a.6, with the same ar-

gument as in the proof theorem a.1, we easily concludes this proof.

a.3 detailed proofs for section 2.3

in this section we show two optimization problems (2.9) and (2.11) satisfy the (  ,   ,   ,   )-

strict saddle propery.

a.3.1 warm up: maximum eigenvalue formulation

recall that we are trying to solve the optimization (2.9), which we restate here.

max t (u, u, u, u),

kuk2 = 1.

(a.140)

. we    rst do a change of

here the tensor t has orthogonal decomposition t = pd
dynamics of the algorithm). in particular, let u =pd
t (u, u, u, u) =pd

coordinates to work in the coordinate system speci   ed by (ai)   s (this does not change the
i=1 xiai (where x     rd), then we can see
4, the optimization problem is equivalent

i . therefore let f (x) =    kxk4

i=1 a   4

i=1 x4

i

to

min

f (x)

s.t.

kxk2

2 = 1

(a.141)

this is a constrained optimization, so we apply the framework developed in section 2.2.3.

195

let c(x) = kxk2

2     1. we    rst compute the lagrangian

l(x,   ) = f (x)       c(x) =    kxk4

4       (kxk2

2     1).

(a.142)

since there is only one constraint, and the gradient when kxk = 1 always have norm 2, we
know the set of constraints satisfy 2-rlicq. in particular, we can compute the correct value

of lagrangian multiplier   ,

     (x) = arg min

   k   xl(x,   )k = arg min

  

dxi=1

(2x3

i +   xi)2 =    2kxk4

4

therefore, the gradient in the tangent space is equal to

  (x) =    xl(x,   )|(x,     (x)) =    f (x)          (x)   c(x)

=    4(x3
= 4(cid:0)(x2

1,         , x3
1     kxk4

d)t     2     (x)(x1,         , xd)t
4)xd(cid:1)
4)x1,         , (x2
d     kxk4

the second-order partial derivative of lagrangian is equal to

m(x) =    2

xxl(x,   )|(x,     (x)) =    2f (x)          (x)   2c(x)

=    12diag(x2
=    12diag(x2

1,         , x2
1,         , x2

d)     2     (x)id
d) + 4kxk4
4id

196

(a.143)

(a.144)

(a.145)

since the variable x has bounded norm, and the function is a polynomial, it   s clear that the

function itself is bounded and all its derivatives are bounded. moreover, all the derivatives

of the constraint are bounded. we summarize this in the following lemma.

lemma a.16. the objective function (2.9) is bounded by 1, its p-th order derivative is
bounded by o(   d) for p = 1, 2, 3. the constraint   s p-th order derivative is bounded by 2, for

p = 1, 2, 3.

therefore the function satisfy all the smoothness condition we need. finally we show the

gradient and hessian of lagrangian satisfy the (  ,   ,   ,   )-strict saddle property. note that

we did not try to optimize the dependency with respect to d.

theorem a.7. the only local minima of optimization problem (2.9) are   ai (i     [d]).
further it satisfy (  ,   ,   ,   )-strict saddle for    = 7/d,    = 3 and   ,    = 1/poly(d).

in order to prove this theorem, we consider the transformed version eq.a.141. we    rst need

following two lemma for points around saddle point and local minimum respectively. we

choose

  0 = (10d)   4,

   = 4  2

0,    = 2d  0, s(x) = {i | |xi| >   0}

(a.146)

where by intuition, s(x) is the set of coordinates whose value is relative large.

lemma a.17. under the choice of parameters in eq.(a.146), suppose k  (x)k       , and
|s(x)|     2. then, there exists   v     t (x) and k  vk = 1, so that   vt m(x)  v        7/d.

proof. suppose |s(x)| = p, and 2     p     d. since k  (x)k        = 4  2
for each i     [d], |[  (x)]i| = 4|(x2

4)xi|     4  2

i     kxk4

0. therefore, we have:

0, by eq.(a.144), we have

   i     s(x),

|x2
i     kxk4

4|       0

(a.147)

197

and thus:

|kxk4

4    

   |kxk4

4    

4    

1
p| = |kxk4
p xi   s(x)
1

x2
i| + |

1

x2
i|

pxi
p xi   [d]   s(x)

1

combined with eq.a.147, this means:

   i     s(x),

|x2
i    

1
p|     3  0

x2
i|       0 +

d     p
p

  2
0     2  0

(a.148)

(a.149)

because of symmetry, wlog we assume s(x) = {1,         , p}. since |s(x)|     2, we can pick
  v = (a, b, 0,         , 0). here a > 0, b < 0, and a2 + b2 = 1. we pick a such that ax1 + bx2 = 0.
the solution is the intersection of a radius 1 circle and a line which passes (0, 0), which
always exists. for this   v, we know k  vk = 1, and   vt x = 0 thus   v     t (x). we have:

1 + 4kxk4

  vt m(x)  v =    (12x2
=     8x2
8
       
p

2b2     4(x2
1a2     8x2
+ 24  0 + 4  0        7/d

4)a2     (12x2

1     kxk4

4))a2     4(x2

2 + 4kxk4
4)b2
2     kxk4

4))b2

(a.150)

which    nishes the proof.

lemma a.18. under the choice of parameters in eq.(a.146), suppose k  (x)k       , and
|s(x)| = 1. then, there is a local minimum x    such that kx     x   k       , and for all x    in the
2   neighborhood of x   , we have   vt m(x   )  v     3 for all   v     t (x   ), k  vk = 1

proof. wlog, we assume s(x) = {1}. then, we immediately have for all i > 1, |xi|       0,
and thus:

1     x2

1 = 1    xi>1

x2
i     1     d  2

0

198

(a.151)

therefore x1     p1     d  2
close to    1. by symmetry, we know wlog, we can assume the case x1    p1     d  2

0 or x1        p1     d  2

0. which means x1 is either close to 1 or

0. let

e1 = (1, 0,         , 0), then we know:

kx     e1k2     (x1     1)2 +xi>1

x2
i     2d  2

0       2

(a.152)

next, we show e1 is a local minimum. according to eq.a.145, we know m(e1) is a diagonal

matrix with 4 on the diagonals except for the    rst diagonal entry (which is equal to    8),
since t (e1) = span{e2,         , ed}, we have:

vt m(e1)v     4kvk2 > 0

for all v     t (e1), v 6= 0

(a.153)

which by theorem a.4 means e1 is a local minimum.

finally, denote t1 = t (e1) be the tangent space of constraint manifold at e1. we know for
all x    in the 2   neighborhood of e1, and for all   v     t (x   ), k  vk = 1:

  vt m(x   )  v      vt m(e1)  v     |  vt m(e1)  v       vt m(x   )  v|

=4kpt1   vk2     8kpt c
=4     12kpt c

1   vk2     km(e1)     m(x   )k

1   vk2     km(e1)     m(x   )kk  vk2

by lemma a.7, we know kpt c

1   vk2     kx        e1k2     4  2. by eq.(a.145), we have:

km(e1)     m(x   )k     km(e1)     m(x   )k    x(i,j)
   xi
4(cid:12)(cid:12)     64d  
(cid:12)(cid:12)   12[e1]2

i + 4ke1k4

i + 4kxk4

4     12x2

|[m(e1)]ij     [m(x   )]ij|

in conclusion, we have   vt m(x   )  v     4     48  2     64d       3 which    nishs the proof.

199

(a.154)

(a.155)

finally, we are ready to prove theorem a.7.

proof of theorem a.7. according to lemma a.17 and lemma a.18, we immediately know

the optimization problem satis   es (  ,   ,   ,   )-strict saddle.

the only thing remains to show is that the only local minima of optimization problem (2.9)

are   ai (i     [d]). which is equivalent to show that the only local minima of the transformed
problem is   ei (i     [d]), where ei = (0,         , 0, 1, 0,         , 0), where 1 is on i-th coordinate.

by investigating the proof of lemma a.17 and lemma a.18, we know these two lemmas
actually hold for any small enough choice of   0 satisfying   0     (10d)   4, by pushing   0     0,
we know for any point satisfying |  (x)|            0, if it is close to some local minimum, it
must satisfy 1 = |s(x)|     supp(x). therefore, we know the only possible local minima are
  ei (i     [d]). in lemma a.18, we proved e1 is local minimum, by symmetry, we    nishes the
proof.

a.3.2 new formulation

in this section we consider our new formulation (2.11). we    rst restate the optimization

problem here:

t (u(i), u(i), u(j), u(j)),

min xi6=j
   i ku(i)k2 = 1.

(a.156)

note that we changed the notation for the variables from ui to u(i), because in later proofs

we will often refer to the particular coordinates of these vectors.

200

similar to the previous section, we perform a change of basis. the e   ect is equivalent to

making ai   s equal to basis vectors ei (and hence the tensor is equal to t =pd

the transformation the equations become

i=1 e   4

i

. after

(a.157)

s.t.

h(u(i), u(j))

   i     [d]

min x(i,j):i6=j
ku(i)k2 = 1
here h(u(i), u(j)) =pd
let u     rd2 be the concatenation of {u(i)} such that uij = u(i)
f (u) = 1

simplify the calculation.

k=1(u(i)

k u(j)

2p(i,j):i6=j h(u(i), u(j)). we can then compute the lagrangian

k )2, (i, j)     [d]2. we divided the objective function by 2 to

j . let ci(u) = ku(i)k2     1 and

l(u,   ) = f (u)    

  ici(u) =

dxi=1

1

2 x(i,j):i6=j

h(u(i), u(j))    

dxi=1

  i(ku(i)k2     1)

(a.158)

the gradients of ci(u)   s are equal to (0,         , 0, 2u(i), 0,         , 0)t , all of these vectors are or-
thogonal to each other (because they have disjoint supports) and have norm 2. therefore

the set of constraints satisfy 2-rlicq. we can then compute the lagrangian multipiers      

as follows

     (u) = arg min

   k   ul(u,   )k = arg min

  

4xi xk

(xj:j6=i

u 2
jkuik       iuik)2

(a.159)

which gives:

     i (u) = arg min

   xk

(xj:j6=i

u 2

jkuik       iuik)2 = xj:j6=i

h(u(j), u(i))

(a.160)

201

therefore, gradient in the tangent space is equal to

  (u) =    ul(u,   )|(u,     (u )) =    f (u)    

nxi=1

     i (u)   ci(u).

(a.161)

the gradient is a d2 dimensional vector (which can be viewed as a d  d matrix corresponding
to entries of u), and we express this in a coordinate-by-coordinate way. for simplicity of

later proof, denote:

(a.162)

(a.163)

(a.164)

  ik(u) = xj:j6=i

[u 2

jk     h(u(j), u(i))] = xj:j6=i

[u 2

jk    

ilu 2
u 2
jl]

dxl=1

then we have:

[  (u)]ik = 2(xj:j6=i
= 2uikxj:j6=i

u 2
jk          i (u))uik

(u 2

jk     h(u(j), u(i)))

= 2uik  ik(u)

similarly we can compute the second-order partial derivative of lagrangian as

m(u) =    2f (u)    

dxi=1

     i   2ci(u).

202

the hessian is a d2    d2 matrix, we index it by 4 indices in [d]. the entries are summarized
below:

   ui   k   

   

[   ul(u,   )]ik(cid:12)(cid:12)(cid:12)(cid:12)(u,     (u ))
2(pj:j6=i u 2

jk          i (u))

4ui   kuik

=

   

   ui   k   

[2(xj:j6=i

if k = k   , i = i   

if k = k   , i 6= i   
if k 6= k   

u 2

jk       )uik](cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(u,     (u ))

2  ik(u)

4ui   kuik

if k = k   , i = i   

if k = k   , i 6= i   
if k 6= k   

(a.165)

[m(u)]ik,i   k    =

=

=

                                             
                                             

0

0

similar to the previous case, it is easy to bound the function value and derivatives of the

function and the constraints.

lemma a.19. the objective function (2.11) and p-th order derivative are all bounded by

poly(d) for p = 1, 2, 3. each constraint   s p-th order derivative is bounded by 2, for p = 1, 2, 3.

therefore the function satisfy all the smoothness condition we need. finally we show the

gradient and hessian of lagrangian satisfy the (  ,   ,   ,   )-strict saddle property. again we

did not try to optimize the dependency with respect to d.

theorem a.8. optimization problem (2.11) has exactly 2d    d! local minimum that corre-
sponds to permutation and sign    ips of ai   s. further, it satisfy (  ,   ,   ,   )-strict saddle for

   = 1 and   ,   ,    = 1/poly(d).

again, in order to prove this theorem, we follow the same strategy: we consider the trans-

formed version eq.a.157. and    rst prove the following lemmas for points around saddle

203

point and local minimum respectively. we choose

  0 = (10d)   6,

   = 2  6

0,    = 2d  0,    =   4

0/4, s(u) = {k | |uk| >   0}

(a.166)

where by intuition, s(u) is the set of coordinates whose value is relative large.

lemma a.20. under the choice of parameters in eq.(a.166), suppose k  (u)k       , and
there exists (i, j)     [d]2 so that s(u(i))     s(u(j)) 6=    . then, there exists   v     t (u) and
k  vk = 1, so that   vt m(u)  v          .

proof. again, since k  (x)k        = 2  6
2|uik  ik(u)|     2  6
0. therefore, have:

0, by eq.(a.163), we have for each i     [d], |[  (x)]ik| =

   k     s(u(i)),

|  ik(u)|       5

0

(a.167)

then, we prove this lemma by dividing it into three cases. note in order to prove that there
exists   v     t (u) and k  vk = 1, so that   vt m(u)  v          ; it su   ces to    nd a vector v     t (u)
and kvk     1, so that vt m(u)v          .

case 1 : |s(u(i))|     2, |s(u(j))|     2, and |s(u(i))     s(u(j))|     2.

204

wlog, assume {1, 2}     s(u(i))     s(u(j)), choose v to be vi1 = ui2
and vj2 =     uj1
other hand, we know m(u) restricted to these 4 coordinates (i1, i2, j1, j2) is

4 , vj1 = uj2
4 . all other entries of v are zero. clearly v     t (u), and kvk     1. on the

4 , vi2 =     ui1

4

                           

2  i1(u)

0

4ui1uj1

0

0

2  i2(u)

0

4ui2uj2

4ui1uj1

0

2  j1(u)

0

0

4ui2uj2

0

2  j2(u)

                           

by eq.(a.167), we know all diagonal entries are     2  5
0.

if ui1uj1ui2uj2 is negative, we have the quadratic form:

vt m(u)v =ui1uj1ui2uj2 +

i2  i1(u) + u 2

i1  i2(u) + u 2

j2  j1(u) + u 2

j1  j2(u)]

[u 2

1
8
1
  4
0 =      
4

          4

0 +   5

0        

(a.168)

(a.169)

if ui1uj1ui2uj2 is positive we just swap the sign of the    rst two coordinates vi1 =     ui2
2 ,
vi2 = ui1

2 and the above argument would still holds.

case 2 : |s(u(i))|     2, |s(u(j))|     2, and |s(u(i))     s(u(j))| = 1.

205

wlog, assume {1, 2}     s(u(i)) and {1, 3}     s(u(j)), choose v to be vi1 = ui2
vj1 = uj3

4 , vi2 =     ui1
4 ,
4 . all other entries of v are zero. clearly v     t (u) and kvk     1.

4 and vj3 =     uj1

on the other hand, we know m(u) restricted to these 4 coordinates (i1, i2, j1, j3) is

                           

2  i1(u)

0

4ui1uj1

0

2  i2(u)

0

4ui1uj1

0

0

0

2  j1(u)

0

2  j3(u)

0

0

0

                           

(a.170)

by eq.(a.167), we know all diagonal entries are     2  5
the quadratic form:

0. if ui1uj1ui2uj3 is negative, we have

vt m(u)v =

ui1uj1ui2uj3 +

i2  i1(u) + u 2

i1  i2(u) + u 2

j3  j1(u) + u 2

j1  j3(u)]

1
2
       

[u 2

1
8
1
  4
0 =      
4

1
2

0 +   5
  4

0        

(a.171)

if ui1uj1ui2uj3 is positive we just swap the sign of the    rst two coordinates vi1 =     ui2
2 ,
vi2 = ui1

2 and the above argument would still holds.

case 3 : either |s(u(i))| = 1 or |s(u(j))| = 1.

wlog, suppose |s(u(i))| = 1, and {1} = s(u(i)), we know:

|(u(i)

1 )2     1|     (d     1)  2

0

(a.172)

on the other hand, since s(u(i))     s(u(j)) 6=    , we have s(u(i))     s(u(j)) = {1}, and thus:

|  j1(u)| = | xi   :i   6=j

u 2

i   1     xi   :i   6=j

h(u(i   ), u(j))|       5

0

(a.173)

206

therefore, we have:

xi   :i   6=j

h(u(i   ), u(j))     xi   :i   6=j

and

i   1       5
u 2

0     u 2

i1       5

0     1     d  2

0

dxk=1

  jk(u) = xi   :i   6=j

dxk=1

u 2

i   k     d xi   :i   6=j

h(u(i   ), u(j))

   d     1     d(1     d  2

0) =    1 + d2  2

0

(a.174)

(a.175)

thus, we know, there must exist some k        [d], so that   jk   (u)         1
we have    large    negative entry on the diagonal of m. since |  j1(u)|       5
wlog, suppose k    = 2, we have |  j2(u)| >   5

0, thus |uj2|       0.

d + d  2

0. this means
0, we know k    6= 1.

choose v to be vj1 = uj2

2 , vj2 =     uj1

2 . all other entries of v are zero. clearly v     t (u) and

kvk     1. on the other hand, we know m(u) restricted to these 2 coordinates (j1, j2) is

         

2  j1(u)

0

0

2  j2(u)

         

we know |uj1| >   0, |uj2|       0, |  j1(u)|       5

0, and   j2(u)         1

d + d  2

0. thus:

vt m(u)v =

  j1(u)u 2

j2 +

  j2(u)u 2
j1

1
2
     7
0     (

1
2
0)  2
0        

1
d     d  2

1
2d

  2
0          

(a.176)

(a.177)

since by our choice of v, we have kvk     1, we can choose   v = v/kvk, and immediately have
  v     t (u) and k  vk = 1, and   vt m(u)  v          .

lemma a.21. under the choice of parameters in eq.(a.166), suppose k  (u)k       , and for
any (i, j)     [d]2 we have s(u(i))    s(u(j)) =    . then, there is a local minimum u     such that

207

ku     u    k       , and for all u    in the 2   neighborhood of u    , we have   vt m(u   )  v     1 for all
  v     t (u   ), k  vk = 1

proof. wlog, we assume s(u(i)) = {i} for i = 1,         , d. then, we immediately have:

   (i, j)     [d]2, j 6= i

(a.178)

|u(i)
|(u(i)
j |       0,
i    p1     d  2

i )2     1|     (d     1)  2
0,
i        p1     d  2

then u(i)
   1. by symmetry, we know wlog, we can assume the case u(i)

0. which means u(i)

0 or u(i)

i

let v     rd2 be the concatenation of {e1, e2,         , ed}, then we have:

is either close to 1 or close to

i    p1     d  2

0 for all i     [d].

ku     v k2 =

dxi=1

ku(i)     eik2     2d2  2

0       2

(a.179)

next, we show v is a local minimum. according to eq.a.165, we know m(v ) is a diagonal

matrix with d2 entries:

[m(v )]ik,ik = 2  ik(v ) = 2xj:j6=i

[v 2
jk    

dxl=1

v 2
il v 2

jl] =                     

2

0

if i 6= k
if i = k

(a.180)

we know the unit vector in the direction that corresponds to [m(v )]ii,ii is not in the tangent

space t (v ) for all i     [d]. therefore, for any v     t (v ), we have

vt m(e1)v     2kvk2 > 0

for all v     t (v ), v 6= 0

(a.181)

which by theorem a.4 means v is a local minimum.

208

finally, denote tv = t (v ) be the tangent space of constraint manifold at v . we know for
all u    in the 2   neighborhood of v , and for all   v     t (x   ), k  vk = 1:

  vt m(u   )  v      vt m(v )  v     |  vt m(v )  v       vt m(u   )  v|

=2kptv   vk2     km(v )     m(u   )kk  vk2
  vk2     km(v )     m(u   )k
=2     2kpt c

v

(a.182)

by lemma a.7, we know kpt c

v

  vk2     ku        v k2     4  2. by eq.(a.165), we have:

km(v )     m(u   )k     km(v )     m(u   )k     x(i,j,k)

|[m(v )]ik,jk     [m(u   )]ik,jk|     100d3  

(a.183)

in conclusion, we have   vt m(u   )  v     2     8  2     100d3       1 which    nishs the proof.

finally, we are ready to prove theorem a.8.

proof of theorem a.8. similarly, (  ,   ,   ,   )-strict saddleimmediately follows from lemma

a.20 and lemma a.21.

the only thing remains to show is that optimization problem (2.11) has exactly 2d    d! local
minimum that corresponds to permutation and sign    ips of ai   s. this can be easily proved

by the same argument as in the proof of theorem a.7.

a.3.3 extending to tensors of di   erent order

in this section we show how to generalize our algorithm to tensors of di   erent orders. as a
8th order tensor (and more generally, 4pth order tensor for p     n +) can always be considered

209

to be a 4th order tensor with components a   i ai ( a   p

i

in general), so it is trivial to generalize

our algorithm to 8th order or any 4pth order.

for tensors of other orders, we need to apply some transformation. as a concrete example,

we show how to transform an orthogonal 3rd order tensor into an orthogonal 4th order tensor.

we    rst need to de   ne a few notations. for third order tensors a, b     rd3, we de   ne
(a     b)i1,i2,...,i6 = ai1,i2,i3bi4,i5,i6(i1, ..., i6     [d]). we also de   ne the partial trace operation
that maps a 6-th order tensor t     rd6 to a 4-th order tensor in rd4:

ptrace(t )i1,i2,i3,i4 =

t (i, i1, i2, i, i3, i4).

dxi=1

basically, the operation views the tensor as a d3   d3 matrix with d2   d2 d   d matrix blocks,
then takes the trace of each matrix block. now given a random variable x     rd3 whose
expectation is an orthogonal third order tensor, we can use these operations to construct an

orthogonal 4-th order tensor:

lemma a.22. suppose the expectation of random variable x     rd3 is an orthogonal 3rd
order tensor:

where ai   s are orthonormal vectors. let x    be an independent sample of x, then we know

e[x] =

a   3
i

,

dxi=1

e[ptrace(x     x   )] =

a   4
i

.

dxi=1

in other words, we can construct random samples whose expectation is equal to a 4-th order

orthogonal tensor.

210

proof. since ptrace and     are all linear operations, by linearity of expectation we know

e[ptrace(x     x   )] = ptrace(e[x]     e[x   ]) = ptrace((

dxi=1

a   3
i )     (

dxi=1

a   3
i )).

we can then expand out the product:

(

dxi=1

a   3
i )     (

dxi=1

a   3
i ) =

dxi=1

a   6

i +xi6=j

a   3
i     a   3

j

.

for the diagonal terms, we know ptrace(a   i 6) = kaik2a   i 4 = a   i 4. for the i 6= j terms, we
j ) = hai, aji a   i 2     a   j 2 = 0 (since ai, aj are orthogonal). therefore we
know ptrace(a   3

i     a   3

must have

ptrace((

dxi=1

a   3
i )     (

dxi=1

a   3
i )) =

dxi=1

ptrace(a   6

i ) +xi6=j

this gives the result.

ptrace(a   3

i     a   3

j ) =

a   4
i

.

dxi=1

using similar operations we can easily convert all odd-order tensors into order 4p(p     n+).
for tensors of order 4p + 2(p     n+), we can simply apply the partial trace and get a tensor
of order 4p with desirable properties. therefore our results applies for all orders of tensors.

211

appendix b

appendix for applying online tensor

methods for learning latent variable

models

b.1 stochastic updates

after obtaining the whitening matrix, we whiten the data g   x,a, g   x,b and g   x,c by linear

operations to get yt

a, yt

b and yt

c     rk:

yt

a :=(cid:10)g   x,a, w(cid:11) , yt

b :=(cid:10)zbg   x,b, w(cid:11) , yt

c :=(cid:10)zcg   x,c, w(cid:11) .

where x     x and t denotes the index of the online data.

212

the stochastic id119 algorithm is obtained by taking the derivative of the loss

function    lt(v)
   vi

:

   lt(v)

   vi

=  

kxj=1

(  0 + 1)(  0 + 2)

2

hvj, vii2 vj    
a(cid:11)(cid:10)  t
(cid:10)  t
  0(  0 + 1)
(cid:10)  t
i,   ya(cid:11)(cid:10)  t

  0(  0 + 1)

i, yt

2

2

i, yt

i, yt

b(cid:11)   yc +
b(cid:11) yc

+

+

(cid:10)vi, yt

a(cid:11)(cid:10)vi, yt
b(cid:11) yt
0(cid:10)  t
i,   ya(cid:11)(cid:10)  t
c       2
a(cid:11)(cid:10)  t
(cid:10)  t
i,   yb(cid:11) yc

i, yt

2

  0(  0 + 1)

i,   yt

b(cid:11)   yc

for i     [k], where yt
whitening step and    is a constant factor that we can set.

b and yt

a, yt

c are the online whitened data points as discussed in the

the iterative updating equation for the stochastic gradient update is given by

  t+1
i       t

i       t    lt

   vi (cid:12)(cid:12)(cid:12)(cid:12)   t

i

(b.1)

for i     [k], where   t is the learning rate,   t
updated eigenvector. we update eigenvectors through

i is the last iteration eigenvector and   t

i is the

  t+1
i       t

i         t

kxj=1h(cid:10)  t

j,   t

ji + shift[  t(cid:10)  t
i(cid:11)2   t

i, yt

a(cid:11)(cid:10)  t

i, yt

c]

b(cid:11) yt

(b.2)

now we shift the updating steps so that they correspond to the centered dirichlet moment

forms, i.e.,

i, yt

i, yt

+   t  2

c] :=   t (  0 + 1)(  0 + 2)

shift[  t(cid:10)  t
a(cid:11)(cid:10)  t
b(cid:11) yt
i,   yb(cid:11)   yc       t   0(  0 + 1)
i,   ya(cid:11)(cid:10)  t
0(cid:10)  t
(cid:10)  t
a(cid:11)(cid:10)  t
      t   0(  0 + 1)

(cid:10)  t
a(cid:11)(cid:10)  t
b(cid:11)   yc
a(cid:11)(cid:10)  t
(cid:10)  t
i,   ya(cid:11)(cid:10)  t
i,   yb(cid:11) yc       t   0(  0 + 1)

(cid:10)  t

i, yt

i, yt

i, yt

i, yt

2

2

2

2

c

b(cid:11) yt
b(cid:11) yc,

i, yt

i, yt

(b.3)

where   ya := et[yt

a] and similarly for   yb and   yc.

213

b.2 proof of algorithm correctness

we now prove the correctness of our algorithm.

first, we compute m2 as just

exh   g   x,c       g   x,b|  a,   b,   ci

where we de   ne

g   x,b

  g   x,b := ex(cid:20)g   x,a     g   x,c(cid:12)(cid:12)(cid:12)(cid:12)   a,   c(cid:21)(cid:18)ex(cid:20)g   x,b     g   x,c(cid:12)(cid:12)(cid:12)(cid:12)   b,   c(cid:21)(cid:19)   
  g   x,c := ex(cid:20)g   x,a     g   x,b(cid:12)(cid:12)(cid:12)(cid:12)   a,   b(cid:21)(cid:18)ex(cid:20)g   x,c     g   x,b(cid:12)(cid:12)(cid:12)(cid:12)   b,   c(cid:21)(cid:19)   
de   ne fa as fa :=      ap    , we obtain m2 = e(cid:2)g   x,a     g   x,a(cid:3) =      ap    (cid:0)ex[  x     x ](cid:1) p   a
= fa(cid:0)ex[  x     x ](cid:1) f    a . note that p is the community connectivity matrix de   ned as p    

  0(  0+1), and e [  i  j] =   i  j

  0(  0+1)   i 6= j, we can get

[0, 1]k  k. now that we know m2, e [  2

i ] =   i(  i+1)
the centered second order moments pairscom as

g   x,c.

pairscom := fa diag(cid:18)(cid:20)   1  1 + 1

  0(  0 + 1)

, . . . ,

  k  k + 1

  0(  0 + 1)(cid:21)(cid:19) f    a

1

= m2    
=

nx xx   x

  0

fa(cid:0)                 diag(cid:0)            (cid:1)(cid:1) f    a

  0 + 1
zcg   x,cgx,bz   b    

  0

  0 + 1(cid:0)  a     a     diag(cid:0)  a     x   a(cid:1)(cid:1)

(b.4)

(b.5)

(b.6)

thus, our whitening matrix is computed. now, our whitened tensor is t is given by

t = t com(w, w, w ) =

1

nxxx (cid:2)(w    fa    0

x )     (w    fa    0

x )(cid:3) ,
x )     (w    fa    0

214

where     0
x

is the centered vector so that e[    0

x ] is diagonal. we then apply the

x         0

x         0

stochastic id119 technique to decompose the third order moment.

b.3 gpu architecture

the algorithm we propose is very amenable to parallelization and is scalable which makes it

suitable to implement on processors with multiple cores in it. our method consists of simple

id202ic operations, thus enabling us to utilize basic id202 subprograms

(blas) routines such as blas i (vector operations), blas ii (matrix-vector operations),

blas iii (matrix-matrix operations), singular value decomposition (svd), and iterative

operations such as stochastic id119 for tensor decomposition that can easily take

advantage of single instruction multiple data (simd) hardware units present in the gpus.

as such, our method is amenable to parallelization and is ideal for gpu-based implementa-

tion.

overview of code design: from a higher level point of view, a typical gpu based computation

is a three step process involving data transfer from cpu memory to gpu global memory,

operations on the data now present in gpu memory and    nally, the result transfer from the

gpu memory back to the cpu memory. we use the cula library for implementing the

id202ic operations.

gpu compute architecture: the gpus achieve massive parallelism by having hundreds of

homogeneous processing cores integrated on-chip. massive replication of these cores provides

the parallelism needed by the applications that run on the gpus. these cores, for the nvidia

gpus, are known as cuda cores, where each core has fully pipelined    oating-point and

integer arithmetic logic units.

in nvidia   s kepler architecture based gpus, these cuda

cores are bunched together to form a streaming multiprocessor (smx). these smx units

215

act as the basic building block for nvidia kepler gpus. each gpu contains multiple smx

units where each smx unit has 192 single-precision cuda cores, 64 double-precision units,

32 special function units, and 32 load/store units for data movement between cores and

memory.

each smx has l1, shared memory and a read-only data cache that are common to all the

cuda cores in that smx unit. moreover, the programmer can choose between di   erent

con   gurations of the shared memory and l1 cache. kepler gpus also have an l2 cache

memory of about 1.5mb that is common to all the on-chip smxs. apart from the above

mentioned memories, kepler based gpu cards come with a large dram memory, also

known as the global memory, whose size is usually in gigabytes. this global memory is also

visible to all the cores. the gpu cards usually do not exist as standalone devices. rather

they are part of a cpu based system, where the cpu and gpu interact with each other via

pci (or pci express) bus.

in order to program these massively parallel gpus, nvidia provides a framework known as

cuda that enables the developers to write programs in languages like c, c++, and fortran

etc. a cuda program constitutes of functions called cuda kernels that execute across

many parallel software threads, where each thread runs on a cuda core. thus the gpu   s

performance and scalability is exploited by the simple partitioning of the algorithm into    xed

sized blocks of parallel threads that run on hundreds of cuda cores. the threads running

on an smx can synchronize and cooperate with each other via the shared memory of that

smx unit and can access the global memory. note that the cuda kernels are launched

by the cpu but they get executed on the gpu. thus compute architecture of the gpu

requires cpu to initiate the cuda kernels.

cuda enables the programming of nvidia gpus by exposing low level api. apart from

cuda framework, nvidia provides a wide variety of other tools and also supports third

party libraries that can be used to program nvidia gpus. since a major chunk of the

216

scienti   c computing algorithms is id202 based, it is not surprising that the standard

id202ic solver libraries like blas and id202 package (lapack) also have

their equivalents for nvidia gpus in one form or another. unlike cuda apis, such libraries

expose apis at a much higher-level and mask the architectural details of the underlying gpu

hardware to some extent thus enabling relatively faster development time.

considering the tradeo   s between the algorithm   s computational requirements, design    ex-

ibility, execution speed and development time, we choose cula-dense as our main im-

plementation library. cula-dense provides gpu based implementations of the lapack

and blas libraries for dense id202 and contains routines for systems solvers, sin-

gular value decompositions, and eigen-problems. along with the rich set of functions that

it o   ers, cula provides the    exibility needed by the programmer to rapidly implement the

algorithm while maintaining the performance. it hides most of the gpu architecture depen-

dent programming details thus making it possible for rapid prototyping of gpu intensive

routines.

the data transfers between the cpu memory and the gpu memory are usually explicitly

initiated by cpu and are carried out via the pci (or pci express) bus interconnecting

the cpu and the gpu. the movement of data bu   ers between cpu and gpu is the most

taxing in terms of time. the bu   er transaction time is shown in the plot in figure b.1.

newer gpus, like kepler based gpus, also support useful features like gpu-gpu direct

data transfers without cpu intervention.

217

cpu   gpu buffer round   trip transaction time

0.9

0.8

0.7

0.6

0.5

)
s
(

0.4

e
m
t

i

0.3

0.2

0.1

0

1

2

3

log(cid:16) bu   er size

8

4

5

6

7

8

(cid:17)

figure b.1: experimentally measured time taken for bu   er transfer between the cpu and
the gpu memory in our system.

cula exposes two important interfaces for gpu programming namely, standard and de-

vice. using the standard interface, the developer can program without worrying about the

underlying architectural details of the gpu as the standard interface takes care of all the

data movements, memory allocations in the gpu and synchronization issues. this however

comes at a cost. for every standard interface function call the data is moved in and out of

the gpu even if the output result of one operation is directly required by the subsequent

operation. this unnecessary movement of intermediate data can dramatically impact the

performance of the program. in order to avoid this, cula provides the device interface. we

use the device interface for stgd in which the programmer is responsible for data bu   er

allocations in the gpu memory, the required data movements between the cpu and gpu,

and operates only on the data in the gpu. thus the subroutines of the program that are

iterative in nature are good candidates for device implementation.

pre-processing and post-processing: the pre-processing involves matrices whose leading

dimension is of the order of number of nodes. these are implemented using the cula

standard interface blas ii and blas iii routines.

pre-processing requires svd computations for the moore-penrose pseudoinverse calculations.

we use cula svd routines since these svd operations are carried out on matrices of

218

n

1e2
1e3
1e4
1e2
1e3
1e4

k

10
10
10
10
10
10

  0 error

time (secs)

0
0
0
1
1
1

0.1200
0.1010
0.0841
0.1455
0.1452
0.1259

0.5
1.2
43.2
0.5
1.2
42.2

table b.1: synthetic simulation results for di   erent con   gurations. running time is the
time taken to run to convergence.

moderate size. we further replaced the cula svd routines with more scalable svd and

pseudo inverse routines using random projections [66] to handle larger datasets such as dblp

dataset in our experiment.

after stgd, the community membership matrix estimates are obtained using blas iii

routines provided by the cula standard interface. the matrices are then used for hypothesis

testing to evaluate the algorithm against the ground truth.

b.4 results on synthetic datasets

homophily is an important factor in social interactions [119]; the term homophily refers

to the tendency that actors in the same community interact more than across di   erent

communities. therefore, we assume diagonal dominated community connectivity matrix

p with diagonal elements equal to 0.9 and o   -diagonal elements equal to 0.1. note that

p need neither be stochastic nor symmetric. our algorithm allows for randomly generated

community connectivity matrix p with support [0, 1]. in this way, we look at general directed

social ties among communities.

we perform experiments for both the stochastic block model (  0 = 0) and the mixed mem-

bership model. for the mixed membership model, we set the concentration parameter   0 = 1.

219

we note that the error is around 8%    14% and the running times are under a minute, when
n     10000 and n     k.

the results are given in table b.1. we observe that more samples result in a more accurate

recovery of memberships which matches intuition and theory. overall, our learning algorithm

performs better in the stochastic block model case than in the mixed membership model

case although we note that the accuracy is quite high for practical purposes. theoretically,

this is expected since smaller concentration parameter   0 is easier for our algorithm to

learn [8]. also, our algorithm is scalable to an order of magnitude more in n as illustrated

by experiments on real-world large-scale datasets.

b.5 comparison of error scores

normalized mutual information (nmi) score [113] is another popular score which is de   ned

di   erently for overlapping and non-overlapping community models. for non-overlapping

block model, ground truth membership for node i is a discrete k-state categorical variable

to obtain. similarly is the empirical distribution of the estimated membership categorical

  block     [k] and the estimated membership is a discretebk-state categorical variable b  block    
[bk]. the empirical distribution of ground truth membership categorical variable   block is easy
variable b  block. nmi for block model is de   ned as

nblock(b  block :   block) :=

h(  block) + h(b  block)     h(  block,b  block)

(cid:16)h(  block) + h(b  block)(cid:17) /2

.

the nmi for overlapping communities is a binary vector instead of a categorical vari-

able [113]. the ground truth membership for node i is a binary vector of length k,   mix,

while the estimated membership for node i is a binary vector of lengthbk, b  mix. this notion

220

coincides with one column of our membership matrices        rk  n and b       rbk  n except

that our membership matrices are stochastic. in other words, we consider all the nonzero

entries of    as 1   s, then each column of our    is a sample for   mix. the m-th entry of this

binary vector is the realization of a random variable   mixm = (  mix)m, whose id203

distribution is

p (  mixm = 1) =

nm
n

, p (  mixm = 0) = 1    

nm
n

,

where nm is the number of nodes in community m. the same holds for b  mixm. the normal-
ized conditional id178 between   mix and b  mix is de   ned as

h(b  mix|  mix)norm :=

h(cid:16)b  mixi|  mixj(cid:17)

h(  mixj )

1

kxj   [k]

min
i   [bk]

(b.7)

where   mixj denotes the jth entry of   mix and similarly for b  mixi. the nmi for overlapping

community is

nmix(b  mix :   mix) := 1    

1

2hh(  mix|b  mix)norm + h(b  mix|  mix)normi .

there are two aspects in evaluating the error. the    rst aspect is the l1 norm error. ac-

cording to equation (b.7), the error function used in nmi score is

h(b  mixi|  mixj )

h(  mixj )

. nmi is

not suitable for evaluating recovery of di   erent sized communities. in the special case of a

pair of extremely sparse and dense membership vectors, depicted in figure b.2, h(  mixj ) is

the same for both the dense and the sparse vectors since they are    ipped versions of each

other (0s    ipped to 1s and vice versa). however, the smaller sized community (i.e.

the

sparser community vector), shown in red in figure b.2, is signi   cantly more di   cult to re-

cover than the larger sized community shown in blue in figure b.2. although this example

is an extreme scenario that is not seen in practice, it justi   es the drawbacks of the nmi.

221

thus, nmi is not suitable for evaluating recovery of di   erent sized communities. in contrast,

dense   1

sparse   2

length n membership vector

0

1

large sized community

small sized community

figure b.2: a special case of a pair of extremely dense and sparse communities. theoreti-
cally, the sparse community is more di   cult to recover than the dense one. however, the nmi
score penalizes both of them equally. note that for dense   1, p (  mix1 = 0) = # of 0s in   1
which is equal to p (  mix2 = 1) = # of 1s in   2
which is
equal to p (  mix2 = 0) = # of 0s in   2

. similarly, p (  mix1 = 1) = # of 1s in   1

. therefore, h(  mix1) = h(  mix2).

n

n

n

n

our error function employs a normalized l1 norm error which penalizes more for larger sized

communities than smaller ones.

the second aspect is the error induced by false pairings of estimated and ground-truth

communities. nmi score selects only the closest estimated community through normal-

ized conditional id178 minimization and it does not account for statistically signi   cant

dependence between an estimated community and multiple ground truth communities and

vice-versa, and therefore it underestimates error. however, our error score does not limit to a

matching between the estimated and ground truth communities: if an estimated community

is found to have statistically signi   cant correlation with multiple ground truth communities

(as evaluated by the p-value), we penalize for the error over all such ground truth commu-

nities. thus, our error score is a harsher measure of evaluation than nmi. this notion of

   soft-matching    between ground-truth and estimated communities also enables validation of

recovery of a combinatorial union of communities instead of single ones.

222

a number of other scores such as    separability   ,    density   ,    cohesiveness    and    id91

coe   cient    [165] are non-statistical measures of faithful community recovery. the scores

of [165] intrinsically aim to evaluate the level of id91 within a community. however our

goal is to measure the accuracy of recovery of the communities and not how well-clustered

the communities are.

banerjee and langford [26] proposed an objective evaluation criterion for id91 which

use classi   cation performance as the evaluation measure. in contrast, we look at how well the

method performs in recovering the hidden communities, and we are not evaluating predictive

performance. therefore, this measure is not used in our evaluation.

finally, we note that cophenetic correlation is another statistical score used for evaluating

id91 methods, but note that it is only valid for hierarchical id91 and it is a

measure of how faithfully a dendrogram preserves the pairwise distances between the original

unmodeled data points [151]. hence, it is not employed in this paper.

223

appendix c

appendix for dictionary learning via

convolutional tensor method

c.1 cumulant form

in [12], it is proved that in ica model, the cumulant of observation x is decomposed into

multi-linear transform of a diagonal cumulant of h. therefore, we aim to    nd the third order

cumulant for input x.

as we know that the rth order moments for variable x is de   ned as

  r := e[xr]     rn  n  n

(c.1)

let us use [  3]i,j,k to denote the (i, j, k)th entry of the third order moment. the relationship

between 3th order cumulant   3and 3th order moment   3is

[  3]i,j,k = [  3]i,j,k     [  2]i,j[  1]k     [  2]i,k[  1]j     [  2]j,k[  1]i + 2[  1]i[  1]j[  1]k

(c.2)

224

therefore the shift tensor is in this format: we know that the shift term

[z]a,b,c := e[xi

a]e[xi

bxi

c] + e[xb]e[xaxi

c] + e[xc]e[xaxb]     2e[xa]e[xb]e[xc],

a, b, c     [n]
(c.3)

it is known from [12] that cumulant decomposition in the 3 order tensor format is

e[x     x     x]     z = xj   [nl]

     jf   j     f   j     f   j

therefore using the khatri-rao product property,

(c.4)

unf old(xj   [nl]

     jf   j     f   j     f   j ) = xj   [nl]

     jf   j (f   j     f   j )    = f         (f        f   )   

(c.5)

therefore the unfolded third order cumulant is decomposed as c3 = f         (f        f   )   .

c.2 proof for main theorem 4.1

our optimization problem is

min
f

kc3   f    (h     g)   k2

f s.t. blkl(f ) = u  diag(fft(fl))  u h, kflk2

2 = 1,   l     [l], (c.6)

where we denote d :=    (h     g)    for simplicity. therefore the objective is to minimize
f . let the svd of d be d = p   q   . since the frobenius norm remains
kc3     f dk2
invariant under orthogonal transformations and full rank diagonal matrix [57], it is obtained

225

that

kc3     f dk2

f = kc3     f p   q   k2

f = kc3q          f pk2

f = kc3q     p         fk2

f

(c.7)

therefore the optimization problem in (4.7) is equivalent to

min

f kc3((h     g)   )           fk2

f s.t. blkl(f ) = u  diag(fft(fl))  u h, kflk2

2 = 1,   l     [l] (c.8)

when (h     g) and    are full column rank.

the full rank condition requires nl < n2 or l < n, and it is a reasonable assumption since

otherwise the    lter estimates are redundant. since (c.8) has block constraints, it can be

broken down in to solving l independent sub-problems

min

fl (cid:13)(cid:13)blkl(m)    blkl(  )        u    diag(fft(fl))    u h(cid:13)(cid:13)2

f

s.t. kflk2

2 = 1,   l     [l].

(c.9)

c.3 parallel inversion of   

we propose an e   cient iterative algorithm to compute       via block matrix inversion theorem[68].

lemma c.1. (parallel inversion of row and column stacked diagonal matrix) let j l =   

be partitioned into a block form:

j l =         

j l   1

o

r

blkl

l(  )

          ,

226

(c.10)

where o :=

                  

blk1
l(  )
...
blkl   1

l

(  )

                  

, and r :=(cid:2)blk1

l   1(  ), . . . , blkl

l   1(  )(cid:3). after inverting blkl

l(  )

which takes o(1) time using o(n) processors, there inverse of    is achieved by

      =         

l(  )   1r)   1

(j l   1     oblkl
   blkl

l(  )   1r(j l   1     oblkl

l(  )   1r)   1

   (j l   1)   1o(blkl
(blkl

l(  )     r(j l   1)   1o)   1

l(  )     r(j l   1)   1o)   1

         

(c.11)

assuming that j l   1 and blkl

l   are invertible.

this again requires inverting r, o and j l   1. recursively applying these block matrix

inversion theorem, the inversion problem is reduced to inverting l2 number of n by n diagonal

matrices with additional id127s as indicated in equation (c.11).

inverting a diagonal matrix results in another diagonal one, and the complexity of invert-

ing n    n diagonal matrix is o(1) with o(n) processors. we can simultaneous invert all
blocks. therefore with o(nl2) processors, we invert all the diagonal matrices in o(1) time.

the recursion takes l steps, for step i     [l] id127 cost is o(log nl) with
o(n2l/ log(nl)) processors. with l iteration, one achieves o(log n + log l) running time

with o(n2l2/(log l + log n)) processors.

227

appendix d

appendix for latent tree learning

via hierarchical tensor method

d.1 additivity of the multivariate information dis-

tance

recall that the additive information distance between nodes two categorical variables xi and

xj was de   ned in [41]. we extend the notation of information distance to high dimensional

variables via de   nition 5.1 and present the proof of its additivity in lemma 5.1 here.

proof.

e[xax   c ] = e[e[xax   c |xb]] = ae[xbx   b ]b   

consider three nodes a, b, c such that there are edges between a and b, and b and c. let
the a = e(xa|xb) and b = e(xc|xb). from de   nition 5.1, we have, assuming that e(xax   a ),

228

e(xbx   b ) and e(xcx   c ) are full rank.

  i(e(xax   c ))

kqi=1

dist(va, vc) =     log

pdet(e(xax   a )) det(e(xcx   c ))

e   dist(va,vc) = det(cid:0)e(xax   a )   1/2u   e(xax   c )v e(xcx   c )   1/2(cid:1)

where k-svd((e(xax   c )) = u  v    ). similarly,

e   dist(va,vb) = det(cid:0)e(xax   a )   1/2u   e(xax   b )w e(xbx   b )   1/2(cid:1)
e   dist(vb,vc) = det(cid:0)e(xbx   b )   1/2w    e(xbx   c )v e(xcx   c )   1/2(cid:1)

where k-svd((e(xax   b )) = u  w    ) and k-svd((e(xbx   c )) = w   v    ).

therefore,

e   (dist(a,b)+dist(b,c)) = det(e(xax   a )   1/2u   e(xax   b )e(xbx   b )   1/2   1/2e(xbx   c )v e(xcx   c )   1/2)

= det(e(xax   a )   1/2u   ae(xbx   b )b   v e(xcx   c )   1/2) = e   dist(va,vc)

we conclude that the multivariate information distance is additive. note that e(cid:2)xax   b(cid:3) =
e(cid:0)e(cid:0)xax   b |xb(cid:1)(cid:1) = e(cid:0)axbx   b(cid:1) = ae(xbx   b ).

we note that when the second moments are not full rank, the above distance can be extended

as follows:

dist(va, vc) =     log

s kqi=1

  i(e(xax   c ))

kqi=1

  i(e(xax   a ))

.

  i(e(xcx   c ))

kqi=1

229

d.2 local recursive grouping

the local recursive grouping (lrg) algorithm is a local divide and conquer procedure for

learning the structure and parameter of the latent tree (algorithm 6). we perform recursive

grouping simultaneously on the sub-trees of the mst. each of the sub-tree consists of an

internal node and its neighborhood nodes. we keep track of the internal nodes of the mst,

and their neighbors. the resultant latent sub-trees after lrg can be merged easily to

recover the    nal latent tree. consider a pair of neighboring sub-trees in the mst. they have

two common nodes (the internal nodes) which are neighbors on mst. firstly we identify

the path from one internal node to the other in the trees to be merged, then compute the

multivariate information distances between the internal nodes and the introduced hidden

nodes. we recover the path between the two internal nodes in the merged tree by inserting

the hidden nodes closely to their surrogate node. secondly, we merge all the leaves which

are not in this path by attaching them to their parent. hence, the recursive grouping can

be done in parallel and we can recover the latent tree structure via this merging method.

lemma d.1. if an observable node vj is the surrogate node of a hidden node hi, then the

hidden node hi can be discovered using vj and the neighbors of vj in the mst.

this is due to the additive property of the multivariate information distance on the tree and

the de   nition of a surrogate node. this observation is crucial for a completely local and

parallel structure and parameter estimation. it is also easy to see that all internal nodes in

the mst are surrogate nodes.

after the parallel construction of the mst, we look at all the internal nodes xint. for
vi     xint, we denote the neighborhood of vi on mst as nbdsub(vi; mst) which is a small
sub-tree. note that the number of such sub-trees is equal to the number of internal nodes

in mst.

230

for any pair of sub-trees, nbdsub(vi; mst) and nbdsub(vj; mst), there are two topological re-

lationships, namely overlapping (i.e., when the sub-trees share at least one node in common)

and non-overlapping (i.e., when the sub-trees do not share any nodes).

since we de   ne a neighborhood centered at vi as only its immediate neighbors and itself

on mst, the overlapping neighborhood pair nbdsub(vi; mst) and nbdsub(vj; mst) can only

have con   icting paths, namely path(vi, vj;n i) and path(vi, vj;n j), if vi and vj are neighbors
in mst.

with this in mind, we locally estimate all the latent sub-trees, denoted as n i, by applying
recursive grouping [41] in a parallel manner on nbdsub(vi; mst),    vi     xint. note that the
latent nodes automatically introduced by rg(vi) have vi as their surrogate. we update the

tree structure by joining each level in a bottom-up manner. the testing of the relationship

among nodes [41] uses the additive multivariate information distance metric (appendix d.1)

  (vi, vj; k) = dist(vi, vk)    dist(vi, vk) to decide whether the nodes vi and vj are parent-child
or siblings. if they are siblings, they should be joined by a hidden parent. if they are parent

and child, the child node is placed as a lower level node and we add the other node as the

single parent node, which is then joined in the next level.

finally, for each internal edge of mst connecting two internal nodes vi and vj, we consider

merging the latent sub-trees.

in the example of two local estimated latent sub-trees in

figure 5.2, we illustrate the complete local merging algorithm that we propose.

d.3 proof sketch for theorem 5.1

we argue for the correctness of the method under exact moments. the sample complexity

follows from the previous works. in order to clarify the proof ideas, we de   ne the notion of

surrogate node [41] as follows.

231

de   nition d.1. surrogate node for hidden node hi on the latent tree t = (v,e) is de   ned
as sg(hi;t ) := arg min
vj   x

dist(vi, vj).

in other words, the surrogate for a hidden node is an observable node which has the minimum

multivariate information distance from the hidden node. see figure 5.2(a), the surrogate

node of h1, sg(h1;t ), is v3, sg(h2;t ) = sg(h3;t ) = v5. note that the notion of the surrogate
node is only required for analysis, and our algorithm does not need to know this information.

the notion of surrogacy allows us to relate the constructed mst (over observed nodes) with

the underlying latent tree. it can be easily shown that contracting the hidden nodes to their

surrogates on latent tree leads to mst. local recursive grouping procedure can be viewed

as reversing these contractions, and hence, we obtain consistent local sub-trees.

we now argue the correctness of the structure union procedure, which merges the local sub-

trees. in each reconstructed sub-tree ni, where vi is the group leader, the discovered hidden
nodes {hi} form a surrogate relationship with vi, i.e. sg(hi;t ) = vi. our merging approach
maintains these surrogate relationships. for example in figure 5.2(d1,d2), we have the path

v3   h1   v5 in n 3 and path v3   h3   h2   v5 in n 5. the resulting path is v3   h1   h3   h2   v5, as
seen in figure 5.2(e). we now argue why this is correct. as discussed before, sg(h1;t ) = v3
and sg(h2;t ) = sg(h3;t ) = v5. when we merge the two subtrees, we want to preserve the
paths from the group leaders to the added hidden nodes, and this ensures that the surrogate

relationships are preserved in the resulting merged tree. thus, we obtain a global consistent

tree structure by merging the local structures. the correctness of parameter learning comes

from the consistency of the tensor decomposition techniques and careful alignments of the

hidden labels across di   erent decompositions. refer to appendix d.4, d.7 for proof details

and the sample complexity.

232

d.4 proof of correctness for lrg

de   nition d.2. a latent tree t   3 is de   ned to be a minimal (or identi   able) latent tree if
it satis   es that each latent variable has at least 3 neighbors.

de   nition d.3. surrogate node for hidden node hi in latent tree t = (v,e) is de   ned as

sg(hi;t ) := arg min
vj   x

dist(vi, vj).

there are some useful observations about the mst in [41] which we recall here.

property d.1 (mst     surrogate neighborhood preservation). the surrogate nodes of any
two neighboring nodes in e are also neighbors in the mst. i.e.,

(hi, hj)     e     (sg(hi), sg(hj))     mst.

property d.2 (mst     surrogate consistency along path). if vj     x and vh     sg   1(vj),
then every node along the path connecting vj and vh belongs to the inverse surrogate set

sg   1(vj), i.e.,

vi     sg   1(vj),    vi     path(vj, vh)

if

vh     sg   1(vj).

the mst properties observed connect the mst over observable nodes with the original

latent tree t . we obtain mst by contracting all the latent nodes to its surrogate node.

233

given that the correctness of clrg algorithm is proved in [41], we prove the equivalence

between the clrg and plrg.

lemma d.2. for any sub-tree pairs nbd[vi; mst] and nbd[vi; mst], there is at most one

overlapping edge. the overlapping edge exists if and only if vi     nbd(vj; mst).

this is easy to see.

lemma d.3. denote the latent tree recovered from nbd[vi; mst] as n i and similarly for
nbd[vj; mst]. the inconsistency, if any, between n i and n j occurs in the overlapping
path(vi, vj;n i) in and path(vi, vj;n j) after lrg implementation on each subtrees.

we now prove the correctness of lrg. let us denote the latent tree resulting from merging

a subset of small latent trees as tlrg(s), where s is the set of center of subtrees that are

merged pair-wisely. clrg algorithm in [41] implements the rg in a serial manner. let us

denote the latent tree learned at iteration i from clrg is tclrg(s), where s is the set of

internal nodes visited by clrg at current iteration . we prove the correctness of lrg by

induction on the iterations.

at the initial step s =    : tclrg = mst and tlrg = mst , thus tclrg = tlrg.

now we assume that for the same set si   1, tclrg = tlrg is true for r = 1, . . . , i     1. at
iteration r = i where clrg employs rg on the immediate neighborhood of node vi on

tclrg(si   1), let us assume that hi is the set of hidden nodes who are immediate neighbors
of i     1. the clrg algorithm thus considers all the neighbors and implements the rg. we
know that the surrogate nodes of every latent node in hi belong to previously visited nodes

si   1. according to property d.1 and d.2, if we contract all the hidden node neighbors to
their surrogate nodes, clrg thus is a rg on neighborhood of i on mst.

as for our lrg algorithm at this step, tlrg(si) is the merging between tlrg(si   1)and n i.
the latent nodes whose surrogate node is j are introduced between the edge (i     1, i). now

234

that we know n i is the rg output from immediate neighborhood of i on mst. therefore,
we proved that tclrg(si) = tlrg(si).

d.5 cross group alignment correction

in order to achieve cross group alignments, tensor decompositions on two cross group triplets

have to be computed. the    rst triplet is formed by three nodes: reference node in group 1,

x1, non-reference node in group 1, x2, and reference node in group 2, x3. the second triplet

is formed by three nodes as well: reference node in group 2, x3, non-reference node in group

2, x4 and reference node in group 1, x1. let us use h1 to denote the parent node in group 1,

and h2 the parent node in group 2.

from trip(x1, x2, x3), we obtain p (h1|x1) =   a, p (x2|h1) = b and p (x3|h1) = p (x3|h2)p (h2|h1)
= de. from trip(x3, x4, x1), we know p (x3|h2) = d  , p (x4|h2) = c   and p (h2|x1) =
p (h2|h1)p (h1|x1) =   e   a, where    is a permutation matrix. we compute    as    =
q(  e   a)(   a)   (de)   (d  ) so that d = (d  )      is aligned with group 1. thus, when all the

parameters in the two groups are aligned by permute group 2 parameters using   , thus the

alignment is completed.

similarly, the alignment correction can be done by calculating the permutation matrices

while merging di   erent threads.

overall, we merge the local structures and align the parameters from lrg locla sub-trees

using procedure 7 and 8.

235

d.6 computational complexity

we recall some notations here: d is the observable node dimension, k is the hidden node

dimension (k     d), n is the number of samples, p is the number of observable nodes, and
z is the number of non-zero elements in each sample.

multivariate information distance estimation involves sparse id127s to com-

pute the pairwise second moments. each observable node has a d   n sample matrix with z
non-zeros per column. computing the product x1xt
2 from a single sample for nodes 1 and 2

requires o(z) time and there are n such sample pair products leading to o(nz) time. there

are o(p2) node pairs and hence the degree of parallelism is o(p2). next, we perform the

k-rank svd of each of these matrices. each svd takes o(d2k) time using classical methods.

using randomized methods [66], this can be improved to o(d + k3).

next on, we construct the mst in o(log p) time per worker with p2 workers. the structure

learning can be done in o(  3) per sub-tree and the local neighborhood of each node can be

processed completely in parallel. we assume that the group sizes    are constant (the sizes

are determined by the degree of nodes in the latent tree and homogeneity of parameters

across di   erent edges of the tree. the parameter estimation of each triplet of nodes consists

of implicit stochastic updates involving products of k    k and d    k matrices. note that we
do not need to consider all possible triplets in groups but each node must be take care by a

triplet and hence there are o(p) triplets. this leads to a factor of o(  k3 +   dk2) time per

worker with p/   degree of parallelism.

at last, the merging step consists of products of k    k and d    k matrices for each edge in
the latent tree leading to o(dk2) time per worker with p/   degree of parallelism.

236

d.7 sample complexity

from [6], we recall the number of samples required for the recovery of the tree structure that

is consistent with the ground truth (for a precise de   nition of consistency, refer to de   nition

2 of [41]).

lemma d.4. if

n >

200k2b2t

(1     distmax)(cid:17)2 +

(cid:16)   2

min
  max

7km 2t
(1     distmax)

,

  2
min
  max

(d.1)

then with id203 at least 1       , proposed algorithm returns bt = t , where
xi,xj   xnqmax{ke[kxik2xjx   j ]k}, max{ke[kxjk2xix   i ]k}o ,

b := max

xi,xj   x(4 ln(4

t := max

m := max

xi   x {kxik} ,

e[kxik2kxjk2]     tr(e[xix   j ]e[xjx   i ])
max{ke[kxjk2xix   i ]k,ke[kxik2xjx   j ]k}
  min := min

{x1,x2}{  (cid:0)e[x1x   2 ](cid:1)}
{x1,x2}{  (cid:0)e[x1x   2 ](cid:1)}

  max := max

n/  )) .

from [7], we recall the sample complexity for the faithful recovery of parameters via tensor

decomposition methods.

we de   ne   p to be the noise raised between empirical estimation of the second order moments

and exact second order moments, and   t to be the noise raised between empirical estimation

of the third order moments and the exact third order moments.

237

lemma d.5. consider positive constants c, c   , c and c   , the following holds. if

  k
  1
k

,

  p     c

  t     c   

  k  3/2

k
k

n     c log(k) + log log    1  3/2

k
  t

1

  p!!!

+

l     poly(k) log(1/  ),

then with id203 at least 1       , tensor decomposition returns (bvi,   i) : i     [k] satisfying,

after appropriate reordering,

1
  2
k

  i

kbvi     vik2     c   (cid:18) 1
|b  i       i|     c     1

  3/2
k

  i

  t +(cid:18)   1
  t +   1  p!

1
     k

+ 1(cid:19)   p(cid:19)

for all i     [k].

we note that   1       2     . . .   k > 0 are the non-zero singular values of the second order
moments,   1       2     . . .       k > 0 are the ground-truth eigenvalues of the third order
moments, and vi are the corresponding eigenvectors for all i     [k].

d.8 e   cient svd using sparsity and dimensionality

reduction

without loss of generality, we assume that a matrix whose svd we aim to compute has no

row or column which is fully zeros, since, if it does have zero entries, such row and columns

can be dropped.

238

let a     rn  n be the matrix to do svd. let        rd    k, where   k =   k with    is a scalar,
usually, in the range [2, 3]. for the ith row of   , if pi |  |(i, :) 6= 0 and pi |  |(:, i) 6= 0,
pi |  |(i, :) = 0 or pi |  |(:, i) = 0, we leave that row blank. let d     rd  d be a diagonal

then there is only one non-zero entry and that entry is uniformly chosen from [  k]. if either

matrix with iid rademacher entries, i.e., each non-zero entry is 1 or    1 with id203
1
2. now, our embedding matrix [46] is s = d  , i.e., we    nd as and then proceed with

the nystrom [85] method. unlike the usual nystrom method [67] which uses a random

matrix for computing the embedding, we improve upon this by using a sparse matrix for the

embedding since the sparsity improves the running time and the memory requirements of

the algorithm.

239

appendix e

appendix for spatial point process

mixture model learning

e.1 morphological basis extraction

we aim to characterize the morphological basis for all cells with di   erent size, orientation,

expression pro   les and spatial distribution. the traditional sparse coding introduces too

many free parameters and is not suitable for compact morphological basis learning. we

instead propose gaussian prior convolutional sparse coding (gpcsc). the intuition for using

convolution is due to the frequent replication of cells of similar shapes and the translation

invariance property. traditional sparse coding would learn both the shape of the cell and the

location of the cell. but the convolutional sparse coding would only learn the shape here.

we characterize cell spatial distribution via decoding the sparse activation map.

to formulate the problem formally: let i be the image observed, then the convolutional sparse

coding model generates observed image i using    lters (resembling cell shapes)f superposed

240

at locations indicated by the activation map m (whose sparsity pattern indicates cell spatial

distribution and activation amplitude indicates gene expression pro   les. )

our goals of segmenting cells, extracting cell basis, and estimating gene pro   les and cell

locations are reduced to this optimization learning problem:

min
fm,m n

m(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)xn

kxm=1
s.t. fm(x, y)     0,kfmk2

i n    

fm     m n

2

f

m(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

f = 1, m (n)

m (x, y)     0.

+xn xm

  km n

mk0 ,

(e.1)

where i n is the nth image associated with the gene we are interested in with dx    dy pixels,
i.e., i n     rd  d.

we call the fm     rd  d    lter, where d is set to capture the local cell morphological informa-
tion. the spatial coe   cient for image i n is denoted as h (n)
m     r(d   d+1)  (d   d+1) which repre-
m(x, y) = 1,

sents the position of the    lter fm being active on image i n. more precisely, if h n
then fm is active at i n(x : x + d     1, y : y + d     1).

e.1.1 gaussian prior convolutional sparse coding

the popular alternating approach between matching pursuit to learn activation map m and

k-svd to learn f is general applicable to any id164 problem in image processing.

however, this approach causes inexact cell number estimation as    lters with multi-modality

(i.e., multiple cells) are learnt. we resolve this issue by proposing an gaussian id203

density function prior on the    lters to guarantee single cell detection and achieve accurate

cell number estimation. the support of m is also limited to the local maxima indicating cell

centers. note that our cell are not donut shaped, and it is reasonable to assume the darkest

point being the cell center.

241

therefore, we optimize over the objective min kpn i n    pm fm     m n
2 +pnpm   km n
mk2

mk0
such that fm are 2     d gaussian densities with priori set top 2 principal radius and orien-
tation. alternating minimization is used to solving the optimization problem. if we de   ne

m, the gradient of the objective reduced to an it-

the residual as pn i n    pnpm bfm     cm n

erative approach of updating    lters, compute residual, optimizing activation map based on

residual, compute residual and updating    lters again. it is easy to see that both    l
   fm

(i, j)

and    l
   hm

(i, j) are convolution of the residual and the other variable rotated by angle   .

e.1.2

image registration/alignment

a structure represents a neuronanatomical region of interest. structures are grouped into

ontologies and organized in a hierarchy or structure graph. we are interested in the so-

matosensory cortex area. so we use the a   ne transform from allen brain institute [1, 115]

to align all the in-situ hybridization images with the atlas brain to extract the correct region.

242

