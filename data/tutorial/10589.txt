exploring the limits of id38

6
1
0
2

 

b
e
f
1
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
0
1
4
2
0

.

2
0
6
1
:
v
i
x
r
a

rafal jozefowicz
oriol vinyals
mike schuster
noam shazeer
yonghui wu

google brain

rafalj@google.com
vinyals@google.com
schuster@google.com
noam@google.com
yonghui@google.com

abstract

in this work we explore recent advances in re-
current neural networks for large scale lan-
guage modeling, a task central to language un-
derstanding. we extend current models to deal
with two key challenges present in this task: cor-
pora and vocabulary sizes, and complex, long
term structure of language. we perform an ex-
haustive study on techniques such as character
convolutional neural networks or long-short
term memory, on the one billion word bench-
mark. our best single model signi   cantly im-
proves state-of-the-art perplexity from 51.3 down
to 30.0 (whilst reducing the number of param-
eters by a factor of 20), while an ensemble of
models sets a new record by improving perplex-
ity from 41.0 down to 23.7. we also release these
models for the nlp and ml community to study
and improve upon.

1. introduction
id38 (lm) is a task central to natural
language processing (nlp) and language understanding.
models which can accurately place distributions over sen-
tences not only encode complexities of language such as
grammatical structure, but also distill a fair amount of in-
formation about the knowledge that a corpora may con-
tain. indeed, models that are able to assign a low probabil-
ity to sentences that are grammatically correct but unlikely
may help other tasks in fundamental language understand-
ing like id53, machine translation, or text
summarization.
lms have played a key role in traditional nlp tasks such
as id103 (mikolov et al., 2010; arisoy et al.,
2012), machine translation (schwenk et al., 2012; vaswani
et al.), or text summarization (rush et al., 2015; filippova
et al., 2015). often (although not always), training better

language models improves the underlying metrics of the
downstream task (such as word error rate for speech recog-
nition, or id7 score for translation), which makes the
task of training better lms valuable by itself.
further, when trained on vast amounts of data, language
models compactly extract knowledge encoded in the train-
ing data. for example, when trained on movie subti-
tles (serban et al., 2015; vinyals & le, 2015), these lan-
guage models are able to generate basic answers to ques-
tions about object colors, facts about people, etc. lastly,
recently proposed sequence-to-sequence models employ
conditional language models (mikolov & zweig, 2012) as
their key component to solve diverse tasks like machine
translation (sutskever et al., 2014; cho et al., 2014; kalch-
brenner et al., 2014) or video generation (srivastava et al.,
2015a).
deep learning and recurrent neural networks (id56s)
have fueled id38 research in the past years
as it allowed researchers to explore many tasks for which
the strong conditional independence assumptions are unre-
alistic. despite the fact that simpler models, such as n-
grams, only use a short history of previous words to predict
the next word, they are still a key component to high qual-
ity, low perplexity lms. indeed, most recent work on large
scale lm has shown that id56s are great in combination
with id165s, as they may have different strengths that
complement id165 models, but worse when considered
in isolation (mikolov et al., 2011; mikolov, 2012; chelba
et al., 2013; williams et al., 2015; ji et al., 2015a; shazeer
et al., 2015).
we believe that, despite much work being devoted to small
data sets like the penn tree bank (ptb) (marcus et al.,
1993), research on larger tasks is very relevant as over   t-
ting is not the main limitation in current language model-
ing, but is the main characteristic of the ptb task. results
on larger corpora usually show better what matters as many
ideas work well on small data sets but fail to improve on

exploring the limits of id38

    we show that an ensemble of a number of different
models can bring down perplexity on this task to 23.7,
a large improvement compared to current state-of-art.
    we share the model and recipes in order to help and

motivate further research in this area.

in section 2 we review important concepts and previous
work on id38. section 3 presents our contri-
butions to the    eld of neural id38, emphasiz-
ing large scale recurrent neural network training. sections
4 and 5 aim at exhaustively describing our experience and
understanding throughout the project, as well as emplacing
our work relative to other known approaches.

2. related work
in this section we describe previous work relevant to the
approaches discussed in this paper. a more detailed dis-
cussion on id38 research is provided in
(mikolov, 2012).

2.1. language models

id38 (lm) has been a central task in nlp.
the goal of lm is to learn a id203 distribution over
sequences of symbols pertaining to a language. much work
has been done on both parametric (e.g., id148)
and non-parametric approaches (e.g., count-based lms).
count-based approaches (based on statistics of id165s)
typically add smoothing which account for unseen (yet pos-
sible) sequences, and have been quite successful. to this
extent, kneser-ney smoothed 5-gram models (kneser &
ney, 1995) are a fairly strong baseline which, for large
amounts of training data, have challenged other paramet-
ric approaches based on neural networks (bengio et al.,
2006).
most of our work is based on recurrent neural networks
(id56) models which retain long term dependencies. to
this extent, we used the long-short term memory model
(hochreiter & schmidhuber, 1997) which uses a gating
mechanism (gers et al., 2000) to ensure proper propaga-
tion of information through many time steps. much work
has been done on small and large scale id56-based lms
(mikolov et al., 2010; mikolov, 2012; chelba et al., 2013;
zaremba et al., 2014; williams et al., 2015; ji et al., 2015a;
wang & cho, 2015; ji et al., 2015b). the architectures that
we considered in this paper are represented in figure 1.
in our work, we train models on the popular one bil-
lion word benchmark, which can be considered to be a
medium-sized data set for count-based lms but a very large
data set for nn-based lms. this regime is most interesting
to us as we believe learning a very good model of human
language is a complex task which will require large models,

figure 1. a high-level diagram of the models presented in this pa-
per. (a) is a standard lstm lm. (b) represents an lm where both
input and softmax embeddings have been replaced by a character
id98. in (c) we replace the softmax by a next character prediction
lstm network.

larger data sets. further, given current hardware trends and
vast amounts of text available on the web, it is much more
straightforward to tackle large scale modeling than it used
to be. thus, we hope that our work will help and motivate
researchers to work on traditional lm beyond ptb     for
this purpose, we will open-source our models and training
recipes.
we focused on a well known, large scale lm benchmark:
the one billion word benchmark data set (chelba et al.,
2013). this data set is much larger than ptb (one thou-
sand fold, 800k word vocabulary and 1b words training
data) and far more challenging. similar to id163 (deng
et al., 2009), which helped advance id161, we
believe that releasing and working on large data sets and
models with clear benchmarks will help advance language
modeling.
the contributions of our work are as follows:

    we explored, extended and tried to unify some of the

current research on large scale lm.

    speci   cally, we designed a softmax loss which is
based on character level id98s, is ef   cient to train,
and is as precise as a full softmax which has orders of
magnitude more parameters.

    our study yielded signi   cant improvements to the
state-of-the-art on a well known, large scale lm task:
from 51.3 down to 30.0 perplexity for single models
whilst reducing the number of parameters by a factor
of 20.

exploring the limits of id38

and thus large amounts of data. further advances in data
availability and computational resources helped our study.
we argue this leap in scale enabled tremendous advances in
deep learning. a clear example found in id161 is
id163 (deng et al., 2009), which enabled learning com-
plex vision models from large amounts of data (krizhevsky
et al., 2012).
a crucial aspect which we discuss in detail in later sections
is the size of our models. despite the large number of pa-
rameters, we try to minimize computation as much as pos-
sible by adopting a strategy proposed in (sak et al., 2014)
of projecting a relatively big recurrent state space down so
that the matrices involved remain relatively small, yet the
model has large memory capacity.

2.2. convolutional embedding models

there is an increased interest in incorporating character-
level inputs to build id27s for various nlp
problems, including part-of-speech tagging, parsing and
id38 (ling et al., 2015; kim et al., 2015;
ballesteros et al., 2015). the additional character informa-
tion has been shown useful on relatively small benchmark
data sets.
the approach proposed in (ling et al., 2015) builds word
embeddings using bidirectional lstms (schuster & pali-
wal, 1997; graves & schmidhuber, 2005) over the charac-
ters. the recurrent networks process sequences of charac-
ters from both sides and their    nal state vectors are concate-
nated. the resulting representation is then fed to a neural
network. this model achieved very good results on a part-
of-speech tagging task.
in (kim et al., 2015), the words characters are processed by
a 1-d id98 (le cun et al., 1990) with max-pooling across
the sequence for each convolutional feature. the result-
ing features are fed to a 2-layer highway network (srivas-
tava et al., 2015b), which allows the embedding to learn se-
mantic representations. the model was evaluated on small-
scale id38 experiments for various languages
and matched the best results on the ptb data set despite
having 60% fewer parameters.

2.3. softmax over large vocabularies

assigning id203 distributions over large vocabularies
is computationally challenging. for modeling language,
maximizing log-likelihood of a given word sequence leads
to optimizing cross-id178 between the target id203
distribution (e.g., the target word we should be predicting),
and our model predictions p. generally, predictions come
from a linear layer followed by a softmax non-linearity:
where zw is the logit correspond-
p(w) =
ing to a word w. the logit is generally computed as an

(cid:80)

exp(zw(cid:48) )

exp(zw)

w(cid:48)   v

n(cid:89)

inner product zw = ht ew where h is a context vector and
ew is a    id27    for w.
the main challenge when |v | is very large (in the order
of one million in this paper) is the fact that computing
all inner products between h and all embeddings becomes
prohibitively slow during training (even when exploiting
matrix-id127s and modern gpus). several
approaches have been proposed to cope with the scaling is-
sue: importance sampling (bengio et al., 2003; bengio &
sen  ecal, 2008), noise contrastive estimation (nce) (gut-
mann & hyv  arinen, 2010; mnih & kavukcuoglu, 2013),
self normalizing partition functions (vincent et al., 2015)
or hierarchical softmax (morin & bengio, 2005; mnih &
hinton, 2009)     they all offer good solutions to this prob-
lem. we found importance sampling to be quite effective
on this task, and explain the connection between it and
nce in the following section, as they are closely related.

3. id38 improvements
recurrent neural networks based lms employ the chain
rule to model joint probabilities over word sequences:

p(w1, . . . , wn ) =

p(wi|w1, . . . , wi   1)

i=1

where the context of all previous words is encoded with an
lstm, and the id203 over words uses a softmax (see
figure 1(a)).

3.1. relationship between noise contrastive

estimation and importance sampling

as discussed in section 2.3, a large scale softmax is neces-
sary for training good lms because of the vocabulary size.
a hierarchical softmax (mnih & hinton, 2009) employs
a tree in which the id203 distribution over words is
decomposed into a product of two probabilities for each
word, greatly reducing training and id136 time as only
the path speci   ed by the hierarchy needs to be computed
and updated. choosing a good hierarchy is important for
obtaining good results and we did not explore this approach
further for this paper as sampling methods worked well for
our setup.
sampling approaches are only useful during training, as
they propose an approximation to the loss which is cheap to
compute (also in a distributed setting)     however, at infer-
ence time one still has to compute the id172 term
over all words. noise contrastive estimation (nce) pro-
poses to consider a surrogate binary classi   cation task in
which a classi   er is trained to discriminate between true
data, or samples coming from some arbitrary distribution.
if both the noise and data distributions were known, the

optimal classi   er would be:

3.2. id98 softmax

exploring the limits of id38

p(y = true|w) =

pd(w)

pd(w) + kpn(w)

where y is the binary random variable indicating whether
w comes from the true data distribution, k is the number of
negative samples per positive word, and pd and pn are the
data and noise distribution respectively (we dropped any
dependency on previous words for notational simplicity).
it is easy to show that if we train a logistic classi   er
p  (y = true|w) =   (s  (w, h)     log kpn(w)) where   
is the logistic function, then, p(cid:48)(w) = sof tmax(s  (w, h))
is a good approximation of pd(w) (s   is a logit which e.g.
an lstm lm computes).
the other technique, which is based on importance sam-
pling (is), proposes to directly approximate the partition
function (which comprises a sum over all words) with an
estimate of it through importance sampling. though the
methods look super   cially similar, we will derive a similar
surrogate classi   cation task akin to nce which arrives at
is, showing a strong connection between the two.
suppose that, instead of having a binary task to decide if
a word comes from the data or from the noise distribution,
we want to identify the words coming from the true data
distribution in a set w = {w1, . . . , wk+1}, comprised of
k noise samples and one data distribution sample. thus,
we can train a multiclass loss over a multinomial random
variable y which maximizes log p(y = 1|w ), assuming
w.l.o.g. that w1     w is always the word coming from true
data. by bayes rule, and ignoring terms that are constant
with respect to y , we can write:

p(y = k|w )    y

pd(wk)
pn(wk)

and, following a similar argument than for nce, if we de-
   ne p(y = k|w ) = sof tmax(s  (wk)    log pn(wk)) then
p(cid:48)(w) = sof tmax(s  (w, h)) is a good approximation of
pd(word). note that the only difference between nce and
is is that, in nce, we de   ne a binary classi   cation task
between true or noise words with a logistic loss, whereas
in is we de   ne a multiclass classi   cation problem with a
softmax and cross id178 loss. we hope that our deriva-
tion helps clarify the similarities and differences between
the two. in particular, we observe that is, as it optimizes
a multiclass classi   cation task (in contrast to solving a bi-
nary task), may be a better choice. indeed, the updates to
the logits with is are tied whereas in nce they are inde-
pendent.

the character-level features allow for a smoother and com-
pact parametrization of the id27s. recent ef-
forts on small scale id38 have used id98
character embeddings for the input embeddings (kim et al.,
2015). although not as straightforward, we propose an ex-
tension to this idea to also reduce the number of param-
eters of the softmax layer. recall from section 2.3 that
the softmax computes a logit as zw = ht ew where h is
a context vector and ew the id27. instead of
building a matrix of |v |    |h| (whose rows correspond to
ew), we produce ew with a id98 over the characters of w as
ew = cn n (charsw)     we call this a id98 softmax. we
used the same network architecture to dynamically gener-
ate the softmax id27s without sharing the pa-
rameters with the input word-embedding sub-network. for
id136, the vectors ew can be precomputed, so there is no
computational complexity increase w.r.t. the regular soft-
max.
we note that, when using an importance sampling loss such
as the one described in section 3.1, only a few logits have
non-zero gradient (those corresponding to the true and sam-
pled words). with a softmax where ew are independently
learned id27s, this is not a problem. but we
observed that, when using a id98, all the logits become
tied as the function mapping from w to ew is quite smooth.
as a result, a much smaller learning rate had to be used.
even with this, the model lacks capacity to differentiate
between words that have very different meanings but that
are spelled similarly. thus, a reasonable compromise was
to add a small correction factor which is learned per word,
such that:

zw = ht cn n (charsw) + ht m corrw

where m is a matrix projecting a low-dimensional embed-
ding vector corrw back up to the dimensionality of the pro-
jected lstm hidden state of h. this amounts to adding a
bottleneck linear layer, and brings the id98 softmax much
closer to our best result, as can be seen in table 1, where
adding a 128-dim correction halves the gap between regu-
lar and the id98 softmax.
aside from a big reduction in the number of parameters
and incorporating morphological knowledge from words,
the other bene   t of this approach is that out-of-vocabulary
(oov) words can easily be scored. this may be useful for
other problems such as machine translation where han-
dling out-of-vocabulary words is very important (luong
et al., 2014). this approach also allows parallel training
over various data sets since the model is no longer explic-
itly parametrized by the vocabulary size     or the language.
this has shown to help when using byte-level input embed-
dings for id39 (gillick et al., 2015),

exploring the limits of id38

and we hope it will enable similar gains when used to map
onto words.

3.3. char lstm predictions

the id98 softmax layer can handle arbitrary words and is
much more ef   cient in terms of number of parameters than
the full softmax matrix.
it is, though, still considerably
slow, as to evaluate perplexities we need to compute the
partition function. a class of models that solve this prob-
lem more ef   ciently are character-level lstms (sutskever
et al., 2011; graves, 2013). they make predictions one
character at a time, thus allowing to compute probabili-
ties over a much smaller vocabulary. on the other hand,
these models are more dif   cult to train and seem to per-
form worse even in small tasks like ptb (graves, 2013).
most likely this is due to the sequences becoming much
longer on average as the lstm reads the input character
by character instead of word by word.
thus, we combine the word and character-level models by
feeding a word-level lstm hidden state h into a small
lstm that predicts the target word one character at a time
(see figure 1(c)). in order to make the whole process rea-
sonably ef   cient, we train the standard lstm model un-
til convergence, freeze its weights, and replace the stan-
dard word-level softmax layer with the aforementioned
character-level lstm.
the resulting model scales independently of vocabulary
size     both for training and id136. however, it does
seem to be worse than regular and id98 softmax     we are
hopeful that further research will enable these models to
replace    xed vocabulary models whilst being computation-
ally attractive.

4. experiments
all experiments were run using the tensorflow system
(abadi et al., 2015), with the exception of some older mod-
els which were used in the ensemble.

4.1. data set

the experiments are performed on the 1b word bench-
mark data set introduced by (chelba et al., 2013), which is
a publicly available benchmark for measuring progress of
statistical id38. the data set contains about
0.8b words with a vocabulary of 793471 words, including
sentence boundary markers. all the sentences are shuf   ed
and the duplicates are removed. the words that are out of
vocabulary (oov) are marked with a special unk token
(there are approximately 0.3% such words).

4.2. model setup

i

n

(cid:80)

the typical measure used for
reporting progress in
id38 is perplexity, which is the aver-
age per-word log-id203 on the holdout data set:
    1
ln pwi . we follow the standard procedure and sum
e
over all the words (including the end of sentence symbol).
we used the 1b word benchmark data set without any pre-
processing. given the shuf   ed sentences, they are input to
the network as a batch of independent streams of words.
whenever a sentence ends, a new one starts without any
padding (thus maximizing the occupancy per batch).
for the models that consume characters as inputs or as tar-
gets, each word is fed to the model as a sequence of charac-
ter ids of preespeci   ed length (see figure 1(b)). the words
were processed to include special begin and end of word to-
kens and were padded to reach the expected length. i.e. if
the maximum word length was 10, the word    cat    would
be transformed to    $cat       due to the id98 model.
in our experiments we found that limiting the maximum
word length in training to 50 was suf   cient to reach very
good results while 32 was clearly insuf   cient. we used
256 characters in our vocabulary and the non-ascii symbols
were represented as a sequence of bytes.

4.3. model architecture

we evaluated many variations of id56 lm architectures.
these include the dimensionalities of the embedding lay-
ers, the state, projection sizes, and number of lstm layers
to use. exhaustively trying all combinations would be ex-
tremely time consuming for such a large data set, but our
   ndings suggest that lstms with a projection layer (i.e.,
a bottleneck between hidden states as in (sak et al., 2014))
trained with truncated bptt (williams & peng, 1990) for
20 steps performed well.
following (zaremba et al., 2014) we use dropout (srivas-
tava, 2013) before and after every lstm layer. the bi-
ases of lstm forget gate were initialized to 1.0 (jozefow-
icz et al., 2015). the size of the models will be described
in more detail in the following sections, and the choices
of hyper-parameters will be released as open source upon
publication.
for any model using character embedding id98s, we
closely follow the architecture from (kim et al., 2015). the
only important difference is that we use a larger number of
convolutional features of 4096 to give enough capacity to
the model. the resulting embedding is then linearly trans-
formed to match the lstm projection sizes. this allows it
to match the performance of regular id27s but
only uses a small fraction of parameters.

exploring the limits of id38

table 1. best results of single models on the 1b word benchmark. our results are shown below previous work.

model

test perplexity number of params [billions]

sigmoid-id56-2048 (ji et al., 2015a)
interpolated kn 5-gram, 1.1b id165s (chelba et al., 2013)
sparse non-negative matrix lm (shazeer et al., 2015)
id56-1024 + maxent 9-gram features (chelba et al., 2013)

lstm-512-512
lstm-1024-512
lstm-2048-512
lstm-8192-2048 (no dropout)
lstm-8192-2048 (50% dropout)
2-layer lstm-8192-1024 (big lstm)
big lstm+id98 inputs

big lstm+id98 inputs + id98 softmax
big lstm+id98 inputs + id98 softmax + 128-dim correction
big lstm+id98 inputs + char lstm predictions

68.3
67.6
52.9
51.3

54.1
48.2
43.7
37.9
32.2
30.6
30.0

39.8
35.8
47.9

4.1
1.76
33
20

0.82
0.82
0.83
3.3
3.3
1.8
1.04
0.29
0.39
0.23

table 2. best results of ensembles on the 1b word benchmark.

model

test perplexity

large ensemble (chelba et al., 2013)
id56+kn-5 (williams et al., 2015)
id56+kn-5 (ji et al., 2015a)
id56+snm10-skip (shazeer et al., 2015)
large ensemble (shazeer et al., 2015)

our 10 best lstm models (equal weights)
our 10 best lstm models (optimal weights)
10 lstms + kn-5 (equal weights)
10 lstms + kn-5 (optimal weights)
10 lstms + snm10-skip (shazeer et al., 2015)

43.8
42.4
42.0
41.3
41.0

26.3
26.1
25.3
25.1
23.7

4.4. training procedure

the models were trained until convergence with an ada-
grad optimizer using a learning rate of 0.2. in all the exper-
iments the id56s were unrolled for 20 steps without ever
resetting the lstm states. we used a batch size of 128.
we clip the gradients of the lstm weights such that their
norm is bounded by 1.0 (pascanu et al., 2012).
using these hyper-parameters we found large lstms to be
relatively easy to train. the same learning rate was used in
almost all of the experiments. in a few cases we had to re-
duce it by an order of magnitude. unless otherwise stated,
the experiments were performed with 32 gpu workers and
asynchronous gradient updates. further details will be fully
speci   ed with the code upon publication.
training a model for such large target vocabulary (793471
words) required to be careful with some details about the
approximation to full softmax using importance sampling.

we used a large number of negative (or noise) samples:
8192 such samples were drawn per step, but were shared
across all the target words in the batch (2560 total, i.e. 128
times 20 unrolled steps). this results in multiplying (2560
x 1024) times (1024 x (8192+1)) (instead of (2560 x 1024)
times (1024 x 793471)), i.e. about 100-fold less computa-
tion.

5. results and analysis
in this section we summarize the results of our experiments
and do an in-depth analysis. table 1 contains all results for
our models compared to previously published work. ta-
ble 2 shows previous and our own work on ensembles of
models. we hope that our encouraging results, which im-
proved the best perplexity of a single model from 51.3 to
30.0 (whilst reducing the model size considerably), and set
a new record with ensembles at 23.7, will enable rapid re-
search and progress to advance id38. for

exploring the limits of id38

this purpose, we will release the model weights and recipes
upon publication.

5.1. size matters

unsurprisingly, size matters: when training on a very large
and complex data set,    tting the training data with an
lstm is fairly challenging. thus, the size of the lstm
layer is a very important factor that in   uences the results,
as seen in table 1. the best models are the largest we were
able to    t into a gpu memory. our largest model was a 2-
layer lstm with 8192+1024 dimensional recurrent state
in each of the layers. increasing the embedding and projec-
tion size also helps but causes a large increase in the num-
ber of parameters, which is less desirable. lastly, training
an id56 instead of an lstm yields poorer results (about 5
perplexity worse) for a comparable model size.

5.2. id173 importance

as shown in table 1, using dropout improves the results.
to our surprise, even relatively small models (e.g., single
layer lstm with 2048 units projected to 512 dimensional
outputs) can over-   t the training set if trained long enough,
eventually yielding holdout set degradation.
using dropout on non-recurrent connections largely miti-
gates these issues. while over-   tting still occurs, there is
no more need for early stopping. for models that had 4096
or less units in the lstm layer, we used 10% dropout prob-
ability. for larger models, 25% was signi   cantly better.
even with such id173, perplexities on the training
set can be as much as 6 points below test.
in one experiment we tried to use a smaller vocabulary
comprising of the 100,000 most frequent words and found
the difference between train and test to be smaller     which
suggests that too much capacity is given to rare words. this
is less of an issue with character id98 embedding models
as the embeddings are shared across all words.

5.3. importance sampling is data ef   cient

table 3 shows the test perplexities of nce vs is loss after a
few epochs of 2048 unit lstm with 512 projection. the is
objective signi   cantly improves the speed and the overall
performance of the model when compared to nce.

5.4. id27s vs character id98

replacing the embedding layer with a parametrized neural
network that process characters of a given word allows the
model to consume arbitrary words and is not restricted to
a    xed vocabulary. this property is useful for data sets
with conversational or informal text as well as for mor-
phologically rich languages. our experiments show that

table 3. the test perplexities of an lstm-2048-512 trained with
different losses versus number of epochs. the model needs about
40 minutes per epoch. first epoch is a bit slower because we
slowly increase the number of workers.

epochs nce

is

training time [hours]

1
5
10
20
50

97
58
53
49
46.1

60
47.5
45
44
43.7

1
4
8
14
34

table 4. nearest neighbors in the character id98 embedding
space of a few out-of-vocabulary words. even for words that
the model has never seen, the model usually still    nds reasonable
neighbors.

word

top-1

top-2

top-3

incerdible
www.a.com

7546

townhal1
komarski

incredible
www.aa.com

7646

townhall
koharski

nonedible

www.aaa.com

7534
djc2

konarski

extendible
www.ca.com

8566

moodswing360

komanski

using character-level embeddings is feasible and does not
degrade performance     in fact, our best single model uses
a character id98 embedding.
an additional advantage is that the number of parameters of
the input layer is reduced by a factor of 11 (though training
speed is slightly worse). for id136, the embeddings
can be precomputed so there is no speed penalty. overall,
the embedding of the best model is parametrized by 72m
weights (down from 820m weights).
table 4 shows a few examples of nearest neighbor embed-
dings for some out-of-vocabulary words when character
id98s are used.

5.5. smaller models with id98 softmax

even with character-level embeddings, the model is still
fairly large (though much smaller than the best competing
models from previous work). most of the parameters are in
the linear layer before the softmax: 820m versus a total of
1.04b parameters.
in one of the experiments we froze the word-lstm after
convergence and replaced the softmax layer with the id98
softmax sub-network. without any    ne-tuning that model
was able to reach 39.8 perplexity with only 293m weights
(as seen in table 1).
as described in section 3.2, adding a    correction    word
embedding term alleviates the gap between regular and

exploring the limits of id38

id98 softmax. indeed, we can trade-off model size versus
perplexity. for instance, by adding 100m weights (through
a 128 dimensional bottleneck embedding) we achieve 35.8
perplexity (see table 1).
to contrast with the id98 softmax, we also evaluated a
model that replaces the softmax layer with a smaller lstm
that predicts one character at a time (see section 3.3). such
a model does not have to learn long dependencies because
the base lstm still operates at the word-level (see fig-
ure 1(c)). with a single-layer lstm of 1024 units we
reached 49.0 test perplexity, far below the best model. in
order to make the comparisons more fair, we performed a
very expensive marginalization over the words in the vo-
cabulary (to rule out words not in the dictionary which the
character lstm would assign some id203). when
doing this marginalization, the perplexity improved a bit
down to 47.9.

figure 2. the difference in log probabilities between the best
lstm and kn-5 (higher is better). the words from the hold-
out set are grouped into 25 buckets of equal size based on their
frequencies.

5.6. training speed

we used 32 tesla k40 gpus to train our models. the
smaller version of the lstm model with 2048 units and
512 projections needs less than 10 hours to reach below
45 perplexity and after only 2 hours of training the model
beats previous state-of-the art on this data set. the best
model needs about 5 days to get to 35 perplexity and 10
days to 32.5. the best results were achieved after 3 weeks
of training. see table 3 for more details.

5.7. ensembles

we averaged several of our best models and we were able
to reach 23.7 test perplexity (more details and results can
be seen in table 2), which is more than 40% improve-

ment over previous work. interestingly, including the best
id165 model reduces the perplexity by 1.2 point even
though the model is rather weak on its own (67.6 perplex-
ity). most previous work had to either ensemble with the
best id165 model (as their id56 only used a limited out-
put vocabulary of a few thousand words), or use id165
features as additional input to the id56. our results, on
the contrary, suggest that id165s are of limited bene   t,
and suggest that a carefully trained lstm lm is the most
competitive model.

5.8. lstms are best on the tail words

figure 2 shows the difference in log probabilities between
our best model (at 30.0 perplexity) and the kn-5. as can be
seen from the plot, the lstm is better across all the buckets
and signi   cantly outperforms kn-5 on the rare words. this
is encouraging as it seems to suggest that lstm lms may
fare even better for languages or data sets where the number
of rare words is larger than traditional id165 models.

5.9. samples from the model

to qualitatively evaluate the model, we sampled many sen-
tences. we discarded short and politically incorrect ones,
but the sample shown below is otherwise    raw    (i.e., not
hand picked). the samples are of high quality     which is
not a surprise, given the perplexities attained     but there are
still some occasional mistakes.
sentences generated by the ensemble (about 26 perplexity):

< s > with even more new technologies coming onto the market
quickly during the past three years , an increasing number of compa-
nies now must tackle the ever-changing and ever-changing environ-
mental challenges online . < s > check back for updates on this
breaking news story . < s > about 800 people gathered at hever
castle on long beach from noon to 2pm , three to four times that of
the funeral cort`ege . < s > we are aware of written instructions
from the copyright holder not to , in any way , mention rosenberg    s
negative comments if they are relevant as indicated in the documents
,     ebay said in a statement . < s > it is now known that coffee and
cacao products can do no harm on the body . < s > yuri zhirkov
was in attendance at the stamford bridge at the start of the second
half but neither drogba nor malouda was able to push on through the
barcelona defence .

6. discussion and conclusions
in this paper we have shown that id56 lms can be trained
on large amounts of data, and outperform competing mod-
els including carefully tuned id165s. the reduction in
perplexity from 51.3 to 30.0 is due to several key compo-
nents which we studied in this paper. thus, a large, regular-
ized lstm lm, with projection layers and trained with an
approximation to the true softmax with importance sam-
pling performs much better than id165s. unlike previ-
ous work, we do not require to interpolate both the id56
lm and the id165, and the gains of doing so are rather
marginal.

words buckets of equal size (less frequent words on the right)0.00.51.01.52.02.5mean difference in log perplexityexploring the limits of id38

by exploring recent advances in model architectures (e.g.
lstms), exploiting small character id98s, and by sharing
our    ndings in this paper and accompanying code and mod-
els (to be released upon publication), we hope to inspire
research on large scale id38, a problem we
consider crucial towards language understanding. we hope
for future research to focus on reasonably sized datasets
taking inspiration from recent advances seen in the com-
puter vision community thanks to efforts such as id163
(deng et al., 2009).

acknowledgements
we thank ciprian chelba, ilya sutskever, and the google
brain team for their help and discussions. we also thank
koray kavukcuoglu for his help with the manuscript.

references
abadi, mart    n, agarwal, ashish, barham, paul, brevdo,
eugene, chen, zhifeng, citro, craig, corrado, greg s.,
davis, andy, dean, jeffrey, devin, matthieu, ghe-
mawat, sanjay, goodfellow, ian, harp, andrew, irv-
ing, geoffrey, isard, michael, jia, yangqing, jozefowicz,
rafal, kaiser, lukasz, kudlur, manjunath, levenberg,
josh, man  e, dan, monga, rajat, moore, sherry, murray,
derek, olah, chris, schuster, mike, shlens, jonathon,
steiner, benoit, sutskever, ilya, talwar, kunal, tucker,
paul, vanhoucke, vincent, vasudevan, vijay, vi  egas,
fernanda, vinyals, oriol, warden, pete, wattenberg,
martin, wicke, martin, yu, yuan, and zheng, xiaoqiang.
tensorflow: large-scale machine learning on heteroge-
neous systems, 2015. url http://tensorflow.
org/. software available from tensor   ow.org.

arisoy, ebru, sainath, tara n, kingsbury, brian, and ram-
abhadran, bhuvana. deep neural network language mod-
els. in proceedings of the naacl-hlt 2012 workshop:
will we ever really replace the id165 model? on the
future of id38 for hlt, pp. 20   28. as-
sociation for computational linguistics, 2012.

ballesteros, miguel, dyer, chris, and smith, noah a.
improved transition-based parsing by modeling char-
arxiv preprint
acters instead of words with lstms.
arxiv:1508.00657, 2015.

bengio, yoshua and sen  ecal, jean-s  ebastien. adaptive im-
portance sampling to accelerate training of a neural prob-
abilistic language model. neural networks, ieee trans-
actions on, 19(4):713   722, 2008.

bengio, yoshua, schwenk, holger, sen  ecal,

jean-
s  ebastien, morin, fr  ederic, and gauvain, jean-luc.
neural probabilistic language models. in innovations in
machine learning, pp. 137   186. springer, 2006.

chelba, ciprian, mikolov, tomas, schuster, mike, ge,
qi, brants, thorsten, koehn, phillipp, and robinson,
tony. one billion word benchmark for measuring
progress in statistical id38. arxiv preprint
arxiv:1312.3005, 2013.

cho, kyunghyun, van merri  enboer, bart, gulcehre,
caglar, bahdanau, dzmitry, bougares, fethi, schwenk,
holger, and bengio, yoshua. learning phrase represen-
tations using id56 encoder-decoder for statistical machine
translation. arxiv preprint arxiv:1406.1078, 2014.

deng, jia, dong, wei, socher, richard, li, li-jia, li, kai,
id163: a large-scale hierarchical
and fei-fei, li.
image database. in id161 and pattern recog-
nition, 2009. cvpr 2009. ieee conference on, pp. 248   
255. ieee, 2009.

filippova, katja, alfonseca, enrique, colmenares, car-
los a, kaiser, lukasz, and vinyals, oriol. sentence com-
pression by deletion with lstms. in proceedings of the
2015 conference on empirical methods in natural lan-
guage processing, pp. 360   368, 2015.

gers, felix a, schmidhuber, j  urgen, and cummins, fred.
learning to forget: continual prediction with lstm. neu-
ral computation, 12(10):2451   2471, 2000.

gillick, dan, brunk, cliff, vinyals, oriol, and subra-
manya, amarnag. multilingual language processing
from bytes. arxiv preprint arxiv:1512.00103, 2015.

graves, alex. generating sequences with recurrent neural

networks. arxiv preprint arxiv:1308.0850, 2013.

graves, alex and schmidhuber, j  urgen.

framewise
phoneme classi   cation with bidirectional lstm and other
neural network architectures. neural networks, 18(5):
602   610, 2005.

gutmann, michael and hyv  arinen, aapo.

noise-
contrastive estimation: a new estimation principle for
unnormalized statistical models. in international con-
ference on arti   cial intelligence and statistics, pp. 297   
304, 2010.

hochreiter, sepp and schmidhuber, j  urgen. long short-
term memory. neural computation, 9(8):1735   1780,
1997.

bengio, yoshua, sen  ecal, jean-s  ebastien, et al. quick
training of probabilistic neural nets by importance sam-
pling. in aistats, 2003.

ji, shihao, vishwanathan, s. v. n., satish, nadathur, an-
derson, michael j., and dubey, pradeep. blackout:
speeding up recurrent neural network language models

exploring the limits of id38

with very large vocabularies. corr, abs/1511.06909,
2015a.
url http://arxiv.org/abs/1511.
06909.

mikolov, tomas and zweig, geoffrey. context dependent
in slt, pp.

recurrent neural network language model.
234   239, 2012.

ji, yangfeng, cohn, trevor, kong, lingpeng, dyer, chris,
and eisenstein, jacob. document context language mod-
els. arxiv preprint arxiv:1511.03962, 2015b.

jozefowicz, rafal, zaremba, wojciech, and sutskever,
ilya. an empirical exploration of recurrent network ar-
in proceedings of the 32nd international
chitectures.
conference on machine learning (icml-15), pp. 2342   
2350, 2015.

kalchbrenner, nal, grefenstette, edward, and blunsom,
phil. a convolutional neural network for modelling sen-
tences. arxiv preprint arxiv:1404.2188, 2014.

kim, yoon, jernite, yacine, sontag, david, and rush,
alexander m. character-aware neural language models.
arxiv preprint arxiv:1508.06615, 2015.

kneser, reinhard and ney, hermann. improved backing-
off for m-gram id38. in acoustics, speech,
and signal processing, 1995. icassp-95., 1995 inter-
national conference on, volume 1, pp. 181   184. ieee,
1995.

krizhevsky, alex, sutskever, ilya, and hinton, geoffrey e.
id163 classi   cation with deep convolutional neural
networks. in advances in neural information processing
systems, pp. 1097   1105, 2012.

le cun, b boser, denker, john s, henderson, d, howard,
richard e, hubbard, w, and jackel, lawrence d. hand-
written digit recognition with a back-propagation net-
work. in advances in neural information processing sys-
tems. citeseer, 1990.

ling, wang, lu    s, tiago, marujo, lu    s, astudillo,
ram  on fernandez, amir, silvio, dyer, chris, black,
alan w, and trancoso, isabel. finding function in form:
compositional character models for open vocabulary
word representation. arxiv preprint arxiv:1508.02096,
2015.

luong, minh-thang, sutskever, ilya, le, quoc v, vinyals,
oriol, and zaremba, wojciech. addressing the rare word
problem in id4. arxiv preprint
arxiv:1410.8206, 2014.

marcus, mitchell p, marcinkiewicz, mary ann, and san-
torini, beatrice. building a large annotated corpus of
english: the id32. computational linguistics,
19(2):313   330, 1993.

mikolov, tom  a  s. statistical language models based on neu-
ral networks. presentation at google, mountain view,
2nd april, 2012.

mikolov, tomas, kara     at, martin, burget, lukas, cer-
nock`y, jan, and khudanpur, sanjeev. recurrent neural
network based language model. in interspeech, vol-
ume 2, pp. 3, 2010.

mikolov, tomas, deoras, anoop, kombrink, stefan, bur-
get, lukas, and cernock`y, jan. empirical evaluation and
combination of advanced id38 techniques.
in interspeech, number s 1, pp. 605   608, 2011.

mnih, andriy and hinton, geoffrey e. a scalable hierar-
chical distributed language model. in advances in neural
information processing systems, pp. 1081   1088, 2009.

mnih, andriy and kavukcuoglu, koray. learning word
embeddings ef   ciently with noise-contrastive estima-
tion. in advances in neural information processing sys-
tems, pp. 2265   2273, 2013.

morin, frederic and bengio, yoshua. hierarchical proba-
bilistic neural network language model. in aistats, vol-
ume 5, pp. 246   252. citeseer, 2005.

pascanu, razvan, mikolov, tomas, and bengio, yoshua.
on the dif   culty of training recurrent neural networks.
arxiv preprint arxiv:1211.5063, 2012.

rush, alexander m, chopra, sumit, and weston, jason. a
neural attention model for abstractive sentence summa-
rization. arxiv preprint arxiv:1509.00685, 2015.

sak, hasim, senior, andrew w, and beaufays, franc  oise.
long short-term memory recurrent neural network archi-
in inter-
tectures for large scale acoustic modeling.
speech, pp. 338   342, 2014.

schuster, mike and paliwal, kuldip k. bidirectional recur-
rent neural networks. signal processing, ieee transac-
tions on, 45(11):2673   2681, 1997.

schwenk, holger, rousseau, anthony, and attik, mo-
hammed. large, pruned or continuous space language
models on a gpu for id151. in
proceedings of the naacl-hlt 2012 workshop: will
we ever really replace the id165 model? on the fu-
ture of id38 for hlt, pp. 11   19. associ-
ation for computational linguistics, 2012.

serban, iulian vlad, sordoni, alessandro, bengio, yoshua,
courville, aaron c., and pineau, joelle. hierarchical
neural network generative models for movie dialogues.
corr, abs/1507.04808, 2015. url http://arxiv.
org/abs/1507.04808.

exploring the limits of id38

shazeer, noam, pelemans, joris, and chelba, ciprian.
sparse non-negative matrix id38 for skip-
grams. proceedings of interspeech, pp. 1428   1432,
2015.

srivastava, nitish.

improving neural networks with

dropout. phd thesis, university of toronto, 2013.

srivastava, nitish, mansimov, elman, and salakhutdinov,
ruslan. unsupervised learning of video representations
using lstms. arxiv preprint arxiv:1502.04681, 2015a.

srivastava, rupesh k, greff, klaus, and schmidhuber,
in advances in
j  urgen. training very deep networks.
neural information processing systems, pp. 2368   2376,
2015b.

sutskever, ilya, martens, james, and hinton, geoffrey e.
generating text with recurrent neural networks. in pro-
ceedings of the 28th international conference on ma-
chine learning (icml-11), pp. 1017   1024, 2011.

sutskever, ilya, vinyals, oriol, and le, quoc v.

se-
in
quence to sequence learning with neural networks.
advances in neural information processing systems, pp.
3104   3112, 2014.

vaswani, ashish, zhao, yinggong, fossum, victoria, and
chiang, david. decoding with large-scale neural lan-
guage models improves translation. citeseer.

vincent, pascal, de br  ebisson, alexandre, and bouthillier,
xavier. ef   cient exact gradient update for training deep
networks with very large sparse targets. in advances in
neural information processing systems, pp. 1108   1116,
2015.

vinyals, oriol and le, quoc. a neural conversational

model. arxiv preprint arxiv:1506.05869, 2015.

wang, tian and cho, kyunghyun. larger-context language

modelling. arxiv preprint arxiv:1511.03729, 2015.

williams, ronald j and peng, jing. an ef   cient gradient-
based algorithm for on-line training of recurrent network
trajectories. neural computation, 2(4):490   501, 1990.

williams, will, prasad, niranjani, mrva, david, ash, tom,
and robinson, tony. scaling recurrent neural network
language models. in acoustics, speech and signal pro-
cessing (icassp), 2015 ieee international conference
on, pp. 5391   5395. ieee, 2015.

zaremba, wojciech, sutskever, ilya, and vinyals, oriol.
recurrent neural network id173. arxiv preprint
arxiv:1409.2329, 2014.

