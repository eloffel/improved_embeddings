sequence-level knowledge distillation

yoon kim

alexander m. rush

yoonkim@seas.harvard.edu

srush@seas.harvard.edu

school of engineering and applied sciences

harvard university

cambridge, ma, usa

6
1
0
2

 

p
e
s
2
2

 

 
 
]
l
c
.
s
c
[
 
 

4
v
7
4
9
7
0

.

6
0
6
1
:
v
i
x
r
a

abstract

id4 (id4) offers a
novel alternative formulation of translation
that is potentially simpler than statistical ap-
proaches. however to reach competitive per-
formance, id4 models need to be exceed-
ingly large. in this paper we consider applying
knowledge distillation approaches (bucila et
al., 2006; hinton et al., 2015) that have proven
successful for reducing the size of neural mod-
els in other domains to the problem of id4.
we demonstrate that standard knowledge dis-
tillation applied to word-level prediction can
be effective for id4, and also introduce two
novel sequence-level versions of knowledge
distillation that further improve performance,
and somewhat surprisingly, seem to elimi-
nate the need for id125 (even when ap-
plied on the original teacher model). our best
student model runs 10 times faster than its
state-of-the-art teacher with little loss in per-
formance.
it is also signi   cantly better than
a baseline model trained without knowledge
distillation: by 4.2/1.7 id7 with greedy de-
coding/id125. applying weight prun-
ing on top of knowledge distillation results in
a student model that has 13   fewer param-
eters than the original teacher model, with a
decrease of 0.4 id7.

1

introduction

id4 (id4) (kalchbrenner
and blunsom, 2013; cho et al., 2014; sutskever et
al., 2014; bahdanau et al., 2015) is a deep learning-
based method for translation that has recently shown
promising results as an alternative to statistical ap-

proaches. id4 systems directly model the proba-
bility of the next word in the target sentence sim-
ply by conditioning a recurrent neural network on
the source sentence and previously generated target
words.

while both simple and surprisingly accurate,
id4 systems typically need to have very high ca-
pacity in order to perform well: sutskever et al.
(2014) used a 4-layer lstm with 1000 hidden units
per layer (herein 4  1000) and zhou et al. (2016) ob-
tained state-of-the-art results on english     french
with a 16-layer lstm with 512 units per layer. the
sheer size of the models requires cutting-edge hard-
ware for training and makes using the models on
standard setups very challenging.

this issue of excessively large networks has been
observed in several other domains, with much fo-
cus on fully-connected and convolutional networks
for multi-class classi   cation. researchers have par-
ticularly noted that large networks seem to be nec-
essary for training, but learn redundant representa-
tions in the process (denil et al., 2013). therefore
compressing deep models into smaller networks has
been an active area of research. as deep learning
systems obtain better results on nlp tasks, compres-
sion also becomes an important practical issue with
applications such as running deep learning models
for speech and translation locally on cell phones.

existing compression methods generally fall into
two categories: (1) pruning and (2) knowledge dis-
tillation. pruning methods (lecun et al., 1990; he
et al., 2014; han et al., 2016), zero-out weights or
entire neurons based on an importance criterion: le-
cun et al. (1990) use (a diagonal approximation to)

the hessian to identify weights whose removal min-
imally impacts the objective function, while han
et al. (2016) remove weights based on threshold-
ing their absolute values. knowledge distillation ap-
proaches (bucila et al., 2006; ba and caruana, 2014;
hinton et al., 2015) learn a smaller student network
to mimic the original teacher network by minimiz-
ing the loss (typically l2 or cross-id178) between
the student and teacher output.

in this work, we investigate knowledge distilla-
tion in the context of id4. we
note that id4 differs from previous work which has
mainly explored non-recurrent models in the multi-
class prediction setting. for id4, while the model
is trained on multi-class prediction at the word-level,
it is tasked with predicting complete sequence out-
puts conditioned on previous decisions. with this
difference in mind, we experiment with standard
knowledge distillation for id4 and also propose
two new versions of the approach that attempt to ap-
proximately match the sequence-level (as opposed
to word-level) distribution of the teacher network.
this sequence-level approximation leads to a sim-
ple training procedure wherein the student network
is trained on a newly generated dataset that is the
result of running id125 with the teacher net-
work.

we run experiments to compress a large state-of-
the-art 4    1000 lstm model, and    nd that with
sequence-level knowledge distillation we are able to
learn a 2   500 lstm that roughly matches the per-
formance of the full system. we see similar results
compressing a 2    500 model down to 2    100 on
a smaller data set. furthermore, we observe that
our proposed approach has other bene   ts, such as
not requiring any id125 at test-time. as a re-
sult we are able to perform greedy decoding on the
2    500 model 10 times faster than id125 on
the 4    1000 model with comparable performance.
our student models can even be run ef   ciently on
a standard smartphone.1 finally, we apply weight
pruning on top of the student network to obtain a
model that has 13   fewer parameters than the origi-
nal teacher model. we have released all the code for
the models described in this paper.2

1https://github.com/harvardnlp/id4-android
2https://github.com/harvardnlp/id195-attn

2 background

2.1 sequence-to-sequence with attention
let s = [s1, . . . , si ] and t = [t1, . . . , tj ] be (random
variable sequences representing) the source/target
sentence, with i and j respectively being the
source/target lengths. machine translation involves
   nding the most probable target sentence given the
source:

p(t| s)

argmax

t   t

where t is the set of all possible sequences. id4
models parameterize p(t| s) with an encoder neural
network which reads the source sentence and a de-
coder neural network which produces a distribution
over the target sentence (one word at a time) given
the source. we employ the attentional architecture
from luong et al. (2015), which achieved state-of-
the-art results on english     german translation.3

2.2 knowledge distillation
knowledge distillation describes a class of methods
for training a smaller student network to perform
better by learning from a larger teacher network
(in addition to learning from the training data set).
we generally assume that the teacher has previously
been trained, and that we are estimating parame-
ters for the student. knowledge distillation suggests
training by matching the student   s predictions to the
teacher   s predictions. for classi   cation this usually
means matching the probabilities either via l2 on
the log scale (ba and caruana, 2014) or by cross-
id178 (li et al., 2014; hinton et al., 2015).

concretely, assume we are learning a multi-class
classi   er over a data set of examples of the form
(x, y) with possible classes v. the usual training
criteria is to minimize nll for each example from
the training data,

lnll(  ) =    

1{y = k} log p(y = k | x;   )

k=1

where 1{  } is the indicator function and p the
distribution from our model (parameterized by   ).

3speci   cally, we use the global-general attention model
with the input-feeding approach. we refer the reader to the orig-
inal paper for further details.

|v|(cid:88)

figure 1: overview of the different knowledge distillation approaches. in word-level knowledge distillation (left) cross-id178
is minimized between the student/teacher distributions (yellow) for each word in the actual target sequence (ecd), as well as
between the student distribution and the degenerate data distribution, which has all of its probabilitiy mass on one word (black). in
sequence-level knowledge distillation (center) the student network is trained on the output from id125 of the teacher network
that had the highest score (acf). in sequence-level interpolation (right) the student is trained on the output from id125 of
the teacher network that had the highest sim with the target sequence (ece).

|v|(cid:88)

this objective can be seen as minimizing the cross-
id178 between the degenerate data distribution
(which has all of its id203 mass on one class)
and the model distribution p(y | x;   ).
in knowledge distillation, we assume access to
a learned teacher distribution q(y | x;   t ), possibly
trained over the same data set. instead of minimiz-
ing cross-id178 with the observed data, we instead
minimize the cross-id178 with the teacher   s prob-
ability distribution,

lkd(  ;   t ) =    

q(y = k | x;   t )  

k=1

log p(y = k | x;   )

where   t parameterizes the teacher distribution and
remains    xed. note the cross-id178 setup is iden-
tical, but the target distribution is no longer a sparse
distribution.4 training on q(y | x;   t ) is attractive
since it gives more information about other classes
for a given data point (e.g.
similarity between
classes) and has less variance in gradients (hinton
et al., 2015).

4 in some cases the id178 of the teacher/student distribu-
tion is increased by annealing it with a temperature term    > 1

  p(y | x)     p(y | x)

1
  

after testing        {1, 1.5, 2} we found that    = 1 worked best.

since this new objective has no direct term for the
training data, it is common practice to interpolate
between the two losses,

l(  ;   t ) = (1       )lnll(  ) +   lkd(  ;   t )

where    is mixture parameter combining the one-hot
distribution and the teacher distribution.

3 knowledge distillation for id4
the large sizes of id4 sys-
tems make them an ideal candidate for knowledge
distillation approaches. in this section we explore
three different ways this technique can be applied to
id4.

3.1 word-level knowledge distillation
id4 systems are trained directly to minimize word
nll, lword-nll, at each position. therefore if
we have a teacher model, standard knowledge distil-
lation for multi-class cross-id178 can be applied.
we de   ne this distillation for a sentence as,

lword-kd =     j(cid:88)

|v|(cid:88)

j=1

k=1

q(tj = k | s, t<j)   
log p(tj = k | s, t<j)

where v is the target vocabulary set. the student
can further be trained to optimize the mixture of

lword-kd and lword-nll. in the context of id4,
we refer to this approach as word-level knowledge
distillation and illustrate this in figure 1 (left).

3.2 sequence-level knowledge distillation
word-level knowledge distillation allows transfer of
these local word distributions. ideally however, we
would like the student model to mimic the teacher   s
actions at the sequence-level. the sequence distri-
bution is particularly important for id4, because
wrong predictions can propagate forward at test-
time.

first, consider the sequence-level distribution
speci   ed by the model over all possible sequences
t     t ,

p(t| s) =

p(tj | s, t<j)

j(cid:89)

j=1

for any length j. the sequence-level negative log-
likelihood for id4 then involves matching the one-
hot distribution over all complete sequences,

1{t = y} log p(t| s)

lseq-nll =    (cid:88)
=     j(cid:88)

|v|(cid:88)

t   t

1{yj = k} log p(tj = k | s, t<j)

j=1

k=1

= lword-nll

where y = [y1, . . . , yj ] is the observed sequence.
of course,
this just shows that from a negative
log likelihood perspective, minimizing word-level
nll and sequence-level nll are equivalent in this
model.

but now consider the case of sequence-level
knowledge distillation. as before, we can simply
replace the distribution from the data with a prob-
ability distribution derived from our teacher model.
however, instead of using a single word prediction,
we use q(t| s) to represent the teacher   s sequence
distribution over the sample space of all possible se-
quences,

lseq-kd =    (cid:88)

t   t

q(t| s) log p(t| s)

note that lseq-kd is inherently different from
lword-kd, as the sum is over an exponential num-
ber of terms. despite its intractability, we posit

that this sequence-level objective is worthwhile. it
gives the teacher the chance to assign probabilities to
complete sequences and therefore transfer a broader
range of knowledge. we thus consider an approxi-
mation of this objective.

our simplest approximation is to replace the

teacher distribution q with its mode,
q(t| s)     1{t = argmax

q(t| s)}

t   t

observing that    nding the mode is itself intractable,
we use id125 to    nd an approximation. the
loss is then

lseq-kd        (cid:88)

1{t =   y} log p(t| s)

t   t

=     log p(t =   y | s)

where   y is now the output from running id125
with the teacher model.
using the mode seems like a poor approximation
for the teacher distribution q(t| s), as we are ap-
proximating an exponentially-sized distribution with
a single sample. however, previous results showing
the effectiveness of id125 decoding for id4
lead us to belief that a large portion of q   s mass lies
in a single output sequence. in fact, in experiments
we    nd that with beam of size 1, q(  y | s) (on aver-
age) accounts for 1.3% of the distribution for ger-
man     english, and 2.3% for thai     english (ta-
ble 1: p(t =   y)).5

to summarize, sequence-level knowledge distil-
lation suggests to: (1) train a teacher model, (2) run
id125 over the training set with this model, (3)
train the student network with cross-id178 on this
new dataset. step (3) is identical to the word-level
nll process except now on the newly-generated
data set. this is shown in figure 1 (center).

5additionally there are simple ways to better approximate
q(t| s). one way would be to consider a k-best list from beam
search and renormalizing the probabilities,

q(t| s)    

(cid:80)

q(t| s)
t   tk

q(t| s)

where tk is the k-best list from id125. this would
increase the training set by a factor of k. a beam of size
5 captures 2.8% of the distribution for german     english,
and 3.8% for thai     english. another alternative is to use a
monte carlo estimate and sample from the teacher model (since
lseq-kd = et   q(t | s)[    log p(t| s) ]). however in practice we
found the (approximate) mode to work well.

3.3 sequence-level interpolation
next we consider integrating the training data back
into the process, such that we train the student
model as a mixture of our sequence-level teacher-
generated data (lseq-kd) with the original training
data (lseq-nll),
l = (1       )lseq-nll +   lseq-kd

=    (1       ) log p(y | s)       

q(t| s) log p(t| s)

(cid:88)

t   t

where y is the gold target sequence.

since the second term is intractable, we could
again apply the mode approximation from the pre-
vious section,

l =    (1       ) log p(y | s)        log p(  y | s)

and train on both observed (y) and teacher-
generated (  y) data. however, this process is non-
ideal for two reasons: (1) unlike for standard knowl-
edge distribution, it doubles the size of the training
data, and (2) it requires training on both the teacher-
generated sequence and the true sequence, condi-
tioned on the same source input. the latter concern
is particularly problematic since we observe that y
and   y are often quite different.

as an alternative, we propose a single-sequence
approximation that is more attractive in this setting.
this approach is inspired by local updating (liang
et al., 2006), a method for discriminative train-
ing in id151 (although to
our knowledge not for knowledge distillation). lo-
cal updating suggests selecting a training sequence
which is close to y and has high id203 under
the teacher model,

  y = argmax

t   t

sim(t, y)q(t| s)

where sim is a function measuring closeness (e.g.
jaccard similarity or id7 (papineni et al., 2002)).
following local updating, we can approximate this
sequence by running id125 and choosing

  y     argmax
t   tk

sim(t, y)

where tk is the k-best list from id125.
we take sim to be smoothed sentence-level id7
(chen and cherry, 2014).

we justify training on   y from a knowledge distil-
lation perspective with the following generative pro-
cess: suppose that there is a true target sequence
(which we do not observe) that is    rst generated
from the underlying data distribution d. and further
suppose that the target sequence that we observe (y)
is a noisy version of the unobserved true sequence:
i.e. (i) t     d, (ii) y      (t), where  (t) is, for ex-
ample, a noise function that independently replaces
each element in t with a random element in v with
some small id203.6 in such a case, ideally the
student   s distribution should match the mixture dis-
tribution,

dseq-inter     (1       )d +   q(t| s)

in this setting, due to the noise assumption, d now
has signi   cant id203 mass around a neighbor-
hood of y (not just at y), and therefore the argmax
of the mixture distribution is likely something other
than y (the observed sequence) or   y (the output from
id125). we can see that   y is a natural approx-
imation to the argmax of this mixture distribution
between d and q(t| s) for some   . we illustrate
this framework in figure 1 (right) and visualize the
distribution over a real example in figure 2.

4 experimental setup
to test out these approaches, we conduct two sets of
id4 experiments: high resource (english     ger-
man) and low resource (thai     english).

the english-german data comes from wmt
2014.7 the training set has 4m sentences and we
take newstest2012/newstest2013 as the dev set and
newstest2014 as the test set. we keep the top 50k
most frequent words, and replace the rest with unk.
the teacher model is a 4    1000 lstm (as in lu-
ong et al. (2015)) and we train two student models:
2    300 and 2    500. the thai-english data comes
from iwslt 2015.8 there are 90k sentences in the
6while we employ a simple (unrealistic) noise function for
illustrative purposes, the generative story is quite plausible if we
consider a more elaborate noise function which includes addi-
tional sources of noise such as phrase reordering, replacement
of words with synonyms, etc. one could view translation hav-
ing two sources of variance that should be modeled separately:
variance due to the source sentence (t     d), and variance due
to the individual translator (y      (t)).

7http://statmt.org/wmt14
8https://sites.google.com/site/iwsltevaluation2015/mt-track

adopt a    ne-tuning approach where we begin train-
ing from a pretrained model (either on original data
or seq-kd data) and train with a smaller learning
rate (0.1). for english-german we generate seq-
inter data on a smaller portion of the training set
(    50%) for ef   ciency.

the above methods are complementary and can
be combined with each other. for example, we
can train on teacher-generated data but still
in-
clude a word-level cross-id178 term between the
teacher/student (seq-kd + word-kd in table 1),
or    ne-tune towards seq-inter data starting from the
baseline model trained on original data (baseline +
seq-inter in table 1).9

5 results and discussion
results of our experiments are shown in table
1. we    nd that while word-level knowledge dis-
tillation (word-kd) does improve upon the base-
line, sequence-level knowledge distillation (seq-
kd) does better on english     german and per-
forms similarly on thai     english. combining
them (seq-kd + word-kd) results in further gains
for the 2    300 and 2    100 models (although not
for the 2    500 model), indicating that these meth-
ods provide orthogonal means of transferring knowl-
edge from the teacher to the student: word-kd is
transferring knowledge at the the local (i.e. word)
level while seq-kd is transferring knowledge at the
global (i.e. sequence) level.

sequence-level interpolation (seq-inter), in addi-
tion to improving models trained via word-kd and
seq-kd, also improves upon the original teacher
model that was trained on the actual data but    ne-
tuned towards seq-inter data (baseline + seq-inter).
in fact, greedy decoding with this    ne-tuned model
has similar performance (19.6) as id125 with
the original model (19.5), allowing for faster decod-
ing even with an identically-sized model.

we hypothesize that sequence-level knowledge
distillation is effective because it allows the student
network to only model relevant parts of the teacher
distribution (i.e. around the teacher   s mode) instead
of    wasting    parameters on trying to model the entire
9for instance,    seq-kd + seq-inter + word-kd    in table
1 means that the model was trained on seq-kd data and    ne-
tuned towards seq-inter data with the mixture cross-id178
loss at the word-level.

figure 2: visualization of sequence-level interpolation on an
example german     english sentence: bis 15 tage vor an-
reise sind zimmer-annullationen kostenlos. we run beam
search, plot the    nal hidden state of the hypotheses using id167
and show the corresponding (smoothed) probabilities with con-
tours. in the above example, the sentence that is at the top of
the beam after id125 (green) is quite far away from gold
(red), so we train the model on a sentence that is on the beam
but had the highest sim (e.g. id7) to gold (purple).

training set and we take 2010/2011/2012 data as the
dev set and 2012/2013 as the test set, with a vocabu-
lary size is 25k. size of the teacher model is 2   500
(which performed better than 4  1000, 2  750 mod-
els), and the student model is 2  100. other training
details mirror luong et al. (2015).

we

evaluate

on
multi-id7.perl,
the following variations:

tokenized id7 with
and
experiment with

word-level knowledge distillation (word-kd)
student is trained on the original data and addition-
ally trained to minimize the cross-id178 of the
teacher distribution at the word-level. we tested
       {0.5, 0.9} and found    = 0.5 to work better.
sequence-level knowledge distillation (seq-kd)
student is trained on the teacher-generated data,
which is the result of running id125 and tak-
ing the highest-scoring sequence with the teacher
model. we use beam size k = 5 (we did not see
improvements with a larger beam).

sequence-level interpolation (seq-inter) stu-
dent is trained on the sequence on the teacher   s beam
that had the highest id7 (beam size k = 35). we

model
english     german wmt 2014
teacher baseline 4    1000 (params: 221m)

baseline + seq-inter

student baseline 2    500 (params: 84m)

student baseline 2    300 (params: 49m)

word-kd
seq-kd
baseline + seq-inter
word-kd + seq-inter
seq-kd + seq-inter
seq-kd + word-kd
seq-kd + seq-inter + word-kd

word-kd
seq-kd
baseline + seq-inter
word-kd + seq-inter
seq-kd + seq-inter
seq-kd + word-kd
seq-kd + seq-inter + word-kd

thai     english iwslt 2015
teacher baseline 2    500 (params: 47m)

baseline + seq-inter

17.7
19.6

14.7
15.4
18.9
18.5
18.3
18.9
18.7
18.8

14.1
14.9
18.1
17.6
17.8
18.2
17.9
18.5

14.3
15.6

id7k=1    k=1 id7k=5    k=5

ppl p(t =   y)

6.7
10.4

8.2
8.0
22.7
11.3
11.8
15.8
10.9
14.8

10.3
10.9
64.4
13.0
14.5
40.8
44.1
97.1

22.9
55.1

1.3%
8.2%

0.9%
1.0%
16.9%
5.7%
6.3%
7.6%
4.1%
7.1%

0.6%
0.7%
14.8%
10.0%
4.3%
5.6%
3.1%
5.9%

2.3%
6.8%

19.5
19.8

17.6
17.7
19.0
18.7
18.5
19.3
18.9
19.2

16.9
17.6
18.1
17.9
18.0
18.5
18.8
18.9

15.7
16.0

   
+1.9
   
+0.7
+4.2
+3.6
+3.6
+4.2
+4.0
+4.1
   
+0.8
+4.0
+3.5
+3.7
+4.1
+3.8
+4.4

   
+1.3
   
+1.2
+2.2
+2.3
+2.4
+3.0
+3.1
+3.6

   
+0.3
   
+0.1
+1.4
+1.1
+0.9
+1.7
+1.3
+1.6
   
+0.7
+1.2
+1.0
+1.1
+1.6
+1.9
+2.0

   
+0.3
   
+0.9
+0.7
+0.4
+1.0
+1.3
+1.5
+1.7

student baseline 2    100 (params: 8m)

10.6
11.8
12.8
12.9
13.0
13.6
13.7
14.2

word-kd
seq-kd
baseline + seq-inter
word-kd + seq-inter
seq-kd + seq-inter
seq-kd + word-kd
seq-kd + seq-inter + word-kd

1.4%
1.4%
6.9%
2.5%
3.2%
3.9%
3.1%
3.2%
table 1: results on english-german (newstest2014) and thai-english (2012/2013) test sets. id7k=1: id7 score with beam
size k = 1 (i.e. greedy decoding);    k=1: id7 gain over the baseline model without any knowledge distillation with greedy
decoding; id7k=5: id7 score with beam size k = 5;    k=5: id7 gain over the baseline model without any knowledge
distillation with beam size k = 5; ppl: perplexity on the test set; p(t =   y): id203 of output sequence from greedy decoding
(averaged over the test set). params: number of parameters in the model. best results (as measured by improvement over the
baseline) within each category are highlighted in bold.

37.0
35.3
125.4
52.8
58.7
106.4
67.4
117.4

12.7
13.6
13.4
13.1
13.7
14.0
14.2
14.4

space of translations. our results suggest that this
is indeed the case:
the id203 mass that seq-
kd models assign to the approximate mode is much
higher than is the case for baseline models trained
on original data (table 1: p(t =   y)). for example,
on english     german the (approximate) argmax
for the 2    500 seq-kd model (on average) ac-
counts for 16.9% of the total id203 mass, while
the corresponding number is 0.9% for the baseline.

this also explains the success of greedy decoding
for seq-kd models   since we are only modeling
around the teacher   s mode, the student   s distribution
is more peaked and therefore the argmax is much
easier to    nd. seq-inter offers a compromise be-
tween the two, with the greedily-decoded sequence
accounting for 7.6% of the distribution.

finally, although past work has shown that mod-
els with lower perplexity generally tend to have

model size
beam = 1 (greedy)
4    1000
2    500
2    300
beam = 5
4    1000
2    500
2    300

gpu

cpu android

425.5
1051.3
1267.8

15.0
63.6
104.3

101.9
181.9
189.1

7.9
22.1
38.4

   
8.8
15.8

   
1.9
3.4

table 2: number of source words translated per second across
gpu (geforce gtx titan x), cpu, and smartphone (samsung
galaxy 6) for the various english     german models. we were
unable to open the 4    1000 model on the smartphone.

higher id7, our results indicate that this is not
necessarily the case. the perplexity of the baseline
2    500 english     german model is 8.2 while the
perplexity of the corresponding seq-kd model is
22.7, despite the fact that seq-kd model does sig-
ni   cantly better for both greedy (+4.2 id7) and
id125 (+1.4 id7) decoding.

5.1 decoding speed
run-time complexity for id125 grows linearly
with beam size. therefore, the fact that sequence-
level knowledge distillation allows for greedy de-
coding is signi   cant, with practical implications for
running id4 systems across various devices. to
test the speed gains, we run the teacher/student mod-
els on gpu, cpu, and smartphone, and check the
average number of source words translated per sec-
ond (table 2). we use a geforce gtx titan x for
gpu and a samsung galaxy 6 smartphone. we    nd
that we can run the student model 10 times faster
with greedy decoding than the teacher model with
id125 on gpu (1051.3 vs 101.9 words/sec),
with similar performance.

5.2 weight pruning
although knowledge distillation enables training
faster models,
the number of parameters for the
student models is still somewhat large (table 1:
params), due to the id27s which dom-
inate most of the parameters.10 for example, on the

model
4    1000
2    500
2    500
2    500
2    500
2    500

19.5
19.3

prune % params id7 ratio
1  
3  
5  
13  
18  
26  

0% 221 m
84 m
0%
42 m
17 m
13 m
8 m

50%
80%
85%
90%

19.3
19.1
18.8
18.5

table 3: performance of student models with varying % of the
weights pruned. top two rows are models without any pruning.
params: number of parameters in the model; prune %: percent-
age of weights pruned based on their absolute values; id7:
id7 score with id125 decoding (k = 5) after retrain-
ing the pruned model; ratio: ratio of the number of parameters
versus the original teacher model (which has 221m parameters).

2    500 english     german model the word em-
beddings account for approximately 63% (50m out
of 84m) of the parameters. the size of word em-
beddings have little impact on run-time as the word
embedding layer is a simple lookup table that only
affects the    rst layer of the model.

we therefore focus next on reducing the mem-
ory footprint of the student models further through
weight pruning. weight pruning for id4 was re-
cently investigated by see et al. (2016), who found
that up to 80     90% of the parameters in a large
id4 model can be pruned with little loss in perfor-
mance. we take our best english     german student
model (2   500 seq-kd + seq-inter) and prune x%
of the parameters by removing the weights with the
lowest absolute values. we then retrain the pruned
model on seq-kd data with a learning rate of 0.2
and    ne-tune towards seq-inter data with a learning
rate of 0.1. as observed by see et al. (2016), re-
training proved to be crucial. the results are shown
in table 3.

our    ndings suggest that compression bene   ts
achieved through weight pruning and knowledge
distillation are orthogonal.11 pruning 80% of the
weight in the 2    500 student model results in a
model with 13   fewer parameters than the original
teacher model with only a decrease of 0.4 id7.
while pruning 90% of the weights results in a more
appreciable decrease of 1.0 id7, the model is

10id27s scale linearly while id56 parameters

11to our knowledge combining pruning and knowledge dis-

scale quadratically with the dimension size.

tillation has not been investigated before.

drastically smaller with 8m parameters, which is
26   fewer than the original teacher model.
5.3 further observations

    for models trained with word-level knowledge
distillation, we also tried regressing the student
network   s top-most hidden layer at each time
step to the teacher network   s top-most hidden
layer as a pretraining step, noting that romero
et al. (2015) obtained improvements with a
similar technique on feed-forward models. we
found this to give comparable results to stan-
dard knowledge distillation and hence did not
pursue this further.

    there have been promising recent results on
eliminating id27s completely and
obtaining word representations directly from
characters with character composition models,
which have many fewer parameters than word
embedding lookup tables (ling et al., 2015a;
kim et al., 2016; ling et al., 2015b; jozefowicz
et al., 2016; costa-jussa and fonollosa, 2016).
combining such methods with knowledge dis-
tillation/pruning to further reduce the memory
footprint of id4 systems remains an avenue
for future work.

student/teacher network as a pretraining step, while
mou et al. (2015) obtain smaller id27s
from a teacher model via regression. there has also
been work on transferring knowledge across differ-
ent network architectures: chan et al. (2015b) show
that a deep non-recurrent neural network can learn
from an id56; geras et al. (2016) train a id98 to
mimic an lstm for id103. kuncoro
et al. (2016) recently investigated knowledge distil-
lation for id170 by having a single
parser learn from an ensemble of parsers.

other approaches for compression involve low
rank factorizations of weight matrices (denton et al.,
2014; jaderberg et al., 2014; lu et al., 2016; prab-
havalkar et al., 2016), sparsity-inducing regularizers
(murray and chiang, 2015), binarization of weights
(courbariaux et al., 2016; lin et al., 2016), and
weight sharing (chen et al., 2015; han et al., 2016).
finally, although we have motivated sequence-level
knowledge distillation in the context of training a
smaller model, there are other techniques that train
on a mixture of the model   s predictions and the data,
such as local updating (liang et al., 2006), hope/fear
training (chiang, 2012), searn (daum  e iii et al.,
2009), dagger (ross et al., 2011), and minimum
risk training (och, 2003; shen et al., 2016).

6 related work

7 conclusion

compressing deep learning models is an active area
of current research. pruning methods involve prun-
ing weights or entire neurons/nodes based on some
criterion. lecun et al. (1990) prune weights based
on an approximation of the hessian, while han et al.
(2016) show that a simple magnitude-based pruning
works well. prior work on removing neurons/nodes
include srinivas and babu (2015) and mariet and
sra (2016). see et al. (2016) were the    rst to ap-
ply pruning to id4, observ-
ing that that different parts of the architecture (in-
put id27s, lstm matrices, etc.) admit
different levels of pruning. knowledge distillation
approaches train a smaller student model to mimic
a larger teacher model, by minimizing the loss be-
tween the teacher/student predictions (bucila et al.,
2006; ba and caruana, 2014; li et al., 2014; hin-
ton et al., 2015). romero et al. (2015) addition-
ally regress on the intermediate hidden layers of the

in this work we have investigated existing knowl-
edge distillation methods for id4 (which work at
the word-level) and introduced two sequence-level
variants of knowledge distillation, which provide
improvements over standard word-level knowledge
distillation.

we have chosen to focus on translation as this
domain has generally required the largest capacity
deep learning models, but the sequence-to-sequence
framework has been successfully applied to a wide
range of tasks including parsing (vinyals et al.,
2015a), summarization (rush et al., 2015), dialogue
(vinyals and le, 2015; serban et al., 2016; li et
al., 2016), ner/pos-tagging (gillick et al., 2016),
image captioning (vinyals et al., 2015b; xu et al.,
2015), video generation (srivastava et al., 2015), and
id103 (chan et al., 2015a). we antici-
pate that methods described in this paper can be used
to similarly train smaller models in other domains.

references
[ba and caruana2014] lei jimmy ba and rich caruana.
in

2014. do deep nets really need to be deep?
proceedings of nips.

[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2015. neural machine
translation by jointly learning to align and translate.
in proceedings of iclr.

[bucila et al.2006] cristian bucila, rich caruana, and
alexandru niculescu-mizil. 2006. model compres-
sion. in proceedings of kdd.

[chan et al.2015a] william chan, navdeep jaitly, quoc
le, and oriol vinyals. 2015a. listen, attend and
spell. arxiv:1508.01211.

[chan et al.2015b] william chan, nan rosemary ke, and
ian laner. 2015b. transfering knowledge from a
id56 to a dnn. arxiv:1504.01483.

[chen and cherry2014] boxing chen and colin cherry.
2014. a systematic comparison of smoothing tech-
niques for sentence-level id7. in proceedings of
the ninth workshop on statistical machine transla-
tion.

[chen et al.2015] wenlin chen, james t. wilson, stephen
tyree, kilian q. weinberger, and yixin chen. 2015.
compressing neural networks with the hashing
trick. in proceedings of icml.

[chiang2012] david chiang.

2012. hope and fear
for discriminative training of statistical translation
models. in jmlr.

[cho et al.2014] kyunghyun cho, bart van merrienboer,
caglar gulcehre, dzmitry bahdanau, fethi bougares,
holger schwenk, and yoshua bengio. 2014. learning
phrase representations using id56 encoder-decoder
for id151. in proceedings of
emnlp.

[costa-jussa and fonollosa2016] marta r. costa-jussa
and jose a.r. fonollosa. 2016. character-based neu-
ral machine translation. arxiv:1603.00810.

[courbariaux et al.2016] matthieu courbariaux,

itay
hubara, daniel soudry, ran el-yaniv, and yoshua
bengio. 2016. binarized neural networks: training
neural networks with weights and activations
constrained to +1 or    1. arxiv:1602.02830.

[daum  e iii et al.2009] hal daum  e iii, john langford,
and daniel marcu. 2009. search-based structured
prediction. machine learning.

[denil et al.2013] misha denil, babak shakibi, laurent
dinh, marc   aurelio ranzato, and nando de freitas.
2013. predicting parameters in deep learning.
in
proceedings of nips.

[denton et al.2014] emily l. denton, wojciech zaremba,
joan bruna, yann lecun, and rob fergus. 2014. ex-
ploiting linear structure within convolutional neural

networks for ef   cient evaluation. in proceedings of
nips.

[geras et al.2016] krzysztof j. geras, abdel rahman mo-
hamed, rich caruana, gregor urban, shengjie wang,
ozlem aslan, matthai philipose, matthew richard-
son, and charles sutton. 2016. blending lstms into
id98s. in proceedings of iclr workshop.

[gillick et al.2016] dan gillick, cliff brunk, oriol
vinyals, and amarnag subramanya. 2016. multilin-
gual language processing from bytes. in proceedings
of naacl.

[han et al.2016] song han, huizi mao, and william j.
dally. 2016. deep compression: compressing deep
neural networks with pruning, trained quantization
and huffman coding. in proceedings of iclr.

[he et al.2014] tianxing he, yuchen fan, yanmin qian,
tian tan, and kai yu. 2014. reshaping deep neu-
ral network for fast decoding by node-pruning. in
proceedings of icassp.

[hinton et al.2015] geoffrey hinton, oriol vinyals, and
jeff dean. 2015. distilling the knowledge in a neural
network. arxiv:1503.0253.

[jaderberg et al.2014] max jaderberg, andrea vedaldi,
and andrew zisserman. 2014. speeding up convo-
lutional neural networks with low rank expansions.
in bmcv.

[jozefowicz et al.2016] rafal jozefowicz, oriol vinyals,
mike schuster, noam shazeer, and yonghui wu.
2016. exploring the limits of id38.
arxiv:1602.02410.

[kalchbrenner and blunsom2013] nal kalchbrenner and
phil blunsom. 2013. recurrent continuous transla-
tion models. in proceedings of emnlp.

[kim et al.2016] yoon kim, yacine jernite, david son-
tag, and alexander m. rush. 2016. character-aware
neural language models. in proceedings of aaai.

[kuncoro et al.2016] adhiguna kuncoro, miguel balles-
teros, lingpeng kong, chris dyer, and noah a. smith.
2016. distilling an ensemble of greedy dependency
in proceedings of
parsers into one mst parser.
emnlp.

[lecun et al.1990] yann lecun, john s. denker, and
sara a. solla. 1990. optimal brain damage. in pro-
ceedings of nips.

[li et al.2014] jinyu li, rui zhao, jui-ting huang, and
yifan gong. 2014. learning small-size dnn with
output-distribution-based criteria. in proceedings of
interspeech.

[li et al.2016] jiwei li, michael galley, chris brockett,
jianfeg gao, and bill dolan. 2016. a diversity-
promoting objective function for neural conversa-
tional models. in proceedings of naacl 2016.

[liang et al.2006] percy liang, alexandre bouchard-
cote, dan klein, and ben taskar. 2006. an end-to-
end discriminative approach to machine translation.
in proceedings of coling-acl.

[lin et al.2016] zhouhan lin, matthieu coubariaux,
roland memisevic, and yoshua bengio. 2016. neural
networks with few multiplications. in proceedings of
iclr.

[ling et al.2015a] wang ling, tiago lui, luis marujo,
ramon fernandez astudillo, silvio amir, chris dyer,
alan w black, and isabel trancoso. 2015a. finding
function in form: composition character models for
open vocabulary word representation. in proceed-
ings of emnlp.

[ling et al.2015b] wang ling,

isabel trancoso, chris
dyer, and alan w black. 2015b. character-based
id4. arxiv:1511.04586.

[lu et al.2016] zhiyun lu, vikas sindhwani, and tara n.
sainath. 2016. learning compact recurrent neural
networks. in proceedings of icassp.

[luong et al.2015] minh-thang luong, hieu pham, and
christopher d. manning. 2015. effective approaches
to attention-based id4.
in
proceedings of emnlp.

[mariet and sra2016] zelda mariet and suvrit sra. 2016.

diversity networks. in proceedings of iclr.

[mou et al.2015] lili mou, ge li, yan xu, lu zhang, and
zhi jin. 2015. distilling id27s: an en-
coding approach. arxiv:1506.04488.

[murray and chiang2015] kenton murray and david
chiang. 2015. auto-sizing neural networks: with
in pro-
applications to id165 language models.
ceedings of emnlp.

[och2003] franz j. och. 2003. minimum error rate
in pro-

training in id151.
ceedings of acl.

[papineni et al.2002] kishore papineni, slim roukos,
2002. id7: a
todd ward, and wei-jing zhu.
method for automatic evaluation of machine trans-
lation. in proceedings of icml.

[prabhavalkar et al.2016] rohit prabhavalkar, ouais al-
sharif, antoine bruguier, and ian mcgraw.
2016.
on the compression of recurrent neural networks
with an application to lvcsr acoustic modeling for
in proceedings of
embedded id103.
icassp.

[romero et al.2015] adriana romero, nicolas ballas,
samira ebrahimi kahou, antoine chassang, carlo
gatta, and yoshua bengio. 2015. fitnets: hints for
thin deep nets. in proceedings of iclr.

[ross et al.2011] stephane ross, geoffrey j. gordon, and
drew bagnell. 2011. a reduction of imitation learn-
ing and id170 to no-regret online
learning. in proceedings of aistats.

[rush et al.2015] alexander m. rush, sumit chopra, and
jason weston. 2015. a neural attention model for
abstractive sentence summarization. in proceedings
of emnlp.

[see et al.2016] abigail see, minh-thang luong, and
christopher d. manning. 2016. compression of neu-
ral machine translation via pruning. in proceedings
of conll.

[serban et al.2016] iulian v. serban, allesandro sordoni,
yoshua bengio, aaron courville, and joelle pineau.
2016. building end-to-end dialogue systems using
generative hierarchical neural network models.
in
proceedings of aaai.

[shen et al.2016] shiqi shen, yong cheng, zhongjun he,
wei he, hua wu, masong sun, and yang liu. 2016.
minimum risk training for neural machine transla-
tion. in proceedings of acl.

[srinivas and babu2015] suraj srinivas and r. venkatesh
babu. 2015. data-free parameter pruning for deep
neural networks. bmvc.

[srivastava et al.2015] nitish srivastava, elman mansi-
mov, and ruslan salakhutdinov. 2015. unsupervised
learning of video representations using lstms.
proceedings of icml.

[sutskever et al.2014] ilya sutskever, oriol vinyals, and
quoc le. 2014. sequence to sequence learning with
neural networks. in proceedings of nips.

[vinyals and le2015] oriol vinyals and quoc le. 2015.
in proceedings of

a neural conversational model.
icml deep learning workshop.

[vinyals et al.2015a] oriol vinyals, lukasz kaiser, terry
koo, slave petrov, ilya sutskever, and geoffrey hin-
ton. 2015a. grammar as a foreign language. in pro-
ceedings of nips.

[vinyals et al.2015b] oriol vinyals, alexander toshev,
samy bengio, and dumitru erhan. 2015b. show and
tell: a neural image caption generator. in proceed-
ings of cvpr.

[xu et al.2015] kelvin xu,

jimma ba, ryan kiros,
kyunghyun cho, aaron courville, ruslan salakhutdi-
nov, richard zemel, and yoshua bengio. 2015. show,
attend and tell: neural image id134
with visual attention. in proceedings of icml.

[zhou et al.2016] jie zhou, ying cao, xuguang wang,
peng li, and wei xu. 2016. deep recurrent models
with fast-forward connections for neural machine
translation. in proceedings of tacl.

