rationalizing neural predictions

tao lei, regina barzilay and tommi jaakkola

computer science and arti   cial intelligence laboratory
{taolei, regina, tommi}@csail.mit.edu

massachusetts institute of technology

6
1
0
2

 

v
o
n
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
5
5
1
4
0

.

6
0
6
1
:
v
i
x
r
a

abstract

prediction without justi   cation has limited ap-
plicability. as a remedy, we learn to extract
pieces of input text as justi   cations     ratio-
nales     that are tailored to be short and co-
herent, yet suf   cient for making the same pre-
diction. our approach combines two modu-
lar components, generator and encoder, which
are trained to operate well together. the gen-
erator speci   es a distribution over text frag-
ments as candidate rationales and these are
passed through the encoder for prediction. ra-
tionales are never given during training.
in-
stead, the model is regularized by desiderata
for rationales. we evaluate the approach on
multi-aspect id31 against manu-
ally annotated test cases. our approach out-
performs attention-based baseline by a signif-
icant margin. we also successfully illustrate
the method on the question retrieval task.1

introduction

1
many recent advances in nlp problems have come
from formulating and training expressive and elabo-
rate neural models. this includes models for senti-
ment classi   cation, parsing, and machine translation
among many others. the gains in accuracy have,
however, come at the cost of interpretability since
complex neural models offer little transparency con-
cerning their inner workings. in many applications,
such as medicine, predictions are used to drive criti-
cal decisions, including treatment options. it is nec-
essary in such cases to be able to verify and under-

1our code and data are available at https://github.

com/taolei87/rid98.

figure 1: an example of a review with ranking in two cate-
gories. the rationale for look prediction is shown in bold.

stand the underlying basis for the decisions.
ide-
ally, complex neural models would not only yield
improved performance but would also offer inter-
pretable justi   cations     rationales     for their predic-
tions.

in this paper, we propose a novel approach to in-
corporating rationale generation as an integral part
of the overall learning problem. we limit ourselves
to extractive (as opposed to abstractive) rationales.
from this perspective, our rationales are simply sub-
sets of the words from the input text that satisfy two
key properties. first, the selected words represent
short and coherent pieces of text (e.g., phrases) and,
second, the selected words must alone suf   ce for
prediction as a substitute of the original text. more
concretely, consider the task of multi-aspect senti-
ment analysis. figure 1 illustrates a product review
along with user rating in terms of two categories or
aspects. if the model in this case predicts    ve star
rating for color, it should also identify the phrase    a
very pleasant ruby red-amber color    as the rationale
underlying this decision.

in most practical applications, rationale genera-

the	beer	was	n   t	what	i	expected,	and	i   m	not	sure	it   s	   true	to	style   ,	but	i	thought	it	was	delicious.	a	very	pleasant	ruby	red-amber	color	with	a	rela9vely	brilliant	   nish,	but	a	limited	amount	of	carbona9on,	from	the	look	of	it.	aroma	is	what	i	think	an	amber	ale	should	be	-	a	nice	blend	of	caramel	and	happiness	bound	together.reviewratingslook: 5 starssmell: 4 starstion must be learned entirely in an unsupervised
manner. we therefore assume that our model with
rationales is trained on the same data as the origi-
nal neural models, without access to additional ra-
tionale annotations. in other words, target rationales
are never provided during training; the intermedi-
ate step of rationale generation is guided only by the
two desiderata discussed above. our model is com-
posed of two modular components that we call the
generator and the encoder. our generator speci   es a
distribution over possible rationales (extracted text)
and the encoder maps any such text to task speci   c
target values. they are trained jointly to minimize
a cost function that favors short, concise rationales
while enforcing that the rationales alone suf   ce for
accurate prediction.

the notion of what counts as a rationale may be
ambiguous in some contexts and the task of select-
ing rationales may therefore be challenging to eval-
uate. we focus on two domains where ambiguity
is minimal (or can be minimized). the    rst sce-
nario concerns with multi-aspect id31
exempli   ed by the beer review corpus (mcauley et
al., 2012). a smaller test set in this corpus iden-
ti   es, for each aspect, the sentence(s) that relate to
this aspect. we can therefore directly evaluate our
predictions on the sentence level with the caveat that
our model makes selections on a    ner level, in terms
of words, not complete sentences. the second sce-
nario concerns with the problem of retrieving similar
questions. the extracted rationales should capture
the main purpose of the questions. we can therefore
evaluate the quality of rationales as a compressed
proxy for the full text in terms of retrieval perfor-
mance. our model achieves high performance on
both tasks. for instance, on the sentiment predic-
tion task, our model achieves extraction accuracy of
96%, as compared to 38% and 81% obtained by the
bigram id166 and a neural attention baseline.

2 related work

developing sparse interpretable models is of con-
siderable interest to the broader research commu-
nity(letham et al., 2015; kim et al., 2015). the need
for interpretability is even more pronounced with
recent neural models. efforts in this area include
analyzing and visualizing state activation (hermans

and schrauwen, 2013; karpathy et al., 2015; li et
al., 2016), learning sparse interpretable word vec-
tors (faruqui et al., 2015b), and linking word vectors
to semantic lexicons or word properties (faruqui et
al., 2015a; herbelot and vecchi, 2015).

beyond learning to understand or further con-
strain the network to be directly interpretable, one
can estimate interpretable proxies that approximate
the network. examples include extracting    if-then   
rules (thrun, 1995) and id90 (craven
and shavlik, 1996) from trained networks. more
recently, ribeiro et al. (2016) propose a model-
agnostic framework where the proxy model
is
learned only for the target sample (and its neighbor-
hood) thus ensuring locally valid approximations.
our work differs from these both in terms of what is
meant by an explanation and how they are derived.
in our case, an explanation consists of a concise yet
suf   cient portion of the text where the mechanism
of selection is learned jointly with the predictor.

attention based models offer another means to ex-
plicate the inner workings of neural models (bah-
danau et al., 2015; cheng et al., 2016; martins
and astudillo, 2016; chen et al., 2015; xu and
saenko, 2015; yang et al., 2015). such models have
been successfully applied to many nlp problems,
improving both prediction accuracy as well as vi-
sualization and interpretability (rush et al., 2015;
rockt  aschel et al., 2016; hermann et al., 2015).
xu et al. (2015) introduced a stochastic attention
mechanism together with a more standard soft at-
tention on image captioning task. our rationale ex-
traction can be understood as a type of stochastic
attention although architectures and objectives dif-
fer. moreover, we compartmentalize rationale gen-
eration from downstream encoding so as to expose
knobs to directly control types of rationales that are
acceptable, and to facilitate broader modular use in
other applications.

finally, we contrast our work with rationale-based
classi   cation (zaidan et al., 2007; marshall et al.,
2015; zhang et al., 2016) which seek to improve pre-
diction by relying on richer annotations in the form
of human-provided rationales.
in our work, ratio-
nales are never given during training. the goal is to
learn to generate them.

3 extractive rationale generation

we formalize here the task of extractive rationale
generation and illustrate it in the context of neural
models. to this end, consider a typical nlp task
where we are provided with a sequence of words
as input, namely x = {x1,       , xl}, where each
xt     rd denotes the vector representation of the i-
th word. the learning problem is to map the input
sequence x to a target vector in rm. for example,
in multi-aspect id31 each coordinate
of the target vector represents the response or rat-
ing pertaining to the associated aspect. in text re-
trieval, on the other hand, the target vectors are used
to induce similarity assessments between input se-
quences. broadly speaking, we can solve the associ-
ated learning problem by estimating a complex pa-
rameterized mapping enc(x) from input sequences
to target vectors. we call this mapping an encoder.
the training signal for these vectors is obtained ei-
ther directly (e.g., multi-id31) or via
similarities (e.g., text retrieval). the challenge is
that a complex neural encoder enc(x) reveals lit-
tle about its internal workings and thus offers little
in the way of justi   cation for why a particular pre-
diction was made.

in extractive rationale generation, our goal is to
select a subset of the input sequence as a rationale.
in order for the subset to qualify as a rationale it
should satisfy two criteria: 1) the selected words
should be interpretable and 2) they ought to suf   ce
to reach nearly the same prediction (target vector)
as the original input.
in other words, a rationale
must be short and suf   cient. we will assume that
a short selection is interpretable and focus on opti-
mizing suf   ciency under cardinality constraints.

we encapsulate the selection of words as a ratio-
nale generator which is another parameterized map-
ping gen(x) from input sequences to shorter se-
quences of words. thus gen(x) must include only a
few words and enc(gen(x)) should result in nearly
the same target vector as the original input passed
through the encoder or enc(x). we can think of the
generator as a tagging model where each word in the
input receives a binary tag pertaining to whether it is
selected to be included in the rationale. in our case,
the generator is probabilistic and speci   es a distri-
bution over possible selections.

the rationale generation task is entirely unsuper-
vised in the sense that we assume no explicit anno-
tations about which words should be included in the
rationale. put another way, the rationale is intro-
duced as a latent variable, a constraint that guides
how to interpret the input sequence. the encoder
and generator are trained jointly, in an end-to-end
fashion so as to function well together.

4 encoder and generator
we use multi-aspect sentiment prediction as a guid-
ing example to instantiate the two key components    
the encoder and the generator. the framework itself
generalizes to other tasks.
encoder enc(  ): given a training instance (x, y)
where x = {xt}l
t=1 is the input text sequence of
length l and y     [0, 1]m is the target m-dimensional
sentiment vector, the neural encoder predicts   y =
enc(x). if trained on its own, the encoder would
aim to minimize the discrepancy between the pre-
dicted sentiment vector   y and the gold target vector
y. we will use the squared error (i.e. l2 distance)
as the sentiment id168,

l(x, y) = (cid:107)  y     y(cid:107)2

2 = (cid:107)enc(x)     y(cid:107)2

2

the encoder could be realized in many ways such
as a recurrent neural network. for example, let
ht = fe(xt, ht   1) denote a parameterized recurrent
unit mapping input word xt and previous state ht   1
to next state ht. the target vector is then generated
on the basis of the    nal state reached by the recur-
rent unit after processing all the words in the input
sequence. speci   cally,

ht = fe(xt, ht   1), t = 1, . . . , l
  y =   e(wehl + be)

generator gen(  ): the rationale generator ex-
tracts a subset of text from the original input x to
function as an interpretable summary. thus the ra-
tionale for a given sequence x can be equivalently
de   ned in terms of binary variables {z1,       , zl}
where each zt     0, 1 indicates whether word xt is
selected or not. from here on, we will use z to
specify the binary selections and thus (z, x) is the
actual rationale generated (selections, input). we
will use generator gen(x) as synonymous with a

l(cid:89)

id203 distribution over binary selections, i.e.,
z     gen(x)     p(z|x) where the length of z varies
with the input x.

in a simple generator, the id203 that the tth
word is selected can be assumed to be conditionally
independent from other selections given the input x.
that is, the joint id203 p(z|x) factors accord-
ing to

p(z|x) =

p(zt|x)

(independent selection)

t=1

the component distributions p(zt|x) can be mod-
eled using a shared bi-directional recurrent neural
      
network. speci   cally, let
f () be the for-
ward and backward recurrent unit, respectively, then

      
f () and

      
ht =
      
ht =

      
f (xt,
      
f (xt,
p(zt|x) =   z(wz[

         
ht   1)
         
ht+1)
      
      
ht] + bz)
ht;

independent but context dependent selection of
words is often suf   cient. however, the model is un-
able to select phrases or refrain from selecting the
same word again if already chosen. to this end, we
also introduce a dependent selection of words,

p(z|x) =

p(zt|x, z1        zt   1)

t=1

which can be also expressed as a recurrent neural
network. to this end, we introduce another hidden
state st whose role is to couple the selections. for
example,

      
p(zt|x, z1,t   1) =   z(wz[
ht;
      
      
ht; zt], st   1)
ht;

st = fz([

      
ht; st   1] + bz)

joint objective: a rationale in our de   nition cor-
responds to the selected words, i.e., {xk|zk = 1}.
we will use (z, x) as the shorthand for this rationale
and, thus, enc(z, x) refers to the target vector ob-
tained by applying the encoder to the rationale as the
input. our goal here is to formalize how the ratio-
nale can be made short and meaningful yet function
well in conjunction with the encoder. our generator
and encoder are learned jointly to interact well but
they are treated as independent units for modularity.

l(cid:89)

the generator is guided in two ways during learn-
ing. first, the rationale that it produces must suf   ce
as a replacement for the input text. in other words,
the target vector (sentiment) arising from the ratio-
nale should be close to the gold sentiment. the cor-
responding id168 is given by

l(z, x, y) = (cid:107)enc(z, x)     y(cid:107)2

2

note that the id168 depends directly (para-
metrically) on the encoder but only indirectly on the
generator via the sampled selection.

second, we must guide the generator to realize
short and coherent rationales. it should select only a
few words and those selections should form phrases
(consecutive words) rather than represent isolated,
disconnected words. we therefore introduce an ad-
ditional regularizer over the selections

   (z) =   1(cid:107)z(cid:107) +   2

|zt     zt   1|

(cid:88)

t

where the    rst term penalizes the number of selec-
tions while the second one discourages transitions
(encourages continuity of selections). note that this
regularizer also depends on the generator only indi-
rectly via the selected rationale. this is because it
is easier to assess the rationale once produced rather
than directly guide how it is obtained.
our    nal cost function is the combination of the
two, cost(z, x, y) = l(z, x, y) +    (z). since the
selections are not provided during training, we min-
imize the expected cost:

ez   gen(x) [cost(z, x, y)]

(cid:88)

min
  e,  g

(x,y)   d

where   e and   g denote the set of parameters of the
encoder and generator, respectively, and d is the
collection of training instances. our joint objective
encourages the generator to compress the input text
into coherent summaries that work well with the as-
sociated encoder it is trained with.

minimizing the expected cost is challenging since
it involves summing over all the possible choices
of rationales z. this summation could potentially
be made feasible with additional restrictive assump-
tions about the generator and encoder. however, we
assume only that it is possible to ef   ciently sample
from the generator.

doubly stochastic gradient we now derive a
sampled approximation to the gradient of the ex-
pected cost objective. this sampled approxima-
tion is obtained separately for each input text x so
as to work well with an overall stochastic gradient
method. consider therefore a training pair (x, y).
for the parameters of the generator   g,

number of reviews
avg length of review
avg correlation between aspects
max correlation between two aspects
number of annotated reviews

1580k
144.9
63.5%
79.1%
994
table 1: statistics of the beer review dataset.

   ez   gen(x) [cost(z, x, y)]

(cid:88)
(cid:88)

z

z

     g

cost(z, x, y)       p(z|x)
cost(z, x, y)       p(z|x)

     g

     g

=

=

   p(z|x)
p(z|x)

using the fact (log f (  ))(cid:48) = f(cid:48)(  )/f (  ), we get

(cid:88)

z

cost(z, x, y)       p(z|x)
(cid:88)

   p(z|x)
p(z|x)
cost(z, x, y)        log p(z|x)

     g

=

z

= ez   gen(x)

cost(z, x, y)

(cid:20)

     g

   p(z|x)

(cid:21)

    log p(z|x)

     g

the last term is the expected gradient where the ex-
pectation is taken with respect to the generator dis-
tribution over rationales z. therefore, we can simply
sample a few rationales z from the generator gen(x)
and use the resulting average gradient in an overall
stochastic gradient method. a sampled approxima-
tion to the gradient with respect to the encoder pa-
rameters   e can be derived similarly,

   ez   gen(x) [cost(z, x, y)]

(cid:88)

z

=

     e
   cost(z, x, y)

     e

   p(z|x)

(cid:20)    cost(z, x, y)

(cid:21)

= ez   gen(x)

     e

choice of recurrent unit we employ recurrent
convolution (rid98), a re   nement of local-ngram
based convolution. rid98 attempts to learn id165
features that are not necessarily consecutive, and
average features in a dynamic (recurrent) fashion.
speci   cally, for bigrams (   lter width n = 2) rid98
computes ht = f (xt, ht   1) as follows

  t =   (w  xt + u  ht   1 + b  )
t =   t (cid:12) c(1)
c(1)
t =   t (cid:12) c(2)
c(2)
ht = tanh(c(2)

t   1 + (1       t) (cid:12) (w1xt)
t   1 + (1       t) (cid:12) (c(1)
t + b)

t   1 + w2xt)

rid98 has been shown to work remarkably in clas-
si   cation and retrieval applications (lei et al., 2015;
lei et al., 2016) compared to other alternatives such
id98s and lstms. we use it for all the recurrent
units introduced in our model.

5 experiments

we evaluate the proposed joint model on two nlp
applications: (1) multi-aspect id31 on
product reviews and (2) similar text retrieval on
askubuntu id53 forum.

5.1 multi-aspect id31
dataset we use the beeradvocate2 review dataset
used in prior work (mcauley et al., 2012).3 this
dataset contains 1.5 million reviews written by the
website users. the reviews are naturally multi-
aspect     each of them contains multiple sentences
describing the overall impression or one particu-
lar aspect of a beer, including appearance, smell
(aroma), palate and the taste. in addition to the writ-
ten text, the reviewer provides the ratings (on a scale
of 0 to 5 stars) for each aspect as well as an overall
rating. the ratings can be fractional (e.g. 3.5 stars),
so we normalize the scores to [0, 1] and use them as
the (only) supervision for regression.

mcauley et al. (2012) also provided sentence-
level annotations on around 1,000 reviews. each
sentence is annotated with one (or multiple) aspect
label, indicating what aspect this sentence covers.

2www.beeradvocate.com
3http://snap.stanford.edu/data/

web-beeradvocate.html

method

appearance

smell

palate

% precision % selected % precision % selected % precision % selected

id166
attention model
generator (independent)
generator (recurrent)

38.3
80.6
94.8
96.3

13
13
13
14

21.6
88.4
93.8
95.1

7
7
7
7

24.9
65.3
79.3
80.2

7
7
7
7

table 2: precision of selected rationales for the    rst three aspects. the precision is evaluated based on whether the selected words
are in the sentences describing the target aspect, based on the sentence-level annotations. best training epochs are selected based
on the objective value on the development set (no sentence annotation is used).

d
id166
260k
id166
1580k
lstm 260k
rid98
260k

d
-
-
200
200

l
-
-
2
2

|  |
mse
2.5m 0.0154
7.3m 0.0100
0.0094
644k
0.0087
323k

table 3: comparing neural encoders with bigram id166 model.
mse is the mean squared error on the test set. d is the amount
of data used for training and development. d stands for the hid-
den dimension, l denotes the depth of network and |  | denotes
the number of parameters (number of features for id166).

we use this set as our test set to evaluate the preci-
sion of words in the extracted rationales.

table 1 shows several statistics of the beer review
dataset. the sentiment correlation between any pair
of aspects (and the overall score) is quite high, get-
ting 63.5% on average and a maximum of 79.1%
(between the taste and overall score).
if directly
training the model on this set, the model can be con-
fused due to such strong correlation. we therefore
perform a preprocessing step, picking    less corre-
lated    examples from the dataset.4 this gives us a
de-correlated subset for each aspect, each contain-
ing about 80k to 90k reviews. we use 10k as the
development set. we focus on three aspects since
the fourth aspect taste still gets > 50% correlation
with the overall sentiment.

sentiment prediction before training the joint
model, it is worth assessing the neural encoder sepa-
rately to check how accurately the neural network
predicts the sentiment. to this end, we compare
neural encoders with bigram id166 model, training
medium and large id166 models using 260k and all

4speci   cally, for each aspect we train a simple linear regres-
sion model to predict the rating of this aspect given the ratings
of the other four aspects. we then keep picking reviews with
largest prediction error until the sentiment correlation in the se-
lected subset increases dramatically.

figure 2: mean squared error of all aspects on the test set (y-
axis) when various percentages of text are extracted as ratio-
nales (x-axis). 220k training data is used.

1580k reviews respectively. as shown in table 3,
the recurrent neural network models outperform the
id166 model for sentiment prediction and also re-
quire less training data to achieve the performance.
the lstm and rid98 units obtain similar test er-
ror, getting 0.0094 and 0.0087 mean squared error
respectively. the rid98 unit performs slightly bet-
ter and uses less parameters. based on the results,
we choose the rid98 encoder network with 2 stack-
ing layers and 200 hidden states.

to train the joint model, we also use rid98 unit
with 200 states as the forward and backward recur-
rent unit for the generator gen(). the dependent
generator has one additional recurrent layer. for this
layer we use 30 states so the dependent version still
has a number of parameters comparable to the inde-
pendent version. the two versions of the generator
have 358k and 323k parameters respectively.

figure 2 shows the performance of our joint de-
pendent model when trained to predict the sentiment
of all aspects. we vary the id173   1 and   2
to show various runs that extract different amount of
text as rationales. our joint model gets performance
close to the best encoder run (with full text) when
few words are extracted.

0.0080.0100.0120.0140.0160%25%50%75%100%0.015id1660.009encoderfigure 3: examples of extracted rationales indicating the sentiments of various aspects. the extracted texts for appearance, smell
and palate are shown in red, blue and green color respectively. the last example is shortened for space.

figure 4: precision (y-axis) when various percentages of text
are extracted as rationales (x-axis) for the appearance aspect.

rationale selection to evaluate the supporting
rationales for each aspect, we train the joint encoder-
generator model on each de-correlated subset. we
set the cardinality id173   1 between values
{2e     4, 3e     4, 4e     4} so the extracted rationale
texts are neither too long nor too short. for simplic-
ity, we set   2 = 2  1 to encourage local coherency
of the extraction.

for comparison we use the bigram id166 model
and implement an attention-based neural network
model. the id166 model successively extracts un-
igram or bigram (from the test reviews) with the
highest feature. the attention-based model learns a
normalized attention vector of the input tokens (us-
ing similarly the forward and backward id56s), then
the model averages over the encoder states accord-
ingly to the attention, and feed the averaged vector
to the output layer. similar to the id166 model, the
attention-based model can selects words based on
their attention weights.

figure 5: learning curves of the optimized cost function on the
development set and the precision of rationales on the test set.
the smell (aroma) aspect is the target aspect.

table 2 presents the precision of the extracted ra-
tionales calculated based on sentence-level aspect
annotations. the   1 id173 hyper-parameter
is tuned so the two versions of our model extract
similar number of words as rationales. the id166
and attention-based model are constrained similarly
for comparison. figure 4 further shows the preci-
sion when different amounts of text are extracted.
again, for our model this corresponds to changing
the   1 id173. as shown in the table and the
   gure, our encoder-generator networks extract text
pieces describing the target aspect with high preci-
sion, ranging from 80% to 96% across the three as-
pects appearance, smell and palate. the id166 base-
line performs poorly, achieving around 30% accu-
racy. the attention-based model achieves reasonable
but worse performance than the rationale generator,
suggesting the potential of directly modeling ratio-
nales as explicit extraction.

a	beer	that	is	not	sold	in	my	neck	of	the	woods	,	but	managed	to	get	while	on	a	roadtrip	.	poured	into	an	imperial	pint	glass	with	a	generous	head	that	sustained	life	throughout	.	nothing	out	of	the	ordinary	here	,	but	a	good	brew	s9ll	.	body	was	kind	of	heavy	,	but	not	thick	.	the	hop	smell	was	excellent	and	en9cing	.	very	drinkablevery	dark	beer	.	pours	a	nice	   nger	and	a	half	of	creamy	foam	and	stays	throughout	the	beer	.	smells	of	co   ee	and	roasted	malt	.	has	a	major	co   ee-like	taste	with	hints	of	chocolate	.	if	you	like	black	co   ee	,	you	will	love	this	porter	.	creamy	smooth	mouthfeel	and	de   nitely	gets	smoother	on	the	palate	once	it	warms	.	it	's	an	ok	porter	but	i	feel	there	are	much	beaer	one	's	out	there	.poured	into	a	sniber	.	produces	a	small	co   ee	head	that	reduces	quickly	.	black	as	night	.	preay	typical	imp	.	roasted	malts	hit	on	the	nose	.	a	liale	sweet	chocolate	follows	.	big	toasty	character	on	the	taste	.	in	between	i	'm	gedng	plenty	of	dark	chocolate	and	some	biaer	espresso	.	it	   nishes	with	hop	biaerness	.	nice	smooth	mouthfeel	with	perfect	carbona9on	for	the	style	.	overall	a	nice	stout	i	would	love	to	have	again	,	maybe	with	some	age	on	it	.i	really	did	not	like	this	.	it	just	seemed	extremely	watery	.	i	dont	'	think	this	had	any	carbona9on	whatsoever	.	maybe	it	was	   at	,	who	knows	?	but	even	if	i	got	a	bad	brew	i	do	n't	see	how	this	would	possibly	be	something	i	'd	get	9me	and	9me	again	.	i	could	taste	the	hops	towards	the	middle	,	but	the	beer	got	preay	nasty	towards	the	boaom	.	i	would	never	drink	this	again	,	unless	it	was	free	.	i	'm	kind	of	upset	i	bought	this	.a	:	poured	a	nice	dark	brown	with	a	tan	colored	head	about	half	an	inch	thick	,	nice	red/garnet	accents	when	held	to	the	light	.	liale	clumps	of	lacing	all	around	the	glass	,	not	too	shabby	.	not	terribly	impressive	though	s	:	smells	like	a	more	guinness-y	guinness	really	,	there	are	some	roasted	malts	there	,	signature	guinness	smells	,	less	burnt	though	,	a	liale	bit	of	chocolate	   	   	m	:	rela9vely	thick	,	it	is	n't	an	export	stout	or	imperial	stout	,	but	s9ll	is	preay	heby	in	the	mouth	,	very	smooth	,	not	much	carbona9on	.	not	too	shabby	d	:	not	quite	as	drinkable	as	the	draught	,	but	s9ll	not	too	bad	.	i	could	easily	see	drinking	a	few	of	these	.id166attentiongen (independent)gen (recurrent)173.9189.1697.41296.5355.9388.11394.91496.3548.5586.41692.91691.2744.7784.1942.2982.31141.21179.81338.31377.11536.71574.41735.11771.63048658310057911131517id166attentiongen (independent)gen (recurrent) 10501000.030.040.050.06gen (recurrent)gen (independent)0.20.40.60.81.0figure 5 shows the learning curves of our model
for the smell aspect.
in the early training epochs,
both the independent and (recurrent) dependent se-
lection models fail to produce good rationales, get-
ting low precision as a result. after a few epochs
of exploration however, the models start to achieve
high accuracy. we observe that the dependent ver-
sion learns more quickly in general, but both ver-
sions obtain close results in the end.

finally we conduct a qualitative case study on
the extracted rationales. figure 3 presents several
reviews, with highlighted rationales predicted by
the model. our rationale generator identi   es key
phrases or adjectives that indicate the sentiment of
a particular aspect.

5.2 similar text retrieval on qa forum
dataset for our second application, we use
the real-world askubuntu5 dataset used in recent
work (dos santos et al., 2015; lei et al., 2016). this
set contains a set of 167k unique questions (each
consisting a question title and a body) and 16k user-
identi   ed similar question pairs. following previ-
ous work, this data is used to train the neural en-
coder that learns the vector representation of the
input question, optimizing the cosine distance (i.e.
cosine similarity) between similar questions against
random non-similar ones. we use the    one-versus-
all    hinge loss (i.e. positive versus other negatives)
for the encoder, similar to (lei et al., 2016). dur-
ing development and testing, the model is used to
score 20 candidate questions given each query ques-
tion, and a total of 400  20 query-candidate question
pairs are annotated for evaluation6.

task/evaluation setup the question descriptions
are often long and fraught with irrelevant details. in
this set-up, a fraction of the original question text
should be suf   cient to represent its content, and be
used for retrieving similar questions. therefore, we
will evaluate rationales based on the accuracy of the
question retrieval task, assuming that better ratio-
nales achieve higher performance. to put this per-
formance in context, we also report the accuracy
when full body of a question is used, as well as ti-
tles alone. the latter constitutes an upper bound on

5askubuntu.com
6https://github.com/taolei87/askubuntu

map (dev) map (test) %words

full title
full body

independent

dependent

56.5
54.2
55.7
56.3
56.1
56.5

60.0
53.0
53.6
52.6
54.6
55.6

10.1
89.9
9.7
19.7
11.6
32.8

table 4: comparison between rationale models (middle and
bottom rows) and the baselines using full title or body (top row).

figure 6: retrieval map on the test set when various percent-
ages of the texts are chosen as rationales. data points corre-
spond to models trained with different hyper-parameters.

the model performance as in this dataset titles pro-
vide short, informative summaries of the question
content. we evaluate the rationales using the mean
average precision (map) of retrieval.

results table 4 presents the results of our ratio-
nale model. we explore a range of hyper-parameter
values7. we include two runs for each version. the
   rst one achieves the highest map on the develop-
ment set, the second run is selected to compare the
models when they use roughly 10% of question text
(7 words on average). we also show the results of
different runs in figure 6. the rationales achieve the
map up to 56.5%, getting close to using the titles.
the models also outperform the baseline of using
the noisy question bodies, indicating the the models   
capacity of extracting short but important fragments.
figure 7 shows the rationales for several questions
in the askubuntu domain, using the recurrent ver-
sion with around 10% extraction. interestingly, the
model does not always select words from the ques-
tion title. the reasons are that the question body
can contain the same or even complementary infor-
mation useful for retrieval. indeed, some rationale
fragments shown in the    gure are error messages,
7  1     {.008, .01, .012, .015},   2 = {0,   1, 2  1}, dropout

    {0.1, 0.2}

gen (independent)gen (recurrent)0.05247.080.06350.540.05852.360.06749.480.05946.020.0751.960.06249.760.07851.540.06447.940.08652.550.06848.930.09553.590.0749.50.10853.150.08152.180.11251.480.08151.840.11654.620.09451.240.12152.120.09452.210.137530.09753.610.16353.20.09854.110.17954.130.12249.030.19352.110.13354.190.26252.320.13550.210.27750.870.13648.220.32853.210.14550.960.32855.610.15552.910.347510.17352.740.37854.930.19752.645.047.850.553.356.05%9%13%16%20%gen (independent)gen (recurrent) 1figure 7: examples of extracted rationales of questions in the askubuntu domain.

which are typically not in the titles but very useful
to identify similar questions.

6 discussion

we proposed a novel modular neural framework
to automatically generate concise yet suf   cient text
fragments to justify predictions made by neural net-
works. we demonstrated that our encoder-generator
framework, trained in an end-to-end manner, gives
rise to quality rationales in the absence of any ex-
plicit rationale annotations. the approach could be
modi   ed or extended in various ways to other appli-
cations or types of data.
choices of enc(  ) and gen(  ). the encoder and
generator can be realized in numerous ways with-
out changing the broader algorithm. for instance,
we could use a convolutional network (kim, 2014;
kalchbrenner et al., 2014), deep averaging net-
work (iyyer et al., 2015; joulin et al., 2016) or a
boosting classi   er as the encoder. when rationales
can be expected to conform to repeated stereotypi-
cal patterns in the text, a simpler encoder consistent
with this bias can work better. we emphasize that,
in this paper, rationales are    exible explanations that
may vary substantially from instance to another. on
the generator side, many additional constraints could
be imposed to further guide acceptable rationales.
dealing with search space. our training method
employs a reinforce-style algorithm (williams,
1992) where the gradient with respect to the param-
eters is estimated by sampling possible rationales.

additional constraints on the generator output can
be helpful in alleviating problems of exploring po-
tentially a large space of possible rationales in terms
of their interaction with the encoder. we could also
apply variance reduction techniques to increase sta-
bility of stochastic training (cf.
(weaver and tao,
2001; mnih et al., 2014; ba et al., 2015; xu et al.,
2015)).

7 acknowledgments

we thank prof. julian mcauley for sharing the re-
view dataset and annotations. we also thank mit
nlp group and the reviewers for their helpful com-
ments. the work is supported by the arabic lan-
guage technologies (alt) group at qatar com-
puting research institute (qcri) within the iyas
project. any opinions,    ndings, conclusions, or rec-
ommendations expressed in this paper are those of
the authors, and do not necessarily re   ect the views
of the funding organizations.

references
jimmy ba, volodymyr mnih, and koray kavukcuoglu.
2015. multiple object recognition with visual atten-
tion. in proceedings of the international conference
on learning representations (iclr).

dzmitry bahdanau, kyunghyun cho, and yoshua ben-
gio.
2015. id4 by jointly
learning to align and translate. in international con-
ference on learning representations.

kan chen, jiang wang, liang-chieh chen, haoyuan
2015. abc-

gao, wei xu, and ram nevatia.

i	accidentally	removed	the	ubuntu	sobware	centre	,	when	i	was	actually	trying	to	remove	my	ubuntu	one	applica9ons	.	although	i	do	n't	remember	directly	uninstalling	the	centre	,	i	think	dele9ng		one	of	those	packages	might	have	triggered	it	.	i	can	not	look	at	history	of	applica9on	changes	,	as	the	sobware	centre	is	missing	.	please	advise	on	how	to	install	,	or	rather	reinstall	,	ubuntu	sobware	centre	on	my	computer	.	how	do	i	install	ubuntu	sobware	centre	applica9on	?i	know	this	will	be	an	odd	ques9on	,	but	i	was	wondering	if	anyone	knew	how	to	install	the	ubuntu	installer	package	in	an	ubuntu	installa9on	.	to	clarify	,	when	you	boot	up	to	an	ubuntu	livecd	,	it	's	got	the	installer	program	available	so	that	you	can	install	ubuntu	to	a	drive	.	naturally	,	this	program	is	not	present	in	the	installed	ubuntu	.	is	there	,	though	,	a	way	to	download	and	install	it	like	other	packages	?	invariably	,	someone	will	ask	what	i	'm	trying	to	do	,	and	the	answer	   	install	installer	package	on	an	installed	system	?what	is	the	easiest	way	to	install	all	the	media	codec	available	for	ubuntu	?	i	am	having	issues	with	mul9ple	applica9ons	promp9ng	me	to	install	codecs	before	they	can	play	my	   les	.	how	do	i		install	media	codecs	?what	should	i	do	when	i	see	<unk>	report	this	<unk>	?	an	unresolvable	problem	occurred	while	ini9alizing	the	package	informa9on	.	please	report	this	bug	against	the	'update-manager	'	package	and	include	the	following	error	message	:	e	:	encountered	a	sec9on	with	no	package	:	header	e	:	problem	with	mergelist	<unk>	e	:	the	package	lists	or	status	   le	could	not	be	parsed	or	opened	.please	any	one	give	the	solu9on	for	this	whenever	i	try	to	convert	the	rpm	   le	to	deb	   le	i	always	get	this	problem	error	:	<unk>	:	not	an	rpm	package	(	or	package	manifest	)	error	execu9ng	``		lang=c	rpm	-qp	--	queryformat	%	{	name	}	<unk>	'	''	:	at	<unk>	line	489	thanks	conver9ng	rpm	   le	to	debian	   ehow	do	i	mount	a	hibernated	par99on	with	windows	8	in	ubuntu	?	i	ca	n't	mount	my	other	par99on	with	windows	8	,	i	have	ubuntu	12.10	amd64	:	error	moun9ng	/dev/sda1	at	<unk>	:	command-line	`mount	-t	``	n[s	''	-o	``	uhelper=udisks2	,	nodev	,	nosuid	,	uid=1000	,	gid=1000	,	dmask=0077	,	fmask=0177	''	``	/dev/sda1	''	``	<unk>	''	'	exited	with	non-zero	exit	status	14	:	windows	is													hibernated	,	refused	to	mount	.	failed	to	mount	'/dev/sda1	'	:	opera9on	not	permiaed	the	n[s	par99on	is	hibernated	.	please	resume	and	shutdown	windows	properly	,	or	mount	the	volume	read-only	with	the	'ro	'	mount	op9onid98: an attention based convolutional neural net-
work for visual id53. arxiv preprint
arxiv:1511.05960.

jianpeng cheng, li dong, and mirella lapata. 2016.
long short-term memory-networks for machine read-
ing. arxiv preprint arxiv:1601.06733.

mark w craven and jude w shavlik. 1996. extract-
ing tree-structured representations of trained networks.
in advances in neural information processing systems
(nips).

cicero dos santos, luciano barbosa, dasha bogdanova,
and bianca zadrozny. 2015. learning hybrid rep-
resentations to retrieve semantically equivalent ques-
tions. in proceedings of the 53rd annual meeting of
the association for computational linguistics and the
7th international joint conference on natural lan-
guage processing (volume 2: short papers), pages
694   699, beijing, china, july. association for com-
putational linguistics.

manaal faruqui, jesse dodge, sujay k. jauhar, chris
2015a.
dyer, eduard hovy, and noah a. smith.
retro   tting word vectors to semantic lexicons. in pro-
ceedings of naacl.

manaal faruqui, yulia tsvetkov, dani yogatama, chris
dyer, and noah a. smith. 2015b. sparse overcom-
plete word vector representations. in proceedings of
acl.

aur  elie herbelot and eva maria vecchi. 2015. build-
ing a shared world: mapping distributional to model-
theoretic semantic spaces. in proceedings of the 2015
conference on empirical methods in natural lan-
guage processing. association for computational lin-
guistics.

karl moritz hermann, tomas kocisky, edward grefen-
stette, lasse espeholt, will kay, mustafa suleyman,
and phil blunsom. 2015. teaching machines to read
and comprehend. in advances in neural information
processing systems, pages 1684   1692.

michiel hermans and benjamin schrauwen.

2013.
training and analysing deep recurrent neural net-
works. in advances in neural information processing
systems, pages 190   198.

mohit iyyer, varun manjunatha, jordan boyd-graber,
and hal daum  e iii. 2015. deep unordered compo-
sition rivals syntactic methods for text classi   cation.
in proceedings of the 53rd annual meeting of the as-
sociation for computational linguistics and the 7th
international joint conference on natural language
processing (volume 1: long papers).

armand joulin, edouard grave, piotr bojanowski, and
tomas mikolov. 2016. bag of tricks for ef   cient text
classi   cation. arxiv preprint arxiv:1607.01759.

nal kalchbrenner, edward grefenstette, and phil blun-
som. 2014. a convolutional neural network for mod-
in proceedings of the 52th annual
elling sentences.
meeting of the association for computational linguis-
tics.

andrej karpathy, justin johnson, and fei-fei li. 2015.
visualizing and understanding recurrent networks.
arxiv preprint arxiv:1506.02078.

b kim, ja shah, and f doshi-velez. 2015. mind the
gap: a generative approach to interpretable feature se-
in advances in neural infor-
lection and extraction.
mation processing systems.

yoon kim. 2014. convolutional neural networks for sen-
tence classi   cation. in proceedings of the empiricial
methods in natural language processing (emnlp
2014).

tao lei, regina barzilay, and tommi jaakkola. 2015.
molding id98s for text: non-linear, non-consecutive
convolutions. in proceedings of the 2015 conference
on empirical methods in natural language process-
ing (emnlp).

tao lei, hrishikesh joshi, regina barzilay, tommi
jaakkola, katerina tymoshenko, alessandro mos-
chitti, and llu    s m`arquez. 2016. semi-supervised
in pro-
question retrieval with gated convolutions.
ceedings of the 2016 conference of the north ameri-
can chapter of the association for computational lin-
guistics: human language technologies (naacl).

benjamin letham, cynthia rudin, tyler h. mccormick,
and david madigan. 2015.
interpretable classi   ers
using rules and bayesian analysis: building a better
stroke prediction model. annals of applied statistics,
9(3):1350   1371.

jiwei li, xinlei chen, eduard hovy, and dan jurafsky.
2016. visualizing and understanding neural models in
nlp. in proceedings of naacl.

iain j marshall, jo  el kuiper, and byron c wallace. 2015.
robotreviewer: evaluation of a system for automati-
cally assessing bias in clinical trials. journal of the
american medical informatics association.

andr  e f. t. martins and ram  on fernandez astudillo.
2016. from softmax to sparsemax: a sparse model
of attention and multi-label classi   cation. corr,
abs/1602.02068.

julian mcauley, jure leskovec, and dan jurafsky. 2012.
learning attitudes and attributes from multi-aspect re-
views. in data mining (icdm), 2012 ieee 12th in-
ternational conference on, pages 1020   1025. ieee.

volodymyr mnih, nicolas heess, alex graves, et al.
2014. recurrent models of visual attention.
in
advances in neural information processing systems
(nips).

marco tulio ribeiro, sameer singh, and carlos guestrin.
2016.     why should i trust you?   : explaining the pre-
dictions of any classi   er. in acm sigkdd interna-
tional conference on knowledge discovery and data
mining (kdd).

tim rockt  aschel, edward grefenstette, karl moritz her-
mann, tom  a  s ko  cisk`y, and phil blunsom. 2016. rea-
soning about entailment with neural attention. in in-
ternational conference on learning representations.
alexander m rush, sumit chopra, and jason weston.
2015. a neural attention model for abstractive sen-
tence summarization. in proceedings of the 2015 con-
ference on empirical methods in natural language
processing.

sebastian thrun. 1995. extracting rules from arti   -
cial neural networks with distributed representations.
in advances in neural information processing systems
(nips).

lex weaver and nigel tao. 2001. the optimal reward
baseline for gradient-based id23. in
proceedings of the seventeenth conference on uncer-
tainty in arti   cial intelligence.

ronald j williams. 1992. simple statistical gradient-
following algorithms for connectionist reinforcement
learning. machine learning.

huijuan xu and kate saenko.

2015. ask, attend
and answer: exploring question-guided spatial atten-
arxiv preprint
tion for visual id53.
arxiv:1511.05234.

kelvin xu, jimmy ba, ryan kiros, kyunghyun cho,
aaron courville, ruslan salakhudinov, rich zemel,
and yoshua bengio. 2015. show, attend and tell: neu-
ral image id134 with visual attention. in
proceedings of the 32nd international conference on
machine learning (icml).

zichao yang, xiaodong he, jianfeng gao, li deng,
and alex smola.
stacked attention net-
works for image id53. arxiv preprint
arxiv:1511.02274.

2015.

omar zaidan, jason eisner, and christine d. piatko.
2007. using    annotator rationales    to improve ma-
chine learning for text categorization. in proceedings
of human language technology conference of the
north american chapter of the association of com-
putational linguistics, pages 260   267.

ye zhang, iain james marshall, and byron c. wallace.
2016. rationale-augmented convolutional neural net-
works for text classi   cation. corr, abs/1605.04469.

