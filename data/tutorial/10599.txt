deep id23 for dialogue generation

jiwei li1, will monroe1, alan ritter2, michel galley3, jianfeng gao3 and dan jurafsky1

1stanford university, stanford, ca, usa

2ohio state university, oh, usa

{jiweil,wmonroe4,jurafsky}@stanford.edu, ritter.1492@osu.edu

3microsoft research, redmond, wa, usa
{mgalley,jfgao}@microsoft.com

6
1
0
2

 

p
e
s
9
2

 

 
 
]
l
c
.
s
c
[
 
 

4
v
1
4
5
1
0

.

6
0
6
1
:
v
i
x
r
a

abstract

recent neural models of dialogue generation
offer great promise for generating responses
for conversational agents, but tend to be short-
sighted, predicting utterances one at a time
while ignoring their in   uence on future out-
comes. modeling the future direction of a di-
alogue is crucial to generating coherent, inter-
esting dialogues, a need which led traditional
nlp models of dialogue to draw on reinforce-
ment learning. in this paper, we show how to
integrate these goals, applying deep reinforce-
ment learning to model future reward in chat-
bot dialogue. the model simulates dialogues
between two virtual agents, using policy gradi-
ent methods to reward sequences that display
three useful conversational properties: infor-
mativity, coherence, and ease of answering (re-
lated to forward-looking function). we evalu-
ate our model on diversity, length as well as
with human judges, showing that the proposed
algorithm generates more interactive responses
and manages to foster a more sustained conver-
sation in dialogue simulation. this work marks
a    rst step towards learning a neural conversa-
tional model based on the long-term success of
dialogues.

introduction

1
neural response generation (sordoni et al., 2015;
shang et al., 2015; vinyals and le, 2015; li et al.,
2016a; wen et al., 2015; yao et al., 2015; luan
et al., 2016; xu et al., 2016; wen et al., 2016; li
et al., 2016b; su et al., 2016) is of growing inter-
est. the lstm sequence-to-sequence (id195)
model (sutskever et al., 2014) is one type of neural
generation model that maximizes the id203 of
generating a response given the previous dialogue
turn. this approach enables the incorporation of rich

context when mapping between consecutive dialogue
turns (sordoni et al., 2015) in a way not possible, for
example, with mt-based dialogue models (ritter et
al., 2011).

despite the success of id195 models in di-
alogue generation,
two problems emerge: first,
id195 models are trained by predicting the next
dialogue turn in a given conversational context using
the maximum-likelihood estimation (id113) objective
function. however, it is not clear how well id113
approximates the real-world goal of chatbot develop-
ment: teaching a machine to converse with humans,
while providing interesting, diverse, and informative
feedback that keeps users engaged. one concrete
example is that id195 models tend to generate
highly generic responses such as    i don   t know    re-
gardless of the input (sordoni et al., 2015; serban
et al., 2016; li et al., 2016a). this can be ascribed
to the high frequency of generic responses found in
the training set and their compatibility with a diverse
range of conversational contexts. yet    i don   t know   
is apparently not a good action to take, since it closes
the conversation down.

another common problem, illustrated in the two
sample conversations on the left of table 1, is that
the system becomes stuck in an in   nite loop of repet-
itive responses. this is due to id113-based id195
models    inability to account for repetition. in exam-
ple 2 (bottom left), the dialogue falls into an in   nite
loop after three turns, with both agents generating
dull, generic utterances like i don   t know what you
are talking about and you don   t know what you are
saying. looking at the entire conversation, utterance
(4) turns out to be a bad action to take because it
offers no way of continuing the conversation.1

1a similar rule is often suggested in improvisational comedy:

https://en.wikipedia.org/wiki/yes,_and...

baseline mutual information model (li et al. 2015)
a: where are you going? (1)
b: i   m going to the restroom. (2)
a: see you later. (3)
b: see you later. (4)
a: see you later. (5)
b: see you later. (6)
...
...
a: how old are you? (1)
b: i   m 16. (2)
a: 16? (3)
b: i don   t know what you are talking about. (4)
a: you don   t know what you are saying. (5)
b: i don   t know what you are talking about . (6)
a: you don   t know what you are saying. (7)
...

proposed id23 model
a: where are you going? (1)
b: i   m going to the police station. (2)
a: i   ll come with you. (3)
b: no, no, no, no, you   re not going anywhere. (4)
a: why? (5)
b: i need you to stay here. (6)
a: i don   t know what you are talking about. (7)
...
a: how old are you? (1)
b: i   m 16. why are you asking? (2)
a i thought you were 12. (3)
b: what made you think so? (4)
a: i don   t know what you are talking about. (5)
b: you don   t know what you are saying. (6)
...
...

table 1: left column: dialogue simulation between two agents using a 4-layer lstm encoder-decoder
trained on the opensubtitles dataset. the    rst turn (index 1) is input by the authors. then the two agents
take turns conversing, taking as input the other agent   s prior generated turn. the output is generated using
the mutual information model (li et al., 2015) in which an n-best list is    rst obtained using id125
based on p(t|s) and reranked by linearly combining the backward id203 p(s|t), where t and s respectively
denote targets and sources. right column: dialogue simulated using the proposed id23
model. the new model has more forward-looking utterances (questions like    why are you asking?    and
offers like    i   ll come with you   ) and lasts longer before it falls into conversational black holes.

these challenges suggest we need a conversa-
tion framework that has the ability to (1) integrate
developer-de   ned rewards that better mimic the true
goal of chatbot development and (2) model the long-
term in   uence of a generated response in an ongoing
dialogue.

to achieve these goals, we draw on the insights of
id23, which have been widely ap-
plied in mdp and pomdp dialogue systems (see re-
lated work section for details). we introduce a neu-
ral id23 (rl) generation method,
which can optimize long-term rewards designed by
system developers. our model uses the encoder-
decoder architecture as its backbone, and simulates
conversation between two virtual agents to explore
the space of possible actions while learning to maxi-
mize expected reward. we de   ne simple heuristic ap-
proximations to rewards that characterize good con-
versations: good conversations are forward-looking
(allwood et al., 1992) or interactive (a turn suggests
a following turn), informative, and coherent. the pa-
rameters of an encoder-decoder id56 de   ne a policy
over an in   nite action space consisting of all possible

utterances. the agent learns a policy by optimizing
the long-term developer-de   ned reward from ongo-
ing dialogue simulations using policy gradient meth-
ods (williams, 1992), rather than the id113 objective
de   ned in standard id195 models.

our model thus integrates the power of id195
systems to learn compositional semantic meanings of
utterances with the strengths of reinforcement learn-
ing in optimizing for long-term goals across a conver-
sation. experimental results (sampled results at the
right panel of table 1) demonstrate that our approach
fosters a more sustained dialogue and manages to
produce more interactive responses than standard
id195 models trained using the id113 objective.

2 related work

efforts to build statistical id71 fall into two
major categories.

the    rst treats dialogue generation as a source-
to-target transduction problem and learns mapping
rules between input messages and responses from a
massive amount of training data. ritter et al. (2011)
frames the response generation problem as a statisti-

cal machine translation (smt) problem. sordoni et
al. (2015) improved ritter et al.   s system by rescor-
ing the outputs of a phrasal smt-based conversation
system with a neural model that incorporates prior
context. recent progress in id195 models inspire
several efforts (vinyals and le, 2015) to build end-
to-end conversational systems which    rst apply an
encoder to map a message to a distributed vector rep-
resenting its semantics and generate a response from
the message vector. serban et al. (2016) propose
a hierarchical neural model that captures dependen-
cies over an extended conversation history. li et al.
(2016a) propose mutual information between mes-
sage and response as an alternative objective function
in order to reduce the proportion of generic responses
produced by id195 systems.

the other line of statistical research focuses on
building task-oriented dialogue systems to solve
domain-speci   c tasks. efforts include statistical
models such as id100 (mdps)
(levin et al., 1997; levin et al., 2000; walker et al.,
2003; pieraccini et al., 2009), pomdp (young et
al., 2010; young et al., 2013; ga  sic et al., 2013a;
ga  sic et al., 2014) models, and models that statisti-
cally learn generation rules (oh and rudnicky, 2000;
ratnaparkhi, 2002; banchs and li, 2012; nio et al.,
2014). this dialogue literature thus widely applies
id23 (walker, 2000; schatzmann
et al., 2006; gasic et al., 2013b; singh et al., 1999;
singh et al., 2000; singh et al., 2002) to train dialogue
policies. but task-oriented rl dialogue systems of-
ten rely on carefully limited dialogue parameters, or
hand-built templates with state, action and reward sig-
nals designed by humans for each new domain, mak-
ing the paradigm dif   cult to extend to open-domain
scenarios.

also relevant is prior work on reinforcement learn-
ing for language understanding - including learning
from delayed reward signals by playing text-based
games (narasimhan et al., 2015; he et al., 2016),
executing instructions for windows help (branavan
et al., 2011), or understanding dialogues that give
navigation directions (vogel and jurafsky, 2010).

our goal is to integrate the id195 and rein-
forcement learning paradigms, drawing on the advan-
tages of both. we are thus particularly inspired by
recent work that attempts to merge these paradigms,
including wen et al. (2016)    training an end-to-end

task-oriented dialogue system that links input repre-
sentations to slot-value pairs in a database    or su
et al. (2016), who combine id23
with neural generation on tasks with real users, show-
ing that id23 improves dialogue
performance.

3 id23 for

open-domain dialogue

in this section, we describe in detail the components
of the proposed rl model.

the learning system consists of two agents. we
use p to denote sentences generated from the    rst
agent and q to denote sentences from the second.
the two agents take turns talking with each other.
a dialogue can be represented as an alternating se-
quence of sentences generated by the two agents:
p1, q1, p2, q2, ..., pi, qi. we view the generated sen-
tences as actions that are taken according to a policy
de   ned by an encoder-decoder recurrent neural net-
work language model.

the parameters of the network are optimized to
maximize the expected future reward using policy
search, as described in section 4.3. policy gradi-
ent methods are more appropriate for our scenario
than id24 (mnih et al., 2013), because we can
initialize the encoder-decoder id56 using id113 pa-
rameters that already produce plausible responses,
before changing the objective and tuning towards a
policy that maximizes long-term reward. id24,
on the other hand, directly estimates the future ex-
pected reward of each action, which can differ from
the id113 objective by orders of magnitude, thus mak-
ing id113 parameters inappropriate for initialization.
the components (states, actions, reward, etc.) of our
sequential decision problem are summarized in the
following sub-sections.

3.1 action
an action a is the dialogue utterance to generate.
the action space is in   nite since arbitrary-length se-
quences can be generated.

3.2 state
a state is denoted by the previous two dialogue turns
[pi, qi]. the dialogue history is further transformed
to a vector representation by feeding the concatena-
tion of pi and qi into an lstm encoder model as

described in li et al. (2016a).

3.3 policy
a policy takes the form of an lstm encoder-decoder
(i.e., prl(pi+1|pi, qi) ) and is de   ned by its param-
eters. note that we use a stochastic representation
of the policy (a id203 distribution over actions
given states). a deterministic policy would result in
a discontinuous objective that is dif   cult to optimize
using gradient-based methods.

3.4 reward
r denotes the reward obtained for each action. in this
subsection, we discuss major factors that contribute
to the success of a dialogue and describe how approx-
imations to these factors can be operationalized in
computable reward functions.

ease of answering a turn generated by a machine
should be easy to respond to. this aspect of a turn
is related to its forward-looking function: the con-
straints a turn places on the next turn (schegloff and
sacks, 1973; allwood et al., 1992). we propose to
measure the ease of answering a generated turn by
using the negative log likelihood of responding to
that utterance with a dull response. we manually con-
structed a list of dull responses s consisting 8 turns
such as    i don   t know what you are talking about   ,
   i have no idea   , etc., that we and others have found
occur very frequently in id195 models of con-
versations. the reward function is given as follows:

r1 =     1
ns

log pid195(s|a)

(1)

1
ns

where ns denotes the cardinality of ns and ns de-
notes the number of tokens in the dull response s.
although of course there are more ways to generate
dull responses than the list can cover, many of these
responses are likely to fall into similar regions in the
vector space computed by the model. a system less
likely to generate utterances in the list is thus also
less likely to generate other dull responses.

pid195

represents the likelihood output by
id195 models. it is worth noting that pid195
is different from the stochastic policy function
prl(pi+1|pi, qi), since the former is learned based
on the id113 objective of the id195 model while
the latter is the policy optimized for long-term future

(cid:88)

s   s

reward in the rl setting. r1 is further scaled by the
length of target s.
information flow we want each agent to con-
tribute new information at each turn to keep the di-
alogue moving and avoid repetitive sequences. we
therefore propose penalizing semantic similarity be-
tween consecutive turns from the same agent. let
hpi and hpi+1 denote representations obtained from
the encoder for two consecutive turns pi and pi+1.
the reward is given by the negative log of the cosine
similarity between them:

r2 =     log cos(hpi, hpi+1) =     log cos

hpi    hpi+1
(cid:107)hpi(cid:107)(cid:107)hpi+1(cid:107)
(2)
semantic coherence we also need to measure the
adequacy of responses to avoid situations in which
the generated replies are highly rewarded but are un-
grammatical or not coherent. we therefore consider
the mutual information between the action a and pre-
vious turns in the history to ensure the generated
responses are coherent and appropriate:

r3 =

1
na

1
nqi

log pbackward

log pid195(a|qi, pi)+

id195 (qi|a)
(3)
pid195(a|pi, qi) denotes the id203 of generat-
ing response a given the previous dialogue utterances
id195 (qi|a) denotes the backward proba-
[pi, qi]. pbackward
bility of generating the previous dialogue utterance
qi based on response a. pbackward
is trained in a simi-
id195
lar way as standard id195 models with sources
and targets swapped. again, to control the in   u-
ence of target length, both log pid195(a|qi, pi) and
id195 (qi|a) are scaled by the length of targets.
log pbackward
the    nal reward for action a is a weighted sum of

the rewards discussed above:

r(a, [pi, qi]) =   1r1 +   2r2 +   3r3

(4)

where   1 +   2 +   3 = 1. we set   1 = 0.25,   2 =
0.25 and   3 = 0.5. a reward is observed after the
agent reaches the end of each sentence.

4 simulation
the central idea behind our approach is to simulate
the process of two virtual agents taking turns talking
with each other, through which we can explore the

state-action space and learn a policy prl(pi+1|pi, qi)
that leads to the optimal expected reward. we adopt
an alphago-style strategy (silver et al., 2016) by
initializing the rl system using a general response
generation policy which is learned from a fully su-
pervised setting.

4.1 supervised learning
for the    rst stage of training, we build on prior work
of predicting a generated target sequence given dia-
logue history using the supervised id195 model
(vinyals and le, 2015). results from supervised
models will be later used for initialization.

we trained a id195 model with attention (bah-
danau et al., 2015) on the opensubtitles dataset,
which consists of roughly 80 million source-target
pairs. we treated each turn in the dataset as a target
and the concatenation of two previous sentences as
source inputs.

4.2 mutual information
samples from id195 models are often times dull
and generic, e.g.,    i don   t know    (li et al., 2016a)
we thus do not want to initialize the policy model
using the pre-trained id195 models because this
will lead to a lack of diversity in the rl models    ex-
periences. li et al. (2016a) showed that modeling
mutual information between sources and targets will
signi   cantly decrease the chance of generating dull
responses and improve general response quality. we
now show how we can obtain an encoder-decoder
model which generates maximum mutual informa-
tion responses.

as illustrated in li et al. (2016a), direct decoding
from eq 3 is infeasible since the second term requires
the target sentence to be completely generated. in-
spired by recent work on sequence level learning
(ranzato et al., 2015), we treat the problem of gen-
erating maximum mutual information response as a
id23 problem in which a reward
of mutual information value is observed when the
model arrives at the end of a sequence.

similar to ranzato et al. (2015), we use policy gra-
dient methods (sutton et al., 1999; williams, 1992)
for optimization. we initialize the policy model prl
using a pre-trained pid195(a|pi, qi) model. given
an input source [pi, qi], we generate a candidate list
a = {  a|  a     prl}. for each generated candi-

date   a, we will obtain the mutual information score
m(  a, [pi, qi]) from the pre-trained pid195(a|pi, qi)
id195(qi|a). this mutual information score
and pbackward
will be used as a reward and back-propagated to the
encoder-decoder model, tailoring it to generate se-
quences with higher rewards. we refer the readers to
zaremba and sutskever (2015) and williams (1992)
for details. the expected reward for a sequence is
given by:

j(  ) = e[m(  a, [pi, qi])]

(5)

the gradient is estimated using the likelihood ratio
trick:

   j(  ) = m(  a, [pi, qi])    log prl(  a|[pi, qi])

(6)

we update the parameters in the encoder-decoder
model using stochastic id119. a curricu-
lum learning strategy is adopted (bengio et al., 2009)
as in ranzato et al. (2015) such that, for every se-
quence of length t we use the id113 loss for the    rst
l tokens and the reinforcement algorithm for the
remaining t     l tokens. we gradually anneal the
value of l to zero. a baseline strategy is employed to
decrease the learning variance: an additional neural
model takes as inputs the generated target and the
initial source and outputs a baseline value, similar
to the strategy adopted by zaremba and sutskever
(2015). the    nal gradient is thus:
   j(  ) =     log prl(  a|[pi, qi])[m(  a, [pi, qi])     b]
(7)

4.3 dialogue simulation between two agents
we simulate conversations between the two virtual
agents and have them take turns talking with each
other. the simulation proceeds as follows: at the
initial step, a message from the training set is fed to
the    rst agent. the agent encodes the input message
to a vector representation and starts decoding to gen-
erate a response output. combining the immediate
output from the    rst agent with the dialogue history,
the second agent updates the state by encoding the
dialogue history into a representation and uses the
decoder id56 to generate responses, which are sub-
sequently fed back to the    rst agent, and the process
is repeated.

figure 1: dialogue simulation between the two agents.

i=t(cid:88)

i=1

optimization we initialize the policy model prl
with parameters from the mutual information model
described in the previous subsection. we then use
id189 to    nd parameters that lead
to a larger expected reward. the objective to maxi-
mize is the expected future reward:

jrl(  ) = eprl(a1:t )[

r(ai, [pi, qi])]

(8)

i=t(cid:88)

i=1

where r(ai, [pi, qi]) denotes the reward resulting
from action ai. we use the likelihood ratio trick
(williams, 1992; glynn, 1990; aleksandrov et al.,
1968) for gradient updates:

   jrl(  )    (cid:88)

i

    log p(ai|pi, qi)

r(ai, [pi, qi])

(9)
we refer readers to williams (1992) and glynn

(1990) for more details.
4.4 curriculum learning
a curriculum learning strategy is again employed
in which we begin by simulating the dialogue for 2
turns, and gradually increase the number of simulated
turns. we generate 5 turns at most, as the number
of candidates to examine grows exponentially in the
size of candidate list. five candidate responses are
generated at each step of the simulation.
5 experimental results
in this section, we describe experimental results
along with qualitative analysis. we evaluate dialogue

generation systems using both human judgments and
two automatic metrics: conversation length (number
of turns in the entire session) and diversity.

5.1 dataset
the dialogue simulation requires high-quality initial
inputs fed to the agent. for example, an initial input
of    why ?    is undesirable since it is unclear how
the dialogue could proceed. we take a subset of
10 million messages from the opensubtitles dataset
and extract 0.8 million sequences with the lowest
likelihood of generating the response    i don   t know
what you are taking about    to ensure initial inputs
are easy to respond to.

5.2 automatic evaluation
evaluating dialogue systems is dif   cult. metrics such
as id7 (papineni et al., 2002) and perplexity have
been widely used for dialogue quality evaluation (li
et al., 2016a; vinyals and le, 2015; sordoni et al.,
2015), but it is widely debated how well these auto-
matic metrics are correlated with true response qual-
ity (liu et al., 2016; galley et al., 2015). since the
goal of the proposed system is not to predict the
highest id203 response, but rather the long-term
success of the dialogue, we do not employ id7 or
perplexity for evaluation2.

2we found the rl model performs worse on id7 score. on
a random sample of 2,500 conversational pairs, single reference
id7 scores for rl models, mutual information models and
vanilla id195 models are respectively 1.28, 1.44 and 1.17.
id7 is highly correlated with perplexity in generation tasks.

............mhow old are you?i   m 16, why are you asking?            i   m 16inputmessage...16?i thought you were 12..........turn1p1,2p1,3turn2q11,1q11,2q21,1q21,2q31,1q31,2.........         tuid56p1n,1p1n,2p1,1p2n,1p2n,2p3n,1p3n,2encodedecodeencodedecodeencodedecodemodel

id195

mutual information

rl

# of simulated turns

2.68
3.40
4.48

table 2: the average number of simulated turns
from standard id195 models, mutual informa-
tion model and the proposed rl model.

length of the dialogue the    rst metric we pro-
pose is the length of the simulated dialogue. we say
a dialogue ends when one of the agents starts gener-
ating dull responses such as    i don   t know    3 or two
consecutive utterances from the same user are highly
overlapping4.

the test set consists of 1,000 input messages. to
reduce the risk of circular dialogues, we limit the
number of simulated turns to be less than 8. results
are shown in table 2. as can be seen, using mutual
information leads to more sustained conversations
between the two agents. the proposed rl model is
   rst trained based on the mutual information objec-
tive and thus bene   ts from it in addition to the rl
model. we observe that the rl model with dialogue
simulation achieves the best evaluation score.

diversity we report degree of diversity by calculat-
ing the number of distinct unigrams and bigrams in
generated responses. the value is scaled by the total
number of generated tokens to avoid favoring long
sentences as described in li et al. (2016a). the re-
sulting metric is thus a type-token ratio for unigrams
and bigrams.

for both the standard id195 model and the pro-
posed rl model, we use id125 with a beam
size 10 to generate a response to a given input mes-
sage. for the mutual information model, we    rst
generate n-best lists using pid195(t|s) and then
linearly re-rank them using pid195(s|t). results
are presented in table 4. we    nd that the proposed
rl model generates more diverse outputs when com-

since the rl model is trained based on future reward rather than
id113, it is not surprising that the rl based models achieve lower
id7 score.

3we use a simple rule matching method, with a list of 8
phrases that count as dull responses. although this can lead
to both false-positives and -negatives, it works pretty well in
practice.

4two utterances are considered to be repetitive if they share

more than 80 percent of their words.

pared against both the vanilla id195 model and
the mutual information model.

model

id195

mutual information

rl

unigram bigram
0.0062
0.011
0.017

0.015
0.031
0.041

table 4: diversity scores (type-token ratios) for the
standard id195 model, mutual information model
and the proposed rl model.

human evaluation we explore three settings for
human evaluation: the    rst setting is similar to what
was described in li et al. (2016a), where we employ
crowdsourced judges to evaluate a random sample of
500 items. we present both an input message and the
generated outputs to 3 judges and ask them to decide
which of the two outputs is better (denoted as single-
turn general quality). ties are permitted. identical
strings are assigned the same score. we measure
the improvement achieved by the rl model over the
mutual information model by the mean difference in
scores between the models.

for the second setting, judges are again presented
with input messages and system outputs, but are
asked to decide which of the two outputs is easier to
respond to (denoted as single-turn ease to answer).
again we evaluate a random sample of 500 items,
each being assigned to 3 judges.

for the third setting, judges are presented with sim-
ulated conversations between the two agents (denoted
as multi-turn general quality). each conversation
consists of 5 turns. we evaluate 200 simulated con-
versations, each being assigned to 3 judges, who are
asked to decide which of the simulated conversations
is of higher quality.

setting

rl-win rl-lose

single-turn general quality
single-turn ease to answer
multi-turn general quality

tie
0.24
0.25
0.16
table 5: rl gains over the mutual information sys-
tem based on pairwise human judgments.

0.36
0.23
0.12

0.40
0.52
0.72

results for human evaluation are shown in table 5.
the proposed rl system does not introduce a signi   -
cant boost in single-turn response quality (winning
40 percent of time and losing 36 percent of time).

mutual information model
i   m 16.
i have no idea.
really?
i don   t know what you are talking about.

input message
how old are you?
what   s your full name?
i don   t want to go home tonight.
do you have any feelings for me?
how much time do you have here? not long enough. sorry, sir.
shall we get started?
do you play football?
we   d have to talk to him.
how come you never say it?
table 3: sampled responses generated from the mutual information models and the proposed rl model.

rl
i   m 16, why are you asking?
what   s yours?
why?
i don   t want to hurt your feelings.
ten seconds.
yes. we   ve got a lot of work to do here.
yes. i love football!
about what ?
i don   t think it   s a good idea to say it.

of course!
no, i don   t
i mean, he   s a good guy
because i don   t want to hurt you.

this is in line with our expectations, as the rl model
is not optimized to predict the next utterance, but
rather to increase long-term reward. the rl sys-
tem produces responses that are signi   cantly easier
to answer than does the mutual information system,
as demonstrated by the single-turn ease to answer
setting (winning 52 percent of time and losing 23
percent of time), and also signi   cantly higher quality
multi-turn dialogues, as demonstrated by the multi-
turn general quality setting (winning 72 percent of
time).

qualitative analysis and discussion we show a
random sample of generated responses in table 3 and
simulated conversations in table 1 at the beginning
of the paper. from table 3, we can see that the
rl based agent indeed generates more interactive
responses than the other baselines. we also    nd that
the rl model has a tendency to end a sentence with
another question and hand the conversation over to
the user. from table 1, we observe that the rl model
manages to produce more interactive and sustained
conversations than the mutual information model.

during error analysis, we found that although we
penalize repetitive utterances in consecutive turns,
the dialogue sometimes enters a cycle with length
greater than one, as shown in table 6. this can be
ascribed to the limited amount of conversational his-
tory we consider. another issue observed is that the
model sometimes starts a less relevant topic during
the conversation. there is a tradeoff between rele-
vance and less repetitiveness, as manifested in the
reward function we de   ne in eq 4.

the fundamental problem, of course, is that the
manually de   ned reward function can   t possibly
cover the crucial aspects that de   ne an ideal conversa-
tion. while the heuristic rewards that we de   ned are
amenable to automatic calculation, and do capture

a: what   s your name ?
b: daniel.
a: how old are you ?
b. twelve. what   s your name ?
a. daniel.
b: how old are you ?
a: twelve. what   s your name ?
b: daniel.
a: how old are you ?
b ...

table 6: an simulated dialogue with a cycle longer
than one.

some aspects of what makes a good conversation,
ideally the system would instead receive real rewards
from humans. another problem with the current
model is that we can only afford to explore a very
small number of candidates and simulated turns since
the number of cases to consider grow exponentially.

6 conclusion

we introduce a id23 framework for
neural response generation by simulating dialogues
between two agents, integrating the strengths of neu-
ral id195 systems and id23
for dialogue. like earlier neural id195 models,
our framework captures the compositional models
of the meaning of a dialogue turn and generates se-
mantically appropriate responses. like reinforce-
ment learning dialogue systems, our framework is
able to generate utterances that optimize future re-
ward, successfully capturing global properties of a
good conversation. despite the fact that our model
uses very simple, operationable heuristics for captur-
ing these global properties, the framework generates
more diverse, interactive responses that foster a more
sustained conversation.

acknowledgement
we would like to thank chris brockett, bill dolan
and other members of the nlp group at microsoft re-
search for insightful comments and suggestions. we
also want to thank kelvin guu, percy liang, chris
manning, sida wang, ziang xie and other members
of the stanford nlp groups for useful discussions.
jiwei li is supported by the facebook fellowship, to
which we gratefully acknowledge. this work is par-
tially supported by the nsf via awards iis-1514268,
iis-1464128, and by the darpa communicating
with computers (cwc) program under aro prime
contract no. w911nf- 15-1-0462. any opinions,
   ndings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily re   ect the views of nsf, darpa,
or facebook.

references
v. m. aleksandrov, v. i. sysoyev, and v. v. shemeneva.
1968. stochastic optimization. engineering cybernet-
ics, 5:11   16.

jens allwood, joakim nivre, and elisabeth ahls  en. 1992.
on the semantics and pragmatics of linguistic feedback.
journal of semantics, 9:1   26.

dzmitry bahdanau, kyunghyun cho, and yoshua bengio.
2015. id4 by jointly learning to
align and translate. in proc. of iclr.

rafael e banchs and haizhou li. 2012. iris: a chat-
oriented dialogue system based on the vector space
model. in proceedings of the acl 2012 system demon-
strations, pages 37   42.

yoshua bengio, j  er  ome louradour, ronan collobert, and
jason weston. 2009. curriculum learning. in pro-
ceedings of the 26th annual international conference
on machine learning, pages 41   48. acm.

srk branavan, david silver, and regina barzilay. 2011.
learning to win by reading manuals in a monte-carlo
framework. in proceedings of the 49th annual meeting
of the association for computational linguistics: hu-
man language technologies-volume 1, pages 268   277.
michel galley, chris brockett, alessandro sordoni,
yangfeng ji, michael auli, chris quirk, margaret
mitchell, jianfeng gao, and bill dolan.
2015.
deltaid7: a discriminative metric for generation
tasks with intrinsically diverse targets. in proc. of acl-
ijcnlp, pages 445   450, beijing, china, july.

milica ga  sic, catherine breslin, matthew henderson,
dongho kim, martin szummer, blaise thomson, pir-
ros tsiakoulis, and steve young. 2013a. pomdp-based

dialogue manager adaptation to extended domains. in
proceedings of sigdial.

milica gasic, catherine breslin, mike henderson,
dongkyu kim, martin szummer, blaise thomson, pir-
ros tsiakoulis, and steve young. 2013b. on-line policy
optimisation of bayesian spoken dialogue systems via
human interaction. in proceedings of icassp 2013,
pages 8367   8371. ieee.

milica ga  sic, dongho kim, pirros tsiakoulis, catherine
breslin, matthew henderson, martin szummer, blaise
thomson, and steve young. 2014. incremental on-
line adaptation of pomdp-based dialogue managers to
extended domains. in proceedings on interspeech.

peter w glynn. 1990. likelihood ratio gradient estima-
tion for stochastic systems. communications of the
acm, 33(10):75   84.

ji he, jianshu chen, xiaodong he, jianfeng gao, lihong
li, li deng, and mari ostendorf. 2016. deep rein-
forcement learning with a natural language action space.
in proceedings of the 54th annual meeting of the asso-
ciation for computational linguistics (volume 1: long
papers), pages 1621   1630, berlin, germany, august.
esther levin, roberto pieraccini, and wieland eckert.
1997. learning dialogue strategies within the markov
in automatic speech
decision process framework.
recognition and understanding, 1997. proceedings.,
1997 ieee workshop on, pages 72   79. ieee.

esther levin, roberto pieraccini, and wieland eckert.
2000. a stochastic model of human-machine interac-
tion for learning dialog strategies. ieee transactions
on speech and audio processing, 8(1):11   23.

jiwei li, michel galley, chris brockett, jianfeng gao, and
bill dolan. 2016a. a diversity-promoting objective
function for neural conversation models. in proc. of
naacl-hlt.

jiwei li, michel galley, chris brockett, georgios sp-
ithourakis, jianfeng gao, and bill dolan. 2016b. a
persona-based neural conversation model. in proceed-
ings of the 54th annual meeting of the association for
computational linguistics (volume 1: long papers),
pages 994   1003, berlin, germany, august.

chia-wei liu, ryan lowe, iulian v serban, michael nose-
worthy, laurent charlin, and joelle pineau. 2016. how
not to evaluate your dialogue system: an empirical
study of unsupervised id74 for dialogue
response generation. arxiv preprint arxiv:1603.08023.
2016.
lstm based conversation models. arxiv preprint
arxiv:1603.09457.

yi luan, yangfeng ji, and mari ostendorf.

volodymyr mnih, koray kavukcuoglu, david silver, alex
graves, ioannis antonoglou, daan wierstra, and mar-
tin riedmiller. 2013. playing atari with deep rein-
forcement learning. nips deep learning workshop.

karthik narasimhan, tejas kulkarni, and regina barzilay.
2015. language understanding for text-based games
using deep id23. arxiv preprint
arxiv:1506.08941.

lasguido nio, sakriani sakti, graham neubig, tomoki
toda, mirna adriani, and satoshi nakamura. 2014.
developing non-goal dialog system based on examples
of drama television. in natural interaction with robots,
knowbots and smartphones, pages 355   361. springer.
alice h oh and alexander i rudnicky. 2000. stochastic
language generation for spoken dialogue systems. in
proceedings of the 2000 anlp/naacl workshop on
conversational systems-volume 3, pages 27   32.

kishore papineni, salim roukos, todd ward, and wei-
jing zhu. 2002. id7: a method for automatic eval-
uation of machine translation. in proceedings of the
40th annual meeting on association for computational
linguistics, pages 311   318.

roberto pieraccini, david suendermann, krishna
dayanidhi, and jackson liscombe. 2009. are we there
yet? research in commercial spoken id71.
in text, speech and dialogue, pages 3   13. springer.

marc   aurelio ranzato, sumit chopra, michael auli,
and wojciech zaremba. 2015. sequence level train-
ing with recurrent neural networks. arxiv preprint
arxiv:1511.06732.

adwait ratnaparkhi. 2002. trainable approaches to sur-
face id86 and their application
to conversational id71. computer speech &
language, 16(3):435   455.

alan ritter, colin cherry, and william b dolan. 2011.
data-driven response generation in social media. in
proceedings of emnlp 2011, pages 583   593.

jost schatzmann, karl weilhammer, matt stuttle, and
steve young. 2006. a survey of statistical user simula-
tion techniques for reinforcement-learning of dialogue
management strategies. the knowledge engineering
review, 21(02):97   126.

emanuel a. schegloff and harvey sacks. 1973. opening

up closings. semiotica, 8(4):289   327.

iulian v serban, alessandro sordoni, yoshua bengio,
aaron courville, and joelle pineau. 2016. building
end-to-end dialogue systems using generative hierar-
chical neural network models. in proceedings of aaai,
february.

lifeng shang, zhengdong lu, and hang li. 2015. neural
in

responding machine for short-text conversation.
proceedings of acl-ijcnlp, pages 1577   1586.

david silver, aja huang, chris j maddison, arthur guez,
laurent sifre, george van den driessche, julian schrit-
twieser, ioannis antonoglou, veda panneershelvam,
marc lanctot, et al. 2016. mastering the game of go
with deep neural networks and tree search. nature,
529(7587):484   489.

satinder p singh, michael j kearns, diane j litman, and
marilyn a walker. 1999. id23 for
spoken dialogue systems. in nips, pages 956   962.

satinder singh, michael kearns, diane j litman, mar-
ilyn a walker, et al. 2000. empirical evaluation of
a id23 spoken dialogue system. in
aaai/iaai, pages 645   651.

satinder singh, diane litman, michael kearns, and mari-
lyn walker. 2002. optimizing dialogue management
with id23: experiments with the nj-
fun system. journal of arti   cial intelligence research,
pages 105   133.

alessandro sordoni, michel galley, michael auli, chris
brockett, yangfeng ji, meg mitchell, jian-yun nie,
jianfeng gao, and bill dolan. 2015. a neural network
approach to context-sensitive generation of conversa-
tional responses. in proceedings of naacl-hlt.

pei-hao su, milica gasic, nikola mrksic, lina rojas-
barahona, stefan ultes, david vandyke, tsung-hsien
wen, and steve young. 2016. continuously learning
neural dialogue management. arxiv.

ilya sutskever, oriol vinyals, and quoc v le. 2014.
sequence to sequence learning with neural networks.
in advances in neural information processing systems,
pages 3104   3112.

richard s sutton, david a mcallester, satinder p singh,
yishay mansour, et al. 1999. id189
for id23 with function approximation.
in nips, volume 99, pages 1057   1063.

oriol vinyals and quoc le. 2015. a neural conversa-
tional model. in proceedings of icml deep learning
workshop.

adam vogel and dan jurafsky. 2010. learning to follow
navigational directions. in proceedings of acl 2010,
pages 806   814.

marilyn a walker, rashmi prasad, and amanda stent.
2003. a trainable generator for recommendations in
multimodal dialog. in proceeedings of interspeech
2003.

marilyn a. walker. 2000. an application of reinforce-
ment learning to dialogue strategy selection in a spoken
dialogue system for email. journal of arti   cial intelli-
gence research, pages 387   416.

tsung-hsien wen, milica gasic, nikola mrk  si  c, pei-hao
su, david vandyke, and steve young. 2015. semanti-
cally conditioned lstm-based natural language gener-
ation for spoken dialogue systems. in proceedings of
emnlp, pages 1711   1721, lisbon, portugal.

tsung-hsien wen, milica gasic, nikola mrksic, lina m
rojas-barahona, pei-hao su, stefan ultes, david
vandyke, and steve young. 2016. a network-based
end-to-end trainable task-oriented dialogue system.
arxiv preprint arxiv:1604.04562.

ronald j williams. 1992. simple statistical gradient-
following algorithms for connectionist reinforcement
learning. machine learning, 8(3-4):229   256.

zhen xu, bingquan liu, baoxun wang, chengjie sun, and
xiaolong wang. 2016. incorporating loose-structured
knowledge into lstm with recall gate for conversation
modeling. arxiv preprint arxiv:1605.05110.

kaisheng yao, geoffrey zweig, and baolin peng. 2015.
attention with intention for a neural network conversa-
tion model. in nips workshop on machine learning
for spoken language understanding and interaction.
steve young, milica ga  si  c, simon keizer, franc  ois
mairesse, jost schatzmann, blaise thomson, and kai
yu. 2010. the hidden information state model: a prac-
tical framework for pomdp-based spoken dialogue man-
agement. computer speech & language, 24(2):150   
174.

steve young, milica gasic, blaise thomson, and jason d
williams. 2013. pomdp-based statistical spoken di-
alog systems: a review. proceedings of the ieee,
101(5):1160   1179.

wojciech zaremba and ilya sutskever. 2015. reinforce-
ment learning id63s. arxiv preprint
arxiv:1505.00521.

