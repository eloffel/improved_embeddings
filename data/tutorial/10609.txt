attentive pooling networks

6
1
0
2

 

b
e
f
1
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
9
0
6
3
0

.

2
0
6
1
:
v
i
x
r
a

cicero dos santos
ming tan
bing xiang
bowen zhou
ibm watson, t.j. watson research center, ny, usa

abstract

in this work, we propose attentive pooling (ap),
a two-way attention mechanism for discrimina-
tive model training. in the context of pair-wise
ranking or classi   cation with neural networks,
ap enables the pooling layer to be aware of
the current input pair, in a way that information
from the two input items can directly in   uence
the computation of each other   s representations.
along with such representations of the paired in-
puts, ap jointly learns a similarity measure over
projected segments (e.g.
trigrams) of the pair,
and subsequently, derives the corresponding at-
tention vector for each input to guide the pooling.
our two-way attention mechanism is a general
framework independent of the underlying repre-
sentation learning, and it has been applied to both
convolutional neural networks (id98s) and re-
current neural networks (id56s) in our studies.
the empirical results, from three very different
benchmark tasks of id53/answer
selection, demonstrate that our proposed mod-
els outperform a variety of strong baselines and
achieve state-of-the-art performance in all the
benchmarks.

1. introduction
neural networks (nn) with attention mechanisms have re-
cently proven to be successful at different id161
(cv) and natural language processing (nlp) tasks such
as image captioning (xu et al., 2015), machine transla-
tion (bahdanau et al., 2015) and factoid question answer-
ing (hermann et al., 2015). however, most recent work on
neural id12 have focused on one-way attention
mechanisms based on recurrent neural networks designed

.

cicerons@us.ibm.com
mingtan@us.ibm.com
bingxia@us.ibm.com
zhou@us.ibm.com

for generation tasks.
another important family of machine learning tasks are
centered around pair-wise ranking or classi   cation, which
have a broad set of applications, including but not limited
to, id53, entailment, id141 and any
other pair-wise matching problems. the current state-of-
the-art models usually include nn-based representation for
the input pair, followed by a discriminative ranking or clas-
si   cation models. for example, a convolution (or a id56)
and a max-pooling is used to independently construct dis-
tributed vector representations of the input pair, followed
by a large-margin training (hu et al., 2014; weston et al.,
2014; shen et al., 2014; dos santos et al., 2015).
the key contribution of this work is that we propose at-
tentive pooling (ap), a two-way attention mechanism, that
signi   cantly improves such discriminative models    perfor-
mance on pair-wise ranking or classi   cation, by enabling a
joint learning of the representations of both inputs as well
as their similarity measurement.
speci   cally, ap enables the pooling layer to be aware of
the current input pair, in a way that information from the
two input items can directly in   uence the computation of
each other   s representations. the main idea in ap consists
of learning a similarity measure over projected segments
(e.g. trigrams) of the two items in the input pair, and us-
ing the similarity scores between the segments to compute
attention vectors in both directions. next, the attention vec-
tors are used to perform pooling.
there are a few key bene   ts of our model.

    thanks to the two-way attention, our model projects
the paired inputs, even though they may not be always
semantically comparable for some applications (e.g.,
questions and answers in id53), into a
common representation space that they can be com-
pared in a more plausible way.

    our model is effective in matching pairs of inputs with

signi   cant length variations.

attentive pooling networks

    the two-way attention mechanism is independent of
the underlying representation learning. for example,
ap can be applied to both id98s and id56s, which is
in contrast to the one-way attention used in the gener-
ation models mostly based on recurrent nets.

in this work, we perform an extensive number of experi-
ments on applying attentive pooling id98s (ap-id98) and
bilstms (ap-bilstm) for the answer selection task. in
this task, given a question q and an candidate answer pool
p = {a1, a2,       , ap} for this question, the goal is to
search for and select the candidate answer a     p that
correctly answers q. we perform experiments with three
publicly available benchmark datasets, which vary in data
scale, complexity and length ratios between question and
answers: insuranceqa, trec-qa and wikiqa. for the
three datasets, ap-id98 and ap-bilstm respectively out-
perform the id98 and the bilstm that do not use attention.
additionally, ap-id98 achieves state-of-the-art results for
the three datasets.
our experimental results also demonstrate that attentive
pooling makes the id98 more robust to large input texts.
this is an important    nding, since recent work have
demonstrated that, in the context of semantically equiva-
lent question retrieval, id98 based representations do not
scale well with the size of the input text (dos santos et al.,
2015). additionally, as ap-id98 does not rely only on the
   nal vector representation to capture interactions between
the input question and answer, it requires much less con-
volutional    lters than the regular id98. it means that ap-
id98-based representations are more compact, which can
help to speed up the training process.
although we demonstrate experimental results for nlp
tasks only, ap is a general method that can be also ap-
plied to different types of nns that perform matching of
two inputs. therefore, we believe that ap can be useful for
different applications, such as id161 and bioin-
formatics.
this paper is organized as follows. in section 2, we de-
scribe two nn architectures for answer selection that have
been recently proposed in the literature. in section 3, we
detail the attentive pooling approach. in section 4, we dis-
cuss some related work. sections 5 and 6 detail our ex-
perimental setup and results, respectively. in section 7 we
present our    nal remarks.

2. neural networks for answer selection
different neural network architectures have been recently
proposed to perform matching of semantically related text
segments (yu et al., 2014; hu et al., 2014; dos santos et al.,
2015; wang & nyberg, 2015; severyn & moschitti, 2015;
tan et al., 2015). in this section we brie   y review two nn

architectures that have previously been applied to the an-
swer selection task: qa-id98 (feng et al., 2015) and qa-
bilstm (tan et al., 2015). given a pair (q, a) consisting
of a question q and a candidate answer a, both networks
score the pair by    rst computing    xed-length independent
continuous vector representations rq and ra, and then com-
puting the cosine similarity between these two vectors.
in figure 1 we present a joint illustration of these two neu-
ral networks. the    rst layer in both qa-id98 and qa-
bilstm transforms each input word w into a    xed-size
real-valued id27 rw     rd. id27s
(wes) are encoded by column vectors in an embedding ma-
trix w 0     rd  |v |, where v is a    xed-sized vocabulary
and d is the dimention of the id27s. given the
input pair (q, a), where the question q contains m tokens
and the candidate answer a contains l tokens, the output
of the    rst layer consists of two sequences of word embed-
dings qemb = {rw1, ..., rwm} and aemb = {rw1, ..., rwl}.
next, qa-id98 and qa-bilstm use different approaches
to process these sequences. while qa-id98 process both
qemb and aemb using a convolution, qa-bilstm uses a
bidirectional long short-term memory id56 (hochreiter
& schmidhuber, 1997) to process these sequences.

figure 1. joint illustration of qa-id98 and qa-bilstm.

attentive pooling networks

2.1. convolution
given the sequence qemb = {rw1, ..., rwm}, let us de   ne
the matrix z q = [z1, ..., zm ] as a matrix where each col-
umn contains a vector zm     rdk that is the concatenation
of a sequence of k id27s centralized in the m-
th word of the question. the output of the convolution with
c    lters over the question q is computed as follows:

q = w 1z q + b1

(1)
where each column m in q     rc  m contains features ex-
tracted in a context window around the m-th word of q. the
matrix w 1 and the vector b1 are parameters to be learned.
the number of convolutional    lters c, and the size of the
word context window k are hyper-parameters to be chosen
by the user.
in a similar manner, and using the same nn parameters w 1
and b1, we compute a     rc  l, the output of the convolu-
tion over the candidate answer a.

a = w 1z a + b1

(2)

2.2. bidirectional lstm (bilstm)

our lstm implementation is similar to the one in (graves
et al., 2013) with minor modi   cation. given the sequence
qemb = {rw1, ..., rwm}, the hidden vector h(t) (with size
h) at the time step t is updated as follows:

it =   (wirwt + uih(t     1) + bi)
ft =   (wf rwt + uf h(t     1) + bf )
ot =   (worwt + uoh(t     1) + bo)
  ct = tanh(wmrwt + umh(t     1) + bm)
ct = it       ct + ft     ct   1
ht = ot     tanh(ct)

(3)
(4)
(5)
(6)
(7)
(8)

in the lstm architecture, there are three gates (input i,
forget f and output o), and a cell memory vector c.    is
the sigmoid function. the input gate can determine how
incoming vectors rwt alter the state of the memory cell.
the output gate can allow the memory cell to have an effect
on the outputs. finally, the forget gate allows the cell to
remember or forget its previous state. w     rh  d, u    
rh  h and b     rh  1 are the network parameters.
single direction lstms suffer a weakness of not utilizing
the contextual information from the future tokens. bidirec-
tional lstm utilizes both the previous and future context
by processing the sequence on two directions, and gener-
ate two independent sequences of lstm output vectors.
one processes the input sequence in the forward direction,
while the other processes the input in the reverse direction.

ht (cid:107)       
      

the output at each time step is the concatenation of the two
output vectors from both directions, ie. ht =
ht. we
de   ne c = 2   h for the notation consistency with the pre-
vious subsection. after computing the hidden state ht for
each time step t, we generate the matrices q     rc  m and
a     rc  l, where the j-th column in q (a) corresponds to
j-th hidden state hj that is computed by the bilstm when
processing q (a). the same network parameters are used to
process both questions and candidate answers.

2.3. scoring and training procedure

given the matrices q and a, we compute the vector repre-
sentations rq     rc and ra     rc by applying a column-wise
max-pooling over q and a, followed by a non-linearity.
formally, the j-th elements of the vectors rq and ra are
compute as follows:

(cid:18)
(cid:18)

(cid:19)
(cid:19)

[rq]j = tanh

max

1<m<m

[qj,m]

[ra]j = tanh

max
1<l<l

[aj,l]

(9)

(10)

the last layer in qa-id98 and qa-bilstm scores the in-
put pair (q,a) by computing the cosine similarity between
the two representations:

s(q, a) =

rq.ra
(cid:107)rq(cid:107)(cid:107)ra(cid:107)

(11)

both networks are trained by minimizing a pairwise rank-
ing id168 over the training set d. the input in each
round is two pairs (q, a+) and (q, a   ), where a+ is a ground
truth answer for q, and a    is an incorrect answer. as in
(weston et al., 2014; hu et al., 2014), we de   ne the train-
ing objective as a hinge loss:

l = max{0, m     s  (q, a+) + s  (q, a   )}

(12)
where m is constant margin, s  (q, a+) and s  (q, a   ) are
scores generated by the network with parameter set   . dur-
ing training, for each question we randomly sample 50 neg-
ative answers from the entire answer set, but only use the
one with the highest score to update the model.
we use stochastic id119 (sgd) to minimize the
id168 with respect to   . the id26 algo-
rithm is used to compute the gradients of the network.

3. attentive pooling networks for answer

selection

attentive pooling is an approach that enables the pool-
ing layer to be aware of the current input pair, in a way
that information from the question q can directly in   uence

attentive pooling networks

the computation of the answer representation ra, and vice
versa. the main idea consists of learning a similarity mea-
sure over the projected segments in the input pairs, and uses
the similarity scores between the segments to compute at-
tention vectors. when ap is applied to id98, which we
call ap-id98, the network learns the similarity measure
over the convolved input sequences. when ap is applied
to bilstm, which we call ap-bilstm, the network learns
the similarity measure over the hidden states produced by
the bilstm when processing the two input sequences. we
use a similarity measure that has a bilinear form but fol-
lowed by a non-linearity.
in fig. 2, we illustrate the application of ap over the out-
put of the convolution or the bilstm to construct the rep-
resentations rq and ra. consider the input pair (q, a) where
the question has size m and the answer has size l1. af-
ter we compute the matrices q     rc  m and a     rc  l,
either by convolution or bilstm, we compute the matrix
g     rm  l as follows:

g = tanh(cid:0)qt u a(cid:1)

(13)
where u     rc  c is a matrix of parameters to be learned
by the nn. when the convolution is used to compute q
and a, the matrix g contains the scores of a soft alignment
between the convolved k-size context windows of q and
a. when the bilstm is used to compute q and a, the
matrix g contains the scores of a soft alignment between
the hidden vectors of each token in q and a.
next, we apply column-wise and row-wise max-poolings
over g to generate the vectors gq     rm and ga     rl,
respectively. formally, the j-th elements of the vectors gq
and ga are computed as follows:

[gq]j = max
1<m<m

[gj,m]

[ga]j = max
1<l<l

[gl,j]

(14)

(15)

we can interpret each element j of the vector ga as an im-
portance score for the context around the j-th word in the
candidate answer a with regard to the question q. like-
wise, each element j of the vector gq can be interpreted as
the importance score for the context around the j-th word
in the question q with regard to the candidate answer a.
next, we apply the softmax function to the vectors gq and
ga to create attention vectors   q and   a. for instance, the
j-th element of the vector   q is computed as follows:

[  q]j =

e[gq]j(cid:88)

e[gq]l

(16)

1<l<m

1in fig. 2, q has a size of    ve and a has a size of seven.

finally, the representations rq and ra are computed as the
dot product between the attention vectors   q and   a and
the output of the convolution (or bilstm) over q and a,
respectively:

rq = q  q

ra = a  a

(17)

(18)

like in qa-id98 and qa-bilstm, the    nal score is also
computed using the cosine similarity between rq and ra.
we use sgd to train ap-id98 and ap-bilstm by mini-
mizing the same pairwise id168 used in qa-id98
and qa-bilstm.

4. related work
traditional work on answer selection have normally used
feature engineering, linguistic tools, or external resources
(yih et al., 2013; wang & manning, 2010; wang et al.,
2007). recently, deep learning (dl) approaches have
been exploited for this task and achieved signi   cant out-
performance compared to traditional non-dl methods. for
example, in (yu et al., 2014; feng et al., 2015; severyn &
moschitti, 2015), the authors generate the representations
of questions and answers separately, and score a qa pair
using a similarity metric on top of these representations.
in wang & nyberg (2015),    rst a joint feature vectors
is learned from a joint long short-term memory (lstm)
model connecting questions and answers, and then the task
is converted into a learning-to-rank problem.
at the same time, attention-based systems have shown very
promising results on a variety of nlp tasks, such as ma-
chine translation (bahdanau et al., 2015; sutskever et al.,
2014), id134 (xu et al., 2015) and factoid
id53 (hermann et al., 2015). such models
learn to focus their attention to speci   c parts of their input.
some recently-proposed approaches introduce attention
mechanisms in the answer selection task. tan et al. (2015)
developed an attentive reader based on bidirectional long
short-term memory, which emphasizes certain part of the
answer according to the question embedding. unlike (tan
et al., 2015), in which attention is imposed only on answer
embedding generation, ap-id98 and ap-bilstm consider
the interdependence between questions and answers.
in the context of two-way attention, two very recent work
are related to ours. rockt  aschel et al. (2015), propose a
two-way attention method that is inspired by bidirectional
lstms that read a sequence and its reverse for improved
encoding. their approach, which is designed for id56s
only, differs in many aspects from the approach described
in this work, which can be easily applied for id98s and
id56s. yin et al. (2015) present a two-way attention mech-
anism that is tailored to id98s. some of the main differ-

attentive pooling networks

figure 2. attentive pooling networks for answer selection.

ences between their approach and this work are: (1) they
use a simple euclidean distance to compute the interde-
pendence between the two input texts, while in this work
we apply similarity metric learning, which has the poten-
tial to learn better ways to measure the interaction between
segments of the input items; (2) the models in (yin et al.,
2015) compute the attention vector using sum-pooling over
the alignment matrix and use the convolutional outputs up-
dated by the attention as the input for another level of con-
volutional layer. in this work we use max-pooling over the
alignment matrix plus softmax, in order to explicitly cre-
ate an attention vector that is used to perform the pooling.
experimental results show that such difference yields sub-
stantial improvement of performance on wikiqa dataset.

5. experimental setup
5.1. datasets

we apply ap-id98, ap-bilstm, qa-id98 and qa-
bilstm to three different answer selection datasets: insur-

anceqa, trec-qa and wikiqa. these datasets contain
text of different domains and have different caracteristics.
table 1 presents some statistics about the datasets, includ-
ing the number of questions in each set, average length of
questions (m) and answers (l), average number of candi-
date answers in the dev/test sets and the average ratio be-
tween the lengths of questions and their ground-truth an-
swers.
insuranceqa2 is a recently released large-scale non-factoid
qa dataset from the insurance domain. this dataset pro-
vides a training set, a validation set, and two test sets.
we do not see obvious categorical differentiation between
questions of the two test sets. for each question in dev/test
sets, there is a set of 500 candidate answers, which include
the ground-truth answers and randomly selected negative
answers. more details can be found in (feng et al., 2015).
trec-qa3 was created by wang et al. (2007) based on

2git clone https://github.com/shuzi/insuranceqa.git
3the
al.,

from (yao

obtained

data

et

is

2013)

attentive pooling networks

table 1. answer selection datasets.

dataset

train

dev

test avg. m avg. l avg. # cand. ans. avg. l/m

insuranceqa
trec-qa
wikiqa

12887
1162
873

1000
65
126

1800x2
68
243

7
8
6

95
28
25

500
38
9

13.8
4.2
5.0

text retrieval conference (trec) qa track (8-13) data.
we follow the exact approach of train/dev/test questions
selection in (wang & nyberg, 2015), in which all ques-
tions with only positive or negative answers are removed.
finally, we have 1162 training questions, 65 development
questions and 68 test questions.
wikiqa4 is an open domain question-answering dataset.
we use the subtask that assumes that there is at least one
correct answer for a question. the corresponding dataset
consists of 20,360 question/candidate pairs in train, 1,130
pairs in dev and 2,352 pairs in test. we adopt the stan-
dard setup of only considering questions that have correct
answers for evaluation.

5.2. id27s

in order to fairly compare our results with the ones in pre-
vious work, we use two different sets of pre-trained word
embeddings. for the insuranceqa dataset, we use the 100-
dimensional vectors that were trained by feng et al. (2015)
using id97 (mikolov et al., 2013). following wang &
nyberg (2015), tan et al. (2015) and yin et al.(2015), for
the trec-qa and the wikiqa datasets we use the 300-
dimensional vectors that were trained using id97 and
are publicly available on the website of this tool5.

5.3. neural networks setup

in table 2, we show the selected hyperparameter values,
which were tuned using the validation sets. we try to use
as much as possible the same hyperparameters for all the
three datasets. the size of the id27s is differ-
ent due to the different pre-trained versions that we used for
insuranceqa and the other two datasets. we use a context
window of size 3 for insuranceqa, while we set this pa-
rameter to 4 for trec-qa and wikiqa. using the selected
hyperparameters, the best results are normally achieved us-
ing between 15 and 25 training epochs. for ap-id98, ap-
bilstm and qa-lstm, we also use a learning rate sched-
ule that decreases the learning rate    according to the train-
ing epoch t. following dos santos & zadrozny (2014), we

http://cs.jhu.edu/  xuchen/packages/
jacana-qa-naacl2013-data-results.tar.bz2

4the data is obtained from (yang et al., 2015)
5https://code.google.com/p/id97/

set the learning rate for epoch t,   t, using the equation:
  t =

.

  
t

in our experiments, the four nn architectures qa-id98,
ap-id98, qa-bilstm and ap-bilstm are implemented
using theano (bergstra et al., 2010).

6. experimental results
6.1. insuranceqa

in table 3, we present the experimental results of the four
nns for the insuranceqa dataset. the results are in terms
of accuracy, which is equivalent to precision at top one.
on the bottom part of this table, we can see that ap-id98
outperforms qa-id98 by a large margin in both test sets,
as well as in the dev set. ap-bilstm also outperforms
the qa-bilstm in all the three sets. ap-id98 and ap-
bilstm have similar performance.
on the top part of table 3 we present the results of two
state-of-the-art systems for this dataset.
in (feng et al.,
2015), the authors present a id98 architecture that is sim-
ilar to qa-id98, but that uses a different similarity met-
ric instead of cosine similarity. in (tan et al., 2015), the
authors use a biltsm architecture that employs unidirec-
tional attention. both ap-id98 and ap-bilstm outper-
form the state-of-the-art systems.

table 3. accuracy of different systems for insuranceqa

system
(feng et al., 2015)
(tan et al., 2015)
qa-id98
qa-bilstm
ap-id98
ap-bilstm

dev test1
65.3
65.4
68.4
68.1
60.2
61.6
66.6
66.6
68.8
69.8
71.7
68.4

test2
61.0
62.2
56.1
63.7
66.3
66.4

one important characteristic of ap-id98 is that it requires
less convolutional    lters than qa-id98. for the insur-
anceqa dataset, ap-id98 uses 10x less    lters (400) than
qa-id98 (4000). using 800    lters in ap-id98 produces
very similar results as using 400. on the other hand, as
also found in (feng et al., 2015), qa-id98 requires at least
2000    lters to achieve more than 60% accuracy on insur-

attentive pooling networks

table 2. neural network hyper-parameters

hyp. hyperpar. name
word emb. size
d
conv. filters / hid.vec. size
c
context window size
k
mbs minibatch size
m
  

loss margin
init. learning rate

ap-id98 qa-id98 ap-bilstm qa-bilstm
100/300
100/300
400
141x2
1
3/4
20
20
0.1
0.5
1.1
1.1

100/300
4000
2
1
0.009
0.05

100/300
141x2
1
20
0.2
1.1

anceqa. ap-id98 needs less    lters because it does not
rely only on the    nal vector representation to capture in-
teractions between the input question and answer. as a re-
sult, although ap-id98 has a more complex architecture,
its training time is two times faster than qa-id98. using
a tesla k20xm, our theano implementation of ap-id98
takes about 16 minutes to complete one epoch (training +
id136 over validation set) for insuranceqa, which con-
sists on processing 1.5 million text segments.
in    gures 3 and 4, we plot the aggregated accuracy of
ap-id98 and qa-id98 for answers up to a certain length
for the test1 and test2 sets, respectively. we can see in
both plots that the performance of both system is better
for shorter answers. however, while the performance of
qa-id98 continues to drop as larger answers are consid-
ered, the performance of ap-id98 seems to be stable after
reaching a length of    90 tokens. these results give support
to our hypothesis that attentive pooling helps the id98 to
become robust to larger input texts.

figure 3. aggregated accuracy for answers up to a certain length
in the insuranceqa test1 set

figure 4. aggregated accuracy for answers up to a certain length
in the insuranceqa test2 set

scorer to compute map and mrr. we can see in table 4
that ap-id98 outperforms qa-id98 by a large margin in
both metrics. ap-bilstm outperforms the qa-bilstm,
but its performance is not as good as the of ap-id98.
on the top part of table 4 we present the results of three re-
cent work that use trec-qa as a benchmark. in (wang &
nyberg, 2015), the authors present an ltsm architecture
for answer selection. their best result consists of a combi-
nation of lstm and the bm25 algorithm. in (severyn &
moschitti, 2015), the authors propose an nn architecture
where the representations created by a convolutional layer
are the input to similarity measure learning. wang & it-
tycheriah (2015) propose a word-alignment-based method
that is suitable for the faq-based qa task. ap-id98 out-
performs the state-of-the-art systems in both metrics, map
and mrr.

6.3. wikiqa

6.2. trec-qa

in table 4, we present the experimental results of the four
nns for the trec-qa dataset. the results are in terms of
mean average precision (map) and mean reciprocal rank
(mrr), which are the metric normally used in previous
work with the same dataset. we use the of   cial trec eval

table 5 shows the experimental results of the four nns
for the wikiqa dataset. like in the other two datasets,
ap-id98 outperforms qa-id98, and ap-bilstm outper-
forms the qa-bilstm. the difference of performance be-
tween ap-id98 and qa-id98 is smaller than the one for
the insuranceqa dataset. we believe it is because the av-
erage size of the answers in wikiqa (25) is much smaller

attentive pooling networks

table 4. performance of different systems for trec-qa

system
wang & nyberg (2015)
severyn & moschitti (2015)
wang & ittycheriah (2015)
qa-bilstm
qa-id98
ap-bilstm
ap-id98

map
0.7134
0.7460
0.7460
0.6750
0.7147
0.7132
0.7530

mrr
0.7913
0.8080
0.8200
0.7723
0.8070
0.8032
0.8511

than in insuranceqa (95). it is expected that attentive pool-
ing bring more impact to the datasets that have larger an-
swer/question lengths.
in table 5 we also present the results of two recent work
that use wikiqa as a benchmark. yang et al.
(2015),
present a bigram id98 model with average pooling.
in
(yin et al., 2015), the authors propose an attention-based
id98. in order to make a fair comparison, in table 5 we
include yin et al.   s result that use id27s only6.
ap-id98 outperforms these two systems in both metrics.

table 5. performance of different systems for wikiqa

system
yang et al. (2015)
yin et al. (2015)
qa-bilstm
qa-id98
ap-bilstm
ap-id98

map
0.6520
0.6600
0.6557
0.6701
0.6705
0.6886

mrr
0.6652
0.6770
0.6695
0.6822
0.6842
0.6957

6.4. attentive pooling visualization

figures 5 and 6 depict two heat maps of two test questions
from insuranceqa that were correctly answered by ap-
id98 and whose answers are more than 100 words long.
the stronger the color of a word in the question (answer),
the larger the attention weight in   q (  a) of the trigram
centered at that word. as we can see in the pictures, the
attentive pooling mechanism is indeed putting more focus
on the segments of the answer that have some interaction
with the question, and vice-verse.

7. conclusions
we present attentive pooling, a two-way attention mech-
anism for discriminative model training. the main con-
tributions of the paper are: (1) ap is more general than

6yin et al.

(yin et al., 2015) report 0.6921(map) and
0.7108(mrr) when using handcrafted features in addition to
id27s.

figure 5. attention heat map from ap-id98 for a correctly se-
lected answer.

recently proposed two-way attention mechanism because:
(a) it learns how to compute interactions between the items
in the input pair; and (b) it can be applied to both id98s and
id56s; (2) we demonstrate that ap can be effectively used
with id98s and bilstm in the context of the answer selec-
tion task, using three different benchmark datasets; (3) our
experimental results demonstrate that ap helps the id98 to
cope with large input texts; (4) we present new state-of-the-
art results for insuranceqa and trec-qa datasets. (5) for
the wikiqa dataset our results are the best reported so far
for methods that do not use handcrafted features.

acknowledgements
the authors would like to thank piero molino for creating
the script used to produce the text heat maps presented in
this work.

references
bahdanau, dzmitry, cho, kyunghyun, and bengio,
yoshua. id4 by jointly learning
to align and translate. in iclr, 2015.

bergstra, james, breuleux, olivier, bastien, fr  ed  eric,
lamblin, pascal, pascanu, razvan, desjardins, guil-
laume, turian, joseph, warde-farley, david, and ben-
gio, yoshua. theano: a cpu and gpu math expression
in proceedings of the python for scienti   c
compiler.
computing conference, 2010.

dos santos, cicero, barbosa, luciano, bogdanova, dasha,
and zadrozny, bianca. learning hybrid representations

attentive pooling networks

natural language sentences. advances in neural infor-
mation processing systems (nips), 2014.

mikolov, tomas, sutskever, ilya, chen, kai, corrado,
greg s., and dean, jeff. distributed representations of
words and phrases and their compositionality. advances
in neural information processing systems (nips), 2013.

rockt  aschel, tim, grefenstette, edward, hermann,
karl moritz, kocisk  y, tom  as, and blunsom, phil. rea-
soning about entailment with neural attention. corr,
abs/1509.06664, 2015.

severyn, aliaksei and moschitti, alessandro. learning to
rank short text pairs with convolutional deep neural net-
works. proceedings of sigir, 2015.

shen, yelong, he, xiaodong, gao, jianfeng, deng,
li, and mesnil, gr  egoire. a latent semantic model
with convolutional-pooling structure for information re-
trieval. in proceedings of the 23rd acm international
conference on conference on information and knowl-
edge management, pp. 101   110, 2014.
isbn 978-1-
4503-2598-1.

sutskever, ilya, vinyals, oriol, and le, quoc v. sequence
to sequence learning with neural networks. advances in
neural information processing systems, 2014.

tan, ming, dos santos, cicero, xiang, bing, and zhou,
bowen. lstm-based deep learning models for non-
factoid answer selection. corr, abs/1511.04108, 2015.

wang, di and nyberg, eric. a long short-term memory
model for answer sentence selection in question answer-
ing. proceedings of the 53rd annual meeting of the as-
sociation for computational linguistics and the 7th in-
ternational joint conference on natural language pro-
cessing, 2015.

wang, mengqiu and manning, christopher. probabilistic
tree-edit models with structured latent variables for tex-
tual entailment and id53. the proceed-
ings of the 23rd international conference on computa-
tional linguistics (coling), 2010.

wang, mengqiu, smith, noah, and teruko, mitamura.
what is the jeopardy model? a quasi-synchronous gram-
mar for qa. the proceedings of emnlp-conll, 2007.

wang, zhiguo and ittycheriah, abraham. faq-based ques-
arxiv preprint

tion answering via word alignment.
arxiv:1507.02628, 2015.

weston,

jason, chopra, sumit, and adams, keith.
#tagspace: semantic embeddings from hashtags. pro-
ceedings of the 2014 conference on empirical methods
in natural language processing (emnlp), 2014.

figure 6. attention heat map from ap-id98 for a correctly se-
lected answer.

to retrieve semantically equivalent questions.
2015.

in acl,

dos santos, cicero nogueira and zadrozny, bianca. learn-
ing character-level representations for part-of-speech
tagging. in proceedings of the 31st international con-
ference on machine learning, jmlr: w&cp volume 32,
beijing, china, 2014.

feng, minwei, xiang, bing, glass, michael r, wang, li-
dan, and zhou, bowen. applying deep learning to
arxiv
answer selection: a study and an open task.
preprint:1508.01585, 2015.

graves, alex, mohamed, abdel-rahman, and hinton, ge-
offrey. id103 with deep recurrent neural
networks. in ieee international conference on acous-
tics, speech and signal processing (icassp), 2013.

hermann, karl moritz, kocisky, tomas, grefenstette, ed-
ward, espeholt, lasse, kay, will, suleyman, mustafa,
and blunsom, phil. teaching machines to read and com-
prehend. in advances in neural information processing
systems 28, pp. 1684   1692, 2015.

hochreiter, sepp and schmidhuber, jurgen. long short-

term memory. neural computation, 1997.

hu, baotian, lu, zhengdong, li, hang, and chen, qingcai.
convolutional neural network architectures for matching

attentive pooling networks

xu, kelvin, ba, jimmy, kiros, ryan, cho, kyunghyun,
courville, aaron c., salakhutdinov, ruslan, zemel,
richard s., and bengio, yoshua. show, attend and tell:
neural image id134 with visual attention.
in proceedings of the 32nd international conference on
machine learning, icml 2015, lille, france, 6-11 july
2015, pp. 2048   2057, 2015.

yang, yi, yih, wen-tau, and meek, christopher. wikiqa: a
challenge dataset for open-domain id53.
in proceedings of conference on empirical methods in
natural language processing (emnlp), 2015.

yao, xuchen, durme, benjamin, and clark, peter. answer
extraction as sequence tagging with tree id153.
proceedings of naacl-hlt, 2013.

yih, wen-tau, chang, ming-wei, meek, christopher, and
pastusiak, andrzej. id53 using enhanced
lexical semantic models. proceedings of the 51st annual
meeting of the association for computational linguist
(acl), 2013.

yin, wenpeng, sch  utze, hinrich, xiang, bing, and zhou,
bowen. abid98: attention-based convolutional neu-
corr,
ral network for modeling sentence pairs.
abs/1512.05193, 2015.

yu, lei, hermann, karl m., blunsom, phil, and pulman,
stephen. deep learning for answer sentence selection.
nips deep learning workshop, 2014.

