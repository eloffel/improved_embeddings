6
1
0
2

 

n
u
j
 

0
1

 
 
]
l
c
.
s
c
[
 
 

5
v
9
0
9
7
0

.

8
0
5
1
:
v
i
x
r
a

id4 of rare words with subword units

rico sennrich and barry haddow and alexandra birch

school of informatics, university of edinburgh

{rico.sennrich,a.birch}@ed.ac.uk, bhaddow@inf.ed.ac.uk

abstract

id4 (id4) mod-
els typically operate with a    xed vocabu-
lary, but translation is an open-vocabulary
problem.
previous work addresses the
translation of out-of-vocabulary words by
backing off to a dictionary.
in this pa-
per, we introduce a simpler and more ef-
fective approach, making the id4 model
capable of open-vocabulary translation by
encoding rare and unknown words as se-
quences of subword units. this is based on
the intuition that various word classes are
translatable via smaller units than words,
for instance names (via character copying
or id68), compounds (via com-
positional translation), and cognates and
loanwords (via phonological and morpho-
logical transformations). we discuss the
suitability of different id40
techniques, including simple character n-
gram models and a segmentation based on
the byte pair encoding compression algo-
rithm, and empirically show that subword
models improve over a back-off dictionary
baseline for the wmt 15 translation tasks
english   german and english   russian
by up to 1.1 and 1.3 id7, respectively.

1 introduction

id4 has recently shown
impressive results (kalchbrenner and blunsom,
2013; sutskever et al., 2014; bahdanau et al.,
2015). however, the translation of rare words
is an open problem. the vocabulary of neu-
ral models is typically limited to 30 000   50 000
words, but translation is an open-vocabulary prob-

the research presented in this publication was conducted
in cooperation with samsung electronics polska sp. z o.o. -
samsung r&d institute poland.

lem, and especially for languages with produc-
tive word formation processes such as aggluti-
nation and compounding, translation models re-
quire mechanisms that go below the word level.
as an example, consider compounds such as the
german abwasser|behandlungs|anlange    sewage
water treatment plant   , for which a segmented,
variable-length representation is intuitively more
appealing than encoding the word as a    xed-length
vector.

for word-level id4 models,

the translation
of out-of-vocabulary words has been addressed
through a back-off to a dictionary look-up (jean et
al., 2015; luong et al., 2015b). we note that such
techniques make assumptions that often do not
hold true in practice. for instance, there is not al-
ways a 1-to-1 correspondence between source and
target words because of variance in the degree of
morphological synthesis between languages, like
in our introductory compounding example. also,
word-level models are unable to translate or gen-
erate unseen words. copying unknown words into
the target text, as done by (jean et al., 2015; luong
et al., 2015b), is a reasonable strategy for names,
but morphological changes and id68 is
often required, especially if alphabets differ.

we investigate id4 models that operate on the
level of subword units. our main goal is to model
open-vocabulary translation in the id4 network
itself, without requiring a back-off model for rare
words. in addition to making the translation pro-
cess simpler, we also    nd that the subword models
achieve better accuracy for the translation of rare
words than large-vocabulary models and back-off
dictionaries, and are able to productively generate
new words that were not seen at training time. our
analysis shows that the neural networks are able to
learn compounding and id68 from sub-
word representations.

this paper has two main contributions:

    we show that open-vocabulary neural ma-

chine translation is possible by encoding
(rare) words via subword units. we    nd our
architecture simpler and more effective than
using large vocabularies and back-off dictio-
naries (jean et al., 2015; luong et al., 2015b).

that they are translatable by a competent transla-
tor even if they are novel to him or her, based
on a translation of known subword units such as
morphemes or phonemes. word categories whose
translation is potentially transparent include:

    we adapt byte pair encoding (bpe) (gage,
1994), a compression algorithm, to the task
of id40. bpe allows for the
representation of an open vocabulary through
a    xed-size vocabulary of variable-length
character sequences, making it a very suit-
able id40 strategy for neural
network models.

2 id4

we follow the id4 archi-
tecture by bahdanau et al. (2015), which we will
brie   y summarize here. however, we note that our
approach is not speci   c to this architecture.

the id4 system is imple-
mented as an encoder-decoder network with recur-
rent neural networks.

the encoder is a bidirectional neural network
with id149 (cho et al., 2014)
that reads an input sequence x = (x1, ..., xm)
and calculates a forward sequence of hidden
      
h m), and a backward sequence
states (
      
      
h j are
(
h 1, ...,
concatenated to obtain the annotation vector hj.

      
h 1, ...,
      
h m). the hidden states

      
h j and

the decoder is a recurrent neural network that
predicts a target sequence y = (y1, ..., yn). each
word yi is predicted based on a recurrent hidden
state si, the previously predicted word yi   1, and
a context vector ci. ci is computed as a weighted
sum of the annotations hj. the weight of each
annotation hj is computed through an alignment
model   ij, which models the id203 that yi is
aligned to xj. the alignment model is a single-
layer feedforward neural network that is learned
jointly with the rest of the network through back-
propagation.

a detailed description can be found in (bah-
danau et al., 2015). training is performed on a
parallel corpus with stochastic id119.
for translation, a id125 with small beam
size is employed.

3 subword translation

the main motivation behind this paper is that
the translation of some words is transparent in

    named entities. between languages that share
an alphabet, names can often be copied from
source to target text. transcription or translit-
eration may be required, especially if the al-
phabets or syllabaries differ. example:
barack obama (english; german)
                      (russian)
                      (ba-ra-ku o-ba-ma) (japanese)
    cognates and loanwords. cognates and loan-
words with a common origin can differ in
regular ways between languages, so that
character-level translation rules are suf   cient
(tiedemann, 2012). example:
claustrophobia (english)
klaustrophobie (german)
                           (klaustrofobi  ) (russian)
    morphologically complex words. words con-
taining multiple morphemes,
for instance
formed via compounding, af   xation, or in-
   ection, may be translatable by translating
the morphemes separately. example:
solar system (english)
sonnensystem (sonne + system) (german)
naprendszer (nap + rendszer) (hungarian)

in an analysis of 100 rare tokens (not among
the 50 000 most frequent types) in our german
training data1, the majority of tokens are poten-
tially translatable from english through smaller
units. we    nd 56 compounds, 21 names,
6 loanwords with a common origin (emanci-
pate   emanzipieren), 5 cases of transparent af   x-
ation (sweetish    sweet    +    -ish        s    lich    s        +
   -lich   ), 1 number and 1 computer language iden-
ti   er.

our hypothesis is that a segmentation of rare
words into appropriate subword units is suf   -
cient to allow for the neural translation network
to learn transparent translations, and to general-
ize this knowledge to translate and produce unseen
words.2 we provide empirical support for this hy-

1primarily parliamentary proceedings and web crawl data.
2not every segmentation we produce is transparent.
while we expect no performance bene   t from opaque seg-
mentations,
i.e. segmentations where the units cannot be
translated independently, our id4 models show robustness
towards oversplitting.

pothesis in sections 4 and 5. first, we discuss dif-
ferent subword representations.

3.1 related work

for id151 (smt), the
translation of unknown words has been the subject
of intensive research.

a large proportion of unknown words are
names, which can just be copied into the tar-
get text if both languages share an alphabet.
if
alphabets differ, id68 is required (dur-
rani et al., 2014). character-based translation has
also been investigated with phrase-based models,
which proved especially successful for closely re-
lated languages (vilar et al., 2007; tiedemann,
2009; neubig et al., 2012).

the segmentation of morphologically complex
words such as compounds is widely used for smt,
and various algorithms for morpheme segmen-
tation have been investigated (nie  en and ney,
2000; koehn and knight, 2003; virpioja et al.,
2007; stallard et al., 2012). segmentation al-
gorithms commonly used for phrase-based smt
tend to be conservative in their splitting decisions,
whereas we aim for an aggressive segmentation
that allows for open-vocabulary translation with a
compact network vocabulary, and without having
to resort to back-off dictionaries.

the best choice of subword units may be task-
speci   c. for id103, phone-level lan-
guage models have been used (bazzi and glass,
2000). mikolov et al. (2012) investigate subword
language models, and propose to use syllables.
for multilingual segmentation tasks, multilingual
algorithms have been proposed (snyder and barzi-
lay, 2008). we    nd these intriguing, but inapplica-
ble at test time.

various techniques have been proposed to pro-
duce    xed-length continuous word vectors based
on characters or morphemes (luong et al., 2013;
botha and blunsom, 2014; ling et al., 2015a; kim
et al., 2015). an effort to apply such techniques
to id4, parallel to ours, has found no signi   cant
improvement over word-based approaches (ling
et al., 2015b). one technical difference from our
work is that the attention mechanism still oper-
ates on the level of words in the model by ling
et al. (2015b), and that the representation of each
word is    xed-length. we expect that the attention
mechanism bene   ts from our variable-length rep-
resentation: the network can learn to place atten-

tion on different subword units at each step. re-
call our introductory example abwasserbehand-
lungsanlange, for which a subid40
avoids the information bottleneck of a    xed-length
representation.

id4 differs from phrase-
based methods in that there are strong incentives to
minimize the vocabulary size of neural models to
increase time and space ef   ciency, and to allow for
translation without back-off models. at the same
time, we also want a compact representation of the
text itself, since an increase in text length reduces
ef   ciency and increases the distances over which
neural models need to pass information.

a simple method to manipulate the trade-off be-
tween vocabulary size and text size is to use short-
lists of unsegmented words, using subword units
only for rare words. as an alternative, we pro-
pose a segmentation algorithm based on byte pair
encoding (bpe), which lets us learn a vocabulary
that provides a good compression rate of the text.

3.2 byte pair encoding (bpe)

byte pair encoding (bpe) (gage, 1994) is a sim-
ple data compression technique that iteratively re-
places the most frequent pair of bytes in a se-
quence with a single, unused byte. we adapt this
algorithm for id40. instead of merg-
ing frequent pairs of bytes, we merge characters or
character sequences.

firstly, we initialize the symbol vocabulary with
the character vocabulary, and represent each word
as a sequence of characters, plus a special end-of-
word symbol         , which allows us to restore the
original id121 after translation. we itera-
tively count all symbol pairs and replace each oc-
currence of the most frequent pair (   a   ,    b   ) with
a new symbol    ab   . each merge operation pro-
duces a new symbol which represents a charac-
ter id165. frequent character id165s (or whole
words) are eventually merged into a single sym-
bol, thus bpe requires no shortlist. the    nal sym-
bol vocabulary size is equal to the size of the initial
vocabulary, plus the number of merge operations
    the latter is the only hyperparameter of the algo-
rithm.

for ef   ciency, we do not consider pairs that
cross word boundaries. the algorithm can thus be
run on the dictionary extracted from a text, with
each word being weighted by its frequency. a
minimal python implementation is shown in al-

algorithm 1 learn bpe operations

import re, collections

def get_stats(vocab):

pairs = collections.defaultdict(int)
for word, freq in vocab.items():

symbols = word.split()
for i in range(len(symbols)-1):

pairs[symbols[i],symbols[i+1]] += freq

return pairs

def merge_vocab(pair, v_in):

v_out = {}
bigram = re.escape(' '.join(pair))
p = re.compile(r'(?<!\s)' + bigram + r'(?!\s)')
for word in v_in:

w_out = p.sub(''.join(pair), word)
v_out[w_out] = v_in[word]

return v_out

vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,

'n e w e s t </w>':6, 'w i d e s t </w>':3}

num_merges = 10
for i in range(num_merges):

pairs = get_stats(vocab)
best = max(pairs, key=pairs.get)
vocab = merge_vocab(best, vocab)
print(best)

    r  
    lo

r   
l o
lo w     low
e r       er  

figure 1: bpe merge operations learned from dic-
tionary {   low   ,    lowest   ,    newer   ,    wider   }.

gorithm 1. in practice, we increase ef   ciency by
indexing all pairs, and updating data structures in-
crementally.

the main difference to other compression al-
gorithms, such as huffman encoding, which have
been proposed to produce a variable-length en-
coding of words for id4 (chitnis and denero,
2015), is that our symbol sequences are still in-
terpretable as subword units, and that the network
can generalize to translate and produce new words
(unseen at training time) on the basis of these sub-
word units.

figure 1 shows a toy example of learned bpe
operations. at test time, we    rst split words into
sequences of characters, then apply the learned op-
erations to merge the characters into larger, known
symbols. this is applicable to any word, and
allows for open-vocabulary networks with    xed
symbol vocabularies.3 in our example, the oov
   lower    would be segmented into    low er     .

3the only symbols that will be unknown at test time are
unknown characters, or symbols of which all occurrences
in the training text have been merged into larger symbols,
like    safeguar   , which has all occurrences in our training text
merged into    safeguard   . we observed no such symbols at
test time, but the issue could be easily solved by recursively
reversing speci   c merges until all symbols are known.

we evaluate two methods of applying bpe:
learning two independent encodings, one for the
source, one for the target vocabulary, or learning
the encoding on the union of the two vocabular-
ies (which we call joint bpe).4 the former has the
advantage of being more compact in terms of text
and vocabulary size, and having stronger guaran-
tees that each subword unit has been seen in the
training text of the respective language, whereas
the latter improves consistency between the source
and the target segmentation. if we apply bpe in-
dependently, the same name may be segmented
differently in the two languages, which makes it
harder for the neural models to learn a mapping
between the subword units. to increase the con-
sistency between english and russian segmenta-
tion despite the differing alphabets, we transliter-
ate the russian vocabulary into latin characters
with iso-9 to learn the joint bpe encoding, then
transliterate the bpe merge operations back into
cyrillic to apply them to the russian training text.5

4 evaluation

we aim to answer the following empirical ques-
tions:

    can we improve the translation of rare and
unseen words in id4
by representing them via subword units?

    which segmentation into subword units per-
forms best in terms of vocabulary size, text
size, and translation quality?

we perform experiments on data from the
shared translation task of wmt 2015.
for
english   german, our training set consists of 4.2
million sentence pairs, or approximately 100 mil-
lion tokens. for english   russian, the training set
consists of 2.6 million sentence pairs, or approx-
imately 50 million tokens. we tokenize and true-
case the data with the scripts provided in moses
(koehn et al., 2007). we use newstest2013 as de-
velopment set, and report results on newstest2014
and newstest2015.

we report results with id7 (mteval-v13a.pl),
and chrf3 (popovi  c, 2015), a character id165
f3 score which was found to correlate well with

4in practice, we simply concatenate the source and target

side of the training set to learn joint bpe.

5since the russian training text also contains words that
use the latin alphabet, we also apply the latin bpe opera-
tions.

human judgments, especially for translations out
of english (stanojevi  c et al., 2015). since our
main claim is concerned with the translation of
rare and unseen words, we report separate statis-
tics for these. we measure these through unigram
f1, which we calculate as the harmonic mean of
clipped unigram precision and recall.6

we perform all experiments with groundhog7
(bahdanau et al., 2015). we generally follow set-
tings by previous work (bahdanau et al., 2015;
jean et al., 2015). all networks have a hidden
layer size of 1000, and an embedding layer size
of 620. following jean et al. (2015), we only keep
a shortlist of    = 30000 words in memory.

during training, we use adadelta (zeiler, 2012),
a minibatch size of 80, and reshuf   e the train-
ing set between epochs. we train a network for
approximately 7 days, then take the last 4 saved
models (models being saved every 12 hours), and
continue training each with a    xed embedding
layer (as suggested by (jean et al., 2015)) for 12
hours. we perform two independent training runs
for each models, once with cut-off for gradient
clipping (pascanu et al., 2013) of 5.0, once with
a cut-off of 1.0     the latter produced better single
models for most settings. we report results of the
system that performed best on our development set
(newstest2013), and of an ensemble of all 8 mod-
els.

we use a beam size of 12 for id125,
with probabilities normalized by sentence length.
we use a bilingual dictionary based on fast-align
(dyer et al., 2013). for our baseline, this serves
as back-off dictionary for rare words. we also use
the dictionary to speed up translation for all ex-
periments, only performing the softmax over a    l-
tered list of candidate translations (like jean et al.
(2015), we use k = 30000; k     = 10).

4.1 subword statistics

apart from translation quality, which we will ver-
ify empirically, our main objective is to represent
an open vocabulary through a compact    xed-size
subword vocabulary, and allow for ef   cient train-
ing and decoding.8

statistics for different segmentations of the ger-

6clipped unigram precision is essentially 1-gram id7

without brevity penalty.

7github.com/sebastien-j/lv_groundhog
8the time complexity of encoder-decoder architectures is
at least linear to sequence length, and oversplitting harms ef-
   ciency.

man side of the parallel data are shown in table
1. a simple baseline is the segmentation of words
into character id165s.9 character id165s allow
for different trade-offs between sequence length
(# tokens) and vocabulary size (# types), depend-
ing on the choice of n. the increase in sequence
length is substantial; one way to reduce sequence
length is to leave a shortlist of the k most frequent
word types unsegmented. only the unigram repre-
sentation is truly open-vocabulary. however, the
unigram representation performed poorly in pre-
liminary experiments, and we report translation re-
sults with a bigram representation, which is empir-
ically better, but unable to produce some tokens in
the test set with the training set vocabulary.

we report statistics for several word segmenta-
tion techniques that have proven useful in previous
smt research,
including frequency-based com-
pound splitting (koehn and knight, 2003), rule-
based hyphenation (liang, 1983), and morfessor
(creutz and lagus, 2002). we    nd that they only
moderately reduce vocabulary size, and do not
solve the unknown word problem, and we thus    nd
them unsuitable for our goal of open-vocabulary
translation without back-off dictionary.

bpe meets our goal of being open-vocabulary,
and the learned merge operations can be applied
to the test set to obtain a segmentation with no
unknown symbols.10
its main difference from
the character-level model is that the more com-
pact representation of bpe allows for shorter se-
quences, and that the attention model operates
on variable-length units.11 table 1 shows bpe
with 59 500 merge operations, and joint bpe with
89 500 operations.

in practice, we did not include infrequent sub-
word units in the id4 network vocabulary, since
there is noise in the subword symbol sets, e.g.
because of characters from foreign alphabets.
hence, our network vocabularies in table 2 are
typically slightly smaller than the number of types
in table 1.

9our character id165s do not cross word boundaries. we
mark whether a subword is word-   nal or not with a special
character, which allows us to restore the original id121.
10joint bpe can produce segments that are unknown be-
cause they only occur in the english training text, but these
are rare (0.05% of test tokens).

11we highlighted the limitations of word-level attention in
section 3.1. at the other end of the spectrum, the character
level is suboptimal for alignment (tiedemann, 2009).

vocabulary

id7

chrf3

target single ens-8 single ens-8

segmentation shortlist

source
name
syntax-based (sennrich and haddow, 2015)
wunk
wdict
c2-50k
bpe-60k bpe
bpe-j90k bpe (joint)

-
-
char-bigram

- 300 000 500 000
- 300 000 500 000
60 000
60 000
90 000

60 000
60 000
90 000

50 000
-
-

24.4
20.6
22.0
22.8
21.5
22.8

-
22.8
24.2
25.3
24.5
24.7

55.3
47.2
50.5
51.9
52.0
51.7

unigram f1 (%)
all
rare oov
37.7
0.0
36.8
30.9
29.3
33.6

- 59.1 46.0
48.9 56.7 20.4
52.4 58.1 36.8
53.5 58.4 40.5
53.9 58.4 40.9
54.1 58.5 41.8

table 2: english   german translation performance (id7, chrf3 and unigram f1) on newstest2015.
ens-8: ensemble of 8 models. best id4 system in bold. unigram f1 (with ensembles) is computed for
all words (n = 44085), rare words (not among top 50 000 in training set; n = 2900), and oovs (not in
training set; n = 1168).

segmentation
none
characters
character bigrams
character trigrams
compound splitting   
morfessor*
hyphenation   
bpe
bpe (joint)
character bigrams
(shortlist: 50 000)

# tokens

# types
100 m 1 750 000
3000
550 m
20 000
306 m
214 m
120 000
102 m 1 100 000
544 000
109 m
404 000
186 m
63 000
112 m
111 m
82 000

# unk
1079
0
34
59
643
237
230
0
32

129 m

69 000

34

table 1: corpus statistics for german training
corpus with different id40 tech-
niques. #unk: number of unknown tokens in
newstest2013.    : (koehn and knight, 2003); *:
(creutz and lagus, 2002);    : (liang, 1983).

4.2 translation experiments

english   german translation results are shown in
table 2; english   russian results in table 3.

our baseline wdict is a word-level model with
a back-off dictionary. it differs from wunk in that
the latter uses no back-off dictionary, and just rep-
resents out-of-vocabulary words as unk12. the
back-off dictionary improves unigram f1 for rare
and unseen words, although the improvement is
smaller for english   russian, since the back-off
dictionary is incapable of transliterating names.

all subword systems operate without a back-off
dictionary. we    rst focus on unigram f1, where
all systems improve over the baseline, especially
for rare words (36.8%   41.8% for en   de;
26.5%   29.7% for en   ru). for oovs,
the
baseline strategy of copying unknown words
works well for english   german. however, when
alphabets differ,
the
subword models do much better.

like in english   russian,

12we use unk for words that are outside the model vo-
cabulary, and oov for those that do not occur in the training
text.

unigram f1 scores indicate that learning the
bpe symbols on the vocabulary union (bpe-
j90k) is more effective than learning them sep-
arately (bpe-60k), and more effective than using
character bigrams with a shortlist of 50 000 unseg-
mented words (c2-50k), but all reported subword
segmentations are viable choices and outperform
the back-off dictionary baseline.

our subword representations cause big im-
provements in the translation of rare and unseen
words, but these only constitute 9-11% of the test
sets. since rare words tend to carry central in-
formation in a sentence, we suspect that id7
and chrf3 underestimate their effect on transla-
tion quality. still, we also see improvements over
the baseline in total unigram f1, as well as id7
and chrf3, and the subword ensembles outper-
form the wdict baseline by 0.3   1.3 id7 and
0.6   2 chrf3. there is some inconsistency be-
tween id7 and chrf3, which we attribute to the
fact that id7 has a precision bias, and chrf3 a
recall bias.

for english   german, we observe the best
id7 score of 25.3 with c2-50k, but the best
chrf3 score of 54.1 with bpe-j90k. for com-
parison to the (to our knowledge) best non-neural
mt system on this data set, we report syntax-
based smt results (sennrich and haddow, 2015).
we observe that our best systems outperform the
syntax-based system in terms of id7, but not
in terms of chrf3. regarding other neural sys-
tems, luong et al. (2015a) report a id7 score of
25.9 on newstest2015, but we note that they use an
ensemble of 8 independently trained models, and
also report strong improvements from applying
dropout, which we did not use. we are con   dent
that our improvements to the translation of rare
words are orthogonal to improvements achievable
through other improvements in the network archi-

tecture, training algorithm, or better ensembles.

for english   russian,

the state of the art is
the phrase-based system by haddow et al. (2015).
it outperforms our wdict baseline by 1.5 id7.
the subword models are a step towards closing
this gap, and bpe-j90k yields an improvement of
1.3 id7, and 2.0 chrf3, over wdict.

as a further comment on our translation results,
we want to emphasize that performance variabil-
ity is still an open problem with id4. on our de-
velopment set, we observe differences of up to 1
id7 between different models. for single sys-
tems, we report the results of the model that per-
forms best on dev (out of 8), which has a stabi-
lizing effect, but how to control for randomness
deserves further attention in future research.

5 analysis

5.1 unigram accuracy

our main claims are that the translation of rare and
unknown words is poor in word-level id4 mod-
els, and that subword models improve the trans-
lation of these word types. to further illustrate
the effect of different subid40s on
the translation of rare and unseen words, we plot
target-side words sorted by their frequency in the
training set.13 to analyze the effect of vocabulary
size, we also include the system c2-3/500k, which
is a system with the same vocabulary size as the
wdict baseline, and character bigrams to repre-
sent unseen words.

figure 2 shows results for the english   german
ensemble systems on newstest2015. unigram
f1 of all systems tends to decrease for lower-
frequency words. the baseline system has a spike
in f1 for oovs, i.e. words that do not occur in
the training text. this is because a high propor-
tion of oovs are names, for which a copy from
the source to the target text is a good strategy for
english   german.

the systems with a target vocabulary of 500 000
words mostly differ in how well they translate
words with rank > 500 000. a back-off dictionary
is an obvious improvement over producing unk,
but the subword system c2-3/500k achieves better
performance. note that all oovs that the back-
off dictionary produces are words that are copied
from the source, usually names, while the subword

13we perform binning of words with the same training set

frequency, and apply bezier smoothing to the graph.

systems can productively form new words such as
compounds.

for the 50 000 most frequent words, the repre-
sentation is the same for all neural networks, and
all neural networks achieve comparable unigram
f1 for this category. for the interval between fre-
quency rank 50 000 and 500 000, the comparison
between c2-3/500k and c2-50k unveils an inter-
esting difference. the two systems only differ in
the size of the shortlist, with c2-3/500k represent-
ing words in this interval as single units, and c2-
50k via subword units. we    nd that the perfor-
mance of c2-3/500k degrades heavily up to fre-
quency rank 500 000, at which point the model
switches to a subword representation and perfor-
mance recovers. the performance of c2-50k re-
mains more stable. we attribute this to the fact
that subword units are less sparse than words. in
our training set, the frequency rank 50 000 corre-
sponds to a frequency of 60 in the training data;
the frequency rank 500 000 to a frequency of 2.
because subword representations are less sparse,
reducing the size of the network vocabulary, and
representing more words via subword units, can
lead to better performance.

the f1 numbers hide some qualitative differ-
ences between systems. for english   german,
wdict produces few oovs (26.5% recall), but
with high precision (60.6%) , whereas the subword
systems achieve higher recall, but lower precision.
we note that the character bigram model c2-50k
produces the most oov words, and achieves rel-
atively low precision of 29.1% for this category.
however, it outperforms the back-off dictionary
in recall (33.0%). bpe-60k, which suffers from
id68 (or copy) errors due to segmenta-
tion inconsistencies, obtains a slightly better pre-
cision (32.4%), but a worse recall (26.6%). in con-
trast to bpe-60k, the joint bpe encoding of bpe-
j90k improves both precision (38.6%) and recall
(29.8%).

for english   russian, unknown names can
only rarely be copied, and usually require translit-
eration. consequently, the wdict baseline per-
forms more poorly for oovs (9.2% precision;
5.2% recall), and the subword models improve
both precision and recall (21.9% precision and
15.6% recall for bpe-j90k). the full unigram f1
plot is shown in figure 3.

vocabulary

id7

chrf3

source

target single ens-8 single ens-8

segmentation shortlist
name
phrase-based (haddow et al., 2015)
wunk
wdict
c2-50k
bpe-60k bpe
bpe-j90k bpe (joint)

-
-
char-bigram

50 000
-
-

- 300 000 500 000
- 300 000 500 000
60 000
60 000
60 000
60 000
90 000 100 000

24.3
18.8
19.1
20.9
20.5
20.4

-
22.4
22.8
24.1
23.6
24.1

53.8
46.5
47.5
49.0
49.8
49.7

unigram f1 (%)
all
rare oov
16.5
0.0
6.6
17.4
15.6
18.3

- 56.0 31.3
49.9 54.2 25.2
51.0 54.8 26.5
51.6 55.2 27.8
52.7 55.3 29.7
53.0 55.8 29.7

table 3: english   russian translation performance (id7, chrf3 and unigram f1) on newstest2015.
ens-8: ensemble of 8 models. best id4 system in bold. unigram f1 (with ensembles) is computed for
all words (n = 55654), rare words (not among top 50 000 in training set; n = 5442), and oovs (not in
training set; n = 851).

50 000

500 000

1

0.8

0.6

0.4

0.2

1
f
m
a
r
g
i
n
u

bpe-j90k
c2-50k
c2-300/500k
wdict
wunk

0
100

101

102

105
training set frequency rank

103

104

106

figure 2: english   german unigram f1 on new-
stest2015 plotted by training set frequency rank
for different id4 systems.

50 000

500 000

1

0.8

0.6

0.4

0.2

1
f
m
a
r
g
i
n
u

bpe-j90k
c2-50k
wdict
wunk

0
100

101

102

105
training set frequency rank

104

103

106

figure 3: english   russian unigram f1 on new-
stest2015 plotted by training set frequency rank
for different id4 systems.

5.2 manual analysis

table 4 shows two translation examples for
the translation direction english   german, ta-
ble 5 for english   russian. the baseline sys-
tem fails for all of the examples, either by delet-
ing content (health), or by copying source words
that should be translated or transliterated. the
subword translations of health research insti-
tutes show that the subword systems are capa-
ble of learning translations when oversplitting (re-
search   fo|rs|ch|un|g), or when the segmentation
does not match morpheme boundaries:
the seg-
mentation forschungs|instituten would be linguis-
tically more plausible, and simpler to align to the
english research institutes, than the segmentation
forsch|ungsinstitu|ten in the bpe-60k system, but
still, a correct translation is produced. if the sys-
tems have failed to learn a translation due to data
sparseness, like for asinine, which should be trans-
lated as dumm, we see translations that are wrong,
but could be plausible for (partial) loanwords (asi-
nine situation   asinin-situation).

the english   russian examples show that
the subword systems are capable of translitera-
tion. however,
id68 errors do occur,
either due to ambiguous id68s, or be-
cause of non-consistent segmentations between
source and target text which make it hard for
the system to learn a id68 mapping.
note that the bpe-60k system encodes mirza-
yeva inconsistently for the two language pairs
(mirz|ayeva         |    |       mir|za|eva). this ex-
ample is still translated correctly, but we observe
spurious insertions and deletions of characters in
the bpe-60k system. an example is the translit-
eration of rak   sk, where a    is inserted and a   
is deleted. we trace this error back to transla-
tion pairs in the training data with inconsistent
segmentations, such as (p|rak|ri|ti         |        |  

sentence
system
health research institutes
source
reference
gesundheitsforschungsinstitute
forschungsinstitute
wdict
c2-50k
fo|rs|ch|un|gs|in|st|it|ut|io|ne|n
gesundheits|forsch|ungsinstitu|ten
bpe-60k
gesundheits|forsch|ungsin|stitute
bpe-j90k
asinine situation
source
reference
dumme situation
asinine situation     unk     asinine
wdict
as|in|in|e situation     as|in|en|si|tu|at|io|n
c2-50k
bpe-60k
as|in|ine situation     a|in|line-|situation
bpe-j90k as|in|ine situation     as|in|in-|situation

table 4: english   german translation example.
   |    marks subword boundaries.

sentence
mirzayeva

system
source
reference                  (mirzaeva)
wdict
mirzayeva     unk     mirzayeva
mi|rz|ay|ev|a         |    |    |     (mi|rz|ae|va)
c2-50k
bpe-60k mirz|ayeva           |    |       (mir|za|eva)
bpe-j90k mir|za|yeva           |    |       (mir|za|eva)
source
reference
wdict
c2-50k
bpe-60k
bpe-j90k

rak   sk
                 (rak   ska)
rak   sk     unk     rak   sk
ra|kf|is|k         |    |    |   (ra|kf|is|k)
rak|f|isk           |  |       (pra|f|isk)
rak|f|isk           |  |         (rak|f|iska)

table 5: english   russian translation examples.
   |    marks subword boundaries.

(pra|krit|i)), from which the translation (rak         )
is erroneously learned. the segmentation of the
joint bpe system (bpe-j90k) is more consistent
(pra|krit|i         |        |   (pra|krit|i)).

system, and that reducing the vocabulary size
of subword models can actually improve perfor-
mance. in this work, our choice of vocabulary size
is somewhat arbitrary, and mainly motivated by
comparison to prior work. one avenue of future
research is to learn the optimal vocabulary size for
a translation task, which we expect to depend on
the language pair and amount of training data, au-
tomatically. we also believe there is further po-
tential in bilingually informed segmentation algo-
rithms to create more alignable subword units, al-
though the segmentation algorithm cannot rely on
the target text at runtime.

while the relative effectiveness will depend on
language-speci   c factors such as vocabulary size,
we believe that subid40s are suit-
able for most language pairs, eliminating the need
for large id4 vocabularies or back-off models.

acknowledgments

we thank maja popovi  c for her implementa-
tion of chrf, with which we veri   ed our re-
implementation. the research presented in this
publication was conducted in cooperation with
samsung electronics polska sp. z o.o. - sam-
sung r&d institute poland. this project received
funding from the european union   s horizon 2020
research and innovation programme under grant
agreement 645452 (qt21).

6 conclusion

references

the main contribution of this paper is that we
show that id4 systems are
capable of open-vocabulary translation by repre-
senting rare and unseen words as a sequence of
subword units.14 this is both simpler and more
effective than using a back-off translation model.
we introduce a variant of byte pair encoding for
id40, which is capable of encod-
ing open vocabularies with a compact symbol vo-
cabulary of variable-length subword units. we
show performance gains over the baseline with
both bpe segmentation, and a simple character bi-
gram segmentation.

our analysis shows that not only out-of-
vocabulary words, but also rare in-vocabulary
words are translated poorly by our baseline id4

14the source code of

the segmentation algorithms
is available at https://github.com/rsennrich/
subword-id4.

dzmitry bahdanau, kyunghyun cho, and yoshua ben-
gio. 2015. id4 by jointly
learning to align and translate. in proceedings of
the international conference on learning represen-
tations (iclr).

issam bazzi and james r. glass. 2000. modeling out-
of-vocabulary words for robust id103.
in sixth international conference on spoken lan-
guage processing, icslp 2000 / interspeech
2000, pages 401   404, beijing, china.

jan a. botha and phil blunsom. 2014. compositional
morphology for word representations and lan-
guage modelling. in proceedings of the 31st inter-
national conference on machine learning (icml),
beijing, china.

rohan chitnis and john denero. 2015. variable-
length word encodings for neural translation
models. in proceedings of the 2015 conference on
empirical methods in natural language processing
(emnlp).

kyunghyun cho, bart van merrienboer, caglar gul-
cehre, dzmitry bahdanau, fethi bougares, hol-
ger schwenk, and yoshua bengio. 2014. learn-
ing phrase representations using id56 encoder   
decoder for id151. in pro-
ceedings of the 2014 conference on empirical meth-
ods in natural language processing (emnlp),
pages 1724   1734, doha, qatar. association for
computational linguistics.

mathias creutz and krista lagus. 2002. unsupervised
discovery of morphemes.
in proceedings of the
acl-02 workshop on morphological and phonolog-
ical learning, pages 21   30. association for compu-
tational linguistics.

nadir durrani, hassan sajjad, hieu hoang, and philipp
koehn. 2014. integrating an unsupervised translit-
eration model into id151.
in proceedings of the 14th conference of the euro-
pean chapter of the association for computational
linguistics, eacl 2014, pages 148   153, gothen-
burg, sweden.

chris dyer, victor chahuneau, and noah a. smith.
2013. a simple, fast, and effective reparame-
terization of ibm model 2.
in proceedings of the
2013 conference of the north american chapter of
the association for computational linguistics: hu-
man language technologies, pages 644   648, at-
lanta, georgia. association for computational lin-
guistics.

philip gage. 1994. a new algorithm for data com-

pression. c users j., 12(2):23   38, february.

barry haddow, matthias huck, alexandra birch, niko-
lay bogoychev, and philipp koehn. 2015. the
edinburgh/jhu phrase-based machine translation
systems for wmt 2015.
in proceedings of the
tenth workshop on id151,
pages 126   133, lisbon, portugal. association for
computational linguistics.

s  bastien jean, kyunghyun cho, roland memisevic,
and yoshua bengio. 2015. on using very large
target vocabulary for id4.
in proceedings of the 53rd annual meeting of the
association for computational linguistics and the
7th international joint conference on natural lan-
guage processing (volume 1: long papers), pages
1   10, beijing, china. association for computa-
tional linguistics.

nal kalchbrenner and phil blunsom. 2013. recurrent
continuous translation models. in proceedings of
the 2013 conference on empirical methods in nat-
ural language processing, seattle. association for
computational linguistics.

yoon kim, yacine jernite, david sontag, and alexan-
der m. rush. 2015. character-aware neural lan-
guage models. corr, abs/1508.06615.

philipp koehn and kevin knight. 2003. empirical
methods for compound splitting.
in eacl    03:
proceedings of the tenth conference on european
chapter of the association for computational lin-
guistics, pages 187   193, budapest, hungary. asso-
ciation for computational linguistics.

philipp koehn, hieu hoang, alexandra birch, chris
callison-burch, marcello federico, nicola bertoldi,
brooke cowan, wade shen, christine moran,
richard zens, chris dyer, ond  rej bojar, alexandra
constantin, and evan herbst. 2007. moses: open
source toolkit for id151.
in proceedings of the acl-2007 demo and poster
sessions, pages 177   180, prague, czech republic.
association for computational linguistics.

franklin m. liang. 1983. word hy-phen-a-tion by
com-put-er. ph.d. thesis, stanford university, de-
partment of linguistics, stanford, ca.

wang ling, chris dyer, alan w. black, isabel tran-
coso, ramon fermandez, silvio amir, luis marujo,
and tiago luis. 2015a. finding function in form:
compositional character models for open vocab-
ulary word representation.
in proceedings of the
2015 conference on empirical methods in natu-
ral language processing (emnlp), pages 1520   
1530, lisbon, portugal. association for computa-
tional linguistics.

wang ling, isabel trancoso, chris dyer, and alan w.
black. 2015b. character-based neural machine
translation. arxiv e-prints, november.

thang luong, richard socher, and christopher d.
manning.
2013. better word representations
with id56s for morphology.
in proceedings of the seventeenth conference on
computational natural language learning, conll
2013, so   a, bulgaria, august 8-9, 2013, pages 104   
113.

thang luong, hieu pham, and christopher d. man-
ning. 2015a. effective approaches to attention-
based id4.
in proceed-
ings of the 2015 conference on empirical meth-
ods in natural language processing, pages 1412   
1421, lisbon, portugal. association for computa-
tional linguistics.

thang luong, ilya sutskever, quoc le, oriol vinyals,
and wojciech zaremba. 2015b. addressing the
rare word problem in id4.
in proceedings of the 53rd annual meeting of the
association for computational linguistics and the
7th international joint conference on natural lan-
guage processing (volume 1: long papers), pages
11   19, beijing, china. association for computa-
tional linguistics.

tomas mikolov, ilya sutskever, anoop deoras, hai-
son le, stefan kombrink, and jan cernock  . 2012.
subword id38 with neural net-
works. unpublished.

j  rg tiedemann. 2012. character-based pivot trans-
lation for under-resourced languages and do-
mains. in proceedings of the 13th conference of the
european chapter of the association for computa-
tional linguistics, pages 141   151, avignon, france.
association for computational linguistics.

david vilar, jan-thorsten peter, and hermann ney.
2007. can we translate letters? in second work-
shop on id151, pages 33   
39, prague, czech republic. association for com-
putational linguistics.

sami virpioja, jaakko j. v  yrynen, mathias creutz,
and markus sadeniemi. 2007. morphology-aware
id151 based on morphs
induced in an unsupervised manner.
in proceed-
ings of the machine translation summit xi, pages
491   498, copenhagen, denmark.

matthew d. zeiler. 2012. adadelta: an adaptive

learning rate method. corr, abs/1212.5701.

graham neubig, taro watanabe, shinsuke mori, and
tatsuya kawahara.
2012. machine translation
without words through substring alignment. in the
50th annual meeting of the association for compu-
tational linguistics, proceedings of the conference,
july 8-14, 2012, jeju island, korea - volume 1: long
papers, pages 165   174.

sonja nie  en and hermann ney. 2000.

improving
smt quality with morpho-syntactic analysis.
in
18th int. conf. on computational linguistics, pages
1081   1085.

razvan pascanu, tomas mikolov, and yoshua ben-
gio. 2013. on the dif   culty of training recurrent
neural networks. in proceedings of the 30th inter-
national conference on machine learning, icml
2013, pages 1310   1318, atlanta, usa.

maja popovi  c. 2015. chrf: character id165 f-score
for automatic mt evaluation. in proceedings of the
tenth workshop on id151,
pages 392   395, lisbon, portugal. association for
computational linguistics.

rico sennrich and barry haddow. 2015. a joint
dependency model of morphological and syntac-
tic structure for id151. in
proceedings of the 2015 conference on empirical
methods in natural language processing, pages
2081   2087, lisbon, portugal. association for com-
putational linguistics.

benjamin snyder and regina barzilay. 2008. unsu-
pervised multilingual learning for morphological
segmentation.
in proceedings of acl-08: hlt,
pages 737   745, columbus, ohio. association for
computational linguistics.

david stallard,

jacob devlin, michael kayser,
yoong keok lee, and regina barzilay. 2012. unsu-
pervised morphology rivals supervised morphol-
ogy for arabic mt. in the 50th annual meeting of
the association for computational linguistics, pro-
ceedings of the conference, july 8-14, 2012, jeju
island, korea - volume 2: short papers, pages 322   
327.

milo   stanojevi  c, amir kamran, philipp koehn, and
ond  rej bojar. 2015. results of the wmt15 met-
rics shared task. in proceedings of the tenth work-
shop on id151, pages 256   
273, lisbon, portugal. association for computa-
tional linguistics.

ilya sutskever, oriol vinyals, and quoc v. le. 2014.
sequence to sequence learning with neural net-
works. in advances in neural information process-
ing systems 27: annual conference on neural infor-
mation processing systems 2014, pages 3104   3112,
montreal, quebec, canada.

j  rg tiedemann. 2009. character-based psmt for
closely related languages. in proceedings of 13th
annual conference of the european association for
machine translation (eamt   09), pages 12   19.

