modeling coverage for id4

zhaopeng tu    zhengdong lu    yang liu    xiaohua liu    hang li   

   noah   s ark lab, huawei technologies, hong kong

{tu.zhaopeng,lu.zhengdong,liuxiaohua3,hangli.hl}@huawei.com

   department of computer science and technology, tsinghua university, beijing

6
1
0
2

 

g
u
a
6

 

 
 
]
l
c
.
s
c
[
 
 

6
v
1
1
8
4
0

.

1
0
6
1
:
v
i
x
r
a

liuyang2011@tsinghua.edu.cn

abstract

attention mechanism has enhanced state-
of-the-art id4
(id4) by jointly learning to align and
translate. it tends to ignore past alignment
information, however, which often leads
to over-translation and under-translation.
to address this problem, we propose
coverage-based id4 in this paper. we
maintain a coverage vector to keep track
of the attention history. the coverage vec-
tor is fed to the attention model to help ad-
just future attention, which lets id4 sys-
tem to consider more about untranslated
source words.
experiments show that
the proposed approach signi   cantly im-
proves both translation quality and align-
ment quality over standard attention-based
id4.1

introduction

1
the past several years have witnessed the rapid
progress of end-to-end neural machine transla-
tion (id4) (sutskever et al., 2014; bahdanau et
al., 2015). unlike conventional statistical ma-
chine translation (smt) (koehn et al., 2003; chi-
ang, 2007), id4 uses a single and large neural
network to model the entire translation process. it
enjoys the following advantages. first, the use of
distributed representations of words can alleviate
the curse of dimensionality (bengio et al., 2003).
second, there is no need to explicitly design fea-
tures to capture translation regularities, which is
quite dif   cult in smt. instead, id4 is capable of
learning representations directly from the training
data. third, long short-term memory (hochre-
iter and schmidhuber, 1997) enables id4 to cap-

1our code is publicly available at https://github.

com/tuzhaopeng/id4-coverage.

ture long-distance reordering, which is a signi   -
cant challenge in smt.

id4 has a serious problem, however, namely
lack of coverage.
in phrase-based smt (koehn
et al., 2003), a decoder maintains a coverage vec-
tor to indicate whether a source word is translated
or not. this is important for ensuring that each
source word is translated in decoding. the decod-
ing process is completed when all source words
are    covered    or translated. in id4, there is no
such coverage vector and the decoding process
ends only when the end-of-sentence mark is pro-
duced. we believe that lacking coverage might
result in the following problems in conventional
id4:

1. over-translation: some words are unneces-

sarily translated for multiple times;

2. under-translation: some words are mistak-

enly untranslated.

speci   cally, in the state-of-the-art attention-based
id4 model (bahdanau et al., 2015), generating a
target word heavily depends on the relevant parts
of the source sentence, and a source word is in-
volved in generation of all target words. as a
result, over-translation and under-translation in-
evitably happen because of ignoring the    cover-
age    of source words (i.e., number of times a
source word is translated to a target word). fig-
ure 1(a) shows an example:
the chinese word
   gu  anb`      is over translated to    close(d)    twice,
while    b`eip`o    (means    be forced to   ) is mistak-
enly untranslated.

in this work, we propose a coverage mechanism
to id4 (id4-coverage) to alleviate the over-
translation and under-translation problems. basi-
cally, we append a coverage vector to the inter-
mediate representations of an id4 model, which
are sequentially updated after each attentive read

(a) over-translation and under-translation
generated by id4.

(b) coverage model alleviates the problems of
over-translation and under-translation.

figure 1: example translations of (a) id4 without coverage, and (b) id4 with coverage. in conven-
tional id4 without coverage, the chinese word    gu  anb`      is over translated to    close(d)    twice, while
   b`eip`o    (means    be forced to   ) is mistakenly untranslated. coverage model alleviates these problems by
tracking the    coverage    of source words.

during the decoding process, to keep track of the
attention history. the coverage vector, when en-
tering into attention model, can help adjust the fu-
ture attention and signi   cantly improve the over-
all alignment between the source and target sen-
tences. this design contains many particular cases
for coverage modeling with contrasting character-
istics, which all share a clear linguistic intuition
and yet can be trained in a data driven fashion. no-
tably, we achieve signi   cant improvement even by
simply using the sum of previous alignment prob-
abilities as coverage for each word, as a success-
ful example of incorporating linguistic knowledge
into neural network based nlp models.

experiments show that id4-coverage sig-
ni   cantly outperforms conventional attention-
based id4 on both translation and alignment
tasks. figure 1(b) shows an example, in which
id4-coverage alleviates the over-translation
and under-translation problems that id4 without
coverage suffers from.
2 background
our work is built on attention-based id4 (bah-
danau et al., 2015), which simultaneously con-
ducts dynamic alignment and generation of the
target sentence, as illustrated in figure 2.
it

figure 2: architecture of attention-based id4.
whenever possible, we omit the source index j to
make the illustration less cluttered.

produces the translation by generating one target
word yi at each time step. given an input sentence
x = {x1, . . . , xj} and previously generated words
{y1, . . . , yi   1}, the id203 of generating next
word yi is

p (yi|y<i, x) = sof tmax(cid:0)g(yi   1, ti, si)(cid:1)

(1)
where g is a non-linear function, and ti is a decod-
ing state for time step i, computed by
ti = f (ti   1, yi   1, si)

(2)
here the activation function f (  ) is a gated re-
current unit (gru) (cho et al., 2014b), and si is

a distinct source representation for time i, calcu-
lated as a weighted sum of the source annotations:

  i,j    hj

(3)

j(cid:88)

si =
      
h (cid:62)
j ;

j=1

      
h (cid:62)
j ]

(cid:62)

(4)

(5)

where hj = [
is the annotation of
xj from a bi-directional recurrent neural net-
work (id56) (schuster and paliwal, 1997), and its
weight   i,j is computed by

(cid:80)j

exp(ei,j)
k=1 exp(ei,k)

  i,j =

and

ei,j = a(ti   1, hj)

= v(cid:62)

a tanh(wati   1 + uahj)

is an attention model that scores how well yi and
hj match. with the attention model, it avoids the
need to represent the entire source sentence with
a single vector. instead, the decoder selects parts
of the source sentence to pay attention to, thus
exploits an expected annotation si over possible
alignments   i,j for each time step i.

however, the attention model fails to take ad-
vantage of past alignment information, which is
found useful to avoid over-translation and under-
translation problems in conventional smt (koehn
et al., 2003). for example, if a source word is
translated in the past, it is less likely to be trans-
lated again and should be assigned a lower align-
ment id203.

3 coverage model for id4
in smt, a coverage set is maintained to keep track
of which source words have been translated (   cov-
ered   ) in the past. let us take x = {x1, x2, x3, x4}
as an example of input sentence. the initial cov-
erage set is c = {0, 0, 0, 0} which denotes that
no source word is yet translated. when a trans-
lation rule bp = (x2x3, ymym+1) is applied, we
produce one hypothesis labelled with coverage
c = {0, 1, 1, 0}. it means that the second and third
source words are translated. the goal is to gener-
ate translation with full coverage c = {1, 1, 1, 1}.
a source word is translated when it is covered by
one translation rule, and it is not allowed to be
translated again in the future (i.e., hard coverage).
in this way, each source word is guaranteed to be
translated and only be translated once. as shown,

figure 3: architecture of coverage-based attention
model. a coverage vector ci   1 is maintained to
keep track of which source words have been trans-
lated before time i. alignment decisions   i are
made jointly taking into account past alignment
information embedded in ci   1, which lets the at-
tention model to consider more about untranslated
source words.

coverage is essential for smt since it avoids gaps
and overlaps in translation of source words.
modeling coverage is also important

for
attention-based id4 models, since they gener-
ally lack a mechanism to indicate whether a cer-
tain source word has been translated, and there-
fore are prone to the    coverage    mistakes: some
parts of source sentence have been translated more
than once or not translated. for id4 models, di-
rectly modeling coverage is less straightforward,
but the problem can be signi   cantly alleviated by
keeping track of the attention signal during the de-
coding process. the most natural way for doing
that would be to append a coverage vector to the
annotation of each source word (i.e., hj), which
is initialized as a zero vector but updated after ev-
ery attentive read of the corresponding annotation.
the coverage vector is fed to the attention model
to help adjust future attention, which lets id4
system to consider more about untranslated source
words, as illustrated in figure 3.

3.1 coverage model

since the coverage vector summarizes the atten-
tion record for hj (and therefore for a small neigh-
bor centering at the jth source word),
it will
discourage further attention to it if it has been
heavily attended, and implicitly push the atten-
tion to the less attended segments of the source
sentence since the attention weights are normal-
ized to one. this can potentially solve both cover-
age mistakes mentioned above, when modeled and
learned properly.

formally, the coverage model is given by

(cid:0)ci   1,j,   i,j,   (hj),   (cid:1)

(6)

ci,j = gupdate

where

    gupdate(  ) is the function that updates ci,j af-
ter the new attention   i,j at time step i in the
decoding process;

    ci,j is a d-dimensional coverage vector sum-
marizing the history of attention till time step
i on hj;

      (hj) is a word-speci   c feature with its own

parameters;

       are auxiliary inputs exploited in different

sorts of coverage models.

equation 6 gives a rather general model, which
could take different function forms for gupdate(  )
and   (  ), and different auxiliary inputs    (e.g.,
previous decoding state ti   1). in the rest of this
section, we will give a number of representative
implementations of the coverage model, which
either leverage more linguistic information (sec-
tion 3.1.1) or resort to the    exibility of neural net-
work approximation (section 3.1.2).
3.1.1 linguistic coverage model
we    rst consider at linguistically inspired model
which has a small number of parameters, as well
as clear interpretation. while the linguistically-
inspired coverage in id4 is similar to that in
smt, there is one key difference: it indicates what
percentage of source words have been translated
(i.e., soft coverage). in id4, each target word yi
is generated from all source words with probabil-
ity   i,j for source word xj. in other words, the
source word xj is involved in generating all tar-
get words and the id203 of generating target
word yi at time step i is   i,j. note that unlike
in smt in which each source word is fully trans-
lated at one decoding step, the source word xj is
partially translated at each decoding step in id4.
therefore, the coverage at time step i denotes the
translated ratio of that each source word is trans-
lated.

we use a scalar (d = 1) to represent linguis-
tic coverage for each source word and employ
an accumulate operation for gupdate. the initial
value of linguistic coverage is zero, which de-
notes that the corresponding source word is not

i(cid:88)

k=1

translated yet. we iteratively construct linguis-
tic coverages through accumulation of alignment
probabilities generated by the attention model,
each of which is normalized by a distinct context-
dependent weight. the coverage of source word
xj at time step i is computed by

ci,j = ci   1,j +

1
  j

  i,j =

1
  j

  k,j

(7)

where   j is a pre-de   ned weight which indicates
the number of target words xj is expected to gener-
ate. the simplest way is to follow xu et al. (2015)
in image-to-caption translation to    x    = 1 for all
source words, which means that we directly use
the sum of previous alignment probabilities with-
out id172 as coverage for each word, as
done in (cohn et al., 2016).

however, in machine translation, different types
of source words may contribute differently to the
generation of target sentence. let us take the
sentence pairs in figure 1 as an example. the
noun in the source sentence    j    ch  ang    is translated
into one target word    airports   , while the adjec-
tive    b`eip`o    is translated into three words    were
forced to   . therefore, we need to assign a dis-
tinct   j for each source word. ideally, we expect
i=1   i,j with i being the total number
of time steps in decoding. however, such desired
value is not available before decoding, thus is not
suitable in this scenario.

  j = (cid:80)i

fertility to predict   j, we introduce the con-
cept of fertility, which is    rstly proposed in word-
level smt (brown et al., 1993). fertility of source
word xj tells how many target words xj produces.
in smt, the fertility is a random variable   j,
whose distribution p(  j =   ) is determined by
the parameters of word alignment models (e.g.,
ibm models). in this work, we simplify and adapt
fertility from the original model and compute the
fertility   j by2

  j = n (xj|x) = n      (uf hj)

(8)
where n     r is a prede   ned constant to denote
the maximum number of target words one source

2fertility in smt is a random variable with a set of fer-
tility probabilities, n(  j|xj) = p(  <j, x), which depends
on the fertilities of previous source words. to simplify the
calculation and adapt it to the attention model in id4, we
de   ne the fertility in id4 as a constant number, which is
independent of previous fertilities.

dependencies.
in this work, we adopt gru for
the gating activation since it is simple yet power-
ful (chung et al., 2014). please refer to (cho et al.,
2014b) for more details about gru.

discussion intuitively, the two types of models
summarize coverage information in    different lan-
guages   . linguistic models summarize coverage
information in human language, which has a clear
interpretation to humans. neural models encode
coverage information in    neural language   , which
can be    understood    by neural networks and let
them to decide how to make use of the encoded
coverage information.

3.2

integrating coverage into id4

although attention based model has the capabil-
ity of jointly making alignment and translation, it
does not take into consideration translation his-
tory. speci   cally, a source word that has sig-
ni   cantly contributed to the generation of target
words in the past, should be assigned lower align-
ment probabilities, which may not be the case in
attention based id4. to address this problem, we
propose to calculate the alignment probabilities by
incorporating past alignment information embed-
ded in the coverage model.
intuitively, at each time step i in the decoding
phase, coverage from time step (i     1) serves as
an additional input to the attention model, which
provides complementary information of that how
likely the source words are translated in the past.
we expect the coverage information would guide
the attention model to focus more on untranslated
source words (i.e., assign higher alignment prob-
abilities).
in practice, we    nd that the coverage
model does ful   ll the expectation (see section 5).
the translated ratios of source words from lin-
guistic coverages negatively correlate to the cor-
responding alignment probabilities.

more formally, we rewrite the attention model

in equation 5 as

ei,j = a(ti   1, hj,ci   1,j)

= v(cid:62)

a tanh(wati   1 + uahj + vaci   1,j)

where ci   1,j is the coverage of source word xj be-
fore time i. va     rn  d is the weight matrix for
coverage with n and d being the numbers of hid-
den units and coverage units, respectively.

figure 4: nn-based coverage model.

word can produce,   (  ) is a logistic sigmoid func-
tion, and uf     r1  2n is the weight matrix. here
we use hj to denote (xj|x) since hj contains in-
formation about the whole input sentence with a
strong focus on the parts surrounding xj (bah-
danau et al., 2015). since   j does not depend on
i, we can pre-compute it before decoding to mini-
mize the computational cost.
3.1.2 neural network based coverage model
we next consider neural network (nn) based
coverage model. when ci,j is a vector (d > 1) and
gupdate(  ) is a neural network, we actually have
an id56 model for coverage, as illustrated in fig-
ure 4. in this work, we take the following form:

ci,j = f (ci   1,j,   i,j, hj, ti   1)

where f (  ) is a nonlinear activation function and
ti   1 is the auxiliary input that encodes past trans-
lation information. note that we leave out the
word-speci   c feature function   (  ) and only take
the input annotation hj as the input to the cov-
erage id56. it is important to emphasize that the
nn-based coverage model is able to be fed with
arbitrary inputs, such as the previous attentional
context si   1. here we only employ ci   1,j for past
alignment information, ti   1 for past translation in-
formation, and hj for word-speci   c bias.3
gating the neural function f (  ) can be either a
simple activation function tanh or a gating func-
tion that proves useful to capture long-distance
3in our preliminary experiments, considering more inputs
(e.g., current and previous attentional contexts, unnormal-
ized attention weights ei,j) does not always lead to better
translation quality. possible reasons include: 1) the inputs
contains duplicate information, and 2) more inputs introduce
more back-propagation paths and therefore make it dif   cult
to train.
in our experience, one principle is to only feed
the coverage model inputs that contain distinct information,
which are complementary to each other.

4 training
we take end-to-end learning for
the id4-
coverage model, which learns not only the pa-
rameters for the    original    id4 (i.e.,    for encod-
ing id56, decoding id56, and attention model)
but also the parameters for coverage modeling
(i.e.,    for annotation and guidance of attention) .
more speci   cally, we choose to maximize the like-
lihood of reference sentences as most other id4
models (see, however (shen et al., 2016)):

(     ,      ) = arg max

log p (yn|xn;   ,   )

(9)

n(cid:88)

  ,  

n=1

no auxiliary objective for the coverage model
with a clearer linguistic interpretation (section
3.1.1), it is possible to inject an auxiliary objec-
tive function on some intermediate representation.
more speci   cally, we may have the following ob-
jective:

(     ,      ) = arg max

(cid:40)
n(cid:88)
(cid:110) j(cid:88)
(  j     i(cid:88)
where the term(cid:8)(cid:80)j
j=1(  j    (cid:80)i

      

n=1

j=1

i=1

  ,  

log p (yn|xn;   ,   )

(cid:111)(cid:41)
i=1   i,j)2;   (cid:9) pe-

  i,j)2;   

nalizes the discrepancy between the sum of align-
ment probabilities and the expected fertility for
linguistic coverage. this is similar to the more
explicit training for fertility as in xu et al. (2015),
which encourages the model to pay equal attention
to every part of the image (i.e.,   j = 1). however,
our empirical study shows that the combined ob-
jective consistently worsens the translation quality
while slightly improves the alignment quality.

our training strategy poses less constraints on
the dependency between   j and the attention than
a more explicit strategy taken in (xu et al., 2015).
we let the objective associated with the transla-
tion quality (i.e., the likelihood) to drive the train-
ing, as in equation 9. this strategy is arguably
advantageous, since the attention weight on a hid-
den state hj cannot be interpreted as the propor-
tion of the corresponding word being translated in
the target sentence. for one thing, the hidden state
hj, after the transformation from encoding id56,
bears the contextual information from other parts
of the source sentence, and thus loses the rigid cor-
respondence with the corresponding word. there-
fore, penalizing the discrepancy between the sum

of alignment probabilities and the expected fertil-
ity does not hold in this scenario.
5 experiments
5.1 setup
we carry out experiments on a chinese-english
translation task. our training data for the trans-
lation task consists of 1.25m sentence pairs ex-
tracted from ldc corpora4 , with 27.9m chinese
words and 34.5m english words respectively. we
choose nist 2002 dataset as our development set,
and the nist 2005, 2006 and 2008 datasets as our
test sets. we carry out experiments of the align-
ment task on the evaluation dataset from (liu and
sun, 2015), which contains 900 manually aligned
chinese-english sentence pairs. we use the case-
insensitive 4-gram nist id7 score (papineni et
al., 2002) for the translation task, and the align-
ment error rate (aer) (och and ney, 2003) for
the alignment task. to better estimate the qual-
ity of the soft alignment probabilities generated
by id4, we propose a variant of aer, naming
saer:
saer = 1     |ma    ms| + |ma    mp|

|ma| + |ms|

where a is a candidate alignment, and s and p
are the sets of sure and possible links in the ref-
erence alignment respectively (s     p ). m de-
notes alignment matrix, and for both ms and mp
we assign the elements that correspond to the ex-
isting links in s and p with probabilities 1 while
assign the other elements with probabilities 0. in
this way, we are able to better evaluate the quality
of the soft alignments produced by attention-based
id4. we use sign-test (collins et al., 2005) for
statistical signi   cance test.

for ef   cient training of the neural networks, we
limit the source and target vocabularies to the most
frequent 30k words in chinese and english, cov-
ering approximately 97.7% and 99.3% of the two
corpora respectively. all the out-of-vocabulary
words are mapped to a special token unk. we set
n = 2 for the fertility model in the linguistic cov-
erages. we train each model with the sentences
of length up to 80 words in the training data. the
id27 dimension is 620 and the size of
a hidden layer is 1000. all the other settings are
the same as in (bahdanau et al., 2015).

4the corpora include ldc2002e18, ldc2003e07,
ldc2004t07,

hansards

portion

of

ldc2003e14,
ldc2004t08 and ldc2005t06.

#params mt05 mt06 mt08
# system
   
23.01
1 moses
23.23
84.3m
2 groundhog
24.84      
+1k
3 + linguistic coverage w/o fertility
24.91      
+3k
4 + linguistic coverage w/ fertility
23.31
5 + nn-based coverage w/o gating (d = 1) +4k
24.67      
6 + nn-based coverage w/ gating (d = 1)
+10k
25.23      
7 + nn-based coverage w/ gating (d = 10) +100k

31.37
30.61
31.26   
32.36      
31.94      
31.94      
32.73      

30.85
31.12
32.16      
32.31      
32.11      
32.16      
32.47      

avg.
28.41
28.32
29.42
29.86
29.12
29.59
30.14

table 1: evaluation of translation quality. d denotes the dimension of nn-based coverages, and     and    
indicate statistically signi   cant difference (p < 0.01) from groundhog and moses, respectively.    +    is
on top of the baseline system groundhog.

we compare our method with two state-of-the-

art models of smt and id45:

    moses (koehn et al., 2007): an open source
phrase-based translation system with default
con   guration and a 4-gram language model
trained on the target portion of training data.
    groundhog (bahdanau et al., 2015): an

attention-based id4 system.

5.2 translation quality
table 1 shows the translation performances mea-
sured in id7 score. clearly the proposed id4-
coverage signi   cantly improves the translation
quality in all cases, although there are still consid-
erable differences among different variants.
parameters coverage model introduces few pa-
rameters. the baseline model (i.e., groundhog)
has 84.3m parameters. the linguistic coverage
using fertility introduces 3k parameters (2k for
fertility model), and the nn-based coverage with
gating introduces 10k  d parameters (6k  d for
gating), where d is the dimension of the coverage
vector. in this work, the most complex coverage
model only introduces 0.1m additional parame-
ters, which is quite small compared to the number
of parameters in the existing model (i.e., 84.3m).
speed introducing the coverage model slows
down the training speed, but not signi   cantly.
when running on a single gpu device tesla k80,
the speed of the baseline model is 960 target words
per second. system 4 (   +linguistic coverage with
fertility   ) has a speed of 870 words per second,
while system 7 (   +nn-based coverage (d=10)   )
achieves a speed of 800 words per second.

5there are recent progress on aggregating multiple mod-
els or enlarging the vocabulary(e.g., in (jean et al., 2015)),
but here we focus on the generic models.

linguistic coverages
(rows 3 and 4): two
observations can be made. first,
the simplest
linguistic coverage (row 3) already signi   cantly
improves translation performance by 1.1 id7
points,
indicating that coverage information is
very important to the attention model. second, in-
corporating fertility model boosts the performance
by better estimating the covered ratios of source
words.

nn-based coverages
(rows 5-7): (1) gating
(rows 5 and 6): both variants of nn-based cover-
ages outperform groundhog with averaged gains
of 0.8 and 1.3 id7 points, respectively.
in-
troducing gating activation function improves the
performance of coverage models, which is consis-
tent with the results in other tasks (chung et al.,
2014). (2) coverage dimensions (rows 6 and 7):
increasing the dimension of coverage models fur-
ther improves the translation performance by 0.6
point in id7 score, at the cost of introducing
more parameters (e.g., from 10k to 100k).6

subjective evaluation we also conduct a sub-
jective evaluation to validate the bene   t of in-
corporating coverage. two human evaluators are
asked to evaluate the translations of 200 source
sentences randomly sampled from the test sets
without knowing from which system a translation
is selected. table 2 shows the results of subjec-
tive evaluation on translation adequacy and    u-
ency.7 groudhog has a low adequacy since 25.0%
of the source words are under-translated. this is

6in a pilot study, further increasing the coverage dimen-
sion only slightly improved the translation performance. one
possible reason is that encoding the relatively simple cover-
age information does not require too many dimensions.

7fluency measures whether the translation is    uent, while
adequacy measures whether the translation is faithful to the
original sentence (snover et al., 2009).

model
groundhog
+ nn cov. w/ gating (d = 10)

adequacy fluency under-
25.0%
16.7%

3.54
3.73

3.06
3.28

translation

over-
translation

4.5%
2.7%

table 2: subjective evaluation of translation adequacy and    uency. the numbers in the last two columns
denote the percentages of source words are under-translated and over-translated, respectively.

(a) groundhog

(b) + nn cov. w/ gating (d = 10)

figure 5: example alignments. using coverage mechanism, translated source words are less likely to
contribute to generation of the target words next (e.g., top-right corner for the    rst four chinese words.).

system
groundhog
+ ling. cov. w/o fertility
+ ling. cov. w/ fertility
+ nn cov. w/o gating (d = 1)
+ nn cov. w/ gating (d = 1)
+ nn cov. w/ gating (d = 10)

saer aer
67.00
54.67
53.55
66.75
52.13
64.85
67.10
54.46
53.51
66.30
64.25
50.50

table 3: evaluation of alignment quality. the
lower the score, the better the alignment quality.

mainly due to the serious under-translation prob-
lems on long sentences that consist of several
sub-sentences, in which some sub-sentences are
completely ignored. incorporating coverage sig-
ni   cantly alleviates these problems, and reduces
33.2% and 40.0% of under-translation and over-
translation errors respectively. bene   ting from
this, coverage model improves both translation ad-
equacy and    uency by around 0.2 points.

5.3 alignment quality
table 3 lists the alignment performances. we
   nd that coverage information improves atten-
tion model as expected by maintaining an annota-

tion summarizing attention history on each source
word. more speci   cally, linguistic coverage with
fertility signi   cantly reduces alignment errors un-
der both metrics, in which fertility plays an impor-
tant role. nn-based coverages, however, does not
signi   cantly reduce alignment errors until increas-
ing the coverage dimension from 1 to 10. it in-
dicates that nn-based models need slightly more
dimensions to encode the coverage information.

figure 5 shows an example. the coverage
mechanism does meet the expectation: the align-
ments are more concentrated and most impor-
tantly, translated source words are less likely to
get involved in generation of the target words next.
for example, the    rst four chinese words are as-
signed lower alignment probabilities (i.e., darker
color) after the corresponding translation    roma-
nia reinforces old buildings    is produced.

5.4 effects on long sentences

following bahdanau et al. (2015), we group sen-
tences of similar lengths together and compute
id7 score and averaged length of translation
for each group, as shown in figure 6. cho et
al. (2014a) show that the performance of ground-

figure 6: performance of the generated translations with respect to the lengths of the input sentences.
coverage models alleviate under-translation by producing longer translations on long sentences.

hog drops rapidly when the length of input sen-
tence increases. our results con   rm these    nd-
ings. one main reason is that groundhog pro-
duces much shorter translations on longer sen-
tences (e.g., > 40, see right panel in figure 6),
and thus faces a serious under-translation prob-
lem. id4-coverage alleviates this problem by
incorporating coverage information into the atten-
tion model, which in general pushes the attention
to untranslated parts of the source sentence and
implicitly discourages early stop of decoding. it
is worthy to emphasize that both nn-based cov-
erages (with gating, d = 10) and linguistic cover-
ages (with fertility) achieve similar performances
on long sentences, recon   rming our claim that the
two variants improve the attention model in their
own ways.

as an example, consider this source sentence in

the test set:

qi  aod  an b  en s`aij`   p    ngj  un d  ef  en 24.3f  en
, t  a z`ai s  an zh  ou qi  an ji  esh`ou sh  oush`u
, qi  udu`   z`ai c     q    ji  an 4 sh`eng 8 f`u .
groundhog translates this sentence into:

jordan achieved an average score of
eight weeks ahead with a surgical oper-
ation three weeks ago .

in which the sub-sentence    , qi  udu`   z`ai c     q    ji  an
4 sh`eng 8 f`u    is under-translated. with the (nn-
based) coverage mechanism, id4-coverage
translates it into:

jordan    s average score points to unk
this year . he received surgery before
three weeks , with a team in the period
of 4 to 8 .

in which the under-translation is recti   ed.

the quantitative and qualitative results show
that the coverage models indeed help to allevi-
ate under-translation, especially for long sentences
consisting of several sub-sentences.

6 related work

our work is inspired by recent works on im-
proving attention-based id4 with techniques that
have been successfully applied to smt. follow-
ing the success of minimum risk training (mrt)
in smt (och, 2003), shen et al. (2016) proposed
mrt for end-to-end id4 to optimize model pa-
rameters directly with respect to evaluation met-
rics. based on the observation that attention-
based id4 only captures partial aspects of atten-
tional regularities, cheng et al. (2016) proposed
agreement-based learning (liang et al., 2006) to
encourage bidirectional id12 to agree
on parameterized alignment matrices. along the
same direction, inspired by the coverage mecha-
nism in smt, we propose a coverage-based ap-
proach to id4 to alleviate the over-translation
and under-translation problems.

independent from our work, cohn et al. (2016)
and feng et al. (2016) made use of the concept of

   fertility    for the attention model, which is sim-
ilar in spirit to our method for building the lin-
guistically inspired coverage with fertility. cohn
et al. (2016) introduced a feature-based fertility
that includes the total alignment scores for the sur-
rounding source words. in contrast, we make pre-
diction of fertility before decoding, which works
as a normalizer to better estimate the coverage ra-
tio of each source word. feng et al. (2016) used
the previous attentional context to represent im-
plicit fertility and passed it to the attention model,
which is in essence similar to the input-feed
method proposed in (luong et al., 2015). compar-
atively, we predict explicit fertility for each source
word based on its encoding annotation, and incor-
porate it into the linguistic-inspired coverage for
attention model.
7 conclusion
we have presented an approach for enhancing
id4, which maintains and utilizes a coverage
vector to indicate whether each source word is
translated or not. by encouraging id4 to pay less
attention to translated words and more attention to
untranslated words, our approach alleviates the se-
rious over-translation and under-translation prob-
lems that traditional attention-based id4 suffers
from. we propose two variants of coverage mod-
linguistic coverage that leverages more lin-
els:
guistic information and nn-based coverage that
resorts to the    exibility of neural network approx-
imation . experimental results show that both
variants achieve signi   cant improvements in terms
of translation quality and alignment quality over
id4 without coverage.
acknowledgement
this work is supported by china national 973
project 2014cb340301. yang liu is supported
by the national natural science foundation of
china (no.
61522204) and the 863 program
(2015aa011808). we thank the anonymous re-
viewers for their insightful comments.

references
[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2015. neural machine
translation by jointly learning to align and translate.
iclr 2015.

[bengio et al.2003] yoshua bengio, r  ejean ducharme,
pascal vincent, and christian janvin. 2003. a neu-
ral probabilistic language model. jmlr.

[brown et al.1993] peter e. brown, stephen a. della
pietra, vincent j. della pietra, and robert l. mer-
cer. 1993. the mathematics of statistical machine
translation: parameter estimation. computational
linguistics, 19(2):263   311.

[cheng et al.2016] yong cheng, shiqi shen, zhongjun
he, wei he, hua wu, maosong sun, and yang liu.
2016. agreement-based joint training for bidirec-
tional attention-based id4.
in ijcai 2016.

[chiang2007] david chiang.

phrase-based translation. cl.

2007.

hierarchical

[cho et al.2014a] kyunghyun cho, bart van merrien-
boer, dzmitry bahdanau, and yoshua bengio.
2014a. on the properties of neural machine trans-
lation: encoder   decoder approaches. in ssst 2014.

[cho et al.2014b] kyunghyun cho, bart van merrien-
boer, caglar gulcehre, fethi bougares, holger
schwenk, and yoshua bengio. 2014b. learning
phrase representations using id56 encoder-decoder
for id151. in emnlp 2014.

[chung et al.2014] junyoung chung, caglar gulcehre,
kyunghyun cho, and yoshua bengio. 2014. em-
pirical evaluation of gated recurrent neural networks
on sequence modeling. arxiv.

[cohn et al.2016] trevor cohn, cong duy vu hoang,
ekaterina vylomova, kaisheng yao, chris dyer,
and gholamreza haffari.
incorporating
structural alignment biases into an attentional
neural translation model. in naacl 2016.

2016.

[collins et al.2005] michael collins, philipp koehn,
and ivona ku  cerov  a. 2005. clause restructuring for
id151. in acl 2005.

[feng et al.2016] shi feng, shujie liu, mu li, and
ming zhou. 2016.
implicit distortion and fertil-
ity models for attention-based encoder-decoder id4
model. arxiv.

[hochreiter and schmidhuber1997] sepp hochreiter
and j  urgen schmidhuber. 1997. long short-term
memory. neural computation.

[jean et al.2015] s  ebastien jean, kyunghyun cho,
roland memisevic, and yoshua bengio. 2015. on
using very large target vocabulary for neural ma-
chine translation. in acl 2015.

[koehn et al.2003] philipp koehn, franz josef och,
and daniel marcu. 2003. statistical phrase-based
translation. in naacl 2003.

[koehn et al.2007] philipp koehn, hieu hoang,
alexandra birch, chris callison-burch, marcello
federico, nicola bertoldi, brooke cowan, wade
shen, christine moran, richard zens, chris dyer,
ondrej bojar, alexandra constantin, and evan
herbst.
2007. moses: open source toolkit for
id151. in acl 2007.

[liang et al.2006] percy liang, ben taskar, and dan
klein. 2006. alignment by agreement. in naacl
2006.

[liu and sun2015] yang liu and maosong sun. 2015.
contrastive unsupervised word alignment with non-
local features. in aaai 2015.

[luong et al.2015] minh-thang luong, hieu pham,
and christopher d manning. 2015. effective ap-
proaches to attention-based neural machine transla-
tion. in emnlp 2015.

[och and ney2003] franz j. och and hermann ney.
2003. a systematic comparison of various statisti-
cal alignment models. computational linguistics,
29(1):19   51.

[och2003] franz josef och. 2003. minimum error rate
in acl

training in id151.
2003.

[papineni et al.2002] kishore papineni, salim roukos,
todd ward, and wei-jing zhu.
2002. id7: a
method for automatic evaluation of machine trans-
lation. in acl 2002.

[schuster and paliwal1997] mike

and
kuldip k paliwal.
1997. bidirectional recur-
rent neural networks. ieee transactions on signal
processing, 45(11):2673   2681.

schuster

[shen et al.2016] shiqi shen, yong cheng, zhongjun
he, wei he, hua wu, maosong sun, and yang liu.
2016. minimum risk training for neural machine
translation. in acl 2016.

[snover et al.2009] matthew snover, nitin madnani,
bonnie j dorr, and richard schwartz. 2009. flu-
ency, adequacy, or hter?: exploring different hu-
man judgments with a tunable mt metric. in wmt
2009, pages 259   268.

[sutskever et al.2014] ilya sutskever, oriol vinyals,
and quoc vv le. 2014. sequence to sequence
learning with neural networks. in nips 2014.

[xu et al.2015] kelvin xu, jimmy ba, ryan kiros,
kyunghyun cho, aaron courville, ruslan salakhut-
dinov, richard zemel, and yoshua bengio. 2015.
show, attend and tell: neural image caption gen-
eration with visual attention. in icml 2015.

