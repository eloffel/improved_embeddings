dialog-based language learning

jason weston

facebook ai research,

new york.

6
1
0
2

 
t
c
o
4
2

 

 
 
]
l
c
.
s
c
[
 
 

7
v
5
4
0
6
0

.

4
0
6
1
:
v
i
x
r
a

jase@fb.com

abstract

a long-term goal of machine learning research is to build an intelligent dialog
agent. most research in natural language understanding has focused on learning
from    xed training sets of labeled data, with supervision either at the word level
(tagging, parsing tasks) or sentence level (id53, machine transla-
tion). this kind of supervision is not realistic of how humans learn, where lan-
guage is both learned by, and used for, communication. in this work, we study
dialog-based language learning, where supervision is given naturally and implic-
itly in the response of the dialog partner during the conversation. we study this
setup in two domains: the babi dataset of [30] and large-scale id53
from [4]. we evaluate a set of baseline learning strategies on these tasks, and show
that a novel model incorporating predictive lookahead is a promising approach for
learning from a teacher   s response. in particular, a surprising result is that it can
learn to answer questions correctly without any reward-based supervision at all.

1

introduction

many of machine learning   s successes have come from supervised learning, which typically involves
employing annotators to label large quantities of data per task. however, humans can learn by acting
and learning from the consequences of (i.e, the feedback from) their actions. when humans act in
dialogs (i.e., make speech utterances) the feedback is from other human   s responses, which hence
contain very rich information. this is perhaps most pronounced in a student/teacher scenario where
the teacher provides positive feedback for successful communication and corrections for unsuccess-
ful ones [11, 28]. however, in general any reply from a dialog partner, teacher or not, is likely to
contain an informative training signal for learning how to use language in subsequent conversations.
in this paper we explore whether we can train machine learning models to learn from dialogs. the
ultimate goal is to be able to develop an intelligent dialog agent that can learn while conducting con-
versations. to do that it needs to learn from feedback that is supplied as natural language. however,
most machine learning tasks in the natural language processing literature are not of this form: they
are either hand labeled at the word level (id52, id39), segment
(chunking) or sentence level (id53) by labelers. subsequently, learning algorithms
have been developed to learn from that kind of supervision. we therefore need to develop evaluation
datasets for the dialog-based language learning setting, as well as developing models and algorithms
able to learn in such a regime.
the contribution of the present work is thus:

feasibility of dialog-based language learning.

    we introduce a set of tasks that model natural feedback from a teacher and hence assess the
    we evaluate some baseline models on this data, comparing to standard supervised learning.
    we introduce a novel forward prediction model, whereby the learner tries to predict the
teacher   s replies to its actions, yielding promising results, even with no reward signal at all.

2 related work

in human language learning the usefulness of social interaction and natural infant directed conversa-
tions is emphasized, see e.g. the review paper [9], although the usefulness of feedback for learning
grammar is disputed [13]. support for the usefulness of feedback is found however in second lan-
guage learning [1] and learning by students [6, 11, 28].
in machine learning, one line of research has focused on supervised learning from dialogs using
neural models [22, 4]. id53 given either a database of knowledge [2] or short stories
[30] can be considered as a simple case of dialog which is easy to evaluate. those tasks typically
do not consider feedback. there is work on the the use of feedback and dialog for learning, notably
for collecting knowledge to answer questions [8, 17], the use of natural language instruction for
learning symbolic rules [10, 5] and the use of binary feedback (rewards) for learning parsers [3].
another setting which uses feedback is the setting of id23, see e.g. [19, 20] for
a summary of its use in dialog. however, those approaches often consider reward as the feedback
model rather than exploiting the dialog feedback per se. nevertheless, id23 ideas
have been used to good effect for other tasks as well, such as understanding text adventure games
[15], image captioning [32], machine translation and summarization [18]. recently, [14] also pro-
posed a reward-based learning framework for learning how to learn.
finally, forward prediction models, which we make use of in this work, have been used for learn-
ing eye tracking [21], controlling robot arms [12] and vehicles [27], and action-conditional video
prediction in atari games [16, 23]. we are not aware of their use thus far for dialog.

3 dialog-based supervision tasks

dialog-based supervision comes in many forms. as far as we are aware it is a currently unsolved
problem which type of learning strategy will work in which setting. in this section we therefore
identify different modes of dialog-based supervision, and build a learning problem for each. the
goal is to then evaluate learners on each type of supervision.
we thus begin by selecting two existing datasets: (i) the single supporting fact problem from the
babi datasets [30] which consists of short stories from a simulated world followed by questions;
and (ii) the movieqa dataset [4] which is a large-scale dataset (    100k questions over     75k
entities) based on questions with answers in the open movie database (omdb). for each dataset
we then consider ten modes of dialog-based supervision. the supervision modes are summarized
in fig. 1 using a snippet of the babi dataset as an example. the same setups are also used for
movieqa, some examples of which are given in fig 2.
we now describe each supervision setup in turn.

imitating an expert student
in task 1 the dialogs take place between a teacher and an expert
student who gives semantically coherent answers. hence, the task is for the learner to imitate that
expert student, and become an expert themselves. for example, imagine the real-world scenario
where a child observes their two parents talking to each other, it can learn but it is not actually
taking part in the conversation. note that our main goal in this paper is to examine how a non-expert
can learn to improve its dialog skills while conversing. the rest of our tasks will hence concentrate
on that goal. this task can be seen as a natural baseline for the rest of our tasks given the same input
dialogs and questions.

positive and negative feedback in task 2, when the learner answers a question the teacher then
replies with either positive or negative feedback. in our experiments the subsequent responses are
variants of    no, that   s incorrect    or    yes, that   s right   . in the datasets we build there are 6 templates
for positive feedback and 6 templates for negative feedback, e.g.    sorry, that   s not it.   ,    wrong   ,
etc. to separate the notion of positive from negative (otherwise the signal is just words with no
notion that yes is better than no) we assume an additional external reward signal that is not part
of the text. as shown in fig. 1 task 2, (+) denotes positive reward external to the dialog (e.g.
feedback provided by another medium, such as a nod of the head from the teacher). this is provided
with every positive response. note the difference in supervision compared to task 1: there every
answer is right and provides positive supervision. here, only the answers the learner got correct

2

figure 1: sample dialogs with differing supervision signals (tasks 1 to 10). in each case the same
example is given for simplicity. black text is spoken by the teacher, red text denotes responses by
the learner, blue text is provided by an expert student (which the learner can imitate), (+) denotes
positive reward external to the dialog (e.g. feedback provided by another medium, such as a nod of
the head from the teacher).

task 1: imitating an expert student
mary went to the hallway.
john moved to the bathroom.
mary travelled to the kitchen.
where is mary? a:kitchen
where is john? a:bathroom

task 3: answers supplied by teacher
mary went to the hallway.
john moved to the bathroom.
mary travelled to the kitchen.
where is mary? a:bedroom
no, the answer is kitchen.
where is john? a:bathroom
correct! (+)
task 5: supporting facts supplied by teacher
mary went to the hallway.
john moved to the bathroom.
mary travelled to the kitchen.
where is mary? a:kitchen
yes, that   s right! (+)
where is john? a:hallway
no, because john moved to the bathroom.
task 7: no feedback
mary went to the hallway.
john moved to the bathroom.
mary travelled to the kitchen.
where is mary? a:kitchen
yes, that   s right!
where is john? a:bathroom
yes, that   s correct!
task 9: asking for corrections
mary went to the hallway.
john moved to the bathroom.
mary travelled to the kitchen.
where is mary? a:kitchen
yes, that   s right! (+)
where is john? a:hallway
no, that   s not right. a:can you help me?
bathroom.

task 2: positive and negative feedback
mary went to the hallway.
john moved to the bathroom.
mary travelled to the kitchen.
where is mary? a:playground
no, that   s incorrect.
where is john? a:bathroom
yes, that   s right! (+)
task 4: hints supplied by teacher
mary went to the hallway.
john moved to the bathroom.
mary travelled to the kitchen.
where is mary? a:bathroom
no, they are downstairs.
where is john? a:kitchen
no, they are upstairs.
task 6: partial feedback
mary went to the hallway.
john moved to the bathroom.
mary travelled to the kitchen.
where is mary? a:kitchen
yes, that   s right!
where is john? a:bathroom
yes, that   s correct! (+)
task 8: imitation and feedback mixture
mary went to the hallway.
john moved to the bathroom.
mary travelled to the kitchen.
where is mary? a:kitchen
where is john? a:bathroom
that   s right! (+)

task 10: asking for supporting facts
mary went to the hallway.
john moved to the bathroom.
mary travelled to the kitchen.
where is mary? a:kitchen
yes, that   s right! (+)
where is john? a:hallway
no, that   s not right. a:can you help me?
a relevant fact is john moved to the bathroom.

have positive supervision. this could clearly be a problem when the learner is unskilled: it will
supply incorrect answers and never (or hardly ever) receive positive responses.

answers supplied by teacher
in task 3 the teacher gives positive and negative feedback as in
task 2, however when the learner   s answer is incorrect, the teacher also responds with the correction.
for example if    where is mary?    is answered with the incorrect answer    bedroom    the teacher
responds    no, the answer is kitchen      , see fig. 1 task 3. if the learner knows how to use this extra
information, it effectively has as much supervision signal as with task 1, and much more than for
task 2.

hints supplied by teacher
in task 4, the corrections provided by the teacher do not provide
the exact answer as in task 3, but only a useful hint. this setting is meant to mimic the real life

3

figure 2: samples from the movieqa dataset [4]. in our experiments we consider 10 different
language learning setups as described in figure 1 and sec. 3. the examples given here are for tasks
2 and 3, questions are in black and answers in red, and (+) indicates receiving positive reward.

task 2: positive and negative feedback
what movies are about open source? revolution os
that   s right! (+)
what movies did darren mcgavin star in? carmen
sorry, that   s not it.
who directed the    lm white elephant? m. curtiz
no, that is incorrect.

task 3: answers supplied by teacher
what    lms are about hawaii? 50 first dates
correct! (+)
who acted in licence to kill? billy madison
no, the answer is timothy dalton.
what genre is saratoga trunk in? drama
yes! (+)

occurrence of being provided only partial information about what you did wrong. in our datasets
we do this by providing the class of the correct answer, e.g.    no, they are downstairs    if the answer
should be kitchen, or    no, it is a director    for the question    who directed monsters, inc.?    (using
omdb metadata). the supervision signal here is hence somewhere in between task 2 and 3.

supporting facts supplied by teacher
in task 5, another way of providing partial supervision
for an incorrect answer is explored. here, the teacher gives a reason (explanation) why the answer
is wrong by referring to a known fact that supports the true answer that the incorrect answer may
contradict. for example    no, because john moved to the bathroom    for an incorrect answer to
   where is john?   , see fig. 1 task 5. this is related to what is termed strong supervision in [30]
where supporting facts and answers are given for id53 tasks.

partial feedback task 6 considers the case where external rewards are only given some of (50%
of) the time for correct answers, the setting is otherwise identical to task 3. this attempts to mimic
the realistic situation of some learning being more closely supervised (a teacher rewarding you for
getting some answers right) whereas other dialogs have less supervision (no external rewards). the
task attempts to assess the impact of such partial supervision.

no feedback in task 7 external rewards are not given at all, only text, but is otherwise identical to
tasks 3 and 6. this task explores whether it is actually possible to learn how to answer at all in such
a setting. we    nd in our experiments the answer is surprisingly yes, at least in some conditions.

imitation and feedback mixture task 8 combines tasks 1 and 2. the goal is to see if a learner
can learn successfully from both forms of supervision at once. this mimics a child both observing
pairs of experts talking (task 1) while also trying to talk (task 2).

asking for corrections another natural way of collecting supervision is for the learner to ask
questions of the teacher about what it has done wrong. task 9 tests one of the most simple instances,
where asking    can you help me?    when wrong obtains from the teacher the correct answer. this is
thus related to the supervision in task 3 except the learner must    rst ask for help in the dialog. this
is potentially harder for a model as the relevant information is spread over a larger context.

asking for supporting facts finally, in task 10, a second less direct form of supervision for the
learner after asking for help is to receive a hint rather than the correct answer, such as    a relevant
fact is john moved to the bathroom    when asking    can you help me?   , see fig. 1 task 10. this is
thus related to the supervision in task 5 except the learner must request help.

in our experiments we constructed the ten supervision tasks for the two datasets which are all avail-
able for download at http://fb.ai/babi. they were built in the following way: for each task we
consider a    xed policy1 for performing actions (answering questions) which gets questions correct
with id203   acc (i.e.
the chance of getting the red text correct in figs. 1 and 2). we thus
can compare different learning algorithms for each task over different values of   acc (0.5, 0.1 and
0.01). in all cases a training, validation and test set is provided. for the babi dataset this consists of

1 since the policy is    xed and actually does not depend on the model being learnt, one could also think of it

as coming from another agent (or the same agent in the past) which in either case is an imperfect expert.

4

1000, 100 and 1000 questions respectively per task, and for movieqa there are     96k,     10k and
    10k respectively. movieqa also includes a knowledge base (kb) of     85k facts from omdb,
the memory network model we employ uses inverted index retrieval based on the question to form
relevant memories from this set, see [4] for more details. note that because the policies are    xed the
experiments in this paper are not in a id23 setting.

figure 3: architectures for (reward-based) imitation and forward prediction.

(a) model for (reward-based) imitation learning

(b) model for forward prediction

4 learning models
our main goal is to explore training strategies that can execute dialog-based language learning. to
this end we evaluate four possible strategies: imitation learning, reward-based imitation, forward
prediction, and a combination of reward-based imitation and forward prediction. we will subse-
quently describe each in turn.
we test all of these approaches with the same model architecture: an end-to-end memory network
(memn2n) [26]. memory networks [29, 26] are a recently introduced model that have been shown
to do well on a number of text understanding tasks, including id53 and dialog [4, 2],
id38 [26] and sentence completion [7]. in particular, they outperform lstms and
other baselines on the babi datasets [30] which we employ with dialog-based learning modi   cations
in sec. 3. they are hence a natural baseline model for us to use in order to explore differing modes
of learning in our setup. in the following we will    rst review memory networks, detailing the explicit
choices of architecture we made, and then show how they can be modi   ed and applied to our setting
of dialog-based language learning.

memory networks a high-level description of the memory network architecture we use is given
in fig. 3 (a). the input is the last utterance of the dialog, x, as well as a set of memories (context)
(c1, . . . , cn ) which can encode both short-term memory, e.g. recent previous utterances and replies,
and long-term memories, e.g. facts that could be useful for answering questions. the context inputs
ci are converted into vectors mi via embeddings and are stored in the memory. the goal is to
produce an output   a by processing the input x and using that to address and read from the memory,
m, possibly multiple times, in order to form a coherent reply. in the    gure the memory is read twice,
which is termed multiple    hops    of attention.
in the    rst step, the input x is embedded using a matrix a of size d    v where d is the embedding
dimension and v is the size of the vocabulary, giving q = ax, where the input x is as a bag-of-
words vector. each memory ci is embedded using the same matrix, giving mi = aci. the output of
addressing and then reading from memory in the    rst hop is:

(cid:88)

o1 =

p1
i mi, p1

i = softmax(q(cid:62)mi).

here, the match between the input and the memories is computed by taking the inner product fol-
lowed by a softmax, yielding p1, giving a id203 vector over the memories. the goal is to select

i

5

memory module controller module input addressing read addressing read internal state vector (initially: query) output	
   memory vectors supervision  (direct or reward-based)  mmqmemory module controller module input output predict response to answer  addressing read addressing read internal state vector (initially: query) addressing candidate(answers(read memory vectors mmqqanswer (action taken)memories relevant to the last utterance x, i.e. the most relevant have large values of p1
i . the output
memory representation o1 is then constructed using the weighted sum of memories, i.e. weighted
by p1. the memory output is then added to the original input, u1 = r1(o1 + q), to form the new
state of the controller, where r1 is a d   d rotation matrix2. the attention over the memory can then
be repeated using u1 instead of q as the addressing vector, yielding:

(cid:88)

o2 =

p2
i mi, p2

i = softmax(u(cid:62)

1 mi),

i

the controller state is updated again with u2 = r2(o2 + u1), where r2 is another d    d matrix to
be learnt. in a two-hop model the    nal output is then de   ned as:
2 ay1, . . . , u(cid:62)

(1)
where there are c candidate answers in y. in our experiments c is the set of actions that occur in
the training set for the babi tasks, and for movieqa it is the set of words retrieved from the kb.
having described the basic architecture, we now detail the possible training strategies we can employ
for our tasks.

  a = softmax(u(cid:62)

2 ayc)

imitation learning this approach involves simply imitating one of the speakers in observed di-
alogs, which is essentially a supervised learning objective3. this is the setting that most existing di-
alog learning, as well as question answer systems, employ for learning. examples arrive as (x, c, a)
triples, where a is (assumed to be) a good response to the last utterance x given context c. in our
case, the whole memory network model de   ned above is trained using stochastic id119
by minimizing a standard cross-id178 loss between   a and the label a.

reward-based imitation if some actions are poor choices, then one does not want to repeat
them, that is we shouldn   t treat them as a supervised objective. in our setting positive reward is
only obtained immediately after (some of) the correct actions, or else is zero. a simple strategy is
thus to only apply imitation learning on the rewarded actions. the rest of the actions are simply
discarded from the training set. this strategy is derived naturally as the degenerate case one obtains
by applying policy gradient [31] in our setting where the policy is    xed (see end of sec. 3). in more
complex settings (i.e. where actions that are made lead to long-term changes in the environment and
delayed rewards) applying id23 algorithms would be necessary, e.g. one could
still use policy gradient to train the memn2n but applied to the model   s own policy, as used in [25].

forward prediction an alternative method of training is to perform forward prediction: the aim
is, given an utterance x from speaker 1 and an answer a by speaker 2 (i.e., the learner), to predict
  x, the response to the answer from speaker 1. that is, in general to predict the changed state of the
world after action a, which in this case involves the new utterance   x.
to learn from such data we propose the following modi   cation to memory networks, also shown
in fig. 3 (b): essentially we chop off the    nal output from the original network of fig. 3 (a) and
replace it with some additional layers that compute the forward prediction. the    rst part of the
network remains exactly the same and only has access to input x and context c, just as before. the
computation up to u2 = r2(o2 + u1) is thus exactly the same as before.
at this point we observe that the computation of the output in the original network, by scoring
candidate answers in eq.
(1) looks similar to the addressing of memory. our key idea is thus
to perform another    hop    of attention but over the candidate answers rather than the memories.
crucially, we also incorporate the information of which action (candidate) was actually selected in
the dialog (i.e. which one is a). after this    hop   , the resulting state of the controller is then used to
do the forward prediction.
concretely, we compute:

o3 =

i (ayi +      [a = yi]),
p3

i = softmax(u(cid:62)
p3

2 ayi),

(2)

i

2optionally, different dictionaries can be used for inputs, memories and outputs instead of being shared.
3imitation learning algorithms are not always strictly supervised algorithms, they can also depend on the
agent   s actions. that is not the setting we use here, where the task is to imitate one of the speakers in a dialog.

6

(cid:88)

table 1: test accuracy (%) on the single supporting fact babi dataset for various supervision
approachess (training with 1000 examples on each) and different policies   acc. a task is successfully
passed if     95% accuracy is obtained (shown in blue).

memn2n
rbi + fp

memn2n
imitation
learning

memn2n

reward-based
imitation (rbi)
0.01
0.5
100
100
91
99
92
99
90
99
100
83
59
98
29
20
98
99
83
99
84
99
9
2

0.1
100
92
96
91
96
81
22
98
89
96
5

memn2n
forward

prediction (fp)
0.01
0.5
29
23
30
93
99
99
66
97
98
100
99
100
99
100
67
28
21
23
48
23
5
4

0.1
30
54
96
99
99
100
98
64
15
30
5

supervision type

  acc =

1 - imitating an expert student
2 - positive and negative feedback
3 - answers supplied by teacher
4 - hints supplied by teacher
5 - supporting facts supplied by teacher
6 - partial feedback
7 - no feedback
8 - imitation + feedback mixture
9 - asking for corrections
10 - asking for supporting facts
number of completed tasks (    95%)
where       is a d-dimensional vector, that is also learnt, that represents in the output o3 the action that
was actually selected. after obtaining o3, the forward prediction is then computed as:

0.1
100
28
37
23
24
22
34
89
30
25
1

0.01
100
21
25
22
27
22
19
82
22
26
1

0.01
100
96
98
100
100
99
99
97
84
91
8

0.5
100
79
83
85
84
90
90
90
85
86
1

0.5
99
99
99
99
100
99
98
99
95
97
10

0.1
99
92
100
100
99
100
99
98
90
95
8

  x = softmax(u(cid:62)

3 a  x1, . . . , u(cid:62)

3 a  x   c)

where u3 = r3(o3 + u2). that is, it computes the scores of the possible responses to the answer a
over   c possible candidates. the mechanism in eq. (2) gives the model a way to compare the most
likely answers to x with the given answer a, which in terms of supervision we believe is critical. for
example in id53 if the given answer a is incorrect and the model can assign high pi to
the correct answer then the output o3 will contain a small amount of      ; conversely, o3 has a large
amount of       if a is correct. thus, o3 informs the model of the likely response   x from the teacher.
training can then be performed using the cross-id178 loss between   x and the label   x, similar to
before. in the event of a large number of candidates   c we subsample the negatives, always keeping
  x in the set. the set of answers y can also be similarly sampled, making the method highly scalable.
a major bene   t of this particular architectural design for forward prediction is that after training
with the forward prediction criterion, at test time one can    chop off    the top again of the model to
retrieve the original memory network model of fig. 3 (a). one can thus use it to predict answers   a
given only x and c. we can thus evaluate its performance directly for that goal as well.
finally, and importantly, if the answer to the response   x carries pertinent supervision information
for choosing   a, as for example in many of the settings of sec. 3 (and fig. 1), then this will be
backpropagated through the model. this is simply not the case in the imitation, reward-shaping [24]
or reward-based imitation learning strategies which concentrate on the x, a pairs.

reward-based imitation + forward prediction as our reward-based imitation learning uses the
architecture of fig. 3 (a), and forward prediction uses the same architecture but with the additional
layers of fig 3 (b), we can learn jointly with both strategies. one simply shares the weights across
the two networks, and performs gradient steps for both criteria, one of each type per action. the
former makes use of the reward signal     which when available is a very useful signal     but fails to
use potential supervision feedback in the subsequent utterances, as described above. it also effec-
tively ignores dialogs carrying no reward. forward prediction in contrast makes use of dialog-based
feedback and can train without any reward. on the other hand not using rewards when available is a
serious handicap. hence, the mixture of both strategies is a potentially powerful combination.

5 experiments

we conducted experiments on the datasets described in section 3. as described before, for each
task we consider a    xed policy for performing actions (answering questions) which gets questions
correct with id203   acc. we can thus compare the different training strategies described in sec.
4 over each task for different values of   acc. hyperparameters for all methods are optimized on the
validation sets. a summary of the results is reported in table 1 for the babi dataset and table 2 for
movieqa. we observed the following results:

7

table 2: test accuracy (%) on the movieqa dataset dataset for various supervision approaches.
numbers in bold are the winners for that task and choice of   acc.

memn2n
imitation
learning

supervision type

  acc =

1 - imitating an expert student
2 - positive and negative feedback
3 - answers supplied by teacher
4 - hints supplied by teacher
5 - supporting facts supplied by teacher
6 - partial feedback
7 - no feedback
8 - imitation + feedback mixture
9 - asking for corrections
10 - asking for supporting facts
mean accuracy

0.5
80
46
48
47
47
48
51
60
48
49
52

0.1
80
29
29
29
28
29
29
50
29
29
36

0.01
80
27
26
26
26
27
27
47
27
27
34

memn2n

reward-based
imitation (rbi)
0.01
0.5
80
80
26
52
27
52
28
51
26
51
49
24
21
22
51
63
26
52
27
52
52
34

0.1
80
32
32
32
32
32
21
53
34
34
38

memn2n
forward

prediction (fp)
0.01
0.5
24
24
24
48
58
60
42
58
33
43
60
58
58
60
23
46
44
67
35
51
52
40

0.1
23
34
57
58
44
58
53
31
52
44
45

memn2n
rbi + fp

0.5
77
68
69
70
66
70
61
72
68
69
69

0.1
77
53
65
54
53
63
56
69
52
53
60

0.01
77
34
62
32
40
62
50
69
39
36
50

    imitation learning, ignoring rewards, is a poor learning strategy when imitating inaccurate

answers, e.g. for   acc < 0.5. for imitating an expert however (task 1) it is hard to beat.

    reward-based imitation (rbi) performs better when rewards are available, particularly in

table 1, but also degrades when they are too sparse e.g. for   acc = 0.01.

    forward prediction (fp) is more robust and has stable performance at different levels of
  acc. however as it only predicts answers implicitly and does not make use of rewards
it is outperformed by rbi on several tasks, notably tasks 1 and 8 (because it cannot do
supervised learning) and task 2 (because it does not take advantage of positive rewards).
    fp makes use of dialog feedback in tasks 3-5 whereas rbi does not. this explains why fp
does better with useful feedback (tasks 3-5) than without (task 2), whereas rbi cannot.
    supplying full answers (task 3) is more useful than hints (task 4) but hints still help fp

more than just yes/no answers without extra information (task 2).

    when positive feedback is sometimes missing (task 6) rbi suffers especially in table 1.

fp does not as it does not use this feedback.

    one of the most surprising results of our experiments is that fp performs well overall,
given that it does not use feedback, which we will attempt to explain subsequently. this is
particularly evident on task 7 (no feedback) where rbi has no hope of succeeding as it has
no positive examples. fp on the other hand learns adequately.

    tasks 9 and 10 are harder for fp as the question is not immediately before the feedback.
    combining rbi and fp ameliorates the failings of each, yielding the best overall results.

one of the most interesting aspects of our results is that fp works at all without any rewards. in
task 2 it does not even    know    the difference between words like    yes    or       correct    vs. words
like    wrong    or    incorrect   , so why should it tend to predict actions that lead to a response like
   yes, that   s right   ? this is because there is a natural coherence to predicting true answers that
leads to greater accuracy in forward prediction. that is, you cannot predict a    right    or    wrong   
response from the teacher if you don   t know what the right answer is. in our experiments our policies
  acc sample negative answers equally, which may make learning simpler. we thus conducted an
experiment on task 2 (positive and negative feedback) of the babi dataset with a much more biased
policy: it is the same as   acc = 0.5 except when the policy predicts incorrectly there is id203
0.5 of choosing a random guess as before, and 0.5 of choosing the    xed answer bathroom. in this
case the fp method obtains 68% accuracy showing the method still works in this regime, although
not as well as before.

6 conclusion

we have presented a set of evaluation datasets and models for dialog-based language learning. the
ultimate goal of this line of research is to move towards a learner capable of talking to humans, such
that humans are able to effectively teach it during dialog. we believe the dialog-based language
learning approach we described is a small step towards that goal.

8

this paper only studies some restricted types of feedback, namely positive feedback and corrections
of various types. however, potentially any reply in a dialog can be seen as feedback, and should be
useful for learning. it should be studied if forward prediction, and the other approaches we tried,
work there too. future work should also develop further evaluation methodologies to test how the
models we presented here, and new ones, work in those settings, e.g.
in more complex settings
where actions that are made lead to long-term changes in the environment and delayed rewards, i.e.
extending to the id23 setting. finally, dialog-based feedback could also be used
as a medium to learn non-dialog based skills, e.g. natural language dialog for completing visual or
physical tasks.

acknowledgments

we thank arthur szlam, y-lan boureau, marc   aurelio ranzato, ronan collobert, michael auli,
david grangier, alexander miller, sumit chopra, antoine bordes and leon bottou for helpful
discussions and feedback, and the facebook ai research team in general for supporting this work.

references
[1] m. a. bassiri.

interactional feedback and the impact of attitude and motivation on noticing l2 form.

english language and literature studies, 1(2):61, 2011.

[2] a. bordes, n. usunier, s. chopra, and j. weston. large-scale simple id53 with memory

networks. arxiv preprint arxiv:1506.02075, 2015.

[3] j. clarke, d. goldwasser, m.-w. chang, and d. roth. driving id29 from the world   s response.
in proceedings of the fourteenth conference on computational natural language learning, pages 18   27.
association for computational linguistics, 2010.

[4] j. dodge, a. gane, x. zhang, a. bordes, s. chopra, a. miller, a. szlam, and j. weston. evaluating

prerequisite qualities for learning end-to-end id71. arxiv preprint arxiv:1511.06931, 2015.

[5] d. goldwasser and d. roth. learning from natural instructions. machine learning, 94(2):205   232, 2014.

[6] r. higgins, p. hartley, and a. skelton. the conscientious consumer: reconsidering the role of assessment

feedback in student learning. studies in higher education, 27(1):53   64, 2002.

[7] f. hill, a. bordes, s. chopra, and j. weston. the goldilocks principle: reading children   s books with

explicit memory representations. arxiv preprint arxiv:1511.02301, 2015.

[8] b. hixon, p. clark, and h. hajishirzi. learning id13s for id53 through con-
in proceedings of the the 2015 conference of the north american chapter of the
versational dialog.
association for computational linguistics: human language technologies, denver, colorado, usa,
2015.

[9] p. k. kuhl. early id146: cracking the speech code. nature reviews neuroscience, 5(11):

831   843, 2004.

[10] g. kuhlmann, p. stone, r. mooney, and j. shavlik. guiding a reinforcement learner with natural language
advice: initial results in robocup soccer. in the aaai-2004 workshop on supervisory control of learning
and adaptive systems, 2004.

[11] a. s. latham. learning through feedback. educational leadership, 54(8):86   87, 1997.

[12] i. lenz, r. knepper, and a. saxena. deepmpc: learning deep latent features for model predictive control.

in robotics science and systems (rss), 2015.

[13] g. f. marcus. negative evidence in id146. cognition, 46(1):53   85, 1993.

[14] t. mikolov, a. joulin, and m. baroni. a roadmap towards machine intelligence.

arxiv:1511.08130, 2015.

arxiv preprint

[15] k. narasimhan, t. kulkarni, and r. barzilay. language understanding for text-based games using deep

id23. arxiv preprint arxiv:1506.08941, 2015.

[16] j. oh, x. guo, h. lee, r. l. lewis, and s. singh. action-conditional video prediction using deep networks

in atari games. in advances in neural information processing systems, pages 2845   2853, 2015.

9

[17] a. pappu and a. rudnicky. predicting tasks in goal-oriented spoken id71 using semantic

knowledge bases. in proceedings of the sigdial, pages 242   250, 2013.

[18] m. ranzato, s. chopra, m. auli, and w. zaremba. sequence level training with recurrent neural networks.

arxiv preprint arxiv:1511.06732, 2015.

[19] v. rieser and o. lemon. id23 for adaptive dialogue systems: a data-driven method-
ology for dialogue management and id86. springer science & business media,
2011.

[20] j. schatzmann, k. weilhammer, m. stuttle, and s. young. a survey of statistical user simulation tech-
niques for reinforcement-learning of dialogue management strategies. the knowledge engineering review,
21(02):97   126, 2006.

[21] j. schmidhuber and r. huber. learning to generate arti   cial fovea trajectories for target detection. inter-

national journal of neural systems, 2(01n02):125   134, 1991.

[22] a. sordoni, m. galley, m. auli, c. brockett, y. ji, m. mitchell, j.-y. nie, j. gao, and b. dolan. a neural
network approach to context-sensitive generation of conversational responses. proceedings of naacl,
2015.

[23] b. c. stadie, s. levine, and p. abbeel. incentivizing exploration in id23 with deep

predictive models. arxiv preprint arxiv:1507.00814, 2015.

[24] p.-h. su, d. vandyke, m. gasic, n. mrksic, t.-h. wen, and s. young. reward shaping with recurrent
neural networks for speeding up on-line policy learning in spoken dialogue systems. arxiv preprint
arxiv:1508.03391, 2015.

[25] s. sukhbaatar, a. szlam, g. synnaeve, s. chintala, and r. fergus. mazebase: a sandbox for learning

from games. corr, abs/1511.07401, 2015. url http://arxiv.org/abs/1511.07401.

[26] s. sukhbaatar, j. weston, r. fergus, et al. end-to-end memory networks. in advances in neural infor-

mation processing systems, pages 2431   2439, 2015.

[27] g. wayne and l. abbott. hierarchical control using networks trained with higher-level forward models.

neural computation, 2014.

[28] m. g. werts, m. wolery, a. holcombe, and d. l. gast. instructive feedback: review of parameters and

effects. journal of behavioral education, 5(1):55   75, 1995.

[29] j. weston, s. chopra, and a. bordes. memory networks. corr, abs/1410.3916, 2014.

[30] j. weston, a. bordes, s. chopra, and t. mikolov. towards ai-complete id53: a set of

prerequisite toy tasks. arxiv preprint arxiv:1502.05698, 2015.

[31] r. j. williams. simple statistical gradient-following algorithms for connectionist id23.

machine learning, 8(3-4):229   256, 1992.

[32] k. xu, j. ba, r. kiros, a. courville, r. salakhutdinov, r. zemel, and y. bengio. show, attend and tell:

neural image id134 with visual attention. arxiv preprint arxiv:1502.03044, 2015.

10

