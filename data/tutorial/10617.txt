5
1
0
2

 

g
u
a
6
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
7
8
4
3
0

.

6
0
5
1
:
v
i
x
r
a

from paraphrase database to compositional paraphrase model and back

john wieting    mohit bansal    kevin gimpel    karen livescu    dan roth   

   university of illinois at urbana-champaign, urbana, il, 61801, usa

{wieting2,danr}@illinois.edu

   toyota technological institute at chicago, chicago, il, 60637, usa

{mbansal,kgimpel,klivescu}@ttic.edu

abstract

the paraphrase database (ppdb; ganitke-
vitch et al., 2013) is an extensive semantic re-
source, consisting of a list of phrase pairs with
(heuristic) con   dence estimates. however, it
is still unclear how it can best be used, due to
the heuristic nature of the con   dences and its
necessarily incomplete coverage. we propose
models to leverage the phrase pairs from the
ppdb to build parametric paraphrase models
that score paraphrase pairs more accurately
than the ppdb   s internal scores while simul-
taneously improving its coverage. they allow
for learning phrase embeddings as well as im-
proved id27s. moreover, we in-
troduce two new, manually annotated datasets
to evaluate short-phrase id141 mod-
els. using our paraphrase model trained using
ppdb, we achieve state-of-the-art results on
standard word and bigram similarity tasks and
beat strong baselines on our new short phrase
paraphrase tasks.1 ,2

1 introduction

paraphrase detection3
is
the task of analyz-
ing two segments of
text and determining if
they have the same meaning despite differences
in structure and wording.
for
a variety of nlp tasks like question answer-
ing (rinaldi et al., 2003; fader et al., 2013), seman-
tic parsing (berant and liang, 2014), textual entail-

is useful

it

1we release our datasets, code, and trained models on the

authors    websites.

2this version differs from the previous one with the inclu-
sion of appendix a, which contains details about new higher
dimensional embeddings we have released. these embeddings
achieve human-level performance on sl999 and ws353.

3see androutsopoulos and malakasiotis (2010) for a survey

on approaches for detecting paraphrases.

ment (bosma and callison-burch, 2007), and ma-
chine translation (marton et al., 2009).

one component of many such systems is a para-
phrase table containing pairs of text snippets, usu-
ally automatically generated,
that have the same
meaning. the most recent work in this area is
the paraphrase database (ppdb; ganitkevitch et
al., 2013), a collection of con   dence-rated para-
phrases created using the pivoting technique of
bannard and callison-burch (2005) over large par-
allel corpora. the ppdb is a massive resource, con-
taining 220 million paraphrase pairs.
it captures
many short paraphrases that would be dif   cult to ob-
tain using any other resource. for example, the pair
{we must do our utmost, we must make every effort}
has little lexical overlap but is present in ppdb. the
ppdb has recently been used for monolingual align-
ment (yao et al., 2013), for predicting sentence sim-
ilarity (bjerva et al., 2014), and to improve the cov-
erage of framenet (rastogi and van durme, 2014).
though already effective for multiple nlp tasks,
we note some drawbacks of ppdb. the    rst is
lack of coverage: to use the ppdb to compare two
phrases, both must be in the database. the second
is that ppdb is a nonparametric paraphrase model;
the number of parameters (phrase pairs) grows with
the size of the dataset used to build it. in practice,
it can become unwieldy to work with as the size of
the database increases. a third concern is that the
con   dence estimates in ppdb are a heuristic com-
bination of features, and their quality is unclear.

we address these issues in this work by intro-
ducing ways to use ppdb to construct paramet-
ric paraphrase models. first we show that initial
skip-gram word vectors (mikolov et al., 2013a) can
be    ne-tuned for the paraphrase task by training
on word pairs from ppdb. we call them para-

gram word vectors. we    nd additive composition
of paragram vectors to be a simple but effective
way to embed phrases for short-phrase paraphrase
tasks. we    nd improved performance by training a
id56 (id56; socher et al., 2010)
directly on phrase pairs from ppdb.

we show that our resulting word and phrase rep-
resentations are effective on a wide variety of tasks,
including two new datasets that we introduce. the
   rst, annotated-ppdb, contains pairs from ppdb
that were scored by human annotators. it can be used
to evaluate paraphrase models for short phrases. we
use it to show that the phrase embeddings produced
by our methods are signi   cantly more indicative of
paraphrasability than the original heuristic scoring
used by ganitkevitch et al. (2013). thus we use the
power of ppdb to improve its contents.

our second dataset, ml-paraphrase,

is a re-
annotation of the bigram similarity corpus from
mitchell and lapata (2010). the task was origi-
nally developed to measure semantic similarity of
bigrams, but some annotations are not congruent
with the functional similarity central to paraphrase
relationships. our re-annotation can be used to
assess id141 capability of bigram composi-
tional models.
in summary, we make the following contributions:
provide new paragram word vectors, learned
using ppdb,
that achieve state-of-the-art per-
formance on the siid113x-999 lexical similarity
task (hill et al., 2014b) and lead to improved perfor-
mance in id31.
provide ways to use ppdb to embed phrases. we
compare additive and id56 composition of para-
gram vectors. both can improve ppdb by re-
ranking the paraphrases in ppdb to improve corre-
lations with human judgments. they can be used as
concise parameterizations of ppdb, thereby vastly
increasing its coverage. we also perform a qualita-
tive analysis of the differences between additive and
id56 composition.
introduce two new datasets. the    rst contains
ppdb phrase pairs and evaluates how well models
can measure the quality of short paraphrases. the
second is a new annotation of the bigram similar-
ity task in mitchell and lapata (2010) that makes it
suitable for evaluating bigram paraphrases.

we release the new datasets, complete with anno-
tation instructions and raw annotations, as well as
our code and the trained models.4

2 related work

there is a vast literature on representing words as
vectors. the intuition of most methods to cre-
ate these vectors (or embeddings) is that similar
words have similar contexts (firth, 1957). ear-
lier models made use of latent semantic analysis
(lsa) (deerwester et al., 1990). recently, more so-
phisticated neural models, work originating with
(bengio et al., 2003), have been gaining popular-
ity (mikolov et al., 2013a; pennington et al., 2014).
these embeddings are now being used in new ways
as they are being tailored to speci   c downstream
tasks (bansal et al., 2014).

phrase representations can be created from
word vectors using compositional models. sim-
ple but effective compositional models were stud-
ied by mitchell and lapata (2008; 2010) and
blacoe and lapata (2012). they compared a va-
riety of binary operations on word vectors and
found that simple point-wise multiplication of
explicit vector
representations performed very
well. other works like zanzotto et al. (2010) and
baroni and zamparelli (2010) also explored compo-
sition using models based on operations of vectors
and matrices.

more

ef   cient

has
neural

recent work

that
shown
embeddings

the
extremely
of
mikolov et al. (2013a) also do well on compo-
sitional tasks simply by adding the word vectors
(mikolov et al., 2013b).
hashimoto et al. (2014)
introduced an alternative id27 and
compositional model based on predicate-argument
structures that does well on two simple com-
position tasks,
including the one introduced by
mitchell and lapata (2010).

an alternative approach to composition, used by
socher et al. (2011), is to train a recursive neural
network (id56) whose structure is de   ned by a bi-
narized parse tree. in particular, they trained their
id56 as an unsupervised autoencoder. the id56
captures the latent structure of composition. recent
work has shown that this model struggles in tasks in-

4available on the authors    websites

volving compositionality (blacoe and lapata, 2012;
hashimoto et al., 2014).5 however, we found suc-
cess using id56s in a supervised setting, similar
to socher et al. (2014), who used id56s to learn
representations for image descriptions. the objec-
tive function we used in this work was motivated
by their multimodal objective function for learning
joint image-sentence representations.

lastly, the ppdb has been used along with other
resources to learn id27s for several
tasks, including semantic similarity, language mod-
eling, predicting human judgments, and classi   -
cation (yu and dredze, 2014; faruqui et al., 2015).
concurrently with our work, it has also been used
to construct paraphrase models for short phrases
(yu and dredze, 2015).

3 new paraphrase datasets

we created two novel datasets:
(1) annotated-
ppdb, a subset of phrase pairs from ppdb which
are annotated according to how strongly they rep-
resent a paraphrase relationship, and (2) ml-
paraphrase, a re-annotation of the bigram similarity
dataset from mitchell and lapata (2010), again an-
notated for strength of paraphrase relationship.

3.1 annotated-ppdb

focus on words,

our motivation for creating annotated-ppdb was
to establish a way to evaluate compositional para-
most ex-
phrase models on short phrases.
isting paraphrase tasks
like
siid113x-999 (hill et al., 2014b), or entire sentences,
such as the microsoft research paraphrase cor-
pus (dolan et al., 2004; quirk et al., 2004). to our
knowledge, there are no datasets that focus on the
paraphrasability of short phrases. thus, we cre-
ated annotated-ppdb so that researchers can focus
on local compositional phenomena and measure the
performance of models directly   avoiding the need
to do so indirectly in a sentence-level task. models
that have strong performance on annotated-ppdb
can be used to provide more accurate con   dence
scores for the paraphrases in the ppdb as well as re-
duce the need for large paraphrase tables altogether.

5we also replicated this approach and found training to be

time-consuming even using low-dimensional word vectors.

annotated-ppdb was created in a multi-step pro-
cess (outlined below) involving various automatic
   ltering steps followed by crowdsourced human an-
notation. one of the aims for our dataset was to col-
lect a variety of paraphrase types   we wanted to in-
clude pairs that were non-trivial to recognize as well
as those with a range of similarity and length. we fo-
cused on phrase pairs with limited lexical overlap to
avoid including those with only trivial differences.

we started with candidate phrases extracted from
the    rst 10m pairs in the xxl version of the ppdb
and then executed the following steps.6
filter phrases for quality: only those phrases
whose tokens were in our vocabulary were retained.7
next, all duplicate paraphrase pairs were removed;
in ppdb, these are distinct pairs that contain the
same two phrases with the order swapped.
filter by lexical overlap: next, we calculated the
word overlap score in each phrase pair and then re-
tained only those pairs that had a score of less than
0.5. by word overlap score, we mean the fraction
of tokens in the smaller of the phrases with leven-
shtein distance     1 to a token in the larger of the
phrases. this was done to exclude less interesting
phrase pairs like hmy dad had, my father hadi or
hballistic missiles, of ballistic missilesi that only dif-
fer in a synonym or the addition of a single word.
select range of paraphrasabilities: to balance our
dataset with both clear paraphrases and erroneous
pairs in ppdb, we sampled 5,000 examples from ten
chunks of the    rst 10m initial phrase pairs where a
chunk is de   ned as 1m phrase pairs.
select range of phrase lengths: we then selected
1,500 phrases from each 5000-example sample that
encompassed a wide range of phrase lengths. to do
this, we    rst binned the phrase pairs by their effec-
tive size. let n1 be the number of tokens of length
greater than one character in the    rst phrase and n2
the same for the second phrase. then the effective
size is de   ned as max(n1, n2). the bins contained
pairs of effective size of 3, 4, and 5 or more, and 500

6note that the con   dence scores for phrase pairs in ppdb
are based on a weighted combination of features with weights
determined heuristically. the con   dence scores were used to
place the phrase pairs into their respective sets (s, m, l, xl,
xxl, etc.), where each larger set subsumes all smaller ones.

7throughout, our vocabulary is de   ned as the most common
100k word types in english wikipedia, following id121
and lowercasing (see   5).

pairs were selected from each bin. this gave us a
total of 15,000 phrase pairs.
prune to 3,000: 3,000 phrase pairs were then se-
lected randomly from the 15,000 remaining pairs to
form an initial dataset, annotated-ppdb-3k. the
phrases were selected so that every phrase in the
dataset was unique.
annotate with mechanical turk: the dataset was
then rated on a scale from 1-5 using amazon me-
chanical turk, where a score of 5 denoted phrases
that are equivalent in a large number of contexts, 3
meant that the phrases had some overlap in mean-
ing, and 1 indicated that the phrases were dissimilar
or contradictory in some way (e.g., can not adopt
and is able to accept).

we only permitted workers whose location was in
the united states and who had done at least 1,000
hits with a 99% acceptance rate. each example
was labeled by 5 annotators and their scores were
averaged to produce the    nal rating. table 1 shows
some statistics of the data. overall, the annotated
data had a mean deviation (md)8 of 0.80. table 1
shows that overall, workers found the phrases to be
of high quality, as more than two-thirds of the pairs
had an average score of at least 3. also from the ta-
ble, we can see that workers had stronger agreement
on very low and very high quality pairs and were
less certain in the middle of the range.
prune to 1,260: to create our    nal dataset,
annotated-ppdb, we selected 1,260 phrase pairs
from the 3,000 annotations. we did this by    rst bin-
ning the phrases into 3 categories: those with scores
in the interval [1, 2.5), those with scores in the in-
terval [2.5, 3.5], and those with scores in the interval
(3.5, 5]. we took the 420 phrase pairs with the low-
est md in each bin, as these have the most agree-
ment about their label, to form annotated-ppdb.

these 1,260 examples were then randomly split
into a development set of 260 examples and a test set
of 1,000 examples. the development set had an md
of 0.61 and the test set had an md of 0.60, indicating
the    nal dataset had pairs of higher agreement than
the initial 3,000.

score range md % of data
[1, 2)
[2, 3)
[3, 4)
[4, 5]

0.66
1.05
0.93
0.59

8.1
20.0
34.9
36.9

table 1: an analysis of annotated-ppdb-3k extracted from
ppdb. the statistics shown are for the splits of the data accord-
ing to the average score by workers. md denotes mean devia-
tion and % of data refers to the percentage of our dataset that
fell into each range.

3.2 ml-paraphrase

newly-annotated

our
second
ml-paraphrase,
similarity
task
mitchell and lapata (2010); we
original annotations as the ml dataset.

based
originally

on

is

the

introduced
to

refer

dataset,
bigram
by
the

the ml dataset consists of human similarity rat-
ings for three types of bigrams: adjective-noun (jn),
noun-noun (nn), and verb-noun (vn). through
manual inspection, we found that the annotations
were not consistent with the notion of similarity
central to paraphrase tasks. for instance, television
set and television programme were the highest rated
phrases in the nn section (based on average anno-
tator score). similarly, one of the highest ranked jn
pairs was older man and elderly woman. this indi-
cates that the annotations re   ect topical similarity in
addition to capturing functional or de   nitional simi-
larity.

therefore, we had the data re-annotated by two
authors of this paper who are native english speak-
ers.9 the bigrams were labeled on a scale from 1-
5 where 5 denotes phrases that are equivalent in a
large number of contexts, 3 indicates the phrases are
roughly equivalent in a narrow set of contexts, and
1 means the phrases are not at all equivalent in any
context. following annotation, we collapsed the rat-
ing scale by merging 4s and 5s together and 1s and
2s together.

statistics for the data are shown in table 2. we
show inter-annotator spearman    and cohen   s    in
columns 2 and 3, indicating substantial agreement
on the jn and vn portions but only moderate agree-
ment on nn. in fact, when evaluating our nn anno-

8md is similar to standard deviation, but uses absolute value
instead of squared value and thus is both more intuitive and less
sensitive to outliers.

9we tried using mechanical turk here, but due to such short
phrases, with few having the paraphrase relationship, workers
did not perform well on the task.

data
jn
nn
vn

ia   
0.87
0.64
0.73

ia    ml comp.    ml human   
0.79
0.58
0.73

0.52
0.49
0.55

0.56
0.38
0.55

table 2: inter-annotator agreement of ml-paraphrase and com-
parison with ml dataset. columns 2 and 3 show the inter-
annotator agreement between the two annotators measured with
spearman    and cohen   s   . column 4 shows the    between
ml-paraphrase and all of the ml dataset. the last column is
the average human    on the ml dataset.

tations against those from the original ml data (col-
umn 4), we    nd    to be 0.38, well below the average
human correlation of 0.49 (   nal column) reported by
mitchell and lapata and also surpassed by pointwise
multiplication (mitchell and lapata, 2010).
this
suggests that the original nn portion, more so than
the others, favored a notion of similarity more re-
lated to association than paraphrase.

4 paraphrase models

we now present parametric paraphrase models and
discuss training. our goal is to embed phrases into
a low-dimensional space such that cosine similarity
in the space corresponds to the strength of the para-
phrase relationship between phrases.

we use a id56 (id56) similar
to that used by socher et al. (2014). we    rst use a
constituent parser to obtain a binarized parse of a
phrase. for phrase p, we compute its vector g(p)
through recursive computation on the parse. that is,
if phrase p is the yield of a parent node in a parse
tree, and phrases c1 and c2 are the yields of its two
child nodes, we de   ne g(p) recursively as follows:

g(p) = f (w [g(c1); g(c2)] + b)

4.1 objective functions
we now present objective functions for training on
pairs extracted from ppdb. the training data con-
sists of (possibly noisy) pairs taken directly from the
original ppdb. in subsequent sections, we discuss
how we extract training pairs for particular tasks.

we assume our training data consists of a set x of
phrase pairs hx1, x2i, where x1 and x2 are assumed
to be paraphrases. to learn the model parame-
ters (w, b, ww), we minimize our objective function
over the data using adagrad (duchi et al., 2011)
with mini-batches. the objective function follows:

min
w,b,ww

1

|x|  xhx1,x2i   x

max(0,        g(x1)    g(x2) + g(x1)    g(t1))

+ max(0,        g(x1)    g(x2) + g(x2)    g(t2))(cid:19)

+   w (kw k2 + kbk2) +   ww kwwinitial     wwk2

(1)

where   w and   ww are id173 parameters,
wwinitial is the initial id27 matrix,    is
the margin (set to 1 in all of our experiments), and
t1 and t2 are carefully-selected negative examples
taken from a mini-batch during optimization.

the intuition for this objective is that we want
the two phrases to be more similar to each other
(g(x1)    g(x2)) than either is to their respective neg-
ative examples t1 and t2, by a margin of at least   .
selecting negative examples to select t1 and t2
in eq. 1, we simply chose the most similar phrase in
the mini-batch (other than those in the given phrase
pair). e.g., for choosing t1 for a given hx1, x2i:

t1 =

argmax

g(x1)    g(t)

t:ht,  i   xb\{hx1,x2i}

where f is an element-wise activation function
(tanh), [g(c1); g(c2)]     r2n is the concatenation
of the child vectors, w     rn  2n is the composi-
tion matrix, b     rn is the offset, and n is the di-
mensionality of the id27s.
if node p
has no children (i.e., it is a single token), we de   ne
g(p) = w (p)
w , where ww is the id27
matrix in which particular word vectors are indexed
using superscripts. the trainable parameters of the
model are w , b, and ww.

where xb     x is the current mini-batch. that is,
we want to choose a negative example ti that is sim-
ilar to xi according to the current model parameters.
the downside of this approach is that we may oc-
casionally choose a phrase ti that is actually a true
paraphrase of xi. we also tried a strategy in which
we selected the least similar phrase that would trig-
ger an update (i.e., g(ti)    g(xi) > g(x1)    g(x2)       ),
but we found the simpler strategy above to work bet-
ter and used it for all experiments reported below.

discussion the objective in eq. 1 is similar to one
used by socher et al. (2014), but with several differ-
ences. their objective compared text and projected
images. they also did not update the underlying
id27s; we do so here, and in a way such
that they are penalized from deviating from their ini-
tialization. also for a given hx1, x2i, they do not
select a single t1 and t2 as we do, but use the en-
tire training set, which can be very expensive with a
large training dataset.

we also experimented with a simpler objective
that sought to directly minimize the squared l2-
norm between g(x1) and g(x2) in each pair, along
with the same id173 terms as in eq. 1.
one problem with this objective function is that the
global minimum is 0 and is achieved simply by driv-
ing the parameters to 0. we obtained much better
results using the objective in eq. 1.

training word paraphrase models to train just
word vectors on word paraphrase pairs (again from
ppdb), we used the same objective function as
above, but simply dropped the composition terms.
this gave us an objective that bears some similarity
to the skip-gram objective with negative sampling
in id97 (mikolov et al., 2013a). both seek
to maximize the dot products of certain word pairs
while minimizing the dot products of others. this
objective function is:

min
ww

1

|x|  xhx1,x2i   x

max(0,        w (x1)

w

   w (x2)

w

5 experiments     word id141

we    rst present experiments on learning lexi-
cal paraphrasability. we train on word pairs
from ppdb and evaluate on the siid113x-999
dataset (hill et al., 2014b), achieving the best results
reported to date.

5.1 training procedure

to learn word vectors that re   ect paraphrasability,
we optimized eq. 2. there are many tunable hyper-
parameters with this objective, so to make training
tractable we    xed the initial learning rates for the
id27s to 0.5 and the margin    to 1. then
we did a coarse grid search over a parameter space
for   ww and the mini-batch size. we considered
  ww values in {10   2, 10   3, ..., 10   7, 0} and mini-
batch sizes in {100, 250, 500, 1000}. we trained
for 20 epochs for each set of hyperparameters using
adagrad (duchi et al., 2011).

for all experiments, we initialized our word
trained using
vectors with skip-gram vectors
id97 (mikolov et al., 2013a). the vectors
were trained on english wikipedia (tokenized and
lowercased, yielding 1.8b tokens).10 we used a
window size of 5 and a minimum count cut-off of
60, producing vectors for approximately 270k word
types. we retained vectors for only the 100k most
frequent words, averaging the rest to obtain a single
vector for unknown words. we will refer to this set
of the 100k most frequent words as our vocabulary.

+ w (x1)

w

   w (t1)

w ) + max(0,        w (x1)

w

   w (x2)

w +

5.2 extracting training data

w (x2)

w

   w (t2)

w )(cid:19) +   ww kwwinitial     wwk2

(2)

it is like eq. 1 except with word vectors replacing
the id56 composition function and with the regular-
ization terms on the w and b removed.

we further found we could improve this model by
incorporating constraints. from our training pairs,
for a given word w, we assembled all other words
that were paired with it in ppdb and all of their lem-
mas. these were then used as constraints during the
pairing process: a word t could only be paired with
w if it was not in its list of assembled words.

for training, we extracted word pairs from the lexi-
cal xl section of ppdb. we used the xl data for
all experiments, including those for phrases. we
used xl instead of xxl because xl has better qual-
ity overall while still being large enough so that we
could be selective in choosing training pairs. there
are a total of 548,085 pairs. we removed 174,766
that either contained numerical digits or words not
in our vocabulary. we then removed 260,425 re-
dundant pairs, leaving us with a    nal training set of
112,894 word pairs.

10we used the december 2, 2013 snapshot.

sl999   

annotator agreement from hill et al. (2014b).11

model
skip-gram
skip-gram
paragram ws
+ constraints

hill et al. (2014b)
hill et al. (2014a)
inter-annotator agreement n/a

-

n
25
1000
25
25
200

0.21
0.38
0.56   
0.58   
0.446
0.52
0.67

table 3: results on the siid113x-999 (sl999) word similarity
task obtained by performing hyperparameter tuning based on
2  ws-s    ws-r and treating sl999 as a held-out test set. n
is word vector dimensionality. a     indicates statistical signi   -
cance (p < 0.05) over the 1000-dimensional skip-gram vectors.

5.3 tuning and evaluation

hyperparameters were tuned using the wordsim-353
(ws353) dataset (finkelstein et al., 2001), speci   -
cally its similarity (ws-s) and relatedness (ws-
r) partitions (agirre et al., 2009). in particular, we
tuned to maximize 2  ws-s correlation minus the
ws-r correlation. the idea was to reward vectors
with high similarity and relatively low relatedness,
in order to target the paraphrase relationship.

on

the

after

siid113x-999

tuning, we

evaluated the best hy-
perparameters
(sl999)
dataset (hill et al., 2014b). we chose sl999 as
our primary test set as it most closely evaluates
the paraphrase relationship. even though ws-s
is a close approximation to this relationship,
it
does not include pairs that are merely associated
and assigned low scores, which sl999 does (see
discussion in hill et al., 2014b).

note that for all experiments we used cosine sim-
ilarity as our similarity metric and evaluated the sta-
tistical signi   cance of dependent correlations using
the one-tailed method of (steiger, 1980).

5.4 results

table 3 shows results on sl999 when improving
the initial word vectors by training on word pairs
from ppdb, both with and without constraints. the
   paragram ws    rows show results when tuning to
maximize 2  ws-s     ws-r. we also show results
for strong skip-gram baselines and the best results
from the literature, including the state-of-the-art re-
sults from hill et al. (2014a) as well as the inter-

the table illustrates that, by training on ppdb,
we can surpass the previous best correlations on
sl999 by 4-6% absolute, achieving the best results
reported to date. we also    nd that we can train
low-dimensional word vectors that exceed the per-
formance of much larger vectors. this is very use-
ful as using large vectors can increase both time and
memory consumption in nlp applications.

to generate word vectors to use for downstream
applications, we chose hyperparameters so as to
maximize performance on sl999.12 these word
vectors, which we refer to as paragram vectors,
had a    of 0.57 on sl999. we use them as initial
word vectors for the remainder of the paper.

5.5 id31

as an extrinsic evaluation of our paragram word
vectors, we used them in a convolutional neu-
ral network (id98) for id31. we
used the simple id98 from kim (2014) and the
binary sentence-level id31 task from
socher et al. (2013). we used the standard data
splits, removing examples with a neutral rating.
we trained on all constituents in the training set
while only using full sentences from development
and test, giving us train/development/test sizes of
67,349/872/1,821.

the id98 uses m-gram    lters, each of which is an
m    n vector. the id98 computes the inner product
between an m-gram    lter and each m-gram in an
example, retaining the maximum match (so-called
   max-pooling   ). the score of the match is a single
dimension in a feature vector for the example, which
is then associated with a weight in a linear classi   er
used to predict positive or negative sentiment.

while kim (2014) used m-gram    lters of sev-
eral lengths, we only used unigram    lters. we
also    xed the word vectors during learning (called
   static    by kim). after learning, the unigram    l-
ters correspond to locations in the    xed word vec-
tor space. the learned classi   er weights represent
how strongly each location corresponds to positive
or negative sentiment. we expect this static id98 to

11hill et al. (2014a) did not report the dimensionality of the

vectors that led to their state-of-the-art results.
12we did not use constraints during training.

word vectors
skip-gram
skip-gram
paragram

n
25
50
25

accuracy (%)

77.0
79.6
80.9

table 4: test set accuracies when comparing embeddings
in a static id98 on the binary id31 task from
socher et al. (2013).

be more effective if the word vector space separates
positive and negative sentiment.

in our experiments, we compared baseline skip-
gram embeddings to our paragram vectors. we
used adagrad learning rate of 0.1, mini-batches of
size 10, and a dropout rate of 0.5. we used 200 un-
igram    lters and recti   ed linear units as the activa-
tion (applied to the    lter output +    lter bias). we
trained for 30 epochs, predicting labels on the de-
velopment set after each set of 3,000 examples. we
recorded the highest development accuracy and used
those parameters to predict labels on the test set.

results are shown in table 4. we see improve-
ments over the baselines when using paragram
vectors, even exceeding the performance of higher-
dimensional skip-gram vectors.

6 experiments     compositional

id141

in this section, we describe experiments on a variety
of compositional phrase-based id141 tasks.
we start with the simplest case of bigrams, and then
proceed to short phrases. for all tasks, we again
train on appropriate data from ppdb and test on
various evaluation datasets, including our two novel
datasets (annotated-ppdb and ml-paraphrase).

6.1 training procedure
we trained our models by optimizing eq. 1 using
adagrad (duchi et al., 2011). we    xed the initial
learning rates to 0.5 for the id27s and
0.05 for the composition parameters, and the mar-
gin to 1. then we did a coarse grid search over a
parameter space for   ww,   w , and mini-batch size.
for   ww, our search space again consisted
of {10   2, 10   3, ..., 10   7, 0},
for   w it was
{10   1, 10   2, 10   3, 0},
and we explored batch
sizes of {100, 250, 500, 1000, 2000}. when ini-
the search
tializing with paragram vectors,
space for   ww was
to be

shifted upwards

{10, 1, 10   1, 10   3, ..., 10   6} to re   ect our
in-
creased con   dence in the initial vectors. we trained
only for 5 epochs for each set of parameters. for
baselines, we used the same initial skip-gram
vectors as in section 5.

6.2 evaluation and baselines
for all experiments, we again used cosine similarity
as our similarity metric and evaluated the statistical
signi   cance using the method of (steiger, 1980).

a baseline used in all compositional experi-
ments is vector addition of skip-gram (or para-
gram) word vectors. unlike explicit word vec-
tors, where point-wise multiplication acts as a con-
junction of features and performs well on composi-
tion tasks (mitchell and lapata, 2008), using addi-
tion with skip-gram vectors (mikolov et al., 2013b)
gives better performance than multiplication.

6.3 bigram paraphrasability

to evaluate our ability to paraphrase bigrams, we
consider the original bigram similarity task from
mitchell and lapata (2010) as well as our newly-
annotated version of it: ml-paraphrase.

extracting training data training data for
these tasks was extracted from the xl por-
tion of ppdb. the bigram similarity task from
mitchell and lapata (2010) contains three types of
bigrams: adjective-noun (jn), noun-noun (nn), and
verb-noun (vn). we aimed to collect pairs from
ppdb that mirrored these three types of bigrams.

we found parsing to be unreliable on such
short segments of text, so we used a pos tag-
ger (manning et al., 2014) to tag the tokens in each
phrase. we then used the word alignments in ppdb
to extract bigrams for training. for jn and nn,
we extracted pairs containing aligned, adjacent to-
kens in the two phrases with the appropriate part-
of-speech tag. thus we extracted pairs like heasy
job, simple taski for the jn section and htown meet-
ing, town councili for the nn section. we used a
different strategy for extracting training data for the
vn subset: we took aligned vn tokens and took the
closest noun after the verb. this was done to approx-
imate the direct object that would have been ide-
ally extracted with a dependency parse. an example
from this section is hachieve goal, achieve aimi.

comp.

model
word vectors n
skip-gram
25
paragram 25
paragram 25
hashimoto et al. (2014)
mitchell and lapata (2010)
human

id56

+
+

mitchell and lapata (2010) bigrams

jn
0.36
0.44   
0.51      
0.49
0.46
-

nn
0.44
0.34
0.40   
0.45
0.49
-

vn
0.36
0.48   
0.50      
0.46
0.38
-

avg
0.39
0.42
0.47
0.47
0.44

-

nn
0.35
0.29

ml-paraphrase
avg
vn
0.42
0.36
0.58       0.46
0.52
0.41

jn
0.32
0.50   
0.57       0.44    0.55   
0.45
0.38
-
-
0.87
0.73

0.39
-
0.64

-

0.75

table 5: results on the test section of the bigram similarity task of mitchell and lapata (2010) and our newly annotated version
(ml-paraphrase). (n) shows the word vector dimensionality and (   comp.   ) shows the composition function used:    +    is vector
addition and    id56    is the id56. the * indicates statistically signi   cant (p < 0.05) over the skip-gram model,
    statistically signi   cant over the {paragram, +} model, and     statistically signi   cant over hashimoto et al. (2014).

we removed phrase pairs that (1) contained words
not in our vocabulary, (2) were redundant with oth-
ers, (3) contained brackets, or (4) had levenshtein
distance     1. the    nal criterion helps to ensure that
we train on phrase pairs with non-trivial differences.
the    nal training data consisted of 133,997 jn pairs,
62,640 vn pairs and 35,601 nn pairs.

baselines
in addition to id56 models, we report
baselines that use vector addition as the composition
function, both with our skip-gram embeddings and
paragram embeddings from section 5.

we also compare to several results from prior
work. when doing so, we took their best correla-
tions for each data subset. that is, the jn and nn re-
sults from mitchell and lapata (2010) use their mul-
tiplicative model and the vn results use their dila-
tion model. from hashimoto et al. (2014) we used
their pas-clblm addl and pas-clblm addnl
models. we note that their vector dimensionalities
are larger than ours, using n = 2000 and 50 respec-
tively.

results results are shown in table 5. we re-
port results on the test portion of the original
mitchell and lapata (2010) dataset (ml) as well as
the entirety of our newly-annotated dataset (ml-
paraphrase). id56 results on ml were tuned on the
respective development sections and id56 results on
ml-paraphrase were tuned on the entire ml dataset.
our id56 model outperforms results from the lit-
erature on most sections in both datasets and its av-
erage correlations are among the highest.13 the one

13the results obtained here differ from those reported in
hashimoto et al. (2014) as we scored their vectors with a
newer python implementation of spearman    that handles ties
(hashimoto, p.c.).

subset of the data that posed dif   culty was the nn
section of the ml dataset. we suspect this is due
to the reasons discussed in section 3.2; for our ml-
paraphrase dataset, by contrast, we do see gains on
the nn section.

we also outperform the strong baseline of adding
1000-dimensional skip-gram embeddings, a model
with 40 times the number of parameters, on our ml-
paraphrase dataset. this baseline had correlations of
0.45, 0.43, and 0.47 on the jn, nn, and vn parti-
tions, with an average of 0.45   below the average
   of the id56 (0.52) and even the {paragram, +}
model (0.46).

interestingly, the type of vectors used to initial-
ize the id56 has a signi   cant effect on performance.
if we initialize using the 25-dimensional skip-gram
vectors, the average    on ml-paraphrase drops to
0.43, below even the {paragram, +} model.

6.4 phrase paraphrasability

in this
section we show that by training a
model based on    ltered phrase pairs in ppdb,
we can actually distinguish between quality para-
phrases and poor paraphrases in ppdb better
than the original heuristic scoring scheme from
ganitkevitch et al. (2013).

extracting training data as before,
training
data was extracted from the xl section of ppdb.
similar to the procedure to create our annotated-
ppdb dataset, phrases were    ltered such that only
those with a word overlap score of less than 0.5
were kept. we also removed redundant phrases and
phrases that contained tokens not in our vocabulary.
the phrases were then binned according to their ef-
fective size and 20,000 examples were selected from

bins of effective sizes of 3, 4, and more than 5, cre-
ating a training set of 60,000 examples. care was
taken to ensure that none of our training pairs was
also present in our development and test sets.

baselines we compare our models with strong
lexical baselines. the    rst, strict word overlap, is
the percentage of words in the smaller phrase that
are also in the larger phrase. we also include a ver-
sion where the words are lemmatized prior to the
calculation.

we also train a support vector regression model
(epsilon-svr) (chang and lin, 2011) on the 33 fea-
tures that are included for each phrase pair in ppdb.
we scaled the features such that each lies in the in-
terval [   1, 1] and tuned the parameters using 5-fold
cross validation on our dev set.14 we then trained on
the entire dev set after    nding the best performing
c and    combination and evaluated on the test set of
annotated-ppdb.

comp. annotated-ppdb

+
+

model
word vectors n
skip-gram
25
paragram 25
paragram 25 id56
ganitkevitch et al. (2013)
word overlap (strict)
word overlap (lemmatized)
ppdb+svr

0.20
0.32   
0.40         
0.25
0.26
0.20
0.33

gram vectors). by using the paragram vectors
to initialize the id56, we reach a correlation of 0.40,
which is better than the ppdb con   dence estimates
by 15% absolute.

we again consider addition of 1000-dimensional
skip-gram embeddings as a baseline, and they con-
tinue to perform strongly (   = 0.37). the id56 ini-
tialized with paragram vectors does reach a higher
   (0.40), but the difference is not statistically signif-
icant (p = 0.16). thus we can achieve similarly-
strong results with far fewer parameters.

this task also illustrates the importance of initial-
izing our id56 model with appropriate word embed-
dings. an id56 initialized with skip-gram vectors
has a modest    of 0.22, well below the    of the id56
initialized with paragram vectors. clearly, ini-
tialization is important when optimizing non-convex
objectives like ours, but it is noteworthy that our best
results came from    rst improving the word vectors
and then learning the composition model, rather than
jointly learning both from scratch.

7 qualitative analysis

score range

[1, 2)
[2, 3)
[3, 4)
[4, 5]

+

2.35
1.56
0.87
0.43

id56
2.08
1.38
0.85
0.47

table 6:
spearman correlation on annotated-ppdb. the *
indicates statistically signi   cant (p < 0.05) over the skip-
gram model, the     indicates statistically signi   cant over the
{paragram, +} model, and the     indicates statistically sig-
ni   cant over ppdb+svr.

results we evaluated on our annotated-ppdb
dataset described in   3.1. table 6 shows the spear-
man correlations on the 1000-example test set. id56
models were tuned on the development set of 260
examples. all other methods had no hyperparame-
ters and therefore required no tuning.

we note that

the con   dence estimates from
ganitkevitch et al. (2013) reach a    of 0.25 on the
test set, similar to the results of strict overlap. while
25-dimensional skip-gram embeddings only reach
0.20, we can improve this to 0.32 by    ne-tuning
them using ppdb (thereby obtaining our para-

14we tuned both parameters over {2   10

, 2   9

, ..., 210}.

table 7: average absolute error of addition and id56 models
on different ranges of gold scores.

we performed a qualitative analysis to uncover
sources of error and determine differences between
adding paragram vectors and using an id56 ini-
tialized with them. to do so, we took the output
of both systems on annotated-ppdb and mapped
their cosine similarities to the interval [1, 5]. we
then computed their absolute error as compared to
the gold ratings.

table 7 shows how the average of these absolute
errors changes with the magnitude of the gold rat-
ings. the id56 performs better (has lower average
absolute error) for less similar pairs. vector addi-
tion only does better on the most similar pairs. this
is presumably because the most positive pairs have
high word overlap and so can be represented effec-
tively with a simpler model.

index

phrase 1

phrase 2

1
2
3
4
5
6
7
8

scheduled to be held in
according to the paper ,

that will take place in

the newspaper reported that

at no cost to
   s surname

could have an impact on
to participate actively
earliest opportunity

does not exceed

without charge to
family name of
may in   uence

to play an active role

early as possible
is no more than

length ratio overlap ratio gold id56 +
4.4
4.1
4.6
4.1
3.2
4.0
2.9
3.5

0.4
0.5
1.0
1.0
0.5
0.67
0.0
0.0

1.0
0.8
0.75
0.67
0.4
0.6
0.67
0.75

4.6
4.6
4.8
4.4
4.6
5.0
4.4
5.0

2.9
2.8
3.1
2.8
4.2
4.8
4.3
4.8

table 8: illustrative phrase pairs from annotated-ppdb with gold similarity > 4. the last three columns show the gold similarity
score, the similarity score of the id56 model, and the similarity score of vector addition. we note that addition performs better
when the pairs have high length ratio (rows 1   2) or overlap ratio (rows 3   4) while the id56 does better when those values are low
(rows 5   6 and 7   8 respectively). boldface indicates smaller error compared to gold scores.

to further investigate the differences between
these models, we removed those pairs with gold
scores in [2, 4], in order to focus on pairs with ex-
treme scores. we identi   ed two factors that dis-
tinguished the performance between the two mod-
els: length ratio and the amount of lexical overlap.
we did not    nd evidence that non-compositional
phrases, such as idioms, were a source of error as
these were not found in ml-paraphrase and only ap-
pear rarely in annotated-ppdb.

we de   ne length ratio as simply the number of
tokens in the smaller phrase divided by the number
of tokens in the larger phrase. overlap ratio is the
number of equivalent tokens in the phrase pair di-
vided by the number of tokens in the smaller of the
two phrases. equivalent tokens are de   ned as to-
kens that are either exact matches or are paired up in
the lexical portion of ppdb used to train the para-
gram vectors.

table 9 shows how the performance of the mod-
els changes under different values of length ratio and
overlap ratio.15 the values in this table are the per-
centage changes in absolute error when using the
id56 over the paragram vector addition model.
so negative values indicate superior performance by
the id56.

a few trends emerge from this table. one is that
as the length ratio increases (i.e., the phrase pairs
are closer in length), addition surpasses the id56
for positive examples. for negative examples, the
trend is reversed. the same trend appears for over-

15the bin delimiters were chosen to be uniform over the
range of output values of the length ratio ([0.4,1] with one out-
lier data point removed) and overlap ratio ([0,1]).

length ratio

positive examples
negative examples

both

overlap ratio

positive examples
negative examples

both

[0, 0.6]
-22.4
-9.9
-13.0
[0, 1
3 ]
-4.5
-11.3
-10.6

(0.6, 0.8]

(0.8, 1]

10.0
-11.1
-6.4
( 1
3 , 2
3 ]
7.0
-7.5
-5.3

35.5
-12.2
-2.0
( 2
3 ,1]
19.4
-15.0
0.0

table 9: comparison of the addition and id56 model on phrase
pairs of different overlap and length ratios. the values in the
table are the percent change in absolute error from the addition
model to the id56 model. negative examples are de   ned as
pairs from annotated-ppdb whose gold score is less than 2 and
positive examples are those with scores greater than 4.    both   
refers to both negative and positive examples.

lap ratio. examples from annotated-ppdb illustrat-
ing these trends on positive examples are shown in
table 8.

when considering both positive and negative ex-
amples (   both   ), we see that the id56 excels on the
most dif   cult examples (large differences in phrase
length and less lexical overlap). for easier exam-
ples, the two fare similarly overall (-2.0 to 0.0%
change), but the id56 does much better on nega-
tive examples. this aligns with the intuition that
addition should perform well when two paraphrastic
phrases have high lexical overlap and similar length.
but when they are not paraphrases, simple addition
is misled and the id56   s learned composition func-
tion better captures the relationship. this may sug-
gest new architectures for modeling composition-
ality differently depending on differences in length
and amount of overlap.

model
n
300
glove
300
paragram300
300
paragram300
inter-annotator agreement    n/a

ws353

sl999

,

,

sl999 ws353 ws-s ws-r
0.571
0.376
0.730
0.667
0.685
0.652
0.67
n/a

0.630
0.814
0.779
n/a

0.579
0.769
0.720
0.756

table 10: evaluation of 300 dimensional paragram vectors on sl999 and ws353. note that the inter-annotator agreement   
was calculated differently for ws353 and sl999. for sl999, the agreement was computed as the average pairwise correlation
between pairs of annotators, while for ws353, agreement was computed as the average correlation between a single annotator with
the average over all other annotators. if one uses the alternative measure of agreement for ws353, the agreement is 0.611, which is
easily beaten by automatic methods (hill et al., 2014b).

model

mitchell and lapata (2010) bigrams

ml-paraphrase

word vectors
glove
paragram300
paragram300

,

,

n
300
ws353 300
300

sl999

comp.

+
+
+

jn
0.40
0.52
0.51

nn
0.46
0.41
0.36

vn
0.37
0.49
0.51

avg
0.41
0.48
0.46

jn nn vn avg
0.39 0.36 0.45 0.40
0.55 0.42 0.55 0.51
0.57 0.39 0.59 0.52

table 11: evaluation of 300 dimensional paragram vectors on the bigram tasks.

8 conclusion

we have shown how to leverage ppdb to learn
state-of-the-art id27s and compositional
models for paraphrase tasks. since ppdb was cre-
ated automatically from parallel corpora, our models
are also built automatically. only small amounts of
annotated data are used to tune hyperparameters.

we also introduced two new datasets to evaluate
compositional models of short paraphrases,    lling a
gap in the nlp community, as currently there are no
datasets created for this purpose. successful mod-
els on these datasets can then be used to extend the
coverage of, or provide an alternative to, ppdb.

there remains a great deal of work to be done
in developing new composition models, whether
with new network architectures or distance func-
tions.
in this work, we based our composi-
tion function on constituent parse trees, but this
may not be the best approach   especially for short
phrases. dependency syntax may be a better al-
ternative (socher et al., 2014). besides improving
composition, another direction to explore is how to
use models for short phrases in sentence-level para-
phrase recognition and other downstream tasks.

appendix a

increasing the dimension of id27s or
training them on more data can have a signi   -
cant positive impact on many tasks   both at the
word level and on downstream tasks. we scaled

model

word vectors
n
glove
300
paragram300,ws353 300
300
paragram300,sl999

comp. annotated-ppdb

+
+
+

0.27
0.43
0.41

table 12: evaluation of 300 dimensional paragram vectors
on annotated-ppdb.

up our original 25-dimensional paragram embed-
dings and modi   ed our training procedure slightly in
order to produce two sets of 300-dimensional para-
gram vectors.16 the vectors outperform our origi-
nal 25-dimensional paragram vectors on all tasks
and achieve human-level performance on sl999 and
ws353. moreover, when simply using vector ad-
dition as a compositional model, they are both on
par with the id56 models we trained speci   cally for
each task. these results can be seen in tables 10, 11,
and 12.

initial embeddings,

the main modi   cation was to use higher-
dimensional
in our case the
pretrained 300-dimensional glove embeddings.17
since ppdb only contains lowercased words, we ex-
tracted only one glove vector per word type (regard-
less of case) by taking the    rst occurrence of each
word in the vocabulary. this is the vector for the
most common casing of the word, and was used as

16both paragram300,ws353 and paragram300,sl999 vec-

tors can be found on the authors    websites.

17we used the glove vectors
of common crawl

lion
available
http://nlp.stanford.edu/projects/glove/

trained on 840 bil-
at

tokens

data,

the word   s single initial vector in our experiments.
this reduced the vocabulary from the original 2.2
million types to 1.7 million.

smaller changes included replacing dot product
with cosine similarity in equation 2 and a change to
the negative sampling procedure. we experimented
with three approaches: max sampling discussed in
section 4.1, rand sampling which is random sam-
pling from the batch, and a 50/50 mixture of max
sampling and rand sampling.

for training data, we selected all word pairs
in the lexical portion of ppdb xl that were in
our vocabulary, removing redundancies. this re-
sulted in 169,591 pairs for training. we trained
our models for 10 epochs and tuned hyperparam-
eters (batch size,   ww,   , and sampling method)
in two ways: maximum correlation on ws353
(paragram300,ws353) and maximum correlation
on sl999 (paragram300,sl999).18 we report re-
sults for both sets of embeddings in tables 10, 11,
and 12, and make both available to the community
in the hope that they may be useful for other down-
stream tasks.

acknowledgements

we thank the editor and the anonymous review-
ers as well as juri ganitkevitch, weiran wang,
and kazuma hashimoto for their valuable com-
ments and technical assistance. we also thank chris
callison-burch, dipanjan das, kuzman ganchev,
ellie pavlick, slav petrov, owen rambow, david
sontag, oscar t  ackstr  om, kapil thadani, lyle un-
gar, benjamin van durme, and mo yu for helpful
conversations. this research was supported by a
google faculty research award to mohit bansal,
karen livescu, and kevin gimpel, the multimodal
information access & synthesis center at uiuc,
part of ccicada, a dhs science and technology
center of excellence, and by darpa under agree-
ment number fa8750-13-2-0008. the views and
conclusions contained herein are those of the authors
and should not be interpreted as necessarily repre-
senting the of   cial policies or endorsements, either
expressed or implied, of darpa or the u.s. gov-
ernment.

18note that if we use the approach in section 5.3 in which we
tune to maximize 2  ws-s correlation minus the ws-r corre-
lation, the sl999    is 0.640, still higher than any other reported
result to the best of our knowledge.

references

eneko agirre, enrique alfonseca, keith hall, jana
kravalova, marius pas  ca, and aitor soroa. 2009. a
study on similarity and relatedness using distributional
and id138-based approaches. in proceedings of hu-
man language technologies: the 2009 annual con-
ference of the north american chapter of the associa-
tion for computational linguistics, pages 19   27. as-
sociation for computational linguistics.

ion androutsopoulos and prodromos malakasiotis.
2010. a survey of id141 and id123
methods. journal of arti   cial intelligence research,
pages 135   187.

colin bannard and chris callison-burch. 2005. para-
phrasing with bilingual parallel corpora. in proceed-
ings of the 43rd annual meeting on association for
computational linguistics, pages 597   604. associa-
tion for computational linguistics.

mohit bansal, kevin gimpel, and karen livescu. 2014.
tailoring continuous word representations for depen-
dency parsing. in proceedings of the annual meeting
of the association for computational linguistics.

marco baroni and roberto zamparelli. 2010. nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space.
in
proceedings of
the 2010 conference on empiri-
cal methods in natural language processing, pages
1183   1193. association for computational linguis-
tics.

yoshua bengio, r  ejean ducharme, pascal vincent, and
christian janvin. 2003. a neural probabilistic lan-
guage model. the journal of machine learning re-
search, 3:1137   1155.

jonathan berant and percy liang. 2014. semantic pars-

ing via id141. in proceedings of acl.

johannes bjerva, johan bos, rob van der goot, and
malvina nissim. 2014. the meaning factory: for-
mal semantics for recognizing id123 and
determining semantic similarity. semeval 2014, page
642.

william blacoe and mirella lapata. 2012. a compari-
son of vector-based representations for semantic com-
position.
in proceedings of the 2012 joint confer-
ence on empirical methods in natural language pro-
cessing and computational natural language learn-
ing, emnlp-conll    12, pages 546   556, strouds-
burg, pa, usa. association for computational lin-
guistics.

wauter bosma and chris callison-burch. 2007. para-
phrase substitution for recognizing textual entail-
ment. in proceedings of the 7th international confer-
ence on cross-language evaluation forum: evalua-
tion of multilingual and multi-modal information re-

trieval, clef   06, pages 502   509, berlin, heidelberg.
springer-verlag.

ral embeddings are born equal.
arxiv:1410.0718.

arxiv preprint

chih-chung chang and chih-jen lin. 2011. libid166:
a library for support vector machines. acm trans-
actions on intelligent systems and technology (tist),
2(3):27.

scott c. deerwester, susan t dumais, thomas k. lan-
dauer, george w. furnas, and richard a. harshman.
1990.
indexing by latent semantic analysis. jasis,
41(6):391   407.

bill dolan, chris quirk, and chris brockett. 2004. un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources.
in pro-
ceedings of coling 2004, pages 350   356, geneva,
switzerland, aug 23   aug 27. coling.

john duchi, elad hazan, and yoram singer.

2011.
adaptive subgradient methods for online learning
and stochastic optimization.
j. mach. learn. res.,
12:2121   2159, july.

anthony fader, luke zettlemoyer, and oren etzioni.
2013. paraphrase-driven learning for open question
answering. in proceedings of the 51st annual meeting
of the association for computational linguistics (vol-
ume 1: long papers), pages 1608   1618, so   a, bul-
garia, august. association for computational linguis-
tics.

manaal faruqui, jesse dodge, sujay kumar jauhar, chris
dyer, eduard hovy, and noah a. smith.
2015.
retro   tting word vectors to semantic lexicons. in pro-
ceedings of the 2015 conference of the north ameri-
can chapter of the association for computational lin-
guistics: human language technologies, pages 1606   
1615.

lev finkelstein, evgeniy gabrilovich, yossi matias,
ehud rivlin, zach solan, gadi wolfman, and eytan
ruppin. 2001. placing search in context: the con-
cept revisited. in proceedings of the 10th international
conference on world wide web, pages 406   414. acm.
j.r. firth. 1957. a synopsis of linguistic theory, 1930-

1955.

juri ganitkevitch, benjamin van durme, and chris
ppdb: the paraphrase
in hlt-naacl, pages 758   764. the as-

callison-burch.
database.
sociation for computational linguistics.

2013.

2014.

kazuma hashimoto, pontus stenetorp, makoto miwa,
and yoshimasa tsuruoka.
jointly learning
word representations and composition functions us-
ing predicate-argument structures. in proceedings of
the 2014 conference on empirical methods in natural
language processing, doha, qatar, october. associa-
tion for computational linguistics.

felix hill, kyunghyun cho, sebastien jean, coline
2014a. not all neu-

devin, and yoshua bengio.

felix hill, roi reichart, and anna korhonen. 2014b.
siid113x-999: evaluating semantic models with (gen-
uine) similarity estimation. corr, abs/1408.3456.

yoon kim. 2014. convolutional neural networks for sen-
tence classi   cation. in proceedings of the 2014 con-
ference on empirical methods in natural language
processing (emnlp), pages 1746   1751, doha, qatar,
october. association for computational linguistics.

christopher d. manning, mihai surdeanu, john bauer,
jenny finkel, steven j. bethard, and david mcclosky.
2014. the stanford corenlp natural language pro-
cessing toolkit. in proceedings of 52nd annual meet-
ing of the association for computational linguistics:
system demonstrations, pages 55   60.

yuval marton, chris callison-burch, and philip resnik.
2009. improved id151 using
monolingually-derived paraphrases. in proceedings of
the 2009 conference on empirical methods in natural
language processing, pages 381   390, singapore, au-
gust. association for computational linguistics.

tomas mikolov, kai chen, greg corrado, and jeffrey
dean. 2013a. ef   cient estimation of word representa-
tions in vector space. arxiv preprint arxiv:1301.3781.
tomas mikolov, ilya sutskever, kai chen, greg s cor-
rado, and jeff dean. 2013b. distributed represen-
tations of words and phrases and their composition-
ality. in advances in neural information processing
systems, pages 3111   3119.

jeff mitchell and mirella lapata. 2008. vector-based
models of semantic composition. in acl, pages 236   
244. citeseer.

jeff mitchell and mirella lapata. 2010. composition in
distributional models of semantics. cognitive science,
34(8):1388   1439.

jeffrey pennington, richard socher, and christopher d
manning. 2014. glove: global vectors for word rep-
resentation. proceedings of the empiricial methods in
natural language processing (emnlp 2014), 12.

chris quirk, chris brockett, and william dolan. 2004.
monolingual machine translation for paraphrase gen-
eration. in dekang lin and dekai wu, editors, pro-
ceedings of emnlp 2004, pages 142   149, barcelona,
spain, july. association for computational linguis-
tics.

pushpendre rastogi and benjamin van durme. 2014.
augmenting framenet via ppdb. in proceedings of
the second workshop on events: de   nition, detec-
tion, coreference, and representation, pages 1   5, bal-
timore, maryland, usa, june. association for compu-
tational linguistics.

fabio rinaldi,

james dowdall, kaarel kaljurand,
2003. exploit-
michael hess, and diego moll  a.
ing paraphrases in a id53 system.
in
proceedings of the second international workshop on
id141, pages 25   32, sapporo, japan, july. as-
sociation for computational linguistics.

richard socher, christopher d manning, and andrew y
ng. 2010. learning continuous phrase representa-
tions and syntactic parsing with recursive neural net-
works. in proceedings of the nips-2010 deep learn-
ing and unsupervised id171 workshop,
pages 1   9.

richard socher, eric h huang, jeffrey pennin, christo-
pher d manning, and andrew y ng. 2011. dynamic
pooling and unfolding recursive autoencoders for para-
phrase detection. in advances in neural information
processing systems, pages 801   809.

richard socher, alex perelygin, jean wu, jason chuang,
christopher d. manning, andrew ng, and christopher
potts. 2013. recursive deep models for semantic
compositionality over a sentiment treebank.
in pro-
ceedings of the 2013 conference on empirical meth-
ods in natural language processing, pages 1631   
1642, seattle, washington, usa, october. association
for computational linguistics.

richard socher, andrej karpathy, quoc v. le, christo-
pher d. manning, and andrew y. ng.
2014.
grounded id152 for    nding and de-
scribing images with sentences. tacl, 2:207   218.

james h steiger.

tests for comparing ele-
ments of a correlation matrix. psychological bulletin,
87(2):245.

1980.

xuchen yao, benjamin van durme, chris callison-
burch, and peter clark. 2013. semi-markov phrase-
based monolingual alignment. in emnlp, pages 590   
600.

mo yu and mark dredze. 2014. improving lexical em-
beddings with semantic knowledge. in proceedings of
the 52nd annual meeting of the association for com-
putational linguistics (volume 2: short papers), pages
545   550, baltimore, maryland, june. association for
computational linguistics.

mo yu and mark dredze. 2015. learning composi-
tion models for phrase embeddings. transactions of
the association for computational linguistics, 3:227   
242.

fabio massimo zanzotto,

ioannis korkontzelos,
francesca fallucchi, and suresh manandhar. 2010.
estimating linear models for compositional dis-
tributional semantics.
in proceedings of the 23rd
international conference on computational linguis-
tics, pages 1263   1271. association for computational
linguistics.

