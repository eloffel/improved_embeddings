a neural knowledge language model

sungjin ahn 1 heeyoul choi 2 tanel p  arnamaa 3 yoshua bengio 1 4

7
1
0
2

 
r
a

m
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
8
1
3
0
0

.

8
0
6
1
:
v
i
x
r
a

abstract

current language models have signi   cant limita-
tion in the ability to encode and decode factual
knowledge. this is mainly because they acquire
such knowledge from statistical co-occurrences al-
though most of the knowledge words are rarely ob-
served. in this paper, we propose a neural knowl-
edge language model (nklm) which combines
symbolic knowledge provided by the knowledge
graph with the id56 language model. by predict-
ing whether the word to generate has an under-
lying fact or not, the model can generate such
knowledge-related words by copying from the
description of the predicted fact. in experiments,
we show that the nklm signi   cantly improves
the performance while generating a much smaller
number of unknown words.

1. introduction

kanye west, a famous <unknown> and the husband of
<unknown>, released his latest album <unknown> in
<unknown>.

a core purpose of language is to communicate knowl-
edge. for human-level language understanding, it is thus of
primary importance for a language model to take advantage
of knowledge. traditional language models are good at cap-
turing statistical co-occurrences of entities as long as they
are observed frequently in the corpus (e.g., words like verbs,
pronouns, and prepositions). however, they are in general
limited in their ability in dealing with factual knowledge be-
cause these are usually represented by named entities such
as person names, place names, years, etc. (as shown in the
above example sentence of kanye west.)
traditional language models have demonstrated to some
extent the ability to encode and decode fatual knowl-
edge (vinyals & le, 2015; serban et al., 2015) when trained

1universit  e de montr  eal, canada 2handong global university,
south korea 3work done during internship at the universit  e de
montr  eal, canada 4cifar senior fellow. correspondence to:
sungjin ahn <sjn.ahn@gmail.com>.

with a very large corpus. however, we claim that simply
feeding a larger corpus into a bigger model hardly results in
a good knowledge language model.
the primary reason for this is the dif   culty in learning
good representations for rare and unknown words. this is a
signi   cant problem because these words are of our primary
interest in knowledge-related applications such as question
answering (iyyer et al., 2014; weston et al., 2016; bordes
et al., 2015) and dialogue modeling (vinyals & le, 2015;
serban et al., 2015). speci   cally, in the recurrent neural
network language model (id56lm) (mikolov et al., 2010),
the computational complexity is linearly dependent on the
number of vocabulary words. thus, including all words of
a language is computationally prohibitive. even if we can
include a very large number of words in the vocabulary,
according to zipf   s law, a large portion of the words will
still be rarely observed in the corpus.
the fact that languages and knowledge can change over time
also makes it dif   cult to simply rely on a large corpus. me-
dia produce an endless stream of new knowledge every day
(e.g., the results of baseball games played yesterday) that
is even changing over time. furthermore, a good language
model should exercise some level of reasoning. for ex-
ample, it may be possible to observe many occurrences of
barack obama   s year of birth and thus able to predict it in a
correlated context. however, one would not expect current
language models to predict, with a proper reasoning, the
blank in    barack obama   s age is
    even if it is only a
simple reformulation of the knowledge on the year of birth1.
in this paper, we propose a neural knowledge language
model (nklm) as a step towards addressing the limitations
of traditional id38 when it comes to exploit-
ing factual knowledge. in particular, we incorporate sym-
bolic knowledge provided by the id13 (nickel
et al., 2015) into the id56lm. this connection makes sense
particularly by observing that facts in id13s
come along with textual representations which are mostly
about rare words in text corpora.
in nklm, we assume that each word generation is either

1we do not investigate the reasoning ability in this paper but
highlight this example because the explicit representation of facts
would help to handle such examples.

a neural knowledge language model

based on a fact or not. thus, at each time step, before gen-
erating a word, we predict whether the word to generate
has an underlying fact or not. as a result, our model pro-
vides predictions over facts in addition to predictions over
words. hence, the previous context information on both
facts and words    ow through an id56 and provide a richer
context. the nklm has two ways to generate a word. one
option is to generate a    vocabulary word    using the vocab-
ulary softmax as is in the id56lm. the other option is to
generate a    knowledge word    by predicting the position of
a word within the textual representation of the predicted
fact. this makes it possible to generate words which are
not in the prede   ned vocabulary and consequently resolves
the rare and unknown word problem. the nklm can also
immediately adapt to adding or modifying knowledge be-
cause the model learns to predict facts, which can easily be
modi   ed without having to retrain the model.
the contributions of the paper are:

    to propose the nklm model to resolve limitations
of traditional language models in dealing with factual
knowledge by using the id13.

    to develop a new dataset called wikifact which can be
used in knowledge-related language models by provid-
ing text aligned with facts.

    to show that the proposed model signi   cantly im-
proves the performance and can generate named en-
tities which in traditional models were treated as un-
known words.

    to propose new id74 that resolve the
problem of the traditional perplexity metric in dealing
with unknown words.

2. related work
there have been remarkable recent advances in language
modeling research based on neural networks (bengio
et al., 2003; mikolov et al., 2010). in particular,
the
id56lms are interesting for their ability to take advantage
of longer-term temporal dependencies without a strong con-
ditional independence assumption. it is especially notewor-
thy that the id56lm using the long short-term memory
(lstm) (hochreiter & schmidhuber, 1997) has recently
advanced to the level of outperforming carefully-tuned tra-
ditional id165 based language models (jozefowicz et al.,
2016).
there have been many efforts to speed up the language
models so that they can cover a larger vocabulary. these
methods approximate the softmax output using hierarchical
softmax (morin & bengio, 2005; mnih & hinton, 2009),
importance sampling (jean et al., 2015), noise contrastive

estimation (mnih & teh, 2012), etc. although helpful to
mitigate the computational problem, these approaches still
suffer from the rare or unknown words problem.
to help deal with the rare/unknown word problem, the
id193 (vinyals et al., 2015) have been adopted
to implement the copy mechanism (gulcehre et al., 2016;
gu et al., 2016) and applied to machine translation and text
summarization. with this approach, the (unknown) word
to copy from the context sentence is inferred from neigh-
boring words. similarly, merity et al. (2016) proposed to
copy from the context sentences and lebret et al. (2016)
from wikipedia infobox. however, because in our case the
context can be very short and often contains no known rele-
vant words (e.g., person names), we cannot use the existing
approach directly.
our knowledge memory is also related to the recent litera-
ture on neural networks with external memory (bahdanau
et al., 2014; weston et al., 2015; graves et al., 2014). in
weston et al. (2015), given simple sentences as facts which
are stored in the external memory, the id53
task is studied. in fact, the tasks that the knowledge-based
language model aims to solve (i.e., predict the next word)
can be considered as a    ll-in-the-blank type of question an-
swering. the idea of jointly using wikipedia and knowledge
graphs has also been used in the context of enriching word
embedding (celikyilmaz et al., 2015; long et al., 2016).
context-dependent (or topic-based) language models have
been studied to better capture long-term dependencies,
by learning some context representation from the history.
(gildea & hofmann, 1999) modeled the topic as a latent
variable and proposed an em-based approach. in (mikolov
& zweig, 2012), the topic features are learned by latent
dirichlet allocation (lda) (blei et al., 2003).

3. model
3.1. preliminary

a topic is associated to topic knowledge and topic descrip-
tion. topic knowledge f is a set of facts {a1, a2, . . . , a|f|}
on the topic and topic description w is a sequence of words
(w1, w2, . . . , w|w|) describing the topic. we can obtain the
topic knowledge from a id13 such as freebase
and the topic description from wikipedia. in the corpus, we
are given pairs of topic knowledge and topic description for
k topics, i.e., {(fk, wk)}k
k=1. in the following, we omit
index k when we indicate an arbitrary topic.
a fact is represented as a triple of subject, relationship, and
object which is associated with a textual representation, e.g.,
(barack obama, married-to, michelle obama). note that
all facts in a topic knowledge have the same subject entity
which is the topic entity itself.

a neural knowledge language model

1, oa

2, . . . , oa

1=   michelle   , oa

we de   ne knowledge words oa of a fact a as a sequence of
n ) from which we can copy a word to
words (oa
generate output. we also maintain a global vocabulary v
containing frequent words. because the words describing
relationships (e.g.,    married to   ) are common and thus can
be generated via the vocabulary v not via copy, we limit
the knowledge words of a fact to be the words for the ob-
ject entity (e.g., oa = (oa
2=   obama   ). in
addition, to make it possible to access the subject words
from the knowledge words, we add a special fact, (topic,
topic itself, topic), to all topic knowledge.
we train the model in a supervised way with labels on facts
and words. this requires aligning words in the topic descrip-
tion with their corresponding facts in the topic knowledge.
speci   cally, given f and w for a topic, we perform simple
string matching between the words in w and all the knowl-
edge words of =    a   foa in such a way to associate fact
a to word w if w appears in knowledge words oa. as a re-
sult, from f and w , we construct a sequence of augmented
observations y = {yt = (wt, at, zt)}t=1:|w|. here, zt is
a binary variable indicating whether wt is observed in the
knowledge words or not:

zt =

if wt     oat,
1,
0, otherwise.

(1)

(cid:40)

in addition, because not all words are associated to a fact
(e.g., words like, is, a, the, have), we introduce a special
fact type, called not-a-fact (naf), and to which assign such
words. the following is an example of an augmented obser-
vation induced from a topic description and knowledge.
example. given a topic on fred rogers with topic descrip-
tion

w =   rogers was born in latrobe, pennsylvania in 1928   

and topic knowledge f = {a42, a83, a0} where

a42 = (fred rogers, place of birth, latrobe pennsylvania)
a83 = (fred rogers, year of birth, 1928)
a0 = (fred rogers, topic itself, fred rogers),

the augmented observation y is

y = {(w=   rogers   , a=0, z=1), (   was   , naf, 0),
(   born   , naf, 0), (   in   , naf, 0), (   latrobe   , 42, 1),
(   pennsylvania   , 42, 1), (   in   , naf, 0), (   1928   , 83, 1)}.

during id136 and training of a topic, we assume that
the topic knowledge f is loaded in the knowledge mem-
ory in a form of a matrix f     rda  |f| where the i-th
column is a fact embedding ai     rda. the fact embed-
ding is the concatenation of subject, relationship, and object
embeddings. we obtain these entity embeddings from a pre-
liminary run of a id13 embedding method such
as transe (bordes et al., 2013). note that we    x the fact
embedding during the training. thus, there is no drift of

t   1 or wv

figure 1. the nklm model. the input consisting of a word (either
t   1) and a fact (at   1) goes into lstm. the lstm   s
wo
output ht together with the knowledge context e generates the
fact key kt. using the fact key, the fact embedding at is retrieved
from the topic knowledge memory. using at and ht, the word
generation source zt is determined, which in turn determines the
next word generation source wv
t is a
symbol taken from the fact description oat.

t . the copied word wo

t or wo

fact embeddings after training and thus the model can deal
with new facts at test time; we learn the embedding of the
topic itself.
for notation, to denote the vector representation of an object
of our interest, we use bold lowercase. for example, the
embedding of a word w is represented by w = w[w]
where wdw  |v| is the id27 matrix, and w[w]
denotes the w-th column of w.

3.2. id136

at each time step, the nklm performs the following four
sub-steps:

1. using both the word and fact predictions of the previ-
ous time step, make an input to the current time step
and update the lstm controller.

2. given the output of the lstm controller, predict a fact

and extract its corresponding embedding.

3. with the extracted fact embedding and the state of the
lstm controller, make a binary decision to determine
the source of word generation.

4. according to the chosen source, generate a word either
from the global vocabulary or by copying a word from
the knowledge words of the selected fact.

a model diagram is depicted in fig. 1. in the following, we
describe these steps in more detail.

    "a1a2a3a4   annaf    ",o1o2o3o4   ontopic knowledge    ".    "    "    "    "    "    "34    "34.    "34,    "34    "    lstmcopyfact searcha neural knowledge language model

3.2.1. input representation and lstm

controller

as shown in fig. 1, the input at time step t is the con-
catenation of three embedding vectors corresponding to
t   1     v, and
a fact at   1, a (global) vocabulary word wv
t   1     oat   1, respectively. how-
a knowledge word wo
ever, because the predicted word comes at a time step
only either from the vocabulary or by copying from the
knowledge words, i.e., wt   1     {wv
t   1} , we set
either wv
t   1 to a zero vector when it is not the
generation source at the previous step. the resulting in-
put representation xt = fconcat(at   1, wv
t   1) is then
fed into the lstm controller, and obtain the output states
ht = flstm(xt, ht   1).

t   1 or wo

t   1, wo

t   1, wo

3.2.2. fact extraction

we then predict a relevant fact at on which the word wt will
be based. predicting a fact is done in two steps.
first, a fact-key kfact     rda is generated by a function
ffactkey(ht, ek) which is in our experiments a multilayer per-
ceptron (mlp) with one hidden layer of relu nonlinearity
and linear outputs. here, ek     rda is the embedding of
the topic knowledge which provides information about what
facts are currently available in the topic knowledge. this
would help the key generator adapt, without retraining, to
changes in the topic knowledge such as removal or modi   -
cation of some facts. our experiments use mean-pooling to
obtain ek, but one can also consider using a more sophis-
ticated method such as the soft-attention mechanism (bah-
danau et al., 2014).
then, using the generated fact-key kfact, we select a fact by
key-value lookup across the knowledge memory f and then
retrieve its embedding at as follows:
exp(k(cid:62)
a(cid:48)   f exp(k(cid:62)
p (a|ht),
a   f
at = f[at].

p (a|ht) =

factf[a(cid:48)])

at = argmax

factf[a])

(cid:80)

(3)

(4)

(2)

,

3.2.3. selecting word generation source

given the context ht and the extracted fact at, the model
decides the source for the next word generation: either from
the vocabulary v or from the knowledge words oat. we
de   ne the id203 of selecting generation-by-copy as:

  zt = p(zt|ht, at) = sigmoid(fcopy(ht, at)).

(5)

here, fcopy is an mlp with one relu hidden layer and a
single linear output unit.
word wt is generated from the source indicated by   zt as

t   1)

algorithm 1 nklm id136 at time step t
1: ## make input
2: xt = fconcat(at   1, wv
t   1, wo
3: ## update lstm controller
4: ht = lstm(ht   1, xt)
5: ## fact prediction and extract embedding
6: at = argmaxa   f p (a|ht, ek)
7: at = f[at]
8: ## decide word generation source
9: zt = i[p(zt|ht, at) > 0.5]
10: if zt == 0 then
11:
12:
13: wo
14: else
15:
16:
17:
18: wv
19: end if

## word generation by copy
nt = argmaxn=0:|oat|   1 p (n|ht, at)
wt = wo
t = 0

## word generation from vocabulary
t = argmaxw   v p (w|ht, at)
wt = wv
t = 0

t = oat[nt]

follows:

wt =

(cid:40)

t     v,
if   zt < 0.5,
wv
t     oat, otherwise.
wo

3.2.4. word generation
generation from vocabulary softmax: for vocabulary
t     v, we follow the usual way of selecting a word
word wv
using the softmax function:

p (wv

t = w|ht, at) =

(cid:80)

exp(k(cid:62)
w(cid:48)   v exp(k(cid:62)

vocaw[w])

vocaw[w(cid:48)])

,

(6)

where kvoca     rdw is obtained by fvoca(ht, at) which is an
mlp with a relu hidden layer and dw linear output units.
generation by copy from knowledge words: to copy
t     oat, we    rst predict the position
a knowledge word wo
of the word within the knowledge words and then copy
the word on the predicted position. this copy-by-position
allows us not to rely on the id27s by instead
learning position embeddings.
one reason to use position prediction is that the traditional
copy mechanism (gulcehre et al., 2016; gu et al., 2016)
is dif   cult to apply to our context because the knowledge
words usually consist of only unknown words and/or are
short in length. furthermore, it makes sense when consid-
ering the fact that we mostly need to copy the knowledge
words in increasing order from the    rst word. for example,
given that the    rst symbol o1 =    michelle    was used in
the previous time step and prior to that other words such

# topics

10k

# toks
1.5m

# uniq toks

78k

# facts
813k

a neural knowledge language model
# relations maxk |fk|

# entities

avgk|fk| maxa |oa|

avga|oa|

560k

1.5k

1k

79

19

2.15

table 1. statistics of the wikifacts-filmactor-v0.1 dataset.

as    president    and    us    were also observed, the model
can easily predict that it is time to select the second symbol,
i.e., o2 =    obama   .
more speci   cally, we    rst generate the position key kpos    
rdo by a function fposkey(ht, at) which is again an mlp
with one hidden layer and linear outputs whose dimension
is the maximum number of positions, e.g., the maximum
max = maxa      f |oa|
length of the knowledge words (e.g., n o
where   f =    kfk). then, the word to copy is chosen by

p (n|ht, at) =

(cid:80)

exp(k(cid:62)
n(cid:48) exp(k(cid:62)

,

posp[n])
posp[n(cid:48)])
p (n|ht, at),

(7)

(8)

nt = argmax
n=0:|oat|   1
t = oat[nt],
wo

(9)
with position n(cid:48) running from 0 to |oat|     1. here,
pdo  n o
max is the matrix of position embeddings of dimen-
sion do. note that n o
max is typically a much smaller number
(e.g., 20 in our experiments) than the size of vocabulary,
and thus the computation for copy is ef   cient. the position
embedding matrix p is learned during training.
although in our experiments we    nd that the simple posi-
tion prediction performs well, we note that one could also
consider a more advanced encoding such as one based on a
convolutional network (kim, 2014) to model the knowledge
words.
to compute p(wt|w<t,f), we    rst obtain {z<t, a<t} from
{w<t} and f using the augmentation procedure, and per-
form the above id136 process with hard decisions taken
about zt and at based on the model   s predictions. the infer-
ence procedure is summarized in algorithm 1.

3.3. learning

we perform supervised learning on the augmented observa-
tion y , similarly to reed & de freitas (2016). that is, given
word observations {yk}k
k=1, our
objective is to maximize the log-likelihood of the augmented
observation w.r.t the model parameter   ,

k=1 and knowledge {fk}k
(cid:88)

      = argmax

log p  (yk|fk).

(10)

  

k

by the chain rule, we can decompose the id203 of the
observation yk as

log p  (yk|fk) =

log p  (yk

t |yk

1:t   1,fk).

(11)

|yk|(cid:88)

t=1

then, after omitting fk and k for simplicity, we can rewrite
the single step id155 as
p  (yt|y1:t   1) = p  (wt, at, zt|ht)

(12)
= p  (wt|at, zt, ht)p  (at|ht)p  (zt|ht).
we maximize the above objective using stochastic gradient
optimization.

4. evaluation
an obstacle in developing the proposed model is the lack
of datasets where the text is aligned with facts at the word
level. while the id32 (ptb) dataset (marcus
et al., 1993) has been frequently used in language mod-
eling, as pointed by merity et al. (2016), its limited vo-
cabulary containing a relatively small amount of named
entities makes it dif   cult to use them for knowledge-related
tasks where rare words are of primary interest; we would
have only a very small amount of words to be associated
with facts. as other larger datasets such as in chelba
et al. (2013) also have problems in licensing or in the for-
mat of the dataset, we produce the wikifacts dataset for
evaluation of the proposed model and the baseline model.
the dataset is freely available in https://bitbucket.
org/skaasj/wikifact_filmactor.

4.1. the wikifacts dataset

in wikifacts, we align wikipedia descriptions with corre-
sponding freebase2 facts. because many freebase topics
provide a link to its corresponding topic in wikipedia, we
choose a set of topics for which both a freebase entity and
a wikipedia description exist. in the experiments, we used
a version called wikifacts-filmactor-v0.1 where
the domain is restricted to the /film/actor in freebase.
we used the summary part (   rst few paragraphs) of the
wikipedia page as the text to be modeled, but discarded
topics for which the number of facts is too large (> 1000) or
the wikipedia description is too short (< 3 sentences). for
the string matching, we also used synonyms and alias infor-
mation provided by id138 (miller, 1995) and freebase.
we augmented the fact set f with the anchor facts a whose
relationship is all set to unknownrelation. that is, ob-
serving that an anchor (a word under a hyperlink) in a
wikipedia description has a corresponding freebase entity
as well as being semantically closely related to the topic

2freebase has migrated to wikidata. www.wikidata.org

a neural knowledge language model

model
id56lm
nklm
no-copy
no-fact-no-copy
no-transe

validation

ppl upp upp-f
56.8
39.4
33.5
27.5
54.9
38.4
40.5
58.0
59.6
48.9

97.9
45.4
93.5
98.8
80.7

test
upp upp-f
58.4
107.0
34.6
48.7
56.4
102.1
107.4
59.3
61.0
85.8

ppl
39.4
28.0
38.3
40.3
49.3

# unk
23247
12523
29756
32671
13903

table 2. we compare four different versions of the nklm to the id56lm on three different perplexity metrics. we used 10k
vocabulary. in no-copy, we disabled the generation-by-copy functionality, and in no-fact-no-copy, using topic knowledge is also
additionally disabled by setting all facts as naf. thus, no-fact-no-copy is very similar to id56lm. in no-transe, we used random
vectors instead of the transe embeddings to initialize the id13 entities. as shown, the nklm shows best performance in all
cases. the no-fact-no-copy performs similar to the id56lm as expected (slightly worse partly because it has a smaller number of model
parameters than that of the id56lm). as expected, no-copy performs better than no-fact-no-copy by using additional information from
the fact embedding, but without the copy mechanism. in the comparison of the nklm and no-copy, we can see the signi   cant gain of
using the copy mechanism to predict named entities. in the last column, we can also see that, with the copy mechanism, the number of
predicting unknown decreases signi   cantly. lastly, we can see that the transe embedding is important.

model
nklm 5k
id56lm 5k
nklm 10k
id56lm 10k
nklm 20k
id56lm 20k
nklm 40k
id56lm 40k

validation

upp upp-f
30.7
48.5
47.6
108.5
45.4
33.5
56.8
97.9
37.9
45.9
72.1
99.5
49.0
44.4
92.3
107.9

ppl
22.8
27.4
27.5
39.4
33.4
57.9
41.4
82.4

test
upp upp-f
31.7
52.0
48.9
118.3
48.7
34.6
58.4
107.0
39.7
49.2
75.5
108.3
52.7
47.1
97.9
116.9

ppl
23.2
27.5
28.0
39.4
34.7
59.3
43.6
86.4

# unk
19557
34994
12523
23247
9677
13773
5809
9009

table 3. the nklm and the id56lm are compared for vocabularies of four different sizes [5k, 10k, 20k, 40k]. as shown, in all
cases the nklm signi   cantly outperforms the id56lm. interestingly, for the standard perplexity (ppl), the gap between the two models
increases as the vocabulary size increases while for upp the gap stays at a similar level regardless of the vocabulary size. this tells us that
the standard perplexity is signi   cantly affected by the unk predictions, because with upp the contribution of unk predictions to the
total perplexity is very small. also, from the upp value for the id56lm, we can see that it initially improves when vocabulary size is
increased as it can cover more words, but decreases back when the vocabulary size is largest (40k) because the rare words are added last
to the vocabulary.

in which the anchor is found, we make a synthetic fact of
the form (topic, unknownrelation, anchor). this po-
tentially compensates for some missing facts in freebase.
because we extract the anchor facts from the full wikipedia
page and they all share the same relation, it is more chal-
lenging for the model to use these anchor facts than using
the freebase facts.
as a result, for each word w in the description, we obtain a
tuple (w, z, a, n, k). here, w is word id, z the copy indicator,
a fact id, n the position to copy from oa if z = 1, and k
topic id. we provide a summary of the dataset statistics in
table 1.

4.2. experiments

4.2.1. setup

we split the dataset into 80/10/10 for train, validation, and
test. as a baseline model, we use the id56lm. for both the
nklm and the id56lm, two-layer lstms with dropout
id173 (zaremba et al., 2014) are used. we tested
models with different numbers of lstm hidden units [200,
500, 1000], and report results from the 1000 hidden-unit
model. for the nklm, we set the symbol embedding di-
mension to 40 and id27 dimension to 400. un-
der this setting, the number of parameters in the nklm is
slightly smaller than that of the id56lm.
we used 100-dimension transe embeddings for freebase
entities and relations, and concatenate the relation and ob-
ject embeddings to obtain fact embeddings. we averaged
all fact embeddings in fk to obtain the topic knowledge em-

a neural knowledge language model

an english [actor]. he was born in [oklahoma] , and died in [oklahoma]. he was married to [charles] [collingwood]
issa serge coelo ( born 1967 ) is a <unk>

warm-up louise allbritton ( 3 july <unk>february 1979 ) was
id56lm a <unk><unk>who was born in <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>
nklm
warm-up
id56lm actor . he is best known for his role as <unk><unk>in the television series <unk>. he also
nklm
warm-up adam wade gontier is a canadian musician and songwriter .
id56lm she is best known for her role as <unk><unk>on the television series <unk>. she has also appeared
nklm
warm-up rory calhoun ( august 8 , 1922 april 28
id56lm , 2010 ) was a <unk>actress . she was born in <unk>, <unk>, <unk>. she was
nklm

, 2008 ) was an american [actor] . he was born in [los] [angeles] california . he was born in

he is best known for his work with the band [three] [days] [grace] . he is the founder of the

[film] director . he is best known for his role as the <unk><unk>in the    lm [un] [taxi] [pour] [aouzou]

table 4. sampled descriptions. given the warm-up phrases, we generate samples from the nklm and the id56lm. we denote the
copied knowledge words by [word] and the unk words by <unk>. overall, the id56lm generates many unks (we used 10k
vocabulary) while the nklm is capable to generate named entities even if the model has not seen some of the words at all during
training. in the    rst case, we found that the generated symbols (words in []) conform to the facts of the topic (louise allbritton) except
that she actually died in mexico, not in oklahoma. (we found that the place of death fact was missing.) while she is an actress, the
model generated a word [actor]. this is because in freebase, there exists only /profession/actor but no /profession/actress. it is also
noteworthy that the nklm fails to use the gender information provided by facts; the nklm uses    he    instead of    she    although the fact
/gender/female is available. from this, we see that if a fact is not detected (i.e., naf), the statistical co-occurrence governs the information
   ow. similarly, in other samples, the nklm generates movie titles (un taxi pour aouzou), band name (three days grace), and place
of birth (los angeles). in addition, to see the nklm   s ability to adapt to knowledge updates without retraining, we changed the fact
/place of birth/oklahoma to /place of birth/chicago and found that the nklm replaces    oklahoma    by    chicago    while keeping other
words the same.

bedding ek. we unrolled the lstms for 30 steps and used
minibatch size 20. we trained the models using stochastic
gradient ascent with gradient clipping range [-5,5]. the
initial learning rate was set to 0.5 for the nklm and 1.5 for
the id56lm, and decayed after every epoch by a factor of
0.98. we trained for 50 epochs and report the results chosen
by the best validation set results.

(cid:80)n

n

4.2.2. the unknown penalized perplexity
the perplexity exp(    1
i=1 log p(wi)) is the standard
performance metric for id38. this, however,
has a problem in evaluating language models for a corpus
containing many named entities: a model can get good
perplexity by accurately predicting unk words as the unk
class. as an extreme example, when all words in a sentence
are unknown words, a model predicting everything as unk
will get a good perplexity. considering that unknown words
provide virtually no useful information, this is clearly a
problem in tasks where named entities are important such
as id53, dialogue modeling, and knowledge
id38.
to this end, we propose a new evaluation metric, called
the unknown-penalized perplexity (upp), and evaluate the
models on this metric as well as the standard perplexity
(ppl). because the actual word underlying the unk should
be one of the out-of-vocabulary (oov) words, in upp we

penalize the likelihood of unknown words as follows:

pupp(wunk) =

p (wunk)

|vtotal \ vvoca| .

here, vtotal is a set of all unique words in the corpus, and
vvoca     vtotal is the global vocabulary used for word genera-
tion. in other words, in upp we assume that the oov set is
equal to vtotal \ vvoca and thus assign a uniform id203
to oov words. in another version, upp-fact, we consider
the fact that the id56lm can also use the knowledge given
to the nklm to some extent, but with limited capability
(because the model is not designed for it). for this, we
assume that the oov set is equal to the total knowledge
words of a topic k, i.e.,

pupp-fact(wunk) =

p (wunk)
|ofk|

,

where ofk =    a   fkoa. in other words, by using upp-
fact, we assume that, for an unknown word, the id56lm can
pick one of the knowledge words with uniform id203.

4.2.3. observations from experiment results

we describe the detail results and analysis on the experi-
ments in detail in the captions of table 2, 3, and 4. our
observations from the experiment results are as follows.

    the nklm outperforms the id56lm in all three per-

plexity measures.

a neural knowledge language model

    the copy mechanism is the key of the signi   cant per-
formance improvement. without the copy mechanism,
the nklm still performs better than the id56lm due
to its usage of the fact information, but the improve-
ment is not so signi   cant.

    the nklm results in a much smaller number of unks

(roughly, a half of the id56lm).

    when no knowledge is available, the nklm performs

as well as the id56lm.

    id13 embedding using transe is an ef   -

cient way of representing facts in our model.

    the nklm generates named entities in the provided
facts whereas the id56lm generates many more
unks.

    the nklm shows its ability to adapt immediately to

the change of the knowledge.

    the standard perplexity is signi   cantly affected by the
prediction accuracy on the unknown words. thus, one
need carefully consider when using it as a metric for
knowledge-related language models.

5. conclusion
in this paper, we presented a novel neural knowledge lan-
guage model (nklm) that brings the symbolic knowledge
from a id13 into the expressive power of id56
language models. the nklm signi   cantly outperforms the
id56lm in terms of perplexity and generates named entities
which are not observed during training, as well as immedi-
ately adapting to changes in knowledge. we believe that the
wikifact dataset introduced in this paper, can be useful in
other knowledge-related language tasks as well. in addition,
the unknown-penalized perplexity introduced in order to
resolve the limitation of the standard perplexity, can also be
useful in evaluating other language tasks.
the task that we investigated in this paper is limited in
the sense that we assume that the true topic of a given
description is known. relaxing this assumption by making
the model search for a proper topic on-the-   y will make
the model more practical and scalable. we believe that
there are many more open research challenges related to the
knowledge language models.

acknowledgments
the authors would like to thank alberto garc    a-dur  an,
caglar gulcehre, chinnadhurai sankar, iulian serban,
sarath chandar, and peter clark for helpful feedbacks and
discussions as well as the developers of theano (bastien
et al., 2012), nserc, cifar, facebook, google, ibm, mi-
crosoft, samsung, and canada research chairs for funding,
and compute canada for computing resources.

references
bahdanau, dzmitry, cho, kyunghyun, and bengio, yoshua.
id4 by jointly learning to align
and translate. arxiv preprint arxiv:1409.0473, 2014.

bastien, fr  ed  eric, lamblin, pascal, pascanu, razvan,
bergstra, james, goodfellow, ian, bergeron, arnaud,
bouchard, nicolas, warde-farley, david, and bengio,
yoshua. theano: new features and speed improvements.
arxiv preprint arxiv:1211.5590, 2012.

bengio, yoshua, ducharme, r  ejean, vincent, pascal, and
jauvin, christian. a neural probabilistic language model.
in journal of machine learning research, 2003.

blei, david m, ng, andrew y, and jordan, michael i. latent
the journal of machine learning

dirichlet allocation.
research, 3:993   1022, 2003.

bordes, antoine, usunier, nicolas, garcia-duran, alberto,
weston, jason, and yakhnenko, oksana. translating
embeddings for modeling multi-relational data. in ad-
vances in neural information processing systems, pp.
2787   2795, 2013.

bordes, antoine, usunier, nicolas, chopra, sumit, and
weston, jason. large-scale simple id53
with memory networks. arxiv preprint arxiv:1506.02075,
2015.

celikyilmaz, asli, hakkani-tur, dilek, pasupat, panupong,
and sarikaya, ruhi. enriching id27s using
id13 for semantic tagging in conversational
id71. in 2015 aaai spring symposium series,
2015.

chelba, ciprian, mikolov, tomas, schuster, mike, ge,
qi, brants, thorsten, koehn, phillipp, and robinson,
tony. one billion word benchmark for measuring
progress in statistical id38. arxiv preprint
arxiv:1312.3005, 2013.

gildea, daniel and hofmann, thomas. topic-based lan-

guage models using em. eurospeech 1999, 1999.

graves, alex, wayne, greg, and danihelka, ivo. neural
turing machines. arxiv preprint arxiv:1410.5401, 2014.

gu, jiatao, lu, zhengdong, li, hang, and li, victor
o. k. incorporating copying mechanism in sequence-
to-sequence learning. corr, abs/1603.06393, 2016.

gulcehre, caglar, ahn, sungjin, nallapati, ramesh, zhou,
bowen, and bengio, yoshua. pointing the unknown
words. acl 2016, 2016.

hochreiter, sepp and schmidhuber, j  urgen. long short-term

memory. neural computation, 9(8):1735   1780, 1997.

a neural knowledge language model

nickel, maximilian, murphy, kevin, tresp, volker, and
gabrilovich, evgeniy. a review of relational machine
learning for id13s: from multi-relational link
prediction to automated id13 construction.
arxiv preprint arxiv:1503.00759, 2015.

reed, scott and de freitas, nando. neural programmer-

interpreters. iclr 2016, 2016.

serban, iulian v, sordoni, alessandro, bengio, yoshua,
courville, aaron, and pineau, joelle. building end-to-
end dialogue systems using generative hierarchical neural
networks. 30th aaai conference on arti   cial intelli-
gence, 2015.

vinyals, oriol and le, quoc. a neural conversational model.

arxiv preprint arxiv:1506.05869, 2015.

vinyals, oriol, fortunato, meire, and jaitly, navdeep.

id193. nips 2015, 2015.

weston, jason, chopra, sumit, and bordes, antoine. mem-

ory networks. iclr 2015, 2015.

weston, jason, bordes, antoine, chopra, sumit, and
mikolov, tomas. towards ai-complete question answer-
ing: a set of prerequisite toy tasks. iclr 2016, 2016.

zaremba, wojciech, sutskever, ilya, and vinyals, oriol.
recurrent neural network id173. arxiv preprint
arxiv:1409.2329, 2014.

iyyer, mohit, boyd-graber, jordan l, claudino, leonardo
max batista, socher, richard, and daum  e iii, hal. a
neural network for factoid id53 over para-
graphs. in emnlp 2014, pp. 633   644, 2014.

jean, sebastien, cho, kyunghyun, memisevic, roland, and
bengio, yoshua. on using very large target vocabulary
for id4. acl 2015, 2015.

jozefowicz, rafal, vinyals, oriol, schuster, mike, shazeer,
noam, and wu, yonghui. exploring the limits of lan-
guage modeling. arxiv preprint arxiv:1602.02410, 2016.

kim, yoon. convolutional neural networks for sentence

classi   cation. emnlp 2014, 2014.

lebret, r  emi, grangier, david, and auli, michael. neural
text generation from structured data with application to
the biography domain. arxiv preprint arxiv:1603.07771,
2016.

long, teng, lowe, ryan, cheung, jackie chi kit, and
precup, doina. leveraging lexical resources for learning
entity embeddings in multi-relational data. 2016.

marcus, mitchell p, marcinkiewicz, mary ann, and san-
torini, beatrice. building a large annotated corpus of
english: the id32. computational linguistics,
19(2):313   330, 1993.

merity, stephen, xiong, caiming, bradbury, james, and
socher, richard. pointer sentinel mixture models. arxiv
preprint arxiv:1609.07843, 2016.

mikolov, tomas and zweig, geoffrey. context dependent
in spoken
recurrent neural network language model.
language technology workshop (slt), 2012 ieee, pp.
234   239. ieee, 2012.

mikolov, tomas, kara     at, martin, burget, lukas, cernock`y,
jan, and khudanpur, sanjeev. recurrent neural network
based language model. in interspeech 2010, vol-
ume 2, pp. 3, 2010.

miller, george a. id138: a lexical database for english.

communications of the acm, 38(11):39   41, 1995.

mnih, andriy and hinton, geoffrey e. a scalable hierar-
chical distributed language model. in advances in neural
information processing systems, pp. 1081   1088, 2009.

mnih, andriy and teh, yee whye. a fast and simple algo-
rithm for training neural probabilistic language models.
icml 2012, 2012.

morin, frederic and bengio, yoshua. hierarchical proba-
bilistic neural network language model. aistats 2005,
pp. 246, 2005.

appendix: heatmaps

a neural knowledge language model

figure 2. this is a heatmap of an example sentence generated by the nklm having a warmup    rory calhoun ( august 8 , 1922 april
28   . the    rst row shows the id203 of selecting copy (equation 5 in section 3.1). the bottom heat map shows the state of the
topic-memory at each time step (equation 2 in section 3.1). in particular, this topic has 8 facts and an additional <naf> fact. for the
   rst six time steps, the model retrieves <naf>from the knowledge memory, copy-switch is off and the words are generated from the
general vocabulary. for the next time step, the model gives higher id203 to three different profession facts:    screenwriter   ,    actor   
and    film producer.    the fact    actor    has the highest id203, copy-switch is higher than 0.5, and therefore    actor    is copied as the
next word. moreover, we see that the model correctly retrieves the place of birth fact and outputs    los angeles.    after that, the model
still predicts the place of birth fact, but copy-switch decides that the next word should come from the general vocabulary, and outputs
   california.   

figure 3. this is an example sentence generated by the nklm having a warmup    louise allbritton ( 3 july <unk>february 1979 )
was   . we see that the model correctly retrieves and outputs the profession (   actor   ), place of birth (   oklahoma   ), and spouse (   charles
collingwood   ) facts. however, the model makes a mistake by retrieving the place of birth fact in a place where the place of death fact is
supposed to be used. this is probably because the place of death fact is missing in this topic memory and then the model searches for a
fact about location, which is somewhat encoded in the place of birth fact. in addition, louise allbritton was a woman, but the model
generates a male profession    actor    and male pronoun    he   . the    actor    is generated because there is no    actress    representation in
freebase.

p_copy_fact,2008)wasanamericanactor.hewasborninlosangelescalifornia.<naf>profession-screenwriterprofession-actorplace_of_birth-los angelesprofession-film producertopic_itself-rory calhoununk_rel-spellboundunk_rel-californiaunk_rel-santa cruzp_copy_factanenglishactor.hewasborninoklahoma,anddiedinoklahoma.hewasmarriedtocharlescollingwood.<naf>education.institution-university of oklahomaperformance.film-son of draculalocation.people_born_here-oklahoma cityperformance.film-the egg and imarriage.type_of_union-marriagemarriage.spouse-charles collingwoodprofession-actortopic_itself-louise allbrittonunk_rel-universal studiosunk_rel-pasadena playhouseunk_rel-pittsburghunk_rel-sitting prettyunk_rel-hollywoodunk_rel-world war iiunk_rel-united service organizations