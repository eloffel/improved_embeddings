first result on arabic id4

amjad almahairi

mila, universit  e de montr  eal
amjad.almahairi@umontreal.ca

kyunghyun cho

new york university

kyunghyun.cho@nyu.edu

nizar habash

new york university
nizar.habash@nyu.edu

aaron courville

mila, universit  e de montr  eal
aaron.courville@umontreal.ca

6
1
0
2

 

n
u
j
 

8

 
 
]
l
c
.
s
c
[
 
 

1
v
0
8
6
2
0

.

6
0
6
1
:
v
i
x
r
a

abstract

id4 has become a ma-
jor alternative to widely used phrase-based
id151. we notice
however that much of research on neural ma-
chine translation has focused on european
languages despite its language agnostic na-
ture.
in this paper, we apply neural ma-
chine translation to the task of arabic transla-
tion (ar   en) and compare it against a stan-
dard phrase-based translation system. we run
extensive comparison using various con   g-
urations in preprocessing arabic script and
show that the phrase-based and neural trans-
lation systems perform comparably to each
other and that proper preprocessing of arabic
script has a similar effect on both of the sys-
tems. we however observe that the neural ma-
chine translation signi   cantly outperform the
phrase-based system on an out-of-domain test
set, making it attractive for real-world deploy-
ment.

introduction

1
id4 (kalchbrenner and blun-
som, 2013; sutskever et al., 2014; cho et al., 2014)
has become a major alternative to the widely used
statistical phrase-based translation system (koehn et
al., 2003), evidenced by the successful entries in
wmt   15 and wmt   16.

previous work on using neural networks for ara-
bic translation has mainly focused on using neural
networks to induce an additional feature for phrase-
based id151 systems (see,
e.g., (devlin et al., 2014; setiawan et al., 2015)).
this hybrid approach has resulted in impressive im-
provement over other systems without any neural

network, which raises a hope that a fully neural
translation system may achieve a even higher trans-
lation quality. we however found no prior work on
applying a fully neural translation system (i.e., neu-
ral machine translation) to arabic translation.

in this paper, our aim is therefore to present
the    rst result on the arabic translation using
id4. on both directions
(ar   en and en   ar), we extensively compare a
vanilla attention-based id4
system (bahdanau et al., 2015) against a vanilla
phrase-based system (moses, (koehn et al., 2003)),
while varying pre-/post-processing routines,
in-
cluding morphology-aware id121 and ortho-
graphic id172, which were found to be cru-
cial in arabic translation (habash and sadat, 2006;
badr et al., 2008; el kholy and habash, 2012).

the experiment reveals that neural machine trans-
lation performs comparably to the standard phrase-
based system. we further observe that the tokeniza-
tion and id172 routines, initially proposed
for phrase-based systems, equally improve the trans-
lation quality of id4. finally,
on the en   ar task, we    nd the neural translation
system to be more robust to the domain shift com-
pared to the phrase-based system.

2 id4

is

an

attention-based

a major workforce behind neural machine trans-
lation
encoder-decoder
model (bahdanau et al., 2015; cho et al., 2015).
this
encoder-decoder model
consists of an encoder, decoder and attention mech-
anism. the encoder, which is often implemented
as a bidirectional recurrent network, reads a source

attention-based

sentence x = (x1, . . . , xtx) and returns a set of
context vectors c = (h1, . . . , htx).
each time t(cid:48), it computes the new hidden state by

the decoder is a recurrent language model. at

zt(cid:48) =   (zt(cid:48)   1,   yt(cid:48)   1, ct(cid:48)),

coder: ct(cid:48) =(cid:80)tx

where    is a recurrent activation function, and zt(cid:48)   1
and   yt(cid:48)   1 are the previous hidden state and previ-
ously decoded target word respectively. ct(cid:48)
is a
time-dependent context vector and is a weighted
sum of the context vectors returned by the en-
t=1   tht, where the attention weight
  t is computed by the attention mechanism fatt:
  t     exp(fatt(zt(cid:48)   1,   yt(cid:48)   1, ht)). in this paper, we
use a feedforward network with a single tanh hid-
den layers to implement fatt.

given a new decoder state zt(cid:48), the conditional dis-
tribution over the next target symbol is computed as

p(yt = w|  y<t, x)     exp(gw(zt(cid:48))),

where gw returns a score for the word w, and v is a
target vocabulary.

the entire model, including the encoder, decoder
and attention mechanism, is jointly tuned to max-
imize the conditional log-id203 of a ground-
truth translation given a source sentence using a
training corpus of parallel sentence pairs. this learn-
ing process is ef   ciently done by stochastic gradient
descent with id26.

subword symbols sennrich et al. (2015), chung
et al. (2016) and luong and manning (2016) showed
that the attention-based neural translation model can
perform well when source and target sentences are
represented as sequences of subword symbols such
as characters or frequent character id165s. this
use of subword symbols elegantly addresses the is-
sue of large target vocabulary in neural networks
(jean et al., 2014), and has become a de facto stan-
dard in id4. therefore, in our
experiments, we use character id165s selected by
byte pair encoding (sennrich et al., 2015).

3 processing of arabic for translation
3.1 characteristics of arabic language
arabic exhibits a rich morphology. this makes ara-
bic challenging for natural language processing and

machine translation. for instance, a single arabic
token     	
    (cid:187)3 (cid:207)     (   and to his vehicle    in english) is
formed by prepending         (   and   ) and     (cid:203)    (   to   ) to

	   (cid:187)3     (   vehicle   ), appending         
the base lexeme    

      (ta
(   his   ) and replacing the feminine suf   x    
marbuta) of the base lexeme to     
0    . this fea-
ture of arabic is challenging, as (1) it increases the
number of out-of-vocabulary tokens, (2) it conse-
quently worsens the issue of data sparsity 1, and
(3) it complicates the word-level correspondence be-
tween arabic and another language in translation.
this is often worsened by the orthographic ambigu-
ity found in arabic scripts, such as the inconsistency
in spelling certain letters.

previous work has thus proposed morphology-
aware id121 and orthographic id172
as two crucial components for building a high qual-
ity phrase-based machine translation system (or its
variants) for arabic (habash and sadat, 2006; badr
et al., 2008; el kholy and habash, 2012). these
techniques have been found very effective in alle-
viating the issue of data sparsity and improving the
generalization to tokens not included in a training
corpus (in their original forms.)

3.2 morphology-aware id121
the goal of morphology-aware id121, or mor-
pheme segmentation (creutz and lagus, 2005) is
to split a word in its surface form into a sequence
of linguistically sound sub-units. contrary to sim-
ple string-based id121 methods, morphology-
aware id121 relies on linguistic knowledge of
a target language (arabic in our case) and applies,
for instance, various morphological or orthographic
adjustments to the resulting sub-units.

in this paper, we investigate the id121
scheme used in the penn arabic treebank (atb,
(maamouri et al., 2004)) which was found to
work well with phrase-based translation system in
(el kholy and habash, 2012). this id121
separates all clitics other than de   nite articles.

when translating to arabic, the decoded sequence
of tokenized symbols must be de-tokenized. this de-
id121 step is not trivial, as it needs to undo
any adjustment (implicitly) made by the tokeniza-
tion algorithm. in this work, we follow the approach

1see sec. 5.2.1 of (cho, 2015) for detailed discussion.

proposed in (badr et al., 2008; salameh et al., 2015).
this approach builds a lookup table from a train-
ing corpus and uses it for mapping a tokenized form
back to its original form. when the tokenized form
is missing in the lookup table, we back off to a num-
ber of hand-crafted de-id121 rules.

3.3 orthographic id172
since the sources of major orthographic ambiguity
are in the letters    alif    and    ya   , we normalize these
letters (and their inconsistent replacements.) fur-
thermore, we replace parentheses    (    and    )    with
special tokens       lrb       and       rrb      , and remove
diacritics.

4 experimental settings
4.1 data preparation
training corpus we combine ldc2004t18,
ldc2004t17 and ldc2007t08 to form a training
parallel corpus. the combined corpus contains ap-
proximately 1.2m sentence pairs, with 33m tokens
on the arabic side. most of the sentences are from
news articles. we ignore sentence pairs which either
side has more than 100 tokens.
in-domain evaluation sets we use the eval-
uation sets from nist 2004 (mt04) and 2005
(mt05) as development and test sets respectively.
in ar   en, we use all four english reference trans-
lations to measure the translation quality. we use
only the    rst english sentence out of four as a source
during en   ar. both of these sets are derived from
news articles, just as the training corpus is.
out-of-domain evaluation set
in the case of
en   ar, we evaluate both phrase-based and neural
translation systems on medar evaluation set (ha-
mon and choukri, 2011). this set has four ara-
bic references per english sentence.
it is derived
from web pages discussing climate changes, signif-
icantly differing from the training corpus. this set
was selected to highlight the robustness to domain
mismatch between training and test sets.

we verify the domain mismatches of the evalua-
tion sets relative to the training corpus by    tting a
5-gram language model on the training corpus and
computing the likelihoods of the evaluation sets, on
the arabic side. as can be seen in table 1, the do-
main of the medar is signi   cantly further away

avg. log-prob.

mt04 mt05 medar
-75.03
-59.74

-55.97

table 1: language model scores of the arabic side. the lan-
guage model was tuned on the training corpus.
from the training corpus than the others are.
note on mt04 and mt05 we noticed that a sig-
ni   cant portion of arabic sentences in mt04 and
mt05 are found verbatim in the training corpus (172
on mt04 and 26 on mt05).
in order to accu-
rately measure the generalization performance, we
removed those duplicates from the evaluation sets.

4.2 machine translation systems
phrase-based machine translation we use
moses (koehn et al., 2007) to build a standard
phrase-based id151 system.
word alignment was extracted by giza++ (och
and ney, 2003), and we used phrases up to 8 words
to build a phrase table. we use the following op-
tions for alignment symmetrization and reordering
model: grow-diag-   nal-and and msd-bidirectional-
fe. kenlm (hea   eld et al., 2013) is used as a
language model and trained on the target side of the
training corpus.
id4 we use a publicly
available implementation of attention-based neural
machine translation.2 for both directions   en   ar
and ar   en   , the encoder is a bidirectional recur-
rent network with two layers of 512  2 gated recur-
rent units (gru, (cho et al., 2014)), and the decoder
a unidirectional recurrent network with 512 gru   s.
each model is trained for approximately seven days
using adadelta (zeiler, 2012) until the cost on the
development set stops improving. we regularize
each model by applying dropout (srivastava et al.,
2014) to the output layer and penalizing the l2 norm
of the parameters (coef   cient 10   4). we use beam
search with width set to 12 for decoding.

4.3 id172 and id121
arabic we test simple id121 (tok) based
on the script from moses, and orthographic normal-
ization (norm), and morphology-aware tokeniza-
tion (atb) using madamira (pasha et al., 2014),
. in the latter scenario, we reverse the id121
before computing id7. note that atb includes

2 https://github.com/nyu-dl/dl4mt-tutorial

arabic

   
   
   
   

   
   
   

tok. norm. atb tok. lower
   
   
   
   
   
   
   
   
   
   
   
   

english
   
   
   
   
   
   
   
   
   
   
   
   

   
   
   

   
   
   
   

   
   

   
   

t
m
s
-
b
p

t
m

l
a
r
u
e
n

en   ar

mt05

medar
   

31.52
33.03
34.98
35.63
35.7
35.98
28.64
29.77
32.53
32.95
33.53
33.62

   

(1.51)
(3.46)
(4.11)
(4.18)
(4.46)

   

(1.13)
(3.89)
(4.31)
(4.89)
(4.98)

8.69
9.78
17.34
17.75
18.67
19.27
11.09
10.15
22.36
22.79
23.11
24.46

(1.09)
(8.65)
(9.06)
(9.98)
(10.58)

   

(-0.94)
(11.27)
(11.70)
(12.02)
(13.37)

ar   en
mt05

48.59
49.44
49.51
49.91
50.67
51.19
47.12
47.63
48.53
47.53
49.21
49.7

   

(0.85)
(0.92)
(1.32)
(2.08)
(2.60)

   

(0.51)
(1.41)
(0.41)
(2.09)
(2.58)

table 2: id7 scores with the improvement over the id121-only models in the parentheses.

norm, and both of them include simple tokeniza-
tion performed by madamira.
english we test simple id121 (tok), lower-
casing (lower) for en   ar and truecasing (true,
(lita et al., 2003)) for ar   en.
byte pair encoding as mentioned earlier
in
sec. 2, we use byte pair encoding (bpe) for neural
machine translation. we apply bpe to the already-
tokenized training corpus to extract a vocabulary of
up to 20k subword symbols. we use the publicly
available script released by sennrich et al. (2015).

5 result and analysis
en   ar from table 2, we observe that the transla-
tion quality improves as a better preprocessing rou-
tine is used. by using the id172 as well as
morphology-aware id121 (tok+norm+atb),
the phrase-based and neural systems each achieve as
much as +4.46 and +4.98 id7 over the baselines,
on mt05. the improvement is even more appar-
ent on the medar whose domain deviates from the
training corpus, con   rming that proper preprocess-
ing of arabic script indeed helps in handling word
tokens that are not present in a training corpus.

we notice that the tested id121 strategies
have nearly identical effect on both the phrase-
based and neural translation systems. the transla-
tion quality of either system is mostly effective by
the id121 strategy employed for arabic, and
is largely insensitive to whether source sentences
in english were lowercased. this re   ects well the
complexity of arabic scripts, compared to english,
discussed earlier in sec. 3.1.

another important observation is that the neu-
ral translation system signi   cantly outperforms the
phrase-based one on the out-of-domain evaluation
set (medar), while they perform comparably to
each other in the case of the in-domain evaluation set
(mt05). we conjecture that this is due to the neural
translation system   s superior generalization capabil-
ity based on its use of continuous space representa-
tions.
ar   en in the last column of table 2, we ob-
serve a trend similar to that from en   ar. first,
both phrase-based and id4
bene   t quite signi   cantly from properly normal-
izing and tokenizing arabic, while they are both
less sensitive to truecasing english.
the best
translation quality using either model was achieved
when all the id121 methods were applied
(ar: tok+norm+atb and en:tok+true), improv-
ing upon the baseline by more than 2+ id7. fur-
thermore, we again observe that the phrase-based
and neural translation systems perform comparably
to each other.

6 conclusion
we have provided    rst results on arabic neural mt,
and performed extensive experiments comparing it
with a standard phrase-based system. we have con-
cluded that neural mt bene   ts from morphology-
based id121 and is robust to domain change.

references
[badr et al.2008] ibrahim badr, rabih zbib, and james
glass. 2008. segmentation for english-to-arabic sta-

in proceedings of the
tistical machine translation.
46th annual meeting of the association for computa-
tional linguistics on human language technologies:
short papers, pages 153   156. association for compu-
tational linguistics.

[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2015. neural machine
translation by jointly learning to align and translate.
in iclr 2015.

[cho et al.2014] kyunghyun cho, bart van merri  enboer,
caglar gulcehre, dzmitry bahdanau, fethi bougares,
holger schwenk, and yoshua bengio. 2014. learning
phrase representations using id56 encoder-decoder
for id151. arxiv:1406.1078.

[cho et al.2015] kyunghyun cho, aaron courville, and
yoshua bengio. 2015. describing multimedia con-
tent using attention-based encoder-decoder networks.
multimedia,
ieee transactions on, 17(11):1875   
1886.

[cho2015] kyunghyun cho.

lan-
guage understanding with distributed representation.
arxiv:1511.07916.

natural

2015.

[chung et al.2016] junyoung chung, kyunghyun cho,
and yoshua bengio.
2016. a character-level de-
coder without explicit segmentation for neural ma-
chine translation. in acl.

[creutz and lagus2005] mathias creutz and krista la-
gus. 2005. unsupervised morpheme segmentation
and morphology induction from text corpora using
morfessor 1.0. helsinki university of technology.

[devlin et al.2014] jacob

devlin,

zbib,
zhongqiang huang, thomas lamar, richard m
schwartz, and john makhoul. 2014. fast and robust
neural network joint models for statistical machine
translation. in acl.

rabih

[el kholy and habash2012] ahmed el kholy and nizar
habash. 2012. orthographic and morphological pro-
cessing for english   arabic statistical machine transla-
tion. machine translation, 26(1-2):25   45.

[habash and sadat2006] nizar habash and fatiha sadat.
2006. arabic preprocessing schemes for statistical
machine translation.

[hamon and choukri2011] olivier hamon and khalid
choukri. 2011. evaluation methodology and results
for english-to-arabic mt. proceedings of mt summit
xiii, pages 480   487.

[hea   eld et al.2013] kenneth

hea   eld,

ivan
pouzyrevsky, jonathan h. clark, and philipp koehn.
2013. scalable modi   ed kneser-ney language model
estimation. in proceedings of the 51st annual meet-
ing of the association for computational linguistics,
pages 690   696, so   a, bulgaria, august.

[jean et al.2014] s  ebastien

jean, kyunghyun cho,
roland memisevic, and yoshua bengio. 2014. on

using very large target vocabulary for neural machine
translation. in acl 2015.

[kalchbrenner and blunsom2013] nal kalchbrenner and
phil blunsom. 2013. recurrent continuous translation
models. in emnlp, pages 1700   1709.

in proceedings of

[koehn et al.2003] philipp koehn, franz josef och, and
daniel marcu. 2003. statistical phrase-based trans-
the 2003 conference
lation.
of the north american chapter of the association
for computational linguistics on human language
technology-volume 1, pages 48   54. association for
computational linguistics.

[koehn et al.2007] philipp koehn, hieu hoang, alexan-
dra birch, chris callison-burch, marcello federico,
nicola bertoldi, brooke cowan, wade shen, chris-
tine moran, richard zens, et al. 2007. moses: open
source toolkit for id151.
in
proceedings of the 45th annual meeting of the acl on
interactive poster and demonstration sessions, pages
177   180. association for computational linguistics.
[lita et al.2003] lucian vlad lita, abe ittycheriah, salim
roukos, and nanda kambhatla. 2003. truecasing.
in proceedings of the 41st annual meeting on associ-
ation for computational linguistics-volume 1, pages
152   159. association for computational linguistics.
and
christopher d manning.
2016. achieving open
vocabulary id4 with hybrid
word-character models. arxiv:1604.00788.

[luong and manning2016] minh-thang

luong

[maamouri et al.2004] mohamed maamouri, ann bies,
tim buckwalter, and wigdan mekki. 2004. the penn
arabic treebank: building a large-scale annotated ara-
in nemlar conference on arabic lan-
bic corpus.
guage resources and tools, volume 27, pages 466   467.
[och and ney2003] franz josef och and hermann ney.
2003. a systematic comparison of various statis-
tical alignment models. computational linguistics,
29(1):19   51.

[pasha et al.2014] arfath

pasha,

mohamed

al-
badrashiny, mona t diab, ahmed el kholy,
ramy eskander, nizar habash, manoj pooleery,
owen rambow, and ryan roth. 2014. madamira: a
fast, comprehensive tool for morphological analysis
in lrec, pages
and disambiguation of arabic.
1094   1101.

[salameh et al.2015] mohammad salameh, colin cherry,
and grzegorz kondrak. 2015. what matters most
in morphologically segmented smt models. syntax,
semantics and structure in statistical translation,
page 65.

[sennrich et al.2015] rico sennrich, barry haddow, and
alexandra birch. 2015. id4
arxiv preprint
of rare words with subword units.
arxiv:1508.07909.

[setiawan et al.2015] hendra

setiawan,

zhongqiang
huang, jacob devlin, thomas lamar, rabih zbib,
richard schwartz, and john makhoul. 2015. statisti-
cal machine translation features with multitask tensor
networks. arxiv:1506.00698.

[srivastava et al.2014] nitish srivastava, geoffrey hin-
ton, alex krizhevsky, ilya sutskever, and ruslan
salakhutdinov. 2014. dropout: a simple way to pre-
vent neural networks from over   tting. the journal of
machine learning research, 15(1):1929   1958.

[sutskever et al.2014] ilya sutskever, oriol vinyals, and
quoc vv le. 2014. sequence to sequence learning
with neural networks. in nips, pages 3104   3112.

[zeiler2012] matthew d zeiler.

an adaptive learning rate method.
arxiv:1212.5701.

2012.

adadelta:
arxiv preprint

