5
1
0
2

 

p
e
s
4
1

 

 
 
]

g
l
.
s
c
[
 
 

1
v
8
7
4
3
0

.

1
0
6
1
:
v
i
x
r
a

deep learning applied to image and text

matching

master thesis in computer science

author:
afroze ibrahim baqapuri
afroze.baqapuri@ep   .ch

supervisors:
dr. fran  ois fleuret
francois.   euret@idiap.ch
dr. eric cosatto
cosatto@nec-labs.com

january 15, 2016

2

contents

1 introduction

1.1 organization of report

9
. . . . . . . . . . . . . . . . . . . . . . . . 10

2 literature review

13
2.1 deep learning in images and id161 . . . . . . . . . . . . 13
2.2 deep learning in text and nlp . . . . . . . . . . . . . . . . . . . 16
2.3 deep learning in image and text multimodal models . . . . . . . . 18
2.3.1
introduction . . . . . . . . . . . . . . . . . . . . . . . . . . 18
2.3.2 id74 . . . . . . . . . . . . . . . . . . . . . . 20
2.3.3
image and text mapping . . . . . . . . . . . . . . . . . . . 21
sentence generation from images . . . . . . . . . . . . . . . 24
2.3.4

3 resources

27
3.1 flickr8k . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
3.2 flickr30k . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
3.3 overfeat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
3.4 id97 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

4 research methodology

31

5 proposed model

37
5.1 abstract architecture . . . . . . . . . . . . . . . . . . . . . . . . . 37
5.2 details visual and textual models . . . . . . . . . . . . . . . . . . 39
5.3 unique training methodology . . . . . . . . . . . . . . . . . . . . 42
5.4 comparison to other models . . . . . . . . . . . . . . . . . . . . . 42

6 experiments and results

45
6.1 preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
6.2 id74 . . . . . . . . . . . . . . . . . . . . . . . . . . 46
6.3 training details . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
6.4 experimental results . . . . . . . . . . . . . . . . . . . . . . . . . 47
i2t and t2i training approach . . . . . . . . . . . . . . . 48

6.4.1

3

6.4.2 results of bow model
. . . . . . . . . . . . . . . . . . . . 49
6.4.3 id97 results . . . . . . . . . . . . . . . . . . . . . . . 49
6.4.4 results of id165 models
. . . . . . . . . . . . . . . . . . 51
6.5 are deep textual models necessary . . . . . . . . . . . . . . . . . 52
6.6 comparison with existing systems . . . . . . . . . . . . . . . . . . 54

7 conclusion

57
7.1 future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59

appendices

a acronyms

61

61

b extra resources

62
b.1 mnist . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
b.2 caltech256 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
b.3 id163
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
b.4 pascal voc 2008 . . . . . . . . . . . . . . . . . . . . . . . . . . . 63

c software

d further results of i2t and t2i training methodologies

bibliography

64

65

65

4

abstract

the ability to describe images with natural language sentences is the hallmark for
image and language understanding. such a system has wide ranging applications
such as annotating images and using natural sentences to search for images.
in this project we focus on the task of bidirectional id162: such a
system is capable of retrieving an image based on a sentence (image search) and
retrieve sentence based on an image query (image annotation). we present a
system based on a global ranking objective function which uses a combination
of convolutional neural networks (id98) and multi layer id88s (mlp).
it takes a pair of image and sentence and processes them in di   erent channels,
   nally embedding it into a common multimodal vector space. these embeddings
encode abstract semantic information about the two inputs and can be compared
using traditional information retrieval approaches. for each such pair, the model
returns a score which is interpretted as a similarity metric. if this score is high,
the image and sentence are likely to convey similar meaning, and if the score is
low then they are likely not to.

the visual input is modeled via deep convolutional neural network. on the
other hand we explore three models for the textual module. the    rst one is
bag of words with an mlp. the second one uses id165s (bigram, trigrams,
and a combination of trigram & skip-grams) with an mlp. the third is more
specialized deep network speci   c for modeling variable length sequences (sse).
we report comparable performance to recent work in the    eld, even though our
overall model is simpler. we also show that the training time choice of how we
can generate our negative samples has a signi   cant impact on performance, and
can be used to specialize the bi-directional system in one particular task.

5

6

acknowledgments

i would like to thank nec and epfl for giving me the opportunity to work
on such an interesting project. special thanks go to bing bai, iain melvin,
eric cosatto, igor durdanovic, and martin renqiang min for the many fruitful
discussions.

7

8

chapter 1
introduction

the ability to describe images with natural language sentences is the hallmark
for image and language understanding. it has signi   cant applications such as
searching images with natural sentences, and automatic captioning of images.
currently, commercially available image searching systems search the text ad-
jacent to an image, instead of searching the content of the image itself. this is
a severe bottleneck on the performance of these systems. advances in this    eld
will have far-reaching e   ects since images are ubiquitous on the internet and we
are always interacting with them.

humans can describe an image with relative ease. however, for comput-
ers this is not a trivial task. the di   culty arises mainly because the two in-
put modalities have very di   erent statistical properties, and cannot be directly
compared. for example, images have spacial dependency, while sentences have
temporal dependency (sequence). but, even for humans, the task is of highly
subjective nature, and various forms of descriptions exist. di   erent descriptions
could focus on di   erent aspects, or di   erent objects in the image, they could also
have di   erent level of detail in describing the content, and they could be abstract
like describing the mood or concrete like describing objects. all of these could
be correct simultaneously correct, yet very di   erent. modeling this variance is
very important for machine learning systems, and it is achieved in high-quality
data sets by having multiple humans describe the same image.

the recent work in this    eld has focus on two approaches: multimodal re-
trieval and sentence generation. multimodal retrieval deals with the tasks of
retrieving sentences from image queries (or image captioning) and retriev-
ing images from sentence queries (or image search). on the other hand, image
based sentence generation systems focus on creating natural and    uent sentences
directly from the image, which describe the contents of the image.

in our work, we focus on the former approach: multimodal retrieval. we

9

design our system taking motivation from traditional retrieval systems, in par-
ticular the supervised semantic indexing (ssi) [bai et al., 2009] system at nec
used for document retrieval. we expand on the system by using separate, spe-
cialized textual and visual models. we use these models to extract semantic
information from the input sentence and image, and then transform it into a
common vector space, where we can easily compare the two modalities using
traditional information retrieval approaches.

the goal of our project is to explore and understand the signi   cance of deep
learning techniques in this task. we use deep learning architectures and con-
cepts within the individual text and image modules. for the visual component
we experiment using deep convolutional neural networks, which have become
synonymous with image recognition [krizhevsky et al., 2012,lecun et al., 1998].
for the textual component we experiment with di   erent features and network
architectures. we try bag of word approach, id165 models, tf-idf features and
convolutional neural network for sequence embedding. in contrast to other pre-
vious work, our model is simpler since we don   t use more complicated models
like recurrent neural networks [karpathy and fei-fei, 2014] or recursive neural
networks [socher et al., 2014]. however, even with the simpler approach, our
model achieves comparable performance to some recent work in the    eld.

1.1 organization of report
the rest of the thesis report is organized as follows. in chapter 2 we present a
through overview of available scienti   c literature in the    eld. we start with deep
learning from a historical perspective and discuss its main ideas and signi   cance,
   rst in the image community and then in text processing. finally we discuss the
recent previous work in the combined    eld of images and texts as a multimodal
input.

following this, in chapter 3 we brie   y describe some of the important re-
sources used in our project. this includes data sets we trained and tested our
model on and some publicly available tools we used in our larger model.

in chapter 4 we provide an overview of research methodology and time line
of experiments we conducted. this chapter serves as an optional section. so the
reader can skip over it without loosing necessary information in understanding
our    nal models and experiments (reference to the chapter will be made whenever
we make a point which needs support from there). the material covers our
experience with the problem how it in   uenced the way we approached the task
(including the set backs we faced).

chapter 5 presents an overview and description of the model we use in our
it also

retrieval system including    ow charts and mathematical description.

10

explains some interesting features of the model, and how does it compare with
recent work.

moving forward, chapter 6 deals with the bulk of the experimentation done
on our model, and the results we get. we start o    by discussing the prepro-
cessing, then give details on the id74 used and meta parameters
selected. finally we conduct several experiments and report their results.

in chapter 7 we sum up our results by comparing them with other recent
models. we then discuss the signi   cance of our results and possible future work
in this direction.

finally, in appendix, we give a brief overview of the software environment
we used to train and test our models, and some additional resources we used
which did not become part of our    nal model.

11

12

chapter 2
literature review

in this section we will provide a summary of important research conducted in the
   eld of deep learning during the last decade. the literature review will comprise
of three parts:

1. deep learning in images and id161.

2. deep learning in text and nlp.

3. deep learning in image and text multimodal modeling.

the last part is most relevant to our task, but we feel a background discussion
is important to understand and appreciate the workings of it.

2.1 deep learning in images and computer vi-

sion

an important aspect of deep learning is to use end-to-end automatically trainable
systems which do not rely on human-designed heuristics. traditional machine
learning systems are divided into two modules. first, the f eatureextractor
module transforms the input data into low dimensional vectors which can be
easily matched and compared, and which are relatively invariant to distortions.
these are then fed into the classif ier module, which is general-purpose and
trainable. a major problem with this approach is that performance is largely
determined by human input, and the feature extractor part is task-speci   c so it
needs to be redone for every little task.

countering this traditional approach, [lecun et al., 1998] showed that hand-
crafted feature extraction can be advantageously replaced with automatic learn-
ing algorithms which operate directly on raw input data. the individual modules

13

can thus be replaced by uni   ed system which optimizes a global performance cri-
terion.

id161, and especially object recognition, has a long history of us-
ing deep multi-layered neural networks. [lecun et al., 1995] did a comparison of
hand-written digit recognition on the mnist data set, in which they compare
the performance of a multi-layered (deep) convolutional neural network (id98)
with traditional machine learning approaches like linear classi   er (logistic re-
gression), principal component analysis, and nearest neighbour classi   er. the
comparison shows that deep id98s outperform the traditional algorithms on
object recognition tasks, and reach state-of-the-art.

the convolutional neural networks are one of the    rst used deep learning
models, and they are biologically inspired variants of the ann. [hubel and
wiesel, 1968] found the existence of receptive    elds (small sub-regions in the
visual    eld) in the visual cortex of brain, and id98s (among other models)
try to emulate this behaviour.
in other words, they are specialized network
architectures speci   cally designed to recognize two dimensional objects, while
being invariant to exact position of the pattern and distortions. in the id98
each unit takes its input from a local receptive    eld on the layer below forcing it
to extract local features. furthermore units within a plane or f eaturemap are
constrained to share a single set of weights, this makes the operations performed
by a feature map to be shift invariant [le cun et al., 1990]. the weight    
sharing technique also reduce the number of free parameters, thereby reducing
the memory requirements and training complexity of the networks.

complete id98s are formed by stacking together multiple convolutional layers
(each with f eaturemap planes and local receptivef ields). sub-sampling layers
are also added improving invariance to shift and distortions. the entire network
is trainable with id119 using the back-propagation procedure. [lecun
et al., 1998] popularized the lenet-5 which is a id98 architecture performing
state-of-the-art on mnist hand writing recognition at the time it was published.
even during 1990   s it was evident that deeper and larger networks have the
tendency to perform better. however, this potential was overshadowed by the
outrageous time and memory which took to train larger networks - and was
not possible during that time. moreover, larger networks - being more powerful
at modeling - also easily over   t to training data, and there were not e   cient
techniques to combat this annoyance. even though near human performance
was reached for simple task like hand written recognition, but this was not
reciprocated to objects recognition in realistic settings which exhibit considerable
variability.

in 2009 [deng et al., 2009] released teh id163 data set comprising over 15
million high-resolution images labeled into more than 22,000 categories. it has

14

been well understood that training complicated data with high variability using
very few examples leads to severe over   tting in the id98s, so the release of this
large-scale data set was a big step for object recognition problem. by this time
computer hardware technology had also progressed enough to train much larger
and deeper networks in reasonable amount of time. [krizhevsky et al., 2012] used
graphical processor units (gpus) for a very fast an e   cient implementation of
their id98, which has 650,000 neurons and 60 million parameters (in contrast
the lenet-5 had 60,000 free parameters). they entered their neural network
in the ilsvrc-2012 competition and achieved a winning top-5 test error rate
of 15.3%, which was considerably better than the state of the art (second-best
entry had 26.2% error rate).

besides larger data set and larger networks, they also empirically showed
the importance of some useful techniques for avoiding over   tting. the easiest
illustrated way is to use data augmentation to increase the size of data set.
they take    ve di   erent patches of the original image (along with their horizon-
tal re   ections) thus increasing the training data by a factor of 10 - although the
additional images are very close the original one. they also also alter intensities
of the colour channels to add further data augmentation. these techniques are
useful for adding more shift, inversion, illumination, and colour invariance to our
model. dropout [hinton et al., 2012] is another technique for combating over-
   tting, by reducing complex-co adaptions of neurons. therefore a single neuron
is forced to learn more robust features without relying on other neighbouring
neurons. pretraining the neural network in a greedy layer-by-layer fashion with
an unsupervised objective function is another popular technique [hinton et al.,
2006, sch  lkopf et al., ]. the intuition behind this idea is that unsupervised
training will give a good initialization of weights for the neural network based
on the actual statistical properties of the data it will be used for (e.g. object
images, human speech, etc.) instead of random initializations which often get
stuck in poor local minimas. following this the network can be f ine    tuned on
a supervised task such as object recognition.

mathematically speaking, the id98 transforms the into a low dimensional
feature vector representation. in this way a good id98 model can also act as
a good feature extractor for images, and the resulting images can be used in
more complicated tasks. [sermanet et al., 2014] display this concept for ob-
ject localization. they train a id98 for classi   cation on the id163 data
set, and for localization take they replace the    nal classi   cation layer with a
regressionnetwork. this regression network is simply an mlp with two hid-
den layers of 4,096 and 1,024 units, connected    nally to the output layer of 4
units which predicts the coordinates of the bounding boxes. the    nal layer
is class-speci   c having 1,000 versions (one for each class) while the rest of the
regression network shares weights. during localization only the regression net-

15

work   s weights are updated, and the remaining larger network only acts as a
feature extractor. they run the classi   er and the regressor simultaneously since
they share most of the network, in this way they get a bounding box for each
class along with a con   dence number based on classi   cation con   dence.

2.2 deep learning in text and nlp

a seminal paper in the domain of statistical learning applied to natural language
processing (nlp) was written by [bengio et al., 2003]. classical machine learning
approaches to nlp calculate id165 conditional probabilities based simply on
the co-occurrence frequencies of words in a document. these models construct
tables of of conditional probabilities for a next given a    xed context (of previous
n     1 words). a fundamental problem with this is the curse of dimensionality,
meaning that the possible combinations of contexts grows exponentially with
n, thus making the models quickly intractable. another problem is that as
the size of context increases, the occurrence of sequences gets extremely rare
in a document, thus undermining the statistical relevance of their id203
distributions.

the authors of the paper attempt to solve this problem by using a neural
network to to learn distributed representation of words. this distributed
representation, which is sometimes termed id27 in modern litera-
ture, is feature vector which conveys semantic and syntactic meaning of the word.
in other words, all the words in the vocabulary can be transformed into a vector
representation of a    xed dimension (usually much smaller than the original size
of the vocabulary). furthermore the joint id203 of the entire sequence (of
arbitrary length) can be expressed as a function of these feature vectors.
in
this paper they propose to use feed forward neural networks to compute the
id203 of the next word in the sequence given the previous n words (in the
form of these word vectors) . the advantage of neural networks is that they can
be trained in trained to jointly learn the word feature vectors (   rst layer of the
neural network) and the parameters of the id203 function (free parame-
ters of the network) using a common global objective function (maximizing log
likelihood).

the advantage of using these word vectors is that it would escape the curse
of dimensionality. the words would now be represented in a    xed real-valued
(comparatively) low-dimensional vector, and similar words would have similar
meanings. an example given by the paper is that the sentences "a cat is walking
in the bedroom" would have similar representation to "the dog was running in a
room", since the model would learn the similarity of the individual words in the
sequence (dog, cat), (the,a), (bedroom, room), etc. it is noteworthy that they

16

report 24% and 8% improvements in terms of perplexity over the best id165
results for the task of predicting the next word in a sequence performed on two
large scale data sets. the idea of using neural networks for id38
in fact dates back to [miikkulainen and dyer, 1991], however the authors of the
paper under discussion were the    rst to propose a large scale statistical model
which learns distributed dense representations of words in a sequence and using
them to automatically estimate the joint id203 function.

building on to this, [collobert and weston, 2008] propose a single uni   ed con-
volutional neural network architecture that performs well for various challenging
nlp tasks, such as part-of-speech tagging, chunking, id39,
and id14. traditional approaches analyze these tasks sepa-
rately, using hand-crafted features speci   c for each task which makes this ap-
proach intractable for complicated tasks. the model they propose - in contrast
- has a deep architecture composing of many layers which can be trained in an
end-to-end fashion. the    rst layer extracts features for each word (word embed-
dings, or distributed word representations). the second layer extracts features
form the sentence treating it as a sequence with structure. variable length in-
puts are incorporated by passing a convolutional neural network over the word
features and then performing max-pooling over each resulting dimension to give
a    x length vector. the following layers are classical nn layers (fully connected).
since all of these tasks are related , they argue that it would make sense to
share some features between these tasks in order to improve the generalization
of the network, and so they propose multitask learning approach to jointly train
the model on all these tasks. they design their network architecture to share the
layers closer to the input (these layers would encode id27s, which
should intuitively be common to all language-related tasks). as we go deeper
into the network, the features extracted become more complex and abstract, and
so the last layers of the network are task speci   c.

they also pretrain their network with unlabeled training data , since it is
available in much vast quantities as compared to labeled data. they train a
language model with an unsupervised ranking criterion, which would predict if
the middle word in the sequence is related to the context or not. for positive
examples they took    xed length phrases from wikipedia, and they generated the
negative examples by substituting the middle word in a valid phrase by any
other random word. they demonstrated that this approach trains a powerful
language model by showing that word vectors which are close to one another in
the embedding space are also close in semantic meaning. for example "france",
"spain" and "italy" have close vector representations to one another, as well as
"scratched", "smashed" and "ripped".

[mikolov et al., 2013a] propose yet another unsupervised approach of learn-

17

ing vector representations of word. they propose a skip gram model, which
predicts the surrounding words in the window given the center word. this is
directly opposite of earlier approaches which predict the centre word giver the
context window. given a sequence of training words, the objective criterion
is to maximize the average log id203 of the words in surrounding, condi-
tional on the centre word. one interesting feature about their model is that
it preserves linear regularities among the learned representations. this makes
it possible to perform interesting analogical reasoning using simple vector arith-
metic. for example, the result of the vector calculation: vec("king") - vec("man")
+ vec("woman") is closest to vec("queen"). also, vec("russia") + vec("river") is
close to vec("volga_river").

furthermore, [mikolov et al., 2013b] extends the previous model to include
vector representations for phrases as well. they based their work on the insight
that idiomatic phrases like "boston globe" and "air canada" can be seman-
tically understood well by combining the individual words within the phrases.
therefore they treated phrases as individual tokens (just like words), but limit-
ing their vocabulary to only those phrases which appear frequently together, and
infrequently in other contexts. they train vector embeddings of dimensionality
300 for these words and phrases, and release the on the internet for public use 1.

2.3 deep learning in image and text multimodal

models

there has been a lot of progress in multi-label classi   cation problem of asso-
ciating images with individual words or tags. however, the more challenging
problem of associating images with complete natural sentences has only recently
started to gain attention. the research in this area has focused primarily on two
tasks, namely:

1. mapping images and sentences into a combined space
2. generating descriptions of images in terms of complete and natural sen-

tences.

the    rst poses it as an information retrieval problem, while the later treats tit
as a id86 problem.

introduction

2.3.1
[hodosh et al., 2013] have written one of the seminal papers in this    eld, provid-
ing an interesting discussion on the problem statement, an in-depth comparison

1https://code.google.com/p/id97/

18

of the available data sets (including what kind of data is required for good mod-
eling), an analysis of modeling techniques employed in the early stages of this
   eld, and a detailed discussion of the various id74 used in di   erent
previous related works.

[hodosh et al., 2013] go into the philosophy of image description by arguing

that there are three di   erent kinds of image descriptions:

1. conceptual descriptions identify what is being depicted in the image. they
are concerned with the concrete descriptions of the depicted scenes and
entities, their attributes and relations, as well as events they participate
in.

2. non-visual descriptions provide additional background information that
cannot be obtained from the image alone, e.g about the situation, time or
location in which the picture was taken.

3. perceptual descriptions capture low-level visual properties of images, e.g if

it is a photograph or a sketch, or what colors or shades dominate.

they argue that out of these three, conceptual descriptions are the most relevant
for image understanding tasks. they observe that using user generated captions
uploaded with images in popular image-sharing websites (such as flickr.com) do
not serve as good training data because people tend to provide information that
could not be easily obtained just from looking at the image itself. for example,
the kind of description our models require is "three people setting up a tent"
while people tend to provide captions like "our trip to the olympic peninsula".
hence they establish a need of data collected purposefully for this speci   c task.
most of good quality data sets are collected visa id104 (for exam-
ple using amazon mechanical turk) in which multiple descriptive captions are
assigned to each image. pascal1k [farhadi et al., 2010], flickr8k [rashtchian
et al., 2010], flickr30k [young et al., 2014], and ms-coco [lin et al., 2014]
are examples of such good quality data sets.

some of the earliest work in this    eld used shallow learning techniques and
   xed (as opposed to learn-able) image and text features. [makadia et al., 2010,
ordonez et al., 2011] use nearest neighbour search knn! (knn!) for image
annotation and image description respectively. on the other hand, [hodosh
et al., 2013] use kernel canonical correlation analysis kernel conanical correla-
tion analysis (kcca). kcca [bach and jordan, 2003] is technique which takes
training data consisting of pairs of corresponding items drawn from two di   er-
ent modalities and    nds maximally correlated linear projections of each set of
items (by    rst mapping the original items into higher-order spaces) into a newly
induced common space. similarly, popular shallow image features include sift
descriptors [lowe, 2004] and simple bag if words (bow) kernel.

19

2.3.2 id74
since image description is a subjective task, the ideal setting for evaluating a sys-
tem would be averaged human judgement. however, since human judgement is
expensive and slow, there have been a number of metrics employed in evaluating
these systems. these metrics can be divided into two categories:

    metrics for text generation systems.
    metrics for ranking systems.
id7 [papineni et al., 2002] and id8 [lin, 2004] scores are popular
metrics in the automatics image description generation systems. originally, they
are standard metrics for machine translation and summarization respectively, but
have been used to evaluate multiple id134 systems [vinyals et al.,
2014,ordonez et al., 2011,gupta et al., 2012]. given a caption c and an image i
with a set of reference captions ri, the id7 score of a proposed image-caption
pair (i, s) is based on the id165 precision of s against ri, while id8 is
based on corresponding id165 recall. if cs(w) is the number of times word w
occurs in s, they are de   ned as:

id7(i, s) =

rou ge(i, s) =

p
w   s min(cs(w), maxr   ricr(w))
p

w   s cs(w)

p

w   r min(cs(w), cr(w))
r   ri

w   r cr(w)

p
p

p

r   ri

[hodosh et al., 2013] try to compare id7 and id8 scores against hu-
man judgements, and examine to what extents do these both agree. based on
results they question these metrics    usefulness for evaluating id134
systems. [reiter and belz, 2009] also argue that they are more useful as metrics
for    uency, but poor measures of content quality of language generation. how-
ever, unless a more suited metric is devised, these score are continue to be used
for evaluating modern id134 systems.

next, we will touch upon metrics which can be used to evaluate the quality of
a ranked list in information retrieval tasks. recall@k (r@k) is the percentage
of test queries for which a model returns the correct result among the top k
results.
it is especially useful in the context of search where a user may be
satis   ed with the    rst k results containing a single relevant item. conversely,
median rank is equal to the value of k for which the r@k is equal to 50%.
the k in r@k typically varies between k = 1, 10. [hodosh et al., 2013] consider
k = 1 as a very strict threshold and, after comparing it with human judgement,
view it as a lower bound on actual performance.

20

in our systems queries can have multiple (variable number) of relevant an-
swers since each test image may be associated with multiple relevant captions,
and each test caption may deem    t for multiple images besides the one it was
originally written for. r-precision [] is the metric of choice in these condi-
tions, since it is a single number which allows us to rank models according to
their overall performance (no threshold like k) the r-precision of a system with
query qi and known relevant results ri is de   ned as its precision at rank ri. in
simpler terms it is the percentage of relevant items among the top ri responses
returned by the system.

image and text mapping

2.3.3
[frome et al., 2013] were one of the    rst ones to use deep learning in ranking
images with text.they call their model devise or deep visual-semantic embed-
ding. their original task was to improve performance of their image classi   cation
system for large number of object categories and for labels on which the visual
system was not trained (zero shot prediction). they propose to leverage infor-
mation from textual information of image to improve their object classi   cation
performance.

they begin by pre-training an e   cient deep convolutional neural network
(id98) for visual object recognition, based on the architecture by [krizhevsky
et al., 2012].
in parallel, they pretrain a simple neural language model well-
suited for learning semantically-meaningful vector representations of individual
words (id27s) using skip-gram text modeling architecture [mikolov
et al., 2013a, mikolov et al., 2013b]. following this, they construct the devise
model by chopping of the top layers of the id98 and re-training it to predict the
id27 vector of the corresponding image label.

they used hinge margin ranking loss criterion in the second phase of their
training, and observe signi   cant improvements over using l2 loss criterion. al-
though they never trained or tested their system for natural image descriptions,
but they did in   uence a lot of work in this    eld. [karpathy et al., 2014] imple-
mented their model for image and sentence mapping to compare performance
against their own system.

[gong et al., 2014] use id98 for modeling images in their image based sen-
tence retrieval system. they    rst embed image and sentence into a common
space and then use it to rank the pair. they perform a comparison between
using 4,096 id98 activations (trained on id163 [deng et al., 2009]) as im-
age features versus kcca with 4,000 dimension    xed features. there was a
reported 9.5% improvement in recall@10 for id98 over kcca, even when the
id98 activations remained    xed and it was not    ne-tuned on image-text data
set.

21

unlike previous work, [karpathy et al., 2014] go on a    ner level and map
fragments of images (objects) and fragments of sentences (dependency tree re-
lations) into a common embedding space. their model works for bi-directional
retrieval: image given a text query, and text given an image query. they inter-
pret an image being made up of multiple entities, and therefore propose to break
it down into more manageable fragments.arti   cial neural network (ann)s are
used to compute vector representation of these image and sentence fragments
in a multimodal embedding space, and the dot product between a pair of these
vectors (one image fragment and one sentence fragment) are interpretted as a
compatibility score. the global image-sentence compatibility score is computed
as a    xed function of their fragments.

their image model comprises of a region convolutional neural network (rid98)

which detects objects in the images and returns their bounding boxes. for image
fragments they use the top 19 locations detected by the rid98 and the complete
image, so each image is broken down into 20 fragments. following this another
id98 is applied on each of these fragments to return a 4,096 dimensional em-
bedding vector representing the image fragment. the architecture of this id98
closely resembles that by [krizhevsky et al., 2012]. on the other hand, the
sentence fragments are considered as edges of the sentence   s dependency tree.
therefore, each sentence fragment consists of two words which are joined in
any stage of the dependency tree. each word in the dictionary of 400,000 is
represented using 1-of-k encoding, and vector embeddings for the words are ob-
tained through an unsupervised objective and    xed throughout the training.
the sentence fragment score is calculated using these id27s as well
as separate embeddings which represent the type of relation between the words.
they plot a matrix where the rows represent the image fragments and the
columns represent the sentence fragments. each element of the matrix (or box)
shows the dot product score between those two multimodal fragments. they
de   ne two kinds of objective functions to train their models:

    fragment alignment objective: this objective explicitly learns the rep-
resentation of sentence fragments in the visual domain. it encodes the in-
tuition that if a sentence fragment is contained in an image, at least one
of the boxes should give a high score with that sentence fragment, while
all other boxes corresponding to images which do not contain this sentence
fragment (in their descriptions) should have a low score with this fragment.
it also favours that there should be at least one high scoring box in each
column of the matrix (meaning every sentence fragment should be matched
by at least one image fragment).

    global ranking objective: this objective tries to enforce that the image-
sentence similarities are consistent with the ground-truth. first the global

22

similarity score is computed by averaging the pairwise fragment scores.
after this the entire system is trained in an end-to-end fashion with the
margin ranking loss criterion.

the model improves performance when using a combination of these two objec-
tive functions. they also report that dividing image in fragments has perfor-
mance improvement versus treating the entire image as a single fragment, and
that dependency tree sentence fragments perform consistently better than bag
of words (bow) and bigram features. finally they found that    ne-tuning the
image score calculating id98 on image-text data further improves results.

[socher et al., 2014] go one step ahead and use the entire sentence depen-
dency tree - as opposed to only its edges as sentence fragments - to model the
sentences using dt-id56 (dependency tree id56s). they
argue this is important for accurately representing complicated sentences in the
visual domain. they test their model against other recurisive and recurrent neu-
ral networks, kcca and bow baseline, concluding that dt-id56 out performs
all of these in the task of image and sentence mapping. dt-id56 also give sim-
ilar vector representations to multiple captions which describe the same image,
adding more weight to their model. the dt-id56 is di   erent from other pre-
vious id56 models [socher et al., 2011] which are based on
constituency tree (ct-id56). they report that the sentence vectors computed
by dt-id56 are more apt at capturing the meaning of the sentence in terms
of "visual representation". moreover dt-id56 vector embeddings are more ro-
bust to changes in syntactic structure or word order as opposed to ct-id56s or
recurrent neural networks. the    nal sentence embedding is a vector of length
50.

for the image side, they train a deep id98    rst using unsupervised objective:
reconstruct the input keeping the neurons sparse, followed by supervised learning
on classifying 14 million images of id163 into 22,000 categories. the id98
used was particularly large, with 1.36 billion parameters, and they achieved
18.3% precision@1 on the full id163 data set. following the id98 training,
they chop o    the last layer to get 4,096 dimensional vector embeddings for the
images.

during the multimodal mapping, the 4096 they transform the image repre-
sentation vector to the same size as sentence vector. following this - like other
similar works - they take a dot product of these to produce a compatibility score
and back propagate error using margin ranking loss criterion. however, the 4,096
dimensional image vector and 50 dimensional sentence vector are    xed and are
not updates during this phase. they report improved results over the pascal1k
data set [farhadi et al., 2010] compared to bag of words (bow), ct-id56, re-
current ann and kcca models.

23

[karpathy and fei-fei, 2014] build upon their prior work on image text
matching using fragments of image and sentence [karpathy et al., 2014] (dis-
cussed above), add a new features to increase the modeling capacity of their
model. their initial approach translated words directly into vector embeddings,
and did not consider word ordering and context. to address this problem they
use a bi-directional recurrent neural network (bid56) to model the input text
sequence and convert it into a complete sentence embedding. they report sig-
ni   cant improvements using this approach.

taking a somewhat di   erent approach, [srivastava and salakhutdinov, 2012]
use multimodal deep id82s (dbm) for this task. the model
learns joint id203 density over the space of multimodal inputs. missing
modalities can be    lled in by sampling from the conditional distributions over
them. for example we can learn the joint id203 p(vimg, vtxt |   ) ,where vimg
is the vector representation of an image and vtxt is the vector representation of
a text sentence. once this distribution is estimated we can draw samples from
the conditional probabilities to    ll in the missing modalities: drawing samples
from p(vtxt | vimg,   ) to give the missing text sentence, and vice versa.

a multimodal dbm is an extension of an dbm to model multimodal inputs.
a dbm is formed by stacking together rbms, in other words a multilayer rbm,
which are usually trained in a greedy layer-wise strategy. they construct two
independent dbms: an image-speci   c dbm and a text-speci   c dbm. next these
two are connected together via another rbm layer to construct the multimodal
dbm. in other words, the outputs of the two dbms are connected to a another
layer of binary hidden units on top of them, called the "joint representation".
the intuition behind this model is that each data modality has very di   erent
statistical properties which make it di   cult for a single hidden layer model to
directly    nd correlations across modalities. this di   culty is overcome by putting
layers of hidden units between the inputs of both modalities (the separate dbms)
they evaluate their model on information retrieval for multimodal and uni-
modal queries. in multimodal query, the aim is to give higher similarity score
to the image and text pairs belonging to the same instance, over false pairings.
while in unimodal query, either text or image is provided, and the model predicts
the missing modality out of a pool of possible options.

sentence generation from images

2.3.4
now we will brie   y touch upon the related task of generating natural sentences
from images. although it is not the focus of our project, but it is still to in-
teresting to see the approach taken in that area, especially since there has been
a growing interest in it recently. [kiros et al., 2014] present a model in which
they use a convolutional neural network (id98) for this task. the id98 has four

24

input pathways, one for an image and the remaining three for words. the idea is
that, given an image and three previous words, the model learns to predict the
next word in the sequence. in this way they learn a joint "multimodal language
model".

recurrent neural networks (id56) are quite popular for text generation, and
so many researchers use them in this task, albeit in di   erent settings [karpathy
and fei-fei, 2014, vinyals et al., 2014]. [vinyals et al., 2014] are in   uenced by
modern ann based machine translation systems, and they employ a encoder-
decoder type architecture for their model. speci   cally, they use a id98 as an
encoder and an id56 as a decoder. the id98 encodes the input image in vector
space feature embeddings which are are input to the id56. the id56 takes
the image encoding as input and the word generated in the current time step
to generate a complete sentence one word at a time. special words have been
marked to tell the model that it is the starting word and for predicting the
   nishing word. they also use long short term memory (lstm) inside the id56
so that it has some memory of older generated words. they train their model to
directly optimize the log likelihood of the target sentence given the input image.
using a similar approach, [karpathy and fei-fei, 2014] also use an id56
trained on multimodal inputs. for each input image feature vector, and the
current generated word (starting at a    xed word) it learns to predict the next
word in a sequence (ending at a    xed word).

25

26

chapter 3
resources

we employed various data sets and resources in our research and experimenta-
tion. below we will provide a brief description for them. most of the high quality
data set is collected through crowd sourcing methods, labeled by human input.
although some large-scale automatically generated data sets exist (for example
using user provided annotations when uploading images on photo-sharing web-
sites). however, [hodosh et al., 2013] makes a convincing case about the lack
of pertinence of these captions with the objectives of our task, and hence we do
not use them (like many researchers in the domain).

3.1 flickr8k
flickr8k 1 is a data set comprising of 8,092 images collected from the flickr
website [rashtchian et al., 2010]. each image has    ve captions along with it,
which describe the contents of the image. the images in this data set focus on
people or animals (mainly dogs) performing some action. the images tend not
to contain well known locations, but are manually selected to depict a variety of
scenes and situations. the images are captioned by human captioners (   ve for
each image) using id104 via amazon   s mechanical turk 2.

in order to avoid grammar mistakes, the captioners (who were only from
us) had to pass a brief quality control test based on spelling and grammar.
following this, they were asked to write sentences that describe the depicted
scenes, situations, events and entities (people, animals, other objects). multiple
captions were collected to model the inherent variability of di   erent humans in
describing the same image. as a consequence, the captions of the same images
are often not direct paraphrases of each other: the same entity or event or

1http://nlp.cs.illinois.edu/hockenmaiergroup/8k-pictures.html
2https://www.mturk.com/

27

situation can be described in multiple ways (man vs. bike rider, doing tricks vs.
jumping), and while everybody mentions the bike rider, not everybody mentions
the crowd or the ramp.

it is worth mentioning that when accessing the data set, we found out that
several of the images had been removed from flicker (the authors gave links
redirecting to the original images 3) so we could only get download 6,793 out of
the total 7,678 images. therefore, although we can use to judge the performance
of our algorithm on the data set, we cannot use it in comparison to previous work.

3.2 flickr30k
flickr30k 4 is an extension over the flickr8k data set, comprising of 31,783
images collected by [young et al., 2014]. similar to its counterpart, each image is
associated with    ve captions written by humans using the amazon   s mechanical
turk. the images consists of everyday activities, events, and scenes.

it is important that the annotators are not familiar with the speci   c entities
and circumstances depicted in them, to avoid that they give overly speci   c anno-
tations. for example the annotations "three people setting up a tent" are more
relevant to the task as compared to "our trip to the olympic peninsula" (for the
same image - the latter is likely an annotation given by a person familiar with
the signi   cance of the image). moreover di   erent annotators use di   erent levels
of speci   city, from describing the overall situation (performing a musical piece)
to speci   c actions (bowing on a violin).this variety of descriptions associated
with the image is necessary to induce similarities between expressions that are
not trivially related by syntactic rewrite rules.

3.3 overfeat
overfeat 5 is a publicly available visual feature extractor based on the convo-
lutional neural network submitted by [sermanet et al., 2014] to ilsvrc-2013
large-scale object recognition competition. at the time of its release it ranked
4th in classi   cation, 1st in localization, and 1st in detection tasks of ilsvrc-13
data sets. we use the accurate version of their model which has 144 million
free parameters and 5.4 billion connections, and reaches performance of 14.18%
on the competition validation set. their network architecture is similar to the
architecture by [krizhevsky et al., 2012], with the addition that it is trained on
images at multiple scales, and it has improved id136 step.

3http://nlp.cs.illinois.edu/hockenmaiergroup/8k-pictures.html
4http://shannon.cs.illinois.edu/denotationgraph/
5http://cilvr.nyu.edu/doku.php?id=software:overfeat:start

28

3.4 id97
id97 6 is a publicly available tool which provides an e   cient implemen-
tation of learning continuous bag of word, and skip-gram based vector repre-
sentations for words. the model and implementation is based on the work of
and released by [mikolov et al., 2013b]. in addition to the implementation, they
also provide vector representations of words and phrases which they learned by
training this model on google news dataset (about 100 billion words). these
are 300-dimensional vectors for 3 million words and phrases. an interesting fea-
ture of these vector representations are that they capture linear regularities in
the language. for example the result of the vector calculation: vec("madrid") -
vec("spain") + vec("france") is closest to vec("paris").

6https://code.google.com/p/id97/

29

30

chapter 4
research methodology

this section will provide an overview of research methodology and time line of
experiments we conducted. most of the experiments mentioned in this chapter
are not directly involved in our    nal results, and so the reader can skip over this
chapter if they wish to. any material which is deemed necessary to understand
our model will be reiterated in the next chapter. the reason for this chapter   s
inclusion is to make the reader understand how our experience with the problem
in   uenced the way we approached the task (including the set backs we faced),
and how we arrived to our    nal model.

we decided to treat the problem of image descriptions as an information
retrieval task, and decided to follow a ranking approach for matching images
and sentences. this decision was in   uenced by prior work at nec relating to
ssi (supervised semantic indexing) [bai et al., 2009] which is a system for doc-
ument retrieval based on a query sentence. the basic idea is to generate vector
embeddings for di   erent texts (query and document) which contain semantic
information about the original text. these embeddings act as abstraction of the
content in text, and can be used for similarity comparisons between di   erent
texts.

ssi is quite similar to our problem, with the di   erence that our inputs are
multimodal. in ssi both query and result are of textual nature, while for us,
one is text and the other is image. the presence of multimodal inputs makes our
task much more challenging, therefore we would need to change the architecture
to cater for this complexity.

we are interested in making our model bi-directional, meaning that it would
be able to retrieve an image for a textual sentence query (image search) and a
sentence for an image query (image annotation). following this goal, we aim
to make our model di   erent in two signi   cant ways from the ssi architecture:

31

1. we will divide our network into a visual model and a textual model.
the visual model will be fed with images and the input for textual model
will be images. the goal of each model will be to e   ciently extract seman-
tic features from their respective input signals.

2. we will make the overall architecture deeper. since both our inputs be-
long to di   erent modalities and di   er in statistical properties, transforming
them into a common embedding space would be more challenging and we
would need the modeling power of deeper networks (with more non linear
layers)

we follow the above guidelines to design our model. for the visual model
we use a id98 (convolutional neural network) initialized from random weights.
for the textual model we use a simple mlp with a single hidden layer, and we
treat the sentence as a bag of words (bow). we trained this model in an end-
to-end fashion with margin ranking criterion. the objective of the criterion is to
maximize the distance between a positive and a negative example (giving higher
score to the positive example and lower to a negative). a negative example is
generated by making a random false pairing with a sentence and image from our
training examples (whose ground truth we know). at each iteration we give a
positive example and a randomly generated negative example. the gradient is
back propagated through the entire network to train it in an end-to-end manner.

however the model failed to perform well when trained on fickr8k data set.
the training error decreased with increasing number of epochs, but the error on
validation set error seem to remain very close to random, as illustrated in figure
4.1

we tried to analyze our results and investigate into a possible reason for
failure. changing di   erent hyper parameters like learning rate, hidden layers
and hidden units did not seem to produce any signi   cant e   ect. since ssi has a
similar architecture and it seems to perform well on only textual input data, so
we could think of three possible reasons of the models failure:

1. more training data required as the model seems to start over-   tting.
2. our id98 not powerful enough to produce good visual representations.
3. our model architecture not well suited for the task.

the id98 we used was made out of 5 convolutional blocks followed by 2 fully
connected layers. each convolution block had a convolution layer, non-linearity
layer, sub-sampling layer, dropout layer and (subtractive) id172 layer.
we decided to    rst verify if the id98 was working as expected by testing it

32

figure 4.1: our model fails to perform on flickr8k data set. error is the
percentage of samples for which the model gives higher score to the positive
sample over the negative sample. 50% error speci   es random performance.

in isolation. we did this by using it for a much simpler task: classi   cation of
handwritten digits from the mnist data base. the model seemed to perform
well without tweeking much parameters, verifying that it was at least functioning
properly (refer to figure 4.2 for classi   cation results).

following this we tried to use it on a more complicated task: classi   cation of
objects into 256 categories from caltech256 data set. however, the model failed
to perform well on this task, as illustrated in figure 4.3. we concluded that
the reason for bad performance on the flickr30k ranking task and caltech256
classi   cation task was due to the same reason. our id98 was not able to model
the complicated data set well enough. in other words, since the number of images
was relatively limited and the amount of variation in the images was rather high
(especially true for flickr30k data set), the id98 was not able to generalize, and
model the relevant statistical properties well enough. therefore we concluded
that we needed to pre-train our network with larger data set before attempting
to train it on these problems. the advantage of pre-training would be that the
model would have very good initialization, and would be able to extract good
visual features from the images which could be used for our task.

33

181624010203040506070# epochserror (percentage)  training setvalidation setfigure 4.2: our randomly initialized id98 performed well on classifying mnist
data set. error is the percentage of samples classi   ed correctly (over 10 cate-
gories).

instead of wasting scarce time to pretrain our network from scratch on a
much larger data set (which could take weeks), we decided to use a publicly
available id98: overfeat 1. for details of this model refer to chapter 3.3.
however, before plugging overfeat network directly into our original task we
decided to    rst test it on classifying caltech256 data (on which our id98 failed).
we chopped o    the last softmax layer of overfeat and replaced it with a small
neural network (1,000 unit linear layer, relu non-linearity, another 1,000 unit
linear layer, and    nally a 256 unit softmax layer). the parameters of id98
were    xed and not    ne-tuned on the new data, and the parameters of the small
network on top were only updated. this model managed to classify caltech256
signi   cantly better than our original randomly initialized id98 (classi   cation
results in 4.4).

we also decided to do a small toy experiment to test the image and text mul-
timodal ranking approach based on caltech256 data set. for the textual input
we used the class labels corresponding to each image, which are incorporated

1http://cilvr.nyu.edu/doku.php?id=software:overfeat:start

34

15101520051015202530# epochserror (percentage)  training setvalidation setfigure 4.3: our randomly initialized id98 failed to perform well on classifying
caltech256 data set. error is the percentage of samples classi   ed correctly
(over 256 categories).

easily since it is a simple id159. often the label were hyphenated
like "baseball-bat" and "bear-mug"; in these cases we decided to break into indi-
vidual words (so images belonging to these images would have two word inputs
instead of the usual one). the total vocabulary size was 322 words. to our satis-
faction, the this toy ranking task also performed well, reinforcing our con   dence
in the model. the results are shown in 4.5. therefore, we    nally decided to do
further experiments with overfeat id98 model.

35

1255075100707580859095100# epochserror (percentage)  training setvalidation setfigure 4.4: overfeat id98 performed well on the classi   cation task on cal-
tech256 data set. error is the percentage of samples classi   ed correctly (over
256 categories).

figure 4.5: overfeat id98 performed well on the ranking task on caltech256
data set. error is the percentage of samples for which the model gives higher
score to the positive sample over the negative sample. 50% error speci   es random
performance.

36

11020304050020406080100# epochserror (percentage)  training setvalidation set151015200246810# epochserror (percentage)  training setvalidation setchapter 5
proposed model

5.1 abstract architecture
our model is inspired by the previous work at nec of ssi (supervised semantic
indexing) [bai et al., 2009] which is a textual retrieval system. we aim to extend
this model to multimodal inputs: in particular images and text. we will focus
on bidirectional retrieval. in other words, retrieving images based on a sentence
query (image search) and retrieving sentences based on an image query (image
annotation). the sentences are complete and natural, constructed by human
annotators, as opposed to individual and isolated tags.

the overall abstract-level architecture of our complete model is illustrated in
figure 5.1. the two sub-models p osn et and n egn et have identical architecture
and they share weights. for a pair of text and image input, they calculate the
score s, which is a measure of the similarity between the corresponding text and
image (based on cosine distance). here spos is the similarity score of the positive
input pair and sneg is the similarity score of negative input pair. positive pair
means that the sentence positively describes the image (as determined by human
judgement). negative pair means that the image and sentence are not related,
and the pair is generated by simply replacing the sentence (or the image) in a
positive pair by another random sentence (or image).

at each iteration we simultaneously present a positive pair and a negative
pair to the p osn et and n egn et respectively. the objective criterion penalizes
if spos is not greater than a certain threshold above sneg. intuitively it means
that the system is trained to give high score to similar pair or sentence and
image, and a low score to a dissimilar pair. therefore the network learns to
score semantic relevance between the pair of multimodal inputs.

as discussed previously, in our data set there are    ve sentences corresponding
to each image. each one of them is labeled by a di   erent person, in order to

37

model natural variance within descriptions. we use all    ve sentences during the
training process, so each image is samples    ve times during one full epoch (in a
random order). we believe it is extremely important to use all    ve sentences to
make the model more robust to variance in natural language.

figure 5.1: overall abstract level architecture of our complete multimodal re-
trieval model. p osn et and n egn et share weights. they generate a score s
based on similarity (and semantic relevance) between a given pair of text (txt)
and image (img) input

next we will go one lower level of abstraction and discuss the inside of the
model which makes p osn et and n egn et. this is illustrated (again in an ab-
stract manner) in figure 5.2. in our system the textual model and visual model
are replaces by ann (arti   cial neural networks). given a sentence and an image,
these anns are trained to transform them into low dimensional high-quality vec-
tor embeddings. vtxt signi   es the vector embedding generated from the textual
input, and vimg is the vector embedding from visual (image) input.

by high-quality we mean that the vectors are embedded in a common em-
bedding space which encodes high level semantic information about the contents
of the image and text while ignoring feature level details (for example that one
is text and one is input). the advantage is that once multimodal input is em-

38

pos netsposmargin ranking criterionneg netsnegtxt1txt2img1img2bedded in a common space we can apply standard vector operations to measure
their similarity and use it for retrieval task.

these high quality embeddings are learned through a global ranking objective
function. below we will brie   y discuss some of the mathematical details of our
model, including the objective function.

errori = max(0, si,neg     si,pos +    )

si =

(cid:13)(cid:13)(cid:13)   vi,img

  vi,img      vi,txt

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)   vi,txt

(cid:13)(cid:13)(cid:13)

  vi,img = fimg(imgi)
  vi,txt = ftxt(txti)

where, fimg(.) signi   es the visual model and ftxt(.) signi   es the textual model(refer

to figure 5.2). additionally imgi and txti are the ith image and sentence (text)
inputs respectively.

the errori denotes the global objective function. for a given margin thresh-
old (   ), if the positive input score si,pos is not at least     bigger than the negative
input score si,pos, then the error gradient propagates throughout the architec-
ture. this is shown in the equation below:

(0, if si,pos     si,neg +    
(0, if si,pos     si,neg +    

1, otherwise

)
)

   1, otherwise

   errori
   si,neg

   errori
   si,pos

=

=

5.2 details visual and textual models
following the deep learning approach we use neural networks for both visual and
textual models. this is because anns can be trained in an end-to-end manner,
optimizing the global objective function, and require minimal preprocessing.
also, as discussed in chapter 2, various ann architectures achieve state of
the art results in various machine learning tasks. we hope to reciprocate these
results for our task.

for the visual model we use a convolutional neural network (id98), since
they have become ubiquitous with image recognition tasks. as discussed in
chapter 4, we experimented with training a id98 from scratch (with random

39

figure 5.2: architecture inside the visual-textual model in the form of a siamese
network. both p osn et and n egn et share this architecture. vtxt and vimg
are the    xed dimensional vector embeddings of the textual and visual input
respectively. the score s is simply the cosine distance between these two vectors.

initialization) on our data set, but that failed to achieve good results. after
some experiments we were convinced of the need of pretraining the id98 with
large image database like the id163. however, doing that from scratch and
achieving state of the art results would take us a very long time, so we decided
to explore publicly available id98 models.

in all our experiments we use the overfeat convolutional neural network
(refer to 3), which is a competition winner from ilsvrc-2013. we chop o    the
last softmax layer of overfeat which leaves us with 4096-dimensional feature
vector. since this is very deep inside the network, we argue that the features
would be high-level and abstract (not spatially dependent and with semantic
information) and we can use them directly in out model. therefore we don   t
update the weights and parameters of the overfeat network, and treat is as a
feature extractor. instead we build our own smaller fully connected ann which
takes overfeat feature vector as input and generates the multimodal image

40

textual modelsi,jcosine distancetxtiimgjvisual modelvtxtvimgembeddings. results show good performance with this approach and we stick
with it throughout our experimentation.

for the textual model we experimented with di   erent architectures and input
features. a brief overview of these is given below. details of these will come in
the next chapter when we discuss experiments.

1. the    rst and simplest model we try is a fully-connected mlp with uni-
gram bag of words (bow) features of dictionary size 5,000. the size of
the input layer is equal to the vocabulary size, with each index mapping to
an individual word. next there are one or two optional hidden layers with
non-linearity, for greater modeling strength. finally the last layer gives
the feature vector vtxt for the entire textual input, and it is of size nemb.
it is also noteworthy that the    rst layer would encode information about
individual words, and thus we experiment initializing it with good publicly
available id27s called id97 (refer to chapter 3)

2. a major problem with the bow model is that it looses all sense of structure
in the sentence (all words are treated equally and there is no information if
a word came before or after another). there are many ways to preserve this
syntactic information in the sentence, and in this model we try the simplest
approach: id165s. we extend our textual model to include bi-gram, tri-
gram, and tri-grams with skip-grams as inputs. the vocabulary size (and
thus the input layer of mlp) is now increased to 50,000 units (based on the
most frequently occurring id165s). although our model still as a whole
treats each id165 as a unit inside a bag (with no connection to other
id165s in the sentence). but, there is still some syntactic information
present inside the individual id165 unit, and so we hope to gain some
performance due to increased syntactic information in our textual input.
we also tried experimenting with using tf-idf features instead of binary
word presence features.

3. we also tried experimenting with the supervised semantic embedding (sse)
architecture [bespalov et al., 2011] as the textual model. sse uses a spe-
cialized ann architecture to deal with variable length sentences and tem-
poral dependencies within the text.
it has been successfully applied to
applications like id31 from customer reviews at nec, which
led to our motivation in using it.
the architecture for sse consists of three parts. an embedding layer
(lookup table) embeds each word into an m-dimensional vector. k ker-
nels are then convolved over the sequence of id27s to give k
sequences of    xed length (phrase level embeddings). finally an averaging

41

layer averages over the length, reducing the sequence to a k-dimensional
vector, which encodes semantic information about the complete sentence.
further layers can be added on top depending on the required functionality.

5.3 unique training methodology
as we explained above, we generate a negative sample by    st taking a positive
sample and then replacing the sentence (or the image) with another random
sentence (or image). so in e   ect we can take two approaches: replacing the
sentence with another sentence (while keeping the image) or replacing the image
with another image (while keeping the sentence). it turns out that this choice
makes a di   erence in the evaluation results, so from now on we will refer to this
choice as the training methodology. this choice is explained in the math-
ematical forumalism below (refer to figure 5.1 in connection to the equations
below)

i2t approach: img1 = img2 , txt1 6= txt2

t2i approach: txt1 = txt2 , img1 6= img2

we will discuss the e   ects of these with empirical results in the next chapter.
however, we would brie   y mention here that the i2t approach gives higher
performance for image annotation systems, while t2i approach gives higher
performance for image search systems. therefore, this methodology of generat-
ing negative samples can be used to specialize the system towards a particular
task (although the system is inherently bi-directional and can still perform the
other task with reasonable accuracy).

5.4 comparison to other models
deep id98s have now becoming ubiquitous with image recognition and feature
extraction, achieving state of the art in many major id161 tasks.
therefore, like most other recent work, we also stick with id98s for our visual
model. however instead of training the the network from scratch, we use a
publicly available model overfeat which achieved very good performance in
ilsvrc-2013. our initial reason for doing this was to not waste extra time
training the model from scratch on large data sets like id163. they train the
model with various special tricks like multiple-image fragments, multiple-scales,
maximizing image localization, introducing color and translation invariance with

42

data augmentation. because of this the model provides excellent feature vector
representation of images, and we feel that our good results on our task are partly
because of the high modeling strength and accuracy of our visual model.

as explained above, we try several approaches with the textual model: sim-
ple bow model, id165 model, and sse model. however our approach is much
simpler than many researchers who have recently started using recurrent or re-
cursive neural networks to model sentences [socher et al., 2014, karpathy and
fei-fei, 2014]. also, unlike [karpathy et al., 2014] we don   t divide our images
and sentences into fragments, but treat them as whole. with our results we
show that these complicated models, which are also harder to train and slower,
are not necessary to achieve good performance on the task of image-sentence
bidirectional retrieval.

furthermore, we show an important feature of these bi-directional retrieval
models: that they can be specialized to one task (either image search or image
retrieval). we use empirical results to show that the methodology of generating
negative sample (hereby referred to as training methodology) has a signi   cant on
the    nal performance of the model (see section 5.3). if we use the i2t approach
the model specializes in image annotation, and if we use t2i it specializes in im-
age search. note that it still remains bi-directional and gives good performance
for the other task as well.

43

44

chapter 6
experiments and results

6.1 preprocessing

the images were all scaled to a constant dimension of 221   221, since the ann
needs to have a constant sized input. this is also the input dimension of the
overfeat id98 which we utilize in our model. we do not crop image to make
sure that a visually signi   cant object is not cropped unintentionally, which will
make it harder to identify it.

we convert all sentences into lower-case, then remove punctuation and any
other non-alphanumeric character from them, and tokenize them into a sequence
of individual words. we    lter out the articles "a", "an", and "the" since we
believe they will not contribute to the semantic value of the sentence. finally
we limit the word vocabulary by number of occurrences. for unigram bow, our
vocabulary contains 5,000 words, while for bi-grams and tri-grams we use 50,000
vocabulary size. we experiment with treating these id165 as both binary
features and tf-idf (term frequency - inverse document frequency) features. the
equation for calculating tf-idf is below:

tf-idf(w, d) = count(w, d)

|w|

   log(

|d|

count(w, d))

here, count(w, d) is the occurrence frequency of word w in a document d,
while count(w, d) is the number of documents containing the word w. |d| is the
size of the vocabulary, while |d| is the total number of documents. tf-idf values
have the advantage of storing relevance information, for example if a word is
frequent in all documents it gets a low tf-idf score. this can be used to    lter out
stop words.

45

6.2 id74
in this sections below we will report the results of our models on sets of mainly
flickr30k data set and occasionally flickr8k .recall@k (r@k) is the most
widely used metric in this domain.it is de   ned as the percentage of test queries
for which a model returns the positive item among the top k results. it is useful
in the context of search where a user may be satis   ed with the    rst k results
containing a single relevant item.

however, this de   nition is ambiguous if there are more than one positive
results for each test query. this is true for imageannotation in our case, since
each image corresponds with 5 sentences. some researchers only consider the
   rst out of 5 sentences in test cases (e.g. [hodosh et al., 2013,gong et al., 2014])
while others include all 5 (e.g. [karpathy et al., 2014]). we will take both into
account, and report the following id74:

    r@k 1st_txt: this is the recall@k taking into consideration only the

   rst out of    ve sentences for each image

    r@k rnd_txt: this is the recall@k taking into consideration one ran-

dom sentence (determined at test time) for each image.

    r@k avg_txt: this is the recall@k taking into consideration all    ve
of the sentences for each image one by one (iteratively), and then taking
average over them. this is the most robust metric, since it does not depend
on the relative position of sentence, and takes all sentences into account.
hence we use it for comparing our models.

    r@k any_txt: this is the recall@k taking into consideration any one
of the    ve sentences. it returns a match if the image matches any one of
the    ve sentences. this is only valid for imagecaption because it needs
multiple positive results for each query.

    med r:

it is the value of k for which the r@k is equal to 50%.

in
search intuition, it is the number of results to be displayed for making the
id203 of correct result appearing in the search results exactly 50%.
when calculating med r we use recall@k: avg_txt.

    rprecision(5): it is the percentage of relevant items among the top 5 re-
sponses returned by the system. we select 5 because that is the maximum
number of positive responses for imageannotation. since image search has
only one relevant result, this metric is not relevant for it.

46

6.3 training details

we train our model in an end-to-end fashion using stochastic id119
(sgd) without momentum and weight decay terms. in all our experiments we
   x the parameters of overfeat network so its weights don   t update with back
propagation. in general it takes 1-2 days (depending on the size of the model)
to fully train our model on the flickr30k data set, using a cpu with 8 cores
(16 threads). typically the network runs for 50 epochs before termination.

out of the flickr8k and flickr30k data set we separate random 1000 images
(and corresponding sentences) as the test set, and 5% of the remaining examples
as a held-out validation set for model selection. we tried several models and
selected the meta-parameters based on performance on the held-out validation
set. we select the parameters mentioned in 6.1 for all our experiments for fair
comparison.

parameter
margin (   )

learning rate (lr)

lr decay

embedding dim. nemb

non-linearity

value
0.15
0.001
linear 1
300 - 1000

relu 2

table 6.1: hyper parameters showing best performance on held-out validation
set (1,539 image and 7,695 sentences).

6.4 experimental results

first of all, we would give a    gure to show the error performance during training.
figure 6.1 shows that our model performs well on the validation set (with error
going below 6%). this is in comparison to our very    rst model which failed to
generalize on the validation set, as seen in figure 4.1. we can also see that the
performance on validation set saturates around the 20th epoch, and we only save
the networks to the point that the performance does not saturate yet on the
validation set. this is done to minimize the chance of over   tting on the training
set.

1lr decreases linearly to a factor of 0.01 over 100 epochs
2recti   ed linear units

47

figure 6.1: training time error on the flickr30k data set. textual model is
a unigram bow based ann with 1 hidden layer and nemb of 1000. error is
the percentage of samples for which the model gives higher score to the positive
sample over the negative sample. 50% error speci   es random performance.

i2t and t2i training approach

6.4.1
as explained in chapter 5.3 we tried to methodologies for generating negative
samples at training time, namely: i2t and t2i. we notice that this choice actu-
ally does impact the performance of our system in a peculiar way; it specializes it
to a particular task. as is apparent from table 6.2, using i2t methodology gives
higher performance for image annotation tasks and t2i methodology gives higher
performance for image annotation task. we compare the performance across sev-
eral models architectures and consistently notice the same phenomenon. hence
for all future results, whenever we refer to image annotation we report i2t
results, and for image search we report t2i results.

intuitively, the reason for this specialization is that i2t approach more closely
matches with image annotation and t2i with image search. in i2t the images
in both positive and negative samples are the same, while the sentences are
di   erent, so the system learns to be better at recognizing changes in sentences
and hence ultimately performs better at image annotation. vice versa holds for

48

11020304002468101214# epochserror (percentage)  validation settraining sett2i and image search.
it is, however, important to note that even after this
specialization the each individually trained system is bi-directional and can do
both tasks with reasonable accuracy (as seen from table 5.3.

to all look at more results (and other id74 for comparison) for
comparing i2t and t2i methodology, please look at table d.1 and table d.2 in
appendix.

training methodology: i2t vs t2i

models

nhu = 300, nemb = 1000, bow

image annotation
i2t
41.3
nhu = 300, nemb = 1000, bow w/o id97 40.48
41.36
43.34
45.4

nhu = 0, nemb = 300, 2g
nhu = 0, nemb = 1000, 2g
nhu = 1000, nemb = 1000, 2g

t2i
39.34
38.94
41.28
42.84
44

image search
i2t
t2i
37.92 41.08
36.92 40.34
42.06
40.2
43.74
40.2
40.54 42.66

table 6.2: comparison between two di   erent training methodologies: i2t and
t2i. bow means unsing unigram word features and 2g means using bigram. all
results are given in recall@10: avg_txt. nhu is the hidden layer in the textual
model, while nemb is the size of embedding dimension.

6.4.2 results of bow model
first we will report the results of our bow model, in which our textual model
is simply an mlp with unigram word features. all information about temporal
dependency in the sentence is lost and we use a dictionary size of 5,000 most
frequently words (after removing "a", "an" and "the"). we also use binary features
so there is no information about multiple occurrences of words in a sentence.

the hidden layer of textual model is    xed to 300 units, since the id97
embeddings have dimensionality of 300. we    xed the hidden layer of the train-
able visual model to 1,000 units, and the size of multimodal embedding vector is
also    xed to 1,000. table 6.3 shows the results on flickr8k test set, and table
6.4 shows results on flickr30k test set (both of size 1,000 images). we use this
model named as our model: bow when comparing results in chapter 7.

inspecting the tables, the results look quite promising, and we are ready to

explore more architectures and parameters.

6.4.3 id97 results
next we investigate the e   ect of id97 embeddings. as stated earlier, we
used these as a way to have good initialization of our textual model. since we

49

metric

rprecision(5)

med r

k

r@k: rnd_txt
r@k: avg_txt

bow model on flickr8k
image annotation

image search

7.32
21

-
22

1
8.7
7.38

2
12
12.5

5
22.5
23.72

10
33.7
35.22

1
6.1
7.3

2
14.7
13.44

5
25.9
25.5

10
37.4
37.74

table 6.3: flickr8k experiments with unigram bow model and id97 ini-
tialization. r@k is recall@k (high is good). med r is the median rank (low
is good). rprecision(5) is the precision at rank 5 (high is good).

metric

rprecision(5)

med r

k

r@k: rnd_txt
r@k: avg_txt
r@k: 1st_txt
r@k: any_txt

our model: bow

image annotation

image search

10.2
17

-
16

1
10.6
10.4
10.7
14.0

2
14.9
16.94
17.5
19.0

5
29.2
29.3
30.4
33.0

10
42.7
41.3
43.8
45.5

1
9.8
10.56
9.6
-

2
19.3
18.04
18.2

-

5
28.9
29.56
30.6

-

10
40.5
41.08
44.2

-

table 6.4: flickr30k experiments with unigram bow model and id97
initialization. r@k is recall@k (high is good). med r is the median rank
(low is good). rprecision(5) is the precision at rank 5 (high is good).

don   t train our model on a large corpus of text, we speculate id97 would
be helpful. we repeat the experiment on flickr30k using bow model, with the
only di   erence that we use randomly initialized network. doing this experiment
is useful because we suggested using id165 models as a second step onto bow
model, however we are aware of no such publicly available id165 embeddings.
the results of the experiment are shown in table 6.5 which we can compare
with table 6.4. a quick comparison between the performance metrics in these
two tables shows that although id97 initialization does help in improving the
accuracy of the system, but the e   ects are not extremely drastic. for example
r@10:avg_txt for image annotation increases by 3.3% while for image search
increases by 3.86%.

the results are signi   cant in suggesting the importance of using id97
for initializing our models, but we feel that the gap may be bridged with using
more complicated models without this initialization.

50

metric

rprecision(5)

med r

k

r@k: rnd_txt
r@k: avg_txt

bow model w/o id97
image annotation

image search

10.08
17

-
18

1
11.2
10.36

2
16.6
17.08

5
31.4
29.5

10
41.2
40.48

1
10.5
10.12

2
17.6
17.26

5
28.4
29.04

10
39.8
40.34

table 6.5: flickr30k experiments without id97 initialization. r@k is re-
call@k (high is good). med r is the median rank (low is good). rprecision(5)
is the precision at rank 5 (high is good).

6.4.4 results of id165 models
we replaced the bow textual model with id165 model. id165 models con-
serve some sentence-order information and so theoretically they act as better
features. in our experiments we tried bigrams (2g), trigrams (3g) and trigrams
+ skip grams (tk3). in all these cases the vocabulary size is    xed to 50,000 which
is the size of input layer of textual model mlp. table 6.6 shows a comparison
between these three di   erent kind of models for our task.

the results show that both bigrams and trigrams perform better in di   erent
settings, while inclusion of skip grams in trigram model consistently resulted in
worst performance.

id165 model comparison

models

evaluation metric

rprecision(5)

2g
10.12
10.52
11.68
11.96
12.2

3g

11.08
11.34
11.0
11.04
11.9

tk3
9.5
9.78
10.7
10.6
10.56

r@10: avg_txt
tk3
2g
3g
42.7
38.46
40.72
41.46
38.82
41.54
43.36
40.04
41.74
43.34
40.96
41.62
42.24 41.16
41.36

nhu = 300, nemb = 1000
nhu = 1000, nemb = 300
nhu = 1000, nemb = 1000
nhu = 0, nemb = 1000
nhu = 0, nemb = 300

table 6.6: comparison between di   erent id165 models, on flickr30k data. 2g
is bi-gram, 3g is trigram and tk3 is combination of tri-gram and skip-gram fea-
tures. in all case size of input vocabulary is 50,000 and all rest model parameters
are similar. the bold    gures show best results in the speci   c row. nhu is the
hidden layer in the textual model, while nemb is the size of embedding dimension.

following this we also experimented on using either binary or tf-idf valued

51

features. intuition suggests that tf-idf encode more information and so should
perform better than binary features. however, empirical evidence suggests oth-
erwise as binary features performed better in most of our models. hence for
future models we stick with binary features. the results are shown in table 6.7

id165 features comparison

models

nhu = 0, nemb = 300, 2g
nhu = 0, nemb = 300, 3g
nhu = 0, nemb = 300, tk3
nhu = 1000, nemb = 300, 3g
nhu = 1000, nemb = 1000, 2g
nhu = 1000, nemb = 1000, 3g
nhu = 1000, nemb = 1000, tk3

evaluation metric

rprecision(5) r@k: avg_txt
tf-idf
binary tf-idf binary
12.2
41.36
42.2
42.24
41.84
11.9
41.16
39.42
10.56
11.34
41.76
41.54
11.68
41.86
43.36
41.74
40.96
11.0
10.7
40.04
40.1

11.58
11.7
10.3
10.28
10.52
11.04
10.58

table 6.7: comparison between di   erent input word features: binary and tf    
idf, on flickr30k data. in all case size of input vocabulary is 50,000 and the
inputs are either 2g, 3g, or tk3, and all rest model parameters are similar. the
bold    gures show best results in the speci   c row. nhu is the hidden layer in the
textual model, while nemb is the size of mbedding dimension.

finally we select the best performing model as having 1,000 unit hidden layers
in both textual and trainable visual models, and also 1,000 unit multimodal
embedding vector. the results of this model on flickr30k are shown in figure
6.8, and we notice signi   cant improvements over the bow model. we use this
model named as our model: id165 when comparing results in chapter 7.

6.5 are deep textual models necessary
now we will, investigate if a deep network is really necessary in the textual
model. we explored several shallow single layered architectures, where the input
id165 input is linearly transformed into the embedding vector. surprisingly,
almost all of the networks we tried reached performance close to the ones we
achieved with a deeper network with non-linearities. in addition to performing
well, these shallow models were approximately 3   faster to train (ignoring the
time required for overfeat feature vector generation - since it is not    ne tuned,
it is equivalent to only single time pass)

we reached the best performance using a 300 dimensional embedding (lin-
early connected with 5,000 input units). the small trainable visual network also

52

metric

rprecision(5)

med r

k

r@k: rnd_txt
r@k: avg_txt
r@k: 1st_txt
r@k: any_txt

our model: id165

image annotation

image search

11.68
15

-
15

1
12.3
11.62
13.0
14.9

2
19.2
18.88
19.7
24.7

5
32
31.44
32.6
39.3

10
44.4
43.36
45.4
50.9

1
11.7
11.44
11.9

-

2
20.4
18.56
20.9

-

5
29.3
31.24
34.9

-

10
44.4
42.66
44.3

-

table 6.8: flickr30k experiments with bigram model and binary inputs. r@k
is recall@k (high is good). med r is the median rank (low is good). rpreci-
sion(5) is the precision at rank 5 (high is good).

does not contain any hidden layer as well, and the 4,096 dimensional overfeat
feature vectors are linearly transformed into the 300 embedding dimension. the
result of this model is in table 6.9. comparison can be made with table 6.8 to
see the performance di   erence as compared to the earlier discussed our model:
id165. we use this model named as our model: shallow when comparing
results in chapter 7.

metric

rprecision(5)

med r

k

r@k: rnd_txt
r@k: avg_txt
r@k: 1st_txt
r@k: any_txt

our model: shallow

image annotation

image search

12.2
17

-
16

1
11.2
12.16
13.7
16.0

2
19.5
19.36
21.7
24.1

5
30.4
30.94
32.9
39.5

10
42.3
42.06
42.9
50.7

1
12.7
12.16
13.2

-

2
19.6
19.36
20.5

-

5
30.2
30.94
33.7

-

10
42.2
42.06
44.0

-

table 6.9: flickr30k experiments with bigram model and binary inputs. r@k
is recall@k (high is good). med r is the median rank (low is good). rpreci-
sion(5) is the precision at rank 5 (high is good).

lastly, we also explored using the sse (semantic sequence embedding) mod-
ule as our textual model. details of architecture are given in chapter 5.
it
has specialized architecture for modeling sequence inside variable length sen-
tences, and similar architectures have achieved good performance in task such

53

as id38 [collobert and weston, 2008] and review based sentiment
analysis [bespalov et al., 2011]. we were hopeful that it would result in perfor-
mance boost on our task as well.

however, empirical results show another story and the model performs not
as well as other previously discussed models. the results of the best model
architecture are shown in table 6.10. the architecture has context window of
size 5 and both word level and phrase level embeddings of 300 dimensions. this
model is initialized with id97, and it is important to note that if we did not
use id97 initialization the performance fell staggeringly to about half the
current performance.

we feel a possible reason of the failure of sse as our textual module is
because there isn   t enough textual training data. our data set is limited to
150,000 sentences (5 sentences for each of approximately 30,000 images). this
makes a total of approximately 1.5-2 million words. in contrast [collobert and
weston, 2008] use 631 million words to train their language model with similar
architecture. the sse architecture is much deeper and has more parameters
than other textual models we tried, it has 4 distinct layers: lookup table with
individual id27s (which we initialized with id97), id98 with
   xed window for phrase level embeddings, averaging layer for complete sentence
layer embedding and    nally a non-linearity followed by a linear layer for greater
modeling capacity. because of this deep architecture, it is not unlikely that it
needs more multimodal data to train well.

secondly, the fact that it id97 boosts its performance (validation error
at train time saturates at about 15% if not using id97 and at about 8% if
using id97 in most models) also means that it is unable to learn good word
embeddings if initialized from random. in contrast when we initialized our bow
model without id97, the di   erence in performance was not much slighter.
this again reinforces our opinion that more data is needed.

6.6 comparison with existing systems
we would like to compare the performance of our model with existing state
of the art systems. however, we noted in chapter 6.2 that there is inherent
ambiguity in the way the most common evaluation metric (recall@k) is de   ned
for this task. in addition to this, researchers often di   er how they present the
results. we use all 5 sentences while evaluating our system. for comparing
image annotation task we will report r@k avg_txt and for comparing
image search task we will report r@k any_txt. here we will follow the
standard by [karpathy et al., 2014], and this is how they report it to the best of
our knowledge.

54

metric

rprecision(5)

med r

k

r@k: rnd_txt
r@k: avg_txt
r@k: 1st_txt
r@k: any_txt

sse for textual model

image annotation

image search

7.84
21

-
24

1
6.6
7.58
8.0
9.6

2
15.2
13.18
13.5
16.1

5
23.9
24.54
24.6
27.2

10
37.1
35.82
35.3
39.8

1
7.4
7.98
7.7
-

2
14.3
13.62
13.4

-

5
22.8
24.16
23.8

-

10
32.5
34.5
35.8

-

table 6.10: flickr30k experiments with sse textual model (window size =5).
r@k is recall@k (high is good). med r is the median rank (low is good).
rprecision(5) is the precision at rank 5 (high is good).

models

flickr30k: comparison
image annotation

image search

devise 3

random ranking

defrag fao + go

sdt-id56 4
defrag 5 fao6
defrag go7

r@1 r@5 r@10 r@1 r@5 r@10
0.1
4.5
9.6
11.0
11.5
12.0
14.2
16.4
14.0
14.9
16.0

1.0
32.7
41.1
34.5
38.4
43.2
44.2
44.5
41.08
42.66
42.06
table 6.11: comparison between existing image text matching systems. r@k
is recall@k (high is good). in our models, for image annotation we report r@k:
any_txt and for image search we report r@k: avg_txt.

(*)defrag fao + go + mil8
defrag (*) + finetune id98

our model: bow
our model: id165
our model: shallow

0.1
6.7
8.9
7.6
8.8
9.9
10.2
10.3
10.56
11.44
12.16

0.5
21.9
29.8
23.8
27.6
30.5
30.8
31.4
29.56
31.24
30.94

0.5
18.1
29.8
28.7
33.2
37.1
37.7
40.2
33.0
39.3
39.5

1.0
29.2
41.1
39.3
44.9
50.0
51.3
54.7
45.5
50.9
50.7

table 6.11 shows that our models are comparable to recent work in the    eld

3 [frome et al., 2013]
4 [socher et al., 2014]
5 [karpathy et al., 2014]
6fragment alignment objective
7global ranking objective
8multi instance learning

55

9, and perform not far from the top results. when comparing the results it is
interesting to note that most recent research has gone into complicated mod-
eling on the textual domain. for example [socher et al., 2014] use sdt-id56
which is dependency-tree based id56. they also try other
types of recurrent and id56s. besides this [karpathy et al.,
2014] use dependency tree edges to fragment their sentences, and so on. in con-
trast we use simple textual model and achieve comparable results. the bulk
of our model   s strength comes from the visual model and the training method-
ology which specializes the performance towards a particular task boosting its
performance. in our experiments we notice that temporal dependency (or infor-
mation about word order) modeled with simple bi-grams or tri-grams is enough
to achieve good results.

9some of these models have been re-implemented by [karpathy et al., 2014]

56

chapter 7
conclusion

the goal of the project was to explore deep learning architectures in the context
of multimodal image and text modeling. our task was based on bi-directional
retrieval. this means that our model would support retrieving images based on a
text query, and also retrieving text sentences based on an image query. applica-
tions of such a system would include automatic image annotation, and sentence
based id162. to design our system we use the ranking approach which
is common to many traditional information retrieval systems. at each iteration
we feed our image with a positive pair and a negative pair of training input. a
positive input is a matched pair of image and sentence (as labeled by a human),
while a negative input is simply a mismatched pair. the model assigns a score
to the positive input and to the negative input, and the objective function is to
make the positive sample score higher than the negative sample. in this way the
model will learn to rank related pair of image and text higher than an arbitrary
unrelated pair.

since images and text have very di   erent statistical properties, it is necessary
to model two di   erent sub-models for them. the idea is that if both sub-models
extract underlying semantic information (such as information about the content)
and encode it into a distributed vector embedding, then a ranking objective func-
tion could align these two embeddings into a common multimodal space. once
this is the case, we can simply compute the distance betwen the two vectors as a
measure of similarity. for the visual model we explored using deep convolutional
neural network (id98). our initial model was initialized from random weights
and it failed to perform well on our tasks. we did some experiments on di   erent
but related task of image recognition on mnist and caltech256, and were able
to conclude lack of pretraining as a reason why the network failed. following
this we adopted overfeat into our visual model, which is a publicly available
id98 which achieved high performance on ilsvrc 2013. with just this change
the network stats to work well on the task, which con   rms our initial suspicion.

57

for our textual module we tried various models and features - some simple and
some more complicated. we explored using bag of words approach as well as
id165s (bigrams, trigrams and skip grams), and tried binary and tf-idf features.
we experimented with using shallow linear architectures, intermediate architec-
ture like mlp and deeper architectures with id27s and convolutional
neural network (for modeling variable length sentence by averaging over a    xed
window).

we evaluate the performance of our models by testing them on flickr8k and
flickr30k training sets. a comparison with recent work in the    eld shows that
even though our models are simpler, we achieve performance comparable to some
of the top results. we believe that a major reason for our good performance is the
performance of the id98, which extracts very powerful and semantically relevant
features. although like other approaches, overfeat is trained on the id163
data base, but it is trained with special tricks like multiple-image fragments,
multiple-scales, maximizing image localization, introducing color and transla-
tion invariance with data augmentation. because of this the network achieves
excellent performance on ilsvrc 2013 [sermanet et al., 2014] challenge, and
this also increases the quality of feature vectors extracted by the model.

for the textual module we observed that even a very shallow module with a
single linear layer achieves very good performance. although using hidden lay-
ers and increasing the dimensionality of the embedding layer does enhance the
performance, but performance is still good with a simple linear layer. this pro-
vides an interesting contrast between the visual module and the textual module.
since only a very well pretrained id98 was able to train on the task. however
a more specialized and deeper architecture (sse) does not perform so well on
the textual side. this provides a fundamental question of how essential is deep
learning in this task? we think that the only reason id98 performs well is that
it was pretrained with a lot of data to extract semantic features from images.
so in order to make a textual model work well we either need a very large scale
image-text data base, or we need to pretrain the deeper textual model on tasks
which allow it to extract semantic information from sentences. an example of
such a task could be document classi   cation.

finally, we show an important feature of our bi-directional retrieval models:
that they can be specialized to one task (either image search or id162).
we use empirical results to show that the methodology of generating negative
sample has a signi   cant on the    nal performance of the model. if we use the
i2t approach the model specializes in image annotation, and if we use t2i it
specializes in image search. note that it still remains bi-directional and gives
good performance for the other task as well.

58

7.1 future work
the simplest addition to our model would be to turn on updating for our id98
while we are training on the image-text data set. this is called    netuning, since
the id98 is pretrained on image data alone. we expect to achieve performance
improvement by this    netuning, as reported by [karpathy et al., 2014] and seen
in the table 6.11. however this will come at a cost of signi   cantly additional
training time, hence it is a trade o   .

as noted in chapter 6.5, we experimented with a more promising model sse
but experienced a performance drop to our surprise. we hypothesize that this
is because the complicated and deeper model needs a larger training data to
perform well, something which is far from true with our flickr30k and flickr8k
data sets. interestingly, this problem is similar to what we faced initially with
our visual model and explained in chapter 4. our uninitialized id98 failed at
the tasks miserably, which convinced us to use a id98 pretrained on id163.
we similarly feel that an sse module trained on large relevant data would sig-
ni   cantly improve our results. we experienced a portion of this when - as we
reported - initializing the    rst layer of the sse with id97 caused the accu-
racy of the model to almost double (albeit still lesser than our simpler models).
this means that although id97 are good id27s, but they are
not enough for the task of multimodal learning since they were initially learned
on just a unimodal language model. the best approach would be to use a large
image-text data base to train our models, however a high quality large scale data
base does not exist in this task yet. regeradless, there are some data sets that
we aim to use in the future.

ms-coco1 is a data set which provides high quality captions labeled by 5
di   erent humans, for more than 100,000 images. although still not an extremely
large data set, it is the    rst we hope to try our models on. some other larger data
sets exist but their usability in image text mapping systems has been questioned.
flickr1m 2 is an example of such data set which contains 1 million images from
flickr website associated with captions added by people uploading the images.
[hodosh et al., 2013] argue that such captions do not act in good association with
images, since humans tend to describe the aspects of the images which are not so
apparent from the image itself. in contrast, our system only wants to know what
is apparent inside the image. [gong et al., 2014] explored using a good quality
dataset (flickr30k) to improve performance on a low quality annotated data
sets (sbu1m and flickr1m). we propose that it would be interesting to explore

1http://mscoco.org/
2http://press.liacs.nl/mir   ickr/

59

the opposite: to pretrain our model with large amount of weakly annotated text
and then    netuning it on data sets like flickr30k and ms-coco.

another exciting area to explore is using data augmentation similar to how
it is being used in visual models for image recognition. in other words we could
arti   cially augment the textual data (number of sentences) matching each im-
age in our training set. there can be three ways of doing this. firstly we could
use the original human labeled sentences as seeds and use text based informa-
tion retrieval systems to retrieve similar sentences from a large textual corpus.
secondly we could design a language model to generate more sentences similar
to the one it is given as input. thirdly, we could use an image-based sentence
generation system to generate additional relevant sentences and use these when
training our retrieval system. once we have a larger amount of text correspond-
ing to images we can better train deeper and high-capacity models, and also
better model inherent variance in describing an image.

the next step is to expand the system with some ideas presented above.
these include,    netuning our id98, pretraining entire system on a weekly anno-
tated large scale data set, and arti   cially augmenting textual data. this is our
   rst experimentation with image text multimodal modeling system, and we aim
to keep on improving it.

60

appendix a
acronyms

ann arti   cial neural network
bow bag of words
id98 convolutional neural network
i2t image to text
kcca kernel conanical correlation analysis
r@k recall at k
mlp multi layer id88
nlp natural language procecesing
relu recti   ed linear unit
id56 recurrent neural network
id56 id56
sgd stochastic id119
sse supervised sequence embedding
ssi supervised semantic indexing
t2i text to image
tf-tdf term frequency - inverse document frequency

61

appendix b
extra resources

b.1 mnist
mnist 1 is a database of hand-written digits consisting of 60,000 training
examples and 10,000 test examples. the digits are size-normalized and centered
in a    xed-size image of 28    28 pixels. the data set has a long history of acting
as a benchmark for measuring performance of visual recognition systems.

b.2 caltech256
caltech256 2 is a database for object recognition from images. it consists of
about 30,000 images belonging to 256 categories (plus clutter). categories range
from car, helicopter, airplane, dog, cat, elephant, spaghetti, ri   e, and other
everyday objects. the images are not left-right aligned which makes the task
relatively harder.

id163

b.3
id163 3 is a large scale image database organized in a hierarchical manner.
it consists of a total of about 14 million images belonging to 22,000 categories.
in addition to this they organize a yearly competition for id161 called
ilsvrc (image net large scale visual recognition challenge). the compe-
tition consists of three individual tasks: object recognition, localization, and
detection.

1http://yann.lecun.com/exdb/mnist/
2http://www.vision.caltech.edu/image_datasets/caltech256/
3http://www.image-net.org/

62

b.4 pascal voc 2008
this was the    rst publicly available data set purposefully collected for the im-
age text ranking task by [farhadi et al., 2010] this data set consists of 1,000
images from pascal voc-2008 object recognition challenge. 50 images are
randomly selected belonging to each of the 20 categories. each image is anno-
tated with 5 descriptive captions using amazon   s mechanical turk, resulting in
5,000 sentences.

although, being the    rst speci   c data set, it su   ers from a number of short-
comings which limit its usefulness. the domain of its images is very limited and
the captions are relatively simple and sometimes containing grammatical and
spelling errors [hodosh et al., 2013].

63

appendix c
software

we trained the neural network models using proprietary machine leaning envi-
ronment developed by nec labs america. milde (machine learning devel-
opment environment) is a software environment for developing and prototyping
applications based on machine learning and statistics.

64

appendix d
further results of i2t and t2i
training methodologies

metric

rprecision(5)

med r

k

r@k: rnd_txt
r@k: avg_txt
r@k: 1st_txt
r@k: any_txt

image annotation

i2t training apprch.

t2i training apprch.

10.2
17

9.66
19

1
10.6
10.4
10.7
14.0

2
14.9
16.94
17.5
19.0

3
29.2
29.3
30.4
33.0

4
42.7
41.3
43.8
45.5

1
8.2
9.48
10.8
12.6

2
14.9
15.76
16.4
18.9

3
27.1
26.8
27.2
31.9

4
35.9
37.92
39.7
42.3

table d.1: experiment shows training approach signi   cantly e   ects results,
int this case i2t approach boosts performance for image annotation. ex-
periment performed with flickr30k data set with unigram bow model and
id97 initialization.
rprecision(5) high is good, med r low is good, and
r@k high is good.

65

metric
med r

k

r@k: rnd_txt
r@k: avg_txt
r@k: 1st_txt

image search
t2i training apprch.

16

i2t training apprch.

18

1
9.8
10.56
9.6

2
19.3
18.04
18.2

3
28.9
29.56
30.6

4
40.5
41.3
44.2

1
10.1
9.48
8.7

2
15.8
15.6
16.1

3
27.3
27.9
30.3

4
39.3
39.34
42.6

table d.2: experiment shows training approach signi   cantly e   ects results, int
this case t2i approach boosts performance for image search. experiment
performed with flickr30k data set with unigram bow model and id97
initialization. med r low is good, and r@k high is good.

66

bibliography

[bach and jordan, 2003] bach, f. r. and jordan, m. i. (2003). kernel indepen-
dent component analysis. the journal of machine learning research, 3:1   48.
[bai et al., 2009] bai, b., weston, j., grangier, d., collobert, r., sadamasa,
k., qi, y., chapelle, o., and weinberger, k. (2009). supervised semantic
in proceedings of the 18th acm conference on information and
indexing.
knowledge management, pages 187   196. acm.

[bengio et al., 2003] bengio, y., ducharme, r., vincent, p., and janvin, c.
(2003). a neural probabilistic language model. the journal of machine learn-
ing research, 3:1137   1155.

[bespalov et al., 2011] bespalov, d., bai, b., qi, y., and shokoufandeh, a.
(2011). sentiment classi   cation based on supervised latent id165 analysis.
in proceedings of the 20th acm international conference on information and
knowledge management, pages 375   382. acm.

[collobert and weston, 2008] collobert, r. and weston, j. (2008). a uni   ed
architecture for natural language processing: deep neural networks with mul-
titask learning. in proceedings of the 25th international conference on machine
learning, pages 160   167. acm.

[deng et al., 2009] deng, j., dong, w., socher, r., li, l.-j., li, k., and fei-fei,
l. (2009). id163: a large-scale hierarchical image database. in computer
vision and pattern recognition, 2009. cvpr 2009. ieee conference on,
pages 248   255. ieee.

[farhadi et al., 2010] farhadi, a., hejrati, m., sadeghi, m. a., young, p.,
rashtchian, c., hockenmaier, j., and forsyth, d. (2010). every picture tells
a story: generating sentences from images. in id161   eccv 2010,
pages 15   29. springer.

[frome et al., 2013] frome, a., corrado, g. s., shlens, j., bengio, s., dean, j.,
mikolov, t., et al. (2013). devise: a deep visual-semantic embedding model.
in advances in neural information processing systems, pages 2121   2129.

67

[gong et al., 2014] gong, y., wang, l., hodosh, m., hockenmaier, j., and
lazebnik, s. (2014). improving image-sentence embeddings using large weakly
annotated photo collections. in id161   eccv 2014, pages 529   545.
springer.

[gupta et al., 2012] gupta, a., verma, y., and jawahar, c. (2012). choosing

linguistics over vision to describe images. in aaai.

[hinton et al., 2006] hinton, g., osindero, s., and teh, y.-w. (2006). a fast
learning algorithm for deep belief nets. neural computation, 18(7):1527   1554.

[hinton et al., 2012] hinton, g. e., srivastava, n., krizhevsky, a., sutskever,
i., and salakhutdinov, r. r. (2012). improving neural networks by preventing
co-adaptation of feature detectors. arxiv preprint arxiv:1207.0580.

[hodosh et al., 2013] hodosh, m., young, p., and hockenmaier, j. (2013). fram-
ing image description as a ranking task: data, models and id74.
journal of arti   cial intelligence research, pages 853   899.

[hubel and wiesel, 1968] hubel, d. h. and wiesel, t. n. (1968). receptive
   elds and functional architecture of monkey striate cortex. the journal of
physiology, 195(1):215   243.

[karpathy and fei-fei, 2014] karpathy, a. and fei-fei, l. (2014). deep visual-
arxiv preprint

semantic alignments for generating image descriptions.
arxiv:1412.2306.

[karpathy et al., 2014] karpathy, a., joulin, a., and li, f. f. f. (2014). deep
fragment embeddings for bidirectional image sentence mapping. in advances
in neural information processing systems, pages 1889   1897.

[kiros et al., 2014] kiros, r., salakhutdinov, r., and zemel, r. (2014). mul-
in proceedings of the 31st international

timodal neural language models.
conference on machine learning (icml-14), pages 595   603.

[krizhevsky et al., 2012] krizhevsky, a., sutskever, i., and hinton, g. e. (2012).
id163 classi   cation with deep convolutional neural networks. in advances
in neural information processing systems, pages 1097   1105.

[le cun et al., 1990] le cun, b. b., denker, j. s., henderson, d., howard,
r. e., hubbard, w., and jackel, l. d. (1990). handwritten digit recogni-
in advances in neural information
tion with a back-propagation network.
processing systems. citeseer.

68

[lecun et al., 1998] lecun, y., bottou, l., bengio, y., and ha   ner, p. (1998).
gradient-based learning applied to document recognition. proceedings of the
ieee, 86(11):2278   2324.

[lecun et al., 1995] lecun, y., jackel, l., bottou, l., brunot, a., cortes, c.,
denker, j., drucker, h., guyon, i., muller, u., sackinger, e., et al. (1995).
comparison of learning algorithms for handwritten digit recognition. in in-
ternational conference on arti   cial neural networks, volume 60, pages 53   60.

[lin, 2004] lin, c.-y. (2004). id8: a package for automatic evaluation of
summaries. in text summarization branches out: proceedings of the acl-04
workshop, pages 74   81.

[lin et al., 2014] lin, t.-y., maire, m., belongie, s., hays, j., perona, p., ra-
manan, d., doll  r, p., and zitnick, c. l. (2014). microsoft coco: common
objects in context. in id161   eccv 2014, pages 740   755. springer.

[lowe, 2004] lowe, d. g. (2004). distinctive image features from scale-invariant

keypoints. international journal of id161, 60(2):91   110.

[makadia et al., 2010] makadia, a., pavlovic, v., and kumar, s. (2010). base-
international journal of id161,

lines for image annotation.
90(1):88   105.

[miikkulainen and dyer, 1991] miikkulainen, r. and dyer, m. g. (1991). nat-
ural language processing with modular pdp networks and distributed lexicon.
cognitive science, 15(3):343   399.

[mikolov et al., 2013a] mikolov, t., chen, k., corrado, g., and dean, j.
(2013a). e   cient estimation of word representations in vector space. corr,
abs/1301.3781.

[mikolov et al., 2013b] mikolov, t., sutskever, i., chen, k., corrado, g., and
dean, j. (2013b). distributed representations of words and phrases and their
compositionality. corr, abs/1310.4546.

[ordonez et al., 2011] ordonez, v., kulkarni, g., and berg, t. l. (2011).
im2text: describing images using 1 million captioned photographs. in ad-
vances in neural information processing systems, pages 1143   1151.

[papineni et al., 2002] papineni, k., roukos, s., ward, t., and zhu, w.-j.
(2002). id7: a method for automatic evaluation of machine translation.
in proceedings of the 40th annual meeting on association for computational
linguistics, pages 311   318. association for computational linguistics.

69

[rashtchian et al., 2010] rashtchian, c., young, p., hodosh, m., and hocken-
maier, j. (2010). collecting image annotations using amazon   s mechanical
turk. in proceedings of the naacl hlt 2010 workshop on creating speech
and language data with amazon   s mechanical turk, pages 139   147. associa-
tion for computational linguistics.

[reiter and belz, 2009] reiter, e. and belz, a. (2009). an investigation into
the validity of some metrics for automatically evaluating natural language
generation systems. computational linguistics, 35(4):529   558.

[sch  lkopf et al., ] sch  lkopf, b., platt, j., and hofmann, t. greedy layer-wise

training of deep networks.

[sermanet et al., 2014] sermanet, p., eigen, d., zhang, x., mathieu, m., fer-
gus, r., and lecun, y. (2014). overfeat: integrated recognition, localization
and detection using convolutional networks. in international conference on
learning representations (iclr 2014). cbls.

[socher et al., 2014] socher, r., karpathy, a., le, q. v., manning, c. d., and
ng, a. y. (2014). grounded id152 for    nding and describ-
ing images with sentences. transactions of the association for computational
linguistics, 2:207   218.

[socher et al., 2011] socher, r., lin, c. c., manning, c., and ng, a. y. (2011).
parsing natural scenes and natural language with id56s. in
proceedings of the 28th international conference on machine learning (icml-
11), pages 129   136.

[srivastava and salakhutdinov, 2012] srivastava, n. and salakhutdinov, r. r.
(2012). multimodal learning with deep id82s. in advances in
neural information processing systems, pages 2222   2230.

[vinyals et al., 2014] vinyals, o., toshev, a., bengio, s., and erhan, d.
(2014). show and tell: a neural image caption generator. arxiv preprint
arxiv:1411.4555.

[young et al., 2014] young, p., lai, a., hodosh, m., and hockenmaier, j.
(2014). from image descriptions to visual denotations: new similarity metrics
for semantic id136 over event descriptions. transactions of the association
for computational linguistics, 2:67   78.

70

