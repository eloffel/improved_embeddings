6
1
0
2

 

p
e
s
4
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
6
5
0
1
0

.

8
0
6
1
:
v
i
x
r
a

morphological priors for probabilistic neural id27s

3525 piedmont rd ne, building 6, suite 500

parminder bhatia   

yik yak, inc.

atlanta, ga

robert guthrie    and jacob eisenstein

school of interactive computing
georgia institute of technology

atlanta, ga 30312 usa

{rguthrie3 + jacobe}@gatech.edu

parminder@yikyakapp.com

abstract

id27s allow natural language pro-
cessing systems to share statistical information
across related words. these embeddings are
typically based on distributional statistics, mak-
ing it dif   cult for them to generalize to rare or
unseen words. we propose to improve word
embeddings by incorporating morphological
information, capturing shared sub-word fea-
tures. unlike previous work that constructs
id27s directly from morphemes,
we combine morphological and distributional
information in a uni   ed probabilistic frame-
work, in which the id27 is a latent
variable. the morphological information pro-
vides a prior distribution on the latent word em-
beddings, which in turn condition a likelihood
function over an observed corpus. this ap-
proach yields improvements on intrinsic word
similarity evaluations, and also in the down-
stream task of part-of-speech tagging.

1

introduction

id27s have been shown to improve many
natural language processing applications, from lan-
guage models (mikolov et al., 2010) to information
extraction (collobert and weston, 2008), and from
parsing (chen and manning, 2014) to machine trans-
lation (cho et al., 2014). id27s leverage
a classical idea in natural language processing: use
distributional statistics from large amounts of unla-
beled data to learn representations that allow sharing

   the    rst
available

two authors contributed equally.
code
at https://github.com/rguthrie3/

is
morphologicalpriorsforwordembeddings.

across related words (brown et al., 1992). while this
approach is undeniably effective, the long-tail nature
of linguistic data ensures that there will always be
words that are not observed in even the largest cor-
pus (zipf, 1949). there will be many other words
which are observed only a handful of times, making
the distributional statistics too sparse to accurately
estimate the 100- or 1000-dimensional dense vectors
that are typically used for id27s. these
problems are particularly acute in morphologically
rich languages like german and turkish, where each
word may have dozens of possible in   ections.

recent work has proposed to address this issue by
replacing word-level embeddings with embeddings
based on subword units: morphemes (luong et al.,
2013; botha and blunsom, 2014) or individual char-
acters (santos and zadrozny, 2014; ling et al., 2015;
kim et al., 2016). such models leverage the fact that
word meaning is often compositional, arising from
subword components. by learning representations of
subword units, it is possible to generalize to rare and
unseen words.

but while morphology and orthography are some-
times a signal of semantics, there are also many cases
similar spellings do not imply similar meanings: bet-
ter-batter, melon-felon, dessert-desert, etc. if each
word   s embedding is constrained to be a determinis-
tic function of its characters, as in prior work, then
it will be dif   cult to learn appropriately distinct em-
beddings for such pairs. automated morphological
analysis may be incorrect: for example, really may
be segmented into re+ally, incorrectly suggesting a
similarity to revise and review. even correct morpho-
logical segmentation may be misleading. consider

that incredible and in   ammable share a pre   x in-,
which exerts the opposite effect in these two cases.1
overall, a word   s observed internal structure gives
evidence about its meaning, but it must be possible to
override this evidence when the distributional facts
point in another direction.

we formalize this idea using the machinery of
probabilistic id114. we treat word em-
beddings as latent variables (vilnis and mccallum,
2014), which are conditioned on a prior distribution
that is based on word morphology. we then maximize
a variational approximation to the expected likeli-
hood of an observed corpus of text,    tting variational
parameters over latent binary id27s. for
common words, the expected id27s are
largely determined by the expected corpus likelihood,
and thus, by the distributional statistics. for rare
words, the prior plays a larger role. since the prior
distribution is a function of the morphology, it is pos-
sible to impute embeddings for unseen words after
training the model.

we model id27s as latent binary vec-
tors. this choice is based on linguistic theories of
lexical semantics and morphology. morphemes are
viewed as adding morphosyntactic features to words:
for example, in english, un- adds a negation feature
(unbelievable), -s adds a plural feature, and -ed adds
a past tense feature (halle and marantz, 1993). sim-
ilarly, the lexicon is often viewed as organized in
terms of features: for example, the word bachelor
carries the features human, male, and unmar-
ried (katz and fodor, 1963). each word   s semantic
role within a sentence can also be characterized in
terms of binary features (dowty, 1991; reisinger et
al., 2015). our approach is more amenable to such
theoretical models than traditional distributed word
embeddings. however, we can also work with the ex-
pected id27s, which are vectors of prob-
abilities, and can therefore be expected to hold the
advantages of dense distributed representations (ben-
gio et al., 2013).

1the confusion is resolved by morphologically analyzing the
second example as (in+   ame)+able, but this requires hierarchi-
cal morphological parsing, not just segmentation.

2 model

the modeling framework is illustrated in figure 1,
focusing on the word sesquipedalianism. this word
is rare, but its morphology indicates several of its
properties: the -ism suf   x suggests that the word is a
noun, likely describing some abstract property; the
sesqui- pre   x refers to one and a half, and so on. if
the word is unknown, we must lean heavily on these
intuitions, but if the word is well attested then we can
rely instead on its examples in use.

it is this reasoning that our modeling framework
aims to formalize. we treat id27s as la-
tent variables in a joint probabilistic model. the prior
distribution over a word   s embedding is conditioned
on its morphological structure. the embedding it-
self then participates, as a latent variable, in a neural
sequence model over a corpus, contributing to the
overall corpus likelihood. if the word appears fre-
quently, then the corpus likelihood dominates the
prior     which is equivalent to relying on the word   s
distributional properties. if the word appears rarely,
then the prior distribution steps in, and gives a best
guess as to the word   s meaning.

before describing these component pieces in detail,
we    rst introduce some notation. the representation
of word w is a latent binary vector bw     {0, 1}k,
where k is the size of each id27. as
noted in the introduction, this binary representation
is motivated by feature-based theories of lexical se-
mantics (katz and fodor, 1963). each word w is
constructed from a set of mw observed morphemes,
mw = (mw,1, mw,2, . . . , mw,mw ). each morpheme
is in turn drawn from a    nite vocabulary of size
vm, so that mw,i     {1, 2, . . . , vm}. morphemes
are obtained from an unsupervised morphological
segmenter, which is treated as a black box. fi-
nally, we are given a corpus, which is a sequence
of words, x = (x1, x2, . . . , xn ), where each word
xt     {1, 2, . . . , vw}, with vw equal to the size of the
vocabulary, including the token (cid:104)unk(cid:105) for unknown
words.

2.1 prior distribution
the key differentiating property of this model is that
rather than estimating id27s directly, we
treat them as a latent variable, with a prior distri-
bution re   ecting the word   s morphological proper-

figure 1: model architecture, applied to the example sequence . . . plagued by sesquipedalianism . . . . blue solid arrows indicate
direct computation, red dashed arrows indicate probabilistic dependency. for simplicity, we present our models as recurrent neural
networks rather than long short-term memories (lstms).

ties. to characterize this prior distribution, each mor-
pheme m is associated with an embedding of its own,
um     rk, where k is again the embedding size. then
for position i of the id27 bw, we have
the following prior,

bw,i     bernoulli

  (

um,i)

,

(1)

(cid:33)

(cid:32)

(cid:88)

m   mw

where   (  ) indicates the sigmoid function. the prior
log-likelihood for a set of id27s is,

logp (b;m, u)

w

vw(cid:88)
vw(cid:88)
vw(cid:88)

w

=

=

=

k(cid:88)
k(cid:88)

i

log p (bw;mw, u)

log p (bw,i;mw, u)

(cid:32) (cid:88)
(cid:32)

m   mw
1       

(cid:33)
(cid:32) (cid:88)

um,i

m   mw

bw,i log   

w

i

+ (1     bw,i) log

(2)

(3)

(4)

(5)

(cid:33)(cid:33)

um,i

.

2.2 expected likelihood
the corpus likelihood is computed via a recurrent
neural network language model (mikolov et al., 2010,
id56lm), which is a generative model of sequences
of tokens. in the id56lm, the id203 of each
word is conditioned on all preceding words through

n(cid:88)

a recurrently updated state vector. this state vector
in turn depends on the embeddings of the previous
words, through the following update equations:

ht =f (bxt, ht   1)
xt+1    multinomial (softmax [vht]) .

(6)
(7)
the function f (  ) is a recurrent update equation; in
the id56, it corresponds to   (  ht   1 + bxt), where
  (  ) is the elementwise sigmoid function. the matrix
v     rv  k contains the    output embeddings    of each
word in the vocabulary. we can then de   ne the condi-
tional log-likelihood of a corpus x = (x1, x2, . . . xn )
as,

log p (x | b) =

log p (xt | ht   1, b).

(8)

t

since ht   1 is deterministically computed from
x1:t   1 (conditioned on b), we can equivalently write
the log-likelihood as,
log p (x | b) =

log p (xt | x1:t   1, b).

(cid:88)

(9)

t

this same notation can be applied to compute the
likelihood under a long-short term memory (lstm)
language model (sundermeyer et al., 2012). the only
difference is that the recurrence function f (  ) from
equation 6 now becomes more complex, including
the input, output, and forget gates, and the recurrent
state ht now includes the memory cell. as the lstm

plaguedbysesquipedalianism...h1h2h3bplaguedbbybsesquipedalianismuplagueubyusesquiuedupedaluianuismupdate equations are well known, we focus on the
more concise id56 notation, but we employ lstms
in all experiments due to their better ability to capture
long-range dependencies.

2.3 variational approximation

(cid:82) p (x1:n , b)db is intractable. we address this is-

id136 on the marginal likelihood p (x1:n ) =

sue by making a variational approximation,

log p (x) = log

= log

(cid:88)
(cid:88)
(cid:20)

b

b

p (x | b)p (b)

p (x | b)p (b)

(cid:21)

q(b)
q(b)
p (x | b)

p (b)
q(b)

(12)
= log eq
   eq[log p (x | b) + log p (b)     log q(b)]
(13)

the variational distribution q(b) is de   ned using a
fully factorized mean    eld approximation,

vw(cid:89)

k(cid:89)

w

i

q(b;   ) =

q(bw,i;   w,i).

(14)

the variational distribution is a product of bernoullis,
with parameters   w,j     [0, 1].
in the evaluations
that follow, we use the expected id27s
q(bw), which are dense vectors in [0, 1]k. we can
then use q(  ) to place a variational lower bound on
the expected conditional likelihood,

even with this variational approximation, the ex-
pected log-likelihood is still intractable to compute.
in recurrent neural network language models, each
word xt is conditioned on the entire prior history,
x1:t   1     indeed, this is one of the key advantages
over    xed-length id165 models. however, this
means that the individual expected log probabilities
involve not just the id27 of xt and its
immediate predecessor, but rather, the embeddings

of all words in the sequence x1:t:

eq [log p (x | b)]

n(cid:88)
n(cid:88)

t

=

=

(cid:88)

eq [log p (xt | x1:t   1, b)]

q({bw : w     x1:t})

t

{bw:w   x1:t}

   log p (xt | x1:t   1, b).

(15)

(16)

(17)

(10)

(11)

we therefore make a further approximation by tak-

ing a local expectation over the recurrent state,

eq [ht]     f (eq [bxt] , eq [ht   1])
(18)
eq [log p (xt | x1:t   1, b)]     log softmax (veq [ht]) .
(19)

this approximation means that we do not propa-
gate uncertainty about ht through the recurrent up-
date or through the likelihood function, but rather, we
use local point estimates. alternative methods such
as id5 (chung et al., 2015) or
sequential monte carlo (de freitas et al., 2000) might
provide better and more principled approximations,
but this direction is left for future work.

variational bounds in the form of equation 13
can generally be expressed as a difference between
an expected log-likelihood term and a term for the
kullback-leibler (kl) divergence between the prior
distribution p (b) and the variational distribution
q(b) (wainwright and jordan, 2008). incorporat-
ing the approximation in equation 19, the resulting
objective is,

n(cid:88)

t

l =

log p (xt | x1:t   1; eq[b])     dkl(q(b) (cid:107) p (b)).

the kl divergence is equal to,
dkl(q(b) (cid:107) p (b))

(20)

(21)

vw(cid:88)
vw(cid:88)

w

k(cid:88)
k(cid:88)

i

=

=

dkl(q(bw,i) (cid:107) p (bw,i))

(cid:88)

um,i)

(cid:88)

  w,i log   (

i

w

m   mw
+ (1       w,i) log(1       (
      w,i log   w,i     (1       w,i) log(1       w,i).
(22)

m   mw

um,i))

each term in the variational bound can be easily
constructed in a computation graph, enabling auto-
matic differentiation and the application of standard
stochastic optimization techniques.

implementation

3
the objective function is given by the variational
lower bound in equation 20, using the approxima-
tion to the conditional likelihood described in equa-
tion 19. this function is optimized in terms of several
parameters:

    the morpheme embeddings, {um}m   1...vm;
    the variational parameters on the word embed-

dings, {  }w   1...vw;

    the output id27s v;
    the parameter of the recurrence function,   .

these parameters is updated via the
each of
rmsprop online learning algorithm (tieleman and
hinton, 2012). the model and baseline (described be-
low) are implemented in blocks (van merri  enboer
et al., 2015). in the remainder of the paper, we refer
to our model as varembed.

3.1 data and preprocessing
all embeddings are trained on 22 million tokens
from the the north american news text (nant)
corpus (graff, 1995). we use an initial vocabu-
lary of 50,000 words, with a special (cid:104)unk(cid:105) token
for words that are not among the 50,000 most com-
mon. we then perform downcasing and convert all
numeric tokens to a special (cid:104)num(cid:105) token. after these

steps, the vocabulary size decreases to 48,986. note
that the method can impute id27s for
out-of-vocabulary words under the prior distribution
p (b;m, u); however, it is still necessary to decide
on a vocabulary size to determine the number of
variational parameters    and output embeddings to
estimate.

unsupervised morphological segmentation is per-
formed using morfessor (creutz and lagus, 2002),
with a maximum of sixteen morphemes per word.
this results in a total of 14,000 morphemes, which
includes stems for monomorphemic words. we do
not rely on any labeled information about morpho-
logical structure, although the incorporation of gold
morphological analysis is a promising topic for future
work.

3.2 learning details
the lstm parameters are initialized uniformly in
the range [   0.08, 0.08]. the id27s are
initialized using pre-trained id97 embeddings.
we train the model for 15 epochs, with an initial
learning rate of 0.01, a decay of 0.97 per epoch, and
minibatches of size 25. we clip the norm of the
gradients (normalized by minibatch size) at 1, using
the default settings in the rmsprop implementation
in blocks. these choices are motivated by prior
work (zaremba et al., 2014). after each iteration, we
compute the objective function on the development
set; when the objective does not improve beyond a
small threshold, we halve the learning rate.

training takes roughly one hour per iteration us-
ing an nvidia 670 gtx, which is a commodity
graphics processing unit (gpu) for gaming. this
is nearly identical to the training time required for
our reimplementation of the algorithm of botha and
blunsom (2014), described below.

3.3 baseline
the most comparable approach is that of botha and
blunsom (2014). in their work, embeddings are es-
timated for each morpheme, as well as for each in-
vocabulary word. the    nal embedding for a word is
then the sum of these embeddings, e.g.,

greenhouse = greenhouse + green + house,

(23)

where the italicized elements represent learned em-
beddings.

we build a baseline that is closely inspired by this
approach, which we call sumembed. the key differ-
ence is that while botha and blunsom (2014) build on
the log-bilinear language model (mnih and hinton,
2007), we use the same lstm-based architecture as
in our own model implementation. this enables our
evaluation to focus on the critical difference between
the two approaches: the use of latent variables rather
than summation to model the id27s. as
with our method, we used pre-trained id97
embeddings to initialize the model.

3.4 number of parameters
the dominant terms in the overall number of parame-
ters are the (expected) id27s themselves.
the variational parameters of the input word em-
beddings,   , are of size k    vw. the output word
embeddings are of size #|h|    vw. the morpheme
embeddings are of size k    vm, with vm (cid:28) vw. in
our main experiments, we set vw = 48, 896 (see
above), k = 128, and #|h| = 128. after including
the character embeddings and the parameters of the
recurrent models, the total number of parameters is
roughly 12.8 million. this is identical to number of
parameters in the sumembed baseline.

4 evaluation

our evaluation compares the following embeddings:

id97 we train the popular id97
of words)
(mikolov et al., 2013), using the

cbow (continuous
model
gensim implementation.

bag

sumembed we compare against the baseline de-
scribed in    3.3, which can be viewed as a
reimplementation of the compositional model
of botha and blunsom (2014).

varembed for our model, we take the expected
embeddings eq[b], and then pass them through
an inverse sigmoid function to obtain values
over the entire real line.

4.1 word similarity
our    rst evaluation is based on two classical word
similarity datasets: wordsim353 (finkelstein et al.,

all words
(4199)
n/a
32.8
33.6

vocab

in
(3997)
34.8
33.5
34.7

24.7
30.2

25.1
31.0

id97
sumembed
varembed
morphemes only
sumembed
varembed

table 2: alignment with lexical semantic features, as measured
by qvec. higher scores are better, with a maximum possible
score of 100.

2001) and the stanford    rare words    (rw) dataset (lu-
ong et al., 2013). we report spearmann   s   , a mea-
sure of rank correlation, evaluating on both the entire
vocabulary as well as the subset of in-vocabulary
words.

as shown in table 1, varembed consistently
outperforms sumembed on both datasets. on the
subset of in-vocabulary words, id97 gives
slightly better results on the wordsim words that are
in the nant vocabulary, but is not applicable to
the complete dataset. on the rare words dataset,
id97 performs considerably worse than both
morphology-based models, matching the    ndings of
luong et al. (2013) and botha and blunsom (2014)
regarding the importance of morphology for doing
well on this dataset.

4.2 alignment with lexical semantic features
recent work questions whether these word similar-
ity metrics are predictive of performance on down-
stream tasks (faruqui et al., 2016). the qvec statis-
tic is another intrinsic evaluation method, which has
been shown to be better correlated with downstream
tasks (tsvetkov et al., 2015). this metric measures
the alignment between id27s and a set
of lexical semantic features. speci   cally, we use the
semcor noun verb supersenses oracle provided at the
qvec github repository.2

as shown in table 2, varembed outperforms
sumembed on the full lexicon, and gives simi-
lar performance to id97 on the set of in-
vocabulary words. we also consider the morpheme
embeddings alone. for sumembed, this means that

2https://github.com/ytsvetko/qvec

id97

sumembed varembed

wordsim353
all words (353)
in-vocab (348)
rare words (rw)
all words (2034)
in-vocab (715)

n/a
51.4

n/a
33.6

42.9
45.9

23.0
37.3

48.8
51.3

24.0
44.1

table 1: word similarity evaluation results, as measured by spearmann   s       100. id97 cannot be evaluated on all words,
because embeddings are not available for out-of-vocabulary words. the total number of words in each dataset is indicated in
parentheses.

we construct the id27 from the sum of
the embeddings for its morphemes, without the ad-
ditional embedding per word. for varembed, we
use the expected embedding under the prior distribu-
tion e[b | c]. the results for these representations
are shown in the bottom half of table 2, revealing
that varembed learns much more meaningful em-
beddings at the morpheme level, while much of the
power of sumembed seems to come from the word
embeddings.

dev
id97
92.42
sumembed 93.26
93.05
varembed

test
92.40
93.26
93.09

table 3: part-of-speech tagging accuracies

4.3 part-of-speech tagging
our    nal evaluation is on the downstream task of
part-of-speech tagging, using the id32.
we build a simple classi   cation-based tagger, us-
ing a feedforward neural network. (this is not in-
tended as an alternative to state-of-the-art tagging
algorithms, but as a comparison of the syntactic
utility of the information encoded in the word em-
beddings.) the inputs to the network are the con-
catenated embeddings of the    ve word neighbor-
hood (xt   2, xt   1, xt, xt+1, xt+2); as in all evalua-
tions, 128-dimensional embeddings are used, so the
total size of the input is 640. this input is fed into
a network with two hidden layers of size 625, and
a softmax output layer over all tags. we train using
rmsprop (tieleman and hinton, 2012).
shown in table 3.

both
morphologically-informed
are
signi   cantly better to id97 (p < .01,
two-tailed binomial test), but the difference between
sumembed and varembed is not signi   cant
at p < .05. figure 2 breaks down the errors by
word frequency. as shown in the    gure, the tagger
based on id97 performs poorly for rare

embeddings

results

are

figure 2: error rates by word frequency.

words, which is expected because these embeddings
are estimated from sparse distributional statistics.
sumembed is slightly better on the rarest words
(the 0     100 group accounts for roughly 10% of
all tokens). in this case, it appears that this simple
additive model is better, since the distributional
statistics are too sparse to offer much improvement.
the probabilistic varembed embeddings are
best for all other frequency groups, showing that it
effectively combines morphology and distributional
statistics.

5 related work
adding side information to id27s
an alternative approach to incorporating additional
information into id27s is to constrain the
embeddings of semantically-related words to be sim-
ilar. such work typically draws on existing lexical

0-100100-10001000-1000010000-100000word frequency in nant0.000.050.100.150.200.250.300.35error rateembeddingvarembedsumembedid97semantic resources such as id138. for example,
yu and dredze (2014) de   ne a joint training objec-
tive, in which the id27 must predict not
only neighboring word tokens in a corpus, but also
related word types in a semantic resource; a similar
approach is taken by bian et al. (2014). alternatively,
faruqui et al. (2015) propose to    retro   t    pre-trained
id27s over a semantic network. both
retro   tting and our own approach treat the true word
embeddings as latent variables, from which the pre-
trained id27s are stochastically emitted.
however, a key difference from our approach is that
the underlying representation in these prior works is
relational, and not generative. these methods can
capture similarity between words in a relational lex-
icon such as id138, but they do not offer a gen-
erative account of how (approximate) meaning is
constructed from orthography or morphology.

embeddings

word
and morphology the
sumembed baseline is based on the work of botha
and blunsom (2014), in which words are segmented
into morphemes using morfessor (creutz and
lagus, 2002), and then word representations are
computed through addition of morpheme represen-
tations. a key modeling difference from this prior
work is that rather than computing id27s
directly and deterministically from subcomponent
embeddings (morphemes or characters, as in (ling
et al., 2015; kim et al., 2016)), we use these
subcomponents to de   ne a prior distribution, which
can be overridden by distributional statistics for
common words. other work exploits morphology
by training id27s to optimize a joint
objective over distributional statistics and rich,
morphologically-augmented part of speech tags (cot-
terell and sch  utze, 2015). this can yield better word
embeddings, but does not provide a way to compute
embeddings for unseen words, as our approach does.
recent work by cotterell et al. (2016) extends the
idea of retro   tting, which was based on semantic
similarity, to a morphological framework. in this
model, embeddings are learned for morphemes as
well as for words, and each id27 is con-
ditioned on the sum of the morpheme embeddings,
using a multivariate gaussian. the covariance of this
gaussian prior is set to the inverse of the number of
examples in the training corpus, which has the effect

of letting the morphology play a larger role for rare
or unseen words. like all retro   tting approaches, this
method is applied in a pipeline fashion after training
id27s on a large corpus; in contrast, our
approach is a joint model over the morphology and
corpus. another practical difference is that cotterell
et al. (2016) use gold morphological features, while
we use an automated morphological segmentation.

latent id27s id27s are
typically treated as a parameter, and are optimized
through point estimation (bengio et al., 2003; col-
lobert and weston, 2008; mikolov et al., 2010). cur-
rent models use id27s with hundreds or
even thousands of parameters per word, yet many
words are observed only a handful of times. it is
therefore natural to consider whether it might be
bene   cial to model uncertainty over word embed-
dings. vilnis and mccallum (2014) propose to model
gaussian densities over dense vector word embed-
dings. they estimate the parameters of the gaussian
directly, and, unlike our work, do not consider us-
ing orthographic information as a prior distribution.
this is easy to do in the latent binary framework
proposed here, which is also a better    t for some the-
oretical models of lexical semantics (katz and fodor,
1963; reisinger et al., 2015). this view is shared by
kruszewski et al. (2015), who induce binary word
representations using labeled data of lexical seman-
tic entailment relations, and by henderson and popa
(2016), who take a mean    eld approximation over
binary representations of lexical semantic features to
induce hyponymy relations.

more broadly, our work is inspired by recent ef-
forts to combine directed id114 with dis-
criminatively trained    deep learning    architectures.
the variational autoencoder (kingma and welling,
2014), neural variational id136 (mnih and gregor,
2014; miao et al., 2016), and black box variational
id136 (ranganath et al., 2014) all propose to use
a neural network to compute the variational approx-
imation. these ideas are employed by chung et al.
(2015) in the variational recurrent neural network,
which places a latent continuous variable at each time
step. in contrast, we have a dictionary of latent vari-
ables     the id27s     which introduce
uncertainty over the hidden state ht in a standard re-
current neural network or lstm. we train this model

by employing a mean    eld approximation, but these
more recent techniques for neural variational infer-
ence may also be applicable. we plan to explore this
possibility in future work.

6 conclusion and future work

we present a model
that uni   es compositional
and distributional perspectives on lexical semantics,
through the machinery of bayesian latent variable
models. in this framework, our prior expectations
of word meaning are based on internal structure, but
these expectations can be overridden by distributional
statistics. the model is based on the very successful
long-short term memory (lstm) for sequence mod-
eling, and while it employs a bayesian justi   cation,
its id136 and estimation are little more compli-
cated than a standard lstm. this demonstrates the
advantages of reasoning about uncertainty even when
working in a    neural    paradigm.

this work represents a    rst step, and we see many
possibilities for improving performance by extending
it. clearly we would expect this model to be more ef-
fective in languages with richer morphological struc-
ture than english, and we plan to explore this possi-
bility in future work. from a modeling perspective,
our prior distribution merely sums the morpheme em-
beddings, but a more accurate model might account
for sequential or combinatorial structure, through
a recurrent (ling et al., 2015), recursive (luong et
al., 2013), or convolutional architecture (kim et al.,
2016). there appears to be no technical obstacle
to imposing such structure in the prior distribution.
furthermore, while we build the prior distribution
from morphemes, it is natural to ask whether char-
acters might be a better underlying representation:
character-based models may generalize well to non-
word tokens such as names and abbreviations, they
do not require morphological segmentation, and they
require a much smaller number of underlying em-
beddings. on the other hand, morphemes encode
rich regularities across words, which may make a
morphologically-informed prior easier to learn than
a prior which works directly at the character level.
it is possible that this tradeoff could be transcended
by combining characters and morphemes in a single
model.

another advantage of latent variable models is that
they admit partial supervision. if we follow tsvetkov
et al. (2015) in the argument that id27s
should correspond to lexical semantic features, then
an inventory of such features could be used as a
source of partial supervision, thus locking dimen-
sions of the id27s to speci   c semantic
properties. this would complement the graph-based
   retro   tting    supervision proposed by faruqui et al.
(2015), by instead placing supervision at the level of
individual words.

acknowledgments

thanks to erica briscoe, martin hyatt, yangfeng ji,
bryan leslie lee, and yi yang for helpful discussion
of this work. thanks also the emnlp reviewers for
constructive feedback. this research is supported by
the defense threat reduction agency under award
hdtra1-15-1-0019.

references
yoshua bengio, r  ejean ducharme, pascal vincent, and
christian janvin. 2003. a neural probabilistic language
model. the journal of machine learning research,
3:1137   1155.

yoshua bengio, aaron courville, and pascal vincent.
2013. representation learning: a review and new per-
spectives. ieee transactions on pattern analysis and
machine intelligence, 35(8):1798   1828.

jiang bian, bin gao, and tie-yan liu. 2014. knowledge-
powered deep learning for id27.
in
machine learning and knowledge discovery in
databases, pages 132   148. springer.

jan a botha and phil blunsom. 2014. compositional mor-
phology for word representations and language mod-
elling. in proceedings of the international conference
on machine learning (icml).

peter f brown, peter v desouza, robert l mercer, vin-
cent j della pietra, and jenifer c lai. 1992. class-
based id165 models of natural language. computa-
tional linguistics, 18(4):467   479.

danqi chen and christopher d manning. 2014. a fast
and accurate dependency parser using neural networks.
in proceedings of empirical methods for natural lan-
guage processing (emnlp), pages 740   750.

kyunghyun cho, bart van merri  enboer, caglar gulcehre,
dzmitry bahdanau, fethi bougares, holger schwenk,
and yoshua bengio. 2014. learning phrase representa-
tions using id56 encoder-decoder for statistical machine

translation. in proceedings of empirical methods for
natural language processing (emnlp).

junyoung chung, kyle kastner, laurent dinh, kratarth
goel, aaron courville, and yoshua bengio. 2015.
a recurrent latent variable model for sequential data.
in neural information processing systems (nips),
montr  eal.

ronan collobert and jason weston. 2008. a uni   ed
architecture for natural language processing: deep neu-
ral networks with multitask learning. in proceedings
of the international conference on machine learning
(icml), pages 160   167.

ryan cotterell and hinrich sch  utze. 2015. morpho-
logical word-embeddings. in proceedings of the north
american chapter of the association for computational
linguistics (naacl), denver, co, may.

ryan cotterell, hinrich sch  utze, and jason eisner. 2016.
morphological smoothing and extrapolation of word
in proceedings of the association for
embeddings.
computational linguistics (acl), berlin, august.

mathias creutz and krista lagus.

2002. unsuper-
vised discovery of morphemes. in proceedings of the
acl-02 workshop on morphological and phonologi-
cal learning-volume 6, pages 21   30. association for
computational linguistics.

jo  ao fg de freitas, mahesan niranjan, andrew h. gee,
and arnaud doucet. 2000. sequential monte carlo
methods to train neural network models. neural com-
putation, 12(4):955   993.

david dowty. 1991. thematic proto-roles and argument

selection. language, pages 547   619.

manaal faruqui, jesse dodge, sujay k jauhar, chris dyer,
eduard hovy, and noah a smith. 2015. retro   tting
in proceedings
word vectors to semantic lexicons.
of the north american chapter of the association for
computational linguistics (naacl), denver, co, may.
manaal faruqui, yulia tsvetkov, pushpendre rastogi,
and chris dyer. 2016. problems with evaluation of
id27s using word similarity tasks. arxiv,
1605.02276.

lev finkelstein, evgeniy gabrilovich, yossi matias, ehud
rivlin, zach solan, gadi wolfman, and eytan ruppin.
2001. placing search in context: the concept revisited.
in www, pages 406   414. acm.

david graff. 1995. north american news text corpus.
morris halle and alec marantz. 1993. distributed mor-
phology and the pieces of in   ection. in kenneth l.
hale and samuel j. keyser, editors, the view from
building 20. mit press, cambridge, ma.

james henderson and diana nicoleta popa. 2016. a
vector space for id65 for entailment.
in proceedings of the association for computational
linguistics (acl), berlin, august.

jerrold j katz and jerry a fodor. 1963. the structure of

a semantic theory. language, pages 170   210.

yoon kim, yacine jernite, david sontag, and alexan-
der m rush. 2016. character-aware neural language
models. in proceedings of the national conference on
arti   cial intelligence (aaai).

diederik p kingma and max welling. 2014. auto-
encoding id58. in proceedings of the in-
ternational conference on learning representations
(iclr).

german kruszewski, denis paperno, and marco baroni.
2015. deriving boolean structures from distributional
vectors. transactions of the association for computa-
tional linguistics, 3:375   388.

wang ling, tiago lu    s, lu    s marujo, ram  on fernandez
astudillo, silvio amir, chris dyer, alan w black, and
isabel trancoso. 2015. finding function in form: com-
positional character models for open vocabulary word
representation. in proceedings of empirical methods
for natural language processing (emnlp), lisbon,
september.

minh-thang luong, richard socher, and christopher d
manning. 2013. better word representations with
in pro-
id56s for morphology.
ceedings of the conference on computational natural
language learning (conll).

yishu miao, lei yu, and phil blunsom. 2016. neural vari-
ational id136 for text processing. in proceedings
of the international conference on machine learning
(icml).

tomas mikolov, martin kara     at, lukas burget, jan cer-
nock`y, and sanjeev khudanpur. 2010. recurrent neu-
ral network based language model. in interspeech,
pages 1045   1048.

tomas mikolov, ilya sutskever, kai chen, greg s cor-
rado, and jeff dean. 2013. distributed representations
of words and phrases and their compositionality. in
advances in neural information processing systems,
pages 3111   3119.

andriy mnih and karol gregor. 2014. neural varia-
tional id136 and learning in belief networks.
in
proceedings of the international conference on ma-
chine learning (icml), pages 1791   1799.

andriy mnih and geoffrey hinton. 2007. three new
id114 for statistical language modelling. in
proceedings of the international conference on ma-
chine learning (icml).

rajesh ranganath, sean gerrish, and david blei. 2014.
in proceedings of
black box variational id136.
the seventeenth international conference on arti   cial
intelligence and statistics, pages 814   822.

drew reisinger, rachel rudinger, francis ferraro, craig
harman, kyle rawlins, and benjamin van durme.

2015. semantic proto-roles. transactions of the asso-
ciation for computational linguistics, 3:475   488.

cicero d. santos and bianca zadrozny. 2014. learning
character-level representations for part-of-speech tag-
ging. in proceedings of the international conference
on machine learning (icml), pages 1818   1826.

martin sundermeyer, ralf schl  uter, and hermann ney.
2012. lstm neural networks for id38.
in proceedings of interspeech.

tijman tieleman and geoffrey hinton. 2012. lecture 6.5:
rmsprop. technical report, coursera neural networks
for machine learning.

yulia tsvetkov, manaal faruqui, wang ling, guillaume
lample, and chris dyer. 2015. evaluation of word
vector representations by subspace alignment. in pro-
ceedings of empirical methods for natural language
processing (emnlp), lisbon, september.

bart van merri  enboer, dzmitry bahdanau, vincent du-
moulin, dmitriy serdyuk, david warde-farley, jan
chorowski, and yoshua bengio. 2015. blocks and fuel:
frameworks for deep learning. corr, abs/1506.00619.
2014. word
corr,

luke vilnis and andrew mccallum.

representations via gaussian embedding.
abs/1412.6623.

martin j. wainwright and michael i. jordan. 2008. graph-
ical models, exponential families, and variational infer-
ence. foundations and trends in machine learning,
1(1-2):1   305.

mo yu and mark dredze. 2014. improving lexical em-
beddings with semantic knowledge. in proceedings of
the association for computational linguistics (acl),
pages 545   550, baltimore, md.

wojciech zaremba, ilya sutskever, and oriol vinyals.
2014. recurrent neural network id173. arxiv
preprint arxiv:1409.2329.

george kingsley zipf. 1949. human behavior and the

principle of least effort. addison-wesley.

