6
1
0
2

 

n
u
j
 

0
2

 
 
]
l
c
.
s
c
[
 
 

1
v
1
2
1
6
0

.

6
0
6
1
:
v
i
x
r
a

quantifying and reducing stereotypes in id27s

tolga bolukbasi1
kai-wei chang2
james zou2
venkatesh saligrama1
adam kalai2
1 boston university, 8 saint mary   s street, boston, ma
2 microsoft research new england, 1 memorial drive, cambridge, ma

tolgab@bu.edu
kw@kwchang.net
jamesyzou@gmail.com
srv@bu.edu
adam.kalai@microsoft.com

abstract

machine learning algorithms are optimized to
model statistical properties of the training data.
if the input data re   ects stereotypes and biases
of the broader society, then the output of the
learning algorithm also captures these stereo-
types.
in this paper, we initiate the study of
gender stereotypes in id27, a pop-
ular framework to represent text data. as their
use becomes increasingly common, applications
can inadvertently amplify unwanted stereotypes.
we show across multiple datasets that the em-
beddings contain signi   cant gender stereotypes,
especially with regard to professions. we created
a novel gender analogy task and combined it
with id104 to systematically quantify
the gender bias in a given embedding. we
developed an ef   cient algorithm that reduces
gender stereotype using just a handful of training
examples while preserving the useful geometric
properties of the embedding. we evaluated our
algorithm on several metrics. while we focus on
male/female stereotypes, our framework may be
applicable to other types of embedding biases.

1. introduction
id27s, trained only on word co-occurrence
in text corpora, capture rich semantic information about
words and their meanings (mikolov et al., 2013b). each
word (or common phrase) w     w is encoded as a
d-dimensional word vector vw     rd. using simple
vector arithmetic, the embeddings are capable of answering
1
analogy puzzles. for instance, man:king :: woman:
returns queen as the answer, and similarly japan is returned

1an analogy puzzle, a:b :: c:d, involves selecting the most

appropriate d given a, b, and c.

2016 icml workshop on #data4good: machine learning in
social good applications, new york, ny, usa. copyright by the
author(s).

41

for paris:france :: tokyo:japan (computer-generated an-
swers are underlined). a number of such embeddings
have been made publicly available including the popular
id97 (mikolov et al., 2013a; mikolov et al.) embed-
ding trained on 3 million words into 300 dimensions, which
we refer to here as the w2vnews embedding because
it was trained on a corpus of text from google news.
these id27s have been used in a variety of
downstream applications (e.g., document ranking (nalis-
nick et al., 2016), id31 (  irsoy & cardie,
2014), and question retrieval (lei et al., 2016)).
while word-embeddings encode semantic information they
also exhibit hidden biases inherent in the dataset they
are trained on. for instance, id27s based on
w2vnews can return biased solutions to analogy puzzles
such as father:doctor :: mother:nurse and man:computer
programmer :: woman:homemaker. other publicly avail-
able embeddings produce similar results exhibiting gender
stereotypes. moreover,
the closest word to the query
black male returns assaulted while the response
to white male is entitled to. this raises serious
concerns about their widespread use.
the prejudices and stereotypes in these embeddings re   ect
biases implicit in the data on which they were trained. the
embedding of a word is typically optimized to predict co-
occuring words in the corpus. therefore, if mother and
nurse frequently co-occur, then the vectors vmother and
vnurse also tend to be more similar and encode the gender
stereotypes. the use of embeddings in applications can
amplify these biases. to illustrate this point, consider web
search where, for example, one recent project has shown
that, when carefully combined with existing approaches,
word vectors can signi   cantly improve web page relevance
results (nalisnick et al., 2016) (note that this work is a proof
of concept     we do not know which, if any, mainstream
search engines presently incorporate id27s).
consider a researcher seeking a summer intern to work on
a machine learning project on deep learning who searches
for, say,    linkedin graduate student machine learning neural
networks.    now, a id27   s semantic knowledge

quantifying and reducing stereotypes in id27s

can improve relevance in the sense that a linkedin web
page containing terms such as    phd student,       embed-
dings,    and    deep learning,    which are related to but
different from the query terms, may be ranked highly in the
results. however, id27s also rank cs research
related terms closer to male names than female names. the
consequence would be, between two pages that differed in
the names mary and john but were otherwise identical, the
search engine would rank john   s higher than mary. in this
hypothetical example, the usage of id27 makes
it even harder for women to be recognized as computer
scientists and would contribute to widening the existing
gender gap in computer science. while we focus on
gender bias, speci   cally male/female, our approach may
be applied to other types of biases.
we propose two methods to systematically quantify the
gender bias in a set of id27s. first, we quantify
how words, such as those corresponding to professions,
are distributed along the direction between embeddings of
he and she. second, we design an algorithm for gener-
ating analogy pairs from an embedding given two seed
words and we use crowdworkers to quantify whether these
embedding analogies re   ect stereotypes. some analogies
re   ect stereotypes such as he:janitor :: she:housekeeper
and he:alcoholism :: she:eating disorders. finally, others
may provoke interesting discussions such as he:realist ::
she:feminist and he:injured :: she:victim.
since biases are cultural, we enlist u.s.-based crowd-
workers to identify analogies to judge whether analogies:
(a) re   ect stereotypes (to understand biases), or (b) are
nonsensical (to ensure accuracy). we    rst establish that
biases indeed exist in the embeddings. we then show that,
surprisingly, information to distinguish stereotypical asso-
ciations like female:homemaker from de   nitional associa-
tions like female:sister can often be removed. we propose
an approach that, given an embedding and only a handful
of words, can reduce the amount of bias present in that
embedding without signi   cantly reducing its performance
on other benchmarks.
contributions. (1) we initiate the study of stereotypes and
biases in id27s. our work follows a large body
of literature on bias in language, but id27s are
of speci   c interest because they are commonly used in ma-
chine learning and they have simple geometric structures
that can be quanti   ed mathematically. (2) we develop two
metrics to quantify gender stereotypes in id27s
based on words associated with professions together with
automatically generated analogies which are then scored
by the crowd. (3) we develop a new algorithm that reduces
gender stereotypes in the embedding using only a handful
of training examples while preserving useful properties of
the embedding.

figure 1. comparison of gender bias of profession words across
two embeddings: id97 trained on googlenews and glove
trained web-crawl texts. the x and y axes show projections onto
the he-she direction in the two embeddings. each dot is one of
249 common profession words. words closest to he, closest to
she, and in between the two are colored in red and shown in the
plot.
prior work. the body of prior work on bias in language
and prejudice in machine learning algorithms is too large to
fully cover here. we note that gender stereotypes have been
shown to develop in children as young as two years old
(turner & gervai, 1995). statistical analyses of language
have shown interesting contrasts between language used to
describe men and women, e.g., in recommendation letters
(schmader et al., 2007). a number of online systems
have been shown to exhibit various biases, such as racial
discrimination in the ads presented to users (sweeney,
2013). approaches to modify classi   cation algorithms to
de   ne and achieve various notions of fairness have been
described in a number of works, see, e.g., (barocas &
selbst, 2014; dwork et al., 2012) and a recent survey
(zliobaite, 2015).

2. implicit stereotypes in id27
stereotyped words. a simple approach to explore how
gender stereotypes manifest in embeddings is to quantify
which words are closer to he versus she in the embedding
space (using other words to capture gender, such as man
and woman, gives similar but noisier results due to their
multiple meanings). we used a list of 215 common
profession names, removing names that are associated with
one gender by de   nition (e.g. waitress, waiter). for each
name, v, we computed its projection onto the gender axis:
v    (vhe     vshe)/||vhe     vshe||2. figure 1 shows the
projection of professions on the w2vnews embedding (x-
axis) and on a different embedding trained by glove on a
dataset of web-crawled texts (y-axis). several professions
are closer to the he or she vector and this is consistent

42

quantifying and reducing stereotypes in id27s

across the embeddings, suggesting that embeddings encode
gender stereotypes.
stereotyped analogies. while professions give easily-
interpretable insights on embedding stereotypes, we devel-
oped a more general method to automatically detect and
quantify gender bias in any id27. embeddings
have shown to perform well in analogy tasks. motivated
by this, we ask the embedding to generate analogous word
pairs for he and she, and use crowd-sourcing to evaluate the
degree of stereotype of each pair.
a desired analogy he:she :: w1:w2 has the following
properties2: 1) the direction of w1-w2 has to align with
he-she; 2) w1 and w2 should be semantically similar, i.e.
||w1     w2||2 is not too large. based on this, given a word
embedding e, we proposed to score analogous pairs by the
following formulation:

sd(wa, wb) =

(wa     wb)    d
||wa     wb||2

s.t. ||wa     wb||2        (1)

where d = (vhe     vshe)/||vhe     vshe||2 is the gender
direction and    is a threshold for similarity.3 we observe
that setting    = 1 often works well in practice;
this
corresponds to requiring that the two words forming the
analogy are signi   cantly closer together than two random
embedding vectors.
from the embedding, we generated the top analogous pairs
with the largest sd scores. to avoid redundancies,
if
multiple pairs share the same wa or wb, we kept only
one pair. then we employed amazon mechanical turk
to evaluate the analogies.
for each analogy, such as
man:woman :: doctor:nurse, we ask the turkers two yes/no
questions to verify if this pairing makes sense as an analogy
and whether it exhibits gender stereotype. every word
pair is judged by 10 turkers, and we used the number of
turkers that rated this pair as stereotyped to quantify the
degree of bias of this analogy. table 1 shows the most
and least stereotypical analogies generated by id97 on
googlenews. overall, 21% and 32% analogy judgments
were stereotypical and nonsensical, respectively, by the
turkers.
3. reducing stereotypes in id27
having demonstrated that id27s contain sub-
stantial stereotypes in both professions and analogies, we
developed a method to reduce these stereotypes while
preserving the desirable geometry of the embedding.
id27s are often trained on a large corpus

2for the ease of presentation, we abuse the notation to use w

to represent a word or a word vector depending on the context.

3we explored alternatives

including a variation of 3-
cosmul (levy & goldberg, 2014) for generating word pairs, and
observe that the proposed approach works the best.

43

(w2vnews is trained on google news corpus with 100
billion words). as a result, it is impractical and even
impossible (the corpus is not publicly accessible) to reduce
the stereotypes during the training of the the word vectors.
therefore, we assume that we are given a a set of word
vectors and aim to remove stereotypes as a post-processing
step.
our approach takes the following as inputs: (1) a word
embedding stored in a matrix e     rn,r, where n is the
number of words and r is the dimension of the latent
(2) a matrix b     rnb,r where each column is
space.
a vector representing a direction of stereotype.
in this
paper, b = vhe     vshe, but in general, b can contain
multiple stereotypes including gender, racism, etc. 4 (3)
a matrix p     rnp,r whose columns correspond to set of
seed words that we want to debias. an example of a seed
word for gender is manager. (4) a matrix a     e whose
columns represent a background set of words. we want the
algorithm to preserve their pairwise distances.5
the goal is to generate a transformation matrix   t     n r,r,
which has the following properties:
    the transformed embeddings are stereotypical-free. that
is every column vectors in p t should be perpendicular to
column vectors in bt (i.e., p t t t bt     0).
    the transformed embeddings preserve the distances
between any two vectors in the matrix a.
let x = t t t , we can capture these two objectives as the
following semi-positive de   nite programming problem.

(cid:107)axat     aat(cid:107)2

f +   (cid:107)p xbt(cid:107)2

f

(2)

min
x(cid:23)0

where (cid:107)   (cid:107)f is the frobenius norm, the    rst term ensures
that the pairwise distances are preserved, and the second
term induces the biases to be small on the seed words. the
user-speci   ed parameter    balances the two terms.
directly solving this sdp optimization problem is chal-
lenging.
in practice, the dimension of matrix a is in
the scale of 400,000    300. the dimensions of the
matrices axat and aat are 400, 000  400, 000, causing
computational and memory issues. we conduct singular
value decomposition on a, such that a = u   v t , where u
and v are orthogonal matrices and    is a diagonal matrix.

(cid:107)axat     aat(cid:107)2

f = (cid:107)a(x     i)at(cid:107)2
= (cid:107)u   v t (x     i)v   u t(cid:107)2
= (cid:107)  v t (x     i)v   (cid:107)2
f .

f

f

(3)

4here we assume the stereotypical directions are given.

in
practice, this can be obtained by subjecting the vectors of the
he and she representing
extreme words in the concept (e.g.
gender.)

5 typically, we can set a to contain the word vectors in e

except the ones in b and p .

quantifying and reducing stereotypes in id27s

ranked as m-f stereotypical by 10/10 workers:

surgeon:nurse

hummer:minivan
athlete:gymnast
chauffeur:nanny
musician:dancer
workout:pilates
carpentry:sewing

doctors:midwives
karate:gymnastics
neurologist:therapist

curator:librarian
beers:cocktails

home depot:jc penney
accountant:paralegal

athletes:gymnasts

woodworking:quilting
hockey:   gure skating
pilots:   ight attendant

paramedic:registered nurse
alcoholism:eating disorders
architect:interior designer
drug traf   cking:prostitution

sopranos:real housewives
weightlifting:gymnastics
addiction:eating disorder

headmaster:guidance counselor
sports illustrated:vanity fair

professor emeritus:associate professor

ranked as m-f stereotypical by 0/10 workers: (random sample of 12 out of 505)

jon:heidi
erick:karla
sausages:buns

ainge:fulmer

allan:lorna

george clooney:penelope cruz

gentlemen:ladies
patriarch:matriarch

christopher:jennifer

leroy:lucille

veterans:servicemen

phillip:belinda

table 1. sample of the top 1,000 analogies generated for he:she :: wa:wb on w2vnews, ordered by the number of workers who judged
them to re   ect stereotypes. the analogies which were rated stereotypical by 10/10 workers are shown and a random sample of twelve
analogies rated as stereotypical by 0/10 workers is shown. overall, 21% of the 1000 analogies were rated as re   ecting gender stereotypes.

f

=

tr(u xu t u x t u t )

the last equality follows the fact that u is an orthogonal
matrix ((cid:107)u xu t(cid:107)2
=
tr(u xx t u t ) = tr(xx t u t u ) = (cid:107)x(cid:107)2
f .)
substituting eq. (3) to eq. (2) gives
(cid:107)  v t (x     i)v   (cid:107)2

f +   (cid:107)p xbt(cid:107)2

(4)

f

min
x(cid:23)0

here   v t (x     i)v    is a 300    300 matrix and can
be solved ef   ciently. the solution t is the debiasing
transformation of the id27.

experimental validation. to validate our debiasing al-
gorithm, we asked turkers to suggest words that are likely
to re   ect gender stereotype (e.g. manager, nurse). we
collected 438 such words, of which a random setup of
350 are used for training as the columns of the p matrix.
the remaining are used for testing. figure 2 illustrates
the results of the algorithm. the blue circles are the 88
gender-stereotype words suggested by the turkers which
form our held-out test set. the green crosses are a random
sample of background words that were not suggested to
have stereotype. most of the stereotype words lie close to
the y = 0 line, consistent with them lies near the midpoint
between he and she. in contrast the background points were
substantially less affected by the debiasing transformation.
we use variances to quantify this result. for each test word
(either gender-stereotypical or background) we project it
onto the he - she direction. then we compute the variance
of the projections in the original embedding and after the
debiasing transformation. for the gender-stereotype test
words,
the variance in the original embedding is 0.02
and the variance after the transformation is 0.001. for
the background words, the variance before and after the
transformation was 0.005 and 0.0055 respectively. this
demonstrates that the transformation was able to reduce
gender stereotype.

figure 2. the changes of word vectors on the gender direction.
the x and y axes show the absolute values of the projections onto
the he-she direction before and after debasing. the solid line is
the diagonal. the blue           are gender-stereotypical words in the
test set, and the green    x    are randomly selected other words that
were not suggested to be gender related.
lastly to verify that the debiasing transformation t pre-
serves the desirable geometric structure of the embedding,
we tested the transformed embedding a several standard
benchmarks that measure whether related words have sim-
ilar embeddings as well as how well the embedding per-
forms in analogy tasks. table 2 shows the results on the
original and the transformed embeddings and the transfor-
mation does not negatively impact the performance.

model
before
after

rg ws353
0.700
0.761
0.764
0.700

rw msr-analogy
0.471
0.472

0.712
0.712

the word
table 2. the columns show the performance of
embeddings on the standard id74. rg (rubenstein
& goodenough, 1965), rw (luong et al., 2013), ws353
(finkelstein et al., 2001), msr-analogy (mikolov et al., 2013b)

44

quantifying and reducing stereotypes in id27s

sweeney, latanya. discrimination in online ad delivery.

queue, 11(3):10, 2013.

turner, patricia j and gervai, judit. a multidimensional
study of gender typing in preschool children and their
parents: personality, attitudes, preferences, behavior,
and cultural differences. developmental psychology, 31
(5):759, 1995.

zliobaite,

indre.

discrimination in machine learning.
arxiv:1511.00148, 2015.

a survey on measuring indirect
arxiv preprint

references
barocas, solon and selbst, andrew d. big data   s disparate

impact. available at ssrn 2477899, 2014.

dwork, cynthia, hardt, moritz, pitassi, toniann, reingold,
through
in innovations in theoretical computer

and zemel, richard.

omer,
awareness.
science conference, 2012.

fairness

finkelstein, lev, gabrilovich, evgeniy, matias, yossi,
rivlin, ehud, solan, zach, wolfman, gadi, and ruppin,
eytan. placing search in context: the concept revisited.
in www. acm, 2001.

  irsoy, ozan and cardie, claire. deep recursive neural
in nips.

networks for compositionality in language.
2014.

lei, tao, joshi, hrishikesh, barzilay, regina, jaakkola,
tommi, katerina tymoshenko, alessandro moschitti,
and marquez, liuis. semi-supervised question retrieval
with gated convolutions. in naacl. 2016.

levy, omer and goldberg, yoav. linguistic regularities
in sparse and explicit word representations. in conll,
2014.

luong, thang, socher, richard, and manning, christo-
pher d. better word representations with recursive
in conll, pp. 104   
neural networks for morphology.
113. citeseer, 2013.

mikolov, tomas, sutskever, ilya, chen, kai, corrado, gre-
gory s., and dean, jeffrey. distributed representations of
words and phrases and their compositionality. in nips.

mikolov, tomas, chen, kai, corrado, greg, and dean,
jeffrey. ef   cient estimation of word representations in
vector space. in iclr, 2013a.

mikolov, tomas, yih, wen-tau, and zweig, geoffrey.
linguistic regularities in continuous space word repre-
sentations. in hlt-naacl, pp. 746   751, 2013b.

nalisnick, eric, mitra, bhaskar, craswell, nick, and
caruana, rich. improving document ranking with dual
id27s. in www, april 2016.

rubenstein, herbert and goodenough, john b. contextual
correlates of synonymy. communications of the acm, 8
(10):627   633, 1965.

jessica,

schmader, toni, whitehead,

a linguistic comparison of

and wysocki,
letters of
vicki h.
recommendation for male and female chemistry and
biochemistry job applicants. sex roles, 57(7-8):509   
514, 2007.

45

