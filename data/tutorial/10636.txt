adversarial deep averaging networks

for cross-lingual sentiment classi   cation

yu sun   

ben athiwaratkun   

ys646@cornell.edu

pa338@cornell.edu

xilun chen   

xlchen@cs.cornell.edu
claire cardie   

cardie@cs.cornell.edu

kilian weinberger   
kqw4@cornell.edu
   dept. of computer science, cornell university, ithaca ny, usa
   dept. of statistical science, cornell university, ithaca ny, usa

8
1
0
2

 

g
u
a
8
1

 

 
 
]
l
c
.
s
c
[
 
 

5
v
4
1
6
1
0

.

6
0
6
1
:
v
i
x
r
a

abstract

in recent years great success has been
achieved in sentiment classi   cation for en-
glish, thanks in part to the availability of co-
pious annotated resources. unfortunately,
most languages do not enjoy such an abun-
dance of labeled data. to tackle the senti-
ment classi   cation problem in low-resource
languages without adequate annotated data,
we propose an adversarial deep averaging
network (adan1) to transfer the knowledge
learned from labeled data on a resource-rich
source language to low-resource languages
where only unlabeled data exists. adan has
two discriminative branches: a sentiment
classi   er and an adversarial language dis-
criminator. both branches take input from a
shared feature extractor to learn hidden rep-
resentations that are simultaneously indica-
tive for the classi   cation task and invariant
across languages. experiments on chinese
and arabic sentiment classi   cation demon-
strate that adan signi   cantly outperforms
state-of-the-art systems.

1

introduction

many state-of-the-art models for sentiment classi-
   cation (socher et al., 2013; iyyer et al., 2015; tai
et al., 2015) are supervised learning approaches
that rely on the availability of an adequate amount
of labeled training data. for a few resource-rich
languages including english, such labeled data is
indeed available. for the vast majority of lan-
guages, however, it is the norm that only a limited
amount of annotated text exists. worse still, many
low-resource languages have no labeled data at all.
to aid the creation of sentiment classi   cation
systems in such low-resource languages, an active

1the source code of adan is available at https://

github.com/ccsasuke/adan

research direction is cross-lingual sentiment clas-
si   cation (clsc) in which the abundant resources
of a source language (likely english, denoted as
source) are leveraged to produce sentiment clas-
si   ers for a target language (target). in gen-
eral, clsc methods make use of general-purpose
bilingual resources     such as hand-crafted bilin-
gual lexica or parallel corpora     to alleviate or
eliminate the need for task-speci   c target an-
notations. in particular, the bilingual resource of
choice for the majority of previous clsc models
is a full-   edged machine translation (mt) sys-
tem (wan, 2008, 2009; lu et al., 2011; zhou et al.,
2016), a component that is expensive to obtain.
in this work, we propose a language-adversarial
training approach that does not need a highly en-
gineered mt system, and requires orders of mag-
nitude less in terms of the size of parallel corpus.
speci   cally, we propose an adversarial deep av-
eraging network (adan) that leverages a set of
bilingual id27s (bwes, zou et al.,
2013) trained on bitexts, in order to eliminate the
need for labeled target training data2.

we introduce the adan model in   2, and in   3
evaluate adan using english as the source with
two target choices: chinese and arabic. adan
is    rst compared to two baseline systems: i) one
trained only on labeled source data, relying on
bwes for cross-lingual generalization; and ii) a
id20 method (chen et al., 2012) that
views the two languages simply as two distinct do-
mains. we then validate adan against two state-
of-the-art clsc methods:
iii) an approach that
employs a powerful mt system, and iv) the cross-
lingual    distillation    approach of xu and yang
(2017) that makes direct use of a parallel corpus

2when not using any target annotations, the setting
is sometimes referred to as unsupervised (in the target lan-
guage) in the literature. similarly, when some labeled data is
used, it is called the semi-supervised setting.

(see   3.2). in all cases, we    nd that adan achieves
statistically signi   cantly better results.

we further investigate the semi-supervised set-
ting, where a small amount of annotated target
data exists, and show that adan continues to out-
perform the alternatives given the same amount
of target supervision (  3.3.1). we provide
an analysis and visualization of adan (  3.3.2),
shedding light on how our approach manages to
achieve its strong cross-lingual performance. ad-
ditionally, we study the bilingual resource that
adan depends on, the bilingual word embed-
dings, and demonstrate that adan   s performance
is robust with respect to the choice of bwes. fur-
thermore, even without the pre-trained bwes (i.e.
using random initialized embeddings), adan out-
performs all but the state-of-the-art mt-based and
distillation systems (  3.3.3). this makes adan
the    rst clsc model that outperforms bwe-
based baseline systems without relying on any
bilingual resources.

a    nal methodological contribution distin-
guishes adan from previous adversarial networks
for text classi   cation (ganin et al., 2016): adan
minimizes the wasserstein distance (arjovsky
et al., 2017) between the feature distributions of
source and target (  2.2), which yields better
performance and smoother training than the stan-
dard grl training method (ganin et al., 2016, see
  3.3.5).

2 the adan model

the central hypothesis of adan is that an ideal
model for clsc should learn features that both
perform well on sentiment classi   cation for the
source, and are invariant with respect to the shift
in language. therefore, as shown in figure 1,
adan has a joint feature extractor f which aims
to learn features that aid prediction of the senti-
ment classi   er p, and hamper the language dis-
criminator q, whose goal is to identify whether an
input text is from source or target. the intu-
ition is that if a well-trained q cannot tell the lan-
guage of a given input using the features extracted
by f,
those features are effectively language-
invariant. q is hence adversarial since it does its
best to identify language from learned features, yet
good performance from q indicates that adan is
not successful in learning language-invariant fea-
tures. upon successful adan training, f should
have learned features discriminative for sentiment

figure 1: adan with chinese as the target language.
the lines illustrate the training    ows and the arrows
indicate forward and/or backward passes. blue lines
show the    ow for english samples while yellow ones
are for chinese. jp and jq are the training objectives of
p and q, respectively (  2.2). the parameters of f, p
and the embeddings are updated together (solid lines).
the parameters of q are updated using a separate opti-
mizer (dotted lines) due to its adversarial objective.

classi   cation, and at the same time providing no
information for the adversarial q to guess the lan-
guage of a given input.

as seen in figure 1, adan is exposed to both
source and target texts during training. un-
labeled source (blue lines) and target (yellow
lines) data go through the language discriminator,
while only the labeled source data pass through
the sentiment classi   er3. the feature extractor and
the sentiment classi   er are then used for target
texts at test time.
in this manner, we can train
adan with labeled source data and only unla-
beled target text. when some labeled target
data exist, adan could naturally be extended to
take advantage of that for improved performance
(  3.3.1).

3   unlabeled    and    labeled    refer to sentiment labels; all

texts are assumed to have the correct language label.

joint feature extractor fthe movie was great.(cid:6959)(cid:7166)(cid:3664)(cid:1747)(cid:2953)(cid:2437)(cid:822)(bilingual) id27sen textsch textssentiment classi   er planguage discriminator qsentiment labeljplanguage scoreench+   -   jq   jq2.1 network architecture

as illustrated in figure 1, adan is a feed-forward
network with two branches. there are three main
components in the network, a joint feature extrac-
tor f that maps an input sequence x to the shared
feature space, a sentiment classi   er p that pre-
dicts the label for x given the feature representa-
tion f(x), and a language discriminator q that
also takes f(x) but predicts a scalar score indicat-
ing whether x is from source or target.
an input document is modeled as a sequence of
words x = w1, . . . , wn, where each w is repre-
sented by its id27 vw (turian et al.,
2010). for improved performance, pre-trained
bilingual id27s (bwes, zou et al.,
2013; gouws et al., 2015) can be employed to
induce bilingual distributed word representations
so that similar words are closer in the embedded
space regardless of language.

a parallel corpus is often required to train high-
quality bwes, making adan implicitly dependent
on the bilingual corpus. however, compared to the
mt systems used in other clsc methods, train-
ing bwes only requires one to two orders of mag-
nitude less parallel data, and some methods only
take minutes to train on a consumer cpu (gouws
et al., 2015), while state-of-the-art mt systems
need days to weeks for training on multiple gpus.
moreover, even with randomly initialized embed-
dings, adan can still outperform some baseline
methods that use pre-trained bwes (  3.3.3). an-
other possibility is to take advantage of the recent
work that trains bwes with no bilingual supervi-
sion (lample et al., 2018).

we adopt the deep averaging network (dan)
by iyyer et al. (2015) for the feature extractor
f. we choose dan for its simplicity to illustrate
the effectiveness of our language-adversarial train-
ing framework, but other architectures can also
be used for the feature extractor (  3.3.4). for
each document, dan takes the arithmetic mean of
the word vectors as input, and passes it through
several fully-connected layers until a softmax for
classi   cation. in adan, f    rst calculates the av-
erage of the word vectors in the input sequence,
then passes the average through a feed-forward
network with relu nonlinearities. the activa-
tions of the last layer in f are considered the ex-
tracted features for the input and are then passed
on to p and q. the sentiment classi   er p and
the language discriminator q are standard feed-

forward networks. p has a softmax layer on top
for sentiment classi   cation and q ends with a lin-
ear layer of output width 1 to assign a language
identi   cation score4.

2.2 adversarial training
for clarity, we    rst introduce an adan variant
where training is done using gradient reversal
layer (ganin et al., 2016), which is denoted as
adan-grl. adan-grl employs standard ad-
versarial training techniques in previous litera-
ture (ganin et al., 2016), but as we will detail later
in this section, the training of adan-grl is less
stable and the performance is worse compared to
our adan model (see   3.3.5 for empirical results).
speci   cally, in adan-grl, q is a binary clas-
si   er with a sigmoid layer on top so that the lan-
guage identi   cation score is always between 0 and
1 and is interpreted as the id203 of whether
an input text x is from source or target given
its hidden features f(x). for training, q is con-
nected to f via a gradient reversal layer (ganin
and lempitsky, 2015), which preserves the input
during the a forward pass but multiplies the gradi-
ents by       during a backward pass.    is a hyper-
parameter that balances the effects that p and q
have on f respectively. this way, the entire net-
work can be trained in its entirety using standard
id26.

unfortunately, researchers have found that the
training of f and q in adan-grl might not be
fully in sync (ganin and lempitsky, 2015), and ef-
forts need to be made to coordinate the adversarial
training. this is achieved by setting    to a non-
zero value only once out of k batches as in practice
we observe that f trains faster than q. here, k is
another hyperparameter that coordinates the train-
ing of f and q. when    = 0, the gradients from
q will not be back-propagated to f. this allows
q more iterations to adapt to f before f makes
another adversarial update.
to illustrate the limitations of adan-grl and
motivate the formal introduction of our adan
model, consider the distribution of the joint hid-
den features f for both source and target in-
stances:

p srcf (cid:44) p (f(x)|x     source)
p tgtf (cid:44) p (f(x)|x     target)

4q simply tries to maximize scores for source texts and

minimize for target, and the scores are not bounded.

algorithm 1 adan training
require: labeled source corpus xsrc; unlabeled
target corpus xtgt; hyperpamameter    > 0,
k     n, c > 0.

1: repeat
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:

(cid:46) q iterations
for qiter = 1 to k do

sample unlabeled batch xsrc     xsrc
sample unlabeled batch xtgt     xtgt
fsrc = f(xsrc)
ftgt = f(xtgt)
lossq =    q(fsrc) + q(ftgt)
update q parameters to minimize lossq
clipw eights(q,   c, c)

(cid:46) feature vectors
(cid:46) eqn (2)

(cid:46) main iteration
sample labeled batch (xsrc, ysrc)     xsrc
sample unlabeled batch xtgt     xtgt
fsrc = f(xsrc)
ftgt = f(xtgt)
loss = lp(p(fsrc); ysrc) +   (q(fsrc)    
(cid:46) eqn (4)
update f, p parameters to minimize loss

q(ftgt))

17:
18: until convergence

in order to learn language-invariant features,
adan trains f to make these two distributions
as close as possible for better cross-lingual gen-
eralization. in particular, as argued by arjovsky
et al. (2017), previous approaches to training ad-
versarial networks such as adan-grl are equiva-
lent to minimizing the jensen-shannon divergence
between two distributions, in our case p srcf and
p tgtf . and because the jensen-shannon diver-
gence suffers from discontinuities, providing less
useful gradients for training f, arjovsky et al.
(2017) propose instead to minimize the wasser-
stein distance and demonstrate its improved sta-
bility for hyperparameter selection.

as a result, departing from the previous adan-
grl training method, in our adan model, we
minimize the wasserstein distance w between
according to the kantorovich-
p srcf
rubinstein duality (villani, 2008):

and p tgtf

w (p srcf , p tgtf ) =
sup
f (x)   p srcf
(cid:107)g(cid:107)l   1

e

[g(f (x))]    

e

f (x(cid:48))   p tgtf

(cid:2)g(f (x

(1)

(cid:48)))(cid:3)

where the supremum (maximum) is taken over the
set of all 1-lipschitz5 functions g. in order to (ap-
proximately) calculate w (p srcf , p tgtf ), we use the
language discriminator q as the function g in (1),
5a function g is 1-lipschitz iff |g(x)     g(y)|     |x     y|
for all x and y.

(cid:48)))(cid:3)

  q

e

e

(cid:2)

f (x)   p srcf

f (x(cid:48))   p tgtf

[q(f(x))]    

whose objective is then to seek the supremum in
(1). to make q a lipschitz function (up to a con-
stant), the parameters of q are always clipped to a
   xed range [   c, c]. let q be parameterized by   q,
then the objective jq of q becomes:
jq(  f )    
max

(2)
q(f(x
intuitively, q tries to output higher scores for
source instances and lower scores for target.
more formally, jq is an approximation of the
wasserstein distance between p srcf and p tgtf in (1).
for the sentiment classi   er p parameterized by
  p, we use the traditional cross-id178 loss, de-
noted as lp(  y, y), where   y and y are the predicted
label distribution and the true label, respectively.
lp is the negative log-likelihood that p predicts
the correct label. we therefore seek the minimum
of the following id168 for p:

jp(  f )     min

  p

[lp(p(f(x)), y)]

e
(x,y)

(3)

finally, the joint feature extractor f parameter-
ized by   f strives to minimize both the sentiment
classi   er loss jp and w (p srcf , p tgtf ) = jq:

jf     min

  f

jp(  f ) +   jq(  f )

(4)

where    is a hyper-parameter that balances the two
branches p and q.
as proved by arjovsky et al. (2017) and ob-
served in our experiments (  3.3.5), minimizing the
wasserstein distance is much more stable w.r.t. hy-
perparameter selection compared to adan-grl,
saving the hassle of carefully varying    during
training (ganin and lempitsky, 2015).
in addi-
tion, adan-grl needs to laboriously coordinate
the alternating training of the two competing com-
ponents by setting the hyperparameter k, which in-
dicates the number of iterations one component is
trained before training the other. the performance
can degrade substantially if k is not properly set.
in our case, however, delicate tuning of k is no
longer necessary since w (p srcf , p tgtf ) is approxi-
mated by maximizing (2); thus, training q to op-
timum using a large k can provide better perfor-
mance (but is slower to train). in our experiments,
we    x    = 0.1 and k = 5 for all experiments
(train 5 q iterations per f and p iteration), and
the performance is stable over a large set of hyper-
parameters (  3.3.5).

adan training is depicted in algorithm 1.

methodology

approach

train-on-source-only id28

dan

id20

machine translation

cld-based cltc

msda (chen et al., 2012)
id28 + mt
dan + mt
cld-kid98 (xu and yang, 2017)
cldfa-kid98 (xu and yang, 2017)

accuracy

chinese
30.58%
29.11%
31.44%
34.01%
39.66%
40.96%
41.82%

arabic
45.83%
48.00%
48.33%
51.67%
52.50%
52.67%   
53.83%   

42.49%  0.19% 54.54%  0.34%
ours
    as xu and yang (2017) did not report results for arabic, these numbers are obtained based on our
reproduction using their code.

adan

table 1: adan performance for chinese (5-cls) and arabic (3-cls) sentiment classi   cation without using labeled
target data. all systems but the cld ones use bwe to map source and target words into the same space.
cld-based cltc represents cross-lingual text classi   cation methods based on cross-lingual distillation (xu and
yang, 2017) and is explained in   3.2. for adan, average accuracy and standard errors over    ve runs are shown.
bold numbers indicate statistical signi   cance over all baseline systems with p < 0.05 under a one-sample t-test.
as a comparison, the supervised english accuracy of our adan model is 58.7% (5-class) and 75.6% (3-class).

3 experiments and discussions

to demonstrate the effectiveness of our model, we
experiment on chinese and arabic sentiment clas-
si   cation, using english as source for both. for
all data used in experiments, id121 is done
using stanford corenlp (manning et al., 2014).

3.1 data
labeled english data. we use a balanced dataset
of 700k yelp reviews from zhang et al. (2015)
with their ratings as labels (scale 1-5). we also
adopt their train-validation split: 650k reviews for
training and 50k form a validation set.
labeled chinese data. since adan does not re-
quire labeled chinese data for training, this an-
notated data is solely used to validate the perfor-
mance of our model. 10k balanced chinese hotel
reviews from lin et al. (2015) are used as valida-
tion set for model selection and parameter tuning.
the results are reported on a separate test set of
another 10k hotel reviews. for chinese, the data
are annotated with 5 labels (1-5).
unlabeled chinese data.
for the unlabeled
target data used in training adan, we use an-
other 150k unlabeled chinese hotel reviews.
english-chinese bilingual id27s.
for chinese, we used the pre-trained bilingual
id27s (bwe) by zou et al. (2013).
their work provides 50-dimensional embeddings
for 100k english words and another set of 100k

chinese words. see   3.3.3 for more experiments
and discussions.
labeled arabic data. we use the bbn ara-
bic id31 dataset (mohammad et al.,
2016) for arabic sentiment classi   cation. the
dataset contains 1200 sentences (600 validation +
600 test) from social media posts annotated with
3 labels (   , 0, +). the dataset also provides ma-
chine translated text to english. since the label set
does not match with the english dataset, we map
all the rating 4 and 5 english instances to + and
the rating 1 and 2 instances to    , while the rating
3 sentences are converted to 0.
unlabeled arabic data. for arabic, no addi-
tional unlabeled data is used. we only use the
text from the validation set (without labels) during
training.
english-arabic bilingual word embed-
dings. for arabic, we train a 300d bilbowa
bwe (gouws et al., 2015) on the united nations
corpus (ziemski et al., 2016).

3.2 cross-lingual sentiment classi   cation

our main results are shown in table 1, which
shows very similar trends for chinese and ara-
bic. before delving into discussions on the per-
formance of adan compared to various baseline
systems in the following paragraphs, we begin by
clarifying the bilingual resources used in all the
methods. note    rst that in all of our experiments,

traditional features like bag of words cannot be
directly used since source and target have
completely different vocabularies. therefore, un-
less otherwise speci   ed, bwes are used as the in-
put representation for all systems to map words
from both source and target into the same
feature space. (the only exceptions are the cld-
based cltc systems of xu and yang (2017) ex-
plained later in this section, which directly make
use of a parallel corpus instead of relying on
bwes.) the same bwes are adopted in all sys-
tems that utilize bwes.

train-on-source-only baselines we start by
considering two baselines that train only on the
source language, english, and rely solely on
the bwes to classify the target. the    rst vari-
ation uses a standard supervised learning algo-
rithm, id28 (lr), shown in row
1 in table 1.
in addition, we evaluate a non-
adversarial variation of adan, just the dan por-
tion of our model (row 2), which is one of the
modern neural models for sentiment classi   ca-
tion. we can see from table 1 that, in compari-
son to adan (bottom line), the train-on-source-
only baselines perform poorly. this indicates that
bwes by themselves do not suf   ce to transfer
knowledge of english sentiment classi   cation to
target.

id20 baselines we next com-
pare adan with id20 baselines,
since id20 can be viewed as a gen-
eralization of the cross-lingual task. nonetheless,
the divergence between languages is much more
signi   cant than the divergence between two do-
mains, which are typically two product categories
in practice. among id20 methods,
the widely-used tca (pan et al., 2011) did not
work since it required quadratic space in terms
of the number of samples (650k). we thus com-
pare to msda (chen et al., 2012), a very effective
method for cross-domain sentiment classi   cation
on amazon reviews. however, as shown in ta-
ble 1 (row 3), msda did not perform competi-
tively. we speculate that this is because many do-
main adaptation models including msda were de-
signed for the use of bag-of-words features, which
are ill-suited in our task where the two languages
have completely different vocabularies.
in sum-
mary, this suggests that even strong domain adap-
tation algorithms cannot be used out of the box

with bwes for the clsc task.

machine translation baselines we then evalu-
ate adan against machine translation baselines
(rows 4-5) that (1) translate the target text into
english and then (2) use the better of the train-
on-source-only models for sentiment classi   ca-
tion. previous studies (banea et al., 2008; salameh
et al., 2015) on sentiment classi   cation for arabic
and european languages claim this mt approach
to be very competitive and    nd that it can some-
times match the state-of-the-art system trained on
that language. for chinese, where translated text
was not provided, we use the commercial google
translate engine6, which is highly engineered,
trained on enormous resources, and arguably one
of the best mt systems currently available. as
shown in table 1, our adan model substantially
outperforms the mt baseline on both languages,
indicating that our adversarial model can success-
fully perform cross-lingual sentiment classi   ca-
tion without any annotated data in the target lan-
guage.

cross-lingual text classi   cation baselines fi-
nally, we conclude adan   s effectiveness by
comparing against a state-of-the-art cross-lingual
text classi   cation (cltc) method (xu and yang,
2017), as sentiment classi   cation is one type of
text classi   cation. they propose a cross-lingual
distillation (cld) method that makes use of soft
source predictions on a parallel corpus to train a
target model (cld-kid98). they further pro-
pose an improved variant (cldfa-kid98) that
utilizes adversarial training to bridge the domain
gap between the labeled and unlabeled texts within
the source and the target language, similar to
the adversarial id20 by ganin et al.
(2016). in other words, cldfa-kid98 consists
of three conceptual adaptation steps: (i) domain
adaptation from source-language labeled texts to
source-language unlabeled texts using adversarial
training; (ii) cross-lingual adaptation using dis-
tillation; and (iii) id20 in the tar-
get language from unlabeled texts to the test set.
note, however, xu and yang (2017) use adversar-
ial training for id20 within a single
language vs. our work that uses adversarial train-
ing directly for cross-lingual generalization.

as shown in table 1, adan signi   cantly
outperforms both variants of cld-kid98 and

6https://translate.google.com

adding labeled chinese reviews for training. we
start by adding 100 labeled reviews and keep dou-
bling the number until 12800. as shown in fig-
ure 2, when adding the same number of labeled
reviews, adan can better utilize the extra supervi-
sion and outperform the dan baseline trained with
combined data, as well as the supervised dan us-
ing only labeled chinese reviews. the margin is
naturally decreasing as more supervision is incor-
porated, but adan is still superior when adding
12800 labeled reviews. on the other hand, the
dan with translation baseline seems unable to ef-
fectively utilize the added supervision in chinese,
and the performance only starts to show a slightly
increasing trend when adding 6400 or more la-
beled reviews. one possible reason is that when
adding to the training data a small number of en-
glish reviews translated from the labeled chinese
data, the training signals they produce might be
lost in the vast number of english training sam-
ples, and thus not effective in improving perfor-
mance. another potentially interesting    nd is that
it seems a very small amount of supervision (e.g.
100 labels) could signi   cantly help dan. how-
ever, with the same number of labeled reviews,
adan still outperforms the dan baseline.

3.3.2 qualitative analysis and visualizations
to qualitatively demonstrate how adan bridges
the distributional discrepancies between english
and chinese instances, id167 (van der maaten
and hinton, 2008) visualizations of the activations
at various layers are shown in figure 3. we ran-
domly select 1000 reviews from the chinese and
english validation sets respectively, and plot the
id167 of the hidden node activations at three lo-
cations in our model: the averaging layer, the end
of the joint feature extractor, and the last hidden
layer in the sentiment classi   er just prior to soft-
max. the train-on-english model is the dan base-
line in table 1. note that there is actually only one
   branch    in this baseline model, but in order to
compare to adan, we conceptually treat the    rst
three layers as the feature extractor.

figure 3a shows that bwes alone do not suf   ce
to bridge the gap between the distributions of the
two languages. to shed more light on the surpris-
ingly clear separation given that individual words
have a mixed distribution in both languages (not
shown in    gure), we    rst try to isolate the content
divergence from the language divergence. in par-
ticular, the english and chinese reviews are not

figure 2: adan performance and standard deviation
for chinese in the semi-supervised setting when using
various amount of labeled chinese data.

achieves a new state of the art performance, in-
dicating that our direct use of adversarial neural
nets for cross-lingual adaptation can be more ef-
fective than chaining three adaptation steps as in
cldfa-kid98. this is the case in spite of the
fact that adan does not explicitly separate lan-
guage variation from domain variation.
in fact,
the monolingual data we use for the source and
target languages is indeed from different domains.
adan   s performance suggests that it could poten-
tially bridge the divergence introduced by both
sources of variation in one shot.
supervised source accuracy by way of com-
parison, it is also instructive to compare adan   s
   transferred    accuracy on the target with
its (supervised) performance on the source.
as shown in the caption of table 1, adan
achieves 58.7% accuracy on english for the 5-
class english-chinese setting, and 75.6% for the
3-class english-arabic setting. the source ac-
curacy for the dan baselines (rows 2 and 5) is
similar to the source accuracy of adan.

3.3 analysis and discussion
since the arabic dataset is small, we choose chi-
nese as an example for our further analysis.
3.3.1 semi-supervised learning
in practice, it is usually not very dif   cult to obtain
at least a small amount of annotated data. adan
can be readily adapted to exploit such extra la-
beled data in the target language, by letting those
labeled instances pass through the sentiment clas-
si   er p as the english samples do during train-
ing. we simulate this semi-supervised scenario by

102103104number of labeled chinese reviews363840424446485052accuracyunsuperviseddandan + mtadanchn onlyfigure 3:
id167 visualizations of activations at various layers for the train-on-source-only baseline model
(top) and adan (bottom). the distributions of the two languages are brought much closer in adan as they are
represented deeper in the network (left to right) measured by the averaged hausdorff distance (see text). the
green circles are two 5-star example reviews (shown below the    gure) that illustrate how the distribution evolves
(zoom in for details).

translations of each other, and in fact may even
come from different domains. therefore, the sep-
aration could potentially come from two sources:
the content divergence between the english and
chinese reviews, and the language divergence of
how words are used in the two languages. to con-
trol for content divergence, we tried plotting (not
shown in    gure) the average id27s of
1000 random chinese reviews and their machine
translations into english using id167, and surpris-
ingly the clear separation was still present. there
are a few relatively short reviews that reside close
to their translations, but the majority still form two
language islands. (the same trend persists when
we switch to a different set of pre-trained bwes,
and when we plot a similar graph for english-
arabic.) when we remove stop words (the most
frequent word types in both languages), the two
islands    nally start to become slightly closer with
less clean boundaries, but the separation remains

clear. we think this phenomenon is interesting,
and a thorough investigation is out of the scope of
this work. we hypothesize that at least in certain
distant language pairs such as english-chinese7,
the divergence between languages may not only
be determined by word semantics, but also largely
depends on how words are used.

furthermore, we can see in figure 3b that
the distributional discrepancies between chinese
and english are signi   cantly reduced after pass-
ing through the joint feature extractor (f). the
learned features in adan bring the distribu-
tions in the two languages dramatically closer
compared to the monolingually trained baseline.
this is shown via the averaged hausdorff dis-
tance (ahd, shapiro and blaschko, 2004), which
measures the distance between two sets of points.

7in a personal correspondence with ahmed elgohary, he
did not observe the same phenomenon between english and
french.

(a) averaging layer outputs(b) joint hidden features(c) sentiment branch outputs(cid:20)(cid:17)(cid:3)train on english2. adani have been here twice and both times have been great. they really have a nice service sta    & very attentive!food is pretty good as well! they seem to be always busy but super glad you are there with them. well done!(cid:2953)(cid:2437)(cid:4913)(cid:7116)(cid:2862),(cid:4670)(cid:2361)(cid:1405)(cid:5606),(cid:3664)(cid:1747)(cid:1705)(cid:1430),(cid:1253)(cid:3962)(cid:1653)(cid:2387)(cid:1626)(cid:1860)(cid:4913),(cid:1966)(cid:2225)(cid:1246)(cid:1604)(cid:7166)(cid:2392)(cid:3661)(cid:1904)(cid:4913)(cid:3)(cid:11)(cid:55)(cid:85)(cid:68)(cid:81)(cid:86)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:29)(cid:3)(cid:57)(cid:72)(cid:85)(cid:92)(cid:3)(cid:74)(cid:82)(cid:82)(cid:71)(cid:3)(cid:75)(cid:82)(cid:87)(cid:72)(cid:79)(cid:17)(cid:54)(cid:88)(cid:85)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:86)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:69)(cid:72)(cid:68)(cid:88)(cid:87)(cid:76)(cid:73)(cid:88)(cid:79)(cid:15)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:86)(cid:72)(cid:85)(cid:89)(cid:76)(cid:70)(cid:72)(cid:3)(cid:76)(cid:86)(cid:3)(cid:89)(cid:72)(cid:85)(cid:92)(cid:3)(cid:74)(cid:82)(cid:82)(cid:71)(cid:30)(cid:3)(cid:90)(cid:76)(cid:79)(cid:79)(cid:3)(cid:86)(cid:87)(cid:68)(cid:92)(cid:3)(cid:68)(cid:74)(cid:68)(cid:76)(cid:81)(cid:17)(cid:3)(cid:55)(cid:75)(cid:72)(cid:85)(cid:72)(cid:3)(cid:76)(cid:86)(cid:3)(cid:73)(cid:82)(cid:82)(cid:71)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:76)(cid:81)(cid:3)(cid:20)(cid:78)(cid:80)(cid:17)(cid:12)(cid:11)(cid:68)(cid:12)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:68)(cid:89)(cid:72)(cid:85)(cid:68)(cid:74)(cid:72)(cid:71)(cid:3)(cid:37)(cid:58)(cid:40)(cid:3)(cid:68)(cid:79)(cid:82)(cid:81)(cid:72)(cid:3)(cid:76)(cid:86)(cid:3)(cid:81)(cid:82)(cid:87)(cid:3)(cid:86)(cid:88)(cid:395)(cid:70)(cid:76)(cid:72)(cid:81)(cid:87)(cid:3)(cid:87)(cid:82)(cid:3)(cid:69)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:79)(cid:68)(cid:81)(cid:74)(cid:88)(cid:68)(cid:74)(cid:72)(cid:3)(cid:74)(cid:68)(cid:83)(cid:17)(cid:11)(cid:69)(cid:12)(cid:3)(cid:36)(cid:87)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:72)(cid:81)(cid:71)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:77)(cid:82)(cid:76)(cid:81)(cid:87)(cid:3)(cid:73)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)(cid:3)(cid:72)(cid:91)(cid:87)(cid:85)(cid:68)(cid:70)(cid:87)(cid:82)(cid:85)(cid:15)(cid:3)(cid:36)(cid:39)(cid:36)(cid:49)(cid:3)(cid:74)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:87)(cid:72)(cid:86)(cid:3)(cid:68)(cid:3)(cid:80)(cid:82)(cid:85)(cid:72)(cid:3)(cid:80)(cid:76)(cid:91)(cid:72)(cid:71)(cid:3)(cid:71)(cid:76)(cid:86)(cid:87)(cid:85)(cid:76)(cid:69)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:69)(cid:72)(cid:87)(cid:90)(cid:72)(cid:72)(cid:81)(cid:3)(cid:79)(cid:68)(cid:81)(cid:74)(cid:88)(cid:68)(cid:74)(cid:72)(cid:86)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:68)(cid:85)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:69)(cid:68)(cid:86)(cid:72)(cid:79)(cid:76)(cid:81)(cid:72)(cid:17)avg hausdor    dist = 0.24avg hausdor    dist = 0.98avg hausdor    dist = 0.25avg hausdor    dist = 0.24avg hausdor    dist = 0.22avg hausdor    dist = 0.08(cid:11)(cid:70)(cid:12)(cid:3)(cid:44)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:79)(cid:68)(cid:86)(cid:87)(cid:3)(cid:75)(cid:76)(cid:71)(cid:71)(cid:72)(cid:81)(cid:3)(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:3)(cid:76)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:87)(cid:72)(cid:91)(cid:87)(cid:3)(cid:70)(cid:79)(cid:68)(cid:86)(cid:86)(cid:76)(cid:393)(cid:72)(cid:85)(cid:3)(cid:69)(cid:72)(cid:73)(cid:82)(cid:85)(cid:72)(cid:3)(cid:86)(cid:82)(cid:73)(cid:87)(cid:80)(cid:68)(cid:91)(cid:15)(cid:3)(cid:40)(cid:81)(cid:74)(cid:79)(cid:76)(cid:86)(cid:75)(cid:3)(cid:85)(cid:72)(cid:89)(cid:76)(cid:72)(cid:90)(cid:86)(cid:3)(cid:73)(cid:82)(cid:85)(cid:80)(cid:3)(cid:70)(cid:79)(cid:88)(cid:86)(cid:87)(cid:72)(cid:85)(cid:86)(cid:3)(cid:69)(cid:68)(cid:86)(cid:72)(cid:71)(cid:3)(cid:82)(cid:81)(cid:3)(cid:79)(cid:68)(cid:69)(cid:72)(cid:79)(cid:86)(cid:3)(cid:11)(cid:76)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:83)(cid:72)(cid:85)(cid:76)(cid:80)(cid:72)(cid:87)(cid:72)(cid:85)(cid:12)(cid:3)(cid:76)(cid:81)(cid:3)(cid:69)(cid:82)(cid:87)(cid:75)(cid:3)(cid:86)(cid:92)(cid:86)(cid:87)(cid:72)(cid:80)(cid:86)(cid:17)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:38)(cid:75)(cid:76)(cid:81)(cid:72)(cid:86)(cid:72)(cid:3)(cid:85)(cid:72)(cid:89)(cid:76)(cid:72)(cid:90)(cid:86)(cid:3)(cid:76)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:69)(cid:68)(cid:86)(cid:72)(cid:79)(cid:76)(cid:81)(cid:72)(cid:3)(cid:71)(cid:82)(cid:3)(cid:81)(cid:82)(cid:87)(cid:3)(cid:68)(cid:79)(cid:76)(cid:74)(cid:81)(cid:3)(cid:90)(cid:72)(cid:79)(cid:79)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:40)(cid:81)(cid:74)(cid:79)(cid:76)(cid:86)(cid:75)(cid:3)(cid:70)(cid:79)(cid:88)(cid:86)(cid:87)(cid:72)(cid:85)(cid:86)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:87)(cid:75)(cid:76)(cid:86)(cid:3)(cid:83)(cid:85)(cid:82)(cid:69)(cid:79)(cid:72)(cid:80)(cid:3)(cid:76)(cid:86)(cid:3)(cid:68)(cid:79)(cid:79)(cid:72)(cid:89)(cid:76)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3)(cid:76)(cid:81)(cid:3)(cid:36)(cid:39)(cid:36)(cid:49)(cid:17)model

dan
dan+mt
adan

random bilbowa zou et al.
29.11%
21.66%
39.66%
37.78%
34.44%
42.95%

28.75%
38.17%
40.51%

table 2: model performance on chinese with various
(b)we initializations.

the ahd between the english and chinese re-
views is provided for all sub-plots in figure 3.

finally, when looking at the last hidden layer
activations in the sentiment classi   er of the base-
line model (figure 3c), there are several notable
clusters of red dots (english data) that roughly cor-
respond to the class labels. these english clusters
are the areas where the classi   er is the most con-
   dent in making decisions. however, most chi-
nese samples are not close to one of those clus-
ters due to the distributional divergence and may
thus cause degraded classi   cation performance in
chinese. on the other hand, the chinese samples
are more in line with the english ones in adan,
which results in the accuracy boost over the base-
line model. in figure 3, a pair of similar english
and chinese 5-star reviews is highlighted to visu-
alize how the distribution evolves at various points
of the network. we can see in 3c that the high-
lighted chinese review gets close to the    positive
english cluster    in adan, while in the baseline, it
stays away from dense english clusters where the
sentiment classi   er trained on english data is not
con   dent to make predictions.

impact of bilingual id27s
3.3.3
in this section we discuss the effect of the bilin-
gual id27s. we start by initializ-
ing the systems with random id27s
(wes), shown in table 2. adan with random
wes outperforms the dan and msda baselines
using bwes and matches the performance of the
lr+mt baseline (table 1), suggesting that adan
successfully extracts features that could be used
for cross-lingual classi   cation tasks without any
bitext. this impressive result vindicates the power
of adversarial training to reduce the distance be-
tween two complex distributions without any di-
rect supervision, which is also observed in other
recent works for different tasks (zhang et al.,
2017; lample et al., 2018).

with the introduction of bwes (column 2 and
3), the performance of adan is further boosted.

model

dan
id98
bilstm
bilstm + dot attn

run time

accuracy
42.95% 0.127 (s/iter)
46.24% 0.554 (s/iter)
44.55% 1.292 (s/iter)
46.41% 1.898 (s/iter)

table 3: performance and speed for various feature ex-
tractor architectures on chinese.

therefore, it seems the quality of the bwes plays
an important role in clsc. to investigate the
impact of the speci   c choice of bwes, we also
trained 100d bilbowa bwes (gouws et al.,
2015) using the un parallel corpus for chinese.
all systems achieve slightly lower performance
compared to the pre-trained bwes from zou et al.
(2013), yet adan still outperforms other baseline
methods (table 2), demonstrating that adan   s ef-
fectiveness is relatively robust with respect to the
choice of bwes. we conjecture that all systems
show inferior results with bilbowa, because it
does not require word alignments during training
as zou et al. (2013) do. by only training on a
sentence-aligned corpus, bilbowa requires less
resources and is much faster to train, potentially at
the expense of quality.

3.3.4 feature extractor architectures
as mentioned in   2.1, the architecture of adan   s
feature extractor is not limited to a deep averag-
ing network (dan), and one can choose differ-
ent feature extractors to suit a particular task or
dataset. while an extensive study of alternative ar-
chitectures is beyond the scope of this work, we in
this section present a brief experiment illustrating
that our adversarial framework works well with
other f architectures. in particular, we consider
two popular choices: i) a id98 (kim, 2014) that
has a 1d convolutional layer followed by a single
fully-connected layer to extract a    xed-length vec-
tor; and ii) a bi-lstm with two variants: one that
takes the average of the hidden outputs of each to-
ken as the feature vector, and one with the dot at-
tention mechanism (luong et al., 2015) that learns
a weighted linear combination of all hidden out-
puts.

as shown in table 3, adan   s performance can
be improved by adopting more sophisticated fea-
ture extractors, at the expense of slower running
time. this demonstrates that adan   s language-
adversarial training framework can be successfully

4 related work

cross-lingual sentiment classi   cation is moti-
vated by the lack of high-quality labeled data in
many non-english languages (bel et al., 2003; mi-
halcea et al., 2007; banea et al., 2008, 2010; soyer
et al., 2015). for chinese and arabic in particular,
there are several representative works (wan, 2008,
2009; he et al., 2010; lu et al., 2011; mohammad
et al., 2016). our work is comparable to these in
objective but very different in method. the work
by wan uses machine translation to directly con-
vert english training data to chinese; this is one
of our baselines. lu et al. (2011) instead uses
labeled data from both languages to improve the
performance on both. other papers make direct
use of a parallel corpus either to learn a bilingual
id194 (zhou et al., 2016) or to
conduct cross-lingual distillation (xu and yang,
2017). zhou et al. (2016) require the translation of
the entire english training set which is prohibitive
for our setting, while adan outperforms xu and
yang (2017)   s approach in our experiments.
id20 tries to learn effective clas-
si   ers for which the training and test samples
are from different underlying distributions (blitzer
et al., 2007; pan et al., 2011; glorot et al., 2011;
chen et al., 2012; liu et al., 2015). this can
be thought of as a generalization of cross-lingual
text classi   cation. however, one main difference
is that, when applied to text classi   cation tasks,
most of these id20 work assumes
a common feature space such as a bag-of-words
representation, which is not available in the cross-
lingual setting. see section 3.2 for experiments
on this. in addition, most works in domain adap-
tation evaluate on adapting product reviews across
domains (e.g. books to electronics), where the di-
vergence in distribution is less signi   cant than that
between two languages.
adversarial networks have enjoyed much suc-
cess in id161 (goodfellow et al., 2014;
ganin et al., 2016). a series of work in image gen-
eration has used architectures similar to ours, by
pitting a neural image generator against a discrim-
inator that learns to classify real versus generated
images (goodfellow et al., 2014). more relevant
to this work, adversarial architectures have pro-
duced the state-of-the-art in unsupervised domain
adaptation for image object recognition: ganin
et al. (2016) train with many labeled source im-
ages and unlabeled target images, similar to our

figure 4: a grid search on k and lambda for adan
(right) and the adan-grl variant (left). numbers in-
dicate the accuracy on the chinese development set.

used with other f choices.
3.3.5 adan hyperparameter stability
in this section, we show that the training of adan
is stable over a large set of hyperparameters, and
provides improved performance compared to the
standard adan-grl.

to verify the superiority of adan, we conduct
a grid search over k and   , which are the two hy-
perparameters shared by adan and adan-grl.
we experiment with k     {1, 2, 4, 8, 16}, and       
{0.00625, 0.0125, 0.025, 0.05, 0.1, 0.2, 0.4, 0.8}.
figure 4 reports the accuracy on the chinese dev
set for both adan variants, and shows higher
accuracy and greater stability over the ganin and
lempitsky (2015) variant. this suggests that
adan overcomes the well-known problem that
adversarial training is sensitive to hyperparameter
tuning.

implementation details

3.4
for all our experiments on both languages, the fea-
ture extractor f has three fully-connected layers
with relu non-linearities, while both p and q
have two. all hidden layers contain 900 hidden
units. batch id172 (ioffe and szegedy,
2015) is used in each hidden layer in p and q.
f does not use batch id172. f and p
are optimized jointly using adam (kingma and
ba, 2015) with a learning rate of 0.0005. q
is trained with another adam optimizer with the
same learning rate. the weights of q are clipped
to [   0.01, 0.01]. we train adan for 30 epochs
and use early stopping to select the best model
on the validation set. adan is implemented in
pytorch (paszke et al., 2017).

klambdalambdaadan without wasserstein distanceadansetup.
in addition, other recent work (arjovsky
et al., 2017; gulrajani et al., 2017) proposes im-
proved methods for training generative adversar-
ial nets. adan proposes language-adversarial
training, the    rst adversarial neural net for cross-
lingual nlp (chen et al., 2016). as of the writing
of this journal paper, there are several other recent
works that adopt adversarial training for cross-
lingual nlp tasks, such as cross-lingual text clas-
si   cation (xu and yang, 2017), cross-lingual word
embedding induction (zhang et al., 2017; lample
et al., 2018) and cross-lingual question similarity
reranking (joty et al., 2017).

5 conclusion and future work

in this work, we presented adan, an adversar-
ial deep averaging network for cross-lingual senti-
ment classi   cation. adan leverages the abundant
labeled resources from english to help sentiment
classi   cation on other languages where little or no
annotated data exist. we validate adan   s effec-
tiveness by experiments on chinese and arabic
sentiment classi   cation, where we have labeled
english data and only unlabeled data in the target
language. experiments show that adan outper-
forms several baselines including domain adapta-
tion models, a competitive mt baseline, and state-
of-the-art cross-lingual text classi   cation meth-
ods. we further show that even without any bilin-
gual resources, adan trained with randomly ini-
tialized embeddings can still achieve encouraging
performance.
in addition, we show that in the
presence of labeled data in the target language,
adan can naturally incorporate this additional su-
pervision and yields even more competitive re-
sults.

for future work, we plan to apply our language-
adversarial training framework to other nlp adap-
tation tasks where explicit id113 training is not fea-
sible due to the lack of direct supervision. our
framework is not limited to sentiment classi   ca-
tion or even to generic text classi   cation: it can
be applied, for example, to phrase-level tagging
tasks (  irsoy and cardie, 2014) where labeled data
might not exist for certain languages. in another
direction, we can look beyond a single source
and target language and utilize our adversarial
training framework for multi-lingual text classi   -
cation.

acknowledgments

we thank the anonymous reviewers and members
of cornell nlp and ml groups for helpful com-
ments. this work was funded in part by a grant
from the darpa deft program.

references

martin arjovsky, soumith chintala, and l  on
bottou. 2017. wasserstein generative adversar-
ial networks. in proceedings of the 34th inter-
national conference on machine learning, vol-
ume 70, pages 214   223.

carmen banea, rada mihalcea, and janyce
wiebe. 2010. multilingual subjectivity: are
in proceedings of
more languages better?
the 23rd international conference on compu-
tational linguistics (coling 2010), pages 28   
36.

carmen banea, rada mihalcea, janyce wiebe,
and samer hassan. 2008. multilingual sub-
jectivity analysis using machine translation. in
proceedings of the 2008 conference on empir-
ical methods in natural language processing,
pages 127   135.

nuria bel, cornelis h. a. koster, and marta ville-
gas. 2003. cross-lingual text categorization. in
research and advanced technology for digital
libraries, pages 126   139, berlin, heidelberg.

john blitzer, mark dredze, and fernando pereira.
2007. biographies, bollywood, boom-boxes
and blenders: id20 for sentiment
in proceedings of the 45th an-
classi   cation.
nual meeting of the association of computa-
tional linguistics, pages 440   447.

minmin chen, zhixiang xu, kilian weinberger,
and fei sha. 2012. marginalized denoising au-
toencoders for id20. in proceed-
ings of the 29th international conference on
machine learning (icml-12), pages 767   774,
new york, ny, usa.

xilun chen, yu sun, ben athiwaratkun, claire
cardie, and kilian weinberger. 2016. ad-
versarial deep averaging networks for cross-
lingual sentiment classi   cation. arxiv e-prints
1606.01614v4.

yaroslav ganin and victor lempitsky. 2015. un-
supervised id20 by backpropaga-
tion. in proceedings of the 32nd international
conference on machine learning.

yaroslav ganin, evgeniya ustinova, hana ajakan,
pascal germain, hugo larochelle, fran  ois
laviolette, mario marchand, and victor lem-
pitsky. 2016. domain-adversarial training of
neural networks. journal of machine learning
research, 17(59):1   35.

id20 for

xavier glorot, antoine bordes, and yoshua ben-
gio. 2011.
large-
scale sentiment classi   cation: a deep learn-
in proceedings of the 28th in-
ing approach.
ternational conference on machine learning
(icml-11), pages 513   520, new york, ny,
usa.

ian goodfellow,

jean pouget-abadie, mehdi
mirza, bing xu, david warde-farley, sherjil
ozair, aaron courville, and yoshua bengio.
2014. generative adversarial nets. in advances
in neural information processing systems 27,
pages 2672   2680.

stephan gouws, yoshua bengio, and greg cor-
rado. 2015.
bilbowa: fast bilingual dis-
tributed representations without word align-
in proceedings of the 32nd interna-
ments.
tional conference on machine learning.

ishaan gulrajani, faruk ahmed, martin arjovsky,
vincent dumoulin, and aaron courville. 2017.
improved training of wasserstein gans. in ad-
vances in neural information processing sys-
tems 30, pages 5767   5777.

yulan he, harith alani, and deyu zhou. 2010.
exploring english lexicon knowledge for chi-
in cips-sighan
nese id31.
joint conference on chinese language pro-
cessing.

sergey ioffe and christian szegedy. 2015. batch
id172: accelerating deep network
training by reducing internal covariate shift. in
proceedings of the 32nd international confer-
ence on machine learning.

ozan   irsoy and claire cardie. 2014. opinion
mining with deep recurrent neural networks.

in proceedings of the conference on empiri-
cal methods in natural language processing,
pages 720   728.

mohit iyyer, varun manjunatha, jordan boyd-
graber, and hal daum   iii. 2015. deep un-
ordered composition rivals syntactic methods
in proceedings of the
for text classi   cation.
53rd annual meeting of the association for
computational linguistics and the 7th interna-
tional joint conference on natural language
processing (volume 1: long papers), pages
1681   1691.

sha   q joty, preslav nakov, llu  s m  rquez, and
israa jaradat. 2017. cross-language learning
with adversarial neural networks: application
in pro-
to community id53.
ceedings of the 21st conference on compu-
tational natural language learning (conll
2017), pages 226   237, vancouver, canada.

yoon kim. 2014. convolutional neural networks
in proceedings of
for sentence classi   cation.
the 2014 conference on empirical methods in
natural language processing (emnlp), pages
1746   1751.

diederik kingma and jimmy ba. 2015. adam: a
method for stochastic optimization. in interna-
tional conference on learning representations.

guillaume

alexis

lample,

conneau,
marc   aurelio ranzato, ludovic denoyer,
and herv   j  gou. 2018. word translation with-
in international conference
out parallel data.
on learning representations.

yiou lin, hang lei, jia wu, and xiaoyu li. 2015.
an empirical study on sentiment classi   cation
of chinese review using id27.
in
proceedings of the 29th paci   c asia confer-
ence on language, information and computa-
tion: posters, pages 258   266.

biao liu, minlie huang, jiashen sun, and xuan
zhu. 2015.
incorporating domain and senti-
ment supervision in representation learning for
id20. in international joint con-
ference on arti   cial intelligence.

bin lu, chenhao tan, claire cardie, and ben-
jamin k. tsou. 2011.
joint bilingual senti-
ment classi   cation with unlabeled parallel cor-

pora. in proceedings of the 49th annual meet-
ing of the association for computational lin-
guistics: human language technologies, pages
320   330.

thang luong, hieu pham, and christopher d.
effective approaches to
manning. 2015.
attention-based id4.
in
proceedings of the 2015 conference on empir-
ical methods in natural language processing,
pages 1412   1421.

laurens van der maaten and geoffrey hinton.
2008. visualizing data using id167. journal
of machine learning research.

christopher d. manning, mihai surdeanu, john
bauer, jenny finkel, steven j. bethard, and
david mcclosky. 2014. the stanford corenlp
natural language processing toolkit. in associ-
ation for computational linguistics (acl) sys-
tem demonstrations, pages 55   60.

rada mihalcea, carmen banea, and janyce
wiebe. 2007. learning multilingual subjective
language via cross-lingual projections. in pro-
ceedings of the 45th annual meeting of the as-
sociation of computational linguistics, pages
976   983.

saif m. mohammad, mohammad salameh, and
svetlana kiritchenko. 2016. how translation
journal of arti   cial intelli-
alters sentiment.
gence research, 55(1):95   130.

sinno j. pan, ivor w. tsang, james t. kwok,
and qiang yang. 2011. id20 via
ieee transac-
transfer component analysis.
tions on neural networks, 22(2):199   210.

adam paszke, sam gross, soumith chintala, gre-
gory chanan, edward yang, zachary devito,
zeming lin, alban desmaison, luca antiga,
and adam lerer. 2017. automatic differenti-
ation in pytorch. in nips 2017 autodiff work-
shop.

mohammad salameh, saif mohammad, and svet-
lana kiritchenko. 2015. sentiment after transla-
tion: a case-study on arabic social media posts.
in proceedings of the 2015 conference of the
north american chapter of the association for
computational linguistics: human language
technologies, pages 767   777.

michael d shapiro and matthew b blaschko.
2004. on hausdorff distance measures. techni-
cal report, technical report um-cs-2004-071.

richard socher, alex perelygin, jean wu, jason
chuang, d. christopher manning, andrew ng,
and christopher potts. 2013. recursive deep
models for semantic compositionality over a
sentiment treebank. in proceedings of the 2013
conference on empirical methods in natural
language processing, pages 1631   1642.

hubert soyer, pontus stenetorp, and akiko
aizawa. 2015. leveraging monolingual data
for crosslingual compositional word representa-
tions. in international conference on learning
representations.

sheng kai tai, richard socher, and d. christo-
pher manning. 2015. improved semantic repre-
sentations from tree-structured long short-term
memory networks. in proceedings of the 53rd
annual meeting of the association for compu-
tational linguistics and the 7th international
joint conference on natural language pro-
cessing (volume 1: long papers), pages 1556   
1566.

joseph turian, lev-arie ratinov, and yoshua
bengio. 2010. word representations: a simple
and general method for semi-supervised learn-
ing. in proceedings of the 48th annual meeting
of the association for computational linguis-
tics, pages 384   394.

c  dric villani. 2008. optimal transport: old and
new, volume 338. springer science & business
media.

xiaojun wan. 2008. using bilingual knowledge
and ensemble techniques for unsupervised chi-
nese id31. in proceedings of the
2008 conference on empirical methods in nat-
ural language processing, pages 553   561.

xiaojun wan. 2009. co-training for cross-lingual
sentiment classi   cation. in proceedings of the
joint conference of the 47th annual meeting of
the acl and the 4th international joint con-
ference on natural language processing of the
afnlp: volume 1 - volume 1, pages 235   243,
stroudsburg, pa, usa.

ruochen xu and yiming yang. 2017. cross-
in

lingual distillation for text classi   cation.

proceedings of the 55th annual meeting of the
association for computational linguistics (vol-
ume 1: long papers), pages 1415   1425.

meng zhang, yang liu, huanbo luan, and
maosong sun. 2017. adversarial training for
unsupervised bilingual lexicon induction.
in
proceedings of the 55th annual meeting of the
association for computational linguistics (vol-
ume 1: long papers), pages 1959   1970, van-
couver, canada.

xiang zhang, junbo zhao, and yann lecun.
2015. character-level convolutional networks
in advances in neural
for text classi   cation.
information processing systems 28, pages 649   
657.

xinjie zhou, xiaojun wan, and jianguo xiao.
2016. cross-lingual sentiment classi   cation
with bilingual id194 learn-
ing. in proceedings of the 54th annual meet-
ing of the association for computational lin-
guistics (volume 1: long papers), pages 1403   
1412.

micha   ziemski, marcin junczys-dowmunt, and
bruno pouliquen. 2016. the united nations par-
allel corpus. in language resources and eval-
uation (lrec   16).

will y. zou, richard socher, daniel cer, and
christopher d. manning. 2013. bilingual word
embeddings for phrase-based machine transla-
tion. in proceedings of the 2013 conference on
empirical methods in natural language pro-
cessing, pages 1393   1398, seattle, washington,
usa.

