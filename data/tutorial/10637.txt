semi-supervised learning for id4

yong cheng#, wei xu#, zhongjun he+, wei he+, hua wu+, maosong sun    and yang liu       

#institute for interdisciplinary information sciences, tsinghua university, beijing, china

   state key laboratory of intelligent technology and systems

tsinghua national laboratory for information science and technology

department of computer science and technology, tsinghua university, beijing, china

+baidu inc., beijing, china

6
1
0
2
 
c
e
d
0
1

 

 
 
]
l
c
.
s
c
[
 
 

3
v
6
9
5
4
0

.

6
0
6
1
:
v
i
x
r
a

chengyong3001@gmail.com weixu@tsinghua.edu.cn

{hezhongjun,hewei06,wu hua}@baidu.com
{sms,liuyang2011}@tsinghua.edu.cn

abstract

while end-to-end neural machine transla-
tion (id4) has made remarkable progress
recently, id4 systems only rely on par-
allel corpora for parameter estimation.
since parallel corpora are usually limited
in quantity, quality, and coverage, espe-
cially for low-resource languages,
it is
appealing to exploit monolingual corpora
to improve id4. we propose a semi-
supervised approach for training id4
models on the concatenation of labeled
(parallel corpora) and unlabeled (mono-
lingual corpora) data. the central idea is
to reconstruct the monolingual corpora us-
ing an autoencoder, in which the source-
to-target and target-to-source translation
models serve as the encoder and decoder,
respectively. our approach can not only
exploit the monolingual corpora of the
target language, but also of the source
language. experiments on the chinese-
english dataset show that our approach
achieves signi   cant
improvements over
state-of-the-art smt and id4 systems.
introduction

1
end-to-end id4 (id4),
which leverages a single, large neural network to
directly transform a source-language sentence into
a target-language sentence, has attracted increas-
ing attention in recent several years (kalchbren-
ner and blunsom, 2013; sutskever et al., 2014;
bahdanau et al., 2015). free of latent structure
design and feature engineering that are critical in
conventional id151 (smt)
(brown et al., 1993; koehn et al., 2003; chi-
ang, 2005), id4 has proven to excel in model-

    yang liu is the corresponding author.

ing long-distance dependencies by enhancing re-
current neural networks (id56s) with the gating
(hochreiter and schmidhuber, 1993; cho et al.,
2014; sutskever et al., 2014) and attention mecha-
nisms (bahdanau et al., 2015).

however, most existing id4 approaches suf-
fer from a major drawback:
they heavily rely
on parallel corpora for training translation mod-
els. this is because id4 directly models the
id203 of a target-language sentence given a
source-language sentence and does not have a sep-
arate language model like smt (kalchbrenner and
blunsom, 2013; sutskever et al., 2014; bahdanau
et al., 2015). unfortunately, parallel corpora are
usually only available for a handful of resource-
rich languages and restricted to limited domains
such as government documents and news reports.
in contrast, smt is capable of exploiting abundant
target-side monolingual corpora to boost    uency
of translations. therefore, the unavailability of
large-scale, high-quality, and wide-coverage par-
allel corpora hinders the applicability of id4.

as a result, several authors have tried to use
abundant monolingual corpora to improve id4.
gulccehre et al. (2015) propose two methods,
which are referred to as shallow fusion and deep
fusion, to integrate a language model into id4.
the basic idea is to use the language model to
score the candidate words proposed by the transla-
tion model at each time step or concatenating the
hidden states of the language model and the de-
coder. although their approach leads to signi   -
cant improvements, one possible downside is that
the network architecture has to be modi   ed to in-
tegrate the language model.

alternatively, sennrich et al. (2015) propose
two approaches to exploiting monolingual corpora
that is transparent to network architectures. the
   rst approach pairs monolingual sentences with
dummy input. then, the parameters of encoder

figure 1: examples of (a) source autoencoder and (b) target autoencoder on monolingual corpora. our
idea is to leverage autoencoders to exploit monolingual corpora for id4. in a source autoencoder, the
      
source-to-target model p (y|x;
   ) serves as an encoder to transform the observed source sentence x
      
into a latent target sentence y (highlighted in grey), from which the target-to-source model p (x(cid:48)|y;
   )
reconstructs a copy of the observed source sentence x(cid:48) from the latent target sentence. as a result,
monolingual corpora can be combined with parallel corpora to train bidirectional id4 models in a
semi-supervised setting.

and attention model are    xed when training on
these pseudo parallel sentence pairs. in the sec-
ond approach, they    rst train a nerual translation
model on the parallel corpus and then use the
learned model to translate a monolingual corpus.
the monolingual corpus and its translations con-
stitute an additional pseudo parallel corpus. simi-
lar ideas have also been suggested in conventional
smt (uef   ng et al., 2007; bertoldi and federico,
2009). sennrich et al. (2015) report that their ap-
proach signi   cantly improves translation quality
across a variety of language pairs.

in this paper, we propose semi-supervised
learning for id4. given la-
beled (i.e., parallel corpora) and unlabeled (i.e.,
monolingual corpora) data, our approach jointly
trains source-to-target and target-to-source trans-
lation models. the key idea is to append a re-
construction term to the training objective, which
aims to reconstruct the observed monolingual cor-
pora using an autoencoder. in the autoencoder, the
source-to-target and target-to-source models serve
as the encoder and decoder, respectively. as the
id136 is intractable, we propose to sample the
full search space to improve the ef   ciency. specif-
ically, our approach has the following advantages:

1. transparent to network architectures: our ap-
proach does not depend on speci   c architec-
tures and can be easily applied to arbitrary
end-to-end id4 systems.

2. both the source and target monolingual cor-
pora can be used: our approach can bene-
   t id4 not only using target monolingual
corpora in a conventional way, but also the
monolingual corpora of the source language.

experiments on chinese-english nist datasets
show that our approach results in signi   cant im-
provements in both directions over state-of-the-art
smt and id4 systems.

2 semi-supervised learning for neural

machine translation
2.1 supervised learning
given a parallel corpus d = {(cid:104)x(n), y(n)(cid:105)}n
n=1,
the standard training objective in id4 is to max-
imize the likelihood of the training data:

l(  ) =

log p (y(n)|x(n);   ),

(1)

n=1

where p (y|x;   ) is a neural translation model and
   is a set of model parameters. d can be seen
as labeled data for the task of predicting a target
sentence y given a source sentence x.
as p (y|x;   ) is modeled by a single, large neu-
ral network, there does not exist a separate target
language model p (y;   ) in id4. therefore, par-
allel corpora have been the only resource for pa-
rameter estimation in most existing id4 systems.
unfortunately, even for a handful of resource-rich

n(cid:88)

bushi yu shalong juxing le huitanbushi yu shalong juxing le huitanbush held a talk with sharonbush held a talk with sharonbush held a talk with sharonbushi yu shalong juxing le huitanencoderdecoderencoderdecoder(a)(b)languages, the available domains are unbalanced
and restricted to government documents and news
reports. therefore, the availability of large-scale,
high-quality, and wide-coverage parallel corpora
becomes a major obstacle for id4.

t=1?

2.2 autoencoders on monolingual corpora
it is appealing to explore the more readily avail-
able, abundant monolingual corpora to improve
id4. let us    rst consider an unsupervised set-
ting: how to train id4 models on a monolingual
corpus t = {y(t)}t

our idea is to leverage autoencoders (vincent et
al., 2010; socher et al., 2011): (1) encoding an ob-
served target sentence into a latent source sentence
using a target-to-source translation model and (2)
decoding the source sentence to reconstruct the
observed target sentence using a source-to-target
model. for example, as shown in figure 1(b),
given an observed english sentence    bush held
a talk with sharon   , a target-to-source translation
model (i.e., encoder) transforms it into a chinese
translation    bushi yu shalong juxing le huitan    that
is unobserved on the training data (highlighted in
grey). then, a source-to-target translation model
(i.e., decoder) reconstructs the observed english
sentence from the chinese translation.
      
   )
be source-to-target and target-to-source transla-
      
tion models respectively, where
   are cor-
responding model parameters. an autoencoder
aims to reconstruct the observed target sentence
via a latent source sentence:
      
   )
      
p (y(cid:48), x|y;
   ,
      
   )

      
  ) and p (x|y;
      
   and

more formally, let p (y|x;

(cid:88)
(cid:88)

p (y(cid:48)|y;

      
   )

      
   )

      
   ,

(2)

=

=

x

,

(cid:124)

(cid:123)(cid:122)

p (x|y;
encoder

(cid:125)

(cid:124)

(cid:123)(cid:122)

p (y(cid:48)|x;
decoder

(cid:125)

x

where y is an observed target sentence, y(cid:48) is a
copy of y to be reconstructed, and x is a latent
source sentence.

we refer to eq. (2) as a target autoencoder. 1
likewise, given a monolingual corpus of source
language s = {x(s)}s
s=1, it is natural to introduce
a source autoencoder that aims at reconstructing

1our de   nition of auotoencoders is inspired by ammar et
al. (2014). note that our autoencoders inherit the same spirit
from conventional autoencoders (vincent et al., 2010; socher
et al., 2011) except that the hidden layer is denoted by a latent
sentence instead of real-valued vectors.

the observed source sentence via a latent target
sentence:

p (x(cid:48)|x;

(cid:88)
(cid:88)

y

y

      
   ,

      
   )
      
p (x(cid:48), y|x;
   )
      
   )

p (y|x;
encoder

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:124)

(cid:123)(cid:122)

p (x(cid:48)|y;
decoder

      
   )

(cid:125)

=

=

.

(3)

please see figure 1(a) for illustration.

2.3 semi-supervised learning
as the autoencoders involve both source-to-target
and target-to-source models, it is natural to com-
bine parallel corpora and monolingual corpora to
learn birectional id4 translation models in a
semi-supervised setting.
formally, given a parallel corpus d =
{(cid:104)x(n), y(n)(cid:105)}n
n=1 , a monolingual corpus of target
language t = {y(t)}t
t=1, and a monolingual cor-
pus of source language s = {x(s)}s
s=1, we intro-
duce our new semi-supervised training objective
as follows:

j(

      
      
   ,
   )
log p (y(n)|x(n);

=

(cid:123)(cid:122)

      
   )

(cid:125)

n=1
source-to-target likelihood
      
   )

log p (x(n)|y(n);

+

n=1
target-to-source likelihood
      
   )

log p (y(cid:48)|y(t);

      
   ,

+  1

(cid:125)

n(cid:88)
(cid:124)
n(cid:88)
(cid:124)
t(cid:88)
(cid:124)
s(cid:88)
(cid:124)

s=1

t=1

+  2

(cid:123)(cid:122)
(cid:123)(cid:122)
(cid:123)(cid:122)

(cid:125)
(cid:125)

target autoencoder
      
log p (x(cid:48)|x(s);
   ,

      
   )
,

(4)

source autoencoder

where   1 and   2 are hyper-parameters for balanc-
ing the preference between likelihood and autoen-
coders.

note that the objective consists of four parts:
source-to-target likelihood, target-to-source likeli-
hood, target autoencoder, and source autoencoder.
in this way, our approach is capable of exploiting
abundant monolingual corpora of both source and
target languages.

(cid:41)

(cid:41)

n=1

t=1

(cid:40) n(cid:88)
t(cid:88)
s(cid:88)
(cid:40) n(cid:88)
t(cid:88)
s(cid:88)

s=1

t=1

n=1

  2

  2

the optimal model parameters are given by

   

      
  

= argmax

log p (y(n)|x(n);

      
   ) +

parallel

log p (y(cid:48)|y(t);

      
   ,

      
   ) +

  1

monolingual

chinese

english

2.56m

67.54m 74.82m
0.21m
0.16m
18.75m 22.32m
451.94m 399.83m
1.34m

0.97m

# sent.
# word
vocab.
# sent.
# word
vocab.

log p (x(cid:48)|x(s);

      
   ,

      
   )

(5)

table 1: characteristics of parallel and monolin-
gual corpora.

      
   
  

= argmax

log p (x(n)|y(n);

      
   ) +

log p (y(cid:48)|y(t);

      
   ,

      
   ) +

  1

log p (x(cid:48)|x(s);

      
   ,

      
   )

(6)

s=1

it is clear that the source-to-target and target-to-
source models are connected via the autoencoder
and can hopefully bene   t each other in joint train-
ing.

2.4 training
we use mini-batch stochastic id119 to
train our joint model. for each iteration, be-
sides the mini-batch from the parallel corpus, we
also construct two additional mini-batches by ran-
domly selecting sentences from the source and tar-
get monolingual corpora. then, gradients are col-
lected from these mini-batches to update model
parameters.

      
the partial derivative of j(
      
   ,
to the source-to-target model
   is given by

      
   ) with respect

=

   j(

n(cid:88)

n=1

+  1

+  2

      
   )

      
   ,
      
  
   
    log p (y(n)|x(n);

      
   )

      
  

   

t(cid:88)
s(cid:88)

t=1

    log p (y(cid:48)|y(t);

      
  

   

    log p (x(cid:48)|x(s);

      
  

s=1

   

      
   ,

      
   )

      
   ,

      
   )

.

(7)

      
   can be cal-

the partial derivative with respect to
culated similarly.

unfortunately, the second and third terms in eq.
(7) are intractable to calculate due to the exponen-
tial search space. for example, the derivative in

the third term in eq. (7) is given by

(cid:80)
(cid:80)
x   x (y) p (x|y;
x   x (y) p (x|y;

      
   )p (y(cid:48)|x;

      
   )     log p (y(cid:48)|x;      

   )

      
   )p (y(cid:48)|x;

  

         
      
   )

. (8)

it is prohibitively expensive to compute the sums
due to the exponential search space of x (y).
alternatively, we propose to use a subset of the
full space   x (y)     x (y) to approximate eq. (8):

(cid:80)
(cid:80)
x      x (y) p (x|y;
x      x (y) p (x|y;

      
   )p (y(cid:48)|x;

      
   )     log p (y(cid:48)|x;      

   )

      
   )p (y(cid:48)|x;

  

         
      
   )

. (9)

in practice, we use the top-k list of candidate
translations of y as   x (y). as |   x (y)| (cid:28) x|(y)|,
it is possible to calculate eq.
(9) ef   ciently by
enumerating all candidates in   x (y). in practice,
we    nd this approximation results in signi   cant
improvements and k = 10 seems to suf   ce to
keep the balance between ef   ciency and transla-
tion quality.

3 experiments
3.1 setup
we evaluated our approach on the chinese-
english dataset.

as shown in table 1, we use both a parallel
corpus and two monolingual corpora as the train-
ing set. the parallel corpus from ldc consists of
2.56m sentence pairs with 67.53m chinese words
and 74.81m english words. the vocabulary sizes
of chinese and english are 0.21m and 0.16m, re-
spectively. we use the chinese and english parts
of the xinhua portion of the gigaword cor-
pus as the monolingual corpora. the chinese
monolingual corpus contains 18.75m sentences
with 451.94m words. the english corpus contains
22.32m sentences with 399.83m words. the vo-
cabulary sizes of chinese and english are 0.97m
and 1.34m, respectively.

figure 2: effect of sample size k on the chinese-
to-english validation set.

figure 4: effect of oov ratio on the chinese-to-
english validation set.

figure 3: effect of sample size k on the english-
to-chinese validation set.

figure 5: effect of oov ratio on the english-to-
chinese validation set.

for chinese-to-english translation, we use the
nist 2006 chinese-english dataset as the vali-
dation set for hyper-parameter optimization and
model selection. the nist 2002, 2003, 2004,
and 2005 datasets serve as test sets. each chi-
nese sentence has four reference translations. for
english-to-chinese translation, we use the nist
datasets in a reverse direction:
treating the    rst
english sentence in the four reference transla-
tions as a source sentence and the original input
chinese sentence as the single reference trans-
lation. the evaluation metric is case-insensitive
id7 (papineni et al., 2002) as calculated by the
multi-id7.perl script.

we compared our approach with two state-of-

the-art smt and id4 systems:

1. moses (koehn et al., 2007): a phrase-based

smt system;

2. id56search (bahdanau et al., 2015): an

attention-based id4 system.

for moses, we use the default setting to train
the phrase-based translation on the parallel corpus
and optimize the parameters of id148
using the minimum error rate training algorithm
(och, 2003). we use the srilm toolkit (stolcke,
2002) to train 4-gram language models.

for id56search, we use the parallel corpus to
train the attention-based neural translation models.
we set the vocabulary size of id27s
to 30k for both chinese and english. we follow
luong et al. (2015) to address rare words.

on top of id56search, our approach is capa-
ble of training bidirectional attention-based neural
translation models on the concatenation of parallel
and monolingual corpora. the sample size k is set
to 10. we set the hyper-parameter   1 = 0.1 and

iterations0.00.51.01.52.02.53.03.54.0id730.030.531.031.532.032.533.033.534.0   104k=15k=10k=5k=1iterations0.00.51.01.52.02.53.03.54.0id715.015.516.016.517.017.5k=15k=10k=5k=1iterations0.00.51.01.52.02.53.03.54.0id730.030.531.031.532.032.533.033.534.0   1040% oov10% oov20% oov30% ooviterations0.00.51.01.52.02.53.03.54.0id76.08.010.012.014.016.018.0   1040% oov10% oov20% oov30% oov  2 = 0 when we add the target monolingual cor-
pus, and   1 = 0 and   2 = 0.1 for source monolin-
gual corpus incorporation. the threshold of gra-
dient clipping is set to 0.05. the parameters of
our model are initialized by the model trained on
parallel corpus.

3.2 effect of sample size k
as the id136 of our approach is intractable, we
propose to approximate the full search space with
the top-k list of candidate translations to improve
ef   ciency (see eq. (9)).

figure 2 shows the id7 scores of various set-
tings of k over time. only the english mono-
lingual corpus is appended to the training data.
we observe that increasing the size of the approx-
imate search space generally leads to improved
id7 scores. there are signi   cant gaps between
k = 1 and k = 5. however, keeping increas-
ing k does not result in signi   cant improvements
and decreases the training ef   ciency. we    nd that
k = 10 achieves a balance between training ef   -
ciency and translation quality. as shown in fig-
ure 3, similar    ndings are also observed on the
english-to-chinese validation set. therefore, we
set k = 10 in the following experiments.

3.3 effect of oov ratio
given a parallel corpus, what kind of monolingual
corpus is most bene   cial for improving transla-
tion quality? to answer this question, we investi-
gate the effect of oov ratio on translation quality,
which is de   ned as

(cid:80)
y   y(cid:74)y /    vdt(cid:75)

|y|

ratio =

,

(10)

where y is a target-language sentence in the mono-
lingual corpus t , y is a target-language word in y,
vdt is the vocabulary of the target side of the par-
allel corpus d.

intuitively, the oov ratio indicates how a sen-
tence in the monolingual resembles the parallel
corpus. if the ratio is 0, all words in the mono-
lingual sentence also occur in the parallel corpus.
figure 4 shows the effect of oov ratio on
the chinese-to-english validation set. only en-
glish monolingual corpus is appended to the par-
allel corpus during training. we constructed four
monolingual corpora of the same size in terms of
sentence pairs.    0% oov    means the oov ra-
tio is 0% for all sentences in the monolingual cor-
pus.    10% oov    suggests that the oov ratio is

no greater 10% for each sentence in the mono-
lingual corpus. we    nd that using a monolingual
corpus with a lower oov ratio generally leads to
higher id7 scores. one possible reason is that
low-oov monolingual corpus is relatively easier
to reconstruct than its high-oov counterpart and
results in better estimation of model parameters.

figure 5 shows the effect of oov ratio on the
english-to-chinese validation set. only english
monolingual corpus is appended to the parallel
corpus during training. we    nd that    0% oov   
still achieves the highest id7 scores.

3.4 comparison with smt
table 2 shows the comparison between moses
and our work. moses used the monolingual
corpora as shown in table 1: 18.75m chinese
sentences and 22.32m english sentences. we
   nd that exploiting monolingual corpora dramat-
ically improves translation performance in both
chinese-to-english and english-to-chinese direc-
tions.

relying only on parallel corpus, id56search
outperforms moses trained also only on par-
allel corpus.
the capability of making
use of abundant monolingual corpora enables
moses to achieve much higher id7 scores than
id56search only using parallel corpus.

but

instead of using all sentences in the monolin-
gual corpora, we constructed smaller monolingual
corpora with zero oov ratio: 2.56m chinese sen-
tences with 47.51m words and 2.56m english
english sentences with 37.47m words.
in other
words, the monolingual corpora we used in the
experiments are much smaller than those used by
moses.

by adding english monolingual corpus, our
approach achieves substantial improvements over
id56search using only parallel corpus (up to
+4.7 id7 points).
in addition, signi   cant im-
provements are also obtained over moses using
both parallel and monolingual corpora (up to +3.5
id7 points).

an interesting    nding is that adding english
monolingual corpora helps to improve english-to-
chinese translation over id56search using only
parallel corpus (up to +3.2 id7 points), sug-
gesting that our approach is capable of improving
id4 using source-side monolingual corpora.

in the english-to-chinese direction, we ob-
in particular, adding chi-

tain similar    ndings.

system

moses

id56search

e

c

training data direction
ce
         
c     e
e     c
          
c     e
          
e     c
         
c    e
e    c
          
c     e
e    c
          
c    e
e    c

nist06

nist02

nist03

nist04

nist05

32.48
14.27
34.59
20.69
30.74
15.71
35.61      ++
17.59++
35.01++
21.12   ++

32.69
18.28
35.21
25.85
35.16
20.76
38.78      ++
23.99 ++
38.20      ++
29.52      ++

32.39
15.36
35.71
19.76
33.75
16.56
38.32      ++
18.95++
37.99      ++
20.49      ++

33.62
13.96
35.56
18.77
34.63
16.85
38.49      ++
18.85++
38.16      ++
21.59      ++

30.23
14.11
33.74
19.74
31.74
15.14
36.45      ++
17.91++
36.07      ++
19.97++

table 2: comparison with moses and id56search. moses is a phrase-based statistical machine
translation system (koehn et al., 2007). id56search is an attention-based id4
system (bahdanau et al., 2015).    ce    donates chinese-english parallel corpus,    c    donates chinese
monolingual corpus, and    e    donates english monolingual corpus.    
    means the corpus is included in
the training data and    means not included.    nist06    is the validation set and    nist02-05    are test
sets. the id7 scores are case-insensitive.    *   : signi   cantly better than moses (p < 0.05);    **   :
signi   cantly better than moses (p < 0.01);   +   : signi   cantly better than id56search (p < 0.05);
   ++   : signi   cantly better than id56search (p < 0.01).

   

method

sennrich et al. (2015)

this work

e

c

training data direction nist06 nist02 nist03 nist04 nist05
ce
          
          
          
          

36.95
28.83
38.78      
23.99
38.20      
29.52      

35.33
19.17
36.45      
17.91
36.07      
19.97      

34.10
19.85
35.61      
17.59
35.01      
21.12      

36.80
20.61
38.32      
18.95
37.99      
20.49

37.99
20.54
38.49   
18.85
38.16
21.59      

c    e
e    c
c    e
e    c
c    e
e    c

table 3: comparison with sennrich et al. (2015). both sennrich et al. (2015) and our approach build
on top of id56search to exploit monolingual corpora. the id7 scores are case-insensitive.    *   :
signi   cantly better than sennrich et al. (2015) (p < 0.05);    **   : signi   cantly better than sennrich et al.
(2015) (p < 0.01).

nese monolingual corpus leads to more bene   ts
to english-to-chinese translation than adding en-
glish monolingual corpus. we also tried to use
both chinese and english monolingual corpora
through simply setting all the    to 0.1 but failed
to obtain further signi   cant improvements.

therefore, our    ndings can be summarized as

follows:

1. adding target monolingual corpus improves
over using only parallel corpus for source-to-
target translation;

2. adding source monolingual corpus also im-
proves over using only parallel corpus for
source-to-target translation, but the improve-
ments are smaller than adding target mono-
lingual corpus;

3. adding both source and target monolingual
corpora does not lead to further signi   cant
improvements.

3.5 comparison with previous work
we re-implemented sennrich et al.
method on top of id56search as follows:

(2015)   s

      
   
  

1. train the target-to-source neural translation
      
model p (x|y;
   ) on the parallel corpus d =
{(cid:104)x(n), y(n)(cid:105)}n
n=1.

2. the trained target-to-source model

is
used to translate a target monolingual corpus
t = {y(t)}t
t=1 into a source monolingual
corpus   s = {  x(t)}t

t=1.

3. the target monolingual corpus is paired with
its translations to form a pseudo parallel cor-
pus, which is then appended to the original
parallel corpus to obtain a larger parallel cor-
pus:   d = d     (cid:104)   s,t (cid:105).

4. re-train the the source-to-target neural trans-
lation model on   d to obtain the    nal model
parameters

      
  

   

.

monolingual

reference

translation

monolingual
reference

translation

hongsen shuo , ruguo you na jia famu gongsi dangan yishenshifa , name
tamen jiang zihui qiancheng .
hongsen said, if any logging companies dare to defy the law, then they will
destroy their own future .
hun sen said , if any of those companies dare defy the law , then they will
have their own fate . [iteration 0]
hun sen said if any tree felling company dared to break the law , then they
would kill themselves . [iteration 40k]
hun sen said if any logging companies dare to defy the law , they would
destroy the future themselves . [iteration 240k]

dan yidan panjue jieguo zuizhong queding , ze bixu zai 30 tian nei zhixing .
but once the    nal verdict is con   rmed , it must be executed within 30 days
.
however , in the    nal analysis , it must be carried out within 30 days .
[iteration 0]
however , in the    nal analysis , the    nal decision will be carried out within
30 days . [iteration 40k]
however , once the verdict is    nally con   rmed , it must be carried out within
30 days . [iteration 240k]

table 4: example translations of sentences in the monolingual corpus during semi-supervised learning.
we    nd our approach is capable of generating better translations of the monolingual corpus over time.

table 3 shows the comparison results. both the
two approaches use the same parallel and mono-
lingual corpora. our approach achieves signi   -
cant improvements over sennrich et al. (2015) in
both chinese-to-english and english-to-chinese
directions (up to +1.8 and +1.0 id7 points).
one possible reason is that sennrich et al. (2015)
only use the pesudo parallel corpus for parame-
ter estimation for once (see step 4 above) while
our approach enables source-to-target and target-
to-source models to interact with each other itera-
tively on both parallel and monolingual corpora.

to some extent, our approach can be seen as an
iterative extension of sennrich et al. (2015)   s ap-
proach: after estimating model parameters on the
pseudo parallel corpus, the learned model param-
eters are used to produce a better pseudo parallel
corpus. table 4 shows example viterbi transla-
tions on the chinese monolingual corpus over it-
erations:

x    = argmax

x

p (y(cid:48)|x;

      
   )p (x|y;

      
   )

.

(11)

(cid:110)

(cid:111)

we observe that the quality of viterbi transla-

tions generally improves over time.

4 related work

our work is inspired by two lines of research: (1)
exploiting monolingual corpora for machine trans-
lation and (2) autoencoders in unsupervised and
semi-supervised learning.

4.1 exploiting monolingual corpora for

machine translation

exploiting monolingual corpora for conventional
smt has attracted intensive attention in recent
years. several authors have introduced transduc-
tive learning to make full use of monolingual
corpora (uef   ng et al., 2007; bertoldi and fed-
erico, 2009). they use an existing translation
model to translate unseen source text, which can
be paired with its translations to form a pseudo
parallel corpus. this process iterates until con-
vergence. while klementiev et al. (2012) pro-
pose an approach to estimating phrase translation
probabilities from monolingual corpora, zhang
and zong (2013) directly extract parallel phrases
from monolingual corpora using retrieval tech-
niques. another important line of research is to
treat translation on monolingual corpora as a de-
cipherment problem (ravi and knight, 2011; dou
et al., 2014).

closely related to gulccehre et al. (2015) and
sennrich et al. (2015), our approach focuses on
learning birectional id4 models via autoen-
coders on monolingual corpora. the major ad-
vantages of our approach are the transparency to
network architectures and the capability to exploit
both source and target monolingual corpora.

4.2 autoencoders in unsupervised and

semi-supervised learning

autoencoders and their variants have been widely
used in unsupervised deep learning ((vincent et
al., 2010; socher et al., 2011; ammar et al., 2014),
just to name a few). among them, socher et al.
(2011)   s approach bears close resemblance to our
approach as they introduce semi-supervised recur-
sive autoencoders for id31. the dif-
ference is that we are interested in making a bet-
ter use of parallel and monolingual corpora while
they concentrate on injecting partial supervision
to conventional unsupervised autoencoders. dai
and le (2015) introduce a sequence autoencoder
to reconstruct an observed sequence via id56s.
our approach differs from sequence autoencoders
in that we use bidirectional translation models as
encoders and decoders to enable them to interact
within the autoencoders.

5 conclusion

we have presented a semi-supervised approach to
training bidirectional id4
models. the central idea is to introduce autoen-
coders on the monolingual corpora with source-to-
target and target-to-source translation models as
encoders and decoders. experiments on chinese-
english nist datasets show that our approach
leads to signi   cant improvements.

as our method is sensitive to the oovs present
in monolingual corpora, we plan to integrate jean
et al. (2015)   s technique on using very large vo-
cabulary into our approach. it is also necessary to
further validate the effectiveness of our approach
on more language pairs and id4 architectures.
another interesting direction is to enhance the
connection between source-to-target and target-to-
source models (e.g., letting the two models share
the same id27s) to help them bene   t
more from interacting with each other.

acknowledgements

this work was done while yong cheng was vis-
iting baidu. this research is supported by the
973 program (2014cb340501, 2014cb340505),
the national natural science foundation of china
(no. 61522204, 61331013, 61361136003), 1000
talent plan grant, tsinghua initiative research
program grants 20151080475 and a google fac-
ulty research award. we sincerely thank the
viewers for their valuable suggestions.

references
waleed ammar, chris dyer, and noah smith. 2014.
conditional random    eld autoencoders for unsuper-
vised structred prediction. in proceedings of nips
2014.

dzmitry bahdanau, kyunghyun cho, and yoshua
2015. id4 by
bengio.
jointly learning to align and translate. in proceed-
ings of iclr.

nicola bertoldi and marcello federico. 2009. domain
adaptation for id151 with
monolingual resources. in proceedings of wmt.

peter f. brown, stephen a. della pietra, vincent j.
della pietra, and robert l. mercer. 1993. the
mathematics of id151: pa-
rameter estimation. computational linguisitics.

david chiang.

2005. a hierarchical phrase-based
in pro-

model for id151.
ceedings of acl.

kyunhyun cho, bart van merri  enboer, dzmitry bah-
danau, and yoshua bengio. 2014. on the properties
of id4: encoder-decoder ap-
proaches. in proceedings of ssst-8.

andrew m. dai and quoc v. le.

supervised sequence learning.
nips.

2015.

semi-
in proceedings of

qing dou, ashish vaswani, and kevin knight. 2014.
beyond parallel data: joint word alignment and de-
in pro-
cipherment improves machine translation.
ceedings of emnlp.

caglar gulccehre, orhan firat, kelvin xu, kyunghyun
cho, lo    c barrault, huei-chi lin, fethi bougares,
holger schwenk, and yoshua bengio. 2015. on
using monolingual corpora in neural machine trans-
lation. arxiv:1503.03535 [cs.cl].

sepp hochreiter and j  urgen schmidhuber. 1993. the
mathematics of id151: pa-
rameter estimation. computational linguisitics.

sebastien jean, kyunghyun cho, roland memisevic,
and yoshua bengio. 2015. on using very large tar-
get vocabulary for id4.
in
proceedings of acl.

jiajun zhang and chengqing zong. 2013. learning
a phrase-based translation model from monolingual
data with application to id20. in pro-
ceedings of acl.

nal kalchbrenner and phil blunsom. 2013. recur-
rent continuous translation models. in proceedings
of emnlp.

alexandre klementiev, ann irvine, chris callison-
burch, and david yarowsky. 2012. toward statisti-
cal machine translation without paralel corpora. in
proceedings of eacl.

philipp koehn, franz j. och, and daniel marcu. 2003.
statistical phrase-based translation. in proceedings
of naacl.

philipp koehn, hieu hoang, alexandra birch, chris
callison-burch, marcello federico, nicola bertoldi,
brooke cowan, wade shen, christine moran,
richard zens, chris dyer, ondrej bojar, alexandra
constantin, and evan herbst. 2007. moses: open
source toolkit for id151. in
proceedings of acl (demo session).

minh-thang luong, ilya sutskever, quoc v. le, oriol
vinyals, and wojciech zaremba. 2015. addressing
the rare word problem in id4.
in proceedings of acl.

franz och. 2003. minimum error rate training in sta-
tistical machine translation. in proceedings of acl.

kishore papineni, salim roukos, todd ward, and wei-
jing zhu. 2002. id7: a methof for automatic eval-
in proceedings of
uation of machine translation.
acl.

sujith ravi and kevin knight. 2011. deciphering for-

eign language. in proceedings of acl.

rico sennrich, barry haddow, and alexandra birch.
2015. improving nerual machine translation models
with monolingual data. arxiv:1511.06709 [cs.cl].

richard socher, jeffrey pennington, eric huang, an-
drew ng, and christopher manning. 2011. semi-
supervised recursive autoencoders for predicting
sentiment distributions. in proceedings of emnlp.

andreas stolcke. 2002. srilm - am extensible lan-

guage modeling toolkit. in proceedings of icslp.

ilya sutskever, oriol vinyals, and quoc v. le. 2014.
sequence to sequence learning with neural net-
works. in proceedings of nips.

nicola uef   ng, gholamreza haffari, and anoop
sarkar. 2007. trasductive learning for statistical
machine translation. in proceedings of acl.

pascal vincent, hugo larochelle,

isabelle lajoie,
yoshua bengio, and pierre-autoine manzagol.
2010. stacked denoising autoencoders: learning
useful representations in a deep network with a local
denoising criterion. journal of machine learning
research.

