can id4 do simultaneous translation?

kyunghyun cho and masha esipova

{kyunghyun.cho,masha.esipova}@nyu.edu

new york university

6
1
0
2

 

n
u
j
 

7

 
 
]
l
c
.
s
c
[
 
 

1
v
2
1
0
2
0

.

6
0
6
1
:
v
i
x
r
a

abstract

we investigate the potential of attention-based
id4 in simultaneous
translation. we introduce a novel decoding
algorithm, called simultaneous greedy decod-
ing, that allows an existing neural machine
translation model to begin translating before
a full source sentence is received. this ap-
proach is unique from previous works on si-
multaneous translation in that segmentation
and translation are done jointly to maximize
the translation quality and that translating each
segment is strongly conditioned on all the pre-
vious segments. this paper presents a    rst
step toward building a full simultaneous trans-
lation system based on neural machine trans-
lation.

1

introduction

simultaneous translation differs from a more usual
consecutive translation. in simultaneous translation,
the objective of a translator, or a translation system,
is de   ned as a combination of quality and delay, as
opposed to consecutive translation in which transla-
tion quality alone matters. in order to minimize de-
lay while maximizing quality, a simultaneous trans-
lator must start generating symbols in a target lan-
guages before a full source sentence is received.

conventional approaches to simultaneous trans-
lation divide the translation process into two
stages (bangalore et al., 2012; fujita et al., 2013;
sridhar et al., 2013; yarmohammadi et al., 2013).
a segmentation algorithm, or model,    rst divides a
source sentence into phrases. each phrase is then

translated by an underlying, often black box, trans-
lation system, largely independent of the preceding
phrases. these two stages are separate from each
other, in that the exchange of information between
these two modules is limited.

in this paper, we study the problem of simulta-
neous translation in the context of neural machine
translation (forcada and   neco, 1997; sutskever et
al., 2014; bahdanau et al., 2015). rather than at-
tempting to build a completely novel model along
with a new training algorithm, we design a novel
decoding algorithm, called simultaneous greedy de-
coding, that is capable of performing simultaneous
translation with a id4 model
trained to maximize the quality of consecutive trans-
lation. unlike previous approaches, our proposal
performs segmentation and translation jointly based
solely on the translation quality (indirectly mea-
sured by the conditional probabilities.) furthermore,
translation of each and every segment is fully con-
ditioned on all the preceding segments through the
hidden states of a recurrent network.

we extensively evaluate the proposed simultane-
ous greedy decoding together with two waiting cri-
teria on three language pairs   en-cs, en-de and en-
ru. our analysis reveals that it is indeed possible
to use an existing id4 sys-
tem for simultaneous translation, and the proposed
algorithm provides a way to control the trade-off be-
tween quality and delay. our qualitative analysis
on en-ru simultaneous translation reveals interest-
ing behaviours such as phrase repetition as well as
premature commitment.

we consider this work as a    rst step toward build-

ing a full simultaneous translation system based on
id4.
in the conclusion, we
identify the following directions for the future re-
search. first, a trainable waiting criterion will allow
a deeper integration between simultaneous decoding
and id4, resulting in a better
simultaneous translation system. second, we need
to eventually develop a learning algorithm speci   -
cally for simultaneous translation.

2 attention-based neural translation
id4 (forcada and   neco,
1997; kalchbrenner and blunsom, 2013; sutskever
et al., 2014; bahdanau et al., 2015), has recently
become a major alternative to the existing statisti-
cal phrase-based machine translation system (koehn
et al., 2003). for instance, in the translation task
of wmt   16, the top rankers for en   cs, en   de,
en   fi and en   ru all used attention-based neural
machine translation (bahdanau et al., 2015; luong
et al., 2015).1

the attention-based id4 is
built as a composite of three modules   encoder, de-
coder and attention mechanism. the encoder is usu-
ally implemented as a recurrent network that reads a
source sentence x = (x1, . . . ,xtx) and returns a set of
context vectors c = {h1, . . . ,htx}, where

ht =   enc(ht   1,xt).

(1)

instead of a vanilla, unidirectional recurrent net-
work (luong et al., 2015), it is possible to use a more
sophisticated network such as a bidirectional recur-
rent network (bahdanau et al., 2015) or a tree-based
recursive network (eriguchi et al., 2016).

the decoder is a conditional

language model
based on a recurrent network (mikolov et al., 2010).
at each time step t(cid:48), it    rst uses the attention mecha-
nism to compute a single time-dependent vector out
of all the context vectors: ct(cid:48) =    tx

t=1   tht, where

  t     exp ( fatt(zt(cid:48)   1,   yt(cid:48)   1,ht)) .

(2)

zt   1 and   yt   1 are the decoder   s hidden state and the
previous target word. this content-based attention
mechanism can be extended to incorporate also the
location of each context vector (luong et al., 2015).

1newstest2016 at http://matrix.statmt.org/

else

  yt = argmaxyt log p(yt|y<t,c)
if s     tx then
write(y,   yt), t     t + 1
c(cid:48)     read(x,   ) if |c(cid:48)| = 0.
if   (c,c   c(cid:48)) then

algorithm 1 simultaneous greedy decoding
require:    , s0, input pipe x, output pipe y
1: initialize s     s0, c     read(x,s), c(cid:48)     {}
2: initialize the decoder   s state z0 based on c
3: while true do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19: end while

c     c   c(cid:48), s     s +    , c(cid:48)     {}
continue
write(y,   yt), t     t + 1

end if
if   yt = (cid:104)eos(cid:105) then

break

end if

end if

else

time-dependent

the decoder updates

its own state with
this
by
zt(cid:48) =   dec(zt(cid:48)   1,   yt(cid:48)   1,ct(cid:48)). the distribution over
the next target word is then computed by

context

vector

p(yt(cid:48) = j|   y<t(cid:48),x)     exp (  oout(zt(cid:48))) .

the initial state of the decoder is often initialized as

z0 =   init(c).

(3)

the whole model, consisting of the encoder, de-
coder and attention mechanism, is jointly trained to
maximize the log-likelihood given a set of n training
<t(cid:48),x n), where   
pairs: max  
denotes all the parameters.

t(cid:48)=1 log p(yn

n=1    t n

t(cid:48)|yn

n    n
1

y

3 simultaneous greedy decoding
we investigate the potential of using a trained neu-
ral machine translation as a simultaneous translator
by introducing a novel decoding algorithm. in this
section, we propose and describe such an algorithm,
called simultaneous greedy decoding, in detail.

3.1 algorithm
an overall algorithm is presented in alg. 1.

input arguments the proposed simultaneous
greedy decoding has two hyperparameters that are
used to control the trade-off between delay and qual-
ity. they are the step size    and the number of
initially-read source symbols s0. we will describe
how the delay is de   ned later in sec. 3.3.

as the goal is to do simultaneous translation, the
algorithm receives two input x and output y pipes
instead of a full source sentence at the beginning.
each pipe comes with two api   s which are write
and read. write(p,x) commits the symbol x to
the pipe p, and read(p,n) receives n symbols from
the pipe p and computes the additional context vec-
tors using the encoder (line 11 of alg. 1).
state the proposed algorithm maintains the fol-
lowing state variables:

1. s: # of received source words
2. t: # of committed target words + 1
3. c = {h1, . . . ,hs}
4. c(cid:48): additional context vectors
5. zt   1: the decoder   s latest hidden state

initialization initially,
the algorithm reads s0
source symbols from x and initialize its context set
c to contain the context vectors h1, . . . ,hs0. based
on this initial context set, the decoder   s hidden state
z0 is initialized according to eq. (3). see lines 2   3.
translation core at each iteration, the algorithm
checks whether the full source sentence has been
read already (line 6).
if so, it simply performs
greedy decoding by committing the most likely tar-
get symbol to y , as would have been done in con-
secutive translation, while incrementing t by one for
each committed target word (line 7).

otherwise, the algorithm reads    more source
symbols from x and forms an additional context set
c(cid:48), if c(cid:48) is currently empty.
it then compares the
conditional distributions over the next target sym-
bols yt given either the current context set c or the
new context set c    c(cid:48) based on a prede   ned crite-
rion   . the criterion should be designed so as to
determine whether the amount of information in c
is enough to make a decision in that the additional
information c(cid:48) would not make a difference. two
such criteria will be described later in sec. 3.2
if this waiting criterion is met (i.e., it is deter-
mined better to consider c(cid:48),) the context set is up-
dated to include the additional context vectors (line

13,) and c(cid:48) is    ushed. if not, the algorithm commits
the most likely target symbols   yt given the current
context set c and increments t by one (line 15).

the iteration terminates when the last committed

symbol   yt is the end-of-sentence symbol.

computational complexity the worse-case
complexity of this simultaneous greedy decoding is
twice as expensive as that of a consecutive greedy
decoding is, with some trivial cacheing of c(cid:48),
log p(yt|y<t,c) and log p(yt|y<t,c   c(cid:48)). the worse
case happens when the waiting criterion is met for
all the source symbols, in which case the proposed
algorithm reduces to the consecutive greedy trans-
lation. the complexity however drops signi   cantly
in practice as the waiting criterion is satis   ed more,
because this reduces the number of context vectors
over which the alignment weights are computed
(see eq. (2).) the empirical evaluation, which will
be presented later in this paper, reveals that we can
control this behaviour by adjusting    and s0.

why    greedy    decoding? it is possible to ex-
tend the proposed algorithm to be less greedy. for
instance, instead of considering a single target word
at a time (   yt), we can let it consider a multi-symbol
sequence at a time. this however increase the com-
putational complexity at each time step greatly, and
we leave it as a future research.

3.2 waiting criteria   

at each point in time, the simultaneous greedy de-
coding makes a decision on whether to wait for a
next batch of source symbols or generate a target
symbol given the current context (c   c(cid:48) in line 10 of
alg. 1.) this decision is made based on a prede   ned
criterion   (c,c    c(cid:48)). in this paper, we investigate
two different, but related criteria.

criterion 1: wait-if-worse the    rst alternative
is wait-if-worse.
it considers whether the con   -
dence in the prediction based on the current source
context (up to s source words) decreases with more
source context (up to s +    source words). this is
detected by comparing the log-probabilities of the
target word selected to be most likely when only the

   rst s source words were considered, i.e.,

  (c,c   c(cid:48)) : (log p(   y|   y<t,c)

> log p(   y|   y<t,c   c(cid:48))),

(4)

where   y = argmaxy p(y|   y<t,c).
criterion 2: wait-if-diff the next alternative,
wait-if-diff, compares the most likely target words
given up to s and up to s+   source words. if they are
same, we commit to this target word. otherwise, we
wait. this is different from the wait-if-worse crite-
rion, because the decrease in the log-id203 of
the most likely word given more source words does
not necessarily imply that the most likely word has
changed. we de   ne this criterion as
  (c,c   c(cid:48)) : (   y (cid:54)=   y(cid:48)),

(5)

where   y(cid:48) = argmaxy log p(y|   y<t,c   c(cid:48)).
other criteria although we consider the two cri-
teria described above, it is certainly possible to de-
sign another criterion. for instance, we have tested
the following id178-based criterion during our
preliminary experiments:
  (c,c   c(cid:48)) : (h (y|   y<t,c) > h (y|   y<t,c   c(cid:48))).

that is, the algorithm should wait for next source
symbols, if the id178 is expected to decrease (i.e.,
if the con   dence in prediction is expected to in-
crease.) this criterion however did not work as well.

3.3 delay in translation
for each decoded target symbol   yt, we can track how
many source symbols were required, i.e., |c     c(cid:48)|
in line 10 of alg. 1. we will use s(t) to denote
this quantity. then, we de   ne the (normalized) total
amount of time spent for translating a give source
sentence, or equivalent delay in translation, as

0 <   (x,   y ) =

1

|x||   y|

|   y|
   

t=1

s(t)     1.

(6)

in the case of full translation, as opposed to si-
multaneous translation,   (x,y ) = 1 for all y . this
follows from the fact that s(t) = |x| for all t.
in
the case of word-by-word translation in which case
|x| = |y|,   (x,y ) = 0.5, because s(t) = t.

alignment vs. delay s(t) however does not ac-
curately re   ect on which source pre   x the t-th target
symbol is conditioned. it is rather s(cid:48)(t) =|c| instead,
which re   ects the alignment between the source and
target symbols. it is thus more useful to inspect s(cid:48)(t)
in order to understand the behaviour of the proposed
simultaneous greedy decoding.

4 related work

perhaps not surprisingly, there are only a small num-
ber of prior work in simultaneous machine transla-
tion. much of those works are done in the context of
speech translation (bangalore et al., 2012; fujita et
al., 2013; sridhar et al., 2013; yarmohammadi et al.,
2013). often, incoming speech is transcribed by an
automatic id103 (asr) system, and the
transcribed text is segmented into a translation unit
largely based on acoustic and linguistic cues (e.g.,
silence or punctuation marks.) each of these seg-
ments is then translated largely independent from
each other by a separate machine translation sys-
tem. the simultaneous greedy decoding with neu-
ral machine translation, proposed in this paper, is
clearly distinguished from these approaches in that
(1) segmentation and translation happen jointly to
maximize the translation quality and (2) each seg-
mentation is strongly dependent on all the previous
segment in both the source and target sentences.

more recently, grissom ii et al. (2014) and oda
et al. (2014) proposed to extend those earlier ap-
proaches by introducing a trainable segmentation
policy which is trained to maximizes the transla-
tion quality. for instance, the trainable policy intro-
duced in (grissom ii et al., 2014) works by keeping
an intermediate source/translation pre   x, querying
an underlying black-box machine translation system
and deciding to commit these intermediate transla-
tions once a while. the policy is trained in the
framework of imitation learning (daum  e iii et al.,
2009). in both of these cases translation is still done
largely per segment, unlike the simultaneous trans-
lation with id4. we however
   nd this idea of learning a policy for simultaneous
decoding be directly applicable to the proposed si-
multaneous greedy decoding and leave it as a future
work.

another major contribution by grissom ii et al.

czech (12.12m)

german (4.15m)

russian (2.32m)

figure 1: quality vs. delay    plots for all the language pair   directions. (cid:78): wait-if-worse (en   ). (cid:72): wait-if-diff (en   ). (cid:52):
wait-if-worse (   en). (cid:79): wait-if-diff (   en). (cid:70): consecutive greedy decoding (en   ). (cid:7): consecutive id125 (en   ). $:
consecutive greedy decoding (   en).    : consecutive id125 (   en). each dashed line connects the points with the same
decoding parameters (   and s0) between translating to and from english. delay   : lower the better. id7: higher the better.

   en

en   

h
c
e
z
c

n
a
m
r
e
g

n
a
i
s
s
u
r

figure 2: quality and delay    per s0 and    . red dash-dot curves (        ): wait-if-worse. blue dashed curves (      ): wait-if-diff.
corpora available from wmt   15.2 all the sen-
(2014) was to let the policy predict the    nal verb of
tences were    rst tokenized3 and segmented into sub-
a source sentence, which is especially useful in the
case of translating from a verb-   nal language (e.g.,
word units using byte pair encoding (bpe) follow-
ing (sennrich et al., 2015b). during training, we
german) to another type of language. they do so by
having a separate verb-conditioned id165 language
only use sentence pairs, where both sides are less
model of all possible source pre   xes. this predic-
than or equal to 50 bpe subword symbols long.
tion, explicit in (grissom ii et al., 2014) is however
done implicitly in id4, where
the decoder acts as strong language model.

newstest-2013 is used as a validation set both to
early-stop training and to extensively evaluate the
proposed simultaneous greedy decoding algorithm.
we use newstest-2015 as a test set to con   rm that the
neural translation models used in the experiments
are reasonably well trained.

5 experimental settings

tasks and corpora we extensively study the
proposed simultaneous greedy decoding algorithm
on three language pairs   en-cs (12.12m sentence
pairs), en-de (4.15m) and en-ru (2.32m)    and in
both directions per pair. we use all the parallel

translation models
in total, we train six sepa-
rate neural translation models, one for each pair   

2http://www.statmt.org/wmt15/
3tokenizer.perl from moses (koehn et al., 2007).

5101520id70.30.40.50.60.70.80.91.0delay101520id70.50.60.70.80.91.0delay5101520id70.30.40.50.60.70.80.91.0delay234567s01.01.52.02.53.0  0.7200.7600.8000.8400.8800.9200.3000.3500.4000.4500.5000.5500.600delay  234567s06.0007.0008.0009.0009.00010.00010.00011.0001.0002.0003.0004.0005.0006.000id7234567s01.01.52.02.53.0  0.7600.8000.8400.8800.9200.3000.3600.4200.4800.5400.600delay  234567s08.0008.8009.60010.40011.20012.0002.0004.0006.0008.00010.00012.000id7234567s01.01.52.02.53.0  0.8600.8800.9000.9200.9400.9600.5700.6000.6300.6600.6900.7200.750delay  234567s015.20015.40015.60015.80016.0009.00010.00011.00012.00013.00014.00015.000id7234567s01.01.52.02.53.0  0.8500.8750.9000.9250.9500.5200.5600.6000.6400.6800.720delay  234567s015.60016.00016.40016.80016.80017.20010.50012.00013.50015.00016.500id7234567s01.01.52.02.53.0  0.7800.8100.8400.8700.9000.9300.3600.4200.4800.5400.6000.660delay  234567s013.20013.20013.50013.50013.80013.80014.10014.10014.40014.7004.5006.0007.5009.00010.50012.00013.500id7234567s01.01.52.02.53.0  0.7800.8100.8400.8700.9000.9300.3600.4200.4800.5400.6000.660delay  234567s013.20013.20013.50013.50013.80013.80014.10014.10014.40014.7004.5006.0007.5009.00010.50012.00013.500id7wait-if-worse

wait-if-diff

(a) from english:    active monitoring will be suggested , and if the disease progresses , they will be offered treatment .   

z
c
   

e
d
   

   
s
c

   
e
d

(b) to english:    one thing is certain : these new provisions will have a negative impact on voter turn-out .   

figure 3: example translation by simultaneous greedy decoding (a) from english and (b) to english. the source and reference
sentences were selected randomly from the development set. the correspondences between chunks of consecutive symbols in the
source and translation are highlighted by background color. the dotted lines indicate the latest source symbol up to which the
context was taken (s(cid:48)(t) from sec. 3.3). best viewed when zoomed digitally.

    ours

(cid:63)

n
e
n ours
e
   

(cid:63)

cs
15.2
13.84
20.47
20.32

de
19.5
21.75
23.96

24

ru
17.77
19.54
22.27
22.44

table 1: id7 scores on the test set (newstest-2015) obtained
by (ours) the models used in this paper and ((cid:63)) from (firat et
al., 2016). although our models use a unidirectional recurrent
net as an encoder, the translation qualities are comparable.
direction. we use a unidirectional recurrent network
with 1028 id149 (gru, (cho et al.,
2014)) as an encoder. a decoder is similarly a recur-
rent neural net with 1028 grus. the soft-alignment
function is a feedforward network with one hidden
layer consisting of 1028 tanh units. each model is
trained with adadelta (zeiler, 2012) until the aver-
age log-id203 on the validation set does not im-
prove, which takes about a week per model.

these trained models do not achieve the state-of-
the-art translation qualities, as they do not exploit
the ensemble technique (sutskever et al., 2014) nor
monolingual corpus (sennrich et al., 2015a), both
of which have been found to be crucial in improv-
ing id4. under these con-
straints, however, the trained models translates as
well as those reported earlier by, for instance, firat
et al. (2016), as can be seen in table 1.

decoding parameters with the proposed simul-
taneous greedy decoding, we vary        {1,2,3} and
s0     {2,3,4,5,6,7}. for each combination, we re-

port both id7 and the delay measure    from
eq. (6).
in order to put the translation quality of
the simultaneous translation in perspective, we re-
port id7 scores from consecutive translation with
both greedy and beamsearch decoding. the beam
width is set to 5, as used in (chung et al., 2016).

6 quantitative analysis
trade-off between quality and delay as shown
in fig. 1, there is a clear trade-off between the trans-
lation quality and delay. this trade-off is observed
regardless of the waiting criterion, although it is
more apparent with the wait-if-diff. out of two
waiting criteria, the wait-if-worse tends to achieve
a better translation quality, while it has substantially
higher delay in translation.

despite this general trend, we notice signi   cant
variations among the three languages we considered.
first, in the case of german, we see the trade-off be-
tween the delay and quality is maintained in both
translation directions (en   de and de   en), while
the delay slightly decreases when translating to en-
glish. a similar trend is observed with czech, except
that the wait-if-diff criterion tends to improve the
translation quality while maintaining the delay. on
the other hand, the experiments with russian show
that this trend is not universal across languages.

in the case of russian, translation to english does
not enjoy any improvement in translation quality, as
consecutive translation to english did (compare (cid:70)

activemonitoringwillbesuggested,andifthediseaseprog   resses,theywillbeo   eredtreatment.<eos>doporu     cujeseaktivn    sledov  an    ,ajestli  zesenemocpro   ch  az    ,budoukdispozici.activemonitoringwillbesuggested,andifthediseaseprog   resses,theywillbeo   eredtreatment.<eos>doporu     cenobudeaktivn    sledov  an    ,ajestli  zesenemocpro   ch  az    ,budoukdispozici.activemonitoringwillbesuggested,andifthediseaseprogres   ses,theywillbeo   eredtreatment.<eos>a   ktive  uberwachungwirdvorgeschlagen,undwenndiekrankheitvoran   kommt,werdensiebehandelt.activemonitoringwillbesuggested,andifthediseaseprogres   ses,theywillbeo   eredtreatment.<eos>a   ktive  uberwachungwirdvorgeschlagen,undwenndiekrankheitfortsch   rei   tet,werdensieauchbehandelt.jednojejist  e:tatonov  austanoven    budoum    tnegativn    dopadnavolebn      u  cast.<eos>onethingiscertain:thisnewclausewillhaveanegativeimpactonvot   erturn   out.jednojejist  e:tatonov  austanoven    budoum    tnegativn    dopadnavolebn      u  cast.<eos>onethingiscertain:thisnewsetofprovisionswillhaveanegativeimpact:ein   sistsicher:dieseneuenbestimmungenwerdensichnegativaufdiewahl   beteiligungauswirken.<eos>e   insbesure:thesenewruleswillhaveanegativeonvot   erturn   out.ein   sistsicher:dieseneuenbestimmungenwerdensichnegativaufdiewahl   beteiligungauswirken.<eos>onethingiscertain:thesenewregulationswillbecomenegativeonvot   erturn   out.(a) wait-if-worse,    = 1, s0 = 2

(b) wait-if-diff,    = 2, s0 = 2

(c) wait-if-diff,    = 1, s0 = 2

figure 4: example translations of    all the effect of vitamin d on cancer is not clear   . the right-most plot was cut off from the
right due to the space constraint. best viewed when zoomed digitally.

vs. p in the right panel of fig. 1.)
instead, there
is a general trend of lowered delay when translat-
ing russian to english compared to the other way
around.

we observe in all the cases that the delay de-
creases when the model translates to english, com-
pared to translating from english, with the excep-
tion of czech and the wait-if-worse criterion. we
conjecture that the richness of morphology in all the
three languages compared to english is behind this
phenomenon. because each word in these languages
has more information than a usual english word, it
becomes easier for the simultaneous greedy decoder
to generate more target symbols per source symbol.
the same explanation applies well to the increase
in delay when translating from english, as the lan-
guages with richer morphology often require com-
plex patterns of agreement across many words.

wait-if-worse vs. wait-if-diff the wait-if-diff
criterion tends to cover wider spectra of the delay
and the translation quality. on the other hand, with
the same set of decoding parameters      and s0   , the
wait-if-worse results in more delayed translation
with less variance in translation quality.
in order
to further examine the difference between these two
criteria, we plot the effect of    and s0 on both the
delay    and translation quality in fig. 2.

in fig. 2, we see a stark difference between these
two criteria. this difference reveals itself when we
inspect the translation quality w.r.t. the decoding pa-
rameters (right panel of each sub-   gure). the wait-
if-worse criterion is clearly more sensitive to s0,
while the wait-if-diff one is more sensitive to    .
the only exception is the case of translating czech
to english, in which case the wait-if-diff behaves
similar to the wait-if-worse when s0     5. on the
other hand, the delay patterns w.r.t. the decoding pa-
rameters are invariant to the choice of criterion.

figure 6: an example of phrase repetition when simultane-
ously translating using the wait-if-diff criterion.
7 qualitative analysis

    

we de   ne a metric of quality-to-delay ratio as
q2d = id7
, where      is an average delay over a test
corpus. we use this ratio, which prefers a translation
system with a high score and low delay, to choose
the best model for each language pair   direction.

in fig. 3, we present the simultaneous translation
examples by the selected models when translating
from english to a target language. we alternate be-
tween red and blue colors to indicate the correspon-
dence between the target symbols and their source
context. the source sentence is divided into chunks
based on s(cid:48)(t) de   ned in sec. 3.3. we see that the
wait-if-worse is relatively conservative than the
wait-if-diff, which was observed earlier in fig. 1.

7.1 ru-en simultaneous translation
here, we take more in-depth look at the simultane-
ous greedy decoding with en-ru as an example.

word/phrase repetition with wait-if-diff
in
fig. 4, we show the russian translations of    all the
effect of vitamin d on cancer is not clear.    with three
different settings. the most obvious observation we
can make is again that the wait-if-worse is much
more conservative than the other criterion (compare
(a) vs. (b   c).)

another noticeable phenomenon happens with the
wait-if-diff, which is that some words or phrases
are repeated as being translated. for instance in
fig. 4 (c), we see that                               (   also, ef-
fect   ) was repeated twice. this has been observed
with other sentences. as another example, see fig. 6

alsothee   ectofvitam   indoncancerisnotclear.<eos>                                         d                                     .alsothee   ectofvitam   indoncancerisnotclear.<eos>                                         d                                     .alsothee   ectofvitam   indoncancerisnotclear.<eos>                                                               d                      .thisdoesnotofcoursemeanthattheactivitiesofthel   h   cwillnecessarilytransformourlives,butitdoesmeanthat,actually,youneverknow...<eos>                          ,              ,                    ,                                ,                              l   h   c                                                                                   ,                      ,                        ,      ,                    ,                                  ...(a) wait-if-worse

figure 5: an example of ru   en simultaneous translation. we observe a similar trend of conservativeness in the wait-if-worse.

(b) wait-if-diff

where                             (   not means   ) was repeated
three times. we conjecture that this is an inherent
weakness in the wait-if-diff criterion which does
not take into account the decrease in con   dence (i.e.,
the output conditional distribution becoming    atter)
unlike the other criterion wait-if-worse.
premature commitment
in russian, a preceding
adjective needs to agree with the following noun.
this is unlike in english, and when translating from
english to russian, premature commitment of adjec-
tives may result in an inaccurate translation. for ex-
ample in fig. 7, we see that the simultaneous greedy
decoding committed too early to plural adjectives
(                           and                               ,    normal   
and    infrared   ) before it saw the noun    photogra-
phy   . the decoder translated    photography   , which
is singular, into                              (   photos   ).

figure 7: an example of premature commitment when simul-
taneously translating using the wait-if-diff criterion.
russian-to-english translation and other lan-
in the other direction (ru   en), we
guage pairs
observed a similar trend. for instance, the wait-if-
worse tends to be more conservative than the wait-
if-diff (see fig. 5), while the latter more often re-
sults in repeated phrases. our manual inspection of
other language pairs revealed similar behaviours.

8 discussion and future research
the quantitative analyses have revealed two major
   ndings. first, we found that it is indeed possible
to use the id4 model trained
without simultaneous translation as a goal for the

purpose of simultaneous machine translation. this
was done simply by a novel simultaneous greedy
decoding algorithm described in alg. 1. the algo-
rithm, which has two adjustable parameters   s0 and
   , allows a user to smoothly trade off translation de-
lay and quality, as shown in fig. 1.

the second    nding is that this trade-off prop-
erty depends heavily on the choice of waiting cri-
terion. two criteria introduced in this paper showed
markedly opposite behaviours, where the wait-if-
worse was sensitive to s0 while the other, wait-
if-diff, was to    . we suspect that this difference
in sensitivity has led to stark qualitative differences
such as the ones discussed in sec. 7.1.

this paper presents the    rst work investigating
the potential for simultaneous machine translation in
a recently introduced framework of neural machine
translation. based on our    ndings, we anticipate fur-
ther research in the following directions.

first, the waiting criteria proposed in this paper
are both manually designed and does not exploit rich
information embedded in the hidden representation
learned by the recurrent neural networks. informa-
tion captured by the hidden states is however dif   -
cult to extract, and we expect a trainable criterion
that takes as input the hidden state to outperform the
manually designed waiting criteria in terms of both
delay and quality

second, all the translation models tested in this
paper were not trained to do simultaneous transla-
tion. more speci   cally, the decoder was exposed to
a full set of context vectors returned by the encoder
only after the full source sentence was read. a train-
ing procedure that addresses this mismatch between
the context sets seen by the decoder during training
and test phases should be designed and evaluated.

      ,                  ,                    ,                                                                                                                                                ,                                      ,                      ,                              .<eos>this,ofcourse,doesnotmeanthattheworkofthebigfrontcol   lierisnecessarilytochangeourlife,butitisonlyshown,infact,thateverythingcanbe.      ,                  ,                    ,                                                                                                                                                ,                                      ,                      ,                              .<eos>this,ofcourse,doesnotmeanthattheworkofthebigfrontcol   lierwillnecessarilychangeourlife,butitisonlyshownthateverythingcanbe.acombinationofnormalandinfra   redphot   ographyproducedthespectac   ularcol   ouring.<eos>                                                                                                    ,                                                                                                        .philipp koehn, hieu hoang, alexandra birch, chris
callison-burch, marcello federico, nicola bertoldi,
brooke cowan, wade shen, christine moran, richard
zens, et al. 2007. moses: open source toolkit for
id151. in proceedings of the
45th annual meeting of the acl on interactive poster
and demonstration sessions, pages 177   180. associa-
tion for computational linguistics.

minh-thang luong, hieu pham, and christopher d
manning. 2015. effective approaches to attention-
arxiv preprint
based id4.
arxiv:1508.04025.

tomas mikolov, martin kara     at, lukas burget, jan cer-
nock`y, and sanjeev khudanpur. 2010. recurrent neu-
ral network based language model. interspeech,
2:3.

yusuke oda, graham neubig, sakriani sakti, tomoki
toda, and satoshi nakamura. 2014. optimizing seg-
mentation strategies for simultaneous speech transla-
tion. in acl, pages 551   556.

rico sennrich, barry haddow, and alexandra birch.
improving id4
arxiv preprint

2015a.
models with monolingual data.
arxiv:1511.06709.

rico sennrich, barry haddow, and alexandra birch.
2015b. id4 of rare words with
subword units. arxiv preprint arxiv:1508.07909.

vivek kumar rangarajan sridhar, john chen, srinivas
bangalore, andrej ljolje, and rathinavelu chengal-
varayan. 2013. segmentation strategies for streaming
speech translation. in hlt-naacl, pages 230   238.

ilya sutskever, oriol vinyals, and quoc vv le. 2014.
sequence to sequence learning with neural networks.
in nips, pages 3104   3112.

mahsa yarmohammadi, vivek kumar rangarajan srid-
har, srinivas bangalore, and baskaran sankaran.
2013. incremental segmentation and decoding strate-
in ijcnlp, pages
gies for simultaneous translation.
1032   1036.

matthew d zeiler. 2012. adadelta: an adaptive learning

rate method. arxiv preprint arxiv:1212.5701.

acknowledgments
kc thanks facebook, google (google faculty
award 2016) and nvidia (gpu center of excel-
lence 2015   2016).

references
dzmitry bahdanau, kyunghyun cho, and yoshua ben-
2015. id4 by jointly

gio.
learning to align and translate. in iclr 2015.

srinivas bangalore, vivek kumar rangarajan sridhar,
prakash kolan, ladan golipour, and aura jimenez.
2012. real-time incremental speech-to-speech trans-
lation of dialogs. in naacl, pages 437   445.

kyunghyun cho, bart van merri  enboer, caglar gul-
cehre, dzmitry bahdanau, fethi bougares, holger
schwenk, and yoshua bengio. 2014. learning phrase
representations using id56 encoder-decoder for statis-
tical machine translation. arxiv:1406.1078.

junyoung chung, kyunghyun cho, and yoshua bengio.
2016. a character-level decoder without explicit seg-
mentation for id4. in acl.

hal daum  e iii, john langford, and daniel marcu. 2009.
search-based id170. machine learn-
ing, 75(3):297   325.

akiko eriguchi, kazuma hashimoto, and yoshimasa
tsuruoka. 2016. tree-to-sequence attentional neural
machine translation. in acl.

orhan firat, kyunghyun cho, and yoshua bengio. 2016.
multi-way, multilingual id4
with a shared attention mechanism. in naacl.

mikel l. forcada and ram  on p.   neco. 1997. recur-
in

sive hetero-associative memories for translation.
iwann   97, pages 453   462.

tomoki fujita, graham neubig, sakriani sakti, tomoki
toda, and satoshi nakamura. 2013. simple, lexi-
calized choice of translation timing for simultaneous
in interspeech, pages 3487   
speech translation.
3491.

alvin grissom ii, he he, jordan l boyd-graber, john
morgan, and hal daum  e iii. 2014. don   t until the
   nal verb wait: id23 for simulta-
neous machine translation. in emnlp, pages 1342   
1352.

nal kalchbrenner and phil blunsom.

2013. recur-
rent continuous translation models. in emnlp, pages
1700   1709.

philipp koehn, franz josef och, and daniel marcu.
2003. statistical phrase-based translation. in proceed-
ings of the 2003 conference of the north american
chapter of the association for computational linguis-
tics on human language technology-volume 1, pages
48   54. association for computational linguistics.

