6
1
0
2

 
l
u
j
 

3

 
 
]
l
c
.
s
c
[
 
 

1
v
8
7
5
0
0

.

7
0
6
1
:
v
i
x
r
a

context-dependent word representation for neural

machine translation

heeyoul choi

samsung electronics
university of montreal
heeyoul@gmail.com

kyunghyun cho

new york university

kyunghyun.cho@nyu.edu

yoshua bengio

university of montreal
cifar senior fellow

yoshua.bengio@umontreal.ca

abstract

we    rst observe a potential weakness of continuous vector representations of sym-
bols in id4. that is, the continuous vector representation,
or a id27 vector, of a symbol encodes multiple dimensions of simi-
larity, equivalent to encoding more than one meaning of the word. this has the
consequence that the encoder and decoder recurrent networks in neural machine
translation need to spend substantial amount of their capacity in disambiguating
source and target words based on the context which is de   ned by a source sen-
tence. based on this observation, in this paper we propose to contextualize the
id27 vectors using a nonlinear bag-of-words representation of the
source sentence. additionally, we propose to represent special tokens (such as
numbers, proper nouns and acronyms) with typed symbols to facilitate translating
those words that are not well-suited to be translated via continuous vectors. the
experiments on en-fr and en-de reveal that the proposed approaches of contex-
tualization and symbolization improves the translation quality of neural machine
translation systems signi   cantly.

1

introduction

id4 is a recently proposed paradigm in machine translation, which is often
entirely built as a single neural network kalchbrenner and blunsom (2013); sutskever et al. (2014);
bahdanau et al. (2015). the id4 system, which often consists of an encoder
and decoder, projects and manipulates a source sequence of discrete linguistic symbols (source
sentence) in a continuous vector space, and decodes a target sequence of symbols (target sentence or
translation.) this is contrary to the conventional machine translation systems, such as phrase-based
id151 koehn et al. (2003), which work directly at the discrete symbol level.
in more detail, the    rst step of any id4 system is to convert each atomic
symbol into a corresponding continuous vector, which is often called as a id27. this step
is done for each source word independently of the other words and results in a source sequence of
id27s. the encoder network, which is often implemented as a recurrent neural network,
encodes this source sentence either into a single context vector sutskever et al. (2014); cho et al.
(2014b) or into a sequence of context vectors kalchbrenner and blunsom (2013); bahdanau et al.
(2015).

1

the decoder network, again a recurrent neural network, generates a translation word-by-word while
being conditioned on the context representation of the source sentence. at each step of generation
in the decoder, the internal hidden state of the decoder is updated    rst. the dot product between
this hidden state and the output id27 vector of each word in the target vocabulary is
computed and normalized across all the target words, resulting in a id203 distribution over the
target vocabulary. a target word is selected based on this distribution, and the whole process is
recursively repeated until the end-of-sequence symbol is generated.
among different variants of id4, the attention-based approach bahdanau
et al. (2015) has recently become de facto standard. it has been found to perform comparably to
or better than the existing phrase-based statistical systems in many language pairs including en-
fr jean et al. (2015a), en-de jean et al. (2015a,b); luong et al. (2015a), en-cs jean et al. (2015b),
and en-zh shen et al. (2015). much of these recent improvements have been made by tackling, e.g.,
the attention mechanism (which is central to the attention-based neural translation system) and the
computational issues arising from having a large target vocabulary.
unlike these recent works, we focus on source- and target-side id27 vectors in this
paper. more speci   cally, we    rst notice that the transformation from and to high-dimensional word
embedding vectors is done for each word largely independent of each other. we conjecture that
only a few axes in this high-dimensional space are relevant given a source sentence and that we
can remove much of the ambiguity in the choice of words by restricting, or turning off, most of the
irrelevant dimensions. we propose to achieve this automated way to turn off some dimensions of
id27s by contextualizing a id27 vector.
in addition to the proposed contextualization of both source and target id27 vectors,
we propose to extend the unknown token replacement technique proposed in luong et al. (2015b)
to multiple token types. this extension, to which we refer as symbolization, introduces multiple
meta-tokens such as the number token and the proper name token in addition to the unknown-
word token. this symbolization effectively remaps rare tokens into more frequent meta-tokens and
thereby results in improved translation quality.
we extensively evaluate the proposed contextualization and symbolization on two language pairs   
en-fr and en-de    with the attention-based id4 model. the experiments re-
veal that the contextualization and symbolization each improves the translation quality by 2 id7
scores, and together by 4 id7 points on en-fr. on en-de, they result in 1   2 id7 score im-
provements each, and together 2.5 id7 score increase.

2 background: id4

in this section, we give a brief overview of id4. more speci   cally, we de-
scribe the attention-based id4 bahdanau et al. (2015) which will be used
in the experiments later. however, we note that the proposed contextualization and symbolization
techniques are generally applicable to any other type of id4 systems such as
the sequence-to-sequence model sutskever et al. (2014).
the attention-based id4 system computes a conditional distribution over
translations given a source sentence x = (wx
p(y = (wy

2 , . . . , wx
2 , . . . , wy

t ):
t (cid:48))|x).

1 , wx
1 , wy

this is done by a neural network that consists of an encoder, a decoder and the attention mechanism.
the encoder is often implemented as a bidirectional recurrent neural network that reads the source
t     v is projected
sentence word-by-word. before being read by the encoder, each source word wx
onto a continuous vector space:

where 1(wx

t ) is a one-hot vector de   ned as

1(wx

t )j =

if j = wx
t
otherwise .

xt = ex1(wx

t ),

(cid:26) 1,

0,

2

(1)

(2)

ex     re  |v | is a source id27 matrix, where e and |v | are the id27
dimension and the vocabulary size, respectively.
the resulting sequence of id27 vectors is then read by the bidirectional encoder recurrent
network which consists of forward and reverse recurrent networks. the forward recurrent network
reads the sequence in the left-to-right order:

      
h t =

      
   (

      
h t   1, xt),

(cid:105)

(cid:104)      

      
h t+1, xt),

      
h 0 and
      
h t

while the reverse network reads it right-to-left:
      
   (

      
h t =
      
where the initial hidden states
h t +1 are initialized as all-zero vectors. the hidden states
from the forward and reverse recurrent networks are concatenated at each time step t to form an
annotation vector h: ht =
. this concatenation results in a context c that is a tuple of
      
annotation vectors: c = {h1, h2, . . . , ht} . the recurrent id180
   are in most
cases either long short-term memory units (lstm, hochreiter and schmidhuber (1997)) or gated
recurrent units (gru, cho et al. (2014a).)
the decoder consists of two sub-components   a recurrent network and the attention mechanism. the
recurrent network in the decoder is a unidirectional language model, which computes the conditional
distribution over the next target word given all the previous target words and the source sentence:

      
   and

h t;

p(wy

t(cid:48)|wy

<t(cid:48), x)

the decoder recurrent network maintains an internal hidden state zt(cid:48). at each time step t(cid:48), it    rst
uses the attention mechanism to select, or weight, the annotation vectors in the context tuple c.
the attention mechanism, which is a feedforward neural network, takes as input both the previous
decoder hidden state, and one of the annotation vectors, and returns a relevant score et(cid:48),t:

et(cid:48),t = fatt(zt(cid:48)   1, ht).

these relevance scores are normalized to be positive and sum to 1:1

we use the normalized scores to compute the weighted sum of the annotation vectors

  t(cid:48),t =

exp(et(cid:48),t)
k=1 exp(et(cid:48),k)

.

(3)

(cid:80)t
t(cid:88)

ct(cid:48) =

  t(cid:48),tht

which will be used by the decoder recurrent network to update its own hidden state by

t=1

zt(cid:48) =   z(zt(cid:48)   1, yt(cid:48)   1, ct(cid:48)).

similarly to the encoder,   z is implemented as either an lstm or gru. yt(cid:48)   1 is a target-side word
embedding vector computed by

yt(cid:48)   1 = ey1(wy

t(cid:48)   1),

(4)

similarly to eq. (1).
the id203 of each word i in the target vocabulary v (cid:48) is computed by

p(wy

t(cid:48) = i|wy

<t(cid:48), x)     exp (ey

i zt(cid:48) + ci) ,
i is the i-th row vector of the target id27 matrix.

where ey
the id4 model is usually trained to maximize the log-id203 of the cor-
rect translation given a source sentence using a large training parallel corpus. this is done by
stochastic id119, where the gradient of the log-likelihood is ef   ciently computed by the
id26 algorithm.

1 for more variants of the attention mechanism, we refer the readers to luong et al. (2015a).

3

word x(cid:48)
notebook

power

axis

1
2
1
2

nearest neighbours
diary notebooks (notebook) sketchbook jottings
palmtop notebooks (notebook) ipaq laptop
powers authority (power) powerbase sovereignity
powers electrohydraulic microwatts hydel (power)

table 1: the placements of the id27 vectors along the principals axes in the local charts
of    notebook    and    power   . in the case of    notebook   , it is clear that the    rst axis corresponds to
different types of books for writing a note, while the second axis corresponds to portable computing
devices. in the case of    power   , the    rst axis corresponds to political or social authority, while the
second axis to physical energy.

3 contextualized id27 vectors

3.1 id27 vectors

one-hot representation of a word in eq. (2) is unique in the sense that each and every word in
a vocabulary is equally distant from every other word. this implies that the words lose all the
information relative to the other words, when represented as a one-hot vector. the meaning of a
word, relative to those of the other words in the vocabulary, is thus learned through the associated
id27 vector (eqs. (1)   (4)) during training. in other words, training brings similar words
close to each other in the id27 space and dissimilar words far away from each other.
this phenomenon of similarity learning via id27 vectors has been observed in many
different natural language processing tasks done with neural networks. already in 1991, miikku-
lainen and dyer miikkulainen and dyer (1991) noticed that training a neural network with one-hot
vectors as its input learns the id27 vectors that    code properties of the input elements
that are most crucial to the task.    based on this observation bengio et al. bengio et al. (2003) pro-
posed to build a neural network based language model and found that it generalizes better to unseen
or rare id165s: the id27 vectors capture similarities between words and the neural net-
work can learn a smooth mapping that automatically generalizes by producing similar outputs for
semantically similar input sequences. the interest in id27 vectors, or distributed repre-
sentation of words, was fueled by the earlier observations that these unsupervised id27
vectors can be used to improve supervised natural language tasks greatly collobert et al. (2011);
turian et al. (2010).

3.2 multiple dimensions of similarity

an important characteristic of the high-dimensional id27 vectors is that it encodes multi-
ple dimensions of similarities. this is necessary in order for a neural network to cope with polysemy.
we can qualitatively check this phenomenon of multiple dimensions of similarities by inspecting a
local chart of the manifold on which the id27 vectors reside.

for any word x(cid:48) under inspection, we    nd the n     1 nearest neighbours(cid:8)x1, . . . , xn   1(cid:9)     ex in
the id27 matrix. the n id27 vectors(cid:8)x(cid:48), x1, . . . , xn   1(cid:9) now characterize

a local chart centered at x(cid:48), and we use principal component analysis (pca) to    nd the correspond-
ing lower-dimensional euclidean space. in this euclidean space, we can inspect the nearest neigh-
bours along each coordinate.2 in table 1, we show two such examples using the id27
vectors trained as a part of the continuous-bag-of-word (cbow) network mikolov et al. (2013).3
these examples clearly show that each id27 vector encodes more than one notions of
similarities. a similar behaviour can only be observed with multi-map id167 van der maaten and
hinton (2012).

2 the code for this analysis is available publicly at https://github.com/kyunghyuncho/

wordvectormanifold.

3 we used the id27 vectors provided as a part of hill et al. (2015).

4

(a)

(b)

figure 1: graphical illustrations of the id4 system (a) without and (b) with
the proposed contextualization.

3.3 contextualized id27s

the fact that each id27 vector represents multiple dimensions of similarity implies that
a subsequent part of a neural network needs to disambiguate the word based on the context in which
the word was used. in the case of id4, we should consider source and target
id27 vectors separately. the encoder, which is implemented as a bidirectional recurrent
network, can disambiguate it by using all the other words in a source sentence. on the other hand,
the decoder can exploit both the previous words in a target sentence as well as all the source words.
already pointed out in 1949 by weaver weaver (1949), much of ambiguity in word meaning can be
resolved by considering surrounding words. the consequence of this in id4
is that the recurrent network, either in the encoder or decoder, needs to remember all the previous
words until the word in question in order to decide its meaning disambiguously. in other words, the
encoder and decoder must sacri   ce their capacity in disambiguating the words. this is undesirable,
as what we truly want the encoder and decoder to do is to capture the higher-level compositional
structures of a sentence that are necessary for translation.
in order to address this issue and to reduce the burden from the recurrent networks in a neural
machine translation system, we propose to contextualize the id27 vectors before being
fed to the recurrent networks as shown in fig. 1. this contextualization disambiguates the word   s
meaning by masking out some dimensions of the id27 vectors based on the context.
let us describe in detail the proposed contextualization. first, we de   ne the context cx as a repre-
sentation of the unordered set of all source words and compute it as the average of the nonlinearly
transformed source id27 vectors, i.e.,

nn  (xt),

(5)

where nn   : re     rc is a feedforward neural network parametrized by   .
then, we compute a context mask from cx for each id27 vector before it is fed into an
input or output recurrent network. this is simply done by

t(cid:88)

t=1

cx =

1
t

where (cid:12) is an element-wise multiplication and    : rc     [0, 1] is an element-wise sigmoid function
and transforms the context to a binary-like mask, with w x and w y (weight matrices), bx and by
(bias vectors) being additional parameters.

xt     xt (cid:12)   (w xcx + bx),
yt     yt (cid:12)   (w ycx + by),

5

recall that we introduced earlier in sec. 3.2 a complicated procedure based on estimating a local
chart on a manifold using pca. compared to that procedure, our proposal for contextualization may
seem simple and perhaps insuf   cient. this is however not a serious issue due to two reasons. first,
the context mask cx is extracted via a highly nonlinear function in eq. (5) which can learn to effec-
tively project axes from the euclidean space, corresponding to a local chart, back to the coordinates
on the manifold. second, as both the id27 vectors are learned together, we expect the
dimensions of the id27 vector to disentangle multiple dimensions of similarity auto-
matically to maximize the use of the proposed contextualization routine. although both of these
are not easily veri   able, we show later in the experiment that the proposed contextualization indeed
improves the translation quality.

4 symbolization

4.1 proper nouns, digits and rare words

the use of continuous vectors as an intermediate representation of source and target sentences in
id4 greatly improves generalization of machine translation by avoiding the
issue of data sparsity (see, e.g., sec. 5.2.1 in cho (2015).) this, however, brings in some unnecessary
complications as well. one such complication is in handling proper nouns, digits and rare words.
first, the rare words, which often occur up to a handful of times in a whole training corpus, are
problematic, because their id27 vectors cannot be well estimated during training. it is
hence a usual practice to map all those rare words whose frequencies are under a prede   ned threshold
to a single token representing an unknown word. this approach has been used without much problem
in id38, where the objective is to score a given sentence. however, it is unacceptable
for a machine translation system to generate these unknown symbols when generating a translation.
the second issue is with digits. clearly the meaning of any digit is not de   ned by its context, and
there is a clear, single meaning associated with each digit, that is the number denoted by it. the
conversion to and from a continuous vector space of a digit may therefore introduce unnecessary
noise, resulting in an incorrect translation.
proper nouns often exhibit both of the above problems. they are often rare, except for the proper
nouns of a famous person, organization, object or location. furthermore, the transformation between
a proper noun and a high-dimensional continuous vector likely introduces noise that is absolutely
not necessary, as the meaning of such a proper noun is    xed to a single entity.
these issues and their underlying causes suggest that it would be bene   cial to treat these three cases
separately from all the other words. this is precisely what we propose to do, and we will describe it
in more detail below.

4.2 previous approaches

in most of these special cases, a source word is directly copied to a target sentence either as it is or
after a simple transformation via dictionary lookup. this property suggests a simple algorithm that
can be run outside id4. assuming that there exists an alignment between a
rare source word and a word or placeholder in the target sentence, we can look up the rare word in a
pre-built dictionary and replace the target-side placeholder with the queried word which can be the
source word itself or its appropriate translation or id68.
based on this observation, jean et al. jean et al. (2015a) earlier proposed a number of heuristics
for handling rare words with the attention-based id4. in their approach, the
attention weights   t(cid:48),t from eq. (3) are used to determine the source word aligned to each of the
unknown tokens in the generated translation, i.e., arg maxt   t(cid:48),t. the source word determined by
this mechanism is translated word-wise by looking up a pre-built dictionary, and the corresponding
unknown token in the translation is replaced with the result of the look-up.
simultaneously, luong et al. luong et al. (2015b) proposed another mechanism that does not require
the attention mechanism. they used an external alignment mechanism, such as ibm model 2 brown
et al. (1993), to    nd the alignment between the source and target words in the training corpus.
this alignment is used to assign multiple, numbered unknown tokens for rare words so that the

6

correspondences between these tokens in the source and target sides are maintained. for instance,
the    rst unknown token in the source side and its corresponding target word will be replaced with
(cid:104)unk1(cid:105), regardless of the target word   s position in the translation. during test time, given a source
sentence and a generated translation, each of the unknown tokens in the translation will be replaced
by querying the corresponding source-side word in a pre-built dictionary.
both of these approaches have been highly effective in improving the translation quality. for in-
stance, jean et al. jean et al. (2015a) reported +3 and +2.5 id7 improvement with the simple
replacement technique based on the attention mechanism on en-fr and en-de, respectively. simi-
larly, luong et al. luong et al. (2015b) reported +1.6   +2.8 id7 improvement on en-fr with their
approach based on the positional unknown tokens (cid:104)unkn(cid:105).

4.3 symbolization of proper nouns, digits and rare words

in this paper, we extend the approach by luong et al. luong et al. (2015b), which is based on the
positional unknown tokens, to include multiple positional special tokens. instead of a single special
token (cid:104)unk(cid:105)n, we propose to use three special tokens:

1. (cid:104)n(cid:105)n: digit
2. (cid:104)s(cid:105)n: proper noun
3. (cid:104)c(cid:105)n: acronym

see table 2 for examples of symbolization.
digit (cid:104)n(cid:105)n the basic idea is to replace any consecutive digits (without blank spaces in-between)
appearing both in the source and target sentences with the special symbol (cid:104)n(cid:105)n, where n denotes its
order in the source sentence. in order to better address the cases of a number followed immediately
by its unit, we separate the unit or any non-digit characters from consecutive digits (e.g.,    137kg   
       137   ,    kg   .) also, we normalize the variations in writing a long digit such as decimal marks (   ,   
vs.    .   ) and digit grouping deliminators (   ,    vs.    .    vs.        ,) when matching digits in the source and
target sides.
proper noun (cid:104)s(cid:105)n
in many of the european languages, which we mainly consider in this paper,
capitalization is used to indicate that a word is a proper noun. therefore, we replace a maximal
consecutive phrase of more than one capitalized words appearing on both the source and target side,
with the special symbol (cid:104)s(cid:105)n. similarly to (cid:104)n(cid:105)n, n denotes its order in the source sentence. we
consider a phrase, as many proper nouns are often proper phrases (e.g.,    new york   .) when spotting
these proper noun phrases, we do not consider non-capitalized functional words such as    of    in order
to properly handle noun phrases in the form of    x of y    (e.g.,    world of warcraft   .)
acronym (cid:104)c(cid:105)n most of the acronyms can be handled similarly to proper nouns, except that there
are cases where acronyms of a single entity differ across languages. for instance, the acronym of
   international monetary fund    is    imf    in english but    fmi    in french. we notice that it is rare to
have more than one such cases in a single sentence pair. we    rst replace all the matching acronym
pairs. if there is only one all-capital word left in each of the source and target sentences, we consider
them a match and replace it with (cid:104)c(cid:105)n.

rule dictionary we construct a mapping rule for each matching pair that was replaced by one of
the special symbols. this rule dictionary is used during test to replace the generated special symbols
in a translation. although it is certainly possible to incorporate external rules into this dictionary, we
do not test it to avoid including any external resource when evaluating the proposed symbolization
technique.
note that this approach of symbolization has been used to a certain extent in more conventional
id151 and language modelling. for more discussion, we refer the reader to
sec. 7.4 of koehn (2010). this approach however has not been adopted widely in neural machine
translation yet.

7

en

fr

en

fr

the world of warcraft and warcraft iii american regional    nals were held at
house of blues on june 2nd and 3rd in san diego , california .
les    nales r  egionales de warcraft iii et de world of warcraft pour l&apos; am  erique du
nord se sont d  eroul  ees au house of blues , les 2 et 3 juin derniers `a san diego .
the (cid:104)s(cid:105)1 and (cid:104)s(cid:105)2 american regional    nals were held at the (cid:104)s(cid:105)3 on june (cid:104)n(cid:105)1     nd and (cid:104)n(cid:105)2
    rd in (cid:104)s(cid:105)4 , california .
les    nales r  egionales de (cid:104)s(cid:105)2 et de (cid:104)s(cid:105)1 pour l&apos; am  erique du nord se sont d  eroul  ees
au (cid:104)s(cid:105)3 , les (cid:104)n(cid:105)1 et (cid:104)n(cid:105)2 juin derniers `a (cid:104)s(cid:105)4 .

the

table 2: examples of symbolization applied to en-fr sentence pairs.      is a special indicator that the
words immediately before and after be merged without a blank space after de-symbolization.

after

before

french

english

after
1.77m 0.44m 1.73m 0.49m
297m
294m 258m 336m
96.1
96.1
98.9

en-fr
symbolization before
unique words
total words
coverage (%)
en-de
before
after
symbolization before
709k
unique words
1,553k 1,035k
117m 102m 110m 95.5m
total words
coverage (%)
97.7
95.3

after
411k

german

english

99.1

98.6

93.3

table 3: corpora statistics

5 experimental settings

5.1 tasks and corpora

we evaluate the id4 systems with and without the proposed methods on
two tasks of english-to-french (en-fr) and english-to-german (en-de) translation. we use all the
parallel corpora made publicly available via wmt   14.4 the en-fr corpora are cleaned following
the procedure in cho et al. (2014b), and after id121, has approximately 250m words (english
side.) the en-de corpora are prepared following the procedure in jean et al. (2015a), resulting in
approximately 100m words (english side.)
we use newstest-2014 and newstest-2013 as the development sets and newsdiscusstest2015 and
newstest2015 as the test sets for en-fr and en-de, respectively.

5.2 vocabulary preparation

both of the corpora go through a minimal set of preprocessing. first, we tokenize them using the
script provided as a part of moses.5 as a comparison, we evaluate a setting where byte pair encoding
(bpe) is used, as a replacement of the proposed symbolization, to extract sub-word symbols sen-
nrich et al. (2015b). this has been found to be an effective and ef   cient approach to handling the
issue of a large target vocabulary sennrich et al. (2015a); firat et al. (2016); chung et al. (2016) and
is known to be able to transliterate rare, proper nouns up to a certain extent sennrich et al. (2015b).
in all the cases, we use up to top-30k most frequent symbols (either tokenized words or bpe-based
sub-words.)
the proposed symbolization is applied right after the initial id121.
in the case of using
bpe, the bpe segmentation is done on the symbolized corpus, and in the test time, the bpe de-
segmentation is followed by the de-symbolization.

4 http://www.statmt.org/wmt14/
5 https://github.com/moses-smt/mosesdecoder

8

beam width
baseline
+context (c)
+symbol (s)
+c+s
+bpe
+bpe+c
+bpe+c+s

en-fr

1

12

1

en-de

12

24.54 (27.02)
27.62 (29.01)
27.05 (29.51)
28.58 (31.22)
27.05 (29.10)
29.65 (31.39)
29.32 (31.33)

27.62 (29.21)
28.87 (30.72)
29.39 (31.83)
30.10 (33.30)
30.35 (32.32)
32.02 (33.80)
31.19 (33.83)

15.01 (15.84)
16.24 (16.68)
17.78 (17.55)
18.87 (18.34)
17.43 (17.51)
19.09 (18.84)
19.54 (19.48)

15.93 (17.05)
17.82 (17.89)
19.45 (19.00)
20.79 (19.55)
20.51 (19.97)
21.95 (20.81)
21.99 (20.97)

table 4: id7 scores on the test sets for en-fr and en-de with two different beam widths. the
scores on the development sets are in the parentheses.

aside the bpe segmentation, we noticed that the number of unique tokens on each corpus greatly
decreases after the proposed symbolization is applied. this improves the coverage by the 30k-word-
large vocabulary as much as 3 percentage points. see table 3 for detailed statistics.

5.3 training

we use the very same attention-based neural translation model from bahdanau et al. (2015). the
only change we make is to use long short-term memory units (lstm) instead of id149
(gru). further, instead of adadelta zeiler (2012), we use adam kingma and ba (2014) for adaptive
learning rate adjustment with gradient clipping (threshold at 1.) as the proposed contextualization
and symbolization do not alter the internals of the id4 model, we use this
model and training con   guration for all the experiments.
we remove any sentence pair if more than 10% and 30% of all the words are out-of-vocabulary after
the symbolization for en-fr and en-de, respectively. also, we only use sentence pairs of which
both sentences are only up to 50 symbols long. we early-stop any training when the id7 score on
a development set does not improve any more.

5.4 evaluation

a simple forward beamsearch is used to approximately    nd the most likely translation given a source
sentence from a trained model. we set the width of the beam to 12 following bahdanau et al. (2015).
the generated translations are scored against the reference translations using    multi-id7.perl    script
from moses.6 this evaluation is done on tokenized sentences.

6 result and analysis

6.1 quantitative analysis

in table 4, we present the translation qualities, measured by id7 on the test sets, of all the trained
models on both en-fr and en-de. the most noticeable observation is that both of the proposed
methods   contextualization and symbolization    improve the translation quality over the baseline
model which is a vanilla attention-based neural translation system (+context and +symbol.) fur-
thermore, the proposed contextualization and symbolization are complementary to each other, and
the most improvement is made when both of them are used together (baseline+context+symbol.)
as expected, the proposed symbolization has a similar effect as the bpe segmentation does (base-
line+symbol vs. baseline+bpe) on both of the language pairs. we observe that the proposed con-
textualization, which was found to be complementary to the symbolization, improves the translation
quality even when the bpe-base subword symbols were used.

6 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/

multi-id7.perl

9

6.2 effect of contextualization

we conjectured earlier in sec. 3.3 that the contextualization helps translation by selectively masking
out irrelevant some dimensions of word meaning based on the context. this directly implies that
the neural translation system with the contextualization will be less confused among many similar
words. browsing through the translations of the source sentences included in the development and
test sets, we have observed many such cases, and the following is one such example:

the scorsese-dicaprio duo seems to have rediscovered the magic that brought
them together in shutter island .

the above sentence is translated by the neural translation system with bpe into

le duo de corsso-dicaprio semble avoir retrouv  e la magie qui les a r  eunis dans
l       le shuttle .

meanwhile, the same sentence is translated into

le duo scorson-dicaprio semble avoir red  ecouvert la magie qui les rassemblait
dans l&apos;     le de shutter .

when the contextualization was used.
without the contextualization, the word rediscovered was translated into    retrouv  e    (   found    ac-
cording to collins french-english dictionary7). this is not an incorrect translation, but the transla-
tion of this word into    red  ecouvert    (   rediscovered    according to the same source8) by the contex-
tualized model is closer to the original english word. this is an example of the source-side context
disambiguating the target word.
let us provide another example in the case of en-de. the following source sentence

there are 13,435 cumulative cases (12,158 aids and 1,317 hiv).

is translated by the neural translation model with bpe into

es gibt 13.335 kumulierte f  alle (12,158 aidg und 1,317 hiv).

the same sentence is translated by the same model with the contextualization into

es gibt 13.435 kumulative f  alle (12.158 aids und 1.317 hiv).

notice that the tokens 13,435 and 13.335 are close to each other in the id27 space,
as they represent close-by numbers. this makes it dif   cult for a vanilla id4
model to distinguish between those two. the contextualization, as seen above, can successfully
disambiguate    13.435    from    13.335    and correctly puts the former.

6.3 effect of symbolization

as brie   y mentioned earlier in sec. 5.2, it is known that id4 transliterates
rare (proper) nouns using bpe-based subword symbols. this is however not perfect, as the rules for
id68 exhibit many exceptions, and statistical generalization often fails to account for rare
proper nouns. for instance, the following sentence

trakhtenberg was the presenter of many programs before hali-gali times.

is translated by the model using bpe-based subword symbols into

7 http://www.collinsdictionary.com/dictionary/french-english/retrouver
8

http://www.collinsdictionary.com/dictionary/french-english/red%c3%

a9couvrir

10

trakttenberg war der moderator vieler programme vor hali-gi male.

it is easy to notice that the model has failed at correctly transliterating    trakhtenberg   . on the
contrary, once the symbolization is used (together with the contextualization), the model correctly
translates the source sentence above into

trakhtenberg war der moderator von vielen programmen vor hali-gali -
zeiten.

a similar behaviour is also observed with    hali-gali   . this example clearly demonstrates the effec-
tiveness of the proposed symbolization, even when the bpe-based subword symbols are used.
the advantage of the symbolization is more apparent when words are used as basic symbols instead
of bpe-based subword symbols. the same source sentence is translated into

unk war der moderator von vielen programmen vor unk .

using the word-based translation model with the contextualization only. by including the symbol-
ization as pre- and post-processing routines, we get

trakhtenberg war der moderator vieler programme , bevor hali-gali -zeiten
unk .

it is clear that both of those proper nouns are correctly translated.

7 conclusions

in this paper, we discussed the shortcoming of the existing id4 in terms of
how a word or a symbol is represented as a continuous vector and fed to recurrent networks. based
on the observation that a id27 vector encodes multiple dimensions of similarities, we
proposed to contextualize it by adaptively masking out each dimension of the source and target
embedding vectors based on the context of a source sentence. in addition to the contextualization
of the id27 vectors, we also propose to symbolize special tokens to facilitate translating
those tokens that are not well-suited for translating via continuous vectors.
the experiments on en-fr and en-de revealed that both the contextualization and symbolization
techniques improve the translation quality of id4 signi   cantly. furthermore,
we con   rmed that these approaches are agnostic to the type of linguistic symbols used to repre-
sent source and target sentences by using byte-pair encoding to segment source and target sen-
tences. especially, the proposed contextualization was found to be complementary to the use of
bpe-based subword symbols, and we    nd it an interesting future work to test the contextualization
with character-level id4 ling et al. (2015); chung et al. (2016).

acknowledgments

the authors would like to thank the developers of theano bastien et al. (2012). we acknowledge
the support of the following agencies for research funding and computing support: nserc, cal-
cul qu  ebec, compute canada, the canada research chairs, cifar and samsung. kc thanks the
support by facebook and google (google faculty award 2016).

references
bahdanau, d., k. cho, and y. bengio, 2015: id4 by jointly learning to

align and translate. in proc. int   l conf. on learning representations (iclr).

bastien, f., p. lamblin, r. pascanu, j. bergstra, i. goodfellow, a. bergeron, n. bouchard, d. warde-
farley, and y. bengio, 2012: theano : new features and speed improvements. arxiv preprint
arxiv:1211.5590.

11

bengio, y., r. ducharme, and p. vincent, 2003: a neural probabilistic language model. the jour-

nal of machine learning research, 3, 1137   1155.

brown, p. f., v. j. d. pietra, s. a. d. pietra, and r. l. mercer, 1993: the mathematics of statistical

machine translation: parameter estimation. computational linguistics, 19(2), 263   311.

cho, k., 2015: natural language understanding with distributed representation. arxiv preprint

arxiv:1511.07916.

cho, k., b. van merrienboer, d. bahdanau, and y. bengio, 2014a: on the properties of neural

machine translation: encoder   decoder approaches. arxiv preprint arxiv:1409.1259.

cho, k., b. van merrienboer, c. gulcehre, f. bougares, h. schwenk, and y. bengio, 2014b: learn-
ing phrase representations using id56 encoder-decoder for id151. in con-
ference on empirical methods in natural language processing (emnlp 2014).

chung, j., k. cho, and y. bengio, 2016: a character-level decoder without explicit segmentation

for id4. arxiv preprint arxiv:1603.06147.

collobert, r., j. weston, l. bottou, m. karlen, k. kavukcuoglu, and p. kuksa, 2011: natural
language processing (almost) from scratch. the journal of machine learning research, 12, 2493   
2537.

firat, o., k. cho, and y. bengio, 2016: multi-way, multilingual id4 with a

shared attention mechanism. arxiv preprint arxiv:1601.01073.

hill, f., k. cho, a. korhonen, and y. bengio, 2015: learning to understand phrases by embedding

the dictionary. arxiv preprint arxiv:1504.00548.

hochreiter, s. and j. schmidhuber, 1997: long short-term memory. neural computation, 9(8),

1735   80.

jean, s., k. cho, r. memisevic, and y. bengio, 2015a: on using very large target vocabulary
for id4. in 53rd annual meeting of the association for computational
linguistics.

jean, s., o. firat, k. cho, r. memisevic, and y. bengio, 2015b: montreal id4
systems for wmt15. in proceedings of the tenth workshop on id151, pp.
134   140.

kalchbrenner, n. and p. blunsom, 2013: recurrent continuous translation models. emnlp, 3(39),

413.

kingma, d. p. and j. l. ba, 2014: adam: a method for stochastic optimization. arxiv preprint

arxiv:1412.6980.

koehn, p., 2010: id151. id151, cambridge uni-

versity press, isbn 9780521874151.

koehn, p., f. j. och, and d. marcu, 2003: statistical phrase-based translation. in proceedings of the
2003 conference of the north american chapter of the association for computational linguistics
on human language technology-volume 1, association for computational linguistics, pp. 48   
54.

ling, w., i. trancoso, c. dyer, and a. w. black, 2015: character-based id4.

arxiv preprint arxiv:1511.04586.

luong, m.-t., h. pham, and c. d. manning, 2015a: effective approaches to attention-based neural

machine translation. arxiv preprint arxiv:1508.04025.

luong, m.-t., i. sutskever, q. v. le, o. vinyals, and w. zaremba, 2015b: addressing the rare

word problem in id4. arxiv preprint arxiv:1410.8206.

miikkulainen, r. and m. g. dyer, 1991: natural language processing with modular neural networks

and distributed lexicon. cognitive science, 15, 343   399.

mikolov, t., g. corrado, k. chen, and j. dean, 2013: ef   cient estimation of word representations

in vector space. in proc. int   l conf. on learning representations (iclr).

sennrich, r., b. haddow, and a. birch, 2015a: improving id4 models with

monolingual data. arxiv preprint arxiv:1511.06709.

   , 2015b: id4 of rare words with subword units. arxiv preprint

arxiv:1508.07909.

12

shen, s., y. cheng, z. he, w. he, h. wu, m. sun, and y. liu, 2015: minimum risk training for

id4. arxiv preprint arxiv:1512.02433.

sutskever, i., o. vinyals, and q. v. le, 2014: sequence to sequence learning with neural networks.

in advances in neural information processing systems (nips).

turian, j., l. ratinov, and y. bengio, 2010: word representations: a simple and general method
for semi-supervised learning. in proceedings of the 48th annual meeting of the association for
computational linguistics, association for computational linguistics, pp. 384   394.

van der maaten, l. and g. hinton, 2012: visualizing non-metric similarities in multiple maps.

machine learning, 87(1), 33   55.

weaver, w., 1949: translation. john wiley and sons.
zeiler, m. d., 2012: adadelta: an adaptive learning rate method. arxiv preprint arxiv:1212.5701.

13

