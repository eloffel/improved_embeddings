improving coreference resolution by learning entity-level distributed

representations

kevin clark

computer science department

stanford university

christopher d. manning

computer science department

stanford university

6
1
0
2

 

n
u
j
 

8

 
 
]
l
c
.
s
c
[
 
 

2
v
3
2
3
1
0

.

6
0
6
1
:
v
i
x
r
a

kevclark@cs.stanford.edu

manning@cs.stanford.edu

abstract

a long-standing challenge in coreference
resolution has been the incorporation of
entity-level information     features de   ned
over clusters of mentions instead of men-
tion pairs. we present a neural net-
work based coreference system that pro-
duces high-dimensional vector represen-
tations for pairs of coreference clusters.
using these representations, our system
learns when combining clusters is de-
sirable. we train the system with a
learning-to-search algorithm that teaches
it which local decisions (cluster merges)
will lead to a high-scoring    nal corefer-
ence partition. the system substantially
outperforms the current state-of-the-art on
the english and chinese portions of the
conll 2012 shared task dataset despite
using few hand-engineered features.

introduction

1
coreference resolution,
the task of identifying
which mentions in a text refer to the same real-
world entity, is fundamentally a id91 prob-
lem. however, many recent state-of-the-art coref-
erence systems operate solely by linking pairs
of mentions together (durrett and klein, 2013;
martschat and strube, 2015; wiseman et al.,
2015).

an alternative approach is to use agglomera-
tive id91, treating each mention as a single-
ton cluster at the outset and then repeatedly merg-
ing clusters of mentions deemed to be referring
to the same entity. such systems can take advan-
tage of entity-level information, i.e., features be-
tween clusters of mentions instead of between just
two mentions. as an example for why this is use-
ful, it is clear that the clusters {bill clinton} and

{clinton, she} are not referring to the same entity,
but it is ambiguous whether the pair of mentions
bill clinton and clinton are coreferent.

previous work has incorporated entity-level in-
formation through features that capture hard con-
straints like having gender or number agreement
between clusters (raghunathan et al., 2010; dur-
rett et al., 2013). in this work, we instead train a
deep neural network to build distributed represen-
tations of pairs of coreference clusters. this cap-
tures entity-level information with a large number
of learned, continuous features instead of a small
number of hand-crafted categorical ones.

using the cluster-pair representations, our net-
work learns when combining two coreference
clusters is desirable. at test time it builds up coref-
erence clusters incrementally, starting with each
mention in its own cluster and then merging a pair
of clusters each step. it makes these decisions with
a novel easy-   rst cluster-ranking procedure that
combines the strengths of cluster-ranking (rah-
man and ng, 2011) and easy-   rst (stoyanov and
eisner, 2012) coreference algorithms.

training incremental coreference systems is
challenging because the coreference decisions fac-
ing a model depend on previous decisions it
has already made. we address this by using a
learning-to-search algorithm inspired by searn
(daum  e iii et al., 2009) to train our neural net-
work. this approach allows the model to learn
which action (a cluster merge) available from the
current state (a partially completed coreference
id91) will eventually lead to a high-scoring
coreference partition.

our system uses little manual feature engineer-
ing, which means it is easily extended to multiple
languages. we evaluate our system on the english
and chinese portions of the conll 2012 shared
task dataset. the cluster-ranking model signi   -
cantly outperforms a mention-ranking model that

does not use entity-level information. we also
show that using an easy-   rst strategy improves the
performance of the cluster-ranking model. our    -
nal system achieves conll f1 scores of 65.29 for
english and 63.66 for chinese, substantially out-
performing other state-of-the-art systems.1

2 system architecture

our cluster-ranking model is a single neural net-
work that learns which coreference cluster merges
are desirable. however, it is helpful to think of
the network as being composed of distinct sub-
networks. the mention-pair encoder produces
distributed representations for pairs of mentions
by passing relevant features through a feedforward
neural network. the cluster-pair encoder pro-
duces distributed representations for pairs of clus-
ters by applying a pooling operation over the rep-
resentations of relevant mention pairs, i.e., pairs
where one mention is in each cluster. the cluster-
ranking model then scores pairs of clusters by
passing their representations through a single neu-
ral network layer.

we also train a mention-ranking model that
scores pairs of mentions by passing their repre-
sentations through a single neural network layer.
its parameters are used to initialize the cluster-
ranking model, and the scores it produces are
used to prune which candidate cluster merges
the cluster-ranking model considers, allowing the
cluster-ranking model to run much faster. the sys-
tem architecture is summarized in figure 1.

figure 1: system architecture. solid arrows indi-
cate one neural network is used as a component of
the other; the dashed arrow indicates other depen-
dencies.

3 building representations

in this section, we describe the neural networks
producing distributed representations of pairs of

1code and trained models are available at https://

github.com/clarkkev/deep-coref.

figure 2: mention-pair encoder.

mentions and pairs of coreference clusters. we as-
sume that a set of mentions has already been ex-
tracted from each document using a method such
as the one in raghunathan et al. (2010).

3.1 mention-pair encoder
given a mention m and candidate antecedent a,
the mention-pair encoder produces a distributed
representation of the pair rm(a, m)     rd with a
feedforward neural network, which is shown in
figure 2. the candidate antecedent may be any
mention that occurs before m in the document
or na, indicating that m has no antecedent. we
also experimented with models based on long
short-term memory recurrent neural networks
(hochreiter and schmidhuber, 1997), but found
these to perform slightly worse when used in
an end-to-end coreference system due to heavy
over   tting to the training data.

input layer. for each mention, the model ex-
tracts various words and groups of words that
are fed into the neural network.
each word
is represented by a vector wi     rdw. each
group of words is represented by the average
of the vectors of each word in the group. for
each mention and pair of mentions, a small
number of binary features and distance fea-
tures are also extracted. distances and men-
tion lengths are binned into one of the buck-
ets [0, 1, 2, 3, 4, 5-7, 8-15, 16-31, 32-63, 64+] and
then encoded in a one-hot vector in addition to be-
ing included as continuous features. the full set
of features is as follows:

embedding features: id27s of the
head word, dependency parent,    rst word, last
word, two preceding words, and two following
words of the mention. averaged word embed-
dings of the    ve preceding words,    ve following

																														mention-pair encoder cluster-pair encoder cluster-ranking model mention-ranking model pretraining, search space pruning 																														candidate antecedent  embeddings candidate antecedent  features  mention features  mention embeddings hidden layer h2 mention-pair representation rm 	input layer h0 hidden layer h1 relu(w1h0 + b1) relu(w2h1 + b2)  relu(w3h2 + b3)  pair and document features  words, all words in the mention, all words in the
mention   s sentence, and all words in the mention   s
document.

additional mention features: the type of the
mention (pronoun, nominal, proper, or list), the
mention   s position (index of the mention divided
by the number of mentions in the document),
whether the mentions is contained in another men-
tion, and the length of the mention in words.

document genre: the genre of the mention   s doc-
ument (broadcast news, newswire, web data, etc.).

distance features: the distance between the men-
tions in sentences, the distance between the men-
tions in intervening mentions, and whether the
mentions overlap.

speaker features: whether the mentions have the
same speaker and whether one mention is the other
mention   s speaker as determined by string match-
ing rules from raghunathan et al. (2010).

string matching features: head match, exact
string match, and partial string match.

the vectors for all of these features are concate-
nated to produce an i-dimensional vector h0, the
input to the neural network. if a = na, the fea-
tures de   ned over mention pairs are not included.
for this case, we train a separate network with an
identical architecture to the pair network except
for the input layer to produce anaphoricity scores.
our set of hand-engineered features is much
smaller than the dozens of complex features typ-
ically used in coreference systems. however, we
found these features were crucial for getting good
model performance. see section 6.1 for a feature
ablation study.

hidden layers. the input gets passed through
three hidden layers of recti   ed linear (relu) units
(nair and hinton, 2010). each unit in a hidden
layer is fully connected to the previous layer:

hi(a, m) = max(0, wihi   1(a, m) + bi)

where w1 is a m1    i weight matrix, w2 is a
m2    m1 matrix, and w3 is a d    m2 matrix.

the output of the last hidden layer is the
the mention pair:

representation for

vector
rm(a, m) = h3(a, m).

figure 3: cluster-pair encoder.

1, mi

clusters

two
2, ..., mi|ci|} and cj = {mj

3.2 cluster-pair encoder
given
{mi
2, ..., mj|cj|},
the cluster-pair encoder produces a distributed
representation rc(ci, cj)     r2d. the architecture
of the encoder is summarized in figure 3.

of mentions
1, mj

=

ci

the

cluster-pair

1), rm(mi

encoder    rst

representations rm(ci, cj)

combines
the information contained in the matrix of
mention-pair
=
2), ..., rm(mi|ci|, mj|cj|)]
1, mj
[rm(mi
to produce rc(ci, cj). this is done by applying a
pooling operation.
in particular it concatenates
the results of max-pooling and average-pooling,
which we found to be slightly more effective than
using either one alone:

1, mj

rc(ci, cj)k =

max{rm(ci, cj)k,  }
avg {rm(ci, cj)k   d,  }
4 mention-ranking model

for 0     k < d
for d     k < 2d

(cid:40)

rather than training a cluster-ranking model from
scratch, we    rst train a mention-ranking model
that assigns each mention its highest scoring can-
didate antecedent. there are two key advantages
of doing this. first, it serves as pretraining for the
cluster-ranking model; in particular the mention-
ranking model learns effective weights for the
mention-pair encoder. second, the scores pro-
duced by the mention-ranking model are used to
provide a measure of which coreference decisions
are easy (allowing for an easy-   rst id91 strat-
egy) and which decisions are clearly wrong (these
decisions can be pruned away, signi   cantly reduc-
ing the search space of the cluster-ranking model).
the mention-ranking model assigns a score
sm(a, m) to a mention m and candidate an-

			 																										cluster-pair representation mention-pair representations pooling !!!  c2  c1 mention-pair encoder !!!  !!!  rc(c1, c2) rm(c1, c2) !!!  tecedent a representing their compatibility for
coreference. this is produced by applying a sin-
gle fully connected layer of size one to the repre-
sentation rm(a, m) produced by the mention-pair
encoder:

sm(a, m) = wmrm(a, m) + bm

where wm is a 1    d weight matrix. at test time,
the mention-ranking model links each mention
with its highest scoring candidate antecedent.

training objective. we train the mention-
ranking model with the slack-rescaled max-
margin training objective from wiseman et al.
(2015), which encourages separation between the
highest scoring true and false antecedents of the
current mention. suppose the training set consists
of n mentions m1, m2, ..., mn . let a(mi) de-
note the set of candidate antecedents of a men-
tion mi (i.e., mentions preceding mi and na), and
t (mi) denote the set of true antecedents of mi
(i.e., mentions preceding mi that are coreferent
with it or {na} if mi has no antecedent). let   ti
be the highest scoring true antecedent of mention
mi:

  ti = argmax
t   t (mi)
then the loss is given by

sm(t, mi)

   (a, mi)(1 + sm(a, mi)     sm(  ti, mi))

max
a   a(mi)

where    (a, mi) is the mistake-speci   c cost func-
tion

n(cid:80)

i=1

                                 

  fn
  fa
  wl
0

   (a, mi) =

if a = na     t (mi) (cid:54)= {na}
if a (cid:54)= na     t (mi) = {na}
if a (cid:54)= na     a /    t (mi)
if a     t (mi)

for    false new,       false anaphoric,       wrong link,   
and correct coreference decisions. the different
error penalties allow the system to be tuned
for coreference id74 by biasing it
towards making more or fewer coreference links.

finding effective error penalties. we    x
  wl = 1.0 and search for   fa and   fn out of
{0.1, 0.2, ..., 1.5} with a variant of grid search.
each new trial uses the unexplored set of hy-
perparameters that has the closest manhattan

distance to the best setting found so far on
the dev set. we stopped the search when all
immediate neighbors (within 0.1 distance) of
the best setting had been explored. we found
(  fn,   fa,   wl) = (0.8, 0.4, 1.0) to be best for
english and (  fn,   fa,   wl) = (0.7, 0.4, 1.0) to
be best for chinese on the conll 2012 data.
we attribute our smaller false new cost from the
one used by wiseman et al. (they set   fn = 1.2)
to using more precise mention detection, which
results in fewer links to na.

training details. we initialized our word em-
beddings with 50 dimensional ones produced by
id97 (mikolov et al., 2013) on the giga-
word corpus for english and 64 dimensional ones
provided by polyglot (al-rfou et al., 2013) for
chinese. averaged id27s were held
   xed during training while the embeddings used
for single words were updated. we set our hid-
den layer sizes to m1 = 1000, m2 = d = 500
and minimized the training objective using rms-
prop (hinton and tieleman, 2012). to regularize
the network, we applied l2 id173 to the
model weights and dropout (hinton et al., 2012)
with a rate of 0.5 on the id27s and the
output of each hidden layer.
pretraining. as in wiseman et al. (2015), we
found that pretraining is crucial for the mention-
ranking model   s success. we pretrained the net-
work in two stages, minimizing the following ob-
jectives from clark and manning (2015):

i=1

i=1

t   t (mi)

f   f (mi)

f   f (mi)

[ max
t   t (mi)

log p(t, mi) + min

[ (cid:80)

all-pairs classi   cation

log(1     p(f, mi))]

top-pairs classi   cation

log p(t, mi) + (cid:80)

    n(cid:80)
    n(cid:80)
log(1     p(f, mi))]
where f(mi) is the set of false antecedents for mi
and p(a, mi) = sigmoid(s(a, mi)). the top-pairs
objective is a middle ground between the all-pairs
classi   cation and mention ranking objectives:
it
only processes high-scoring mentions, but is prob-
abilistic rather than max-margin. we    rst pre-
trained the network with all-pairs classi   cation for
150 epochs and then with top-pairs classi   cation
for 50 epochs. see section 6.1 for experiments on
the two-stage pretraining.

5 cluster-ranking model
although a strong coreference system on its own,
the mention-ranking model has the disadvantage
of only considering local information between
pairs of mentions, so it cannot consolidate infor-
mation at the entity-level. we address this prob-
lem by training a cluster-ranking model that scores
pairs of clusters instead of pairs of mentions.

given two clusters of mentions ci and cj, the
cluster-ranking model produces a score sc(ci, cj)
representing their compatibility for coreference.
this is produced by applying a single fully con-
nected layer of size one to the representation
rc(ci, cj) produced by the cluster-pair encoder:

sc(ci, cj) = wcrc(ci, cj) + bc

where wc is a 1    2d weight matrix. our
cluster-ranking approach also uses a measure of
anaphoricity, or how likely it is for a mention m to
have an antecedent. this is de   ned as

sna(m) = wnarm(na, m) + bna

where wna is a 1    d matrix.
5.1 cluster-ranking policy network
at test time, the cluster ranker iterates through ev-
ery mention in the document, merging the current
mention   s cluster with a preceding one or perform-
ing no action. we view this procedure as a sequen-
tial decision process where at each step the algo-
rithm observes the current state x and performs
some action u.
speci   cally, we de   ne a state x = (c, m) to
consist of c = {c1, c2, ...}, the set of existing
coreference clusters, and m, the current mention
being considered. at a start state, each cluster in
c contains a single mention. let cm     c be the
cluster containing m and a(m) be a set of candi-
date antecedents for m: mentions occurring previ-
ously in the document. then the available actions
u (x) from x are

    merge[cm, c], where c is a cluster contain-
ing a mention in a(m). this combines cm
and c into a single coreference cluster.

    pass. this leaves the id91 unchanged.
after determining the new id91 c(cid:48) based on
the existing id91 c and action u, we con-
sider another mention m(cid:48) to get the next state
x(cid:48) = (c(cid:48), m(cid:48)).

using the scoring functions sc and sna, we de-
   ne a policy network    that assigns a id203
distribution over u (x) as follows:

  (merge[cm, c]|x)     esc(cm,c)
  (pass|x)     esna(m)

during id136,    is executed by taking the
highest-scoring (most probable) action at each
step.

5.2 easy-first cluster ranking
the last detail needed is the ordering in which
to consider mentions. cluster-ranking models in
prior work order the mentions according to their
positions in the document, processing them left-
to-right (rahman and ng, 2011; ma et al., 2014).
however, we instead sort the mentions in de-
scending order by their highest scoring candidate
coreference link according to the mention-ranking
model. this causes id136 to occur in an easy-
   rst fashion where hard decisions are delayed until
more information is available. easy-   rst orderings
have been shown to improve the performance of
other incremental coreference strategies (raghu-
nathan et al., 2010; stoyanov and eisner, 2012)
because they reduce the problem of errors com-
pounding as the algorithm runs.
we also    nd it bene   cial to prune the set of
candidate antecedents a(m) for each mention m.
rather than using all previously occurring men-
tions as candidate antecedents, we only include
high-scoring ones, which greatly reduces the size
of the search space. this allows for much faster
learning and id136; we are able to remove over
95% of candidate actions with no decrease in the
model   s performance. for both of these two pre-
processing steps, we use s(a, m)     s(na, m) as
the score of a coreference link between a and m.

5.3 deep learning to search
we face a sequential prediction problem where fu-
ture observations (visited states) depend on previ-
ous actions. this is challenging because it violates
the common i.i.d. assumption made in machine
learning. learning-to-search algorithms are effec-
tive for this sort of problem, and have been applied
successfully to coreference resolution (daum  e iii
and marcu, 2005; clark and manning, 2015) as
well as other id170 tasks in natu-
ral language processing (daum  e iii et al., 2014;

algorithm 1 deep learning to search

for i = 1 to num epochs do

initialize the current training set    =    
for each example (x, y)     d do

run the policy    to completion from start state x to obtain a trajectory of states {x1, x2, ..., xn}
for each state xi in the trajectory do

for each possible action u     u (xi) do

execute u on xi and then run the reference policy   ref until reaching an end state e
assign u a cost by computing the loss on the end state: l(u) = l(e, y)

end for
add the state xi and associated costs l to   

end for

update    with id119, minimizing(cid:80)

end for

(cid:80)
u   u (x)   (u|x)l(u)

(x,l)     

end for

chang et al., 2015a).

we train the cluster-ranking model using a
learning-to-search algorithm inspired by searn
(daum  e iii et al., 2009), which is described in al-
gorithm 1. the algorithm takes as input a dataset
d of start states x (in our case documents with
each mention in its own singleton coreference
cluster) and structured labels y (in our case gold
coreference clusters). its goal is to train the pol-
icy    so when it executes from x, reaching a    -
nal state e, the resulting loss l(e, y) is small. we
use the negative of the b3 coreference metric for
this loss (bagga and baldwin, 1998). although
our system evaluation also includes the muc (vi-
lain et al., 1995) and ceaf  4 (luo, 2005) metrics,
we do not incorporate them into the loss because
muc has the    aw of treating all errors equally and
ceaf  4 is slow to compute.

for each example (x, y)     d, the algorithm ob-
tains a trajectory of states x1, x2, ..., xn visited by
the current policy by running it to completion (i.e.,
repeatedly taking the highest scoring action until
reaching an end state) from the start state x. this
exposes the model to states at train time similar to
the ones it will face at test time, allowing it to learn
how to cope with mistakes.

given a state x in a trajectory, the algorithm
then assigns a cost l(u) to each action u     u (x)
by executing the action,    rolling out    from the
resulting state with a reference policy   ref until
reaching an end state e, and computing the result-
ing loss l(e, y). this rolling out procedure allows
the model to learn how a local action will affect the
   nal score, which cannot be otherwise computed
because coreference id74 do not de-

compose over cluster merges. the policy network
is then trained to minimize the risk associated with

taking each action:(cid:80)

u   u (x)   (u|x)l(u).

reference policies typically refer to the gold la-
bels to    nd actions that are likely to be bene   cial.
our reference policy   ref takes the action that in-
creases the b3 score the most each step, break-
ing ties randomly.
it is generally recommended
to use a stochastic mixture of the reference pol-
icy and the current learned policy during rollouts
when the reference policy is not optimal (chang et
al., 2015b). however, we    nd only using the refer-
ence policy (which is close to optimal) to be much
more ef   cient because it does not require neural
network computations and is deterministic, which
means the costs of actions can be cached.

training details. we update    using rmsprop
and apply dropout with a rate of 0.5 to the in-
put layer. for most experiments, we initialize the
mention-pair encoder component of the cluster-
ranking model with the learned weights from the
mention-ranking model, which we    nd to greatly
improve performance (see section 6.2).

runtime. the full cluster-ranking system runs
end-to-end in slightly under 1 second per docu-
ment on the english test set when using a gpu
(including scoring all pairs of mentions with the
mention-ranking model for search-space pruning).
this means the bottleneck for the overall system is
the syntactic parsing required for mention detec-
tion (about 4 seconds per document on the english
test set).

english f1 chinese f1

all-pairs top-pairs english f1 chinese f1

model
full model

    mention
    genre
    distance
    speaker
    matching

65.52
   1.27
   0.25
   2.42
   1.26
   2.07

64.41
   0.74
   2.91
   2.41
   0.93
   3.44

table 1: conll f1 scores of the mention-ranking
model on the dev sets without mention, docu-
ment genre, distance, speaker, and string matching
hand-engineered features.

6 experiments and results

experimental setup. we run experiments on
the english and chinese portions of the conll
2012 shared task data (pradhan et al., 2012).
the models are evaluated using three of the most
popular coreference metrics: muc, b3, and
entity-based ceaf (ceaf  4). we generally
report the average f1 score (conll f1) of the
three, which is common practice in coreference
evaluation. we used the most recent version of the
conll scorer (version 8.01), which implements
the original de   nitions of the metrics.

mention detection. our experiments were run
using system-produced predicted mentions. we
used the rule-based mention detection algorithm
from raghunathan et al.
(2010), which    rst
extracts pronouns and maximal np projections
as candidate mentions and then    lters this set
with rules that remove spurious mentions such as
numeric entities and pleonastic it pronouns.

6.1 mention-ranking model experiments

feature ablations. we performed a feature ab-
lation study to determine the importance of the
hand-engineered features included in our model.
the results are shown in table 1. we    nd the
small number of non-embedding features substan-
tially improves model performance, especially the
distance and string matching features. this is un-
surprising, as the additional features are not eas-
ily captured by id27s and historically
such features have been very important in corefer-
ence resolvers (bengtson and roth, 2008).
the importance of pretraining. we evaluate
the bene   t of the two-step pretraining for the

yes
yes
no
no

yes
no
yes
no

65.52
   0.36
   0.54
   3.58

64.41
   0.24
   0.33
   5.43

table 2: conll f1 scores of the mention-ranking
model on the dev sets with different pretraining
methods.

model
full model

    pretraining
    easy-first
    l2s

english f1 chinese f1

66.01
   5.01
   0.15
   0.32

64.86
   6.85
   0.12
   0.25

table 3: conll f1 scores of the cluster-ranking
model on the dev sets with various ablations.
    pretraining:
initializing model parameters
randomly instead of from the mention-ranking
model,     easy-first: iterating through mentions
in order of occurrence instead of according to their
highest scoring candidate coreference link,     l2s:
training on a    xed trajectory of correct actions in-
stead of using learning to search.

mention-ranking model and report results in ta-
ble 2. consistent with wiseman et al. (2015), we
   nd pretraining to greatly improve the model   s ac-
curacy. we note in particular that the model ben-
e   ts from using both pretraining steps from sec-
tion 4, which more smoothly transitions the model
from a mention-pair classi   cation objective that is
easy to optimize to a max-margin objective better
suited for a ranking task.

6.2 cluster-ranking model experiments
we evaluate the importance of three key details of
the cluster ranker: initializing it with the mention-
ranking model   s weights, using an easy-   rst order-
ing of mentions, and using learning to search. the
results are shown in table 3.
pretrained weights. we compare initializing
the cluster-ranking model randomly with initial-
izing it with the weights learned by the mention-
ranking model. using pretrained weights greatly
improves performance. we believe the cluster-
ranking model has dif   culty learning effective
weights from scratch due to noise in the signal
coming from cluster-level decisions (an overall
bad cluster merge may still involve a few cor-

rect pairwise links) and the smaller amount of
data used to train the cluster-ranking model (many
possible actions are pruned away during prepro-
cessing). we believe the score would be even
lower without search-space pruning, which stops
the model from considering many bad actions.
easy-first cluster ranking. we compare the ef-
fectiveness of easy-   rst cluster-ranking with the
commonly used left-to-right approach. using a
left-to-right strategy simply requires changing the
preprocessing step ordering the mentions so men-
tions are sorted by their position in the document
instead of their highest scoring coreference link
according to the mention-ranking model. we    nd
the easy-   rst approach slightly outperforms us-
ing a left-to-right ordering of mentions. we be-
lieve this is because delaying hard decisions until
later reduces the problem of early mistakes caus-
ing later decisions to be made incorrectly.
learning to search. we also compare learning to
search with the simpler approach of training the
model on a trajectory of gold coreference deci-
sions (i.e., training on a    xed cost-sensitive clas-
si   cation dataset). using this approach signi   -
cantly decreases performance. we attribute this to
the model not learning how to deal with mistakes
when it only sees correct decisions during training.

6.3 capturing semantic similarity
using semantic information to improve corefer-
ence accuracy has had mixed in results in previous
research, and has been called an    uphill battle    in
coreference resolution (durrett and klein, 2013).
however, id27s are well known for
being effective at capturing semantic relatedness,
and we show here that neural network coreference
models can take advantage of this.

perhaps the case where semantic similarity is
most important is in linking nominals with no head
match (e.g.,    the nation    and    the country   ). we
compare the performance of our neural network
model with our earlier statistical system (clark
and manning, 2015) at classifying mention pairs
of this type as being coreferent or not. the neu-
ral network shows substantial improvement (18.9
f1 vs. 10.7 f1) on this task compared to the more
modest improvement it gets at classifying any pair
of mentions as coreferent (68.7 f1 vs. 66.1 f1).
some example wins are shown in table 4. these
types of coreference links are quite rare in the
conll data (about 1.2% of the positive coref-

anaphor

antecedent
the country   s leftist rebels the guerrillas
the company
the suicide bombing
the gun
the u.s. carrier

the new york    rm
the attack
the ri   e
the ship

table 4: examples of nominal coreferences with
no head match that the neural model gets correct,
but the system from clark and manning (2015)
gets incorrect.

erence links in the test set), so the improvement
does not signi   cantly contribute to the    nal sys-
tem   s score, but it does suggest progress on this
dif   cult type of coreference problem.

6.4 final system performance
in table 5 we compare the results of our system
with state-of-the-art approaches for english and
chinese. our mention-ranking model surpasses
all previous systems. we attribute its improvement
over the neural mention ranker from wiseman et
al. (2015) to our model using a deeper neural net-
work, pretrained id27s, and more so-
phisticated pretraining.

information.

the improvement

the cluster-ranking model improves results fur-
ther across both languages and all evaluation met-
rics, demonstrating the utility of incorporating
entity-level
is
largest in ceaf  4, which is encouraging because
ceaf  4 is the most recently proposed metric, de-
signed to correct    aws in the other two (luo,
2005). we believe entity-level information is par-
ticularly useful for preventing bad merges between
large clusters (see figure 4 for an example). how-
ever, it is worth noting that in practice the much
more complicated cluster-ranking model brings
only fairly modest gains in performance.

7 related work

there has been extensive work on machine learn-
ing approaches to coreference resolution (soon et
al., 2001; ng and cardie, 2002), with mention-
ranking models being particularly popular (denis
and baldridge, 2007; durrett and klein, 2013;
martschat and strube, 2015).

we train a neural mention-ranking model in-
spired by wiseman et al. (2015) as a starting point,
but then use it to pretrain a cluster-ranking model
that bene   ts from entity-level information. wise-

prec.

muc

rec.

f1

prec.

b3
rec.

f1

prec.

ceaf  4
rec.

f1

avg. f1

conll 2012 english test data

76.12

69.38

65.64

56.01

59.44

52.98

clark and manning (2015)
peng et al. (2015)
wiseman et al. (2015)
wiseman et al. (2016)
nn mention ranker
nn cluster ranker

   

76.23
77.49
79.77
78.93

   

69.31
69.75
69.10
69.75

72.59
72.22
72.60
73.42
74.05
74.06

   

66.07
66.83
69.68
70.08

   

55.83
56.95
56.37
56.98

60.44
60.50
60.52
61.50
62.32
62.86

   

59.41
62.14
63.02
62.48

   

54.88
53.85
53.59
55.82

conll 2012 chinese test data

chen & ng (2012)
bj  orkelund & kuhn (2014)
nn mention ranker
nn cluster ranker

64.69
69.39
72.53
73.85

59.92
62.57
65.72
65.42

62.21
65.80
68.96
69.38

60.26
61.64
65.49
67.53

51.76
53.87
56.87
56.41

55.69
57.49
60.88
61.47

51.61
59.33
61.93
62.84

58.84
54.65
57.11
57.62

56.02
56.37
57.05
57.70
57.92
58.96

54.99
56.89
59.42
60.12

63.02
63.03
63.39
64.21
64.76
65.29

57.63
60.06
63.09
63.66

table 5: comparison with the current state-of-the-art approaches on the conll 2012 test sets. nn
mention ranker and nn cluster ranker are contributions of this work.

figure 4: thanks to entity-level information, the cluster-ranking model correctly declines to merge these
two large clusters when running on the test set. however, the mention-ranking model incorrectly links
the russian president and president clinton   s, which greatly reduces the    nal precision score.

man et al. (2016) extend their mention-ranking
model by incorporating entity-level information
produced by a recurrent neural network running
over the candidate antecedent-cluster. however,
this is an augmentation to a mention-ranking
model, and not fundamentally a id91 model
as our cluster ranker is.

entity-level information has also been incorpo-
rated in coreference systems using joint id136
(mccallum and wellner, 2003; poon and domin-
gos, 2008; haghighi and klein, 2010) and systems
that build up coreference clusters incrementally
(luo et al., 2004; yang et al., 2008; raghunathan
et al., 2010). we take the latter approach, and
in particular combine the cluster-ranking (rah-
man and ng, 2011; ma et al., 2014) and easy-   rst
(stoyanov and eisner, 2012; clark and manning,
2015) id91 strategies. these prior systems
all express entity-level information in the form of
hand-engineered features and constraints instead
of entity-level distributed representations that are
learned from data.

we train our system using a learning-to-search
algorithm similar to searn (daum  e iii et al.,
2009). learning-to-search style algorithms have
been employed to train coreference resolvers on
trajectories of decisions similar to those that would

be seen at test-time by daum  e et al. (2005), ma et
al. (2014), and clark and manning (2015). other
works use structured id88 models for the
same purpose (stoyanov and eisner, 2012; fer-
nandes et al., 2012; bj  orkelund and kuhn, 2014).

8 conclusion

we have presented a coreference system that cap-
tures entity-level information with distributed rep-
resentations of coreference cluster pairs. these
learned, dense, high-dimensional feature vectors
provide our cluster-ranking coreference model
with a strong ability to distinguish bene   cial clus-
ter merges from harmful ones. the model is
trained with a learning-to-search algorithm that al-
lows it to learn how local decisions will affect
the    nal coreference score. we evaluate our sys-
tem on the english and chinese portions of the
conll 2012 shared task and report a substantial
improvement over the current state-of-the-art.

acknowledgments

we thank will hamilton, jon gauthier, and the
anonymous reviewers for their thoughtful com-
ments and suggestions. this work was supported
by nsf award iis-1514268.

																														russian president vladimir putin, his, the russian  president,  president clinton   s, bill clinton, mr. clinton   s he     , { { } } incorrect link predicted by the mention-ranking model     ,     , references
[al-rfou et al.2013] rami al-rfou, bryan perozzi, and
steven skiena. 2013. polyglot: distributed word
representations for multilingual nlp. conference
on natural language learning (conll), pages
183   192.

[bagga and baldwin1998] amit bagga

and breck
baldwin. 1998. algorithms for scoring coreference
in the first international conference on
chains.
language resources and evaluation workshop on
linguistics coreference, pages 563   566.

[bengtson and roth2008] eric bengtson and dan roth.
2008. understanding the value of features for coref-
erence resolution. in empirical methods in natural
language processing (emnlp), pages 294   303.

[bj  orkelund and kuhn2014] anders bj  orkelund and
jonas kuhn. 2014. learning structured id88s
for coreference resolution with latent antecedents
in association of compu-
and non-local features.
tational linguistics (acl), pages 47   57.

[chang et al.2015a] kai-wei chang, he he, hal
2015a. learn-
arxiv preprint

daum  e iii, and john langford.
ing to search for dependencies.
arxiv:1503.05615.

[chang et al.2015b] kai-wei chang, akshay krishna-
murthy, alekh agarwal, hal daum  e iii, and john
langford. 2015b. learning to search better than
in international conference on ma-
your teacher.
chine learning (icml).

[chen and ng2012] chen chen and vincent ng. 2012.
combining the best of two worlds: a hybrid ap-
proach to multilingual coreference resolution.
in
proceedings of the joint conference on empirical
methods in natural language processing and con-
ference on computational natural language learn-
ing - shared task, pages 56   63.

[clark and manning2015] kevin clark and christo-
pher d. manning. 2015. entity-centric coreference
resolution with model stacking. in association for
computational linguistics (acl).

[daum  e iii and marcu2005] hal daum  e iii and daniel
marcu. 2005. a large-scale exploration of effec-
tive global features for a joint entity detection and
in empirical methods in natural
tracking model.
language processing (emnlp), pages 97   104.

[daum  e iii et al.2009] hal daum  e iii, john langford,
and daniel marcu. 2009. search-based structured
prediction. machine learning, 75(3):297   325.

[daum  e iii et al.2014] hal daum  e iii, john langford,
and stephane ross. 2014. ef   cient programmable
learning to search. arxiv preprint arxiv:1406.1837.

[durrett and klein2013] greg durrett and dan klein.
2013. easy victories and uphill battles in corefer-
in empirical methods in natural
ence resolution.
language processing (emnlp), pages 1971   1982.

[durrett et al.2013] greg durrett, david leo wright
hall, and dan klein. 2013. decentralized entity-
level modeling for coreference resolution. in asso-
ciation for computational linguistics (acl), pages
114   124.

[fernandes et al.2012] eraldo rezende

fernandes,
c    cero nogueira dos santos, and ruy luiz milidi  u.
2012.
latent structure id88 with feature
induction for unrestricted coreference resolution. in
proceedings of the joint conference on empirical
methods in natural language processing and
conference on computational natural language
learning - shared task, pages 41   48.

[haghighi and klein2010] aria haghighi

and dan
klein. 2010. coreference resolution in a modu-
in human language
lar, entity-centered model.
technology and north american association for
computational linguistics (hlt-naacl), pages
385   393.

[hinton and tieleman2012] geoffrey hinton and tij-
men tieleman. 2012. lecture 6.5-rmsprop: divide
the gradient by a running average of its recent mag-
nitude. coursera: neural networks for machine
learning, 4.

[hinton et al.2012] geoffrey e hinton, nitish srivas-
tava, alex krizhevsky, ilya sutskever, and rus-
lan r salakhutdinov. 2012. improving neural net-
works by preventing co-adaptation of feature detec-
tors. arxiv preprint arxiv:1207.0580.

[hochreiter and schmidhuber1997] sepp hochreiter
and j  urgen schmidhuber. 1997. long short-term
memory. neural computation, 9(8):1735   1780.

ittycheriah,
[luo et al.2004] xiaoqiang luo, abe
hongyan jing, nanda kambhatla,
and salim
roukos. 2004. a mention-synchronous coreference
resolution algorithm based on the bell tree.
in
association for computational linguistics (acl),
page 135.

[luo2005] xiaoqiang luo. 2005. on coreference reso-
lution performance metrics. in empirical methods
in natural language processing (emnlp), pages
25   32.

[ma et al.2014] chao ma,

janardhan rao doppa,
j walker orr, prashanth mannem, xiaoli fern, tom
dietterich, and prasad tadepalli. 2014. prune-and-
score: learning for greedy coreference resolution.
in empirical methods in natural language process-
ing (emnlp).

[denis and baldridge2007] pascal denis and jason
baldridge. 2007. a ranking approach to pronoun
in international joint conferences on
resolution.
arti   cial intelligence (ijcai), pages 1588   1593.

[martschat and strube2015] sebastian martschat and
michael strube. 2015. latent structures for corefer-
ence resolution. transactions of the association for
computational linguistics (tacl), 3:405   418.

[mccallum and wellner2003] andrew mccallum and
ben wellner. 2003. toward conditional models of
identity uncertainty with application to proper noun
coreference. in proceedings of the ijcai workshop
on information integration on the web.

[vilain et al.1995] marc vilain, john burger, john ab-
erdeen, dennis connolly, and lynette hirschman.
a model-theoretic coreference scoring
1995.
in proceedings of the 6th conference on
scheme.
message understanding, pages 45   52.

[wiseman et al.2015] sam wiseman, alexander m.
rush, stuart m. shieber, and jason weston. 2015.
learning anaphoricity and antecedent ranking fea-
in association of
tures for coreference resolution.
computational linguistics (acl), pages 92   100.

[wiseman et al.2016] sam wiseman, alexander m.
rush, and stuart m. shieber. 2016. learning global
features for coreference resolution. in human lan-
guage technology and north american association
for computational linguistics (hlt-naacl).

[yang et al.2008] xiaofeng yang, jian su, jun lang,
chew lim tan, ting liu, and sheng li.
2008.
an entity-mention model for coreference resolution
with inductive logic programming. in association of
computational linguistics (acl), pages 843   851.

[mikolov et al.2013] tomas mikolov, ilya sutskever,
kai chen, greg s corrado, and jeff dean. 2013.
distributed representations of words and phrases
in advances in neu-
and their compositionality.
ral information processing systems (nips), pages
3111   3119.

[nair and hinton2010] vinod nair and geoffrey e.
hinton. 2010. recti   ed linear units improve re-
stricted id82s. in international con-
ference on machine learning (icml), pages 807   
814.

[ng and cardie2002] vincent ng and claire cardie.
2002.
improving machine learning approaches to
coreference resolution. in association of computa-
tional linguistics (acl), pages 104   111.

[peng et al.2015] haoruo peng, kai-wei chang, and
dan roth. 2015. a joint framework for coreference
resolution and mention head detection. conference
on natural language learning (conll), 51:12.

[poon and domingos2008] hoifung poon and pedro
domingos. 2008. joint unsupervised coreference
resolution with markov logic. in empirical methods
in natural language processing (emnlp), pages
650   659.

[pradhan et al.2012] sameer pradhan, alessandro mos-
chitti, nianwen xue, olga uryupina, and yuchen
zhang. 2012. conll-2012 shared task: modeling
multilingual unrestricted coreference in ontonotes.
in proceedings of the joint conference on empirical
methods in natural language processing and con-
ference on computational natural language learn-
ing - shared task, pages 1   40.

[raghunathan et al.2010] karthik raghunathan, heey-
oung lee, sudarshan rangarajan, nathanael cham-
bers, mihai surdeanu, dan jurafsky, and christo-
pher manning. 2010. a multi-pass sieve for coref-
erence resolution. in empirical methods in natural
language processing (emnlp), pages 492   501.

[rahman and ng2011] altaf rahman and vincent ng.
2011. narrowing the modeling gap: a cluster-
ranking approach to coreference resolution. jour-
nal of arti   cial intelligence research (jair), pages
469   521.

[soon et al.2001] wee meng soon, hwee tou ng, and
daniel chung yong lim. 2001. a machine learning
approach to coreference resolution of noun phrases.
computational linguistics, 27(4):521   544.

[stoyanov and eisner2012] veselin stoyanov and jason
eisner.
2012. easy-   rst coreference resolution.
in international conference on computational lin-
guistics (coling), pages 2519   2534.

