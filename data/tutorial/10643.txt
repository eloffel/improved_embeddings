7
1
0
2

 

n
a
j
 

7
2

 
 
]
l
c
.
s
c
[
 
 

2
v
1
8
7
1
0

.

6
0
6
1
:
v
i
x
r
a

very deep convolutional networks

for text classi   cation

alexis conneau

facebook ai research
aconneau@fb.com

holger schwenk

facebook ai research
schwenk@fb.com

yann le cun

facebook ai research

yann@fb.com

lo    c barrault

lium, university of le mans, france

loic.barrault@univ-lemans.fr

abstract

the dominant approach for many nlp
tasks are recurrent neural networks, in par-
ticular lstms, and convolutional neural
networks. however, these architectures
are rather shallow in comparison to the
deep convolutional networks which have
pushed the state-of-the-art in computer vi-
sion. we present a new architecture (vd-
id98) for text processing which operates
directly at the character level and uses
only small convolutions and pooling oper-
ations. we are able to show that the per-
formance of this model increases with the
depth: using up to 29 convolutional layers,
we report improvements over the state-of-
the-art on several public text classi   cation
tasks. to the best of our knowledge, this is
the    rst time that very deep convolutional
nets have been applied to text processing.

1 introduction

the goal of natural language processing (nlp) is
to process text with computers in order to analyze
it, to extract information and eventually to rep-
resent the same information differently. we may
want to associate categories to parts of the text
(e.g. id52 or id31), struc-
ture text differently (e.g. parsing), or convert it
to some other form which preserves all or part of
the content (e.g. machine translation, summariza-
tion). the level of granularity of this processing
can range from individual characters to subword
units (sennrich et al., 2016) or words up to whole
sentences or even paragraphs.

after a couple of pioneer works (bengio et al.
(2001), collobert and weston (2008), collobert et
al. (2011) among others), the use of neural net-
works for nlp applications is attracting huge in-

terest in the research community and they are sys-
tematically applied to all nlp tasks. however,
while the use of (deep) neural networks in nlp
has shown very good results for many tasks, it
seems that they have not yet reached the level to
outperform the state-of-the-art by a large margin,
as it was observed in id161 and speech
recognition.

convolutional neural networks, in short con-
vnets, are very successful in id161. in
early approaches to id161, handcrafted
features were used, for instance    scale-invariant
feature transform (sift)   (lowe, 2004), followed
by some classi   er. the fundamental idea of con-
vnets(lecun et al., 1998) is to consider feature
extraction and classi   cation as one jointly trained
task. this idea has been improved over the years,
in particular by using many layers of convolutions
and pooling to sequentially extract a hierarchical
representation(zeiler and fergus, 2014) of the in-
put. the best networks are using more than 150
layers as in (he et al., 2016a; he et al., 2016b).

many nlp approaches consider words as ba-
sic units. an important step was the introduction
of continuous representations of words(bengio et
al., 2003). these id27s are now the
state-of-the-art in nlp. however, it is less clear
how we should best represent a sequence of words,
e.g. a whole sentence, which has complicated syn-
tactic and semantic relations.
in general, in the
same sentence, we may be faced with local and
long-range dependencies. currently, the main-
stream approach is to consider a sentence as a se-
quence of tokens (characters or words) and to pro-
cess them with a recurrent neural network (id56).
tokens are usually processed in sequential order,
from left to right, and the id56 is expected to
   memorize    the whole sequence in its internal
states. the most popular and successful id56 vari-
ant are certainly lstms(hochreiter and schmid-

dataset label
yelp p.

+1

amz p.

3(/5)

sogou

   sports   

yah. a.

   computer,
internet   

sample
been going to dr. goldberg for over 10 years. i think i was one of his 1st
patients when he started at mhmg. hes been great over the years and is really
all about the big picture. [...]
i love this show, however, there are 14 episodes in the    rst season and this dvd
only shows the    rst eight. [...]. i hope the bbc will release another dvd that
contains all the episodes, but for now this one is still somewhat enjoyable.
ju4 xi1n hua2 she4 5 yue4 3 ri4 , be3i ji1ng 2008 a4o yu4n hui4 huo3 ju4 jie1
li4 ji1ng guo4 shi4 jie4 wu3 da4 zho1u 21 ge4 che2ng shi4
   what should i look for when buying a laptop? what is the best brand and
what   s reliable?   ,   weight and dimensions are important if you   re planning to
travel with the laptop. get something with at least 512 mb of ram. [..] is a
good brand, and has an easy to use site where you can build a custom laptop.   

table 1: examples of text samples and their labels.

huber, 1997)     there are many works which have
shown the ability of lstms to model long-range
dependencies in nlp applications, e.g. (sunder-
meyer et al., 2012; sutskever et al., 2014) to name
just a few. however, we argue that lstms are
generic learning machines for sequence process-
ing which are lacking task-speci   c structure.

we propose the following analogy.

it is well
known that a fully connected one hidden layer
neural network can in principle learn any real-
valued function, but much better results can be
obtained with a deep problem-speci   c architec-
ture which develops hierarchical representations.
by these means, the search space is heavily con-
strained and ef   cient solutions can be learned with
id119. convnets are namely adapted
for id161 because of the compositional
structure of an image. texts have similar proper-
ties : characters combine to form id165s, stems,
words, phrase, sentences etc.

we believe that a challenge in nlp is to develop
deep architectures which are able to learn hierar-
chical representations of whole sentences, jointly
with the task. in this paper, we propose to use deep
architectures of many convolutional layers to ap-
proach this goal, using up to 29 layers. the design
of our architecture is inspired by recent progress
in id161, in particular (simonyan and
zisserman, 2015; he et al., 2016a).

this paper is structured as follows. there have
been previous attempts to use convnets for text
processing. we summarize the previous works in
the next section and discuss the relations and dif-
ferences. our architecture is described in detail
in section 3. we have evaluated our approach on

several sentence classi   cation tasks, initially pro-
posed by (zhang et al., 2015). these tasks and
our experimental results are detailed in section 4.
the proposed deep convolutional network shows
signi   cantly better results than previous convnets
approach. the paper concludes with a discus-
sion of future research directions for very deep ap-
proach in nlp.

2 related work

there is a large body of research on sentiment
analysis, or more generally on sentence classi   ca-
tion tasks.
initial approaches followed the clas-
sical two stage scheme of extraction of (hand-
crafted) features,
followed by a classi   cation
stage. typical features include bag-of-words or n-
grams, and their tf-idf. these techniques have
been compared with convnets by (zhang et al.,
2015; zhang and lecun, 2015). we use the
same corpora for our experiments. more recently,
words or characters, have been projected into a
low-dimensional space, and these embeddings are
combined to obtain a    xed size representation of
the input sentence, which then serves as input for
the classi   er. the simplest combination is the
element-wise mean. this usually performs badly
since all notion of token order is disregarded.

another class of approaches are recursive neu-
ral networks. the main idea is to use an ex-
ternal tool, namely a parser, which speci   es the
order in which the id27s are com-
bined. at each node, the left and right context are
combined using weights which are shared for all
nodes (socher et al., 2011). the state of the top
node is fed to the classi   er. a recurrent neural net-

work (id56) could be considered as a special case
of a recursive nn: the combination is performed
sequentially, usually from left to right. the last
state of the id56 is used as    xed-sized representa-
tion of the sentence, or eventually a combination
of all the hidden states.

first works using convolutional neural networks
for nlp appeared in (collobert and weston, 2008;
collobert et al., 2011). they have been subse-
quently applied to sentence classi   cation (kim,
2014; kalchbrenner et al., 2014; zhang et al.,
2015). we will discuss these techniques in more
detail below.
if not otherwise stated, all ap-
proaches operate on words which are projected
into a high-dimensional space.

a rather shallow neural net was proposed
in (kim, 2014): one convolutional layer (using
multiple widths and    lters) followed by a max
pooling layer over time. the    nal classi   er uses
one fully connected layer with drop-out. results
are reported on six data sets, in particular stanford
sentiment treebank (sst). a similar system was
proposed in (kalchbrenner et al., 2014), but us-
ing    ve convolutional layers. an important differ-
ence is also the introduction of multiple temporal
k-max pooling layers. this allows to detect the k
most important features in a sentence, independent
of their speci   c position, preserving their relative
order. the value of k depends on the length of
the sentence and the position of this layer in the
network. (zhang et al., 2015) were the    rst to per-
form id31 entirely at the character
level. their systems use up to six convolutional
layers, followed by three fully connected classi   -
cation layers. convolutional kernels of size 3 and
7 are used, as well as simple max-pooling layers.
another interesting aspect of this paper is the in-
troduction of several large-scale data sets for text
classi   cation. we use the same experimental set-
ting (see section 4.1). the use of character level
information was also proposed by (dos santos and
gatti, 2014): all the character embeddings of one
word are combined by a max operation and they
are then jointly used with the id27 in-
formation in a shallow architecture. in parallel to
our work, (yang et al., 2016) proposed a based hi-
erarchical attention network for document classi-
   cation that perform an attention    rst on the sen-
tences in the document, and on the words in the
sentence. their architecture performs very well
on datasets whose samples contain multiple sen-

tences.

in the id161 community, the com-
bination of recurrent and convolutional networks
in one architecture has also been investigated,
with the goal to    get the best of both worlds   ,
e.g.
(pinheiro and collobert, 2014). the same
idea was recently applied to sentence classi   ca-
tion (xiao and cho, 2016). a convolutional net-
work with up to    ve layers is used to learn high-
level features which serve as input for an lstm.
the initial motivation of the authors was to ob-
tain the same performance as (zhang et al., 2015)
with networks which have signi   cantly fewer pa-
rameters. they report results very close to those
of (zhang et al., 2015) or even outperform con-
vnets for some data sets.

in summary, we are not aware of any work
that uses vgg-like or resnet-like architecture
to go deeper than than six convolutional layers
(zhang et al., 2015) for sentence classi   cation.
deeper networks were not tried or they were re-
ported to not improve performance. this is in
sharp contrast to the current trend in computer vi-
sion where signi   cant improvements have been re-
ported using much deeper networks(krizhevsky et
al., 2012), namely 19 layers (simonyan and zis-
serman, 2015), or even up to 152 layers (he et al.,
2016a). in the remainder of this paper, we describe
our very deep convolutional architecture and re-
port results on the same corpora than (zhang et
al., 2015). we were able to show that performance
improves with increased depth, using up to 29 con-
volutional layers.

3 vdid98 architecture

the overall architecture of our network is shown
in figure 1. our model begins with a look-up ta-
ble that generates a 2d tensor of size (f0, s) that
contain the embeddings of the s characters. s is
   xed to 1024, and f0 can be seen as the    rgb   
dimension of the input text.

we    rst apply one layer of 64 convolutions of
size 3, followed by a stack of temporal    convolu-
tional blocks   . inspired by the philosophy of vgg
and resnets we apply these two design rules: (i)
for the same output temporal resolution, the layers
have the same number of feature maps, (ii) when
the temporal resolution is halved, the number of
feature maps is doubled. this helps reduce the
memory footprint of the network. the networks
contains 3 pooling operations (halving the tempo-

fc(2048, nclasses)

fc(2048, 2048), relu

fc(4096, 2048), relu

output: 512 x k

k-max pooling, k=8

convolutional block, 3, 512

convolutional block, 3, 512

output: 512 x s/8

pool/2

optional
shortcut

optional
shortcut

convolutional block, 3, 256

optional
shortcut

convolutional block, 3, 256
convolutional block, 3, 256

output: 256 x s/4

pool/2

optional
shortcut

convolutional block, 3, 128

optional
shortcut

convolutional block, 3, 128

output: 128 x s/2

pool/2

optional
shortcut

optional
shortcut

convolutional block, 3, 64

convolutional block, 3, 64

output: 64 x s

3, temp conv, 64

output: 16 x s

lookup table, 16

input :  1 x s

text

figure 1: vdid98 architecture.

ral resolution each time by 2), resulting in 3 levels
of 128, 256 and 512 feature maps (see figure 1).
the output of these convolutional blocks is a ten-
2p with p = 3
sor of size 512    sd, where sd = s
the number of down-sampling operations. at this
level of the convolutional network, the resulting
tensor can be seen as a high-level representation
of the input text. since we deal with padded in-
put text of    xed size, sd is constant. however,
in the case of variable size input, the convolu-
tional encoder provides a representation of the in-
put text that depends on its initial length s. repre-
sentations of a text as a set of vectors of variable
size can be valuable namely for neural machine
translation, in particular when combined with an
attention model.
in figure 1, temporal convolu-
tions with kernel size 3 and x feature maps are
denoted    3, temp conv, x   , fully connected
layers which are linear projections (matrix of size
i    o) are denoted    fc(i, o)    and    3-max
pooling, stride 2    means temporal max-
pooling with kernel size 3 and stride 2.

most of the previous applications of convnets
to nlp use an architecture which is rather shal-
low (up to 6 convolutional layers) and combines
convolutions of different sizes, e.g. spanning 3, 5
and 7 tokens. this was motivated by the fact that
convolutions extract id165 features over tokens
and that different id165 lengths are needed to
model short- and long-span relations. in this work,
we propose to create instead an architecture which
uses many layers of small convolutions (size 3).
stacking 4 layers of such convolutions results in a
span of 9 tokens, but the network can learn by it-
self how to best combine these different    3-gram
features    in a deep hierarchical manner. our ar-
chitecture can be in fact seen as a temporal adap-
tation of the vgg network (simonyan and zisser-
man, 2015). we have also investigated the same
kind of    resnet shortcut    connections as in (he
et al., 2016a), namely identity and 1    1 convolu-
tions (see figure 1).

for the classi   cation tasks in this work, the tem-
poral resolution of the output of the convolution
blocks is    rst down-sampled to a    xed dimension
using k-max pooling. by these means, the net-
work extracts the k most important features, inde-
pendently of the position they appear in the sen-
tence. the 512    k resulting features are trans-
formed into a single vector which is the input to
a three layer fully connected classi   er with relu
hidden units and softmax outputs. the number of

relu

temporal batch norm

3, temp conv, 256

relu

temporal batch norm

3, temp conv, 256

figure 2: convolutional block.

output neurons depends on the classi   cation task,
the number of hidden units is set to 2048, and k
to 8 in all experiments. we do not use drop-out
with the fully connected layers, but only temporal
batch id172 after convolutional layers to
regularize our network.

convolutional block
each convolutional block (see figure 2) is a se-
quence of two convolutional
layers, each one
followed by a temporal batchnorm (ioffe and
szegedy, 2015) layer and an relu activation. the
kernel size of all the temporal convolutions is 3,
with padding such that the temporal resolution is
preserved (or halved in the case of the convolu-
tional pooling with stride 2, see below). steadily
increasing the depth of the network by adding
more convolutional layers is feasible thanks to the
limited number of parameters of very small con-
volutional    lters in all layers. different depths
of the overall architecture are obtained by vary-
ing the number of convolutional blocks in between
the pooling layers (see table 2). temporal batch
id172 applies the same kind of regulariza-
tion as batch id172 except that the activa-
tions in a mini-batch are jointly normalized over
temporal (instead of spatial) locations. so, for a
mini-batch of size m and feature maps of tempo-
ral size s, the sum and the standard deviations re-
lated to the batchnorm algorithm are taken over
|b| = m    s terms.

we explore three types of down-sampling be-

tween blocks ki and ki+1 (figure 1) :

(i) the    rst convolutional

layer of ki+1 has

stride 2 (resnet-like).

(ii) ki is followed by a k-max pooling layer
where k is such that the resolution is halved

(kalchbrenner et al., 2014).

(iii) ki is followed by max-pooling with kernel

size 3 and stride 2 (vgg-like).

all these types of pooling reduce the temporal res-
olution by a factor 2. at the    nal convolutional
layer, the resolution is thus sd.

depth:
conv block 512
conv block 256
conv block 128
conv block 64
first conv. layer
#params [in m]

9
2
2
2
2
1
2.2

17
4
4
4
4
1
4.3

29
4
4
10
10
1
4.6

49
6
10
16
16
1
7.8

table 2: number of conv. layers per depth.
in this work, we have explored four depths for
our networks: 9, 17, 29 and 49, which we de-
   ne as being the number of convolutional lay-
ers. the depth of a network is obtained by sum-
ming the number of blocks with 64, 128, 256 and
512    lters, with each block containing two con-
volutional layers.
in figure 1, the network has
2 blocks of each type, resulting in a depth of
2    (2 + 2 + 2 + 2) = 16. adding the very    rst
convolutional layer, this sums to a depth of 17 con-
volutional layers. the depth can thus be increased
or decreased by adding or removing convolutional
blocks with a certain number of    lters. the best
con   gurations we observed for depths 9, 17, 29
and 49 are described in table 2. we also give the
number of parameters of all convolutional layers.

4 experimental evaluation

4.1 tasks and data
in the id161 community, the availabil-
ity of large data sets for id164 and im-
age classi   cation has fueled the development of
new architectures. in particular, this made it pos-
sible to compare many different architectures and
to show the bene   t of very deep convolutional net-
works. we present our results on eight freely avail-
able large-scale data sets introduced by (zhang et
al., 2015) which cover several classi   cation tasks
such as id31, topic classi   cation or
news categorization (see table 3). the number of
training examples varies from 120k up to 3.6m,
and the number of classes is comprised between 2
and 14. this is considerably lower than in com-
puter vision (e.g. 1 000 classes for id163).

data set
ag   s news
sogou news
dbpedia
yelp review polarity
yelp review full
yahoo! answers
amazon review full
amazon review polarity

#train
120k
450k
560k
560k
650k
1 400k
3 000k
3 600k

#test
7.6k
60k
70k
38k
50k
60k
650k
400k

#classes classi   cation task

4 english news categorization
5 chinese news categorization
14 ontology classi   cation
2 id31
5 id31
10 topic classi   cation
5 id31
2 id31

table 3: large-scale text classi   cation data sets used in our experiments. see (zhang et al., 2015) for a
detailed description.

this has the consequence that each example in-
duces less gradient information which may make
it harder to train large architectures. it should be
also noted that some of the tasks are very ambigu-
ous, in particular id31 for which it
is dif   cult to clearly associate    ne grained labels.
there are equal numbers of examples in each class
for both training and test sets. the reader is re-
ferred to (zhang et al., 2015) for more details on
the construction of the data sets. table 4 summa-
rizes the best published results on these corpora
we are aware of. we do not use    thesaurus data
augmentation    or any other preprocessing, except
lower-casing. nevertheless, we still outperform
the best convolutional neural networks of (zhang
et al., 2015) for all data sets. the main goal of our
work is to show that it is possible and bene   cial
to train very deep convolutional networks as text
encoders. data augmentation may improve our re-
sults even further. we will investigate this in future
research.

4.2 common model settings

the following settings have been used in all
our experiments. they were found to be best
in initial experiments.
following (zhang et
al., 2015), all processing is done at the char-
acter level which is the atomic representation
of a sentence, same as pixels for images. the
dictionary consists of the following characters
   abcdefghijklmnopqrstuvwxyz0123456
789-,;.!?:   "/| #$%  &*     +=<>()[]{}   
plus a special padding, space and unknown token
which add up to a total of 69 tokens. the input
text is padded to a    xed size of 1014,
larger
text are truncated. the character embedding is
of size 16. training is performed with sgd,
using a mini-batch of size 128, an initial learning

rate of 0.01 and momentum of 0.9. we follow
the same training procedure as in (zhang et al.,
2015). we initialize our convolutional
layers
following (he et al., 2015). one epoch took from
24 minutes to 2h45 for depth 9, and from 50
minutes to 7h (on the largest datasets) for depth
29. it took between 10 to 15 epoches to converge.
the implementation is done using torch 7. all
experiments are performed on a single nvidia
k40 gpu. unlike previous research on the use
of convnets for text processing, we use temporal
batch norm without dropout.

4.3 experimental results
in this section, we evaluate several con   gurations
of our model, namely three different depths and
three different pooling types (see section 3). our
main contribution is a thorough evaluation of net-
works of increasing depth using an architecture
with small temporal convolution    lters with dif-
ferent types of pooling, which shows that a signif-
icant improvement on the state-of-the-art con   gu-
rations can be achieved on text classi   cation tasks
by pushing the depth to 29 convolutional layers.

our deep architecture works well on big data
sets in particular, even for small depths. table
5 shows the test errors for depths 9, 17 and 29 and
for each type of pooling : convolution with stride
2, k-max pooling and temporal max-pooling. for
the smallest depth we use (9 convolutional layers),
we see that our model already performs better than
zhang   s convolutional baselines (which includes
6 convolutional layers and has a different archi-
tecture) on the biggest data sets : yelp full, ya-
hoo answers and amazon full and polarity. the
most important decrease in classi   cation error can
be observed on the largest data set amazon full
which has more than 3 million training samples.

ag

dbp.

sogou

[zhang]

yelp p.
ngrams
[zhang]

n-tfidf n-tfidf n-tfidf
[zhang]
[zhang]

corpus:
method
author
error
[yang]
table 4: best published results from previous work. zhang et al. (2015) best results use a thesaurus data
augmentation technique (marked with an    ). yang et al. (2016)   s hierarchical methods is particularly
adapted to datasets whose samples contain multiple sentences.

amz. f. amz. p.
conv
conv
[zhang]
[zhang]
40.43   
4.93   
36.4

yelp f.
conv
[zhang]
37.95   

[xiao]
28.26
24.2

conv+id56

yah. a.

1.31

4.36

2.81

7.64

-

-

-

-

-

-

depth pooling

ag sogou dbp. yelp p. yelp f. yah. a. amz. f. amz. p.
convolution
10.17
kmaxpooling 9.83
maxpooling
9.17
convolution
9.29
kmaxpooling 9.39
maxpooling
8.88
convolution
9.36
kmaxpooling 8.67
maxpooling
8.73

37.63
38.04
36.73
36.10
37.41
36.07
35.28
37.00
35.74

28.10
28.24
27.60
27.35
28.25
27.51
27.17
27.16
26.57

38.52
39.19
37.95
37.50
38.81
37.39
37.58
38.39
37.00

5.01
5.27
4.88
4.96
5.05
4.50
4.35
4.63
4.28

1.64
1.56
1.35
1.42
1.61
1.40
1.36
1.41
1.29

4.94
5.69
4.70
4.53
5.43
4.41
4.28
4.94
4.31

4.22
3.58
3.70
3.94
3.51
3.54
3.61
3.18
3.36

9
9
9
17
17
17
29
29
29

table 5: testing error of our models on the 8 data sets. no id174 or augmentation is used.

we also observe that for a small depth, temporal
max-pooling works best on all data sets.

depth improves performance. as we increase
the network depth to 17 and 29, the test errors
decrease on all data sets, for all types of pooling
(with 2 exceptions for 48 comparisons). going
from depth 9 to 17 and 29 for amazon full re-
duces the error rate by 1% absolute. since the
test is composed of 650k samples, 6.5k more
test samples have been classi   ed correctly. these
improvements, especially on large data sets, are
signi   cant and show that increasing the depth is
useful for text processing. overall, compared
to previous state-of-the-art, our best architecture
with depth 29 and max-pooling has a test error of
37.0 compared to 40.43%. this represents a gain
of 3.43% absolute accuracy. the signi   cant im-
provements which we obtain on all data sets com-
pared to zhang   s convolutional models do not in-
clude any data augmentation technique.

max-pooling performs better than other pool-
ing types.
in terms of pooling, we can also see
that max-pooling performs best overall, very close
to convolutions with stride 2, but both are signi   -
cantly superior to k-max pooling.

both pooling mechanisms perform a max oper-
ation which is local and limited to three consec-
utive tokens, while k-max polling considers the
whole sentence at once. according to our exper-

iments, it seems to hurt performance to perform
this type of max operation at intermediate layers
(with the exception of the smallest data sets).

our models outperform state-of-the-art con-
vnets. we obtain state-of-the-art results for all
data sets, except ag   s news and sogou news
which are the smallest ones. however, with our
very deep architecture, we get closer to the state-
of-the-art which are ngrams tf-idf for these data
sets and signi   cantly surpass convolutional mod-
els presented in (zhang et al., 2015). as observed
in previous work, differences in accuracy between
shallow (tf-idf) and deep (convolutional) mod-
els are more signi   cant on large data sets, but we
still perform well on small data sets while getting
closer to the non convolutional state-of-the-art re-
sults on small data sets. the very deep models
even perform as well as ngrams and ngrams-tf-
idf respectively on the id31 task
of yelp review polarity and the ontology classi-
   cation task of the dbpedia data set. results of
yang et al. (only on yahoo answers and amazon
full) outperform our model on the yahoo answers
dataset, which is probably linked to the fact that
their model is task-speci   c to datasets whose sam-
ples that contain multiple sentences like (question,
answer). they use a hierarchical attention mecha-
nism that apply very well to documents (with mul-
tiple sentences).

going even deeper degrades accuracy. short-
cut connections help reduce the degradation.
as described in (he et al., 2016a), the gain in accu-
racy due to the the increase of the depth is limited
when using standard convnets. when the depth
increases too much, the accuracy of the model gets
saturated and starts degrading rapidly. this degra-
dation problem was attributed to the fact that very
deep models are harder to optimize. the gradi-
ents which are backpropagated through the very
deep networks vanish and sgd with momentum
is not able to converge to a correct minimum of
the id168. to overcome this degradation
of the model, the resnet model introduced short-
cut connections between convolutional blocks that
allow the gradients to    ow more easily in the net-
work (he et al., 2016a).

we evaluate the impact of shortcut connections
by increasing the number of convolutions to 49
layers. we present an adaptation of the resnet
model to the case of temporal convolutions for text
(see figure 1). table 6 shows the evolution of the
test errors on the yelp review full data set with or
without shortcut connections. when looking at the
column    without shortcut   , we observe the same
degradation problem as in the original resnet ar-
ticle: when going from 29 to 49 layers, the test
error rate increases from 35.28 to 37.41 (while the
training error goes up from 29.57 to 35.54). when
using shortcut connections, we observe improved
results when the network has 49 layers: both the
training and test errors go down and the network is
less prone to under   tting than it was without short-
cut connections.

while shortcut connections give better results
when the network is very deep (49 layers), we
were not able to reach state-of-the-art results with
them. we plan to further explore adaptations of
residual networks to temporal convolutions as we
think this a milestone for going deeper in nlp.
residual units (he et al., 2016a) better adapted to
the text processing task may help for training even
deeper models for text processing, and is left for
future research.

exploring these models on text classi   cation
tasks with more classes sounds promising.
note that one of the most important difference
between the classi   cation tasks discussed in this
work and id163 is that the latter deals with
1000 classes and thus much more information is
back-propagated to the network through the gra-

depth without shortcut with shortcut

9
17
29
49

37.63
36.10
35.28
37.41

40.27
39.18
36.01
36.15

table 6: test error on the yelp full data set for all
depths, with or without residual connections.

dients. exploring the impact of the depth of tem-
poral convolutional models on categorization tasks
with hundreds or thousands of classes would be an
interesting challenge and is left for future research.

5 conclusion

we have presented a new architecture for nlp
which follows two design principles: 1) operate at
the lowest atomic representation of text, i.e. char-
acters, and 2) use a deep stack of local operations,
i.e. convolutions and max-pooling of size 3, to
learn a high-level hierarchical representation of a
sentence. this architecture has been evaluated on
eight freely available large-scale data sets and we
were able to show that increasing the depth up to
29 convolutional layers steadily improves perfor-
mance. our models are much deeper than pre-
viously published convolutional neural networks
and they outperform those approaches on all data
sets. to the best of our knowledge, this is the    rst
time that the    bene   t of depths    was shown for
convolutional neural networks in nlp.

eventhough text follows human-de   ned rules
and images can be seen as raw signals of our en-
vironment, images and small texts have similar
properties. texts are also compositional for many
languages. characters combine to form id165s,
stems, words, phrase, sentences etc. these simi-
lar properties make the comparison between com-
puter vision and natural language processing very
pro   table and we believe future research should
invest into making text processing models deeper.
our work is a    rst attempt towards this goal.

in this paper, we focus on the use of very deep
convolutional neural networks for sentence classi-
   cation tasks. applying similar ideas to other se-
quence processing tasks, in particular neural ma-
chine translation is left for future research.
it
needs to be investigated whether these also bene   t
from having deeper convolutional encoders.

references
yoshua bengio, rejean ducharme, and pascal vin-
cent. 2001. a neural probabilistic language model.
in nips, volume 13, pages 932   938, vancouver,
british columbia, canada.

yoshua bengio, r  ejean ducharme, pascal vincent, and
christian jauvin. 2003. a neural probabilistic lan-
guage model. journal of machine learning research,
3(feb):1137   1155.

ronan collobert and jason weston. 2008. a uni   ed
architecture for natural language processing: deep
neural networks with multitask learning. in icml,
pages 160   167, helsinki, finland.

ronan collobert, jason weston lon bottou, m. karlen,
k. kavukcuoglu, and p. kuksa.
2011. natural
language processing (almost) from scratch. jmlr,
pages 2493   2537.

c    cero nogueira dos santos and maira gatti. 2014.
deep convolutional neural networks for sentiment
analysis of short texts.
in coling, pages 69   78,
dublin, ireland.

kaiming he, xiangyu zhang, shaoqing ren, and jian
sun. 2015. delving deep into recti   ers: surpass-
ing human-level performance on id163 classi   -
cation.
in proceedings of the ieee international
conference on id161, pages 1026   1034,
santiago, chile.

kaiming he, xiangyu zhang, shaoqing ren, and jian
sun.
2016a. deep residual learning for image
recognition. in proceedings of the ieee conference
on id161 and pattern recognition, pages
770   778, las vegas, nevada, usa.

kaiming he, xiangyu zhang, shaoqing ren, and jian
identity mappings in deep residual
sun. 2016b.
networks.
in european conference on computer
vision, pages 630   645, amsterdam, netherlands.
springer.

sepp hochreiter and j  urgen schmidhuber.

1997.
long short-term memory. neural computation,
9(8):1735   1780.

sergey ioffe and christian szegedy. 2015. batch nor-
malization: accelerating deep network training by
reducing internal covariate shift.
in icml, pages
448   456, lille, france.

nal kalchbrenner, edward grefenstette, and phil blun-
som. 2014. a convolutional neural network for
modelling sentences.
in proceedings of the 52nd
annual meeting of the association for computa-
tional linguistics, pages 655   665, baltimore, mary-
land, usa.

yoon kim.

2014. convolutional neural networks
for sentence classi   cation.
in proceedings of the
2014 conference on empirical methods in natural
language processing (emnlp), pages 1746   1751,

doha, qatar. association for computational lin-
guistics.

alex krizhevsky, ilya sutskever, and geoffrey e hin-
ton. 2012. id163 classi   cation with deep con-
volutional neural networks. in advances in neural
information processing systems, pages 1097   1105,
lake tahoe, california, usa.

yann lecun, l  eon bottou, yoshua bengio, and patrick
haffner. 1998. gradient-based learning applied to
document recognition. proceedings of the ieee,
86(11):2278   2324.

david g lowe. 2004. distinctive image features from
international journal of

scale-invariant keypoints.
id161, 60(2):91   110.

pedro ho pinheiro and ronan collobert. 2014. re-
current convolutional neural networks for scene la-
beling. in icml, pages 82   90, beijing, china.

rico sennrich, barry haddow, and alexandra birch.
2016. id4 of rare words with
subword units. pages 1715   1725.

karen simonyan and andrew zisserman. 2015. very
deep convolutional networks for large-scale image
recognition. in iclr, san diego, california, usa.

richard socher, jeffrey pennington, eric h huang,
andrew y ng, and christopher d manning. 2011.
semi-supervised recursive autoencoders for predict-
ing sentiment distributions.
in proceedings of the
conference on empirical methods in natural lan-
guage processing, pages 151   161, edinburgh, uk.
association for computational linguistics.

martin sundermeyer, ralf schl  uter, and hermann ney.
2012. lstm neural networks for language model-
ing. in interspeech, pages 194   197, portland, ore-
gon, usa.

ilya sutskever, oriol vinyals, and quoc v. le. 2014.
sequence to sequence learning with neural net-
works.
in nips, pages 3104   3112, montreal,
canada.

yijun xiao and kyunghyun cho.

2016. ef   cient
character-level document classi   cation by combin-
ing convolution and recurrent layers.

zichao yang, diyi yang, chris dyer, xiaodong he,
alex smola, and eduard hovy. 2016. hierarchi-
cal attention networks for document classi   cation.
in proceedings of naacl-hlt, pages 1480   1489,
san diego, california, usa.

matthew d zeiler and rob fergus. 2014. visualizing
and understanding convolutional networks. in eu-
ropean conference on id161, pages 818   
833, zurich, switzerland. springer.

xiang zhang and yann lecun. 2015. text understand-
ing from scratch. arxiv preprint arxiv:1502.01710.

xiang zhang, junbo zhao, and yann lecun. 2015.
character-level convolutional networks for text clas-
si   cation.
in nips, pages 649   657, montreal,
canada.

