cfo: conditional focused neural id53

with large-scale knowledge bases

zihang dai   

carnegie mellon university

lei li   

toutiao.com

wei xu

baidu research

dzihang@andrew.cmu.edu

lileicc@gmail.com

xuwei06@baidu.com

6
1
0
2

 
l
u
j
 

4

 
 
]
l
c
.
s
c
[
 
 

2
v
4
9
9
1
0

.

6
0
6
1
:
v
i
x
r
a

abstract

how can we enable computers to automat-
ically answer questions like    who created
the character harry potter   ? carefully
built knowledge bases provide rich sources
of facts. however,
it remains a chal-
lenge to answer factoid questions raised
in natural language due to numerous ex-
pressions of one question.
in particular,
we focus on the most common questions
    ones that can be answered with a sin-
gle fact in the knowledge base. we pro-
pose cfo, a conditional focused neural-
network-based approach to answering fac-
toid questions with knowledge bases. our
approach    rst zooms in a question to
   nd more probable candidate subject men-
tions, and infers the    nal answers with
a uni   ed conditional probabilistic frame-
work. powered by deep recurrent neural
networks and neural embeddings, our pro-
posed cfo achieves an accuracy of 75.7%
on a dataset of 108k questions     the largest
public one to date. it outperforms the cur-
rent state of the art by an absolute margin
of 11.8%.
introduction

1
community-driven id53 (qa) web-
sites such as quora, yahoo-answers, and an-
swers.com are accumulating millions of users and
hundreds of millions of questions. a large portion
of the questions are about facts or trivia.
it has
been a long pursuit to enable machines to answer
such questions automatically.

in recent years,

several efforts have been
made on utilizing open-domain knowledge bases
a knowledge
to answer

factoid questions.

   part of the work was done while at baidu.

lately,

several

base (kb) consists of structured representation
of facts in the form of subject-relation-object
large-scale general-
triples.
purpose kbs have been constructed,
including
yago (suchanek et al., 2007), freebase (bol-
lacker et al., 2008), nell (carlson et al., 2010),
and dbpedia (lehmann et al., 2014). typically,
structured queries with prede   ned semantics (e.g.
sparql) can be issued to retrieve speci   ed facts
from such kbs. thus, answering factoid questions
will be straightforward once they are converted
into the corresponding structured form. however,
due to complexity of language, converting natural
language questions to structure forms remains an
open challenge.

among all sorts of questions, there is one cat-
egory that only requires a single fact (triple) in
kb as the supporting evidence. as a typical ex-
the question    who created the charac-
ample,
ter harry potter    can be answered with the sin-
gle fact (harrypotter, charactercreatedby,
j.k.rowling).
in this work, we refer to such
questions as single-fact questions. previously, it
has been observed that single-fact questions con-
stitute the majority of factoid questions in commu-
nity qa sites (fader et al., 2013). despite the sim-
plicity, automatically answering such questions re-
mains far from solved     the latest best result on a
dataset of 108k single-fact questions is only 63.9%
in terms of accuracy (bordes et al., 2015).

to    nd the answer to a single-fact question, it
suf   ces to identify the subject entity and relation
(implicitly) mentioned by the question, and then
forms a corresponding structured query. the prob-
lem can be formulated into a probabilistic form.
given a single-fact question q,    nding the subject-
relation pair   s,   r from the kb k which maximizes
the id155 p(s, r|q), i.e.

  s,   r = arg max

s,r   k

p(s, r|q)

(1)

based on the formulation (1), the central prob-
lem is to estimate the conditional distribution
p(s, r|q). it is very challenging because of a) the
vast amount of facts     a large-scale kb such as
freebase contains billions of triples, b) the huge
variety of language     there are multiple aliases
for an entity, and numerous ways to compose a
question, c) the severe sparsity of supervision    
most combinations of s, r, q are not expressed in
training data. faced with these challenges, exist-
ing methods have exploited to incorporate prior
knowledge into semantic parsers, to design mod-
els and representations with better generalization
property, to utilize large-margin ranking objective
to estimate the model parameters, and to prune the
search space during id136. noticeably, mod-
els based on neural networks and distributed rep-
resentations have largely contributed to the recent
progress (see section 2).

in this paper, we propose cfo, a novel method
to answer single-fact questions with large-scale
knowledge bases. the contributions of this paper
are,

    we employ a fully probabilistic treatment of
the problem with a novel conditional param-
eterization using neural networks,
    we propose the focused pruning method to re-
duce the search space during id136, and
    we investigate two variations to improve the
generalization of representations for millions
of entities under highly sparse supervision.

in experiments, cfo achieves 75.7% in terms of
top-1 accuracy on the largest dataset to date, out-
performing the current best record by an absolute
margin of 11.8%.

2 related work

the research of kb supported qa has evolved
from earlier domain-speci   c qa (zelle and
mooney, 1996; tang and mooney, 2001; liang
et al., 2013) to open-domain qa based on large-
scale kbs. an important line of research has been
trying to tackle the problem by id29,
which directly parses natural language questions
into structured queries (liang et al., 2011; cai
and yates, 2013; kwiatkowski et al., 2013; yao
and van durme, 2014). recent progresses in-
clude designing kb speci   c logical representation
and parsing grammar (berant et al., 2013), using
distant supervision (berant et al., 2013), utilizing

paraphrase information (fader et al., 2013; be-
rant and liang, 2014), requiring little question-
answer pairs (reddy et al., 2014), and exploit-
ing ideas from agenda-based parsing (berant and
liang, 2015).

in contrast, another line of research tackles
the problem by deep learning powered similarity
matching. the core idea is to learn semantic repre-
sentations of both the question and the knowledge
from observed data, such that the correct support-
ing evidence will be the nearest neighbor of the
question in the learned vector space. thus, a main
difference among several approaches lies in the
neural networks proposed to represent questions
and kb elements. while (bordes et al., 2014b;
bordes et al., 2014a; bordes et al., 2015; yang et
al., 2014) use relatively shallow embedding mod-
els to represent the question and knowledge, (yih
et al., 2014; yih et al., 2015) employ a convolu-
tional neural network (id98) to produce the repre-
sentation. in the latter case, both the question and
the relation are treated as a sequence of letter-tri-
gram patterns, and fed into two parameter shared
id98s to get their embeddings. what   s more, in-
stead of measuring the similarity between a ques-
tion and an evidence triple with a single model as
in (bordes et al., 2015), (yih et al., 2014; yih et al.,
2015) adopt a multi-stage approach. in each stage,
one element of the triple is compared with the
question to produce a partial similarity score by
a dedicated model. then, these partial scores are
combined to generate the overall measurement.

our proposed method is closely related to the
second line of research, since neural models are
employed to learn semantic representations. as in
(bordes et al., 2015; yih et al., 2014), we focus
on single-fact questions. however, we propose to
use recurrent neural networks (id56) to produce
the question representation. more importantly, our
method follows a probabilistic formulation, and
our parameterization relies on factors other than
similarity measurement.

besides kb-based qa, our work is also loosely
related to work using deep learning systems in qa
tasks with free text evidences. for example, (iyyer
et al., 2014) focuses questions from the quiz bowl
competition with id56. new
architectures including memory networks (weston
et al., 2015), dynamic memory networks (kumar
et al., 2015), and more (peng et al., 2015; lee et
al., 2015) have been explored under the babi syn-

thetic qa task (weston et al., 2016). in addition,
(hermann et al., 2015) seeks to answer cloze style
questions based on news articles.

3 overview

in this section, we formally formulate the problem
of single-fact id53 with knowledge
bases. a knowledge base k contains three com-
ponents: a set of entities e, a set of relations r,
and a set of facts f = {(cid:104)s, r, o(cid:105)}     e    r    e,
where s, o     e are the subject and object enti-
ties, and r     r is a binary relation. e(r), e(s) are
the vector representations of a relation and an en-
tity, respectively. s     r indicates that there exists
some entity o such that (cid:104)s, r, o(cid:105)     f. for single-
fact questions, a common assumption is that the
answer entity o and some triple (cid:104)si, rk, o(cid:105)     f
reside in the given knowledge base. the goal of
our model is to    nd such subject si and relation rk
mentioned or implied in the question. once found,
a structured query (e.g. in sparql) can be con-
structed to retrieve the result entity.

3.1 conditional factoid factorization
given a question q, the joint conditional probabil-
ity of subject-relation pairs p(s, r|q) can be used
to retrieve the answer using the exact id136
de   ned by eq. (1). however, since there can be
millions of entities and thousands of relations in
a knowledge base, it is less effective to model
p(s, r|q) directly.
instead, we propose a condi-
tional factoid factorization,

p(s, r|q) = p(r|q)    p(s|q, r)

(2)

and utilize two neural networks to parameter-
ize each component, p(r|q) and p(s|q, r), respec-
tively. hence, our proposed method contains two
phases: inferring the implied relation r from the
question q, and inferring the mentioned subject en-
tity s given the relation r and the question q.
there is an alternative factorization p(s, r|q) =
p(s|q)   p(r|s, q). however, it is rather challenging
to estimate p(s|q) directly due to the vast amount
of entities (> 106) in a kb. in comparison, our
proposed factorization takes advantage of the rel-
atively limited number of relations (on the order
of thousands). what   s more, by exploiting addi-
tional information from the candidate relation r,
it   s more feasible to model p(s|q, r) than p(s|q),
leading to more robust estimation.

a key difference from prior multi-step approach
is that our method do not assume any indepen-
dence between the target subject and relation given
a question, as does in the prior method (yih et al.,
2014). it proves effective in our experiments.

id136 via focused pruning

3.2
as de   ned by the eq. (1), a solution needs to con-
sider all available subject-relation pairs in the kb
as candidates. with a large-scale kb, the number
of candidates can be notoriously large, resulting in
a extremely noisy candidate pool. we propose a
method to prune the candidate space. the pruning
is equivalent to a function that takes a kb k and
a question q as input, and outputs a much limited
set c of candidate subject-relation pairs.

h(k, q)     c

(3)
cs and cr are used to represent the subject and re-
lation candidates, respectively.

the fundamental intuition for pruning is that the
subject entity must be mentioned by some textual
substring (subject mention) in the question. thus,
the candidate space can be restricted to entities
whose name/alias matches an id165 of the ques-
tion, as in (yih et al., 2014; yih et al., 2015; bor-
des et al., 2015). we refer to this straight-forward
method as id165 pruning. by considering all n-
grams, this approach usually achieves a high recall
rate. however, the candidate pool is still noisy due
to many non-subject-mention id165s.

our key idea is to reduce the noise by guiding
the pruning method   s attention to more probable
parts of a question. an observation is that cer-
tain parts of a sentence are more likely to be the
subject mention than others. for example,    harry
potter    in    who created the character harry pot-
ter    is more likely than    the character   ,    charac-
ter harry   , etc. speci   cally, our method employs
a deep network to identify such focus segments in
a question. this way, the candidate pool can be
not only more compact, but also signi   cantly less
noisy.

finally, combing the ideas of eq.(2) and (3), we
propose an approximate solution to the problem
de   ned by eq. (1)

  s,   r     arg max

p(s|q, r)p(r|q)

s,r   c
4 proposed cfo
in this section, we    rst review the gated recurrent
unit (gru), an id56 variant extensively used in

(4)

this work. then, we describe the model parame-
terization of p(r|q) and p(s|q, r), and the focused
pruning method in id136.

4.1 review: id149
in this work we employ gru (cho et al., 2014) as
the id56 structure. at time step t, a gru com-
putes its hidden state ht using the following com-
pound functions

z = sigmoid (wxzxt + whzht   1 + bz)
r = sigmoid (wxrxt + whrht   1 + br)
  h = tanh (wxhxt + r     whhht   1 + bh)
ht = z     ht   1 + (1     z)       h

(5)
(6)
(7)
(8)

where w{  }, and b{  } are all trainable parameters.
to better capture the context information on both
sides, two grus with opposite directions can be
combined to form a bidirectional gru (bigru).

4.2 model parameterization
relation network in this work, the id203
of relations given a question, p(r|q), is modeled
by the following network

exp(cid:0)v(r, q)(cid:1)
r(cid:48) exp(cid:0)v(r(cid:48), q)(cid:1)
(cid:80)

p  r (r|q) =

(9)

where the relation scoring function v(r, q) mea-
sures the similarity between the question and the
relation

v(r, q) = f (q)(cid:62)e(r)

(10)

e(r) is the trainable embedding of the relation
(randomly initialized in this work) and f (q) com-
putes the semantic question embedding. speci   -
cally, the question q is represented as a sequence
of tokens (potentially with unknown ones). then,
the question embedding model f consists of a
id27 layer to transform tokens into
distributed representations, a two-layer bigru to
capture the question semantics, and a linear layer
to project the    nal hidden states of the bigru into
the same vector space as e(r).

subject network as introduced in section 3, the
factor p(s|q, r) models the    tness of a subject s
appearing in the question q, given the main topic
is about the relation r. thus, two forces a) the
raw context expressed by q, and b) the candidate
topic described by r, jointly impact the    tness of

the subject s. for simplicity, we use two additive
terms to model the joint effect

exp(cid:0)u(s, r, q)(cid:1)
s(cid:48) exp(cid:0)u(s(cid:48), r, q)(cid:1)
(cid:80)

(11)

p  s(s|q, r) =

where u(s, r, q) is the subject scoring function,

u(s, r, q) = g(q)(cid:62)e(s) +   h(r, s)

(12)

g(q) is another semantic question embedding,
e(s) is a vector representation of a subject, h(r, s)
is the subject-relation score, and    is the weight
parameter used to trade off the two sources.
firstly, the context score g(q)(cid:62)e(s) models the
intrinsic plausibility that the subject s appears in
the question q using vector space similarity. as
g(q)(cid:62)e(s) has the same form as equation (10),
we let g adpot the same model structure as f.
however, initializing e(s) randomly and training
it with supervised signal, just like training e(r),
is insuf   cient in practice     while a large-scale
kb has millions of subjects, only thousands of
question-triple pairs are available for training. to
alleviate the problem, we seek two potential solu-
tions: a) pretrained embeddings, and b) type vec-
tor representation.

the pretrained embedding approach utilizes un-
supervised method to train entity embedings. in
particular, we employ the transe (bordes et al.,
2013), which trains the embedings of entities and
relations by enforcing e(s) + e(r) = e(o) for
every observed triple (s, r, o)     k. as there
exists other improved variants (gu et al., 2015),
transe scales the best when kb size grows.

alternatively, type vector is a    xed (not train-
able) vector representation of entities using type
information. since each entity in the kb has one
or more prede   ned types, we can encode the en-
tity as a vector (bag) of types. each dimension of
a type vector is either 1 or 0, indicating whether
the entity is associated with a speci   c type or not.
thus, the dimensionality of a type vector is equal
to the number of types in kb. under this setting,
with e(s) being a binary vector, let g(q) be a con-
tinuous vector with arbitrary value range can be
problematic. therefore, when type vector is used
as e(s), we add a sigmoid layer upon the    nal lin-
ear projection of g, squashing each element of g(q)
to the range [0, 1].

compared to the    rst solution, type vector is
fully based on the type pro   le of an entity, and
requires no training. as a bene   t, considerably

hs is de   ned as

p  (w|q)

  w = arg max
w   w(q)
c = {(s, r) : m(s,   w), s     r}

(14)

where m(s,   w) represents some prede   ned match
between the subject s and the predicted subject
mention   w.
intuitively, this pruning method re-
sembles the human behavior of    rst identifying the
subject mention with the help of context, and then
using it as the key word to search the kb.
to illustrate the effectiveness of this idea, we
parameterize p  (w|q) with a general-purpose neu-
ral labeling model, which consists of a word em-
bedding layer, two layers of bigru, and a linear-
chain conditional random    eld (crf). thus, given
a question q of length t , the score of a sequence
label con   guration y     rt is

t(cid:88)

t(cid:88)

s(y, q) =

h(q)t,yt +

ayt   1,yt

t=1

t=2

where h(q) is the hidden output of the top-layer
bigru, a is the transition matrix possesed by the
crf, and [  ]i,j indicates the matrix element on row
i collum j.
finally, the match function m(s,   w) is simply
de   ned as either strict match between an alias of
s and   w, or approximate match provided by the
freebase entity suggest api 1. note that more
elaborative match function can further boost the
performance, but we leave it for future work.

5 parameter estimation

in this section, we discuss the parameter estima-
tion for the neural models presented in section 4.
with standard parameterization, the focused la-
beling model p  (w|q) can be directly trained by
id113 (id113) and back-
propagation. thus, we omit the discussion here,
and refer readers to (huang et al., 2015) for de-
tails. also, we leave the problem of how to obtain
the training data to section 6.

5.1 decomposable log-likelihood
to estimate the parameters of p  r (r|q) and
p  s(s|r, q), id113 can be utilized to maximize the
empirical (log-)likelihood of subject-relation pairs

1the approximate match is used only when there is no
strict match. the suggest api takes a string as input, and
returns no more than 20 potentially matched entities.

figure 1: overall structure of the subject network. sigmoid
layer is added only when type vector is used as e(s).

fewer parameters are needed. also, given the type
information is discriminative enough, using type
vector will lead to easier generalization. however,
containing only type information can be very re-
strictive.

in addition to the context score, we use the
subject-relation score h(r, s) to capture the com-
patibility that s and r show up together. intuitively,
for an entity to appear in a topic characterized by
a relation, a necessary condition will be that the
entity has the relation connected to it. inspired by
this structural regularity, in the simplest manner,
we instantiate the idea with an indicator function,

h(r, s) = 1(s     r)

(13)

as there exists other more sophisticated statistical
parameterizations, the proposed approach is able
to capture the core idea of the structural regularity
without any parameter. finally, putting two scores
together, fig.1 summarizes the overall structure of
the subject network.

4.3 focused pruning
as discussed in section 3.2, id165 pruning is
still subject to large amount of noise in id136
due to many non-subject-mention id165s. moti-
vated by this problem, we propose to reduce such
noise by focusing on more probable candidates us-
ing a special-purpose sequence labeling network.
basically, a sequence labeling model is trained to
tag some consecutive tokens as the subject men-
tion. following this idea, during id136, only
the most probable id165 predicted by the model
will be retained, and then used as the subject men-
tion to generate the candidate pool c. hence, we
refer to this method as focused pruning. formally,
let w(q) be all the id165s of the question q,
p(w|q) be the id203 that the id165 w is the
subject mention of q, the focused pruning function

who created  ...  potter?    (    $)    (       )    (    ()    (    ))    (    *)   linear projection(+ sigmoid)    (    )    (    (|    ,    $)bigruwordembed.concatbigrugiven the associated question. following this idea,
let {s(i), r(i), q(i)}n
i=1 be the training dataset, the
id113 solution takes the form

n(cid:88)

(cid:16)

  id113 = arg max

log p  r (r(i)|q(i))

  r,  s

(cid:17)
+ log p  s(s(i)|r(i), q(i))

i=1

the prede   ned margin. similarly, the id168
w.r.t   s takes the form

n(cid:88)

ms(cid:88)

l(  s) =

(15)

i=1

j=1

max(cid:2)0,   s     u(s(i), r(i), q(i))
+ u(s(j), r(i), q(i))(cid:3)

(18)

n(cid:88)
n(cid:88)

i=1

i=1

note that there is no shared parameter between
p  s(s|q, r) and p  r (r|q). 2 therefore, the same so-
lution can be reached by separately optimizing the
two log terms, i.e.

  id113
r = arg max

  r

  id113
s = arg max

  s

log p  r (r(i)|q(i))

(16)

log p  s(s(i)|r(i), q(i))

it is important to point out that the decomposabil-
ity does not always hold. for example, when the
parametric form of h(s, r) depends on the embed-
ding of r, the two terms will be coupled and joint
optimization must be performed. from this per-
spective, the simple form of h(s, r) also eases the
training by inducing the decomposability.

5.2 approximation with negative samples
as the two problems de   ned by equation (16) take
the standard form of classi   cation, theoretically,
cross id178 can used as the training objective.
however, computing the partition function is often
intractable, especially for p  s(s|r, q), since there
can be millions of entities in the kb. faced with
this problem, classic solutions include contrastive
estimation (smith and eisner, 2005), importance
sampling approximation (bengio et al., 2003), and
hinge loss with negative samples (collobert and
weston, 2008).

in this work, we utilize the hinge loss with nega-
tive samples as the training objective. speci   cally,
the id168 w.r.t   r has the form

n(cid:88)

mr(cid:88)

i=1

j=1

max(cid:2)0,   r     v(r(i), q(i))
+ v(r(j), q(i))(cid:3)

(17)

l(  r) =

where r(j) is one of the mr negative samples (i.e.
s(i) (cid:54)    r(j)) randomly sampled from r, and   r is

2id27s are not shared across models.

despite the negative sample based approximation,
there is another practical dif   culty when type vec-
tor is used as the subject representation. speci   -
cally, computing the value of u(s(j), r(i), q(i)) re-
quires to query the kb for all types of each nega-
tive sample s(j). so, when ms is large, the train-
ing can be extremely slow due to the limited band-
width of kb query. consequently, under the set-
ting of type vector, we instead resort to the follow-
ing type-wise binary cross-id178 loss

(cid:16)
  l(  s) =     n(cid:88)
k(cid:88)
(cid:3) log(cid:2)1     g(q(i))k
+(cid:2)1     e(s(i))k

e(s(i))k log g(q(i))k

(cid:3)(cid:17) (19)

k=1

i=1

where k is the total number of types, g(q)k and
e(s(i))k are the k-th element of g(q) and e(s(i))
respectively.
intuitively, with sigmoid squashed
output, g(q) can be regarded as k binary classi-
   ers, one for each type. hence, g(q)k reprents the
predicted id203 that the subject is associated
with the k-th type.

6 experiments
in this section, we conduct experiments to evaluate
the proposed system empirically.

6.1 dataset and knowledge base
we train and evaluate our method on the simple-
questions dataset3     the largest question-triple
dataset.
it consists of 108,442 questions written
in english by human annotators. each question
is paired with a subject-relation-object triple from
freebase. we follow the same splitting for train-
ing (70%), validation (10%) and testing (20%) as
(bordes et al., 2015). we use the same subset of
freebase (fb5m) as our knowledge base so that
the results are directly comparable.
it includes
4,904,397 entities, 7,523 relations, and 22,441,880
facts.

there are alternative datasets available, such
as webquestions (berant et al., 2013) and

3https://research.facebook.com/

researchers/1543934539189348

free917 (cai and yates, 2013). however, these
datasets are quite restricted in sample size     the
former includes 5,810 samples (train + test) and
the latter includes 917 ones. they are fewer than
the number of relations in freebase.

to train the focused labeling model, the infor-
mation about whether a word is part of the sub-
ject mention is needed. we obtain such informa-
tion by reverse linking from the ground-truth sub-
ject to its mention in the question. given a ques-
tion q corresponding to subject s, we match the
name and aliases of s to all id165s that can be
generated from q. once a match is found, we la-
bel the matched id165 as the subject mention.
in the case of multiple matches, only the longest
matched id165 is used as the correct one.

6.2 evaluation and baselines
for evaluation, we consider the same metric in-
troduced in (bordes et al., 2015), which takes the
prediction as correct if both the subject and rela-
tion are correctly retrieved. based on this met-
ric, we compare cfo with a few baseline systems,
which include both the memory network qa sys-
tem (bordes et al., 2015), and systems with al-
ternative components and parameterizations from
existing work (yih et al., 2014; yih et al., 2015).
we did not compare with alternative subject net-
works because the only existing method (yih et al.,
2014) relies on unique textual name of each entity,
which does not generally hold in knowledge bases
(except in reverb). alternative approaches for
pruning method, relation network, and entity rep-
resentation are described below.

pruning methods we consider two baseline
methods previously used to prune the search
space. the    rst baseline is the id165 pruning
method introduced in section 3, as it has been suc-
cessfully used in previous work (yih et al., 2014;
yih et al., 2015). basically, it establishes the
candidate pool by retaining subject-relation pairs
whose subject can be linked to one of the id165s
generated from the question. the second one is n-
gram+, a revised version of the id165 pruning
with additional heuristics (bordes et al., 2015). in-
stead of considering all id165s that can be linked
to entities in kb, heuristics related to overlapping
id165s, stop words, interrogative pronouns, and
so on are exploited to further shrink the id165
pool. accordingly, the search space is restricted to
subject-relation pairs whose subject can be linked

to one of the remaining id165s after applying the
heuristic    ltering.

relation scoring network we compare our pro-
posed method with two previously used models.
the    rst baseline is the embedding average model
(embed-avg) used in (bordes et al., 2014a; bor-
des et al., 2014b; bordes et al., 2015). basically,
it takes the element-wise average of the word em-
beddings of the question to be the question rep-
resentation. the second one is the letter-tri-gram
id98 (ltg-id98) used in (yih et al., 2014; yih
et al., 2015), where the question and relation are
separately embedded into the vector space by two
parameter shared ltg-id98s. 4 in addition, (yih
et al., 2014; yih et al., 2015) observed better per-
formance of the ltg-id98 when substituting the
subject mention with a special symbol. naturally,
this can be combined with the proposed focused
labeling, since the latter is able to identify the po-
tential subject mention in the question. so, we
train another ltg-id98 with symbolized ques-
tions, which is denoted as ltg-id98+. note that
this model is only tested when the focused labeling
pruning is used.

entity representation in section 4.2, we de-
scribe two possible ways to improve the vector
representation of the subject, transe pretrained
embedding and type vectors. to evaluate their ef-
fectiveness, we also include this variation in the
experiment, and compare their performance with
randomly initialized entity embeddings.

6.3 experiment setting
during training, all id27s are initial-
ized using the pretrained glove (pennington et al.,
2014), and then    ne tuned in subsequent train-
ing. the id27 dimension is set to
300, and the bigru hidden size 256. for pre-
training the entity embeddings using transe (see
section 4.2), only triples included in fb5m are
used. all other parameters are randomly ini-
tialized uniformly from [   0.08, 0.08], following
(graves, 2013). both hinge loss margins   s and
  r are set to 0.1. negative sampling sizes ms and
mr are both 1024.

for optimization, parameters are trained using
mini-batch adagrad (duchi et al., 2011) with mo-
mentum (pham et al., 2015). learning rates are

4in freebase, each prede   ned relation has a single human-

recognizable reference form, usually a sequence of words.

pruning
method

relation
network

memory network

entity representation

random pretrain type vec

62.9 63.9   

id165

id165+

focused
pruning

embed-avg
ltg-id98
bigru
embed-avg
ltg-id98
bigru
embed-avg
ltg-id98
ltg-id98+
bigru

39.4
32.8
43.7
53.8
46.3
58.3
71.4
67.6
70.2
75.2

42.2
36.8
46.7
57.0
50.9
61.6
71.7
67.9
70.4
75.5

50.9
45.6
55.7
58.7
56.0
62.6
72.1
68.6
71.1
75.7

table 1: accuracy on simplequestions testing set.     indi-
cates using ensembles. id165+ uses additional heuristics.
the proposed cfo (focused pruning + bigru + type vector)
achieves the top accuracy.

tuned to be 0.001 for question embedding with
type vector, 0.03 for ltg-id98 methods, and 0.02
for rest of the models. momentum rate is set to
0.9 for all models, and the mini-batch size is 256.
in addition, vertical dropout (pham et al., 2014;
zaremba et al., 2014) is used to regularize all bi-
grus in our experiment. 5

6.4 results
trained on 75,910 questions, our proposed model
and baseline methods are evaluated on the testing
set with 21,687 questions. table 1 presents the ac-
curacy of those methods. we evaluated all combi-
nations of pruning methods, relation networks and
entity representation schemes, as well as the result
from memory network, as described in section
6.1. cfo (focused pruning + bigru + type vec-
tor) achieves the best performance, outperforming
all other methods by substantial margins.

by inspecting vertically within each cell in ta-
ble 1, for the same pruning methods and entity rep-
resentation scheme, bigru based relation scor-
ing network boosts the accuracy by 3.5 % to 4.8%
compared to the second best alternative. this ev-
idence suggests the superiority of id56 in captur-
ing semantics of question utterances. surprisingly,
it turns out that embed-avg achieves better per-
formance than the more complex ltg-id98.

by inspecting table 1 horizontally, type vec-
tor based representation constantly leads to bet-
ter performance, especially when id165 pruning
is used. it suggests that under sparse supervision,
training high-quality distributed knowledge repre-

sentations remains a challenging problem. that
said, pretraining entity embeddings with transe
indeed gives better performance compared to ran-
dom initialization, indicating the future potential
of unsupervised methods in improving continuous
id99.

in addition, all systems using our proposed fo-
cused pruning method outperform their counter-
parts with alternative pruning methods. without
using ensembles, cfo is already better than the
memory network ensembles by 11.8%.
it sub-
stantiates the general effectiveness of the focused
pruning with subject labeling method regardless of
other sub-modules.

6.5 effectiveness of pruning

according to the results in section 6.4, the focused
pruning plays a critical role in achieving the best
performance. to get a deeper understanding of its
effectiveness, we analyze how the pruning meth-
ods affect the accuracy of the system. due to space
limit, we focus on systems with bigru as the re-
lation scoring function and type vector as the en-
tity representation.

table 2 summarizes the recall     the percent-
age of pruned subject-relation candidates contain-
ing the answer     and the resulting accuracy.
the single-subject case refers to the scenario that
there is only one candidate entity in cs (possi-
bly with multiple relations), and the multi-subject
case means there are multiple entities in cs. as
the table shows, focused pruning achieves com-
parable recall rate to id165 pruning.6 given
the state-of-the-art performance of sequence la-
beling systems, this result should not be surpris-
ing. thus, the difference in performances entirely
comes from their resulting accuracy. notice that
there exists a huge accuracy gap between the two
cases. essentially, in the single-candidate case, the
system only need to identify the relation based on
the more robust model p  r (r|q). in contrast, under
the multi-candidate case, the system also relies on
p  s(s|q, r), which has signi   cantly more parame-
ters to estimate, and thus is less robust. conse-
quently, by only focusing on the most probable
sub-string, the proposed focused pruning produces
much more single-candidate situations, leading to
a better overall accuracy.

5for more details, source code is available at http://

zihangdai.github.io/cfo for reference.

6less than 3% of the recalled candidates rely on approxi-

mate matching in the focused pruning.

pruning method

pruning
recall

id136 accuracy within the recalled

multi-subject case

overall
accuracy

id165
id165+
focused pruning

94.8%
92.9%
94.9% 9925 / 10705 = 92.7%

= 85.7% 12051 / 20533 = 58.7% 55.7%
= 91.3% 13460 / 20017 = 67.2% 62.6%
6482 / 9876 = 65.6% 75.7%

single-subject case
18 / 21
126 / 138

table 2: comparison of different space pruning methods. id165+ uses additional heuristics. single- and multi-subject refers
to the number of distinct subjects in candidates. the proposed focused pruning achieves best scores.

6.6 additional analysis
in the aforementioned experiments, we have kept
the focused labeling model and the subject scoring
network    xed. to further understand the impor-
tance and sensitivity of this speci   c model design,
we investigate some variants of these two models.

alternative focus with crf id56-crf based
models have achieved the state-of-the-art perfor-
mance on various sequence labeling tasks (huang
et al., 2015; lu et al., 2015). however, the la-
beling task we consider here is relatively unso-
phisticated in the sense that there are only two
categories of labels - part of subject string (sub)
or not (o). thus, it   s worth investigating whether
id56 (bigru in our case) is still a critical com-
ponent when the task gets simple. hence, we es-
tablish a crf baseline which uses traditional fea-
tures as input. speci   cally, the model is trained
with stanford crf-ner toolkit 7 on the same
reversely linked labeling data (section 6.1). for
evaluation, we directly compare the sentence level
accuracy of these two models on the test portion
of the labeling data. a sentence labeling is con-
sidered correct only when all tokens are correctly
labeled. 8 it turns out the id56-crf achieves an
accuracy of 95.5% while the accuracy of feature
based crf is only 91.2%. based on the result, we
conclude that bigru plays a crucial role in our
focused pruning module.

subject scoring with average embedding as
discussed in section 4.2, the subject network g is
chosen to be the same as f, mainly relying on a
two-layer bigru to produce the semantic ques-
tion embeding. although it is a natural choice, it
remains unclear whether the    nal performance is
sensitive to this design. motivated by this ques-
tion, we substitute the bigru with an embed-
avg model, and evalute the system performance.

7http://nlp.stanford.edu/software/

crf-ner.shtml

8as f -1 score is usually used as the metric for sequence

labeling, sentence level accuracy is more informative here.

relation network

embed-avg
ltg-id98
ltg-id98+
bigru

subject network

embed-avg bigru

71.6
68.0
70.4
75.4

72.1
68.6
71.1
75.7

table 3: system performance with different subject network
structures.

for this experiment, we always use focused prun-
ing and type vector, but vary the structure of the
relation scoring network to allow high-order inter-
action across models. the result is summarized in
table 3.
insepcting the table horizontally, when
bigru is employed as the subject network, the
accuracy is consistently higher regardless of re-
lation network structures. however, the margin
is quite narrow, especially compared to the effect
of varying the relation network structure the same
way. we suspect this difference re   ects the fact
that modeling p(s|r, q) is intrinsically more chal-
lenging than modeling p(r|q). it also suggests that
learning smooth entity representations with good
discriminative power remains an open problem.

7 conclusion

in this paper, we propose cfo, a novel approach
to single-fact id53. we employ a
conditional factoid factorization by inferring the
target relation    rst and then the target subject as-
sociated with the candidate relations. to resolve
the representation for millions of entities, we pro-
posed type-vector scheme which requires no train-
ing. our focused pruning largely reduces the can-
didate space without loss of recall rate, leading
to signi   cant improvement of overall accuracy.
compared with multiple baselines across three as-
pects, our method achieves the state-of-the-art ac-
curacy on a 108k question dataset, the largest pub-
licly available one. future work could be extend-
ing the proposed method to handle more complex
questions.

references
[bengio et al.2003] yoshua bengio, r  ejean ducharme, pas-
cal vincent, and christian janvin. 2003. a neural proba-
bilistic language model. the journal of machine learning
research, 3:1137   1155.

[berant and liang2014] jonathan berant and percy liang.
2014. id29 via id141. in proceedings
of acl, volume 7, page 92.

[berant and liang2015] jonathan berant and percy liang.
2015.
imitation learning of agenda-based semantic
parsers. transactions of the association for computa-
tional linguistics, 3:545   558.

[berant et al.2013] jonathan berant, andrew chou, roy
frostig, and percy liang. 2013. id29 on free-
in proceedings of the
base from question-answer pairs.
2013 conference on empirical methods in natural lan-
guage processing, pages 1533   1544.

[bollacker et al.2008] kurt bollacker, colin evans, praveen
paritosh, tim sturge, and jamie taylor. 2008. freebase: a
collaboratively created graph database for structuring hu-
man knowledge. in proceedings of the 2008 acm sig-
mod international conference on management of data,
pages 1247   1250. acm.

[bordes et al.2013] antoine bordes, nicolas usunier, alberto
garcia-duran, jason weston, and oksana yakhnenko.
2013.
translating embeddings for modeling multi-
relational data. in advances in neural information pro-
cessing systems, pages 2787   2795.

[bordes et al.2014a] antoine bordes, sumit chopra, and ja-
son weston. 2014a. id53 with subgraph
in proceedings of the 2014 conference
embeddings.
on empirical methods in natural language processing,
pages 615   620.

[bordes et al.2014b] antoine bordes,

jason weston, and
nicolas usunier. 2014b. open id53 with
weakly supervised embedding models. in machine learn-
ing and knowledge discovery in databases, pages 165   
180. springer.

[bordes et al.2015] antoine bordes, nicolas usunier, sumit
chopra, and jason weston.
2015. large-scale sim-
ple id53 with memory networks. arxiv
preprint arxiv:1506.02075.

[cai and yates2013] qingqing cai and alexander yates.
2013. large-scale id29 via schema match-
in acl (1), pages 423   433.
ing and lexicon extension.
citeseer.

[carlson et al.2010] andrew carlson,

justin betteridge,
bryan kisiel, burr settles, estevam r hruschka jr, and
2010. toward an architecture for
tom m mitchell.
in aaai, volume 5,
never-ending language learning.
page 3.

[cho et al.2014] kyunghyun cho, bart van merrienboer,
c   aglar g  ulc  ehre, dzmitry bahdanau, fethi bougares,
holger schwenk, and yoshua bengio. 2014. learning
phrase representations using id56 encoder-decoder for
id151. in proceedings of the 2014
conference on empirical methods in natural language
processing, pages 1724   1734.

[collobert and weston2008] ronan collobert and jason we-
ston. 2008. a uni   ed architecture for natural language
processing: deep neural networks with multitask learn-
ing. in proceedings of the 25th international conference
on machine learning, pages 160   167. acm.

[duchi et al.2011] john duchi, elad hazan, and yoram
singer. 2011. adaptive subgradient methods for online
learning and stochastic optimization. the journal of ma-
chine learning research, 12:2121   2159.

[fader et al.2013] anthony fader, luke s zettlemoyer, and
oren etzioni. 2013. paraphrase-driven learning for open
id53. in acl (1), pages 1608   1618. cite-
seer.

[graves2013] alex graves.

generating se-
quences with recurrent neural networks. arxiv preprint
arxiv:1308.0850.

2013.

[gu et al.2015] kelvin gu, john miller, and percy liang.
2015. traversing id13s in vector space. in
proceedings of the 2015 conference on empirical meth-
ods in natural language processing, pages 318   327.

[hermann et al.2015] karl moritz hermann, tomas kocisky,
edward grefenstette, lasse espeholt, will kay, mustafa
suleyman, and phil blunsom. 2015. teaching machines
to read and comprehend. in advances in neural informa-
tion processing systems, pages 1684   1692.

[huang et al.2015] zhiheng huang, wei xu, and kai yu.
2015. bidirectional lstm-crf models for sequence tag-
ging. corr, abs/1508.01991.

[iyyer et al.2014] mohit

iyyer,

boyd-graber,
leonardo claudino, richard socher, and hal daum  e iii.
2014. a neural network for factoid id53
in empirical methods in natural
over paragraphs.
language processing.

jordan

[kumar et al.2015] ankit kumar, ozan irsoy, jonathan su,
james bradbury, robert english, brian pierce, peter on-
druska, ishaan gulrajani, and richard socher. 2015. ask
me anything: dynamic memory networks for natural lan-
guage processing. arxiv preprint arxiv:1506.07285.

[kwiatkowski et al.2013] tom kwiatkowski, eunsol choi,
yoav artzi, and luke zettlemoyer. 2013. scaling se-
mantic parsers with on-the-   y ontology matching. in pro-
ceedings of the 2013 conference on empirical methods in
natural language processing.

[lee et al.2015] moontae lee, xiaodong he, wen-tau yih,
jianfeng gao, li deng, and paul smolensky. 2015. rea-
soning in vector space: an exploratory study of question
answering. arxiv preprint arxiv:1511.06426.

[lehmann et al.2014] jens lehmann, robert

isele, max
jakob, anja jentzsch, dimitris kontokostas, pablo n
mendes, sebastian hellmann, mohamed morsey, patrick
van kleef, s  oren auer, et al. 2014. dbpedia-a large-scale,
multilingual knowledge base extracted from wikipedia.
semantic web journal, 5:1   29.

[liang et al.2011] percy liang, michael i jordan, and dan
klein. 2011. learning dependency-based compositional
semantics. in association for computational linguistics
(acl), pages 590   599.

[liang et al.2013] percy liang, michael i jordan, and dan
klein. 2013. learning dependency-based compositional
semantics. computational linguistics, 39(2):389   446.

[yih et al.2014] wen-tau yih, xiaodong he, and christopher
meek. 2014. id29 for single-relation ques-
tion answering. in proceedings of acl.

[yih et al.2015] wen-tau yih, ming-wei chang, xiaodong
he, and jianfeng gao. 2015. id29 via staged
query graph generation: id53 with knowl-
edge base. in proceedings of acl.

[zaremba et al.2014] wojciech zaremba, ilya sutskever, and
oriol vinyals. 2014. recurrent neural network regular-
ization. corr, abs/1409.2329.

[zelle and mooney1996] john m zelle and raymond j
mooney. 1996. learning to parse database queries using
inductive logic programming. in proceedings of the na-
tional conference on arti   cial intelligence, pages 1050   
1055.

[lu et al.2015] zefu lu, lei li, and wei xu. 2015. twisted
in bay

recurrent network for id39.
area machine learning symposium.

[peng et al.2015] baolin peng, zhengdong lu, hang li, and
kam-fai wong. 2015. towards neural network-based
reasoning. arxiv preprint arxiv:1508.05508.

[pennington et al.2014] jeffrey pennington, richard socher,
and christopher d. manning. 2014. glove: global vec-
tors for word representation. in proceedings of the 2014
conference on empirical methods in natural language
processing, pages 1532   1543.

[pham et al.2014] vu pham, th  eodore bluche, christopher
kermorvant, and j  er  ome louradour. 2014. dropout im-
proves recurrent neural networks for handwriting recogni-
tion. in frontiers in handwriting recognition (icfhr),
2014 14th international conference on, pages 285   290.
ieee.

[pham et al.2015] hieu pham, zihang dai, and lei li. 2015.
on optimization algorithms for recurrent networks with
long short-term memory. in bay area machine learning
symposium.

[reddy et al.2014] siva reddy, mirella lapata, and mark
steedman. 2014. large-scale id29 without
question-answer pairs. transactions of the association for
computational linguistics, 2:377   392.

[smith and eisner2005] noah a smith and jason eisner.
2005. contrastive estimation: training log-linear mod-
in proceedings of the 43rd an-
els on unlabeled data.
nual meeting on association for computational linguis-
tics, pages 354   362. association for computational lin-
guistics.

[suchanek et al.2007] fabian m suchanek, gjergji kasneci,
and gerhard weikum. 2007. yago: a core of semantic
knowledge. in proceedings of the 16th international con-
ference on world wide web, pages 697   706. acm.

[tang and mooney2001] lappoon r tang and raymond j
mooney. 2001. using multiple clause constructors in in-
ductive logic programming for id29. in ma-
chine learning: ecml 2001, pages 466   477. springer.

[weston et al.2015] jason weston, sumit chopra, and an-
toine bordes. 2015. memory networks. in international
conference on learning representations (iclr2015).

[weston et al.2016] jason weston, antoine bordes, sumit
chopra, and tomas mikolov. 2016. towards ai-complete
id53: a set of prerequisite toy tasks.
in
international conference on learning representations
(iclr2016).

[yang et al.2014] min-chul yang, nan duan, ming zhou,
and hae-chang rim. 2014. joint relational embeddings
for knowledge-based id53. in proceedings
of the 2014 conference on empirical methods in natural
language processing, pages 645   650.

[yao and van durme2014] xuchen yao

and benjamin
van durme. 2014. information extraction over structured
data: id53 with freebase. in proceedings
of acl.

