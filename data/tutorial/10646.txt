de-identi   cation of patient notes with recurrent neural networks

franck dernoncourt   , ji young lee   , peter szolovits

mit

cambridge, ma, usa

{francky,jjylee,psz}@mit.edu

  ozlem uzuner
suny albany

albany, ny, usa

ouzuner@albany.edu

6
1
0
2

 

n
u
j
 

0
1

 
 
]
l
c
.
s
c
[
 
 

1
v
5
7
4
3
0

.

6
0
6
1
:
v
i
x
r
a

abstract

objective: patient notes in electronic health records (ehrs) may contain critical information for medical
investigations. however, the vast majority of medical investigators can only access de-identi   ed notes,
in order to protect the con   dentiality of patients. in the united states, the health insurance portability
and accountability act (hipaa) de   nes 18 types of protected health information (phi) that needs to be re-
moved to de-identify patient notes. manual de-identi   cation is impractical given the size of ehr databases,
the limited number of researchers with access to the non-de-identi   ed notes, and the frequent mistakes of
human annotators. a reliable automated de-identi   cation system would consequently be of high value.
materials and methods: we introduce the    rst de-identi   cation system based on arti   cial neural networks
(anns), which requires no handcrafted features or rules, unlike existing systems. we compare the perfor-
mance of the system with state-of-the-art systems on two datasets: the i2b2 2014 de-identi   cation challenge
dataset, which is the largest publicly available de-identi   cation dataset, and the mimic de-identi   cation
dataset, which we assembled and is twice as large as the i2b2 2014 dataset.
results: our ann model outperforms the state-of-the-art systems. it yields an f1-score of 97.85 on the
i2b2 2014 dataset, with a recall 97.38 and a precision of 97.32, and an f1-score of 99.23 on the mimic
de-identi   cation dataset, with a recall 99.25 and a precision of 99.06.
conclusion: our    ndings support the use of anns for de-identi   cation of patient notes, as they show
better performance than previously published systems while requiring no feature engineering.

introduction and related work

1
in many countries such as the united states, med-
ical professionals are strongly encouraged to adopt
electronic health records (ehrs) and may face    -
nancial penalties if they fail to do so (desroches
et al., 2013; wright et al., 2013). the centers for
medicare & medicaid services have paid out more
than $30 billion in ehr incentive payments to hos-
pitals and providers who have attested to meaningful
use as of march 2015. medical investigations may
greatly bene   t from the resulting increasingly large
ehr datasets. one of the key components of ehrs
is patient notes:
the information they contain can

    these authors contributed equally to this work.

be critical for a medical investigation because much
information present in texts cannot be found in the
other elements of the ehr. however, before patient
notes can be shared with medical investigators, some
types of information, referred to as protected health
information (phi), must be removed in order to pre-
serve patient con   dentiality.
in the united states,
the health insurance portability and accountabil-
ity act (hipaa) (of   ce for civil rights, 2002) de-
   nes 18 different types of phi, ranging from patient
names to phone numbers. table 1 presents the ex-
haustive list of phi types as de   ned by hipaa.

the task of removing phi from a patient note
is referred to as de-identi   cation, since the patient

cannot be identi   ed once phi is removed. de-
identi   cation can be either manual or automated.
manual de-identi   cation means that the phi are la-
beled by human annotators. there are three main
shortcomings of this approach. first, only a re-
stricted set of individuals is allowed to access the
identi   ed patient notes,
thus the task cannot be
crowdsourced. second, humans are prone to mis-
takes. (neamatullah et al., 2008) asked 14 clinicians
to detect phi in approximately 130 patient notes: the
results of the manual de-identi   cation varied from
clinician to clinician, with recall ranging from 0.63
to 0.94. (douglass et al., 2005; douglas et al., 2004)
reported that annotators were paid us$50 per hour
and read 20,000 words per hour at best. as a matter
of comparison, the mimic dataset (goldberger et
al., 2000; saeed et al., 2011), which contains data
from 50,000 intensive care unit (icu) stays, con-
sists of 100 million words. this would require 5,000
hours of annotation, which would cost us$250,000
at the same pay rate. given the annotators    spotty
performance, each patient note would have to be an-
notated by at least two different annotators, so it
would cost at least us$500,000 to de-identify the
notes in the mimic dataset.

automated de-identi   cation systems can be clas-
si   ed into two categories: rule-based systems and
machine-learning-based systems. rule-based sys-
tems typically rely on patterns, expressed as reg-
ular expressions and gazetteers, de   ned and tuned
by humans. they do not require any labeled data
(aside from labels required for evaluating the sys-
tem), and are easy to implement, interpret, main-
tain, and improve, which explains their large pres-
ence in the industry (chiticariu et al., 2013). how-
ever, they need to be meticulously    ne-tuned for
each new dataset, are not robust to language changes
(e.g., variations in word forms, typographical errors,
or infrequently used abbreviations), and cannot eas-
ily take into account the context (e.g.,    mr. parkin-
son    is phi, while    parkinson   s disease    is not phi).
rule-based systems are described in (berman, 2003;
beckwith et al., 2006; fielstein et al., 2004; friedlin
and mcdonald, 2008; gupta et al., 2004; morrison
et al., 2009; neamatullah et al., 2008; ruch et al.,
2000; sweeney, 1996; thomas et al., 2002).

to alleviate some downsides of the rule-based
systems, there have been many attempts to use su-

pervised machine learning algorithms to de-identify
patient notes by training a classi   er to label each
word as phi or not phi, sometimes distinguish-
ing between different phi types. common sta-
tistical methods include id90 (szarvas et
al., 2006), id148, support vector ma-
chines (guo et al., 2006; uzuner et al., 2008; hara,
2006), and conditional random    elds (aberdeen et
al., 2010), the latter being employed in most of the
state-of-the-art systems. for a thorough review of
existing systems, see (meystre et al., 2010; stubbs
et al., 2015). all these methods share two down-
sides:
they require a decent sized labeled dataset
and much feature engineering. as with rules, qual-
ity features are challenging and time-consuming to
develop.

recent approaches to natural language process-
ing based on arti   cial neural networks (anns) do
not require handcrafted rules or features, as they
can automatically learn effective features by per-
forming composition over tokens which are repre-
sented as vectors, often called token embeddings.
the token embeddings are jointly learned with the
other parameters of the ann. they can be initial-
ized randomly, but can be pre-trained using large
unlabeled datasets typically based on token co-
occurrences (mikolov et al., 2013b; collobert et al.,
2011; pennington et al., 2014). the latter often per-
forms better, since the pre-trained token embeddings
explicitly encode many linguistic regularities and
patterns. as a result, methods based on anns have
shown promising results for various tasks in natu-
ral language processing, such as language model-
ing (mikolov et al., 2010), text classi   cation (socher
et al., 2013; kim, 2014; blunsom et al., 2014; lee
and dernoncourt, 2016), id53 (we-
ston et al., 2015; wang and nyberg, 2015), machine
translation (bahdanau et al., 2014; tamura et al.,
2014; sundermeyer et al., 2014), as well as named
entity recognition (collobert et al., 2011; lample
et al., 2016; labeau et al., 2015). a few meth-
ods also use vector representations of characters as
inputs in order to either replace or augment token
embeddings (kim et al., 2015; lample et al., 2016;
labeau et al., 2015).

inspired by the performance of anns for var-
ious other nlp tasks,
this article introduces the
   rst de-identi   cation system based on anns. un-

phi categories
age

contact

date

id

location

name

profession

phi types
ages     90
ages < 90
telephone and fax numbers
electronic mail addresses
urls or ip addresses*
dates (month and day parts)
year
holidays
day of the week
social security numbers
medical record numbers
account numbers
certi   cate or license numbers
vehicle or device identi   ers
biometric identi   ers or full face photographic images*
addresses and their components smaller than a state
state
country
employers
hospital name
ward name
names of patients and family members
provider name
profession

hipaa

i2b2 mimic

x

x
x
x
x

x
x
x
x
x
x
x

x

x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x

x

x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x

table 1: phi types as de   ned by hipaa, i2b2, and mimic. phi categories are de   ned in the i2b2 dataset. the phi types marked
with * do not appear in either dataset.

like other machine learning based systems, anns
do not require manually-curated features, such as
those based on id157 and gazetteers.
we show that anns achieve state-of-the-art results
on de-identi   cation of two different datasets for pa-
tient notes, the i2b2 2014 challenge dataset and the
mimic dataset.

2 methods and materials

we    rst present a de-identi   er we developed based
on a conditional random    eld (crf) model in sec-
tion 2.1. this de-identi   er yields state-of-the-art
results on the i2b2 2014 dataset, which is the ref-
erence dataset for comparing de-identi   cation sys-
tems. this system will be used as a challenging
baseline for the ann model that we will present in
section 2.2. the ann model outperforms the crf
model, as outlined in section 3.

2.1 crf model

in the crf model, each patient note is tokenized
and features are extracted for each token. during
the training phase, the crf   s parameters are opti-
mized to maximize the likelihood of the gold stan-
dard labels. during the test phase, the crf pre-
dicts the labels. the performance of a crf model
depends mostly on the quality of its features. we
used a combination of id165, morphological, or-
thographic, and gazetteer features. these are sim-
ilar to features used in the best-performing crf-
based competitors in the i2b2 challenge (yang and
garibaldi, 2015; liu et al., 2015).

in order to effectively incorporate context when
predicting a label, the features for a given token are
computed based on that token and on the four sur-
rounding tokens.

2.2 ann model
the main components of the ann model are re-
current neural networks (id56s). in particular, we
use a type of id56 called long short term mem-
ory (lstm) (hochreiter and schmidhuber, 1997),
as discussed in section 2.2.1.

the system is composed of three layers:

tion 2.2.2),

    character-enhanced token embedding layer (sec-
    label prediction layer (section 2.2.3),
    label
sequence optimization layer
2.2.4).

(section

the character-enhanced token embedding layer
maps each token into a vector representation. the
sequence of vector representations corresponding to
a sequence of tokens are input to the label prediction
layer, which outputs the sequence of vectors con-
taining the id203 of each label for each corre-
sponding token. lastly, the sequence optimization
layer outputs the most likely sequence of predicted
labels based on the sequence of id203 vec-
tors from the previous layer. all layers are learned
jointly. figure 1 shows the ann architecture.

in the following, we denote scalars in italic low-
ercase (e.g., k, bf ), vectors in bold lowercase (e.g.,
s, xi), and matrices in italic uppercase (e.g., wf )
symbols. we use the colon notations xi:j and vi:j
to denote the sequence of scalars (xi, . . . , xj), and
vectors (vi, vi+1, . . . , vj), respectively.
2.2.1 bidirectional lstm

id56 is a neural network architecture designed to
handle input sequences of variable sizes, but it fails
to model long term dependencies. lstm is a type of
id56 that mitigates this issue by keeping a memory
cell that serves as a summary of the preceding ele-
ments of an input sequence. more speci   cally, given
a sequence of vectors x1, x2, . . . , xn, at each step
t = 1, . . . , n, an lstm takes as input xt, ht   1, ct   1
and produces the hidden state ht and the memory
cell ct based on the following formulas:
it =   (wi [xt; ht   1; ct   1] + bi)
ct = (1     it) (cid:12) ct   1

+ it (cid:12) tanh(wc [xt; ht   1] + bc)]

ot =   (wo [xt; ht   1; ht   1] + bo)
ht = ot (cid:12) tanh(ct)

are weight matrices

and
where wi, wc, wo
bi, bc, bo are bias vectors used in the input gate,
memory cell, and output gate calculations,
re-
spectively. the symbols   (  ) and tanh(  ) refer to
the element-wise sigmoid and hyperbolic tangent
functions, and (cid:12) is the element-wise multiplication.
h0 = c0 = 0.

      
h 2, . . . ,

a bidirectional lstm consists of a forward
lstm and a backward lstm, where the for-
      
      
ward lstm calculates the forward hidden states
h n), and the backward lstm calcu-
      
(
h 1,
lates the backward hidden states (
h n)
by feeding the input sequence in the backward order,
from xn to x1.

      
h 2, . . . ,

      
h 1,

depending on the application of the lstm, one
might need an output sequence corresponding to
each element in the sequence, or a single output that
summarizes the whole sequence. in the former case,
the output sequence h1, h2, . . . , hn of the lstm is
obtained by concatenating the hidden states of the
      
forward and the backward lstms for each element
i.e.,
h t) for t = 1, . . . , n. in the lat-
ter case, the output is obtained by concatenating the
last hidden states of the forward and the backward
lstms i.e.,

      
ht = (

      
h = (

      
h t;

      
h n).

      
h n;

2.2.2 character-enhanced token embedding

layer

the character-enhanced token embedding layer
takes a token as input and outputs its vector repre-
sentation. the latter results from the concatenation
of two different types of embeddings: the    rst one
directly maps a token to a vector, while the second
one comes from the output of a character-level token
encoder.
the direct mapping vt (  ) from token to vec-
tor, often called a token (or word) embedding,
can be pre-trained on large unlabeled datasets us-
ing programs such as id97 (mikolov et al.,
2013b; mikolov et al., 2013a; mikolov et al., 2013c)
or glove (pennington et al., 2014), and can be
learned jointly with the rest of the model. token
embeddings, often learned by sampling token co-
occurrence distributions, have desirable properties
such as locating semantically similar words closely
in the vector space, hence leading to state-of-the-art
performance for various tasks.

while the token embeddings capture the seman-

figure 1: architecture of the arti   cial neural network (ann) model. id56 stands for recurrent neural network. the type of id56
used in this model is long short term memory (lstm). n is the number of tokens, and xi is the ith token. vt is the mapping
from tokens to token embeddings. (cid:96)(i) is the number of characters and xi,j is the jth character in the ith token. vc is the mapping
      
from characters to character embeddings. ei is the character-enhanced token embeddings of the ith token.
di is the output of the
lstm of label prediction layer, ai is the id203 vector over labels, yi is the predicted label of the ith token.

tics of tokens to some degree, they may still suf-
fer from data sparsity. for example, they cannot
account for out-of-vocabulary tokens, misspellings,
and different noun forms or verb endings. one so-
lution to remediate some of these issues would be to
lemmatize tokens before training, but this approach
may fail to retain some useful information such as
the distinction between some verb and noun forms.
we address this issue by using character-based
token embeddings, which incorporate each individ-
ual character of a token to generate its vector rep-
resentation. this approach enables the model to
learn sub-token patterns such as morphemes (e.g.,
suf   x or pre   x) and roots, thereby capturing out-
of-vocabulary tokens, different surface forms, and
other information not contained in the token embed-
dings.

let xi,1, . . . , xi,(cid:96)(i) be the sequence of characters
that comprise the ith token xi, where (cid:96)(i) is the num-
ber of characters in xi. the character-level token

encoder generates the character-based token embed-
ding of xi by    rst mapping each character xi,j to a
vector vc(xi,j), called a character embedding, via
the mapping vc(  ). then the sequence vc(xi,j) is
passed to a bidirectional lstm, which outputs the
character-based token embedding

      
bi

as a result, the    nal output ei of the character-
enhanced token embedding layer for ith token xi is
the concatenation of the token embedding vt (xi)
      
bi . in
and the character-based token embedding
summary, when the character-enhanced token em-
bedding layer receives a sequence of tokens x1:n as
input, it will output the sequence of token embed-
dings e1:n.

2.2.3 label prediction layer

the label prediction layer takes as input the se-
quence of vectors e1:n,
the outputs of the
character-enhanced token embedding layer, and out-
puts a1:n, where the tth element of an is the proba-

i.e.,

id56id56id56id56id56id56id56id56id56id56id56id56id56id56id56id56id56id56id56id56id56id56id56id56vtvtvtvcvcvcvcvcvcvcvcvcbility that the nth token has the label t. the labels
are either one of the phi types or non-phi. for ex-
ample, if one aims to predict all 18 hipaa-de   ned
phi types, there would be 19 different labels.

the label prediction layer contains a bidirectional
lstm that takes the input sequence e1:n and gener-
      
ates the corresponding output sequence
d1:n. each
      
di of the lstm is given to a feed-forward
output
neural network with one hidden layer, which outputs
the corresponding id203 vector ai.
2.2.4 label sequence optimization layer

the label sequence optimization layer takes the
sequence of id203 vectors a1:n from the label
prediction layer as input, and outputs a sequence of
labels y1:n, where yi is the label assigned to the to-
ken ti.

the simplest strategy to select the label yi would
be to choose the label that has the highest id203
in ai, i.e. yi = argmaxk ai[k]. however, this greedy
approach fails to take into account the dependencies
between subsequent labels. for example, it may be
more likely to have a token with the phi type state
followed by a token with the phi type zip than any
other phi type. even though the label prediction
layer has the capacity to capture such dependencies
to a certain degree, it may be preferable to allow the
model to directly learn these dependencies in the last
layer of the model.

one way to model such dependencies is to incor-
porate a matrix t that contains the transition proba-
bilities between two subsequent labels. t [i, j] is the
id203 that a token with label i is followed by a
token with the label j. the score of a label sequence
y1:n is de   ned as the sum of the probabilities of in-
dividual labels and the transition probabilities:

n(cid:88)

n(cid:88)

s(y1:n) =

ai[yi] +

t [yi   1, yi].

i=1

i=2

these scores can be turned into probabilities of the
label sequences by taking a softmax function over all
possible label sequences. during the training phase,
the objective is to maximize the log id203 of
the gold label sequence. in the testing phase, given
an input sequence of tokens, the corresponding se-
quence of predicted labels is chosen as the one that
maximizes the score.

3 experiments and results
3.1 datasets
we evaluate our two models on two datasets: i2b2
2014 and mimic de-identi   cation datasets. the
i2b2 2014 dataset was released as part of the 2014
i2b2/uthealth shared task track 1 (stubbs et al.,
2015).
it is the largest publicly available dataset
for de-identi   cation. ten teams participated in this
shared task, and 22 systems were submitted. as a
result, we used the i2b2 2014 dataset to compare our
models against state-of-the-art systems.

the mimic de-identi   cation dataset was cre-
ated for this work as follows. the mimic-iii
dataset (johnson et al., 2016; goldberger et al.,
2000; saeed et al., 2011) contains data for 61,532
icu stays over 58,976 hospital admissions for
46,520 patients, including 2 million patient notes.
in order to make the notes publicly available, a rule-
based de-identi   cation system (douglass, 2005;
douglass et al., 2005; douglas et al., 2004) was
written for the speci   c purpose of de-identifying pa-
tient notes in mimic, leveraging dataset-speci   c in-
formation such as the list of patient names or ad-
dresses. the system favors recall over precision:
there are virtually no false negatives, while there are
numerous false positives. to create the gold stan-
dard mimic de-identi   cation dataset, we selected
1,635 discharge summaries, each belonging to a dif-
ferent patient, containing a total of 60.7k phi in-
stances. we then annotated the phi instances de-
tected by the rule-based system as true positives or
false positives. we found that 15% of the phi in-
stances detected by the rule-based system were false
positives.

table 1 introduces the phi types and table 2
presents the datasets    sizes. for the test set, we used
the of   cial test set for the i2b2 dataset, which is
40% of the dataset; we randomly selected 20% of
the mimic dataset as the test set for this dataset.

vocabulary size
number of notes
number of tokens
number of phis
number of phi tokens

i2b2
46,803
1,304
984,723
28,867
41,355

mimic
69,525
1,635

2,945,228

60,725
78,633

table 2: overview of the i2b2 and mimic datasets.

3.2 id74
to assess the performance of the two models, we
computed the precision, recall, and f1-score. let
tp be the number of true positives, fp the number
of false positives, and fn the number of false neg-
atives. precision, recall, and f1-score are de   ned
as follows: precision = t p
t p +f n ,
and f1-score = 2   precision   recall
intuitively, preci-
precision+recall .
sion is the proportion of the predicted phi labels that
are gold labels, recall is the proportion of the gold
phi labels that are correctly predicted, and f1-score
is the harmonic mean of precision and recall.

t p +f p , recall = t p

3.3 training and hyperparameters
the model is trained using stochastic gradient de-
scent, updating all parameters, i.e., token embed-
dings, character embeddings, parameters of bidirec-
tional lstms, and transition probabilities, at each
gradient step. for id173, dropout is applied
to the character-enhanced token embeddings before
the label prediction layer. below are the choices of
hyperparameters and token embeddings, optimized
using a subset of the training set:

mension: 25

    character embedding dimension: 25
    character-based token embedding lstm di-
    token embedding dimension: 100
    label prediction lstm dimension: 100
    dropout id203: 0.5
we tried pre-training token embeddings on the
i2b2 2014 dataset and the mimic dataset1 using
id97 and glove. both id97 and glove
were trained using a window size of 10, a minimum
vocabulary count of 5, and 15 iterations. additional
parameters of id97 were the negative sampling
and the model type, which were set to 10 and skip-
gram, respectively. we also experimented with the
publicly available2 token embeddings such as glove
trained on wikipedia and gigaword 5 (parker et al.,
2011). the results were quite robust to the choice
of the pre-trained token embeddings. the glove
embeddings trained on wikipedia articles yielded
slightly better results, and we chose them for the rest
of this work.

1for mimic, we used the entire dataset containing 2 million

notes and 800 million tokens.

2http://nlp.stanford.edu/projects/glove/

3.4 results
all results were computed using the of   cial evalua-
tion script from the i2b2 2014 de-identi   cation chal-
lenge. table 3 presents the main results, based on
binary token-based precision, recall, and f1-score
for hipaa-de   ned phi only. these phi types are
the most important since only those are required
to be removed by law. on the i2b2 dataset, our
ann model has a higher f1-score and recall than
our crf model as well as the best system from
the i2b2 2014 de-identi   cation challenge, which was
the nottingham system (yang and garibaldi, 2015).
the only freely available, off-the-shelf program for
de-identi   cation, called the mitre identi   cation
scrubber toolkit (mist) (aberdeen et al., 2010),
performed poorly. combining the outputs of our
ann and crf models, by considering a token to
be phi if it is identi   ed as such by either model, fur-
ther increases the performance in terms of f1-score
and recall.

it should be noted that the nottingham system
was speci   cally    ne-tuned for the i2b2 dataset as
well as the i2b2 evaluation script. for example,
the nottingham system post-processes the detected
phi terms in order to match the offset of the gold
phi tokens, such as modifying    mr:6746781    to
   6746782    and    mwfs    to    m   ,    w   ,    f   ,    s   .

on the mimic dataset, our ann model also has a
higher f1-score and recall than our crf model. in-
terestingly, combining the outputs of our ann and
crf models did not increase the f1-score, because
precision was negatively impacted. however, the
recall did bene   t from combining the two models.
mist was much more competitive on this dataset.

we calculated the statistical signi   cance of the
differences in precision, recall, and f1-score be-
tween the crf and ann models using approximate
randomization with 9999 shuf   es. the signi   cance
levels of the differences in precision, recall, and f1-
score are 0.37, 0.02, 0.22 for the i2b2 dataset, and
0.08, 0.00, 0.00 for the mimic dataset, respectively.

3.5 error analysis
figure 2 shows the binary token-based f1-scores
for each phi category. the ann model outper-
forms the crf model on all categories for both
datasets, with the exception of the id (which mostly

model

nottingham

mist
crf
ann

crf + ann

precision
99.000
95.288
98.560
98.320
97.920

i2b2
recall
96.680
75.691
96.528
97.380
97.835

f1-score
97.680
84.367
97.533
97.848
97.877

precision

-

97.739
99.060
99.208
98.820

mimic
recall

-

97.164
98.987
99.251
99.398

f1-score

-

97.450
99.023
99.229
99.108

table 3: performance (%) on the phi as de   ned in the hipaa. we evaluated the systems based on the detection of phi token versus
non-phi token (i.e., binary hipaa token-based evaluation). the best performance for each metric on each dataset is highlighted in
bold. nottingham is the best performing system from the 2014 i2b2/uthealth shared task track 1. mist, the mitre identi   cation
scrubber toolkit, is a freely available de-identi   cation program. crf is the model based on conditional random field, ann is
the model based on arti   cial neural network, and crf+ann is the result obtained by combining the outputs of the crf model
and the ann model. the nottingham system could not be run on the mimic dataset, as it is not publicly available.

figure 2: binary token-based f1-scores for each phi category. the evaluation is based on phi types that are de   ned by hipaa
as well as additional phi types speci   c to each dataset. each phi category and the corresponding phi types are de   ned in table
1. the profession category exists only in the i2b2 dataset, and was removed from the graph to avoid distorting the y-axis:
the f1-scores are 72.014, 82.035, and 81.664 with the crf, ann, and crf+ann, respectively. for the same reason, the age
category in mimic was removed: the f1-scores are 80.851, 81.481, and 92.308 with the crf, ann, and crf+ann, respectively.

contains medical record numbers) category in the
i2b2 dataset. this is due to the fact that the crf
model uses sophisticated regular expression fea-
tures that are tailored to detect id patterns such as
   38:z8912708g   .

another interesting difference between the ann
and the crf results is the profession category:
the ann signi   cantly outperforms the crf. the
reason behind this result is that the embeddings of
the tokens that represent a profession tend to be
close in the token embedding space, which allows
the ann model to generalize well. we tried assem-
bling various gazetteers for the profession cate-
gory, but all of them were performing signi   cantly
worse than the ann model.

table 4 presents some examples of gold phi in-
stances correctly predicted by the ann model that

the crf model failed to predict, and conversely.
this illustrates that the ann model ef   ciently copes
with the diversity of the contexts in which tokens
appear, whereas the crf model can only address
the contexts that are manually encoded as features.
in other words, the ann model   s intrinsic    exibil-
ity allows it to better capture the variance in hu-
man languages than the crf model. for exam-
ple, it would be challenging and time-consuming to
engineer features for all possible contexts such as
   had a stroke at 80   ,    quit smoking in 08   ,    on the
29th of this month   , and    his friend epstein   . the
ann model is also very robust to variations in sur-
face forms, such as misspellings (e.g.,    in teh late
60s   ,    khazakhstani   ,    01/19/:0   ),
id121s
(e.g.,    results02/20/2087   ,    mc # 0937884date   ),
and different phrases referring to the same seman-

agecontactdateidlocationnamephi category9293949596979899100f1-score (%)i2b2contactdateidlocationnamephi category9293949596979899100f1-score (%)mimiccrfanncrf+annphi category

ann

age

father had a stroke at 80 and died of ?another stroke at age 83.

personal data and overall health: now 63, despite his

fh: father: died @ 52 from etoh abuse (unclear exact etiology)

tobacco: smoked from age 7 to 15, has not smoked since 15.
history of present illness 86f reports worsening b/l leg pain.

by phone, dr. ivan guy. call w/ questions 86383. keith gilbert,
h/o paroxysmal a   b vna 171-311-7974 ======= medications

contact

crf

hpi: 53rhm who going to bed wednesday was in usoh, but
tobacco: quit at 38 y/o; etoh: 1-2 beers/week; caffeine:

date

during his may hospitalization he had dysphagia

she is looking forward to a good christmas. she is here today

id

location

name

profession

social history: divorced, quit smoking in 08, sober x 10 yrs,

she is to see him on the 29th of this month at 1:00 p.m.

he did have a renal biopsy in teh late 60s adn thus will look for results,
results02/20/2087 na 135, k 3.2 (l), cl 96 (l), co2 30.6, bun 1
jose church, m.d. /ray dd: 01/18/20 dt: 01/19/:0 dv: 01/18/20
placed 3/23 for bradycardia. p/g model # 5435, serial # 4712198,
consult notept: ulysses ogrady mc # 0937884date: 10/07/69
works in programming at audiovox. formerly at brightpoint.

he has remote travel hx to the rockefeller centre, more recent global

history of present illness: pt is a 59 yo khazakhstani male, with

who was admitted to san rafael mount hospital following a syncopal
nauseas and was brought to rafael mount ed. five weeks ago prior

anemia: on admission to rafael hospital, hb/hct: 11.6/35.5.
atch: 655-75-45 dear harry and yair: my thanks for your kind
patient lives in flint with his friend epstein. he has 3 children.

health care proxy-yes, son (west) allergies dutasteride - cough,

social history: married, glazier, 3 grown adult children
has vna. former civil engineer, supervisor, consultant.

he was formerly self-employed as a cpa and would often travel
communications senior manager, marketing, worked for brinker

and concrete finisher (25yrs). he is a veteran.

former tobacco user, works part time in securities.

dd:05/05/2095 dt:05/05/2095 wk:65255 :4653

no growth to date specimen: 38:z8912708g collected

2nd set biomarkers (wph): creatine kinase isoenzymes
hospitalized 2115 tch for romi 2120 tch new onset

lab tests amador: the lab results show good levels of

10mg po qd : 05/10/2066 - 04/15/2068 act : rosenberg

128 williams ct m oscar, johnny hyderabad, wi 62297
social history: he is retried motor vehicle body repairer.

table 4: examples of correctly detected phi instances (in bold) by the ann and crf models for the i2b2 dataset. the examples
in the ann column are only predicted by the ann model and not predicted by the crf model, and conversely. typographical
errors are from the original text.

tic meaning (e.g.,    san rafael mount hospital   ,
   rafael mount   ,    rafael hospital   ). furthermore,
the ann model is able to detect many phi instances
despite not having explicit gazetteers, as examples
in the location and profession categories il-
lustrate. we conjecture that the character-enhanced
token embeddings contain rich enough information
to effectively function as gazetteers, as tokens with
similar semantics are closely located in the vector
representation (mikolov et al., 2013b; collobert et
al., 2011; kim et al., 2015).

on the other hand, crf is good at rarely occur-
ring patterns that are written in highly specialized
regular expression patterns (e.g.,    38:z8912708g   ,
   53rhm   ) or
tokens that are included in the

gazetteers (e.g.,    christmas   ,    wph   ,    rosenberg   ,
   motor vehicle body repairer   ).
for example,
the phi token    christmas    only occurs in the test
set, and unless the context gives a strong indica-
tion, the ann model cannot detect it, whereas the
crf model could, as long as it is included in the
gazetteers.

3.6 effect of training set size
figure 3 shows the impact of the training set size
on the performance of the models on the mimic
dataset. when the training set size is very lim-
ited, the crf performs slightly better than the ann
model, since the crf model can leverage hand-
crafted features without much training data. as the

3.7 ablation analysis

in order to quantify the importance of various ele-
ments of the ann model, we tried 4 variations of the
model, eliminating different elements one at a time.
figure 4 presents the results of the ablation tests. re-
moving either the label sequence optimization layer,
pre-trained token embeddings, or token embeddings
slightly decreased the performance. surprisingly,
the ann performed pretty well with only charac-
ter embeddings and without the token embeddings,
and eliminating the character embeddings was more
detrimental than eliminating the token embeddings.
this suggests that the character-based token embed-
dings may be capturing not only the sub-token level
features, but also the semantics of the tokens them-
selves.

4 conclusions

we proposed the    rst system based on ann for
patient note de-identi   cation. it outperforms state-
of-the-art systems based on crf on two datasets,
while requiring no handcrafted features. utilizing
both the token and character embeddings, the sys-
tem can automatically learn effective features from
data by    ne-tuning the parameters. it jointly learns
the parameters for the embeddings, the bidirectional
lstms as well as the label sequence optimization,
and can make use of token embeddings pre-trained
on large unlabeled datasets. quantitative and qual-
itative analysis of the ann and crf models indi-
cates that the ann model better incorporates con-
text and is more    exible to variations inherent in hu-
man languages than the crf model.

from the viewpoint of deploying an off-the-
shelf de-identi   cation system, our results in table 3
demonstrate recall on the mimic discharge sum-
maries over 99%, which is quite encouraging. fig-
ure 2, however, shows that the f1-score on the
name category, probably the most sensitive phi
type, falls just below 98% for the ann model. we
anticipate that adding gazetteer features based on the
local institution   s patient and staff census should im-
prove this result, which will be explored in future
work.

figure 3: impact of the training set size on the binary hipaa
token-based f1-scores on the mimic dataset. 100% training
set size refers to using all of the dataset minus the test set.

figure 4: ablation test performance based on binary hipaa
token-based evaluation. ann is the model based on arti   cial
neural network. - seq opt is the ann model without the la-
bel sequence optimization layer. - pre-train is the ann model
where token embeddings are initialized with random values in-
stead of pre-trained embeddings. - token emb is the ann model
using only character-based token embeddings, without token
embeddings. - character emb is the ann model using only to-
ken embeddings, without character-based token embeddings.

training set size increases, the ann model starts to
signi   cantly outperform the crf model, since the
parameters including the embeddings are automat-
ically    ne-tuned with more data, and therefore the
features learned by the ann model become increas-
ingly more re   ned than the manually handcrafted
features. as a result, combining the outputs of the
crf and ann models increases the f1-score over
the ann model only for small training set size and
yields a less competitive f1-score than the ann
model for bigger training set size.

050100training set size (%)97.097.598.098.599.099.5100.0f1-score (%)crfanncrf+ann889092949698100f1-score (%)- char emb- token emb- pre-train- seq optannmodeli2b2mimicfunding

the project was supported by philips research. the
content is solely the responsibility of the authors and
does not necessarily represent the of   cial views of
philips research.

acknowledgments

we warmly thank michele filannino, alistair john-
son, and tom pollard for their helpful suggestions
and technical assistance.

references
[aberdeen et al.2010] john aberdeen, samuel bayer,
reyyan yeniterzi, ben wellner, cheryl clark, david
hanauer, bradley malin, and lynette hirschman.
2010. the mitre identi   cation scrubber toolkit:
design, training, and assessment. international jour-
nal of medical informatics, 79(12):849   859.

[bahdanau et al.2014] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio.
2014. neural machine
translation by jointly learning to align and translate.
arxiv preprint arxiv:1409.0473.

[beckwith et al.2006] bruce a beckwith, rajeshwarri
mahaadevan, ulysses j balis, and frank kuo. 2006.
development and evaluation of an open source
software tool for deidenti   cation of pathology re-
ports. bmc medical informatics and decision making,
6(1):1.

[berman2003] jules j berman. 2003. concept-match
medical data scrubbing: how pathology text can be
used in research. archives of pathology & laboratory
medicine, 127(6):680   686.

[blunsom et al.2014] phil blunsom, edward grefenstette,
nal kalchbrenner, et al. 2014. a convolutional neu-
ral network for modelling sentences. in proceedings
of the 52nd annual meeting of the association for
computational linguistics. proceedings of the 52nd
annual meeting of the association for computational
linguistics.

[chiticariu et al.2013] laura chiticariu, yunyao li, and
frederick r reiss. 2013. rule-based information ex-
traction is dead! long live rule-based information ex-
in emnlp, pages 827   832, octo-
traction systems!
ber.

[collobert et al.2011] ronan collobert, jason weston,
l  eon bottou, michael karlen, koray kavukcuoglu,
and pavel kuksa. 2011. natural language process-
ing (almost) from scratch. the journal of machine
learning research, 12:2493   2537.

[desroches et al.2013] catherine m desroches, chantal
worzala, and scott bates. 2013. some hospitals are
falling behind in meeting meaningful use criteria and
could be vulnerable to penalties in 2015. health af-
fairs, 32(8):1355   1360.

[douglas et al.2004] m douglas, gd clifford, a reisner,
gb moody, and rg mark. 2004. computer-assisted
de-identi   cation of free text in the mimic ii database.
in computers in cardiology, 2004, pages 341   344.
ieee.

[douglass et al.2005] mm douglass, gd cliffford,
a reisner, wj long, gb moody, and rg mark.
2005. de-identi   cation algorithm for free-text nurs-
in computers in cardiology, 2005, pages
ing notes.
331   334. ieee.

[douglass2005] margaret douglass. 2005. computer-
assisted de-identi   cation of free-text nursing notes.
master   s thesis, massachusetts institute of technol-
ogy.

[fielstein et al.2004] em fielstein, sh brown,

and
t speroff. 2004. algorithmic de-identi   cation of
va medical exam text for hipaa privacy compliance:
preliminary    ndings. medinfo, 1590.
jeff

and
clement j mcdonald. 2008. a software tool for
removing patient identifying information from clin-
journal of the american medical
ical documents.
informatics association, 15(5):601   610.

[friedlin and mcdonald2008] f

friedlin

[goldberger et al.2000] ary l goldberger, luis an
amaral, leon glass, jeffrey m hausdorff, plamen ch
ivanov, roger g mark, joseph e mietus, george b
moody, chung-kang peng, and h eugene stanley.
2000. physiobank, physiotoolkit, and physionet com-
ponents of a new research resource for complex phys-
iologic signals. circulation, 101(23):e215   e220.
[guo et al.2006] yikun guo, robert gaizauskas,

ian
roberts, george demetriou, and mark hepple. 2006.
identifying personal health information using support
vector machines. in i2b2 workshop on challenges in
natural language processing for clinical data, pages
10   11.

[gupta et al.2004] dilip gupta, melissa saul, and john
gilbertson. 2004. evaluation of a deidenti   cation
(de-id) software engine to share pathology reports and
clinical documents for research. american journal of
clinical pathology, 121(2):176   186.

[hara2006] kazuo hara. 2006. applying a id166 based
chunker and a text classi   er to the deid challenge. in
i2b2 workshop on challenges in natural language pro-
cessing for clinical data, pages 10   11. am med in-
form assoc.

[hochreiter and schmidhuber1997] sepp hochreiter and
j  urgen schmidhuber. 1997. long short-term memory.
neural computation, 9(8):1735   1780.

[johnson et al.2016] alistair e. w. johnson, tom j. pol-
lard, lu shen, li wei lehman, mengling feng, mo-
hammad ghassemi, benjamin moody, peter szolovits,
leo anthony celi, and roger g. mark.
2016.
mimic-iii, a freely accessible critical care database.
scienti   c data, (in press).

[kim et al.2015] yoon kim, yacine jernite, david son-
2015. character-
arxiv preprint

tag, and alexander m rush.
aware neural
arxiv:1508.06615.

language models.

[kim2014] yoon kim. 2014. convolutional neural net-
in proceedings of
works for sentence classi   cation.
the 2014 conference on empirical methods in natural
language processing, pages 1746   1751. association
for computational linguistics.

[labeau et al.2015] matthieu labeau, kevin l  oser, and
alexandre allauzen. 2015. non-lexical neural archi-
tecture for    ne-grained id52. in proceedings
of the 2015 conference on empirical methods in nat-
ural language processing, pages 232   237, lisbon,
portugal, september. association for computational
linguistics.

[lample et al.2016] guillaume lample, miguel balles-
teros, sandeep subramanian, kazuya kawakami, and
chris dyer. 2016. neural architectures for named en-
tity recognition. arxiv preprint arxiv:1603.01360.

[lee and dernoncourt2016] ji young lee and franck
dernoncourt. 2016. sequential short-text classi   ca-
tion with recurrent and convolutional neural networks.
in human language technologies 2016: the confer-
ence of the north american chapter of the association
for computational linguistics, naacl hlt 2016.

[liu et al.2015] zengjian liu, yangxin chen, buzhou
tang, xiaolong wang, qingcai chen, haodi li,
jingfeng wang, qiwen deng, and suisong zhu.
2015. automatic de-identi   cation of electronic medi-
cal records using token-level and character-level con-
ditional random    elds. journal of biomedical infor-
matics, 58:s47   s52.

[meystre et al.2010] stephane m meystre, f jeffrey
friedlin, brett r south, shuying shen, and matthew h
samore. 2010. automatic de-identi   cation of textual
documents in the electronic health record: a review of
recent research. bmc medical research methodology,
10(1):1.

[mikolov et al.2010] tomas mikolov, martin kara     at,
lukas burget, jan cernock`y, and sanjeev khudan-
pur. 2010. recurrent neural network based language
model. in interspeech, volume 2, page 3.

[mikolov et al.2013a] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013a. ef   cient estima-
tion of word representations in vector space. arxiv
preprint arxiv:1301.3781.

[mikolov et al.2013b] tomas mikolov,

ilya sutskever,
kai chen, greg s corrado, and jeff dean. 2013b.
distributed representations of words and phrases and
in advances in neural infor-
their compositionality.
mation processing systems, pages 3111   3119.

[mikolov et al.2013c] tomas mikolov, wen-tau yih, and
geoffrey zweig. 2013c. linguistic regularities in con-
tinuous space word representations. in hlt-naacl,
pages 746   751.

[morrison et al.2009] frances p morrison, li li, al-
bert m lai, and george hripcsak. 2009. repurposing
the clinical record: can an existing natural language
jour-
processing system de-identify clinical notes?
nal of the american medical informatics association,
16(1):37   39.

[neamatullah et al.2008] ishna neamatullah, margaret m
douglass, h lehman li-wei, andrew reisner, mauri-
cio villarroel, william j long, peter szolovits,
george b moody, roger g mark, and gari d clifford.
2008. automated de-identi   cation of free-text med-
ical records. bmc medical informatics and decision
making, 8(1):1.

[of   ce for civil rights2002] hhs

for
civil rights.
2002. standards for privacy of in-
dividually identi   able health information.    nal rule.
federal register, 67(157):53181.

of   ce

[parker et al.2011] robert parker, david graff, junbo
kong, ke chen, and kazuaki maeda. 2011. en-
glish gigaword    fth edition, linguistic data consor-
tium. technical report, technical report. linguistic
data consortium, philadelphia.

[pennington et al.2014] jeffrey

pennington,

richard
socher, and christopher d manning. 2014. glove:
global vectors for word representation. proceedings
of
the empiricial methods in natural language
processing (emnlp 2014), 12:1532   1543.

[ruch et al.2000] patrick ruch, robert h baud, anne-
marie rassinoux, pierrette bouillon, and gilbert
robert. 2000. medical document anonymization with
a semantic lexicon. in proceedings of the amia sym-
posium, page 729. american medical informatics as-
sociation.

[saeed et al.2011] mohammed saeed, mauricio villar-
roel, andrew t reisner, gari clifford, li-wei
lehman, george moody, thomas heldt, tin h kyaw,
benjamin moody, and roger g mark. 2011. mul-
tiparameter intelligent monitoring in intensive care
ii (mimic-ii): a public-access intensive care unit
database. critical care medicine, 39(5):952.

[socher et al.2013] richard socher, alex perelygin,
jean y wu, jason chuang, christopher d manning,
andrew y ng, and christopher potts. 2013. recur-
sive deep models for semantic compositionality over a
sentiment treebank. in proceedings of the conference

health information from clinic narratives. journal of
biomedical informatics, 58:s30   s38.

on empirical methods in natural language processing
(emnlp), volume 1631, page 1642. citeseer.

[stubbs et al.2015] amber stubbs, christopher kot   la,
and   ozlem uzuner. 2015. automated systems for
the de-identi   cation of longitudinal clinical narratives:
overview of 2014 i2b2/uthealth shared task track 1.
journal of biomedical informatics, 58:s11   s19.

[sundermeyer et al.2014] martin sundermeyer, tamer
alkhouli, joern wuebker, and hermann ney. 2014.
translation modeling with bidirectional recurrent neu-
ral networks. in emnlp, pages 14   25.

[sweeney1996] latanya sweeney.

1996. replacing
personally-identifying information in medical records,
in proceedings of the amia an-
the scrub system.
nual fall symposium, page 333. american medical in-
formatics association.

[szarvas et al.2006] gy  orgy szarvas, rich  ard farkas, and
andr  as kocsor. 2006. a multilingual named entity
recognition system using boosting and c4.5 decision
tree learning algorithms. in discovery science, pages
267   278. springer.

[tamura et al.2014] akihiro tamura, taro watanabe, and
eiichiro sumita. 2014. recurrent neural networks for
word alignment model. in acl (1), pages 1470   1480.
[thomas et al.2002] sean m thomas, burke mamlin,
gunther schadow, and clement mcdonald. 2002. a
successful technique for removing names in pathol-
ogy reports using an augmented search and replace
method. in proceedings of the amia symposium, page
777. american medical informatics association.

[uzuner et al.2008]

  ozlem uzuner, tawanda c sibanda,
yuan luo, and peter szolovits. 2008. a de-identi   er
for medical discharge summaries. arti   cial intelli-
gence in medicine, 42(1):13   35.

[wang and nyberg2015] di wang and eric nyberg.
2015. a long short-term memory model for answer
sentence selection in id53. in proceed-
ings of the 53rd annual meeting of the association
for computational linguistics and the 7th interna-
tional joint conference on natural language process-
ing (volume 2: short papers), pages 707   712, beijing,
china, july. association for computational linguis-
tics.

[weston et al.2015] jason weston, antoine bordes, sumit
chopra, and tomas mikolov. 2015. towards ai-
complete id53: a set of prerequisite toy
tasks. arxiv preprint arxiv:1502.05698.

[wright et al.2013] adam wright, stanislav henkin,
joshua feblowitz, allison b mccoy, david w bates,
and dean f sittig. 2013. early results of the mean-
ingful use program for electronic health records. new
england journal of medicine, 368(8):779   780.

[yang and garibaldi2015] hui yang and jonathan m
garibaldi. 2015. automatic detection of protected

