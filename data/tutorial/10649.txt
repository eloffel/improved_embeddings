6
1
0
2
 
c
e
d
6
1

 

 
 
]
i

a
.
s
c
[
 
 

2
v
7
6
4
2
0

.

7
0
6
1
:
v
i
x
r
a

log-linear id56s : towards recurrent neural

networks with flexible prior knowledge

marc dymetman

chunyang xiao

xerox research centre europe, grenoble, france
{marc.dymetman,chunyang.xiao}@xrce.xerox.com

november 2016

abstract

we introduce ll-id56s (log-linear id56s), an extension of re-
current neural networks that replaces the softmax output layer by a
log-linear output layer, of which the softmax is a special case. this
conceptually simple move has two main advantages. first, it allows
the learner to combat training data sparsity by allowing it to model
words (or more generally, output symbols) as complex combinations
of attributes without requiring that each combination is directly ob-
served in the training data (as the softmax does). second, it permits
the inclusion of    exible prior knowledge in the form of a priori speci-
   ed modular features, where the neural network component learns to
dynamically control the weights of a log-linear distribution exploiting
these features.

we conduct experiments in the domain of language modelling of
french, that exploit morphological prior knowledge and show an im-
portant decrease in perplexity relative to a baseline id56.

we provide other motivating iillustrations, and    nally argue that
the log-linear and the neural-network components contribute comple-
mentary strengths to the ll-id56: the ll aspect allows the model to
incorporate rich prior knowledge, while the nn aspect, according to
the    representation learning    paradigm, allows the model to discover
novel combination of characteristics.

this is an updated version of the e-print arxiv:1607.02467, in partic-
ular now including experiments.

1

1

introduction

recurrent neural networks (goodfellow et al., 2016, chapter 10) have re-
cently shown remarkable success in sequential data prediction and have been
applied to such nlp tasks as language modelling (mikolov et al., 2010),
machine translation (sutskever et al., 2014; bahdanau et al., 2015), parsing
(vinyals et al., 2014), id86 (wen et al., 2015) and
dialogue (vinyals and le, 2015), to name only a few. specially popular
id56 architectures in these applications have been models able to exploit
long-distance correlations, such as lstms (hochreiter and schmidhuber,
1997; gers et al., 2000) and grus (cho et al., 2014), which have led to
groundbreaking performances.

id56s (or more generally, neural networks), at the core, are machines
that take as input a real vector and output a real vector, through a combi-
nation of linear and non-linear operations.

when working with symbolic data, some conversion from these real vec-
tors from and to discrete values, for instance words in a certain vocabulary,
becomes necessary. however most id56s have taken an oversimpli   ed view
of this mapping.
in particular, for converting output vectors into distri-
butions over symbolic values, the mapping has mostly been done through
a softmax operation, which assumes that the id56 is able to compute a
real value for each individual member of the vocabulary, and then converts
this value into a id203 through a direct exponentiation followed by a
id172.

this rather crude    softmax approach   , which implies that the output
vector has the same dimensionality as the vocabulary, has had some serious
consequences.

to focus on only one symptomatic defect of this approach, consider the
following. when using words as symbols, even large vocabularies cannot
account for all the actual words found either in training or in test, and the
models need to resort to a catch-all    unknown    symbol unk, which provides
a poor support for prediction and requires to be supplemented by diverse
pre- and post-processing steps (luong et al., 2014; jean et al., 2015). even
for words inside the vocabulary, unless they have been witnessed many times
in the training data, prediction tends to be poor because each word is an    is-
land   , completely distinct from and without relation to other words, which
needs to be predicted individually.

one partial solution to the above problem consists in changing the gran-
ularity by moving from word to character symbols (sutskever et al., 2011;
ling et al., 2015). this has the bene   t that the vocabulary becomes much

2

figure 1: a generic id56.

smaller, and that all the characters can be observed many times in the train-
ing data. while character-based id56s have thus some advantages over
word-based ones, they also tend to produce non-words and to necessitate
longer prediction chains than words, so the jury is still out, with emerging
hybrid architectures that attempt to capitalize on both levels (luong and
manning, 2016).

here, we propose a di   erent approach, which removes the constraint that
the dimensionality of the id56 output vector has to be equal to the size of
the vocabulary and allows generalization across related words. however, its
crucial bene   t is that it introduces a principled and powerful way of
incorporating prior knowledge inside the models.

the approach involves a very direct and natural extension of the softmax,
by considering it as a special case of an conditional exponential family, a
class of models better known as id148 and widely used in    pre-
nn    nlp. we argue that this simple extension of the softmax allows the
resulting    log-linear id56    to compound the aptitude of id148
for exploiting prior knowledge and prede   ned features with the aptitude of
id56s for discovering complex new combinations of predictive traits.

3

                    1         +1           1                +1                                                               ,       1         ,             ,    +1         +1                  +2                     ,       1         ,             ,    +1                softmax softmax softmax 2 log-linear id56s

2.1 generic id56s

let us    rst recap brie   y the generic notion of id56, abstracting away from
di   erent styles of implementation (lstm (hochreiter and schmidhuber,
1997; graves, 2012), gru (cho et al., 2014), id12 (bahdanau
et al., 2015), di   erent number of layers, etc.).

an id56 is a generative process for predicting a sequence of symbols
x1, x2, . . . , xt, . . ., where the symbols are taken in some vocabulary v , and
where the prediction can be conditioned by a certain observed context c.
this generative process can be written as:

p  (xt+1|c, x1, x2, . . . , xt),

where    is a real-valued parameter vector.1 generically, this conditional
id203 is computed according to:

ht = f  (c; xt, ht   1),
a  ,t = g  (ht),
p  ,t = softmax(a  ,t),
xt+1     p  ,t(  ) .

(1)

(2)

(3)

(4)
here ht   1 is the hidden state at the previous step t     1, xt is the output
symbol produced at that step and f   is a neural-network based function
(e.g. a lstm network) that computes the next hidden state ht based on
c, xt, and ht   1. the function g  ,2 is then typically computed through an
mlp, which returns a real-valued vector a  ,t of dimension |v |. this vector is
then normalized into a id203 distribution over v through the softmax
transformation:

softmax(a  ,t)(x) = 1/z exp(a  ,t(x)),

with the id172 factor:

z =

(cid:88)

x(cid:48)   v

exp(a  ,t(x(cid:48))),

1we will sometimes write this as p  (xt+1|c; x1, x2, . . . , xt) to stress the di   erence be-
tween the    context    c and the pre   x x1, x2, . . . , xt. note that some id56s are    non-
conditional   , i.e. do not exploit a context c.

2we do not distinguish between the parameters for f and for g, and write    for both.

4

and    nally the next symbol xt+1 is sampled from this distribution. see
figure 1.

training of such a model is typically done through back-propagation of

the cross-id178 loss:

    log p  (  xt+1|x1, x2, . . . , xt; c),

where   xt+1 is the actual symbol observed in the training set.

2.2 id148

de   nition

id148 play a considerable role in statistics and machine learning;
special classes are often known through di   erent names depending on the ap-
plication domains and on various details: exponential families (typically for
unconditional versions of the models) (nielsen and garcia, 2009) maximum
id178 models (berger et al., 1996; jaynes, 1957), conditional random    elds
(la   erty et al., 2001), binomial and multinomial id28 (hastie
et al., 2001, chapter 4). these models have been especially popular in nlp,
for example in language modelling (rosenfeld, 1996), in sequence labelling
(la   erty et al., 2001), in machine translation (berger et al., 1996; och and
ney, 2002), to name only a few.

here we follow the exposition (jebara, 2013), which is useful for its broad
applicability, and which de   nes a conditional log-linear model     which we
could also call a conditional exponential family     as a model of the form
(in our own notation):

(cid:16)

(cid:17)

p(x| k, a) =

1

z(k, a)

b(k, x) exp

a(cid:62)  (k, x)

.

(5)

let us describe the notation:

    x is a variable in a set v , which we will take here to be discrete (i.e.
countable), and sometimes    nite.3 we will use the terms domain or
vocabulary for this set.

    k is the conditioning variable (also called condition).

3the model is applicable over continuous (measurable) spaces, but to simplify the
exposition we will concentrate on the discrete case, which permits to use sums instead of
integrals.

5

    a is a parameter vector in rd, which (for reasons that will appear later)

we will call the adaptor vector.4

       is a feature function (k, x)     rd; note that we sometimes write
(x; k) or (k; x) instead of (k, x) to stress the fact that k is a condi-
tion.

    b is a nonnegative function (k, x)     r+; we will call it the background

function of the model.5

    z(k, a), called the partition function, is a id172 factor:

z(k, a) =

b(k, x) exp

a(cid:62)  (k, x)

.

(cid:16)

(cid:17)

(cid:88)

x

(cid:16)

(cid:16)

(cid:16)

(cid:17)

(cid:17)

when the context is unambiguous, we will sometimes leave the condition k
as well as the parameter vector a implicit, and also simply write z instead
of z(k, a); thus we will write:

p(x) =

1
z

b(x) exp

a(cid:62)  (x)

or more compactly:

p(x)     b(x) exp

a(cid:62)  (x)

.

(cid:17)

,

(6)

(7)

the background as a    prior   

bility distribution over v (that is,(cid:80)

if in equation (7) the background function is actually a normalized proba-
x b(x) = 1) and if the parameter vector

a is null, then the distribution p is identical to b.

suppose that we have an initial belief that the parameter vector a should

be close to a0, then by reparametrizing equation (7) in the form:

a(cid:48)(cid:62)  (x)

p(x)     b(cid:48)(x) exp
(8)
0   (x)) and a(cid:48) = a     a0, then our initial belief is
with b(cid:48)(x) = b(x) exp(a(cid:62)
represented by taking a(cid:48) = 0. in other words, we can always assume that
our initial belief is represented by the background id203 b(cid:48) along with
a null parameter vector a(cid:48) = 0. deviations from this initial belief are then
representation by variations of the parameter vector away from 0 and a

,

4in the nlp literature, this parameter vector is often denoted by   .
5jebara (2013) calls it the prior of the family.

6

simple form of id173 can be obtained by penalizing some p-norm
||a(cid:48)||p of this parameter vector.6

gradient of cross-id178 loss

an important property of id148 is that they enjoy an extremely
intuitive form for the gradient of their log-likelihood (aka cross-id178 loss).
if   x is a training instance observed under condition k, and if the current
model is p(x|a, k) according to equation (5), its likelihood loss at   x is de   ned
as:     log l =     log p(  x|a, k). then a simple calculation shows that the
gradient     log l

(also called the    fisher score    at   x) is given by:

   a

=   (  x; k)    (cid:88)

x   v

    log l

   a

p(x|a, k)   (x; k).

(9)

in other words, the gradient is minus the di   erence between the model
expectation of the feature vector and its actual value at   x.7

2.3 log-linear id56s

we can now de   ne what we mean by a log-linear id56. the model, illus-
trated in figure 2, is similar to a standard id56 up to two di   erences:

the    rst di   erence is that we allow a more general form of input to
the network at each time step; namely, instead of allowing only the latest
symbol xt to be used as input, along with the condition c, we now allow
an arbitrary feature vector   (c, x1, . . . , xt) to be used as input; this feature
vector is of    xed dimensionality |  |, and we allow it to be computed in

6contrarily to the generality of the presentation by jebara (2013), many presentations
of id148 in the nlp context do not make an explicit reference to b, which is
then implicitely taken to be uniform. however, the more statistically oriented presenta-
tions (jordan, 20xx; nielsen and garcia, 2009) of the strongly related (unconditional)
exponential family models do, which makes the mathematics neater and is necessary in
presence of non-   nite or continuous spaces. one advantage of the explicit introduction of
b, even for    nite spaces, is that it makes it easier to speak about the prior knowledge we
have about the overall process.

7more generally, if we have a training set consisting of n pairs of the form (  xn; kn),

then the gradient of the log-likelihood for this training set is given by:

(cid:32)

n(cid:88)

n=1

  (  xn; kn)    (cid:88)

x   v

    log l

   a

=

(cid:33)

p(x|a, kn)   (x; kn)

.

in other words, this gradient is the di   erence between the feature vectors at the true labels
minus the expected feature vectors under the current distribution (jebara, 2013).

7

figure 2: a log-linear id56.

an arbitrary (but deterministic) way from the combination of the currently
known pre   x x1, . . . , xt   1, xt and the context c. this is a relatively minor
change, but one that usefully expands the expressive power of the network.
we will sometimes call the    features the input features.

the second, major, di   erence is the following. we do compute a  ,t
in the same way as previously from ht, however, after this point, rather
than applying a softmax to obtain a distribution over v , we now apply a
log-linear model. while for the standard id56 we had:

p  ,t(xt+1) = softmax(a  ,t)(xt+1),

in the ll-id56, we de   ne:

p  ,t(xt+1)     b(c, x1, . . . , xt, xt+1) exp

(cid:16)

a  ,t

(cid:17)

(cid:62)   (c, x1, . . . , xt, xt+1)

.

(10)

in other words, we assume that we have a priori    xed a certain background
function b(k, x), where the condition k is given by k = (c, x1, . . . , xt),
and also de   ned m features de   ning a feature vector   (k, xt+1), of    xed
dimensionality |  | = m . we will sometimes call these features the output
features. note that both the background and the features have access to the
context k = (c, x1, . . . , xt).

8

       +1                           ,    +1         +2             ,    +1     (   ,        +1)           1                           ,       1                      ,       1     (   ,           1) ll     (   ,           1,   )     (   ,           1,   )                                   ,             +1             ,         (   ,        ) ll     (   ,        ,   )     (   ,        ,   ) ll     (   ,        +1,   )     (   ,        +1,   ) in figure 2, we have indicated with ll (loglinear) the operation (10)
that combines a  ,t with the feature vector   (c, x1, . . . , xt, xt+1) and the back-
ground b(c, x1, . . . , xt, xt+1) to produce the id203 distribution p  ,t(xt+1)
over v . we note that, here, a  ,t is a vector of size |  |, which may or may
not be equal to the size |v | of the vocabulary, by contrast to the case of the
softmax of figure 1.

overall, the ll-id56 is then computed through the following equations:

ht = f  (  (c, x1, . . . , xt), ht   1),
a  ,t = g  (ht),

p  ,t(x)     b(c, x1, . . . , xt, x)    exp
xt+1     p  ,t(  ) .

(cid:16)

(cid:62)   (c, x1, . . . , xt, x)

a  ,t

(cid:17)

,

(11)

(12)

(13)

(14)

for prediction, we now use the combined process p  , and we train this pro-
cess, similarly to the id56 case, according to its cross-id178 loss relative
to the actually observed symbol   x:

    log p  (  xt+1|c, x1, x2, . . . , xt).

(15)

at training time, in order to use this loss for id26 in the id56,
we have to be able to compute its gradient relative to the previous layer,
namely a  ,t. from equation (9), we see that this gradient is given by:

(cid:33)

p(x|a  ,t, k)   (k; x)

      (k;   xt+1),

(16)

(cid:32)(cid:88)

x   v

with k = c, x1, x2, . . . , xt.

this equation provides a particularly intuitive formula for the gradient,
namely, as the di   erence between the expectation of   (k; x) according to
the log-linear model with parameters a  ,t and the observed value   (k;   xt+1).
however, this expectation can be di   cult to compute. for a    nite (and not
too large) vocabulary v , the simplest approach is to simply evaluate the
right-hand side of equation (13) for each x     v , to normalize by the sum to
obtain p  ,t(x), and to weight each   (k; x) accordingly. for standard id56s
(which are special cases of ll-id56s, see below), this is actually what the
simpler approaches to computing the softmax gradient do, but more sophis-
ticated approaches have been proposed, such as employing a    hierarchical
softmax    (morin and bengio, 2005). in the general case (large or in   nite v ),

9

the expectation term in (19) needs to be approximated, and di   erent tech-
niques may be employed, some speci   c to id148 (elkan, 2008;
jebara, 2013), some more generic, such as contrastive divergence (hinton,
2002) or importance sampling; a recent introduction to these generic meth-
ods is provided in (goodfellow et al., 2016, chapter 18), but, despite its
practical importance, we will not pursue this topic further here.

2.4 ll-id56s generalize id56s

it is easy to see that ll-id56s generalize id56s. consider a    nite vocab-
ulary v , and the |v |-dim    one-hot    representation of x     v , relative to a
certain    xed ordering of the elements of v :

onehot(x) = [0, 0,

. . .

. . . 0].

1,
   
x

we assume (as we implicitly did in the discussion of standard id56s) that
c is coded through some    xed-vector and we then de   ne:

  (c, x1, . . . , xt) = c     onehot(xt) ,

(17)
where     denotes vector concatenation; thus we    forget    about the initial
portion x1, . . . , xt   1 of the pre   x, and only take into account c and xt,
encoded in a similar way as in the case of id56s.
we then de   ne b(x) to be uniformly 1 for all x     v (   uniform back-

ground   ), and    to be:

  (c, x1, . . . , xt, xt+1) = onehot(xt+1).

neither b nor    depend on c, x1, . . . , xt, and we have:

p  ,t(xt+1)     b(xt+1) exp

(cid:62)   (xt+1)

a  ,t

= exp a  ,t(xt+1),

(cid:16)

(cid:17)

in other words:

p  ,t = softmax(a  ,t).

thus, we are back to the de   nition of id56s in equations (1-4). as for the
gradient computation of equation (19):

p(x|a  ,t, k)   (k; x)

      (k;   xt+1),

(18)

(cid:32)(cid:88)

x   v

(cid:33)

10

it takes the simple form:

(cid:33)

    onehot(  xt+1),

(19)

p  ,t(x) onehot(x)

(cid:32)(cid:88)

x   v

in other words this gradient is the vector     of dimension |v |, with coordi-
nates i     1, . . . ,|v | corresponding to the di   erent elements x(i) of v , where:

(cid:26) p  ,t(x(i))     1

p  ,t(x(i))

   i =

if x(i) =   xt+1,
for the other x(i)   s.

(20a)

(20b)

this corresponds to the computation in the usual softmax case.

3 a motivating illustration: rare words

we now come back to the our starting point in the introduction: the problem
of unknown or rare words, and indicate a way to handle this problem with
ll-id56s, which may also help building intuition about these models.

let us consider some moderately-sized corpus of english sentences, tok-
enized at the word level, and then consider the vocabulary v1, of size 10k,
consisting of the 9999 most frequent words to occur in this corpus plus one
special symbol unk used for tokens not among those words (   unknown
words   ).

after replacing the unknown words in the corpus by unk, we can train
a language model for the corpus by training a standard id56, say of the
lstm type. note that if translated into a ll-id56 according to section
2.4, this model has 10k features (9999 features for identity with a speci   c
frequent word, the last one for identity with the symbol unk), along with
a uniform background b.

this model however has some serious shortcomings, in particular:
    suppose that none of the two tokens grenoble and 37 belong to v1
(i.e. to the 9999 most frequent words of the corpus), then the learnt
model cannot distinguish the id203 of the two test sentences: the
cost was 37 euros / the cost was grenoble euros.

    suppose that several sentences of the form the cost was nn euros ap-
pear in the corpus, with nn taking (say) values 9, 13, 21, all belonging
to v1, and that on the other hand 15 also belongs to v1, but appears
in non-cost contexts; then the learnt model cannot give a reasonable

11

id203 to the cost was 15 euros, because it is unable to notice the
similarity between 15 and the tokens 9, 13, 21.

let   s see how we can improve the situation by moving to a ll-id56.
we start by extending v1 to a much larger    nite set of words v2, in
particular one that includes all the words in the union of the training and
test corpora,8 and we keep b uniform over v2. concerning the    (input)
features, for now we keep them at their standard id56 values (namely as in
(17)). concerning the    features, we keep the 9999 word-identity features
that we had, but not the unk-identity one; however, we do add some new
features (say   10000       10020):

    a binary feature   10000(x) =   number(x) that tells us whether the token

x can be a number;

    a binary feature   10001(x) =   location(x) that tells us whether the

token x can be a location, such as a city or a country;

    a few binary features   noun(x),   adj(x), ..., covering the main pos   s
for english tokens. note that a single word may have simultaneously
several such features    ring, for instance    ies is both a noun and a
verb.9

    some other features, covering other important classes of words.

each of the   1, ...,   10020 features has a corresponding weight that we

index in a similar way a1, ..., a10020.

note again that we do allow the features to overlap freely, nothing pre-
venting a word to be both a location and an adjective, for example (e.g.
nice in we visited nice / nice    owers were seen everywhere), and to also
appear in the 9999 most frequent words. for exposition reasons (ie in order
to simplify the explanations below) we will suppose that a number n will
always    re the feature   number, but no other feature, apart from the case
where it also belongs to v1, in which case it will also    re the word-identity
feature that corresponds to it, which we will denote by      n , with   n     9999.

why is this model superior to the standard id56 one?
to answer this question, let   s consider the encoding of n in    feature
space, when n is a number. there are two slightly di   erent cases to look
at:

8we will see later that the restriction that v is    nite can be lifted.
9rather than using the notation   10000, ..., we sometimes use the notation   number, ...,

for obvious reasons of clarity.

12

1. n does not belong to v1. then we have   10000 =   number = 1, and

  i = 0 for other i   s.

2. n belongs to v1. then we have   10000 =   number = 1,      n = 1 and

  i = 0 for other i   s.

let us now consider the behavior of the ll-id56 during training, when
at a certain point, let   s say after having observed the pre   x the cost was, it
is now coming to the prediction of the next item xt+1 = x, which we assume
is actually a number   x = n in the training sample.

we start by assuming that n does not belong to v1.
let us consider the current value a = a  ,t of the weight vector calculated

by the network at this point. according to equation (9), the gradient is:

=   (n )    (cid:88)

x

    log l

   a

p(x|a)   (x),

where l is the cross-id178 loss and p is the id203 distribution asso-
ciated with the log-linear weights a.

in our case the    rst term is a vector that is null everywhere but on
coordinate   number, on which it is equal to 1. as for the second term, it can
be seen as the model average of the feature vector   (x) when x is sampled
according to p(x|a). one can see that this vector has all its coordinates in
the interval [0, 1], and in fact strictly between 0 and 1.10 as a consequence,
the gradient     log l
is strictly positive on the coordinate   number and strictly
negative on all the other coordinates. in other words, the id26
signal sent to the neural network at this point is that it should modify its
parameters    in such a way as to increase the anumber weight, and decrease
all the other weights in a.

   a

   a

a slightly di   erent situation occurs if we assume now that n belongs to
v1. in that case   (n ) is null everywhere but on its two coordinates   number
and      n , on which it is equal to 1. by the same reasoning as before we see
that the gradient     log l
is then strictly positive on the two corresponding
coordinates, and strictly negative everywhere else. thus, the signal sent to
the network is to modify its parameter towards increasing the anumber and
a   n weights, and decrease them everywhere else.

overall, on each occurrence of a number in the training set, the network
is then learning to increase the weights corresponding to the features (either
10this last fact is because, for a vector a with    nite coordinates, p(x|a) can never be 0,
and also because we are making the mild assumption that for any feature   i, there exist
x and x(cid:48) such that   i(x) = 0,   i(x(cid:48)) = 1; the strict inequalities follow immediately.

13

both anumber and a   n or only anumber, depending on whether n is in v1 or not)
   ring on this number, and to decrease the weights for all the other features.
this contrasts with the behavior of the previous id56 model where only in

the case of n     v1 did the weight a   n change. this means that at the end
of training, when predicting the word xt+1 that follows the pre   x the cost
was, the ll-id56 network will have a tendency to produce a weight vector
a  ,t with especially high weight on anumber, some positive weights on those
a   n for which n has appeared in similar contexts, and negative weights on
features not    ring in similar contexts.11

now, to come back to our initial example, let us compare the situa-
tion with the two next-word predictions the cost was 37 and the cost was
grenoble. the ll-id56 model predicts the next word xt+1 with id203:

(cid:16)

(cid:17)

p  ,t(xt+1)     exp

(cid:62)   (xt+1)

a  ,t

.

while the prediction xt+1 = 37    res the feature   number, the prediction
xt+1 = grenoble does not    re any of the features that tend to be active in the
context of the pre   x the cost was, and therefore p  ,t(37) (cid:29) p  ,t(grenoble).
this is in stark contrast to the behavior of the original id56, for which both
37 and grenoble were undistinguishable unknown words.

we note that, while the model is able to capitalize on the generic notion
of number through its feature   number, it is also able to learn to privilege
certain speci   c numbers belonging to v1 if they tend to appear more fre-
quently in certain contexts. a log-linear model has the important advantage
of being able to handle redundant features12 such as   number and     3 which
both    re on 3. depending on prior expectations about typical texts in the
domain being handled, it may then be useful to introduce features for distin-
guishing between di   erent classes of numbers, for instance    small numbers   
or    year-like numbers   , allowing the ll-id56 to make useful generalizations
based on these features. such features need not be binary, for example a
small-number feature could take values decreasing from 1 to 0, with the
higher values reserved for the smaller numbers.

while our example focussed on the case of numbers, it is clear that our
observations equally apply to other features that we mentioned, such as

11if only numbers appeared in the context the cost was, then this would mean all    non-
numeric    features, but such words as high, expensive, etc. may of course also appear, and
their associated features would also receive positive increments.

12this property of id148 was what permitted a fundamental advance in sta-
tistical machine translation beyond the initial limited noisy-channel models, by allowing
a freer combination of di   erent assesments of translation quality, without having to bother
about overlapping assesments (berger et al., 1996; och and ney, 2002).

14

  location(x), which can serve to generalize predictions in such contexts as
we are travelling to.

in principle, generally speaking, any features that can support gener-
alization, such as features representing semantic classes (e.g. nodes in the
id138 hierarchy), morphosyntactic classes (lemma, gender, number, etc.)
or the like, can be useful.

4 some potential applications

the extension from softmax to log-linear outputs, while formally simple,
opens a signi   cant range of potential applications other than the handling
of rare words. we now brie   y sketch a few directions.

a priori constrained sequences for some applications, sequences to be
generated may have to respect certain a priori constraints. one such case
is the approach to id29 of (xiao et al., 2016), where starting
from a natural language question an id56 decoder produces a sequential
encoding of a logical form, which has to conform to a certain grammar. the
model used is implicitely a simple case of ll-id56, where (in our present
terminology) the output feature vector    remains the usual onehot, but the
background b is not uniform anymore, but constrains the generated sequence
to conform to the grammar.

language model adaptation we saw earlier that by taking b to be uni-
form and    to be a onehot, an ll-id56 is just a standard id56. the opposite
extreme case is obtained by supposing that we already know the exact gen-
erative process for producing xt+1 from the context k = c, x1, x2, . . . , xt. if
we de   ne b(k;  ) = b(k; x) to be identical to this true underlying process,
then in order to have the best performance in test, it is su   cient for the
adaptor vector a  ,t to be equal to the null vector, because then, according
to (13), p  ,t(x)     b(k; x) is equal to the underlying process. the task for
the id56 to learn a    such that a  ,t is null or close to null is an easy one
(just take the higher level parameter matrices to be null or close to null),
and in this case the adaptor has actually nothing to adapt to.

a more interesting, intermediary, case is when b(k; x) is not too far from
the true process. for example, b could be a word-based language model
(id165 type, lstm type, etc.) trained on some large monolingual corpus,
while the current focus is on modeling a speci   c domain for which much less
data is available. then training the id56-based adaptor a   on the speci   c
domain data would still be able to rely on b for test words not seen in the

15

speci   c data, but learn to upweight the prediction of words often seen in
these speci   c data.13

input features in a standard id56, a word xt is vector-encoded through a
one-hot representation both when it is produced as the current output of the
network but also when it is used as the next input to the network. in section
3, we saw the interest of de   ning the    output    features    to go beyond
word-identity features     i.e. beyond the identi   cation   (x) = onehot(x)
   , but we kept the    input    features as in standard id56s, namely we kept
  (x) = onehot(x) . however, let us note an issue there. this usual encoding
of the input x means that if x = 37 has rarely (or not at all) been seen in the
training data, then the network will have few clues to distinguish this word
from another rarely observed word (for example the adjective preposterous)
when computing f   in equation 11. the network, in the context of the
pre   x the cost was, is able to give a reasonable id203 to 37 thanks
to   . however, when assessing the id203 of euros in the context of
the pre   x the cost was 37, this is not distinguished by the network from
the pre   x the cost was preposterous, which would not allow euros as the
next word. a promising way to solve this problem here is to take    =   ,
namely to encode the input x using the same features as the output x. this
allows the network to    see    that 37 is a number and that preposterous is an
adjective, and to compute its hidden state based on this information. we
should note, however, that there is no requirement that    be equal to    in
general; the point is that we can include in    features which can help the
network predict the next word.

z(a) =(cid:80)

in   nite domains in the example of section 3, the vocabulary v2 was large,
but    nite. this is quite arti   cial, especially if we want to account for words
representing numbers, or words taken in some open-ended set, such as entity
names. let us go back to the equation (5) de   ning id148, and let
us ignore the context k for simplicity: p(x|a) = 1

z(a) b(x) exp(cid:0)a(cid:62)  (x)(cid:1) , with
x   v b(x) exp(cid:0)a(cid:62)  (x)(cid:1). when v is    nite, then the id172

factor z(a) is also    nite, and therefore the id203 p(x|a) is well de   ned;
in particular, it is well-de   ned when b(x) = 1 uniformly. however, when
v is (countably) in   nite, then this is unfortunately not true anymore. for
instance, with b(x) = 1 uniformly, and with a = 0, then z(a) is in   nite and
the id203 is unde   ned. by contrast, let   s assume that the background

13for instance, focussing on the simple case of an adaptor over a onehot   , as soon as
a  ,t(k; x) is positive on a certain word x, then the id203 of this word is increased
relative to what the background indicates.

16

function b is in l1(v ), i.e. (cid:80)

x   v b(x) <    . let   s also suppose that the
feature vector    is uniformly bounded (that is, all its coordinates   i are such
that    x     v,   i(x)     [  i,   i], for   i,   i     r). then, for any a, z(a) is    nite,
and therefore p(x|a) is well-de   ned.
thus, the standard id56s, which have (implicitely) a uniform background
b, have no way to handle in   nite vocabularies, while ll-id56s, by using
a    nite-mass b, can. one simple way to ensure that property on tokens
representing numbers, for example, is to associate them with a geometric
background distribution, decaying fast with their length, and a similar treat-
ment can be done for named entities.

condition-based priming many applications of id56s, such as machine
translation (sutskever et al., 2014) or id86 (wen
et al., 2015), etc., depend on a condition c (source sentence, semantic repre-
sentation, etc.). when translated into ll-id56s, this condition is taken into
account through the input feature vector   (c, x1, . . . , xt) = c     onehot(xt),
see (17), but does not appear in b(c, x1, . . . , , xt; xt+1) = b(xt+1) = 1 or
  (c, x1, . . . , xt; xt+1) = onehot(xt+1).

however, there is opportunity for exploiting the condition inside b or   . to
sketch a simple example, in id86, one may be able to prede   ne some weak
unigram language model for the realization that depends on the semantic
input c, for example by constraining named entities that appear in the
realization to have some evidence in the input. such a language model can be
usefully represented through the background process b(c, x1, . . . , xt; xt+1) =
b(c; xt+1), providing a form of    priming    for the combined ll-id56, helping
it to avoid irrelevant tokens.

a similar approach was recently exploited in goyal et al. (2016), in the
context of a character-based id195 lstm for generating utterances from
input    dialog acts   . in this approach, the background b, formulated as a
weighted    nite-state automaton over characters, is used both for encour-
aging the system to generate character strings that correspond to possible
dictionary words, as well as to allow it to generate strings corresponding to
such non-dictionary tokens as named-entities, numbers, addresses, and the
like, but only when such strings have evidence in the input dialog act.

17

5 experiments: french language model using mor-

phological features

5.1 datasets

our datasets are based on the annotated french corpora14 provided by the
universal dependencies initiative15. these corpora are tagged at the pos
level as well as at the dependency level. in our experiments, we only exploit
the pos annotations, and we use lowercased versions of the corpora.

table 2 shows the sentence sizes of our di   erent datasets, and table 2

overall statistics in terms of word-tokens and word-types.

training
13,826

validation
728

test1
298

test2
1596

table 1: number of sentences in the di   erent datasets. the ud (universal
dependency) training set (14554 sents.) is the union of our training and
validation sets. the ud validation set is our test2 set, while the ud test
set is our test1 set.

total
tences
16448

sen-

tokens

463069

avg.
length
28.15

sent.

types

42894

token-type
ratio
10.80

table 2: data statistics.

5.2 features

the corpora provide pos and morphological tags for each word token in
the context of the sentence in which it appears. table 3 shows the 52 tags
that we use, which we treat as binary features. in addition, we select the m
most frequent word types appearing in the entire corpus, and we use m + 1
additional binary features which identify whether a given word is identical
to one of the m most frequent words, or whether it is outside this set. in
total, we then use m + 53 binary features.

we collect all the word types appearing in the entire corpus and we
associate with each a binary vector of size m + 53 which is the boolean
union of the binary vectors associated with all the tokens for that type.

14http://universaldependencies.org/#fr.
15http://universaldependencies.org (version 1).

18

prontype:dem
prontype:in
prontype:int
prontype:neg
prontype:prs
prontype:rel
re   ex:yes
tense:fut
tense:imp
tense:past
tense:pres
verbform:fin
verbform:inf
verbform:part

pos:adj
pos:adp
pos:adv
pos:aux
pos:conj
pos:det
pos:intj
pos:nil
pos:noun
pos:num
pos:part
pos:pron
pos:propn
pos:punc
pos:scon
pos:sym
pos:verb
pos:x
pos:

case:abl
case:acc
case:nom
case:voc
de   nite:def
de   nite:ind
degree:cmp
gender:fem
gender:masc
gender:neut
mood:cnd
mood:imp
mood:indn
mood:subt
number:plurj
number:sing
person:1
person:2
person:3

table 3: pos and morphological features.

in case of an ambiguous word, the binary vector may have ones on several
pos simultaneously.16 thus, here, we basically use the corpus as a proxy
for a morphological analyser of french, and we do not use the contextual
information provided by the token-level tags.

5.3 models

in these experiments, we use a    nite word vocabulary v consisting of the
42894 types found in the entire corpus (including the validation and test
sets). we then compare our ll-id56 with a vanilla id56, both over this
vocabulary v . thus none of the models has unknown words. both models
are implemented in keras (chollet, 2015) over a theano (theano develop-
ment team, 2016) backend.

the baseline id56 is using one-hot encodings for the words in v , and
consists of an embedding layer of dimension 256 followed by two lstm

16thus for instance, currently, the vector associated with the word le has ones not only
on the det (determiner) and the pron (pronoun) features, but also on the propn
(proper noun) feature, due to the appearance of the name le in the corpus...

19

(hochreiter and schmidhuber, 1997) layers of dimension 256, followed by a
dense layer and    nally a softmax layer both of dimension |v |. the lstm
sequence length for predicting the next word is    xed at 8 words. sgd is
done through rmsprop and the learning rate is    xed at 0.001.

the ll-id56 has the same architecture and parameters, but for the
following di   erences. first, the direct embedding of the input words is
replaced by an embedding of dimension 256 of the representation of the
words in the space of the m + 53 features (that is the input feature vector   
is of dimension m + 53). this is followed by the same two lstms as before,
both of dimension 256. this is now followed by a dense layer of output
dimension m + 53 (the a   weights over the output feature vector   , here
identical to   .). this layer is then transformed in a deterministic way into
a id203 distribution over v , after incorporation of a    xed background
id203 distribution b over v . this background b has been precomputed
as the unigram id203 distribution of the word types over the entire
corpus.17

5.4 results

table 4 shows the perplexity results we obtain for di   erent cases of the ll-
id56s as compared to the baseline id56. we use a notation such as ll-id56
(2500) to indicate a value m = 2500 for the number of frequent word types
considered as features. for each model, we stopped the training after the
validation loss (not shown) did not improve for three epochs.

17we thus use also the test corpora to estimate these unigram probabilities. this is
because the background requires to have some estimate of the id203 of all the words
it may encounter not only in training but also in test. however, this method is only a
proxy to a proper estimate of the background for all possible words, which we leave to
future development. we note that, similarly, for the baseline id56, we need to know
before hand all words it may encounter, otherwise we have to resort to the unk category,
which we did not want to do in order to be able to do a direct comparison of perplexities.

20

id56
ll-id56 (10000)
ll-id56 (5000)
ll-id56 (3000)
ll-id56 (2500)
ll-id56 (2000)
ll-id56 (1000)
ll-id56 (500)
ll-id56 (10)

training test1
4.17
4.68
4.33
4.65
4.74
4.77
4.84
4.93
5.30

6.07
5.17
5.13
5.11
5.08
5.11
5.13
5.20
5.45

test2
6.15
5.17
5.12
5.09
5.07
5.10
5.13
5.18
5.44

table 4: log-perplexities per word (base e) of di   erent models. the per-
plexity per word corresponding to a log-perplexity of 5.08 (resp. 6.07) is 161
(resp. 433).

we observe a considerable improvement of perplexity between the base-
line and all the ll-id56 models, the largest one being for m = 2500    
where the perplexity is divided by a factor of 433/161 (cid:39) 2.7     with some
tendency of the models to degrade when m becomes either very large or
very small.

an initial, informal, qualitative look at the sentences generated by the
id56 model on the one hand and by the best ll-id56 model on the other
hand, seems to indicate a much better ability of the ll-id56 to account for
agreement in gender and number at moderate distances (see table 5), but
a proper evaluation has not yet been performed.

21

elle

est

tr`es
souvent
repr  esent  ee

en
r  eaction
`a

l   

image
de

la

r  epublique

en
1999
.

pos:adj,

pos:aux,

topform:elle, pos:pron, pos:propn, gender:fem, num-
ber:sing, person:3, prontype:prs
topform:est,
pos:noun,
pos:propn, pos:sconj, pos:verb, pos:x, gender:fem,
gender:masc, mood:ind, number:sing, person:3, tense:pres,
verbform:fin
topform:tr`es, pos:adv
topform:souvent, pos:adv
topform:@nottop, pos:verb, gender:fem, number:sing,
tense:past, verbform:part
topform:en, pos:adp, pos:adv, pos:pron, person:3
topform: r  eaction, pos:noun, gender:fem, number:sing
topform:`a, pos:adp, pos:aux, pos:noun, pos:verb,
mood:ind, number:sing, person:3, tense:pres, verbform:fin
topform:l   ,
pos:pron,
pos:propn, de   nite:def, gender:fem, gender:masc, num-
ber:sing, person:3, prontype:prs
topform:image, pos:noun, gender:fem, number:sing
topform:de, pos:adp, pos:det, pos:propn, pos:x,
de   nite:ind, gender:fem, gender:masc, number:plur, num-
ber:sing, prontype:dem
topform:la, pos:adv, pos:det, pos:noun, pos:pron,
pos:propn, pos:x, de   nite:def, gender:fem, gender:masc,
number:sing, person:3, prontype:prs
topform:r  epublique, pos:noun, pos:propn, gender:fem,
number:sing
topform:en, pos:adp, pos:adv, pos:pron, person:3
topform:1999, pos:num
topform:., pos:punct

pos:part,

pos:det,

table 5: example of a sentence generated by ll-id56 (2500). the right
column shows the non-null features for each word. note that repr  esent  ee,
which is not among the most frequent 2500 words (topform:@nottop),
has proper agreement (gender:fem, number:sing) with the distant pronoun
elle.19

19as a side remark, we observe some    awed features, due to a small number of gold-
annotation errors, such as the fact that `a appears both with the correct pos:adp (ad-
position - a generic term covering prepositions), but also some impossible pos   s (aux,
noun,verb). we have not attempted to    lter out these (relatively rare) gold-annotation
mistakes, but doing so could only improve the results.

22

6 discussion

ll-id56s simply extend id56s by replacing the softmax parametrization of
the output with a log-linear one, but this elementary move has two major
consequences.
the    rst consequence is that two elements x, x(cid:48)     v , rather than being
individuals without connections, can now share attributes. this is a funda-
mental property for linguistics, where classical approaches represent words
as combination of    linguistic features   , such as pos, lemma, number, gen-
der, case, tense, aspect, register, etc. with the standard id56 softmax
approach, two words that are di   erent on even a single dimension have to
be predicted independently, which can only be done e   ectively in presence
of large training sets. in the ll-id56 approach, by associating di   erent   
features to the di   erent linguistic    features   , the model can learn to pre-
dict a plural number based on observation of plural numbers, an accusative
based on the observation of accusatives, and so on, and then predict word
forms that are combinations that have never been observed in the training
data. we saw an example of this phenomenon in the experiments of section
5.20 if the linguistic features encompass semantic classes (possibly provided
by id138, or else by semantically-oriented embeddings) then generaliza-
tions become possible over these semantic classes also. by contrast, in the
softmax case, not only the models are de   cient in presence of sparsity of
training data for word forms, but they also require to waste capacity of the
id56 parameters    to make them able to map to the large a   vectors that
are required to discriminate between the many elements of v ; with ll-based
id56s, the parametrization a   can in principle be smaller, because fewer   
features need to be speci   ed to obtain word level predictions.

the second consequence is that we can exploit rich prior knowledge
through the input features   , the background b, and the output features
  . we already gave some illustrations of incorporating prior knowledge in
this way, but there are many other possibilities. for example, in a dialogue
application that requires some answer utterances to contain numerical data
that can only be obtained by access to a knowledge base, a certain binary
   expert feature      e(k; x) could take the value 1 if and only if x is either
a non-number word or a speci   c number n obtained by some (more or less
complex) process exploiting the context k in conjunction with the knowl-
edge base.
in combination with a background b and other features in   ,

20similar observations have been done, in the quite di   erent    factored    model recently

proposed by garc    a-mart    nez et al. (2016).

23

who would be responsible for the linguistic quality of the answer utterance,
the   e feature, when activated, would ensure that if a number is produced
at this point, it is equal to n, but would not try to decide at exactly which
point a number should be produced (this is better left to the    language
specialists   : b and the other features). whether the feature   e is activated
would be decided by the id56: a large value of the e coordinate of a  ,t would
activate the feature, a small (close to null) value deactivate it.21

we conclude by a remark concerning the complementarity of the log-
linear component and the neural network component in the ll-id56 ap-
proach. on its own, as has been amply demonstrated in recent years, a
standard softmax-based id56 is already quite powerful. on its own, a
stand-alone log-linear model is also quite powerful, as older research also
demonstrated. roughly, the di   erence between a log-linear model and a
ll-id56 model is that in the    rst, the log-linear weights (in our notation,
a) are    xed after training, while in the ll-id56 they dynamically vary un-
der the control of the neural network component.22 however, the strengths
of the two classes of models lie in di   erent areas. the log-linear model is
very good at exploiting prior knowledge in the form of complex features,
but it has no ability to discover new combinations of features. on the other
hand, the id56 is very good at discovering which combinations of character-
istics of its input are predictive of the output (representation learning), but
is ill-equipped for exploiting prior knowledge. we argue that the ll-id56
approach is a way to capitalize on these complementary qualities.

21the idea is reminiscent of the approach of le et al. (2016), who use lstm-based
mixtures of experts for a similar purpose; the big di   erence is that here, instead of using
a linear mixture, we use a    log-linear mixture   , i.e. our features are combined multi-
plicatively rather than additively, with exponents given by the id56, that is they are
   collaborating   , while in their approach the experts are    competing   : their expert cor-
responding to   e needs to decide on its own at which exact point it should produce the
number, rather than relying on the linguistic specialist to do it.
this    multiplicative    aspect of the ll-id56s can be related to the product of experts in-
troduced by hinton (2002). however, in his case, the focus is on learning the individual
experts, which are then combined through a direct product, not involving exponentiations,
and therefore not in the log-linear class. in our case, the focus is on exploiting prede   ned
experts (or features), but on letting a    controlling    id56 decide about their exponents.

22note how a standard log-linear model with onehot features over v would not make
sense: with a    xed, it would always predict the same distribution for the next word.
by contrast, a ll-id56 over the same features does make sense: it is a standard id56.
standard id148 have to employ more interesting features.

24

acknowledgments

we thank salah ait-mokhtar, matthias gall  e, claire gardent,   eric gaussier,
raghav goyal and florent perronnin for discussions at various stages of this
research.

references

bahdanau, d., cho, k., and bengio, y. (2015). id4

by jointly learning to align and translate. iclr, pages 1   15.

berger, a. l., della pietra, s. a., and della pietra, v. j. (1996). a max-
imum id178 approach to natural language processing. computational
linguistics, 22(1):39   71.

cho, k., van merrienboer, b., bahdanau, d., and bengio, y. (2014). on the
properties of id4: encoder-decoder approaches.
proceedings of ssst-8, eighth workshop on syntax, semantics and struc-
ture in statistical translation, pages 103   111.

chollet, f. (2015). keras. https://github.com/fchollet/keras.

elkan, c. (2008). id148 and conditional random    elds. tutorial

notes at cikm, 8:1   12.

garc    a-mart    nez, m., barrault, l., and bougares, f. (2016). factored neural

machine translation. arxiv :1609.04621.

gers, f. a., schmidhuber, j. a., and cummins, f. a. (2000). learning
to forget: continual prediction with lstm. neural comput., 12(10):2451   
2471.

goodfellow, i., bengio, y., and courville, a. (2016). deep learning. book

in preparation for mit press.

goyal, r., dymetman, m., and gaussier, e. (2016). natural language gen-
eration through character-based id56s with finite-state prior knowl-
edge. in proc. coling, osaka, japan.

graves, a. (2012). supervised sequence labelling with recurrent neural net-

works, volume 385. springer.

25

hastie, t., tibshirani, r., and friedman, j. (2001). the elements of statis-
tical learning. springer series in statistics. springer new york inc., new
york, ny, usa.

hinton, g. e. (2002). training products of experts by minimizing contrastive

divergence. neural computation, 14(8):1771   1800.

hochreiter, s. and schmidhuber, j. (1997). long short-term memory. neural

computation, 9(8):1735   1780.

jaynes, e. t. (1957). id205 and statistical mechanics. phys.

rev., 106:620   630.

jean, s., cho, k., memisevic, r., and bengio, y. (2015). on using very
large target vocabulary for id4. proceedings
of the 53rd annual meeting of the association for computational lin-
guistics and the 7th international joint conference on natural language
processing (volume 1: long papers), 000:1   10.

jebara, t. (2013). id148, id28 and conditional
random fields. lecture notes: www.cs.columbia.edu/~jebara/6772/
notes/notes4.pdf.

jordan, m.

(20xx).

lec-
http://people.eecs.berkeley.edu/~jordan/courses/

exponential

family:

basics.

the

ture notes:
260-spring10/other-readings/chapter8.pdf.

la   erty, j. d., mccallum, a., and pereira, f. c. n. (2001). conditional
random    elds: probabilistic models for segmenting and labeling sequence
data. in proceedings of the eighteenth international conference on ma-
chine learning, icml    01, pages 282   289, san francisco, ca, usa. mor-
gan kaufmann publishers inc.

le, p., dymetman, m., and renders, j.-m. (2016). lstm-based mixture-
of-experts for knowledge-aware dialogues. in proceedings of the 1st work-
shop on representation learning for nlp, pages 94   99, berlin, germany.
association for computational linguistics.

ling, w., trancoso, i., dyer, c., and black, a. w. (2015). character-based

id4. iclr   16, pages 1   11.

luong, m.-t. and manning, c. d. (2016). achieving open vocabu-
lary id4 with hybrid word-character models.
arxiv:1604.00788v2.

26

luong, m.-t., sutskever, i., le, q. v., vinyals, o., and zaremba, w.
(2014). addressing the rare word problem in neural machine trans-
lation. arxiv:1410.8206v3.

mikolov, t., kara   at, m., burget, l., cernocky, j., and khudanpur, s.
(2010). recurrent neural network based language model. interspeech,
(september):1045   1048.

morin, f. and bengio, y. (2005). hierarchical probabilistic neural network
language model. proceedings of the tenth international workshop on
arti   cial intelligence and statistics, pages 246   252.

nielsen, f. and garcia, v. (2009). statistical exponential families: a digest

with    ash cards. arxiv:0911.4863.

och, f. j. and ney, h. (2002). discriminative training and maximum en-
tropy models for id151. in proceedings of the 40th
annual meeting on association for computational linguistics, acl    02,
pages 295   302, stroudsburg, pa, usa. association for computational
linguistics.

rosenfeld, r. (1996). a maximum id178 approach to adaptive statistical

language modelling. computer speech & language, 10(3):187   228.

sutskever, i., martens, j., and hinton, g. (2011). generating text with

recurrent neural networks. neural networks, 131(1):1017   1024.

sutskever, i., vinyals, o., and le, q. v. (2014). sequence to sequence learn-
ing with neural networks. in advances in neural information processing
systems, pages 3104   3112.

theano development team (2016).

for fast computation of mathematical expressions.
abs/1605.02688.

theano: a python framework
arxiv e-prints,

vinyals, o., kaiser, l., koo, t., petrov, s., sutskever, i., and hinton, g.

(2014). grammar as a foreign language. arxiv: 1412.7449v3.

vinyals, o. and le, q.

(2015).

a neural conversational model.

arxiv:1506.05869.

wen, t., gasic, m., mrksic, n., su, p., vandyke, d., and young, s. j.
(2015). semantically conditioned lstm-based id86
for spoken dialogue systems. in proceedings of the 2015 conference on

27

empirical methods in natural language processing, emnlp 2015, lis-
bon, portugal, september 17-21, 2015, pages 1711   1721.

xiao, c., dymetman, m., and gardent, c. (2016). sequence-based struc-
tured prediction for id29.
in proceedings of the 54th an-
nual meeting of the association for computational linguistics (volume
1: long papers), pages 1341   1350, berlin, germany. association for com-
putational linguistics.

28

