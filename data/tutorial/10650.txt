problems with evaluation of id27s

using word similarity tasks

manaal faruqui1 yulia tsvetkov1 pushpendre rastogi2 chris dyer1

1language technologies institute, carnegie mellon university
2department of computer science, johns hopkins university

{mfaruqui,ytsvetko,cdyer}@cs.cmu.edu, pushpendre@jhu.edu

6
1
0
2

 

n
u
j
 

2
2

 
 
]
l
c
.
s
c
[
 
 

3
v
6
7
2
2
0

.

5
0
6
1
:
v
i
x
r
a

abstract

lacking standardized extrinsic evaluation
methods for vector
representations of
words,
the nlp community has relied
heavily on word similarity tasks as a proxy
for intrinsic evaluation of word vectors.
word similarity evaluation, which corre-
lates the distance between vectors and hu-
man judgments of    semantic similarity   
is attractive, because it is computationally
inexpensive and fast.
in this paper we
present several problems associated with
the evaluation of word vectors on word
similarity datasets, and summarize exist-
ing solutions. our study suggests that the
use of word similarity tasks for evaluation
of word vectors is not sustainable and calls
for further research on evaluation meth-
ods.

1 introduction

despite the ubiquity of word vector representa-
tions in nlp, there is no consensus in the com-
munity on what is the best way for evaluating
word vectors. the most popular intrinsic eval-
uation task is the word similarity evaluation.
in
word similarity evaluation, a list of pairs of words
along with their similarity rating (as judged by hu-
man annotators) is provided. the task is to mea-
sure how well the notion of word similarity ac-
cording to humans is captured by the word vector
representations. table 1 shows some word pairs
along with their similarity judgments from ws-
353 (finkelstein et al., 2002), a popular word sim-
ilarity dataset.

let a, b be two words, and a, b     rd be their
corresponding word vectors in a d-dimensional
vector space. word similarity in the vector-space

word1
love
stock
money
development
lad

word2
sex
jaguar
cash
issue
brother

similarity score [0,10]

6.77
0.92
9.15
3.97
4.46

table 1: sample word pairs along with their hu-
man similarity judgment from ws-353.

can be obtained by computing the cosine similar-
ity between the word vectors of a pair of words:

cosine(a, b) =

a    b

kak kbk

(1)

where, kak is the    2-norm of the vector, and
a    b is the dot product of the two vectors. once
the vector-space similarity between the words is
computed, we obtain the lists of pairs of words
sorted according to vector-space similarity, and
human similarity. computing spearman   s correla-
tion (myers and well, 1995) between these ranked
lists provides some insight
the
learned word vectors capture intuitive notions of
word similarity.

into how well

word similarity evaluation is attractive, be-
cause it
is computationally inexpensive and
fast,
leading to faster prototyping and devel-
opment of word vector models.
the origin
of word similarity tasks can be tracked back
to rubenstein and goodenough (1965) who con-
structed a list of 65 word pairs with annotations
of human similarity judgment. they created this
dataset to validate the veracity of the distributional
hypothesis (harris, 1954) according to which the
meaning of words is evidenced by the context they
occur in. they found a positive correlation be-
tween contextual similarity and human-annotated
similarity of word pairs. since then, the lack of a
standard evaluation method for word vectors has
led to the creation of several ad hoc word sim-
ilarity datasets. table 2 provides a list of such

dataset
rg
mc
ws-353
yp-130
mturk-287
mturk-771
men
rw
verb
siid113x

reference
rubenstein and goodenough (1965)

word pairs
65
30 miller and charles (1991)
353
finkelstein et al. (2002)
130 yang and powers (2006)
287
771 halawi et al. (2012)
3000
2034
144
999 hill et al. (2014)

bruni et al. (2012)
luong et al. (2013)
baker et al. (2014)

radinsky et al. (2011)

table 2: word similarity datasets.

benchmarks obtained from wordvectors.org
(faruqui and dyer, 2014a).

in this paper, we give a comprehensive analysis
of the problems that are associated with the eval-
uation of word vector representations using word
similarity tasks.1 we survey existing literature to
construct a list of such problems and also sum-
marize existing solutions to some of the problems.
our    ndings suggest that word similarity tasks are
not appropriate for evaluating word vector repre-
sentations, and call for further research on better
evaluation methods

2 problems

we now discuss the major issues with evaluation
of word vectors using word similarity tasks, and
present existing solutions (if available) to address
them.

2.1 subjectivity of the task
the notion of word similarity is subjective and
is often confused with relatedness. for example,
cup, and coffee are related to each other, but not
similar. coffee refers to a plant (a living organ-
ism) or a hot brown drink, whereas cup is a man-
made object, which contains liquids, often coffee.
nevertheless, cup and coffee are rated more sim-
ilar than pairs such as car and train in ws-353
(finkelstein et al., 2002). such anomalies are also
found in recently constructed datasets like men
(bruni et al., 2012). thus, such datasets unfairly

1 an alternative to correlation-based word similarity eval-
uation is the word analogy task, where the task is to    nd the
missing word b    in the relation: a is to a    as b is to b   , where
a, a    are related by the same relation as a, a   . for exam-
ple, king : man :: queen : woman. mikolov et al. (2013b)
showed that this problem can be solved using the vector off-
set method: b   
    b     a + a   . levy and goldberg (2014a)
show that solving this equation is equivalent to computing
a linear combination of word similarities between the query
word b   , with the given words a, b, and b   . thus, the results
we present in this paper naturally extend to the word analogy
tasks.

penalize word vector models that capture the fact
that cup and coffee are dissimilar.

this

to address

in an attempt

limitation,
agirre et al. (2009) divided ws-353 into two sets
containing word pairs exhibiting only either sim-
ilarity or relatedness. recently, hill et al. (2014)
constructed a new word similarity dataset (sim-
lex), which captures the degree of similarity be-
tween words, and related words are considered
dissimilar. even though it is useful to separate the
concept of similarity and relatedness, it is not clear
as to which one should the word vector models be
expected to capture.

2.2 semantic or task-speci   c similarity?
distributional word vector models capture some
aspect of word co-occurrence statistics of the
words in a language (levy and goldberg, 2014b;
levy et al., 2015). therefore, to the extent these
models produce semantically coherent represen-
tations, it can be seen as evidence of the distri-
butional hypothesis of harris (1954). thus, word
embeddings like skip-gram, cbow, glove, lsa
(turney and pantel, 2010; mikolov et al., 2013a;
pennington et al., 2014) which are trained on
word co-occurrence counts can be expected to
capture semantic word similarity, and hence can
be evaluated on word similarity tasks.

word vector representations which are trained
as part of a neural network to solve a partic-
ular task (apart from word co-occurrence pre-
diction) are called distributed id27s
(collobert and weston, 2008), and they are task-
speci   c in nature. these embeddings capture task-
speci   c word similarity, for example, if the task
is of id52, two nouns cat and man might
be considered similar by the model, even though
they are not semantically similar. thus, evaluating
such task-speci   c id27s on word sim-
ilarity can unfairly penalize them. this raises the
question: what kind of word similarity should be
captured by the model?

2.3 no standardized splits & over   tting
to obtain generalizable machine learning mod-
els,
it is necessary to make sure that they do
not over   t to a given dataset. thus, the datasets
are usually partitioned into a training, devel-
opment and test set on which the model
is
trained, tuned and    nally evaluated, respectively
(manning and sch  tze, 1999). existing word sim-
ilarity datasets are not partitioned into training, de-

the

some of

the word

embeddings

velopment and test sets. therefore, optimizing the
word vectors to perform better at a word similar-
ity task implicitly tunes on the test set and over-
   ts the vectors to the task. on the other hand, if
researchers decide to perform their own splits of
the data, the results obtained across different stud-
ies can be incomparable. furthermore, the aver-
age number of word pairs in the word similarity
datasets is small (    781, cf. table 2), and parti-
tioning them further into smaller subsets may pro-
duce unstable results.
we now present

solutions
suggested by previous work to avoid over   t-
ting of word vectors to word similarity tasks.
faruqui and dyer (2014b), and lu et al. (2015)
evaluate
exclusively
on word similarity and word analogy tasks.
faruqui and dyer (2014b) tune their embedding
on one word similarity task and evaluate them
on all other tasks. this ensures that their vec-
tors are being evaluated on held-out datasets.
lu et al. (2015) propose to directly evaluate the
generalization of a model by measuring the
performance of a single model on a large gamut
of tasks. this evaluation can be performed in two
different ways:
(1) choose the hyperparameters
with best average performance across all tasks,
(2) choose the hyperparameters that beat
the
baseline vectors on most tasks.2 by selecting the
hyperparameters that perform well across a range
of tasks, these methods ensure that the obtained
vectors are generalizable.
stratos et al. (2015)
divided each word similarity dataset individually
into tuning and test set and reported results on the
test set.

2.4 low correlation with extrinsic evaluation

word similarity evaluation measures how well the
notion of word similarity according to humans
is captured in the vector-space word representa-
tions. word vectors that can capture word simi-
larity might be expected to perform well on tasks
that require a notion of explicit semantic sim-
ilarity between words like id141, entail-
ment. however, it has been shown that no strong
correlation is found between the performance
of word vectors on word similarity and extrin-
sic evaluation nlp tasks like text classi   cation,
parsing, id31 (tsvetkov et al., 2015;

2baseline vectors can be any off-the-shelf vector models.

schnabel et al., 2015).3 an absence of strong cor-
relation between the word similarity evaluation
and downstream tasks calls for alternative ap-
proaches to evaluation.

2.5 absence of statistical signi   cance
there has been a consistent omission of sta-
tistical signi   cance for measuring the differ-
ence in performance of
two vector models
on word similarity tasks.
statistical signi   -
cance testing is important for validating met-
ric gains in nlp (berg-kirkpatrick et al., 2012;
s  gaard et al., 2014), speci   cally while solving
non-convex objectives where results obtained due
to optimizer instability can often lead to incor-
rect
id136s (clark et al., 2011). the prob-
lem of statistical signi   cance in word similarity
evaluation was    rst systematically addressed by
shalaby and zadrozny (2015), who used steiger   s
test (steiger, 1980)4 to compute how signi   cant
the difference between rankings produced by two
different models is against the gold ranking. how-
ever, their method needs explicit ranked list of
words produced by the models and cannot work
when provided only with the correlation ratio of
each model with the gold ranking. this problem
was solved by rastogi et al. (2015), which we de-
scribe next.

rastogi et al. (2015) observed that the improve-
ments shown on small word similarity task
datasets by previous work were insigni   cant. we
now brie   y describe the method presented by
them to compute statistical signi   cance for word
similarity evaluation. let a and b be the rankings
produced by two word vector models over a list of
words pairs, and t be the human annotated rank-
ing. let rat , rbt and rab denote the spearman   s
correlation between a : t , b : t and a : b
resp. and   rat ,   rbt and   rab be their empirical
estimates. rastogi et al. (2015) introduce   r
p0 as
the minimum required difference for signi   cance
(mrds) which satis   es the following:

(rab < r)   (|  rbt      rat | <   r

p0) =    pval > p0
(2)
here pval is the id203 of the test statistic un-
der the null hypothesis that rat = rbt found us-

3in these studies, extrinsic evaluation tasks are those tasks
that use the dimensions of word vectors as features in a ma-
chine learning model. the model learns weights for how im-
portant these features are for the extrinsic task.

4a quick

tutorial

on steiger   s

test & scripts:

http://www.philippsinger.info/?p=347

ing the steiger   s test. the above conditional en-
sures that if the empirical difference between the
rank correlations of the scores of the competing
methods to the gold ratings is less than   r
p0 then
either the true correlation between the competing
methods is greater than r, or the null hypothesis
of no difference has p-value greater than p0.   r
p0
depends on the size of the dataset, p0 and r and
rastogi et al. (2015) present its values for com-
mon word similarity datasets. reporting statisti-
cal signi   cance in this way would help estimate
the differences between word vector models.

2.6 frequency effects in cosine similarity
the most common method of measuring the sim-
ilarity between two words in the vector-space is
to compute the cosine similarity between the cor-
responding word vectors. cosine similarity im-
plicitly measures the similarity between two unit-
length vectors (eq. 1). this prevents any bi-
ases in favor of frequent words which are longer
as they are updated more often during training
(turian et al., 2010).

ideally, if the geometry of embedding space is
primarily driven by semantics, the relatively small
number of frequent words should be evenly dis-
tributed through the space, while large number of
rare words should cluster around related, but more
frequent words. however, it has been shown that
vector-spaces contain hubs, which are vectors that
are close to a large number of other vectors in
the space (radovanovi  c et al., 2010). this prob-
lem manifests in word vector-spaces in the form
of words that have high cosine similarity with a
large number of other words (dinu et al., 2014).
schnabel et al. (2015) further re   ne this hubness
problem to show that there exists a power-law re-
lationship between the frequency-rank5 of a word
and the frequency-rank of its neighbors. specif-
ically, they showed that the average rank of the
1000 nearest neighbors of a word follows:

nn-rank     1000    word-rank0.17

(3)

this shows that pairs of words which have
similar frequency will be closer in the vector-
space, thus showing higher word similarity than
they should according to their word meaning.
even though newer datasets of word similar-
ity sample words from different frequency bins

5the rank of a word in vocabulary of the corpus sorted in

decreasing order of frequency.

(luong et al., 2013; hill et al., 2014),
this still
does not solve the problem that cosine similar-
ity in the vector-space gets polluted by frequency-
based effects. different distance id172
schemes have been proposed to downplay the fre-
quency/hubness effect when computing nearest
neighbors in the vector space (dinu et al., 2014;
toma  ev et al., 2011), but their applicability as an
absolute measure of distance for word similarity
tasks still needs to investigated.

2.7 inability to account for polysemy

many words have more than one meaning in a lan-
guage. for example, the word bank can either cor-
respond to a    nancial institution or to the land near
a river. however in ws-353, bank is given a sim-
ilarity score of 8.5/10 to money, signifying that
bank is a    nancial institution. such an assump-
tion of one sense per word is prevalent in many of
the existing word similarity tasks, and it can in-
correctly penalize a word vector model for captur-
ing a speci   c sense of the word absent in the word
similarity task.

introduced

to account for sense-speci   c word similarity,
huang et al. (2012)
the stanford
contextual word similarity dataset (scws), in
which the task is to compute similarity between
two words given the contexts they occur in.
for example, the words bank and money should
have a low similarity score given the contexts:
the river   , and    the
   along the east bank of
basis of all money laundering   . using cues from
the word   s context,
the correct word-sense can
be identi   ed and the appropriate word vector
can be used. unfortunately, word senses are
also ignored by majority of the frequently used
word vector models like skip-gram and glove.
however, there has been progress on obtaining
multiple vectors per word-type to account for dif-
ferent word-senses (reisinger and mooney, 2010;
huang et al., 2012;
neelakantan et al., 2014;
jauhar et al., 2015; rothe and sch  tze, 2015).

3 conclusion

in this paper we have identi   ed problems associ-
ated with word similarity evaluation of word vec-
tor models, and reviewed existing solutions wher-
ever possible. our study suggests that the use of
word similarity tasks for evaluation of word vec-
tors can lead to incorrect id136s and calls for
further research on evaluation methods.

until a better solution is found for intrinsic eval-
uation of word vectors, we suggest task-speci   c
evaluation: word vector models should be com-
pared on how well they can perform on a down-
stream nlp task. although task-speci   c evalu-
ation produces different rankings of word vector
models for different tasks (schnabel et al., 2015),
this is not necessarily a problem because different
vector models capture different types of informa-
tion which can be more or less useful for a partic-
ular task.

references
[agirre et al.2009] eneko agirre, enrique alfonseca,
keith hall, jana kravalova, marius pa  sca, and aitor
soroa. 2009. a study on similarity and relatedness
using distributional and id138-based approaches.
in proc. of naacl.

[baker et al.2014] simon baker, roi reichart, and
anna korhonen. 2014. an unsupervised model
for instance level subcategorization acquisition. in
proc. of emnlp.

[berg-kirkpatrick et al.2012] taylor berg-kirkpatrick,
david burkett, and dan klein. 2012. an empiri-
cal investigation of statistical signi   cance in nlp. in
proc. of emnlp.

[bruni et al.2012] elia bruni, gemma boleda, marco
baroni, and nam-khanh tran. 2012. distributional
semantics in technicolor. in proc. of acl.

[clark et al.2011] jonathan h clark, chris dyer, alon
lavie, and noah a smith. 2011. better hypothesis
testing for id151: control-
ling for optimizer instability. in proc. of acl.

[collobert and weston2008] ronan collobert and ja-
son weston. 2008. a uni   ed architecture for natu-
ral language processing: deep neural networks with
multitask learning. in proc. of icml.

[dinu et al.2014] georgiana dinu, angeliki lazaridou,
and marco baroni.
improving zero-shot
learning by mitigating the hubness problem. arxiv
preprint arxiv:1412.6568.

2014.

[faruqui and dyer2014a] manaal faruqui and chris
dyer. 2014a. community evaluation and exchange
of word vectors at wordvectors.org. in proc. of acl:
system demo.

[faruqui and dyer2014b] manaal faruqui and chris
dyer. 2014b. improving vector space word repre-
sentations using multilingual correlation.
in proc.
of eacl.

[finkelstein et al.2002] lev

evgeniy
gabrilovich, yossi matias, ehud rivlin, zach
solan, gadi wolfman, and eytan ruppin. 2002.

finkelstein,

placing search in context: the concept revisited.
acm transactions on information systems, 20(1).

[halawi et al.2012] guy halawi, gideon dror, evgeniy
gabrilovich, and yehuda koren. 2012. large-scale
learning of word relatedness with constraints.
in
proc. of sigkdd.

[harris1954] zellig harris. 1954. distributional struc-

ture. word, 10(23):146   162.

[hill et al.2014] felix hill, roi reichart, and anna ko-
rhonen. 2014. siid113x-999: evaluating semantic
models with (genuine) similarity estimation. corr,
abs/1408.3456.

[huang et al.2012] eric h huang, richard socher,
christopher d manning, and andrew y ng. 2012.
improving word representations via global context
and multiple word prototypes. in proc. of acl.

[jauhar et al.2015] sujay kumar jauhar, chris dyer,
and eduard hovy. 2015. ontologically grounded
multi-sense representation learning for semantic
vector space models. in proc. naacl.

[levy and goldberg2014a] omer levy and yoav gold-
berg. 2014a. linguistic regularities in sparse and
explicit word representations. in proc. of conll.

[levy and goldberg2014b] omer levy and yoav gold-
berg. 2014b. neural id27 as implicit
id105. in proc. of nips.

[levy et al.2015] omer levy, yoav goldberg, and ido
improving distributional similarity
dagan. 2015.
with lessons learned from id27s. tacl,
3:211   225.

[lu et al.2015] ang lu, weiran wang, mohit bansal,
kevin gimpel, and karen livescu. 2015. deep
multilingual correlation for improved word embed-
dings. in proc. of naacl.

[luong et al.2013] minh-thang

richard
socher, and christopher d. manning. 2013. better
word representations with id56s
for morphology. in proc. of conll.

luong,

[manning and sch  tze1999] christopher d. manning
and hinrich sch  tze. 1999. foundations of statisti-
cal natural language processing. mit press, cam-
bridge, ma, usa.

[mikolov et al.2013a] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013a. ef   cient estima-
tion of word representations in vector space. arxiv
preprint arxiv:1301.3781.

[mikolov et al.2013b] tomas mikolov, wen-tau yih,
and geoffrey zweig. 2013b. linguistic regularities
in continuous space word representations. in proc.
of naacl.

[miller and charles1991] george miller and walter
charles. 1991. contextual correlates of semantic
similarity.
in language and cognitive processes,
pages 1   28.

[stratos et al.2015] karl stratos, michael collins, and
daniel hsu. 2015. model-based id27s
from decompositions of count matrices. in proc. of
acl.

[toma  ev et al.2011] nenad

toma  ev,

milo  
and mirjana
radovanovic, dunja mladenic,
ivanovic.
2011. a probabilistic approach to
nearest-neighbor classi   cation: naive hubness
bayesian knn. in proc. of cikm.

[tsvetkov et al.2015] yulia tsvetkov, manaal faruqui,
wang ling, guillaume lample, and chris dyer.
2015. evaluation of word vector representations by
subspace alignment. in proc. of emnlp.

[turian et al.2010] joseph turian, lev ratinov, and
yoshua bengio. 2010. word representations: a sim-
ple and general method for semi-supervised learn-
ing. in proc. of acl.

[turney and pantel2010] peter d. turney and patrick
pantel. 2010. from frequency to meaning : vector
space models of semantics. jair, pages 141   188.

[yang and powers2006] dongqiang yang and david
m. w. powers. 2006. verb similarity on the tax-
onomy of id138.
in 3rd international id138
conference.

[myers and well1995] jerome l. myers and arnold d.
well. 1995. research design & statistical analysis.
routledge.

[neelakantan et al.2014] arvind neelakantan, jeevan
shankar, alexandre passos, and andrew mccallum.
2014. ef   cient non-parametric estimation of multi-
ple embeddings per word in vector space. in proc.
of emnlp.

[pennington et al.2014] jeffrey pennington, richard
socher, and christopher d. manning. 2014. glove:
global vectors for word representation. in proc. of
emnlp.

[radinsky et al.2011] kira

eugene
radinsky,
agichtein, evgeniy gabrilovich,
and shaul
markovitch. 2011. a word at a time: computing
word relatedness using temporal semantic analysis.
in proc. of www.

[radovanovi  c et al.2010] milo   radovanovi  c, alexan-
dros nanopoulos, and mirjana ivanovi  c.
2010.
hubs in space: popular nearest neighbors in high-
dimensional data. the journal of machine learning
research, 11:2487   2531.

[rastogi et al.2015] pushpendre rastogi, benjamin
van durme, and raman arora. 2015. multiview
lsa: representation learning via generalized cca. in
proc. of naacl.

[reisinger and mooney2010] joseph reisinger

and
raymond j. mooney.
2010. multi-prototype
vector-space models of word meaning. in proc. of
naacl.

[rothe and sch  tze2015] sascha rothe and hinrich
sch  tze. 2015. autoextend: extending word em-
beddings to embeddings for synsets and lexemes. in
proc. of acl.

[rubenstein and goodenough1965] herbert

ruben-
stein and john b. goodenough. 1965. contextual
correlates of synonymy. commun. acm, 8(10).

[schnabel et al.2015] tobias schnabel, igor labutov,
david mimno, and thorsten joachims. 2015. eval-
uation methods for unsupervised id27s.
in proc. of emnlp.

[shalaby and zadrozny2015] walid

and
wlodek zadrozny.
2015. measuring semantic
relatedness using mined semantic analysis. corr,
abs/1512.03465.

shalaby

[s  gaard et al.2014] anders s  gaard, anders

jo-
hannsen, barbara plank, dirk hovy, and h  c-
tor mart  nez alonso. 2014. what   s in a p-value in
nlp? in proc. of conll.

[steiger1980] james h steiger. 1980. tests for com-
paring elements of a correlation matrix. psycholog-
ical bulletin, 87(2):245.

