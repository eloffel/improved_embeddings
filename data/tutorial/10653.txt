pointing the unknown words

caglar gulcehre

universit  e de montr  eal

sungjin ahn

universit  e de montr  eal

ramesh nallapati

ibm t.j. watson research

bowen zhou

ibm t.j. watson research

yoshua bengio

universit  e de montr  eal
cifar senior fellow

abstract

the problem of rare and unknown words
is an important issue that can potentially
effect the performance of many nlp sys-
tems, including both the traditional count-
based and the deep learning models. we
propose a novel way to deal with the rare
and unseen words for the neural network
models using attention. our model uses
two softmax layers in order to predict the
next word in conditional language mod-
els: one predicts the location of a word
in the source sentence, and the other pre-
dicts a word in the shortlist vocabulary. at
each time-step, the decision of which soft-
max layer to use choose adaptively made
by an mlp which is conditioned on the
context. we motivate our work from a psy-
chological evidence that humans naturally
have a tendency to point towards objects in
the context or the environment when the
name of an object is not known. we ob-
serve improvements on two tasks, neural
machine translation on the europarl en-
glish to french parallel corpora and text
summarization on the gigaword dataset
using our proposed model.

introduction

1
words are the basic input/output units in most of
the nlp systems, and thus the ability to cover a
large number of words is a key to building a ro-
bust nlp system. however, considering that (i)
the number of all words in a language including
named entities is very large and that (ii) language
itself is an evolving system (people create new
words), this can be a challenging problem.

a common approach followed by the recent
neural network based nlp systems is to use a

softmax output layer where each of the output di-
mension corresponds to a word in a prede   ned
word-shortlist. because computing high dimen-
sional softmax is computationally expensive, in
practice the shortlist is limited to have only top-
k most frequent words in the training corpus. all
other words are then replaced by a special word,
called the unknown word (unk).

the shortlist approach has two fundamental
problems. the    rst problem, which is known as
the rare word problem, is that some of the words
in the shortlist occur less frequently in the train-
ing set and thus are dif   cult to learn a good repre-
sentation, resulting in poor performance. second,
it is obvious that we can lose some important in-
formation by mapping different words to a single
dummy token unk. even if we have a very large
shortlist including all unique words in the training
set, it does not necessarily improve the test perfor-
mance, because there still exists a chance to see an
unknown word at test time. this is known as the
unknown word problem.
in addition, increasing
the shortlist size mostly leads to increasing rare
words due to zipf   s law.

these two problems can be particularly critical
in language understanding tasks such as factoid
id53 (bordes et al., 2015) where the
words that we are interested in are often named en-
tities which are usually unknown or rare words.

in a similar situation, where we have a limited
information on how to call an object of interest, it
seems that humans (and also some primates) have
an ef   cient behavioral mechanism of drawing at-
tention to the object: pointing (matthews et al.,
2012). pointing makes it possible to deliver in-
formation and to associate context to a particular
object without knowing how to call it. in partic-
ular, human infants use pointing as a fundamental
communication tool (tomasello et al., 2007).

in this paper, inspired by the pointing behav-

6
1
0
2

 

g
u
a
1
2

 

 
 
]
l
c
.
s
c
[
 
 

3
v
8
4
1
8
0

.

3
0
6
1
:
v
i
x
r
a

ior of humans and recent advances in the atten-
tion mechanism (bahdanau et al., 2014) and the
id193 (vinyals et al., 2015), we pro-
pose a novel method to deal with the rare or un-
known word problem. the basic idea is that we
can see in many nlp problems as a task of predict-
ing target text given context text, where some of
the target words appear in the context as well. we
observe that in this case we can make the model
learn to point a word in the context and copy it to
the target text, as well as when to point. for exam-
ple, in machine translation, we can see the source
sentence as the context, and the target sentence as
what we need to predict.
in figure 1, we show
an example depiction of how words can be copied
from source to target in machine translation. al-
though the source and target languages are differ-
ent, many of the words such as named entities are
usually represented by the same characters in both
languages, making it possible to copy. similarly,
in text summarization, it is natural to use some
words in the original text in the summarized text
as well.

speci   cally, to predict a target word at each
timestep, our model    rst determines the source of
the word generation, that is, on whether to take
one from a prede   ned shortlist or to copy one from
the context. for the former, we apply the typical
softmax operation, and for the latter, we use the
attention mechanism to obtain the pointing soft-
max id203 over the context words and pick
the one of high id203. the model learns this
decision so as to use the pointing only when the
context includes a word that can be copied to the
target. this way, our model can predict even the
words which are not in the shortlist, as long as
it appears in the context. although some of the
words still need to be labeled as unk, i.e., if it is
neither in the shortlist nor in the context, in ex-
periments we show that this learning when and
where to point improves the performance in ma-
chine translation and text summarization.

the rest of the paper is organized as follows. in
the next section, we review the related works in-
cluding id193 and previous approaches
to the rare/unknown problem.
in section 3, we
review the id4 with atten-
tion mechanism which is the baseline in our ex-
periments. then, in section 4, we propose our
method dealing with the rare/unknown word prob-
lem, called the pointer softmax (ps). the exper-

figure 1: an example of how copying can happen
for machine translation. common words that ap-
pear both in source and the target can directly be
copied from input to source. the rest of the un-
known in the target can be copied from the input
after being translated with a dictionary.

imental results are provided in the section 5 and
we conclude our work in section 6.

2 related work

the attention-based pointing mechanism is intro-
duced    rst in the id193 (vinyals et al.,
2015). in the id193, the output space of
the target sequence is constrained to be the obser-
vations in the input sequence (not the input space).
instead of having a    xed dimension softmax out-
put layer, softmax outputs of varying dimension is
dynamically computed for each input sequence in
such a way to maximize the attention id203
of the target input. however, its applicability is
rather limited because, unlike our model, there is
no option to choose whether to point or not; it al-
ways points. in this sense, we can see the pointer
networks as a special case of our model where we
always choose to point a context word.

several approaches have been proposed towards
solving the rare words/unknown words problem,
which can be broadly divided into three categories.
the    rst category of the approaches focuses on
improving the computation speed of the softmax
output so that it can maintain a very large vocabu-
lary. because this only increases the shortlist size,
it helps to mitigate the unknown word problem,
but still suffers from the rare word problem. the
hierarchical softmax (morin and bengio, 2005),
importance sampling (bengio and sen  ecal, 2008;
jean et al., 2014), and the noise contrastive esti-
mation (gutmann and hyv  arinen, 2012; mnih and
kavukcuoglu, 2013) methods are in the class.

the second category, where our proposed
method also belongs to, uses information from the
context. notable works are (luong et al., 2015)
and (hermann et al., 2015).
in particular, ap-
plying to machine translation task, (luong et al.,
2015) learns to point some words in source sen-
tence and copy it to the target sentence, similarly

guillaumeet cesaront une voiture id7e a lausanne.guillaumeand cesarhave a blue car in lausanne.copycopycopyfrench:english:to our method. however, it does not use atten-
tion mechanism, and by having    xed sized soft-
max output over the relative pointing range (e.g.,
-7, . . . , -1, 0, 1, . . . , 7), their model (the posi-
tional all model) has a limitation in applying to
more general problems such as summarization and
id53, where, unlike machine trans-
lation, the length of the context and the pointing
locations in the context can vary dramatically. in
id53 setting, (hermann et al., 2015)
have used placeholders on named entities in the
context. however, the placeholder id is directly
predicted in the softmax output rather than predict-
ing its location in the context.

the third category of the approaches changes
the unit of input/output itself from words to a
smaller resolution such as characters (graves,
2013) or bytecodes (sennrich et al., 2015; gillick
et al., 2015). although this approach has the
main advantage that it could suffer less from the
rare/unknown word problem, the training usually
becomes much harder because the length of se-
quences signi   cantly increases.

simultaneously to our work, (gu et al., 2016)
and (cheng and lapata, 2016) proposed models
that learn to copy from source to target and both
papers analyzed their models on summarization
tasks.

3 id4 model

with attention

as the baseline id4 sys-
tem, we use the model proposed by (bahdanau et
al., 2014) that learns to (soft-)align and translate
jointly. we refer this model as id4.

      
h 1, . . . ,

the encoder of the id4 is a bidirectional
id56 (schuster and paliwal, 1997). the forward
id56 reads input sequence x = (x1, . . . , xt )
in left-to-right direction, resulting in a sequence
      
of hidden states (
h t ). the backward
id56 reads x in the reversed direction and outputs
      
      
h t ). we then concatenate the hidden
h 1, . . . ,
(
states of forward and backward id56s at each time
step and obtain a sequence of annotation vectors
. here, ||
(h1, . . . , ht ) where hj =
denotes the concatenation operator. thus, each an-
notation vector hj encodes information about the
j-th word with respect to all the other surrounding
words in both directions.

(cid:104)      
h j||      

(cid:105)

h j

in the decoder, we usually use gated recur-
rent unit (gru) (cho et al., 2014; chung et al.,

2014). speci   cally, at each time-step t, the soft-
alignment mechanism    rst computes the relevance
weight etj which determines the contribution of
annotation vector hj to the t-th target word. we
use a non-linear mapping f (e.g., mlp) which
takes hj, the previous decoder   s hidden state st   1
and the previous output yt   1 as input:

etj = f (st   1, hj, yt   1).

the outputs etj are then normalized as follows:

(cid:80)t

exp(etj)
k=1 exp(etk)

ltj =

.

(1)

we call ltj as the relevance score, or the align-

ment weight, of the j-th annotation vector.

the relevance scores are used to get the context
vector ct of the t-th target word in the translation:

t(cid:88)

ct =

ltjhj ,

j=1

the hidden state of the decoder st is computed
based on the previous hidden state st   1, the con-
text vector ct and the output word of the previous
time-step yt   1:

st = fr(st   1, yt   1, ct),

(2)

where fr is gru.

we use a deep output layer (pascanu et al.,
2013) to compute the conditional distribution over
words:

p(yt = a|y<t, x)    

(cid:16)

exp

  a
(wo,bo)fo(st, yt   1, ct)

(cid:17)

(3)

,

where w is a learned weight matrix and b is a
bias of the output layer. fo is a single-layer feed-
forward neural network.   (wo,bo)(  ) is a function
that performs an af   ne transformation on its input.
and the superscript a in   a indicates the a-th col-
umn vector of   .

the whole model, including both the encoder
and the decoder, is jointly trained to maximize the
(conditional) log-likelihood of target sequences
given input sequences, where the training corpus
is a set of (xn, yn)   s. figure 2 illustrates the ar-
chitecture of the id4.

be generated at the time step, we obtain the loca-
tion of the context word lt from the location soft-
max. the key to making this possible is decid-
ing when to use the shortlist softmax or the lo-
cation softmax at each time step. in order to ac-
complish this, we introduce a switching network
to the model. the switching network, which is
a multilayer id88 in our experiments, takes
the representation of the context sequence (similar
to the input annotation in id4) and the previous
hidden state of the output id56 as its input. it out-
puts a binary variable zt which indicates whether
to use the shortlist softmax (when zt = 1) or the
location softmax (when zt = 0). note that if the
word that is expected to be generated at each time-
step is neither in the shortlist nor in the context se-
quence, the switching network selects the shortlist
softmax, and then the shortlist softmax predicts
unk. the details of the pointer softmax model can
be seen in figure 3 as well.

figure 3: a depiction of the pointer softmax (ps)
architecture. at each timestep, lt, ct and wt for
the words over the limited vocabulary (shortlist)
is generated. we have an additional switching
variable zt that decides whether to use vocabulary
word or to copy a word from the source sequence.

more speci   cally, our goal is to maximize the
id203 of observing the target word sequence
y = (y1, y2, . . . , yty ) and the word generation
source z = (z1, z2, . . . , zty ), given the context se-
quence x = (x1, x2, . . . , xtx):

p  (y, z|x) =

p  (yt, zt|y<t, z<t, x).

(4)

t=1

note that the word observation yt can be either
a word wt from the shortlist softmax or a loca-
tion lt from the location softmax, depending on
the switching variable zt.

considering this, we can factorize the above

ty(cid:89)

figure 2: a depiction of neural machine transla-
tion architecture with attention. at each timestep,
the model generates the attention distribution lt.
we use lt and the encoder   s hidden states to obtain
the context ct. the decoder uses ct to predict a
vector of probabilities for the words wt by using
vocabulary softmax.

4 the pointer softmax
in this section, we introduce our method, called as
the pointer softmax (ps), to deal with the rare and
unknown words. the pointer softmax can be an
applicable approach to many nlp tasks, because
it resolves the limitations about unknown words
for neural networks. it can be used in parallel with
other existing techniques such as the large vocabu-
lary trick (jean et al., 2014). our model learns two
key abilities jointly to make the pointing mech-
anism applicable in more general settings: (i) to
predict whether it is required to use the pointing
or not at each time step and (ii) to point any lo-
cation of the context sequence whose length can
vary widely over examples. note that the pointer
networks (vinyals et al., 2015) are in lack of the
ability (i), and the ability (ii) is not achieved in the
models by (luong et al., 2015).

to achieve this, our model uses two softmax
output layers, the shortlist softmax and the loca-
tion softmax. the shortlist softmax is the same
as the typical softmax output layer where each
dimension corresponds a word in the prede   ned
word shortlist. the location softmax is a pointer
network where each of the output dimension cor-
responds to the location of a word in the context
sequence. thus, the output dimension of the loca-
tion softmax varies according to the length of the
given context sequence.

at each time-step, if the model decides to use
the shortlist softmax, we generate a word wt from
the shortlist. otherwise, if it is expected that the
context sequence contains a word which needs to

h2hth1   stctywtyt-1vocabulary softmaxattention distribution (lt) source sequencex2xtx1   biid56target sequencest-1h2hth1   stctztyltywtyt-1vocabulary softmaxpointer distribution (lt) source sequencepoint & copyx2xtx1   biid56target sequencep1-pst-1equation further
p(y, z|x) =

(cid:89)
(cid:89)

t   tw

t(cid:48)   tl

p(wt, zt|(y, z)<t, x)   

p(lt(cid:48), zt(cid:48)|(y, z)<t(cid:48), x).

(5)

here, tw is a set of time steps where zt = 1, and tl
is a set of time-steps where zt = 0. and, tw   tl =
{1, 2, . . . , ty} and tw     tl =    . we denote all
previous observations at step t by (y, z)<t. note
also that ht = f ((y, z)<t).

then, the joint probabilities inside each product

can be further factorized as follows:
p(wt, zt|(y, z)<t) = p(wt|zt = 1, (y, z)<t)   
(6)

p(zt = 1|(y, z)<t)

p(lt, zt|(y, z)<t) = p(lt|zt = 0, (y, z)<t)   
(7)

p(zt = 0|(y, z)<t)

here, we omitted x which is conditioned on all
probabilities in the above.

the switch id203 is modeled as a multi-

layer id88 with binary output:

p(zt = 1|(y, z)<t, x) =   (f (x, ht   1;   )) (8)
p(zt = 0|(y, z)<t, x) = 1       (f (x, ht   1;   )). (9)
and p(wt|zt = 1, (y, z)<t, x) is the shortlist
softmax and p(lt|zt = 0, (y, z)<t, x) is the lo-
cation softmax which can be a pointer network.
  (  ) stands for the sigmoid function,   (x) =
exp(-x)+1

given n such context and target sequence pairs,
our training objective is to maximize the following
log likelihood w.r.t. the model parameter   

.

1

arg max

  

1
n

log p  (yn, zn|xn).

(10)

4.1 basic components of the pointer softmax
in this section, we discuss practical details of the
three fundamental components of the pointer soft-
max. the interactions between these components
and the model is depicted in figure 3.
location softmax lt
: the location of the word
to copy from source text to the target is predicted
by the location softmax lt. the location soft-
max outputs the id155 distribu-
tion p(lt|zt = 0, (y, z)<t, x). for models using the

n(cid:88)

n=1

attention mechanism such as id4, we can reuse
the id203 distributions over the source words
in order to predict the location of the word to point.
otherwise we can simply use a pointer network of
the model to predict the location.

shortlist softmax wt
: the subset of the words
in the vocabulary v is being predicted by the
shortlist softmax wt.
switching network dt
: the switching network
dt is an mlp with sigmoid output function that
outputs a scalar id203 of switching between
lt and wt, and represents the conditional prob-
ability distribution p(zt|(y, z)<t, x). for id4
model, we condition the mlp that outputs the
switching id203 on the representation of the
context of the source text ct and the hidden state
of the decoder ht. note that, during the training,
dt is observed, and thus we do not have to sample.
the output of the pointer softmax, ft will be the
concatenation of the the two vectors, dt    wt and
(1     dt)    lt.

at test time, we compute eqn. (6) and (7) for
all shortlist word wt and all location lt, and pick
the word or location of the highest id203.

5 experiments

in this section, we provide our main experimen-
tal results with the pointer softmax on machine
translation and summarization tasks.
in our ex-
periments, we have used the same baseline model
and just replaced the softmax layer with pointer
softmax layer at the language model. we use the
adadelta (zeiler, 2012) learning rule for the train-
ing of id4 models. the code for pointer softmax
model is available at https://github.com/
caglar/pointer_softmax.

5.1 the rarest word detection
we construct a synthetic task and run some prelim-
inary experiments in order to compare the results
with the pointer softmax and the regular softmax   s
performance for the rare-words. the vocabulary
size of our synthetic task is |v |= 600 using se-
quences of length 7. the words in the sequences
are sampled according to their unigram distribu-
tion which has the form of a geometric distribu-
tion. the task is to predict the least frequent word
in the sequence according to unigram distribution
of the words. during the training, the sequences
are generated randomly. before the training, val-

idation and test sets are constructed with a    xed
seed.

we use a gru layer over the input sequence
and take the last-hidden state, in order to get the
summary ct of the input sequence. the wt, lt
are only conditioned on ct, and the mlp pre-
dicting the dt is conditioned on the latent repre-
sentations of wt and lt. we use minibatches of
size 250 using adam adaptive learning rate algo-
rithm (kingma and adam, 2015) using the learn-
ing rate of 8    10   4 and hidden layers with 1000
units.

we train a model with pointer softmax where
we assign pointers for the rarest 60 words and the
rest of the words are predicted from the shortlist
softmax of size 540. we observe that increasing
the inverse temperature of the sigmoid output of
dt to 2, in other words making the decisions of dt
to become sharper, works better, i.e. dt =   (2x).
at the end of training with pointer softmax we
obtain the error rate of 17.4% and by using soft-
max over all 600 tokens, we obtain the error-rate
of 48.2%.

5.2 summarization
in these series of experiments, we use the anno-
tated gigaword corpus as described in (rush et al.,
2015). moreover, we use the scripts that are made
available by the authors of (rush et al., 2015) 1
to preprocess the data, which results to approxi-
mately 3.8m training examples. this script gen-
erates about 400k validation and an equal number
of test examples, but we use a randomly sampled
subset of 2000 examples each for validation and
testing. we also have made small modi   cations to
the script to extract not only the tokenized words,
but also system-generated named-entity tags. we
have created two different versions of training data
for pointers, which we call unk-pointers data and
entity-pointers data respectively.

for the unk-pointers data, we trim the vocabu-
lary of the source and target data in the training set
and replace a word by the unk token whenever
a word occurs less than 5 times in either source
or target data separately. then, we create pointers
from each unk token in the target data to the posi-
tion in the corresponding source document where
the same word occurs in the source, as seen in the
data before unk   s were created. it is possible that
the source can have an unk in the matching posi-

1https://github.com/facebook/namas

tion, but we still created a pointer in this scenario
as well. the resulting data has 2.7 pointers per
100 examples in the training set and 9.1 pointers
rate in the validation set.

in the entity-pointers data, we exploit

the
named-entity tags in the annotated corpus and    rst
anonymize the entities by replacing them with an
integer-id that always starts from 1 for each doc-
ument and increments from left to right. entities
that occur more than once in a single document
share the same id. we create the anonymization at
token-level, so as to allow partial entity matches
between the source and target for multi-token en-
tities. next, we create a pointer from the target
to source on similar lines as before, but only for
exact matches of the anonymized entities. the re-
sulting data has 161 pointers per 100 examples in
the training set and 139 pointers per 100 examples
in the validation set.

if there are multiple matches in the source,
either in the unk-pointers data or the entity-
pointers data, we resolve the con   ict in favor of
the    rst occurrence of the matching word in the
source document.
in the unk data, we model
the unk tokens on the source side using a sin-
gle placeholder embedding that is shared across
all documents, and in the entity-pointers data, we
model each entity-id in the source by a distinct
placeholder, each of which is shared across all
documents.

in all our experiments, we use a bidirectional
gru-id56 (chung et al., 2014) for the encoder
and a uni-directional id56 for the decoder. to
speed-up training, we use the large-vocabulary
trick (jean et al., 2014) where we limit the vocab-
ulary of the softmax layer of the decoder to 2000
words dynamically chosen from the words in the
source documents of each batch and the most com-
mon words in the target vocabulary. in both ex-
periments, we    x the embedding size to 100 and
the hidden state dimension to 200. we use pre-
trained id97 vectors trained on the same cor-
pus to initialize the embeddings, but we    netune
them by backpropagating through the pre-trained
embeddings during training. our vocabulary sizes
are    xed to 125k for source and 75k for target for
both experiments.

we use the reference data for pointers for the
model only at the training time. during the test
time, the switch makes a decision at every timestep
on which softmax layer to use.

for evaluation, we use full-length id8 f1 us-
ing the of   cial evaluation tool 2. in their work, the
authors of (bahdanau et al., 2014) use full-length
id8 recall on this corpus, since the maximum
length of limited-length version of id8 recall
of 75 bytes (intended for duc data) is already
long for gigaword summaries. however, since
full-length recall can unfairly reward longer sum-
maries, we also use full-length f1 in our experi-
ments for a fair comparison between our models,
independent of the summary length.

the experimental results comparing the pointer
softmax with id4 model are displayed in ta-
ble 1 for the unk pointers data and in table 2
for the entity pointers data. as our experiments
show, pointer softmax improves over the baseline
id4 on both unk data and entities data. our
hope was that the improvement would be larger
for the entities data since the incidence of point-
ers was much greater. however, it turns out this
is not the case, and we suspect the main reason
is anonymization of entities which removed data-
sparsity by converting all entities to integer-ids
that are shared across all documents. we believe
that on de-anonymized data, our model could help
more, since the issue of data-sparsity is more acute
in this case.

table 1: results on gigaword corpus when point-
ers are used for unks in the training data, using
id8-f1 as the evaluation metric.

id8-1 id8-2 id8-l
id4 + lvt
34.87
id4 + lvt + ps 35.19

16.54
16.66

32.27
32.51

table 2: results on anonymized gigaword corpus
when pointers are used for entities, using id8-
f1 as the evaluation metric.

id8-1 id8-2 id8-l
34.89
id4 + lvt
id4 + lvt + ps 35.11

16.78
16.76

32.37
32.55

in table 3, we provide the results for summa-
rization on gigaword corpus in terms of recall as
also similar comparison is done by (rush et al.,
2015). we observe improvements on all the scores
with the addition of pointer softmax. let us note

2http://www.beid8.com/pages/default.

aspx

table 3: results on gigaword corpus for model-
ing unk   s with pointers in terms of recall.

id8-1 id8-2 id8-l
id4 + lvt
36.45
id4 + lvt + ps 37.29

33.90
34.70

17.41
17.75

that, since the test set of (rush et al., 2015) is not
publicly available, we sample 2000 texts with their
summaries without replacement from the valida-
tion set and used those examples as our test set.

in table 4 we present a few system gener-
ated summaries from the pointer softmax model
trained on the unk pointers data. from those ex-
amples, it is apparent that the model has learned to
accurately point to the source positions whenever
it needs to generate rare words in the summary.

5.3 id4
in our id4 (id4) experi-
ments, we train id4 models with attention over
the europarl corpus (bahdanau et al., 2014) over
the sequences of length up to 50 for english to
french translation. 3. all models are trained with
early-stopping which is done based on the negative
log-likelihood (nll) on the development set. our
evaluations to report the performance of our mod-
els are done on newstest2011 by using blue
score. 4

we use 30, 000 tokens for both the source and
the target language shortlist vocabularies (1 of the
token is still reserved for the unknown words).
the whole corpus contains 134, 831 unique en-
glish words and 153, 083 unique french words.
we have created a word-level dictionary from
french to english which contains translation of
15,953 words that are neither in shortlist vocab-
ulary nor dictionary of common words for both
the source and the target. there are about 49, 490
words shared between english and french parallel
corpora of europarl.

during the training, in order to decide whether
to pick a word from the source sentence using at-
tention/pointers or to predict the word from the
short-list vocabulary, we use the following sim-
if the word is not in the short-list
ple heuristic.

3in our experiments, we use an existing code, pro-
vided
in https://github.com/kyunghyuncho/
dl4mt-material, and on the original model we only
changed the last softmax layer for our experiments

4we compute the id7 score using the multi-blue.perl

script from moses on tokenized sentence pairs.

table 4: generated summaries from id4 with ps. boldface words are the words copied from the source.

source #1

target #1
id4+ps #1

source #2

target #2
id4+ps #2
source #3

target #3
id4+ps #3

china    s tang gonghong set a world record with a clean and
jerk lift of ### kilograms to win the women    s over-## kilogram
weightlifting title at the asian games on tuesday .
china    s tang <unk>,sets world weightlifting record
china    s tang gonghong wins women    s weightlifting weightlift-
ing title at asian games
owing to criticism , nbc said on wednesday that it was ending
a three-month-old experiment that would have brought the    rst
liquor advertisements onto national broadcast network television
.
advertising : nbc retreats from liquor commercials
nbc says it is ending a three-month-old experiment
a senior trade union of   cial here wednesday called on ghana    s
government to be     mindful of the plight     of the ordinary people
in the country in its decisions on tax increases .
tuc of   cial,on behalf of ordinary ghanaians
ghana    s government urged to be mindful of the plight

vocabulary, we    rst check if the same word yt ap-
pears in the source sentence. if it is not, we then
check if a translated version of the word exists in
the source sentence by using a look-up table be-
tween the source and the target language. if the
word is in the source sentence, we then use the lo-
cation of the word in the source as the target. oth-
erwise we check if one of the english senses from
the cross-language dictionary of the french word
is in the source. if it is in the source sentence, then
we use the location of that word as our translation.
otherwise we just use the argmax of lt as the tar-
get.

for switching network dt, we observed that us-
ing a two-layered mlp with noisy-tanh activation
(gulcehre et al., 2016) function with residual con-
nection from the lower layer (he et al., 2015) ac-
tivation function to the upper hidden layers im-
proves the id7 score about 1 points over the
dt using relu activation function. we initialized
the biases of the last sigmoid layer of dt to    1
such that if dt becomes more biased toward choos-
ing the shortlist vocabulary at the beginning of the
training. we renormalize the gradients if the norm
of the gradients exceed 1 (pascanu et al., 2012).

table 5: europarl dataset (en-fr)

id7-4
id4
20.19
id4 + ps 23.76

in table 5, we provided the result of id4 with
pointer softmax and we observe about 3.6 id7
score improvement over our baseline.

figure 4: a comparison of the validation learning-
curves of the same id4 model
trained with
pointer softmax and the regular softmax layer. as
can be seen from the    gures, the model trained
with pointer softmax converges faster than the reg-
ular softmax layer. switching network for pointer
softmax in this figure uses relu activation func-
tion.

in figure 4, we show the validation curves
of the id4 model with attention and the id4
model with shortlist-softmax layer. pointer soft-
max converges faster in terms of number of mini-
batch updates and achieves a lower validation
negative-log-likelihood (nll) (63.91) after 200k
updates over the europarl dataset than the id4

0510152025303540# iterations (x5000 minibatches)60708090100110120130140150valid nlloriginal modelpointer softmaxmodel with shortlist softmax trained for 400k
minibatch updates (65.26). pointer softmax con-
verges faster than the model using the short-
list softmax, because the targets provided to the
pointer softmax also acts like guiding hints to the
attention.

6 conclusion
in this paper, we propose a simple extension to
the traditional soft attention-based shortlist soft-
max by using pointers over the input sequence. we
show that the whole model can be trained jointly
with single objective function. we observe no-
ticeable improvements over the baselines on ma-
chine translation and summarization tasks by us-
ing pointer softmax. by doing a very simple mod-
i   cation over the id4, our model is able to gen-
eralize to the unseen words and can deal with rare-
words more ef   ciently. for the summarization
task on gigaword dataset, the pointer softmax was
able to improve the results even when it is used
together with the large-vocabulary trick.
in the
case of id4, we observed
that the training with the pointer softmax is also
improved the convergence speed of the model as
well. for french to english machine translation
on europarl corpora, we observe that using the
pointer softmax can also improve the training con-
vergence of the model.

references
[bahdanau et al.2014] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2014. neural machine
translation by jointly learning to align and translate.
corr, abs/1409.0473.

[bengio and sen  ecal2008] yoshua bengio and jean-
s  ebastien sen  ecal.
2008. adaptive importance
sampling to accelerate training of a neural proba-
bilistic language model. neural networks, ieee
transactions on, 19(4):713   722.

[bordes et al.2015] antoine bordes, nicolas usunier,
sumit chopra, and jason weston. 2015. large-
scale simple id53 with memory net-
works. arxiv preprint arxiv:1506.02075.

[cheng and lapata2016] jianpeng cheng and mirella
neural summarization by ex-
arxiv preprint

lapata.
tracting sentences and words.
arxiv:1603.07252.

2016.

[cho et al.2014] kyunghyun

bart
van merri  enboer, caglar gulcehre, dzmitry
bahdanau, fethi bougares, holger schwenk,
and yoshua bengio.
learning phrase

2014.

cho,

representations using id56 encoder-decoder
id151.
arxiv:1406.1078.

for
arxiv preprint

[chung et al.2014] junyoung chung, c   aglar g  ulc  ehre,
kyunghyun cho, and yoshua bengio. 2014. em-
pirical evaluation of gated recurrent neural networks
on sequence modeling. corr, abs/1412.3555.

[gillick et al.2015] dan gillick, cliff brunk, oriol
vinyals, and amarnag subramanya. 2015. mul-
arxiv
tilingual language processing from bytes.
preprint arxiv:1512.00103.

[graves2013] alex graves.

quences with recurrent neural networks.
preprint arxiv:1308.0850.

2013. generating se-
arxiv

[gu et al.2016] jiatao gu, zhengdong lu, hang li,
and victor ok li. 2016.
incorporating copying
mechanism in sequence-to-sequence learning. arxiv
preprint arxiv:1603.06393.

[gulcehre et al.2016] caglar

marcin
moczulski, misha denil, and yoshua bengio.
2016. noisy id180. arxiv preprint
arxiv:1603.00391.

gulcehre,

[gutmann and hyv  arinen2012] michael u gutmann
and aapo hyv  arinen. 2012. noise-contrastive esti-
mation of unnormalized statistical models, with ap-
plications to natural image statistics. the journal of
machine learning research, 13(1):307   361.

[he et al.2015] kaiming he, xiangyu zhang, shao-
qing ren, and jian sun.
2015. deep resid-
ual learning for mage recognition. arxiv preprint
arxiv:1512.03385.

[hermann et al.2015] karl moritz hermann, tomas
kocisky, edward grefenstette, lasse espeholt, will
kay, mustafa suleyman, and phil blunsom. 2015.
teaching machines to read and comprehend. in ad-
vances in neural information processing systems,
pages 1684   1692.

[jean et al.2014] s  ebastien jean, kyunghyun cho,
roland memisevic, and yoshua bengio. 2014. on
using very large target vocabulary for neural ma-
chine translation. arxiv preprint arxiv:1412.2007.

[kingma and adam2015] diederik p kingma

and
jimmy ba adam. 2015. a method for stochastic
in international conference on
optimization.
learning representation.

[luong et al.2015] minh-thang luong, ilya sutskever,
quoc v le, oriol vinyals, and wojciech zaremba.
2015. addressing the rare word problem in neural
machine translation. in proceedings of acl.

[matthews et al.2012] danielle matthews,

tanya
behne, elena lieven, and michael tomasello.
2012. origins of the human pointing gesture: a
training study. developmental science, 15(6):817   
829.

support: nserc, samsung, calcul qu  ebec, com-
pute canada, the canada research chairs and ci-
far. c. g. thanks for ibm t.j. watson research
for funding this research during his internship be-
tween october 2015 and january 2016.

[mnih and kavukcuoglu2013] andriy mnih and koray
kavukcuoglu. 2013. learning id27s
ef   ciently with noise-contrastive estimation. in ad-
vances in neural information processing systems,
pages 2265   2273.

[morin and bengio2005] frederic morin and yoshua
bengio. 2005. hierarchical probabilistic neural net-
work language model. in aistats, volume 5, pages
246   252. citeseer.

[pascanu et al.2012] razvan pascanu, tomas mikolov,
and yoshua bengio. 2012. on the dif   culty of
training recurrent neural networks. arxiv preprint
arxiv:1211.5063.

[pascanu et al.2013] razvan pascanu, caglar gulcehre,
kyunghyun cho, and yoshua bengio. 2013. how
to construct deep recurrent neural networks. arxiv
preprint arxiv:1312.6026.

[rush et al.2015] alexander m. rush, sumit chopra,
and jason weston. 2015. a neural attention model
corr,
for abstractive sentence summarization.
abs/1509.00685.

[schuster and paliwal1997] mike

kuldip k paliwal.
rent neural networks.
transactions on, 45(11):2673   2681.

schuster

and
1997. bidirectional recur-
signal processing, ieee

[sennrich et al.2015] rico sennrich, barry haddow,
and alexandra birch. 2015. neural machine trans-
arxiv
lation of rare words with subword units.
preprint arxiv:1508.07909.

[theano development team2016] theano

develop-
ment team. 2016. theano: a python framework
for fast computation of mathematical expressions.
arxiv e-prints, abs/1605.02688, may.

[tomasello et al.2007] michael tomasello, malinda
carpenter, and ulf liszkowski. 2007. a new look at
infant pointing. child development, 78(3):705   722.

[vinyals et al.2015] oriol vinyals, meire fortunato,
and navdeep jaitly. 2015. id193. in ad-
vances in neural information processing systems,
pages 2674   2682.

[zeiler2012] matthew d zeiler.

2012. adadelta:
an adaptive learning rate method. arxiv preprint
arxiv:1212.5701.

acknowledgments
we would like to thank the developers of theano 5,
for developing such a powerful tool for scienti   c
computing (theano development team, 2016).
we acknowledge the support of the following or-
ganizations for research funding and computing

5http://deeplearning.net/software/

theano/

