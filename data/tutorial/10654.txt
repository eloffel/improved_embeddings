7
1
0
2

 
r
p
a
4
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
4
0
6
5
0

.

8
0
6
1
:
v
i
x
r
a

modeling human reading with neural attention

michael hahn

frank keller

institute for language, cognition and computation

school of informatics, university of edinburgh
10 crichton street, edinburgh eh8 9ab, uk

s1582047@inf.ed.ac.uk

keller@inf.ed.ac.uk

abstract

when humans read text,
they    xate some
words and skip others. however, there have
been few attempts to explain skipping behav-
ior with computational models, as most ex-
isting work has focused on predicting read-
ing times (e.g., using surprisal). in this pa-
per, we propose a novel approach that models
both skipping and reading, using an unsuper-
vised architecture that combines a neural at-
tention with autoencoding, trained on raw text
using id23. our model ex-
plains human reading behavior as a tradeoff
between precision of language understanding
(encoding the input accurately) and economy
of attention (   xating as few words as possi-
ble). we evaluate the model on the dundee
eye-tracking corpus, showing that it accurately
predicts skipping behavior and reading times,
is competitive with surprisal, and captures
known qualitative features of human reading.

1 introduction

humans read text by making a sequence of    xations
and saccades. during a    xation, the eyes land on a
word and remain fairly static for 200   250 ms. sac-
cades are the rapid jumps that occur between    xa-
tions, typically lasting 20   40 ms and spanning 7   
9 characters (rayner, 1998). readers, however, do
not simply    xate one word after another; some sac-
cades go in reverse direction, and some words are
   xated more than once or skipped altogether.

a range of computational models have been
developed to account for human eye-movements
in reading (rayner and reichle, 2010), including

models of saccade generation in cognitive psy-
chology, such as ez-reader (reichle et al., 1998,
2003, 2009), swift (engbert et al., 2002, 2005),
or the bayesian model of bicknell and levy (2010).
more recent approaches use machine learning mod-
els trained on eye-tracking data to predict human
reading patterns (nilsson and nivre, 2009, 2010;
hara et al., 2012; matthies and s  gaard, 2013).
both types of models involve theoretical assump-
tions about human eye-movements, or at least re-
quire the selection of relevant eye-movement fea-
tures. model parameters have to be estimated in a
supervised way from eye-tracking corpora.

that do not

unsupervised approaches,

involve
training the model on eye-tracking data, have also
been proposed. a key example is surprisal, which
measures the predictability of a word in context, de-
   ned as the negative logarithm of the conditional
id203 of the current word given the preced-
ing words (hale, 2001; levy, 2008). surprisal is
computed by a language model, which can take the
form of a probabilistic grammar, an id165 model,
or a recurrent neural network. while surprisal
has been shown to correlate with word-by-word
reading times (mcdonald and shillcock, 2003a,b;
demberg and keller, 2008; frank and bod, 2011;
smith and levy, 2013), it cannot explain other as-
pects of human reading, such as reverse saccades,
re-   xations, or skipping. skipping is a particularly
intriguing phenomenon: about 40% of all words are
skipped (in the dundee corpus, see below), without
apparent detriment to text understanding.

in this paper, we propose a novel model architec-
ture that is able to explain which words are skipped

and which ones are    xated, while also predicting
reading times for    xated words. our approach is
completely unsupervised and requires only unla-
beled text for training.

compared to language as a whole, reading is a
recent innovation in evolutionary terms, and peo-
ple learning to read do not have access to compe-
tent readers    eye-movement patterns as training data.
this suggests that human eye-movement patterns
emerge from general principles of language pro-
cessing that are independent of reading. our start-
ing point is the tradeoff hypothesis: human read-
ing optimizes a tradeoff between precision of lan-
guage understanding (encoding the input accurately)
and economy of attention (   xating as few words as
possible). based on the tradeoff hypothesis, we ex-
pect that humans only    xate words to the extent nec-
essary for language understanding, while skipping
words whose contribution to the overall meaning can
be inferred from context.

in order to test these assumptions, this paper in-

vestigates the following questions:

1. can the tradeoff hypothesis be implemented
in an unsupervised model that predicts skipping
and reading times in quantitative terms? in par-
ticular, can we compute surprisal based only on
the words that are actually    xated?

2. can the tradeoff hypothesis explain known
qualitative features of human    xation patterns?
these include dependence on word frequency,
word length, predictability in context, a con-
trast between content and function words, and
the statistical dependence of the current    xa-
tion on previous    xations.

to investigate these questions, we develop a generic
architecture that combines neural language model-
ing with recent ideas on integrating recurrent neural
networks with mechanisms of attention, which have
shown promise both in nlp and in id161.
we train our model end-to-end on a large text cor-
pus to optimize a tradeoff between minimizing input
reconstruction error and minimizing the number of
words    xated. we evaluate the model   s reading be-
havior against a corpus of human eye-tracking data.
apart from the unlabeled training corpus and the
generic architecture, no further assumptions about

language structure are made     in particular, no lex-
icon or grammar or otherwise labeled data is re-
quired.

our unsupervised model is able to predict human
skips and    xations with an accuracy of 63.7%. this
compares to a baseline of 52.6% and a supervised
accuracy of 69.9%. for    xated words, the model
signi   cantly predicts human reading times in a lin-
ear mixed effects analysis. the performance of our
model is comparable to surprisal, even though it only
   xates 60.4% of all input words. furthermore, we
show that known qualitative features of human    x-
ation sequences emerge in our model without addi-
tional assumptions.

2 related work

a range of attention-based neural network archi-
tectures have recently been proposed in the litera-
ture, showing promise in both nlp and computer vi-
sion (e.g., mnih et al., 2014; bahdanau et al., 2015).
such architectures incorporate a mechanism that al-
lows the network to dynamically focus on a re-
stricted part of the input. attention is also a central
concept in cognitive science, where it denotes the
focus of cognitive processing. in both language pro-
cessing and visual processing, attention is known to
be limited to a restricted area of the visual    eld, and
shifts rapidly through eye-movements (henderson,
2003).

attention-based neural architectures either em-
ploy soft attention or hard attention. soft attention
distributes real-valued attention values over the in-
put, making end-to-end training with gradient de-
scent possible. hard attention mechanisms make
discrete choices about which parts of the input to
focus on, and can be trained with reinforcement
learning (mnih et al., 2014). in nlp, soft attention
can mitigate the dif   culty of compressing long se-
quences into    xed-dimensional vectors, with ap-
plications in machine translation (bahdanau et al.,
2015) and id53 (hermann et al.,
2015). in id161, both types of atten-
tion can be used for selecting regions in an image
(ba et al., 2015; xu et al., 2015).

3 the neat reading model

autoencoding.

the point of departure for our model is the trade-
off hypothesis (see section 1): reading optimizes a
tradeoff between precision of language understand-
ing and economy of attention. we make this idea ex-
plicit by proposing neat (neural attention trade-
off), a model that reads text and attempts to re-
construct it afterwards. while reading, the network
chooses which words to process and which words
to skip. the tradeoff hypothesis is formalized us-
ing a training objective that combines accuracy of
reconstruction with economy of attention, encourag-
ing the network to only look at words to the extent
that is necessary for reconstructing the sentence.

3.1 architecture

we use a neural sequence-to-sequence architecture
(sutskever et al., 2014) with a hard attention mech-
anism. we illustrate the model in figure 1, op-
erating on a three-word sequence www. the most
basic components are the reader, labeled r, and
the decoder. both of them are recurrent neural
networks with long short-term memory (lstm,
hochreiter and schmidhuber, 1997) units. the re-
current reader network is expanded into time steps
r0, . . . , r3 in the    gure. it goes over the input se-
quence, reading one word wi at a time, and con-
verts the word sequence into a sequence of vec-
tors h0, . . . , h3. each vector hi acts as a    xed-
dimensionality encoding of
the word sequence
w1, . . . , wi that has been read so far. the last vec-
tor h3 (more generally hn for sequence length n),
which encodes the entire input sequence, is then fed
into the input layer of the decoder network, which
attempts to reconstruct the input sequence www. it is
also realized as a recurrent neural network, collapsed
into a single box in the    gure. it models a id203
distribution over word sequences, outputting a prob-
ability distribution pdecoder(wi|www1,...,i   1, hn ) over the
vocabulary in the i-th step, as is common in neu-
ral id38 (mikolov et al., 2010). as the
decoder has access to the vector representation cre-
ated by the reader network, it ideally is able to as-
sign the highest id203 to the word sequence www
that was actually read. up to this point, the model
is a standard sequence-to-sequence architecture re-
constructing the input sequence, that is, performing

as a basic model of human processing, neat
contains two further components. first, experimen-
tal evidence shows that during reading, humans con-
stantly make predictions about the upcoming input
(e.g., van gompel and pickering, 2007). as a model
of this behavior, the reader network at each time step
outputs a id203 distribution pr over the lex-
icon. this distribution describes which words are
likely to come next (i.e., the reader network per-
forms id38). unlike the modeling per-
formed by the decoder, pr, via its recurrent connec-
tions, has access to the previous context only.

second, we model skipping by stipulating that
only some of the input words wi are fed into the
reader network r, while r receives a special vec-
tor representation, containing no information about
the input word, in other cases. these are the words
that are skipped. in neat, at each time step dur-
ing reading, the attention module a decides whether
the next word is shown to the reader network or not.
when humans skip a word, they are able to identify
it using parafoveal preview (rayner, 2009). thus,
we can assume that the choice of which words to
skip takes into account not only the prior context
but also a preview of the word itself. we therefore
allow the attention module to take the input word
into account when making its decision. in addition,
the attention module has access to the previous state
hi   1 of the reader network, which summarizes what
has been read so far. to allow for interaction be-
tween skipping and prediction, we also give the at-
tention module access to the id203 of the in-
put word according to the prediction pr made at the
last time step. if we write the decision made by a
as   i     {0, 1}, where   i = 1 means that word wi is
shown to the reader and 0 means that it is not, we
can write the id203 of showing word wi as:

p(  i = 1|      1...i   1, www)
= pa(wi, hi   1, pr(wi|www1...i   1,       1...i   1))

(1)

we implement a as a feed-forward network, fol-
lowed by taking a binary sample   i.

we obtain the surprisal of an input word by taking
the negative logarithm of the id155

of this word given the context words that precede it:

surp(wi|www1...i   1) =     log pr(wi|www1...i   1,       1...i   1)

neural architecture, the objective (4), and the corpus
from which the sequences www are drawn.

(2)

3.3 training

as a consequence of skipping, not all input words
are accessible to the reader network. therefore, the
id203 and surprisal estimates it computes cru-
cially only take into account the words that have ac-
tually been    xated. we will refer to this quantity as
the restricted surprisal, as opposed to full surprisal,
which is computed based on all prior context words.
the key quantities for predicting human reading
are the    xation probabilities in equation (1), which
model    xations and skips, and restricted surprisal in
equation (2), which models the reading times of the
words that are    xated.

3.2 model objective

given network parameters    and a sequence www
of words,
the network stochastically chooses a
sequence        according to (1) and incurs a loss
l(      |www,   ) for id38 and reconstruc-
tion:
l(      |www,   ) =       

log pr(wi|www1,...,i   1,       1,...,i   1;   )

i

log pdecoder(wi|www1,...,i   1; hn ;   )

      

i

(3)

where pr(wi, . . . ) denotes the output of the reader af-
ter reading wi   1, and pdecoder(wi| . . . ; hn ) is the out-
put of the decoder at time i     1, with hn being the
vector representation created by the reader network
for the entire input sequence.

to implement the tradeoff hypothesis, we train
neat to solve id38 and reconstruc-
tion with minimal attention, i.e., the network mini-
mizes the expected loss:

q(  ) := ewww,       [l(      |www,   ) +       k      k   1]

(4)

where word sequences www are drawn from a corpus,
and        is distributed according to p(      |www,   ) as de-
   ned in (1). in (4), k      k   1 is the number of words
shown to the reader, and    > 0 is a hyperparameter.
the term       k      k   1 encourages neat to attend to as
few words as possible.

note that we make no assumption about linguis-
tic structure     the only ingredients of neat are the

we follow previous approaches to hard attention in
using a combination of id119 and rein-
forcement learning, and separate the training of the
recurrent networks from the training of a. to train
the reader r and the decoder, we temporarily re-
move the attention network a, set            binom(n, p)
(n sequence length, p a hyperparameter), and mini-
mize e[l(www|  ,       )] using stochastic id119,
sampling a sequence        for each input sequence. in
effect, neat is trained to perform reconstruction
and id38 when there is noise in the
input. after r and the decoder have been trained,
we    x their parameters and train a using the re-
inforce rule (williams, 1992), which performs
stochastic id119 using the estimate

1
|b|    

www   b;      

(l(      |www,   ) +       k      k   1)      a (log p(      |www,   ))

(5)
for the gradient      aq. here, b is a minibatch,        is
sampled from p(      |www,   ), and   a        is the set of
parameters of a. for reducing the variance of this
estimator, we subtract in the i-th step an estimate of
the expected loss:

u (www,       1...i   1) := e      i...n [l(      1...i   1      i...n|www,   )
+       k      k   1]

(6)

we compute the expected loss using an lstm
that we train simultaneously with a to predict l +
  k      k   1 based on www and       1...i   1. to make learn-
ing more stable, we add an id178 term encourag-
ing the distribution to be smooth, following xu et al.
(2015). the parameter updates to a are thus:

   
www,      

   

i

(l(      |www,   ) +   k      k   1     u (www,       1...i   1))

        a (log p(  i|      1...i   1, www,   ))

(7)

           a    

www,      

h[p(  i|      1,...,i   1, www,   )]!

   

i

where    is a hyperparameter, and h the id178.

w1

pr1

h0

a

r1

w2

pr2

h1

a

r2

r0

w3

pr3

a

h2

r3

decoder

figure 1: the architecture of the proposed model, reading a three-word input sequence w1, w2, w3. r is the reader network and pr
the id203 distribution it computes in each time step. a is the attention network. at each time step, the input, its id203

according to pr, and the previous state hi   1 of r are fed into a, which then decides whether the word is read or skipped.

4 methods

our aim is to evaluate how well neat predicts hu-
man    xation behavior and reading times. further-
more, we want show that known qualitative prop-
erties emerge from the tradeoff hypothesis, even
though no prior knowledge about useful features is
hard-wired in neat.

4.1 training setup

for both the reader and the decoder networks, we
choose a one-layer lstm network with 1,000 mem-
ory cells. the attention network is a one-layer feed-
forward network. for the loss estimator u , we use
a bidirectional lstm with 20 memory cells. input
data is split into sequences of 50 tokens, which are
used as the input sequences for neat, disregard-
ing sentence boundaries. id27s have
100 dimensions, are shared between the reader and
the attention network, and are only trained during
the training of the reader. the vocabulary consists
of the 10,000 most frequent words from the train-
ing corpus. we trained neat on the training set of
the daily mail section of the corpus described by
hermann et al. (2015), which consists of 195,462
articles from the daily mail newspaper, contain-
ing approximately 200 million tokens. the recur-
rent networks and the attention network were each
trained for one epoch. for initialization, weights are
drawn from the uniform distribution. we set    = 5.0,
   = 5.0, and used a constant learning rate of 0.01 for
a.

4.2 corpus

for evaluation, we use the english section of the
dundee corpus (kennedy and pynte, 2005), which
consists of 20 texts from the independent, anno-
tated with eye-movement data from ten english na-

tive speakers. each native speakers read all 20 texts
and answered a comprehension question after each
text. we split the dundee corpus into a develop-
ment and a test set, with texts 1   3 constituting the
development set. the development set consists of
78,300 tokens, and the test set of 281,911 tokens.
for evaluation, we removed the datapoints removed
by demberg and keller (2008), mainly consisting of
words at the beginning or end of lines, outliers, and
cases of track loss. furthermore, we removed data-
points where the word was outside of the vocabulary
of the model, and those datapoints mapped to posi-
tions 1   3 or 48   50 of a sequence when splitting the
data. after preprocessing, 62.9% of the development
tokens and 64.7% of the test tokens remained. to ob-
tain the number of    xations on a token and reading
times, we used the eye-tracking measures computed
by demberg and keller (2008). the overall    xation
rate was 62.1% on the development set, and 61.3%
on the test set.

the development set was used to run preliminary
versions of the human evaluation studies, and to de-
termine the human skipping rate (see section 5). all
the results reported in this paper were computed on
the test set, which remained unseen until the model
was    nal.

5 results and discussion

throughout this section, we consider the following
baselines for the attention network: random atten-
tion is de   ned by            binom(n, p), with p = 0.62,
the human    xation rate in the development set. for
full attention, we take        = 1, i.e., all words are
   xated. we also derive    xation predictions from
full surprisal, word frequency, and word length by
choosing a threshold such that the resulting    xation
rate matches the human    xation rate on the develop-

ment set.

5.1 quantitative properties

by averaging over all possible    xation sequences,
neat de   nes for each word in a sequence a prob-
ability that it will be    xated. this id203 is
not ef   ciently computable, so we approximate it by
sampling a sequence        and taking the probabilities
p(  i = 1|  1...i   1, www) for i = 1, . . . , 50. these sim-
ulated    xation probabilities can be interpreted as
de   ning a distribution of attention over the input
sequence. figure 2 shows heatmaps of the simu-
lated and human    xation probabilities, respectively,
for the beginning of a text from the dundee cor-
pus. while some differences between simulated and
human    xation probabilities can be noticed, there
are similarities in the general qualitative features of
the two heatmaps. in particular, function words and
short words are less likely to be    xated than content
words and longer words in both the simulated and
the human data.

reconstruction and id38 we
   rst evaluate neat intrinsically by measuring how
successful
the network is at predicting the next
word and reconstructing the input while minimiz-
ing the number of    xations. we compare perplex-
ity on reconstruction and id38 for
           p(      |www,   ). in addition to the baselines, we run
neat on the    xations generated by the human read-
ers of the dundee corpus, i.e., we use the human    x-
ation sequence as        instead of the    xation sequence
generated by a to compute perplexity. this will tell
us to what extent the human behavior minimizes the
neat objective (4).

the results are given in table 1. in all settings, the
   xation rates are similar (60.4% to 62.1%) which
makes the perplexity    gures directly comparable.
while neat has a higher perplexity on both tasks
compared to full attention, it considerably outper-
forms random attention. it also outperforms the
word length, word frequency, and full surprisal base-
lines. the perplexity on human    xation sequences is
similar to that achieved using word frequency. based
on these results, we conclude that reinforce suc-
cessfully optimizes the objective (4).

likelihood of fixation data human reading be-
havior is stochastic in the sense that different runs of

eye-tracking experiments such as the ones recorded
in the dundee corpus yield different eye-movement
sequences. neat is also stochastic, in the sense that,
given a word sequence w, it de   nes a id203 dis-
tribution over    xation sequences       . ideally, this dis-
tribution should be close to the actual distribution of
   xation sequences produced by humans reading the
sequence, as measured by perplexity.

we    nd that the perplexity of the    xation se-
quences produced by the ten readers in the dundee
corpus under neat is 1.84. a perplexity of 2.0
corresponds to the random baseline binom(n, 0.5),
and a perplexity of 1.96 to random attention
binom(n, 0.62). as a lower bound on what can
achieved with models disregarding the context, us-
ing the human    xation rates for each word as proba-
bilities, we obtain a perplexity of 1.68.

accuracy of fixation sequences previous work
on supervised models
for modeling    xations
(nilsson and nivre, 2009; matthies and s  gaard,
2013) has been evaluated by measuring the overlap
of the    xation sequences produced by the models
with those in the dundee corpus. for neat, this
method of evaluation is problematic as differences
between model predictions and human data may be
due to differences in the rate of skipping, and due
to the inherently stochastic nature of    xations. we
therefore derive model predictions by rescaling the
simulated    xation probabilities so that their aver-
age equals the    xation rate in the development set,
and then greedily take the maximum-likelihood se-
quence. that is, we predict a    xation if the rescaled
id203 is greater than 0.5, and a skip otherwise.
as in previous work, we report the accuracy of    xa-
tions and skips, and also separate f1 scores for    x-
ations and skips. as lower and upper bounds, we
use the random baseline            binom(n, 0.62) and the
agreement of the ten human readers, respectively.
the results are shown in table 2. neat clearly
outperforms the random baseline and shows results
close to full surprisal (where we apply the same
rescaling and thresholding as for neat). this is re-
markable given that neat has access to only 60.4%
of the words in the corpus in order to predict skip-
ping, while full surprisal has access to all the words.
word frequency and word length perform well, al-
most reaching the performance of supervised mod-

the

decision

of

the human

fertility

and

embryology authority

(hfea)

to

allow a

couple

to

select

genetically

their

next

baby was

bound

to

raise

concerns

that

advances

in

biotechnology

are

racing

ahead

of

our

ability

to

control

the

consequences.

the

couple

at

the

centre

of

this

case

have

a

son who

suffers

from a

potentially

fatal

disorder

and whose

best

hope

is

a marrow transplant

from a

sibling,

so

the

stakes

of

this

decision

are

particularly

high.

the hfea   s

critics

believe

that

it

sanctions

   designer

babies   

and

does

not

show respect

for

the

sanctity

of

individual

life. certainly,

the

authority   s

backing

for

shahana

and raj hashmi   s

plea

for

genetic

screening

raises

fundamental

questions

the

decision

of

the human

fertility

and

embryology authority

(hfea)

to

allow a

couple

to

select

genetically

their

next

baby was

bound

to

raise

concerns

that

advances

in

biotechnology

are

racing

ahead

of

our

ability

to

control

the

consequences.

the

couple

at

the

centre

of

this

case

have

a

son who

suffers

from a

potentially

fatal

disorder

and whose

best

hope

is

a marrow transplant

from a

sibling,

so

the

stakes

of

this

decision

are

particularly

high.

the hfea   s

critics

believe

that

it

sanctions

   designer

babies   

and

does

not

show respect

for

the

sanctity

of

individual

life. certainly,

the

authority   s

backing

for

shahana

and raj hashmi   s

plea

for

genetic

screening

raises

fundamental

questions

figure 2: top: heatmap showing human    xation probabilities, as estimated from the ten readers in the dundee corpus. in cases of

track loss, we replaced the missing value with the corresponding reader   s overall    xation rate. bottom: heatmap showing    xation

probabilities simulated by neat. color gradient ranges from blue (low id203) to red (high id203); words without color

are at the beginning or end of a sequence, or out of vocabulary.

id38 180
4.5
reconstruction
fixation rate
60.4% 62.1%

neat rand. att. word len. word freq. full surp. human
218/170
39/31
61.3%/72.0% 100%

230
40
62.1%

219
39
62.1%

211
34
62.1%

full att.
107
1.6

333
56

table 1: performance on id38 and reconstruction as measured by perplexity. random attention is an upper bound

on perplexity, while full attention is a lower bound. for the human baseline, we give two    gures, which differ in the treatment of

missing data. the    rst    gure is obtained when replacing missing values with a random variable        binom(n, 0.61); the second

results from replacing missing values with 1.

els. this shows that the bulk of skipping behavior
is already explained by word frequency and word
length effects. note, however, that neat is com-
pletely unsupervised, and does not know that it has
to pay attention to word frequency; this is something
the model is able to infer.

restricted surprisal and reading times to
evaluate the predictions neat makes for reading
times, we use linear mixed-effects models contain-
ing restricted surprisal derived from neat for the
dundee test set. the mixed models also include a
set of standard baseline predictors, viz., word length,
log word frequency, log frequency of the previous
word, launch distance, landing position, and the po-
sition of the word in the sentence. we treat partici-
pants and items as random factors. as the dependent
variable, we take    rst pass duration, which is the
sum of the durations of all    xations from    rst enter-
ing the word to    rst leaving it. we compare against
full surprisal as an upper bound and against ran-
dom surprisal as a lower bound. random surprisal
is surprisal computed by a model with random at-

tention; this allows us to assess how much surprisal
degrades when only 60.4% of all words are    xated,
but no information is available as to which words
should be    xated. the results in table 3 show that
restricted surprisal as computed by neat, full sur-
prisal, and random surprisal are all signi   cant pre-
dictors of reading time.

in order to compare the three surprisal estimates,
we therefore need a measure of effect size. for this,
we compare the model    t of the three mixed ef-
fects models using deviance, which is de   ned as
the difference between the log likelihood of the
model under consideration minus the log likelihood
of the baseline model, multiplied by    2. higher de-
viance indicates greater improvement in model    t
over the baseline model. we    nd that the mixed
model that includes restricted surprisal achieves a
deviance of 867, compared to the model contain-
ing only the baseline features. with full surprisal,
we obtain a deviance of 980. on the other hand, the
model including random surprisal achieves a lower
deviance of 832.

this shows that restricted surprisal as computed

neat

acc
63.7

f1   x
70.4

f1skip
53.0

supervised models

69.5
nilsson and nivre (2009)
matthies and s  gaard (2013) 69.9

75.2
72.3

62.6
66.1

human performance and baselines

random baseline
full surprisal
word frequency
word length
human

52.6
64.1
67.9
68.4
69.5

62.1
70.7
74.0
77.1
76.6

37.9
53.6
58.3
49.0
53.6

table 2: evaluation of    xation sequence predictions against hu-

(intercept)
word length
previous word freq.
prev. word fixated
launch distance
obj. landing pos.
word pos. in sent.
log word freq.
resid. random surprisal
resid. restr. surprisal
resid. full surprisal

  
247.43
12.92
   5.28
   24.67
-0.01
   8.07
   0.10
   1.59
2.69
2.75
2.99

t

se
34.68*
7.14
0.21
60.62*
0.28    18.34*
0.81    30.55*
0.01    0.37
0.20    41.25*
0.03    2.98*
0.21    7.73*
29.27*
0.10
23.66*
0.12
0.12
25.23*

man data. for the human baseline, we predicted the n-th reader   s

table 3: linear mixed effects models for    rst pass duration.

   xations by taking the    xations of the n + 1-th reader (with

the    rst part of the table shows the coef   cients, standard er-

missing values replaced by reader average), averaging the re-

rors, and t values for the predictors in the baseline model. the

sulting scores over the ten readers.

second part of the table gives the corresponding values for ran-

by neat not only signi   cantly predicts reading
times, it also provides an improvement in model    t
compared to the baseline predictors. such an im-
provement is also observed with random surprisal,
but restricted surprisal achieves a greater improve-
ment in model    t. full surprisal achieves an even
greater improvement, but this is not unexpected, as
full surprisal has access to all words, unlike neat or
random surprisal, which only have access to 60.4%
of the words.

5.2 qualitative properties

we now examine the second key question we de-
   ned in section 1, investigating the qualitative fea-
tures of the simulated    xation sequences. we will
focus on comparing the predictions of neat with
that of word frequency, which performs comparably
at the task of predicting    xation sequences (see sec-
tion 5.1). we show neat nevertheless makes rele-
vant predictions that go beyond frequency.

fixations of successive words while predictors
derived from word frequency treat
the decision
whether to    xate or skip words as independent, hu-
mans are more likely to    xate a word when the pre-
vious word was skipped (rayner, 1998). this effect
is also seen in neat. more precisely, both in the
human data and in the simulated    xation data, the
conditional    xation id203 p(  i = 1|  i   1 = 1)
is lower than the marginal id203 p(  i = 1).

dom surprisal, restricted surprisal computed by neat, and full

surprisal, residualized against the baseline predictors, in three

models obtained by adding these predictors.

the ratio of these probabilities is 0.85 in the human
data, and 0.81 in neat. the threshold predictor de-
rived from word frequency also shows this effect (as
the frequencies of successive words are not indepen-
dent), but it is weaker (ratio 0.91).

to further test the context dependence of neat   s
   xation behavior, we ran a mixed model predict-
ing the    xation probabilities simulated by neat,
with items as random factor and the log frequency
of word i as predictor. adding   i   1 as a predic-
tor results in a signi   cant improvement in model
   t (deviance = 4,798, t = 71.3). this shows that
neat captures the context dependence of    xation
sequences to an extend that goes beyond word fre-
quency alone.

parts of speech part of speech categories are
known to be a predictor of    xation probabilities,
with content words being more likely to be    xated
than function words (carpenter and just, 1983). in
table 4, we give the simulated    xation probabilities
and the human    xation probabilities estimated from
the dundee corpus for the tags of the universal pos
tagset (petrov et al., 2012), using the pos annotation
of barrett et al. (2015). we again compare with the
probabilities of a threshold predictor derived from

adj
adp
adv
conj
det
noun
num
pron
prt
verb
x
spearman   s   
pearson   s r
mse

human
78.9 (2)
46.1 (8)
70.4 (3)
36.7 (11)
45.2 (9)
80.3 (1)
63.3 (6)
49.2 (7)
37.4 (10)
66.7 (5)
68.6 (4)

neat
72.8 (1)
53.8 (8)
67.2 (4)
50.7 (9)
44.8 (11)
69.8 (2)
71.5 (3)
57.0 (7)
46.7 (10)
64.7 (5)
67.8 (3)
0.85
0.92
57

word freq.
98.4 (3)
21.6 (9)
96.4 (4)
14.6 (10)
22.9 (8)
98.7 (2)
99.5 (1)
42.6 (7)
13.9 (11)
74.4 (5)
69.0 (6)
0.84
0.94
450

table 4: actual and simulated    xation probabilities (in %) by

pos tag, with the ranks given in brackets, and correlations and

mean squared error relative to human data.

word frequency.1 neat captures the differences be-
tween pos categories well, as evidenced by the high
correlation coef   cients. the content word categories
adj, adv, noun, verb and x consistently show
higher probabilities than the function word cate-
gories. while the correlation coef   cients for word
frequency are very similar, the numerical values of
the simulated probabilities are closer to the human
ones than those derived from word frequency, which
tend towards more extreme values. this difference
can be seen clearly if we compare the mean squared
error, rather than the correlation, with the human    x-
ation probabilities (last row of table 4).

correlations with known predictors
in the lit-
erature, it has been observed that skipping correlates
with predictability (surprisal), word frequency, and
word length (rayner, 1998, p. 387). these correla-
tions are also observed in the human skipping data
derived from dundee, as shown in table 5. (hu-
man    xation probabilities were obtained by averag-
ing over the ten readers in dundee.)

comparing the known predictors of skipping with
neat   s simulated    xation probabilities, similar cor-
relations as in the human data are observed. we ob-
serve that the correlations with surprisal are stronger

1we omit the tag    .    for punctuation, as punctuation charac-

ters are not treated as separate tokens in dundee.

restricted surprisal
full surprisal
log word freq.
word length

human
0.465
0.512

neat
0.762
0.720
   0.608    0.760
0.521

0.663

table 5: correlations between human and neat    xation prob-

abilities and known predictors

in neat, considering both restricted surprisal and
full surprisal as measures of predictability.

6 conclusions

we investigated the hypothesis that human read-
ing strategies optimize a tradeoff between precision
of language understanding and economy of atten-
tion. we made this idea explicit in neat, a neural
reading architecture with hard attention that can be
trained end-to-end to optimize this tradeoff. exper-
iments on the dundee corpus show that neat pro-
vides accurate predictions for human skipping be-
havior. it also predicts reading times, even though it
only has access to 60.4% of the words in the cor-
pus in order to estimate surprisal. finally, we found
that known qualitative properties of skipping emerge
in our model, even though they were not explicitly
included in the architecture, such as context depen-
dence of    xations, differential skipping rates across
parts of speech, and correlations with other known
predictors of human reading behavior.

references

ba, jimmy, ruslan r. salakhutdinov, roger b.
grosse, and brendan j. frey. 2015. learning
wake-sleep recurrent id12.
in ad-
vances in neural information processing sys-
tems. pages 2575   2583.

bahdanau, dzmitry, kyunghyun cho, and yoshua
bengio. 2015. id4 by
jointly learning to align and translate. in proceed-
ings of the international conference on learning
representations.

barrett, maria,   zeljko agi  c, and anders s  gaard.
2015. the dundee treebank.
in proceedings
of the 14th international workshop on treebanks
and linguistic theories. pages 242   248.

bicknell, klinton and roger levy. 2010. a ratio-
nal model of eye movement control in reading.
in proceedings of the 48th annual meeting of the
association for computational linguistics. pages
1168   1178.

carpenter, p. a. and m. a. just. 1983. what your
eyes do while your mind is reading. in k. rayner,
editor, eye movements in reading, academic
press, new york, pages 275   307.

demberg, vera and frank keller. 2008.

data
from eye-tracking corpora as evidence for theo-
ries of syntactic processing complexity. cognition
109(2):193   210.

long short-term memory. neural computation
9(8):1735   1780.

kennedy, alan and jo  el pynte. 2005. parafoveal-on-
foveal effects in normal reading. vision research
45(2):153   168.

levy, roger. 2008.

expectation-based syntactic

comprehension. cognition 106(3):1126   1177.

matthies, franz and anders s  gaard. 2013. with
blinkers on: robust prediction of eye movements
across readers. in proceedings of the conference
on empirical methods in natural language pro-
cessing. pages 803   807.

engbert, ralf, andr  e longtin, and reinhold kliegl.
2002. a dynamical model of saccade generation
in reading based on spatially distributed lexical
processing. vision research 42(5):621   636.

mcdonald, scott a. and richard c. shillcock.
2003a. eye movements reveal the on-line compu-
tation of lexical probabilities during reading. psy-
chological science 14(6):648   652.

engbert, ralf, antje nuthmann, eike m. richter,
and reinhold kliegl. 2005. swift: a dynami-
cal model of saccade generation during reading.
psychological review 112(4):777   813.

mcdonald, scott a. and richard c. shillcock.
2003b. low-level predictive id136 in reading:
the in   uence of transitional probabilities on eye
movements. vision research 43(16):1735   1751.

frank, s.l. and r. bod. 2011.

insensitivity of the
human sentence-processing system to hierarchi-
cal structure. psychological science 22:829   834.

hale, john. 2001. a probabilistic earley parser as a
psycholinguistic model. in proceedings of con-
ference of the north american chapter of the
association for computational linguistics. vol-
ume 2, pages 159   166.

hara, tadayoshi, daichi mochihashi yoshinobu
kano, and akiko aizawa. 2012. predicting word
   xations in text with a crf model for capturing
general reading strategies among readers. in pro-
ceedings of the 1st workshop on eye-tracking and
natural language processing. pages 55   70.

henderson, john. 2003. human gaze control in real-
world scene perception. trends in cognitive sci-
ences 7:498   504.

hermann, karl moritz, tom  a  s ko  cisk`y, ed-
ward grefenstette, lasse espeholt, will kay,
mustafa suleyman, and phil blunsom. 2015.
teaching machines to read and comprehend.
arxiv:1506.03340.

hochreiter, sepp and j  urgen schmidhuber. 1997.

mikolov, tom  a  s, martin kara     at, luk  a  s burget, jan
  cernock  y, and sanjeev khudanpur. 2010. recur-
rent neural network based language model.
in
proceedings of interspeech. pages 1045   1048.

mnih, volodymyr, nicolas heess, alex graves, and
others. 2014. recurrent models of visual atten-
tion. in advances in neural information process-
ing systems. pages 2204   2212.

nilsson, mattias and joakim nivre. 2009. learn-
ing where to look: modeling eye movements in
reading.
in proceedings of the 13th conference
on computational natural language learning.
pages 93   101.

nilsson, mattias and joakim nivre. 2010. towards
a data-driven model of eye movement control in
reading. in proceedings of the workshop on cog-
nitive modeling and computational linguistics.
pages 63   71.

petrov, slav, dipanjan das, and ryan t. mcdon-
ald. 2012. a universal part-of-speech tagset. in
proceedings of the 8th international conference
on language resources and evaluation. pages
2089   2096.

rayner, k. 1998. eye movements in reading and in-
formation processing: 20 years of research. psy-
chological bulletin 124(3):372   422.

rayner, keith. 2009. eye movements in reading:
models and data. journal of eye movement re-
search 2(5):1   10.

rayner, keith and erik d. reichle. 2010. models of
the reading process. wiley interdisciplinary re-
views: cognitive science 1(6):787   799.

reichle, e. d., a. pollatsek, d. l. fisher, and
k. rayner. 1998. toward a model of eye move-
ment control in reading. psychological review
105(1):125   157.

reichle, e. d., t. warren, and k. mcconnell. 2009.
using ez reader to model the effects of higher
level language processing on eye movements dur-
ing reading. psychonomic bulletin & review
16(1):1   21.

reichle, erik d., keith rayner, and alexander pol-
the ez reader model of eye-
latsek. 2003.
movement control
in reading: comparisons to
other models. behavioral and brain sciences
26(04):445   476.

smith, nathaniel j. and roger levy. 2013. the ef-
fect of word predictability on reading time is log-
arithmic. cognition 128(3):302   319.

sutskever, ilya, oriol vinyals, and quoc vv le.
2014. sequence to sequence learning with neu-
ral networks. in advances in neural information
processing systems. pages 3104   3112.

van gompel, roger pg and martin j. pickering.
2007. syntactic parsing. in the oxford handbook
of psycholinguistics, oxford university press,
pages 289   307.

williams, ronald j. 1992.

simple statistical
gradient-following algorithms for connectionist
id23. machine learning 8(3-
4):229   256.

xu, kelvin,

jimmy ba, ryan kiros, aaron
courville, ruslan salakhutdinov, richard zemel,
and yoshua bengio. 2015. show, attend and tell:
neural image id134 with visual at-
tention. arxiv:1502.03044.

