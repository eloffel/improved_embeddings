diachronic id27s reveal statistical laws of

semantic change

william l. hamilton, jure leskovec, dan jurafsky

department of computer science, stanford university, stanford ca, 94305

wleif,jure,jurafsky@stanford.edu

8
1
0
2

 
t
c
o
5
2

 

 
 
]
l
c
.
s
c
[
 
 

6
v
6
9
0
9
0

.

5
0
6
1
:
v
i
x
r
a

abstract

understanding how words change their
meanings over time is key to models of
language and cultural evolution, but his-
torical data on meaning is scarce, mak-
ing theories hard to develop and test.
id27s show promise as a di-
achronic tool, but have not been carefully
evaluated. we develop a robust method-
ology for quantifying semantic change
by evaluating id27s (ppmi,
svd, id97) against known historical
changes. we then use this methodology
to reveal statistical laws of semantic evo-
lution. using six historical corpora span-
ning four languages and two centuries, we
propose two quantitative laws of seman-
tic change: (i) the law of conformity   the
rate of semantic change scales with an in-
verse power-law of word frequency; (ii)
the law of innovation   independent of fre-
quency, words that are more polysemous
have higher rates of semantic change.

introduction

1
shifts in word meaning exhibit systematic regu-
larities (br  eal, 1897; ullmann, 1962). the rate
of semantic change, for example,
is higher in
some words than others (blank, 1999)     com-
pare the stable semantic history of cat (from proto-
germanic kattuz,    cat   ) to the varied meanings of
english cast:    to mould   ,    a collection of actors   ,
   a hardened bandage   , etc. (all from old norse
kasta,    to throw   , simpson et al., 1989).

various hypotheses have been offered about
such regularities in semantic change, such as an in-
creasing subjecti   cation of meaning, or the gram-
maticalization of id136s (e.g., geeraerts, 1997;
blank, 1999; traugott and dasher, 2001).

but many core questions about semantic change
remain unanswered. one is the role of fre-
quency. frequency plays a key role in other lin-
guistic changes, associated sometimes with faster
change   sound changes like lenition occur in
more frequent words   and sometimes with slower
change   high frequency words are more resistant
to morphological id173 (bybee, 2007;
pagel et al., 2007; lieberman et al., 2007). what
is the role of word frequency in meaning change?
another unanswered question is the relationship
between semantic change and polysemy. words
gain senses over time as they semantically drift
(br  eal, 1897; wilkins, 1993; hopper and trau-
gott, 2003), and polysemous words1 occur in
more diverse contexts, affecting lexical access
speed (adelman et al., 2006) and rates of l2
learning (crossley et al., 2010). but we don   t
know whether the diverse contextual use of pol-
ysemous words makes them more or less likely
to undergo change (geeraerts, 1997; winter et
al., 2014; xu et al., 2015). furthermore, poly-
semy is strongly correlated with frequency   high
frequency words have more senses (zipf, 1945;
  ilgen and karaoglan, 2007)   so understanding
how polysemy relates to semantic change requires
controling for word frequency.

answering these questions requires new meth-
ods that can go beyond the case-studies of a few
words (often followed over widely different time-
periods) that are our most common diachronic
data (br  eal, 1897; ullmann, 1962; blank, 1999;
hopper and traugott, 2003; traugott and dasher,
2001). one promising avenue is the use of distri-
butional semantics, in which words are embedded
in vector spaces according to their co-occurrence
relationships (bullinaria and levy, 2007; turney
and pantel, 2010), and the embeddings of words
1we use    polysemy    here to refer to related senses as well

as rarer cases of accidental homonymy.

figure 1: two-dimensional visualization of semantic change in english using sgns vectors.2 a, the word gay shifted from
meaning    cheerful    or    frolicsome    to referring to homosexuality. b, in the early 20th century broadcast referred to    casting
out seeds   ; with the rise of television and radio its meaning shifted to    transmitting signals   . c, awful underwent a process of
pejoration, as it shifted from meaning    full of awe    to meaning    terrible or appalling    (simpson et al., 1989).

are then compared across time-periods. this new
direction has been effectively demonstrated in a
number of case-studies (sagi et al., 2011; wijaya
and yeniterzi, 2011; gulordava and baroni, 2011;
jatowt and duh, 2014) and used to perform large-
scale linguistic change-point detection (kulkarni
et al., 2014) as well as to test a few speci   c hy-
potheses, such as whether english synonyms tend
to change meaning in similar ways (xu and kemp,
2015). however, these works employ widely dif-
ferent embedding approaches and test their ap-
proaches only on english.

in this work, we develop a robust methodol-
ogy for quantifying semantic change using embed-
dings by comparing state-of-the-art approaches
(ppmi, svd, id97) on novel benchmarks.

we then apply this methodology in a large-scale
cross-linguistic analysis using 6 corpora spanning
200 years and 4 languages (english, german,
french, and chinese). based on this analysis, we
propose two statistical laws relating frequency and
polysemy to semantic change:
    the law of conformity: rates of semantic
change scale with a negative power of word
frequency.
    the law of innovation: after controlling for
frequency, polysemous words have signi   -
cantly higher rates of semantic change.

2 diachronic embedding methods

the following sections outline how we construct
diachronic (historical) id27s, by    rst
constructing embeddings in each time-period and
then aligning them over time, and the metrics that

2appendix b details the visualization method.

we use to quantify semantic change. all of the
learned embeddings and the code we used to ana-
lyze them are made publicly available.3

2.1 embedding algorithms
we use three methods to construct word em-
beddings within each time-period: ppmi, svd,
and sgns (i.e., id97).4 these distributional
methods represent each word wi by a vector wi
that captures information about its co-occurrence
statistics. these methods operationalize the    dis-
tributional hypothesis    that word semantics are im-
plicit in co-occurrence relationships (harris, 1954;
firth, 1957). the semantic similarity/distance be-
tween two words is approximated by the cosine
similarity/distance between their vectors (turney
and pantel, 2010).

2.1.1 ppmi
in the ppmi representations, the vector embedding
for word wi     v contains the positive point-wise
mutual information (ppmi) values between wi and
a large set of pre-speci   ed    context    words. the
word vectors correspond to the rows of the matrix
mppmi     r|v|  |vc| with entries given by

(cid:26)

(cid:18)   p(wi, cj)

(cid:19)

(cid:27)

      , 0

mppmi

log

  p(w)  p(cj)

i,j = max

,
(1)
where cj     vc is a context word and    > 0
is a negative prior, which provides a smooth-
ing bias (levy et al., 2015). the   p correspond
to the smoothed empirical probabilities of word

3http://nlp.stanford.edu/projects/histwords
4synchronic applications of these three methods are re-

viewed in detail in levy et al. (2015).

language description

name
engall english
english
engfic
coha
english
freall
french
gerall german
chiall
chinese

google books (all genres)
fiction from google books
genre-balanced sample
google books (all genres)
google books (all genres)
google books (all genres)

tokens
8.5    1011
7.5    1010
4.1    108
1.9    1011
4.3    1010
6.0    1010

years
1800-1999
1800-1999
1810-2009
1800-1999
1800-1999
1950-1999

table 1: six large historical datasets from various languages and sources are used.

(co-)occurrences within    xed-size sliding win-
dows of text. clipping the ppmi values above zero
ensures they remain    nite and has been shown to
dramatically improve results (bullinaria and levy,
2007; levy et al., 2015); intuitively, this clipping
ensures that the representations emphasize posi-
tive word-word correlations over negative ones.
2.1.2 svd
svd embeddings correspond to low-dimensional
approximations of the ppmi embeddings learned
via singular value decomposition (levy et al.,
2015). the vector embedding for word wi is given
by

wsvd

i = (u    )i ,

(2)
where mppmi = u  v(cid:62) is the truncated singular
value decomposition of mppmi and        [0, 1] is
an eigenvalue weighting parameter. setting    < 1
has been shown to dramatically improve embed-
ding qualities (turney and pantel, 2010; bulli-
naria and levy, 2012). this svd approach can
be viewed as a generalization of latent seman-
tic analysis (landauer and dumais, 1997), where
the term-document matrix is replaced with mppmi.
compared to ppmi, svd representations can be
more robust, as the id84 acts
as a form of id173.
2.1.3 skip-gram with negative sampling
sgns    id97    embeddings are optimized to
predict co-occurrence relationships using an ap-
proximate objective known as    skip-gram with
negative sampling    (mikolov et al., 2013).
in
sgns, each word wi is represented by two dense,
low-dimensional vectors: a word vector (wsgns
)
and context vector (id19ns
). these embeddings are
optimized via stochastic id119 so that

i

i

  p(ci|wi)     exp(wsgns

(3)
where p(ci|wi) is the empirical id203 of see-
ing context word ci within a    xed-length window

   id19ns

),

j

i

of text, given that this window contains wi. the
sgns optimization avoids computing the normal-
izing constant in (3) by randomly drawing    neg-
ative    context words, cn, for each target word and
ensuring that exp(wsgns
) is small for these
examples.

  id19ns

n

i

sgns has the bene   t of allowing incremental
initialization during learning, where the embed-
dings for time t are initialized with the embed-
dings from time t         (kim et al., 2014).

2.2 datasets, pre-processing, and

hyperparameters

we trained models on the 6 datasets described
in table 1, taken from google id165s (lin et
al., 2012) and the coha corpus (davies, 2010).
the google id165 datasets are extremely large
(comprising    6% of all books ever published), but
they also contain many corpus artifacts due, e.g.,
to shifting sampling biases over time (pechenick
et al., 2015). in contrast, the coha corpus was
carefully selected to be genre-balanced and rep-
resentative of american english over the last 200
years, though as a result it is two orders of mag-
nitude smaller. the coha corpus also contains
pre-extracted word lemmas, which we used to val-
idate that our results hold at both the lemma and
raw token levels. all the datasets were aggregated
to the granularity of decades.5

we follow the recommendations of levy et al.
(2015) in setting the hyperparameters for the em-
bedding methods, though preliminary experiments
were used to tune key settings. for all methods,
we used symmetric context windows of size 4 (on
each side). for sgns and svd, we use embed-
dings of size 300. see appendix a for further im-
plementation and pre-processing details.

5the 2000s decade of the google data was discarded due
to shifts in the sampling methodology (michel et al., 2011).

2.3 aligning historical embeddings
in order to compare word vectors from differ-
ent time-periods we must ensure that the vectors
are aligned to the same coordinate axes. ex-
plicit ppmi vectors are naturally aligned, as each
column simply corresponds to a context word.
low-dimensional embeddings will not be natu-
rally aligned due to the non-unique nature of the
svd and the stochastic nature of sgns. in par-
ticular, both these methods may result in arbi-
trary orthogonal transformations, which do not af-
fect pairwise cosine-similarities within-years but
will preclude comparison of the same word across
time. previous work circumvented this problem
by either avoiding low-dimensional embeddings
(e.g., gulordava and baroni, 2011; jatowt and
duh, 2014) or by performing heuristic local align-
ments per word (kulkarni et al., 2014).

we use orthogonal procrustes to align the
learned low-dimensional embeddings. de   ning
w(t)     rd  |v| as the matrix of id27s
learned at year t, we align across time-periods
while preserving cosine similarities by optimizing:

r(t) = arg min
q(cid:62)q=i

(cid:107)qw(t)     w(t+1)(cid:107)f ,

(4)

with r(t)     rd  d. the solution corresponds
to the best rotational alignment and can be ob-
tained ef   ciently using an application of svd
(sch  onemann, 1966).

2.4 time-series from historical embeddings
diachronic id27s can be used in two
ways to quantify semantic change: (i) we can mea-
sure changes in pair-wise word similarities over
time, or (ii) we can measure how an individual
word   s embedding shifts over time.
time-series measuring
pair-wise
how the cosine-similarity between pairs of words
changes over time allows us to test hypotheses
about speci   c linguistic or cultural shifts in a con-
trolled manner. we quantify shifts by computing
the similarity time-series

similarity

s(t)(wi, wj) = cos-sim(w(t)
i

, w(t)
j )

(5)

between two words wi and wj over a time-period
(t, ..., t +    ). we then measure the spearman
correlation (  ) of this series against time, which
allows us to assess the magnitude and signi   -
cance of pairwise similarity shifts; since the spear-
man correlation is non-parametric, this measure

essentially detects whether the similarity series in-
creased/decreased over time in a signi   cant man-
ner, regardless of the    shape    of this curve.6

individual

semantic

measuring
displacement after
aligning the embeddings for
time-
periods, we can use the aligned word vectors to
compute the semantic displacement that a word
has undergone during a certain time-period.
in
particular, we can directly compute the cosine-
distance between a word   s representation for
different
i.e. cos-dist(wt, wt+   ),
as a measure of semantic change. we can also
use this measure to quantify    rates    of semantic
change for different words by looking at
the
displacement between consecutive time-points.

time-periods,

3 comparison of different approaches

we compare the different distributional ap-
proaches on a set of benchmarks designed to test
their scienti   c utility. we evaluate both their syn-
chronic accuracy (i.e., ability to capture word sim-
ilarity within individual time-periods) and their di-
achronic validity (i.e., ability to quantify semantic
changes over time).

3.1 synchronic accuracy
we evaluated the synchronic (within-time-period)
accuracy of the methods using a standard modern
benchmark and the 1990s portion of the engall
data. on bruni et al. (2012)   s men similarity task
of matching human judgments of word similari-
ties, svd performed best (   = 0.739), followed
by ppmi (   = 0.687) and sgns (   = 0.649).
these results echo the    ndings of levy et al.
(2015), who found svd to perform best on sim-
ilarity tasks while sgns performed best on anal-
ogy tasks (which are not the focus of this work).

3.2 diachronic validity
we evaluate the diachronic validity of the methods
on two historical semantic tasks: detecting known
shifts and discovering shifts from data. for both
these tasks, we performed detailed evaluations on
a small set of examples (28 known shifts and the
top-10    discovered    shifts by each method). us-
ing these reasonably-sized evaluation sets allowed
the authors to evaluate each case rigorously using
existing literature and historical corpora.

6other metrics or change-point detection approaches, e.g.

mean shifts (kulkarni et al., 2014) could also be used.

word
gay
fatal
awful
nice
broadcast
monitor
record
guy
call

moving towards
homosexual, lesbian
illness, lethal
disgusting, mess
pleasant, lovely
transmit, radio
display, screen
tape, album
fellow, man
phone, message

shift start
moving away
ca 1950
happy, showy
fate, inevitable
<1800
impressive, majestic <1800
ca 1890
re   ned, dainty
ca 1920
scatter, seed
   
ca 1930
ca 1920
   
ca 1850
   
   
ca 1890

source
(kulkarni et al., 2014)
(jatowt and duh, 2014)
(simpson et al., 1989)
(wijaya and yeniterzi, 2011)
(jeffers and lehiste, 1979)
(simpson et al., 1989)
(kulkarni et al., 2014)
(wijaya and yeniterzi, 2011)
(simpson et al., 1989)

table 2: set of attested historical shifts used to evaluate the methods. the examples are taken from previous works on semantic
change and from the oxford english dictionary (oed), e.g. using    obsolete    tags. the shift start points were estimated using
attestation dates in the oed. the    rst six examples are words that shifted dramatically in meaning while the remaining four are
words that acquired new meanings (while potentially also keeping their old ones).

detecting known shifts. first, we
tested
whether the methods capture known historical
shifts in meaning. the goal in this task is for
the methods to correctly capture whether pairs of
words moved closer or further apart in semantic
space during a pre-determined time-period. we
use a set of independently attested shifts as an
evaluation set (table 2). for comparison, we eval-
uated the methods on both the large (but messy)
engall data and the smaller (but clean) coha
data. on this task, all the methods performed
almost perfectly in terms of capturing the correct
directionality of the shifts (i.e.,
the pairwise
similarity series have the correct sign on their
spearman correlation with time), but there were
some differences in whether the methods deemed
the shifts statistically signi   cant at the p < 0.05
level.7 overall, sgns performed the best on the
full english data, but its performance dropped
signi   cantly on the smaller coha dataset, where
svd performed best. ppmi was noticeably worse
than the other two approaches (table 3).

from data. we

discovering shifts
tested
whether the methods discover reasonable shifts
by examining the top-10 words that changed the
most from the 1900s to the 1990s according to
the semantic displacement metric introduced in
section 2.4 (limiting our analysis to words with
relative frequencies above 10   5 in both decades).
we used the engfic data as the most-changed
list for engall was dominated by scienti   c
terms due to changes in the corpus sample. table
4 shows the top-10 words discovered by each
method. these shifts were judged by the authors
as being either clearly genuine, borderline, or
sgns performed by
clearly corpus artifacts.

7all subsequent signi   cance tests are at p < 0.05.

method corpus

ppmi

svd

sgns

engall
coha
engall
coha
engall
coha

% correct %sig.
51.9
52.4
81.5
62.5
88.9
50.0

77.1
85.7
92.6
95.8
100.0
87.5

table 3: performance on detection task, i.e. ability to cap-
ture the attested shifts from table 2. sgns performs the
best on the engall corpus, whereas svd performs the
best on coha. note: these results use an improved and
corrected experimental protocol compared to earlier versions
of this work. the general trends are consistent, but the ab-
solute numbers for all methods are lower. see the appendix
for details, and please use these revised numbers for future
comparisons.

far the best on this task, with 70% of its top-10
list corresponding to genuine semantic shifts,
followed by 40% for svd, and 10% for ppmi.
however, a large portion of the discovered words
for ppmi (and less so svd) correspond to bor-
derline cases, e.g. know, that have not necessarily
shifted signi   cantly in meaning but that occur in
different contexts due to global genre/discourse
shifts. the poor quality of the nearest neighbors
generated by the ppmi algorithm   which are
skewed by ppmi   s sensitivity to rare events   also
made it dif   cult to assess the quality of its discov-
ered shifts. svd was the most sensitive to corpus
artifacts (e.g., co-occurrences due to cover pages
and advertisements), but it still captured a number
of genuine semantic shifts.

we opted for this small evaluation set and re-
lied on detailed expert judgments to minimize am-
biguity; each potential shift was analyzed in detail
by consulting consulting existing literature (espe-
cially the oed; simpson et al., 1989) and all dis-
agreements were discussed.

table 5 details representative example shifts in

method top-10 words that changed from 1900s to 1990s
ppmi
svd
sgns

know, got, would, decided, think, stop, remember, started, must, wanted
harry, headed, calls, gay, wherever, male, actually, special, cover, naturally
wanting, gay, check, starting, major, actually, touching, harry, headed, romance

table 4: top-10 english words with the highest semantic displacement values between the 1900s and 1990s. bolded entries
correspond to real semantic shifts, as deemed by examining the literature and their nearest neighbors; for example, headed
shifted from primarily referring to the    top of a body/entity    to referring to    a direction of travel.    underlined entries are
borderline cases that are largely due to global genre/discourse shifts; for example, male has not changed in meaning, but its
usage in discussions of    gender equality    is relatively new. finally, unmarked entries are clear corpus artifacts; for example,
special, cover, and romance are artifacts from the covers of    ction books occasionally including advertisements etc.

word
wanting

language nearest-neighbors in 1900s
english

lacking, de   cient, lacked, lack, needed wanted, something, wishing, anything,

nearest-neighbors in 1990s

asile

french

widerstand german

refuge, asiles, hospice, vieillards, in-
   rmerie
scheiterte, volt, stromst  arke,
brechen

leisten,

anybody
demandeurs, refuge, hospice, visas, ad-
mission
opposition, verfolgung, nationalsozialis-
tische, nationalsozialismus, kollaboration

table 5: example words that changed dramatically in meaning in three languages, discovered using sgns embeddings. the
in english, wanting
examples were selected from the top-10 most-changed lists between 1900s and 1990s as in table 4.
underwent subjecti   cation and shifted from meaning    lacking    to referring to subjective    desire   , as in    the education system
is wanting    (1900s) vs.    i   ve been wanting to tell you    (1990s). in french asile (   asylum   ) shifted from primarily referring
to    hospitals, or in   rmaries    to also referring to    asylum seekers, or refugees   . finally, in german widerstand (   resistance   )
gained a formal meaning as referring to the local german resistance to nazism during world war ii.

english, french, and german. chinese lacks suf-
   cient historical data for this task, as only years
1950-1999 are usable; however, we do still see
some signi   cant changes for chinese in this short
time-period, such as        (   virus   ) moving closer
to        (   computer   ,    = 0.89).

3.3 methodological recommendations

ppmi is clearly worse than the other two meth-
ods; it performs poorly on all the benchmark tasks,
is extremely sensitive to rare events, and is prone
to false discoveries from global genre shifts. be-
tween svd and sgns the results are somewhat
equivocal, as both perform best on two out of the
four tasks (synchronic accuracy, engall detec-
tion, coha detection, discovery). overall, svd
performs best on the synchronic accuracy task and
has higher average accuracy on the    detection   
task, while sgns performs best on the    discov-
ery    task. these results suggest that both these
methods are reasonable choices for studies of se-
mantic change but that they each have their own
tradeoffs: svd is more sensitive, as it performs
well on detection tasks even when using a small
dataset, but this sensitivity also results in false dis-
coveries due to corpus artifacts. in contrast, sgns
is robust to corpus artifacts in the discovery task,
but it is not sensitive enough to perform well on the

detection task with a small dataset. qualitatively,
we found sgns to be most useful for discovering
new shifts and visualizing changes (e.g., figure 1),
while svd was most effective for detecting subtle
shifts in usage.

4 statistical laws of semantic change
we now show how diachronic embeddings can be
used in a large-scale cross-linguistic analysis to re-
veal statistical laws that relate frequency and pol-
ysemy to semantic change. in particular, we ana-
lyze how a word   s rate of semantic change,

   (t)(wi) = cos-dist(w(t)
i

, w(t+1)

i

)

(6)

depends on its frequency, f (t)(wi) and a measure
of its polysemy, d(t)(wi) (de   ned in section 4.4).
4.1 setup
we present results using sgns embeddings. us-
ing all four languages and all four conditions for
english (engall, engfic, and coha with and
without lemmatization), we performed regression
analysis on rates of semantic change,    (t)(wi);
thus, we examined one data-point per word for
each pair of consecutive decades and analyzed
how a word   s frequency and polysemy at time t
correlate with its degree of semantic displacement
over the next decade. to ensure the robustness of

top-10 most polysemous
top-10 least polysemous

yet, always, even, little, called, also, sometimes, great, still, quite
photocopying, retrieval, thirties, mom, sweater, forties, seventeenth,
   fteenth, holster, postage

table 6: the top-10 most and least polysemous words in the engfic data. words like yet, even, and still are used in many
diverse ways and are highly polysemous. in contrast, words like photocopying, postage, and holster tend to be used in very
speci   c well-clustered contexts, corresponding to a single sense; for example, mail and letter are both very likely to occur in
the context of postage and are also likely to co-occur with each other, independent of postage.

figure 2: higher frequency words have lower rates of change (a), while polysemous words have higher rates of change (b).
the plots show robust id75    ts (huber, 2011) with 95% cis on the 2000s decade of the coha lemma data.

our results, we analyzed only non-stop words that
occurred more than 500 times in both decades con-
tributing to a change (lower-frequency words tend
to lack suf   cient co-occurrence data across years).
we also log-transformed the semantic displace-
ment scores and normalized the scores to have
zero mean and unit variance; we denote these nor-
malized scores by      (t)(wi).

though sgns and svd embeddings per-
formed similarly in our evaluation tasks, we opted
to use the sgns embeddings since they provide
a better estimate of the relationship between fre-
quency and semantic change. with svd embed-
dings the effect of frequency is confounded by the
fact that high frequency words have less    nite-
sample variance in their co-occurrence estimates,
which makes the word vectors of high frequency
words appear more stable between corpora, re-
gardless of any real semantic change. the sgns
embeddings do not suffer from this issue because
they are initialized with the embeddings of the pre-
vious decade.8

we performed our analysis using a linear mixed
model with random intercepts per word and    xed

8in fact, the sgns embeddings may even be biased in the
other direction, since higher frequency words undergo more
sgd updates    away    from this initialization.

(cid:17)

d(t)(wi)

(cid:17)

(cid:16)

+  d log

f (t)(wi)

effects per decade; i.e., we    t   f ,   d, and   t s.t.

(cid:16)
wi    wi     v, t     {t0, ..., tn},

     (t)(wi) =   f log
+   t + zwi +  (t)
(7)
where zwi     n (0,   wi) is the random intercept
wi     n (0,   ) is an error term.
for word wi and  (t)
  f ,   d and   t correspond to the    xed effects for
frequency, polysemy and the decade t, respec-
tively9. intuitively, this model estimates the effects
of frequency and polysemy on semantic change,
while controlling for temporal trends and correct-
ing for the fact that measurements on same word
will be correlated across time. we    t (7) using the
standard restricted maximum likelihood algorithm
(mcculloch and neuhaus, 2001; appendix c).

4.2 overview of results
we    nd that, across languages, rates of semantic
change obey a scaling relation of the form
   (wi)     f (wi)  f    d(wi)  d,

(8)

with   f < 0 and   d > 0. this    nding implies that
frequent words change at slower rates while pol-
ysemous words change faster, and that both these
relations scale as power laws.

9note that time is treated as a categorical variable, as each

decade has its own    xed effect.

4.3 law of conformity: frequently used

words change at slower rates

using the model in equation (7), we found that
the logarithm of a word   s frequency, log(f (wi)),
has a signi   cant and substantial negative effect on
rates of semantic change in all settings (figures 2a
and 3a). given the use of log-transforms in pre-
processing the data this implies rates of semantic
change are proportional to a negative power (  f )
of frequency, i.e.

   (wi)     f (wi)  f ,

(9)

   
with   f
guages/datasets.

[   1.24,   0.30]

across

lan-

4.4 law of innovation: polysemous words

change at faster rates

there is a common hypothesis in the linguistic lit-
erature that    words become semantically extended
by being used in diverse contexts    (winter et al.,
2014), an idea that dates back to the writings of
br  eal (1897). we tested this notion by examining
the relationship between polysemy and semantic
change in our data.

quantifying polysemy

measuring word polysemy is a dif   cult and
fraught task, as even    ground truth    dictionaries
differ in the number of senses they assign to words
(simpson et al., 1989; fellbaum, 1998). we cir-
cumvent this issue by measuring a word   s contex-
tual diversity as a proxy for its polysemousness.
the intuition behind our measure is that words
that occur in many distinct, unrelated contexts will
tend to be highly polysemous. this view of pol-
ysemy also    ts with previous work on semantic
change, which emphasizes the role of contextual
diversity (br  eal, 1897; winter et al., 2014).

we measure a word   s contextual diversity, and
thus polysemy, by examining its neighborhood in
an empirical co-occurrence network. we con-
struct empirical co-occurrence networks for the
top-10,000 non-stop words of each language using
the ppmi measure de   ned in section 2. in these
networks words are connected to each other if they
co-occur more than one would expect by chance
(after smoothing). the polysemy of a word is then
measured as its local id91 coef   cient within

(cid:80)

this network (watts and strogatz, 1998):

i{ppmi(ci, cj) > 0}

d(wi) =    

ci,cj   nppmi(wi)
|nppmi(wi)|(|nppmi(wi)|     1)

,
(10)
where nppmi(wi) = {wj : ppmi(wi, wj) > 0}.
this measure counts the proportion of wi   s neigh-
bors that are also neighbors of each other. accord-
ing to this measure, a word will have a high clus-
tering coef   cient (and thus a low polysemy score)
if the words that it co-occurs with also tend to co-
occur with each other. polysemous words that are
contextually diverse will have low id91 co-
ef   cients, since they appear in disjointed or unre-
lated contexts.

variants of this measure are often used in word-
sense discrimination and correlate with, e.g., num-
ber of senses in id138 (dorow and widdows,
2003; ferret, 2004). however, we found that
it was slightly biased towards rating contextually
diverse discourse function words (e.g., also) as
highly polysemous, which needs to be taken into
account when interpreting our results. we opted to
use this measure, despite this bias, because it has
the strong bene   t of being clearly interpretable: it
simply measures the extent to which a word ap-
pears in diverse textual contexts. table 6 gives ex-
amples of the least and most polysemous words in
the engfic data, according to this score.

as expected, this measure has signi   cant intrin-
sic positive correlation with frequency. across
datasets, we found pearson correlations in the
range 0.45 < r < 0.8 (all p < 0.05), con   rm-
ing frequent words tend to be used in a greater di-
versity of contexts. as a consequence of this high
correlation, we interpret the effect of this measure
only after controlling for frequency (this control is
naturally captured in equation (7)).

polysemy and semantic change
after    tting the model in equation (7), we found
that the logarithm of the polysemy score exhibits a
strong positive effect on rates of semantic change,
throughout all four languages (figure 3b). as with
frequency, the relation takes the form of a power
law

   (wi)     d(wi)  d,

(11)

with a language/corpus dependent scaling constant
in   d     [0.08, 0.53]. the distribution of polysemy
scores varies substantially across languages, so the

figure 3: a, the estimated linear effect of log-frequency (     f ) is signi   cantly negative across all languages. from the coha
data, we also see that the result holds regardless of whether lemmatization is used. b, analogous trends hold for the linear effect
of the polysemy score (     d), which is signi   cantly positive across all conditions. the magnitudes of     f and     d vary signi   cantly
across languages, indicating language-speci   c variation within the general scaling trends. 95% cis are shown.

large range for this constant is not surprising.10

note that this relationship between polysemy
and semantic change is a complete reversal from
what one would expect according to d(wi)   s pos-
itive correlation with frequency;
i.e., since fre-
quency and polysemy are highly positively cor-
related, one would expect them to have similar
effects on semantic change, but we found that
the effect of polysemy completely reversed after
controlling for frequency. figure 2b shows the
relationship of polysemy with rates of semantic
change in the coha lemma data after regress-
ing out effect of frequency (using the method of
graham, 2003).

5 discussion
we show how distributional methods can reveal
statistical laws of semantic change and offer a ro-
bust methodology for future work in this area.

our work builds upon a wealth of previous
research on quantitative approaches to semantic
change, including prior work with distributional
methods (sagi et al., 2011; wijaya and yeniterzi,
2011; gulordava and baroni, 2011; jatowt and
duh, 2014; kulkarni et al., 2014; xu and kemp,
2015), as well as recent work on detecting the
emergence of novel word senses (lau et al., 2012;
mitra et al., 2014; cook et al., 2014; mitra et al.,
2015; frermann and lapata, 2016). we extend
these lines of work by rigorously comparing dif-
ferent approaches to quantifying semantic change
and by using these methods to propose new statis-
tical laws of semantic change.

the two statistical laws we propose have strong
implications for future work in historical seman-

10for example, the engall polysemy scores have an ex-

cess kurtosis that is 25% larger than gerall.

the law of conformity   frequent words
tics.
change more slowly   clari   es frequency   s role
in semantic change. future studies of semantic
change must account for frequency   s conforming
effect: when examining the interaction between
some linguistic process and semantic change, the
law of conformity should serve as a null model in
which the interaction is driven primarily by under-
lying frequency effects.

the law of

innovation   polysemous words
change more quickly   quanti   es the central role
polysemy plays in semantic change, an issue that
has concerned linguists for more than 100 years
(br  eal, 1897). previous works argued that seman-
tic change leads to polysemy (wilkins, 1993; hop-
per and traugott, 2003). however, our results
show that polysemous words change faster, which
suggests that polysemy may actually lead to se-
mantic change.

these empirical statistical laws also lend them-
selves to various causal mechanisms. the law
of conformity might be a consequence of learn-
ing: perhaps people are more likely to use rare
words mistakenly in novel ways, a mechanism for-
malizable by bayesian models of word learning
and corresponding to the biological notion of ge-
netic drift (reali and grif   ths, 2010). or per-
haps a sociocultural conformity bias makes people
less likely to accept novel innovations of common
words, a mechanism analogous to the biological
process of purifying selection (boyd and richer-
son, 1988; pagel et al., 2007). moreover, such
mechanisms may also be partially responsible for
the law of innovation. highly polysemous words
tend to have more rare senses (kilgarriff, 2004),
and rare senses may be unstable by the law of con-
formity. while our results cannot con   rm such

causal links, they nonetheless highlight a new role
for frequency and polysemy in language change
and the importance of distributional models in his-
torical research.
acknowledgments
the authors thank d. friedman, r. sosic, c. man-
ning, v. prabhakaran, and s. todd for their helpful
comments and discussions. we also thank s. tsut-
sui for catching a typo in equation (4), which is
present in previous versions, and astrid van agge-
len for catching transcription errors in previous
versions of tables 2 and 3. we are also indebted
to our anonymous reviewers. w.h. was supported
by an nserc pgs-d grant and the sap stanford
graduate fellowship. w.h., d.j., and j.l. were
supported by the stanford data science initiative,
and nsf awards iis-1514268, iis-1149837, and
iis-1159679.

references
james s. adelman, gordon d. a. brown, and jos  e f.
quesada. 2006. contextual diversity, not word fre-
quency, determines word-naming and lexical deci-
sion times. psychol. sci., 17(9):814   823.

steven bird, ewan klein, and edward loper. 2009.
natural language processing with python. o   reilly
media, inc.

andreas blank. 1999. why do new meanings occur?
a cognitive typology of the motivations for lexical
semantic change. in peter koch and andreas blank,
editors, historical semantics and cognition. walter
de gruyter, berlin, germany.

robert boyd and peter j richerson. 1988. culture and
the evolutionary process. university of chicago
press, chicago, il.

elia bruni, gemma boleda, marco baroni, and nam-
khanh tran. 2012. id65 in tech-
nicolor. in proc. acl, pages 136   145.

michel br  eal. 1897. essai de s  emantique: science des

signi   cations. hachette, paris, france.

john a. bullinaria and joseph p. levy. 2007. ex-
tracting semantic representations from word co-
occurrence statistics: a computational study. behav.
res. methods, 39(3):510   526.

john a. bullinaria and joseph p. levy. 2012. ex-
tracting semantic representations from word co-
occurrence statistics:
stop-lists, id30, and
svd. behav. res. methods, 44(3):890   907.

paul cook, jey han lau, diana mccarthy, and timo-
thy baldwin. 2014. novel word-sense identi   ca-
tion. in proc. coling, pages 1624   1635.

scott crossley, tom salsbury, and danielle mcna-
mara. 2010. the development of polysemy and
frequency use in english second language speakers.
language learning, 60(3):573   605.

mark davies.

the corpus of historical
american english: 400 million words, 1810-2009.
http://corpus.byu.edu/coha/.

2010.

beate dorow and dominic widdows. 2003. discov-
ering corpus-speci   c word senses. in proc. eacl,
pages 79   82.

christiane fellbaum. 1998. id138. wiley online

library.

olivier ferret. 2004. discovering word senses from
a network of lexical cooccurrences. in proc. col-
ing, page 1326.

j.r. firth. 1957. a synopsis of linguistic theory,
1930-1955. in studies in linguistic analysis. spe-
cial volume of the philological society. basil black-
well, oxford, uk.

lea frermann and mirella lapata. 2016. a bayesian
model of diachronic meaning change. trans. acl,
4:31   45.

dirk geeraerts.

1997. diachronic prototype se-
mantics: a contribution to historical lexicology.
clarendon press, oxford, uk.

michael h. graham.

confronting multi-
collinearity in ecological multiple regression. ecol-
ogy, 84(11):2809   2815.

2003.

kristina gulordava and marco baroni. 2011. a dis-
tributional similarity approach to the detection of
semantic change in the google books ngram cor-
pus. in proc. gems 2011 workshop on geometri-
cal models of natural language semantics, pages
67   71. association for computational linguistics.

zellig s. harris. 1954. distributional structure. word,

10:146   162.

paul j. hopper and elizabeth closs traugott. 2003.
grammaticalization. cambridge university press,
cambridge, uk.

peter j huber. 2011. robust statistics. springer.

adam jatowt and kevin duh. 2014. a framework
for analyzing semantic change of words across time.
in proc. acm/ieee-cs conf. on digital libraries,
pages 229   238. ieee press.

j.l. bybee. 2007. frequency of use and the organi-
zation of language. oxford university press, new
york city, ny.

r. jeffers and ilse lehiste. 1979. principles and meth-
ods for historical linguistics. mit press, cam-
bridge, ma.

adam kilgarriff. 2004. how dominant is the common-
est sense of a word? in text, speech and dialogue,
pages 103   111. springer.

yoon kim, yi-i. chiu, kentaro hanaki, darshan
hegde, and slav petrov. 2014. temporal analysis
of language through neural language models. arxiv
preprint arxiv:1405.3515.

vivek kulkarni, rami al-rfou, bryan perozzi, and
steven skiena. 2014. statistically signi   cant de-
tection of linguistic change. in proc. www, pages
625   635.

across timescales. natural language engineering,
21(05):773   798.

mark pagel, quentin d. atkinson, and andrew meade.
2007.
frequency of word-use predicts rates of
lexical evolution throughout indo-european history.
nature, 449(7163):717   720.

eitan adam pechenick, christopher m. danforth, and
peter sheridan dodds. 2015. characterizing the
google books corpus: strong limits to id136s of
socio-cultural and linguistic evolution. plos one,
10(10).

thomas k. landauer and susan t. dumais.

1997.
a solution to plato   s problem: the latent semantic
analysis theory of acquisition, induction, and repre-
sentation of knowledge. psychol. rev., 104(2):211.

f. reali and t. l. grif   ths. 2010. words as alle-
les: connecting language evolution with bayesian
learners to models of genetic drift. proc. r. soc.
b, 277(1680):429   436.

jey han lau, paul cook, diana mccarthy, david new-
man, and timothy baldwin. 2012. word sense in-
duction for novel sense detection. in proc. eacl,
pages 591   601.

omer levy, yoav goldberg, and ido dagan. 2015. im-
proving distributional similarity with lessons learned
from id27s. trans. acl, 3.

erez lieberman, jean-baptiste michel, joe jackson,
tina tang, and martin a. nowak. 2007. quantify-
ing the evolutionary dynamics of language. nature,
449(7163):713   716.

yuri lin, jean-baptiste michel, erez lieberman aiden,
jon orwant, will brockman, and slav petrov. 2012.
syntactic annotations for the google books ngram
in proc. acl, system demonstrations,
corpus.
pages 169   174.

charles e mcculloch and john m neuhaus. 2001.
wiley-

generalized linear mixed models.
interscience, hoboken, nj.

jean-baptiste michel, yuan kui shen, aviva presser
aiden, adrian veres, matthew k. gray, joseph p.
pickett, dale hoiberg, dan clancy, peter norvig,
jon orwant, and others. 2011. quantitative analysis
of culture using millions of digitized books. sci-
ence, 331(6014):176   182.

tomas mikolov, ilya sutskever, kai chen, greg s. cor-
rado, and jeff dean. 2013. distributed representa-
tions of words and phrases and their compositional-
ity. in advances in neural information processing
systems, pages 3111   3119.

eyal sagi, stefan kaufmann, and brady clark. 2011.
tracing semantic change with latent semantic analy-
sis. in kathryn allan and justyna a. robinson, edi-
tors, current methods in historical semantics, page
161. de gruyter mouton, berlin, germany.

peter h sch  onemann. 1966. a generalized solution of
the orthogonal procrustes problem. psychometrika,
31(1):1   10.

j.s. seabold and j. perktold.

2010. statsmodels:
econometric and statistical modeling with python.
in proc. 9th python in science conference.

john andrew simpson, edmund sc weiner, et al.
1989. the oxford english dictionary, volume 2.
clarendon press oxford, oxford, uk.

elizabeth closs traugott and richard b dasher. 2001.
regularity in semantic change. cambridge univer-
sity press, cambridge, uk.

peter d. turney and patrick pantel. 2010. from fre-
quency to meaning: vector space models of seman-
tics. j. artif. intell. res., 37(1):141   188.

s. ullmann. 1962. semantics: an introduction to the
science of meaning. barnes & noble, new york
city, ny.

laurens van der maaten and geoffrey hinton. 2008.
visualizing data using id167. journal of machine
learning research, 9(2579-2605):85.

duncan j watts and steven h strogatz. 1998. col-
lective dynamics of    small-world   networks. nature,
393(6684):440   442.

sunny mitra, ritwik mitra, martin riedl, chris bie-
mann, animesh mukherjee, and pawan goyal.
2014. that   s sick dude!: automatic identi   cation
of word sense change across different timescales. in
proc. acl.

derry tanti wijaya and reyyan yeniterzi. 2011. un-
derstanding semantic change of words over cen-
turies. in proc. workshop on detecting and exploit-
ing cultural diversity on the social web, pages 35   
40. acm.

sunny mitra, ritwik mitra, suman kalyan maity,
martin riedl, chris biemann, pawan goyal, and
animesh mukherjee.
2015. an automatic ap-
proach to identify word sense changes in text media

david p wilkins. 1993. from part to person: natu-
ral tendencies of semantic change and the search for
cognates. cognitive anthropology research group
at the max planck institute for psycholinguistics.

b. winter, graham thompson, and matthias urban.
2014. cognitive factors motivating the evolution
of word meanings: evidence from corpora, be-
havioral data and encyclopedic network structure.
in proc. evolang, pages 353   360.

yang xu and charles kemp. 2015. a computational
evaluation of two laws of semantic change. in proc.
annual conf. of the cognitive science society.

yang xu, terry regier, and barbara c. malt. 2015.
historical semantic chaining and ef   cient commu-
nication: the case of container names. cognitive
science.

george kingsley zipf. 1945. the meaning-frequency
relationship of words. j. gen. psychol., 33(2):251   
256.

bahar   ilgen and bahar karaoglan. 2007.

investiga-
tion of zipf   s    law-of-meaning   on turkish corpora.
in international symposium on computer and infor-
mation sciences, pages 1   6. ieee.

a hyperparameter and pre-processing

details

for all datasets, words were lowercased and
stripped of punctuation. for the google datasets
we built models using the top-100000 words by
their average frequency over the entire histori-
cal time-periods, and we used the top-50000 for
coha. during model learning we also discarded
all words within a year that occurred below a cer-
tain threshold (500 for the google data, 100 for the
coha data).

moved with id203 pr(wi) = 1    (cid:113) 10   5

for all methods, we used the hyperparameters
recommended in levy et al. (2015). for the con-
text word distributions in all methods, we used
context distribution smoothing with a smoothing
parameter of 0.75. note that for sgns this cor-
responds to smoothing the unigram negative sam-
pling distribution. for both, sgns and ppmi, we
set the negative sample prior    = log(5), while we
set this value to    = 0 for svd, as this improved
results. when using sgns on the google data,
we also subsampled, with words being random re-
f (wi), as
recommended by levy et al. (2015) and mikolov
et al. (2013). furthermore, to improve the com-
putational ef   ciency of sgns (which works with
text streams and not co-occurrence counts), we
downsampled the larger years in the google n-
gram data to have at most 109 tokens. no such
subsampling was performed on the coha data.
for all methods, we de   ned the context set to
simply be the same vocabulary as the target words,

as is standard in most word vector applications
(levy et al., 2015). however, we found that the
ppmi method bene   ted substantially from larger
contexts (similar results were found in bullinaria
and levy, 2007), so we did not remove any low-
frequency words per year from the context for that
method. the other embedding approaches did not
appear to bene   t from the inclusion of these low-
frequency terms, so they were dropped for compu-
tational ef   ciency.

for sgns, we used the implementation pro-
vided in levy et al. (2015). the implementations
for ppmi and svd are released with the code
package associated with this work.

b visualization algorithm
to visualize semantic change for a word wi in two
dimensions we employed the following procedure,
which relies on the id167 embedding method
(van der maaten and hinton, 2008) as a subrou-
tine:

1. find the union of the word wi   s k nearest

neighbors over all necessary time-points.

2. compute the id167 embedding of these
words on the most recent (i.e., the modern)
time-point.

3. for each of the previous time-points, hold
all embeddings    xed, except for the target
word   s (i.e., the embedding for wi), and op-
timize a new id167 embedding only for the
target word. we found that initializing the
embedding for the target word to be the cen-
troid of its k(cid:48)-nearest neighbors in a time-
point was highly effective.

thus, in this procedure the background words are
always shown in their    modern    positions, which
makes sense given that these are the current mean-
ings of these words. this approximation is neces-
sary, since in reality all words are moving.

c regression analysis details
in addition to the pre-processing mentioned in the
main text, we also normalized the contextual di-
versity scores d(wi) within years by subtracting
the yearly median. this was necessary because
there was substantial changes in the median con-
textual diversity scores over years due to changes
in corpus sample sizes etc. we removed stop
words using the available lists in python   s nltk

points. however, we now require at least 5
time points to have a minimum amount of ro-
bustness.

    in earlier versions, the sgns model used the
incorrect date for the start of the shift for the
word gay.

a script for replicating the numbers in table 3,
using this revised methodology, is now available
in the github repo associated with this work. note
also that not all pairs in table 2 are actually used
for evaluation in all settings (e.g., for coha, due
to not having enough samples).

package (bird et al., 2009). we follow kim et al.
(2014) and allow a buffer period for the historical
word vectors to initialize; we use a buffer period
of four decades from the    rst usable decade and
only measure changes after this period.

when analyzing the effects of frequency and
contextual diversity, the model contained    xed ef-
fects for these features and for time along with
random effects for word identity. we opted not
to control for pos tags in the presented results,
as contextual diversity is co-linear with these tags
(e.g., adverbs are more contextual diverse than
nouns), and the goal was to demonstrate the main
effect of contextual diversity across all word types.
the linear mixed models, we used
the python statsmodels package with re-
stricted id113 (reml)
(seabold and perktold, 2010). all mentioned
signi   cance scores were computed according to
wald   s z-tests.

to    t

d revisions to the methodology for
   detecting known shifts    (table 3)

the methodology for    detecting known shifts    has
been improved to correct for certain issues, most
prominently:

    in earlier versions, the inclusion/exclusion of
word pairs in particular time points based
on frequency cutoffs was unnecessarily strict
and not properly detailed.
in the previous
versions, we used the same cutoffs as for the
analysis in section 4 (i.e., frequencies had to
be above 10   5), but this was not clear in the
text. in this revised version, we compute co-
sine similarities for pairs of words in a time
period if both are above the minimum count
for the embedding construction (100 occur-
rences for coha and 500 for the engall
corpus). this results in lower scores over-
all but is more re   ective of how downstream
users make use of our embeddings and re-
   ects the exact results one obtains by run-
ning our off-the-shelf embeddings (available
on the project website) through the evalua-
tion. for time points where one of the words
in a pair is below the threshold, we simply
discard these time points from the spearman
correlation.

    in earlier versions, spearman correlations
were computed for pairs with less than 5 time

