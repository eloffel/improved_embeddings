4
1
0
2
 
c
e
d
9
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
7
6
5
5

.

2
1
4
1
:
v
i
x
r
a

deep speech: scaling up end-to-end

id103

awni hannun   , carl case, jared casper, bryan catanzaro, greg diamos, erich elsen,

ryan prenger, sanjeev satheesh, shubho sengupta, adam coates, andrew y. ng

baidu research     silicon valley ai lab

abstract

we present a state-of-the-art id103 system developed using end-to-
end deep learning. our architecture is signi   cantly simpler than traditional speech
systems, which rely on laboriously engineered processing pipelines; these tradi-
tional systems also tend to perform poorly when used in noisy environments. in
contrast, our system does not need hand-designed components to model back-
ground noise, reverberation, or speaker variation, but instead directly learns a
function that is robust to such effects. we do not need a phoneme dictionary,
nor even the concept of a    phoneme.    key to our approach is a well-optimized
id56 training system that uses multiple gpus, as well as a set of novel data syn-
thesis techniques that allow us to ef   ciently obtain a large amount of varied data
for training. our system, called deep speech, outperforms previously published
results on the widely studied switchboard hub5   00, achieving 16.0% error on the
full test set. deep speech also handles challenging noisy environments better than
widely used, state-of-the-art commercial speech systems.

1

introduction

top id103 systems rely on sophisticated pipelines composed of multiple algorithms
and hand-engineered processing stages. in this paper, we describe an end-to-end speech system,
called    deep speech   , where deep learning supersedes these processing stages. combined with a
language model, this approach achieves higher performance than traditional methods on hard speech
recognition tasks while also being much simpler. these results are made possible by training a large
recurrent neural network (id56) using multiple gpus and thousands of hours of data. because this
system learns directly from data, we do not require specialized components for speaker adaptation
or noise    ltering. in fact, in settings where robustness to speaker variation and noise are critical,
our system excels: deep speech outperforms previously published methods on the switchboard
hub5   00 corpus, achieving 16.0% error, and performs better than commercial systems in noisy
id103 tests.
traditional speech systems use many heavily engineered processing stages, including specialized
input features, acoustic models, and id48 (id48s). to improve these pipelines,
domain experts must invest a great deal of effort tuning their features and models. the introduction
of deep learning algorithms [27, 30, 15, 18, 9] has improved speech system performance, usually
by improving acoustic models. while this improvement has been signi   cant, deep learning still
plays only a limited role in traditional speech pipelines. as a result, to improve performance on a
task such as recognizing speech in a noisy environment, one must laboriously engineer the rest of
the system for robustness. in contrast, our system applies deep learning end-to-end using recurrent
neural networks. we take advantage of the capacity provided by deep learning systems to learn
from large datasets to improve our overall performance. our model is trained end-to-end to produce

   contact author: awnihannun@baidu.com

1

transcriptions and thus, with suf   cient data and computing power, can learn robustness to noise or
speaker variation on its own.
tapping the bene   ts of end-to-end deep learning, however, poses several challenges: (i) we must
   nd innovative ways to build large, labeled training sets and (ii) we must be able to train networks
that are large enough to effectively utilize all of this data. one challenge for handling labeled data
in speech systems is    nding the alignment of text transcripts with input speech. this problem has
been addressed by graves, fern  andez, gomez and schmidhuber [13], thus enabling neural net-
works to easily consume unaligned, transcribed audio during training. meanwhile, rapid training of
large neural networks has been tackled by coates et al. [7], demonstrating the speed advantages of
multi-gpu computation. we aim to leverage these insights to ful   ll the vision of a generic learning
system, based on large speech datasets and scalable id56 training, that can surpass more compli-
cated traditional methods. this vision is inspired partly by the work of lee et. al. [27] who applied
early unsupervised id171 techniques to replace hand-built speech features.
we have chosen our id56 model speci   cally to map well to gpus and we use a novel model par-
tition scheme to improve parallelization. additionally, we propose a process for assembling large
quantities of labeled speech data exhibiting the distortions that our system should learn to handle.
using a combination of collected and synthesized data, our system learns robustness to realistic
noise and speaker variation (including lombard effect [20]). taken together, these ideas suf   ce to
build an end-to-end speech system that is at once simpler than traditional pipelines yet also performs
better on dif   cult speech tasks. deep speech achieves an error rate of 16.0% on the full switchboard
hub5   00 test set   the best published result. further, on a new noisy id103 dataset of
our own construction, our system achieves a word error rate of 19.1% where the best commercial
systems achieve 30.5% error.
in the remainder of this paper, we will introduce the key ideas behind our id103 system.
we begin by describing the basic recurrent neural network model and training framework that we
use in section 2, followed by a discussion of gpu optimizations (section 3), and our data capture
and synthesis strategy (section 4). we conclude with our experimental results demonstrating the
state-of-the-art performance of deep speech (section 5), followed by a discussion of related work
and our conclusions.

2 id56 training setup

the core of our system is a recurrent neural network (id56) trained to ingest speech spectrograms
and generate english text transcriptions. let a single utterance x and label y be sampled from a
training set x = {(x(1), y(1)), (x(2), y(2)), . . .}. each utterance, x(i), is a time-series of length t (i)
where every time-slice is a vector of audio features, x(i)
, t = 1, . . . , t (i). we use spectrograms as
t
our features, so x(i)
t,p denotes the power of the p   th frequency bin in the audio frame at time t. the
goal of our id56 is to convert an input sequence x into a sequence of character probabilities for the
transcription y, with   yt = p(ct|x), where ct     {a,b,c, . . . , z, space, apostrophe, blank}.
our id56 model is composed of 5 layers of hidden units. for an input x, the hidden units at layer
l are denoted h(l) with the convention that h(0) is the input. the    rst three layers are not recurrent.
for the    rst layer, at each time t, the output depends on the spectrogram frame xt along with a
context of c frames on each side.1 the remaining non-recurrent layers operate on independent data
for each time step. thus, for each time t, the    rst 3 layers are computed by:

t = g(w (l)h(l   1)
h(l)

t

+ b(l))

where g(z) = min{max{0, z}, 20} is the clipped recti   ed-linear (relu) activation function and
w (l), b(l) are the weight matrix and bias parameters for layer l.2 the fourth layer is a bi-directional
recurrent layer [38]. this layer includes two sets of hidden units: a set with forward recurrence,

1we typically use c     {5, 7, 9} for our experiments.
2the relu units are clipped in order to keep the activations in the recurrent layer from exploding; in practice

the units rarely saturate at the upper bound.

2

h(f ), and a set with backward recurrence h(b):

h(f )
t = g(w (4)h(3)
h(b)
t = g(w (4)h(3)

t + w (f )
t + w (b)

r h(f )
r h(b)

t   1 + b(4))
t+1 + b(4))

note that h(f ) must be computed sequentially from t = 1 to t = t (i) for the i   th utterance, while
the units h(b) must be computed sequentially in reverse from t = t (i) to t = 1.

the    fth (non-recurrent) layer takes both the forward and backward units as inputs h(5)
t =
g(w (5)h(4)
. the output layer is a standard softmax function
that yields the predicted character probabilities for each time slice t and character k in the alphabet:

t + b(5)) where h(4)

t = h(f )

t + h(b)

t

t,k =   yt,k     p(ct = k|x) =
h(6)

(cid:80)

k h(5)

exp(w (6)
j exp(w (6)

t + b(6)
k )
t + b(6)
j )

j h(5)

.

k

and b(6)

k denote the k   th column of the weight matrix and k   th bias, respectively.

here w (6)
once we have computed a prediction for p(ct|x), we compute the ctc loss [13] l(  y, y) to measure
the error in prediction. during training, we can evaluate the gradient      yl(  y, y) with respect to
the network outputs given the ground-truth character sequence y. from this point, computing the
gradient with respect to all of the model parameters may be done via back-propagation through the
rest of the network. we use nesterov   s accelerated gradient method for training [41].3

figure 1: structure of our id56 model and notation.

the complete id56 model is illustrated in figure 1. note that its structure is considerably simpler
than related models from the literature [14]   we have limited ourselves to a single recurrent layer
(which is the hardest to parallelize) and we do not use long-short-term-memory (lstm) circuits.
one disadvantage of lstm cells is that they require computing and storing multiple gating neu-
ron responses at each step. since the forward and backward recurrences are sequential, this small
additional cost can become a computational bottleneck. by using a homogeneous model we have
made the computation of the recurrent activations as ef   cient as possible: computing the relu out-
puts involves only a few highly optimized blas operations on the gpu and a single point-wise
nonlinearity.

3we use momentum of 0.99 and anneal the learning rate by a constant factor, chosen to yield the fastest

convergence, after each epoch through the data.

3

2.1 id173

while we have gone to signi   cant lengths to expand our datasets (c.f. section 4), the recurrent
networks we use are still adept at    tting the training data. in order to reduce variance further, we use
several techniques.
during training we apply a dropout [19] rate between 5% - 10%. we apply dropout in the feed-
forward layers but not to the recurrent hidden activations.
a commonly employed technique in id161 during network evaluation is to randomly
jitter inputs by translations or re   ections, feed each jittered version through the network, and vote
or average the results [23]. such jittering is not common in asr, however we found it bene   cial to
translate the raw audio    les by 5ms (half the    lter bank step size) to the left and right, then forward
propagate the recomputed features and average the output probabilities. at test time we also use an
ensemble of several id56s, averaging their outputs in the same way.

2.2 language model

when trained from large quantities of labeled speech data, the id56 model can learn to produce
readable character-level transcriptions. indeed for many of the transcriptions, the most likely char-
acter sequence predicted by the id56 is exactly correct without external language constraints. the
errors made by the id56 in this case tend to be phonetically plausible renderings of english words   
table 1 shows some examples. many of the errors occur on words that rarely or never appear in our
training set. in practice, this is hard to avoid: training from enough speech data to hear all of the
words or language constructions we might need to know is impractical. therefore, we integrate our
system with an id165 language model since these models are easily trained from huge unlabeled
text corpora. for comparison, while our speech datasets typically include up to 3 million utterances,
the id165 language model used for the experiments in section 5.2 is trained from a corpus of 220
million phrases, supporting a vocabulary of 495,000 words.4

id56 output
what is the weather like in bostin right now what is the weather like in boston right now
prime miniter nerenr modi
arther n tickets for the game

prime minister narendra modi
are there any tickets for the game

decoded transcription

table 1: examples of transcriptions directly from the id56 (left) with errors that are    xed by addi-
tion of a language model (right).
given the output p(c|x) of our id56 we perform a search to    nd the sequence of characters c1, c2, . . .
that is most probable according to both the id56 output and the language model (where the language
model interprets the string of characters as words). speci   cally, we aim to    nd a sequence c that
maximizes the combined objective:

q(c) = log(p(c|x)) +    log(plm(c)) +    word count(c)

where    and    are tunable parameters (set by cross-validation) that control the trade-off between
the id56, the language model constraint and the length of the sentence. the term plm denotes the
id203 of the sequence c according to the id165 model. we maximize this objective using a
highly optimized id125 algorithm, with a typical beam size in the range 1000-8000   similar
to the approach described by hannun et al. [16].

3 optimizations

as noted above, we have made several design decisions to make our networks amenable to high-
speed execution (and thus fast training). for example, we have opted for homogeneous recti   ed-
linear networks that are simple to implement and depend on just a few highly-optimized blas
calls. when fully unrolled, our networks include almost 5 billion connections for a typical utterance

4we use the kenlm toolkit [17] to train the id165 language models in our experiments.

4

and thus ef   cient computation is critical to make our experiments feasible. we use multi-gpu
training [7, 23] to accelerate our experiments, but doing this effectively requires some additional
work, as we explain.

3.1 data parallelism

, h(i+1)

in order to process data ef   ciently, we use two levels of data parallelism. first, each gpu processes
many examples in parallel. this is done in the usual way by concatenating many examples into a
single matrix. for instance, rather than performing a single matrix-vector multiplication wrht in the
recurrent layer, we prefer to do many in parallel by computing wrht where ht = [h(i)
, . . .]
t
(where h(i)
corresponds to the i   th example x(i) at time t). the gpu is most ef   cient when ht is
t
relatively wide (e.g., 1000 examples or more) and thus we prefer to process as many examples on
one gpu as possible (up to the limit of gpu memory).
when we wish to use larger minibatches than a single gpu can support on its own we use data
parallelism across multiple gpus, with each gpu processing a separate minibatch of examples and
then combining its computed gradient with its peers during each iteration. we typically use 2   or
4   data parallelism across gpus.
data parallelism is not easily implemented, however, when utterances have different lengths since
they cannot be combined into a single id127. we resolve the problem by sorting
our training examples by length and combining only similarly-sized utterances into minibatches,
padding with silence when necessary so that all utterances in a batch have the same length. this
solution is inspired by the itpack/ellpack sparse matrix format [21]; a similar solution was
used by the sutskever et al. [42] to accelerate id56s for text.

t

3.2 model parallelism

data parallelism yields training speedups for modest multiples of the minibatch size (e.g., 2 to
4), but faces diminishing returns as batching more examples into a single gradient update fails to
improve the training convergence rate. that is, processing 2   as many examples on 2   as many
gpus fails to yield a 2   speedup in training. it is also inef   cient to    x the total minibatch size but
spread out the examples to 2   as many gpus: as the minibatch within each gpu shrinks, most
operations become memory-bandwidth limited. to scale further, we parallelize by partitioning the
model (   model parallelism    [7, 10]).
our model is challenging to parallelize due to the sequential nature of the recurrent layers. since
the bidirectional layer is comprised of a forward computation and a backward computation that are
independent, we can perform the two computations in parallel. unfortunately, naively splitting the
id56 to place h(f ) and h(b) on separate gpus commits us to signi   cant data transfers when we go to
compute h(5) (which depends on both h(f ) and h(b)). thus, we have chosen a different partitioning
of work that requires less communication for our models: we divide the model in half along the time
dimension.
all layers except the recurrent layer can be trivially decomposed along the time dimension, with the
   rst half of the time-series, from t = 1 to t = t (i)/2, assigned to one gpu and the second half
to another gpu. when computing the recurrent layer activations, the    rst gpu begins computing
the forward activations h(f ), while the second begins computing the backward activations h(b). at
the mid-point (t = t (i)/2), the two gpus exchange the intermediate activations, h(f )
t /2 and h(b)
t /2
and swap roles. the    rst gpu then    nishes the backward computation of h(b) and the second gpu
   nishes the forward computation of h(f ).

3.3 striding

we have worked to minimize the running time of the recurrent layers of our id56, since these are
the hardest to parallelize. as a    nal optimization, we shorten the recurrent layers by taking    steps   
(or strides) of size 2 in the original input so that the unrolled id56 has half as many steps. this is
similar to a convolutional network [25] with a step-size of 2 in the    rst layer. we use the cudnn
library [2] to implement this    rst layer of convolution ef   ciently.

5

dataset
wsj
switchboard
fisher
baidu

type
read

conversational
conversational

read

hours

speakers

80
300
2000
5000

280
4000
23000
9600

table 2: a summary of the datasets used to train deep speech. the wall street journal, switchboard
and fisher [3] corpora are all published by the linguistic data consortium.

4 training data

large-scale deep learning systems require an abundance of labeled data. for our system we need
many recorded utterances and corresponding english transcriptions, but there are few public datasets
of suf   cient scale. to train our largest models we have thus collected an extensive dataset consisting
of 5000 hours of read speech from 9600 speakers. for comparison, we have summarized the labeled
datasets available to us in table 2.

4.1 synthesis by superposition

to expand our potential training data even further we use data synthesis, which has been successfully
applied in other contexts to amplify the effective number of training samples [37, 26, 6]. in our work,
the goal is primarily to improve performance in noisy environments where existing systems break
down. capturing labeled data (e.g., read speech) from noisy environments is not practical, however,
and thus we must    nd other ways to generate such data.
to a    rst order, audio signals are generated through a process of superposition of source signals.
we can use this fact to synthesize noisy training data. for example, if we have a speech audio track
x(i) and a    noise    audio track   (i), then we can form the    noisy speech    track   x(i) = x(i) +   (i) to
simulate audio captured in a noisy environment. if necessary, we can add reverberations, echoes or
other forms of damping to the power spectrum of   (i) or x(i) and then simply add them together to
make fairly realistic audio scenes.
there are, however, some risks in this approach. for example, in order to take 1000 hours of clean
speech and create 1000 hours of noisy speech, we will need unique noise tracks spanning roughly
1000 hours. we cannot settle for, say, 10 hours of repeating noise, since it may become possible
for the recurrent network to memorize the noise track and    subtract    it out of the synthesized data.
thus, instead of using a single noise source   (i) with a length of 1000 hours, we use a large number
of shorter clips (which are easier to collect from public video sources) and treat them as separate
sources of noise before superimposing all of them:   x(i) = x(i) +   (i)
when superimposing many signals collected from video clips, we can end up with    noise    sounds
that are different from the kinds of noise recorded in real environments. to ensure a good match
between our synthetic data and real data, we rejected any candidate noise clips where the average
power in each frequency band differed signi   cantly from the average power observed in real noisy
recordings.

1 +   (i)

2 + . . ..

4.2 capturing lombard effect

one challenging effect encountered by id103 systems in noisy environments is the
   lombard effect    [20]: speakers actively change the pitch or in   ections of their voice to overcome
noise around them. this (involuntary) effect does not show up in recorded speech datasets since
they are collected in quiet environments. to ensure that the effect is represented in our training data
we induce the lombard effect intentionally during data collection by playing loud background noise

6

through headphones worn by a person as they record an utterance. the noise induces them to in   ect
their voice, thus allowing us to capture the lombard effect in our training data.5

5 experiments

we performed two sets of experiments to evaluate our system.
in both cases we use the model
described in section 2 trained from a selection of the datasets in table 2 to predict character-level
transcriptions. the predicted id203 vectors and language model are then fed into our decoder
to yield a word-level transcription, which is compared with the ground truth transcription to yield
the word error rate (wer).

5.1 conversational speech: switchboard hub5   00 (full)

to compare our system to prior research we use an accepted but highly challenging test set, hub5   00
(ldc2002s23). some researchers split this set into    easy    (switchboard) and    hard    (callhome)
instances, often reporting new results on the easier portion alone. we use the full set, which is the
most challenging case and report the overall word error rate.
we evaluate our system trained on only the 300 hour switchboard conversational telephone speech
dataset and trained on both switchboard (swb) and fisher (fsh) [3], a 2000 hour corpus collected
in a similar manner as switchboard. many researchers evaluate models trained only with 300 hours
from switchboard conversational telephone speech when testing on hub5   00. in part this is because
training on the full 2000 hour fisher corpus is computationally dif   cult. using the techniques men-
tioned in section 3 our system is able perform a full pass over the 2300 hours of data in just a few
hours.
since the switchboard and fisher corpora are distributed at a sample rate of 8khz, we compute
spectrograms of 80 linearly spaced log    lter banks and an energy term. the    lter banks are computed
over windows of 20ms strided by 10ms. we did not evaluate more sophisticated features such as the
mel-scale log    lter banks or the mel-frequency cepstral coef   cients.
speaker adaptation is critical to the success of current asr systems [44, 36], particularly when
trained on 300 hour switchboard. for the models we test on hub5   00, we apply a simple form of
speaker adaptation by normalizing the spectral features on a per speaker basis. other than this, we
do not modify the input features in any way.
for decoding, we use a 4-gram language model with a 30,000 word vocabulary trained on the fisher
and switchboard transcriptions. again, hyperparameters for the decoding objective are chosen via
cross-validation on a held-out development set.
the deep speech swb model is a network of 5 hidden layers each with 2048 neurons trained on
only 300 hour switchboard. the deep speech swb + fsh model is an ensemble of 4 id56s each
with 5 hidden layers of 2304 neurons trained on the full 2300 hour combined corpus. all networks
are trained on inputs of +/- 9 frames of context.
we report results in table 3. the model from vesely et al. (dnn-gmm smbr) [44] uses a se-
quence based id168 on top of a dnn after using a typical hybrid dnn-id48 system to
realign the training set. the performance of this model on the combined hub5   00 test set is the best
previously published result. when trained on the combined 2300 hours of data the deep speech sys-
tem improves upon this baseline by 2.4% absolute wer and 13.0% relative. the model from maas
et al. (dnn-id48 fsh) [28] achieves 19.9% wer when trained on the fisher 2000 hour corpus.
that system was built using kaldi [32], state-of-the-art open source id103 software.
we include this result to demonstrate that deep speech, when trained on a comparable amount of
data is competitive with the best existing asr systems.

5we have experimented with noise played through headphones as well as through computer speakers. using
headphones has the advantage that we obtain    clean    recordings without the background noise included and
can add our own synthetic noise afterward.

7

model
vesely et al. (gmm-id48 bmmi) [44]
vesely et al. (dnn-id48 smbr) [44]
maas et al. (dnn-id48 swb) [28]
maas et al. (dnn-id48 fsh) [28]
seide et al. (cd-dnn) [39]
kingsbury et al. (dnn-id48 smbr hf) [22]
sainath et al. (id98-id48) [36]
soltau et al. (mlp/id98+i-vector) [40]
deep speech swb
deep speech swb + fsh

swb
18.6
12.6
14.6
16.0
16.1
13.3
11.5
10.4
20.0
12.6

ch
33.0
24.1
26.3
23.7
n/a
n/a
n/a
n/a
31.8
19.3

full
25.8
18.4
20.5
19.9
n/a
n/a
n/a
n/a
25.9
16.0

table 3: published error rates (%wer) on switchboard dataset splits. the columns labeled    swb   
and    ch    are respectively the easy and hard subsets of hub5   00.

5.2 noisy speech

few standards exist for testing noisy speech performance, so we constructed our own evaluation set
of 100 noisy and 100 noise-free utterances from 10 speakers. the noise environments included a
background radio or tv; washing dishes in a sink; a crowded cafeteria; a restaurant; and inside a car
driving in the rain. the utterance text came primarily from web search queries and text messages, as
well as news clippings, phone conversations, internet comments, public speeches, and movie scripts.
we did not have precise control over the signal-to-noise ratio (snr) of the noisy samples, but we
aimed for an snr between 2 and 6 db.
for the following experiments, we train our id56s on all the datasets (more than 7000 hours) listed
in table 2. since we train for 15 to 20 epochs with newly synthesized noise in each pass, our model
learns from over 100,000 hours of novel data. we use an ensemble of 6 networks each with 5 hidden
layers of 2560 neurons. no form of speaker adaptation is applied to the training or evaluation sets.
we normalize training examples on a per utterance basis in order to make the total power of each
example consistent. the features are 160 linearly spaced log    lter banks computed over windows
of 20ms strided by 10ms and an energy term. audio    les are resampled to 16khz prior to the
featurization. finally, from each frequency bin we remove the global mean over the training set
and divide by the global standard deviation, primarily so the inputs are well scaled during the early
stages of training.
as described in section 2.2, we use a 5-gram language model for the decoding. we train the lan-
guage model on 220 million phrases of the common crawl6, selected such that at least 95% of the
characters of each phrase are in the alphabet. only the most common 495,000 words are kept, the
rest remapped to an unknown token.
we compared the deep speech system to several commercial speech systems: (1) wit.ai, (2) google
speech api, (3) bing speech and (4) apple dictation.7
our test is designed to benchmark performance in noisy environments. this situation creates chal-
lenges for evaluating the web speech apis: these systems will give no result at all when the snr is
too low or in some cases when the utterance is too long. therefore we restrict our comparison to the
subset of utterances for which all systems returned a non-empty result.8 the results of evaluating
each system on our test    les appear in table 4.
to evaluate the ef   cacy of the noise synthesis techniques described in section 4.1, we trained two
id56s, one on 5000 hours of raw data and the other trained on the same 5000 hours plus noise. on
the 100 clean utterances both models perform about the same, 9.2% wer and 9.0% wer for the

6commoncrawl.org
7wit.ai and google speech each have http-based apis. to test apple dictation and bing speech, we used
a kernel extension to loop audio output back to audio input in conjunction with the os x dictation service and
the windows 8 bing id103 api.

8this leads to much higher accuracies than would be reported if we attributed 100% error in cases where an

api failed to respond.

8

clean trained model and the noise trained model respectively. however, on the 100 noisy utterances
the noisy model achieves 22.6% wer over the clean model   s 28.7% wer, a 6.1% absolute and
21.3% relative improvement.

system
apple dictation
bing speech
google api
wit.ai
deep speech

clean (94) noisy (82) combined (176)

14.24
11.73
6.64
7.94
6.56

43.76
36.12
30.47
35.06
19.06

26.73
22.05
16.72
19.41
11.85

table 4: results (%wer) for 5 systems evaluated on the original audio. scores are reported only
for utterances with predictions given by all systems. the number in parentheses next to each dataset,
e.g. clean (94), is the number of utterances scored.

6 related work

several parts of our work are inspired by previous results. neural network acoustic models and other
connectionist approaches were    rst introduced to speech pipelines in the early 1990s [1, 34, 11].
these systems, similar to dnn acoustic models [30, 18, 9], replace only one stage of the speech
recognition pipeline. mechanically, our system is similar to other efforts to build end-to-end speech
systems from deep learning algorithms. for example, graves et al. [13] have previously introduced
the    connectionist temporal classi   cation    (ctc) id168 for scoring transcriptions produced
by id56s and, with id137, have previously applied this approach to speech [14]. we sim-
ilarly adopt the ctc loss for part of our training procedure but use much simpler recurrent networks
with recti   ed-linear activations [12, 29, 31]. our recurrent network is similar to the bidirectional
id56 used by hannun et al. [16], but with multiple changes to enhance its scalability. by focusing
on scalability, we have shown that these simpler networks can be effective even without the more
complex lstm machinery.
our work is certainly not the    rst to exploit scalability to improve performance of dl algorithms.
the value of scalability in deep learning is well-studied [8, 24] and the use of parallel processors
(including gpus) has been instrumental to recent large-scale dl results [43, 24]. early ports of dl
algorithms to gpus revealed signi   cant speed gains [33]. researchers have also begun choosing
designs that map well to gpu hardware to gain even more ef   ciency, including convolutional [23,
4, 35] and locally connected [7, 5] networks, especially when optimized libraries like cudnn [2]
and blas are available. indeed, using high-performance computing infrastructure, it is possible
today to train neural networks with more than 10 billion connections [7] using clusters of gpus.
these results inspired us to focus    rst on making scalable design choices to ef   ciently utilize many
gpus before trying to engineer the algorithms and models themselves.
with the potential to train large models, there is a need for large training sets as well. in other    elds,
such as id161, large labeled training sets have enabled signi   cant leaps in performance
as they are used to feed larger and larger dl systems [43, 23]. in id103, however,
such large training sets are less common, with typical benchmarks having training sets ranging
from tens of hours (e.g. the wall street journal corpus with 80 hours) to several hundreds of hours
(e.g. switchboard and broadcast news). larger benchmark datasets, such as the fisher corpus [3]
with 2000 hours of transcribed speech, are rare and only recently being studied. to fully utilize
the expressive power of the recurrent networks available to us, we rely not only on large sets of
labeled utterances, but also on synthesis techniques to generate novel examples. this approach is
well known in id161 [37, 26, 6] but we have found this especially convenient and effective
for speech when done properly.

7 conclusion

we have presented an end-to-end deep learning-based speech system capable of outperforming exist-
ing state-of-the-art recognition pipelines in two challenging scenarios: clear, conversational speech

9

and speech in noisy environments. our approach is enabled particularly by multi-gpu training and
by data collection and synthesis strategies to build large training sets exhibiting the distortions our
system must handle (such as background noise and lombard effect). combined, these solutions en-
able us to build a data-driven speech system that is at once better performing than existing methods
while no longer relying on the complex processing stages that had stymied further progress. we
believe this approach will continue to improve as we capitalize on increased computing power and
dataset sizes in the future.

acknowledgments

we are grateful to jia lei, whose work on dl for speech at baidu has spurred us forward, for his
advice and support throughout this project. we also thank ian lane, dan povey, dan jurafsky, dario
amodei, andrew maas, calisa cole and li wei for helpful conversations.

references

[1] h. bourlard and n. morgan. connectionist id103: a hybrid approach. kluwer

academic publishers, norwell, ma, 1993.

[2] s. chetlur, c. woolley, p. vandermersch, j. cohen, j. tran, b. catanzaro, and e. shelhamer.

cudnn: ef   cient primitives for deep learning.

[3] c. cieri, d. miller, and k. walker. the fisher corpus: a resource for the next generations of

speech-to-text. in lrec, volume 4, pages 69   71, 2004.

[4] d. c. ciresan, u. meier, j. masci, l. m. gambardella, and j. schmidhuber. flexible, high
in international joint

performance convolutional neural networks for image classi   cation.
conference on arti   cial intelligence, pages 1237   1242, 2011.

[5] d. c. ciresan, u. meier, and j. schmidhuber. multi-column deep neural networks for image

classi   cation. in id161 and pattern recognition, pages 3642   3649, 2012.

[6] a. coates, b. carpenter, c. case, s. satheesh, b. suresh, t. wang, d. j. wu, and a. y. ng.
text detection and character recognition in scene images with unsupervised id171.
in international conference on document analysis and recognition, 2011.

[7] a. coates, b. huval, t. wang, d. j. wu, a. y. ng, and b. catanzaro. deep learning with

cots hpc. in international conference on machine learning, 2013.

[8] a. coates, h. lee, and a. y. ng. an analysis of single-layer networks in unsupervised feature

learning. in 14th international conference on ai and statistics, pages 215   223, 2011.

[9] g. dahl, d. yu, l. deng, and a. acero. context-dependent pre-trained deep neural networks
for large vocabulary id103. ieee transactions on audio, speech, and language
processing, 2011.

[10] j. dean, g. s. corrado, r. monga, k. chen, m. devin, q. v. le, m. z. mao, m. ranzato,
in

a. senior, p. tucker, k. yang, and a. y. ng. large scale distributed deep networks.
advances in neural information processing systems 25, 2012.

[11] d. ellis and n. morgan. size matters: an empirical study of neural network training for large
vocabulary continuous id103. in icassp, volume 2, pages 1013   1016. ieee,
1999.

[12] x. glorot, a. bordes, and y. bengio. deep sparse recti   er neural networks. in 14th interna-

tional conference on arti   cial intelligence and statistics, pages 315   323, 2011.

[13] a. graves, s. fern  andez, f. gomez, and j. schmidhuber. connectionist temporal classi   cation:
labelling unsegmented sequence data with recurrent neural networks. in icml, pages 369   
376. acm, 2006.

[14] a. graves and n. jaitly. towards end-to-end id103 with recurrent neural net-

works. in icml, 2014.

[15] r. grosse, r. raina, h. kwong, and a. y. ng. shift-invariance sparse coding for audio classi-

   cation. arxiv preprint arxiv:1206.5241, 2012.

10

[16] a. y. hannun, a. l. maas, d. jurafsky, and a. y. ng. first-pass large vocabulary con-
abs/1408.2873, 2014.

tinuous id103 using bi-directional recurrent dnns.
http://arxiv.org/abs/1408.2873.

[17] k. hea   eld, i. pouzyrevsky, j. h. clark, and p. koehn. scalable modi   ed kneser-ney lan-
in proceedings of the 51st annual meeting of the association for

guage model estimation.
computational linguistics, so   a, bulgaria, 8 2013.

[18] g. hinton, l. deng, d. yu, g. dahl, a. mohamed, n. jaitly, a. senior, v. vanhoucke,
p. nguyen, t. sainath, and b. kingsbury. deep neural networks for acoustic modeling in
id103. ieee signal processing magazine, 29(november):82   97, 2012.
[19] g. hinton, n. srivastava, a. krizhevsky, i. sutskever, and r. r. salakhutdinov.

improv-
ing neural networks by preventing co-adaptation of feature detectors. abs/1406.7806, 2014.
http://arxiv.org/abs/1406.7806.

[20] j.-c. junqua. the lombard re   ex and its role on human listeners and automatic speech recog-

nizers. journal of the acoustical society of america, 1:510   524, 1993.

[21] d. r. kincaid, t. c. oppe, and d. m. young. itpackv 2d users guide. 1989.
[22] b. kingsbury, t. sainath, and h. soltau. scalable minimum bayes risk training of deep neural

network acoustic models using distributed hessian-free optimization. in interspeech, 2012.

[23] a. krizhevsky, i. sutskever, and g. hinton. id163 classi   cation with deep convolutional
neural networks. in advances in neural information processing systems 25, pages 1106   1114,
2012.

[24] q. le, m. ranzato, r. monga, m. devin, k. chen, g. corrado, j. dean, and a. ng. building
high-level features using large scale unsupervised learning. in international conference on
machine learning, 2012.

[25] y. lecun, b. boser, j. s. denker, d. henderson, r. e. howard, w. hubbard, and l. d. jackel.
id26 applied to handwritten zip code recognition. neural computation, 1:541   
551, 1989.

[26] y. lecun, f. j. huang, and l. bottou. learning methods for generic object recognition with
invariance to pose and lighting. in id161 and pattern recognition, volume 2, pages
97   104, 2004.

[27] h. lee, p. pham, y. largman, and a. y. ng. unsupervised id171 for audio classi   ca-
tion using convolutional id50. in advances in neural information processing
systems, pages 1096   1104, 2009.

[28] a. l. maas, a. y. hannun, c. t. lengerich, p. qi, d. jurafsky, and a. y. ng.

increasing
deep neural network acoustic model size for large vocabulary continuous id103.
abs/1406.7806, 2014. http://arxiv.org/abs/1406.7806.

[29] a. l. maas, a. y. hannun, and a. y. ng. recti   er nonlinearities improve neural network
in icml workshop on deep learning for audio, speech, and language

acoustic models.
processing, 2013.

[30] a. mohamed, g. dahl, and g. hinton. acoustic modeling using id50. ieee

transactions on audio, speech, and language processing, (99), 2011.

[31] v. nair and g. e. hinton. recti   ed linear units improve restricted id82s. in

27th international conference on machine learning, pages 807   814, 2010.

[32] d. povey, a. ghoshal, g. boulianne, l. burget, o. glembek, k. vesel  y, n. goel, m. han-
nemann, p. motlicek, y. qian, p. schwarz, j. silovsky, and g. stemmer. the kaldi speech
recognition toolkit. in asru, 2011.

[33] r. raina, a. madhavan, and a. ng. large-scale deep unsupervised learning using graphics

processors. in 26th international conference on machine learning, 2009.

[34] s. renals, n. morgan, h. bourlard, m. cohen, and h. franco. connectionist id203
estimators in id48 id103. ieee transactions on speech and audio processing,
2(1):161   174, 1994.

[35] t. sainath, b. kingsbury, a. mohamed, g. dahl, g. saon, h. soltau, t. beran, a. aravkin,
and b. ramabhadran. improvements to deep convolutional neural networks for lvcsr. in
asru, 2013.

11

[36] t. n. sainath, a. rahman mohamed, b. kingsbury, and b. ramabhadran. deep convolutional

neural networks for lvcsr. in icassp, 2013.

[37] b. sapp, a. saxena, and a. y. ng. a fast data collection and augmentation procedure for

object recognition. in aaai twenty-third conference on arti   cial intelligence, 2008.

[38] m. schuster and k. k. paliwal. id182. ieee transactions

on signal processing, 45(11):2673   2681, 1997.

[39] f. seide, g. li, x. chen, and d. yu. feature engineering in context-dependent deep neural

networks for conversational speech transcription. in asru, 2011.

[40] h. soltau, g. saon, and t. n. sainath. joint training of convolutional and non-convolutional

neural networks. in icassp, 2014.

[41] i. sutskever, j. martens, g. dahl, and g. hinton. on the importance of momentum and initial-

ization in deep learning. in 30th international conference on machine learning, 2013.

[42] i. sutskever, o. vinyals, and q. v. le. sequence to sequence learning with neural networks.

2014. http://arxiv.org/abs/1409.3215.

[43] c. szegedy, w. liu, y. jia, p. sermanet, s. reed, d. anguelov, d. erhan, v. vanhoucke, and

a. rabinovich. going deeper with convolutions. 2014.

[44] k. vesely, a. ghoshal, l. burget, and d. povey. sequence-discriminative training of deep

neural networks. in interspeech, 2013.

12

