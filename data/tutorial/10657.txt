proc. of the 8th eur. conf. on python in science (euroscipy 2015)

13

text comparison using word vector representations

and id84

hendrik heuer      

!

6
1
0
2

 
l
u
j
 

2

 
 
]
l
c
.
s
c
[
 
 

1
v
4
3
5
0
0

.

7
0
6
1
:
v
i
x
r
a

abstract   this paper describes a technique to compare large text sources
using word vector representations (id97) and id84 (t-
sne) and how it can be implemented using python. the technique provides a
bird   s-eye view of text sources, e.g. text summaries and their source material,
and enables users to explore text sources like a geographical map. word
vector representations capture many linguistic properties such as gender, tense,
plurality and even semantic concepts like "capital city of". using dimensionality
reduction, a 2d map can be computed where semantically similar words are
close to each other. the technique uses the id97 model from the gensim
python library and id167 from scikit-learn.

index terms   text comparison, topic comparison, id97, id167

1 introduction
when summarizing a large text, only a subset of the available
topics and stories can be taken into account. the decision
which topics to cover is largely editorial. this paper introduces
a tool that assists this editorial process using word vector
representations and id84. it enables a user
to visually identify agreement and disagreement between two
text sources.

there are a variety of different ways to approach the
problem of visualizing the topics present in a text. the simplest
approach is to look at unique words and their occurrences and
visualize the words in a list. topics could also be visualized
using word clouds, where the font size of a word is determined
by the frequency of the word. word clouds have a variety of
shortcomings: they can only visualize small subsets, they focus
on the most frequent words and they do not take synonyms
and semantically similar words into account.

this paper describes a human-computer interaction-inspired
approach of comparing two text sources. the approach yields
a bird   s-eye view of different
including text
summaries and their source material, and enables users to
explore a text source like a geographical map. as similar
words are close to each other, the user can visually identify
clusters of topics that are present in the text.

text sources,

this paper describes a tool, which can be used to visualize
the topics in a single text source as well as to compare different
* corresponding author: hendrikh@kth.se
    aalto university, finland
copyright c    2015 hendrik heuer. this is an open-access article dis-
tributed under
the creative commons attribution li-
cense, which permits unrestricted use, distribution, and reproduction in
any medium, provided the original author and source are credited.
http://creativecommons.org/licenses/by/3.0/

the terms of

text sources. to compare the topics in source a and source
b, three different sets of words can be computed: a set of
unique words in source a, a set of unique words in source b
as well as the intersection set of words both in source a and
b. these three sets are then plotted at the same time. for this,
a colour is assigned to each set of words. this enables the
user to visually compare the different text sources and makes
it possible to see which topics are covered where. the user
can explore the word map and zoom in and out. he or she
can also toggle the visibility, i.e. show and hide, certain word
sets.

the comparison can be used to visualize the difference
between a text summary and its source material. it can also
help to compare wikipedia revisions in regards to the topics
they cover. another possible application is the visualization
of heterogeneous data sources like a list of search queries and
keywords.

the github repository of the tool includes an online demo
[heu15]. the tool can be used to explore the precomputed
topic sets of the game of thrones wikipedia article revisions
from 2013 and 2015. the repository also includes the pre-
computed topic sets for the wikipedia article revisions for the
articles on world war 2, facebook, and the united states of
america.

2 distributional semantic models
the distributional hypothesis by harris states that words
with similar meaning occur in similar contexts [sah05]. this
implies that the meaning of a word can be inferred from its dis-
tribution across contexts. the goal of id65
is to    nd a representation, e.g. a vector, that approximates the
meaning of a word [bru14]. the traditional approach to statis-
tical modeling of language is based on counting frequencies of
occurrences of short word sequences of length up to n and did
not exploit distributed representations [cun15]. distributional
semantics takes word co-occurrence in context windows into
account.

the general

idea behind word space models is to use
distributional statistics to generate high-dimensional vector
spaces, where a word is represented by a context vector
that encodes semantic similarity [sah05]. the representations
are called distributed representations because the features
are not mutually exclusive and because their con   gurations
correspond to the variations seen in the observed data [cun15].

14

proc. of the 8th eur. conf. on python in science (euroscipy 2015)

lecun et al. provide the example of a news story. when the
task is to predict the next word in a news story, the learned
word vectors for tuesday and wednesday will be very similar
as they can be easily replaced by each other when used in a
sentence [cun15].

there are a variety of computational models that implement
the distributional hypothesis,
including id97 [che13],
glove [pen14], dependency-based id27s [lev14]
and random indexing [sah05]. there are a variety of python
implementations of these techniques. id97 is available in
gensim [   reh10]. for glove, the c source code was ported
to python [gau15, kul15]. the dependency-based word em-
beddings by levy and goldberg are implemented in spacy
[hon15]. random indexing is available in an implementation
by joseph turian [tur10].

for this paper, id97 was selected because mikolov et
al. provide 1.4 million pre-trained entity vectors trained on 100
billion words from various news articles in the google news
dataset [che13]. however, other models might perform equally
well for the purpose of text comparison. moreover, custom
word vectors trained on a large domain-speci   c dataset, e.g.
the wikipedia encyclopedia for the wikipedia revision com-
parison, could potentially yield even better results.

2.1 id97
id97 is a tool developed by mikolov, sutskever, chen,
corrado, and dean at google. the two model architectures
in the c tool were made available under an open-source
license [mik13]. gensim provides a python reimplementation
of id97 [   reh10].

word vectors encode semantic meaning and capture many
different degrees of similarity [lev14]. id97 word vectors
can capture linguistic properties such as gender, tense, plural-
ity, and even semantic concepts such as "is the capital city
of". id97 captures domain similarity while other more
dependency-based approaches capture functional similarity.

in the id97 vector space, id202 can be used
to exploit the encoded dimensions of similarity. using this, a
computer system can complete tasks like the scholastic as-
sessment test (sat) analogy quizzes, that measure relational
similarity.

king    man + woman     queen

it works for the superlative:

f astest     f ast + slow     slowest

as well as the past participle:

woken    wake + be     been

it can infer the finnish national sport from the german
national sport.

f ootball     germany + finland     hockey

based on the last name of the current prime minister of the
united kingdom, it identi   es the last name of the german
bundeskanzlerin:

cameron    england + germany     merkel

fig. 1: clusters of semantically similar words emerge when the
id97 vectors are projected down to 2d using id167.

the analogies can also be applied to the national dish of a
country:

haggis    scotland + germany     currywurst

fig. 1 shows the clusters of semantically similar words and
how they form semantic units, which can be easily interpreted
by humans.

3 id84 with id167
t-distributed stochastic neighbour embedding (id167) is a
id84 technique that retains the local struc-
ture of data and that helps to visualize large real-world datasets
with limited computational demands [maa08]. vectors that are
similar in a high-dimensional vector space get represented
by two- or three-dimensional vectors that are close to each
other in the two- or three-dimensional vector space. dissimilar
in the two- or three-
high-dimensional vectors are distant
dimensional vector space. meanwhile,
the global structure
of the data and the presence of clusters at several scales
is revealed. id167 is well-suited for high-dimensional data
that
low-dimensional
manifolds [maa08].

lies on several different, but related,

id167 achieves this by minimizing the kullback-leibler
divergence between the joint probabilities of
the high-
dimensional data and the low-dimensional representation. the
id181 measures the faithfulness with
which a id203 distribution q represents a id203
distribution p by a discrete scalar and equals zero if the
distributions are the same [maa08]. the kullback-leibler
divergence is minimized using the id119 method.
in contrast to other stochastic neighbor embedding methods
that use gaussian distributions, it uses a student t-distribution.

4 implementation
the text comparison tool implements a work   ow that consists
of a python tool for the back-end and a javascript
tool
for the front-end. with the python tool, a text is converted
into a collection of two-dimensional word vectors. these are
visualized using the javascript front-end. with the javascript
front-end, the user can explore the word map and zoom in and
out to investigate both the local and the global structure of the
text sources. the javascript front-end can be published online.
the work   ow of the tool includes the following four steps:

text comparison using word vector representations and id84

15

fig. 2: in the id84 step, the word vectors are
projected down to 2d.

4.1 pre-processing
in the pre-processing step, all sentences are tokenized to
extract single words. the id121 is done using the penn
treebank tokenizer implemented in the natural language
processing toolkit (nltk) for python [bir09]. alternatively,
this could also be achieved with a regular expression.

using a hash map, all words are counted. only unique
words, i.e. the keys of the hash map, are taken into account for
the id84. the 3000 most frequent english
words according to a frequency list collected from wikipedia
are ignored to reduce the amount of data.

4.2 word representations
for all unique non-frequent words, the word representation
vectors are collected from the id97 model from the
gensim python library [   reh10]. each word is represented
by an n-dimensional vector (n=300, informed by the best
accuracy in [mik13] and following the default in [che13]).
from gensim.models import id97

model = id97.load_id97_format(

word_vectors_filename, binary=true

)

for word in words:

if word in model:

print model[word]

4.3 id84
the resulting n-dimensional id97 vectors are projected
down to 2d using the id167 python implementation in scikit-
learn [ped11].

in the id84 step,

the n-dimensional
word vectors are projected down to a two-dimensional space
so that they can be easily visualized in a 2d coordinate system
(see fig. 2).

for the implementation, the id167 implementation in scikit-

learn is used:
from sklearn.manifold import tsne

tsne = tsne(n_components=2)
tsne.fit_transform(word_vectors)

4.4 visualization
after the id84, the vectors are exported
to a json    le. the vectors are visualized using the d3.js
javascript data visualization library [bos12]. using d3.js, an

fig. 3: global clusters of the wikipedia articles on the united states
(left), game of thrones (middle), and world war 2 (right).

interactive map was developed. with this map, the user can
move around and zoom in and out. the colour coding helps to
judge the ratio of dissimilar and similar words. at the global
scale, the map can be used to assess how similar two text
sources are to each other. at the local scale, clusters of similar
words can be explored.

5 results
as with many unsupervised methods, the evaluation can be
dif   cult and the quality of the visualization is hard to quantify.
the goal of this section is, therefore, to introduce relevant use
cases and illustrate how the technique can be applied.

the    ow described in the previous section can be applied
to different revisions of wikipedia articles. for this, a conve-
nience sample of the most popular articles in 2013 from the
english wikipedia was used. for each article, the last revision
from the 31st of december 2013 and the most recent revision
on the 26th of may 2015 were collected. the assumption
was that popular articles will attract suf   cient changes to be
interesting to compare. the list of the most popular wikipedia
articles includes facebook, game of thrones,
the united
states, and world war 2.

the article on game of thrones was deemed especially
illustrative for the task of comparing the topics in a text,
as the storyline of the tv show developed between the two
different snapshot dates as new characters were introduced.
other characters became less relevant and were removed
from the article. the article on world war 2 was especially
interesting as one of the motivations for the topic tool is to
   nd subtle changes in data.

fig. 3 shows how different

the
full group of words on the minimum zoom setting, of the
wikipedia articles on the united states, game of thrones and
world war 2 are.

the global cluster,

i.e.

fig. 4 shows four screenshots of the visualization of the
wikipedia articles on the united states including an overview
and detail views that only show the intersection set of words,
words only present in the 2013 revision of the article, and
words only present in the 2015 revision of the article.

when applied to game of thrones, it is e.g. easy to visually
compare names that were removed since 2013 and that were
added in 2015 (fig. 5). using the online demo available
[heu15],
this technique can be applied to the wikipedia
articles on the united states and world war 2.

the technique can also be applied to visualize the google
search history of an individual. similar words are represented
by similar vectors. thus, terms related to different topics,

16

proc. of the 8th eur. conf. on python in science (euroscipy 2015)

   ndings from the literature review of my master   s thesis into
a useable tool. distributed representations are an active    eld of
research. new    ndings on word, sentence or paragraph vectors
can be easily integrated into the work   ow of the tool.

both the front-end and the back-end of the implementation
were made available on github under gnu general public
license 3 [heu15]. the repository includes the necessary
python code to collect the id97 representations using
gensim, to project them down to 2d using id167 and to output
them as json. the repository also includes the front-end code
to explore the json    le as a geographical map.

the tool can be used in addition to id96 tech-
niques like lda. it enables the comparison of large text
sources at a glance and is aimed at similar text sources with
subtle differences.

fig. 4: topic comparison of the wikipedia article on the united
states. in the top left, all words in both texts are plotted. on the
top right, only the intersection set of words is shown. in the bottom
left, only the words present in the 2013 revision are displayed. in the
bottom right, only the words present in the 2015 revision are shown.

fig. 5: names present in the wikipedia article on game of thrones.
red names were added to the 2015 revision, orange names removed.
white names are present in both revisions.

e.g. technology, philosophy or music, will end up in separate
clusters.

6 conclusion
id97 word vector representations and id167 dimension-
ality reduction can be used to provide a bird   s-eye view
of different text sources, including text summaries and their
source material. this enables users to explore a text source
like a geographical map.

the paper gives an overview of an ongoing investigation of
the usefulness of word vector representations and dimension-
ality reduction in the text and topic comparison context. the
major    aw of this paper is that the introduced text visualization
and text comparison approach is not validated empirically.

as many researchers publish their source code under open
source licenses and as the python community embraces and
supports these publications, it was possible to integrate the

references
[bir09] s. bird, e. klein, and e. loper, natural language processing

with python, 1st ed. o   reilly media, inc., 2009.
[bos12] m. bostock, d3.js - data-driven documents. 2012.
[bru14] e. bruni, n. k. tran, and m. baroni,    multimodal distributional
semantics,    j. artif. int. res., vol. 49, no. 1, pp. 1   47, jan.
2014.

[che13] t. mikolov, k. chen, g. corrado, and j. dean, id97. 2013.
available: https://code.google.com/p/id97/. [accessed 18-
aug-2015].

[cun15] y. lecun, y. bengio, and g. hinton,    deep learning,    nature,

vol. 521, no. 7553, pp. 436   444, may 2015.

[gau15] j. gauthier, glove.py. github, 2015. available: https://github.

com/hans/glove.py. [accessed: 06-aug-2015].

[heu15] h. heuer, topic comparison tool. github, 2015. available:
https://github.com/h10r/topic_comparison_tool. [accessed: 06-
aug-2015].

[hon15] m. honnibal, spacy. 2015. available: https://honnibal.github.io/

spacy/. [accessed: 06-aug-2015].

[kul15] m. kula, glove-python. github, 2015. available: https://github.

com/maciejkula/glove-python. [accessed: 06-aug-2015].

[lev14] o. levy and y. goldberg,    dependency-based word embed-
dings,    in proceedings of the 52nd annual meeting of the
association for computational linguistics (volume 2: short
papers), baltimore, maryland, 2014, pp. 302   308.

[maa08] l. van der maaten and g. hinton,    visualizing data using t-
sne,    journal of machine learning research, vol. 9, no.
2579   2605, p. 85, 2008.

[mik13] t. mikolov, k. chen, g. corrado, and j. dean,    ef   cient esti-
mation of word representations in vector space,    corr, vol.
abs/1301.3781, 2013.

[ped11] f. pedregosa, g. varoquaux, a. gramfort, v. michel, b. thirion,
o. grisel, m. blondel, p. prettenhofer, r. weiss, v. dubourg,
j. vanderplas, a. passos, d. cournapeau, m. brucher, and
e. duchesnay,    scikit-learn: machine learning in python,   
journal of machine learning research, vol. 12, pp. 2825   2830,
2011.

[pen14] j. pennington, r. socher, and c. d. manning,    glove: global
vectors for word representation,    in proceedings of emnlp,
2014.

[sah05] m. sahlgren,    an introduction to random indexing,    in meth-
ods and applications of semantic indexing workshop at the
7th international conference on terminology and knowledge
engineering, tke, 2005, vol. 5.

[online].

available:

[tur10] j. turian, random indexing word representations. github,
https://github.com/turian/
[accessed: 06-aug-

2010.
random-indexing-wordrepresentations.
2015].
  reh  u  rek and p. sojka,    software framework for topic mod-
elling with large corpora,    in proceedings of the lrec 2010
workshop on new challenges for nlp frameworks, valletta,
malta, 2010, pp. 45   50.

[   reh10] r.

