hierarchical character-word models for language identi   cation

aaron jaech1 george mulcaire2 shobhit hathi2 mari ostendorf1 noah a. smith2

1electrical engineering

2computer science & engineering

university of washington, seattle, wa, usa

6
1
0
2

 

g
u
a
0
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
0
3
0
3
0

.

8
0
6
1
:
v
i
x
r
a

ajaech@uw.edu, gmulc@uw.edu, shathi@uw.edu
ostendor@uw.edu, nasmith@cs.washington.edu

abstract

social media messages    brevity and un-
conventional spelling pose a challenge to
language identi   cation. we introduce a
hierarchical model that learns character
and contextualized word-level represen-
tations for language identi   cation. our
method performs well against strong base-
lines, and can also reveal code-switching.

introduction

1
language identi   cation (language id), despite be-
ing described as a solved problem more than ten
years ago (mcnamee, 2005), remains a dif   cult
problem. particularly when working with short
texts, informal styles, or closely related language
pairs, it is an active area of research (gella et al.,
2014; wang et al., 2015; baldwin and lui, 2010).
these dif   cult cases are often found in social me-
dia content. progress on language id in social
media is needed especially since all downstream
tasks, like machine translation or semantic pars-
ing, depend on correct language id.

this paper brings continuous representations
for language data, which have produced new states
of the art for id38 (mikolov et
al., 2010), machine translation (bahdanau et al.,
2014), and other tasks, to language id. we adapt
a hierarchical character-word neural architecture
from kim et al. (2015), demonstrating that it
works well for language id. our model, which we
call c2v2l (   character to vector to language   ) is
hierarchical in the sense that it explicitly builds
a continuous representation for each word from
its character sequence, capturing orthographic and
morphology-related patterns, and then combines
those word level representations in context,    nally
classifying the full word sequence. our model
does not require any special handling of casing or

punctuation nor do we need to remove the urls,
usernames or hashtags, and it is trained end-to-end
using standard procedures.

we demonstrate the model   s state-of-the-art
performance in experiments on two dif   cult lan-
guage id datasets consisting of tweets. this hier-
archical technique works better than previous clas-
si   ers using character or word id165 features
as well as a similar neural model that treats an
entire tweet as a single character sequence. we
   nd further that the model can bene   t from addi-
tional out-of-domain data, unlike much previous
work, and with very little modi   cation can anno-
tate word-level code-switching. we also con   rm
that smoothed character id165 language models
perform very well for language id tasks.

2 model

our model has two main components,
though
they are trained together, end-to-end.1 the    rst,
   char2vec,    applies a convolutional neural net-
work (id98) to a whitespace-delimited word   s
unicode character sequence, providing a word
vector. the second is a bidirectional lstm recur-
rent neural network (id56) that maps a sequence
of such word vectors to a label (a language).

2.1 char2vec
the    rst layer of char2vec embeds characters. an
embedding is learned for each unicode code point
that appears at least twice in the training data, in-
cluding punctuation, emoji, and other symbols. if
c is the set of characters then we let the size of the
character embedding layer be d = (cid:100)log2 |c|(cid:101). (if
each dimension of the character embedding vector
holds just one bit of information then d bits should
be enough to uniquely encode each character.)
the character embedding matrix is q     rd  |c|.
1code will be made available on github after publication.

words are given to the model as a sequence of
characters. when each character in a word of
length l is replaced by its embedding vector we get
a matrix c     rd  (l+2). there are l + 2 columns
in c because padding characters are added to the
left and right of each word.

the char2vec architecture uses two sets of    l-
ter banks. the    rst set is comprised of matrices
hai     rd  3 where i ranges from 1 to n1. the ma-
trix c is narrowly convolved with each of the hai,
a bias term ba is added and an relu non-linearity,
relu(x) = max(0, x), is applied to produce an
output t1 = relu(conv(c, ha) + ba). t1 is
of size n1    l with one row for each of the    lters
and one column for each of the characters in the
input word. since each of the hai is a    lter with a
width of three characters, the columns of t1 each
hold a representation of a character tri-gram. dur-
ing training, we apply dropout on t1 to regularize
the model. the matrix t1 is then convolved with a
second set of    lters hbi     rn1  w where bi ranges
from 1 to 3n2 and n2 controls the number of    l-
ters of each of the possible widths, w = 3, 4, or
5. another convolution and relu non-linearity is
applied to get t2 = relu(conv(t1, hb) + bb).
max-pooling across time is used to create a    x-
sized vector y from t2. the dimension of y is
3n2, corresponding to the number of    lters used.
similar to kim et al. (2015) who use a highway
network after the max-pooling layer, we apply a
residual network layer. both highway and resid-
ual network layers allow values from the previous
layer to pass through unchanged but the residual
layer is preferred in our case because it uses half
as many parameters (he et al., 2015). the resid-
ual network uses a matrix w     r3n2  3n2 and bias
vector b3 to create the vector z = y+fr(y) where
fr(y) = relu(wy + b3). the resulting vector
z is used as a id27 vector in the word-
level lstm portion of the model.

there are three differences between our version
of the model and the one described by kim et al.
(2015). first, we use two layers of convolution in-
stead of just one, inspired by (ling et al., 2015a)
which uses a 2-layer lstm for character model-
ing. second, we use the relu function as a non-
linearity as opposed to the tanh function. relu
has been highly successful in id161 ap-
plications in conjunction with convolutional layers
(jarrett et al., 2009). finally, we use a residual net-
work layer instead of a highway network layer af-

ter the max-pooling step, to reduce the model size.

figure 1: c2v2l model architecture. the model
takes the word    esfuezo,    a misspelling of the
spanish word    esfuerzo,    and maps it to a word
vector via the two id98 layers and the residual
layer. the word vector is then combined with oth-
ers via the lstm, and the words    language pre-
dictions averaged to get a tweet prediction.

it is possible to use bi-lstms instead of con-
volutional layers in char2vec as done by ling et
al. (2015a). we explored this option in prelim-
inary experiments and found that using convolu-
tional layers has several advantages, including a
large improvement in speed for both the forward
and backward pass, many fewer parameters, and
improved language id accuracy.

2.2 sentence-level language id
the sequence of id27 vectors is pro-
cessed by a bi-lstm, which outputs a sequence
of vectors, [v1, v2, v3 . . . vt ] where t is the num-
ber of words in the tweet. all lstm gates are

used as de   ned by sak et al. (2014). dropout is
used as a regularizer on the inputs to the lstm
(zaremba et al., 2014). the output vectors vi are
transformed into id203 distributions over the
set of languages by applying an af   ne transforma-
tion followed by a softmax:

pi = fl(vi) =

(cid:80)t

exp(avi + b)
t=1 exp(avt + b)

(these word-level predictions, we will see in   5.4,
are useful for annotating code-switching.) the
sentence-level prediction ps is then given by an
average of the word-level language predictions:

t(cid:88)

i=1

pi

ps =

1
t

the    nal af   ne transformation can be inter-
preted as a language embedding, where each lan-
guage is represented by a vector of the same di-
mensionality as the lstm outputs. the goal of
the lstm then is (roughly) to maximize the dot
product of each word   s representation with the lan-
guage embedding(s) for that sentence. the only
supervision in the model comes from computing
the loss of sentence-level predictions.

3 tasks and datasets
we consider two datasets: tweetlid and twit-
ter70. summary statistics for each of the datasets
are provided in table 1 including the number of
training examples per dataset.

3.1 tweetlid
the tweetlid dataset (zubiaga et al., 2014)
comes from a language id shared task that fo-
cused on six commonly spoken languages of the
iberian peninsula: spanish, portuguese, catalan,
galician, english, and basque. there are approx-
imately 15,000 tweets in the training data and
25,000 in the test set. the data is unbalanced,
with the majority of examples being in the span-
ish language. the    undetermined    label (   und   ),
comprising 1.4% of the training data, is used for
tweets that use only non-linguistic tokens or be-
long to an outside language. additionally, some
tweets are ambiguous (   amb   ) among a set of
languages (2.3%), or code-switch between lan-
guages (2.4%). the evaluation criteria take into
account all of these factors, requiring prediction of
at least one acceptable language for an ambiguous

tweet or all languages present for a code-switched
tweet. the fact that hundreds of tweets were la-
beled ambiguous or undetermined by annotators
who were native speakers of these languages re-
veals the dif   culty of this task.

for tweets labeled as ambiguous or contain-
ing multiple languages, the training objective dis-
tributes the    true    id203 mass evenly across
each of the label languages, e.g. 50% spanish and
50% catalan.

the tweetlid shared task had two tracks: one
that restricted participants to only use the of   cial
training data and another that was unconstrained,
allowing the use of any external data. there were
12 submissions in the constrained track and 9
in the unconstrained track. perhaps surprisingly,
most participants performed worse on the uncon-
strained track than they did on the constrained one.

as supplementary data for our unconstrained-
track experiments, we collected data from
wikipedia for each of the six languages in the
tweetlid corpus. participants in the tweetlid
shared task also used wikipedia as a data source
for the unconstrained track. we split the text
into 25,000 sentence fragments per language,
with each fragment of length comparable to that
of a tweet. the wikipedia sentence fragments
are easily distinguished from tweets. wikipedia
fragments are more formal and are more likely to
use complex words; for example, one fragment
to
reads    ring homomorphisms are identical
monomorphisms in the category of rings.   
in
contrast, tweets tend to use variable spelling and
more simple words, as in    haaaaallelujaaaaah
http://t.co/axwzunxk06   
and    @justinbieber:
love
http://t.co/xegaxbl6cc
http://t.co/749s6xkkgk awe       . previous work
con   rms that language id is more challenging on
social media text than sentence fragments taken
from more formal text, like wikipedia (carter,
2012). despite the domain mismatch, we    nd in
  5.2 that additional text at training time is useful
for our model.

you mommy

the tweetlid training data is too small to di-
vide into training and validation sets. we created a
tuning set by adding samples taken from twitter70
and from the 2014 workshop on computational
approaches to code switching (solorio et al.,
2014) to the of   cial tweetlid training data. we
used this augmented datset with a 4:1 train/eval

tweetlid

twitter70

tweets

14,991

character vocab

languages

code-switching?

balanced?

956
6
yes
no

58,182
5,796

70

not labeled

roughly

table 1: dataset characteristics.

split for hyperparameter tuning.2

3.2 twitter70
the twitter70 dataset was published by the twit-
ter language engineering team in november
2015.3 the languages come from the afroasiatic,
dravidian, indo-european, sino-tibetan, and tai-
kadai families. each person who wants to use
the data must redownload the tweets using the
twitter api. in between the time when the data
was published and when it is downloaded, some
of the tweets can be lost due to account deletion
or changes in privacy settings. at the time when
the data was published there were approximately
1,500 tweets for each language. we were able
to download 82% of the tweets but the amount
that we could access varied by language with as
many as 1,569 examples for sindhi and as few as
371 and 39 examples for uyghur and oriya, re-
spectively. the median number of tweets per lan-
guage was 1,083. to our knowledge, there are no
published benchmarks on this dataset.

unlike tweetlid, the twitter70 data has no un-
known or ambiguous labels. some tweets do con-
tain code-switching but that is not labeled as such;
a single language is assigned. there is no prede-
   ned test set so we used the last digit of the identi-
   cation number to partition them. identi   ers end-
ing in zero (15%) were used for the test set and
those ending in one (5%) were used for tuning.

when processing the input at

the character
level, the vocabulary for each data source is de-
   ned as the set of unicode code-points that oc-
cur at least twice in the training data: 956 and
5,796 characters for tweetlid and twitter70, re-
2we used this augmented data to tune hyperparameters for
both constrained and unconstrained models. however, after
setting hyperparameters, we trained our constrained model
using only the of   cial training data, and the unconstrained
model using only the training data + wikipedia. thus, no
extra data was used to learn actual model parameters for the
constrained case.

3for clarity, we refer to this data as    twitter70    but it can
be found in the twitter blog post under the name    recall ori-
ented.    see http://t.co/eovqa0t79j

spectively. a small number of languages, such as
mandarin, are responsible for most of the charac-
ters in the twitter70 vocabulary.

one recent work processed the input one byte at
a time instead of by character (gillick et al., 2015).
in early experiments, we found that when using
bytes the model would often make mistakes that
should have been much more obvious from the or-
thography alone. we do not recommend using the
byte sequence for language id.

implementation details

4
4.1 preprocessing
an advantage of the hybrid character-word model
is that only limited preprocessing is required. the
runtime of training char2vec is proportional to
the longest word in a minibatch. the data con-
tains many long and repetitive character sequences
such as    hahahaha...    or    arghhhhh...   . to deal
with these, we restricted any sequence of repeat-
ing characters to at most    ve repetitions where the
repeating pattern can be from one to four charac-
ters. there are many tweets that string together
large numbers of twitter username or hashtags
without spaces between them. these create ex-
tra long    words    that cause our implementation to
use more memory and do extra computation dur-
ing training. to solve this we enforce the con-
straint that there must be a space before any url,
username, or hashtag. to deal with the few re-
maining extra-long character sequences, we force
word breaks in non-space character sequences ev-
ery 40 bytes. this primarily affects languages that
are not space-delimited like chinese. we do not
perform any special handling of casing or punctu-
ation nor do we need to remove the urls, user-
names or hashtags as has been done in previous
work (zubiaga et al., 2014). the same preprocess-
ing is used when training the id165 models.

4.2 training and tuning
training is done using minibatches of size 25 and
a learning rate of 0.001 using the adam method
for optimization (kingma and ba, 2014). for the
twitter70 dataset we used 5% held out data for
tuning and 15% for evaluation. to tune, we trained
15 models with random hyperparameters and se-
lected the one that performed the best on the de-
velopment set. training is done for 80,000 mini-
batches for tweetlid and 100,000 for the twit-
ter70 dataset.

parameter

tweetlid twitter70

1st conv. layer (n1)
2nd conv. layer (n2)

lstm
dropout

total params.

50
93
23
25%
193k

59
108
38
30%
346k

table 2: hyperparameter settings for selected
models.

there are only four hyperparameters to tune for
each model: the number of    lters in the    rst con-
volutional layer, the number of    lters in the sec-
ond convolutional layer, the size of the word-level
lstm vector, and the dropout rate. we designed
our model to have as few hyperparameters as pos-
sible so that we could be more con   dent that our
models were properly tuned. the selected hy-
perparameter values are listed in table 2. there
are 193k parameters in the tweetlid model and
427k in the twitter70 model.

5 experiments

the studies below on language iden-
for all
ti   cation, we compare to two baselines:
i)
langid.py, a popular open-source language id
package, and ii) a classi   er using id165 charac-
ter language models. for the tweetlid dataset,
additional comparisons are included as described
next. in addition, we test our model   s word level
performance on a code-switching dataset.

the    rst baseline, based on the langid.py
package, uses a naive bayes classi   er over byte
id165 features (lui and baldwin, 2012). the
pretrained model distributed with the package is
designed to perform well on a wide range of do-
mains, and achieved high performance on    mi-
croblog messages    (tweets) in the original paper.
langid.py uses feature selection for domain
adaptation and to reduce the model size; thus, re-
training it on in-domain data as we do in this pa-
per does not provide an entirely fair comparison.
however, we include it for its popularity and im-
portance.

the second baseline is built

from charac-
ter id165 language models for each language
it assigns each tweet according to (cid:96)    =
(cid:96).
arg max(cid:96) p(tweet | (cid:96)), i.e., applying bayes    rule
with a uniform class prior (dunning, 1994). for
tweetlid, the rare    und    was handled with a re-
jection model. speci   cally, after (cid:96)    is chosen,

a log likelihood ratio test is applied to decide
whether to reject the decision in favor of the    und   
class, using the language models for (cid:96)    and    und   
with a threshold chosen to optimize f1 on the
dev set. the models were trained using witten-
bell smoothing (bell et al., 1989), but otherwise
the default parameters of the srilm toolkit (stol-
cke, 2002) were used.4 tweetlid model train-
ing ignores tweets labeled as ambiguous or con-
taining multiple languages, and the unconstrained
tweetlid models use a simple interpolation of
tweetlid and wikipedia component models. the
id165 order was chosen to minimize perplex-
ity using 5-fold cross validation, yielding n = 5
for tweetlid and twitter70, and n = 6 for
wikipedia.

note that both of these baselines are generative,
learning separate models for each language.
in
contrast, the neural network models explored here
are trained on all languages, so parameters may be
shared across languages. in particular, for the hier-
archcial model, a character sequence correspond-
ing to a word in more than one language (e.g.
   no    in english and portuguese) has a language-
independent id27.

5.1 tweetlid: constrained track
in the constrained track of the 2014 shared task,
hurtado et al. (2014) attained the highest perfor-
mance (75.2 macroaveraged f1). they used a
set of one-vs-all id166 classi   ers with character
id165 features, and returned all languages for
which the classi   cation con   dence was above a
   xed threshold. this provides our third, strongest
baseline.

in the unconstrained track, the winning team
was gamallo et al. (2014), using a naive bayes
classi   er on word unigrams. they incorporated
wikipedia text to train their model, and were the
only team in the competition whose unconstrained
model outperformed their constrained one. we
compare to their constrained-track result here.

we also consider a version of our model,
   c2l,    which uses only the char2vec component
of c2v2l, treating the entire tweet as a single
word. this tests the value of the intermediate
word representations in c2v2l; c2l has no ex-
plicit word representations. hyperparameter tun-
ing was carried out separately for c2l.

4witten-bell smoothing works well when working with
comparatively small vocabularies such as with character sets.

results the    rst column of table 3 shows the
aggregate results across all labels. our model
achieves the state of the art on this task, surpass-
ing the shared task winner, hurtado et al. (2014).
as expected, c2l fails to match the performance
of c2v2l, demonstrating that there is value in the
hierarchical representations. the performance of
the id165 lm baseline is notably strong, beating
eleven out of the twelve submissions to the tweet-
lid shared task. we also report category-speci   c
performance for our models and baselines in table
3. note that performance on underrepresented cat-
egories such as    glg    and    und    is much lower than
the other categories. the category breakdown is
not available for previously published results.

one important advantage of our model is its
ability to handle special categories of tokens that
would otherwise require special treatment as out-
of-vocabulary symbols, such as urls, hashtags,
emojis, usernames, etc. anecdotally, we observe
that the input gates of the word-level lstm are
less likely to open for these special classes of to-
kens. this is consistent with the hypothesis that
the model has learned to ignore tokens that are
non-informative with respect to language id.

5.2 tweetlid: unconstrained track
we augmented c2v2l   s training data with 25,000
fragments of wikipedia text, weighting the tweet-
lid training examples ten times more strongly.
after training on the combined data, we       ne-
tune    the model on the tweetlid data for 2,000
minibatches. we found this stage necessary to cor-
rect for bias away from the undetermined language
category, which does not occur in the wikipedia
data. the same hyperparameters were used as in
the constrained experiment.

for

the id165 baseline, we interpolated
the language models trained on tweetlid and
wikipedia for each language.
interpolation
weights given to the wikipedia language mod-
els, set by cross-validation, ranged from 16% for
spanish to 39% for galician, the most and least
common labels respectively.

we also compare to the unconstrained-track re-
sults of hurtado et al. (2014) and gamallo et al.
(2014).

results the results for these experiments are
given in table 4. like gamallo et al. (2014), we
see a bene   t from the use of out-of-domain data,
giving a new state of the art on this task as well.

model
hurtado et al. (2014)
gamallo et al. (2014)
id165 lm
c2v2l

f1
   
69.7
   4.5
75.3 +2.7
74.7
   0.3
77.1 +0.9

table 4: f1 scores for the unconstrained data track
of the tweetlid language id task.     measures
change in absolute f1 score from the constrained
condition.

model

f1
langid.py 87.9
5-gram lm 93.8
91.2

c2v2l (ours)

table 6: f1 scores on the twitter70 dataset.

overall the id165 language model does not ben-
e   t from wikipedia, but we observe that if the
undetermined category, which is not observed in
wikipedia training data, is ignored, then there is a
net gain in performance.

in table 5, we show the top seven neighbors to
selected input words based on cosine similarity.
in the left column we see that words with simi-
lar features, such as the presence of    n   t    contrac-
tion, can be grouped together by char2vec. in the
middle column, an out-of-vocabulary username
is supplied and similar usernames are retrieved.
when working with id165 features, removing
usernames is common, but some previous work
demonstrates that they still carry useful informa-
tion for predicting the language of the tweet (jaech
and ostendorf, 2015). the third example,   noite   
(the portuguese word for    night   ), shows that the
id27s are largely invariant to changes
in punctuation and capitalization.

5.3 twitter70
we compare c2v2l to langid.py and the 5-
gram language model on the twitter70 dataset; see
table 6. since this data has not been published on,
these are the only two comparisons we have. al-
though the 5-gram model achieves the best perfor-
mance, the results are virtually identical to those
for c2v2l except for the closely-related bosnian-
croatian language pair.

the lowest performance for all the models is on
closely related language pairs. for example using
the c2v2l model, the f1 score for danish is only
62.7 due to confusion with the mutually intelligble

model
id165 lm
langid.py
hurtado et al. (2014)
gamallo et al. (2014)
c2l
c2v2l

avg. f1
75.0
68.9
75.2
72.6
72.7
76.2

eng
74.8
65.9

spa
94.2
92.0

cat
82.7
72.9

eus
74.8
70.6

por
93.4
89.8

glg
49.5
52.7

und amb
38.9
87.0
83.8
18.8

73.0
75.6

93.8
94.7

82.6
85.3

75.7
82.7

89.4
91.0

57.0
58.5

18.0
27.2

92.1
94.5

table 3: f1 scores on the tweetlid language id task (constrained track), averaged and per language
category (including undetermined and ambiguous). the scores for hurtado et al. (2014) and gamallo et
al. (2014) are as reported in zubiaga et al. (2014); per-language scores are not available.

@maria_sanchez

noite

couldn   t
can   t
   don   t
ain   t
don   t
didn   t
can   t
   rst

0.84 @ainhooa_sanchez
0.80 @ronal2sanchez:
@maria_lsantos
0.80
0.79
@jordi_sanchez
@marialouca?
0.79
@mariona_g9
0.78
0.77
@mario_casas_

0.85 noite
noite.
0.71
noite?
0.68
0.66
noite..
noite,
0.66
noitee
0.65
0.65
noiteee

0.99
0.98
0.98
0.96
0.95
0.92
0.90

table 5: top seven most similar words from the training data and their cosine similarities for inputs
   couldn   t   ,    @maria_sanchez   , and    noite   .

norwegian (van bezooijen et al., 2008). distin-
guishing bosnian from croatian, two varieties of
a single language, is also dif   cult. languages that
have unique orthographies such as greek and ko-
rean are identi   ed with near perfect accuracy by
each of the models.

a potential advantage of the c2v2l model over
the id165 models is the ability to share informa-
tion between related languages.
in figure 2 we
show a id167 plot of the language embedding
vectors taken from the softmax layer of our model
trained with a rank constraint of 10 on the softmax
layer. many of the languages in the plot appear
close to related languages.5 another advantage of
the c2v2l model is that the word-level predic-
tions provide an indication of code-switching, ex-
plored next.

5.4 code-switching
because c2v2l produces language predictions
for every word before making the tweet-level pre-
diction, the same architecture can be used in word-
level analysis of code-switched text, switching be-
tween multiple languages. training a model that
predicts code-switching at the token level requires
5the rank constraint was added for visualization; with-
out it, the model learns language embeddings which are all
roughly orthogonal to each other, making id167 visualiza-
tion dif   cult.

figure 2: id167 plot of language embedding vec-
tors.

a dataset that has language labels at this level.
we used the spanish-english dataset from the
emnlp 2014 shared task on language identitica-
tion in code-switched data (solorio et al., 2014):
a collection of monolingual and code-switched
tweets in english and spanish.

to train and predict at the word level, we sim-

amharicarabicbulgarianbengalitibetanbosniancatalankurdishczechwelshdanishgermandivehigreekenglishspanishestonianbasquepersianfinnishfrenchgujaratihebrewhindihindi   latincroatianhaitianhungarianarmenianindonesianicelandicitalianjapanesegeorgiancambodiankannadakoreanlaolithuanianlatvianmalayalammarathiburmesenepalidutchnorwegianoriyapunjabipolishpashtoportugueseromanianrussiansindhisinhalaslovaksloveneserbianswedishtamilteluguthaitagalogturkishuighurukranianurduvietnamesechinesetaiwanese   400   2000200   400   2000200400ply remove the    nal average over the word pre-
dictions, and calculate the loss as the sum of the
cross-id178 between each word   s prediction and
the corresponding gold label. both the char2vec
and word lstm components of the model archi-
tecture are unaffected, other than retraining their
parameters.6 to tune hyperparameters, we trained
10 models with random parameter settings on 80%
of the data from the training set, and chose the set-
tings from the model that performed best on the re-
maining 20%. we then retrained on the full train-
ing set with these settings.

c2v2l performed well at this task, scoring 95.1
f1 for english (which would have achieved sec-
ond place in the shared task, out of eight entries),
94.1 for spanish (second place), 36.2 for named
entities (fourth place) and 94.2 for other (third
place).7 while our code-switching results are not
quite state-of-the-art, they show that our model
learns to make accurate word-level predictions.

6 related work

the task of language id has a long history both
in the speech domain (house and neuburg, 1977)
and for text (cavnar and trenkle, 1994). previous
work on the text domain mostly uses word or char-
acter id165 features combined with linear classi-
   ers (hurtado et al., 2014; gamallo et al., 2014).
recently published work by radford and gall  
(2016) showed that combining an id165 lan-
guage model classi   er (similar to our id165
baseline) with information from the twitter social
graph improves language id on tweetlid from
74.7 to 76.6 f1 which is only slightly better than
our model   s performance of 76.2.

bergsma et al. (2012) created their own mul-
tilingual twitter dataset and tested both a dis-
criminative model based on id165s plus hand-
crafted features and a compression-based classi-
   er. since the twitter api requires researchers
to re-download tweets based on their identi   ers,
published datasets quickly go out of date when the
tweets in question are no longer available online,
making it dif   cult to compare against prior work.
several other studies have investigated the use
of character sequence models in language pro-

6potentially, both sentence-level and word-level supervi-
sion could be used to train the same model, but we leave that
for future work.

7full results for the 2014 shared task are omitted for
space but can be found at http://emnlp2014.org/
workshops/codeswitch/results.php.

cessing.
these techinques were    rst applied
only to create id27s (dos santos and
zadrozny, 2015; dos santos and guimaraes, 2015)
and then later extended to have the word embed-
dings feed directly into a word level id56. ap-
plications include part-of-speech (pos) tagging
(ling et al., 2015b), id38 (ling et
al., 2015a), id33 (ballesteros et al.,
2015), translation (ling et al., 2015b), and slot    ll-
ing text analysis (jaech et al., 2016). the work is
divided in terms of whether the character sequence
is modeled with an lstm or id98, though virtu-
ally all now leverage the resulting word vectors in
a word-level id56. we are not aware of prior re-
sults comparing lstms and id98s on a speci   c
task, but the reduction in model size compared to
word-only systems is reported to be much higher
for lstm architectures. all analyses report that
the greatest improvements in performance from
character sequence models are for infrequent and
previously unseen words, as expected.

in addition to the 2014 workshop on compu-
tational approaches to code switching (solorio
et al., 2014), word-level language identi   cation
in code-switched text has been studied by mandal
et al. (2015) in the context of id53
and by garrette et al. (2015) in the context of lan-
guage modeling for document transcription. both
used primarily character id165 features. the use
of character representations is well motivated for
code-switching lid since the presence of multi-
ple languages means that one is more likely to en-
counter a previously unseen word.

7 conclusion

we present c2v2l, a hierarchical neural model
for language id that outperforms previous work
on the challenging tweetlid task. we also    nd
that smoothed character id165 language models
can work well as classi   ers for language id for
short texts. without feature engineering, our n-
gram model beat eleven out of the twelve sub-
missions in the tweetlid shared task, and gives
the best reported performance on the twitter70
dataset, where training data for some languages
is quite small. in future work, we plan to further
adapt c2v2l for analyzing code-switching, hav-
ing found that it already offers good performance
without any change to the architecture.

references
[bahdanau et al.2014] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2014. neural machine
translation by jointly learning to align and translate.
arxiv preprint arxiv:1409.0473.

[baldwin and lui2010] timothy baldwin and marco
lui. 2010. language identi   cation: the long and
the short of the matter. in human language tech-
nologies: the 2010 annual conference of the north
american chapter of the association for computa-
tional linguistics, pages 229   237. association for
computational linguistics.

[ballesteros et al.2015] miguel ballesteros, chris dyer,
and noah smith. 2015. improved transition-based
parsing by modeling characters instead of words
in proc. conf. empirical methods
with lstms.
natural language process. (emnlp), pages 349   
359.

[bell et al.1989] timothy bell,

ian h witten, and
john g cleary. 1989. modeling for text compres-
sion. acm computing surveys (csur), 21(4):557   
591.

[bergsma et al.2012] shane bergsma, paul mcnamee,
mossaab bagdouri, clayton fink, and theresa wil-
son. 2012. language identi   cation for creating
in proc. of
language-speci   c twitter collections.
the second workshop on language in social media,
pages 65   74. association for computational lin-
guistics.

[carter2012] simon christopher carter. 2012. explo-
ration and exploitation of multilingual data for sta-
tistical machine translation. ph.d. thesis, univer-
sity of amsterdam, may.

[cavnar and trenkle1994] william b. cavnar

and
john m. trenkle. 1994. id165-based text cate-
in in proc. of sdair-94, 3rd annual
gorization.
symposium on document analysis and information
retrieval, pages 161   175.

[dos santos and guimaraes2015] cicero dos santos
and victor guimaraes. 2015. boosting named en-
tity recognition with neural character embeddings.
in proc. acl named entities workshop, pages
25   33.

[dos santos and zadrozny2015] cicero dos santos and
bianca zadrozny. 2015. learning character-level
representations for part-of-speech tagging. in proc.
int. conf. machine learning (icml).

[dunning1994] ted dunning. 1994. statistical identi-
   cation of language. technical report, computing
research laboratory, new mexico state university,
march.

[garrette et al.2015] dan garrette, hannah alpert-
abrams, taylor berg-kirkpatrick, and dan klein.
2015. unsupervised code-switching for multilin-
in proc.
gual historical document transcription.
conf. north american chapter assoc. for compu-
tational linguistics (naacl).

[gella et al.2014] spandana gella, kalika bali, and
monojit choudhury. 2014.    ye word kis lang ka
hai bhai?   : testing the limits of word level language
int. conf. natural lan-
identi   cation.
guage processing (icon).

in proc.

[gillick et al.2015] dan gillick, cliff brunk, oriol
vinyals, and amarnag subramanya. 2015. mul-
arxiv
tilingual language processing from bytes.
preprint arxiv:1512.00103.

[he et al.2015] kaiming he, xiangyu zhang, shao-
qing ren, and jian sun.
2015. deep resid-
ual learning for image recognition. arxiv preprint
arxiv:1512.03385.

[house and neuburg1977] arthur s house and ed-
ward p neuburg. 1977. toward automatic identi-
   cation of the language of an utterance. the jour-
nal of the acoustical society of america, 62(3):708   
713.

[hurtado et al.2014] llu  s f hurtado, ferran pla, and
mayte gim  nez. 2014. elirf-upv en tweetlid:
identi   caci  n del idioma en twitter. in tweetlid@
sepln.

[jaech and ostendorf2015] aaron jaech and mari os-
tendorf. 2015. what your username says about you.
proc. conf. empirical methods natural language
process. (emnlp).

[jaech et al.2016] aaron jaech, larry heck, and mari
ostendorf. 2016. id20 of recurrent
neural networks for natural language understanding.
in proc. conf. int. speech communication assoc.
(interspeech).

[jarrett et al.2009] kevin jarrett, koray kavukcuoglu,
marc   aurelio ranzato, and yann lecun.
2009.
what is the best multi-stage architecture for ob-
ject recognition? in 2009 ieee 12th international
conference on id161, pages 2146   2153.
ieee.

[kim et al.2015] yoon kim, yacine jernite, david son-
2015. character-
arxiv preprint

tag, and alexander m rush.
aware neural
arxiv:1508.06615.

language models.

[kingma and ba2014] diederik kingma and jimmy
ba. 2014. adam: a method for stochastic opti-
mization. arxiv preprint arxiv:1412.6980.

[gamallo et al.2014] pablo gamallo, marcos garcia,
and susana sotelo. 2014. comparing ranking-based
and naive bayes approaches to language detection
on tweets. in tweetlid@ sepln.

[ling et al.2015a] wang ling, tiago lu  s, lu  s
marujo, ram  n fernandez astudillo, silvio amir,
chris dyer, alan w black, and isabel trancoso.
2015a. finding function in form: compositional

[wang et al.2015] pidong wang, nikhil bojja, and
shivasankari kannan. 2015. a language detection
system for short chats in mobile games. in proceed-
ings of the third international workshop on natural
language processing for social media, pages 20   
28, denver, colorado, june. association for com-
putational linguistics.

[zaremba et al.2014] wojciech

ilya
sutskever, and oriol vinyals.
recur-
rent neural network id173. arxiv preprint
arxiv:1409.2329.

zaremba,
2014.

[zubiaga et al.2014] arkaitz zubiaga,

inaki san vi-
cente, pablo gamallo, jos   ramom pichel campos,
i  aki alegr  a loinaz, nora aranberri, aitzol ezeiza,
and v  ctor fresno-fern  ndez. 2014. overview of
tweetlid: tweet language identi   cation at sepln
2014. in tweetlid@ sepln, pages 1   11.

character models for open vocabulary word repre-
sentation. proc. conf. empirical methods natural
language process. (emnlp).

[ling et al.2015b] wang ling, isabel trancoso, chris
dyer, and alan black.
character-
based id4. arxiv preprint
arxiv:1511.04586v1.

2015b.

[lui and baldwin2012] marco lui and timothy bald-
win. 2012.
langid. py: an off-the-shelf language
identi   cation tool. in proc. of the acl 2012 system
demonstrations, pages 25   30. association for com-
putational linguistics.

[mandal et al.2015] soumik mandal, somnath baner-
jee, sudip kumar naskar, paolo rosso, and sivaji
bandyopadhyay. 2015. adaptive voting in multiple
classi   er systems for word level language identi   ca-
tion. in the working notes in forum for information
retrieval evaluation (fire 2015).

[mcnamee2005] paul mcnamee.

2005. language
identi   cation: a solved problem suitable for un-
j. comput. sci. coll.,
dergraduate instruction.
20(3):94   101, february.

[mikolov et al.2010] tomas mikolov, martin kara     t,
lukas burget, jan cernock`y, and sanjeev khudan-
pur. 2010. recurrent neural network based lan-
guage model. in proc. conf. int. speech communi-
cation assoc. (interspeech), volume 2, page 3.

[radford and gall  2016] will radford and matthias
gall  . 2016. discriminating between similar lan-
arxiv
guages in twitter using label propagation.
preprint arxiv:1607.05408.

[sak et al.2014] hasim sak, andrew w senior, and
fran  oise beaufays.
long short-term
memory recurrent neural network architectures for
in proc. conf. int.
large scale acoustic modeling.
speech communication assoc. (interspeech),
pages 338   342.

2014.

[solorio et al.2014] thamar solorio, elizabeth blair,
suraj maharjan, steven bethard, mona diab, mah-
moud gohneim, abdelati hawwari, fahad al-
ghamdi, julia hirschberg, alison chang, et al.
2014. overview for the    rst shared task on lan-
guage identi   cation in code-switched data. in pro-
ceedings of the first workshop on computational
approaches to code switching, pages 62   72.

[stolcke2002] andreas stolcke.

srilm-an
in proc.
extensible id38 toolkit.
conf. int. speech communication assoc. (inter-
speech), volume 2002, page 2002.

2002.

[van bezooijen et al.2008] ren  e van

bezooijen,
charlotte gooskens, sebastian k  rschner, and anja
2008. linguistic factors of mutual
sch  ppert.
intelligibility in closely related languages.
in
article presented at the symposium on receptive
multilingualism, part ii (organized by jd ten thije),
aila 2008 conference   multilingualism: challenges
and opportunities   , essen, pages 24   29.

