bag of tricks for ef   cient text classi   cation

armand joulin

edouard grave

piotr bojanowski

tomas mikolov

{ajoulin,egrave,bojanowski,tmikolov}@fb.com

facebook ai research

6
1
0
2

 

g
u
a
9

 

 
 
]
l
c
.
s
c
[
 
 

3
v
9
5
7
1
0

.

7
0
6
1
:
v
i
x
r
a

abstract

this paper explores a simple and ef   cient
our ex-
baseline for text classi   cation.
periments show that our
fast
text classi-
   er fasttext is often on par with deep
learning classi   ers in terms of accuracy, and
many orders of magnitude faster for training
and evaluation. we can train fasttext on
more than one billion words in less than ten
minutes using a standard multicore cpu, and
classify half a million sentences among 312k
classes in less than a minute.

1 introduction

text classi   cation is an important task in natural
language processing with many applications, such
as web search, information retrieval, ranking and
classi   cation (deerwester et al., 1990;
document
pang and lee, 2008).
recently, models based
on neural networks have become increasingly
popular
zhang and lecun, 2015;
conneau et al., 2016). while these models achieve
very good performance in practice, they tend to be
relatively slow both at train and test time, limiting
their use on very large datasets.

(kim, 2014;

linear

are
for

classi   ers

meanwhile,

strong baselines

of-
text
ten considered as
(joachims, 1998;
classi   cation
problems
fan et al., 2008).
mccallum and nigam, 1998;
despite their simplicity,
they often obtain state-
of-the-art performances if the right features are
used (wang and manning, 2012).
they also
have the potential
to scale to very large cor-
pus (agarwal et al., 2014).

in this work, we explore ways to scale these
baselines to very large corpus with a large output
space, in the context of text classi   cation. inspired
by the recent work in ef   cient word representation
learning (mikolov et al., 2013; levy et al., 2015),
we show that linear models with a rank constraint
and a fast loss approximation can train on a billion
words within ten minutes, while achieving perfor-
mance on par with the state-of-the-art. we evalu-
ate the quality of our approach fasttext1 on two
different tasks, namely tag prediction and sentiment
analysis.

2 model architecture

a simple and ef   cient baseline for
sentence
classi   cation is to represent sentences as bag of
words (bow) and train a linear classi   er, e.g., a
id28 or an id166 (joachims, 1998;
fan et al., 2008). however,
linear classi   ers do
not share parameters among features and classes.
this possibly limits their generalization in the
context of large output space where some classes
have very few examples.
common solutions
to this problem are to factorize the linear clas-
(schutze, 1992;
si   er
mikolov et al., 2013)
use multilayer
neural
(collobert and weston, 2008;
networks
zhang et al., 2015).

into low rank matrices

or

to

figure 1 shows a simple linear model with rank
constraint. the    rst weight matrix a is a look-up
table over the words. the word representations are
then averaged into a text representation, which is in
turn fed to a linear classi   er. the text representa-

1https://github.com/facebookresearch/fasttext

output

hidden

x1

x2

. . .

xn    1

xn

figure 1: model architecture of fasttext for a sentence with
n ngram features x1, . . . , xn . the features are embedded and
averaged to form the hidden variable.

tion is an hidden variable which can be potentially
be reused. this architecture is similar to the cbow
model of mikolov et al. (2013), where the middle
word is replaced by a label. we use the softmax
function f to compute the id203 distribution
over the prede   ned classes. for a set of n doc-
uments, this leads to minimizing the negative log-
likelihood over the classes:

   

1
n

n

x

n=1

yn log(f (baxn)),

where xn is the normalized bag of features of the n-
th document, yn the label, a and b the weight matri-
ces. this model is trained asynchronously on mul-
tiple cpus using stochastic id119 and a
linearly decaying learning rate.

2.1 hierarchical softmax
when the number of classes is large, computing the
linear classi   er is computationally expensive. more
precisely, the computational complexity is o(kh)
where k is the number of classes and h the di-
mension of the text representation. in order to im-
prove our running time, we use a hierarchical soft-
max (goodman, 2001) based on the huffman cod-
ing tree (mikolov et al., 2013). during training, the
computational complexity drops to o(h log2(k)).

the hierarchical softmax is also advantageous at
test time when searching for the most likely class.
each node is associated with a id203 that is the
id203 of the path from the root to that node. if
the node is at depth l + 1 with parents n1, . . . , nl, its
id203 is

p (nl+1) =

l

y

i=1

p (ni).

this means that the id203 of a node is always
lower than the one of its parent. exploring the tree
with a depth    rst search and tracking the maximum
id203 among the leaves allows us to discard
any branch associated with a small id203. in
practice, we observe a reduction of the complexity
to o(h log2(k)) at test time. this approach is fur-
ther extended to compute the t -top targets at the
cost of o(log(t )), using a binary heap.

2.2 id165 features
bag of words is invariant to word order but taking
explicitly this order into account is often computa-
tionally very expensive.
instead, we use a bag of
id165s as additional features to capture some par-
tial information about the local word order. this
is very ef   cient in practice while achieving compa-
rable results to methods that explicitly use the or-
der (wang and manning, 2012).

we maintain a fast and memory ef   cient
mapping of the id165s by using the hashing
trick (weinberger et al., 2009) with the same hash-
ing function as in mikolov et al. (2011) and 10m
bins if we only used bigrams, and 100m otherwise.

3 experiments

we evaluate fasttext on two different
tasks.
first, we compare it to existing text classifers on the
problem of id31. then, we evaluate
its capacity to scale to large output space on a tag
prediction dataset. note that our model could be im-
plemented with the vowpal wabbit library,2 but we
observe in practice, that our tailored implementation
is at least 2-5   faster.

and

employ

evaluation

baselines. we

and
datasets

3.1 id31
datasets
the
protocol
same
8
the id165s
of zhang et al. (2015). we report
from zhang et al. (2015),
and tfidf baselines
level convolutional
as well as
model
(char-id98) of zhang and lecun (2015),
the character based convolution recurrent net-
work (char-cid56) of (xiao and cho, 2016) and
the very deep convolutional network (vdid98)
we also compare
of conneau et al. (2016).

the character

2using the options --nn, --ngrams and --log multi

model

ag sogou dbp yelp p. yelp f. yah. a. amz. f. amz. p.

bow (zhang et al., 2015)
ngrams (zhang et al., 2015)
ngrams tfidf (zhang et al., 2015)
char-id98 (zhang and lecun, 2015)
char-cid56 (xiao and cho, 2016)
vdid98 (conneau et al., 2016)

88.8
92.0
92.4
87.2
91.4
91.3

92.9
97.1
97.2
95.1
95.2
96.8

96.6
98.6
98.7
98.3
98.6
98.7

92.2
95.6
95.4
94.7
94.5
95.7

58.0
56.3
54.8
62.0
61.8
64.7

68.9
68.5
68.5
71.2
71.7
73.4

54.6
54.3
52.4
59.5
59.2
63.0

90.4
92.0
91.5
94.5
94.1
95.7

fasttext, h = 10
fasttext, h = 10, bigram
table 1: test accuracy [%] on sentiment datasets. fasttext has been run with the same parameters for all the datasets. it has
10 hidden units and we evaluate it with and without bigrams. for char-id98, we show the best reported numbers without data
augmentation.

72.0
72.3

55.8
60.2

91.2
94.6

93.8
95.7

60.4
63.9

98.1
98.6

91.5
92.5

93.9
96.8

zhang and lecun (2015)

conneau et al. (2016)

fasttext

small char-id98 big char-id98

depth=9

depth=17

depth=29

h = 10, bigram

ag

sogou
dbpedia
yelp p.
yelp f.
yah. a.
amz. f.
amz. p.

1h
-
2h
-
-
8h
2d
2d

3h
-
5h
-
-
1d
5d
5d

24m
25m
27m
28m
29m
1h
2h45
2h45

37m
41m
44m
43m
45m
1h33
4h20
4h25

51m
56m
1h
1h09
1h12
2h
7h
7h

1s
7s
2s
3s
4s
5s
9s
10s

table 2: training time for a single epoch on id31 datasets compared to char-id98 and vdid98.

following their evaluation
to tang et al. (2015)
protocol. we report
their main baselines as
well as their two approaches based on recurrent
networks (conv-gid56 and lstm-gid56).

results. we present the results in figure 1. we
use 10 hidden units and run fasttext for 5
epochs with a learning rate selected on a valida-
tion set from {0.05, 0.1, 0.25, 0.5}. on this task,
adding bigram information improves the perfor-
mance by 1-4%. overall our accuracy is slightly
better than char-id98 and char-cid56 and, a bit
worse than vdid98. note that we can increase
the accuracy slightly by using more id165s, for
example with trigrams, the performance on sogou
goes up to 97.1%. finally, figure 3 shows that
our method is competitive with the methods pre-
sented in tang et al. (2015). we tune the hyper-
parameters on the validation set and observe that
using id165s up to 5 leads to the best perfor-
mance. unlike tang et al. (2015), fasttext does
not use pre-trained id27s, which can be
explained the 1% difference in accuracy.

model

yelp   13 yelp   14 yelp   15

imdb

59.8
id166+tf
59.7
id98
conv-gid56
63.7
lstm-gid56 65.1

fasttext

64.2

61.8
61.0
65.5
67.1

66.2

62.4
61.5
66.0
67.6

66.6

40.5
37.5
42.5
45.3

45.2

table 3: comparision with tang et al. (2015). the hyper-
parameters are chosen on the validation set. we report the test
accuracy.

training time. both char-id98 and vdid98 are
trained on a nvidia tesla k40 gpu, while our
models are trained on a cpu using 20 threads. ta-
ble 2 shows that methods using convolutions are sev-
eral orders of magnitude slower than fasttext.
while it is possible to have a 10   speed up for
char-id98 by using more recent cuda implemen-
tations of convolutions, fasttext takes less than
a minute to train on these datasets. the gid56s
method of tang et al. (2015) takes around 12 hours
per epoch on cpu with a single thread. our speed-

input

taiyoucon 2011 digitals: individuals digital pho-
tos from the anime convention taiyoucon 2011 in
mesa, arizona. if you know the model and/or the
character, please comment.

2012 twin cities pride 2012 twin cities pride pa-
rade

beagle enjoys the snowfall

prediction

#cosplay

#minneapolis

#snow

tags

#24mm #anime #animeconvention
#arizona #canon #con #convention
#cos #cosplay #costume #mesa #play
#taiyou #taiyoucon

#2012twincitiesprideparade
neapolis #mn #usa

#min-

#2007 #beagle #hillsboro #january
#maddison #maddy #oregon #snow

christmas

euclid avenue

#christmas

#cameraphone #mobile

#newyorkcity

#cleveland #euclidavenue

table 4: examples from the validation set of yfcc100m dataset obtained with fasttext with 200 hidden units and bigrams.
we show a few correct and incorrect tag predictions.

up compared to neural network based methods in-
creases with the size of the dataset, going up to at
least a 15,000   speed-up.

3.2 tag prediction
dataset and baselines. to test scalability of
our approach,
further evaluation is carried on
the yfcc100m dataset
(thomee et al., 2016)
which consists of almost 100m images with cap-
tions, titles and tags. we focus on predicting the
tags according to the title and caption (we do not
use the images). we remove the words and tags
occurring less than 100 times and split the data
into a train, validation and test set.
the train
set contains 91,188,648 examples (1.5b tokens).
the validation has 930,497 examples and the test
set 543,424. the vocabulary size is 297,141 and
there are 312,116 unique tags. we will release a
script that recreates this dataset so that our numbers
could be reproduced. we report precision at 1.

we consider a frequency-based baseline which
tag. we also com-
predicts the most frequent
pare with tagspace (weston et al., 2014), which is
a tag prediction model similar to ours, but based on
the wsabie model of weston et al. (2011). while
the tagspace model is described using convolutions,
we consider the linear version, which achieves com-
parable performance but is much faster.

results and training time. table 5 presents a
comparison of fasttext and the baselines. we
run fasttext for 5 epochs and compare it
to tagspace for two sizes of the hidden layer, i.e., 50

model

freq. baseline
tagspace, h = 50
tagspace, h = 200

prec@1

2.2
30.1
35.6

running time

train

-

3h8
5h32

test

-
6h
15h

6m40
7m47
10m34
13m38

fasttext, h = 50
31.2
fasttext, h = 50, bigram 36.7
fasttext, h = 200
41.1
fasttext, h = 200, bigram 46.1
table 5: prec@1 on the test set for tag prediction on
yfcc100m. we also report the training time and test time.
test time is reported for a single thread, while training uses 20
threads for both models.

48s
50s
1m29
1m37

and 200. both models achieve a similar perfor-
mance with a small hidden layer, but adding bi-
grams gives us a signi   cant boost in accuracy. at
test time, tagspace needs to compute the scores
for all the classes which makes it relatively slow,
while our fast id136 gives a signi   cant speed-up
when the number of classes is large (more than 300k
here). overall, we are more than an order of mag-
nitude faster to obtain model with a better quality.
the speedup of the test phase is even more signi   -
cant (a 600   speedup). table 4 shows some quali-
tative examples.

4 discussion and conclusion

in this work, we propose a simple baseline method
for text classi   cation. unlike unsupervisedly trained
word vectors from id97, our word features can

be averaged together to form good sentence repre-
sentations. in several tasks, fasttext obtains per-
formance on par with recently proposed methods in-
spired by deep learning, while being much faster.
although deep neural networks have in theory much
higher representational power than shallow models,
it is not clear if simple text classi   cation problems
such as id31 are the right ones to eval-
uate them. we will publish our code so that the
research community can easily build on top of our
work.

acknowledgement. we thank gabriel synnaeve,
herv  e g  egou, jason weston and l  eon bottou for
their help and comments. we also thank alexis con-
neau, duyu tang and zichao zhang for providing us
with information about their methods.

references

[agarwal et al.2014] alekh agarwal, olivier chapelle,
miroslav dud    k, and john langford. 2014. a reliable
effective terascale linear learning system. jmlr.

[collobert and weston2008] ronan collobert and jason
weston. 2008. a uni   ed architecture for natural lan-
guage processing: deep neural networks with multi-
task learning. in icml.

[conneau et al.2016] alexis conneau, holger schwenk,
lo    c barrault, and yann lecun. 2016. very deep con-
volutional networks for natural language processing.
arxiv preprint arxiv:1606.01781.

[deerwester et al.1990] scott deerwester, susan t du-
mais, george w furnas, thomas k landauer, and
richard harshman. 1990. indexing by latent semantic
analysis. journal of the american society for informa-
tion science.

[fan et al.2008] rong-en fan, kai-wei chang, cho-jui
hsieh, xiang-rui wang, and chih-jen lin. 2008. li-
blinear: a library for large linear classi   cation. jmlr.
[goodman2001] joshua goodman. 2001. classes for fast

maximum id178 training. in icassp.

[joachims1998] thorsten joachims. 1998. text catego-
rization with support vector machines: learning with
many relevant features. springer.

[kim2014] yoon kim. 2014. convolutional neural net-

works for sentence classi   cation. in emnlp.

[levy et al.2015] omer levy, yoav goldberg, and ido
dagan. 2015. improving distributional similarity with
lessons learned from id27s. tacl.

[mccallum and nigam1998] andrew mccallum and ka-
mal nigam. 1998. a comparison of event models for

naive bayes text classi   cation. in aaai workshop on
learning for text categorization.

[mikolov et al.2011] tom  a  s mikolov, anoop deoras,
daniel povey, luk  a  s burget, and jan   cernock`y. 2011.
strategies for training large scale neural network lan-
guage models.
in workshop on automatic speech
recognition and understanding. ieee.

[mikolov et al.2013] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013. ef   cient estimation
of word representations in vector space. arxiv preprint
arxiv:1301.3781.

[pang and lee2008] bo pang and lillian lee.

2008.
opinion mining and id31. foundations
and trends in information retrieval.

[schutze1992] hinrich schutze. 1992. dimensions of

meaning. in supercomputing.

[tang et al.2015] duyu tang, bing qin, and ting liu.
2015. document modeling with gated recurrent neural
network for sentiment classi   cation. in emnlp.

[thomee et al.2016] bart thomee, david a shamma,
gerald friedland, benjamin elizalde, karl ni, dou-
glas poland, damian borth, and li-jia li.
2016.
yfcc100m: the new data in multimedia research. vol-
ume 59, pages 64   73. acm.

[wang and manning2012] sida wang and christopher d
manning. 2012. baselines and bigrams: simple, good
sentiment and topic classi   cation. in acl.

[weinberger et al.2009] kilian weinberger, anirban das-
gupta, john langford, alex smola, and josh atten-
berg. 2009. feature hashing for large scale multitask
learning. in icml.

[weston et al.2011] jason weston, samy bengio, and
nicolas usunier. 2011. wsabie: scaling up to large
vocabulary image annotation. in ijcai.

[weston et al.2014] jason weston, sumit chopra, and
keith adams. 2014. #tagspace: semantic embed-
dings from hashtags. in emnlp.

[xiao and cho2016] yijun xiao and kyunghyun cho.
2016. ef   cient character-level document classi   cation
by combining convolution and recurrent layers. arxiv
preprint arxiv:1602.00367.

[zhang and lecun2015] xiang zhang and yann lecun.
2015. text understanding from scratch. arxiv preprint
arxiv:1502.01710.

[zhang et al.2015] xiang zhang, junbo zhao, and yann
lecun. 2015. character-level convolutional networks
for text classi   cation. in nips.

