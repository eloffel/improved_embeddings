siamese cbow: optimizing id27s

for sentence representations

tom kenter1

maarten de rijke1
tom.kenter@uva.nl alborisov@yandex-team.ru derijke@uva.nl

alexey borisov1, 2

1 university of amsterdam, amsterdam

2 yandex, moscow

6
1
0
2

 

n
u
j
 

5
1

 
 
]
l
c
.
s
c
[
 
 

1
v
0
4
6
4
0

.

6
0
6
1
:
v
i
x
r
a

abstract

we present the siamese continuous bag of
words (siamese cbow) model, a neural
network for ef   cient estimation of high-
quality sentence embeddings. averaging
the embeddings of words in a sentence
has proven to be a surprisingly success-
ful and ef   cient way of obtaining sen-
tence embeddings. however, word em-
beddings trained with the methods cur-
rently available are not optimized for the
task of sentence representation, and, thus,
likely to be suboptimal. siamese cbow
handles this problem by training word em-
beddings directly for the purpose of be-
ing averaged. the underlying neural net-
work learns id27s by predict-
ing, from a sentence representation,
its
surrounding sentences. we show the ro-
bustness of the siamese cbow model by
evaluating it on 20 datasets id30 from
a wide variety of sources.

introduction

1
id27s have proven to be bene   cial
in a variety of tasks in nlp such as machine
translation (zou et al., 2013), parsing (chen and
manning, 2014), semantic search (reinanda et al.,
2015; voskarides et al., 2015), and tracking the
meaning of words and concepts over time (kim
et al., 2014; kenter et al., 2015).
it is not evi-
dent, however, how id27s should be
combined to represent larger pieces of text, like
sentences, paragraphs or documents. surprisingly,
simply averaging id27s of all words in
a text has proven to be a strong baseline or feature
across a multitude of tasks (faruqui et al., 2014;
yu et al., 2014; gershman and tenenbaum, 2015;
kenter and de rijke, 2015).

id27s, however, are not optimized
speci   cally for representing sentences. in this pa-
per we present a model for obtaining word embed-
dings that are tailored speci   cally for the task of
averaging them. we do this by directly including
a comparison of sentence embeddings   the aver-
aged embeddings of the words they contain   in
the cost function of our network.

id27s are typically trained in a fast
and scalable way from unlabeled training data. as
the training data is unlabeled, id27s
are usually not task-speci   c. rather, word embed-
dings trained on a large training corpus, like the
ones from (collobert and weston, 2008; mikolov
et al., 2013b) are employed across different tasks
(socher et al., 2012; kenter and de rijke, 2015;
hu et al., 2014). these two qualities   (i) being
trainable from large quantities of unlabeled data
in a reasonable amount of time, and (ii) robust
performance across different tasks   are highly de-
sirable and allow id27s to be used in
many large-scale applications.
in this work we
aim to optimize id27s for sentence
representations in the same manner. we want
to produce general purpose sentence embeddings
that should score robustly across multiple test sets,
and we want to leverage large amounts of unla-
beled training material.

in the id97 algorithm, mikolov et al.
(2013a) construe a supervised training criterion
for obtaining id27s from unsupervised
data, by predicting, for every word, its surround-
ing words. we apply this strategy at the sentence
level, where we aim to predict a sentence from its
adjacent sentences (kiros et al., 2015; hill et al.,
2016). this allows us to use unlabeled training
data, which is easy to obtain; the only restriction
is that documents need to be split into sentences
and that the order between sentences is preserved.
the main research question we address is

whether directly optimizing id27s for
the task of being averaged to produce sentence em-
beddings leads to id27s that are better
suited for this task than id97 does. there-
fore, we test the embeddings in an unsupervised
learning scenario. we use 20 evaluation sets that
stem from a wide variety of sources (newswire,
video descriptions, dictionary descriptions, mi-
croblog posts). furthermore, we analyze the time
complexity of our method and compare it to our
baselines methods.
    we present siamese cbow, an ef   cient neural
network architecture for obtaining high-quality
id27s, directly optimized for sen-
tence representations;
    we evaluate the embeddings produced by
siamese cbow on 20 datasets, originating
from a range of sources (newswire,
tweets,
video descriptions), and demonstrate the robust-
ness of embeddings across different settings.

summarizing, our main contributions are:

2 siamese cbow
we present the siamese continuous bag of words
(cbow) model, a neural network for ef   cient
estimation of high-quality sentence embeddings.
quality should manifest itself in embeddings of
semantically close sentences being similar to one
another, and embeddings of semantically different
sentences being dissimilar. an ef   cient and sur-
prisingly successful way of computing a sentence
embedding is to average the embeddings of its
constituent words. recent work uses pre-trained
id27s (such as id97 and glove)
for this task, which are not optimized for sentence
representations. following these approaches, we
compute sentence embeddings by averaging word
embeddings, but we optimize id27s
directly for the purpose of being averaged.

2.1 training objective
we construct a supervised training criterion by
having our network predict sentences occurring
next to each other in the training data. speci   cally,
for a pair of sentences (si, sj), we de   ne a proba-
bility p(si, sj) that re   ects how likely it is for the
sentences to be adjacent to one another in the train-
ing data. we compute the id203 p(si, sj) us-
ing a softmax function:

p  (si, sj) =

,

(1)

(cid:80)

ecos(s  
i ,s  
j )
s(cid:48)   s ecos(s  

i ,s  (cid:48) )

where s  
x denotes the embedding of sentence sx,
based on the model parameters   .
in theory,
the summation in the denominator of equation 1
should range over all possible sentences s, which
is not feasible in practice. therefore, we replace
the set s with the union of the set s+ of sentences
that occur next to the sentence si in the training
data, and s   , a set of n randomly chosen sen-
tences that are not observed next to the sentence
si in the training data. the id168 of the
network is categorical cross-id178:

l =     (cid:88)

p(si, sj)    log(p  (si, sj)),

sj   {s+     s   }

where p(  ) is the target id203 the network
should produce, and p  (  ) is the prediction it es-
timates based on parameters   , using equation 1.
the target distribution simply is:

(cid:26) 1|s+| ,

0,

if sj     s+
if sj     s   .

p(si, sj) =

i.e.,
if there are 2 positive examples (the sen-
tences preceding and following the input sentence)
and 2 negative examples, the target distribution is
(0.5, 0.5, 0, 0).

2.2 network architecture
figure 1 shows the architecture of the proposed
siamese cbow network. the input is a projec-
tion layer that selects embeddings from a word
embedding matrix w (that is shared across inputs)
for a given input sentence. the id27s
are averaged in the next layer, which yields a sen-
tence representation with the same dimensionality
as the input id27s (the boxes labeled
averagei in figure 1). the cosine similarities be-
tween the sentence representation for sentencei
and the other sentences are calculated in the penul-
timate layer and a softmax is applied in the last
layer to produce the    nal id203 distribution.

2.3 training
the weights in the id27 matrix are the
only trainable parameters in the siamese cbow
network. they are updated using stochastic gradi-
ent descent. the initial learning rate is monoton-
ically decreased proportionally to the number of
training batches.
3 experimental setup
to test the ef   cacy of our siamese network for
producing sentence embeddings we use multiple

figure 1: siamese cbow network architecture. (input projection layer omitted.)

test sets. we use siamese cbow to learn word
embeddings from an unlabeled corpus. for every
sentence pair in the test sets, we compute two sen-
tence representations by averaging the word em-
beddings of each sentence. words that are miss-
ing from the vocabulary and, hence, have no word
embedding, are omitted. the cosine similarity be-
tween the two sentence vectors is produced as a
   nal semantic similarity score.

as we want a clean way to directly evalu-
ate the embeddings on multiple sets we train our
model and the models we compare with on ex-
actly the same training data. we do not com-
pute extra features, perform extra preprocessing
steps or incorporate the embeddings in supervised
training schemes. additional steps like these are
very likely to improve evaluation scores, but they
would obscure our main evaluation purpose in this
paper, which is to directly test the embeddings.

3.1 data

we use the toronto book corpus1 to train word
embeddings. this corpus contains 74,004,228
already pre-processed sentences in total, which
are made up of 1,057,070,918 tokens, originating
from 7,087 unique books. in our experiments, we
consider tokens appearing 5 times or more, which
leads to a vocabulary of 315,643 words.

3.2 baselines

we employ two baselines for producing sentence
embeddings in our experiments. we obtain simi-
larity scores between sentence pairs from the base-
lines in the same way as the ones produced by
siamese cbow, i.e., we calculate the cosine sim-
ilarity between the sentence embeddings they pro-
duce.

1the corpus can be downloaded from http://www.

cs.toronto.edu/  mbweb/; cf. (zhu et al., 2015).

average word

id97 we
embeddings
trained with id97.2 we use both architec-
tures, skipgram and cbow, and apply default
settings: minimum word frequency 5, word
embedding size 300, context window 5, sample
threshold 10-5, no hierarchical softmax, 5 negative
examples.

skip-thought as a second baseline we use the
sentence representations produced by the skip-
thought architecture (kiros et al., 2015).3 skip-
thought is a recently proposed method that learns
sentence representations in a different way from
ours, by using recurrent neural networks. this al-
lows it to take word order into account. as it trains
sentence embeddings from unlabeled data, like we
do, it is a natural baseline to consider.

both methods are trained on the toronto book
corpus, the same corpus used to train siamese
cbow. we should note that as we use skip-
thought vectors as trained by kiros et al. (2015),
skip-thought has an advantage over both id97
and siamese cbow as the vocabulary used for
encoding sentences contains 930,913 words, three
times the size of the vocabulary that we use.

3.3 evaluation
we use 20 semeval datasets from the semeval se-
mantic textual similarity task in 2012, 2013, 2014
and 2015 (agirre et al., 2012; agirre et al., 2013;
agirre et al., 2014; agirre et al., 2015), which con-
sist of sentence pairs from a wide array of sources
(e.g., newswire, tweets, video descriptions) that
have been manually annotated by multiple human
assessors on a 5 point scale (1: semantically unre-
lated, 5: semantically similar). in the ground truth,
the    nal similarity score for every sentence pair is

2the

code

is

available

from https://code.

google.com/archive/p/id97/.

3the code and the trained models can be down-
from https://github.com/ryankiros/

loaded
skip-thoughts/.

id27s sentence iid27s sentence i-1wwwaverageaverageprediction......id27s sentence i+1average...negative example 1average...negative example naverage......cosine layersoftmaxid27matrix wwwwwi,1i,2i,...ii-1i+1neg 1neg ntable 1: results on semeval datasets in terms of pearson   s r (spearman   s r). highest scores, in terms
of pearson   s r, are displayed in bold. siamese cbow runs statistically signi   cantly different from the
id97 cbow baseline runs are marked with a    . see   3.3 for a discussion of the statistical test used.

w2v skipgram

w2v cbow

skip-thought

siamese cbow

dataset
2012
msrpar
msrvid
onwn
smteuroparl
smtnews
2013
fnwn
onwn
smt
headlines
2014
onwn
deft-forum
deft-news
headlines
images
tweet-news
2015
answ-forums
answ-students
belief
headlines
images

.4379    (.4311)
.4522    (.4759)
.6444    (.6475)
.4503    (.5449)
.3902    (.4153)

.2322    (.2235)
.4985    (.5227)
.3312    (.3356)
.6534    (.6516)

.6073    (.6554)
.4082    (.4188)
.5913    (.5754)
.6364    (.6260)
.6497    (.6484)
.7315    (.7128)

.3740 (.3991)
.5213 (.5519)
.6040 (.6476)
.3071 (.5238)
.4487 (.3617)

.3419 (.3521)
.5099 (.5450)
.6320 (.6440)
.3976 (.5310)
.4462 (.3901)

.0560 (.0843)
.5807 (.5829)
.6045 (.6431)
.4203 (.4999)
.3911 (.3628)

.3480 (.3401)
.4745 (.5509)
.1838 (.2843)
.5935 (.6044)

.2736 (.2867)
.5165 (.6008)
.2494 (.2919)
.5730 (.5766)

.3124 (.3511)
.2418 (.2766)
.3378 (.3498)
.3861 (.3909)

.4682 (.5161)
.3736 (.3737)
.4617 (.4762)
.4031 (.3910)
.4257 (.4233)
.5138 (.5297)

.5848 (.6676)
.3193 (.3810)
.5906 (.5678)
.5790 (.5544)
.5131 (.5288)
.6336 (.6544)

.1892 (.1463)
.3233 (.2654)
.2435 (.2635)
.1875 (.0754)
.2454 (.1611)

.6068 (.6887)
.3339 (.3507)
.5737 (.5577)
.5455 (.5095)
.5056 (.5213)
.6897 (.6615)

.1767 (.1294)
.3344 (.2742)
.3277 (.3280)
.1806 (.0765)
.2292 (.1438)

.2784 (.1909)
.2661 (.2068)
.4584 (.3368)
.1248 (.0464)
.2100 (.1220)

.2181 (.1469)
.3671    (.2824)
.4769 (.3184)
.2151    (.0846)
.2560    (.1467)

the mean of the annotator judgements, and as such
can be a    oating point number like 2.685.

the evaluation metric used by semeval, and
hence by us, is pearson   s r. as spearman   s r is
often reported as well, we do so too.

statistical signi   cance to see whether siamese
cbow yields signi   cantly different scores for
the same input sentence pairs from id97
cbow   the method it is theoretically most sim-
ilar to   we compute wilcoxon signed-rank test
statistics between all runs on all evaluation sets.
runs are considered statistically signi   cantly dif-
ferent for p-values < 0.0001.

3.4 network
to comply with results reported in other research
(mikolov et al., 2013b; kusner et al., 2015) we

   x the embedding size to 300 and only consider
words appearing 5 times or more in the training
corpus. we use 2 negative examples (see   4.2.2
for an analysis of different settings). the embed-
dings are initialized randomly, by drawing from
a normal distribution with    = 0.0 and    = 0.01.
the batch size is 100. the initial learning rate    is
0.0001, which we obtain by observing the loss on
the training data. training consists of one epoch.

we use theano (theano development team,
2016) to implement our network.4 we ran our ex-
periments on gpus in the das5 cluster (bal et al.,
2016).

4the code for siamese cbow is available under
an open-source license at https://bitbucket.org/
tomkenter/siamese-cbow.

4 results

in this section we present the results of our ex-
periments, and analyze the stability of siamese
cbow with respect to its (hyper)parameters.

4.1 main experiments
in table 1, the results of siamese cbow on 20
semeval datasets are displayed, together with the
results of the baseline systems. as we can see
from the table, siamese cbow outperforms the
baselines in the majority of cases (14 out of 20).
the very low scores of skip-thought on msrpar
appear to be a glitch, which we will ignore.

it is interesting to see that for the set with
the highest average sentence length (2013 smt,
with 24.7 words per sentence on average) siamese
cbow is very close to skip-thought, the best per-
forming baseline. in terms of lexical term over-
lap, unsurprisingly, all methods have trouble with
the sets with little overlap (2013 fnwn, 2015
answers-forums, which both have 7% lexical over-
lap). it is interesting to see, however, that for the
next two sets (2015 belief and 2012 msrpar, 11%
and 14% overlap respectively) siamese cbow
manages to get the best performance. the high-
est performance on all sets is 0.7315 pearson   s r
of siamese cbow on the 2014 tweet-news set.
this    gure is not very far from the best perform-
ing semeval run that year which has 0.792 pear-
son   s r. this is remarkable as siamese cbow is
completely unsupervised, while the ntnu system
which scored best on this set (lynum et al., 2014)
was optimized using multiple training sets.
in recent work, hill et al. (2016) present fast-
sent, a model similar to ours (see   5 for a more
elaborate discussion); results are not reported for
all evaluation sets we use, and hence, we compare
the results of fastsent and siamese cbow sepa-
rately, in table 2.

fastsent and siamese cbow each outperform
the other on half of the evaluation sets, which
clearly suggests that the differences between the
two methods are complementary.5

4.2 analysis
next, we investigate the stability of siamese
cbow with respect to its hyper-parameters.
in

5the comparison is to be interpreted with caution as it is
not evident what vocabulary was used for the experiments in
(hill et al., 2016); hence, the differences observed here might
simply be due to differences in vocabulary coverage.

table 2: results on semeval 2014 datasets in
terms of pearson   s r (spearman   s r). highest
scores (in pearson   s r) are displayed in bold. fast-
sent results are reprinted from (hill et al., 2016)
where they are reported in two-digit precision.
dataset
onwn
deft-forum
deft-news
headlines
images
tweet-news

siamese cbow
.6073 (.6554)
.4082 (.4188)
.5913 (.5754)
.6364 (.6260)
.6497 (.6484)
.7315 (.7128)

.74 (.70)
.41 (.36)
.58 (.59)
.57 (.59)
.74 (.78)
.63 (.66)

fastsent

particular, we look into stability across iterations,
different numbers of negative examples, and the
dimensionality of the embeddings. other parame-
ter settings are set as reported in   3.4.
4.2.1 performance across iterations
ideally, the optimization criterion of a learning al-
gorithm ranges over the full domain of its loss
function. as discussed in   2, our id168
only observes a sample. as such, convergence is
not guaranteed. regardless, an ideal learning sys-
tem should not    uctuate in terms of performance
relative to the amount of training data it observes,
provided this amount is substantial: as training
proceeds the performance should stabilize.

to see whether the performance of siamese
cbow    uctuates during training we monitor it
during 5 epochs; at every 10,000,000 examples,
and at the end of every epoch. figure 2 displays
the results for all 20 datasets. we observe that
on the majority of datasets the performance shows
very little variation. there are three exceptions.
the performance on the 2014 deft-news dataset
steadily decreases while the performance on 2013
onwn steadily increases, though both seem to
stabilize at the end of epoch 5. the most no-
table exception, however, is 2012 msrvid, where
the score, after an initial increase, drops consis-
tently. this effect might be explained by the fact
that this evaluation set primarily consists of very
short sentences   it has the lowest average sen-
tence length of all set: 6.63 with a standard de-
viation of 1.812. therefore, a 300-dimensional
representation appears too large for this dataset;
this hypothesis is supported by the fact that 200-
dimensional embeddings work slightly better for
this dataset (see figure 4).

figure 2: performance of siamese cbow across 5 iterations.

4.2.2 number of negative examples
in figure 3, the results of siamese cbow in terms
of pearson   s r are plotted for different numbers
of negative examples. we observe that on most
sets, the number of negative examples has lim-
ited effect on the performance of siamese cbow.
choosing a higher number, like 10, occasionally
leads to slightly better performance, e.g., on the
2013 fnwn set. however, a small number like 1
or 2 typically suf   ces, and is sometimes markedly
better, e.g., in the case of the 2015 belief set. as

parameter, setting the number of negative exam-
ples to 1 or 2 should be the default choice.

4.2.3 number of dimensions
figure 4 plots the results of siamese cbow for
different numbers of vector dimensions. we ob-
serve from the    gure that for some sets (most
notably 2014 deft-forum, 2015 answ-forums and
2015 belief) increasing the number of embed-
ding dimensions consistently yields higher perfor-
mance. a dimensionality that is too low (50 or

figure 3: performance of siamese cbow with
different numbers of negative examples.

figure 4: performance of siamese cbow across
number of embedding dimensions.

a high number of negative examples comes at a
substantial computational cost, we conclude from
the    ndings presented here that, although siamese
cbow is robust against different settings of this

100) invariably leads to inferior results. as, sim-
ilar to a higher number of negative examples, a
higher embedding dimension leads to higher com-
putational costs, we conclude from these    ndings

epoch 1 - batch 2epoch 1 - batch 4epoch 1 - batch 6end of epoch 1epoch 2 - batch 2epoch 2 - batch 4epoch 2 - batch 6end of epoch 2epoch 3 - batch 2epoch 3 - batch 4epoch 3 - batch 6end of epoch 3epoch 4 - batch 2epoch 4 - batch 4epoch 4 - batch 6end of epoch 4epoch 5 - batch 2epoch 5 - batch 4epoch 5 - batch 6end of epoch 50.10.20.30.40.50.60.70.8pearson's r2012 msrpar 2012 msrvid 2012 onwn 2012 smteuroparl 2012 smtnews 2013 fnwn 2013 onwn 2013 smt 2013 headlines 2014 onwn 2014 deft-forum 2014 deft-news 2014 headlines 2014 images 2014 tweet-news 2015 answers-forums 2015 answers-students 2015 belief 2015 headlines 2015 images 2012 msrpar2012 msrv 012 onwn2012 smteuroparl2012 smtnews2013 fnwn2013 onwn2013 smt2013 headlines2014 onwn2014 deft-forum2014 deft-news2014 headlines2014 images2014 tweet-news2015 answ-forums2015 answ-students2015 belief2015 headlines2015 images0.00.10.20.30.40.50.60.70.8pearson's rneg 1neg 2neg 5neg 102012 msrpar2012 msrv 012 onwn2012 smteuroparl2012 smtnews2013 fnwn2013 onwn2013 smt2013 headlines2014 onwn2014 deft-forum2014 deft-news2014 headlines2014 images2014 tweet-news2015 answ-forums2015 answ-students2015 belief2015 headlines2015 images0.00.10.20.30.40.50.60.70.8pearson's r50d100d200d300d600d1200dthat a moderate number of dimensions (200 or
300) is to be preferred.

4.3 time complexity
for learning systems, time complexity comes into
play in the training phase and in the prediction
phase. for an end system employing sentence em-
beddings, the complexity at prediction time is the
most crucial factor, which is why we omit an anal-
ysis of training complexity. we focus on compar-
ing the time complexity for generating sentence
embeddings for siamese cbow, and compare it
to the baselines we use.
the complexity of all algorithms we consider is
o(n), i.e., linear in the number of input terms. as
in practice the number of arithmetic operations is
the critical factor in determining computing time,
we will now focus on these.

both id97 and the siamese cbow com-
pute embeddings of a text t = t1, . . . , t|t| by av-
eraging the term embeddings. this requires |t|   1
vector additions, and 1 multiplication by a scalar
value (namely, 1/|t|). the skip-thought model is
a recurrent neural network with gru cells, which
computes a set of equations for every term t in t ,
which we reprint for reference (kiros et al., 2015):

rt =   (wrxt + urht   1)
zt =   (wzxt + uzht   1)
t
h
ht = (1     zt) (cid:12) ht   1 + zt (cid:12) h

= tanh(wxt + u(rt (cid:12) ht   1))

t

as we can see from the formulas, there are 5|t|
vector additions (+/-), 4|t| element-wise multipli-
cations by a vector, 3|t| element-wise operations
and 6|t| id127s, of which the lat-
ter, the id127s, are most expensive.
this considerable difference in numbers of
arithmetic operations is also observed in practice.
we run tests on a single cpu, using identical code
for extracting sentences from the evaluation sets,

table 3: time spent per method on all 20 semeval
datasets, 17,608 sentence pairs, and the average
time spent on a single sentence pair (time in sec-
onds unless indicated otherwise).

siamese cbow (300d)
id97 (300d)
skip-thought (1200d)

20 sets
7.7
7.0
98,804.0

1 pair
0.0004
0.0004
5.6

for every method. the sentence pairs are pre-
sented one by one to the models. we disregard
the time it takes to load models. speedups might
of course be gained for all methods by presenting
the sentences in batches to the models, by com-
puting sentence representations in parallel and by
running code on a gpu. however, as we are inter-
ested in the differences between the systems, we
run the most simple and straightforward scenario.
table 3 lists the number of seconds each method
takes to generate and compare sentence embed-
dings for an input sentence pair. the difference
between id97 and siamese cbow is because
of a different implementation of word lookup.
we conclude from the observations presented
here, together with the results in   4.1, that in a set-
ting where speed at prediction time is pivotal, sim-
ple averaging methods like id97 or siamese
cbow are to be preferred over more involved
methods like skip-thought.

4.4 qualitative analysis
as siamese cbow directly averages word em-
beddings for sentences, we expect it to learn that
words with little semantic impact have a low vec-
tor norm. indeed, we    nd that the 10 words with
lowest vector norm are to, of, and, the, a, in, that,
with, on, and as. at the other side of the spec-
trum we    nd many personal pronouns: had, they,
we, me, my, he, her, you, she, i, which is natural
given that the corpus on which we train consists of
   ction, which typically contains dialogues.

it is interesting to see what the differences in
related words are between siamese cbow and
id97 when trained on the same corpus. for
example, for a cosine similarity > 0.6, the words
related to her in id97 space are she, his, my
and hers. for siamese cbow, the only closely
related word is she. similarly, for the word me,
id97    nds him as most closely related word,
while siamese cbow comes up with i and my.
it seems from these few examples that siamese
cbow learns to be very strict in choosing which
words to relate to each other.

from the results presented in this section we
conclude that optimizing id27s for
the task of being averaged across sentences with
siamese cbow leads to embeddings that are ef-
fective in a large variety of settings. furthermore,
siamese cbow is robust to different parameter
settings and its performance is stable across itera-

tions. lastly, we show that siamese cbow is fast
and ef   cient in computing sentence embeddings at
prediction time.

5 related work

a distinction can be made between supervised
approaches for obtaining representations of short
texts, where a model is optimised for a speci   c
scenario, given a labeled training set, and unsu-
pervised methods, trained on unlabeled data, that
aim to capture short text semantics that are robust
across tasks. in the    rst setting, word vectors are
typically used as features or network initialisations
(kenter and de rijke, 2015; hu et al., 2014; sev-
eryn and moschitti, 2015; yin and sch  utze, 2015).
our work can be classi   ed in the latter category of
unsupervised approaches.

many models related to the one we present here
are used in a multilingual setting (hermann and
blunsom, 2014b; hermann and blunsom, 2014a;
lauly et al., 2014). the key difference between
this work and ours is that in a multilingual setting
the goal is to predict, from a distributed represen-
tation of an input sentence, the same sentence in a
different language, whereas our goals is to predict
surrounding sentences.

wieting et al. (2016) apply a model similar to
ours in a related but different setting where ex-
plicit semantic knowledge is leveraged. as in
our setting, id27s are trained by av-
eraging them. however, unlike in our proposal, a
margin-based id168 is used, which involves
a parameter that has to be tuned. furthermore, to
select negative examples, at every training step,
a computationally expensive comparison is made
between all sentences in the training batch. the
most crucial difference is that a large set of phrase
pairs explicitly marked for semantic similarity has
to be available as training material. obtaining
such high-quality training material is non-trivial,
expensive and limits an approach to settings for
which such material is available. in our work, we
leverage unlabeled training data, of which there is
a virtually unlimited amount.
as detailed in   2, our network predicts a sen-
tence from its neighbouring sentences. the no-
tion of learning from context sentences is also ap-
plied in (kiros et al., 2015), where a recurrent
neural network is employed. our way of aver-
aging the vectors of words contained in a sen-
tence is more similar to the cbow architecture

of id97 (mikolov et al., 2013a), in which all
context word vectors are aggregated to predict the
one omitted word. a crucial difference between
our approach and the id97 cbow approach
is that we compare sentence representations di-
rectly, rather than comparing a (partial) sentence
representation to a word representation. given
the correspondence between id97   s cbow
model and ours, we included it as a baseline in
our experiments in   3. as the skip-gram architec-
ture has proven to be a strong baseline too in many
settings, we include it too.

yih et al. (2011) also propose a siamese ar-
chitecture. short texts are represented by tf-idf
vectors and a linear combination of input weights
is learnt by a two-layer fully connected network,
which is used to represent the input text. the co-
sine similarity between pairs of representations is
computed, but unlike our proposal, the differences
between similarities of a positive and negative sen-
tence pair are combined in a logistic id168.
finally, independently from our work, hill et
al. (2016) also present a log-linear model. rather
than comparing sentence representations to each
other, as we propose, words in one sentence are
compared to the representation of another sen-
tence. as both input and output vectors are
learnt, while we tie the parameters across the en-
tire model, hill et al. (2016)   s model has twice as
many parameters as ours. most importantly, how-
ever, the cost function used in (hill et al., 2016)
is crucially different from ours. as words in sur-
rounding sentences are being compared to a sen-
tence representation, the    nal layer of their net-
work produces a softmax over the entire vocabu-
lary. this is fundamentally different from the    -
nal softmax over cosines between sentence repre-
sentations that we propose. furthermore, the soft-
max over the vocabulary is, obviously, of vocab-
ulary size, and hence grows when bigger vocabu-
laries are used, causing additional computational
cost.
in our case, the size of the softmax is the
number of positive plus negative examples (see
  2.1). when the vocabulary grows, this size is un-
affected.

6 conclusion

we have presented siamese cbow, a neural net-
work architecture that ef   ciently learns word em-
beddings optimized for producing sentence repre-
sentations. the model is trained using only unla-

beled text data. it predicts, from an input sentence
representation, the preceding and following sen-
tence.

we evaluated the model on 20 test sets and
show that in a majority of cases, 14 out of 20,
siamese cbow outperforms a id97 base-
line and a baseline based on the recently pro-
posed skip-thought architecture. as further analy-
sis on various choices of parameters show that the
method is stable across settings, we conclude that
siamese cbow provides a robust way of generat-
ing high-quality sentence representations.

word and sentence embeddings are ubiquitous
and many different ways of using them in su-
pervised tasks have been proposed. it is beyond
the scope of this paper to provide a comprehen-
sive analysis of all supervised methods using word
or sentence embeddings and the effect siamese
cbow would have on them. however, it would
be interesting to see how siamese cbow embed-
dings would affect results in supervised tasks.

lastly, although we evaluated siamese cbow
on sentence pairs, there is no theoretical limitation
restricting it to sentences. it would be interesting
to see how embeddings for larger pieces of texts,
such as documents, would perform in document
id91 or    ltering tasks.

acknowledgments

the authors wish to express their gratitude for the
valuable advice and relevant pointers of the anony-
mous reviewers. many thanks to christophe van
gysel for implementation-related help. this re-
search was supported by ahold, amsterdam data
science, the bloomberg research grant program,
the dutch national program commit, else-
vier, the european community   s seventh frame-
work programme (fp7/2007-2013) under grant
agreement nr 312827 (vox-pol), the esf re-
search network program elias, the royal dutch
academy of sciences (knaw) under the elite
network shifts project, the microsoft research
ph.d. program, the netherlands escience center
under project number 027.012.105, the nether-
lands institute for sound and vision, the nether-
lands organisation for scienti   c research (nwo)
under project nrs 727.011.005, 612.001.116,
hor-11-10, 640.006.013, 612.066.930, ci-14-
25, sh-322-15, 652.002.001, 612.001.551,
the
yahoo faculty research and engagement pro-
gram, and yandex. all content represents the

opinion of the authors, which is not necessarily
shared or endorsed by their respective employers
and/or sponsors.

references
[agirre et al.2012] eneko agirre, mona diab, daniel
cer, and aitor gonzalez-agirre. 2012. semeval-
2012 task 6: a pilot on semantic textual similarity.
in proceedings of the first joint conference on lex-
ical and computational semantics-volume 1: pro-
ceedings of the main conference and the shared task,
and volume 2: proceedings of the sixth interna-
tional workshop on semantic evaluation (semeval
2012), pages 385   393.

[agirre et al.2013] eneko agirre, daniel cer, mona
diab, aitor gonzalez-agirre, and weiwei guo.
2013. sem 2013 shared task: semantic textual sim-
ilarity, including a pilot on typed-similarity. in sec-
ond joint conference on lexical and computational
semantics (*sem), volume 1: proceedings of the
main conference and the shared task (*sem 2013),
pages 32   43.

[agirre et al.2014] eneko agirre, carmen banea,
claire cardie, daniel cer, mona diab, aitor
gonzalez-agirre, weiwei guo, rada mihalcea,
german rigau, and janyce wiebe. 2014. semeval-
2014 task 10: multilingual
semantic textual
in proceedings of the 8th international
similarity.
workshop on semantic evaluation (semeval 2014),
pages 81   91.

[agirre et al.2015] eneko agirre, carmen banea,
claire cardie, daniel cer, mona diab, aitor
gonzalez-agirre, weiwei guo,
i nigo lopez-
gazpio, montse maritxalar, rada mihalcea,
german rigau, larraitz uria, and janyce wiebe.
2015. semeval-2015 task 2: semantic textual simi-
larity, english, spanish and pilot on interpretability.
in proceedings of the 9th international workshop
on semantic evaluation (semeval 2015), pages
252   263.

[bal et al.2016] henri bal, dick epema, cees de laat,
rob van nieuwpoort, john romein, frank seinstra,
cees snoek, and harry wijshoff. 2016. a medium-
scale distributed system for computer science re-
search: infrastructure for the long term. computer,
49(5):54   63.

[chen and manning2014] danqi chen and christo-
pher d manning. 2014. a fast and accurate depen-
dency parser using neural networks. in proceedings
of the conference on empirical methods in natural
language processing (emnlp 2014), pages 740   
750.

[collobert and weston2008] ronan collobert and ja-
son weston. 2008. a uni   ed architecture for natu-
ral language processing: deep neural networks with

in proceedings of the 25th in-
multitask learning.
ternational conference on machine learning (icml
2008), pages 160   167.

[faruqui et al.2014] manaal faruqui, jesse dodge, su-
jay k jauhar, chris dyer, eduard hovy, and noah a.
smith. 2014. retro   tting word vectors to seman-
tic lexicons. in proceedings of the north american
chapter of the association for computational lin-
guistics (naacl 2014).

[gershman and tenenbaum2015] samuel j. gershman
and joshua b. tenenbaum. 2015. phrase similarity
in humans and machines. in proceedings of the 37th
annual conference of the cognitive science society,
pages 776   781.

[hermann and blunsom2014a] karl moritz hermann
and phil blunsom. 2014a. multilingual distributed
in pro-
representations without word alignment.
ceedings of the international conference on learn-
ing representations (iclr 2014).

[hermann and blunsom2014b] karl moritz hermann
and phil blunsom. 2014b. multilingual models for
in proceeed-
compositional distributed semantics.
ings of the 52nd annual meeting of the association
for computational linguistics (acl 2014), pages
58   68.

[hill et al.2016] felix hill, kyunghyun cho, and anna
korhonen. 2016. learning distributed representa-
tions of sentences from unlabelled data. in proceed-
ings of the north american chapter of the associa-
tion for computational linguistics (naacl 2016).

[hu et al.2014] baotian hu, zhengdong lu, hang li,
and qingcai chen.
2014. convolutional neural
network architectures for matching natural language
sentences. in advances in neural information pro-
cessing systems (nips 2014), pages 2042   2050.

[kenter and de rijke2015] tom kenter and maarten
de rijke. 2015. short text similarity with word em-
in proceedings of the 24th acm inter-
beddings.
national on conference on information and knowl-
edge management (cikm 2015), pages 1411   1420.

[kenter et al.2015] tom kenter, melvin wevers, pim
huijnen, and maarten de rijke. 2015. ad hoc mon-
itoring of vocabulary shifts over time. in proceed-
ings of the 24th acm international on conference
on information and knowledge management (cikm
2015), pages 1191   1200.

[kim et al.2014] yoon kim,

i yi-chiu., kentaro
hanaki, darshan hegde, and slav petrov. 2014.
temporal analysis of language through neural lan-
guage models. proceeedings of the 52nd annual
meeting of the association for computational lin-
guistics (acl 2014), pages 61   65.

processing systems 28 (nips 2015), pages 3294   
3302. curran associates, inc.

[kusner et al.2015] matt kusner, yu sun, nicholas
kolkin, and kilian q weinberger. 2015. from word
embeddings to document distances. in proceedings
of the 32nd international conference on machine
learning (icml 2015), pages 957   966.

[lauly et al.2014] stanislas lauly, hugo larochelle,
mitesh khapra, balaraman ravindran, vikas c
raykar, and amrita saha. 2014. an autoencoder
approach to learning bilingual word representations.
in advances in neural information processing sys-
tems (nips 2014), pages 1853   1861.

[lynum et al.2014] andr  e lynum, partha pakray, bj  orn
gamb  ack, and sergio jimenez. 2014. ntnu: mea-
suring semantic similarity with sublexical feature
representations and soft cardinality. in proceedings
of the 8th international workshop on semantic eval-
uation (semeval 2014), pages 448   453.

[mikolov et al.2013a] tomas mikolov, kai chen,
greg s. corrado, and jeffrey dean. 2013a. ef   cient
estimation of word representations in vector space.
arxiv e-prints, 1301.3781.

[mikolov et al.2013b] tomas mikolov, ilya sutskever,
kai chen, greg s. corrado, and jeff dean. 2013b.
distributed representations of words and phrases
in advances in neural
and their compositionality.
information processing systems (nips 2013), pages
3111   3119.

[reinanda et al.2015] ridho reinanda, edgar meij, and
maarten de rijke. 2015. mining, ranking and rec-
in proceedings of the
ommending entity aspects.
38th international acm sigir conference on re-
search and development in information retrieval
(sigir 2015), pages 263   272.

[severyn and moschitti2015] aliaksei severyn

and
alessandro moschitti. 2015. learning to rank short
text pairs with convolutional deep neural networks.
in proceedings of
the 38th international acm
sigir conference on research and development
in information retrieval
(sigir 2015), pages
373   382.

[socher et al.2012] richard socher, brody huval,
christopher d manning, and andrew y ng.
2012. semantic compositionality through recursive
in proceedings of the 2012
matrix-vector spaces.
joint conference on empirical methods in natural
language processing and computational natural
language learning (emnlp-conll 2012), pages
1201   1211.

[kiros et al.2015] ryan kiros, yukun zhu, ruslan r
salakhutdinov, richard zemel, raquel urtasun, an-
tonio torralba, and sanja fidler.
skip-
thought vectors. in advances in neural information

2015.

[theano development team2016] theano

develop-
ment team. 2016. theano: a python framework
for fast computation of mathematical expressions.
arxiv e-prints, abs/1605.02688.

[voskarides et al.2015] nikos voskarides, edgar meij,
manos tsagkias, maarten de rijke, and wouter
weerkamp. 2015. learning to explain entity re-
in proceedings
lationships in id13s.
of the 53rd annual meeting of the association for
computational linguistics and the 7th interna-
tional joint conference on natural language pro-
cessing of the asian federation of natural language
processing (acl-ijcnlp 2015), pages 564   574.

[wieting et al.2016] john wieting, mohit bansal,
kevin gimpel, and karen livescu.
2016. to-
wards universal paraphrastic sentence embeddings.
proceedings of
the international conference on
learning representations (iclr 2016).

[yih et al.2011] wentau yih, kristina toutanova,
john c. platt, and christopher meek.
2011.
learning discriminative projections for text sim-
in proceedings of the fifteenth
ilarity measures.
conference on computational natural language
learning, pages 247   256.

[yin and sch  utze2015] wenpeng yin and hinrich
sch  utze. 2015. convolutional neural network for
in proceedings of the
paraphrase identi   cation.
north american chapter of
the association for
computational linguistics (naacl 2015), pages
901   911.

[yu et al.2014] lei yu, karl moritz hermann, phil
blunsom, and stephen pulman. 2014. deep learn-
in nips 2014
ing for answer sentence selection.
deep learning and representation learning work-
shop.

[zhu et al.2015] yukun zhu, ryan kiros, rich zemel,
ruslan salakhutdinov, raquel urtasun, antonio
torralba, and sanja fidler. 2015. aligning books
and movies: towards story-like visual explanations
by watching movies and reading books. in proceed-
ings of the ieee international conference on com-
puter vision, pages 19   27.

richard

[zou et al.2013] will y. zou,

socher,
daniel m. cer, and christopher d. manning.
2013. bilingual id27s for phrase-based
machine translation. in proceedings of the confer-
ence on empirical methods in natural language
processing (emnlp 2013), pages 1393   1398.

