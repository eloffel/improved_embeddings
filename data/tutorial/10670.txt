increasing the interpretability of recurrent neural networks

using id48

6
1
0
2

 

p
e
s
0
3

 

 
 
]
l
m

.
t
a
t
s
[
 
 

2
v
0
2
3
5
0

.

6
0
6
1
:
v
i
x
r
a

viktoriya krakovna
department of statistics, harvard university
finale doshi-velez
department of computer science, harvard university

vkrakovna@fas.harvard.edu

finale@seas.harvard.edu

abstract

as deep neural networks continue to revolu-
tionize various application domains, there is in-
creasing interest in making these powerful mod-
els more understandable and interpretable, and
narrowing down the causes of good and bad
predictions. we focus on recurrent neural net-
works (id56s), state of the art models in speech
recognition and translation. our approach to
increasing interpretability is by combining an
id56 with a hidden markov model (id48), a
simpler and more transparent model. we explore
various combinations of id56s and id48s: an
id48 trained on lstm states; a hybrid model
where an id48 is trained    rst,
then a small
lstm is given id48 state distributions and
trained to    ll in gaps in the id48   s performance;
and a jointly trained hybrid model. we    nd
that the lstm and id48 learn complementary
information about the features in the text.

1. introduction
following the recent progress in deep learning, researchers
and practitioners of machine learning are recognizing the
importance of understanding and interpreting what goes
on inside these black box models. recurrent neural net-
works have recently revolutionized id103 and
translation, and these powerful models could be very useful
in other applications involving sequential data. however,
adoption has been slow in applications such as health care,
where practitioners are reluctant to let an opaque expert
system make crucial decisions. if we can make the inner
workings of id56s more interpretable, more applications
can bene   t from their power.

2016 icml workshop on human interpretability in machine
learning (whi 2016), new york, ny, usa. copyright by the
author(s).

46

there are several aspects of what makes a model or
algorithm understandable to humans. one aspect is model
complexity or parsimony. another aspect is the ability
to trace back from a prediction or model component to
particularly in   uential features in the data (r  uping, 2006)
(kim et al., 2015). this could be useful for understanding
mistakes made by neural networks, which have human-
level performance most of the time, but can perform very
poorly on seemingly easy cases. for instance, convolu-
tional networks can misclassify adversarial examples with
very high con   dence (nguyen et al., 2015), and made
headlines in 2015 when the image tagging algorithm in
google photos mislabeled african americans as gorillas.
it   s reasonable to expect recurrent networks to fail in
similar ways as well.
it would thus be useful to have
more visibility into where these sorts of errors come from,
i.e. which groups of features contribute to such    awed
predictions.
several promising approaches to interpreting id56s have
been developed recently. che et al. (2015) have approached
this by using gradient boosting trees to predict lstm
output probabilities and explain which features played a
part in the prediction. they do not model the internal
structure of the lstm, but instead approximate the entire
architecture as a black box. karpathy et al. (2016) showed
that in lstm language models, around 10% of the memory
state dimensions can be interpreted with the naked eye by
color-coding the text data with the state values; some of
them track quotes, brackets and other clearly identi   able
aspects of the text. building on these results, we take a
somewhat more systematic approach to looking for inter-
pretable hidden state dimensions, by using id90
to predict individual hidden state dimensions (figure 2).
we visualize the overall dynamics of the hidden states by
coloring the training data with the id116 clusters on the
state vectors (figures 3b, 3d).
we explore several methods for building interpretable mod-
els by combining lstms and id48s. the existing body

are id56s and id48s more interpretable when combined?

of literature mostly focuses on methods that speci   cally
train the id56 to predict id48 states (bourlard & morgan,
1994) or posteriors (maas et al., 2012), referred to as hybrid
or tandem methods respectively. we    rst investigate an
approach that does not require the id56 to be modi   ed
in order to make it understandable, as the interpretation
happens after the fact. here, we model the big picture of
the state changes in the lstm, by extracting the hidden
states and approximating them with a continuous emission
hidden markov model (id48). we then take the reverse
approach where the id48 state probabilities are added
to the output layer of the lstm (see figure 1). the
lstm model can then make use of the information from
the id48, and    ll in the gaps when the id48 is not
performing well, resulting in an lstm with a smaller num-
ber of hidden state dimensions that could be interpreted
individually (figures 3, 4).

2. methods
we compare a hybrid id48-lstm approach with a con-
tinuous emission id48 (trained on the hidden states of
a 2-layer lstm), and a discrete emission id48 (trained
directly on data).

2.1. lstm models

we use a character-level lstm with 1 layer and no
dropout, based on the element-research library. we train
the lstm for 10 epochs, starting with a learning rate of
1, where the learning rate is halved whenever exp(   lt) >
exp(   lt   1) + 1, where lt is the log likelihood score at
epoch t. the l2-norm of the parameter gradient vector is
clipped at a threshold of 5.

2.2. id48

the id48 training procedure is as follows:
initialization of id48 hidden states:

(discrete id48) random multinomial draw for each
time step (i.i.d. across time steps).

(continuous id48) id116 clusters    t on lstm
states, to speed up convergence relative to random
initialization.

at each iteration:

1. sample states using forward filtering backwards

sampling algorithm (ffbs, rao & teh (2013)).

2. sample transition parameters from a multinomial-
dirichlet posterior. let nij be the number of tran-
sitions from state i to state j. then the posterior

figure 1: hybrid id48-lstm algorithm.

distribution of the i-th row of transition matrix t
(corresponding to transitions from state i) is:

ti     mult(nij|ti)dir(ti|  )

where    is the dirichlet hyperparameter.

3. (continuous id48) sample multivariate normal
emission parameters from normal-inverse-wishart
posterior for state i:

  i,   i     n (y|  i,   i)n (  i|0,   i)iw(  i)

(discrete id48) sample the emission parameters
from a multinomial-dirichlet posterior.

evaluation:
we evaluate the methods on how well they predict the next
observation in the validation set. for the id48 models,
we do a forward pass on the validation set (no backward
pass unlike the full ffbs), and compute the id48 state
distribution vector pt for each time step t. then we
compute the predictive likelihood for the next observation
as follows:

n(cid:88)

n(cid:88)

p (yt+1|pt) =

ptxt    txt,xt+1    p (yt+1|xt+1)

xt=1

xt+1=1

where n is the number of hidden states in the id48.

47

are id56s and id48s more interpretable when combined?

figure 2: decision tree predicting an individual hidden state dimension of the hybrid algorithm based on the preceding
characters on the linux data. the hidden state dimensions of the 10-state hybrid mostly track comment characters.

2.3. hybrid models

our main hybrid model is put together sequentially, as
shown in figure 1. we    rst run the discrete id48 on the
data, outputting the hidden state distributions obtained by
the id48   s forward pass, and then add this information
to the architecture in parallel with a 1-layer lstm. the
linear layer between the lstm and the prediction layer is
augmented with an extra column for each id48 state. the
lstm component of this architecture can be smaller than
a standalone lstm, since it only needs to    ll in the gaps
in the id48   s predictions. the id48 is written in python,
and the rest of the architecture is in torch.
we also build a joint hybrid model, where the lstm and
id48 are simultaneously trained in torch. we imple-
mented an id48 torch module, optimized using stochastic
id119 rather than ffbs. similarly to the sequen-
tial hybrid model, we concatenate the lstm outputs with
the id48 state probabilities.

3. experiments
we test the models on several text data sets on the character
level: the penn tree bank (5m characters), and two data
sets used by karpathy et al. (2016), tiny shakespeare (1m
characters) and linux kernel (5m characters). we chose
k = 20 for the continuous id48 based on a pca analysis
of the lstm states, as the    rst 20 components captured
almost all the variance.
table 1 shows the predictive log likelihood of the next

text character for each method. on all text data sets, the
hybrid algorithm performs a bit better than the standalone
lstm with the same lstm state dimension. this effect
gets smaller as we increase the lstm size and the id48
makes less difference to the prediction (though it can
still make a difference in terms of interpretability). the
hybrid algorithm with 20 id48 states does better than
the one with 10 id48 states. the joint hybrid algorithm
outperforms the sequential hybrid on shakespeare data, but
does worse on ptb and linux data, which suggests that the
joint hybrid is more helpful for smaller data sets. the joint
hybrid is an order of magnitude slower than the sequential
hybrid, as the sgd-based id48 is slower to train than the
ffbs-based id48.
we interpret the id48 and lstm states in the hybrid
algorithm with 10 lstm state dimensions and 10 id48
states in figures 3 and 4, showing which features are
identi   ed by the id48 and lstm components. in figures
3a and 3c, we color-code the training data with the 10
id48 states.
in figures 3b and 3d, we apply id116
id91 to the lstm state vectors, and color-code the
training data with the clusters. the id48 and lstm states
pick up on spaces, indentation, and special characters in
the data (such as comment symbols in linux data). we see
some examples where the id48 and lstm complement
each other, such as learning different things about spaces
and comments on linux data, or punctuation on the shake-
speare data. in figure 2, we see that some individual lstm
hidden state dimensions identify similar features, such as
comment symbols in the linux data.

48

are id56s and id48s more interpretable when combined?

(a) hybrid id48 component: colors correspond to 10 id48
states. blue cluster identi   es spaces. green cluster (with white
font) identi   es punctuation and ends of words. purple cluster picks
up on some vowels.

(b) hybrid lstm component: colors correspond to 10 id116
clusters on hidden state vectors. yellow cluster (with red font)
identi   es spaces. grey cluster identi   es punctuation (except
commas). purple cluster    nds some    y    and    o    letters.

figure 3: visualizing id48 and lstm states on shakespeare data for the hybrid with 10 lstm state dimensions and
10 id48 states. the id48 and lstm components learn some complementary features in the text: while both learn to
identify spaces, the lstm does not completely identify punctuation or pick up on vowels, which the id48 has already
done.

(c) hybrid id48 component: colors correspond to 10 id48
states. distinguishes comments and indentation spaces (green with
yellow font) from other spaces (purple). red cluster (with yellow
font) identi   es punctuation and brackets. green cluster (yellow
font) also    nds capitalized variable names.

(d) hybrid lstm component: colors correspond to 10 id116
clusters on hidden state vectors. distinguishes comments, spaces
at beginnings of lines, and spaces between words (red with white
font) from indentation spaces (green with yellow font). opening
brackets are red (yellow font) and closing brackets are green (white
font).

figure 4: visualizing id48 and lstm states on linux data for the hybrid with 10 lstm state dimensions and 10 id48
states. the id48 and lstm components learn some complementary features in the text related to spaces and comments.

49

are id56s and id48s more interpretable when combined?

table 1: predictive loglikelihood comparison on the text
data sets (sorted by validation set performance).

a
t
a
d

e
r
a
e
p
s
e
k
a
h
s

l
e
n
r
e
k
x
u
n
i
l

k
n
a
b
e
e
r
t
n
n
e
p

d
o
h
t
e

m

s
r
e
t
e
m
a
r
a
p

s
m
i
d
m
t
s
l

continuous id48 1300
650
discrete id48
discrete id48
1300
865
lstm
1515
hybrid
2165
hybrid
2130
lstm
joint hybrid
1515
2780
hybrid
3430
hybrid
4445
hybrid
3430
joint hybrid
lstm
3795
5095
hybrid
6510
hybrid
4445
joint hybrid
5860
lstm
hybrid
7160
7160
joint hybrid
1000
discrete id48
2000
discrete id48
1215
lstm
joint hybrid
2215
2215
hybrid
3215
hybrid
4830
joint hybrid
2830
lstm
hybrid
3830
4830
hybrid
4845
lstm
5845
joint hybrid
5845
hybrid
hybrid
6845
9260
joint hybrid
7260
lstm
8260
hybrid
hybrid
9260
continuous id48 1000
500
discrete id48
1000
discrete id48
715
lstm
1215
hybrid
joint hybrid
1215
1715
hybrid
1830
lstm
2330
hybrid
2830
joint hybrid
hybrid
2830
3345
lstm
3845
hybrid
4345
hybrid
6260
joint hybrid
lstm
5260
5760
hybrid
hybrid
6260

5
5
5
10
5
10
10
15
10
15
15
20
15
20
20
20

5
5
5
5
10
10
10
10
15
15
15
15
20
20
20
20
100

5
5
5
5
10
10
10
10
15
15
15
20
20
20
20

s
e
t
a
t
s

m
m
h
20
10
20

10
20

10
10
20
10
10

20
10
10

20
10
10
20

10
10
20
10

10
20

10
10
20
10

10
20
20
10
20

10
10
20

10
10
20

10
20
10

10
20

n
o
i
t
a
d
i
l
a
v
-2.74
-2.69
-2.5
-2.41
-2.3
-2.26
-2.23
-2.21
-2.19
-2.16
-2.13
-2.12
-2.1
-2.07
-2.05
-2.03
-2.03
-2.02
-1.97
-2.76
-2.55
-2.54
-2.35
-2.33
-2.25
-2.18
-2.17
-2.14
-2.07
-2.03
-2.00
-1.96
-1.96
-1.90
-1.88
-1.87
-1.85
-2.58
-2.43
-2.28
-2.22
-2.14
-2.08
-2.06
-1.99
-1.94
-1.94
-1.93
-1.82
-1.81
-1.8
-1.73
-1.72
-1.72
-1.71

g
n
i
n
i
a
r
t
-2.75
-2.68
-2.49
-2.35
-2.26
-2.18
-2.12
-2.18
-2.08
-2.04
-1.95
-2.07
-1.95
-1.92
-1.87
-1.97
-1.83
-1.85
-1.88
-2.7
-2.5
-2.48
-2.26
-2.26
-2.16
-2.08
-2.07
-2.05
-1.97
-1.9
-1.88
-1.84
-1.83
-1.76
-1.73
-1.73
-1.71
-2.58
-2.43
-2.28
-2.22
-2.15
-2.08
-2.07
-1.99
-1.95
-1.95
-1.94
-1.83
-1.82
-1.81
-1.74
-1.73
-1.72
-1.71

50

4. conclusion and future work
hybrid id48-id56 approaches combine the interpretabil-
ity of id48s with the predictive power of id56s. some-
times, a small hybrid model can perform better than a
standalone lstm of the same size. we use visualizations
to show how the lstm and id48 components of the hy-
brid algorithm complement each other in terms of features
learned in the data.

references
bourlard, herve and morgan, nelson.

connectionist
id103: a hybrid approach, volume 247 of
kluwer international series in engineering and computer
science. kluwer academic publishers, boston, 1994.

che, zhengping, purushotham, sanjay, and liu, yan.
distilling knowledge from deep networks with
applications to healthcare domain. neural information
processing systems workshop on machine learning for
healthcare (mlhc), 2015.

johnson,

karpathy, andrej,

justin, and li, fei-fei.
visualizing and understanding recurrent networks.
international conference for learning representations
workshop track, 2016.

kim, been, shah, julie a., and doshi-velez, finale. mind
the gap: a generative approach to interpretable feature
selection and extraction. in cortes, corinna, lawrence,
neil d., lee, daniel d., sugiyama, masashi, and
garnett, roman (eds.), neural information processing
systems (nips), pp. 2260   2268, 2015.

maas, a., le, q., o   neil, t., vinyals, o., nguyen, p., and
ng, a. recurrent neural networks for noise reduction
in proceedings of interspeech,
in robust asr.
2012.

nguyen, anh mai, yosinski, jason, and clune, jeff.
high
deep neural networks are easily fooled:
con   dence predictions for unrecognizable images.
in
ieee conference on id161 and pattern
recognition, cvpr 2015, boston, ma, usa, june 7-12,
2015, pp. 427   436, 2015.

rao, vinayak and teh, yee whye. fast mcmc sampling
journal
for markov jump processes and extensions.
of machine learning research, 14:3207   3232, 2013.
arxiv:1208.4818.

r  uping, stefan. learning interpretable models. phd thesis,

dortmund university of technology, 2006.

