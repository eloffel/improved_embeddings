7
1
0
2

 

p
e
s
2
2

 

 
 
]
e
n
.
s
c
[
 
 

4
v
5
0
3
1
0

.

6
0
6
1
:
v
i
x
r
a

under review as a conference paper at iclr 2017

zoneout: regularizing id56s by randomly
preserving hidden activations

david krueger1,(cid:63), tegan maharaj2,(cid:63), j  nos kram  r2
mohammad pezeshki1 nicolas ballas1, nan rosemary ke2, anirudh goyal1
yoshua bengio1   , aaron courville1   , christopher pal2
1 mila, universit   de montr  al, firstname.lastname@umontreal.ca.
2   cole polytechnique de montr  al, firstname.lastname@polymtl.ca.
(cid:63) equal contributions.    cifar senior fellow.    cifar fellow.

abstract

we propose zoneout, a novel method for regularizing id56s. at each timestep,
zoneout stochastically forces some hidden units to maintain their previous values.
like dropout, zoneout uses random noise to train a pseudo-ensemble, improving
generalization. but by preserving instead of dropping hidden units, gradient
information and state information are more readily propagated through time, as
in feedforward stochastic depth networks. we perform an empirical investigation
of various id56 regularizers, and    nd that zoneout gives signi   cant performance
improvements across tasks. we achieve competitive results with relatively simple
models in character- and word-level language modelling on the id32
and text8 datasets, and combining with recurrent batch id172 (cooijmans
et al., 2016) yields state-of-the-art results on permuted sequential mnist.

1

introduction

regularizing neural nets can signi   cantly improve performance, as indicated by the widespread use
of early stopping, and success of id173 methods such as dropout and its recurrent variants
(hinton et al., 2012; srivastava et al., 2014; zaremba et al., 2014; gal, 2015). in this paper, we
address the issue of id173 in recurrent neural networks (id56s) with a novel method called
zoneout.
id56s sequentially construct    xed-length representations of arbitrary-length sequences by folding
new observations into their hidden state using an input-dependent transition operator. the repeated
application of the same transition operator at the different time steps of the sequence, however, can
make the dynamics of an id56 sensitive to minor perturbations in the hidden state; the transition
dynamics can magnify components of these perturbations exponentially. zoneout aims to improve
id56s    robustness to perturbations in the hidden state in order to regularize transition dynamics.
like dropout, zoneout injects noise during training. but instead of setting some units    activations to
0 as in dropout, zoneout randomly replaces some units    activations with their activations from the
previous timestep. as in dropout, we use the expectation of the random noise at test time. this results
in a simple id173 approach which can be applied through time for any id56 architecture, and
can be conceptually extended to any model whose state varies over time.
compared with dropout, zoneout is appealing because it preserves information    ow forwards and
backwards through the network. this helps combat the vanishing gradient problem (hochreiter, 1991;
bengio et al., 1994), as we observe experimentally.
we also empirically evaluate zoneout on classi   cation using the permuted sequential mnist
dataset, and on language modelling using the id32 and text8 datasets, demonstrat-
ing competitive or state of the art performance across tasks.
in particular, we show that zo-
neout performs competitively with other proposed id173 methods for id56s, includ-
ing recently-proposed dropout variants. code for replicating all experiments can be found at:
http://github.com/teganmaharaj/zoneout

1

under review as a conference paper at iclr 2017

2 related work

2.1 relationship to dropout

zoneout can be seen as a selective application of dropout to some of the nodes in a modi   ed
computational graph, as shown in figure 1. in zoneout, instead of dropping out (being set to 0),
units zone out and are set to their previous value (ht = ht   1). zoneout, like dropout, can be viewed
as a way to train a pseudo-ensemble (bachman et al., 2014), injecting noise using a stochastic
   identity-mask    rather than a zero-mask. we conjecture that identity-masking is more appropriate for
id56s, since it makes it easier for the network to preserve information from previous timesteps going
forward, and facilitates, rather than hinders, the    ow of gradient information going backward, as we
demonstrate experimentally.

figure 1: zoneout as a special case of dropout;   ht is the unit h   s hidden activation for the next time
step (if not zoned out). zoneout can be seen as applying dropout on the hidden state delta,   ht     ht   1.
when this update is dropped out (represented by the dashed line), ht becomes ht   1.

2.2 dropout in id56s

initially successful applications of dropout in id56s (pham et al., 2013; zaremba et al., 2014) only
applied dropout to feed-forward connections (   up the stack   ), and not recurrent connections (   forward
through time   ), but several recent works (semeniuta et al., 2016; moon et al., 2015; gal, 2015)
propose methods that are not limited in this way. bayer et al. (2013) successfully apply fast dropout
(wang & manning, 2013), a deterministic approximation of dropout, to id56s.
semeniuta et al. (2016) apply recurrent dropout to the updates to lstm memory cells (or gru
states), i.e. they drop out the input/update gate in lstm/gru. like zoneout, their approach prevents
the loss of long-term memories built up in the states/cells of grus/lstms, but zoneout does this
by preserving units    activations exactly. this difference is most salient when zoning out the hidden
states (not the memory cells) of an lstm, for which there is no analogue in recurrent dropout.
whereas saturated output gates or output nonlinearities would cause recurrent dropout to suffer from
vanishing gradients (bengio et al., 1994), zoned-out units still propagate gradients effectively in this
situation. furthermore, while the recurrent dropout method is speci   c to lstms and grus, zoneout
generalizes to any model that sequentially builds distributed representations of its input, including
vanilla id56s.
also motivated by preventing memory loss, moon et al. (2015) propose id56drop. this technique
amounts to using the same dropout mask at every timestep, which the authors show results in improved
performance on id103 in their experiments. semeniuta et al. (2016) show, however, that
past states    in   uence vanishes exponentially as a function of dropout id203 when taking the
expectation at test time in id56drop; this is problematic for tasks involving longer-term dependencies.
gal (2015) propose another technique which uses the same mask at each timestep. motivated by
variational id136, they drop out the rows of weight matrices in the input and output embeddings
and lstm gates, instead of dropping units    activations. the proposed variational id56 technique
achieves single-model state-of-the-art test perplexity of 73.4 on word-level language modelling of
id32.

2.3 relationship to stochastic depth

zoneout can also be viewed as a per-unit version of stochastic depth (huang et al., 2016), which
randomly drops entire layers of feed-forward residual networks (resnets (he et al., 2015)). this is

2

under review as a conference paper at iclr 2017

equivalent to zoning out all of the units of a layer at the same time. in a typical id56, there is a new
input at each timestep, causing issues for a naive implementation of stochastic depth. zoning out an
entire layer in an id56 means the input at the corresponding timestep is completely ignored, whereas
zoning out individual units allows the id56 to take each element of its input sequence into account.
we also found that using residual connections in recurrent nets led to instability, presumably due
to the parameter sharing in id56s. concurrent with our work, singh et al. (2016) propose zoneout
for resnets, calling it skipforward. in their experiments, zoneout is outperformed by stochastic
depth, dropout, and their proposed swapout technique, which randomly drops either or both of the
identity or residual connections. unlike singh et al. (2016), we apply zoneout to id56s, and    nd it
outperforms stochastic depth and recurrent dropout.

2.4 selectively updating hidden units

like zoneout, clockwork id56s (koutnik et al., 2014) and hierarchical id56s (hihi & bengio,
1996) update only some units    activations at every timestep, but their updates are periodic, whereas
zoneout   s are stochastic. inspired by clockwork id56s, we experimented with zoneout variants that
target different update rates or schedules for different units, but did not    nd any performance bene   t.
hierarchical multiscale lstms (chung et al., 2016) learn update probabilities for different units
using the straight-through estimator (bengio et al., 2013; courbariaux et al., 2015), and combined
with recently-proposed layer id172 (ba et al., 2016), achieve competitive results on a variety
of tasks. as the authors note, their method can be interpreted as an input-dependent form of adaptive
zoneout.
in recent work, ha et al. (2016) use a hypernetwork to dynamically rescale the row-weights of a
primary lstm network, achieving state-of-the-art 1.21 bpc on character-level id32 when
combined with layer id172 (ba et al., 2016) in a two-layer network. this scaling can be
viewed as an adaptive, differentiable version of the variational lstm (gal, 2015), and could similarly
be used to create an adaptive, differentiable version of zoneout. very recent work conditions zoneout
probabilities on suprisal (a measure of the discrepancy between the predicted and actual state), and
sets a new state of the art on enwik8 (rocki et al., 2016).

3 zoneout and preliminaries

we now explain zoneout in full detail, and compare with other forms of dropout in id56s. we start
by reviewing recurrent neural networks (id56s).

3.1 recurrent neural networks

recurrent neural networks process data x1, x2, . . . , xt sequentially, constructing a corresponding
sequence of representations, h1, h2, . . . , ht . each hidden state is trained (implicitly) to remember
and emphasize all task-relevant aspects of the preceding inputs, and to incorporate new inputs via
a transition operator, t , which converts the present hidden state and input into a new hidden state:
ht = t (ht   1, xt). zoneout modi   es these dynamics by mixing the original transition operator   t
with the identity operator (as opposed to the null operator used in dropout), according to a vector of
bernoulli masks, dt:

zoneout:

t = dt (cid:12)   t + (1     dt) (cid:12) 1

dropout:

t = dt (cid:12)   t + (1     dt) (cid:12) 0

3.2 long short-term memory

in long short-term memory id56s (lstms) (hochreiter & schmidhuber, 1997), the hidden state is
divided into memory cell ct, intended for internal long-term storage, and hidden state ht, used as a
transient representation of state at timestep t. in the most widely used formulation of an lstm (gers
et al., 2000), ct and ht are computed via a set of four    gates   , including the forget gate, ft, which
directly connects ct to the memories of the previous timestep ct   1, via an element-wise multiplication.
large values of the forget gate cause the cell to remember most (not all) of its previous value. the
other gates control the    ow of information in (it, gt) and out (ot) of the cell. each gate has a weight
matrix and bias vector; for example the forget gate has wxf , whf , and bf . for brevity, we will write
these as wx, wh, b.

3

under review as a conference paper at iclr 2017

an lstm is de   ned as follows:

it, ft, ot =   (wxxt + whht   1 + b)

gt = tanh(wxgxt + whght   1 + bg)
ct = ft (cid:12) ct   1 + it (cid:12) gt
ht = ot (cid:12) tanh(ct)

a naive application of dropout in lstms would zero-mask either or both of the memory cells and
hidden states, without changing the computation of the gates (i, f, o, g). dropping memory cells, for
example, changes the computation of ct as follows:

ct = dt (cid:12) (ft (cid:12) ct   1 + it (cid:12) gt)

alternatives abound, however; masks can be applied to any subset of the gates, cells, and states.
semeniuta et al. (2016), for instance, zero-mask the input gate:

ct = (ft (cid:12) ct   1 + dt (cid:12) it (cid:12) gt)

when the input gate is masked like this, there is no additive contribution from the input or hidden
state, and the value of the memory cell simply decays according to the forget gate.

(a)

(b)

figure 2: (a) zoneout, vs (b) the recurrent dropout strategy of (semeniuta et al., 2016) in an lstm.
dashed lines are zero-masked; in zoneout, the corresponding dotted lines are masked with the
corresponding opposite zero-mask. rectangular nodes are embedding layers.

in zoneout, the values of the hidden state and memory cell randomly either maintain their previous
value or are updated as usual. this introduces stochastic identity connections between subsequent
time steps:

ct = dc
ht = dh

t (cid:12) ct   1 + (1     dc
t (cid:12) ht   1 + (1     dh

t ) (cid:12)(cid:0)ft (cid:12) ct   1 + it (cid:12) gt
t ) (cid:12)(cid:0)ot (cid:12) tanh(cid:0)ft (cid:12) ct   1 + it (cid:12) gt

(cid:1)

(cid:1)(cid:1)

we usually use different zoneout masks for cells and hiddens. we also experiment with a variant of
recurrent dropout that reuses the input dropout mask to zoneout the corresponding output gates:

ct = (ft (cid:12) ct   1 + dt (cid:12) it (cid:12) gt)
ht = ((1     dt) (cid:12) ot + dt (cid:12) ot   1) (cid:12) tanh(ct)

the motivation for this variant is to prevent the network from being forced (by the output gate) to
expose a memory cell which has not been updated, and hence may contain misleading information.

4 experiments and discussion

we evaluate zoneout   s performance on the following tasks: (1) character-level language modelling
on the id32 corpus (marcus et al., 1993); (2) word-level language modelling on the
id32 corpus (marcus et al., 1993); (3) character-level language modelling on the text8
corpus (mahoney, 2011); (4) classi   cation of hand-written digits on permuted sequential mnist
(pmnist) (le et al., 2015). we also investigate the gradient    ow to past hidden states, using
pmnist.

4

under review as a conference paper at iclr 2017

4.1 id32 language modelling dataset

the id32 language model corpus contains 1 million words. the model is trained to predict
the next word (evaluated on perplexity) or character (evaluated on bpc: bits per character) in a
sequence. 1

4.1.1 character-level

for the character-level task, we train networks with one layer of 1000 hidden units. we train lstms
with a learning rate of 0.002 on overlapping sequences of 100 in batches of 32, optimize using adam,
and clip gradients with threshold 1. these settings match those used in cooijmans et al. (2016).
we also train grus and tanh-id56s with the same parameters as above, except sequences are non-
overlapping and we use learning rates of 0.001, and 0.0003 for grus and tanh-id56s respectively.
small values (0.1, 0.05) of zoneout signi   cantly improve generalization performance for all three
models. intriguingly, we    nd zoneout increases training time for gru and tanh-id56, but decreases
training time for lstms.
we focus our investigation on lstm units, where the dynamics of zoning out states, cells, or both
provide interesting insight into zoneout   s behaviour. figure 3 shows our exploration of zoneout in
lstms, for various zoneout probabilities of cells and/or hiddens. zoneout on cells with id203
0.5 or zoneout on states with id203 0.05 both outperform the best-performing recurrent dropout
(p = 0.25). combining zc = 0.5 and zh = 0.05 leads to our best-performing model, which achieves
1.27 bpc, competitive with recent state-of-the-art set by (ha et al., 2016). we compare zoneout
to recurrent dropout (for p     {0.05, 0.2, 0.25, 0.5, 0.7}), weight noise (   = 0.075), norm stabilizer
(   = 50) (krueger & memisevic, 2015), and explore stochastic depth (huang et al., 2016) in a
recurrent setting (analagous to zoning out an entire timestep). we also tried a shared-mask variant of
zoneout as used in pmnist experiments, where the same mask is used for both cells and hiddens.
neither stochastic depth or shared-mask zoneout performed as well as separate masks, sampled per
unit. figure 3 shows the best performance achieved with each regularizer, as well as an unregularized
lstm baseline. results are reported in table 1, and learning curves shown in figure 4.
low zoneout probabilities (0.05-0.25) also improve over baseline in grus and tanh-id56s, reducing
bpc from 1.53 to 1.41 for gru and 1.67 to 1.52 for tanh-id56. similarly, low zoneout probabilities
work best on the hidden states of lstms. for memory cells in lstms, however, higher probabilities
(around 0.5) work well, perhaps because large forget-gate values approximate the effect of cells
zoning out. we conjecture that best performance is achieved with zoneout lstms because of the
stability of having both state and cell. the id203 that both will be zoned out is very low, but
having one or the other zoned out carries information from the previous timestep forward, while
having the other react    normally    to new information.

4.1.2 word-level

for the word-level task, we replicate settings from zaremba et al. (2014)   s best single-model perfor-
mance. this network has 2 layers of 1500 units, with weights initialized uniformly [-0.04, +0.04].
the model is trained for 14 epochs with learning rate 1, after which the learning rate is reduced by a
factor of 1.15 after each epoch. gradient norms are clipped at 10.
with no dropout on the non-recurrent connections (i.e. zoneout as the only id173), we do not
achieve competitive results. we did not perform any search over models, and conjecture that the large
model size requires id173 of the feed-forward connections. adding zoneout (zc = 0.25 and
zh = 0.025) on the recurrent connections to the model optimized for dropout on the non-recurrent
connections however, we are able to improve test perplexity from 78.4 to 77.4. we report the best
performance achieved with a given technique in table 1.

1 these metrics are deterministic functions of negative log-likelihood (nll). speci   cally, perplexity is

exponentiated nll, and bpc (id178) is nll divided by the natural logarithm of 2.

5

under review as a conference paper at iclr 2017

figure 3: validation bpc (bits per character) on character-level id32, for different
probabilities of zoneout on cells zc and hidden states zh (left), and comparison of an unregularized
lstm, zoneout zc = 0.5, zh = 0.05, stochastic depth zoneout z = 0.05, recurrent dropout p = 0.25,
norm stabilizer    = 50, and weight noise    = 0.075 (right).

figure 4: training and validation bits-per-character (bpc) comparing lstm id173 methods
on character-level id32 (left) and text8. (right)

4.2 text8

enwik8 is a corpus made from the    rst 109 bytes of wikipedia dumped on mar. 3, 2006. text8 is a
"clean text" version of this corpus; with html tags removed, numbers spelled out, symbols converted
to spaces, all lower-cased. both datasets were created and are hosted by mahoney (2011).
we use a single-layer network of 2000 units, initialized orthogonally, with batch size 128, learning
rate 0.001, and sequence length 180. we optimize with adam (kingma & ba, 2014), clip gradients to
a maximum norm of 1 (pascanu et al., 2012), and use early stopping, again matching the settings of
cooijmans et al. (2016). results are reported in table 1, and figure 4 shows training and validation
learning curves for zoneout (zc = 0.5, zh = 0.05) compared to an unregularized lstm and to
recurrent dropout.

4.3 permuted sequential mnist

in sequential mnist, pixels of an image representing a number [0-9] are presented one at a time,
left to right, top to bottom. the task is to classify the number shown in the image. in pmnist , the
pixels are presented in a (   xed) random order.
we compare recurrent dropout and zoneout to an unregularized lstm baseline. all models have a
single layer of 100 units, and are trained for 150 epochs using rmsprop (tieleman & hinton, 2012)
with a decay rate of 0.5 for the moving average of gradient norms. the learning rate is set to 0.001
and the gradients are clipped to a maximum norm of 1 (pascanu et al., 2012).

6

1611162126epoch1.41.61.82.02.2validation bpczh = 0.5zc = 0.5zh = 0.05zc = 0.05zc = 0.5, zh = 0.5zc = 0.05, zh = 0.05zc = 0.5, zh = 0.051611162126313641465156616671768186epoch1.41.61.82.02.2validation bpcunregularized lstmzoneoutstochastic depthrecurrent dropoutnorm stabilizerweight noise051015202530epochs1.01.52.02.53.03.5bits per characterunregularized lstm (training)unregularized lstm (validation)recurrent dropout (training)recurrent dropout (validation)zoneout (training)zoneout (validation)0510152025303540epochs1.01.52.02.53.0bits per characterunregularized lstm (training)unregularized lstm (validation)recurrent dropout (training)recurrent dropout (validation)zoneout (training)zoneout (validation)under review as a conference paper at iclr 2017

as shown in figure 5 and table 2, zoneout gives a signi   cant performance boost compared to the
lstm baseline and outperforms recurrent dropout (semeniuta et al., 2016), although recurrent batch
id172 (cooijmans et al., 2016) outperforms all three. however, by adding zoneout to the
recurrent batch normalized lstm, we achieve state of the art performance. for this setting, the
zoneout mask is shared between cells and states, and the recurrent dropout id203 and zoneout
probabilities are both set to 0.15.

table 1: validation and test results of different models on the three language modelling tasks. results
are reported for the best-performing settings. performance on char-ptb and text8 is measured in bits-
per-character (bpc); word-ptb is measured in perplexity. for char-ptb and text8 all models are
1-layer unless otherwise noted; for word-ptb all models are 2-layer. results above the line are from
our own implementation and experiments. models below the line are: nr-dropout (non-recurrent
dropout), v-dropout (variational dropout), rbn (recurrent batchnorm), h-lstm+ln (hyperlstm
+ layernorm), 3-hm-lstm+ln (3-layer hierarchical multiscale lstm + layernorm).

model

unregularized lstm

weight noise
norm stabilizer
stochastic depth
recurrent dropout

zoneout

nr-dropout (zaremba et al., 2014)

v-dropout (gal, 2015)

rbn (cooijmans et al., 2016)
h-lstm + ln (ha et al., 2016)

3-hm-lstm + ln (chung et al., 2016)

char-ptb
test
1.356
1.344
1.352
1.343
1.286
1.252

valid
1.466
1.507
1.459
1.432
1.396
1.362

   
   
   

   

1.281

   
   
1.32
1.250
1.24

word-ptb
valid
test
114.5
120.7

   
   
   
91.6
81.4
82.2
   
   
   
   

   
   
   
87.0
77.4
78.4
73.4
   
   
   

text8

valid
1.396
1.356
1.382
1.337
1.386
1.331

   
   
   
   
   

test
1.408
1.367
1.398
1.343
1.401
1.336

   
   
1.36
   
1.29

table 2: error rates on the pmnist digit classi   cation task. zoneout outperforms recurrent dropout,
and sets state of the art when combined with recurrent batch id172.

model

unregularized lstm

recurrent dropout p = 0.5
zoneout zc = zh = 0.15

recurrent batchnorm

recurrent batchnorm & zoneout zc = zh = 0.15

valid
0.092
0.083
0.063

-

0.045

test
0.102
0.075
0.069
0.046
0.041

figure 5: training and validation error rates for an unregularized lstm, recurrent dropout, and
zoneout on the task of permuted sequential mnist digit classi   cation.

7

under review as a conference paper at iclr 2017

4.4 gradient flow

we investigate the hypothesis that identity connections introduced by zoneout facilitate gradient
   ow to earlier timesteps. vanishing gradients are a perennial issue in id56s. as effective as
many techniques are for mitigating vanishing gradients (notably the lstm architecture hochreiter
& schmidhuber (1997)), we can always imagine a longer sequence to train on, or a longer-term
dependence we want to capture.
we compare gradient    ow in an unregularized lstm to zoning out (stochastic identity-mapping)
and dropping out (stochastic zero-mapping) the recurrent connections after one epoch of training on
pmnist. we compute the average gradient norms (cid:107)    l
(cid:107) of loss l with respect to cell activations ct
at each timestep t, and for each method, normalize the average gradient norms by the sum of average
gradient norms for all timesteps.
figure 6 shows that zoneout propagates gradient information to early timesteps much more effectively
than dropout on the recurrent connections, and even more effectively than an unregularized lstm.
the same effect was observed for hidden states ht.

   ct

figure 6: normalized(cid:80)(cid:107)    l

   ct

zoneout (zc = 0.5), dropout (zc = 0.5), and an unregularized lstm on one epoch of pmnist

(cid:107) of loss l with respect to cell activations ct at each timestep t for

.

5 conclusion

we have introduced zoneout, a novel and simple regularizer for id56s, which stochastically preserves
hidden units    activations. zoneout improves performance across tasks, outperforming many alterna-
tive regularizers to achieve results competitive with state of the art on the id32 and text8
datasets, and state of the art results on pmnist. while searching over zoneout probabilites allows
us to tune zoneout to each task, low zoneout probabilities (0.05 - 0.2) on states reliably improve
performance of existing models.
we perform no hyperparameter search to achieve these results, simply using settings from the previous
state of the art. results on pmnist and word-level id32 suggest that zoneout works
well in combination with other regularizers, such as recurrent batch id172, and dropout
on feedforward/embedding layers. we conjecture that the bene   ts of zoneout arise from two main
factors: (1) introducing stochasticity makes the network more robust to changes in the hidden state;
(2) the identity connections improve the    ow of information forward and backward through the
network.

acknowledgments

we are grateful to hugo larochelle, jan chorowski, and students at mila, especially   a  glar
g  l  ehre, marcin moczulski, chiheb trabelsi, and christopher beckham, for helpful feedback
and discussions. we thank the developers of theano (theano development team, 2016), fuel,
and blocks (van merri  nboer et al., 2015). we acknowledge the computing resources provided by
computecanada and calculquebec. we also thank ibm and samsung for their support. we would
also like to acknowledge the work of pranav shyam on learning id56 hierarchies. this research was
developed with funding from the defense advanced research projects agency (darpa) and the air

8

under review as a conference paper at iclr 2017

force research laboratory (afrl). the views, opinions and/or    ndings expressed are those of the
authors and should not be interpreted as representing the of   cial views or policies of the department
of defense or the u.s. government.

references
lei jimmy ba, ryan kiros, and geoffrey e. hinton. layer id172. corr, abs/1607.06450,

2016. url http://arxiv.org/abs/1607.06450.

philip bachman, ouais alsharif, and doina precup. learning with pseudo-ensembles. in advances

in neural information processing systems, pp. 3365   3373, 2014.

j. bayer, c. osendorfer, d. korhammer, n. chen, s. urban, and p. van der smagt. on fast dropout

and its applicability to recurrent networks. arxiv e-prints, november 2013.

yoshua bengio, patrice simard, and paolo frasconi. learning long-term dependencies with gradient

descent is dif   cult. neural networks, ieee transactions on, 5(2):157   166, 1994.

yoshua bengio, nicholas l  onard, and aaron c. courville. estimating or propagating gradients
through stochastic neurons for conditional computation. corr, abs/1308.3432, 2013. url
http://arxiv.org/abs/1308.3432.

junyoung chung, sungjin ahn, and yoshua bengio. hierarchical multiscale recurrent neural networks.

corr, abs/1609.01704, 2016. url http://arxiv.org/abs/1609.01704.

tim cooijmans, nicolas ballas, c  sar laurent, caglar gulcehre, and aaron courville. recurrent

batch id172. arxiv preprint arxiv:1603.09025, 2016.

matthieu courbariaux, yoshua bengio, and jean-pierre david. binaryconnect: training deep neural

networks with binary weights during propagations. in nips, pp. 3123   3131, 2015.

yarin gal. a theoretically grounded application of dropout in recurrent neural networks. arxiv

e-prints, december 2015.

felix a. gers, j  rgen schmidhuber, and fred a. cummins. learning to forget: continual prediction

with lstm. neural computation, 12(10):2451   2471, 2000.

david ha, andrew m. dai, and quoc v. le. hypernetworks. corr, abs/1609.09106, 2016. url

http://arxiv.org/abs/1609.09106.

kaiming he, xiangyu zhang, shaoqing ren, and jian sun. deep residual learning for image

recognition. arxiv preprint arxiv:1512.03385, 2015.

salah el hihi and yoshua bengio. hierarchical recurrent neural networks for long-term dependencies.

in advances in neural information processing systems. 1996.

geoffrey e hinton, nitish srivastava, alex krizhevsky, ilya sutskever, and ruslan r salakhutdinov.
improving neural networks by preventing co-adaptation of feature detectors. arxiv preprint
arxiv:1207.0580, 2012.

sepp hochreiter. untersuchungen zu dynamischen neuronalen netzen. master   s thesis, institut fur

informatik, technische universitat, munchen, 1991.

sepp hochreiter and j  rgen schmidhuber. long short-term memory. neural computation, 9(8):

1735   1780, 1997.

gao huang, yu sun, zhuang liu, daniel sedra, and kilian weinberger. deep networks with

stochastic depth. arxiv preprint arxiv:1603.09382, 2016.

diederik kingma and jimmy ba. adam: a method for stochastic optimization. arxiv preprint

arxiv:1412.6980, 2014.

jan koutnik, klaus greff, faustino gomez, and juergen schmidhuber. a clockwork id56. arxiv

preprint arxiv:1402.3511, 2014.

9

under review as a conference paper at iclr 2017

david krueger and roland memisevic. regularizing id56s by stabilizing activations. arxiv preprint

arxiv:1511.08400, 2015.

quoc v le, navdeep jaitly, and geoffrey e hinton. a simple way to initialize recurrent networks of

recti   ed linear units. arxiv preprint arxiv:1504.00941, 2015.

matt mahoney. about the test data, 2011. url http://mattmahoney.net/dc/textdata.

mitchell p marcus, mary ann marcinkiewicz, and beatrice santorini. building a large annotated

corpus of english: the id32. computational linguistics, 19(2):313   330, 1993.

taesup moon, heeyoul choi, hoshik lee, and inchul song. id56drop: a novel dropout for id56s in

asr. automatic id103 and understanding (asru), 2015.

razvan pascanu, tomas mikolov, and yoshua bengio. understanding the exploding gradient problem.

corr, abs/1211.5063, 2012. url http://arxiv.org/abs/1211.5063.

v. pham, t. bluche, c. kermorvant, and j. louradour. dropout improves recurrent neural networks

for handwriting recognition. arxiv e-prints, november 2013.

kamil rocki, tomasz kornuta, and tegan maharaj. surprisal-driven zoneout. corr, abs/1610.07675,

2016. url http://arxiv.org/abs/1610.07675.

stanislau semeniuta, aliaksei severyn, and erhardt barth. recurrent dropout without memory loss.

arxiv preprint arxiv:1603.05118, 2016.

s. singh, d. hoiem, and d. forsyth. swapout: learning an ensemble of deep architectures. arxiv

e-prints, may 2016.

nitish srivastava, geoffrey hinton, alex krizhevsky, ilya sutskever, and ruslan salakhutdinov.
dropout: a simple way to prevent neural networks from over   tting. the journal of machine
learning research, 15(1):1929   1958, 2014.

theano development team. theano: a python framework for fast computation of mathematical

expressions. arxiv e-prints, abs/1605.02688, may 2016.

tijmen tieleman and geoffrey hinton. lecture 6.5-rmsprop: divide the gradient by a running
average of its recent magnitude. coursera: neural networks for machine learning, 4:2, 2012.

bart van merri  nboer, dzmitry bahdanau, vincent dumoulin, dmitriy serdyuk, david warde-farley,
jan chorowski, and yoshua bengio. blocks and fuel: frameworks for deep learning. corr,
abs/1506.00619, 2015.

sida wang and christopher manning. fast dropout training. in proceedings of the 30th international

conference on machine learning, pp. 118   126, 2013.

wojciech zaremba, ilya sutskever, and oriol vinyals. recurrent neural network id173. arxiv

preprint arxiv:1409.2329, 2014.

10

under review as a conference paper at iclr 2017

6 appendix

6.1 static identity connections experiment

this experiment was suggested by anonreviewer2 during the iclr review process with the goal
of disentangling the effects zoneout has (1) through noise injection in the training process and (2)
through identity connections. based on these results, we observe that noise injection is essential for
obtaining the id173 bene   ts of zoneout.
in this experiment, one zoneout mask is sampled at the beginning of training, and used for all
examples. this means the identity connections introduced are static across training examples (but
still different for each timestep). using static identity connections resulted in slightly lower training
(but not validation) error than zoneout, but worse performance than an unregularized lstm on both
train and validation sets, as shown in figure 7.

figure 7: training and validation curves for an lstm with static identity connections compared
to zoneout (both zc = 0.5 and zh = 0.05) and compared to a vanilla lstm, showing that static
identity connections fail to capture the bene   ts of zoneout.

11

16111621263136414651566166717681869196101epoch0.81.01.21.41.61.82.02.2bpcvanilla lstm (validation)vanilla lstm (training)zoneout (validation)zoneout (training)static identity connections (validation)static identity connections (training)