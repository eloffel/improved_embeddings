global neural id35 parsing with optimality guarantees

kenton lee

mike lewis

luke zettlemoyer

computer science & engineering

university of washington

seattle, wa 98195

{kentonl,id113wis,lsz}@cs.washington.edu

6
1
0
2

 

p
e
s
4
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
2
3
4
1
0

.

7
0
6
1
:
v
i
x
r
a

abstract

we introduce the    rst global recursive neural
parsing model with optimality guarantees dur-
ing decoding. to support global features, we
give up dynamic programs and instead search
directly in the space of all possible subtrees.
although this space is exponentially large in
the sentence length, we show it is possible
to learn an ef   cient a* parser. we augment
existing parsing models, which have informa-
tive bounds on the outside score, with a global
model that has loose bounds but only needs
to model non-local phenomena. the global
model is trained with a novel objective that en-
courages the parser to search both ef   ciently
and accurately. the approach is applied to
id35 parsing, improving state-of-the-art accu-
racy by 0.4 f1. the parser    nds the optimal
parse for 99.9% of held-out sentences, explor-
ing on average only 190 subtrees.

1

introduction

recursive neural models perform well for many
id170 problems, in part due to their
ability to learn representations that depend globally
on all parts of the output structures. however, global
models of this sort are incompatible with existing
exact id136 algorithms, since they do not de-
compose over substructures in a way that allows ef-
fective id145. existing work has
therefore used greedy id136 techniques such as
id125 (vinyals et al., 2015; dyer et al., 2015)
or reranking (socher et al., 2013). we introduce
the    rst global recursive neural parsing approach

with optimality guarantees for decoding and use it
to build a state-of-the-art id35 parser.

to enable learning of global representations, we
modify the parser to search directly in the space of
all possible parse trees with no dynamic program-
ming. optimality guarantees come from a    search,
which provides a certi   cate of optimality if run to
completion with a heuristic that is a bound on the
future cost. generalizing a    to global models is
challenging; these models also break the locality as-
sumptions used to ef   ciently compute existing a   
heuristics (klein and manning, 2003; lewis and
steedman, 2014). rather than directly replacing lo-
cal models, we show that they can simply be aug-
mented by adding a score from a global model that
is constrained to be non-positive and has a trivial
upper bound of zero. the global model, in effect,
only needs to model the remaining non-local phe-
nomena. in our experiments, we use a recent fac-
tored a    id35 parser
(lewis et al., 2016) for the
local model, and we train a tree-lstm (tai et al.,
2015) to model global structure.

finding a model that achieves these a    guar-
antees in practice is a challenging learning prob-
lem. traditional id170 objectives fo-
cus on ensuring that the gold parse has the high-
est score (collins, 2002; huang et al., 2012). this
condition is insuf   cient in our case, since it does
not guarantee that the search will terminate in sub-
exponential time. we instead introduce a new ob-
jective that optimizes ef   ciency as well as accuracy.
our id168 is de   ned over states of the a   
search agenda, and it penalizes the model whenever
the top agenda item is not a part of the gold parse.

explored

agenda

unexplored

fruit    ies

like bananas
?
s

fruit    ies
?

n p

like bananas
?

s\n p

   ies

n p\n p

fruit

n p/n p

   ies
n p

like

(s\n p )/n p

bananas

n p

   

   ies
s\n p

like

(s\s)/n p

like bananas
?

s\s

fruit
n p

fruit    ies
?

s

   ies

fruit
n p n p\n p (s\n p )/n p
s\n p

like

n p

<

bananas

n p

>

<

fruit

   ies

like

n p/n p n p (s\n p )/n p
s\n p

n p

>

bananas

n p

>

<

s

   ies

fruit
n p n p\n p

<

n p

s

like

(s\n p )/n p
s\n p

bananas

n p

>

fruit

   ies
n p/n p n p
>

n p

   ies

n p\n p

fruit

n p/n p

   ies
n p

   

like

(s\n p )/n p

bananas

n p

fruit
n p

   ies
s\n p

like

(s\s)/n p

like

   ies

fruit
n p s\n p (s\s)/n p
s\s

s

<

s

fruit
   ies
n p s\n p

<

s

like

(s\s)/n p
s\s

bananas

n p

>

bananas

n p

>

<

(a) the search space in chart parsing, with one node for
each labeling of a span.

(b) the search space in this work, with one node for each
partial parse.

figure 1: illustrations of id35 parsing as hypergraph search, showing partial views of the search space. weighted hyperedges
from child nodes to a parent node represent rule productions scored by a parsing model. a path starting at    , for example the
set of bolded hyperedges, represents the derivation of a parse. during decoding, we    nd the highest scoring path to a complete
parse. both    gures show an ideal exploration that ef   ciently    nds the optimal path. figure 1a depicts the traditional search space,
and figure 1b depicts the search space in this work. hyperedge scores can only depend on neighboring nodes, so our model can
condition on the entire parse structure, at the price of an exponentially larger search space.

minimizing this loss encourages the model to return
the correct parse as quickly as possible.

the combination of global representations and
optimal decoding enables our parser to achieve
state-of-the-art accuracy for combinatory catego-
rial grammar (id35) parsing. despite being in-
tractable in the worst case, the parser in practice is
highly ef   cient. it    nds optimal parses for 99.9% of
held out sentences while exploring just 190 subtrees
on average   allowing it to outperform id125
in both speed and accuracy.

2 overview

parsing as hypergraph search many parsing al-
gorithms can be viewed as a search problem, where
parses are speci   ed by paths through a hypergraph.
a node y in this hypergraph is a labeled span, rep-
resenting structures within a parse tree, as shown in
figure 1. each hyperedge e in the hypergraph rep-
resents a rule production in a parse. the head node

of the hyperedge head(e) is the parent of the rule
production, and the tail nodes of the hyperedge are
the children of the rule production. for example,
consider the hyperedge in figure 1b whose head is
like bananas. this hyperedge represents a forward
application rule applied to its tails, like and bananas.

to de   ne a path in the hypergraph, we    rst in-
clude a special start node     that represents an empty
parse.     has outgoing hyperedges that reach ev-
ery leaf node, representing assignments of labels to
words (supertag assignments in figure 1). we then
de   ne a path to be a set of hyperedges e starting at
    and ending at a single destination node. a path
therefore speci   es the derivation of the parse con-
structed from the labeled spans at each node. for
example, in figure 1, the set of bolded hyperedges
form a path deriving a complete parse.

each hyperedge e is weighted by a score s(e)
from a parsing model. the score of a path e is the

sum of its hyperedge scores:

g(e) =

s(e)

(cid:88)

e   e

viterbi decoding is equivalent to    nding the highest
scoring path that forms a complete parse.

search on parse forests traditionally, the hyper-
graph represents a packed parse chart. in this work,
our hypergraph instead represents a forest of parses.
figure 1 contrasts the two representations.

in the parse chart, labels on the nodes represent
local properties of a parse, such as the category of a
span in figure 1a. as a result, multiple parses that
contain the same property include the same node in
their path, (e.g. the node spanning the phrase fruit
   ies with category np). the number of nodes in
this hypergraph is polynomial in the sentence length,
permitting exhaustive exploration (e.g. cky pars-
ing). however, the model scores can only depend on
local properties of a parse. we refer to these models
as locally factored models.

in contrast, nodes in the parse forest are labeled
with entire subtrees, as shown in figure 1b. for ex-
ample, there are two nodes spanning the phrase fruit
   ies with the same category np but different inter-
nal substructures. while the parse forest requires an
exponential number of nodes in the hypergraph, the
model scores can depend on entire subtrees.
a    parsing a    parsing has been successfully ap-
plied in locally factored models (klein and man-
ning, 2003; lewis and steedman, 2014; lewis et
al., 2015; lewis et al., 2016). we present a special
case of a    parsing that is conceptually simpler, since
the parse forest constrains each node to be reachable
via a unique path. during exploration, we maintain
the unique (and therefore highest scoring) path to a
hyperedge e, which we de   ne as path(e).
similar to the standard a    search algorithm, we
maintain an agenda a of hyperedges to explore and
a forest f of explored nodes that initially contains
only the start node    .

each hyperedge e in the agenda is sorted by the
sum of its inside score g(path(e)) and an admissible
heuristic h(e). a heuristic h(e) is admissible if it
is an upper bound of the sum of hyperedge scores
leading to any complete parse reachable from e (the
viterbi outside score). the ef   ciency of the search

improves when this bound is tighter.

at every step, the parser removes the top of the
agenda, emax = argmaxe   a(g(path(e)) + h(e)).
emax is expanded by combining head(emax) with
previously explored parses from f to form new hy-
peredges. these new hyperedges are inserted into
a, and head(emax) is added it to f. we repeat
these steps until the    rst complete parse y    is ex-
plored. the bounds provided by h(e) guarantee that
the path to y    has the highest possible score. fig-
ure 1b shows an example of the agenda and the ex-
plored forest at the end of perfectly ef   cient search,
where only the optimal path is explored.

approach the enormous search space described
above presents a challenge for an a    parser, since
computing a tight and admissible heuristic is dif   -
cult when the model does not decompose locally.

our key insight in addressing this challenge is that
existing locally factored models with an informative
a    heuristic can be augmented with a global score
(section 3). by constraining the global score to be
non-positive, the a    heuristic from the locally fac-
tored model is still admissible.

while the heuristic from the local model offers
some estimate of the future cost, the ef   ciency of
the parser requires learning a well-calibrated global
score, since the heuristic becomes looser as the
global score provides stronger penalties (section 5).
as we explore the search graph, we incrementally
construct a neural network, which computes repre-
sentations of the parses and allows id26
of errors from bad search steps (section 4).

in the following sections, we present our ap-
proach in detail, assuming an existing locally fac-
tored model slocal(e) for which we can ef   ciently
compute an admissible a    heuristic h(e).

in the experiments, we apply our model to id35
parsing, using the locally factored model and a   
heuristic from lewis et al. (2016).

3 model

our model scores a hyperedge e by combining the
score from the local model with a global score that
conditions on the entire parse at the head node:

s(e) = slocal(e) + sglobal(e)

in sglobal(e), we    rst compute a hidden representa-
tion encoding the parse structure of y = head(e).
we use a variant of the tree-lstm (tai et al., 2015)
connected to a bidirectional lstm (hochreiter and
schmidhuber, 1997) at the leaves. the combination
of linear and tree lstms allows the hidden repre-
sentation of partial parses to condition on both the
partial structure and the full sentence. figure 2 de-
picts the neural network that computes the hidden
representation for a parse.

formally, given a sentence (cid:104)w1, w2, . . . , wn(cid:105), we
compute hidden states ht and cell states ct in the for-
ward lstm for 1 < t     n:

it =  (wi[ct   1, ht   1, xt] + bi)
ot =  (wo[  ct, ht   1, xt] + bo)
  ct = tanh(wc[ht   1, xt] + bc)
ct =it       ct + (1     it)     ct   1
ht =ot     tanh(ct)

t and h(cid:48)

where    is the logistic sigmoid,     is the component-
wise product, and xt denotes a learned word embed-
ding for wt. we also construct a backward lstm,
which produces the analogous hidden and cell states
starting at the end of the sentence, which we denote
as c(cid:48)
t respectively. the start and end latent
states, c   1, h   1, c(cid:48)
n+1, are learned embed-
dings. this variant of the lstm includes peephole
connections and couples the input and forget gates.
the bidirectional lstm over the words serves
as a base case when we recursively compute a hid-
den representation for the parse y using the tree-
structured generalization of the lstm:

n+1, and h(cid:48)

i [cl, hl, cr, hr, xy] + br
i )
f [cl, hl, cr, hr, xy] + br
f )
o )

iy =   (w r
fy =   (w r
oy =   (w r
clr = fy     cl + (1     fy)     cr

o [(cid:101)cy, hl, hr, xy] + br
(cid:101)cy = tanh(w r
cy = iy    (cid:101)cy + (1     iy)     clr

c [hl, hr, xy] + br
c )

hy = oy     tanh(cy)

where the weights and biases are parametrized by
the rule r that produces y from its children, and xy
denotes a learned embedding for the category at the
root of y. for example, in id35, the rule would cor-
respond to the id35 combinator, and the label would

s

np

s\np

np/np

np

(s\np)/np

np

fruit

   ies

like

bananas

figure 2: visualization of the tree-lstm which computes
vector embeddings for each parse node. the leaves of the tree-
lstm are connected to a bidirectional lstm over words, en-
coding lexical information within and outside of the parse.

correspond to the id35 category.

we assume that nodes are binary, unary, or leaves.
their left and right latent states, cl, hl, cr, and hr are
de   ned as follows:

    in a binary node, cl and hl are the cell and hid-
den states of the left child, and cr and hr are
the cell and hidden states of the right child.
    in a unary node, cl and hl are learned embed-
dings, and cr and hr are the cell and hidden
states of the singleton child.
    in a leaf node, let w denote the index of the
corresponding word. then cl and hl are cw and
hw from the forward lstm, and cr and hr are
c(cid:48)
w and h(cid:48)

w from the backward lstm.

the cell state of the recursive unit is a linear com-

bination of the intermediate cell state(cid:101)cy, the left cell
decides the weights for (cid:101)cy, and the forget gate fy

state cl, and the right cell state cr. to preserve the
normalizing property of coupled gates, we perform
coupling in a hierarchical manner: the input gate iy

shares the remaining weights between cl and cr.

given the hidden representation hy at the root, we

score the global component as follows:
sglobal(e) = log(  (w    hy))

this de   nition of the global score ensures that it is
non-positive   an important property for id136.

id136

4
using the hyperedge scoring model s(e) described
in section 3, we can    nd the highest scoring path
that derives a complete parse tree by using the a   
parsing algorithm described in section 2.

fruit

   ies

like

n p/n p n p (s\n p )/n p
s\n p

n p

>

s

bananas

n p

>

<

fruit

   ies

like

n p/n p n p (s\n p )/n p
s\n p

n p

>

s

bananas

n p

>

<

fruit

   ies
n p/n p n p
>

n p

   

sglobal(e)
+ slocal(e)

fruit

n p/n p

   ies
n p

fruit

   ies
n p/n p n p
>

n p

sglobal(eglobal)

fruit

   ies
n p/n p n p
>

n p

slocal(elocal)

fruit

n p/n p

   ies
n p

figure 3: the hyperedge on the left requires computing both
the local and global score when placed on the agenda. splitting
the hyperedge, as shown on the right, saves expensive compu-
tation of the global score if the local score alone indicates that
the parse is not worth exploring.

admissible a    heuristic since our full model
adds non-positive global scores to the existing lo-
cal scores, path scores under the full model cannot
be greater than path scores under the local model.
upper bounds for path scores under the local model
also hold for path scores under the full model, and
we simply reuse the a    heuristic from the local
model to guide the full model during parsing without
sacri   cing optimality guarantees.

incremental neural network construction the
recursive hidden representations used in sglobal(e)
can be computed in constant time during parsing.
when scoring a new hyperedge, its children must
have been previously scored. instead of computing
the full recursion, we reuse the existing latent states
of the children and compute sglobal(e) with an in-
cremental forward pass over a single recursive unit
in the neural network. by maintain the latent states
of each parse, we incrementally build a single dag-
structured lstm mirroring the explored subset of
the hypergraph. this not only enables quick for-
ward passes during decoding, but also allows back-
propagation through the search space after decoding,
which is crucial for ef   cient learning (see section 5).

also split between the two new hyperedges:

s(elocal) = slocal(elocal)
s(eglobal) = sglobal(eglobal)

intuitively, this transformation requires a    to verify
that the local score is good enough before comput-
ing the global score, which requires an incremental
forward pass over a recursive unit in the neural net-
work. in the example, this involves    rst summing
the supertag scores of fruit and    ies and inserting
the result back into the agenda. the score for ap-
plying the forward application rule to the recursive
representations is only computed if that item appears
again at the head of the agenda. in practice, the lazy
global scoring reduces the number of recursive units
by over 91%, providing a 2.4x speed up.

5 learning

during training (algorithm 1), we assume access to
sentences labeled with gold parse trees   y and gold
derivations   e. the gold derivation   e is a path from
    to   y in the parse forest.

a    search with our global model is not guar-
anteed to terminate in sub-exponential time. this
creates challenges for learning   for example, it is
not possible in practice to use the standard struc-
tured id88 update (collins, 2002), because the
search procedure rarely terminates early in training.
other common id168s assume inexact search
(huang et al., 2012), and do not optimize ef   ciency.
is
tightly coupled with the search procedure. during
parsing, we would like hyperedges from the gold
derivation to appear at the top of the agenda a.
when this condition does not hold, a    is searching
inef   ciently, and we refer to this as a violation of the
agenda, which we formally de   ne as:

instead, we optimize a new objective that

v(   e,a) = max

e   a (g(path(e)) + h(e))
    max
e   a      e

(g(path(e)) + h(e))

lazy global scoring the global score is expensive
to compute. we introduce an optimization to avoid
computing it when provably unnecessary. we split
each hyperedge e into two successive hyperedges,
elocal and eglobal, as shown in figure 3. the score
for e, previously s(e) = slocal(e) + sglobal(e), is

where g(path(e)) is the score of the unique path to
e, and h(e) is the a    heuristic. if all violations are
zero, we    nd the gold parse without exploring any
incorrect partial parses   maximizing both accuracy
and ef   ciency. figure 1b shows such a case   if any
other nodes were explored, they would be violations.

update
greedy
max violation maxt

all violations (cid:80)t

loss(v)
v1
t=1 vt
t=1 vt

table 1: id168s optimized by the different update meth-
ods. the updates depend on the list of t non-zero violations,
v = (cid:104)v1,v2, . . . ,vt(cid:105), as de   ned in section 5.

in existing work on violation-based updates, com-
parisons are only made between derivations with the
same number of steps (huang et al., 2012; clark et
al., 2015)   whereas our de   nition allows subtrees
of arbitrary spans to compete with each other, be-
cause hyperedges are not explored in a    xed order.
our violations also differ from huang et al.   s in that
we optimize ef   ciency as well as accuracy.

we de   ne id168s over these violations,
which are minimized to encourage correct and ef-
   cient search. during training, we parse each sen-
tence until either the gold parse is found or we reach
computation limits. we record v, the list of non-
zero violations of the agenda a observed:
v = (cid:104)v(   e,a) | v(   e,a) > 0(cid:105)

we can optimize several id168s over v, as
de   ned in table 1. the greedy and max-violation
updates are roughly analogous to the violation-
   xing updates proposed by huang et al. (2012), but
adapted to exact agenda-based parsing. we also
introduce a new all-violations update, which min-
imizes the sum of all observed violations. the all-
violations update encourages correct parses to be ex-
plored early (similar to the greedy update) while be-
ing robust to parses with multiple deviations from
the gold parse (similar to the max-violation update).

the violation losses are optimized with subgra-
dient descent and id26. for our experi-
ments, slocal(e) and h(e) are kept constant. only the
parameters    of sglobal(e) are updated. therefore, a
subgradient of a violation v(   e,a) can be computed
by summing subgradients of the global scores.
   v(   e,a)

   sglobal(e)

   sglobal(e)

(cid:88)

    (cid:88)

     

=
e   path(emax)

     

e   path(  emax)

     

where emax denotes the hyperedge at the top of the
agenda a and   emax denotes the hyperedge in the
gold derivation   e that is closest to the top of a.

push(a, e)

while |a       e| > 0 and size ok(f,a) do

v        
f        
a        
for e     tag(x) do

(cid:46) initialize list of violations v
(cid:46) initialize forest f
(cid:46) initialize agenda a

algorithm 1 violation-based learning algorithm
de   nitions d is the training data containing input sentences
x and gold derivations   e. e variables denote scored hyper-
edges. tag(x) returns a set of scored pre-terminals for every
word. add(f, y) adds partial parse y to forest f. rules(f,
y) returns the set of scored hyperedges that can be created by
combining y with entries in f . size ok(f, a) returns whether
the sizes of the forest and agenda are within prede   ned limits.
1: function violations(   e, x,   )
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: function learn(d)
for i = 1 to t do
17:
18:
19:
20:
21:
22:

for x,   e     d do
v     violations(   e, x,   )
l     loss(v)
       optimize(l,   )

emax     extract max(a)
add(f, head(emax))
for e     rules(f, head(emax),   ) do

(cid:46) record violation
(cid:46) pop agenda
(cid:46) explore hyperedge

append(v, v(   e,a))

if v(   e,a) > 0 then

(cid:46) expand hyperedge

push(a, e)

return v

return   

6 experiments
6.1 data
we trained our parser on sections 02-21 of id35-
bank (hockenmaier and steedman, 2007), using
section 00 for development and section 23 for test.
to recover a single gold derivation for each sentence
to use during training, we    nd the right-most branch-
ing parse that satis   es the gold dependencies.

6.2 experimental setup
for the local model, we use the supertag-factored
model of lewis et al. (2016). here, slocal(e) cor-
responds to a supertag score if a head(e) is a leaf
and zero otherwise. the outside score heuristic is
computed by summing the maximum supertag score
for every word outside of each span. in the reported
results, we back off to the supertag-factored model
after the forest size exceeds 500,000, the agenda size
exceeds 2 million, or we build more than 200,000 re-
cursive units in the neural network.

dev f1 test f1
model
83.8
c & c
86.3
c & c + id56
xu (2016)
87.5
vaswani et al. (2016) 87.8
87.5
supertag-factored
global a   
88.4

85.2
87.0
87.8
88.3
88.1
88.7

table 2: labeled f1 for id35bank dependencies on the id35-
bank development and test set for our system global a    and
the baselines.

our full system is trained with all-violations up-
dates. during training, we lower the forest size
limit to 2000 to reduce training times. the model is
trained for 30 epochs using adam (kingma and ba,
2014), and we use early stopping based on develop-
ment f1. the lstm cells and hidden states have 64
dimensions. we initialize word representations with
pre-trained 50-dimensional embeddings from turian
et al. (2010). embeddings for categories have 16 di-
mensions and are randomly initialized. we also ap-
ply dropout with a id203 of 0.4 at the word em-
bedding layer during training. since the structure of
the neural network is dynamically determined, we
do not use mini-batches. the neural networks are
implemented using the id98 library,1 and the id35
parser is implemented using the easysrl library.2
the code is available online.3

6.3 baselines
we compare our parser to several baseline id35
parsers: the c&c parser (clark and curran, 2007);
c&c + id56 (xu et al., 2015), which is the c&c
parser with an id56 supertagger; xu (2016), a
lstm shift-reduce parser; vaswani et al. (2016)
who combine a bidirectional lstm supertagger
with a id125 parser using global features
(clark et al., 2015); and supertag-factored (lewis
et al., 2016), which uses deterministic a    decoding
and an lstm id55 model.

6.4 parsing results
table 2 shows parsing results on the test set. our
global features let us improve over the supertag-
factored model by 0.6 f1. vaswani et al. (2016) also

1https://github.com/clab/id98
2https://github.com/mikelewis0/easysrl
3https://github.com/kentonl/neuralid35

dev f1 optimal explored
model
supertag-factored
87.5
    dynamic program 87.5
span-factored
87.9
    dynamic program 87.8
global a   
88.4
    lexical inputs
87.8
    lexical context
88.1

100.0% 402.5
97.1%
99.9%
99.5%
99.8%
99.6%
99.4%

17119.6
176.5
578.5
309.6
538.5
610.5

table 3: ablations of our full model (global a   ) on the de-
velopment set. explored refers to the size of the parse forest.
results show the importance of global features and lexical in-
formation in context.

use global features, but our optimal decoding leads
to an improvement of 0.4 f1.

although we observed an overall improvement in
parsing performance, the supertag accuracy was not
signi   cantly different after applying the parser.

on the test data, the parser    nds the optimal parse
for 99.9% sentences before reaching our computa-
tional limits. on average, we parse 27.1 sentences
per second,4 while exploring only 190.2 subtrees.

6.5 model ablations
we ablate various parts of the model to determine
how they contribute to the accuracy and ef   ciency of
the parser, as shown in table 3. for each model, the
comparisons include the average number of parses
explored and the percentage of sentences for which
an optimal parse can be found without backing off.

structure ablation we    rst ablate the global
score, sglobal(y), from our model, thus relying en-
tirely on the local supertag-factors that do not explic-
itly model the parse structure. this ablation allows
id145 and is equivalent to the back-
off model (supertag-factored in table 3). surpris-
ingly, even in the exponentially larger search space,
the global model explores fewer nodes than the
supertag-factored model   showing that the global
model ef   ciently prune large parts of the search
space. this effect is even larger when not using dy-
namic programming in the supertag-factored model.

global structure ablation to examine the impor-
tance of global features, we ablate the recursive hid-
den representation (span-factored in table 3). the
model in this ablation decomposes over labels for

4we use a single 3.5ghz cpu core.

u.s.
n/n (n/n )\(n/n )

small

business

n

n/n

<

>

n

n p

sdcl

is

one
(sdcl\n p )/n p n
n p
>

sdcl\n p

figure 4: example of an incorrect partial parse that appears
syntactically plausible in isolation. the full sentence is    indeed,
for many japanese trading companies, the favorite u.s. small
business is one whose research and development can be milked
for future japanese use.    the global model heavily penalizes
this garden path, thereby avoiding regions that lead to dead ends
and allowing the global model to explore fewer nodes.

spans, as in durrett and klein (2015). in this model,
the recursive unit uses, instead of latent states from
its children, the latent states of the backward lstm
at the start of the span and the latent states of the for-
ward lstm at the end of the span. therefore, this
model encodes the lexical information available in
the full model but does not encode the parse struc-
ture beyond the local rule production. while the dy-
namic program allows this model to    nd the optimal
parse with fewer explorations, the lack of global fea-
tures signi   cantly hurts its parsing accuracy.
lexical ablation we also show lexical ablations
instead of structural ablations. we remove the bidi-
rectional lstm at the leaves, thus delexicalizing the
global model. this ablation degrades both accuracy
and ef   ciency, showing that the model uses lexical
information to discriminate between parses.

to understand the importance of contextual infor-
mation, we also perform a partial lexical ablation by
using id27s at the leaves instead of the
bidirectional lstm, thus propagating only lexical
information from within the span of each parse. the
degradation in f1 is about half of the degradation
from the full lexical ablation, suggesting that a sig-
ni   cant portion of the lexical cues comes from the
context of a parse. figure 4 illustrates the impor-
tance of context with an incorrect partial parse that
appears syntactically plausible in isolation. these
bottom-up garden paths are typically problematic
for parsers, since their incompatibility with the re-
maining sentence is dif   cult to recognize until later
stages of decoding. however, our global model
learns to heavily penalize these garden paths by us-
ing the context provided by the bidirectional lstm

dev f1 optimal explored
update
greedy
87.9
max-violation 88.1
all-violations 88.4

2313.8
217.3
309.6

99.2%
99.9%
99.8%

<

table 4: parsing results trained with different update methods.
our system uses all-violations updates and is the most accurate.

%
1
f

88

87

86

85

84

83

all violations

greedy

max violation

1

2

training epoch

3

figure 5: learning curves for the    rst 3 training epochs on the
development set when training with different updates strategies.
the all-violations update shows the fastest convergence.

and avoid paths that lead to dead ends or bad regions
of the search space.

6.6 update comparisons
table 4 compares the different violation-based
learning objectives, as discussed in section 5. our
novel all-violation updates outperform the alterna-
tives. we attribute this improvement to the robust-
ness over poor search spaces, which the greedy up-
date lacks, and the incentive to explore good parses
early, which the max-violation update lacks. learn-
ing curves in figure 5 show that the all-violations
update also converges more quickly.

6.7 decoder comparisons
lastly, to show that our parser is both more accurate
and ef   cient than other decoding methods, we de-
code our full model using best-   rst search, rerank-
ing, and id125. table 5 shows the f1 scores
with and without the backoff model, the portion of
the sentences that each decoder is able to parse, and
the time spent decoding relative to the a    parser.
in the best-   rst search comparison, we do not in-
clude the informative a    heuristic, and the parser
completes very few parses before reaching computa-
tional limits   showing the importance of heuristics
in large search spaces. in the reranking comparison,

dev f1 dev f1

decoder
global a   
88.4
best-   rst
87.5
10-best reranking
87.9
100-best reranking
88.2
2-best id125 88.2
4-best id125 88.3
8-best id125 88.2

relative
time

293.4x

    backoff
88.4 (99.8%) 1x
2.8 (6.7%)
87.9 (99.7%) 8.5x
88.0 (99.4%) 72.3x
85.7 (94.0%) 2.0x
88.1 (99.2%) 6.7x
86.8 (98.1%) 26.3x

table 5: comparison of various decoders using the same model
from our full system (global a   ). we report f1 with and with-
out the backoff model, the percentage of sentences that the de-
coder can parse, and the time spent decoding relative to a   .

we obtain n-best lists from the backoff model and
rerank each result with the full model. in the beam
search comparison, we use the approach from clark
et al. (2015) which greedily    nds the top-n parses
for each span in a bottom-up manner. results indi-
cate that both approximate methods are less accurate
and slower than a   .

7 related work

many id170 problems are based
around dynamic programs, which are incompatible
with id56s because of their real-
valued latent variables. some recent models have
neural factors (durrett and klein, 2015), but these
cannot condition on global parse structure, making
them less expressive. our search explores fewer
nodes than dynamic programs, despite an exponen-
tially larger search space, by allowing the recursive
neural network to guide the search.

previous work on id170 with re-
cursive or recurrent neural models has used beam
search   e.g.
in shift reduce parsing (dyer et al.,
2015), string-to-tree transduction (vinyals et al.,
2015), or reranking (socher et al., 2013)   at the cost
of potentially recovering suboptimal solutions. for
our model, id125 is both less ef   cient and
less accurate than optimal a    decoding.
in the
non-neural setting, zhang et al. (2014) showed that
global features with greedy id136 can improve
id33. the id35 id125 parser
of clark et al. (2015), most related to this work, also
uses global features. by using neural representations
and exact search, we improve over their results.

a    parsing has been previously proposed for lo-

cally factored models (klein and manning, 2003;
pauls and klein, 2009; auli and lopez, 2011; lewis
and steedman, 2014). we generalize these methods
to enable global features. vaswani and sagae (2016)
apply best-   rst search to an unlabeled shift-reduce
parser. their use of error states is related to our
global model that penalizes local scores. we demon-
strated that best-   rst search is infeasible in our set-
ting, due to the larger search space.

a close integration of learning and decoding has
been shown to be bene   cial for structured predic-
tion. searn (daum  e iii et al., 2009) and dag-
ger (ross et al., 2011) learn greedy policies to pre-
dict structure by sampling classi   cation examples
over actions from single states. we similarly gen-
erate classi   cation examples over hyperedges in the
agenda, but actions from multiple states compete
against each other. other learning objectives that up-
date parameters based on a beam or agenda of par-
tial structures have also been proposed (collins and
roark, 2004; daum  e iii and marcu, 2005; huang et
al., 2012; andor et al., 2016; wiseman and rush,
2016), but the impact of search errors is unclear.

8 conclusion

we have shown for the    rst time that a parsing model
with global features can be decoded with optimal-
ity guarantees. this enables the use of powerful re-
cursive neural networks for parsing without resort-
ing to approximate decoding methods. experiments
show that this approach is effective for id35 pars-
ing, resulting in a new state-of-the-art parser. in fu-
ture work, we will apply our approach to other struc-
tured prediction tasks, where neural networks   and
greedy id125   have become ubiquitous.

acknowledgements

we thank luheng he, julian michael, and mark
yatskar for valuable discussion, and the anonymous
reviewers for feedback and comments.

this work was supported by the nsf (iis-
1252835, iis-1562364), darpa under the deft
program through the afrl (fa8750-13-2-0019),
an allen distinguished investigator award, and a
gift from google.

references
daniel andor, chris alberti, david weiss, aliaksei
severyn, alessandro presta, kuzman ganchev, slav
petrov, and michael collins. 2016. globally normal-
ized transition-based neural networks. in proceed-
ings of the 54th annual meeting of the association for
computational linguistics, pages 2442   2452.

michael auli and adam lopez. 2011. ef   cient id35
in pro-
parsing: a* versus adaptive id55.
ceedings of the 49th annual meeting of the associa-
tion for computational linguistics: human language
technologies-volume 1.

stephen clark and james r curran.

2007. wide-
coverage ef   cient statistical parsing with id35 and
computational linguistics,
id148.
33(4).

stephen clark, darren foong, luana bulat, and wenduan
xu. 2015. the java version of the c&c parser: ver-
sion 0.95. technical report, university of cambridge
computer laboratory, august.

incremental
michael collins and brian roark. 2004.
in proceed-
parsing with the id88 algorithm.
ings of the 42nd annual meeting on association for
computational linguistics, page 111. association for
computational linguistics.

michael collins. 2002. discriminative training meth-
ods for id48: theory and experi-
ments with id88 algorithms. in proceedings of
the acl-02 conference on empirical methods in nat-
ural language processing-volume 10. association for
computational linguistics.

hal daum  e iii and daniel marcu.

2005. learning
as search optimization: approximate large margin
methods for id170. in proceedings of
the 22nd international conference on machine learn-
ing, pages 169   176. acm.

hal daum  e iii, john langford, and daniel marcu. 2009.
search-based id170. machine learn-
ing, 75(3):297   325.

greg durrett and dan klein. 2015. neural crf parsing.
in proceedings of the association for computational
linguistics.

chris dyer, miguel ballesteros, wang ling, austin
matthews, and noah a. smith. 2015. transition-
based id33 with stack long short-
term memory. in proc. acl.

sepp hochreiter and j  urgen schmidhuber. 1997. long
short-term memory. neural computation, 9(8):1735   
1780.

julia hockenmaier and mark steedman. 2007. id35-
bank: a corpus of id35 derivations and dependency
structures extracted from the id32. com-
putational linguistics, 33(3).

liang huang, suphan fayong, and yang guo. 2012.
in pro-
structured id88 with inexact search.
ceedings of the 2012 conference of the north ameri-
can chapter of the association for computational lin-
guistics: human language technologies, pages 142   
151. association for computational linguistics.

diederik kingma and jimmy ba.

2014. adam: a
method for stochastic optimization. arxiv preprint
arxiv:1412.6980.

dan klein and christopher d manning. 2003. a* pars-
ing: fast exact viterbi parse selection. in proceed-
ings of the 2003 conference of the north american
chapter of the association for computational linguis-
tics on human language technology-volume 1.

mike lewis and mark steedman. 2014. a* id35 pars-
ing with a supertag-factored model. in proceedings of
the 2014 conference on empirical methods in natural
language processing.

mike lewis, luheng he, and luke zettlemoyer. 2015.
joint a* id35 parsing and semantic role labelling.
in empirical methods in natural language process-
ing.

mike lewis, kenton lee, and luke zettlemoyer. 2016.
lstm id35 parsing. in proceedings of the 15th an-
nual conference of the north american chapter of the
association for computational linguistics.

adam pauls and dan klein. 2009. k-best a* pars-
ing. in proceedings of the joint conference of the 47th
annual meeting of the acl and the 4th international
joint conference on natural language processing of
the afnlp: volume 2-volume 2, pages 958   966. as-
sociation for computational linguistics.

st  ephane ross, geoffrey j. gordon, and drew bagnell.
2011. a reduction of imitation learning and struc-
tured prediction to no-regret online learning.
in
proceedings of the fourteenth international confer-
ence on arti   cial intelligence and statistics, aistats
2011, fort lauderdale, usa, april 11-13, 2011, pages
627   635.

richard socher, john bauer, christopher d manning, and
andrew y ng. 2013. parsing with compositional
vector grammars. in proceedings of the acl confer-
ence.

kai sheng tai, richard socher, and christopher d man-
ning. 2015. improved semantic representations from
tree-structured id137.
arxiv preprint arxiv:1503.00075.

joseph turian, lev ratinov, and yoshua bengio. 2010.
word representations: a simple and general method
for semi-supervised learning. in proceedings of the
48th annual meeting of the association for computa-
tional linguistics.

ashish vaswani and kenji sagae. 2016. ef   cient struc-
tured id136 for transition-based parsing with

neural networks and error states. transactions of the
association for computational linguistics.

ashish vaswani, yonatan bisk, kenji sagae, and ryan
in pro-
musa. 2016. id55 with lstms.
ceedings of the 15th annual conference of the north
american chapter of the association for computa-
tional linguistics.

oriol vinyals, lukasz kaiser, terry koo, slav petrov,
ilya sutskever, and geoffrey hinton. 2015. gram-
in advances in neural
mar as a foreign language.
information processing systems.

sam wiseman and alexander m rush. 2016. sequence-
to-sequence learning as beam-search optimization.
in proceedings of emnlp.

wenduan xu, michael auli, and stephen clark. 2015.
id35 id55 with a recurrent neural network.
volume 2: short papers, page 250.

wenduan xu. 2016. lstm shift-reduce id35 parsing
. in empirical methods in natural language process-
ing.

yuan zhang, tao lei, regina barzilay, and tommi
jaakkola. 2014. greed is good if randomized: new
id136 for id33. in proceedings of
emnlp.

