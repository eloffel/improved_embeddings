neural network-based word alignment through score aggregation

jo  el legrand1,2,    and michael auli3 and ronan collobert3

1 idiap research institute, martigny, switzerland

2 ecole polytechnique f  ed  erale de lausanne (epfl), lausanne, switzerland

3 facebook ai research, menlo park

abstract

we present a simple neural network for
word alignment that builds source and tar-
get word window representations to com-
pute alignment scores for sentence pairs.
to enable unsupervised training, we use
an aggregation operation that summarizes
the alignment scores for a given target
word. a soft-margin objective increases
scores for true target words while de-
creasing scores for target words that are
not present. compared to the popular
fast align model, our approach improves
alignment accuracy by 7 aer on english-
czech, by 6 aer on romanian-english
and by 1.7 aer on english-french align-
ment.

1

introduction

word alignment is the task of    nding the cor-
respondence between source and target words in
a pair of sentences that are translations of each
other. generative models for this task (brown
et al., 1990; och and ney, 2003; vogel et al., 1996)
still form the basis for many machine translation
systems (koehn et al., 2003; chiang, 2007).

recent neural approaches include yang et al.
(2013) who introduce a feed-forward network-
based model trained on alignments that were gen-
erated by a traditional generative model. this
treats potentially erroneous alignments as super-
vision. tamura et al. (2014) sidesteps this issue by
negative sampling to train a recurrent-neural net-
work on unlabeled data. they optimize a global
loss that requires an expensive id125 to ap-
proximate the sum over all alignments.

   this work was conducted while the    rst author did an

internship at facebook ai research.

in this paper we introduce a word alignment
model that is simpler in structure and which re-
lies on a more tractable training procedure. our
model is a neural network that extracts context in-
formation from source and target sentences and
then computes simple dot products to estimate
alignment links. our objective function is word-
factored and does not require the expensive com-
putation associated with global id168s. the
model can be easily trained on unlabeled data via a
novel but simple aggregation operation which has
been successfully applied in the id161
literature (pinheiro and collobert, 2015). the ag-
gregation combines the scores of all source words
for a particular target word and promotes source
words which are likely to be aligned with a given
target word according to the knowledge the model
has learned so far. at test time, the aggregation op-
eration is removed and source words are aligned to
target words by choosing the highest scoring can-
didates (  2,   3).
we evaluate several forms for our aggregation
operation such as computing the sum, max and
logsumexp over alignment scores. results on
english-french, english-romanian, and czech-
english alignment show that our model signif-
icantly outperforms fast align, a popular log-
linear reparameterization of ibm model 2 (dyer
et al., 2013;   4).
2 aggregation model

in the following, we consider a target-source sen-
tence pair (e, f ), with e = (e1, . . . , e|e|) and
f = (f1, . . . , f|f|). words are represented by
fj and ei, which are indices in source and target
dictionaries. for simplicity, we assume here that
word indices are the only feature fed to our archi-
tecture. given a source word fj and a target word
ei, our architecture embeds a window (of size df

win

6
1
0
2

 

n
u
j
 

0
3

 
 
]
l
c
.
s
c
[
 
 

1
v
0
6
5
9
0

.

6
0
6
1
:
v
i
x
r
a

figure 1: illustration of the model. the two networks nete and netf compute representations for source
and target words. the score of an alignment link is a simple dot product between those source and target
word representations. the aggregation operation summarizes the alignment scores for each target word.

and de
win, respectively) centered around each of
these words into a demb-dimensional vector space.
the embedding operation is performed with two
distinct neural networks:

nete([e]

de
win
i

)     rdemb

and

df
netf ([f ]
win
j

)     rdemb ,

where we denote the window operator as

[x]d

i = (xi   d/2, . . . , xi+d/2) .

the matching score between a source word fj and
a target word ei is then given by the dot-product:

de
s(i, j) = nete([e]
win
i

)    netf ([f ]

df
win
j

) .

(1)

if ei is aligned to fai, the score s(i, ai) should be
high, while scores s(i, j)    j (cid:54)= ai should be low.
2.1 unsupervised training
in this paper, we consider an unsupervised setup
where the alignment is not known at training time.
we thus cannot minimize or maximize matching
scores (1) in a direct manner. instead, given a tar-
get word ei we consider the aggregated matching
scores over the source sentence:

where aggr is an aggregation operator (  2.2).
consider a matching (positive) sentence pair
(e+, f ) and a negative sentence pair (e   , f ).
given a word at index i+ in the positive target
sentence, we want to maximize the aggregated
score saggr(i+, f ) (1     i+     |e+|) because we
know it should be aligned to at least one source
word.1 conversely, given a word at index i    in
the negative target sentence, we want to minimize
saggr(i   , f ) (1     i   
|) because it is unlikely
that the source sentence can explain the negative
target word. following these principles, we con-
sider a simple soft-margin loss:

    |e   

   
l(e+, e

, f ) =

+

|e+|(cid:88)
|e   |(cid:88)

i+=1

i   =1

log(1 + e

   saggr(i+,f ))

log(1 + e+saggr(i   ,f )) .

(3)

training is achieved by minimizing (3) and by
sampling over triplets (e+, e   , f ) from the train-
ing data.

saggr(i, f ) =

|f|

aggr
j=1

s(i, j) ,

(2)

1we discuss how we handle unaligned target words in
  2.3. also, depending on the decoding algorithm the model
can be used to predict many-to-many alignments.

target:deembe1e2......e|e|...netedembdotproductsource:f1f2......f|f|netfdembdfemb...s(i,j)|e||f|aggr|e||f|(cid:88)

2.2 choosing the aggregation
the aggregation operation (2) is only present dur-
ing training and acts as a    lter which aims to ex-
plain a given target word ei by one or more source
words. if we had the word alignments, then we
would sum over the source words fj aligned with
ei. however, in our setup alignments are not avail-
able at training time, so we must rely on what the
model has learned so far to    lter the source words.
we consider the following strategies:

    sum: ignore the knowledge learned so far,
and assign the same weight to all source
words fj to explain ei.2 in this case, we have

saggr(i, f ) =

s(i, j) .

j=1

    max:
encourage the best aligned source
word fj, according to what the model has
learned so far. in this case, the aggregation
is written as:

saggr(i, f ) =

|f|
max
j=1

s(i, j) .

    lse: give similar weights to source words
with similar scores. this can be achieved
with a logsumexp aggregation operation
(also called logadd), and is de   ned as:

       |f|(cid:88)

j=1

       ,

saggr(i, f ) =

1
r

log

er s(i, j)

(4)

where r is a positive scalar (to be chosen)
controlling the smoothness of the aggrega-
tion. for small r, the aggregation is equiva-
lent to a sum, and for large r, the aggregation
acts as a max.

2.3 decoding
at test time, we align each target word ei with
the source word fj for which the matching score
s(i, j) in (1) is highest.3 however, not every target
word is aligned, so we consider only alignments
with a matching score above a threshold:

   
s(i, j) >   

(ei) +      

   

(ei) ,

(5)

2this can be seen by observing that the gradients for all

source words are the same.

3this may result in a source word being aligned to multi-

ple target words.

where    is a tunable hyper-parameter, and
   

   

e

  

(ei) =

(cid:2)s(k, j

)(cid:3)

{  ek=ei       e,   fj          f   }

is the expectation over all training sentences   e con-
   
taining the word ei, and all words   f
j belonging to
a corresponding negative source sentence   f   , and
     (ei) is the respective variance.
3 neural network architecture
our model consists of two convolutional neural
networks nete and netf as shown in (1). both of
them take the same form, so we detail only the tar-
get architecture.

3.1 id27s
de
are embedded into
the discrete features [e]
win
i
a de
emb-dimensional vector space via a lookup-
table operation as    rst introduced in bengio et al.
(2000):

xe
i = ltw e([e]
= (ltw e(ei   de

de
win
i

)
win/2), . . . , ltw e(ei+de

win/2)) ,
where the lookup-table operation applied at index
k returns the kth column of the parameter matrix
w e:

ltw e(k) = w e   , k .
the matrix w e is of size |v e|    de
is the target vocabulary, and de
bedding size for the target words.

emb, where v e
emb is the word em-

3.2 convolutional layers
the id27s output by the lookup-table
are concatenated and fed through two successive
1-d convolution layers. the convolutions use a
step size of one and extract context features for
each word. the kernel sizes ke
2 determine
the size of the window de
2     1 over
which features will be extracted by nete. in order
to obtain windows centered around each word, we
2)/2   1 padding words at the beginning
add (ke
and at the end of each sentence.
the    rst layer id98e applies the linear transfor-
2 times to consecutive spans

1 and ke
1 + ke

win = ke

1+ke

mation m e,1 exactly ke
of size ke

1 to the de

win words in a given window:

            ltw e([e]

...
ltw e([e]

ke
i   a)
1

ke
1
i+a)

             ,

id98e(xe

i ) = m e,1

2

hu  (de

2 (cid:99), m e,1     rde

where a = (cid:98) ke
1) is a
hu is the number of
matrix of parameters, and de
hidden units (hu). the outputs of the    rst layer
id98e are concatenated to form a matrix of size
ke
2 de

hu which is fed to the second layer:

emb ke

i ))

2 de

nete(xe

i ) = m e,2 tanh(id98e(xe

(6)
where m e,2     rdemb  (ke
hu) is a matrix of pa-
rameters, and the tanh(  ) operation is applied el-
ement wise. the parameters w e, m e,1 and m e,2
are trained by stochastic id119 to mini-
mize the loss (3) introduced in   2.1.
3.3 additional features
in addition to the raw word indices, we consider
two additional discrete features which were han-
dled in the same way as word features by introduc-
ing an additional lookup-table for each of them.
the output of all lookup-tables was concatenated,
and fed to the two-layer neural network architec-
ture (6).
distance to the diagonal. this feature can be
computed for a target word ei and a source word
fj:

(cid:12)(cid:12)(cid:12)(cid:12) i

|e|    

j
|f|

(cid:12)(cid:12)(cid:12)(cid:12) ,

diag(i, j) =

this feature allows the model to learn that aligned
sentence pairs use roughly the same word order
and that alignment links remain close to the di-
agonal. we use this feature only for the source
network because it encodes relative position infor-
mation which only needs to be encoded once. if
we would use absolute position instead, then we
would need to encode this information both on the
source and the target side.
part-of-speech words pairs that are good transla-
tions of each other are likely to carry the same part
of speech in both languages (melamed, 1995). we
therefore add the part-of-speech information to the
model.
char id165. we consider unigram character
position features. let k be the maximum size for
a word in a dictionary. we denote the dictionary
of characters as c. every character is represented
by its index c (with 1 < c < |c|). we associate
every character c at position k with a vector at po-
sition ((k     1)     |c|) + c in a lookup-table. for a
given word, we extract all unigram character po-
sition embeddings, and average them to obtain a
character embedding for a given word.

4 experiments
4.1 datasets
we use the english-french hansards corpus as
distributed by the naacl 2003 shared task (mi-
halcea and pedersen, 2003). this dataset con-
tains 1.1m sentence pairs and the test and vali-
dation sets contain 447 and 37 examples respec-
tively. we also evaluate on the romanian-english
dataset of the acl 2005 shared task (martin et al.,
2005) comprising 48k sentence pairs for training,
248 for testing and 17 for validation. for english-
czech experiments, we use the wmt news com-
mentary corpus for training (150k sentence pairs)
and a set of 515 sentences for testing (bojar and
prokopov  a, 2006).

4.2 evaluation
our models are evaluated in terms of precision, re-
call, f-measure and alignment error rate (aer).
we train models in each language direction and
then symmetrize the resulting alignments using
either the intersection or the grow-diag-   nal-and
heuristic (och and ney, 2003; koehn et al., 2003).
we validated the choice of symmetrization heuris-
tic on each language pair and chose the best one
for each model considering the two aforemen-
tioned types as well as grow-diag-   nal and grow-
diag.

additionally, we train phrase-based machine
translation models with our alignments using the
popular moses toolkit (koehn et al., 2007). for
english-french, we train on the news commentary
corpus v10, for english-czech we used news com-
mentary corpus v11, and for romanian-english
we used the europarl corpus v8. we tuned our
models on the wmt2015 test set for english-
czech as well as for romanian-english;
for
english-french we tuned on the wmt2014 test
set. final results are reported on the wmt2016
test set for english-czech as well as romanian-
english, and for english-french we report results
on the wmt2015 test set (as there is no track for
this language-pair in 2016).

we compare our model to fast align, a popu-
lar log-linear reparameterization of ibm model 2
(dyer et al., 2013).

4.3 setup
the kernel sizes of the target network nete(  ) are
set to ke
2 = 3 for all language pairs. the
kernel sizes of the source network netf (  ) are set

1 = ke

2 = 1.

1 = kf

to kf
2 = 3 for romanian-english as well as
english-czech; and for english-french we used
1 = kf
kf
the number of hidden units are de

hu =
256 and demb is set to 256, the source vf and tar-
get ve dictionaries consist of the 30k most com-
mon words for english, french and romanian,
and 80k for czech. all other words are mapped to
a unique unk token. the id27 sizes
emb and df
emb, as well as the char-id165 embed-
de
ding size is 128. for lse, we set r = 1 in (4).

hu = df

we initialize the id27s with a sim-
ple pca computed over the matrix of word co-
occurrence counts (lebret and collobert, 2014).
the co-occurrence counts were computed over the
common crawl corpus provided by wmt16. for
id52 we used the stanford parser
on english-french data, and marmot (mueller
et al., 2013) for romanian-english as well as
english-czech.

we trained 4 systems for the ensembles, each
using a different random seed to vary the weight
initialization as well as the shuf   ing of the training
set. we averaged the alignment scores predicted
by each system before decoding. the alignment
threshold variables      (ei) and      (ei) for decod-
ing (  2.3) were estimated on 1000 random training
sentences, using 100 negative sentences for each
of them. words not appearing in this training sub-
set were assigned      (ei) =      (ei) = 0.
win > 1 and df

win > 1, we
saw a tendency of aligning frequent words regard-
less on if they appeared in the center of the context
window or not. for instance, a common mistake
would be to align    the cat sat   , with    padding
le chat   . to prevent such behavior, we occasion-
ally replaced the center word in a target window
by a random word during training. we do this for
every second training example on average and we
tuned this rate on the validation set.

for systems where de

4.4 results

we    rst explore different choices for the aggre-
gation operator (  2.2), followed by an ablation to
investigate the impact of the different additional
features (  3.3). next we compare to the fast
align baseline. finally, we evaluate our align-
ments within a full translation system for all lan-
guage pairs.

4.4.1 aggregation operation
table 1 shows that the logsumexp (lse) aggre-
gator performs best on all datasets for every direc-
tion as well as in the symmetrized setting using the
grow-diag-   nal heuristic. all results are based on
a single model trained with the    distance to the di-
agonal    feature detailed above.4 we therefore use
lse for the remaining experiments.

en-fr
fr-en
symmetrized
ro-en
en-ro
symmetrized
en-cz
cz-en
symmetrized

max sum lse
15.1
18.1
15.8
20.7
12.8
14.8
37.6
42.2
35.7
40.4
32.2
36.4
24.5
27.9
24.5
26.5
21.0
21.8

23.0
26.9
24.1
42.0
40.2
35.6
35.6
33.6
32.7

table 1: alignment error rates for different aggre-
gation operations in each language direction and
with grow-diag-   nal-and symmetrization.

4.4.2 additional features
table 2 shows the effect of the different input fea-
tures. both pos and the distance to the diago-
nal feature signi   cantly improve accuracy. po-
sition information via the    distance to the diago-
nal    feature is helpful for all language pairs, and
pos information is more effective for romanian-
english and english-czech which involve mor-
phologically rich languages. we use the pos and
   distance to the diagonal feature    for the remaining
experiments.
4.4.3 comparison with the baseline
in the following results we label our model as
nnsa (neural network score aggregation). on
english-french data (table 3) our model outper-
forms the baseline (dyer et al., 2013) in each indi-
vidual language direction as well as for the sym-
metrized setting. with an ensemble of four mod-
els, we outperform the baseline by 1.7 aer (from
11.4 to 9.7), and with an individual model we out-
perform it by 1.2 aer (from 11.4 to 10.2). note
that the choice of symmetrization heuristic greatly
affects accuracy, both for the baseline and nnsa.

4we use kernel sizes ke

1 = ke

2 = 3 and kf

1 = kf

2 = 1 for

all language pairs in this experiment.

english-french

romanian-english

english-czech

en-fr
22.2
20.9
15.1
13.2

fr-en
24.2
23.9
15.8
12.1

sym ro-en en-ro
45.5
15.7
42.9
15.3
12.8
35.7
32.2
10.2

47.0
45.3
37.6
33.1

sym en-cz cz-en
36.3
40.3
33.7
36.9
32.2
24.5
22.9
27.8

36.9
35.6
24.8
24.6

sym
29.5
28.2
21.0
19.9

words
+ pos
+ diag
+ pos + diag

table 2: alignment error rates using different input features in each language direction and with grow-
diag-   nal-and symmetrization.

p

r

f1 aer

p

r

f1 aer

english-french
baseline
nnsa
+ ensemble
french-english
baseline
nnsa
+ ensemble
symmetrized
baseline (inter)
nnsa (gdfa)
+ ensemble

49.6
64.7
61.5

52.9
61.7
62.6

69.6
60.4
59.3

89.8
80.7
85.8

88.4
86.3
86.7

84.0
88.5
89.9

63.9
71.8
71.6

66.2
72.0
72.7

76.1
71.8
71.4

16.7
13.2
11.6

16.2
12.1
11.6

11.4
10.2
9.7

table 3: english-french results on the test set in
terms of precision (p), recall (r), f-score (f1) and
aer; ensemble denotes a combination of four sys-
tems and we use the intersection (inter) and grow-
diag-   nal-and symmetrization (gdfa) heuristics.

on romanian-english (table 4) our model out-
performs the baseline in both directions as well.
adding ensembles further improves accuracy and
leads to a signi   cant improvement of 6 aer over
the best symmetrized baseline result (from 32 to
26).

on english-czech (table 5) our model outper-
forms the baseline in both directions as well. we
added the character feature to better deal with the
morphologically rich nature of czech and the fea-
ture reduced aer by 2.1 in the symmetrized set-
ting. an ensemble improved accuracy further and
led to a 7 aer improvement over the best sym-
metrized baseline result (from 22.8 to 15.8).
4.4.4 id7 evaluation
table 6 presents the id7 evaluation of our align-
ments. for each language-pair, we select the best
alignment model reported in tables 3, 4 and 5, and
align the training data. we use the alignments to

romanian-english
baseline
nnsa
+ ensemble
english-romanian
baseline
nnsa
+ ensemble
symmetrized
baseline (gdfa)
nnsa (gdfa)
+ ensemble

70.0
75.1
75.8

71.3
78.1
78.4

69.5
74.1
73.0

61.0
65.2
62.8

60.8
61.7
63.2

66.5
71.8
74.5

65.2
69.8
68.7

65.6
69.0
70.0

68.0
73.0
73.7

34.8
30.2
31.3

34.4
31.1
30.0

32.0
27.0
26.0

table 4: romanian-english results (cf. table 3).

run the standard phrase-based training pipeline us-
ing those alignments. our id7 results show the
average id7 score and standard deviation for
   ve runs of minimum error rate training (mert;
och 2003).

our alignments achieve slightly better results
for romanian-english as well as english-czech
while performing on par with fast align on
english-french translation.

5 analysis

in this section, we analyze the word representa-
tions learned by our model. we    rst focus on the
source representations: given a source window,
we obtain its distributional representation and then
compute the euclidean distance to all other source
windows in the training corpus. table 7 shows
the nearest windows for two source windows; the
closest windows tend to have similar meanings.

we then analyze the relation between source
and target representations: given a source win-
dow we compute the alignment scores for all tar-
get sentences in the training corpus. table 8 shows
for two source windows which target words have

p

r

f1 aer

the voting process

english-czech
baseline
nnsa
+ char id165
+ ensemble
czech-english
baseline
nnsa
+ char id165
+ ensemble
symmetrized
baseline (inter)
nnsa (gdfa)
+ char id165
+ ensemble

68.4
72.0
73.8
78.8

68.6
74.1
78.1
79.1

88.1
75.7
76.9
78.9

73.3
74.3
75.4
77.2

74.0
74.0
74.1
77.7

66.6
80.3
81.3
83.2

70.7
73.1
74.6
78.0

71.2
74.0
76.1
78.4

76.0
76.3
79.1
81.0

26.6
24.6
23.2
20.0

25.7
22.9
21.4
18.7

22.8
19.9
17.8
15.8

table 5: czech-english results (cf. table 3).

nnsa

french-english
romanian-english
czech-english

baseline
25.4    0.1 25.5    0.1
21.3    0.1 21.6    0.1
17.2    0.1 17.6    0.1
table 6: average id7 score and standard devia-
tion for    ve runs of mert.

the largest alignment scores. the example    in
working together    is particularly interesting since
the aligned target words collabore, coordon  es,
and concert  es mean collaborate, coordinated, and
concerted, which all carry the same meaning as
the source window phrase.

6 conclusion
in this paper, we present a simple neural network
alignment model trained on unlabeled data. our
model computes alignment scores as dot prod-
ucts between representations of windows around
source and target words. we apply an aggrega-
tion operation borrowed from the computer vi-
sion literature to make unsupervised training pos-
sible. the aggregation operation acts as a    lter
over alignment scores and allows us to determine
which source words explain a given target word.

we improve over fast align, a popular log-
linear reparameterization of ibm model 2 (dyer
et al., 2013) by up to 6 aer on romanian-
english, 7 aer on english-czech data and 1.7
aer on english-french alignment. furthermore,

in working together
for working together
with working together
from working together
about working together

the voting area
the voting power
the voting rules
the voting system
by working together
the voting patterns
the voting ballots
and working together
their voting patterns while working together

table 7: analysis of source window represen-
tations. each column shows a window over the
source sentence followed by several close neigh-
bors in terms of euclidean distance (among the 30
nearest).

the voting process

in working together

vote

voteraient

votent
voter
votant
scrutin
suffrage
proc  edure
investiture
  elections

travaill  e

travailleront
collaboration

travaillant
oeuvrant
concerts
coordon  es
concert
collabore
coop  eration

table 8: analysis of source and target represen-
tations. each column shows a source window and
the target words which are most aligned according
to our model.

we evaluated our model as part of a full machine
translation pipeline and showed that our align-
ments are better or on par compared to fast align
in terms of id7.

references
yoshua bengio, r  ejean ducharme, and pascal
a neural probabilistic language

vincent.
model. in nips, 2000.

ond  rej bojar and magdalena prokopov  a. czech-
in proceedings of
english word alignment.
the fifth international conference on language
resources and evaluation (lrec 2006), 2006.
peter f. brown, john cocke, stephen della
pietra, vincent j. della pietra, frederick je-
linek, john d. lafferty, robert l. mercer, and
paul s. roossin. a statistical approach to ma-
chine translation. computational linguistics,
1990.

stephan vogel, hermann ney, and christoph till-
mann. id48-based word alignment in statis-
tical translation. in proc. of coling, 1996.

nan yang, shujie liu, mu li, ming zhou, and
nenghai yu. word alignment modeling with
context dependent deep neural network.
in
proc. of acl, 2013.

david chiang. hierarchical phrase-based trans-

lation. computational linguistics, 2007.

chris dyer, victor chahuneau, and noah a.
smith. a simple, fast, and effective reparame-
terization of ibm model 2. in proc. of naacl,
2013.

philipp koehn, franz j. och, and daniel marcu.
statistical phrase-based translation. in proc. of
naacl, 2003.

philipp koehn, hieu hoang, alexandra birch,
chris callison-burch, marcello federico,
nicola bertoldi, brooke cowan, wade shen,
christine moran, richard zens, chris dyer,
ond  rej bojar, alexandra constantin, and evan
herbst. moses: open source toolkit for statisti-
cal machine translation. in proceedings of the
45th annual meeting of the acl on interactive
poster and demonstration sessions, acl    07,
2007.

r  emi lebret and ronan collobert. word em-
in proc. of

beddings through hellinger pca.
eacl, 2014.

joel martin, rada mihalcea, and ted pedersen.
word alignment for languages with scarce
resources. in proc. of wpt, 2005.

dan i. melamed. automatic evaluation and uni-
form    lter cascades for inducing n-best transla-
tion lexicons. in third workshop on very large
corpora, 1995.

rada mihalcea and ted pedersen. an evaluation
exercise for word alignment. in proc. of wpt,
2003.

thomas mueller, helmut schmid, and hinrich
sch  utze. ef   cient higher-order crfs for mor-
phological tagging. in proceedings of the 2013
conference on empirical methods in natural
language processing, 2013.

franz j. och and hermann ney. a systematic
comparison of various statistical alignment
models. computational linguistics, 2003.

franz josef och. minimum error rate training in
id151. in proc of acl,
2003.

pedro o. pinheiro and ronan collobert. from
image-level to pixel-level labeling with con-
volutional networks. in proc. of cvpr, 2015.
akihiro tamura, taro watanabe, and eiichiro
sumita. recurrent neural networks for word
alignment model. in proc. of acl, 2014.

