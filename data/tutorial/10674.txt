a strong baseline for learning cross-lingual id27s

from sentence alignments

omer levy   

university of washington

seattle, wa

omerlevy@gmail.com

anders s  gaard   

university of copenhagen

copenhagen, denmark
soegaard@di.ku.dk

yoav goldberg

bar-ilan university
ramat-gan, israel

yoav.goldberg@gmail.com

7
1
0
2

 

n
a
j
 

9

 
 
]
l
c
.
s
c
[
 
 

2
v
6
2
4
5
0

.

8
0
6
1
:
v
i
x
r
a

abstract

while cross-lingual id27s
have been studied extensively in recent
years, the qualitative differences between
the different algorithms remain vague. we
observe that whether or not an algorithm
uses a particular feature set (sentence ids)
accounts for a signi   cant performance gap
among these algorithms. this feature set
is also used by traditional alignment al-
gorithms, such as ibm model-1, which
demonstrate similar performance to state-
of-the-art embedding algorithms on a va-
riety of benchmarks. overall, we observe
that different algorithmic approaches for
utilizing the sentence id feature space re-
sult in similar performance. this paper
draws both empirical and theoretical par-
allels between the embedding and align-
ment literature, and suggests that adding
additional sources of information, which
go beyond the traditional signal of bilin-
gual sentence-aligned corpora, may sub-
stantially improve cross-lingual word em-
beddings, and that future baselines should
at least take such features into account.

1 introduction

space.

embedding

the vocabularies of
in one

algorithms
cross-lingual word
two or
try to represent
common continu-
more
languages
ous vector
these vectors can be
used to improve monolingual word similarity
(faruqui and dyer, 2014) or support cross-lingual
transfer
in this
work, we focus on the second (cross-lingual)
aspect of these embeddings, and try to determine
what makes some embedding approaches better

(gouws and s  gaard, 2015).

   these authors contributed equally to this work.

than others on a set of translation-oriented bench-
marks. while cross-lingual id27s
have been used for a variety of cross-lingual
transfer tasks, we prefer evaluating on translation-
oriented benchmarks, rather than across speci   c
nlp tasks, since the translation setting allows
for a cleaner examination of cross-lingual sim-
ilarity. another important delineation of this
work is that we focus on algorithms that rely
on sentence-aligned data; in part, because these
algorithms are particularly interesting for low-
resource languages, but also to make our analysis
and comparison with alignment algorithms more
focused.

we observe that the top performing embed-
ding algorithms share the same underlying fea-
ture space     sentence ids     while their differ-
ent algorithmic approaches seem to have a neg-
ligible impact on performance. we also notice
that several statistical alignment algorithms, such
as ibm model-1 (brown et al., 1993), operate un-
der the same data assumptions. speci   cally, we
   nd that using the translation probabilities learnt
by model-1 as the cross-lingual similarity func-
tion (in place of the commonly-used cosine simi-
larity between id27s) performs on-par
with state-of-the-art cross-lingual embeddings on
word alignment and bilingual dictionary induction
tasks.
in other words, as long as the similarity
function is based on the sentence id feature space
and the embedding/alignment algorithm itself is
not too na    ve, the actual difference in performance
between different approaches is marginal.

this leads us to revisit another statistical align-
ment algorithm from the literature that uses the
same sentence-based signal     the dice aligner
(och and ney, 2003). we    rst observe that the
vanilla dice aligner is signi   cantly outperformed
by the model-1 aligner. we then recast dice as the
dot-product between two word vectors (based on

the sentence id feature space), which allows us to
generalize it, resulting in an embedding model that
is as effective as model-1 and other sophisticated
state-of-the-art embedding methods, but takes a
fraction of the time to train.

existing approaches for creating cross-lingual
id27s are typically restricted to train-
ing bilingual embeddings, mapping exactly two
languages onto a common space. we show that
our generalization of the dice coef   cient can be
augmented to jointly train multi-lingual embed-
dings for any number of languages. we do this
by leveraging the fact that the space of sentence
ids is shared among all languages in the paral-
lel corpus; the verses of the bible, for example,
are identical across all translations.
introducing
this multi-lingual signal shows a signi   cant per-
formance boost, which eclipses the variance in
performance among pre-existing embedding algo-
rithms.

contributions we    rst establish the importance
of the sentence id feature space for cross-lingual
id27 algorithms through experiments
across several
translation-oriented benchmarks.
we then compare cross-lingual id27
algorithms to traditional word alignment algo-
rithms that also rely on sentence id signals. we
show that a generalization of one of these, the
dice aligner, is a very strong baseline for cross-
lingual id27 algorithms, performing
better than several state-of-the-art algorithms, es-
pecially when exploiting a multi-lingual signal.
our code and data are publicly available.1

2 background: cross-lingual

embeddings

previous approaches to cross-lingual word em-
beddings can be divided into three categories,
according to assumptions on the training data.
the    rst category assumes word-level align-
ments,
in the form of bilingual dictionaries
(mikolov et al., 2013a;
xiao and guo, 2014)
automatically produced word alignments
or
(klementiev et al., 2012;
zou et al., 2013;
faruqui and dyer, 2014).
sizable bilingual
dictionaries are not available for many language
pairs, and the quality of automatic word alignment
greatly affects the quality of the embeddings.
it
is also unclear whether the embedding process

1bitbucket.org/omerlevy/xling_embeddings

provides signi   cant added value beyond the
initial word alignments (zou et al., 2013). we
therefore exclude these algorithms for this study,
also in order to focus our analysis and make
the comparison with traditional word alignment
algorithms more straightforward.

the second category makes a much weaker as-
sumption, document-level alignments, and uses
comparable texts in different languages (not nec-
essarily translations) such as wikipedia articles or
news reports of the same event. algorithms in this
category try to leverage massive amounts of data
to make up for the lack of lower-level alignments
(s  gaard et al., 2015; vuli  c and moens, 2016).

algorithms in the third category take the middle
ground; they use sentence-level alignments, com-
mon in legal translations and religious texts. also
known as    parallel corpora   , sentence-aligned
data maps each sentence (as a whole) to its trans-
lation. we focus on this third category, because
it does not require the strict assumption of word-
aligned data (which is dif   cult to obtain), while
still providing a cleaner and more accurate signal
than document-level alignments (which have been
shown, in monolingual data, to capture mainly
syntagmatic relations (sahlgren, 2006)). in   6, we
provide evidence to the hypothesis that sentence-
aligned data is indeed far more informative than
document-aligned data.

algorithms that rely on sentence-aligned data
typically create intermediate sentence represen-
tations from each sentence   s constituent words.
hermann and blunsom (2014) proposed a deep
neural model, bicvm, which compared the
two sentence representations at the    nal layer,
while chandar et al. (2014) proposed a shal-
lower autoencoder-based model, representing both
source and target language sentences as the same
intermediate sentence vector. recently, a simpler
model, bilbowa (gouws et al., 2015), showed
similar performance without using a hidden
sentence-representation layer, giving it a dramatic
speed advantage over its predecessors. bilbowa
is essentially an extension of skip-grams with neg-
ative sampling (sgns) (mikolov et al., 2013b),
which simultaneously optimizes each word   s sim-
ilarity to its inter-lingual context (words that ap-
peared in the aligned target language sentence)
and its intra-lingual context (as in the original
monolingual model).
luong et al. (2015) pro-
posed a similar sgns-based model over the same

features.

we study which factors determine the suc-
cess of cross-lingual id27 algorithms
that use sentence-aligned data, and evaluate them
against baselines from the statistical machine
translation literature that incorporate the same data
assumptions. we go on to generalize one of these,
the dice aligner, showing that one variant is a
much stronger baseline for cross-lingual word em-
bedding algorithms than standard baselines.

finally, we would like to point out the work of
upadhyay et al. (2016), who studied how different
data assumptions affect embedding quality in both
monolingual and cross-lingual tasks. our work fo-
cuses on one speci   c data assumption (sentence-
level alignments) and only on cross-lingual us-
age. this more restricted setting allows us to:
(a) compare embeddings to alignment algorithms,
(b) decouple the feature space from the algorithm,
and make a more speci   c observation about the
contribution of each component to the end result.
in that sense, our    ndings complement those of
upadhyay et al. (2016).

3 which features make better
cross-lingual embeddings?

we group state-of-the-art cross-lingual embedding
algorithms according to their feature sets, and
compare their performance on two cross-lingual
benchmarks: word alignment and bilingual dic-
tionary induction. in doing so, we hope to learn
which features are more informative.

3.1 features of sentence-aligned data

we observe that cross-lingual embeddings typi-
cally use parallel corpora in one of two ways:

source + target language words each word
w is represented using all the other words that ap-
peared with it in the same sentence (source lan-
guage words) and all the words that appeared in
target language sentences that were aligned to sen-
tences in which the word w appeared (target lan-
guage words). this representation also stores the
number of times each pair of word w and feature
(context) word f co-occurred.

these features are analogous to the ones used
by vuli  c and moens (2016) for document-aligned
data, and can be built in a similar manner: create a
pseudo-bilingual sentence from each aligned sen-
tence, and for each word in question, consider all

the other words in this sentence as its features. bil-
bowa (gouws et al., 2015) also uses a similar set
of features, but restricts the source language words
to those that appeared within a certain distance
from the word in question, and de   nes a slightly
different interaction with target language words.

sentence ids here, each word is represented by
the set of sentences in which it appeared, indiffer-
ent to the number of times it appeared in each one.
this feature set is also indifferent to the word or-
dering within each sentence. this approach is im-
plicitly used by chandar et al. (2014), who encode
the bag-of-words representations of parallel sen-
tences into the same vector. thus, each word is not
matched directly to another word, but rather used
to create the sentence   s language-independent rep-
resentation. s  gaard et al. (2015) use similar fea-
tures, document ids, for leveraging comparable
wikipedia articles in different languages. in   6 we
show that when using sentence ids, even a small
amount of sentence-aligned data is more powerful
than a huge amount of comparable documents.

3.2 experiment setup
algorithms we use the four algorithms men-
tioned in   3.1: bilbowa (gouws et al., 2015),
bwe-skipgram (vuli  c and moens, 2016), bilin-
gual autoencoders (chandar et al., 2014), and in-
verted index (s  gaard et al., 2015). while both
bwe-skipgram and inverted index were origi-
nally trained on document-aligned data, in this
work, we apply them to sentence-aligned data.

data christodouloupoulos and steedman (2015)
collected translations of the bible (or parts of it)
in over 100 languages, naturally aligned by book,
chapter, and verse (31,102 verses in total).2 this
corpus allows us to evaluate methods across many
different
languages, while controlling for the
training set   s size. the corpus was decapitalized
and tokenized using white spaces after splitting at
punctuation.

benchmarks we measure the quality of each
embedding using both manually annotated word
alignment datasets and bilingual dictionaries.
we use 16 manually annotated word alignment
datasets     hansards3 and data from four other
sources (graca et al., 2008; lambert et al., 2005;
mihalcea and pedersen, 2003;

2homepages.inf.ed.ac.uk/s0787820/bible/
3www.isi.edu/natural-language/download/hansard/

holmqvist and ahrenberg, 2011;
cakmak et al., 2012)     as well as 16 bilingual
dictionaries from wiktionary.

in the word alignment benchmark, each word
in a given source language sentence is aligned
with the most similar target language word from
the target language sentence     this is exactly the
same greedy decoding algorithm that is imple-
mented in ibm model-1 (brown et al., 1993). if a
source language word is out of vocabulary, it is not
aligned with anything, whereas target language
out-of-vocabulary words are given a default mini-
mal similarity score, and never aligned to any can-
didate source language word in practice. we use
the inverse of alignment error rate (1-aer) as de-
scribed in koehn (2010) to measure performance,
where higher scores mean better alignments.

high quality, freely available, manually anno-
tated word alignment datasets are rare, especially
for non-european languages. we therefore include
experiments on bilingual dictionary induction. we
obtain bilingual dictionaries from wiktionary for
   ve non-indo-european languages, namely: ara-
bic, finnish, hebrew, hungarian, and turkish (all
represented in the edinburgh bible corpus). we
emphasize that unlike most previous work, we ex-
periment with    nding translation equivalents of
all words and do not    lter the source and tar-
get language words by part of speech. we use
precision-at-one (p@1), essentially selecting the
closest target-language word to the given source-
language word as the translation of choice. this
often means that 100% precision is unattainable,
since many words have multiple translations.

hyperparameters levy et al. (2015) exposed a
collection of hyperparameters that affect the per-
formance of monolingual embeddings. we as-
sume that the same is true for cross-lingual embed-
dings, and use their recommended settings across
all algorithms (where applicable). speci   cally, we
used 500 dimensions for every algorithm, context
distribution smoothing with    = 0.75 (applica-
ble to bilbowa and bwe-skipgram), the sym-
metric version of svd (applicable to inverted in-
dex), and run iterative algorithms for 100 epochs
to ensure convergence (applicable to all algorithms
except inverted index). for bilbowa   s monolin-
gual context window, we used the default of 5.
similarity is always measured by the vectors    co-
sine. most importantly, we use a shared vocab-
ulary, consisting of every word that appeared at

least twice in the corpus (tagged with language
id). while hyperparameter tuning could admit-
tedly affect results, we rarely have data for reliably
tuning hyperparameters for truly low-resource lan-
guages.

3.3 results

table 1 shows that the two algorithms based on
the sentence-id feature space perform consistently
better than those using source+target words. we
suspect that the source+target feature set might
be capturing more information than is actually
needed for translation, such as syntagmatic or top-
ical similarity between words (e.g.    dog           ken-
nel   ). this might be distracting for cross-lingual
tasks such as word alignment and bilingual dictio-
nary induction. sentence id features, on the other
hand, are simpler, and might therefore contain a
cleaner translation-oriented signal.

it is important to state that, in absolute terms,
these results are quite poor. the fact that the
best inverse aer is around 50% calls into ques-
tion the ability to actually utilize these embed-
dings in a real-life scenario. while one may
suggest that this is a result of the small train-
ing dataset (edinburgh bible corpus), previous
work (e.g.
(chandar et al., 2014)) used an even
smaller dataset (the    rst 10k sentences in europarl
(koehn, 2005)). to ensure that our results are not
an artifact of the edinburgh bible corpus, we re-
peated our experiments on the full europarl corpus
(180k sentences) for a subset of languages (en-
glish, french, and spanish), and observed similar
trends. as this is a comparative study focused on
analyzing the qualitative differences between al-
gorithms, we place the issue of low absolute per-
formance aside for the moment, and reopen it in
  5.4.

4 comparing cross-lingual embeddings

with traditional alignment methods

sentence ids are not unique to modern embedding
methods, and have been used by statistical ma-
chine translation from the very beginning. in par-
ticular, the dice coef   cient (och and ney, 2003),
which is often used as a baseline for more sophis-
ticated alignment methods, measures the cross-
lingual similarity of words according to the num-
ber of aligned sentences in which they appeared.
ibm model-1 (brown et al., 1993) also makes ex-
actly the same data assumptions as other sentence-

source+target words

sentence ids

bilbowa

bwe

bilingual

skipgram autoencoders

en
fr
en
es
en
pt
en
fr
en
es
en
ro
en
sv
en
tr
en
fr
en
es
en
pt
en
ar
en
   
en
he
en
hu
en
tr

fr
en
es
en
pt
en
fr
en
es
en
ro
en
sv
en
tr
en
fr
en
es
en
pt
en
ar
en
   
en
he
en
hu
en
tr
en

) grac   a
r
e
a
-
1
(

hansards

lambert

mihalcea

holmqvist

cakmak

wiktionary

t
n
e
m
n
g
i
l

a
d
r
o

w

)
1
@
p
(
n
o
i
t
c
u
d
n
i

y
r
a
n
o
i
t
c
i
d

average*

top 1

.3653
.3264
.2723
.2953
.3716
.3949
.3189
.3206
.1576
.1617
.1621
.1598
.2092
.2121
.1302
.1479
.1096
.1305
.0630
.0650
.1384
.1573
.0385
.0722
.0213
.0527
.0418
.0761
.0533
.0810
.0567
.0851
.1640

0

.3538
.3676
.3156
.3740
.3983
.4272
.3109
.3314
.1897
.2073
.1848
.2042
.2373
.2853
.1547
.1571
.2176
.2358
.1246
.1399
.3869
.4119
.1364
.2408
.1280
.1877
.1403
.1791
.2299
.2759
.2207
.2598
.2505
3.5

.4376
.4488
.5000
.5076
.4449
.4474
.4083
.4218
.2960
.2905
.2366
.2545
.2746
.2994
.2256
.2661
.2475
.2762
.2738
.3012
.3281
.3661
.0995
.1958
.0887
.1597
.0985
.1701
.1679
.2234
.1770
.2069
.2856

15

inverted
index
.3499
.3995
.3443
.4545
.3263
.3902
.3336
.3749
.2268
.2696
.1951
.2133
.2357
.2881
.1731
.2665
.3125
.3466
.3135
.3574
.3866
.4190
.1364
.2825
.1367
.2477
.1284
.2179
.2182
.3204
.2245
.2835
.2867
13.5

table 1: the performance of four state-of-the-art cross-lingual embedding methods. * averages across two different metrics.

id methods. it therefore makes sense to use dice
similarity and the translation probabilities derived
from ibm model-1 as baselines for cross-lingual
embeddings that use sentence ids.

in fact,

from table 2 we learn that the existing em-
bedding methods are not really better than ibm
their average performance
model-1.
is even slightly lower than model-1   s.
al-
though bilingual autoencoders, inverted index,
and model-1 re   ect entirely different algorithmic
approaches (respectively: neural networks, matrix
factorization, and em), the overall difference in
performance seems to be rather marginal. this
suggests that the main performance factor is not
the algorithm, but the feature space: sentence ids.
however, dice also relies on sentence ids, yet
its performance is signi   cantly worse. we suggest
that dice uses the sentence-id feature set na    vely,

resulting in degenerate performance with respect
to the other methods. in the following section, we
analyze this shortcoming and show that general-
izations of dice actually do yield similar perfor-
mance model-1 and other sentence-id methods.

5 generalized dice

in this section, we show that the dice coef   -
cient (och and ney, 2003) can be seen as the dot-
product between two word vectors represented
over the sentence-id feature set. after providing
some background, we demonstrate the mathemat-
ical connection between dice and word-feature
matrices. we then introduce a new variant of dice,
sid-sgns, which performs on-par with model-1
and the other embedding algorithms. this vari-
ant is able to seaid113ssly leverage the multi-lingual
nature of sentence ids, giving it a small but signif-

embeddings

bilingual

autoencoders

en
fr
en
es
en
pt
en
fr
en
es
en
ro
en
sv
en
tr
en
fr
en
es
en
pt
en
ar
en
   
en
he
en
hu
en
tr

fr
en
es
en
pt
en
fr
en
es
en
ro
en
sv
en
tr
en
fr
en
es
en
pt
en
ar
en
   
en
he
en
hu
en
tr
en

) grac   a
r
e
a
-
1
(

hansards

lambert

mihalcea

holmqvist

cakmak

wiktionary

t
n
e
m
n
g
i
l

a
d
r
o

w

)
1
@
p
(
n
o
i
t
c
u
d
n
i

y
r
a
n
o
i
t
c
i
d

average
top 1

.4376
.4488
.5000
.5076
.4449
.4474
.4083
.4218
.2960
.2905
.2366
.2545
.2746
.2994
.2256
.2661
.2475
.2762
.2738
.3012
.3281
.3661
.0995
.1958
.0887
.1597
.0985
.1701
.1679
.2234
.1770
.2069
0.2856

8

inverted
index
.3499
.3995
.3443
.4545
.3263
.3902
.3336
.3749
.2268
.2696
.1951
.2133
.2357
.2881
.1731
.2665
.3125
.3466
.3135
.3574
.3866
.4190
.1364
.2825
.1367
.2477
.1284
.2179
.2182
.3204
.2245
.2835
0.2867

12

alignment
algorithms
ibm

dice

.3355
.3470
.3919
.3120
.3569
.3598
.3614
.3663
.2057
.1947
.2030
.1720
.2435
.2541
.2285
.2458
.1104
.1330
.1072
.1417
.1384
.1719
.0449
.0610
.0423
.0463
.0358
.0328
.0569
.0737
.0406
.0820
0.1843

0

model-1

.4263
.4248
.4251
.4243
.4729
.4712
.4360
.4499
.2400
.2443
.2335
.2214
.3405
.3559
.3154
.3494
.1791
.1816
.0903
.1131
.3779
.4358
.1316
.2873
.1340
.2394
.1224
.2000
.2219
.2985
.1985
.3073
0.2922

12

table 2: the performance of embedding and alignment methods based on the sentence id feature set.

icant edge over model-1.

5.1 word-feature matrices

in the word similarity literature,
it is common
to represent words as real-valued vectors and
compute their    semantic    similarity with vec-
tor similarity metrics, such as the cosine of
two vectors. these word vectors are tradition-
ally derived from sparse word-feature matrices,
either by using the matrix   s rows as-is (also
known as    explicit    representation) or by induc-
ing a lower-dimensional representation via matrix
factorization (turney and pantel, 2010). many
modern methods, such as those in id97
(mikolov et al., 2013b), also create vectors by fac-
torizing word-feature matrices, only without rep-
resenting these matrices explicitly.

formally, we are given a vocabulary of words
vw and a feature space (   vocabulary of features   )
vf . these features can be, for instance, the set of
sentences comprising the corpus. we then de   ne
a matrix m of |vw | rows and |vf | columns. each
entry in m represents some statistic pertaining to
that combination of word and feature. for exam-
ple, mw,f could be the number of times the word
w appeared in the document f .

the matrix m is typically processed into a
   smarter    matrix that re   ects the strength of as-
sociation between each given word w and feature
f . we present three common association met-
rics: l1 row id172 (equation (1)), in-
verse document frequency (idf, equation (2)),
and pointwise mutual information (pmi, equa-
tion (3)). the following equations show how to

compute their respective matrices:

m l1

w,f = i(w,f )

i(w,   )

m idf

w,f = log |vf |

i(w,   )

m p m i

w,f = log #(w,f )  #(   ,   )

#(w,   )  #(   ,f )

(1)

(2)

(3)

where #(  ,   ) is the co-occurrence count function,
i(  ,   ) is the co-occurrence indicator function, and
    is a wildcard.4

to obtain word vectors of lower dimension-
the processed matrix
ality (vf may be huge),
is then decomposed,
typically with svd. an
alternative way to create low-dimensional word
vectors without explicitly constructing m is to
use the negative sampling algorithm (sgns)
(mikolov et al., 2013b).5 this algorithm factor-
izes m p m i using a weighted non-linear objective
(levy and goldberg, 2014).

5.2 reinterpreting the dice coef   cient
in id151, the dice coef-
   cient is commonly used as a baseline for word
alignment (och and ney, 2003). given sentence-
aligned data, it provides a numerical measure of
how likely two words     a source-language word
ws and a target-language word wt     are each
other   s translation:

dice(ws, wt) = 2  s(ws,wt)

s(ws,   )  s(   ,wt)

(4)

where s(  ,   ) is the number of aligned sentences in
the data where both arguments occurred.

we claim that

this metric is mathemati-
cally equivalent to the dot-product of two l1-
normalized sentence-id word-vectors, multiplied
by 2. in other words, if we use the combination of
sentence-id features and l1-id172 to cre-
ate our word vectors, then for any ws and wt:

ws    wt = dice(ws,wt)

2

(5)

to demonstrate this claim, let us look at the dot-

product of ws and wt:

ws    wt = pi (cid:16) i(ws,i)

i(wt,   )(cid:17)
i(ws,   )    i(wt,i)

(6)

4a function with a wildcard should be interpreted as
i(w,    ) =

the sum of all possible instantiations, e.g.
px i(w, x).

5for consistency with prior art, we refer to this algorithm
as sgns (skip-grams with negative sampling), even when it
is applied without the skip-gram feature model.

where i is the index of an aligned sentence. since
i(ws,    ) = s(ws,    ) and i(wt,    ) = s(   , wt), and
both are independent of i, we can rewrite the equa-
tion as follows:

ws    wt = pi i(ws,i)  i(wt,i)
s(ws,   )  s(   ,wt)

(7)

since i(w, i) is an indicator function of whether
the word w appeared in sentence i, it stands to rea-
son that the product i(ws, i)    i(wt, i) is an indica-
tor of whether both ws and wt appeared in i. ergo,
the numerator of equation (7) is exactly the num-
ber of aligned sentences in which both ws and wt
occurred: s(ws, wt). therefore:

ws    wt = s(ws,wt)

s(ws,   )  s(   ,wt) = dice(ws,wt)

2

(8)

implies that

this theoretical result
the cross-
lingual similarity function derived from embed-
dings based on sentence ids is essentially a gener-
alization of the dice coef   cient.

5.3 sgns with sentence ids
the dice coef   cient appears to be a particu-
larly na    ve variant of matrix-based methods that
use sentence ids.
for example, inverted in-
dex (s  gaard et al., 2015)), which uses svd over
idf followed by l2 id172 (instead of
l1 id172), shows signi   cantly better per-
formance. we propose using a third vari-
ant, sentence-id sgns (sid-sgns), which sim-
ply applies sgns (mikolov et al., 2013b) to the
word/sentence-id matrix (see   5.1).

table 3 compares its performance (bilingual
sid-sgns) to the other methods, and shows that
indeed, this algorithm behaves similarly to other
sentence-id-based methods. we observe similar
results for other variants as well, such as svd over
positive pmi (not shown).

5.4 embedding multiple languages
up until now, we used bilingual data to train cross-
lingual embeddings, even though our parallel cor-
pus (the bible) is in fact multi-lingual. can we
make better use of this fact?

an elegant property of the sentence-id feature
set is that it is a truly inter-lingual representation.
this means that multiple languages can be repre-
sented together in the same matrix before factor-
izing it. this raises a question: does dimensional-
ity reduction over a multi-lingual matrix produce
better cross-lingual vectors than doing so over a
bilingual matrix?

prior art

this work

bilingual

autoencoders

en
fr
en
es
en
pt
en
fr
en
es
en
ro
en
sv
en
tr
en
fr
en
es
en
pt
en
ar
en
   
en
he
en
hu
en
tr

fr
en
es
en
pt
en
fr
en
es
en
ro
en
sv
en
tr
en
fr
en
es
en
pt
en
ar
en
   
en
he
en
hu
en
tr
en

) grac   a
r
e
a
-
1
(

hansards

lambert

mihalcea

holmqvist

cakmak

wiktionary

t
n
e
m
n
g
i
l

a
d
r
o

w

)
1
@
p
(
n
o
i
t
c
u
d
n
i

y
r
a
n
o
i
t
c
i
d

average
top 1

.4376
.4488
.5000
.5076
.4449
.4474
.4083
.4218
.2960
.2905
.2366
.2545
.2746
.2994
.2256
.2661
.2475
.2762
.2738
.3012
.3281
.3661
.0995
.1958
.0887
.1597
.0985
.1701
.1679
.2234
.1770
.2069
0.2856

2

ibm

inverted
index model-1
.3499
.3995
.3443
.4545
.3263
.3902
.3336
.3749
.2268
.2696
.1951
.2133
.2357
.2881
.1731
.2665
.3125
.3466
.3135
.3574
.3866
.4190
.1364
.2825
.1367
.2477
.1284
.2179
.2182
.3204
.2245
.2835
0.2867

.4263
.4248
.4251
.4243
.4729
.4712
.4360
.4499
.2400
.2443
.2335
.2214
.3405
.3559
.3154
.3494
.1791
.1816
.0903
.1131
.3779
.4358
.1316
.2873
.1340
.2394
.1224
.2000
.2219
.2985
.1985
.3073
0.2922

0

8

bilingual multilingual
sid-sgns
sid-sgns

.4167
.4300
.4200
.3610
.3983
.4272
.3810
.3806
.2471
.2415
.1986
.1914
.2373
.2853
.1547
.1571
.3182
.3379
.3268
.3483
.3869
.4119
.1364
.2408
.1280
.1877
.1403
.1791
.2299
.2759
.2207
.2598
0.2830

0

.4433
.4632
.4893
.5015
.4047
.4151
.4091
.4302
.2989
.3049
.2514
.2753
.2737
.3195
.2404
.2945
.3304
.3893
.3509
.3868
.4058
.4376
.1605
.3082
.1591
.2584
.1448
.2403
.2482
.3372
.2437
.3080
0.3289

22

table 3: the performance of sid-sgns compared to state-of-the-art cross-lingual embedding methods and traditional align-
ment methods.

we test our hypothesis by comparing the per-
formance of embeddings trained with sid-sgns
over all 57 languages of the bible corpus to that
of the bilingual embeddings we used earlier. this
consistently improves performance across all the
development benchmarks, providing a 4.69% av-
erage increase in performance (table 3). with this
advantage, sid-sgns performs signi   cantly bet-
ter than the other methods combined.6 this result
is similar in vein to recent    ndings in the parsing
literature (ammar et al., 2016; guo et al., 2016),
where multi-lingual transfer was shown to im-
prove upon bilingual transfer.

in absolute terms, multilingual sid-sgns   s

6we observed a similar increase in performance when ap-
plying the multi-lingual signal to s  gaard et al.   s (2015) idf-
based method and to svd over positive pmi.

performance is still very low. however, this exper-
iment demonstrates that one way of making sig-
ni   cant improvement in cross-lingual embeddings
is by considering additional sources of informa-
tion, such as the multi-lingual signal demonstrated
here. we hypothesize that, regardless of the algo-
rithmic approach, relying solely on sentence ids
from bilingual parallel corpora will probably not
be able to improve much beyond ibm model-1.

6 data paradigms

in   2, we assumed that using sentence-aligned
data is a better approach than utilizing document-
aligned data. is this the case?

to compare the data paradigms, we run the
same algorithm, sid-sgns, also on document

the bible wikipedia

grac   a

hansards

lambert

wiktionary

en
fr
en
es
en
fr
en
es
en
fr
en
es

fr
en
es
en
fr
en
es
en
fr
en
es
en

average
top 1

.3169
.3089
.3225
.3207
.3661
.3345
.2161
.2123
.3232
.3418
.3307
.3509
.3121

10

.2602
.2440
.2429
.2504
.2365
.1723
.1215
.1027
.3889
.4135
.3262
.3310
.2575

2

table 4: the performance of sid-sgns with sentence-
aligned data from the bible (31,102 verses) vs document-
aligned data from wikipedia (195,000 documents).

ids from wikipedia.7 we use the bilingual (not
multilingual) version for both data types to con-
trol for external effects. during evaluation, we use
a common vocabulary for both sentence-aligned
and document-aligned embeddings.

table 4 shows that using sentence ids from the
bible usually outperforms wikipedia. this re-
markable result, where a small amount of paral-
lel sentences is enough to outperform one of the
largest collections of multi-lingual texts in exis-
tence, indicates that document-aligned data is an
inferior paradigm for translation-related tasks such
as word alignment and dictionary induction.

7 conclusions

in this paper, we draw both empirical and the-
oretical parallels between modern cross-lingual
id27s based on sentence alignments
and traditional word alignment algorithms. we
show the importance of sentence id features and
present a new, strong baseline for cross-lingual
id27s, inspired by the dice aligner.
our results suggest that apart from faster algo-
rithms and more compact representations, recent
cross-lingual id27 algorithms are still
unable to outperform the traditional methods by a
signi   cant margin. however, introducing our new
multi-lingual signal considerably improves perfor-
mance. therefore, we hypothesize that the infor-
mation in bilingual sentence-aligned data has been

7we

use

the word-document matrix mined

by
s  gaard et al. (2015), which contains only a subset of
our target languages: english, french, and spanish.

thoroughly mined by existing methods, and sug-
gest that future work explore additional sources of
information in order to make substantial progress.

acknowledgments

the work was supported in part by the euro-
pean research council (grant number 313695)
and the israeli science foundation (grant number
1555/15). we would like to thank sarath chan-
dar for helping us run bilingual autoencoders on
large datasets.

references
[ammar et al.2016] waleed ammar, george mulcaire,
miguel ballesteros, chris dyer, and noah smith.
2016. many languages, one parser. transactions
of the association for computational linguistics,
4:431   444.

[brown et al.1993] peter f brown, vincent j della
pietra, stephen a della pietra, and robert l mer-
cer. 1993. the mathematics of statistical machine
translation: parameter estimation. computational
linguistics, 19(2):263   311.

[cakmak et al.2012] talha cakmak, s  uleyman acar,
2012. word alignment for

and g  ulsen eyrigit.
english-turkish language pair. in lrec.

[chandar et al.2014] sarath a p chandar, stanislas
lauly, hugo larochelle, mitesh khapra, balaraman
ravindran, vikas c raykar, and amrita saha. 2014.
an autoencoder approach to learning bilingual word
representations.
in z. ghahramani, m. welling,
c. cortes, n. d. lawrence, and k. q. weinberger,
editors, advances in neural information processing
systems 27, pages 1853   1861. curran associates,
inc.

[christodouloupoulos and steedman2015] christos

christodouloupoulos and mark steedman. 2015.
a massively parallel corpus: the bible in 100
languages. language resources and evaluation,
49(2):375   395.

[faruqui and dyer2014] manaal faruqui and chris
dyer. 2014. improving vector space word represen-
tations using multilingual correlation. in proceed-
ings of the 14th conference of the european chap-
ter of the association for computational linguistics,
pages 462   471, gothenburg, sweden, april. asso-
ciation for computational linguistics.

[gouws and s  gaard2015] stephan gouws and anders
s  gaard. 2015. simple task-speci   c bilingual word
embeddings. in proceedings of the 2015 conference
of the north american chapter of the association
for computational linguistics: human language
technologies, pages 1386   1390, denver, colorado,
may   june. association for computational linguis-
tics.

[gouws et al.2015] stephan gouws, yoshua bengio,
and greg corrado. 2015. bilbowa: fast bilingual
distributed representations without word alignments.
in proceedings of the 32nd international conference
on machine learning, icml 2015, lille, france, 6-
11 july 2015, pages 748   756.

[graca et al.2008] joao graca, joana pardal, lu    sa co-
heur, and diamantino caseiro. 2008. building a
golden collection of parallel multi-language word
alignments. in lrec.

[guo et al.2016] jiang guo, wanxiang che, david
yarowsky, haifeng wang, and ting liu. 2016. a
representation learning framework for multi-source
transfer parsing.
the thir-
tieth aaai conference on arti   cial intelligence,
aaai   16, pages 2734   2740. aaai press.

in proceedings of

[hermann and blunsom2014] karl moritz hermann
and phil blunsom. 2014. multilingual distributed
representations without word alignment. in pro-
ceedings of iclr, april.

[holmqvist and ahrenberg2011] maria holmqvist and
lars ahrenberg. 2011. a gold standard for english-
swedish word alignment. in nodalida.

[klementiev et al.2012] alexandre klementiev,

ivan
inducing
titov, and binod bhattarai.
crosslingual distributed representations of words. in
proceedings of coling 2012, pages 1459   1474,
mumbai, india, december. the coling 2012 or-
ganizing committee.

2012.

[koehn2005] philipp koehn. 2005. europarl: a paral-
lel corpus for id151. in mt
summit, volume 5, pages 79   86.

[koehn2010] philipp koehn. 2010. statistical ma-
chine translation. cambridge university press,
new york, ny, usa, 1st edition.

[lambert et al.2005] patrik lambert, adria de gispert,
rafael banchs, and jose marino. 2005. guidelines
for word aligment evaluation and manual alignment.
language resources and evaluation, 39(4):267   
285.

[levy and goldberg2014] omer levy and yoav gold-
berg. 2014. neural id27s as implicit
id105. in advances in neural infor-
mation processing systems 27: annual conference
on neural information processing systems 2014,
december 8-13 2014, montreal, quebec, canada,
pages 2177   2185.

in proceedings of the 2015 conference of the north
american chapter of the association for computa-
tional linguistics: human language technologies,
pages 151   159, denver, colorado, may   june. as-
sociation for computational linguistics.

[mihalcea and pedersen2003] rada mihalcea and ted
pedersen. 2003. an evaluation exercise for word
alignment. in hlt-naacl 2003 workshop: build-
ing and using parallel texts: data driven machine
translation and beyond.

[mikolov et al.2013a] tomas mikolov, quoc v le, and
exploiting similarities
arxiv

ilya sutskever.
among languages for machine translation.
preprint arxiv:1309.4168.

2013a.

[mikolov et al.2013b] tomas mikolov, ilya sutskever,
kai chen, gregory s. corrado, and jeffrey dean.
2013b. distributed representations of words and
phrases and their compositionality.
in advances
in neural information processing systems, pages
3111   3119.

[och and ney2003] franz josef och and hermann ney.
2003. a systematic comparison of various statisti-
cal alignment models. computational linguistics,
29(1):19   51.

[sahlgren2006] magnus sahlgren. 2006. the word-

space model. ph.d. thesis, stockholm university.

[s  gaard et al.2015] anders s  gaard,

  zeljko agi  c,
h  ector mart    nez alonso, barbara plank, bernd
bohnet, and anders johannsen. 2015. inverted in-
dexing for cross-lingual nlp. in proceedings of the
53rd annual meeting of the association for compu-
tational linguistics and the 7th international joint
conference on natural language processing (vol-
ume 1: long papers), pages 1713   1722, beijing,
china, july. association for computational linguis-
tics.

[turney and pantel2010] peter d. turney and patrick
pantel. 2010. from frequency to meaning: vec-
tor space models of semantics. journal of arti   cial
intelligence research, 37(1):141   188.

[upadhyay et al.2016] shyam upadhyay, manaal
faruqui, chris dyer, and dan roth. 2016. cross-
lingual models of id27s: an empirical
in proceedings of the 54th annual
comparison.
meeting of
the association for computational
linguistics (volume 1: long papers), pages 1661   
1670, berlin, germany, august. association for
computational linguistics.

[levy et al.2015] omer levy, yoav goldberg, and ido
dagan. 2015.
improving distributional similarity
with lessons learned from id27s. trans-
actions of the association for computational lin-
guistics, 3:211   225.

[vuli  c and moens2016] ivan vuli  c and marie-francine
moens. 2016. bilingual distributed word repre-
sentations from document-aligned comparable data.
journal of arti   cial intelligence research, 55:953   
994.

[luong et al.2015] minh-thang luong, hieu pham,
and christopher manning. 2015. bilingual word
representations with monolingual quality in mind.

[xiao and guo2014] min xiao and yuhong guo. 2014.
distributed word representation learning for cross-
lingual id33. in proceedings of the

eighteenth conference on computational natural
language learning, pages 119   129, ann arbor,
michigan, june. association for computational lin-
guistics.

[zou et al.2013] will y. zou, richard socher, daniel
cer, and christopher d. manning. 2013. bilingual
id27s for phrase-based machine trans-
lation.
in proceedings of the 2013 conference on
empirical methods in natural language process-
ing, pages 1393   1398, seattle, washington, usa,
october. association for computational linguistics.

