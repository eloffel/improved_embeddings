when are tree structures necessary for deep learning of

representations?

jiwei li1, minh-thang luong1, dan jurafsky1 and eduard hovy2

1computer science department, stanford university, stanford, ca 94305

2language technology institute, carnegie mellon university, pittsburgh, pa 15213

jiweil,lmthang,jurafsky@stanford.edu

ehovy@andrew.cmu.edu

5
1
0
2

 

g
u
a
8
1

 

 
 
]
i

a
.
s
c
[
 
 

5
v
5
8
1
0
0

.

3
0
5
1
:
v
i
x
r
a

abstract

recursive neural models, which use syn-
tactic parse trees to recursively generate
representations bottom-up, are a popular
architecture. but
there have not been
rigorous evaluations showing for exactly
which tasks this syntax-based method is
appropriate.
in this paper we bench-
mark recursive neural models against se-
quential recurrent neural models (simple
recurrent and lstm models), enforcing
apples-to-apples comparison as much as
possible. we investigate 4 tasks: (1) sen-
timent classi   cation at the sentence level
and phrase level; (2) matching questions
to answer-phrases;
(3) discourse pars-
ing; (4) semantic id36 (e.g.,
component-whole between nouns).
our goal is to understand better when,
and why, recursive models can outper-
form simpler models. we    nd that re-
cursive models help mainly on tasks (like
semantic id36) that require
associating headwords across a long dis-
tance, particularly on very long sequences.
we then introduce a method for allowing
recurrent models to achieve similar per-
formance: breaking long sentences into
clause-like units at punctuation and pro-
cessing them separately before combin-
ing. our results thus help understand the
limitations of both classes of models, and
suggest directions for improving recurrent
models.

learning

introduction

1
deep
learn
low-
dimensional,
for word
tokens, mostly from large-scale data corpus (e.g.,
(mikolov et al., 2013; le and mikolov, 2014;

based methods
vectors

real-valued

collobert et al., 2011)), successfully capturing
syntactic and semantic aspects of text.

for tasks where the inputs are larger text units
(e.g., phrases, sentences or documents), a compo-
sitional model is    rst needed to aggregate tokens
into a vector with    xed dimensionality that can be
used as a feature for other nlp tasks. models for
achieving this usually fall into two categories: re-
current models and recursive models:

recurrent models (also referred to as sequence
models) deal successfully with time-series data
(pearlmutter, 1989; dorffner, 1996) like speech
(robinson et al., 1996; lippmann, 1989; graves et
al., 2013) or handwriting recognition (graves and
schmidhuber, 2009; graves, 2012). they were ap-
plied early on to nlp (elman, 1990), modeling a
sentence as tokens processed sequentially, at each
step combining the current token with previously
built embeddings. recurrent models can be ex-
tended to bidirectional ones from both left-to-right
and right-to-left. these models generally consider
no linguistic structure aside from word order.

recursive neural models (also referred to as
tree models), by contrast, are structured by syn-
tactic parse trees.
instead of considering tokens
sequentially, recursive models combine neighbors
based on the recursive structure of parse trees,
starting from the leaves and proceeding recur-
sively in a bottom-up fashion until the root of
the parse tree is reached. for example, for the
phrase the food is delicious, following the oper-
ation sequence ( (the food) (is delicious) ) rather
than the sequential order (((the food) is) delicious).
many recursive models have been proposed (e.g.,
(paulus et al., 2014; irsoy and cardie, 2014)), and
applied to various nlp tasks, among them en-
tailment (bowman, 2013; bowman et al., 2014),
id31 (socher et al., 2013;
irsoy
and cardie, 2013; dong et al., 2014), question-
answering (iyyer et al., 2014), relation classi   ca-
tion (socher et al., 2012; hashimoto et al., 2013),

and discourse (li and hovy, 2014).

one possible advantage of recursive models is
their potential for capturing long-distance depen-
dencies: two tokens may be structurally close to
each other, even though they are far away in word
sequence. for example, a verb and its correspond-
ing direct object can be far away in terms of tokens
if many adjectives lies in between, but they are ad-
jacent in the parse tree (irsoy and cardie, 2013).
but we don   t know if this advantage is truly im-
portant, and if so for which tasks, or whether other
issues are at play. indeed, the reliance of recursive
models on parsing is also a potential disadvan-
tage, given that parsing is relatively slow, domain-
dependent, and can be errorful.

on the other hand, recent progress in multi-
ple sub   elds of neural nlp has suggested that re-
current nets may be suf   cient to deal with many
of the tasks for which recursive models have
been proposed. recurrent models without parse
structures have shown good results in sequence-
to-sequence generation (sutskever et al., 2014)
for machine translation (e.g., (kalchbrenner and
blunsom, 2013; 3; luong et al., 2014)), pars-
ing (vinyals et al., 2014), and sentiment, where
for example recurrent-based paragraph vectors (le
and mikolov, 2014) outperform recursive models
(socher et al., 2013) on the stanford sentiment-
bank dataset.

our goal in this paper is thus to investigate a
number of tasks with the goal of understanding
for which kinds of problems recurrent models may
be suf   cient, and for which kinds recursive mod-
els offer speci   c advantages. we investigate four
tasks with different properties.

    binary sentiment classi   cation at the sen-
tence level (pang et al., 2002) and phrase
level (socher et al., 2013) that focus on
understanding the role of recursive models
in dealing with semantic compositionally in
various scenarios such as different lengths of
inputs and whether or not supervision is com-
prehensive.

    phrase matching on the umd-qa dataset
(iyyer et al., 2014) can help see the difference
between outputs from intermediate compo-
nents from different models, i.e., representa-
tions for intermediate parse tree nodes and
outputs from recurrent models at different
time steps.
it also helps see whether pars-

ing is useful for    nding similarities between
question sentences and target phrases.

    semantic relation classi   cation on the
semeval-2010 (hendrickx et al., 2009) data
can help understand whether parsing is help-
ful in dealing with long-term dependencies,
such as relations between two words that are
far apart in the sequence.

    discourse parsing (rst dataset) is useful
for measuring the extent to which parsing im-
proves discourse tasks that need to combine
meanings of larger text units. discourse pars-
ing treats elementary discourse units (edus)
as basic units to operate on, which are usually
short clauses. the task also sheds light on
the extent to which syntactic structures help
acquire shot text representations.

the principal motivation for this paper is to un-
derstand better when, and why, recursive models
are needed to outperform simpler models by en-
forcing apples-to-apples comparison as much as
possible. this paper applies existing models to
existing tasks, barely offering novel algorithms or
tasks. our goal is rather an analytic one, to inves-
tigate different versions of recursive and recurrent
models. this work helps understand the limita-
tions of both classes of models, and suggest direc-
tions for improving recurrent models.

the rest of this paper organized as follows: we
detail versions of recursive/recurrent models in
section 2, present the tasks and results in section
3, and conclude with discussions in section 4.

2 recursive and recurrent models
2.1 notations
we assume that the text unit s, which could
be a phrase, a sentence or a document, is com-
prised of a sequence of tokens/words: s =
{w1, w2, ..., wns}, where ns denotes the num-
ber of tokens in s. each word w is associated
with a k-dimensional vector embedding ew =
{e1
w }. the goal of recursive and re-
current models is to map the sequence to a k-
dimensional es, based on its tokens and their cor-
respondent embeddings.
standard recurrent/sequence models a re-
current network successively takes word wt at
step t, combines its vector representation et with
the previously built hidden vector ht   1 from time

w, ..., ek

w, e2

t     1, calculates the resulting current embedding
ht, and passes it to the next step. the embedding
ht for the current time t is thus:

ht = f (w    ht   1 + v    et)

(1)

where w and v denote compositional matrices. if
ns denotes the length of the sequence, hns repre-
sents the whole sequence s.
standard recursive/tree models standard re-
cursive models work in a similar way, but process-
ing neighboring words by parse tree order rather
than sequence order.
it computes a representa-
tion for each parent node based on its immediate
children recursively in a bottom-up fashion until
reaching the root of the tree. for a given node   
in the tree and its left child   left (with representa-
tion eleft) and right child   right (with representation
eright), the standard recursive network calculates e  
as follows:

(cid:34) it

ft
ot
lt

(cid:35)

=

(cid:35)

(cid:34)   

  
  
tanh

(cid:34)

w   

(cid:35)

ht   1
et

ct = ft    ct   1 + it    lt

(5)

(6)

t = ot    ct
hs
where w     r4k  2k.
the
phrase/sentence level are predicted representations
outputted from the last time step.

labels at

(7)

tree lstms recent research has extended the
lstm idea to tree-based structures (zhu et al.,
2015; tai et al., 2015) that associate memory and
forget gates to nodes of the parse trees.

bi-directional lstms these
directional models and lstms.

combine bi-

e   = f (w    e  left + v    e  right)

(2)

3 experiments

bidirectional models
(schuster and paliwal,
1997) add bidirectionality to the recurrent frame-
work where embeddings for each time are calcu-
lated both forwardly and backwardly:

t = f (w        h   
h   
t = f (w        h   
h   

t   1 + v        et)
t+1 + v        et)

(3)

normally,    nal representations for sentences can
be achieved either by concatenating vectors calcu-
lated from both directions [e   
] or using fur-
ther compositional operation to preserve vector di-
mensionality

1 , e   

ns

t , h   
t ])

ht = f (wl    [h   

(4)
where wl denotes a k    2k dimensional matrix.
long short term memory (lstm) lstm
models (hochreiter and schmidhuber, 1997) are
de   ned as follows: given a sequence of inputs
x = {x1, x2, ..., xnx}, an lstm associates each
timestep with an input, memory and output gate,
respectively denoted as it, ft and ot. we notation-
ally disambiguate e and h: et denotes the vector
for individual text units (e.g., word or sentence) at
time step t, while ht denotes the vector computed
by the lstm model at time t by combining et and
ht   1.    denotes the sigmoid function. the vector
representation ht for each time-step t is given by:

in this section, we detail our experimental settings
and results. we consider the following tasks, each
representative of a different class of nlp tasks.
1. binary sentiment classi   cation on the pang et
al. (2002) dataset. this addresses the issues where
supervision only appears globally after a long se-
quence of operations.
2. sentiment classi   cation on the stanford
sentiment treebank (socher et al., 2013): com-
prehensive labels are found for words and phrases
where local compositionally (such as from nega-
tion, mood, or others cued by phrase-structure) is
to be learned.
3. sentence-target matching on the umd-qa
dataset (iyyer et al., 2014): learns matches be-
tween target and components in the source sen-
tences, which are parse tree nodes for recursive
models and different time-steps for recurrent mod-
els.
4.
semantic relation classi   cation on the
semeval-2010 task (hendrickx et al., 2009).
learns long-distance relationships between two
words that may be far apart sequentially.
5. discourse parsing (li et al., 2014; hernault et
al., 2010): learns sentence-to-sentence relations
based on calculated representations.

in each case we followed the protocols de-
scribed in the original papers. we    rst group the
algorithm variants into two groups as follows:

    standard tree models vs standard sequence
models vs standard bi-directional sequence
models

    lstm tree models, lstm sequence models

vs lstm bi-directional sequence models.

we employed standard training frameworks for
neural models: for each task, we used stochas-
tic gradient decent using adagrad (duchi et al.,
2011) with minibatches (cotter et al., 2011). pa-
rameters are tuned using the development dataset
if available in the original datasets or from cross-
validation if not. derivatives are calculated from
standard back-propagation (goller and kuchler,
1996). parameters to tune include size of mini
batches, learning rate, and parameters for l2 pe-
nalizations. the number of running iterations
is treated as a parameter to tune and the model
achieving best performance on the development
set is used as the    nal model to be evaluated.

for settings where no repeated experiments are
performed, the bootstrap test is adopted for sta-
tistical signi   cance testing (efron and tibshirani,
1994). test scores that achieve signi   cance level
of 0.05 are marked by an asterisk (*).

3.1 stanford sentiment treebank
task description we start with the stanford
sentiment treebank (socher et al., 2013). this
dataset contains gold-standard labels for every
parse tree constituent, from the sentence to phrases
to individual words.

of course, any conclusions drawn from imple-
menting sequence models on a dataset that was
based on parse trees may have to be weakened,
since sequence models may still bene   t from the
way that the dataset was collected. nevertheless
we add an evaluation on this dataset because it has
been a widely used benchmark dataset for neural
model evaluations.

for recursive models, we followed the proto-
cols in socher et al. (2013) where node embed-
dings in the parse trees are obtained from recur-
sive models and then fed to a softmax classi   er.
we transformed the dataset for recurrent model
use as illustrated in figure 1. each phrase is recon-
structed from parse tree nodes and treated as a sep-
arate data point. as the treebank contains 11,855
sentences with 215,154 phrases, the reconstructed
dataset for recurrent models comprises 215,154
examples. models are evaluated at both the phrase

level (82,600 instances) and the sentence root level
(2,210 instances).

tree
sequence
p-value
bi-sequence
p-value

fine-grained
0.433
0.420 (-0.013)
0.042*
0.435 (+0.08)
0.078

binary
0.815
0.807 (-0.007)
0.098
0.816 (+0.002)
0.210

table 1: test set accuracies on the stanford senti-
ment treebank at root level.

tree
sequence
p-value
bi-sequence
p-value

fine-grained
0.820
0.818 (-0.002)
0.486
0.826 (+0.06)
0.148

binary
0.860
0.864 (+0.004)
0.305
0.862 (+0.002)
0.450

table 2: test set accuracies on the stanford senti-
ment treebank at phrase level.

results are shown in table 1 and 21. when
comparing the standard version of tree models
to sequence models, we    nd it helps a bit at
root level identi   cation (for sequences but not bi-
sequences), but yields no signi   cant improvement
at the phrase level.
lstm tai et al. (2015) discovered that lstm
tree models generate better performances in terms
of sentence root level evaluation than sequence
models. we explore this task a bit more by training
deeper and more sophisticated models. we exam-
ine the following three models:

1. tree-structured lstm models (tai et al.,

2015)2.

2. deep bi-lstm sequence models (denoted as
sequence) that treat the whole sentence as
just one sequence.

3. deep bi-lstm hierarchical sequence mod-
els (denoted as hierarchical sequence) that
   rst slice the sentence into a sequence of sub-
sentences by using a look-up table of punc-
tuations (i.e., comma, period, question mark
and exclamation mark). the representation
for each sub-sentence is    rst computed sep-
arately, and another level of sequence lstm
1the performance of our implementations of recursive
models is not exactly identical to that reported in socher et
al. (2013), but the relative difference is around 1% to 2%.

2tai et al.. achieved 0.510 accuracy in terms of    ne-
grained evaluation at the root level as reported in (tai et al.,
2015), similar to results from our implementations (0.504).

figure 1: transforming stanford sentiment treebank to sequences for sequence models.

model

tree lstm
bi-sequence
hier-sequence

all-   ne
83.4 (0.3)
83.3 (0.4)
82.9 (0.3)

root-   ne
50.4 (0.9)
49.8 (0.9)
50.7 (0.8)

root-coarse
86.7 (0.5)
86.7 (0.5)
86.9 (0.6)

table 3: test set accuracies on the stanford sen-
timent treebank with deviations. for our exper-
iments, we report accuracies over 20 runs with
standard deviation.

(two-tailed p-value equals 0.041*, and only at the
root level, with p-value for the phrase level at
0.376). the hierarchical sequence model achieves
the same performance with a p-value of 0.198.

discussion the results above suggest
that
clausal segmentation of long sentences offers a
slight performance boost, a result also supported
by the fact that very little difference exists between
the three models for phrase-level sentiment eval-
uation. clausal segmentation of long sentences
thus provides a simple approximation to parse-tree
based models.

we suggest a few reasons for this slightly better
performances introduced by clausal segmentation:

1. treating clauses as basic units (to the extent
that punctuation approximates clauses) pre-
serves the semantic structure of text.

2. semantic compositions such as negations or
conjunctions usually appear at
the clause
level. working on clauses individually
and then combining them model inter-clause
compositions.

3. errors are back-propagated to individual to-
kens using fewer steps in id187
than in standard models. consider a movie
review    simple as the plot was , i still like it a
lot   . with standard recurrent models it takes
12 steps before the prediction error gets back
to the    rst token    simple   :

figure 2: illustration of two sequence models. a,
b, c, d denote clauses or sub sentences separated
by punctuation.

(one-directional) is then used to join the sub-
sentences. illustrations are shown in figure2.

we consider the third model because the dataset
used in tai et al. (2015) contains long sentences
and the evaluation is performed only at the sen-
tence root level. since a parsing algorithm will
naturally break long sentences into sub-sentences,
we   d like to know whether any performance boost
is introduced by the intra-clause parse tree struc-
ture or just by this broader segmentation of a
sentence into clause-like units; this latter advan-
tage could be approximated by using punctuation-
based approximations to clause boundaries.

we run 15 iterations for each algorithm. pa-
rameters are harvested at the end of each iteration;
those performing best on the dev set are used on
the test set. the whole process takes roughly 15-
20 minutes on a single gpu machine3. for a more
convincing comparison, we did not use the boot-
strap test where parallel examples are generated
from one same dataset. instead, we repeated the
aforementioned procedure for each algorithm 20
times and report accuracies with standard devia-
tion in table 3.

tree lstms are equivalent or marginally bet-
ter than standard bi-directional sequence model

3tesla k40m, 2880 cuda cores.

tree
sequence
p-value
bi-sequence
p-value

standard
0.745
0.733 (-0.012)
0.060
0.754 (+0.09)
0.058

lstm
0.774
0.783 (+0.008)
0.136
0.790 (+0.016)
0.024*

table 4: test set accuracies on the pang   s senti-
ment dataset using standard model settings.

it takes multiple steps before sentiment related ev-
idence comes up to the surface. it is therefore un-
clear whether local compositional operators (such
as negation) can be learned; there is only a small
amount of training data (around 8,000 examples)
and the sentiment supervision only at the level of
the sentence may not be easy to propagate down to
deeply buried local phrases.

3.3 question-answer matching
task description:
in the question-answering
dataset qanta5, each answer is a token or short
phrase. the task is different from standard gener-
ation focused qa task but formalized as a multi-
class classi   cation task that matches a source
question with a candidates phrase from a prede-
   ned pool of candidate phrases we give an illus-
trative example here:

question: he left un   nished a novel whose title
character forges his father   s signature to get out
of school and avoids the draft by feigning desire
to join. name this german author of the magic
mountain and death in venice.

answer: thomas mann from the pool of
phrases. other candidates might include george
washington, charlie chaplin, etc.

the model of iyyer et al. (2014) minimizes the
distances between answer embeddings and node
embeddings along the parse tree of the question.
concretely, let c denote the correct answer to ques-
tion s, with embedding (cid:126)c, and z denoting any ran-
dom wrong answer. the objective function sums
over the dot product between representation for
every node    along the question parse trees and
the answer representations:

(cid:88)

(cid:88)

l =

     [parse tree]

z

max(0, 1   (cid:126)c  e   +(cid:126)z  e  ) (8)

where e   denotes the embedding for parse tree
node calculated from the recursive neural model.
5http://cs.umd.edu/  miyyer/qblearn/. be-
cause the publicly released dataset is smaller than the version
used in (iyyer et al., 2014) due to privacy issues, our numbers
are not comparable to those in (iyyer et al., 2014).

figure 3: sentiment prediction using a one-
directional (left to right) lstm. decisions at each
time step are made by feeding embeddings calcu-
lated from the lstm into a softmax classi   er.
error   lot   a   it   like   still   i   ,   was
   plot    the   as   simple
in a hierarchical model, the second clause is
compacted into one component, and the error
propagation is thus given by:
error    second-clause        rst-clause    
was   plot   the   as   simple.
propagation with clause segmentation con-
sists of only 8 operations. such a procedure
thus tends to attenuate the gradient vanish-
ing problem, potentially yielding better per-
formance.

label

sentiment

the original dataset

3.2 binary sentiment classi   cation (pang)
task description: the
dataset
(2002) consists of sentences
of pang et al.
for each sentence.
with a sentiment
into train-
we divide
ing(8101)/dev(500)/testing(2000).
no pre-
training procedure as described in socher et al.
(2011b) is employed. id27s are
initialized using skip-grams and kept    xed in
the learning procedure. we trained skip-gram
embeddings on the wikipedia+gigaword dataset
using the id97 package4.
sentence level
embeddings are fed into a sigmoid classi   er.
performances for 50 dimensional vectors are
given in the table below:

discussion why don   t parse trees help on this
task? one possible explanation is the distance
of the supervision signal from the local composi-
tional structure. the pang et al. dataset has an av-
erage sentence length of 22.5 words, which means

4https://code.google.com/p/id97/

here the parse trees are dependency parses follow-
ing (iyyer et al., 2014).

by adjusting the framework to recurrent mod-
els, we minimize the distance between the answer
embedding and the embeddings calculated from
each timestep t of the sequence:

max(0, 1     (cid:126)c    et + (cid:126)z    et)

(9)

(cid:88)

(cid:88)

t   [1,ns]

z

l =

at test time, the model chooses the answer (from
the set of candidates) that gives the lowest loss
score. as can be seen from results presented in
table 5, the difference is only signi   cant for the
lstm setting between the tree model and the
sequence model; no signi   cant difference is ob-
served for other settings.

tree
sequence
p-value
bi-sequence
p-value

standard
0.523
0.525 (+0.002)
0.490
0.530 (+0.007)
0.075

lstm
0.558
0.546 (-0.012)
0.046*
0.564 (+0.006)
0.120

table 5: test set accuracies for umd-qa dataset.

discussion the umd-qa task represents a
group of situations where because we have in-
suf   cient supervision about matching (it   s hard
to know which node in the parse tree or which
timestep provides the most direct evidence for the
answer), decisions have to be made by looking at
and iterating over all subunits (all nodes in parse
trees or timesteps). similar ideas can be found in
pooling structures (e.g. socher et al. (2011a)).

the results above illustrate that for tasks where
we try to align the target with different source
components (i.e., parse tree nodes for tree mod-
els and different time steps for sequence models),
components from sequence models are able to em-
bed important information, despite the fact that se-
quence model components are just sentence frag-
ments and hence usually not linguistically mean-
ingful components in the way that parse tree con-
stituents are.

3.4 semantic relationship classi   cation
task description: semeval-2010 task 8 (hen-
drickx et al., 2009) is to    nd semantic rela-
tionships between pairs of nominals, e.g.,
in
   my [apartment]e1 has a pretty large [kitchen]e2   
classifying the relation between [apartment] and

[kitchen] as component-whole. the dataset con-
tains 9 ordered relationships, so the task is formal-
ized as a 19-class classi   cation problem, with di-
rected relations treated as separate labels; see hen-
drickx et al. (2009; socher et al. (2012) for details.
for the recursive implementations, we follow
the neural framework de   ned in socher et al.
(2012). the path in the parse tree between the two
nominals is retrieved, and the embedding is calcu-
lated based on recursive models and fed to a soft-
max classi   er6. retrieved paths are transformed
for the recurrent models as shown in figure 5.

figure 4: illustration of models for semantic re-
lationship classi   cation.

discussion unlike for earlier tasks, here recur-
sive models yield much better performance than
the corresponding recurrent versions for all ver-
sions (e.g., standard tree vs. standard sequence,
p = 0.004). these results suggest that it is the
need to integrate structures far apart in the sen-
tence that characterizes the tasks where recursive
models surpass recurrent models. in parse-based
models, the two target words are drawn together
much earlier in the decision process than in recur-
rent models, which must remember one target un-
til the other one appears.

3.5 discourse parsing
task description: our    nal
task, discourse
parsing based on the rst-dt corpus (carlson et

6(socher et al., 2012) achieve state-of-art performance
by combining a sophisticated model, mv-id56, in which
each word is presented with both a matrix and a vector with
human-feature engineering. again, because mv-id56 is dif-
   cult to adapt to a recurrent version, we do not employ this
state-of-the-art model, adhering only to the general versions
of recursive models described in section 2, since our main
goal is to compare equivalent recursive and recurrent models
rather than implement the state of the art.

tree
sequence
p-value
bi-sequence
p-value

standard
0.748
0.712 (-0.036)
0.004*
0.730 (-0.018)
0.017*

lstm
0.767
0.740 (-0.027)
0.020*
0.752 (-0.014)
0.041*

table 6: test set accuracies on the semeval-2010
semantic relationship classi   cation task.

figure 5: an illustration of discourse parsing.
[e1, e2, ...] denote edus (elementary discourse
units), each consisting of a sequence of tokens.
[r12, r34, r56] denote relationships to be classi   ed.
a binary classi   cation model is    rst used to decide
whether two edus should be merged and a multi-
class classi   er is then used to decide the relation
type.

al., 2003), is to build a discourse tree for a doc-
ument, based on assigning rhetorical structure
theory (rst) relations between elementary dis-
course units (edus). because discourse relations
express the coherence structure of discourse, they
presumably express different aspects of compo-
sitional meaning than sentiment or nominal rela-
tions. see hernault et al. (2010) for more details
on discourse parsing and the rst-dt corpus.

representations for adjacent edus are fed into
binary classi   cation (whether two edus are re-
lated) and multi-class relation classi   cation mod-
els, as de   ned in li et al. (2014). related edus
are then merged into a new edu, the representa-
tion of which is obtained through an operation of
neural composition based on the previous two re-
lated edus. this step is repeated until all units
are merged.

discourse parsing takes edus as the basic units
to operate on; edus are short clauses, not full sen-
tences, with an average length of 7.2 words. re-
cursive and recurrent models are applied on edus
to create embeddings to be used as inputs for dis-
course parsing. we use this task for two rea-
sons: (1) to illustrate whether syntactic parse trees
are useful for acquiring representations for short
clauses. (2) to measure the extent to which pars-

ing improves discourse tasks that need to combine
the meanings of larger text units.

models are traditionally evaluated in terms of
three metrics, i.e., spans7, nuclearity8, and identi-
fying the rhetorical relation between two clauses.
due to space limits, we only focus the last one,
rhetorical relation identi   cation, because (1) rela-
tion labels are treated as correct only if spans and
nuclearity are correctly labeled (2) relation identi-
   cation between clauses offer more insights about
model   s abilities to represent sentence semantics.
in order to perform a plain comparison, no addi-
tional human-developed features are added.

tree
sequence
p-value
bi-sequence
p-value

standard
0.568
0.572 (+0.004)
0.160
0.578 (+0.01)
0.054

lstm
0.564
0.563 (-0.002)
0.422
0.575 (+0.012)
0.040*

table 7: test set accuracies for relation identi   ca-
tion on rst discourse parsing data set.

discussion we see no large differences between
equivalent recurrent and recursive models. we
suggest two possible explanations. (1) edus tend
to be short; thus for some clauses, parsing might
not change the order of operations on words. even
for those whose orders are changed by parse trees,
the in   uence of short phrases on the    nal represen-
tation may not be great enough. (2) unlike earlier
tasks, where text representations are immediately
used as inputs into classi   ers, the algorithm pre-
sented here adopts additional levels of neural com-
position during the process of edu merging. we
suspect that neural layers may act as information
   lters, separating the informational chaff from the
wheat, which in turn makes the model a bit more
immune to the initial inputs.

4 discussion
we compared recursive and recurrent neural mod-
els for representation learning on 5 distinct nlp
tasks in 4 areas for which recursive neural models
are known to achieve good performance (socher
et al., 2012; socher et al., 2013; li et al., 2014;
iyyer et al., 2014).

as with any comparison between models, our
results come with some caveats: first, we ex-
plore the most general or basic forms of recur-

7on blank tree structures.
8on tree structures with nuclearity indication.

sive/recurrent models rather than various sophis-
ticated algorithm variants. this is because fair
comparison becomes more and more dif   cult as
models get complex (e.g.,
the number of lay-
ers, number of hidden units within each layer,
etc.). thus most neural models employed in this
work are comprised of only one layer of neural
compositions   despite the fact that deep neural
models with multiple layers give better results.
our conclusions might thus be limited to the al-
gorithms employed in this paper, and it is unclear
whether they can be extended to other variants or
to the latest state-of-the-art. second, in order to
compare models    fairly   , we force every model to
be trained exactly in the same way: adagrad with
minibatches, same set of initializations, etc. how-
ever, this may not necessarily be the optimal way
to train every model; different training strategies
tailored for speci   c models may improve their per-
formances. in that sense, our attempts to be    fair   
in this paper may nevertheless be unfair.

pace these caveats, our conclusions can be sum-

marized as follows:

    in tasks like semantic id36, in
which single headwords need to be associ-
ated across a long distance, recursive models
shine. this suggests that for the many other
kinds of tasks in which long-distance seman-
tic dependencies play a role (e.g., translation
between languages with signi   cant reorder-
ing like chinese-english translation), syntac-
tic structures from recursive models may of-
fer useful power.

    tree models tend to help more on long se-
quences than shorter ones with suf   cient su-
pervision:
tree models slightly help root
level identi   cation on the stanford sentiment
treebank, but do not help much at the phrase
level. adopting bi-directional versions of re-
current models seem to largely bridge this
gap, producing equivalent or sometimes bet-
ter results.

    on long sequences where supervision is not
suf   cient, e.g., in pang at al.,   s dataset (super-
vision only exists on top of long sequences),
no signi   cant difference is observed between
tree based and sequence based models.

    in cases where tree-based models do well, a
simple approximation to tree-based models

seems to improve recurrent models to equiv-
alent or almost equivalent performance: (1)
break long sentences (on punctuation) into a
series of clause-like units, (2) work on these
clauses separately, and (3) join them together.
this model sometimes works as well as tree
models for the sentiment task, suggesting
that one of the reasons tree models help is
by breaking down long sentences into more
manageable units.

    despite that the fact that components (out-
puts from different
time steps) in recur-
rent models are not linguistically meaningful,
they may do as well as linguistically mean-
ingful phrases (represented by parse tree
nodes) in embedding informative evidence,
as demonstrated in umd-qa task. indeed,
recent work in parallel with ours (bowman
et al., 2015) has shown that recurrent models
like lstms can discover implicit recursive
compositional structure.

5 acknowledgments

we would especially like to thank richard socher
and kai-sheng tai for insightful comments, ad-
vice, and suggestions. we would also like to thank
sam bowman, ignacio cases, jon gauthier, kevin
gu, gabor angeli, sida wang, percy liang and
other members of the stanford nlp group, as well
as the anonymous reviewers for their helpful ad-
vice on various aspects of this work. we acknowl-
edge the support of nvidia corporation with the
donation of tesla k40 gpus we gratefully ac-
knowledge support from an enlight foundation
graduate fellowship, a gift from bloomberg l.p.,
the defense advanced research projects agency
(darpa) deep exploration and filtering of text
(deft) program under air force research lab-
oratory (afrl) contract no. fa8750-13-2-0040,
and the nsf via award iis-1514268. any opin-
ions,    ndings, and conclusions or recommenda-
tions expressed in this material are those of the
authors and do not necessarily re   ect the views of
bloomberg l.p., darpa, afrl, nsf, or the us
government.

references
dzmitry bahdanau, kyunghyun cho, and yoshua ben-
gio. 2014. id4 by jointly

learning to align and translate.
arxiv:1409.0473.

arxiv preprint

samuel r bowman, christopher potts, and christo-
pher d manning.
2014. recursive neural net-
works for learning logical semantics. arxiv preprint
arxiv:1406.1827.

samuel r bowman, christopher d manning, and
christopher potts. 2015. tree-structured compo-
sition in neural networks without tree-structured ar-
chitectures. arxiv preprint arxiv:1506.04834.

samuel r bowman. 2013. can recursive neural tensor
arxiv preprint

networks learn logical reasoning?
arxiv:1312.6192.

lynn carlson, daniel marcu,

and mary ellen
okurowski. 2003. building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
in current and new directions in discourse and di-
alogue text, speech and language technology. vol-
ume 22. springer.

ronan collobert, jason weston, l  eon bottou, michael
karlen, koray kavukcuoglu, and pavel kuksa.
2011. natural language processing (almost) from
the journal of machine learning re-
scratch.
search, 12:2493   2537.

andrew cotter, ohad shamir, nati srebro, and karthik
sridharan. 2011. better mini-batch algorithms via
accelerated gradient methods. in advances in neu-
ral information processing systems, pages 1647   
1655.

li dong, furu wei, chuanqi tan, duyu tang, ming
zhou, and ke xu. 2014. adaptive recursive neural
network for target-dependent twitter sentiment clas-
si   cation. in proceedings of the 52nd annual meet-
ing of the association for computational linguis-
tics, pages 49   54.

georg dorffner. 1996. neural networks for time series

processing. in neural network world.

john duchi, elad hazan, and yoram singer. 2011.
adaptive subgradient methods for online learning
and stochastic optimization. the journal of ma-
chine learning research, 12:2121   2159.

bradley efron and robert j tibshirani. 1994. an in-

troduction to the bootstrap. crc press.

jeffrey l elman. 1990. finding structure in time.

cognitive science, 14(2):179   211.

christoph goller and andreas kuchler. 1996. learn-
ing task-dependent distributed representations by
id26 through structure. in neural net-
works, 1996., ieee international conference on,
volume 1, pages 347   352. ieee.

alex graves and juergen schmidhuber. 2009. of   ine
handwriting recognition with multidimensional re-
current neural networks. in advances in neural in-
formation processing systems, pages 545   552.

alex graves, abdel-rahman mohamed, and geoffrey
hinton. 2013. id103 with deep recur-
rent neural networks. in acoustics, speech and sig-
nal processing (icassp), 2013 ieee international
conference on, pages 6645   6649. ieee.

alex graves. 2012.

supervised sequence labeling
with recurrent neural networks, in studies in com-
putational intelligence. volume 385. springer.

kazuma hashimoto, makoto miwa, yoshimasa tsu-
ruoka, and takashi chikayama. 2013. simple cus-
tomization of id56s for seman-
tic relation classi   cation. in emnlp, pages 1372   
1376.

iris hendrickx, su nam kim, zornitsa kozareva,
preslav nakov, diarmuid   o s  eaghdha, sebastian
pad  o, marco pennacchiotti, lorenza romano, and
stan szpakowicz.
semeval-2010 task
8: multi-way classi   cation of semantic relations
in proceedings of
between pairs of nominals.
the workshop on semantic evaluations: recent
achievements and future directions, pages 94   99.
association for computational linguistics.

2009.

hugo hernault, helmut prendinger, mitsuru ishizuka.
2010. hilda: a discourse parser using support vector
machine classi   cation. dialogue & discourse, 1(3).

sepp hochreiter and j  urgen schmidhuber.

1997.
neural computation,

long short-term memory.
9(8):1735   1780.

ozan irsoy and claire cardie. 2013. bidirectional re-
cursive neural networks for token-level labeling with
structure. arxiv preprint arxiv:1312.0493.

ozan irsoy and claire cardie. 2014. deep recursive
neural networks for compositionality in language.
in advances in neural information processing sys-
tems, pages 2096   2104.

mohit iyyer, jordan boyd-graber, leonardo claudino,
richard socher, and hal daum  e iii. 2014. a neural
network for factoid id53 over para-
graphs. in proceedings of the 2014 conference on
empirical methods in natural language processing
(emnlp), pages 633   644.

nal kalchbrenner and phil blunsom. 2013. recurrent
in emnlp, pages

continuous translation models.
1700   1709.

quoc v le and tomas mikolov. 2014. distributed
representations of sentences and documents. arxiv
preprint arxiv:1405.4053.

jiwei li and eduard hovy. 2014. a model of coher-
ence based on distributed sentence representation.
in proceedings of the 2014 conference on empirical
methods in natural language processing (emnlp)

jiwei li, rumeng li, and eduard hovy. 2014. recur-
sive deep models for discourse parsing. in proceed-
ings of the 2014 conference on empirical methods

richard socher, alex perelygin, jean y wu, jason
chuang, christopher d manning, andrew y ng,
and christopher potts. 2013. recursive deep mod-
els for semantic compositionality over a sentiment
treebank. in proceedings of the conference on em-
pirical methods in natural language processing
(emnlp), pages 1631   1642.

ilya sutskever, oriol vinyals, and quoc vv le. 2014.
sequence to sequence learning with neural net-
works. in advances in neural information process-
ing systems, pages 3104   3112.

kai sheng tai, richard socher, and christopher d
manning. improved semantic representations from
tree-structured id137.
acl. 2015.

oriol vinyals, lukasz kaiser, terry koo, slav petrov,
2014.
arxiv preprint

ilya sutskever, and geoffrey hinton.
grammar as a foreign language.
arxiv:1412.7449.

xiaodan zhu, parinaz sobihani, and hongyu guo.
long short-term memory over recursive
2015.
structures. in proceedings of the 32nd international
conference on machine learning (icml-15), pages
1604   1612.

in natural language processing (emnlp), pages
2061   2069.

richard p lippmann. 1989. review of neural net-
works for id103. neural computation,
1(1):1   38.

thang luong,

ilya sutskever, quoc v le, oriol
vinyals, and wojciech zaremba. 2014. addressing
the rare word problem in id4.
proceedings of acl. 2015.

tomas mikolov, wen-tau yih, and geoffrey zweig.
2013. linguistic regularities in continuous space
word representations. in hlt-naacl, pages 746   
751.

bo pang, lillian lee, and shivakumar vaithyanathan.
2002. thumbs up?: sentiment classi   cation using
machine learning techniques. in proceedings of the
acl-02 conference on empirical methods in natural
language processing-volume 10, pages 79   86. as-
sociation for computational linguistics.

romain paulus, richard socher, and christopher d
manning. 2014. global belief recursive neural net-
works. in advances in neural information process-
ing systems, pages 2888   2896.

barak a pearlmutter. 1989. learning state space tra-
jectories in recurrent neural networks. neural com-
putation, 1(2):263   269.

tony robinson, mike hochberg, and steve renals.
1996. the use of recurrent neural networks in con-
in automatic speech
tinuous id103.
and speaker recognition, pages 233   258. springer.

mike schuster and kuldip k paliwal. 1997. bidirec-
tional recurrent neural networks. signal processing,
ieee transactions on, 45(11):2673   2681.

richard socher, eric h huang, jeffrey pennin, christo-
pher d manning, and andrew y ng. 2011a. dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. in advances in neural in-
formation processing systems, pages 801   809.

richard socher, jeffrey pennington, eric h huang,
andrew y ng, and christopher d manning. 2011b.
semi-supervised recursive autoencoders for predict-
in proceedings of the
ing sentiment distributions.
conference on empirical methods in natural lan-
guage processing, pages 151   161. association for
computational linguistics.

richard socher, brody huval, christopher d manning,
and andrew y ng. 2012. semantic compositional-
ity through recursive matrix-vector spaces. in pro-
ceedings of the 2012 joint conference on empiri-
cal methods in natural language processing and
computational natural language learning, pages
1201   1211. association for computational linguis-
tics.

