6
1
0
2

 

y
a
m
3
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
8
3
2
4
0

.

5
0
6
1
:
v
i
x
r
a

semantic spaces

yuri i. manin1, matilde marcolli2

1max   planck   institut f  ur mathematik, bonn, germany,

2california institute of technology, pasadena, usa

abstract. any natural language can be considered as a tool for producing
large databases (consisting of texts, written, or discursive). this tool for its descrip-
tion in turn requires other large databases (dictionaries, grammars etc.). nowadays,
the notion of database is associated with computer processing and computer mem-
ory. however, a natural language resides also in human brains and functions in
human communication, from interpersonal to intergenerational one. we discuss in
this survey/research paper mathematical, in particular geometric, constructions,
which help to bridge these two worlds. in particular, in this paper we consider the
vector space model of semantics based on frequency matrices, as used in natural
language processing. we investigate underlying geometries, formulated in terms
of grassmannians, projective spaces, and    ag varieties. we formulate the relation
between vector space models and semantic spaces based on semic axes in terms of
projectability of subvarieties in grassmannians and projective spaces. we interpret
latent semantics as a geometric    ow on grassmannians. we also discuss how to
formulate g  ardenfors    notion of    meeting of minds    in our geometric setting.

o interior do exterior do interior
pascal mercier
   nachtzug nach lissabon   

1. introduction:

linguistics, semiotics, and topology

one of the basic    meta   physical    principles of classical physics consisted in the

subdivision of informational content of any physical model into two parts:

    a description of the con   guration and phase spaces of the studied system;
    a description of the time evolution law (usually a vector    eld in the phase

space).

1

2

some of the recent approaches to semantics of natural languages describe various
versions of    spaces of meanings    which we consider as a metaphorical analog of
con   guration spaces: cf. comprehensive accounts [g  a00], [g  a14]. for g  ardenfors,
semantics is (in particular) meeting of minds, and the space of meanings is the
space where minds meet.

our initial motivation for undertaking this survey and the research summarised
in this paper was our desire to introduce    a time dimension    in this discussion, to
see a discourse or reception of a text as a path in the appropriate space of meanings.
in particular, we wanted to use mathematical models in order to bridge the
approaches to semantics reviewed in [g  a14], neurolinguistic studies reviewed in
[jele94], [inle04], and neurobiological studies of neural mechanism involved in
coping with tasks related to orientation in physical space (see [cuit08], [cuitv-
cyo13] an brief survey for mathematicians [ma15].)

in the remaining part of the introduction we will give a very short list of several
approaches to description of    meaning    using geometric/topological representations
and/or metaphors.

1.1. semic axes.

in the following it is essential to keep in mind that core
   meanings    are generally assigned not to    words    but to    lexemes   . according to
[me16], p. 240, lexeme is    a word taken in one well de   ned sense     more precisely,
a set of all word forms and analytical form phrases that di   er only by in   ectional
signi   cations.   

example ( [me16], p. 135): lexeme take(v ) includes the following lexical items:

take, takes, took, taking, . . . , have taken, has taken, . . . , have been taken, . . .

the tag (v) here means that our lexeme refers to the word    take    understood

as a verb rather than a noun.

when one extracts a vocabulary of lexemes from a dictionary of words, one
should do    id30    (extracting roots of words),    tagging    etc., cf. a more de-
tailed description in sec. 3 of [tupa10].

we will allow ourselves the use of the term    word    in place of    lexeme    when it

cannot lead to a confusion.

the approach to encoding of meaning, or    sense    of lexemes, brie   y surveyed
in [gui08], starts with postulating a list of    semes    such as animate, inanimate,
actor, process etc.

the meaning is speci   ed by listing a subset of semes.

3

in the respective geometric picture, n semes are represented by basis vectors ei,
i = 1, . . . , n , of rn , and meanings are represented by (a subset of) vertices of the
unit cube [0, 1]n . p. guiraud actually prefers the    bisemic    description, in which
meanings are represented by a subset of vertices of [   1, 1]n . sign changes of basic
coordinates represent the complementarity relations such as in animate/inanimate.
a qualitative weakening of the bisemic model allows meanings to be represented
by points in rn that are localised near the boundary of the unit cube, but not nec-
essarily coincide with its vertices. a nice illustration is given on p. 59 of [g  a14]. it
represents bisemes in a two   dimensional    emotional space    r2 whose bisemic axes
represent dichotomies pleasure/displeasure and high/low whereas, say, the quadrant
   low pleasure    accommodates lexemes content, serene, calm, relaxed, sleepy.

some of the largest subsets of the space of meanings that can accommodate, say,

path of a narrative, might encode notions related to
    senses: vision, hearing, feelings, time, space . . .
    some subregions like    far away     near    ,    quiet     loud   ,    past     future   
    regions related to    me   , to    other people    ,    unrelated to humans   , etc.
what is important is that we should construct this semantic space at    rst in a
way maximally independent of the    natural language    we choose, and that it will
widen at each stage of construction in order to accommodate new words, sentences,
languages etc.

1.2. semic axes and neural encoding of place    eld recognition. we
want to derive from semantics of a natural language a structure encoding it that
would be a space covered by subsets, say, ui. (some) non-empty    nite intersections
should correspond to words or short sentences, paths through this space should
correspond to texts.

a nice example of this is provided in [li], together with a picture representing
symbolically two di   erent subsets of semantic space in two possible mutual rela-
tionships: (i) inclusion of one in another, and (ii) non   empty intersection without
inclusion.

this picture illustrates the di   erence between usages of words which and that in

the following two sentences:

correct use of that:    ti   any likes shoes that are expensive   .
   the set of things called shoes includes both expensive and inexpensive shoes,
so when we say    that are expensive,    we are talking only about a subset of the set
of all things called shoes.   

4

correct use of which:    ti   any likes emeralds, which are expensive   .

   the set of things called emeralds are all expensive, so the clause    which are
expensive    talks about the whole set of emeralds. there is no inexpensive subset
of emeralds.    which are expensive    simply gives you additional information about
this whole set    ([li]).

this basic picture representing meanings by domains in the space of meanings
and the relationship of intersection/inclusion between the respective domains    ts
very well the studies aimed to the understanding how brain copes with multiple
tasks of orienting and navigating in the world, cf. [cuit08], [cuitvcyo13], [yo14],
and references therein.

the brain of an animal must be able to reconstruct, say, a map of its environ-
ment and its current position in it, using only the action potentials (spikes) of the
relevant cell groups. in laboratory experiments it is found that stimuli related to
the positions are naturally divided into groups, and with each group a certain type
of neural activity is associated.
in [cuit08] and [yo14], it is postulated that a
given domain of stimuli can be modelled via a topological, or metric stimuli space
x. furthermore, brain reaction to a point in x is modelled by spiking activity of
certain    nite set of neurons n x. the list of subsets of n x consisting of subsets
whose neurons can be activated simultaneously, corresponds to a certain covering of
x. thus this covering can be described by a binary code, and relations of intersec-
tion/inclusion between domains coincide with the relations of intersection/inclusion
between the respective code words. for more details, see [ma15].

1.3. meaning   text model. in the model of semic axes, there is one intrinsic
source of incompleteness: as p. guiraud says ([gui68], p. 157), the lexical units
(corresponding to vertices of [   1, 1]n )    must in addition be associated in syntagms,
each one of them constitutes a    sense   . but there again we must setup rules for
combinations, for the sense supposes that certain syntagms are permitted, other
excluded.    the di   erence between which and that discussed above is precisely an
example of such syntagms.

this problem is very systematically addressed in the model    meaning   text   
which i. mel     cuk and his collaborators have been developing for several decades:
see [me16] for its most recent summary and further references.

in this model, meaning of a text of language l    is exclusively [ . . . ] linguistic
meaning    that can be extracted    only on the basis of the mastery of l, without the

5

participation of common sense, encyclopaedic knowledge, logic etc. ([me16], sec.
3.2.2).

on the other hand, geometry/topology    gures in this model mainly as a tool
for producing graphs of various levels of linguistic representation. each such graph
consists of several vertices, certain pairs of which are connected by edges. more-
over, both vertices and edges are additionally marked. for example, on the level
of the surface   syntactic structure, a sentence is represented by the graph, whose
vertices are marked by lexemes corresponding to the words in this sentence, and by
additional information encoding the passage from the lexeme to the word. edges
of this graph are marked by technical terms expressing syntactic relations between
the respective pair of words.

below we illustrate this principle by presenting the surface   syntactic graph of
the    rst line of a sonnet by michelangelo. we are very grateful to i. mel     cuk who
produced for us this graph and allowed us to reproduce it here.

1.4. neurolinguistic data. there exists a large body of neuroimaging studies
of production and perception of spoken language. in the survey [inle04], the reader
will    nd descriptions of methodology used and results obtained in    the enterprise
of relating the function components of word production, such as lexical selection,

6

phonological code retrieval, and syllabi   cation, to regions in a cerebral network   
([inle04], p. 102.)

an illustration of segment of lexical production network ([jele94], p. 826) shows

fascinating parallels with the meaning   text model.

due to the vastness of semantic space needed to accommodate all meanings
expressible in a natural language, direct comparison with the neural encoding of
place    eld recognition as in [cuit08] is not yet feasible. however, the development
of new methods of studying and collecting databases of results allows us to hope
that such comparison will become possible.
in this paper, we try to contribute
some mathematical tools that may be useful for this endeavor.

1.5. this report. most of the approaches discussed above directly appeal to
the linguistic intuition and communicative experience of scientists, experimenters,
and participants in experiments. information obtained by the respective methods
should be considered as local data about semantic space, and/or about short paths
in it.

on the other hand, if we want to obtain mathematical models of topology of
semantic spaces and of longer routes in such a space, expressed by texts of the
size, say, of a chapter in    war and peace   , we may turn to the statistical natural
language processing.

then, in the    rst approximation, a text becomes a point in the space of paths
in the semantic space, and we discuss here approaches to studying the topology of
such spaces appealing mostly to the data about frequencies of lexemes and other
text fragments taking in account their linear ordering in the text. semantics of such
fragments as it is represented by dictionaries and experiments is thus put aside to
a certain degree, although not fully.

in the main body of this article, we will describe some mathematical tools that
can be used for the introduction of    time dimension    in the study of texts. they
will refer to the geometry of real projective spaces and real grassmannians. passage
from texts to the relevant geometry is based here on the vector space models of
semantics (vsm) surveyed in [tupa10], and we will brie   y explain this model for
further use.

in section 2 we discuss how the frequency matrix of the vsm approach, that
counts occurrences of lexemes in contexts in a given corpus of texts, determines
a point in a grassmannian. we show that, in the case of a large vocabulary of

7

lexemes and a smaller number of contexts, the condition that the resulting point
lies in the positive grassmannian provides a geometric test for the property that a
choice of lexemes gives a good semantic disambiguation of the contexts. a similar
condition holes in the case of a small number of lexemes, where one wants to test if a
set contexts would disambiguate the words semantically. this geometric viewpoint
takes into account the fact that contexts come with a speci   c ordering by occurrence
in a text.

in section 3 we discuss other geometric models associated to the frequency ma-
trices of the vsm approach, which also takes into account the speci   c ordering of
contexts in a text. we assign to a text a piecewise geodesic path of points in a
projective space. instead of measuring semantic relatedness in terms of angle dis-
tances between the semantic vectors of the frequency matrices, as it is customary
to do in natural language processing, we compare the paths in an ambient pro-
jective space through a geometric distance function between (geodesic) polygonal
curves, which is known to be computable in polynomial time. in a variant of this
construction, we also consider assigning to the frequency matrix a point in a    ag
variety, where the    ag corresponds to the span of successive semantic vectors for
the successive contexts ordered by occurrence in a text. again semantic similarity
can be measured in terms of the geodesic distance in the    ag variety, with respect
to its natural metric as a quotient of lie groups.

in section 4 we consider the case where lexemes are grouped together according
to some semantic axes, either by explicit semantic tagging (supervised learning) or
just by grouping together lexemes with similar occurrences in contexts (unsuper-
vised learning). in both cases, we describe the process of passing from frequency
matrices for a given corpus of text, computed with respect to a dictionary of lex-
emes, to density matrices with respect to a semantic dictionary, where identi   cation
of lexemes by semantic criteria has already occurred. when we view the frequencies
as determining points in grassmannians, we can view geometrically this operation
as a projection between two grassmannian. the question of whether one can avoid
loss of semantic information in this process, when applied to a given collection of
texts, is then interpreted in terms of whether the points corresponding to these
texts lie on a subvariety of the grassmannian that can be isomorphically projected
to the other grassmannians. a similar condition arises when we assign to a given
text a piecewise geodesic path in a projective space as discussed in section 3.

in section 5 we connect the geometric setting described in the previous section
with the point of view of persistent topology. according to our previous construc-

8

tion, a large corpus of texts determines a corresponding set of points in an ambient
grassmannian, where we assume that the same    xed dictionary of lexemes (or
semes) is used to analyze all texts in the corpus. we then show that one can
identify more re   ned forms of semantic relatedness between these points. these
are topological in nature and arise from constructing vietoris   rips simplicial com-
plexes at varying scales, associated to the set of points in the ambient variety and
computing their persistent homology. we discuss possible relations to the use of
persistent topology in the theory of neural codes.

in section 6 we show that the latent semantics technique for dealing with very
sparse frequency matrices in the vsm approach, which identi   es lower dimensional
subspaces (latent meanings) through singular value decomposition, can be inter-
preted in terms of the geometry of grassmannians described in section 2, as a
riccati    ow on the ambient grassmannian.

in section 7 we discuss how to implement, in our geometric setting, a model anal-
ogous to g  ardenfors       meeting of minds   , where common meaning between di   erent
users communicating with one another is achieved as via a    xed point problem in a
convex semantic space. we suggest that a similar idea can be implemented in our
setting if di   erent users come to somewhat di   erent semantic interpretations of a
given texts, on the bases of semantic interpretations based on other texts available
to them, under the assumption that users have access to di   erent (partially over-
lapping) corpora of texts. we then describe the procedure of    meeting of minds   
as the construction of a geodesic barycenter in the ambient geometric space of the
distribution of points obtained by the users, possibly weighted according to some
measure of    reliability    of the di   erent corpora used for semantic interpretation.

in section 8 we discuss how to compensate for the fact that the frequency dis-
tribution for words in a dictionary is skewed towards the more frequent and less
semantically signi   cant words according to zipf   s law.

2. vector space models of semantics

2.1. texts and their processing. a concrete vsm starts with a large corpus
of natural language texts and produces from it a matrix of numbers (frequences).
the intermediate steps of this production are subdivided into two groups: (i) lin-
guistic processing, and (ii) statistic processing.

for us, linguistic processing results in the creation of the relevant vocabulary of
lexemes where we understand    lexeme    as in 1.3 above. each text is also represented

9

as a sequence of the relevant lexemes, although from the description of [tupa10]
it becomes clear that at least some fragments of it are modelled by their surface   
syntactic structures in the sense of meaning   text model.

we accept this as a reasonable approximation to the procedures described in

sec. 3 of [tupa10].

statistic processing, as we mentioned, produces a (normalised) matrix of fre-

quencies, see [tupa10], sec. 4.

in the typical case called    the term   document matrix    in [tupa10], rows of the
matrix are labelled by lexemes (   terms   ), whereas columns are labelled by texts in
our collection.

in another typical case called    the word   context matrix    ([tupa10], sec. 2.5),
the texts, already at the stage of linguistic processing, are represented as a union of
   contexts   . here again the rows of matrix are labelled by lexemes, whereas columns
are labelled by contexts.

finally, matrix entries as we mentioned characterise correlations between the
lexemes and text/contexts. we will treat in more detail some cases below, and
address the question of    smoothing   .

2.2.    time dimension    and other linear orderings. any vocabulary of
lexemes, or contexts, must be in the    nal count also presented as linearly ordered
dictionary. this ordering might be totally irrelevant to the situation under study (as
e.g. alphabetic ordering). it can take into account the order of    rst appearance of
the respective lexeme in the text. finally, it can be a zipf   s   like ordering according
to diminishing frequency rate.

a considerable part of statistical characteristics of a vsm does not depend of the
chosen orderings (although the mode of their usage might depend on it). however,
for the purpose of our paper this might become essential, and we will pay due
attention to it.

2.3. notation and assumptions. we will consider word   contexts matrices

described above in one of two possible extreme subcases.

(a). large vocabulary case. in this setting, we assume that our vocabulary of
lexemes is su   ciently large and includes at least all the lexemes that appear in the
texts (excluding words with large occurrences in all contexts such as    and    or    the   
in an english text that are semantically less informative). moreover, we assume
that the size of the vocabulary is large compared to the number of contexts in the

10

texts. in this case, one aims at selecting from the large dictionary choices of words
that best represent the given contexts semantically.

(b). information retrieval case. in this case we consider a vocabulary that is
small compared to the number of contexts, as would be the case with a choice of
words used in a query. in this case one aims at selecting among the various contexts
in a given corpus those that best match semantically the chosen words in the query.

let d be our vocabulary, and m = #d be the number of lexemes in it.
a given text t is then endowed with a set of subtexts called contexts: c(t ) =
{c1, . . . , cn }. typical examples of contexts are: sentences, paragraphs, or else
windows of certain length around each word/lexeme.

2.4. matrix of frequencies. following [tupa10], one produces from these
data an n    m matrix of frequencies p = p (t ) with entries pij. here pij is the
estimated id203 (frequency) of occurrence of the word wi     d in the context
cj     c(t ). in the vsm model, one usually considers also the matrix x = x(t )
with entries x = (xij),

xij = max{0, log(cid:18) pij

pi   p   j(cid:19)},

where pi    = pj pij is the estimated id203 of the word wi     d and p   j =
pi pij is the estimated id203 of the context cj     c(t ). the condition that

pij = pi   p   j corresponds to statistical independence of word wi and context cj,
while pij > pi   p   j signals the presence of a semantic relation between them.

more precisely, the formula pi    = pj pij gives the frequency of appearance in

the text in the case where contexts do not overlap whereas their union is the whole
text.

in the more general case where contexts may overlap one still uses the same
matrix but now its entries are the frequencies of appearance across all contexts (or,
equivalently, the frequencies of appearance in the text, weighted by some multiplic-
ities that keep track of when a word appears in the intersection of more than one
context).

if a word is in the intersection of two adjacent contexts j and j +1, then it a   ects

the counting in both pij and pi,j+1, so pj pij is still the id172 factor.

the typical example of this    overlapping contexts    method is the original shan-
non 3-gram model: here one has probabilities (based on frequencies) for occurrences

11

of 3 words in a row. for example, one can have a word sequence a-b-c-d where the
three words a-b-c have a very high id203 of occurring together, while the
id203 of the triple b-c-d is very low. this suggests that it is a-b-c rather than
b-c-d that clari   es better the semantic meaning of the words b and c, and that the
semantic meaning of d will more likely be clari   ed by the trigrams that follow like
c-d-e and d-e-f, with the following words e,f, rather than by b-c-d.

2.5. large vocabulary case. here we have n     m . the dictionary d
includes (at least) all the (relevant) lexemes that occur in the text t , and the
number of contexts in which the text t is subdivided is smaller than the number
of words in the dictionary.

the statistical semantics hypothesis states that statistical patterns of word us-
age in texts determine their semantical meaning, and in particular that (parts of)
text that have similar vectors in the above frequency matrices also have similar
meanings.

let r = rank(p ) be the rank of the matrix p (t ). in the case of large dictionary,
we have r     n . under the statistical semantics hypothesis, the rank r measures
the largest number of words and contexts that the text t disambiguates semanti-
cally. namely, the linear dependence of frequency vectors is interpreted as revealing
the presence of underlying semantic relations. when r = n , all the contexts in t
have a choice of corresponding words that they semantically disambiguate.

in the case where r = n , the matrix p (t ) of a text t determines a point p(t )
in the real grassmannian gr(n, m ) of n -planes in real euclidean space rm . sim-
ilarly, if rank(x(t )) = n , the matrix x(t ) determines a point x(t )     gr(n, m ).
for simplicity, we argue about the matrix p (t ). when not otherwise stated, the
same will apply to x(t ). let m = m(t ) be the set of subsets of {1, . . . , m } of car-
dinality n , such that the determinant of the corresponding minor is    i (p (t )) 6= 0.
the set m determines a matroid stratum sm     gr(n, m ), with p (t )     m.

in the case n     m , instead of working with a    xed (large) dictionary d for all
texts, it is convenient, given a text t , to discard all the words in d that do not
appear anywhere in t , as the text does not have any relevance for those words.
thus, we can assume that d = d(t ), with #d(t ) = m (t ) is the list of words
that appear in t (with a suitable stop list). a text t has a linear ordering, which
induces an ordering on the set d(t ) that lists words in order of apparition in t .
we identify d(t ) with the set {1, . . . , m (t )} using this ordering. similarly, the
set c(t ) of contexts is also ordered by how they are ordered in the text t , and we

12

identify c(t ) with {1, . . . , n (t )} using this ordering. the order of apparition of
words in the text t is relevant to the semantic interpretation of the text, as the
   rst occurrence of a word is the    rst instance where a semantic interpretation for
that word is required.

consider then the set of subsets i = {i1, . . . , in } of [m ] := {1, . . . , m } with
i1 < i2 < . . . < in . these correspond to choices of words wi1 , . . . , win in d(t ),
such that the order of apparition of these words in the text t is respected, and we
consider the frequency vectors pik := (pik,j)j for the occurrence of the word wik
in the context cj. we consider the gale ordering on these subsets i. namely, two
such subsets i = {i1, . . . , in } and j = {j1, . . . , jn }, with i1 < i2 <          < in and
j1 < j2 <          < jn , we have i    g j i    i1     j1, i2     j2, . . . , in     jn . the gale
ordering corresponds therefore to the relative position of words wik and wjk in the
dictionary d(t ) according to    rst apparition in t .

the original dictionary d also has an ordering, and therefore the smaller dic-
tionary d(t ) also has an induced ordering, which is di   erent than the order of
apparition in the text t . one then has some permutation        sm , such that
the gale ordering described above corresponds to the ordering i       j, namely
     1i    g      1j.

the condition that, for one of these subsets i, the corresponding minor of the ma-
trix p (t ) has vanishing determinant    i(p (t )) = 0 means that there is a linear de-
pendence between the vectors pik , hence under the statistical semantics hypothesis
a semantic relation between the wik . thus, the matroid stratum sm     gr(n, m )
containing the point p(t )     sm determined by the text t describes, for the given
contexts ci of the text t , all the choices of words wik , k = 1, . . . , n in the dictio-
nary for which the semantic vectors pik are independent. this can be seen as the
maximal amount of semantic information that can be extracted from the text and
its contexts.

recall that the positive (or totally non-negative) grassmannian gr   0(n, m ) is
the subset gr   0(n, m )     gr(n, m ) of matrices a such that for all    i (a)     0, for
i as above. the intersections of the matroid strata with the positive grassmannians
s    0
m = sm     gr   0(n, m ) are cells, the positroid cells of [pos06].
in particular, the condition that the point p(t ) lies in the positroid cell s    0

m , that
is, that all    i(a) > 0, for all i     m, is equivalent to the existence of continuous
paths   i, for each i     m, where   i (0) = p (t ) and   i(1) is a matrix where the
i-minor is the identity, and for all t     [0, 1] one has   i(t)     s    0
m . this condition

13

can be regarded as expressing the fact that the choice of words wi1 , . . . , win for the
contexts c1, . . . , cn of the text t contains a maximal amount of semantic informa-
tion. indeed, the case where the corresponding minor would be the identity, would
correspond to a case where the word wik is entirely speci   ed semantically by the
context ck and by none of the other cj with j 6= k.

2.6. information retrieval case. we now focus on the other case mentioned
above, the    information retrieval    setting, where we have n     m , that is, where
the list of words is, for example, the list of words in a query, and one wants to
locate texts, or contexts within a text, that are semantically most relevant for that
query. in this case, we can assume that the number of words searched is no greater
than the number of contexts.

the setting is similar to what we described before, except that we now consider
the case where the matrix p (t ) determines a point in the grassmannian gr(m, n ).
the minors i = {i1, . . . , im } correspond to choices of contexts cik in the text t in
response to a query given by the words wk. as before the condition    i(p (t )) > 0
corresponds to those assignments of a context to each word of the query that best
matches it semantically.

2.7. literary texts and their statistical processing. d. yu. manin in
his article [man12] suggests that literary texts (prose/poetry) require qualitatively
di   erent methods of statistical processing in order to make explicit what puts them
apart from texts produced in ordinary speech.

here we only mention a di   erent kind of contexts used there ([man12], p. 286).

namely, a context in his sense is a fragment of text with a blank, a hole where
di   erent words might occur, like    a   *   b   . this would allow one to extract statisti-
cal data allowing one to say that    words x and y often occur in the same contexts   .
presumably, this fact would then re   ect semantic relationships between x and y.

in the limiting case where x can occur in all the same contexts as y, and with
the same frequencies, that would mean that x and y are exact synonyms. or, if
x can share contexts with u and v, but u and v do not share contexts, then they
probably represent two very distinct meanings of x.

in this paper, we do not try to study semantic spaces and paths in them relevant

to this approach. we only mention that it might be a very interesting project.

14

3. projective spaces and flag varieties

we describe here two variants of the construction above, aimed at encoding
more explicitly the fact that a linguistic text has an ordered linear structure that is
crucial to its semantic interpretation. we propose two modi   cations of the geometry
described above that better encode this fact. one is based on regarding a text
subdivided into contexts, as a collection of points determining a path in a projective
space, rather than as a single point in a grassmannian. the second is in terms of
points in a    ag variety.

3.1. texts as paths in projective spaces. here we again consider the case
where we have some    xed large vocabulary d of lexemes of size m = #d, which
contains at least all the words in the given text t . we also subdivide the text into
contexts ck, as before, but we do not necessarily assume that the total number n of
contexts is smaller than m . indeed, in this setting we could be dealing with a large
corpus of texts and a large number of contexts. we again consider the semantic
vectors pk(t ) = (pik)i   d that collect the probabilities (frequencies) of occurrence
of words wi     d in the context ck of t . we regard each pk as determining a point
pk in the projective space pm    1     gr(1, m ). thus, a text t here corresponds to
an ordered n -tuple of points in pm    1, where n is the number of contexts. we
can think of this collection of points as an oriented path by drawing geodesic arcs
between consecutive points. we denote by   (t ) the resulting path associated to a
text t .

given di   erent texts t and t    , the comparison at the level of semantic vectors
can be performed, in this setting, by computing the distance between the corre-
sponding paths in the same ambient pm    1. this can be computed as the fr  echet
distance between the two polygonal curves. the latter is de   ned as the in   mum
over reparameterizations by [0, 1] of the maximum over t     [0, 1] of the distance
between corresponding points

  (  (t ),   (t    )) = inf
  ,      

max
t   [0,1]

df s(  (t),       (t)),

where    : [0, 1]       (t ) and        : [0, 1]       (t    ) are parameterizations of the two
curves by [0, 1], and df s(x, y) is the fubini-study metric on pm    1. the fr  echet
distance for polygonal curves is computable in polynomial time ([algo95]).

3.2. texts as points in    ag varieties. another way to keep track of the linear
ordering of contexts in a given text is by building larger subspaces, as more and

15

more contexts in the given texts are encountered in a linear reading of the text.
thus, if pk(t ) = (pik)i   d are the semantic vectors as above, one considers the
vector spaces vk = span{pj : j = 1, . . . , k}. the spaces v1     v2                  vn form
a    ag in rm . we denote by f (d1, . . . , d   ) the    ag varieties of    ags w1              w   
with dim(wk/wk   1) = dk. we associate to a text t the point of the corresponding
   ag variety f (1, . . . , 1, m     n ) determines by the    ag v1     v2                  vn with
vk = span{pj : j = 1, . . . , k}.

the natural fubini   study metric on projective spaces has an analog for grass-
mannians and    ag varieties. it is obtained from the curvature form of the    rst chern
class of the determinant line bundle of a hermitian vector bundles ([dem88]), or else
by considering these varieties as quotients of su (n) by subgroups, with the metric
induced from the bivariant metric of su (n) ([gri74]). thus, one can compare texts
viewed as points in grassmannians or in    ag varieties, by measuring their distance
with respect to this metric.

4. from lexemes to semantic dictionaries

we now consider a setting where, instead of a    lexemes dictionary    d of words,
one passes to a    semantic dictionary    s where lexemes are grouped together ac-
cording to some semantic description. this can happen in two di   erent ways, based
on supervised or unsupervised learning.

(a) supervised learning. in this case, also referred to as    sense tagging    (see
[masch99]), lexemes are grouped together into semantic categories by assigning
appropriate tagging.
in this setting, the type of question we look at is to what
extent the information contained in the semantic vectors computed for the initial
lexical vocabulary still retains the correct information when passing to a quotient
that corresponds to the identi   cation by semantic categories.

(b) unsupervised learning.

in this case, sense tags are not assigned, so that
one cannot identify directly the corresponding semantic categories, but one can
still obtain a    sense discrimination    by grouping together words into unlabelled
groups using the information contained in the semantic vectors. in this setting, we
will show that the resulting grouping can be studied in terms of persistent topology
([ca09]).

4.1. supervised learning. we consider the case where we associate to texts
t points p(t ) in a grassmannian (either gr(n, m ) or gr(m, n ) depending on

16

relative size of vocabulary and contexts). we consider the case n < m . the other
possibility can be treated similarly.

we want to consider also the case where we deal not with a single text t but
with a corpus consisting of several texts. in this case, we need to assume that the
vocabulary d, with m = #d, is large enough to include all words that occur in
all the texts of the corpus. moreover, if we choose an ordering of the dictionary, as
discussed previously, by order of apparition in a text, we can extend the order to
the whole corpus, by choosing an order in which the di   erent texts in the corpus
are looked at. for the model with points in grassmannians, or in    ag varieties,
we consider the case where the number n of contexts is    xed across all texts in
the corpus. in the more general case where the number n = n (t ) varies across
texts, we will be working with the model in which texts determine a sequence of
points and an oriented polygonal path in a    xed projective space. in both cases,
the question will be the behavior of the locus (in the grassmannian,    ag variety,
or projective space) determined by the semantic vectors of all texts in the corpus,
under a projection map that corresponds to passing from the lexical to the semantic
dictionary.

4.2. points in grassmannians and flag varieties. at the level of the
matrix p (t ) and the corresponding point p(t ) in the grassmannian gr(n, m ), one
can view the operation of passing from the lexemes in d to the semantic categories
in s as the e   ect of a projection   m,m     : gr(n, m )    gr(n, m    ), where m         m
is the size of the set of semantic categories considered, m     = #s.

we regard a corpus c = {t } of texts t as a discrete sampling of a subvariety
of the grassmannian gr(n, m ), under the hypothesis that the number of contexts
is    xed and the size of the dictionary d is also    xed for all t     c. we denote by
  c = {p(t )}t    c the    nite set of points on gr(n, m ) corresponding to the texts in
the corpus. given the    nite set   c, we consider possible algebraic subvarieties xc    
gr(n, m ) that interpolate the points p(t )       c, namely algebraic subvarieties xc
of gr(n, m ) with   c     xc.

we recall some results about projectability of subvarieties of grassmannians, see
[arra05]. a subvariety x     gr(n, m ) is k   projectable, for some 0     k     n     1,
under   m,m     : gr(n, m )    gr(n, m    ) if any two n    planes in the image of x only
meet along linear spaces of dimension less than k. the case k = n corresponds to
x being isomorphically projectable to gr(n, m    ). note that k   projectability also
implies that no two n    planes in x can intersect in dimension greater than or equal

17

to k.

if the variety xc associated to a corpus c of texts is k   projectable to gr(n, m    ),
this means that the n -planes given by the images   m,m     (p(t )) and   m,m     (p(t    ))
of any two points p(t ), p(t    ), with t, t         c, will intersect in at most a (k     1)-
dimensional space.

the size of the intersection between the n -planes of t and t     is a measure of de-
pendence between the respective semantic vectors, hence of the semantic relatedness
of the two texts. if in the variety xc every two n    planes intersect in dimension less
than k, but the variety is not k-projectable under   m,m     : gr(n, m )    gr(n, m    ),
this means that there is loss of semantic information in the matching of words (and
their semantic categories) to contexts in the texts of the corpus.

there are strong algebro   geometric restrictions on k-projectable varieties. for
example, it is shown in [arra05] that the veronese embedding of pn is the only
variety in gr(d     1, dn + d     1) that can be projected to gr(d     1, n + 2d     3) so
that any two (d     1)-planes meet in at most one point.

we have only discussed here the case where we associate texts to points in
grassmannians. the case of points in    ag varieties is similar, with similar questions
about k-projectable subvarieties.

4.3. paths in projective spaces. we then consider the case where the size
n = n (t ) of contexts in a text is varying with t     c. in this case, instead of
working with texts de   ning points in a grassmannian, it is more convenient to
adopt the viewpoint where texts determine polygonal paths in a projective space
pm    1 with m = #d the size of the dictionary. in this case, we are looking at a
similar question about k   projectable subvarieties in projective spaces.

more precisely, we consider again algebraic subvarieties xc of pm    1 that contain
all the paths   (t ) for t     c. as a weaker condition, we can just assume that the
variety xc contains the set of points   c = {pk(t ) : t     c, k = 1, . . . , n (t )}. if
xc is also geodesically complete, then it contains also the paths   (t ).

we then consider a projection   m,m    

: pm    1    pm       1 that corresponds to
performing some identi   cation of the vocabulary by grouping lexemes according to
a choice of semantic categories, with m     = #s.

we are then looking at the problem of whether it is possible to project isomor-
phically a subvariety xc of pm    1 that contains the points   c (and possibly the

18

collection of paths   (t )) to the quotient pm       1. again, there are strong restric-
tions on the existence of such isomorphically projectable subvarieties. for example,
it is shown in [ar01] that the only n-dimensional variety that can be isomorphi-
cally projected from gr(1, 2n + 1) to gr(1, n) is the veronese variety, that is, the
embedding of pn in gr(1, 2n + 1) via opn (1)   d.

when the variety xc is not isomorphically projectable from pm    1 to pm       1,
there is some loss of information in the semantic vectors, when the identi   cation
of words according to semantic tags is performed.
in such cases, which will be
typical in view of the very restrictive condition of isomorphic projectability, one
can describe the e   ect of the identi   cation on semantic vectors by analyzing the
change of topology in the polygonal path   c =    t    c  (t ). we describe ways of
approaching computationally such topology changes.

4.4. persistent topology.

it was understood in recent years that clusters
of data points can exhibit interesting topological structure that can be useful in
analyzing large data set, see [ca09] for a general introduction and overview of the
   eld of persistent topology. applications of persistent topology to linguistics were
recently discussed in [porghgucldmar15].

given a set    of points in a metric space, one considers a family of simplicial
complexes, parameterized by a real number    > 0, the so called vietoris   rips
complexes r(  ,   ). here the n   th term rn(  ,   ) is the vector space spanned by all
the unordered (n + 1)-tuples of points in    where all pairs have distance at most   .
there are inclusion maps r(  ,   1)       r(  ,   2) when   1 <   2. these induce maps
in homology hn(r(  ,   1))     hn(r(  ,   2)). in analyzing the dependence on    of
the ranks of these homology groups one discards as    noise    those generators that
arise and disappear within a small range of values of   , while one regards those
generators that persist for su   ciently long intervals of values of   , the    persistent
generators   , as signaling the presence of actual structure in the data.

persistent topology of the set   c in the grassmannian. persistent topology can
also be used to enrich the semantic comparison of di   erent texts, when we assign
to each text in a corpus a point in a grassmannian or in a    ag variety, as discussed
above. the simplest level of comparison would be to cluster together the points
corresponding to the various texts by separating them into groups according to the
relative distances in the ambient metric. the resulting groups are dependent upon
the scale of the neighborhoods of points, and the number of di   erent groups of
semantic similarity correspond to the rank of the zeroth order persistent homology

19

of the vietoris   rips complex. thus, more re   ned information about how texts
cluster together by semantic similarity is obtained by additionally considering also
the    rst and higher dimensional persistent homology.

we consider, as above, a projection   m,m     : gr(n, m )    gr(n, m    ) and the
image   m,m     (  c). in the case where the set of points   c does not    t on an inter-
polating variety that is isomorphically projectable, we can analyze the change in
the semantic proximity of texts by analyzing the di   erences between the persistent
topology of   c and of   m,m     (  c). this can be seen by computing the number of
persistent generators, in various degrees, of the homology of the respective vietoris   
rips complexes. the case of points in    ag varieties can be treated analogously to
the case of points in grassmannians.

persistent topology and paths in pm    1. in a similar manner, one can use per-
sistent topology to analyze syntactic proximity of texts in the point of view where
we assign to each text in a corpus a path in projective space pm    1. in this case,
we again associate to a corpus c a simplicial complex, where the zero-cells are all
the points pk(t )     pm    1, for all texts t     c, and all the one-cells are the geodesic
arcs connecting consecutive pairs of points pk(t ) and pk+1(t ). the higher dimen-
sional skeleta are then constructed as in the vietoris   rips complex, by adding an
n-dimensional simplex whenever an n + 1-tuple of points {pk0 (t0), . . . , pkn (tn)}
where the geodesic distances between all pairs of these points are less than a    xed
scale   . this may require introducing additional one-cells in the complex.

as in the case of points in grassmannians and    ag varieties, when we consider a
projection   m,m     : pm    1    pm       1, we can study the e   ect of the projection on the
persistent topology of the set of paths   c =    t    c  (t ) and its image   m,m     (  c),
by associating complexes as indicated above to   c and   m,m     (  c) and comparing
generators of the respective persistent homologies.

4.5. unsupervised learning. in the case of unsupervised learning, a group-
ing corresponding to    sense discrimination    is obtained solely on the basis of the
semantic vectors and the position of the corresponding points in the ambient vari-
ety, without any external tagging of words by semantic categories. in the setting
of unsupervised learning, the grouping together of subsets of the m lexical dimen-
sions into putative semantic categories is itself performed solely on the basis of the
semantic vectors. a simple way to search for semantic relatedness in an unsuper-
vised context is to identify frequent co-occurrences within the same contexts (see
section 2.4 of [tupa10]). many co-occurrences arise for purely syntactic reasons,

20

but those tend to be between words that belong to di   erent parts of speech, while
co-occurrences that carry semantic signi   cance are more often found between words
in the same part of speech, see [buhi06], [chiabrp90], [schpe93], [tupa10].

4.6. syntactic dependence of semantic vectors. clearly, the vectors
pk(t ) = (pik)i   d, associated to the contexts ck in a text, depend on both syn-
tactic and semantic information and there is a priori no obvious way to distinguish
between the dependence on syntax and on semantics. however, a possible way
to make these semantic vectors more syntax independent would be to consider a
training corpus of di   erent language translations of the same texts, with marked
matching paragraphs and matching word dictionaries, and average the semantic
vectors pk(t, l) over the set of languages l. this can be done either by simply
averaging the vectors, or else by considering the corresponding points pk(t, l), for
all languages l, in the    xed ambient pm    1, and replace them by the barycenter
  pk(t ) computed with respect to the fubini-study metric on pm    1. this has the ef-
fect of reducing the purely syntactic contribution, especially if the set of languages
chosen contains languages with su   ciently di   erent set of syntactic parameters.
of course, it is not possible to entirely decouple semantics from syntax, as the
syntactic-semantic interface is very rich (see for example [ha13], [va05]), but this
averaging method can at least partially reduce the in   uence of those e   ects that
are due to syntax alone.

5. geodesically convex neighborhoods and semantic spaces

in the setting above, we have associated to texts in a corpus a collection of points
(or of paths) in an ambient geometric space (a grassmannian, or a    ag variety, or
a projective space). we have also seen that, when we group together words in the
lexicon by semantic categories, geometrically we look at how the set of points and
paths behaves under a projection map of the ambient variety. in this section we
use the same general geometric picture, and we consider coverings by convex open
sets. these local neighborhoods correspond to grouping together texts by semantic
similarity. the convexity property corresponds to the possibility of interpolation
and will be compared in section 7 with the approach of g  ardenfors on conceptual
spaces as    meeting of minds   , cf. [g  a00], [g  a14], [wag  a13].

5.1. geodesic convexity and good coverings. recall that a subset u     x
in a riemannian manifold x is said to be geodesically convex if for arbitrary points
p 6= p        u there is a distance minimizing geodesic arc connecting them that is

21

entirely contained in u . in particular, a geodesically convex u is topologically a
contractible set. moreover, a non   empty    nite intersection of geodesically convex
open sets ui is also a geodesically convex open set. if x is compact, we can assume
the number of open sets in such a covering to be    nite. their size (measured as the
diameter) in such a covering is bounded. we then say that u   = {ui(  )}n
i=1 is a
good      covering of the compact riemannian manifold x, if the ui are geodesically
convex with    = maxi{diam(ui(  ))}.

in particular, we consider such coverings for the grassmannians gr(n, m    ),    ag
varieties f (1, . . . , 1, m         n ), and projective spaces pm       1, with the respective
metrics discussed above, and where m     is the size of the semantic vocabulary, after
semantic identi   cations have been performed on the initial lexical vocabulary of size
m     m    , as discussed in the previous section. we view points pk(t ) that lie within
the same convex neighborhood ui(  ) of a good   -open covering by geodesically con-
vex sets as being semantically related. in particular, we are interested in considering
good   -open coverings that are generated by starting with a collection uk(  , t ) of
geodesic balls of radius   /2 centered at the points pk(t ) associated to a text t in
a corpus. consider the case where pk(t )     pm       1. the cases of points in grass-
mannians and    ag varieties are analogous. we construct an   -open covering of the
ambient variety by starting with the collection {uk(  , t ) : k = 1, . . . , n (t ); t     c}
and we complete it to an   -open covering of pm       1 by adding enough additional sets
ui(  ) covering the complement of yc,   :=    k,t uk(  , t ). we let u   denote the result-
ing covering of pm       1 and we write u  (c)     u   for the covering of yc,       pm       1
by the {uk(  , t ) : k = 1, . . . , n (t ); t     c}.

as it is customary in topology, we can associate to a given good   -covering
u  (c) the simplicial complex given by its   cech complex n   (c,   ) := n   (u  (c)),
with geometric realization n (c,   ) := |n   (c,   )|. note that, while the geometric
realization of the   cech complex of the full covering u   of pm       1 is just homotopy
equivalent to pm       1 (see [se68] and also [dui] for a generalization), the geometric
realizations n (c,   ) of the subcomplexes u  (c) of u   in general depend on the corpus
c and will in general not be homotopy equivalent to the ambient space.

one can then study, for a given corpus of texts c, how the homotopy type, and
invariants such as homology, of the simplicial space n (c,   ) vary with the scale   .
according to the usual approach of persistent topology, those features that change
rapidly with the scale are attributed to random    uctuation, while persistent features
can be identi   ed with actual structures.

22

5.2. geodesically convex neighborhoods,   cech complexes, and neural
codes. as in the previous subsection, we consider simplicial complexes n   (c,   )
obtained as the   cech complex of the collection u  (c) of the geodesically convex balls
uk(  , t ) of diameter    around the points pk(t )     pm       1, for k = 1, . . . , n (t ), the
number of contexts in the text t and for t varying in a given corpus c. their
geometric realizations are denoted, as above, by n (c,   ) = |n   (c,   )|.

following the approach of [cuitvcyo13], we associate a code c = c(c,   ) to the
collection u  (c) of geodesically convex balls. this is a code c     {0, 1}m, where
m = pt    c n (t ). here we assume chosen an ordering of the texts t     c, with

n = #c, so that we identify the set of contexts

{c1(t1), . . . , cn(t1)(t1), . . . , c1(tn), . . . , cn(tn)(tn)}

of the entire corpus c with the set {1, . . . , m}. the code words w     c are those
elements w     {0, 1}m such that

   
    \i   supp(w)

uki (  , ti)   
   

\   
    [j /   supp(w)

ukj (  , tj)   
   

6=    ,

where supp(w) = {i     {1, . . . , m} : wi = 1}.

according to the    nerve theorem    ([ha02], corollary 4g.3), as discussed in
[cuitvcyo13] and [ma15], the homotopy type of the space yc,   =    k,t uk(  , t )
is equal to the homotopy type of the nerve n (c,   ) of the complex n   (c,   ).
in
particular, the persistent homology of yc,   is the same as the persistent homology
of n (c,   ).

this is the setting used in [cuitvcyo13] to reconstruct information about the
topology of the stimulus space from knowledge of the associated neural code. neu-
ral codes and the problem of how they encode the structure of the stimulus space
have been studied extensively in neuroscience, especially in relation to vision (see
[cuitvcyo13] and references therein). the study of neural codes in the linguistic
setting is presently less extensive: neural codes for syntax, based on data of neuro-
surgical procedures, have been studied (see [biksza09]). a detailed criticism of a
possible linguistic approach to neurosemantics is given for instance in [eli05], while
a proposal for semantic representation of linguistic data via shared neural codes
(for auditory, visual or somatosensory inputs) is analyzed in [poe06].

23

we argue for a proposal of the simplicial complexes n   (c,   ) and their persistent
homotopy type as possible computational models of neural codes for neuroseman-
tics, at least up to homotopy. namely, instead of the usual approach to measuring
semantic relatedness of texts on the basis of angular distances of semantic vec-
tors, one can consider topological notions of relatedness and proximity, in terms of
deformability and homotopy equivalence of the complexes n   (c,   ).

6. spectral decompositions and riccati    ows

6.1. singular value decomposition. typically, the word   document seman-
tic matrices discussed above are very sparse, with often only a small percentage of
entries being non   zero. it is known that this creates problems in measuring seman-
tic similarity with the usual cosine method (see [tupa10]), as the method easily
assigns zero to non co   occurring words even though they are semantically related.

in order to circumvent this problem, one can perform a dimensional reduction
based on a singular value decomposition (svd). this represents the semantic matrix
p as a product u   v    , where u and v are, respectively, and m    m and an n    n
unitary matrix and    is an n    m matrix with the singular values on the diagonal,
of rank r equal to the rank of the original semantic matrix.

6.2. latent semantics. the technique known as    latent semantics    (see
section 4.3 of [tupa10]) then considers truncations of the matrix u   v    to a rank
k < r approximation uk  kv   
k obtained by considering only the k largest singular
values. this has the e   ect of creating a low-dimensional linear mapping between
words and contexts, which reduces noise and improves the estimates of semantic
similarity, or    discover latent meaning    in the terminology used in vector space
semantics.

thus, according to this procedure, the process of analyzing semantic relatedness
based on the given word-context semantic matrix, involves a singular value decom-
position and a truncation according to the largest singular values. we will see in
the rest of this section that these operations also have a very natural geometric
interpretation in terms of the geometry of projective spaces and grassmannians.

6.3. term co-occurrence matrix.

in order to obtain the singular value
decomposition and restrict to the largest k singular values, one considers the sym-
metric matrix a = p    p , the term co-occurrence matrix, and its spectral decompo-
sition. the truncations discussed above can then be obtained by applying power

24

methods to separate out the span of the eigenvectors of the largest k eigenvalues
of a = p    p from the complementary space. for further discussions of    semantic
spectrum    and    eigenword    decomposition see for instance [dhfu15], [wida11].

6.4. perron   frobenius and riccati equation. if there is only one top eigen-
value one can apply the usual perron   frobenius theory. let sp(a) = {  1, . . . ,   n }
with |  1| > |  2|                  |  n |. in the case we considered in the previous sections
where n     m and the rank is n , the matrix a determines an action on pn    1,
and the sequence of points xm = amx0, for an assigned initial point x0     pn    1,
converges to the point in pn    1 corresponding to the line spanned by the perron   
frobenius eigenvector of a. moreover, as discussed in [amma86], [maam92], in a
local chart corresponding to vectors with    rst component equal to one, we have

a : xm = (cid:18) 1

ym(cid:19) 7    xm+1 = axm = (cid:18) 1

ym+1(cid:19) ,

with

where

ym+1 =

a3 + a4ym
a1 + a2ym

a = (cid:18) a1 a2

a3 a4(cid:19) ,

where a4 is an (n     1)    (n     1)-matrix and a1 a number. the recursion relation
of the sequence ym is then given by

ym+1     ym = (a3 + a4ym     yma1     yma2ym)(a1 + a2ym)   1.

the above can be viewed as a discretization of the matrix riccati equation

d
dt

y(t) = a3 + a4y(t)     y(t)a1     y(t)a2y(t),

in particular, both equations have the same stationary solutions given by solutions
to

a3 + a4y     ya1     ya2y = 0.

thus, in order to    nd the limit x = limm xm, or equivalently the stationary solution
ym+1 = ym of the di   erence equation above, one can consider the riccati    ow to the

25

same    xed point. for this reformulation of the perron   frobenius theory in terms
of a matrix riccati equation in a projective space, see [amma86], [maam92].

6.5. latent semantics and    ows on grassmannians. in a similar way, it
is shown in [amma86], [maam92] that the selection of the span of the eigenvectors
of the k largest eigenvalues of the matrix a = p    p can be performed dynamically
in terms of a riccati    ow on the grassmannian g(k, n ). more precisely, for a given
k-dimensional vector space v     g(k, n ) and a matrix a     gln , we have av    
g(k, n ) given by av = {av : v     v }. thus, given an initial point v0     g(k, n )
one can consider the power sequence vm+1 = avm. if spec(a) = {  1, . . . ,   n }
with

|  1|     |  2|     |  k| > |  k=1|              |  n |,

and u is the span of the eigenvectors corresponding to   i with i = 1, . . . , k, then
the sequence of points vm in g(k, n ) converge to the point corresponding to the
space u , for every choice of initial v0 with v0     w = {0}, where w is the span of
the eigenvectors with eigenvalues   i with i = k + 1, . . . , n .

for a choice of complementary subspaces u     g(k, n ) and w     g(n     k, n ),
and a morphism l     hom(u, w ), consider the element ul     g(k, n ) given by the
subspace

ul = {(cid:18) u

lu(cid:19) | u     u }     u     w.

if the matrix a in the decomposition u     w has the form

a = (cid:18) a1 a2
a3 a4(cid:19)

then

aul = u(a3+a4l)(a1+a2l)   1

in this local chart on the grassmannian g(k, n ). thus, one obtains a corresponding
sequence

lm+1 = (a3 + a4lm)(a1 + a2lm)   1

which can be written as a di   erence equation

lm+1     lm = (a3 + a4lm     lma1     lma2lm)(a1 + a2lm)   1.

26

as before, the stationary solutions can be equivalently obtained as the stationary
points of the matrix riccati    ow

d
dt

l(t) = a3 + a4l(t)     l(t)a1     l(t)a2l(t).

this shows that the latent semantics method based on singular value decomposition
and truncation to the top k singular values for p can be reformulated in terms of
a geometric    ow on a grassmannian.

7. relation to g  ardenfors       meeting of minds   

7.1. where the minds meet. in [g  a14], g  ardenfors developed an approach to
semantic spaces based on the metaphor    meeting of minds    (see [wag  a13]) and on
models of    conceptual spaces    developed in [g  a00]. the main idea is that meaning
is emergent in communication (see section 5.1 of [g  a14]). typically, coming to
a common understanding of meaning in communication is seen as a    xed point
problem taking place in a convex space which describes some con   guration domain,
such as colors, some kind of actions, etc. communication is modeled in terms of
a partitioning of this domain determined by the transmitter and a sample set of
points in the domain obtained by the received, and the common understanding is
achieved by the construction of a voronoi partition common to both sets of points,
see section 5.4.1 of [g  a14].

in our setting, the geometry of semantic spaces is not dictated by conceptual
spaces determined by preassigned external semantic categories as in [g  a14], but
rather the geometry of an ambient space (a grassmannian, or a set of paths in a
projective space) built out of corpora of texts and the frequencies of occurrences of
lexemes in contexts of these texts. however, we can still develop an approach to
communication as a    xed point problem leading to a common semantic interpreta-
tion between di   erent users, which resembles, in a di   erent geometric setting, the
   meeting of minds    approach of g  ardenfors.

consider a set a of di   erent users. all users have access to the same dictionary
d of lexemes, while each user        a has access to a certain corpus of texts c  , and
derives semantic information from the analysis of occurrences of the words of d in
the contexts of the texts t     c  . thus, each user        a obtains a matrix p  (t ) of
semantic vectors for each text t     c  . assuming each user has analyzed the entire
corpus c  , and used the information available in all texts t     c   to obtain semantic

27

information, we obtain, for each lexeme wk     d and for each user        a, a semantic
vector p  ,k = (p  ,ki), where the index i ranges over all the contexts ci(t ) of all the
texts t , listed in a given order in the corpus c  . we can view all these semantic
vectors p  ,k inside a larger vector space that corresponds to the union c =      c  ,
where we add zero entries to the vector p  ,k whenever a certain text t in some
corpus c   is not also contained in c  . in this way, for a given lexeme wk     d, the
di   erent users arrive at somewhat di   erent semantic interpretations, depending on
the di   erent texts they had access to. this di   erence is measured by the di   erent
position of the vectors p  ,k in this ambient space. in a similar way, if we consider
the entire dictionary, or just some subset of lexemes, we obtain for each user a
di   erent semantic matrix p  , computed as above over all texts t in c =      c  . as
before, we regard these matrices as points p   in a grassmannian gr(m, n ), where
m is the number of lexemes considered and n is the overall number of contexts in
all the texts in the entire union c of corpora. here we typically are in the situation
were we are seeking a common semantic understanding of a small number of lexemes
using a large number of context and corpora, hence m < n .

given this    nite collection {p  }     a of points in a grassmannian gr(m, n ),
which represents the di   erent positions in semantic space the di   erent users ar-
rived at by analyzing the occurrence of the same list of lexemes in the corpora
available to them, we need a simple geometric procedure that arrives to a com-
mon position in semantic space and that can be implemented interactively as a
sequence of approximations. a simple such procedure consists of taking the geo-
desic barycenter of the set {p  }     a. in fact, more generally one can considered a
weighted distribution of the points p  , where each p   is assigned a weight          0

with p        = 1. the additional information contained in the weights      can be

some a priori knowledge of the higher reliability or relevance of some corpora c  
with respect to others, which would make the semantic matrix p   obtained by some
user more reliable than that obtained by some other user. given the set {p  } in
gr(m, n ) and the respective weights     , the barycenter pb is determined by the
condition

x  

      2(p  , pb) = min

      2(p  , p)},

p   gr(m,n)

{x  

where   (x, y) is the geodesic distance. assuming that all the points p   lie su   ciently
close to each other (as would be the case if there is enough overlap between the cor-
pora available to di   erent users) so that they are contained in a single geodesically

28

convex neighborhood u     gr(m, n ), the potential function

v (p) = x  

      2(p  , p)

is a strictly convex function on the neighborhood u and has therefore a unique
minimum. the barycenter is then the point pb where v (p) achieves its minimum.
it can be also described as the unique    xed point of the map p 7    p     h    v (p),
where    v = g(dv,   ), g being the riemannian metric tensor, and h is a    nite
increment in a discretized id119. recursively, pb is then approximated
by pk+1 = pk     h    v (pk).

in a similar way, one can consider simplicial vietoris   rips complexes n   (c  ,   )
obtained by di   erent users based on di   erent corpora c  . after again considering
them inside a larger common projective space, one can construct a new complex
which is their common barycentric subdivision. the homotopy type and persistent
homology of the resulting complex can then be treated as a model of the    meeting
of minds    in our setting.

8. semantic vectors, zipf    s law, and kolmogorov complexity

8.1. zipf    s law. as observed in [lowe01], constructions of semantic spaces
based on semantic vectors should take into consideration the fact that the distribu-
tion of linguistic data is skewed towards high count data, according to the empirical
zipf   s law.

given a corpus of texts c and a word w (in the sense of a lexeme from a dic-
tionary of words), let fc(w) denote the number of tokens of the given word that
appear in the corpus, and fc(w)/n the relative frequencies, where n = #c is the
size of the corpus. let {wk} be an enumeration of the dictionary words by decreas-
ing frequencies. then zipf   s law states that log(fc(wk)) =   (n )     b log(k), for
a constant   (n ) depending on the corpus size and with the power law b satisfy-
ing b     1. it was shown in [ma13] that if one postulates that, in zipf   s original
explanation as   minimization of e   ort   , the word    e   ort    means kolmogorov   s com-
plexity, then zipf   s law with exponent 1 becomes a consequence of properties of the
related universal levin id203 distribution.

in the construction of semantic spaces, when one counts co   occurrences of words
in given contexts with certain given vocabulary lexemes, one encounters a situation

29

where low frequency words may be more signi   cant for semantic association, but
produce very sparse semantic matrices, while high frequency words provide more
reliable statistics, but are less signi   cant in determining semantic association, as
they tend to appear in almost every context. semantic vectors based on low fre-
quency words will have high variance, and zipf   s law predicts that the amount of
additional data required in order to reduce the variability is expressed by a power
law relation.

8.2. latent semantic analysis. in latent semantic analysis this phenomenon
is accounted for by introducing weights assigned to the vocabulary entries, so that
the estimated id203 (frequency) pc(w,    ) of co-occurrence of a given word w
with a given lexeme     in a context c is weighted by s(   )   1 log(1 + pc(w,    )), where
the denominator is given by the id178 s(   ) =    pc   c(t ) pc(   ) log(pc(   )), with

pc(   ) the id203 of occurrence of     in the context c. in this way, if     is equally
distributed in all contexts, as one expects for the most frequent words, the id178
is maximal and the weighted co-occurrence is less signi   cant, while if     is likely
to occur only in a smaller number of contexts the co-occurrence is weighted more,
as more semantically signi   cant. other methods that can be used for taking into
account the e   ects due to zipf   s law and the di   erent semantic signi   cance of words
with di   erent frequencies are surveyed in [lowe01].

this type of considerations based on zipf   s law apply to any suitable construc-
tion of semantic spaces, including the geometric construction we discussed in the
previous sections. in particular, one can similarly consider introducing appropriate
weights in the construction of the semantic matrix p (t ) of a text, that we discussed
before, so that, in addition to counting occurrences in contexts c     c(t ), one also
keeps into account how uniform or non   uniform the distribution over contexts is,
measured in terms of the shannon id178 of the resulting id203 distribution.

references

[algo95] h. alt, m. godau, computing the fr  echet distance between two polyg-

onal curves, int. j. comput. geom. appl. vol.5 (1995) 75   91.

[amma86] g. ammar, c. martin, the geometry of matrix eigenvalue methods,

acta appl. math. vol.5 (1986) n.3, 239   278.

[ar01] e. arrondo, projections of grassmannians of lines and characterization

of veronese varieties, j. alg. geom., vol.1 (2001) 165   192.

30

[arra05] e. arrondo, r. paoletti, characterization of veronese varieties via
projections in grassmannians, in    projective varieties with unexpected proper-
ties: a volume in memory of giuseppe veronese    (c. ciliberto, a.v. geramita,
b. harbourne, r.m. mir  o-roig, k. ranestad eds.) walter de gruyter, 2005.

[biksza09] d. bickerton, e. szathm  ary, biological foundations and origin of

syntax, mit press, 2009.

[buhi06] a. budanitsky, g. hirst, evaluating id138-based measures of se-

mantic distance, computational linguistics, vol.32 (2006) n.1, 13   47.

[ca09] g. carlsson. topology and data. bull. ams, vol. 46 (2009), no. 2,

255   308.

[chiabrp90] c. chiarello, c. burgess, l. richards, a. pollock, semantic and
associative priming in the cerebral hemispheres: some words do, some words don?t
. . . sometimes, some places, brain and language, vol.38 (1990) 7   104.

[cuit08] c. curto, v. itskov. cell groups reveal structure of stimulus space.
(available

plos computational biology, vol. 4, issue 10, october 2008, 13 pp.
online).

[cuitvcyo13] c. curto, v. itskov, a. veliz-cuba, n. youngs. the neural
ring: an algebraic tool for analysing the intrinsic structure of neural codes. bull.
math. biology, 75(9), pp. 1571   1611, 2013.

[dem88] j.p. demailly, vanishing theorems for tensor powers of a positive vector
bundle, in    geometry and analysis on manifolds    (katata/kyoto, 1987), pp.86   105,
lecture notes in math., vol.1339, springer, 1988.

[dhfu15] p. dhillon, d.p. foster, l.h. ungar, eigenwords: spectral word em-

beddings, journal of machine learning research 16 (2015)

[dui] d. dugger, d.c. isaksen, hypercovers in topology, preprint

http://www.math.uiuc.edu/k-theory/0528/

[eli05] c. eliasmith, neurosemantics and categories, in    handbook of catego-

rization in cognitive science   , pp.1035   1054, elsevier, 2005.

[g  a00] p. g  ardenfors. conceptual spaces: the geometry of thought. cambridge,

mass. mit press, 2000.

[g  a14] p. g  ardenfors. geometry of meaning: semantics based on conceptual

spaces. cambridge, mass. mit press, 2014, 343+xii pp.

31

[gri74] p. gri   ths, on cartan   s method of lie groups and moving frames as
applied to uniqueness and existence questions in di   erential geometry, duke math.
j., vol.41 (1974) 775   814.

[gui68] p. guiraud. the semic matrices of meaning. social science information,

7(2), 1968, pp. 131   139.

[ha02] a. hatcher. algebraic topology. cup, cambridge, 2002.
[ha13] m. hackl, the syntax-semantics interface, lingua, vol. 130 (2013) 66   87.

[inle04] p. indefrey, w. j. m. levelt. the spatial and temporal signatures of

word production components. cognition 92, 2004, pp. 101   144.

[jele94] j. d. lescheniak, w. j. m. levelt. word frequency e   ects in speech
production: retrieval of syntactic information and of phonological form. journ. of
experimental psychology: learning, memory and cognition, 20, 1994, pp. 824   843.

[li] l. lica. the distinction between which and that. with diagrams.
http://home.earthlink.net/ llica/wichthat.htm

[lowe01] w. lowe, towards a theory of semantic space, in    proceedings of the

23rd conference of the cognitive science society    (2001), pp. 576   581.

[maam92] c. martin, g. ammar, the geometry of the matrix riccati equation
and associated eigenvalue methods, in    the riccati equation   , pp.113   126, comm.
control engrg. ser., springer, 1991.

[ma13] yu. i. manin. zipf    s law and l. levin   s id203 distributions. func-
tional analysis and its applications, vol. 48, no. 2, 2014. doi 10.107/s10688-014-
0052-1. preprint arxiv:1301.0427

[ma15] yu. i. manin. neural codes and homotopy types: mathematical models of
place    eld recognition. moscow math. journal, vol. 15, oct.   dec. 2015, pp. 1   8 .
arxiv:1501.00897

[man12] d. yu. manin. the right word in the left place: measuring lexical

foregrounding in poetry and prose. www.researchgate.net

[masch99] c.d. manning, h. schuetze, foundations of statistical natural lan-

guage processing, mit press, 1999.

[me16] i. mel     cuk. language: from meaning to text.. ed. by d. beck. moscow

& boston, 2016.

[poe06] d. poeppel, language: specifying the site of modality-independent mean-

ing, current biology, vol.16 (2006) n.21, r930   r932.

32

[porghgucldmar15] a. port, i. gheorghita, d. guth, j.m clark, c. liang,

s. dasu, m. marcolli, persistent topology of syntax, arxiv:1507.05134

[pos06] a. postnikov. total positivity, grassmannians and networks, preprint

arxiv:math/0609764 [math.co].

[schpe93] h. sch  utze, j. pedersen, (1993). a vector model for syntagmatic and
paradigmatic relatedness, in    making sense of words   , pp. 104   113, oxford, 1993

[se68] g. segal, classifying spaces and spectral sequences, inst. hautes etudes

sci. publ. math. vol.34 (1968) 105   112.

[tupa10] p.d. turney, p. pantel, from frequency to meaning: vector space models

of semantics, journal of arti   cial intelligence research, vol.37 (2010) 141   188.

[va05] r.d. van valin, jr. exploring the syntax-semantics interface, cambridge

university press, 2005.

[wag  a13] m. warglien, p. g  ardenfors, semantics, conceptual spaces and the

meeting of minds, synthese, vol.190 (2013) n.12, 2165   2193.

[wida11] p. wittek, s. dar  anyi, spectral composition of semantic spaces, in

   quantum interaction   , lecture notes in computer science, vol.7052 (2011), pp. 60   
70.

[yo14] n. e. youngs. the neural ring: using algebraic geometry to analyse neural

rings. arxiv:1409.2544 [q-bio.nc], 108 pp.

