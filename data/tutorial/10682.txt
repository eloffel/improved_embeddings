coverage embedding models for id4

haitao mi baskaran sankaran zhiguo wang abe ittycheriah   

t.j. watson research center

ibm

1101 kitchawan rd, yorktown heights, ny 10598

6
1
0
2

 

g
u
a
9
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
8
4
1
3
0

.

5
0
6
1
:
v
i
x
r
a

{hmi, bsankara, zhigwang, abei}@us.ibm.com
abstract

word in future translation. this mechanism avoids
the repeating or dropping translation problems.

in this paper, we enhance the attention-based
id4 (id4) by adding
explicit coverage embedding models to alle-
viate issues of repeating and dropping trans-
lations in id4. for each source word, our
model starts with a full coverage embedding
vector to track the coverage status, and then
keeps updating it with neural networks as
the translation goes.
experiments on the
large-scale chinese-to-english task show that
our enhanced model improves the translation
quality signi   cantly on various test sets over
the strong large vocabulary id4 system.

1

introduction

id4 (id4) has gained pop-
ularity in recent years (e.g. (bahdanau et al., 2014;
jean et al., 2015; luong et al., 2015; mi et al.,
2016b; li et al., 2016)), especially for the attention-
based models of bahdanau et al. (2014). the at-
tention at each time step shows which source word
the model should focus on to predict the next tar-
get word. however, the attention in each step only
looks at the previous hidden state and the previous
target word, there is no history or coverage infor-
mation typically for each source word. as a result,
this kind of model suffers from issues of repeating
or dropping translations.

the traditional id151
(smt) systems (e.g. (koehn, 2004)) address the
above issues by employing a source side    cover-
age vector    for each sentence to indicate explicitly
which words have been translated, which parts have
not yet. a coverage vector starts with all zeros,
meaning no word has been translated. if a source
word at position j got translated, the coverage vector
sets position j as 1, and they won   t use this source

    work done while at

cheriah@google.com.

ibm. to contact abe, aitty-

however, it is not easy to adapt the    coverage vec-
tor    to id4 directly, as attentions are soft probabili-
ties, not 0 or 1. and smt approaches handle one-to-
many fertilities by using phrases or hiero rules (pre-
dict several words in one step), while id4 systems
only predict one word at each step.

in order to alleviate all those issues, we borrow
the basic idea of    coverage vector   , and introduce
a coverage embedding vector for each source word.
we keep updating those embedding vectors at each
translation step, and use those vectors to track the
coverage information.

here is a brief description of our approach. at the
beginning of translation, we start from a full cover-
age embedding vector for each source word. this
is different from the    coverage vector    in smt in
following two aspects:

    each source word has its own coverage embed-
ding vector, instead of 0 or 1, a scalar, in smt,
    we start with a full embedding vector for each
word, instead of 0 in smt.
after we predict a translation word yt at time step
t, we need to update each coverage embedding vec-
tor accordingly based on the attentions in the current
step. our motivation is that if we observe a very high
attention over xi in this step, there is a high chance
that xi and yt are translation equivalent. so the em-
bedding vector of xi should come to empty (a zero
vector) in a one-to-one translation case, or subtract
the embedding of yt for the one-to-many translation
case. an empty coverage embedding of a word xi in-
dicates this word is translated, and we can not trans-
late xi again in future. empirically, we model this
procedure by using neural networks (gated recurrent
unit (gru) (cho et al., 2014) or direct subtraction).
large-scale experiments over chinese-to-english
on various test sets show that our method improves
the translation quality signi   cantly over the large vo-
cabulary id4 system (section 5).

figure 1: the architecture of attention-based id4.the source sentence is x = (x1, ..., xl) with length l, the translation
is y    = (y   1, ..., y   m) with length m.       hi and       hi are bi-directional encoder states.   t,j is the attention id203 at time
t, position j. ht is the weighted sum of encoding states. st is a hidden state. ot is an output state. another one layer
neural network projects ot to the target output vocabulary, and conducts softmax to predict the id203 distribution
over the output vocabulary. the attention model (in right gray box) is a two layer feedforward neural network, at,j is
an intermediate state, then another layer converts it into a real number et,j, the    nal attention id203 at position j
is   t,j. we plug coverage embedding models into id4 model by adding an input ct   1,xj to at,j (the red dotted line).
2 id4

as shown in figure 1, attention-based neural ma-
chine translation (bahdanau et al., 2014) is an
encoder-decoder network. the encoder employs a bi-
directional recurrent neural network to encode the
source sentence x = (x1, ..., xl), where l is the
sentence length, into a sequence of hidden states
h = (h1, ..., hl), each hi is a concatenation of a left-
to-right       hi and a right-to-left       hi,

hi =(cid:34)      h i

      h i(cid:35) =(cid:34)      f (xi,      h i+1)
      f (xi,      h i   1)(cid:35) ,

where       f and       f are two grus.

given the encoded h, the decoder predicts the
translation by maximizing the conditional
target
translation y    =
log-id203 of the correct
(y   
1, ...y   
m), where m is the sentence length. at each
time t, the id203 of each word yt from a target
vocabulary vy is:
p(yt|h, y

(1)
where g is a two layer feed-forward neural network
(ot is a intermediate state) over the embedding of the
previous word y   
t   1, and the hidden state st. the st
is computed as:

   
1) = g(st, y

   
t   1..y

   
t   1),

st = q(st   1, y

   
t   1, ht)

(2)

(3)

where q is a gru, ht is a weighted sum of h, the
weights,   , are computed with a two layer feed-
forward neural network r:

i=1 (  t,i          h i)(cid:35) ,
ht =(cid:34)(cid:80)l
i=1 (  t,i          h i)
(cid:80)l
exp{r(st   1, hi, y   
t   1)}
(cid:80)l
k=1 exp{r(st   1, hk, y   
3 coverage embedding models

t   1)}

  t,i =

(4)

our basic idea is to introduce a coverage embedding
for each source word, and keep updating this em-
bedding at each time step. thus, the coverage em-
bedding for a sentence is a matrix, instead of a vec-
tor in smt. as different words have different fertili-
ties (one-to-one, one-to-many, or one-to-zero), sim-
ilar to id27s, each source word has its
own coverage embedding vector. for simplicity, the
number of coverage embedding vectors is the same
as the source word vocabulary size.

at the beginning of our translation, our cover-
age embedding matrix (c0,x1, c0,x2, ...c0,xl) is initial-
ized with the coverage embedding vectors of all the
source words.

then we update them with neural networks (a
gru (section 3.1.1) or a subtraction (section 3.1.2))

 st 1st   oty1      y|vy|      ht=lxi=1(   ti    hi)lxi=1(   ti   !hi)x1xl  h1  hl !hl !h1         x2 !h2  h2x1xl  h1  hl !hl !h1           hj !hjxj         y   t 1y   tstct 1,xj   t,j=exp(et,j)pli=1exp(et,i)at,jet,jet,let,1   t,1   t,2   t,l3.2 objectives
we integrate our coverage embedding models into
the attention id4 (bahdanau et al., 2014) by
adding ct   1,xj
to the    rst layer of the attention
model (shown in the red dotted line in figure 1).

hopefully, if yt is partial translation of xj with a
id203   t,j, we only remove partial information
of ct   1,xj . in this way, we enable coverage embed-
ding c0,xj to encode fertility information of xj.

as we have mentioned, in the end of translation,
we want all the coverage embedding vectors to be
close to zero. so we also minimize the absolute val-
ues of embedding matrixes as

   

  

= arg max

  

   n
t   1..y

   n
1 )

n(cid:88)n=1(cid:40) m(cid:88)t=1
l(cid:88)i=1

      

log p(y

   n
t

|xn, y
||cm,xi||(cid:41),

(6)

where    is the coef   cient of our coverage model.

as suggested by mi et al. (2016a), we can also use
some supervised alignments in our training. then,
we know exactly when each ct,xj should become
close to zero after step t. thus, we rede   ne equa-
tion 6 as:

   

  

= arg max

  

log p(y

   n
t

   n
t   1..y

   n
1 )

n(cid:88)n=1(cid:40) m(cid:88)t=1
l(cid:88)i=1

      

(

m(cid:88)j=axi

|xn, y
||cj,xi||)(cid:41),

(7)
where axi is the maximum index on the target sen-
tence xi can be aligned to.
4 related work
there are several parallel and independent related
work (tu et al., 2016; feng et al., 2016; cohn et
al., 2016). tu et al. (2016) is the most relevant one.
in their paper, they also employ a gru to model
the coverage vector. one main difference is that
our model introduces a speci   c coverage embedding
vector for each source word, in contrast, their work
initializes the word coverage vector with a scalar
with a uniform distribution. another difference lays
in the fertility part, tu et al. (2016) add an accu-
mulate operation and a fertility function to simulate

figure 2: the coverage embedding model with a gru at
time step t     1 and t. c0,1 to c0,l are initialized with the
word coverage embedding matrix

until we translation all the source words.

in the middle of translation, some coverage em-
beddings should be close to zero, which indicate
those words are covered or translated, and can not be
translated in future steps. thus, in the end of transla-
tion, the embedding matrix should be close to zero,
which means all the words are covered.

in the following part, we    rst show two updating
methods, then we list the id4 objective that takes
into account the embedding models.

3.1 updating methods
3.1.1 updating with a gru

figure 2 shows the updating method with a gru.
then, at time step t, we feed yt and   t,j to the cov-
erage model (shown in figure 2),

zt,j =   (w zyyt + w z    t,j + u zct   1,xj )
rt,j =   (w ryyt + w r    t,j + u rct   1,xj )
  ct,xj = tanh(w yt + w     t,j + rt,j     u ct   1,xj )
ct,xj = zt,j     ct   1,xj + (1     zt,j)       ct,xj ,
where, zt is the update gate, rt is the reset gate,   ct is
the new memory content, and ct is the    nal memory.
the matrix w zy, w z  , u z, w ry, w r  , u r, w y,
w    and u are shared across different position j.    
is a pointwise operation.

3.1.2 updating as subtraction

another updating method is to subtract the em-
bedding of yt directly from the coverage embedding
ct,xj with a weight   t,j as

ct,xj = ct   1,xj       t,j     (w y   cyt),

(5)
where w y   c is a matrix that coverts word embed-
ding of yt to the same size of our coverage embed-
ding vector c.

ct 1,jct 2,jyt 1ct 2,l            ct,lct 1,lct,jyt         t 1,j   t,jct,1ct 1,1ct 2,1yt 1            yt         t 1,j   t,jct 2,x1ct 2,xjct 2,xlct 1,xlct 1,xjct 1,x1ct,x1ct,xjct,xlthe process of one-to-many alignments. in our ap-
proach, we add fertility information directly to cov-
erage embeddings, as each source word has its own
embedding. the last difference is that our baseline
system (mi et al., 2016b) is an extension of the large
vocabulary id4 of jean et al. (2015) with candi-
date list decoding and unk replacement, a much
stronger baseline system.

cohn et al. (2016) augment the attention model
with well-known features in traditional smt, in-
cluding positional bias, markov conditioning, fertil-
ity and agreement over translation directions. this
work is orthogonal to our work.

5 experiments
5.1 data preparation
we run our experiments on chinese to english task.
we train our machine translation systems on two
training sets. the    rst training corpus consists of
approximately 5 million sentences available within
the darpa bolt chinese-english task. the sec-
ond training corpus adds hk law, hk hansard and
un data, the total number of training sentence pairs
is 11 million. the chinese text is segmented with
a segmenter trained on ctb data using conditional
random    elds (crf).

our development set is the concatenation of sev-
eral tuning sets (gale dev, p1r6 dev, and dev 12)
released under the darpa gale program. the de-
velopment set is 4491 sentences in total. our test
sets are nist mt06, mt08 news, and mt08 web.
for all id4 systems, the full vocabulary sizes for
thr two training sets are 300k and 500k respectively.
the coverage embedding vector size is 100. in the
training procedure, we use adadelta (zeiler, 2012)
to update model parameters with a mini-batch size
80. following mi et al. (2016b), the output vocab-
ulary for each mini-batch or sentence is a sub-set of
the full vocabulary. for each source sentence, the
sentence-level target vocabularies are union of top
2k most frequent target words and the top 10 candi-
dates of the word-to-word/phrase translation tables
learned from    fast align    (dyer et al., 2013). the
maximum length of a source phrase is 4. in the train-
ing time, we add the reference in order to make the
translation reachable.

following jean et al. (2015), we dump the align-

ments, attentions, for each sentence, and replace
unks with the word-to-word translation model or
the aligned source word.

our traditional smt system is a hybrid syntax-
based tree-to-string model (zhao and al-onaizan,
2008), a simpli   ed version of liu et al. (2009) and
cmejrek et al. (2013). we parse the chinese side
with berkeley parser, and align the bilingual sen-
tences with giza++. then we extract hiero and
tree-to-string rules on the training set. our two 5-
gram language models are trained on the english
side of the parallel corpus, and on monolingual
corpora (around 10 billion words from gigaword
(ldc2011t07)), respectively. as suggestion by
zhang (2016), id4 systems can achieve better re-
sults with the help of those monolingual corpora. we
tune our system with pro (hopkins and may, 2011)
to minimize (ter- id7)/2 on the development set.

5.2 translation results
table 1 shows the results of all systems on 5 million
training set. the traditional syntax-based system
achieves 9.45, 12.90, and 17.72 on mt06, mt08
news, and mt08 web sets respectively, and 13.36
on average in terms of (ter- id7)/2. the large-
vocabulary id4 (lvid4), our baseline, achieves
an average (ter- id7)/2 score of 15.74, which is
about 2 points worse than the hybrid system.

we test four different settings for our coverage

embedding models:

    ugru : updating with a gru;
    usub: updating as a subtraction;
    ugru + usub: combination of two methods
(do not share coverage embedding vectors);
    +obj.: ugru + usub plus an additional objec-
tive in equation 61.
ugru improves the translation quality by 1.3
points on average over lvid4. and ugru + usub
achieves the best average score of 13.14, which is
about 2.6 points better than lvid4. all the im-
provements of our coverage embedding models over
lvid4 are statistically signi   cant with the sign-
test of collins et al. (2005). we believe that we need
to explore more hyper-parameters of +obj. in order
to get even better results over ugru + usub.

1we use two   s for ugru and usub separately, and we test
  gru = 1    10   4 and   sub = 1    10   2 in our experiments.

single system

tree-to-string

lvid4
ugru
usub

ugru +usub

+obj.

s
r
u
o

mt06

mt08

news

web

bp id7 t-b
bp id7 t-b
bp id7 t-b
0.95 34.93
0.94 31.12 12.90 0.90 23.45 17.72
9.45
0.96 34.53 12.25 0.93 28.86 17.40 0.97 26.78 17.57
0.92 35.59 10.71 0.89 30.18 15.33 0.97 27.48 16.67
0.91 35.90 10.29 0.88 30.49 15.23 0.96 27.63 16.12
0.92 36.60
0.89 31.86 13.69 0.95 27.12 16.37
0.90 31.83 14.20 0.95 28.28 15.73
0.93 36.80

9.36
9.78

avg.

t-b
13.36
15.74
14.24
13.88
13.14
13.24

table 1: single system results in terms of (ter-id7)/2 (the lower the better) on 5 million chinese to english
training set. id4 results are on a large vocabulary (300k) and with unk replaced. ugru : updating with a gru;
usub: updating as a subtraction; ugru + usub: combination of two methods (do not share coverage embedding
vectors); +obj.: ugru + usub with an additional objective in equation 6, we have two   s for ugru and usub
separately, and we test   gru = 1    10   4 and   sub = 1    10   2.
single system mt06

mt08

web

news

bp t-b bp t-b bp t-b

avg.
t-b
tree-to-string 0.90 8.70 0.84 12.65 0.84 17.00 12.78
0.96 9.78 0.94 14.15 0.97 15.89 13.27
0.97 8.62 0.95 12.79 0.97 15.34 12.31

lvid4
ugru

table 2: single system results in terms of (ter-id7)/2
on 11 million set. id4 results are on a large vocabulary
(500k) and with unk replaced. due to the time limita-
tion, we only have the results of ugru system.

table 2 shows the results of 11 million sys-
tems, lvid4 achieves an average (ter-id7)/2
of 13.27, which is about 2.5 points better than 5
million lvid4. the result of our ugru cover-
age model gives almost 1 point gain over lvid4.
those results suggest that the more training data we
use, the stronger the baseline system becomes, and
the harder to get improvements. in order to get a rea-
sonable or strong id4 system, we have to conduct
experiments over a large-scale training set.

5.3 alignment results
table 3 shows the f1 scores on the alignment test set
(447 hand aligned sentences). the maxent model
is trained on 67k hand-aligned data, and achieves
an f1 score of 75.96. for id4 systems, we dump
alignment matrixes, then, for each target word we
only add the highest id203 link if it is higher
than 0.2. results show that our best coverage model,
ugru + usub, improves the f1 score by 2.2 points
over the sorce of lvid4.

we also check the repetition statistics of id4

system
maxent
lvid4
ugru
usub

s
r
u
o

ugru +usub

+obj.

pre.
74.86
47.88
51.11
49.07
49.46
49.78

rec.
77.10
41.06
41.42
42.49
43.83
41.73

f1
75.96
44.21
45.76
45.55
46.47
45.40

table 3: alignment f1 scores of different models.

outputs. we simply compute the number of repeated
phrases (length longer or equal than 4 words) for
each sentence. on mt06 test set, the 5 million
lvid4 has 209 repeated phrases, our ugru sys-
tem reduces it signi   cantly to 79, ugru +usub and
+obj. only have 50 and 47 repeated phrases, re-
spectively. the 11 million lvid4 gets 115 re-
peated phrases, and ugru reduces it further down to
16. those trends hold across other test sets. those
statistics show that a larger training set or coverage
embedding models alleviate the repeating problem
in id4.

6 conclusion

in this paper, we propose simple, yet effective, cov-
erage embedding models for attention-based id4.
our model learns a special coverage embedding vec-
tor for each source word to start with, and keeps up-
dating those coverage embeddings with neural net-
works as the translation goes. experiments on the
large-scale chinese-to-english task show signi   cant
improvements over the strong lvid4 system.

acknowledgment
we thank the anonymous reviewers for their useful
comments.

references
d. bahdanau, k. cho, and y. bengio. 2014. neural
machine translation by jointly learning to align and
translate. arxiv e-prints, september.

kyunghyun cho, bart van merrienboer, dzmitry bah-
danau, and yoshua bengio. 2014. on the properties
of id4: encoder-decoder ap-
proaches. corr, abs/1409.1259.

martin cmejrek, haitao mi, and bowen zhou. 2013.
flexible and ef   cient hypergraph interactions for joint
hierarchical and forest-to-string decoding. in proceed-
ings of the 2013 conference on empirical methods in
natural language processing, pages 545   555, seat-
tle, washington, usa, october. association for com-
putational linguistics.

t. cohn, c. d. v. hoang, e. vymolova, k. yao, c. dyer,
and g. haffari. 2016. incorporating structural align-
ment biases into an attentional neural translation
model. arxiv e-prints, january.

michael collins, philipp koehn, and ivona kucerova.
2005. clause restructuring for statistical machine
in proceedings of acl, pages 531   540,
translation.
ann arbor, michigan, june.

chris dyer, victor chahuneau, and noah a. smith.
2013. a simple, fast, and effective reparameterization
of ibm model 2. in proceedings of the 2013 confer-
ence of the north american chapter of the associa-
tion for computational linguistics: human language
technologies, pages 644   648, atlanta, georgia, june.
association for computational linguistics.
s. feng, s. liu, m. li, and m. zhou. 2016.

implicit
distortion and fertility models for attention-based
encoder-decoder id4 model. arxiv e-prints, jan-
uary.

mark hopkins and jonathan may. 2011. tuning as rank-

ing. in proceedings of emnlp.

s  ebastien jean, kyunghyun cho, roland memisevic, and
yoshua bengio. 2015. on using very large target vo-
cabulary for id4. in proceed-
ings of acl, pages 1   10, beijing, china, july.

philipp koehn. 2004. pharaoh: a id125 decoder
for phrase-based id151 mod-
els. in proceedings of amta, pages 115   124.

xiaoqing li, jiajun zhang, and chengqing zong. 2016.
towards zero unknown word in neural machine trans-
in proceedings of ijcai 2016, pages 2852   
lation.
2858, new york, ny, usa, july.

yang liu, haitao mi, yang feng, and qun liu. 2009.
joint decoding with multiple translation models.
in
proceedings of the joint conference of the 47th an-
nual meeting of the acl and the 4th international
joint conference on natural language processing
of
the afnlp: volume 2 - volume 2, acl    09,
pages 576   584, stroudsburg, pa, usa. association
for computational linguistics.

thang luong, hieu pham, and christopher d. manning.
2015. effective approaches to attention-based neu-
in proceedings of the 2015
ral machine translation.
conference on empirical methods in natural lan-
guage processing, pages 1412   1421, lisbon, portu-
gal, september. association for computational lin-
guistics.

haitao mi, zhiguo wang, and abe ittycheriah. 2016a.
supervised attentions for id4.
in proceedings of emnlp, austin, usa, november.
haitao mi, zhiguo wang, and abe ittycheriah. 2016b.
vocabulary manipulation for neural machine transla-
in proceedings of acl, berlin, germany, au-
tion.
gust.

z. tu, z. lu, y. liu, x. liu, and h. li. 2016. coverage-
based id4. arxiv e-prints,
january.

matthew d. zeiler. 2012. adadelta: an adaptive

learning rate method. corr.

jiajun zhang. 2016. exploiting source-side monolingual
data in id4. in proceedings of
emnlp 2016, austin, texas, usa, november.

bing zhao and yaser al-onaizan. 2008. generalizing lo-
cal and non-local word-reordering patterns for syntax-
based machine translation. in proceedings of the con-
ference on empirical methods in natural language
processing, emnlp    08, pages 572   581, strouds-
burg, pa, usa. association for computational lin-
guistics.

