vocabulary manipulation for id4   

haitao mi

zhiguo wang

abe ittycheriah

t.j. watson research center

ibm

6
1
0
2

 

y
a
m
0
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
9
0
2
3
0

.

5
0
6
1
:
v
i
x
r
a

{hmi, zhigwang, abei}@us.ibm.com

abstract

in order to capture rich language phenom-
ena, id4 models
have to use a large vocabulary size, which
requires high computing time and large
memory usage. in this paper, we alleviate
this issue by introducing a sentence-level
or batch-level vocabulary, which is only a
very small sub-set of the full output vocab-
ulary. for each sentence or batch, we only
predict the target words in its sentence-
level or batch-level vocabulary.
thus,
we reduce both the computing time and
the memory usage. our method simply
takes into account the translation options
of each word or phrase in the source sen-
tence, and picks a very small target vocab-
ulary for each sentence based on a word-
to-word translation model or a bilingual
phrase library learned from a traditional
machine translation model. experimen-
tal results on the large-scale english-to-
french task show that our method achieves
better translation performance by 1 id7
point over the large vocabulary neural ma-
chine translation system of jean et al.
(2015).

1

introduction

id4 (id4) (bahdanau et
al., 2014) has gained popularity in recent two
years. but it can only handle a small vocabulary
size due to the computational complexity. in or-
der to capture rich language phenomena and have
a better word coverage, id4
models have to use a large vocabulary.

jean et al. (2015) alleviated the large vocabu-
lary issue by proposing an approach that partitions
the training corpus and de   nes a subset of the full
target vocabulary for each partition. thus, they
only use a subset vocabulary for each partition in

   accepted as a short paper in acl 2016.

the training procedure without increasing compu-
tational complexity. however, there are still some
drawbacks of jean et al. (2015)   s method. first,
the importance sampling is simply based on the
sequence of training sentences, which is not lin-
guistically motivated, thus, translation ambiguity
may not be captured in the training. second, the
target vocabulary for each training batch is    xed
in the whole training procedure. third, the target
vocabulary size for each batch during training still
needs to be as large as 30k, so the computing time
is still high.

in this paper, we alleviate the above issues by
introducing a sentence-level vocabulary, which is
very small compared with the full target vocab-
ulary.
in order to capture the translation am-
biguity, we generate those sentence-level vocab-
ularies by utilizing word-to-word and phrase-to-
phrase translation models which are learned from
a traditional phrase-based machine translation sys-
tem (smt). another motivation of this work is to
combine the merits of both traditional smt and
id4, since training an id4 system usually takes
several weeks, while the word alignment and rule
extraction for smt are much faster (can be done
in one day). thus, for each training sentence,
we build a separate target vocabulary which is the
union of following three parts:

    target vocabularies of word and phrase trans-
lations that can be applied to the current sen-
tence. (to capture the translation ambiguity)
    top 2k most frequent target words. (to cover
the unaligned target words)
    target words in the reference of the current
sentence. (to make the reference reachable)
as we use mini-batch in the training procedure,
we merge the target vocabularies of all the sen-
tences in each batch, and update only those re-
lated parameters for each batch. in addition, we
also shuf   e the training sentences at the begin-
ning of each epoch, so the target vocabulary for
a speci   c sentence varies in each epoch.
in the
id125 for the development or test set, we

the id203 of each word yt from a target vo-
cabulary vy is:

p(yt|h, y

   
t   1..y

   
1)     exp(g(st, y

   
t   1, ht)),

(1)

where g is a multi layer feed-forward neural net-
work, which takes the embedding of the previous
word y   
t   1, the hidden state st, and the context
state ht as input. the output layer of g is a tar-
get vocabulary vo, yt     vo in the training pro-
cedure. vo is originally de   ned as the full target
vocabulary vy (cho et al., 2014). we apply the
softmax function over the output layer, and get the
id203 of p(yt|h, y   
1). in section 3, we
differentiate vo from vy by adding a separate and
sentence-dependent vo for each source sentence.
in this way, we enable to maintain a large vy, and
use a small vo for each sentence.

t   1..y   

the st is computed as:

(2)

(3)

(4)

st = q(st   1, y

   
t   1, ct)
i=1 (  ti          h i)(cid:35) ,
ct =(cid:34)(cid:80)l
i=1 (  ti          h i)
(cid:80)l

where q is a gru, ct is a weighted sum of h, the
weights,   , are computed with a feed-forward neu-
ral network r:

  ti =

exp{r(st   1, hi, y   
t   1)}
(cid:80)l
k=1 exp{r(st   1, hk, y   

t   1)}

3 target vocabulary

the output of function g is the id203 distri-
bution over the target vocabulary vo. as vo is de-
   ned as vy in cho et al. (2014), the softmax func-
tion over vo requires to compute all the scores for
all words in vo, and results in a high computing
complexity. thus, bahdanau et al. (2014) only
uses top 30k most frequent words for both vo and
vy, and replaces all other words as unknown words
(unk).

3.1 target vocabulary manipulation
in this section, we aim to use a large vocabulary
of vy (e.g. 500k, to have a better word cover-
age), and, at the same, to reduce the size of vo
as small as possible (in order to reduce the com-
puting time). our basic idea is to maintain a sep-
arate and small vocabulary vo for each sentence
so that we only need to compute the id203
distribution of g over a small vocabulary for each

figure 1: the attention-based id4 architecture.
      hi and       hi are bi-directional encoder states.   tj is
the attention prob at time t, position j. ht is the
weighted sum of encoding states. st is the hidden
state. ot is an intermediate output state. a single
feedforward layer projects ot to a target vocabu-
lary vo, and applies softmax to predict the proba-
bility distribution over the output vocabulary.

apply the similar procedure for each source sen-
tence, except the third bullet (as we do not have
the reference) and mini-batch parts. experimen-
tal results on large-scale english-to-french task
(section 5) show that our method achieves signi   -
cant improvements over the large vocabulary neu-
ral machine translation system.

2 id4

as shown in figure 1, id4
(bahdanau et al., 2014) is an encoder-decoder net-
work. the encoder employs a bi-directional recur-
rent neural network to encode the source sentence
x = (x1, ..., xl), where l is the sentence length,
into a sequence of hidden states h = (h1, ..., hl),
each hi is a concatenation of a left-to-right       hi and
a right-to-left       hi,

hi =(cid:34)      h i

      h i(cid:35) =(cid:34)      f (xi,      h i+1)
      f (xi,      h i   1)(cid:35) ,

where       f and       f are two id149
(gru).

given h, the decoder predicts the target transla-
tion by maximizing the conditional log-id203
of the correct translation y    = (y   
m), where
m is the length of target sentence. at each time t,

1, ...y   

    t1   tlst 1st   otht=lxi=1(   ti    hi)lxi=1(   ti   !hi)x1xl  h1  hl !hl !h1         x2 !h2  h2   t2y   t 1vosentence. thus, we introduce a sentence-level vo-
cabulary vx to be our vo, which depends on the
sentence x. in the following part, we show how
we generate the sentence-dependent vx.

the    rst objective of our method aims to cap-
ture the real translation ambiguity for each word,
and the target vocabulary of a sentence vo = vx
is supposed to cover as many as those possible
translation candidates. take the english to chi-
nese translation for example, the target vocabulary
for the english word bank should contain y    nh  ang
(a    nancial institution) and h  e`an (sloping land) in
chinese.

so we    rst use a word-to-word translation dic-
tionary to generate some target vocaularies for x.
given a dictionary d(x) = [y1, y2, ...], where x is
a source word, [y1, y2, ...] is a sorted list of candi-
date translations, we generate a target vocabulary
x for a sentence x = (x1, ..., xl) by merging all
v d
the candidates of all words x in x.

v d
x =

d(xi)

l(cid:91)i=1

as the word-to-word translation dictionary only
focuses on the source words,
it can not cover
the target unaligned functional or content words,
where the traditional phrases are designed for this
purpose. thus, in addition to the word dictio-
nary, given a word aligned training corpus, we
also extract phrases p (x1...xi) = [y1, ..., yj],
where x1...xi is a consecutive source words, and
[y1, ..., yj] is a list of target words1. for each sen-
tence x, we collect all the phrases that can be ap-
plied to sentence x, e.g. x1...xi is a sub-sequence
of sentence x.

v p
x =

(cid:91)   xi...xj   subseq(x)

p (xi...xj),

where subseq(x) is all the possible sub-sequence
of x with a length limit.

in order to cover target un-aligned functional
words, we need top n most common target words.

v t
x = t (n).

training: in our training procedure, our op-
timization objective is to maximize the log-
likelihood over the whole training set.
in order
1here we change the de   nition of a phrase in traditional
smt, where the [y1, ...yj] should also be a consecutive target
words. but our task in this paper is to get the target vocabu-
lary, so we only care about the target word set, not the order.

x , v p
x
x , we also need to include the target words

to make the reference reachable, besides v d
and v t
in the reference y,

v r

x = (cid:91)   yi   y

yi,

where x and y are a translation pair. so for each
sentence x, we have a target vocabulary vx:

vx = v d

x     v p

x     v t

x     v r

x

then, we start our mini-batch training by ran-
domly shuf   ing the training sentences before each
epoch. for simplicity, we use the union of all vx
in a batch,

vo = vb = vx1     vx2     ...vxb,

where b is the batch size. this merge gives an
advantage that vb changes dynamically in each
epoch, which leads to a better coverage of param-
eters.

decoding: different from the training, the target

vocabulary for a sentence x is
x     v p

vo = vx = v d

x     v t
x ,

and we do not use mini-batch in decoding.
4 related work
to address the large vocabulary issue in id4,
jean et al. (2015) propose a method to use differ-
ent but small sub vocabularies for different parti-
tions of the training corpus. they    rst partition
the training set. then, for each partition, they cre-
ate a sub vocabulary vp, and only predict and ap-
ply softmax over the vocabularies in vp in training
procedure. when the training moves to the next
partition, they change the sub vocabulary set ac-
cordingly.

noise-contrastive estimation (gutmann and hy-
varinen, 2010; mnih and teh, 2012; mikolov et
al., 2013; mnih and kavukcuoglu, 2013) and hi-
erarchical classes (mnih and hinton, 2009) are in-
troduced to stochastically approximate the target
word id203. but, as suggested by jean et al.
(2015), those methods are only designed to reduce
the time complexity in training, not for decoding.
5 experiments
5.1 data preparation
we run our experiments on english to french (en-
fr) task. the training corpus consists of approx-
imately 12 million sentences, which is identical

set

train

development

v p
x
73.6
73.5

10
82.1
80.0

v d
x
20
87.8
85.5

50
93.5
91.0

x

v p
x     v d
20
89.4
88.4

10
86.6
86.6

50
93.7
91.7

v p
x     v d
x     v t
x
50
10
20
92.7
96.2
94.2
91.7
92.7
94.3

dev.

train

ours

system

30k
2067

30k
6153

30k
2080

jean (2015)

sentence mini-batch sentence

table 1: the average reference coverage ratios (in word-level) on the training and development sets. we
x , and top 2k most common words for
use    xed top 10 candidates for each phrase when generating v p
x . then we check various top n (10, 20, and 50) candidates for the word-to-word dictionary for v d
x .
v t
sets. for each source sentence x, v    
x here is a
set of target word indexes (the vocabulary size is
500k, others are mapped to unk). the average
reference vocabulary size v r
x for each sentence is
23.7 on the training set (22.6 on the dev. set). the
word-to-word dictionary v d
x has a better cover-
age than phrases v p
x , and when we combine the
three sets we can get better coverage ratios. those
statistics suggest that we can not use each of them
alone due to the low reference coverage ratios.
the last three columns show three combinations,
all of which have higher than 90% coverage ratios.
as there are many combinations, training an id4
system is time consuming, and we also want to
keep the output vocabulary size small (the setting
in the last column in table 1 results in an average
11k vocabulary size for mini-batch 80), thus, in
the following part, we only run one combination
(top 10 candidates for both v p
x , and top
2k for v t
x ), where the full sentence coverage ratio
is 20.7% on the development set.

table 2: average vocabulary size for each sen-
tence or mini-batch (80 sentences). the full vo-
cabulary is 500k, all other words are unks.
to the set of jean et al. (2015) and sutskever et
al. (2014). our development set is the concatena-
tion of news-test-2012 and news-test-2013, which
has 6003 sentences in total. our test set has 3003
sentences from wmt news-test 2014. we evalu-
ate the translation quality using the case-sensitive
id7-4 metric (papineni et al., 2002) with the
multi-id7.perl script.

x and v d

same as jean et al. (2015), our full vocabu-
lary size is 500k, we use adadelta (zeiler, 2012),
and mini-batch size is 80. given the training set,
we    rst run the    fast align    (dyer et al., 2013) in
one direction, and use the translation table as our
word-to-word dictionary. then we run the reverse
direction and apply    grow-diag-   nal-and    heuris-
tics to get the alignment. the phrase table is ex-
tracted with a standard algorithm in moses (koehn
et al., 2007).

in the decoding procedure, our method is very
similar to the    candidate list    of jean et al. (2015),
except that we also use bilingual phrases and we
only include top 2k most frequent target words.
following jean et al. (2015), we dump the align-
ments for each sentence, and replace unks with
the word-to-word dictionary or the source word.

5.2 results
5.2.1 reference reachability
the reference coverage or reachability ratio is very
important when we limit the target vocabulary for
each source sentence, since we do not have the ref-
erence in the decoding time, and we do not want
to narrow the search space into a bad space. ta-
ble 1 shows the average reference coverage ratios
(in word-level) on the training and development

5.2.2 average size of vo
with the setting shown in bold column in ta-
ble 1, we list average vocabulary size of jean et al.
(2015) and ours in table 2. jean et al. (2015)    x
the vocabulary size to 30k for each sentence and
mini-batch, while our approach reduces the vocab-
ulary size to 2080 for each sentence, and 6153 for
each mini-batch. especially in the decoding time,
our vocabulary size for each sentence is about 14.5
times smaller than 30k.

5.2.3 translation results
the red solid line in figure 2 shows the learn-
ing curve of our method on the development set,
which picks at epoch 7 with a id7 score of
30.72. we also    x id27s at epoch
5, and continue several more epochs. the corre-
sponding blue dashed line suggests that there is no
signi   cant difference between them.

we also run two more experiments: v d

x     v t
x
and v p
x in train-
ing). the    nal results on the test set are 34.20

x separately (always have v r

x    v t

top n common words

id7 on dev.

avg. size of vo = v p

x     v d

x     v t

x

50

30.61
202

200
30.65
324

500
30.70
605

1000
30.70
1089

2000
30.72
2067

10000
30.69
10029

table 3: given a trained id4 model, we decode the development set with various top n most common
target words. for en-fr task, the results suggest that we can reduce the n to 50 without losing much in
terms of id7 score. the average size of vo is reduced to as small as 202, which is signi   cant lower
than 2067 (the default setting we use in our training).

single system

moses from cho et al. (2014)
candidate list
+unk replace

jean (2015)

voc. manipulation

ours

+unk replace
best from durrani et al. (2014)

test
dev.
n/a 33.30
29.32 33.36
29.98 34.11
30.15 34.45
30.72 35.11
n/a 37.03

table 4: single system results on en-fr task.

figure 2: the learning curve on the development
set. an epoch means a complete update through
the full training set.

and 34.23 separately. those results suggest that
we should use both the translation dictionary and
phrases in order to get better translation quality.

table 4 shows the single system results on en-
fr task. the standard moses in cho et al. (2014)
on the test set is 33.3. our target vocabulary ma-
nipulation achieves a id7 score of 34.45 on the
test set, and 35.11 after the unk replacement. our
approach improves the translation quality by 1.0
id7 point on the test set over the method of
jean et al. (2015). but our single system is still
about 2 points behind of the best phrase-based sys-
tem (durrani et al., 2014).

5.2.4 decoding with different top n most

common target words

another interesting question is what is the perfor-
mance if we vary the size top n most common
target words in v t
x . as the training for id4 is
time consuming, we vary the size n only in the de-

coding time. table 3 shows the id7 scores on
the development set. when we reduce the n from
2000 to 50, we only loss 0.1 points, and the av-
erage size of sentence level vo is reduced to 202,
which is signi   cant smaller than 2067 (shown in
table 2). but we should notice that we train our
id4 model in the condition of the bold column in
table 2, and only test different n in our decoding
procedure only. thus there is a mismatch between
the training and testing when n is not 2000.

5.2.5 speed

in terms of speed, as we have different code bases2
between jean et al. (2015) and us, it is hard to con-
duct an apple to apple comparison. so, for sim-
plicity, we run another experiment with our code
base, and increase vb size to 30k for each batch
(the same size in jean et al. (2015)). results show
that increasing the vb to 30k slows down the train-
ing speed by 1.5 times.

6 conclusion

in this paper, we address the large vocabulary is-
sue in id4 by proposing to
use a sentence-level target vocabulary vo, which
is much smaller than the full target vocabulary vy.
the small size of vo reduces the computing time of
the softmax function in each predict step, while the
large vocabulary of vy enable us to model rich lan-
guage phenomena. the sentence-level vocabulary
vo is generated with the traditional word-to-word
and phrase-to-phrase translation libraries. in this
way, we decrease the size of output vocabulary vo
under 3k for each sentence, and we speedup and
improve the large-vocabulary id4 system.

2two code bases share the same architecture, initial states,
and hyper-parameters. we simulate jean et al. (2015)   s work
with our code base in the both training and test procedures,
the    nal results of our simulation are 29.99 and 34.16 on dev.
and test sets respectively. those scores are very close to jean
et al. (2015).

 27 27.5 28 28.5 29 29.5 30 30.5 31 31.5 1 2 3 4 5 6 7 8 9 10 117thid7epochlearning curvefixed word-embeddingsacknowledgment
we thank the anonymous reviewers for their com-
ments.

references
d. bahdanau, k. cho, and y. bengio. 2014. neural
machine translation by jointly learning to align
and translate. arxiv e-prints, september.

kyunghyun cho, bart van merrienboer, caglar gul-
cehre, dzmitry bahdanau, fethi bougares, holger
schwenk, and yoshua bengio.
2014. learning
phrase representations using id56 encoder   decoder
for id151. in proceedings of
emnlp, pages 1724   1734, doha, qatar, october.

nadir durrani, barry haddow, philipp koehn, and
kenneth hea   eld. 2014. edinburghs phrase-based
in pro-
machine translation systems for wmt-14.
ceedings of wmt, pages 97   104, baltimore, mary-
land, usa, june.

chris dyer, victor chahuneau, and noah a. smith.
2013. a simple, fast, and effective reparameteriza-
in proceedings of the 2013
tion of ibm model 2.
conference of the north american chapter of the
association for computational linguistics: human
language technologies, pages 644   648, atlanta,
georgia, june. association for computational lin-
guistics.

michael gutmann and aapo hyvarinen. 2010. noise-
contrastive estimation: a new estimation principle
for unnormalized statistical models. in proceedings
of aistats.

s  ebastien jean, kyunghyun cho, roland memisevic,
and yoshua bengio. 2015. on using very large tar-
get vocabulary for id4.
in
proceedings of acl, pages 1   10, beijing, china,
july.

p. koehn, h. hoang, a. birch, c. callison-burch,
m. federico, n. bertoldi, b. cowan, w. shen,
c. moran, r. zens, c. dyer, o. bojar, a. constantin,
and e. herbst. 2007. moses: open source toolkit
in proceedings
for id151.
of acl.

tomas mikolov, kai chen, greg corrado, and jeffrey
dean. 2013. ef   cient estimation of word represen-
tations in vector space. in international conference
on learning representations: workshops track.

andriy mnih and geoffrey hinton. 2009. a scal-
able hierarchical distributed language model. in ad-
vances in neural information processing systems,
volume 21, pages 1081   1088.

andriy mnih and koray kavukcuoglu. 2013. learning
id27s ef   ciently with noise-contrastive
in proceedings of nips, pages 2265   
estimation.
2273.

andriy mnih and yee whye teh. 2012. a fast and
simple algorithm for training neural probabilistic
in proceedings of the 29th in-
language models.
ternational conference on machine learning, pages
1751   1758.

kishore papineni, salim roukos, todd ward, and wei-
jing zhu. 2002. id7: a method for automatic
in proceedings
evaluation of machine translation.
of acl, pages 311   318, philadephia, usa, july.

ilya sutskever, oriol vinyals, and quoc v. le. 2014.
sequence to sequence learning with neural net-
works. in proceedings of nips, pages 3104   3112,
quebec, canada, december.

matthew d. zeiler. 2012. adadelta: an adaptive

learning rate method. corr.

