gated word-character recurrent language model

6
1
0
2

 
t
c
o
3
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
0
0
7
1
0

.

6
0
6
1
:
v
i
x
r
a

yasumasa miyamoto
center for data science
new york university

yasumasa.miyamoto@nyu.edu

abstract

we introduce a recurrent neural network lan-
guage model (id56-lm) with long short-
term memory (lstm) units that utilizes both
character-level and word-level inputs. our
model has a gate that adaptively    nds the op-
timal mixture of the character-level and word-
level inputs. the gate creates the    nal vec-
tor representation of a word by combining
two distinct representations of the word. the
character-level inputs are converted into vec-
tor representations of words using a bidirec-
tional lstm. the word-level inputs are pro-
jected into another high-dimensional space by
a word lookup table. the    nal vector rep-
resentations of words are used in the lstm
language model which predicts the next word
given all the preceding words. our model
with the gating mechanism effectively utilizes
the character-level inputs for rare and out-of-
vocabulary words and outperforms word-level
language models on several english corpora.

1

introduction

recurrent neural networks (id56s) achieve state-of-
the-art performance on fundamental tasks of natural
language processing (nlp) such as language model-
ing (id56-lm) (j  zefowicz et al., 2016; zoph et al.,
2016). id56-lms are usually based on the word-
level information or subword-level information such
as characters (mikolov et al., 2012), and predictions
are made at either word level or subword level re-
spectively.

in word-level lms, the id203 distribution
over the vocabulary conditioned on preceding words

kyunghyun cho
courant institute of

mathematical sciences

& centre for data science

new york university

kyunghyun.cho@nyu.edu

is computed at the output layer using a softmax func-
tion. 1 word-level lms require a prede   ned vocab-
ulary size since the computational complexity of a
softmax function grows with respect to the vocab-
ulary size. this closed vocabulary approach tends
to ignore rare words and typos, as the words do not
appear in the vocabulary are replaced with an out-
of-vocabulary (oov) token. the words appearing
in vocabulary are indexed and associated with high-
dimensional vectors. this process is done through a
word lookup table.

although this approach brings a high degree of
freedom in learning expressions of words, infor-
mation about morphemes such as pre   x, root, and
suf   x is lost when the word is converted into an
index. also, word-level language models require
some heuristics to differentiate between the oov
words, otherwise it assigns the exactly same vector
to all the oov words. these are the major limita-
tions of word-level lms.

in order to alleviate these issues, we introduce
an id56-lm that utilizes both character-level and
word-level inputs. in particular, our model has a gate
that adaptively choose between two distinct ways to
represent each word: a word vector derived from the
character-level information and a word vector stored
in the word lookup table. this gate is trained to
make this decision based on the input word.

according to the experiments, our model with the
gate outperforms other models on the id32
(ptb), bbc, and imdb movie review datasets.
also, the trained gating values show that the gating
mechanism effectively utilizes the character-level

1softmax function is de   ned as f (xi) = exp xi

.

(cid:80)

k exp xk

information when it encounters rare words.
related work character-level language models that
make word-level prediction have recently been pro-
posed. ling et al.
(2015a) introduce the compo-
sitional character-to-word (c2w) model that takes
as input character-level representation of a word
and generates vector representation of the word us-
ing a bidirectional lstm (graves and schmidhu-
ber, 2005). kim et al. (2015) propose a convolu-
tional neural network (id98) based character-level
language model and achieve the state-of-the-art per-
plexity on the ptb dataset with a signi   cantly fewer
parameters.

moreover, word   character hybrid models have
been studied on different nlp tasks. kang et
al. (2011) apply a word   character hybrid language
model on chinese using a neural network language
model (bengio et al., 2003). santos and zadrozny
(2014) produce high performance part-of-speech
taggers using a deep neural network that learns
character-level representation of words and asso-
ciates them with usual word representations. bo-
janowski et al. (2015) investigate id56 models that
predict characters based on the character and word
level inputs. luong and manning (2016) present
word   character hybrid id4
systems that consult the character-level information
for rare words.

2 model description

the model architecture of the proposed word   
character hybrid language model is shown in fig. 1.

id27 at each time step t, both the
word lookup table and a bidirectional lstm take
the same word wt as an input. the word-level input
is projected into a high-dimensional space by a word
lookup table e     r|v |  d, where |v | is the vocabu-
lary size and d is the dimension of a word vector:

wt = e(cid:62)wwt,
xword

(1)
where wwt     r|v | is a one-hot vector whose i-th el-
ement is 1, and other elements are 0. the character   
level input is converted into a word vector by us-
ing a bidirectional lstm. the last hidden states of
forward and reverse recurrent networks are linearly

figure 1: the model architecture of the gated word-character
recurrent language model. wt is an input word at t. xword
is
wt
a word vector stored in the word lookup table. xchar
wt is a word
vector derived from the character-level input. gwt is a gating
value of a word wt.   wt+1 is a prediction made at t.

combined:

wt, hr

wt + b,

xchar
wt = wf hf

wt + wrhr

(2)
wt     rd are the last states of
where hf
the forward and the reverse lstm respectively.
wf , wr     rd  d and b     rd are trainable param-
wt     rd is the vector representation of
eters, and xchar
the word wt using a character input. the generated
and xchar
vectors xword
wt are mixed by a gate gwt as
v(cid:62)
g xword

(cid:16)

(cid:17)

wt

gwt =   
wt + bg
xwt = (1     gwt) xword

wt + gwtxchar
wt ,

(3)

where vg     rd is a weight vector, bg     r is
a bias scalar,   (  ) is a sigmoid function. this gate
value is independent of a time step. even if a word
appears in different contexts, the same gate value
is applied. hashimoto and tsuruoka (2016) apply
a very similar approach to compositional and non-
compositional phrase embeddings and achieve state-
of-the-art results on compositionality detection and
verb disambiguation tasks.
id38 the output vector xwt is used
as an input to a lstm language model. since the
id27 part is independent from the lan-
guage modeling part, our model retains the    exibil-
ity to change the architecture of the language model-
ing part. we use the architecture similar to the non-
regularized lstm model by zaremba et al. (2014).

model

gated word & char, adaptive
gated word & char, adaptive (pre-train)
gated word & char, g = 0.25
gated word & char, g = 0.25 (pre-train)
gated word & char, g = 0.5
gated word & char, g = 0.5 (pre-train)
gated word & char, g = 0.75
gated word & char, g = 0.75 (pre-train)
word only
character only
word & character
word & character (pre-train)
non-regularized lstm (zaremba, 2014)

ptb

validation

117.49
117.03
119.45
117.01
126.01
117.54
135.58
179.69
118.03
132.45
125.05
122.31
120.7

test
113.87
112.90
115.55
113.52
121.99
113.03
135.00
172.85
115.65
126.80
121.09
118.85
114.5

bbc

validation

78.56
80.37
79.67
80.07
89.27
82.09
105.54
132.96
84.47
88.03
88.77
84.27

-

test
87.16
87.51
88.04
87.99
94.91
88.61
111.47
136.01
90.90
97.71
95.44
91.24

-

imdb

validation

71.99
71.16
71.81
70.60
106.78
109.69
115.58
106.31
72.42
98.10
77.94
80.60

-

test
72.29
71.49
72.14
70.87
107.33
110.28
116.02
106.86
72.75
98.59
78.29
81.01

-

table 1: validation and test perplexities on id32 (ptb), bbc, imdb movie reviews datasets.

one step of lstm computation corresponds to

ft =    (wf xwt + uf ht   1 + bf )
it =    (wixwt + uiht   1 + bi)
  ct = tanh (w  cxwt + u  cht   1 + b  c)
ot =    (woxwt + uoht   1 + bo)
ct = ft (cid:12) ct   1 + it (cid:12)   ct
ht = ot (cid:12) tanh (ct) ,

(4)

where ws, us     rd  d and bs     rd for s    
{f, i,   c, o} are parameters of lstm cells.   (  ) is
an element-wise sigmoid function, tanh(  ) is an
element-wise hyperbolic tangent function, and (cid:12) is
an element-wise multiplication.

the hidden state ht

lowed by a softmax function:

pr (wt+1 = k|w<t+1) =

is af   ne-transformed fol-

exp(cid:0)v(cid:62)
k(cid:48) exp(cid:0)v(cid:62)
(cid:80)

(cid:1)
k(cid:48)ht + bk(cid:48)(cid:1) ,

k ht + bk

(5)
where vk is the k-th column of a parameter matrix
v     rd  |v | and bk is the k-th element of a bias
vector b     rd. in the training phase, we minimizes
the negative log-likelihood with stochastic gradient
descent.

3 experimental settings

we test    ve different model architectures on the
three english corpora. each model has a unique
id27 method, but all models share the
same lstm id38 architecture, that

has 2 lstm layers with 200 hidden units, d = 200.
except for the character only model, weights in the
id38 part are initialized with uniform
random variables between -0.1 and 0.1. weights of
a bidirectional lstm in the id27 part
are initialized with xavier initialization (glorot and
bengio, 2010). all biases are initialized to zero.

stochastic gradient decent (sgd) with mini-batch
size of 32 is used to train the models. in the    rst k
epochs, the learning rate is 1. after the k-th epoch,
the learning rate is divided by l each epoch. k man-
ages learning rate decay schedule, and l controls
speed of decay. k and l are tuned for each model
based on the validation dataset.

n

(cid:17)

(cid:80)n

i=1 log p(wi|w<i)

(cid:16)    1

as the standard metric for id38,
perplexity (ppl) is used to evaluate the model per-
formance. perplexity over the test set is computed
as ppl = exp
, where n
is the number of words in the test set, and p(wi|w<i)
is the id155 of a word wi given all
the preceding words in a sentence. we use theano
(2016) to implement all the models. the code for
the models is available from https://github.com/
nyu-dl/gated_word_char_rlm.

3.1 model variations
word only (baseline) this is a traditional word-
level language model and is a baseline model for our
experiments.
character only this is a language model where
each input word is represented as a character se-

ptb

# sentences
# word
# sentences
# word

bbc
imdb # sentences

# word

train validation
42k
888k
37k
890k
930k
21m

3k
70k
2k
49k
153k
3m

test
4k
79k
2k
53k
152k
3m

table 2: the size of each dataset.

quence similar to the c2w model in (ling et al.,
2015a). the bidirectional lstms have 200 hidden
units, and their weights are initialized with xavier
initialization. in addition, the weights of the forget,
input, and output gates are scaled by a factor of 4.
the weights in the lstm language model are also
initialized with xavier initialization. all biases are
initialized to zero. a learning rate is    xed at 0.2.
word & character this model simply concate-
nates the vector representations of a word con-
structed from the character input xchar
wt and the word
input xword
to get the    nal representation of a word
wt
xwt , i.e.,

xwt =(cid:2) xchar

wt ; xword
wt

(cid:3) .

(6)

wt

before being concatenated, the dimensions of xchar
wt
and xword
are reduced by half to keep the size of xwt
comparably to other models.
gated word & character, fixed value
this
model uses a globally constant gating value to com-
bine vector representations of a word constructed
from the character input xchar
and the word input
wt
xword
wt

as

bbc we use the bbc corpus prepared by greene
& cunningham (2006). we use 10k most frequent
words and 62 characters. in the training phase, we
use sentences with less than 50 words.
imdb movie reviews we use the imdb move
review corpus prepared by maas et al. (2011). we
use 30k most frequent words and 74 characters. in
the training phase, we use sentences with less than
50 words. in the validation and test phases, we use
sentences with less than 500 characters.

3.3 pre-training
for the word   character hybrid models, we applied
a pre-training procedure to encourage the model
to use both representations. the entire model is
trained only using the word-level input for the    rst
m epochs and only using the character-level input in
the next m epochs. in the    rst m epochs, a learn-
ing rate is    xed at 1, and a smaller learning rate
0.1 is used in the next m epochs. after the 2m-th
epoch, both the character-level and the word-level
inputs are used. we use m = 2 for ptb and bbc,
m = 1 for imdb.
lample et al.

(2016) report that a pre-trained
word lookup table improves performance of their
word & character hybrid model on named entity
recognition (ner). in their method, word embed-
dings are    rst trained using skip-id165 (ling et
al., 2015b), and then the id27s are    ne-
tuned in the main training phase.

xwt = (1     g) xword

wt + gxchar
wt ,

(7)

4 results and discussion

where g is some number between 0 and 1. we
choose g = {0.25, 0.5, 0.75}.
gated word & character, adaptive this model
uses adaptive gating values to combine vector repre-
sentations of a word constructed from the character
input xchar

wt and the word input xword
wt

as the eq (3).

3.2 datasets
id32 we use the id32 corpus
(marcus et al., 1993) preprocessed by mikolov et
(2010). we use 10k most frequent words and
al.
51 characters.
in the training phase, we use only
sentences with less than 50 words.

4.1 perplexity
table 1 compares the models on each dataset. on
the ptb and imdb movie review dataset, the gated
word & character model with a    xed gating value,
gconst = 0.25, and pre-training achieves the lowest
perplexity . on the bbc datasets, the gated word
& character model without pre-training achieves the
lowest perplexity.

even though the model with    xed gating value
performs well, choosing the gating value is not clear
and might depend on characteristics of datasets such
as size. the model with adaptive gating values does
not require tuning it and achieves similar perplexity.

(a) gated word & character.

(b) gated word & character with pre-taining.

figure 2: a log-log plot of frequency ranks and gating values trained in the gated word & character models with/without pre-
training.

4.2 values of word   character gate

the bbc and imdb datasets
retain out-of-
vocabulary (oov) words while the oov words
have been replaced by <unk> in the id32
dataset. on the bbc and imdb datasets, our model
assigns a signi   cantly high gating value on the un-
known word token unk compared to the other words.
we observe that pre-training results the different
distributions of gating values. as can be seen in
fig. 2 (a), the gating value trained in the gated word
& character model without pre-training is in general
higher for less frequent words, implying that the re-
current language model has learned to exploit the
spelling of a word when its word vector could not
have been estimated properly. fig. 2 (b) shows that
the gating value trained in the gated word & charac-
ter model with pre-training is less correlated with the
frequency ranks than the one without pre-training.
the pre-training step initializes a word lookup table
using the training corpus and includes its informa-
tion into the initial values. we hypothesize that the
recurrent language model tends to be word   input   
oriented if the informativeness of word inputs and
character inputs are not balanced especially in the
early stage of training.

although the recurrent language model with or
without pre-training derives different gating values,
the results are still similar. we conjecture that the
   exibility of modulating between word-level and
character-level representations resulted in a better
language model in multiple ways.

overall, the gating values are small. however,

this does not mean the model does not utilize the
character-level inputs. we observed that the word
vectors constructed from the character-level inputs
usually have a larger l2 norm than the word vec-
tors constructed from the word-level inputs do. for
instance, the mean values of l2 norm of the 1000
most frequent words in the imdb training set are
52.77 and 6.27 respectively. the small gate values
compensate for this difference.

5 conclusion
we introduced a recurrent neural network language
model with lstm units and a word   character gate.
our model was empirically found to utilize the
character-level input especially when the model en-
counters rare words. the experimental results sug-
gest the gate can be ef   ciently trained so that the
model can    nd a good balance between the word-
level and character-level inputs.

acknowledgments
this work is done as a part of the course ds-
ga 1010-001 independent study in data science
at the center for data science, new york univer-
sity. kc thanks the support by facebook, google
(google faculty award 2016) and nvidia (gpu
center of excellence 2015-2016). ym thanks ken-
taro hanaki, israel malkin, and tian wang for their
helpful feedback. kc and ym thanks the anony-
mous reviewers for their insightful comments and
suggestions.

references
[bengio et al.2003] yoshua bengio, r  jean ducharme,
pascal vincent, and christian janvin. 2003. a neu-
ral probabilistic language model. journal of machine
learning research, 3:1137   1155.

[bojanowski et al.2015] piotr bojanowski,

[dos santos and zadrozny2014] c  cero nogueira

armand
joulin, and tomas mikolov. 2015. alternative struc-
tures for character-level id56s. corr, abs/1511.06303.
dos
2014.
learning
for part-of-speech
in proceedings of the 31th international
icml 2014,

santos and bianca zadrozny.
character-level
representations
tagging.
conference on machine learning,
beijing, china, 21-26 june 2014, pages 1818   1826.

[glorot and bengio2010] xavier glorot and yoshua ben-
gio. 2010. understanding the dif   culty of training
deep feedforward neural networks. in proceedings of
the thirteenth international conference on arti   cial
intelligence and statistics, aistats 2010, chia la-
guna resort, sardinia, italy, may 13-15, 2010, pages
249   256.

[graves and schmidhuber2005] alex graves and j  rgen
schmidhuber. 2005. framewise phoneme classi   ca-
tion with bidirectional lstm and other neural network
architectures. neural networks, 18(5-6):602   610.

greene

[greene and cunningham2006] derek

and
padraig cunningham. 2006. practical solutions to the
problem of diagonal dominance in kernel document
id91. in machine learning, proceedings of the
twenty-third international conference (icml 2006),
pittsburgh, pennsylvania, usa, june 25-29, 2006,
pages 377   384.

[hashimoto and tsuruoka2016] kazuma hashimoto and
yoshimasa tsuruoka. 2016. adaptive joint learning
of compositional and non-compositional phrase em-
beddings. corr, abs/1603.06067.

[j  zefowicz et al.2016] rafal j  zefowicz, oriol vinyals,
mike schuster, noam shazeer, and yonghui wu.
2016. exploring the limits of id38.
corr, abs/1602.02410.

[kang et al.2011] moonyoung kang, tim ng, and long
2011. mandarin word-character hybrid-
nguyen.
in inter-
input neural network language model.
speech 2011, 12th annual conference of the in-
ternational speech communication association, flo-
rence, italy, august 27-31, 2011, pages 625   628.

[kim et al.2015] yoon kim, yacine jernite, david son-
tag, and alexander m. rush. 2015. character-aware
neural language models. corr, abs/1508.06615.

[lample et al.2016] guillaume lample, miguel balles-
teros, sandeep subramanian, kazuya kawakami, and
chris dyer. 2016. neural architectures for named en-
tity recognition. corr, abs/1603.01360.

[ling et al.2015a] wang ling, tiago lu  s, lu  s marujo,
r  mon fernandez astudillo, silvio amir, chris dyer,
alan w black, and isabel trancoso. 2015a. finding
function in form: compositional character models for
open vocabulary word representation. emnlp.

[ling et al.2015b] wang ling, yulia tsvetkov, silvio
amir, ramon fermandez, chris dyer, alan w. black,
isabel trancoso, and chu-cheng lin. 2015b. not
all contexts are created equal: better word represen-
tations with variable attention. in proceedings of the
2015 conference on empirical methods in natural
language processing, emnlp 2015, lisbon, portu-
gal, september 17-21, 2015, pages 1367   1372.
luong

and
christopher d. manning.
2016. achieving open
vocabulary id4 with hybrid
word-character models. corr, abs/1604.00788.

[luong and manning2016] minh-thang

[maas et al.2011] andrew l. maas, raymond e. daly,
peter t. pham, dan huang, andrew y. ng, and
christopher potts. 2011. learning word vectors for
in the 49th annual meeting of
id31.
the association for computational linguistics: hu-
man language technologies, proceedings of the con-
ference, 19-24 june, 2011, portland, oregon, usa,
pages 142   150.

[marcus et al.1993] mitchell p. marcus, beatrice san-
torini, and mary ann marcinkiewicz. 1993. building
a large annotated corpus of english: the penn tree-
bank. computational linguistics, 19(2):313   330.

[mikolov et al.2010] tomas mikolov, martin kara     t,
luk  s burget, jan cernock  , and sanjeev khudan-
pur. 2010. recurrent neural network based language
model. in interspeech 2010, 11th annual confer-
ence of the international speech communication as-
sociation, makuhari, chiba, japan, september 26-30,
2010, pages 1045   1048.

[mikolov et al.2012] tomas mikolov,

ilya sutskever,
anoop deoras, hai-son le, and stefan kombrink.
2012. subword id38 with neural net-
works.

[theano development team2016] theano development
team. 2016. theano: a python framework for fast
computation of mathematical expressions. arxiv e-
prints, abs/1605.02688, may.

[zaremba et al.2014] wojciech zaremba, ilya sutskever,
and oriol vinyals. 2014. recurrent neural network
id173. corr, abs/1409.2329.

[zoph et al.2016] barret zoph, ashish vaswani, jonathan
may, and kevin knight.
fast
noise-contrastive estimation for large id56 vocabularies.
naacl.

simple,

2016.

