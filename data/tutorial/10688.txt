mixing dirichlet topic models and id27s to make lda2vec

christopher moody

stitch fix one montgomery tower, suite 1200

san francisco, california 94104, usa

chrisemoody@gmail.com

6
1
0
2

 

y
a
m
6

 

 
 
]
l
c
.
s
c
[
 
 

1
v
9
1
0
2
0

.

5
0
6
1
:
v
i
x
r
a

abstract

that

distributed dense word vectors have been
shown to be effective at capturing token-
level semantic and syntactic regularities
in language, while topic models can form
interpretable representations over docu-
ments. in this work, we describe lda2vec,
a model
learns dense word vec-
tors jointly with dirichlet-distributed la-
tent document-level mixtures of topic vec-
tors. in contrast to continuous dense doc-
ument representations,
this formulation
produces sparse, interpretable document
mixtures through a non-negative simplex
constraint. our method is simple to in-
corporate into existing automatic differen-
tiation frameworks and allows for unsu-
pervised id194s geared
for use by scientists while simultaneously
learning word vectors and the linear rela-
tionships between them.

1

introduction

topic models are popular for their ability to or-
ganize document collections into a smaller set of
prominent themes. in contrast to dense distributed
representations, these document and topic repre-
sentations are generally accessible to humans and
more easily lend themselves to being interpreted.
this interpretability provides additional options to
highlight the patterns and structures within our
systems of documents. for example, using latent
dirichlet allocation (lda) topic models can re-
veal cluster of words within documents (blei et
al., 2003), highlight temporal trends (charlin et
al., 2015), and infer networks of complementary
products (mcauley et al., 2015). see blei et al.
(2010) for an overview of topic modelling in do-
mains as diverse as id161, genetic mark-
ers, survey data, and social network data.

figure 1: lda2vec builds representations over both
words and documents by mixing id97   s skip-
gram architecture with dirichlet-optimized sparse
topic mixtures.
the various components and
transformations present in the diagram are de-
scribed in the text.

0.34-0.10.1741%26%34%-1.4-0.5-1.4-1.9-1.70.750.96-0.7-1.9-0.2-1.10.6-1.2-1.2-0.2-0.7-0.4-0.7-0.3-0.3-1.90.85-0.6-0.3-0.5-2.60.45-1.3-0.6-0.8sally   said   the   fox   jumped   over   the    sally   said   the   fox   jumped   over   the#topics#topicsfox   #hidden units#topics#hidden units#hidden units#hidden unitsskip grams from sentences34%32%34%t=0word vectornegative sampling losstopic matrixdocument proportiondocument weightdocument vectorcontext vectorsparsedocument proportionsx+41%26%34%t=1099%1%0%t=   timedense vector approaches to building document
representations also exist: le and mikolov (2014)
propose paragraph vectors that are predictive of
bags of words within paragraphs, kiros et al.
(2015) build vectors that reconstruct the sentence
sequences before and after a given sentence, and
ghosh et al. (2016) construct contextual lstms
that predict proceeding sentence features. prob-
abilistic topic models tend to form documents as
a sparse mixed-membership of topics while neu-
ral network models tend to model documents as
dense vectors. by virtue of both their sparsity
and low-dimensionality, representations from the
former are simpler to inspect and more immedi-
ately yield high level intuitions about the under-
lying system (although not without hazards, see
chang et al. (2009)). this paper explores hybrid
approaches mixing sparse document representa-
tions with dense word and topic vectors.

unfortunately, crafting a new probabilistic topic
model requires deriving a new approximation, a
procedure which takes substantial expertise and
must be customized to every model. as a result,
prototypes are time-consuming to develop and
changes to model architectures must be carefully
considered. however, with modern automatic dif-
ferentiation frameworks the practitioner can focus
development time on the model design rather than
the model approximations. this expedites the pro-
cess of evaluating which model features are rele-
vant. this work takes advantage of the chainer
(tokui et al., 2015) framework to quickly develop
models while also enabling us to utilize gpus to
dramatically improve computational speed.

finally, traditional topic models over text do not
take advantage of recent advances in distributed
word representations which can capture semanti-
cally meaningful regularities between tokens. the
examination of word co-occurrences has proven
to be a fruitful research paradigm. for example,
mikolov et al. (2013) utilize skipgram negative-
sampling (sgns) to train id27s using
word-context pairs formed from windows mov-
ing across a text corpus. these vector represen-
tations ultimately encode remarkable linearities
such as king     man + woman = queen.
in
fact, levy and goldberg (2014c) demonstrate that
this is implicitly factorizing a variant of the point-
wise mutual information (pmi) matrix that em-
phasizes predicting frequent co-occurrences over
rare ones. closely related to the pmi matrix, pen-

nington et al. (2014) factorize a large global word
count co-occurrence matrix to yield more ef   cient
and slightly more performant computed embed-
dings than sgns. once created, these represen-
tations are then useful for information retrieval
(manning et al., 2009) and parsing tasks (levy
and goldberg, 2014a). in this work, we will take
advantage of word-level representations to build
document-level abstractions.

this paper extends distributed word representa-
tions by including interpretable document repre-
sentations and demonstrate that model id136
can be performed and extended within the frame-
work of automatic differentiation.
2 model
this section describes the model for lda2vec.
we are interested in modifying the skipgram
negative-sampling (sgns) objective in (mikolov
et al., 2013) to utilize document-wide feature
vectors while simultaneously learning continuous
document weights loading onto topic vectors. the
network architecture is shown in figure 1.
the total loss term l in (1) is the sum of the
skipgram negative sampling loss (sgns) lneg
with the addition of a dirichlet-likelihood term
over document weights, ld that will be discussed
later. the loss is conducted using a context vector,
(cid:126)cj, pivot word vector (cid:126)wj, target word vector (cid:126)wi,
and negatively-sampled word vector (cid:126)wl.

ij

l = ld +   ijlneg
lneg
ij = log   ((cid:126)cj    (cid:126)wi) +   n

ij

(1)
l=0 log   (   (cid:126)cj    (cid:126)wl)
(2)

in our experiments,

2.1 word representation
as in mikolov et al. (2013), pairs of pivot and
target words (j, i) are extracted when they co-
occur in a moving window scanning across the
corpus.
the window con-
tains    ve tokens before and after the pivot to-
ken. for every pivot-target pair of words the
pivot word is used to predict the nearby target
word. each word is represented with a    xed-
length dense distributed-representation vector, but
unlike mikolov et al. (2013) the same word vec-
tors are used in both the pivot and target represen-
tations. the sgns loss shown in (2) attempts to
discriminate context-word pairs that appear in the
corpus from those randomly sampled from a    neg-
ative    pool of words. this loss is minimized when

the observed words are completely separated from
the marginal distribution. the distribution from
which tokens are drawn is u  , where u denotes
the overall word frequency normalized by the to-
tal corpus size. unless stated otherwise, the neg-
ative sampling power beta is set to 3/4 and the
number of negative samples is    xed to n = 15
as in mikolov et al. (2013). note that a distribu-
tion of u0.0 would draw negative tokens from the
vocabulary with no notion of popularity while a
distribution proportional with u1.0 draws from the
empirical unigram distribution. compared to the
unigram distribution, the choice of u3/4 slightly
emphasizes choosing infrequent words for nega-
tive samples. in contrast to optimizing the softmax
cross id178, which requires modelling the over-
all popularity of each token, negative sampling fo-
cuses on learning word vectors conditional on a
context by drawing negative samples from each to-
ken   s marginal popularity in the corpus.

2.2 id194s
lda2vec embeds both words and document vectors
into the same space and trains both representations
simultaneously. by adding the pivot and docu-
ment vectors together, both spaces are effectively
joined. mikolov et al. (2013) provide the intuition
that word vectors can be summed together to form
a semantically meaningful combination of both
words. for example, the vector representation for
germany + airline is similar to the vector for
luf thansa. we would like to exploit the additive
property of word vectors to construct a meaning-
ful sum of word and document vectors. for exam-
ple, if as lda2vec is scanning a document the jth
word is germany, then neighboring words are
predicted to be similar such as f rance, spain,
and austria. but if the document is speci   cally
about airlines, then we would like to construct a
document vector similar to the word vector for
airline. then instead of predicting tokens simi-
lar to germany alone, predictions similar to both
the document and the pivot word can be made
such as: luf thansa, condor f lugdienst, and
aero lloyd. motivated by the meaningful sums
of words vectors, in lda2vec the context vector is
explicitly designed to be the sum of a document
vector and a word vector as in (3):

(cid:126)cj = (cid:126)wj + (cid:126)dj

(3)

this models document-wide relationships by
preserving (cid:126)dj for all word-context pairs in a docu-
ment, while still leveraging local inter-word rela-
tionships id30 from the interaction between
the pivot word vector (cid:126)wj and target word (cid:126)wi. the
document and word vectors are summed together
to form a context vector that intuitively captures
long- and short-term themes, respectively. in order
to prevent co-adaptation, we also perform dropout
on both the unnormalized document vector (cid:126)dj and
the pivot word vector (cid:126)wj (hinton et al., 2012).

2.2.1 document mixtures
if we only included structure up to this point, the
model would produce a dense vector for every
document. however, lda2vec strives to form in-
terpretable representations and to do so an addi-
tional constraint is imposed such that the docu-
ment representations are similar to those in tradi-
tional lda models. we aim to generate a docu-
ment vector from a mixture of topic vectors and to
do so, we begin by constraining the document vec-
tor (cid:126)dj to project onto a set of latent topic vectors
(cid:126)t0, (cid:126)t1, ..., (cid:126)tk:

(cid:126)dj = pj0  (cid:126)t0+pj1  (cid:126)t1+...+pjk   (cid:126)tk+...+pjn   (cid:126)tn (4)
each weight 0     pjk     1 is a fraction that de-
notes the membership of document j in the topic
k. for example, the twenty newsgroups model
described later has 11313 documents and n = 20
topics so j = 0...11312, k = 0...19. when the
word vector dimension is set to 300, it is assumed
that the document vectors (cid:126)dj, word vectors (cid:126)wi and
topic vectors (cid:126)tk all have dimensionality 300. note
that the topics (cid:126)tk are shared and are a common
component to all documents but whose strengths
are modulated by document weights pjk that are
unique to each document. to aid interpretabil-
ity, the document memberships are designed to be
non-negative, and to sum to unity. to achieve this
constraint, a softmax transform maps latent vec-
tors initialized in r300 onto the simplex de   ned
by pjk. the softmax transform naturally enforces
the constraint that   kpjk = 1 and allows us inter-
pret memberships as percentages rather than un-
bounded weights.

formulating the mixture in (4) as a sum en-
sures that topic vectors (cid:126)tk, document vectors (cid:126)dj
and word vectors (cid:126)wi, operate in the same space.
as a result, what words (cid:126)wi are most similar to

any given topic vector (cid:126)tk can be directly calcu-
lated. while each topic is not literally a token
present in the corpus, it   s similarity to other tokens
is meaningful and can be measured. furthermore,
by examining the list of most similar words one
can attempt to interpret what the topic represents.
for example, by calculating the most similar to-
ken to any topic vector (e.g. argmaxi((cid:126)t0    (cid:126)wi))
one may discover that the    rst topic vector (cid:126)t0 is
similar to the tokens pitching, catcher, and braves
while the second topic vector (cid:126)t1 may be similar to
jesus, god, and faith. this provides us the option
to interpret the    rst topic as baseball topic, and as
a result the    rst component in every document pro-
portion pj0 indicates how much document j is in
the baseball topic. similarly, the second topic may
be interpreted as christianity and the second com-
ponent of any document proportion pj1 indicates
the membership of that document in the christian-
ity topic.

2.2.2 sparse memberships
finally, the document weights pij are sparsi   ed by
optimizing the document weights with respect to a
dirichlet likelihood with a low concentration pa-
rameter   :

ld =     jk (       1) log pjk

(5)

the overall objective in (5) measures the like-
lihood of document j in topic k summed over all
available documents. the strength of this term is
modulated by the tuning parameter   . this simple
likelihood encourages the document proportions
coupling in each topic to be sparse when    < 1
and homogeneous when    > 1. to drive in-
terpretability, we are interested in    nding sparse
memberships and so set    = n   1 where n is
the number of topics. we also    nd that setting
the overall strength of the dirichlet optimization
to    = 200 works well. document proportions
are initialized to be relatively homogeneous, but
as time progresses, the ld encourages document
proportions vectors to become more concentrated
(e.g. sparser) over time. in experiments without
this sparsity-inducing term (or equivalently when
   = 1) the document weights pij tend to have
id203 mass spread out among all elements.
without any sparsity inducing terms the existence
of so many non-zero weights makes interpreting
the document vectors dif   cult. furthermore, we

# of topics
20
30
40
50
20
30
40
50

   topic coherences
0.567
0.555
0.553
0.547
0.563
0.564
0.552
0.558

0.75
0.75
0.75
0.75
1.00
1.00
1.00
1.00

figure 2: average topic coherences found by
lda2vec in the twenty newsgroups dataset are
given. the topic coherence has been demonstrated
to correlate with human evaluations of topic mod-
els (r  oder et al., 2015). the number of topics
chosen is given, as well as the negative sampling
exponent parameter   . compared to    = 1.00,
   = 0.75 draws more rare words as negative sam-
ples. the best topic coherences are found in mod-
els n = 20 topics and a    = 0.75.

   nd that the topic basis are also strongly affected,
and the topics become incoherent.

2.3 preprocessing and training
the objective in (1) is trained in individual mini-
batches at a time while using the adam optimizer
(kingma and ba, 2014) for two hundred epochs
across the dataset. the dirichlet likelihood term
ld is typically computed over all documents, so
in modifying the objective to minibatches we ad-
just the loss of the term to be proportional to the
minibatch size divided by the size of the total cor-
pus. our software is open source, available on-
line, documented and unit tested1. finally, the
top ten most likely words in a given topic are
submitted to the online palmetto2 topic quality
measuring tool and the coherence measure cv is
recorded. after evaluating multiple alternatives,
cv is the recommended coherence metric in r  oder
et al. (2015). this measure averages the normal-
ized pointwise mutual information (npmi) for ev-
ery pair of words within a sliding window of size
110 on an external corpus and returns mean of the
npmi for the submitted set of words. token-to-
word similarity is evaluated using the 3cosmul
measure (levy and goldberg, 2014b).

1the code for lda2vec is available online at https://

github.com/cemoody/lda2vec

2the online evaluation tool can be accessed at http://

palmetto.aksw.org/palmetto-webapp/

topic label
top tokens

topic coherence

   space   
astronomical
astronomy
satellite
planetary
telescope
0.712

   encryption   
encryption
wiretap
encrypt
escrow
clipper
0.675

   x windows   
mydisplay
xlib
window
cursor
pixmap
0.472

   middle east   
armenian
lebanese
muslim
turk
sy
0.615

figure 3: topics discovered by lda2vec in the twenty newsgroups dataset. the inferred topic label is
shown in the    rst row. the tokens with highest similarity to the topic are shown immediately below.
note that the twenty newsgroups corpus contains corresponding newsgroups such as sci.space, sci.crypt,
comp.windows.x and talk.politics.mideast.

3 experiments

3.1 twenty newsgroups

this section details experiments in discovering the
salient topics in the twenty newsgroups dataset,
a popular corpus for machine learning on text.
each document in the corpus was posted to one
of twenty possible newsgroups. while the text
of each post is available to lda2vec, each of the
newsgroup partitions is not revealed to the algo-
rithm but is nevertheless useful for post-hoc qual-
itative evaluations of the discovered topics. the
corpus is preprocessed using the data loader avail-
able in scikit-learn (pedregosa et al., 2012) and to-
kens are identi   ed using the spacy parser (honni-
bal and johnson, 2015). words are lemmatized to
group multiple in   ections into single tokens. to-
kens that occur fewer than ten times in the cor-
pus are removed, as are tokens that appear to be
urls, numbers or contain special symbols within
their orthographic forms. after preprocessing, the
dataset contains 1.8 million observations of 8,946
unique tokens in 11,313 documents. word vec-
tors are initialized to the pretrained values found
in mikolov et al. (2013) but otherwise updates are
allowed to these vectors at training time.

a range of lda2vec parameters are evaluated by
varying the number of topics n     20, 30, 40, 50
and the negative sampling exponent        0.75, 1.0.
the best topic coherences were achieved with
n = 20 topics and with negative sampling power
   = 0.75 as summarized in figure 2. we brie   y
experimented with variations on dropout ratios but
we did not observe any substantial differences.

figure 3 lists four example topics discovered in
the twenty newsgroups dataset. each topic is as-
sociated with a topic vector that lives in the same
space as the trained word vectors and listed are

the most similar words to each topic vector. the
   rst topic shown has high similarity to the tokens
astronomical, astronomy, satellite, planetary, and
telescope and is thus likely a    space   -related topic
similar to the    sci.space    newsgroup. the second
example topic is similar to words semantically re-
lated to    encryption   , such as clipper and encrypt,
and is likely related to the    sci.crypt    newsgroup.
the third and four example topics are    x win-
dows    and    middle east    which likely belong to
the    comp.windows.x    and    talk.politics.mideast   
newsgroups.

3.2 hacker news comments corpus
this section evaluates lda2vec on a very large cor-
pus of hacker news 3 comments. hacker news
is social content-voting website and community
whose focus is largely on technology and en-
trepreneurship. in this corpus, a single document
is composed of all of the words in all comments
posted to a single article. only stories with more
than 10 comments are included, and only com-
ments from users with more than 10 comments
are included. we ignore other metadata such as
votes, timestamps, and author identities. the raw
dataset 4 is available for download online. the
corpus is nearly    fty times the size of the twenty
newsgroups corpus which is suf   cient for learn-
ing a specialized vocabulary. to take advantage
of this rich corpus, we use the spacy to tokenize
whole noun phrases and entities at once (honni-
bal and johnson, 2015). the speci   c id121
procedure5 is also available online, as are the pre-

3see https://news.ycombinator.com/
4the raw dataset

is freely available at https://

zenodo.org/record/45901

5the id121 procedure is available online at
https://github.com/cemoody/lda2vec/blob/
master/lda2vec/preprocess.py

   housing issues   
more housing
basic income
new housing
house prices
short-term rentals

   internet portals   
ddg.
bing
google+
ddg
igoogle

   bitcoin   
btc
bitcoins
mt. gox
mtgox
gox

   compensation   
current salary
more equity
vesting
equity
vesting schedule thunderbolt

   gadget hardware   
the surface pro
hdmi
glossy screens
mac pro

figure 4: topics discovered by lda2vec in the hacker news comments dataset. the inferred topic label
is shown in the    rst row. we form tokens from noun phrases to capture the unique vocabulary of this
specialized corpus.

comic sans
typeface
arial
helvetica

arti   cial sweeteners black holes
glucose
fructose
hfcs
sugars
sugar
soylent
paleo diet
diet
carbohydrates

particles
consciousness
galaxies
quantum mechanics times new roman
universe
dark matter
big bang
planets
entanglement

font
new logo
anonymous pro
baskerville
serif font

functional programming san francisco
fp
haskell
oop
functional languages
monads
lisp
clojure
category theory
oo

new york
palo alto
nyc
new york city
sf
mountain view
seattle
los angeles
boston

figure 5: given an example token in the top row, the most similar words available in the hacker news
comments corpus are reported.

processed datasets 6 results. this allows us to cap-
ture phrases such as community policing measure
and prominent    gures such as steve jobs as single
tokens. however, this id121 procedure gen-
erates a vocabulary substantially different from the
one available in the palmetto topic coherence tool
and so we do not report topic coherences on this
corpus. after preprocessing, the corpus contains
75 million tokens in 66 thousand documents with
110 thousand unique tokens. unlike the twenty
newsgroups analysis, word vectors are initialized
randomly instead of using a library of pretrained
vectors.

we train an lda2vec model using 40 topics and
256 hidden units and report the learned topics that
demonstrate the themes present in the corpus. fur-
thermore, we demonstrate that word vectors and
semantic relationships speci   c to this corpus are
learned.

in figure 4    ve example topics discovered by
lda2vec in the hacker news corpus are listed.
these topics demonstrate that the major themes
of the corpus are reproduced and represented in
learned topic vectors in a similar fashion as in
lda (blei et al., 2003). the    rst, which we

6a tokenized dataset is freely available at https://

zenodo.org/record/49899

hand-label housing issues has prominent tokens
relating to housing policy issues such as hous-
ing supply (e.g. more housing), and costs (e.g.
basic income and house prices). another topic
lists major internet portals, such as the privacy-
conscious search engine    duck duck go    (in the
corpus abbreviated as ddg), as well as other ma-
jor search engines (e.g. bing), and home pages
(e.g. google+, and igoogle). a third topic is that
of the popular online curency and payment sys-
tem bitcoin, the abbreviated form of the currency
btc, and the now-defunct bitcoin trading platform
mt. gox. a fourth topic considers salaries and
compensation with tokens such as current salary,
more equity and vesting, the process by which em-
ployees secure stock from their employers. a    fth
example topic is that of technological hardware
like hdmi and glossy screens and includes de-
vices such as the surface pro and mac pro.

figure 5 demonstrates that token similarities are
learned in a similar fashion as in sgns (mikolov
et al., 2013) but specialized to the hacker news
corpus. tokens similar to the token arti   cial
sweeteners include other sugar-related tokens like
fructose and food-related tokens such as paleo
diet.
tokens similar to black holes include
physics-related concepts such as galaxies and dark

- browser +

query
california + technology
digital + currency
javascript
server
mark zuckerberg -
facebook + amazon
nlp - text + image
snowden - united states
+ sweden
surface pro - microsoft +
amazon

result
silicon valley
bitcoin
node.js

jeff bezos

id161
assange

kindle

figure 6: example linear relationships discov-
ered by lda2vec in the hacker news comments
dataset. the    rst column indicates the example
input query, and the second column indicates the
token most similar to the input.

matter. the hacker news corpus devotes a sub-
stantial quantity of text to fonts and design, and the
words most similar to comic sans are other pop-
ular fonts (e.g. times new roman and helvetica)
as well as font-related concepts such as typeface
and serif font. tokens similar to functional pro-
gramming demonstrate similarity to other com-
puter science-related tokens while tokens similar
to san francisco include other large american
cities as well smaller cities located in the san fran-
cisco bay area.

figure 6 demonstrates that in addition to learn-
ing topics over documents and similarities to word
tokens, linear regularities between tokens are also
learned. the    query    column lists a selection of
tokens that when combined yield a token vector
closest to the token shown in the    result    column.
the subtractions and additions of vectors are eval-
uated literally, but instead take advantage of the
3cosmul objective (levy and goldberg, 2014b).
the results show that relationships between tokens
important to the hacker news community exists
between the token vectors. for example, the vec-
tor for silicon valley is similar to both california
and technology, bitcoin is indeed a digital cur-
rency, node.js is a technology that enables run-
ning javascript on servers instead of on client-
side browsers, jeff bezos and mark zuckerberg are
ceos of amazon and facebook respectively, nlp
and id161 are    elds of machine learn-
ing research primarily dealing with text and im-
ages respectively, edward snowden and julian as-

sange are both whistleblowers who were primar-
ily located in the united states and sweden and
   nally the kindle and the surface pro are both
tablets manufactured by amazon and microsoft re-
spectively.
in the above examples semantic re-
lationships between tokens encode for attributes
and features including: location, currencies, server
v.s. client, leadership    gures, machine learning
   elds, political    gures, nationalities, companies
and hardware.

3.3 conclusion
this work demonstrates a simple model, lda2vec,
that extends sgns (mikolov et al., 2013) to build
unsupervised id194s that yield
coherent topics. word, topic, and document vec-
tors are jointly trained and embedded in a common
representation space that preserves semantic reg-
ularities between the learned word vectors while
still yielding sparse and interpretable document-
to-topic proportions in the style of lda (blei et
al., 2003). topics formed in the twenty news-
groups corpus yield high mean topic coherences
which have been shown to correlate with hu-
man evaluations of topics (r  oder et al., 2015).
when applied to a hacker news comments cor-
pus, lda2vec discovers the salient topics within
this community and learns linear relationships be-
tween words that allow it solve word analogies in
the specialized vocabulary of this corpus. finally,
we note that our method is simple to implement in
automatic differentiation frameworks and can lead
to more readily interpretable unsupervised repre-
sentations.

references
david m blei, andrew y ng, and michael i jordan.
2003. id44. the journal of
machine learning research, 3:993   1022, march.

david blei, lawrence carin, and david dunson. 2010.
probabilistic topic models. ieee signal processing
magazine, 27(6):55   65.

j chang, s gerrish, and c wang. 2009. reading tea
leaves: how humans interpret topic models. ad-
vances in . . . .

laurent charlin, rajesh ranganath, james mcinerney,
and david m blei. 2015. dynamic poisson factor-
ization. acm, september.

shalini ghosh, oriol vinyals, brian strope, scott roy,
2016. contextual

tom dean, and larry heck.

jeffrey pennington, richard socher, and christopher
manning. 2014. glove: global vectors for word
in proceedings of the 2014 con-
representation.
ference on empirical methods in natural language
processing (emnlp), pages 1532   1543, strouds-
burg, pa, usa. association for computational lin-
guistics.

michael r  oder, andreas both, and alexander hinneb-
urg. 2015. exploring the space of topic coherence
measures. acm, february.

s tokui, k oono, s hido, ca san mateo, and
j clayton.
2015. chainer: a next-generation
open source framework for deep learning. learn-
ingsys.org.

lstm (clstm) models for large scale nlp tasks.
arxiv.org, february.

geoffrey e hinton, nitish srivastava, alex krizhevsky,
ilya sutskever, and ruslan r salakhutdinov. 2012.
improving neural networks by preventing co-
adaptation of feature detectors. arxiv.org, july.

matthew honnibal and mark johnson. 2015. an im-
proved non-monotonic transition system for de-
in proceedings of the 2015
pendency parsing.
conference on empirical methods in natural lan-
guage processing, pages 1373   1378, stroudsburg,
pa, usa. association for computational linguis-
tics.

diederik kingma and jimmy ba. 2014. adam: a
method for stochastic optimization. arxiv.org, de-
cember.

ryan kiros, yukun zhu, ruslan r salakhutdinov,
richard zemel, raquel urtasun, antonio torralba,
and sanja fidler. 2015. skip-thought vectors. ad-
vances in neural . . . , pages 3294   3302.

quoc v le and tomas mikolov.

dis-
tributed representations of sentences and docu-
ments. icml, pages 1188   1196.

2014.

omer levy and yoav goldberg. 2014a. dependency-
in proceedings of the
based id27s.
52nd annual meeting of the association for compu-
tational linguistics (volume 2: short papers), pages
302   308, stroudsburg, pa, usa. association for
computational linguistics.

omer levy and yoav goldberg. 2014b. linguistic
regularities in sparse and explicit word represen-
tations. conll, pages 171   180.

omer levy and yoav goldberg. 2014c. neural word
embedding as implicit id105. ad-
vances in neural information processing . . . , pages
2177   2185.

christopher d manning, prabhakar raghavan, and
hinrich schutze. 2009. introduction to information
retrieval. cambridge university press, cambridge.

julian mcauley, rahul pandey, and jure leskovec.
2015. inferring networks of substitutable and com-
plementary products. acm, new york, new york,
usa, august.

tomas mikolov, ilya sutskever, kai chen 0010, gre-
gory s corrado, and jeffrey dean.
2013. dis-
tributed representations of words and phrases and
their compositionality. nips, pages 3111   3119.

fabian pedregosa, ga  el varoquaux, alexandre gram-
fort, vincent michel, bertrand thirion, olivier
grisel, mathieu blondel,
peter prettenhofer,
ron weiss, vincent dubourg, jake vanderplas,
alexandre passos, david cournapeau, matthieu
brucher, matthieu perrot, and   edouard duchesnay.
2012. scikit-learn: machine learning in python.
arxiv.org, january.

