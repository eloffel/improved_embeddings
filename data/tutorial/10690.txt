8
1
0
2

 

v
o
n
5
2

 

 
 
]

g
l
.
s
c
[
 
 

3
v
7
8
0
5
0

.

5
0
6
1
:
v
i
x
r
a

id97 is a special case of kernel

correspondence analysis and kernels for natural

language processing

hirotaka niitsuma

department of computer science

okayama university

okayama, japan

niitsuma@cs.okayama-u.ac.jp

minho lee

graduate school of science and technology

kyungpook national university

daegu, south korea

mholee@knu.ac.kr

november 27, 2018

abstract

we show that correspondence analysis (ca) is equivalent to de   ning a gini in-
dex with appropriately scaled one-hot encoding. using this relation, we introduce
a nonlinear kernel extension to ca. this extended ca gives a known analysis for
natural language via specialized kernels that use an appropriate contingency table.
we propose a semi-supervised ca, which is a special case of the kernel extension
to ca. because ca requires excessive memory if applied to numerous categories,
ca has not been used for natural language processing. we address this problem
by introducing delayed evaluation to randomized singular value decomposition.
the memory-ef   cient ca is then applied to a word-vector representation task. we
propose a tail-cut kernel, which is an extension to the skip-gram within the kernel
extension to ca. our tail-cut kernel outperforms existing word-vector representa-
tion methods.

1

introduction

principal component analysis (pca) is a form of unsupervised feature extractor. when
applied to chi-squared distances of categorical data, pca becomes correspondence
analysis (ca). ca can extract numeric vector features from categorical data with-
out supervised labeling. the simplest numerical representation for categorical data

1

is a histogram-based representation such as tf-idf or a    bag-of-words   . many appli-
cations use such simple representations. however, histogram-based representations
cannot make use of information about correlations within the data. ca enables the
representation of both histograms and correlations in data.

the most popular problem involving categorical data is natural language processing
(nlp). however, ca has not been applied to nlp because most nlp problems involve
a large number of categories. for example, the entire wikipedia text comprises more
than 10,000 different words. because ca requires excessive memory if applied to
numerous categories, ca has not been used for nlp problems involving more than
10,000 categories.

ca is implemented by singular value decomposition (svd) of a contingency table.
in many categorical problems, the contingency table is sparsely populated. random-
ized svd [6] is an appropriate svd method for sparse matrices. however, ca requires
dense matrix computation even for a sparse contingency table, which makes very large
demands on memory resources. this research aims to address this problem. we pro-
pose using a randomized svd with delayed evaluation to avoid expanding the sparse
matrix into a dense matrix. we refer to this process as the delayed sparse randomized
svd (dssvd) algorithm. we show that ca with dssvd can be applied to nlp
problems.

neural-network-based approaches are the most popular feature extractors used in
nlp. of these, id97 [10] is well known. usually, such an approach will involve
many parameters, which do not have explicit meanings in most cases. these parameters
have to be tuned by grid searching or manual parameter tuning, which is dif   cult in
the absence of explicit meanings for the parameters. this parameter problem with
neural-network-based approaches also gives rise to domain problems. for example,
if id97 is tuned for application to restaurant reviews, the tuning may not be
appropriate for movie reviews.

in most cases, the weight values used in neural networks are initialized as random
values, which means that the computed results will always be different. for example,
word-vector representations using id97 will always be different, even when the
same parameter values are used, because of random initial values. this adds to the
dif   culty of parameter tuning.

since ca is pca of a contingency table, always the same result is computed. from
this viewpoint, the ca approach is better than neural-network-based approaches. al-
though the latter have these issues, they can be used to approximate any nonlinear
function, which means that they can be used for a wide variety of problems. however,
because ca is a form of linear analysis, it is not directly applicable to nonlinear prob-
lems. to address this issue, this research introduces a nonlinear kernel extension to
ca. we can then show that this nonlinear ca approach is better in accuracy compared
with recent neural-network-based approaches. in particular, we focus on comparison
with respect to word-vector representation tasks. to distinguish linear and nonlinear
ca, we refer to the linear ca as lca.

2

2 ca

ca is a statistical visualization method for picturing the associations between the levels
of a two-way contingency table. as an illustration, consider the contingency table
shown in table 1. this is well known as    fisher   s data    [4] and represents the eye and
hair color of people in caithness, scotland. the ca of these data yields the graphical
display presented in figure 1, which shows the correspondence between eye and hair
color.

table 1 shows the joint population distribution of the categorical variable for eye

color:

xeye     { blue light medium dark}.

and the categorical variable for hair color:

xhair     { fair red medium dark black}

the visualization is based on    one-hot encodings    and the    indicator matrices    of cat-
egorical variables. for example, a one-hot encoding eeye and indicator matrix heye of
the categorical variable xeye can be de   ned as:

eeye(blue) = (1, 0, 0, 0)t
eeye(light) = (0, 1, 0, 0)t
eeye(medium) = (0, 0, 1, 0)t
eeye(dark) = (0, 0, 0, 1)t

                  

heye =

eeye(eye color of 1st person))t

...

0, 0, 1, 0

...

eeye(eye color of n-th person))t

                   .

(1)

(2)

in the following, xr and xc denote categorical variables representing the row and
column of a contingency table, respectively. er(xr(a)) and ec(xc(a)) denote one-hot
encodings of xr and xc for the a-th instance.

table 1: fisher   s data.

xeye

blue
light

fair
326
688
medium 343
98

dark

xhair

red medium dark
110
38
188
116
412
84
48
681

241
584
909
403

black

3
4
26
85

3

figure 1: visualizing fisher   s data.

3 covariance based on gini index

consider the following relation [16] about the variance of continuous data.

lemma 1. the variance of continuous data can be expressed as the sum of the differ-
ences of individual instances:

var(x) =

1
n

(x(a)       x)2

n(cid:88)
n(cid:88)

a=1

n(cid:88)

a=1

b=1

=

1
2n2

(x(a)     x(b))2

(cid:80)n

(3)

(4)

is the average

where {x(1), x(2), ..., x(n)} are continuous sample data.   x =
b=1 x(b)
value of the sample data. var(x) is the variance of the continuous data.

n

4

proof. let us expand (3) and (4).

1
2n2

(

(

a=1

a=1

a=1

a=1

a=1

1
n

1
n

1
n

(cid:33)

(x(a)       x)2

x(a)2)     n  x2

(cid:33)
(cid:33)

(x(a)     x(b))2

x(a)2 +   x2     2x(a)  x

x(a)2) + n  x2     2n  x  x

n(cid:88)
(cid:32) n(cid:88)
(cid:32)
n(cid:88)
(cid:32)
n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)
(cid:0)(x(a)2 + x(b)2     2x(a)x(b))(cid:1)
(cid:33)
(cid:32)
n(cid:88)
(cid:32)
n(cid:88)
(cid:32)
n(cid:88)
(cid:32)
n(cid:88)
(cid:32) n(cid:88)

x(a)2     2n  xn  x

x(a)2     2n2   x2

n(cid:88)
n(cid:88)

n(cid:88)
(cid:33)

(cid:33)
(cid:33)

x(a)2     2

x(a)2     2

1
2n2

1
2n2

1
2n2

1
2n2

1
2n2

x(a)n  x

(cid:33)

x(a)(

x(b))

2n

2n

2n

2n

a=1

a=1

a=1

a=1

a=1

a=1

a=1

b=1

b=1

b=1

x(a)2     n  x2

1
n

=

=

=

=

=

=

=

=

=

1
n

a=1

we see the expanded the equations (5) and (6) are same.

gini(x) =

1
2n2

where

|x(a)     x(b)| =

n(cid:88)

n(cid:88)
(cid:40)

a=1

|x(a)     x(b)|

b=1

1 x(a) (cid:54)= x(b)
0 x(a) = x(b).

5

using the same formulation about the sum of the differences between individual

instances for categorical data gives a gini index [5]:

(5)

(6)

(7)

(8)

table 2: contingency table with high correlation [12, 11].

xc
xc
2
0
100
1

xc
1
100
0
0

xc
3
0
0
100

xr

xr
1
xr
2
xr
3

here, x is a categorical variable that takes one of the values in {x1, x2, ..., xm}. rewrit-
ing this formulation with one-hot encoding gives:

e(x1) = (1, 0, 0, 0, ...)t
e(x2) = (0, 1, 0, 0, ...)t.
...
e(xm) = (0, 0, 0, ..., 1)t

this is more similar to the continuous case:

n(cid:88)

n(cid:88)

gini(x) =

1
2n2

a=1

b=1

2

|e(x(a))     e(x(b))|2

(9)

(10)

.

using this one-hot encoding, we can also de   ne the covariance of categorical data.
mc}.
consider the categorical variables xr     {xr
if the given sample categorical data is

mr} and xc     {xc

2, ..., xc

2, ..., xr

1, xc

1, xr

{(xr, xc)} ={(xr(1), xc(1)), (xr(2), xc(2)), ...,
(xr(a), xc(a)), ..., (xr(n), xc(n))},

we can de   ne the covariance of xr and xc:

covwrong(xr, xc) =

1
4n2

n(cid:88)

n(cid:88)

a=1

b=1

|e(xr(a))     e(xr(b))|
  |e(xc(a))     e(xc(b))|.

(11)

(12)

okada [12, 11] showed that this de   nition is invalid by considering the contingency
table shown in table 2. in this contingency table, xr and xc are highly correlated and
2) reduces the correlation between xr and xc. however,
the instance (xr, xc) = (xr
the instance (xr, xc) = (xr
2) increases the covariance in the formulation (12). to
avoid such an invalid increase, okada de   ned the covariance using rotated one-hot
encoding [11].

3, xc
3, xc

de   nition 1. the covariance of categorical variables xr and xc is the maximized value

6

:

cov(xr, xc)

= maximize

r

1
2n2

n(cid:88)

n(cid:88)

a=1

b=1

(er(xr(a))     er(xr(a)))t
1
2
r(ec(xc(a))     ec(xc(b))

subject to

rtr = e

(13)

where r is a rotation matrix that maximizes the covariance. the vectors er(xr) and

ec(xc) are one-hot encodings of xr and xc.

in terms of this de   nition, the instance (xr, xc) = (xr

2) reduces the covariance.
in this respect, this de   nition is better than (12). expanding the maximization problem,
(13) gives a simpli   ed form.

3, xc

lemma 2. the maximization problem (13) is equivalent to

cov(xr, xc) =maximize

r

subject to

tr(rt  )

1
2
rtr = e,

(14)

where

   = n/n     rct/n2,
r = n1,
c = nt1.

(15)
(16)
(17)
n is an mr    mc contingency table, with entries nij giving the frequency with which
row categorical variable xr = xr
i occurs together with column categorical variable
xc = xc
j. r denotes the vector of row marginals and c is the vector of column marginals.
1 = (1, 1, 1...)t.

7

proof. consider the expansion of (13):

n(cid:88)

n(cid:88)

a=1

b=1

1
2n2

=

1
4n2 tr(rt

= tr(rt(

1
2n

    1
2n2

n(cid:88)

b=1

a=1

n(cid:88)
n(cid:88)
n(cid:88)

a=1

1
2

(er(xr(a))     er(xr(b)))t
r(ec(xc(a))     ec(xc(b)))

(er(xr(a))     er(xr(b)))

(ec(xc(a))     ec(xc(b)))t)

er(xr(a))ec(xc(a))t

er(xr(a))

ec(xc(b))t))

n(cid:88)

b=1

=

=

1
2
1
2

where

a=1
h r th c

tr(rt(

tr(rt(

n
n

    rct

n
    rct

n2 )) =

n2 ))
1
2

tr(rt  ),

(18)

h r = [er(xr(a1)), er(xr(a2)), ..., er(xr(an))]t
h c = [ec(xc(a1)), ec(xc(a2)), ..., ec(xc(an))]t.

(19)
h r is the n    mr indicator matrix of xr. h c is the n    mc indicator matrix of xc. the
contingency table n can be constructed using the matrix product of the two indicator
matrices:

n = h r th c.

(20)

substituting (18) into (13) gives (14).

we can solve the maximization problem (14) using svd.

theorem 1. r = u v t is a local optimum of the maximization problem (14). here,

u sv t =   

(21)

is an svd of   .

proof. local optima of the maximization problem are given by differentiating the la-
grangian:

(22)
where    is a lagrange multiplier. the differentiation of this lagrangian with re-

l = tr(rt  )     tr(  t(rtr     e)),

spect to r gives the stationary condition:

rt   = (   +   t).

(23)

8

this result shows that rt   must be a symmetric matrix.
consider the svd:

u sv t =   

(24)

for the case r = u v t. here, rt   is the symmetric matrix:

rt   = v u tu sv t = v sv t

and r is the rotation matrix:

rtr = v u tu v t = e.

r = u v t satis   es the stationary condition (23) and the constraint of lemma 2. we
can therefore conclude that r = u v t is a local optimum of the probleam of lemma
2.

theorem 2. when all singular values of    are positive, r = u v t is the global opti-
mum for the maximization problem (14).

proof. substituting u sv t =    into (14) gives:

tr(rt  ) = tr(rtu sv t) = tr(v trtu s).

note that v trtu = q = [qij] is also a rotation matrix. consider

tr(qs) = tr(qd(s)) =

qiisi,

(cid:88)

i

where s = (s1, s2, ...) is the vector of the singular values of   . because q is a rotation
matrix,    i|qii|     1. then,

si.

(25)

(cid:88)
the case q = e gives the upper limit:(cid:88)

qiisi    (cid:88)
(cid:88)

i

i

qiisi =

1si =

i

i

(cid:88)

i

si.

in our experiments, we did not    nd a case for which    had a large negative singu-
lar value. in the following, we assume that r = u v t is the global optimum of the
maximization problem (14).

if negative singular values appear, we can use the following theorem.

theorem 3. consider the following optimization problem for a given matrix   :

maximize

r

subject to

tr(rt  )

rtr = e.

9

the global optimum of this optimization problem is:

here,

r = u d(sgn(s))v t.

u d(s)v t =   

is an svd of the matrix   , where s = (s1, s2, ...) is the vector of the singular values
for the matrix   .
proof. consider the case for which some of the singular values are negative. in such a

case, the upper limit (25) becomes:(cid:88)
qiisi    (cid:88)
(cid:88)

(cid:88)

i

i

the case q = d(sgn(s)) gives the upper limit:

sgn(si)si =

qiisi =

sgn(si)si =

(cid:88)
(cid:88)

i

(26)

|si|.

|si|.

i

i

i

4 lca

first, we introduce generalized singular value decomposition (gsvd).
de   nition 2. generalized singular value decomposition (gsvd) of a given matrix   
with diagonal weight matrices d(r) and d(c) is the decomposition:

where

  u   s   v t =   

  u = d(r)1/2   u ,
  v = d(c)1/2   v .

(27)

(28)
(29)

d(v) denotes the diagonal matrix for which diagonal entries are the components of
vector v. the vectors r and c are given weight vectors.   u and   v are given by ordinary
svd:

  u   s   v t =     

(30)

where

(31)
note that this decomposition maintains the perpendicularity of the base vectors in

     = d(r)   1/2  d(c)   1/2.

the decomposed space with the weight matrices:

  u td(r)   1   u =   v td(c)   1   v = e.

(32)

using gsvd, we can de   ne the well-known analysis for categorical data 1.
1http://forrest.psych.unc.edu/research/vista-frames/pdf/chap11.pdf

10

de   nition 3. the liner correspondence analysis (lca) of a given contingency table
n is gsvd:

  u   s   v t =   

with weight matrices d(r) and d(c). here,

   = n/n     rct/n2,
r =   1,
c =   t1.

lemma 3. lca is equivalent to the maximization problem:

maximize

  r

subject to

tr(   rt     )

1
2
  rt   r = e,

and has the solution:

here,   u and   v are given by ordinary svd:

  r =   u   v t.

where

  u   s   v t =     

     = d(r)   1/2  d(c)   1/2.

proof. lca is the gsvd of   . the gsvd is the svd of     . applying theorem 1 to
the svd of      gives the required result.

(21) and (30) enable the svd   u   s   v t =      to be rewritten as the following maxi-

mization problem based on one-hot encoding.

theorem 4. lca is equivalent to the maximization problem:

(cid:88)

a,b

1
4n2

  rt[  er(xr
= e

(  er(xr(a))       er(xr(b)))t

  r(  ec(xc(a))       ec(xc(b)))
2), ...]t   r[  ec(xc

1),   er(xr

1),   ec(xc

2), ...]

maximize

  r

subject to

where

are scaled one-hot encodings.

  er(xr) = d(r)   1er(xr),
  ec(xc) = d(c)   1ec(xc)

11

(33)

(34)
(35)
(36)

(37)

(38)

(39)

(40)

(41)

(42)
(43)

proof. substitute the following relations into the maximization problem (37):

tr(   rt     ) = tr(   rtd(r)   1  d(c)   1)
  rtd(r)   1   rd(c)   1 = d(c)1/2   rt   rd(c)   1/2 = e
      rt   r = e

where

  r = d(r)1/2   rd(c)1/2.

substituting these relations in (37) gives:

maximize

  r

subject to

tr(   rtd(r)   1  d(c)   1)
1
2
  rtd(r)   1   rd(c)   1 = e.

note that:

1),   er(xr

2), ...,   er(xr
[  er(xr
= d(r)   1[er(xr
1), er(xr
= d(r)   1e = d(r)   1
2), ...,   ec(xc
[  ec(xc
= d(c)   1[ec(xc
1), ec(xc
= d(c)   1e = d(c)   1.

1),   ec(xc

nr )]
2), ..., er(xr

nr )]

nc )]
2), ..., ec(xc

nc)]

(44)

(45)

(46)

(47)

(48)

substituting this relation into problem (47) yields the optimization problem (41).

this maximization problem de   nes the rotated gini index using scaled one-hot
encoding. we can therefore say that lca is equivalent to de   ning a gini index using
scaled and rotated one-hot encoding.

5 nonlinear extension

we now consider extending the optimization problem (41) using one-hot encoding on
a nonlinear mapped space.

de   nition 4. a nonlinear extension to ca can be expressed as:

maximize

r

subject to

(cid:88)

1
4n2

(  r(  er(xr(a))) (cid:9)r   r(  er(xr(b))))t

a,b

   r(  c(  ec(xc(a))) (cid:9)c   c(  ec(xc(b))))

rt[  r(  er(xr
r[  c(  ec(xc

1)),   r(  er(xr
1)),   c(  ec(xc

2)), ...]t
2)), ...] = e

(49)

12

where   r,   c are nonlinear mappings. (cid:9)r,(cid:9)c are subtraction operators on the non-
linear mapped spaces. the summation operator performs cumulative addition on the

nonlinear mapped spaces: (cid:88)

xi = x1     x2     ...

where     is an addition operator on the nonlinear mapped spaces. we refer to this
formulation (49) as kernel correspondence analysis (kca).

to be able to use the kernel trick, we assume the following rules about subtract and

add operations.
assumption 1.

2

1 (cid:9) xy t

x(y1 (cid:9)c y2)t = xy t
(x1 (cid:9)r x1)y t = x1y t (cid:9) x2y t
x (cid:9) (y (cid:9) z) = (x     z) (cid:9) y
(x (cid:9) y )     z = (x     z) (cid:9) y
(x (cid:9) y ) (cid:9) z = x (cid:9) (z     y )
x     (y (cid:9) z) = (x     y ) (cid:9) z

(50)
(51)
(52)
(53)
(54)
(55)
because (cid:9)r,(cid:9)c,(cid:9), and     are nonlinear operators, these relations are not valid
in general. however, moving left-hand-side operators to the right-hand side in these
relations can move (cid:9) outside the expression. moving (cid:9) to the extreme right enables
(cid:9) to require evaluation only once.
when     = +, expanding (49) using these expansion rules gives the following

theorem.
theorem 5. if     = + and the rules in assumption 1 are valid, we can introduce
kernel matrices kr and kc. using the kernel matrices, the maximization problem (49)
becomes:

maximize

r

subject to

n (cid:9) 1

tr(rtkr(

1
2
rtkrrkc = e.

1
n

n2 rct)kc)

(56)

note that this formulation requires (cid:9) to be evaluated only once.
specifying the operators and kernel matrices enables access to various known anal-
yses for categorical data and nlp. table 3 gives the relation between the speci   cations
and known methods.

5.1 semi-supervised ca
consider the case where we wish to manually tune the distance between one-hot en-
codings using tuning ratio tables   r(xr, x(cid:48)r) and   c(xc, x(cid:48)c).
a)       er(xr
a)       ec(xc

b)) = (  er(xr
b)) = (  ec(xc
for this case, we can de   ne the following problem.

a)) (cid:9)r   r(  er(xr
a)) (cid:9)c   c(  ec(xc

  r(  er(xr
  c(  ec(xc

b))  r(xr
b))  c(xc

a, xr
b)
a, xc
b)

(57)

13

table 3: relations between known methods and kernel specializations.
name
lca
gini index [12, 11] e
sgns [10, 8]
e
glove [13]
e

x (cid:9) y
kr
d(r)   1 d(c)   1 x     y
x     y
(log x     log y     log k)
(log x     log y + bw + bc)

e
e
e

kc

table 4: comparisons using the text8 corpus.

method
cbow
sgns
glove
fasttext
tail-cut
lca n   at
lca nskip
sca+men
sca+m.turk

sim
0.388
0.674
0.431
0.655
0.762
0.749
0.741
0.743
0.741

rel men m.turk
0.579
0.438
0.654
0.608
0.508
0.466
0.623
0.609
0.649
0.667
0.668
0.680
0.657
0.640
0.636
0.665
0.658
0.798

0.383
0.561
0.421
0.636
0.682
0.671
0.672
0.770
0.672

rare
0.050
0.027
0.118
0.059
0.121
0.127
0.135
0.136
0.136

s999
0.075
0.215
0.096
0.223
0.212
0.218
0.211
0.210
0.211

red: best result

magenta: 2nd best

de   nition 5. semi-supervised correspondence analysis (sca) can be expressed as:

maximize

r

subject to

1

2n2 tr(rtd(r)   1(n     (  rn  c)
    (  rn)     (n  c))d(c)   1)

rtd(r)   1rd(c)   1 = e
r = (  rn)     (n  c)1
c = ((  rn)     (n  c))t1
  r = [  r
ij]
  c = [  c
ij]

(58)

where     is the hadamard product.

this problem is de   ned by considering     = + and (57). the tuning tables   r and
  c can be regarded as supervised training data. however, this method can also be based
on unsupervised training data like pca. we refer to this process as semi-supervised
correspondence analysis (sca).

14

6 delayed sparse matrix

ca is ordinary svd:

  u   s   v t = d(r)   1/2(cid:0)n/n     rct/n2(cid:1) d(c)   1/2.

(59)
because rct is a dense matrix, n/n     rct/n2 is also a dense matrix, even when n
is a sparse matrix. this is the reason why the ca approach makes such a demand on
memory resources.

however, computing the dense matrix can be avoided by delayed evaluation. con-
sider multiplying by an arbitrary matrix z on both the left-hand and right-hand side of
(59).

the right-hand-side multiplication can be expressed similarly:

(cid:16)

left-dot(z) = lamda(z)(

zd(r)   1/2n/n     zd(r)   1/2rct/n2(cid:17)
d(r)   1/2(cid:16)

right-dot(z) = lamda(z)(

n/nd(c)   1/2z     rct/n2d(c)   1/2z

d(c)   1/2)

(cid:17)

).

(60)

(61)

randomized svd requires only a multiplying operation on the matrix to be decom-
posed, as for the power method. we can execute the randomized svd using left-dot(z)
and right-dot(z) without involving the expanded matrix (59). because this scheme
can avoid computing the dense matrix, there is a reduction in both computing time and
memory requirements. we refer to this scheme as the delayed sparse randomized svd
(dssvd) algorithm. python implementation of this ca is provided in https://
github.com/niitsuma/delayedsparse/blob/master/delayedsparse/
ca.py

7 word representation

this research discusses the application of ca to word-vector representation tasks. con-
sider the following contingency table for some given training-text data:

nskip = [nskip

ij ] = [#(wi, tj)],

(62)

where #(w, t) is the number of times that the word w appears in the context t. based
on this table, mikolov et al. [10] introduced vector representations of words, referred
to as id97. #(w, t) is computed by using the skip-gram model. however, the
skip-gram model requires random sampling, which gives different results for each com-
putation. this research uses the following    xed representation.

#(w1    k w2)

this notation represents the number of subsentences for which an arbitrary k words
appear between words w1 and w2. for example, consider the sentence:

15

   this is this is this is this is this.   

in this sentence, the number of times    is    occurs three words after    this    is:

#(this         is) = #(this    2 is) = 3.

   is    also appears in other locations.

#(this    0 is) = 4, #(this    1 is) = 0, #(this    2 is) = 3, ...

given an appropriate window size w , this equation can represent a relation similar to
the skip-gram.

n   at = [n   at

ij ] = [

#(wi    k wj)]

(63)

this cannot ignore noise relations when w is large. to ignore noise, we introduce the
following weighted sum:

w(cid:88)

k=0

w(cid:88)

ncut = [ncut

ij ] = [

#(wi    k wj)  (wi, wj, k)]

(64)

where

  (wi, wj, k) =

(cid:40)

k=0

1
0

(#(wi    k wj) > #(wi)#(wj )
(#(wi    k wj)     #(wi)#(wj )

n2

n2

)
).

#(w) is the number of times that the word w appears in all the training text. n is the
total number of words in the given training text. the weighted sum can be introduced
using a kernel extension similar to sca. we refer to this extension as the    tail-cut
kernel   . we can compute the lca of nskip and n   at and the kca of ncut.

8 experiments
this section compares various word-vector representation tasks using the text8 corpus2
.

8.1 delayed sparse randomized svd
figures 2 and 3 show the computing times and the required memory for lca, respec-
tively. the horizontal axis is the size of the training data. the initial section of the
text8 corpus was used as the training data for the lca. the experiments were carried
out in a gentoo linux environment using an intel i7-3770k 3.50 ghz processor. note
that the vertical axes have logarithmic scales.

the lca was computed using svd with the numpy library, randomized svd
with the the scikit-learn library, and the dssvd. dssvd was 100 times faster than
svd with numpy and 10 times faster than randomized svd. the memory required
for dssvd was 10% of that required for svd of numpy and 20% of that required for

2 http://mattmahoney.net/dc/text8.zip

16

figure 2: svd comparison time.

figure 3: svd comparison memory.

17

246810training data size (mb)102103time (sec)delayed sparserandmozed svdnumpy svd246810training data size (mb)106107memory (kb)delayed sparserandmozed svdnumpy svdrandomized svd. when using the whole text8 corpus, the differences became more
emphatic. because of excessive memory requirements, using ca for nlp is impossible
without dssvd. python code of this experimets is provided in https://github.
com/niitsuma/delayedsparse/blob/master/demo-ca.sh

8.2 word representation
we evaluated the english word-vector representation by focusing on the similarity be-
tween words using six test datasets.

sim: wordsim similarity [15]

rel: wordsim relatedness [1]

men: men dataset [3]

m.turk: mechanical turk dataset [14]

rare: words dataset [9]

s999: siid113x-999 dataset [7].

table 4 shows a comparison between methods for the whole text8 corpus. evalu-
ation with these six test datasets provided a ranking of similarity among words. the
evaluation values are spearman   s rank correlation coef   cients of the ranking of simi-
larity among words. for comparison, we show the results for skip-gram with negative
sampling (sgns) [10], continuous bag-of-words (cbow) [10], glove [13], and fast-
text [2].

in most cases, the tail-cut kernel provided the best or almost-best results. the lca
with n   at also provided some of the best results. however, the lca with n   at results
were drastically affected by the window-size parameter. lca for nskip also showed
instability, whereas the tail-cut kernel provided stable results. for window sizes larger
than 30, its result changes become insigni   cant. this implies that the tail-cut kernel is
relatively independent of the window size parameter, thereby possibly decreasing the
number of parameters by one.

sca based on lca for nskip were also evaluated. the sca used men data and
m.turk data as the supervised training data. sca outperformed lca for much of
the test data. these results demonstrate that sca can work effectively. although
the word-vector representation task is unsupervised learning, sca can use supervised
data within the word-vector representation task. part of codes of this experiments is
provided in https://github.com/niitsuma/wordca

9 conclusion

we have proposed a memory-ef   cient ca method based on randomized svd. the al-
gorithm also drastically reduces the computation time. this ef   cient ca can be applied

18

to the word-vector representation task. the experimental results show that ca can out-
perform existing methods in the word-vector representation task. we have further pro-
posed the tail-cut kernel, which is an extension of the skip-gram approach within kca.
again, the tail-cut kernel outperformed existing word-vector representation methods.

references

[1] eneko agirre, enrique alfonseca, keith hall, jana kravalova, marius pas  ca,
and aitor soroa. a study on similarity and relatedness using distributional and
id138-based approaches. in proceedings of human language technologies:
the 2009 annual conference of the north american chapter of the association
for computational linguistics, pages 19   27. association for computational lin-
guistics, 2009.

[2] piotr bojanowski, edouard grave, armand joulin, and tomas mikolov. enrich-
ing word vectors with subword information. arxiv preprint arxiv:1607.04606,
2016.

[3] elia bruni, gemma boleda, marco baroni, and nam khanh tran. distributional
semantics in technicolor. in proceedings of the 50th annual meeting of the asso-
ciation for computational linguistics, pages 136   145. association for computa-
tional linguistics, july 2012.

[4] r. a. fisher. the precision of discriminant functions. annals of eugenics,

10:422   429, 1940.

[5] c.w. gini. variability and mutability, contribution to the study of statistical dis-
tributions and relations. studi economico-giuridici della r. universita de cagliari
(1912). reviewed in: light, r.j., margolin, b.h.: an analysis of variance for cate-
gorical data. j. american statistical association, 66:534   544, 1971.

[6] n. halko, p. g. martinsson, and j. a. tropp. finding structure with random-
ness: probabilistic algorithms for constructing approximate matrix decomposi-
tions. siam review, 53(2):217   288, may 2011.

[7] felix hill, roi reichart, and anna korhonen. siid113x-999: evaluating semantic
models with genuine similarity estimation. comput. linguist., 41(4):665   695,
december 2015.

[8] omer levy and yoav goldberg. neural id27 as implicit matrix fac-
torization. in proceedings of the 27th international conference on neural infor-
mation processing systems, pages 2177   2185, 2014.

[9] thang luong, richard socher, and christopher manning. better word represen-
in proceedings of the
tations with id56s for morphology.
seventeenth conference on computational natural language learning, pages
104   113, august 2013.

19

[10] tomas mikolov, ilya sutskever, kai chen, greg s corrado, and jeff dean. dis-
tributed representations of words and phrases and their compositionality. in pro-
ceedings of the 26th international conference on neural information processing
systems, pages 3111   3119. 2013.

[11] hirotaka niitsuma and takashi okada. covariance and pca for categorical vari-
ables. in proceedings of the 9th paci   c-asia conference on knowledge discovery
and data mining, pages 523   528, 2005.

[12] t. okada. a note on covariances for categorical data. in k.s. leung, l.w. chan,
and h. meng, editors, intelligent data engineering and automated learning -
ideal 2000, 2000.

[13] jeffrey pennington, richard socher, and christopher d. manning. glove: global
vectors for word representation. in proceedings of the 2014 conference on em-
pirical methods in natural language processing, pages 1532   1543, 2014.

[14] kira radinsky, eugene agichtein, evgeniy gabrilovich, and shaul markovitch.
a word at a time: computing word relatedness using temporal semantic analysis.
in proceedings of the 20th international conference on world wide web, pages
337   346, 2011.

[15] torsten zesch, christof m  uller, and iryna gurevych. using wiktionary for com-
puting semantic relatedness. in proceedings of the 23rd national conference on
arti   cial intelligence, pages 861   866, 2008.

[16] y. zhang, h. wu, and l. cheng. some new deformation formulas about vari-
ance and covariance. in proceedings of international conference on modelling,
identi   cation and control, pages 987   992, june 2012.

20

