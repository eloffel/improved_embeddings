4
1
0
2

 
r
p
a
4
2

 

 
 
]
e
n
.
s
c
[
 
 

5
v
6
2
0
6

.

2
1
3
1
:
v
i
x
r
a

how to construct deep recurrent neural networks

razvan pascanu1, caglar gulcehre1, kyunghyun cho2, and yoshua bengio1

1d  epartement d   informatique et de recherche op  erationelle, universit  e de montr  eal,

{pascanur, gulcehrc}@iro.umontreal.ca, yoshua.bengio@umontreal.ca

2department of information and computer science, aalto university school of science,

kyunghyun.cho@aalto.fi

abstract

in this paper, we explore different ways to extend a recurrent neural network
(id56) to a deep id56. we start by arguing that the concept of depth in an id56
is not as clear as it is in feedforward neural networks. by carefully analyzing
and understanding the architecture of an id56, however, we    nd three points of
an id56 which may be made deeper; (1) input-to-hidden function, (2) hidden-to-
hidden transition and (3) hidden-to-output function. based on this observation, we
propose two novel architectures of a deep id56 which are orthogonal to an earlier
attempt of stacking multiple recurrent layers to build a deep id56 (schmidhu-
ber, 1992; el hihi and bengio, 1996). we provide an alternative interpretation
of these deep id56s using a novel framework based on neural operators. the
proposed deep id56s are empirically evaluated on the tasks of polyphonic music
prediction and id38. the experimental result supports our claim
that the proposed deep id56s bene   t from the depth and outperform the conven-
tional, shallow id56s.

1

introduction

recurrent neural networks (id56, see, e.g., rumelhart et al., 1986) have recently become a popular
choice for modeling variable-length sequences. id56s have been successfully used for various task
such as id38 (see, e.g., graves, 2013; pascanu et al., 2013a; mikolov, 2012; sutskever
et al., 2011), learning id27s (see, e.g., mikolov et al., 2013a), online handwritten recog-
nition (graves et al., 2009) and id103 (graves et al., 2013).
in this work, we explore deep extensions of the basic id56. depth for feedforward models can
lead to more expressive models (pascanu et al., 2013b), and we believe the same should hold for
recurrent models. we claim that, unlike in the case of feedforward neural networks, the depth of an
id56 is ambiguous. in one sense, if we consider the existence of a composition of several nonlinear
computational layers in a neural network being deep, id56s are already deep, since any id56 can
be expressed as a composition of multiple nonlinear layers when unfolded in time.
schmidhuber (1992); el hihi and bengio (1996) earlier proposed another way of building a deep
id56 by stacking multiple recurrent hidden states on top of each other. this approach poten-
tially allows the hidden state at each level to operate at different timescale (see, e.g., hermans and
schrauwen, 2013). nonetheless, we notice that there are some other aspects of the model that may
still be considered shallow. for instance, the transition between two consecutive hidden states at
a single level is shallow, when viewed separately.this has implications on what kind of transitions
this model can represent as discussed in section 3.2.3.
based on this observation, in this paper, we investigate possible approaches to extending an id56
into a deep id56. we begin by studying which parts of an id56 may be considered shallow. then,

1

(cid:110)(cid:16)
tn(cid:88)
n(cid:88)

n=1

t=1

for each shallow part, we propose an alternative deeper design, which leads to a number of deeper
variants of an id56. the proposed deeper variants are then empirically evaluated on two sequence
modeling tasks.
the layout of the paper is as follows. in section 2 we brie   y introduce the concept of an id56. in
section 3 we explore different concepts of depth in id56s. in particular, in section 3.3.1   3.3.2 we
propose two novel variants of deep id56s and evaluate them empirically in section 5 on two tasks:
polyphonic music prediction (boulanger-lewandowski et al., 2012) and id38. finally
we discuss the shortcomings and advantages of the proposed models in section 6.

2 recurrent neural networks

a recurrent neural network (id56) is a neural network that simulates a discrete-time dynamical
system that has an input xt, an output yt and a hidden state ht. in our notation the subscript t
represents time. the dynamical system is de   ned by

(1)
(2)
where fh and fo are a state transition function and an output function, respectively. each function is
parameterized by a set of parameters;   h and   o.

ht = fh(xt, ht   1)
yt = fo(ht),

given a set of n training sequences d =
of an id56 can be estimated by minimizing the following cost function:

1 ), . . . , (x(n)
tn

1 , y(n)

(x(n)

, y(n)
tn

)

, the parameters

n=1

(cid:17)(cid:111)n

j(  ) =

1
n

d(y(n)

t

, fo(h(n)

t

)),

(3)

where h(n)
and b, such as euclidean distance or cross-id178.

t = fh(x(n)

t   1) and h(n)

, h(n)

t

0 = 0. d(a, b) is a prede   ned divergence measure between a

2.1 conventional recurrent neural networks

(cid:0)w(cid:62)ht   1 + u(cid:62)xt
(cid:1) ,
(cid:0)v(cid:62)ht

(cid:1)

a conventional id56 is constructed by de   ning the transition function and the output function as

ht = fh(xt, ht   1) =   h
yt = fo(ht, xt) =   o

(4)
(5)
where w, u and v are respectively the transition, input and output matrices, and   h and   o are
element-wise nonlinear functions. it is usual to use a saturating nonlinear function such as a logistic
sigmoid function or a hyperbolic tangent function for   h. an illustration of this id56 is in fig. 2
(a).
the parameters of the conventional id56 can be estimated by, for instance, stochastic gradient de-
scent (sgd) algorithm with the gradient of the cost function in eq. (3) computed by id26
through time (rumelhart et al., 1986).

3 deep recurrent neural networks

3.1 why deep recurrent neural networks?

deep learning is built around a hypothesis that a deep, hierarchical model can be exponentially
more ef   cient at representing some functions than a shallow one (bengio, 2009). a number of
recent theoretical results support this hypothesis (see, e.g., le roux and bengio, 2010; delalleau
and bengio, 2011; pascanu et al., 2013b). for instance, it has been shown by delalleau and bengio
(2011) that a deep sum-product network may require exponentially less units to represent the same
function compared to a shallow sum-product network. furthermore, there is a wealth of empirical
evidences supporting this hypothesis (see, e.g., goodfellow et al., 2013; hinton et al., 2012b,a).
these    ndings make us suspect that the same argument should apply to recurrent neural networks.

2

3.2 depth of a recurrent neural network

figure 1: a conventional recurrent neural network unfolded in time.

the depth is de   ned in the case of feedforward neural networks as having multiple nonlinear layers
between input and output. unfortunately this de   nition does not apply trivially to a recurrent neural
network (id56) because of its temporal structure. for instance, any id56 when unfolded in time as
in fig. 1 is deep, because a computational path between the input at time k < t to the output at time
t crosses several nonlinear layers.
a close analysis of the computation carried out by an id56 (see fig. 2 (a)) at each time step individ-
ually, however, shows that certain transitions are not deep, but are only results of a linear projection
followed by an element-wise nonlinearity. it is clear that the hidden-to-hidden (ht   1     ht), hidden-
to-output (ht     yt) and input-to-hidden (xt     ht) functions are all shallow in the sense that there
exists no intermediate, nonlinear hidden layer.
we can now consider different types of depth of an id56 by considering those transitions separately.
we may make the hidden-to-hidden transition deeper by having one or more intermediate nonlinear
layers between two consecutive hidden states (ht   1 and ht). at the same time, the hidden-to-
output function can be made deeper, as described previously, by plugging, multiple intermediate
nonlinear layers between the hidden state ht and the output yt. each of these choices has a different
implication.

3.2.1 deep input-to-hidden function

a model can exploit more non-temporal structure from the input by making the input-to-hidden
function deep. previous work has shown that higher-level representations of deep networks tend to
better disentangle the underlying factors of variation than the original input (goodfellow et al., 2009;
glorot et al., 2011b) and    atten the manifolds near which the data concentrate (bengio et al., 2013).
we hypothesize that such higher-level representations should make it easier to learn the temporal
structure between successive time steps because the relationship between abstract features can gen-
erally be expressed more easily. this has been, for instance, illustrated by the recent work (mikolov
et al., 2013b) showing that id27s from neural language models tend to be related to their
temporal neighbors by simple algebraic relationships, with the same type of relationship (adding a
vector) holding over very different regions of the space, allowing a form of analogical reasoning.
this approach of making the input-to-hidden function deeper is in the line with the standard practice
of replacing input with extracted features in order to improve the performance of a machine learning
model (see, e.g., bengio, 2009). recently, chen and deng (2013) reported that a better speech
recognition performance could be achieved by employing this strategy, although they did not jointly
train the deep input-to-hidden function together with other parameters of an id56.

3.2.2 deep hidden-to-output function

a deep hidden-to-output function can be useful to disentangle the factors of variations in the hidden
state, making it easier to predict the output. this allows the hidden state of the model to be more
compact and may result in the model being able to summarize the history of previous inputs more
ef   ciently. let us denote an id56 with this deep hidden-to-output function a deep output id56
(do-id56).
instead of having feedforward,
intermediate layers between the hidden state and the output,
boulanger-lewandowski et al. (2012) proposed to replace the output layer with a conditional gen-

3

yt-1xt-1ht-1xthtytxt+1ht+1yt+1(a) id56

(b) dt-id56

(b*) dt(s)-id56

(c) dot-id56

(d) stacked id56

figure 2: illustrations of four different recurrent neural networks (id56). (a) a conventional id56.
(b) deep transition (dt) id56. (b*) dt-id56 with shortcut connections (c) deep transition, deep
output (dot) id56. (d) stacked id56

erative model such as restricted id82s or neural autoregressive distribution estima-
tor (larochelle and murray, 2011). in this paper we only consider feedforward intermediate layers.

3.2.3 deep hidden-to-hidden transition

the third knob we can play with is the depth of the hidden-to-hidden transition. the state transition
between the consecutive hidden states effectively adds a new input to the summary of the previous
inputs represented by the    xed-length hidden state. previous work with id56s has generally limited
the architecture to a shallow operation; af   ne transformation followed by an element-wise nonlin-
earity. instead, we argue that this procedure of constructing a new summary, or a hidden state, from
the combination of the previous one and the new input should be highly nonlinear. this nonlinear
transition could allow, for instance, the hidden state of an id56 to rapidly adapt to quickly changing
modes of the input, while still preserving a useful summary of the past. this may be impossible to
be modeled by a function from the family of generalized linear models. however, this highly non-
linear transition can be modeled by an mlp with one or more hidden layers which has an universal
approximator property (see, e.g., hornik et al., 1989).
an id56 with this deep transition will be called a deep transition id56 (dt-id56) throughout re-
mainder of this paper. this model is shown in fig. 2 (b).
this approach of having a deep transition, however, introduces a potential problem. as the in-
troduction of deep transition increases the number of nonlinear steps the gradient has to traverse
when propagated back in time, it might become more dif   cult to train the model to capture long-
term dependencies (bengio et al., 1994). one possible way to address this dif   culty is to introduce
shortcut connections (see, e.g., raiko et al., 2012) in the deep transition, where the added shortcut
connections provide shorter paths, skipping the intermediate layers, through which the gradient is
propagated back in time. we refer to an id56 having deep transition with shortcut connections by
dt(s)-id56 (see fig. 2 (b*)).
furthermore, we will call an id56 having both a deep hidden-to-output function and a deep transi-
tion a deep output, deep transition id56 (dot-id56). see fig. 2 (c) for the illustration of dot-id56.
if we consider shortcut connections as well in the hidden to hidden transition, we call the resulting
model dot(s)-id56.
an approach similar to the deep hidden-to-hidden transition has been proposed recently by pinheiro
and collobert (2014) in the context of parsing a static scene. they introduced a recurrent convolu-
tional neural network (rid98) which can be understood as a recurrent network whose the transition
between consecutive hidden states (and input to hidden state) is modeled by a convolutional neural
network. the rid98 was shown to speed up scene parsing and obtained the state-of-the-art result
in stanford background and sift flow datasets. ko and dieter (2009) proposed deep transitions
for gaussian process models. earlier, valpola and karhunen (2002) used a deep neural network to
model the state transition in a nonlinear, dynamical state-space model.

3.2.4 stack of hidden states

an id56 may be extended deeper in yet another way by stacking multiple recurrent hidden layers
on top of each other (schmidhuber, 1992; el hihi and bengio, 1996; jaeger, 2007; graves, 2013).

4

xtht-1htytxtht-1htytxtht-1htytxtht-1htytxtht-1htytzt-1ztwe call this model a stacked id56 (sid56) to distinguish it from the other proposed variants. the
goal of a such model is to encourage each recurrent level to operate at a different timescale.
it should be noticed that the dt-id56 and the sid56 extend the conventional, shallow id56 in
different aspects. if we look at each recurrent level of the sid56 separately, it is easy to see that
the transition between the consecutive hidden states is still shallow. as we have argued above, this
limits the family of functions it can represent. for example, if the structure of the data is suf   ciently
complex, incorporating a new input frame into the summary of what had been seen up to now might
be an arbitrarily complex function. in such a case we would like to model this function by something
that has universal approximator properties, as an mlp. the model can not rely on the higher layers
to do so, because the higher layers do not feed back into the lower layer. on the other hand, the
sid56 can deal with multiple time scales in the input sequence, which is not an obvious feature of
the dt-id56. the dt-id56 and the sid56 are, however, orthogonal in the sense that it is possible to
have both features of the dt-id56 and the sid56 by stacking multiple levels of dt-id56s to build
a stacked dt-id56 which we do not explore more in this paper.

3.3 formal descriptions of deep id56s

here we give a more formal description on how the deep transition recurrent neural network (dt-
id56) and the deep output id56 (do-id56) as well as the stacked id56 are implemented.

3.3.1 deep transition id56

we noticed from the state transition equation of the dynamical system simulated by id56s in eq. (1)
that there is no restriction on the form of fh. hence, we propose here to use a multilayer id88
to approximate fh instead.
in this case, we can implement fh by l intermediate layers such that

(cid:0)w(cid:62)

(cid:0)w(cid:62)

(cid:0)         1

(cid:0)w(cid:62)

1 ht   1 + u(cid:62)xt

(cid:1)(cid:1)(cid:1)(cid:1) ,

ht = fh(xt, ht   1) =   h

l   l   1

l   1  l   2

where   l and wl are the element-wise nonlinear function and the weight matrix for the l-th layer.
this id56 with a multilayered transition function is a deep transition id56 (dt-id56).
an illustration of building an id56 with the deep state transition function is shown in fig. 2 (b).
in the illustration the state transition function is implemented with a neural network with a single
intermediate layer.
this formulation allows the id56 to learn a non-trivial, highly nonlinear transition between the
consecutive hidden states.

3.3.2 deep output id56

similarly, we can use a multilayer id88 with l intermediate layers to model the output function
fo in eq. (2) such that

(cid:0)v(cid:62)

(cid:0)v(cid:62)

(cid:0)         1

(cid:0)v(cid:62)

1 ht

(cid:1)(cid:1)(cid:1)(cid:1) ,

yt = fo(ht) =   o

l   l   1

l   1  l   2

where   l and vl are the element-wise nonlinear function and the weight matrix for the l-th layer.
an id56 implementing this kind of multilayered output function is a deep output recurrent neural
network (do-id56).
fig. 2 (c) draws a deep output, deep transition id56 (dot-id56) implemented using both the deep
transition and the deep output with a single intermediate layer each.

3.3.3 stacked id56

the stacked id56 (schmidhuber, 1992; el hihi and bengio, 1996) has multiple levels of transition
functions de   ned by

(cid:16)

(cid:17)

w(cid:62)

t   1 + u(cid:62)

l h(l   1)

l h(l)

t

,

h(l)
t = f (l)

h (h(l   1)

t

, h(l)

t   1) =   h

5

t

is the hidden state of the l-th level at time t. when l = 1, the state is computed using xt
. the hidden states of all the levels are recursively computed from the bottom level

where h(l)
t
instead of h(l   1)
l = 1.
once the top-level hidden state is computed, the output can be obtained using the usual formula-
tion in eq. (5). alternatively, one may use all the hidden states to compute the output (hermans
and schrauwen, 2013). each hidden state at each level may also be made to depend on the input
as well (graves, 2013). both of them can be considered approaches using shortcut connections
discussed earlier.
the illustration of this stacked id56 is in fig. 2 (d).

4 another perspective: neural operators

in this section, we brie   y introduce a novel approach with which the already discussed deep tran-
sition (dt) and/or deep output (do) recurrent neural networks (id56) may be built. we call this
approach which is based on building an id56 with a set of prede   ned neural operators, an operator-
based framework.
in the operator-based framework, one    rst de   nes a set of operators of which each is implemented
by a multilayer id88 (mlp). for instance, a plus operator     may be de   ned as a function
receiving two vectors x and h and returning the summary h(cid:48) of them:

where we may constrain that the dimensionality of h and h(cid:48) are identical. additionally, we can
de   ne another operator (cid:66) which predicts the most likely output symbol x(cid:48) given a summary h, such
that

h(cid:48) = x     h,

x(cid:48) = (cid:66)h

it is possible to de   ne many other operators, but in this paper, we stick to these two operators which
are suf   cient to express all the proposed types of id56s.

it is clear to see that the plus operator     and the predict
operator (cid:66) correspond to the transition function and the
output function in eqs. (1)   (2). thus, at each step, an
id56 can be thought as performing the plus operator to
update the hidden state given an input (ht = xt     ht   1)
and then the predict operator to compute the output (yt =
(cid:66)ht = (cid:66)(xt     ht   1)). see fig. 3 for the illustration of
how an id56 can be understood from the operator-based
framework.
each operator can be parameterized as an mlp with one
or more hidden layers, hence a neural operator, since we
cannot simply expect the operation will be linear with
respect to the input vector(s). by using an mlp to im-
plement the operators, the proposed deep transition, deep
output id56 (dot-id56) naturally arises.
this framework provides us an insight on how the con-
structed id56 be regularized. for instance, one may regularize the model such that the plus operator
    is commutative. however, in this paper, we do not explore further on this approach.
note that this is different from (mikolov et al., 2013a) where the learned embeddings of words
happened to be suitable for algebraic operators. the operator-based framework proposed here is
rather geared toward learning these operators directly.

figure 3: a view of an id56 under the
operator-based framework:     and (cid:66) are
the plus and predict operators, respec-
tively.

5 experiments

we train four types of id56s described in this paper on a number of benchmark datasets to evaluate
their performance. for each benchmark dataset, we try the task of predicting the next symbol.

6

xtht-1htyt+the task of predicting the next symbol is equivalent to the task of modeling the distribution over a
sequence. for each sequence (x1, . . . , xt ), we decompose it into

p(x1, . . . , xt ) = p(x1)

p(xt | x1, . . . , xt   1),

t(cid:89)

t=2

and each term on the right-hand side will be replaced with a single timestep of an id56. in this
setting, the id56 predicts the id203 of the next symbol xt in the sequence given the all previous
symbols x1, . . . xt   1. then, we train the id56 by maximizing the log-likelihood.
we try this task of modeling the joint distribution on three different tasks; polyphonic music predic-
tion, character-level and word-level id38.
we test the id56s on the task of polyphonic music prediction using three datasets which are notting-
ham, jsb chorales and musedata (boulanger-lewandowski et al., 2012). on the task of character-
level and word-level id38, we use id32 corpus (marcus et al., 1993).

5.1 model descriptions

we compare the conventional recurrent neural network (id56), deep transition id56 with shortcut
connections in the transition mlp (dt(s)-id56), deep output/transition id56 with shortcut connec-
tions in the hidden to hidden transition mlp (dot(s)-id56) and stacked id56 (sid56). see fig. 2
(a)   (d) for the illustrations of these models.

id56

dt(s)-id56 dot(s)-id56

sid56
2 layers

music

language

notthingam

jsb chorales

musedata

char-level

word-level

# units

# parameters

# units

# parameters

# units

# parameters

# units

# parameters

# units

# parameters

600
465k
200
75k
600
465k
600
420k
200
4.04m

400,400
585k
400,400
585k
400,400
585k
400,400
540k
200,200
6.12m

400,400,400

400,400,400

400,400,400

745k

745k

745k

790k

400,400,600

200,200,200

6.16m

400
550k
400
550k
600

1185k

400
520k
400
8.48m

table 1: the sizes of the trained models. we provide the number of hidden units as well as the total
number of parameters. for dt(s)-id56, the two numbers provided for the number of units mean
the size of the hidden state and that of the intermediate layer, respectively. for dot(s)-id56, the
three numbers are the size of the hidden state, that of the intermediate layer between the consecutive
hidden states and that of the intermediate layer between the hidden state and the output layer. for
sid56, the number corresponds to the size of the hidden state at each level
the size of each model is chosen from a limited set {100, 200, 400, 600, 800} to minimize the val-
idation error for each polyphonic music task (see table. 1 for the    nal models). in the case of
id38 tasks, we chose the size of the models from {200, 400} and {400, 600} for
word-level and character-level tasks, respectively. in all cases, we use a logistic sigmoid function as
an element-wise nonlinearity of each hidden unit. only for the character-level id38
we used recti   ed linear units (glorot et al., 2011a) for the intermediate layers of the output function,
which gave lower validation error.

5.2 training

we use stochastic id119 (sgd) and employ the strategy of clipping the gradient proposed
by pascanu et al. (2013a). training stops when the validation cost stops decreasing.
polyphonic music prediction: for nottingham and musedata datasets we compute each gradient
step on subsequences of at most 200 steps, while we use subsequences of 50 steps for jsb chorales.

7

we do not reset the hidden state for each subsequence, unless the subsequence belongs to a different
song than the previous subsequence.
the cutoff threshold for the gradients is set to 1. the hyperparameter for the learning rate schedule1
is tuned manually for each dataset. we set the hyperparameter    to 2330 for nottingham, 1475 for
musedata and 100 for jsb chroales. they correspond to two epochs, a single epoch and a third of
an epoch, respectively.
the weights of the connections between any pair of hidden layers are sparse, having only 20 non-
zero incoming connections per unit (see, e.g., sutskever et al., 2013). each weight matrix is rescaled
to have a unit largest singular value (pascanu et al., 2013a). the weights of the connections between
the input layer and the hidden state as well as between the hidden state and the output layer are
initialized randomly from the white gaussian distribution with its standard deviation    xed to 0.1
and 0.01, respectively.
in the case of deep output functions (dot(s)-id56), the weights of the
connections between the hidden state and the intermediate layer are sampled initially from the white
gaussian distribution of standard deviation 0.01. in all cases, the biases are initialized to 0.
to regularize the models, we add white gaussian noise of standard deviation 0.075 to each weight
parameter every time the gradient is computed (graves, 2011).
id38: we used the same strategy for initializing the parameters in the case of lan-
guage modeling. for character-level modeling, the standard deviations of the white gaussian distri-
butions for the input-to-hidden weights and the hidden-to-output weights, we used 0.01 and 0.001,
respectively, while those hyperparameters were both 0.1 for word-level modeling. in the case of
dot(s)-id56, we sample the weights of between the hidden state and the recti   er intermediate
layer of the output function from the white gaussian distribution of standard deviation 0.01. when
using recti   er units (character-based id38) we    x the biases to 0.1.
in id38, the learning rate starts from an initial value and is halved each time the vali-
dation cost does not decrease signi   cantly (mikolov et al., 2010). we do not use any id173
for the character-level modeling, but for the word-level modeling we use the same strategy of adding
weight noise as we do with the polyphonic music prediction.
for all the tasks (polyphonic music prediction, character-level and word-level id38),
the stacked id56 and the dot(s)-id56 were initialized with the weights of the conventional id56
and the dt(s)-id56, which is similar to layer-wise pretraining of a feedforward neural network (see,
e.g., hinton and salakhutdinov, 2006). we use a ten times smaller learning rate for each parameter
that was pretrained as either id56 or dt(s)-id56.

id56 dt(s)-id56 dot(s)-id56 sid56 dot(s)-id56*

notthingam 3.225
jsb chorales
8.338
6.990

musedata

3.206
8.278
6.988

3.215
8.437
6.973

3.258
8.367
6.954

2.95
7.92
6.59

table 2: the performances of the four types of id56s on the polyphonic music prediction. the
numbers represent negative log-probabilities on test sequences. (*) we obtained these results using
dot(s)-id56 with lp units in the deep transition, maxout units in the deep output function and
dropout (gulcehre et al., 2013).

5.3 result and analysis

5.3.1 polyphonic music prediction

the log-probabilities on the test set of each data are presented in the    rst four columns of tab. 2. we
were able to observe that in all cases one of the proposed deep id56s outperformed the conventional,
shallow id56. though, the suitability of each deep id56 depended on the data it was trained on.
the best results obtained by the dt(s)-id56s on notthingam and jsb chorales are close to, but

1 we use at each update   , the following learning rate      =

, where   0 and    indicate respec-
tively when the learning rate starts decreasing and how quickly the learning rate decreases. in the experiment,
we set   0 to coincide with the time when the validation error starts increasing for the    rst time.

max(0,       0)

1+

1

  

8

worse than the result obtained by id56s trained with the technique of fast dropout (fd) which are
3.09 and 8.01, respectively (bayer et al., 2013).
in order to quickly investigate whether the proposed deeper variants of id56s may also bene   t from
the recent advances in feedforward neural networks, such as the use of non-saturating activation
functions2 and the method of dropout. we have built another set of dot(s)-id56s that have the
recently proposed lp units (gulcehre et al., 2013) in deep transition and maxout units (goodfellow
et al., 2013) in deep output function. furthermore, we used the method of dropout (hinton et al.,
2012b) instead of weight noise during training. similarly to the previously trained models, we
searched for the size of the models as well as other learning hyperparameters that minimize the
validation performance. we, however, did not pretrain these models.
the results obtained by the dot(s)-id56s having lp and maxout units trained with dropout are
shown in the last column of tab. 2. on every music dataset the performance by this model is sig-
ni   cantly better than those achieved by all the other models as well as the best results reported with
recurrent neural networks in (bayer et al., 2013). this suggests us that the proposed variants of deep
id56s also bene   t from having non-saturating activations and using dropout, just like feedforward
neural networks. we reported these results and more details on the experiment in (gulcehre et al.,
2013).
we, however, acknowledge that the model-free state-of-the-art results for the both datasets were
obtained using an id56 combined with a conditional generative model, such as restricted boltz-
mann machines or neural autoregressive distribution estimator (larochelle and murray, 2011), in
the output (boulanger-lewandowski et al., 2012).

character-level

word-level

id56 dt(s)-id56 dot(s)-id56 sid56
1.412
1.414
117.7
110.0

1.386
107.5

1.409
112.0

   
1.411
1232

(cid:63)

1.243
1173

table 3: the performances of the four types of id56s on the tasks of id38. the
numbers represent bit-per-character and perplexity computed on test sequence, respectively, for
the character-level and word-level modeling tasks.     the previous/current state-of-the-art results
obtained with shallow id56s. (cid:63) the previous/current state-of-the-art results obtained with id56s
having long-short term memory units.

5.3.2 id38

on tab. 3, we can see the perplexities on the test set achieved by the all four models. we can clearly
see that the deep id56s (dt(s)-id56, dot(s)-id56 and sid56) outperform the conventional, shal-
low id56 signi   cantly. on these tasks dot(s)-id56 outperformed all the other models, which
suggests that it is important to have highly nonlinear mapping from the hidden state to the output in
the case of id38.
the results by both the dot(s)-id56 and the sid56 for word-level modeling surpassed the previous
best performance achieved by an id56 with 1000 long short-term memory (lstm) units (graves,
2013) as well as that by a shallow id56 with a larger hidden state (mikolov et al., 2011), even when
both of them used dynamic evaluation3. the results we report here are without dynamic evaluation.
for character-level modeling the state-of-the-art results were obtained using an optimization method
hessian-free with a speci   c type of id56 architecture called mid56 (mikolov et al., 2012a) or
a id173 technique called adaptive weight noise (graves, 2013). our result, however, is
better than the performance achieved by conventional, shallow id56s without any of those advanced

2 note that it is not trivial to use non-saturating id180 in conventional id56s, as this may cause
the explosion of the activations of hidden states. however, it is perfectly safe to use non-saturating activation
functions at the intermediate layers of a deep id56 with deep transition.

1reported by mikolov et al. (2012a) using mid56 with hessian-free optimization technique.
2reported by mikolov et al. (2011) using the dynamic evaluation.
3reported by graves (2013) using the dynamic evaluation and weight noise.
3dynamic evaluation refers to an approach where the parameters of a model are updated as the valida-

tion/test data is predicted.

9

id173 methods (mikolov et al., 2012b), where they reported the best performance of 1.41
using an id56 trained with the hessian-free learning algorithm (martens and sutskever, 2011).

6 discussion

in this paper, we have explored a novel approach to building a deep recurrent neural network (id56).
we considered the structure of an id56 at each timestep, which revealed that the relationship be-
tween the consecutive hidden states and that between the hidden state and output are shallow. based
on this observation, we proposed two alternative designs of deep id56 that make those shallow re-
lationships be modeled by deep neural networks. furthermore, we proposed to make use of shortcut
connections in these deep id56s to alleviate a problem of dif   cult learning potentially introduced
by the increasing depth.
we empirically evaluated the proposed designs against the conventional id56 which has only a
single hidden layer and against another approach of building a deep id56 (stacked id56, graves,
2013), on the task of polyphonic music prediction and id38.
the experiments revealed that the id56 with the proposed deep transition and deep output (dot(s)-
id56) outperformed both the conventional id56 and the stacked id56 on the task of language
modeling, achieving the state-of-the-art result on the task of word-level id38. for
polyphonic music prediction, a different deeper variant of an id56 achieved the best performance
for each dataset. importantly, however, in all the cases, the conventional, shallow id56 was not able
to outperform the deeper variants. these results strongly support our claim that an id56 bene   ts
from having a deeper architecture, just like feedforward neural networks.
the observation that there is no clear winner in the task of polyphonic music prediction suggests
us that each of the proposed deep id56s has a distinct characteristic that makes it more, or less,
suitable for certain types of datasets. we suspect that in the future it will be possible to design and
train yet another deeper variant of an id56 that combines the proposed models together to be more
robust to the characteristics of datasets. for instance, a stacked dt(s)-id56 may be constructed by
combining the dt(s)-id56 and the sid56.
in a quick additional experiment where we have trained dot(s)-id56 constructed using non-
saturating nonlinear id180 and trained with the method of dropout, we were able to
improve the performance of the deep recurrent neural networks on the polyphonic music prediction
tasks signi   cantly. this suggests us that it is important to investigate the possibility of applying
recent advances in feedforward neural networks, such as novel, non-saturating id180
and the method of dropout, to recurrent neural networks as well. however, we leave this as future
research.
one practical issue we ran into during the experiments was the dif   culty of training deep id56s. we
were able to train the conventional id56 as well as the dt(s)-id56 easily, but it was not trivial to
train the dot(s)-id56 and the stacked id56. in this paper, we proposed to use shortcut connections
as well as to pretrain them either with the conventional id56 or with the dt(s)-id56. we, however,
believe that learning may become even more problematic as the size and the depth of a model
increase. in the future, it will be important to investigate the root causes of this dif   culty and to
explore potential solutions. we    nd some of the recently introduced approaches, such as advanced
id173 methods (pascanu et al., 2013a) and advanced optimization algorithms (see, e.g.,
pascanu and bengio, 2013; martens, 2010), to be promising candidates.

acknowledgments

we would like to thank the developers of theano (bergstra et al., 2010; bastien et al., 2012). we
also thank justin bayer for his insightful comments on the paper. we would like to thank nserc,
compute canada, and calcul qu  ebec for providing computational resources. razvan pascanu is
supported by a deepmind fellowship. kyunghyun cho is supported by fics (finnish doctoral
programme in computational sciences) and    the academy of finland (finnish centre of excellence
in computational id136 research coin, 251170)   .

10

references
bastien, f., lamblin, p., pascanu, r., bergstra, j., goodfellow, i. j., bergeron, a., bouchard, n.,
and bengio, y. (2012). theano: new features and speed improvements. deep learning and
unsupervised id171 nips 2012 workshop.

bayer, j., osendorfer, c., korhammer, d., chen, n., urban, s., and van der smagt, p. (2013). on

fast dropout and its applicability to recurrent networks. arxiv:1311.0701 [cs.ne].

bengio, y. (2009). learning deep architectures for ai. found. trends mach. learn., 2(1), 1   127.
bengio, y., simard, p., and frasconi, p. (1994). learning long-term dependencies with gradient

descent is dif   cult. ieee transactions on neural networks, 5(2), 157   166.

bengio, y., mesnil, g., dauphin, y., and rifai, s. (2013). better mixing via deep representations.

in icml   13.

bergstra, j., breuleux, o., bastien, f., lamblin, p., pascanu, r., desjardins, g., turian, j., warde-
in

farley, d., and bengio, y. (2010). theano: a cpu and gpu math expression compiler.
proceedings of the python for scienti   c computing conference (scipy). oral presentation.

boulanger-lewandowski, n., bengio, y., and vincent, p. (2012). modeling temporal dependencies
in high-dimensional sequences: application to polyphonic music generation and transcription. in
icml   2012.

chen, j. and deng, l. (2013). a new method for learning deep recurrent neural networks.

arxiv:1311.6091 [cs.lg].

delalleau, o. and bengio, y. (2011). shallow vs. deep sum-product networks. in nips.
el hihi, s. and bengio, y. (1996). hierarchical recurrent neural networks for long-term dependen-

cies. in nips 8. mit press.

glorot, x., bordes, a., and bengio, y. (2011a). deep sparse recti   er neural networks. in aistats.
glorot, x., bordes, a., and bengio, y. (2011b). id20 for large-scale sentiment classi-

   cation: a deep learning approach. in icml   2011.

goodfellow, i., le, q., saxe, a., and ng, a. (2009). measuring invariances in deep networks. in

nips   09, pages 646   654.

goodfellow, i. j., warde-farley, d., mirza, m., courville, a., and bengio, y. (2013). maxout

networks. in icml   2013.

graves, a. (2011). practical variational id136 for neural networks. in j. shawe-taylor, r. zemel,
p. bartlett, f. pereira, and k. weinberger, editors, advances in neural information processing
systems 24, pages 2348   2356.

graves, a. (2013). generating sequences with recurrent neural networks. arxiv:1308.0850

[cs.ne].

graves, a., liwicki, m., fernandez, s., bertolami, r., bunke, h., and schmidhuber, j. (2009). a
novel connectionist system for improved unconstrained handwriting recognition. ieee transac-
tions on pattern analysis and machine intelligence.

graves, a., mohamed, a., and hinton, g. (2013). id103 with deep recurrent neural

networks. icassp.

gulcehre, c., cho, k., pascanu, r., and bengio, y. (2013). learned-norm pooling for deep feedfor-

ward and recurrent neural networks. arxiv:1311.1780 [cs.ne].

hermans, m. and schrauwen, b. (2013). training and analysing deep recurrent neural networks. in

advances in neural information processing systems 26, pages 190   198.

hinton, g., deng, l., dahl, g. e., mohamed, a., jaitly, n., senior, a., vanhoucke, v., nguyen, p.,
sainath, t., and kingsbury, b. (2012a). deep neural networks for acoustic modeling in speech
recognition. ieee signal processing magazine, 29(6), 82   97.

hinton, g. e. and salakhutdinov, r. (2006). reducing the dimensionality of data with neural net-

works. science, 313(5786), 504   507.

hinton, g. e., srivastava, n., krizhevsky, a., sutskever, i., and salakhutdinov, r. (2012b). im-
proving neural networks by preventing co-adaptation of feature detectors. technical report,
arxiv:1207.0580.

11

hornik, k., stinchcombe, m., and white, h. (1989). multilayer feedforward networks are universal

approximators. neural networks, 2, 359   366.

jaeger, h. (2007). discovering multiscale dynamical features with hierarchical echo state networks.

technical report, jacobs university.

ko, j. and dieter, f. (2009). gp-bayes   lters: bayesian    ltering using gaussian process prediction

and observation models. autonomous robots.

larochelle, h. and murray, i. (2011). the neural autoregressive distribution estimator. in pro-
ceedings of the fourteenth international conference on arti   cial intelligence and statistics (ais-
tats   2011), volume 15 of jmlr: w&cp.

le roux, n. and bengio, y. (2010). id50 are compact universal approximators.

neural computation, 22(8), 2192   2207.

marcus, m. p., marcinkiewicz, m. a., and santorini, b. (1993). building a large annotated corpus

of english: the id32. computational linguistics, 19(2), 313   330.

martens, j. (2010). deep learning via hessian-free optimization. in l. bottou and m. littman, ed-
itors, proceedings of the twenty-seventh international conference on machine learning (icml-
10), pages 735   742. acm.

martens, j. and sutskever, i. (2011). learning recurrent neural networks with hessian-free opti-

mization. in proc. icml   2011. acm.

mikolov, t. (2012). statistical language models based on neural networks. ph.d. thesis, brno

university of technology.

mikolov, t., kara     at, m., burget, l., cernocky, j., and khudanpur, s. (2010). recurrent neural
network based language model. in proceedings of the 11th annual conference of the international
speech communication association (interspeech 2010), volume 2010, pages 1045   1048.
international speech communication association.

mikolov, t., kombrink, s., burget, l., cernocky, j., and khudanpur, s. (2011). extensions of recur-
rent neural network language model. in proc. 2011 ieee international conference on acoustics,
speech and signal processing (icassp 2011).

mikolov, t., sutskever, i., deoras, a., le, h., kombrink, s., and cernocky, j. (2012a). subword

id38 with neural networks. unpublished.

mikolov, t., sutskever,

i., deoras, a., le, h.-s., kombrink, s.,

(2012b).

j.
(http://www.   t.vutbr.cz/ imikolov/id56lm/char.pdf).

id38 with

subword

neural

networks.

and cernocky,
preprint

mikolov, t., sutskever, i., chen, k., corrado, g., and dean, j. (2013a). distributed representations
of words and phrases and their compositionality. in advances in neural information processing
systems 26, pages 3111   3119.

mikolov, t., chen, k., corrado, g., and dean, j. (2013b). ef   cient estimation of word represen-
tations in vector space. in international conference on learning representations: workshops
track.

pascanu, r. and bengio, y. (2013). revisiting natural gradient for deep networks. technical report,

arxiv:1301.3584.

pascanu, r., mikolov, t., and bengio, y. (2013a). on the dif   culty of training recurrent neural

networks. in icml   2013.

pascanu, r., montufar, g., and bengio, y. (2013b). on the number of response regions of deep feed

forward networks with piece-wise linear activations. arxiv:1312.6098[cs.lg].

pinheiro, p. and collobert, r. (2014). recurrent convolutional neural networks for scene labeling.

in proceedings of the 31st international conference on machine learning, pages 82   90.

raiko, t., valpola, h., and lecun, y. (2012). deep learning made easier by linear transformations in
id88s. in proceedings of the fifteenth internation conference on arti   cial intelligence and
statistics (aistats 2012), volume 22 of jmlr workshop and conference proceedings, pages
924   932. jmlr w&cp.

rumelhart, d. e., hinton, g. e., and williams, r. j. (1986). learning representations by back-

propagating errors. nature, 323, 533   536.

12

schmidhuber, j. (1992). learning complex, extended sequences using the principle of history com-

pression. neural computation, (4), 234   242.

sutskever, i., martens, j., and hinton, g. (2011). generating text with recurrent neural networks. in
l. getoor and t. scheffer, editors, proceedings of the 28th international conference on machine
learning (icml 2011), pages 1017   1024, new york, ny, usa. acm.

sutskever, i., martens, j., dahl, g., and hinton, g. (2013). on the importance of initialization and

momentum in deep learning. in icml.

valpola, h. and karhunen, j. (2002). an unsupervised id108 method for nonlinear

dynamic state-space models. neural comput., 14(11), 2647   2692.

13

