video description using bidirectional recurrent

neural networks

  alvaro peris1, marc bola  nos2,3, petia radeva2,3, and francisco casacuberta1

1 prhlt research center, universitat polit`ecnica de val`encia, valencia (spain)

2 universitat de barcelona, barcelona (spain)
3 id161 center, bellaterra (spain)

{lvapeab,fcn}@prhlt.upv.es

{marc.bolanos,petia.ivanova}@ub.edu

abstract. although traditionally used in the machine translation    eld,
the encoder-decoder framework has been recently applied for the genera-
tion of video and image descriptions. the combination of convolutional
and recurrent neural networks in these models has proven to outper-
form the previous state of the art, obtaining more accurate video descrip-
tions. in this work we propose pushing further this model by introducing
two contributions into the encoding stage. first, producing richer im-
age representations by combining object and location information from
convolutional neural networks and second, introducing bidirectional
recurrent neural networks for capturing both forward and backward
temporal relationships in the input frames.

keywords: video description, id4, birectional
recurrent neural networks, lstm, convolutional neural networks.

1

introduction

automatic generation of image descriptions is a recent trend in computer vi-
sion that represents an interesting, but di   cult task. this has been possible due
to the dramatic advances in convolutional neural network (id98) models that
allowed to outperform the state-of-the-art algorithms in many id161
problems: object recognition, id164, activity recognition, etc. gener-
ating descriptions of videos represents an even more challenging task that could
lead to multiple applications (e.g. video indexing and retrieval, movie description
for multimedia applications or for blind people or id176). we
can imagine the amount of data generated in youtube (every day people watch
hundreds of millions of hours of video and generate billions of views) and how
video description would help to categorize them, provide an e   cient retrieval
mechanism and serve as a summarization tool for blind people.

however, the problem of video description generation has several proper-
ties that make it specially di   cult. besides the signi   cant amount of image
information to analyze, videos may have a variable number of images and can
be described with sentences of di   erent length. furthermore, the descriptions

6
1
0
2
 
c
e
d
2
1

 

 
 
]

v
c
.
s
c
[
 
 

2
v
0
9
3
3
0

.

4
0
6
1
:
v
i
x
r
a

2

  a. peris, m. bola  nos, p. radeva and f. casacuberta

of videos use to be high-level summaries that not necessarily are expressed in
terms of the objects, actions and scenes observed in the images. there are many
open research questions in this    eld requiring deep video understanding. some
of them are how to e   ciently extract important elements from the images (e.g.
objects, scenes, actions), to de   ne the local (e.g.    ne-grained motion) and global
spatio-temporal information, determine the salient content worth to describe,
and generate the    nal video description. all these speci   c questions need the
attention of id161, machine translation and natural language under-
standing communities in order to be solved.

in this work, we propose to enrich the state-of-the-art architecture using
bidirectional neural networks for modeling relationships in two temporal direc-
tions. furthermore, we test the inclusion of supplementary features, which help
to detect contextual information from the scene where the video takes place.

2 related work

although the problem of video captioning recently appeared thanks to the new
learning capabilities o   ered by deep learning techniques, the general pipeline
adopted in these works resembles the traditional encoder-decoder methodology
used in machine translation (mt). the main di   erence is that, in the encoder
step, instead of generating a compact representation of the source language sen-
tence, we generate a representation of the images belonging to the video.

mt aims to automatically translate text or speech from a source to a tar-
get language. within the last decades, the prevailing approach is the statistical
one [5]. the application of connectionist models in the area has drawn much
the attention of researchers in the last years. moreover, a new approach to mt
has been recently proposed: the so-called id4, where
the translation process is carried out by a means of a large recurrent neural
network (id56) [11]. these systems rely on the encoder-decoder framework: an
encoder id56 produces a compact representation of an input sentence in the
source language, and the decoder id56 takes this representation and generates
the corresponding target language sentence. both id56s usually make use of
gated units, such as the popular long short-term memory (lstm) [4], in order
to cope with long-term relationships.

the recent reintroduction of deep learning in the id161    eld
through id98s [6], has allowed to obtain new and richer image representations
compared to the traditional hand-crafted ones. these networks have demon-
strated to be a powerful tool to extract feature representations for several kinds of
id161 problems like on objects [10] or scenes [18] recognition. thanks
to the id98s ability to serve as knowledge transfer mechanisms, they have also
been usually used as feature extractors.

the majority of the works devoted to generate textual descriptions from
single images also follow the encoder-decoder architecture. in the encoding stage,
they apply a combination of id98 and lstm for describing the input image.
in the decoding stage, an lstm is in charge of receiving the image information

video description using id182

3

and generating, word by word, a    nal description of the image [15]. the problem
of video captioning is similar. seminal works applied methodologies inspired by
classical mt [9]. nevertheless, more recent works following the encoder-decoder
approach, obtained state-of-the-art performances [14,16].

we present a new methodology for natural language video description that
makes use of deeper structures and a double-way analysis of the input video.
we propose to use as a base architecture the one introduced in [16]. on the top
of it, our contributions are twofold. first, we produce richer image representa-
tions by combining complementary id98s for detecting objects and contextual
information from the input images. second, we introduce a bidirectional lstm
(blstm) network in the encoding stage, which has the ability to learn forward
and backward long-term relationships on the input sequence. moreover, we make
our code public1.

3 methodology

an overview of our proposal is depicted in fig. 1. we propose an encoder-decoder
approach consisting of four stages, using both id98s and lstms for describing
images and for modeling their temporal relationship, respectively.

fig. 1: general scheme of our proposed methodology.

first (blue in the scheme), we apply two state of the art id98 models for

extracting complementary features on each of the raw images from the video.

second (red in the scheme), considering we need to describe the actions
performed in consecutive frames, we apply a blstm for capturing temporal
relationships and complementary information by taking a look at the action in
a forward and in a backward manner.

third (yellow in the scheme), the two output vectors from forward and back-
ward lstm models of the previous step are concatenated together with the
id98 output for each image and are fed to a soft attention model in the decoder.
this model decides on which parts of the input video should focus for emitting
the next word, considering the description generated so far.

fourth (green in the scheme), an lstm network generates the video caption
from the representation obtained in previous stages. the variable-length caption

1 https://github.com/lvapeab/abivirnet

id98(    )id98(    )id98(     )...lstm j=1lstm j=jlstm j=2lstm j=jlstm j=2lstm j=1......lstm t=1lstm t=2......soft attentionmodellstm t=tencoderdecoder...twoelephantswater......4

  a. peris, m. bola  nos, p. radeva and f. casacuberta

is obtained word by word, using a softmax function on the top of this lstm
network.

3.1 encoder

given the video description problem,
in the encoding stage we need to prop-
erly characterize the video for 1) un-
derstanding which kind of objects and
structures appear in the images, and
2) modeling their relationships and
actions along time.

for tackling the    rst part of the
problem, several kinds of pretrained
id98s may be used for describing
the images, which can be distin-
guished by the di   erent architectures
or by the di   erent datasets used for
training. although an extended com-
parison and combinations of mod-
els could be used for applying this
characterization, we propose combin-
ing object and context-related in-
formation. for this purpose we use
a googlenet architecture [12] sepa-
rately trained on two datasets, one for
objects (ilsvrc dataset [10]), and
the other for scenes (places 205 [18]).
the combination of these two kinds of data can inform about the objects ap-
pearing and their surroundings, being ideal for the problem at hand. for a given
video, the id98s generate a sequence vc of j d-dimensional feature vectors,

fig. 2: forward layer lstm unit for the
encoder. the output depends on the pre-
vious hidden state (vf
j   1) and the cur-
rent feature vector from the video ex-
tracted by the id98 (xj). input, output
and forget gates module the amount of
information that    ows across the unit.

x1, . . . , xj with xj     rd for 1     j     j, where j is the number of frames in the

video.

to solve the second problem, a blstm processes the sequence vc, generating
a new sequence vbi = v1, . . . , vj of j vectors. bid137 are composed
of two independent lstm layers namely, forward and backward. both layers are
analogue, but the latter processes the input sequence reversed in time.

j be the forward layer hidden state at the time-step j, and let cf

id137 have, in addition to the classical hidden state, a memory
state. let vf
j be
its memory state. the hidden state vf
j controlled by an output
gate of
j. the current memory state depends on an updated memory state, and
on the previous memory state, cf
j   1, respectively modulated by the forget and
input gates, f f
j is obtained by applying
a logistic non-linear function to the input and the previous hidden state. each
lstm gate has associated two weight matrices, accounting for the input and

j. the updated memory state   cf

j is computed as cf

j and if

inputgatexjvfj   1forgetgateifjtanhxjvfj   1  cfj(cid:12)(cid:12)ffjcfj   1+cfjvfj(cid:12)gateoutputxjvfj   1ofj   1xjvfj   1video description using id182

5

the previous hidden state. such matrices must be estimated on a training set.
figure 2 shows an illustration of an lstm unit. the same architecture applies
to the backward layer, but dependencies    ow from the next time-step to the
previous one. since forward and backward layers are independent, they have
di   erent weight matrices to estimate.

each feature vector vj computed by the blstm results as the concatenation

of the forward and backward hidden states: vj = [vf
being d the size of each forward and backward hidden state.

j; vb

j ]     r2  d for 1     j     j,

finally, the encoder combines the sequences vc and vbi by concatenating
the vectors from the id98 and from the blstm, producing a    nal sequence v

of j feature vectors w1, . . . , wj , wj = [xj; vj]     rd+2  d for 1     j     j.

3.2 decoder

the decoder is an lstm network,
which acts as a language model,
conditioned by the information pro-
vided by the encoder. this network
is equipped with an attention mech-
anism [1,16]: a soft alignment model,
implemented as a single-layered per-
ceptron, that helps the decoder to
know where to look at for generat-
ing each output word. given the se-
quence v generated by the encoder,
at each decoding time-step t the at-
tention mechanism weights the j fea-
ture vectors and combines them into

a single context vector zt     rd+2  d.

the decoder lstm is de   ned sim-
ilarly to the forward layer from the
encoder, but it takes into account
the previously generated word and
the context vector from the attention
mechanism, in addition to its previous hidden state. the last word representa-

fig. 3: decoder lstm unit. the output
depends on the previous hidden state
(ht), the id27 of the previ-
ously generated word (e(yt   1)) and the
context vector provided by the attention
mechanism (zt).

tion is provided by a id27 matrix e     rm  v , being m the size of

the id27 and v the size of the vocabulary. e is estimated together
with the rest of the model parameters.

a id203 distribution over the vocabulary of output words is de   ned from
the hidden state ht, by means of a softmax function. this function represents
the id155 of a word given an input video v and its history (the
previously generated words): p(yt|y1, . . . , yt   1, v). following [11], a beam-search
method is used to    nd the caption with highest id155.

inputgateztht   1forgetgateittanhztht   1  ct(cid:12)(cid:12)ftct   1+ctht(cid:12)gateoutpute(yt   1)e(yt   1)ztht   1e(yt   1)ztht   1e(yt   1)ot   16

  a. peris, m. bola  nos, p. radeva and f. casacuberta

4 results

in this section we describe the datasets and metrics used for evaluating and
comparing our model to the video captioning state of the art.

4.1 dataset

the microsoft research video description corpus (msvd) [2] is a dataset
composed of 1970 open domain clips collected from youtube and annotated us-
ing a crowd sourcing platform. each video has a variable number of captions,
written by di   erent users. we used the splits made by [14,16], separating the
dataset in 1200 videos for training, 100 for validation and the remaining 670
for testing. during training, the clips and each of their captions were treated
separately, accounting for a total of more than 80, 000 training samples.

4.2 id74

in order to evaluate and compare the results of the di   erent models we used
the standardized coco-caption evaluation package [3], which provides several
metrics for text description comparison. we used three main metrics, all of them
presented from 0 (minimum quality) to 100 (maximum quality):

id7 [8]: this metric compares the ratio of id165 structures that are

shared between the system hypotheses and the reference sentences.

meteor [7]: it was introduced to solve the lack of the recall component
when computing id7. it computes the f1 score of precision and recall between
hypotheses and references.

cider [13]: similarly to id7, it computes the number of matching n-

grams, but penalizes any id165 frequently found in the whole training set.

4.3 experimental results

on all the tests we used a batch size of 64, the learning rate was automatically set
by the adadelta [17] method and, as the authors in [16] reported, we applied a
frame subsampling, picking only one image every 26 frames for reducing the com-
putational load. the parameters of the network were randomly initialized. an
evaluation on the validation set was performed every 1000 updates. the learning
process was stopped when the reported error increased after 5 evaluations.

for each con   guration we run 10 experiments. at each of them, we ran-
domly set the value of the critical model hyperparameters. such hyperparame-
ters and their tested ranges are m     [300, 700], |ht|     [1000, 3000]. when using
the blstm encoder, we performed an additional selection on |vj|     [100, 2100].
for each con   guration, the best model with respect to the id7 measure
on the validation set was selected. in table 1 we report the results of the best
models on the test set. the    rst row correspond to the result obtained with our
system with the object features from [16]. the con   gurations reported below

video description using id182

7

model

id7 [%] meteor [%] cider [%]

objects*
objects + blstm
objects + scenes
objects + scenes + blstm

51.5
53.6
52.6
52.8

32.5
32.6
32.5
31.3

66.0
66.4
67.0
67.2

table 1. text generation results for each model on the msvd dataset. the results
below the horizontal line are our proposals. *model from [16] only with object features
evaluated on our system.

the horizontal line are our proposals, where scenes indicates we use scene-related
features concatenated to objects and blstm denotes the use of the additional
blstm encoder.

5 discussion and conclusions

analyzing the obtained results, a clear improvement trend can be derived when
applying the blstm as a temporal id136 mechanism. the blstm addition
when using objects features allows to improve the result on all metrics, obtain-
ing a bene   t of more than 2 id7 points. adding scenes-related features also
slightly improves the result, although it is not as remarkable as the blstm im-
provement. the combination of objects+scenes+blstm o   ers the best cider
performance, nevertheless, this result is slightly below the objects+blstm one
on the other metrics. this behaviour is probably due to the signi   cant increase
on the number of parameters to learn. it should be investigated whether the re-
duction of the number of parameters by reducing the size of the id98 features,
or the use of larger datasets could lead to further improvements.

in conclusion, we have presented a new methodology for natural language
video description that takes pro   t from a bidirectional analysis of the input
sequence. this architecture has the ability to learn forward and backward long-
term relationships on the input images. additionally, the use of complementary
object and scene-related image features has proved to obtain a richer video
representation. the improvements have allowed the method to outperform the
state-of-the-art results in the problem at hand.

these results suggest that deep structures help to transfer the knowledge
from the input sequence of frames to the output natural language caption. hence,
the next step to take must delve into the application deeper modeling structures,
such as 3d id98s and multi-layered id137.

acknowledgments. this work was partially founded by tin2012-38187-c03-
01, sgr 1219, prometeoii/2014/030 and by a travel grant by the miprcv
network. p. radeva is partially supported by an icrea academia2014 grant.
we acknowledge nvidia for the donation of a gpu used in this work.

8

  a. peris, m. bola  nos, p. radeva and f. casacuberta

references

1. dzmitry bahdanau, kyunghyun cho, and yoshua bengio. neural machine trans-
lation by jointly learning to align and translate. in proceedings of the international
conference on learning representations (arxiv:1409.0473), 2015.

2. david l chen and william b dolan. collecting highly parallel data for para-
phrase evaluation. in proceedings of the 49th annual meeting of the association
for computational linguistics (vol. 1), pages 190   200, 2011.

3. xinlei chen, hao fang, tsung-yi lin, ramakrishna vedantam, saurabh gupta,
piotr doll  ar, and c lawrence zitnick. microsoft coco captions: data collection
and evaluation server. arxiv preprint arxiv:1504.00325, 2015.

4. sepp hochreiter and j  urgen schmidhuber. long short-term memory. neural com-

putation, 9(8):1735   1780, 1997.

5. philipp koehn. id151. cambridge university press, 2010.
6. alex krizhevsky, ilya sutskever, and geo   rey e hinton. id163 classi   cation
with deep convolutional neural networks. in advances in neural information pro-
cessing systems, pages 1097   1105, 2012.

7. alon lavie and michael j denkowski. the meteor metric for automatic evaluation

of machine translation. machine translation, 23(2-3):105   115, 2009.

8. kishore papineni, salim roukos, todd ward, and wei-jing zhu. id7: a method
for automatic evaluation of machine translation. in proceedings of the 40th annual
meeting on association for computational linguistics, pages 311   318, 2002.

9. marcus rohrbach, wei qiu, ivan titov, stefan thater, manfred pinkal, and bernt
schiele. translating video content to natural language descriptions. in proceedings
of the ieee international conference on id161 and pattern recogni-
tion, pages 433   440, 2013.

10. olga russakovsky, jia deng, hao su, jonathan krause, sanjeev satheesh, sean
ma, zhiheng huang, andrej karpathy, aditya khosla, michael bernstein, alexan-
der c. berg, and li fei-fei. id163 large scale visual recognition challenge.
international journal of id161, 115(3):211   252, 2015.

11. ilya sutskever, oriol vinyals, and quoc v le. sequence to sequence learning
in advances in neural information processing systems,

with neural networks.
volume 27, pages 3104   3112. 2014.

12. christian szegedy, wei liu, yangqing jia, pierre sermanet, scott reed, dragomir
anguelov, dumitru erhan, vincent vanhoucke, and andrew rabinovich. going
deeper with convolutions. in proceedings of the ieee conference on computer
vision and pattern recognition, pages 1   9, 2015.

13. ramakrishna vedantam, c lawrence zitnick, and devi parikh. cider: consensus-
in proceedings of the ieee conference on

based image description evaluation.
id161 and pattern recognition, pages 4566   4575, 2015.

14. subhashini venugopalan, marcus rohrbach, je   rey donahue, raymond mooney,
trevor darrell, and kate saenko. sequence to sequence-video to text. in proceed-
ings of the ieee international conference on id161, pages 4534   4542,
2015.

15. oriol vinyals, alexander toshev, samy bengio, and dumitru erhan. show and
tell: a neural image caption generator. in proceedings of the ieee conference on
id161 and pattern recognition, pages 3156   3164, 2015.

16. li yao, atousa torabi, kyunghyun cho, nicolas ballas, christopher pal, hugo
larochelle, and aaron courville. describing videos by exploiting temporal struc-
ture. in proceedings of the ieee international conference on id161,
pages 4507   4515, 2015.

video description using id182

9

17. matthew d zeiler. adadelta: an adaptive learning rate method. arxiv preprint

arxiv:1212.5701, 2012.

18. bolei zhou, agata lapedriza, jianxiong xiao, antonio torralba, and aude oliva.
learning deep features for scene recognition using places database. in advances
in neural information processing systems, pages 487   495, 2014.

