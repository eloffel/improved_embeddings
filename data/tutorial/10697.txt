generative adversarial text to image synthesis

scott reed, zeynep akata, xinchen yan, lajanugen logeswaran
bernt schiele, honglak lee
1 university of michigan, ann arbor, mi, usa (umich.edu)
2 max planck institute for informatics, saarbr  ucken, germany (mpi-inf.mpg.de)

reedscot1, akata2, xcyan1, llajan1
schiele2,honglak1

6
1
0
2

 

n
u
j
 

5

 
 
]
e
n
.
s
c
[
 
 

2
v
6
9
3
5
0

.

5
0
6
1
:
v
i
x
r
a

abstract

automatic synthesis of realistic images from text
would be interesting and useful, but current ai
systems are still far from this goal. however, in
recent years generic and powerful recurrent neu-
ral network architectures have been developed
to learn discriminative text feature representa-
tions. meanwhile, deep convolutional generative
adversarial networks (gans) have begun to gen-
erate highly compelling images of speci   c cat-
egories, such as faces, album covers, and room
interiors. in this work, we develop a novel deep
architecture and gan formulation to effectively
bridge these advances in text and image model-
ing, translating visual concepts from characters
to pixels. we demonstrate the capability of our
model to generate plausible images of birds and
   owers from detailed text descriptions.

1. introduction
in this work we are interested in translating text in the form
of single-sentence human-written descriptions directly into
image pixels. for example,    this small bird has a short,
pointy orange beak and white belly    or    the petals of this
   ower are pink and the anther are yellow   . the problem of
generating images from visual descriptions gained interest
in the research community, but it is far from being solved.
traditionally this type of detailed visual information about
an object has been captured in attribute representations -
distinguishing characteristics the object category encoded
into a vector (farhadi et al., 2009; kumar et al., 2009;
parikh & grauman, 2011; lampert et al., 2014), in partic-
ular to enable zero-shot visual recognition (fu et al., 2014;
akata et al., 2015), and recently for conditional image gen-
eration (yan et al., 2015).
while the discriminative power and strong generalization

proceedings of the 33 rd international conference on machine
learning, new york, ny, usa, 2016. jmlr: w&cp volume
48. copyright 2016 by the author(s).

figure 1. examples of generated images from text descriptions.
left: captions are from zero-shot (held out) categories, unseen
text. right: captions are from the training set.

properties of attribute representations are attractive, at-
tributes are also cumbersome to obtain as they may require
domain-speci   c knowledge.
in comparison, natural lan-
guage offers a general and    exible interface for describing
objects in any space of visual categories. ideally, we could
have the generality of text descriptions with the discrimi-
native power of attributes.
recently, deep convolutional and recurrent networks for
text have yielded highly discriminative and generaliz-
able (in the zero-shot learning sense) text representations
learned automatically from words and characters (reed
et al., 2016). these approaches exceed the previous state-
of-the-art using attributes for zero-shot visual recognition
on the caltech-ucsd birds database (wah et al., 2011),
and also are capable of zero-shot caption-based retrieval.
motivated by these works, we aim to learn a mapping di-
rectly from words and characters to image pixels.
to solve this challenging problem requires solving two sub-
problems:    rst, learn a text feature representation that cap-
tures the important visual details; and second, use these fea-

this small bird has a pink breast and crown, and black primaries and secondaries.the flower has petals that are bright pinkish purple with white stigmathis magnificent fellow is almost all black with a red crest, and white cheek patch.this white and yellow flower have thin white petals and a round yellow stamengenerative adversarial text to image synthesis

tures to synthesize a compelling image that a human might
mistake for real. fortunately, deep learning has enabled
enormous progress in both subproblems - natural language
representation and image synthesis - in the previous several
years, and we build on this for our current task.
however, one dif   cult remaining issue not solved by deep
learning alone is that the distribution of images conditioned
on a text description is highly multimodal, in the sense that
there are very many plausible con   gurations of pixels that
correctly illustrate the description. the reverse direction
(image to text) also suffers from this problem but learning
is made practical by the fact that the word or character se-
quence can be decomposed sequentially according to the
chain rule; i.e. one trains the model to predict the next
token conditioned on the image and all previous tokens,
which is a more well-de   ned prediction problem.
this conditional multi-modality is thus a very natural ap-
plication for id3 (goodfellow
et al., 2014), in which the generator network is optimized to
fool the adversarially-trained discriminator into predicting
that synthetic images are real. by conditioning both gen-
erator and discriminator on side information (also studied
by mirza & osindero (2014) and denton et al. (2015)), we
can naturally model this phenomenon since the discrimina-
tor network acts as a    smart    adaptive id168.
our main contribution in this work is to develop a sim-
ple and effective gan architecture and training strat-
egy that enables compelling text to image synthesis of
bird and    ower images from human-written descriptions.
we mainly use the caltech-ucsd birds dataset and the
oxford-102 flowers dataset along with    ve text descrip-
tions per image we collected as our evaluation setting. our
model is trained on a subset of training categories, and we
demonstrate its performance both on the training set cate-
gories and on the testing set, i.e.    zero-shot    text to image
synthesis. in addition to birds and    owers, we apply our
model to more general images and text descriptions in the
ms coco dataset (lin et al., 2014).
2. related work
key challenges in multimodal learning include learning
a shared representation across modalities, and to predict
missing data (e.g. by retrieval or synthesis) in one modal-
ity conditioned on another. ngiam et al. (2011) trained a
stacked multimodal autoencoder on audio and video sig-
nals and were able to learn a shared modality-invariant rep-
resentation. srivastava & salakhutdinov (2012) developed
a deep id82 and jointly modeled images and
text tags. sohn et al. (2014) proposed a multimodal con-
ditional prediction framework (hallucinating one modality
given the other) and provided theoretical justi   cation.
many researchers have recently exploited the capability of

deep convolutional decoder networks to generate realistic
images. dosovitskiy et al. (2015) trained a deconvolutional
network (several layers of convolution and upsampling) to
generate 3d chair renderings conditioned on a set of graph-
ics codes indicating shape, position and lighting. yang et al.
(2015) added an encoder network as well as actions to this
approach. they trained a recurrent convolutional encoder-
decoder that rotated 3d chair models and human faces con-
ditioned on action sequences of rotations. reed et al. (2015)
encode transformations from analogy pairs, and use a con-
volutional decoder to predict visual analogies on shapes,
video game characters and 3d cars.
id3 (goodfellow et al., 2014)
have also bene   ted from convolutional decoder networks,
for the generator network module. denton et al. (2015)
used a laplacian pyramid of adversarial generator and dis-
criminators to synthesize images at multiple resolutions.
this work generated compelling high-resolution images
and could also condition on class labels for controllable
generation. radford et al. (2016) used a standard convolu-
tional decoder, but developed a highly effective and stable
architecture incorporating batch id172 to achieve
striking image synthesis results.
the main distinction of our work from the conditional
gans described above is that our model conditions on text
descriptions instead of class labels. to our knowledge it
is the    rst end-to-end differentiable architecture from the
character level to pixel level. furthermore, we introduce a
manifold interpolation regularizer for the gan generator
that signi   cantly improves the quality of generated sam-
ples, including on held out zero shot categories on cub.
the bulk of previous work on multimodal learning from
images and text uses retrieval as the target task, i.e. fetch
relevant images given a text query or vice versa. how-
ever, in the past year, there has been a breakthrough in
using recurrent neural network decoders to generate text
descriptions conditioned on images (vinyals et al., 2015;
mao et al., 2015; karpathy & li, 2015; donahue et al.,
2015). these typically condition a long short-term mem-
ory (hochreiter & schmidhuber, 1997) on the top-layer
features of a deep convolutional network to generate cap-
tions using the ms coco (lin et al., 2014) and other cap-
tioned image datasets. xu et al. (2015) incorporated a re-
current visual attention mechanism for improved results.
other tasks besides conditional generation have been con-
sidered in recent work. ren et al. (2015) generate answers
to questions about the visual content of images. this ap-
proach was extended to incorporate an explicit knowledge
base (wang et al., 2015). zhu et al. (2015) applied se-
quence models to both text (in the form of books) and
movies to perform a joint alignment.

generative adversarial text to image synthesis

in contemporary work mansimov et al. (2016) generated
images from text captions, using a variational recurrent
autoencoder with attention to paint the image in multiple
steps, similar to draw (gregor et al., 2015). impressively,
the model can perform reasonable synthesis of completely
novel (unlikely for a human to write) text such as    a stop
sign is    ying in blue skies   , suggesting that it does not sim-
ply memorize. while the results are encouraging, the prob-
lem is highly challenging and the generated images are not
yet realistic, i.e., mistakeable for real. our model can in
many cases generate visually-plausible 64  64 images con-
ditioned on text, and is also distinct in that our entire model
is a gan, rather only using gan for post-processing.
building on ideas from these many previous works, we
develop a simple and effective approach for text-based
image synthesis using a character-level text encoder and
class-conditional gan. we propose a novel architecture
and learning strategy that leads to compelling visual re-
sults. we focus on the case of    ne-grained image datasets,
for which we use the recently collected descriptions for
caltech-ucsd birds and oxford flowers with 5 human-
generated captions per image (reed et al., 2016). we train
and test on class-disjoint sets, so that test performance can
give a strong indication of generalization ability which we
also demonstrate on ms coco images with multiple ob-
jects and various backgrounds.
3. background
in this section we brie   y describe several previous works
that our method is built upon.

3.1. id3
id3 (gans) consist of a gen-
erator g and a discriminator d that compete in a two-
player minimax game: the discriminator tries to distin-
guish real training data from synthetic images, and the gen-
erator tries to fool the discriminator. concretely, d and g
play the following game on v(d,g):

min
g

max
d

v (d, g) = ex   pdata(x)[log d(x)]+

(1)

ex   pz(z)[log(1     d(g(z)))]

goodfellow et al. (2014) prove that this minimax game has
a global optimium precisely when pg = pdata, and that un-
der mild conditions (e.g. g and d have enough capacity)
pg converges to pdata. in practice, in the start of training
samples from d are extremely poor and rejected by d with
high con   dence. it has been found to work better in prac-
tice for the generator to maximize log(d(g(z))) instead of
minimizing log(1     d(g(z))).
3.2. deep symmetric structured joint embedding
to obtain a visually-discriminative vector representation of
text descriptions, we follow the approach of reed et al.

n(cid:88)

n=1

1
n

(2016) by using deep convolutional and recurrent text en-
coders that learn a correspondence function with images.
the text classi   er induced by the learned correspondence
function ft is trained by optimizing the following struc-
tured loss:

   (yn, fv(vn)) +    (yn, ft(tn))

(2)

where {(vn, tn, yn) : n = 1, ..., n} is the training data set,
    is the 0-1 loss, vn are the images, tn are the correspond-
ing text descriptions, and yn are the class labels. classi   ers
fv and ft are parametrized as follows:

fv(v) = arg max

y   y

ft(t) = arg max

y   y

et   t (y)[  (v)t   (t))]
ev   v(y)[  (v)t   (t))]

(3)

(4)

where    is the image encoder (e.g. a deep convolutional
neural network),    is the text encoder (e.g. a character-
level id98 or lstm), t (y) is the set of text descriptions
of class y and likewise v(y) for images. the intuition here
is that a text encoding should have a higher compatibility
score with images of the correspondong class compared to
any other class and vice-versa.
to train the model a surrogate objective related to equa-
tion 2 is minimized (see akata et al. (2015) for details). the
resulting gradients are backpropagated through    to learn
a discriminative text encoder. reed et al. (2016) found that
different text encoders worked better for cub versus flow-
ers, but for full generality and robustness to typos and large
vocabulary, in this work we always used a hybrid character-
level convolutional-recurrent network.
4. method
our approach is to train a deep convolutional generative
adversarial network (dc-gan) conditioned on text fea-
tures encoded by a hybrid character-level convolutional-
recurrent neural network. both the generator network g
and the discriminator network d perform feed-forward in-
ference conditioned on the text feature.

4.1. network architecture
we use the following notation. the generator network is
denoted g : rz    rt     rd, the discriminator as d :
rd    rt     {0, 1}, where t is the dimension of the text
description embedding, d is the dimension of the image,
and z is the dimension of the noise input to g. we illustrate
our network architecture in figure 2.
in the generator g,    rst we sample from the noise prior
z     rz     n (0, 1) and we encode the text query t us-
ing text encoder   . the description embedding   (t) is    rst
compressed using a fully-connected layer to a small dimen-
sion (in practice we used 128) followed by leaky-relu and

generative adversarial text to image synthesis

figure 2. our text-conditional convolutional gan architecture. text encoding   (t) is used by both generator and discriminator. it is
projected to a lower-dimensions and depth concatenated with image feature maps for further stages of convolutional processing.

then concatenated to the noise vector z. following this, in-
ference proceeds as in a normal deconvolutional network:
we feed-forward it through the generator g; a synthetic im-
age   x is generated via   x     g(z,   (t)). image generation
corresponds to feed-forward id136 in the generator g
conditioned on query text and a noise sample.
in the discriminator d, we perform several layers of stride-
2 convolution with spatial batch id172 (ioffe &
szegedy, 2015) followed by leaky relu. we again reduce
the dimensionality of the description embedding   (t) in a
(separate) fully-connected layer followed by recti   cation.
when the spatial dimension of the discriminator is 4    4,
we replicate the description embedding spatially and per-
form a depth concatenation. we then perform a 1    1 con-
volution followed by recti   cation and a 4    4 convolution
to compute the    nal score from d. batch id172 is
performed on all convolutional layers.

4.2. matching-aware discriminator (gan-cls)
the most straightforward way to train a conditional gan
is to view (text, image) pairs as joint observations and train
the discriminator to judge pairs as real or fake. this type of
conditioning is naive in the sense that the discriminator has
no explicit notion of whether real training images match
the text embedding context.
however, as discussed also by (gauthier, 2015),
the
dynamics of learning may be different from the non-
conditional case. in the beginning of training, the discrim-
inator ignores the conditioning information and easily re-
jects samples from g because they do not look plausible.
once g has learned to generate plausible images, it must
also learn to align them with the conditioning information,
and likewise d must learn to evaluate whether samples
from g meet this conditioning constraint.
in naive gan, the discriminator observes two kinds of in-
puts: real images with matching text, and synthetic images
with arbitrary text. therefore, it must implicitly separate
two sources of error: unrealistic images (for any text), and

algorithm 1 gan-cls training algorithm with step size
  , using minibatch sgd for simplicity.
1: input: minibatch images x, matching text t, mis-

matching   t, number of training batch steps s

h       (t) {encode matching text description}
  h       (  t) {encode mis-matching text description}
z     n (0, 1)z {draw sample of random noise}
  x     g(z, h) {forward through generator}
sr     d(x, h) {real image, right text}
sw     d(x,   h) {real image, wrong text}
sf     d(  x, h) {fake image, right text}
ld     log(sr) + (log(1     sw) + log(1     sf ))/2
lg     log(sf )

2: for n = 1 to s do
3:
4:
5:
6:
7:
8:
9:
10:
11: d     d          ld/   d {update discriminator}
12:
13: g     g          lg/   g {update generator}
14: end for

realistic images of the wrong class that mismatch the con-
ditioning information. based on the intuition that this may
complicate learning dynamics, we modi   ed the gan train-
ing algorithm to separate these error sources. in addition
to the real / fake inputs to the discriminator during train-
ing, we add a third type of input consisting of real im-
ages with mismatched text, which the discriminator must
learn to score as fake. by learning to optimize image / text
matching in addition to the image realism, the discrimina-
tor can provide an additional signal to the generator.
algorithm 1 summarizes the training procedure. after en-
coding the text, image and noise (lines 3-5) we generate the
fake image (  x, line 6). sr indicates the score of associat-
ing a real image and its corresponding sentence (line 7), sw
measures the score of associating a real image with an ar-
bitrary sentence (line 8), and sf is the score of associating
a fake image with its corresponding text (line 9). note that
we use    ld/   d to indicate the gradient of d   s objective
with respect to its parameters, and likewise for g. lines
11 and 13 are meant to indicate taking a gradient step to
update network parameters.

this flower has small, round violet petals with a dark purple center    z ~ n(0,1)this flower has small, round violet petals with a dark purple centergenerator networkdiscriminator network  (t)x := g(z,  (t))d(x   ,  (t))generative adversarial text to image synthesis

4.3. learning with manifold interpolation (gan-int)
deep networks have been shown to learn representations
in which interpolations between embedding pairs tend to
be near the data manifold (bengio et al., 2013; reed et al.,
2014). motivated by this property, we can generate a large
amount of additional text embeddings by simply interpolat-
ing between embeddings of training set captions. critically,
these interpolated text embeddings need not correspond to
any actual human-written text, so there is no additional la-
beling cost. this can be viewed as adding an additional
term to the generator objective to minimize:

et1,t2   pdata [log(1     d(g(z,   t1 + (1       )t2)))]

(5)

where z is drawn from the noise distribution and    inter-
polates between text embeddings t1 and t2. in practice we
found that    xing    = 0.5 works well.
because the interpolated embeddings are synthetic, the dis-
criminator d does not have    real    corresponding image
and text pairs to train on. however, d learns to predict
whether image and text pairs match or not. thus, if d does
a good job at this, then by satisfying d on interpolated text
embeddings g can learn to    ll in gaps on the data manifold
in between training points. note that t1 and t2 may come
from different images and even different categories.1

4.4. inverting the generator for style transfer
if the text encoding   (t) captures the image content (e.g.
   ower shape and colors), then in order to generate a real-
istic image the noise sample z should capture style factors
such as background color and pose. with a trained gan,
one may wish to transfer the style of a query image onto
the content of a particular text description. to achieve this,
one can train a convolutional network to invert g to regress
from samples   x     g(z,   (t)) back onto z. we used a
simple squared loss to train the style encoder:

lstyle = et,z   n (0,1)||z     s(g(z,   (t)))||2

2

(6)

where s is the style encoder network. with a trained gen-
erator and style encoder, style transfer from a query image
x onto text t proceeds as follows:

s     s(x),   x     g(s,   (t))

where   x is the result image and s is the predicted style.
5. experiments
in this section we    rst present results on the cub dataset
of bird images and the oxford-102 dataset of    ower im-
ages. cub has 11,788 images of birds belonging to one of

1in our experiments, we used    ne-grained categories (e.g.
birds are similar enough to other birds,    owers to other    owers,
etc.), and interpolating across categories did not pose a problem.

200 different categories. the oxford-102 contains 8,189
images of    owers from 102 different categories.
as in akata et al. (2015) and reed et al. (2016), we split
these into class-disjoint training and test sets. cub has
150 train+val classes and 50 test classes, while oxford-102
has 82 train+val and 20 test classes. for both datasets, we
used 5 captions per image. during mini-batch selection for
training we randomly pick an image view (e.g. crop,    ip)
of the image and one of the captions.
for text features, we    rst pre-train a deep convolutional-
recurrent text encoder on structured joint embedding of
text captions with 1,024-dimensional googlenet image
embedings (szegedy et al., 2015) as described in subsec-
tion 3.2. for both oxford-102 and cub we used a hybrid
of character-level convnet with a recurrent neural network
(char-id98-id56) as described in (reed et al., 2016). note,
however that pre-training the text encoder is not a require-
ment of our method and we include some end-to-end results
in the supplement. the reason for pre-training the text en-
coder was to increase the speed of training the other com-
ponents for faster experimentation. we also provide some
qualitative results obtained with ms coco images of the
validation set to show the generalizability of our approach.
we used the same gan architecture for all datasets. the
training image size was set to 64    64    3. the text en-
coder produced 1, 024-dimensional embeddings that were
projected to 128 dimensions in both the generator and dis-
criminator before depth concatenation into convolutional
feature maps.
as indicated in algorithm 1, we take alternating steps of
updating the generator and the discriminator network. we
used the same base learning rate of 0.0002, and used the
adam solver (ba & kingma, 2015) with momentum 0.5.
the generator noise was sampled from a 100-dimensional
unit normal distribution. we used a minibatch size of 64
and trained for 600 epochs. our implementation was built
on top of dcgan.torch2.

5.1. qualitative results
we compare the gan baseline, our gan-cls with image-
text matching discriminator (subsection 4.2), gan-int
learned with text manifold interpolation (subsection 4.3)
and gan-int-cls which combines both.
results on cub can be seen in figure 3. gan and gan-
cls get some color information right, but the images do
not look real. however, gan-int and gan-int-cls
show plausible images that usually match all or at least part
of the caption. we include additional analysis on the ro-
bustness of each gan variant on the cub dataset in the
supplement.

2https://github.com/soumith/dcgan.torch

generative adversarial text to image synthesis

figure 3. zero-shot (i.e. conditioned on text from unseen test set categories) generated bird images using gan, gan-cls, gan-int
and gan-int-cls. we found that interpolation regularizer was needed to reliably achieve visually-plausible results.

figure 4. zero-shot generated    ower images using gan, gan-cls, gan-int and gan-int-cls. all variants generated plausible
images. although some shapes of test categories were not seen during training (e.g. columns 3 and 4), the color information is preserved.

results on the oxford-102 flowers dataset can be seen in
figure 4. in this case, all four methods can generate plau-
sible    ower images that match the description. the basic
gan tends to have the most variety in    ower morphology
(i.e. one can see very different petal types if this part is left
unspeci   ed by the caption), while other methods tend to
generate more class-consistent images. we speculate that
it is easier to generate    owers, perhaps because birds have
stronger structural regularities across species that make it
easier for d to spot a fake bird than to spot a fake    ower.
many additional results with gan-int and gan-int-
cls as well as gan-e2e (our end-to-end gan-int-cls
without pre-training the text encoder   (t)) for both cub
and oxford-102 can be found in the supplement.

5.2. disentangling style and content
in this section we investigate the extent to which our model
can separate style and content. by content, we mean the
visual attributes of the bird itself, such as shape, size and
color of each body part. by style, we mean all of the other
factors of variation in the image such as background color
and the pose orientation of the bird.
the text embedding mainly covers content information and
typically nothing about style, e.g. captions do not mention
the background or the bird pose. therefore, in order to
generate realistic images then gan must learn to use noise
sample z to account for style variations.
to quantify the degree of disentangling on cub we set up
two prediction tasks with noise z as the input: pose veri   -

a tiny bird, with a tiny beak, tarsus and feet, a blue crown, blue coverts, and black cheek patchthis small bird has a yellow breast, brown crown, and black superciliaryan all black bird with a distinct thick, rounded bill.this bird is different shades of brown all over with white and black spots on its head and backgan - clsgan - intgangan - int- clsthe gray bird has a light grey head and grey webbed feetgtgan - clsgan - intgangan - int - clsthis flower is white and pink in color, with petals that have veins.these flowers have petals that start off white in color and end in a dark purple towards the tips.bright droopy yellow petals with burgundy streaks, and a yellow stigma.a flower with long pink petals and raised orange stamen.the flower shown has a blue petals with a white pistil in the centergtgenerative adversarial text to image synthesis

figure 5. roc curves using cosine distance between predicted
style vector on same vs. different style image pairs. left: im-
age pairs re   ect same or different pose. right: image pairs re   ect
same or different average background color.

cation and background color veri   cation. for each task, we
   rst constructed similar and dissimilar pairs of images and
then computed the predicted style vectors by feeding the
image into a style encoder (trained to invert the input and
output of generator). if gan has disentangled style using
z from image content, the similarity between images of the
same style (e.g. similar pose) should be higher than that of
different styles (e.g. different pose).
to recover z, we inverted the each generator network as
described in subsection 4.4. to construct pairs for veri   ca-
tion, we grouped images into 100 clusters using id116
where images from the same cluster share the same style.
for background color, we clustered images by the average
color (rgb channels) of the background; for bird pose, we
clustered images by 6 keypoint coordinates (beak, belly,
breast, crown, forehead, and tail).
for evaluation, we compute the actual predicted style vari-
ables by feeding pairs of images style encoders for gan,
gan-cls, gan-int and gan-int-cls. we verify the
score using cosine similarity and report the au-roc (aver-
aging over 5 folds). as a baseline, we also compute cosine
similarity between text features from our text encoder.
we present results on figure 5. as expected, captions alone
are not informative for style prediction. moreover, con-
sistent with the qualitative results, we found that models
incorporating interpolation regularizer (gan-int, gan-
int-cls) perform the best for this task.

5.3. pose and background style transfer
we demonstrate that gan-int-cls with trained style en-
coder (subsection 4.4) can perform style transfer from an
unseen query image onto a text description. figure 6 shows
that images generated using the inferred styles can accu-
rately capture the pose information.
in several cases the
style transfer preserves detailed background information
such as a tree branch upon which the bird is perched.
disentangling the style by gan-int-cls is interesting be-
cause it suggests a simple way of generalization. this way

figure 6. transfering style from the top row (real) images to the
content from the query text, with g acting as a deterministic de-
coder. the bottom three rows are captions made up by us.
we can combine previously seen content (e.g. text) and pre-
viously seen styles, but in novel pairings so as to generate
plausible images very different from any seen image during
training. another way to generalize is to use attributes that
were previously seen (e.g. blue wings, yellow belly) as in
the generated parakeet-like bird in the bottom row of fig-
ure 6. this way of generalization takes advantage of text
representations capturing multiple visual aspects.

5.4. sentence interpolation
figure 8 demonstrates the learned text manifold by inter-
polation (left). although there is no ground-truth text for
the intervening points, the generated images appear plau-
sible. since we keep the noise distribution the same, the
only changing factor within each row is the text embedding
that we use. note that interpolations can accurately re   ect
color information, such as a bird changing from blue to red
while the pose and background are invariant.
as well as interpolating between two text encodings, we
show results on figure 8 (right) with noise interpolation.
here, we sample two random noise vectors. by keeping the
text encoding    xed, we interpolate between these two noise
vectors and generate bird images with a smooth transition
between two styles by keeping the content    xed.

5.5. beyond birds and    owers
we trained a gan-cls on ms-coco to show the gen-
eralization capability of our approach on a general set of
images that contain multiple objects and variable back-
grounds. we use the same text encoder architecture,
same gan architecture and same hyperparameters (learn-
ing rate, minibatch size and number of epochs) as in cub

the bird has a yellow breast with grey features and a small beak.this is a large white bird with black wings and a red head.a small bird with a black head and wings and features grey wings.this bird has a white breast, brown and white coloring on its head and wings, and a thin pointy beak.a small bird with white base and black stripes throughout its belly, head, and feathers.a small sized bird that has a cream belly and a short pointed bill.this bird is completely red.this bird is completely white.this is a yellow bird. the wings are bright blue.text descriptions(content)images (style)generative adversarial text to image synthesis

figure 7. generating images of general concepts using our gan-cls on the ms-coco validation set. unlike the case of cub and
oxford-102, the network must (try to) handle multiple objects and diverse backgrounds.

roughly correspond to the query, but aligndraw samples
more noticably re   ect single-word changes in the selected
queries from that work. incorporating temporal structure
into the gan-cls generator network could potentially im-
prove its ability to capture these text variations.
6. conclusions
in this work we developed a simple and effective model
for generating images based on detailed visual descriptions.
we demonstrated that the model can synthesize many plau-
sible visual interpretations of a given text caption. our
manifold interpolation regularizer substantially improved
the text to image synthesis on cub. we showed disentan-
gling of style and content, and bird pose and background
transfer from query images onto text descriptions. finally
we demonstrated the generalizability of our approach to
generating images with multiple objects and variable back-
grounds with our results on ms-coco dataset. in future
work, we aim to further scale up the model to higher reso-
lution images and add more types of text.
acknowledgments
this work was supported in part by nsf career
iis-1453651, onr n00014-13-1-0762 and nsf cmmi-
1266184.

references
akata, z., reed, s., walter, d., lee, h., and schiele, b.
evaluation of output embeddings for fine-grained im-
age classi   cation. in cvpr, 2015.

ba, j. and kingma, d. adam: a method for stochastic

optimization. in iclr, 2015.

bengio, y., mesnil, g., dauphin, y., and rifai, s. better

figure 8. left: generated bird images by interpolating between
two sentences (within a row the noise is    xed). right: interpolat-
ing between two randomly-sampled noise vectors.

and oxford-102. the only difference in training the text
encoder is that coco does not have a single object cat-
egory per class. however, we can still learn an instance
level (rather than category level) image and text matching
function, as in (kiros et al., 2014).
samples and ground truth captions and their corresponding
images are shown on figure 7. a common property of all
the results is the sharpness of the samples, similar to other
gan-based image synthesis models. we also observe di-
versity in the samples by simply drawing multiple noise
vectors and using the same    xed text encoding.
from a distance the results are encouraging, but upon
close inspection it is clear that the generated scenes are
not usually coherent; for example the human-like blobs in
the baseball scenes lack clearly articulated parts.
in fu-
ture work, it may be interesting to incorporate hierarchical
structure into the image synthesis model in order to better
handle complex multi-object scenes.
a qualitative comparison with aligndraw (mansimov
et al., 2016) can be found in the supplement. gan-
cls generates sharper and higher-resolution samples that

a group of people on skis stand on the snow.a table with many plates of food and drinkstwo giraffe standing next to each other in a forest.a large blue octopus kite flies above the people having fun at the beach.a man in a wet suit riding a surfboard on a wave.two plates of food that include beans, guacamole and rice.a green plant that is growing out of the ground.there is only one horse in the grassy field.a pitcher is about to throw the ball to the batter. a sheep standing in a open grass field.a picture of a very clean living room.a toilet in a small room with a window and unfinished walls.gtoursgtoursgtours   blue bird with black beak           red bird with black beak      small blue bird with black wings           small yellow bird with black wings      this bird is bright.           this bird is dark.      this bird is completely red with black wings      this is a yellow bird. the wings are bright blue      this bird is all blue, the top part of the bill is blue, but the bottom half is white   generative adversarial text to image synthesis

mixing via deep representations. in icml, 2013.

denton, e. l., chintala, s., fergus, r., et al. deep gener-
ative image models using a laplacian pyramid of adver-
sarial networks. in nips, 2015.

donahue, j., hendricks, l. a., guadarrama, s., rohrbach,
m., venugopalan, s., saenko, k., and darrell, t. long-
term recurrent convolutional networks for visual recog-
nition and description. in cvpr, 2015.

dosovitskiy, a., tobias springenberg, j., and brox, t.
learning to generate chairs with convolutional neural
networks. in cvpr, 2015.

farhadi, a., endres, i., hoiem, d., and forsyth, d. de-

scribing objects by their attributes. in cvpr, 2009.

fu, y., hospedales, t. m., xiang, t., fu, z., and gong, s.
transductive multi-view embedding for zero-shot recog-
nition and annotation. in eccv, 2014.

gauthier, j. conditional generative adversarial nets for

convolutional face generation. technical report, 2015.

goodfellow, i., pouget-abadie, j., mirza, m., xu, b.,
warde-farley, d., ozair, s., courville, a., and bengio,
y. generative adversarial nets. in nips, 2014.

gregor, k., danihelka, i., graves, a., rezende, d., and
wierstra, d. draw: a recurrent neural network for image
generation. in icml, 2015.

hochreiter, s. and schmidhuber, j. long short-term mem-

ory. neural computation, 9(8):1735   1780, 1997.

ioffe, s. and szegedy, c. batch id172: accelerat-
ing deep network training by reducing internal covariate
shift. in icml, 2015.

karpathy, a. and li, f. deep visual-semantic alignments

for generating image descriptions. in cvpr, 2015.

kiros, r., salakhutdinov, r., and zemel, r. s. unify-
ing visual-semantic embeddings with multimodal neural
language models. in acl, 2014.

mansimov, e., parisotto, e., ba, j. l., and salakhutdi-
nov, r. generating images from captions with attention.
iclr, 2016.

mao, j., xu, w., yang, y., wang, j., and yuille, a. deep
captioning with multimodal recurrent neural networks
(m-id56). iclr, 2015.

mirza, m. and osindero, s. conditional generative adver-

sarial nets. arxiv preprint arxiv:1411.1784, 2014.

ngiam, j., khosla, a., kim, m., nam, j., lee, h., and ng,

a. y. multimodal deep learning. in icml, 2011.

parikh, d. and grauman, k. relative attributes. in iccv,

2011.

radford, a., metz, l., and chintala, s. unsupervised rep-
resentation learning with deep convolutional generative
adversarial networks. 2016.

reed, s., sohn, k., zhang, y., and lee, h. learning to dis-
entangle factors of variation with manifold interaction.
in icml, 2014.

reed, s., zhang, y., zhang, y., and lee, h. deep visual

analogy-making. in nips, 2015.

reed, s., akata, z., lee, h., and schiele, b. learning deep
in

representations for    ne-grained visual descriptions.
cvpr, 2016.

ren, m., kiros, r., and zemel, r. exploring models and

data for image id53. in nips, 2015.

sohn, k., shang, w., and lee, h.

deep learning with variation of information.
2014.

improved multimodal
in nips,

srivastava, n. and salakhutdinov, r. r. multimodal learn-

ing with deep id82s. in nips, 2012.

szegedy, c., liu, w., jia, y., sermanet, p., reed, s.,
anguelov, d., erhan, d., vanhoucke, v., and rabi-
novich, a. going deeper with convolutions. in cvpr,
2015.

kumar, n., berg, a. c., belhumeur, p. n., and nayar, s. k.
attribute and simile classi   ers for face veri   cation. in
iccv, 2009.

vinyals, o., toshev, a., bengio, s., and erhan, d. show
in cvpr,

and tell: a neural image caption generator.
2015.

lampert, c. h., nickisch, h., and harmeling, s. attribute-
based classi   cation for zero-shot visual object catego-
rization. tpami, 36(3):453   465, 2014.

wah, c., branson, s., welinder, p., perona, p., and be-
longie, s. the caltech-ucsd birds-200-2011 dataset.
2011.

lin, t.-y., maire, m., belongie, s., hays, j., perona, p.,
ramanan, d., doll  ar, p., and zitnick, c. l. microsoft
coco: common objects in context. in eccv. 2014.

wang, p., wu, q., shen, c., hengel, a. v. d., and dick, a.
explicit knowledge-based reasoning for visual question
answering. arxiv preprint arxiv:1511.02570, 2015.

generative adversarial text to image synthesis

xu, k., ba, j., kiros, r., courville, a., salakhutdinov, r.,
zemel, r., and bengio, y. show, attend and tell: neural
image id134 with visual attention. in icml,
2015.

yan, x., yang, j., sohn, k., and lee, h. attribute2image:
image generation from visual attributes.

conditional
arxiv preprint arxiv:1512.00570, 2015.

yang, j., reed, s., yang, m.-h., and lee, h. weakly-
supervised disentangling with recurrent transformations
for 3d view synthesis. in nips, 2015.

zhu, y., kiros, r., zemel, r., salakhutdinov, r., urta-
sun, r., torralba, a., and fidler, s. aligning books
and movies: towards story-like visual explanations by
watching movies and reading books. in iccv, 2015.

