robsut wrod reocginiton via semi-character recurrent neural network   

keisuke sakaguchi    kevin duh    matt post    benjamin van durme      

   center for language and speech processing, johns hopkins university

   human language technology center of excellence, johns hopkins university

{keisuke,kevinduh,post,vandurme}@cs.jhu.edu

7
1
0
2

 

b
e
f
7

 

 
 
]
l
c
.
s
c
[
 
 

2
v
4
1
2
2
0

.

8
0
6
1
:
v
i
x
r
a

abstract

language processing mechanism by humans is generally
than computers. the cmabrigde uinervtisy
more robust
(cambridge university) effect from the psycholinguistics
literature has demonstrated such a robust word processing
mechanism, where jumbled words (e.g. cmabrigde / cam-
bridge) are recognized with little cost. on the other hand,
computational models for word recognition (e.g. spelling
checkers) perform poorly on data with such noise.
inspired by the    ndings from the cmabrigde uinervtisy ef-
fect, we propose a word recognition model based on a semi-
character level recurrent neural network (scid56). in our
experiments, we demonstrate that scid56 has signi   cantly
more robust performance in word id147 (i.e.
word recognition) compared to existing spelling checkers and
character-based convolutional neural network. furthermore,
we demonstrate that the model is cognitively plausible by
replicating a psycholinguistics experiment about human read-
ing dif   culty using our model.

introduction

despite the rapid improvement in natural language process-
ing by computers, humans still have advantages in situations
where the text contains noise. for example, the following
sentences, introduced by a psycholinguist (davis 2003), pro-
vide a great demonstration of the robust word recognition
mechanism in humans.

aoccdrnig to a rscheearch at cmabrigde uinervtisy,
it deosn   t mttaer in waht oredr the ltteers in a wrod
are, the olny iprmoetnt tihng is taht the frist and lsat
ltteer be at the rghit pclae. the rset can be a toatl
mses and you can sitll raed it wouthit porbelm. tihs
is bcuseae the huamn mnid deos not raed ervey lteter
by istlef, but the wrod as a wlohe.
this example shows the cmabrigde uinervtisy (cam-
bridge university) effect, which demonstrates that human
reading is resilient to (particularly internal) letter transpo-
sition.

   parts of this manuscript are intentionally jumbled to demon-
strate the robust word processing ability of you, the reader.
copyright c(cid:13) 2017, association for the advancement of arti   cial
intelligence (www.aaai.org). all rights reserved.

figure 1: schematic illustration of semi-character recurrent
neural network (scid56).

robustness is an important and useful property for various
tasks in natural language processing, and we propose a com-
putational model which replicates this robust word recogni-
tion mechanism. the model is based on a standard recurrent
neural network (id56) with a memory cell as in long short-
term memory (hochreiter and schmidhuber 1997). we use
an id56 because it has shown to be state-of-the-art language
modeling (mikolov et al. 2010) and it is also    exible to real-
ize the    ndings from the cmabrigde uinervtisy effect. tech-
nically, the input layer of our model consists of three sub-
vectors: beginning (b), internal (i), and ending (e) charac-
ter(s) of the input word (figure 1). this semi-character level
recurrent neural network is referred as scid56.

first, we review previous work on the robust word recog-
nition mechanism from psycholinguistics literature. next,
we describe technical details of scid56 which capture the
robust human mechanism using recent developments in neu-
ral networks. as closely related work, we explain character-
based convolutional neural network (charid98) proposed
by kim et al. (2015). our experiments show that the scid56
signi   cantly outperforms commonly used spelling checkers
and charid98 by (at least) 42% for jumbled word correction
and 3% and 14% in other noise types (insertion and dele-
tion). we also show that scid56 replicates recent    ndings
from psycholinguistics experiments on reading dif   culty de-
pending on the position of jumbled letters, which indicates
that scid56 successfully mimics (at least a part of) the ro-
bust word recognition mechanism by humans.

bielstmbielstmbielstmbielstm(cid:1)(cid:1)(cid:1)aoccdrnigsoftmaxsoftmaxsoftmaxsoftmax(cid:1)(cid:1)(cid:1)toarscheearch(cid:1)(cid:1)(cid:1)accordingtoaresearchcond.
n
int
end
beg

example

the boy could not solve the problem so he asked for help.
the boy cuold not slove the probelm so he aksed for help.
the boy coudl not solev the problme so he askde for help.
the boy oculd not oslve the rpoblem so he saked for help.

10.4
11.4   
12.6   
13.0   

15.0
17.6   
17.5   
21.5   

236
244   
246   
259   

# of    xations regression(%) avg. fixation (ms)

table 1: example sentences and results for measures of    xation excerpt from rayner et al. (2006). there are 4 conditions: n =
normal text; int = internally jumbled letters; end = letters at word endings are jumbled; beg = letters at word beginnings are
jumbled. entries with     have statistically signi   cant difference from the condition n (p < 0.01) and those with     and     differ
from     and     with p < 0.01 respectively.

demonstrate that a priming effect remains when inserting a
character into a word (e.g. juastice-justice).

another popular experimental paradigm in psycholinguis-
tics is eye-movement tracking. in comparison to the masked
priming technique, eye-movement paradigm provides data
from normal reading process by participants. regarding
word recognition, the eye-tracking method has shown the
relationship between a word dif   culty and the eye    xation
time on the word: when a word is dif   cult to process, av-
erage time to    xation becomes long. in addition, words that
are dif   cult to process often induce regressions to words pre-
viously read.

with the eye-movement paradigm, rayner et al. (2006)
and johnson, perea, and rayner (2007) conduct detailed ex-
periments on the robust word recognition mechanism with
jumbled letters. they show that letter transposition affects
   xation time measures during reading depending on which
part of the word is jumbled. table 1 presents the result
from rayner et al. (2006). it is obvious that people can read
smoothly (i.e. smaller number of    xations, regression, and
average of    xation duration) when a given sentence has no
noise (referring to this condition as n). when the charac-
ters at the beginning of words are jumbled (referring to this
condition as beg), participants have more dif   culty (e.g.
longer    xation time). the other two conditions, where words
are internally jumbled (int) or letters at word endings are
jumbled (end), have similar amount of effect, although the
number of    xations between them showed a statistically sig-
ni   cant difference (p < 0.01). in short, the reading dif   culty
with different jumble conditions is summarized as follows:
n < int     end < beg.

it may be surprising that there is statistically signi   cant
difference between end and beg conditions despite the
difference being very subtle (i.e.    xing either the    rst or the
last character). this result demonstrates the importance of
beginning letters for human word recognition.2

semi-character recurrent neural network

in order to achieve the human-like robust word processing
mechanism, we propose a semi-character based recurrent
neural network (scid56). the model takes a semi-character
vector (x) for a given jumbled word, and predicts a (correctly

2while there is still ongoing debate in the psycholinguistics
community as to exactly how (little) the order of internal letters
matter, here we follow the formulation of rayner et al. (2006), con-
sidering only the letter order distinctions of beg, int, and end.

figure 2: example of the masked priming procedure.

raeding wrods with jumbled lettres

sentence processing with jumbled words has been a ma-
jor research topic in psycholinguistics literature. one pop-
ular experimental paradigm is masked priming, in which
a (lower-cased) stimulus, called prime, is presented for a
short duration (e.g. 60 milliseconds) followed by the (upper-
cased) target word, and participants are asked to judge
whether the target word exists in english as quickly as pos-
sible (figure 2).1 the prime is consciously imperceptible
due to the instantaneous presentation but it proceeds to vi-
sual word recognition by participants. the masked priming
paradigm allows us to investigate the machinery of lexical
processing and the effect of prime in a pure manner.

forster et al. (1987) show that a jumbled word (e.g.
gadren-garden) facilitates primes as large as identity
primes (garden-garden) and these results have been con-
   rmed in cases where the transposed letters are not adja-
cent (caniso-casino) (perea and lupker 2004) and even
more extreme cases (sdiwelak-sidewalk) (guerrera and
forster 2008).

these    ndings about robust word processing mechanism
by humans have been further investigated by looking at
other types of noise in addition to simple letter transpo-
sitions. humphreys, evett, and quinlan (1990) show that
deleting a letter in a word still produces signi   cant prim-
ing effect (e.g. blck-black), and similar results have been
shown in other research (peressotti and grainger 1999;
grainger et al. 2006). van assche and grainger (2006)

1there is another variant for masked priming technique, where
backward mask is inserted between the prime and target in addition
to the forward mask.

forward mask(500 milliseconds)gardengadren########prime(60 milliseconds)targetspelled) word (y) at each time step. the structure of scid56
is based on a standard recurrent neural network, where cur-
rent input (x) and previous information is connected through
hidden states (h) by applying a certain (e.g. sigmoid) func-
tion (g) with linear transformation parameters (w ) and the
bias (b) at each time step (t).

one critical issue of vanilla recurrent neural networks is
that it is unable to learn long distance dependency in the
inputs due to the vanishing gradient (bengio, simard, and
frasconi 1994). to address the problem, hochreiter and
schmidhuber (1997) introduced long short-term memory
(lstm), which is able to learn long-term dependencies by
adding a memory cell (c). the memory cell has an ability to
discard or keep previous information in its state. technically,
the lstm architecture is given by the following equations,
(1)
(2)
(3)
(4)
(5)
(6)
where    is the (element-wise) sigmoid function and (cid:12) is the
element-wise multiplication.

in =    (wi [hn   1, xn] + bi)
fn =    (wf [hn   1, xn] + bf )
on =    (wo [hn   1, xn] + bo)
gn =    (wg [hn   1, xn] + bg)
cn = fn (cid:12) cn   1 + in (cid:12) gn
hn = on (cid:12) tanh (cn)

while a standard input vector for id56 derives from either
a word or a character, the input vector in scid56 consists of
three sub-vectors (bn, in, en) that correspond to the charac-
ters    position.

(7)

(8)

the    rst and third sub-vectors (bn, en) represent the    rst
and last character of the n-th word. these two sub-vectors
are therefore one-hot representations. the second sub-vector
(in) represents a bag of characters of the word without the
initial and    nal positions. for example, the word    univer-
sity    is represented as bn = {u = 1}, en = {y = 1}, and
in = {e = 1, i = 2, n = 1, s = 1, r = 1, t = 1, v = 1},
with all the other elements being zero. the size of sub-
vectors (bn, in, en) is equal to the number of characters (n)
in our language, and xn has therefore the size of 3n by con-
catenating the sub-vectors.

regarding the    nal output (i.e. predicted word yn), the
hidden state vector (hn) of the lstm is taken as input to
the following softmax function layer with a    xed vocabulary
size (v).

(cid:80)
exp (wh    hn)
v exp (wh    hn)

yn =

we use the cross-id178 training criterion applied to the
output layer as in most lstm id38 works;
the model learns the weight matrices (w ) to maximize the
likelihood of the training data. this should approximately
correlate with maximizing the number of exact word match
in the predicted outputs. figure 1 shows a pictorial overview
of scid56.

(cid:34)bn

(cid:35)

xn =

in
en

in order to check if the scid56 can recognize the jumbled
words correctly, we test it in id147 experiments.
if the hypothesis about the robust word processing mecha-
nism is correct, scid56 will also be able to read sentences
with jumbled words robustly.

character-based neural network

another possible approach to deal with reading jumbled
words by neural networks is (pure) character-level neu-
ral network (sutskever, martens, and hinton 2011), where
both input and output are characters instead of words. the
character-based neural networks have been investigated and
used for a variety of nlp tasks such as segmentation (chru-
pala 2013), id33 (ballesteros, dyer, and
smith 2015), machine translation (ling et al. 2015), and text
id172 (chrupa  a 2014).

for id147, schmaltz et al. (2016) uses
character-level convolutional neural networks (charid98)
proposed by kim et al. (2015), in which the input is char-
acter but the prediction is at the word-level. more techni-
cally, according to kim et al. (2015), charid98 concatenates
the character embedding vectors into a matrix pn     rd  l
whose k-th column corresponds to the k-th character embed-
ding vector (size of d) of n-th word which contains l char-
acters. a narrow convolution is applied between p and    lter
h     rd  w of width w, and then feature map fn     rl   w+1
is obtained by the following transformation3 with a bias b.

fn = tanh(tr(pn[:, k : k + w     1]h t ) + b)

(9)
this is interpreted as a process of capturing important fea-
ture f with    lter h to maximize the predicted word repre-
sentation yn by the max-over-time:

yn = max

k

fn[k]

(10)

although charid98 and scid56 have some similarity in
terms of using a recurrent neural network, charid98 is able
to store richer representation than scid56. in the follow-
ing section, we compare the performance of charid98 and
scid56 with respect to jumbled word recognition task.

experiments

we conducted id147 experiments to judge how
well scid56 can recognize noisy word sentences. in or-
der to make the task more realistic, we tested three differ-
ent noise types: jumble, delete, and insert, where the jum-
ble changes the internal characters (e.g. cambridge     cm-
barigde), delete randomly deletes one of the internal char-
acters (cambridge     camridge), and insert randomly in-
serts an alphabet into an internal position (cambridge    
cambpridge). none of the noise types change the    rst and
last characters. we used id32 for training, tuning,
and testing.4

3in the equation, pn[:, k : k+w   1] means the k-to-(k+w   1)-

th column of pn.

4section 2-21 for

tuning, and 23 for
test https://catalog.ldc.upenn.edu/ldc99t42. the
data includes 39,832 sentences in training set (898k/950k tokens

training, 22 for

original

correct

charid98
(kim et al.)

enchant

commercial a

commercial b

scid56
(proposed)

::::

::::::::

searcher at::::::

research at cambridge ::::::

timing is ::taxi the:::::
worthy :::::

tourist and::sat letter be at the::::fruit :::pile . the ::::

parallel .::::mips is ::::abuse the human :::trim::::deck not :::rake ::::

minority , it ::::deck nt :::::mother in :::wait or the letters in a::::wood are ,
reset can be a total :::uses
latter by

aoccdrnig to a rscheearch at cmabrigde uinervtisy , it deos n   t mttaer in waht oredr the ltteers in a
wrod are , the olny iprmoetnt tihng is taht the frist and lsat ltteer be at the rghit pclae . the rset can be
a toatl mses and you can sitll raed it wouthit porbelm . tihs is bcuseae the huamn mnid deos not raed
ervey lteter by istlef , but the wrod as a wlohe .
according to a researcher at cambridge university , it does n   t matter in what order the letters in a word
are , the only important thing is that the    rst and last letter be at the right place . the rest can be a total
mess and you can still read it without problem . this is because the human mind does not read every
letter by itself , but the word as a whole .
according to a ::::::
the :::tony ::::::
vermont :::::
and you can :::vital :::rake it :::::
leftist , but the ::::wood as a whole .
ecuadoran to a ::::::
, the only :::::::::
mess and you can still read it ::::
letter by::::
occurring to a :::::::
inertias , it does n   t matter in what order the letters in a word are
:::::::
, the only impotent thing is that the    rst and last letter be at the right place . the rest can be a total mess
and you can still read it ::::
bcuseae the human mind does not read every letter by
:::istle , but the word as a whole .
aoccdrnig to a :::::::
word are , the only :::::::
a total mess and you can still read it :::::
every letter by itself , but the word as a whole .
according to a ::::::
research at cambridge university , it does n   t matter in what order the letters in a word
are , the only important thing is that the    rst and last letter be at the right place . the rest can be a total
mess and you can still read it without problem . this is because the human mind does not read every
letter by itself , but the word as a whole .

uinervtisy , it does n   t matter in what order the letters in a
iprmoetnt thing is that the    rst and last letter be at the right place . the rest can be
bcuseae the human mind does not read

nerviness , it does n   t matter in what order the letters in a word are
freest and ::slat letter be at the right place . the rest can be a total
ceausescu , the human mind does not read :::::
hervey

brigade :::::::
omnipresent thing is that the ::::
outhit :::::
leftist , but the word as a whole .

wouthit problem . :::tihs is :::::

outhit problem . this is ::::::

scholarch at cambridge ::::::

corbel . :::tish is::::::::

rscheearch at ::::::::

cmabrigde ::::::::

survey ::::

::::::::

table 2: example id147 outputs for the cmabrigde uinervtisy sentences. words that the system failed to correct
are ::::::::

underlined. charid98 stands for the character-based convolutional neural network by kim et al. (2015).

the input layer of scid56 consists of a vector with length
of 76 (a-z, a-z and 24 symbol characters). the hidden layer
units had size 650, and total vocabulary size was set to 10k.
we applied one type of noise to every word, but words with
numbers (e.g. 1980s) and short words (length     3) were
not subjected to jumbling, and therefore these words were
excluded in evaluation. we trained the model by running 5
epochs with (mini) batch size 20. we set the backpropaga-
tion through time (bptt) parameter to 3: scid56 updates
weights for previous two words (xn   2, xn   1) and the cur-
rent word (xn).

for comparison, we evaluated charid98 on the same
training data5, and also compared widely-used spelling
checkers (enchant6, commercial a, and commercial b7).

id147 results
table 2 presents example outputs for the cmabrigde uin-
ervtisy sentence by each model.8 it may be surprising that

are covered by the top 10k vocabulary), 1,700 sentences in the tun-
ing set (coverage 38k/40k), and 2,416 sentences in test set (cover-
age 54k/56k).

5for charid98, we employed the codebase available at
https://github.com/yoonkim/lstm-char-id98.git

6http://www.abisource.com/projects/

enchant/

7we anonymized the name of the commercial product.
8the cmabrigde uinervtisy sentences contains jumbling as
well as deletion, insertion, and replacement of characters. note that

charid98 (kim et al.)
enchant
scid56 (proposed)

jumble delete
21.30
17.17
37.01
57.15
98.96
85.74

insert
35.00
88.54
96.70

table 3: id147 accuracy (%) with different er-
ror types on the entire test set.

charid98 (kim et al.)
enchant
commercial a
commercial b
scid56 (proposed)

jumble delete
19.76
16.18
35.37
57.59
60.19
54.81
54.26
71.67
85.56
99.44

insert
35.53
89.63
93.52
73.52
97.04

table 4: id147 accuracy (%) with different er-
ror types on the subset of test set (50 sentences).

charid98 performs poorly compared with other spelling
checkers. this is probably because the charid98 highly de-
pends on the order of characters in the word and the trans-
posed characters adversely affected the recognition perfor-
mance. enchant, commercial a, and commercial b tend to
fail long word correction. this may be because these mod-
els are not designed for severely jumbled input but they

we used a single scid56 (jumble), and didn   t train scid56 sepa-
rately for each error type in this example.

   accuracy (%)
1
3
5
10

98.69
98.96
98.91
98.95

sd
0.53
0.45
0.40
0.43

table 5: scid56 accuracy (%) on jumbled word recognition
with different bptt parameters. there were no statistically
signi   cant differences among them.

units acc (%)
24.65
48.43
73.32
84.82
94.15
96.90
98.48
98.39

5
10
15
20
30
40
50
60

sd size (kb)
2.59
3.26
3.65
2.39
1.54
1.26
0.94
0.81

236
435
632
830
1,255
1,670
2,092
2,514

table 6: scid56 accuracy (%), the standard deviation, and
the size of model    le (kb) on jumbled word recognition
with respect to the number of units of lstm.

cause the information lost by deleting character is signif-
icant. for example, when the word place has dropped the
character l, the surface form becomes pace, which is also a
valid word. also, the word mess with e being deleted pro-
duces the form of mss, which can be recovered as mess,
mass, miss, etc. in the cmabrigde uinervtisy sentences, in
both cases, the local context support other phrase such as    at
the right pace/place    and    a total mass/mess   . these exam-
ples clearly demonstrate that deleting characters harm the
word recognition more signi   cantly than other noise types.
all the models perform relatively well on insert noise, in-
dicating that adding extraneous information by inserting a
letter does not change the original information signi   cantly.
with respect to a learning curve on scid56 (figure 3,
top), we found that the model achieves 0.9 (in accuracy) at
the 15,000-th iteration of the mini batch. this can be made
within a hour with a cpu machine, which demonstrates sim-
plicity of scid56 compared with charid98.

figure 3 also shows the effect of bptt size (  ), and the
accuracy on test set is presented in table 5. as explained,   
indicates the context length of updates during training. it is
surprising that the longer contexts (   = 5, 10) do not nec-
essarily yield better performance. this is probably because
it rarely happens that the context plays an important role on
distinguishing ambiguous representation (e.g. anagrams) in
scid56. if we take closer look at the learning curve (table 3,
bottom), however, there is a clear gap in learning ef   ciency
between with and without contexts (i.e.   =1 vs. the rest).

finally, we reduced the number of units in the hidden
layer to see the model size and performance of scid56. sur-
prisingly, as table 6 presents, scid56 with 50 units already
achieves comparable results to 650 units (table 3). the re-
sult suggests that 50 units (2 mb) are enough to distinguish
10k english words.

figure 3: learning curve of training scid56 with different
bptt parameter (on dev set):    rst 40k iterations (top) and
its enlarged view between 20k and 40k iterations (bottom).

are likely to depend on id153 between the incor-
rect and correct words. while these existing models struggle
with correcting cmabrigde uinervtisy sentence, we see that
scid56 demonstrates signi   cantly better recognition ability.
the only error in scid56 may be because the last charac-
ter (rscheearch) activated the scid56 nodes strongly toward
research instead of researcher.9

table 3 and 4 show the overall result on the test set with
respect to noise type. we also tested id147 on a
small subset (50 sentences) because of the api limits etc. of
commercial systems. overall, as seen in the example above,
scid56 signi   cantly outperforms the other spelling checker
models across all three noise types. since scid56 is espe-
cially designed for jumbled word recognition, it is not sur-
prising that it performs particularly well on jumble noise.
however, it is striking that scid56 outperforms the other
models in deletion and insertion errors as well.10 this clearly
demonstrates the robustness of scid56.

the relatively large drop in delete in scid56 may be be-

9there is also an deletion of    r   .
10it is important to note that some commercial systems have con-

straints of the model size (church, hart, and gao 2007).

0500010000150002000025000300003500040000number of iterations020406080100accuracy (%)  =10  =5  =3  =12000025000300003500040000number of iterations919293949596979899100accuracy (%)  =10  =5  =3  =1cond.
int
end
beg
all

example

as a relust , the lnik beewetn the fureuts and sctok mretkas rpiped arapt .
as a rtelus , the lkni betwene the feturus and soctk msatrek rpepid atarp .
as a lesurt , the lnik bweteen the utufers and tocsk makrtes pipred arpat .
as a strule , the lnik eewtneb the eftusur and okcst msretak ipdepr prtaa .

accuracy
98.96
98.68   
98.12   
96.79   

table 7: example sentences and results for id147 accuracy by scid56 variants depending on different jumble
conditions: int = internal letters are jumbled; end = letters at word endings are jumbled; beg = letters at word beginnings
are jumbled; all = all letters are jumbled. entries with     have a difference with marginal signi   cance from the condition int
(p = 0.07) and those with     and     differ from     and     with p < 0.01 respectively.

cond. examples of errors (correct/wrong)
int

once/once, under/under, also/also, there/three,
form/from, fares/fears, trail/trial, broad/board

end being/begin, quiet/quite, bets/best, stayed/steady,

heat/hate, lost/lots + same errors in int

beg several/reveal, growth/worth, host/shot, credi-
tors/directors, views/wives + same errors in int
licensed/declines,

all under/trend,

center/recent,

stop/tops + same errors in int, end, & beg

table 8: error analysis of scid56 variants.

corroboration with psycholinguistic experiments
as seen in the literature review in psycholinguistics, the po-
sition of jumbled characters affects the cognitive load of hu-
man word recognition. we investigate this phenomenon with
scid56 by manipulating the structure of input vector. we
replicate the experimental paradigm in rayner et al. (2006),
but using scid56 rather than human subjects. we trained
scid56 variants depending on different jumble conditions:
int, end, beg, and all. int is the same model as ex-
plained in the previous section (xn = [bn, in, en]t ). end
represents an input word as a concatenation of the initial
character vector (b) and a vector for the rest of characters
(xn = [bn, in + en]t ). in other words, in end model, the
internal and last characters are subject to jumbling. beg
model combines a vector for the    nal character (e) and a
vector for the rest of characters (xn = [bn + in, en]t ). in
other words, initial and internal characters are subject to
jumbling. in all model, all the letters are subject to jum-
ble (e.g. research vs. eesrhrca) and represented as a single
vector (xn = [bn + in + en]t ). this is exactly the same as
bag of characters. we trained all the scid56 variants with
   = 3, the number of hidden layer units being 650, and total
vocabulary size to be 10k.

table 7 shows the result. while all the variants of scid56
achieve high accuracy, the statistical test revealed that int
and end have a difference with statistically marginal signif-
icance (p = 0.07). there are statistically signi   cant differ-
ences (p < 0.01) both in end&beg and beg&all. from
the results, the word recognition dif   culty of different jum-
bled types is summarized as int     end < beg < all,
which is the same order as the    nding in table 1. it is not
surprising that int outperform the other variants because it
has richer representation in xn (twice or three times larger
than the other variants). however, it is interesting to see that

end outperforms beg both in rayner et al. (2006) and our
experiment despite that the size of xn between end and
beg models are equal. this suggests the scid56 replicates
(at least a part of) the human word recognition mechanism,
in which the    rst letter is more important and informative
than the last one in english.

for qualitative analysis, table 8 shows some errors (cor-
rect/wrong) that each variant made. all the scid56 vari-
ants often fail to recognize capitalized    rst character (e.g.
once/once, under/under), speci   cally when the word is at
the beginning of the sentence. other than the capitalization
errors, most errors come from anagrams. for example, errors
in int (the original scid56) are internally anagrammable
words (e.g. there/three, form/from). end model made errors
on words that are anagrammable with the    rst character be-
ing    xed (e.g. being/begin, quiet/quite). beg model, on the
other hand, failed to recognize anagrammable words with
the last character being the same (e.g. creditors vs. directors,
views vs. wives). in addition, beg model often ignores the
   rst (upper-cased) character of the word (e.g. several/reveal,
growth/worth). finally, all model failed to recognize ana-
grammable words (e.g. center/recent, licensed vs. declines).
although scid56 generally disambiguated anagrammable
words successfully from context, all these examples from
the error analysis are straightforward and convincing when
we consider the characteristics of each variant of scid56.

summary

we have presented a semi-character recurrent neural net-
work model, scid56, which is inspired by the robust word
recognition mechanism known in psycholinguistics litera-
ture as the cmabrigde uinervtisy effect. despite the model   s
simplicity compared to character-based convolutional neu-
ral networks (charid98), it signi   cantly outperforms widely
used spelling checkers with respect to various noise types.
we also have demonstrated a similarity between scid56
and human word recognition mechanisms, by showing
that scid56 replicates a psycholinguistics experiment about
word recognition dif   culty in terms of the position of jum-
bled characters.

there are a variety of potential nlp applications for
scid56 where robustness plays an important role, such as
normalizing social media text (e.g. cooooolll     cool), post-
processing of ocr text, and modeling morphologically rich
languages, which could be explored with this model in fu-
ture work.

guage model. in interspeech 2010, 11th annual con-
ference of the international speech communication asso-
ciation, makuhari, chiba, japan, september 26-30, 2010,
1045   1048.
perea, m., and lupker, s. j. 2004. can caniso activate
casino? transposed-letter similarity effects with nonadjacent
letter positions. journal of memory and language 51(2):231
    246.
peressotti, f., and grainger, j. 1999. the role of letter iden-
tity and letter position in orthographic priming. perception
& psychophysics 61(4):691   706.
rayner, k.; white, s. j.; johnson, r. l.; and liversedge,
s. p. 2006. raeding wrods with jubid113d lettres: there is a
cost. psychological science 17(3):192   193.
schmaltz, a.; kim, y.; rush, a. m.; and shieber, s. 2016.
sentence-level grammatical error identi   cation as sequence-
in proceedings of the 11th work-
to-sequence correction.
shop on innovative use of nlp for building educational ap-
plications, 242   251. san diego, ca: association for com-
putational linguistics.
sutskever, i.; martens, j.; and hinton, g. e. 2011. gener-
in proceedings
ating text with recurrent neural networks.
of the 28th international conference on machine learning
(icml-11), 1017   1024.
van assche, e., and grainger, j. 2006. a study of relative-
journal of ex-
position priming with superset primes.
perimental psychology: learning, memory, and cognition
32(2):399.

references

2003.

uinervtisy.

cmabrigde

aoccdrnig to a rscheearch
http://www.mrc-

ballesteros, m.; dyer, c.; and smith, n. a. 2015. improved
transition-based parsing by modeling characters instead of
words with lstms. in proceedings of the 2015 conference on
empirical methods in natural language processing, 349   
359. lisbon, portugal: association for computational lin-
guistics.
bengio, y.; simard, p.; and frasconi, p. 1994. learning
long-term dependencies with id119 is dif   cult.
ieee transactions on neural networks 5(2):157   166.
chrupala, g. 2013. text segmentation with character-level
text embeddings. arxiv preprint arxiv:1309.4628.
chrupa  a, g. 2014. normalizing tweets with edit scripts
in proceedings of the
and recurrent neural embeddings.
52nd annual meeting of the association for computational
linguistics (volume 2: short papers), 680   686. baltimore,
maryland: association for computational linguistics.
church, k.; hart, t.; and gao, j. 2007. compressing tri-
gram language models with golomb coding. in proceedings
of the 2007 joint conference on empirical methods in nat-
ural language processing and computational natural lan-
guage learning (emnlp-conll), 199   207. prague, czech
republic: association for computational linguistics.
davis, m.
at
cbu.cam.ac.uk/people/matt.davis/cmabridge/.
forster, k. i.; davis, c.; schoknecht, c.; and carter, r.
1987. masked priming with graphemically related forms:
repetition or partial activation? the quarterly journal of
experimental psychology 39(2):211   251.
grainger, j.; granier, j.-p.; farioli, f.; van assche, e.; and
van heuven, w. j. 2006. letter position information and
printed word perception: the relative-position priming con-
straint. journal of experimental psychology: human per-
ception and performance 32(4):865.
guerrera, c., and forster, k. 2008. masked form priming
with extreme transposition. language and cognitive pro-
cesses 23(1):117   142.
hochreiter, s., and schmidhuber, j. 1997. long short-term
memory. neural computation 9(8):1735   1780.
humphreys, g. w.; evett, l. j.; and quinlan, p. t. 1990.
orthographic processing in visual word identi   cation. cog-
nitive psychology 22(4):517     560.
johnson, r. l.; perea, m.; and rayner, k. 2007. transposed-
letter effects in reading: evidence from eye movements and
parafoveal preview. journal of experimental psychology:
human perception and performance 33(1):209.
kim, y.; jernite, y.; sontag, d.; and rush, a. m. 2015.
character-aware neural language models. arxiv preprint
arxiv:1508.06615.
ling, w.; trancoso, i.; dyer, c.; and black, a. w. 2015.
character-based id4. arxiv preprint
arxiv:1511.04586.
mikolov, t.; kara     at, m.; burget, l.; cernock  y, j.; and khu-
danpur, s.
2010. recurrent neural network based lan-

