temporal attention model for id4

baskaran sankaran haitao mi yaser al-onaizan abe ittycheriah

ibm t.j. watson research center

1101 kitchawan rd, yorktown heights, ny 10598

6
1
0
2

 

g
u
a
9

 

 
 
]
l
c
.
s
c
[
 
 

1
v
7
2
9
2
0

.

8
0
6
1
:
v
i
x
r
a

{bsankara, hmi, onaizan, abei}@us.ibm.com

abstract

attention-based id4
(id4) models suffer from attention de   -
ciency issues as has been observed in recent
research. we propose a novel mechanism
to address some of these limitations and im-
prove the id4 attention. speci   cally, our ap-
proach memorizes the alignments temporally
(within each sentence) and modulates the at-
tention with the accumulated temporal mem-
ory, as the decoder generates the candidate
translation. we compare our approach against
the baseline id4 model and two other related
approaches that address this issue either ex-
plicitly or implicitly. large-scale experiments
on two language pairs show that our approach
achieves better and robust gains over the base-
line and related id4 approaches. our model
further outperforms strong smt baselines in
some settings even without using ensembles.

introduction

1
id4 (id4)
is gaining
signi   cant interest in recent years (kalchbrenner
and blunsom, 2013; sutskever et al., 2014; bah-
danau et al., 2014; jean et al., 2015; luong et al.,
2015). they have also been successful in reaching
performance comparable to the traditional statistical
machine translation (smt) models and has attained
state of the art performances against the smt for
certain language pairs (jean et al., 2015; luong et
al., 2015), albeit with ensembling.

most of the recent models are driven by some
form of attention originally proposed by bahdanau
et al. (2014). the attention mechanism enables the
model to dynamically compose source representa-
tions for each timestep during decoding, instead of a
single, static representation. speci   cally the mecha-
nism allows the model to accord varying attention to

different parts of source sentence while generating
successive target words.

while the attention mechanism remembers the
source words over fairly long distances, the atten-
tion in each step only looks at the previous hidden
state and the source annotations. the model does
not directly encode information as to which source
words/ fragments have been focused in the earlier
timesteps. this de   ciency manifests in two distinct
but related issues, where the attention i) focuses on
same source word/ fragment beyond what is needed,
leading to repeated words in the translation and ii)
fails to focus on some part of the input and thus
missing it completely in the translation leading to
lack of adequacy.

table 1 shows illustrative examples of these is-
sues for two distinct language pairs. it is interest-
ing to note that sometimes the system repeats even
a fairly long phrase as we seen in the japanese ex-
ample.
in reality, the id4 model can repeat the
fragments several times; we have recorded as high
as 8 repetitions in certain cases. additionally, the
system completely misses some salient parts of the
sentence; for example it frequently fails to translate
the verbs in japanese. the problem is exacerbated
for longer sentences or for language pairs with com-
plex reordering. these issues has attracted consid-
erable interest in the recent past (luong et al., 2015;
xu et al., 2015, inter alia).

unlike the earlier works, we take a different ap-
proach in this paper and directly save the attention
at each timestep. we then use this temporal memory
along with that of the source annotations to mod-
ulate the attention to produce the response at the
current timestep. the use of temporally aggregated
alignments can be seen as a memory-network in-
spired extension to the id4 attention.

the model naturally allows one-to-many and
many-to-many alignments to be captured if sup-

source:

reference:

candidate:

source:

die teilnehmer der proteste , die am donnerstag um 6:30 am morgens vor dem mcdonald    s in der 40th
street und in der madison avenue begannen , forderten , dass die kassierer und k  oche von fast - food -
restaurants einen mindestlohn von 15 us-dollar die stunde erhalten , was mehr als einer verdoppelung
des jetzigen mindestlohns entspricht .
participants of the protest that began at 6.30 a.m. on thursday near the mcdonald   s on 40th street and
madison avenue demanded that cashiers and cooks of the fast food chain be paid at least 15 dollars/hour,
i.e. more than double their present wages.
the protests that began on thursday at 06:30 before the mcdonald    s at mcdonald    s at mcdonald    s
on 40th street and madison avenue demanded that a minimum wage of 15 dollars would receive
a minimum wage of 15 dollars per hour , equivalent to doubling the current minimum wage .
special attention is being paid to the tokyo gubernatorial election because it is perceived as a litmus test
for the upcoming house of councillors election , particularly in the metropolitan areas where nonpartisan
voters predominate .

reference: z m                ,         5                  z  z                      4                        

        l             

candidate:          z           ,          z           , 5            [m             l                    

table 1: examples for phrase repetitions and inadequacy issues in id4 shown for german-english and english-japanese settings.
the repeated phrases in the translations are shown by underlined text, while the missing source content are boldfaced. notice that
the repetitions need not be consecutive, making the issue even more critical.

ported by the data (effectively modeling high fer-
tility words), but otherwise overriding them if the
weights are small (thus preventing repetitions). in
contrast to the alternative approaches, our model is
simpler as it avoids additional parameters. large-
scale experiments on two language pairs show that
our approach improves the translation quality over
the baseline id4 system signi   cantly, and also sur-
passes two other related approaches.

we    rst give an outline of the id4 model with
attention (bahdanau et al., 2014) in    2. we then
explain our augmented attention model employing
temporal alignments (   3). we then turn to dis-
cuss two alternative models for improving attention
i) coverage embedding (     4.1) inspired by the tra-
ditional smt and ii) local id12 (     4.2).
we go over the related work in    5 and then present
our experiments and results in    6 before concluding.

2 id4

as shown in figure 1, attention-based id4 (bah-
danau et al., 2014) is an encoder-decoder network.
the encoder employs a bi-directional id56 to en-
code the source sentence x = (x1, ..., xl) into a se-
quence of hidden states h = (h1, ..., hl), where l is
the length of the source sentence. each hi is a con-
catenation of a left-to-right       hi and a right-to-left       hi

id56:

hi =(cid:34)      h i

      h i(cid:35) =(cid:34)      f (xi,      h i+1)
      f (xi,      h i   1)(cid:35)

where       f and       f are two id149 (gru)
introduced by cho et al. (2014).

given the encoded h, the decoder predicts the
translation by maximizing the conditional
target
translation y    =
log-id203 of the correct
(y   
1, ...y   
m), where m is the length of the target. at
each time t, the id203 of each word yt from a
target vocabulary vy is:
   
1) = g(st, y

   
t   1, ht),

   
t   1..y

(1)

p(yt|h, y

where g is a two layer feed-forward network (ot be-
ing an intermediate state) over the embedding of the
previous target word (y   
t   1), the decoder hidden state
(st), and the weighted sum of encoder states h (ht).
a single feedforward layer then projects ot to the
target vocabulary and applies softmax to predict the
id203 distribution over the output vocabulary.

we compute st with a two layer gru as:

(cid:48)
t = u(st   1, y

   
t   1).

s

(2)

st = q(s

(3)
where s(cid:48)
t is an intermediate state. the two gru
units u and q together with the attention constitute

(cid:48)
t, ht)

figure 1: attention-based id4 architecture with the attention model illustrated within the box on the right. unlike bahdanau et
al. (2014) our architecture employs a conditional gru layer that has two gru units surrounding an attention mechanism.2 see the
accompanying text for description. the dotted (red) connection is used in the coverage-embedding model (sec. 4.1).

the conditional gru layer. and ht is computed as:

(4)

(5)

i=1 (  t,i          h i)(cid:35) ,
ht =(cid:34)(cid:80)l
i=1 (  t,i          h i)
(cid:80)l

the attention model (in the right box) is a two
layer feed-forward network r, with at,j being an in-
termediate state and another layer converting it into
a real number et,j. the alignment weights   , are
computed from the two layer feed-forward network
r as:

  t,i =

exp{r(s(cid:48)
t, hi)}
(cid:80)l
j=1 exp{r(s(cid:48)

t, hj)}

  t,j are actually the soft alignment probabilities,
denoting the id203 of aligning the target word
at timestep t to source position j.
3 temporal attention model
in this section, we propose a novel mechanism to
memorize the alignments temporally from the pre-
vious timesteps and then use those to modulate the
attention in the subsequent timesteps. intuitively we
would like the attention to be aware of what it fo-
cussed on in the previous steps. the vanilla attention

2same as

the decoder gru introduced in session-
https://github.com/nyu-dl/dl4mt-

the dl4mt-tutorial:

2 of
tutorial/tree/master/session2.

figure 2: temporal attention model: bt,j is the alignment
value prior to id172. we modulate the attention by
looking at historical alignments bh
t followed by a id172
over all the source positions. intuitively, we take into account
the entire attention matrix instead of just one dimension.

model expects this to be captured by longer short-
term memory in the decoder hidden state. how-
ever this only encodes indirect information about the
alignments in previous timestep(s). in our approach,
we propose to directly use the historical alignment
information to better modulate the attention.

figure 2 illustrates the temporally modulated

    t1   tlst 1st   oty1      y|vy|atjetj   tj=exp(etj)pli=1exp(eti)etl      ct 1,jht=lxi=1(   ti    hi)lxi=1(   ti   !hi)x1xl  h1  hl !hl !h1         x2 !h2  h2x1xl  h1  hl !hl !h1           hj !hjxj            t2et1y   t 1y   ts0ts0tct 1,jct 2,jyt 1ct 2,l            ct,lct 1,lct,jyt         t 1,j   t,jct,1ct 1,1ct 2,1atjetjetl      x1xl  h1  hl !hl !h1           hj !hjxj         et1s0t   t,j=bt,jpli=1bt,ibt,j=exp(et,j)bht,jbht,j=t 1xk=1exp(ek,j)alignment bt,j at decoder timestep t and source pos-
tion j. the et,j are modulated based on the historical
alignments bh

t,j:

bh
t,j =

t   1(cid:88)k=1

exp(ek,j)

(6)

where bh
t,j denotes the aggregated alignments until
(but not including) the current timestep t. the mod-
ulated alignment bt,j then takes the form:

bt,j =

exp(et,j)

bh
t,j

(7)

however at timestep t = 1, we set the modulated
alignment to be b1,j = exp(e1,j) as there is no previ-
ous history. we then compute the attention weights
  t,j by normalizing along the input sequence length:

  t,j =

bt,j
i=1 bt,i

(cid:80)l

(8)

in this way, we explicitly force the model to re-
member all its previous decisions, which we be-
lieve, will be particularly useful in language pairs
with complex reordering3.

it would be interesting to see how the model per-
forms if we limit the temporal attention to the last
n timesteps, instead of the entire history. thus the
model can forget older decisions, which arguably
might not be relevant going forward (we do not ad-
dress this question here, but leave it for future exper-
imentation).

3.1 a perspective from memory networks
neural memory networks (graves et al., 2014; we-
ston et al., 2014; sukhbaatar et al., 2015) have
gained substantial interest in past two years. the key
idea in mns is to augment the neural networks to ef-
fectively interface with memory so that the network
can selectively choose to perform read or write op-
erations as required. while the memory content are
encoded by the hidden states and weights, the power
of such models manifest in their ability to memorize/
recall facts to/ from the memory potentially over
long term and/ or large volume (of encoded facts).

3we also tried adding an extra connection instead from tem-
poral alignments and learn its parameters, but that performed
poorly than directly modulating the attention.

figure 3: the coverage embedding model with a gru at time
step t     1 and t. c0,1 to c0,l are initialized with the word cover-
age embedding matrix.

the id4 attention can be considered as a sim-
ple example for mns, where the memory stores the
source annotation at the sentence level. our ap-
proach could be seen from the perspective of mem-
ory networks (mns) in that we further extend the at-
tention to memorize its decisions temporally during
decoding.

compared to the typical mns, our model differs
in terms of what, where and how we memorize. in
our work, the memory is volatile as we only use
the aggregated alignments within that sentence; it is
not reused when the decoder moves to the next one.
secondly, we do not store the alignments externally
or by gating it; instead we directly use it to modu-
late the alignments. finally, while the conventional
memory networks would have one or more layers of
memory, whose parameters are trained as part of the
model (sukhbaatar et al., 2015), our model memo-
rizes the alignment vectors (and not the model pa-
rameters) in the attention.

4 alternative models for attention
in this section we discuss two alternative approaches
that address de   ciency in the attention model either
explicitly or implicitly.

4.1 coverage embedding model
traditional
id151 (e.g.
(koehn, 2004)) decoders usually employ a source
side    coverage vector    to keep track of the source
words that are already translated. at the begin-
ning the coverage vector is initialized to be all zeros,
which means no word has been translated. once the
word at source position j is translated, its index in
the coverage vector is set to 1; and the decoder won   t
translate the word again in that sentence.

ct 1,jct 2,jyt 1ct 2,l            ct,lct 1,lct,jyt         t 1,j   t,jct,1ct 1,1ct 2,1the coverage embedding model (mi et al., 2016)
seeks to adapt the notion of    coverage vector    for
id4. under this, each source word xi is assigned a
coverage embedding,4 which intuitively encodes its
fertility5 to be one of the three possible cases: one-
to-one, one-to-many or one-to-zero.

figure 3 illustrates the coverage embedding
model. during training, at timestep 0, the coverage
embeddings of the input sequence are initialized by
table look-up from the coverage embedding matrix,
yielding c0,1, c0,2, ...c0,l. at time step t, the model
uses the coverage embedding ct   1,j in the attention
model (shown by the dotted line in figure 1). the
ct,j is subsequently updated by the attention weights
  t,j, predicted target yt and embedding from the pre-
vious timestep ct   1,j (see figure 3) with a gru as:

zt,j =   (w zyyt + w z    t,j + u zct   1,j)
rt,j =   (w ryyt + w r    t,j + u rct   1,j)
  ct,j = tanh(w yt + w     t,j + rt,j     u ct   1,j)
ct,j = zt,j     ct   1,j + (1     zt,j)       ct,j,

where, zt is the update gate, rt is the reset gate,   ct is
the new memory content, and ct is the    nal memory.
the matrix w zy, w z  , u z, w ry, w r  , u r, w y,
w    and u are shared across different position j.    
is a pointwise operation.

4.2 local attention model
the local attention (luong et al., 2015) was origi-
nally motivated as an alternative to the vanilla global
attention model (bahdanau et al., 2014). unlike the
global attention, which attends to all source words
at each timestep, the local attention selectively lim-
its the focus to a local window of source sentence
at each step. the window approach thus avoids the
id203 mass being allocated to the positions out-
side the window and so the resulting attention can
be expected to be sharper than the global attention.
we compare our temporal attention to the local at-
tention model as they both attempt to modulate the
attention, albeit in distinct ways.

the two variants of the local attention differ in
whether the local window at each timestep is cho-
sen based on i) monotone assumption (local-m) be-
4following mi et al. (2016), we set ct,j dimension to 100.
5fertility of a word is its yield in target language measured

by the # of words

tween source and target or ii) a parameterized and
predictive model (local-p) that predicts the source
alignment position for the given timestep. interest-
ingly the published work employs the local attention
only for en-de translation; for the reverse de-en di-
rection it only shows the results for global attention
(table 3 in luong et al. (2015)). it is not clear if the
local attention was helpful at all in the latter setting.
we only consider local-m attention for our experi-
ments in this paper.

5 related work

recently xu et al.
(2015) proposed a doubly
stochastic attention model for image caption gener-
ation, which is very close to our work in terms of
its motivation. the decoder generates a description
as the attention model attends to a speci   c location
in the image. while the attention at every step sums

to 1 ((cid:80)i   ti = 1), the same is not true for across

all the timesteps for a    xed position i. thus, the de-
coder can completely ignore some parts of the image
(equivalently sentences in the case of id4). to ad-
dress this, they add a penalty term to the cost, which
softly encourages the attention to focus on every part
of the image. in contrast, our model enforces this
explicitly and provides a stronger prior than the soft
penalty. our experiments with the penalty term did
not perform better than the baseline.

the notion of coverage vector in the phrase-based
smt has inspired similar approaches for id4 (tu
et al., 2016; mi et al., 2016). the former employs
a gru to model the coverage vector, which are ini-
tialized with a uniform distribution. this model is
very similar to the coverage-embedding model (de-
scribed in      4.1) that we compare against in our
experiments. one main difference is that the lat-
ter model initializes each source word from a spe-
ci   c coverage embedding matrix. secondly tu et
al. (2016) add an accumulate operation and a fertil-
ity function to simulate the one-to-many alignments
scenario, whereas (mi et al., 2016) add fertility in-
formation directly to coverage embeddings, as each
source word has its own embedding.

alternately feng et al. (2016) proposes an addi-
tional recurrent layer that has explicit connections
from previous attention id56 hidden state, previ-
ous target label and current attention generated con-

text. while this model retains the previous attention
information, it is only through the attention gener-
ated context (in contrast, we memorize the attention
weights across each timestep). they also condition
the decoder during training to explicitly model the
distortion and fertility through additional terms to
the cost. however the major drawback is that their
approach was tested only on a small dataset (0.5m
sentences) and hence it is not clear if the improve-
ments would hold when applied to large datasets.

cohn et al. (2016) take a different approach and
augment the attention model with well-known fea-
tures in traditional smt, including positional bias,
markov conditioning, fertility and bilingual symme-
try feature. the features are added as extra com-
ponents to the pre-normalized attention where the
features are computed dynamically from the paral-
lel sentences and the weights are learned through
backprop. the last bilingual symmetry feature is
captured by an additional term in the id168.
they only experiment their model on simulated low-
resource setting and so it is unclear if the gains carry
over to the large data id4.

in the context of continuous id103
(sr), chorowski et al. (2014) proposed a soft align-
ment model based on id4 attention as part of a hy-
brid end-to-end sr system. the nn component was
used for generating phonemes from the raw speech
signals, wherein the attention model included the
relative position information and a penalty term to
constrain the attention in a roughly monotone path.
while the one-to-one, monotone relationship be-
tween input and output serves well for speech recog-
nition, this will not be applicable for the id4.

6 experiments
6.1 datasets
we run our experiments on: chinese-english (zh-
en) and german-english (de-en).
zh-en: the training corpus is from the darpa
bolt chinese-english task consisting of about 5m
sentences and includes a mix of newswire, broad-
cast news, webblog from various sources. our dev
set is the concatenation of several tuning sets, hav-
ing 4491 sentences. three test-sets are nist mt06
(1664), mt08 news (691), and mt08 web (666).
de-en: the training corpus consists of about 4m

sentence pairs, which is a sub-set of wmt14. the
development and test sets are wmt-13 and wmt-
14 testsets (3000 sentences each) respectively.

6.2 id4 experimental setting

for the id4 systems, we use the vocabulary sizes
of 300k and 60k for the zh-en and de-en respec-
tively. we used the embedding dimension of 620 for
de-en and 300 for the other two language pairs6.
we    x the id56 gru layers to be of 1000 cells
each. we use adadelta (zeiler, 2012) or sgd7 for
optimization and update the model parameters with
a mini-batch size of 80.

for the large vocabulary zh-en setting, we use
a modi   ed vocabulary manipulation method (jean
et al., 2015) during training and restrict the out-
put vocabulary for each mini-batch with a candi-
date list. in generating the candidate list, we used
the top 2k most frequent target words along with
top-k8 candidates from the respective word-to-word
translation table learned using    fast align    (dyer et
al., 2013). we also augment the candidate list with
the tree-to-string phrases, where for many-to-many
phrases9, we assumed each word in source phrase to
be aligned to every other word in the target side of
the rule. following jean et al. (2015), we also added
the words from the target sides of the training data
to the candidate list only during training, to ensure
that the translations are reachable.

during translation, we dump the alignments (from
the attention mechanism for each timestep for each
sentence), and use these alignments to replace the
unk tokens either with potential targets (obtained
from the word translation table) or with the source
word (if no target was found).

considering the morphological richness of ger-
man, we follow the approach by sennrich et al.
(2015) and segment the german and english sides
into subwords by using byte-pair encoding (gage,
1994). we segment the two corpora separately with

6reducing the embedding dimensions allows us to speed up

training especially given the larger output vocabulary sizes.

7most of the times, we found them to be comparable in per-
formance across different languages. and we do not present a
detailed comparison here as it is beyond the scope of this paper.

8we    xed k = 10
9we    xed the maximum source side length to be 4.

system

mt06

smt
lvid4
+ temporal attention
mi et al. (2016)
tu et al. (2016)10

id7 (tb)
bp
34.93 (9.45)
0.95
34.53 (12.25)
0.96
36.34 (9.95)
0.92
0.91
35.34 (10.78)
n/a 32.47 (n/a)

mt08

news
id7 (tb)
31.12 (12.90)
28.86 (17.40)
31.49 (13.93)
29.85 (15.38)

bp
0.94
0.93
0.88
0.88

web
id7 (tb)
23.45 (17.72)
26.78 (17.57)
27.29 (16.04)
27.35 (16.67)

bp
0.90
0.97
0.94
0.96

n/a 25.23 (n/a)

table 2: zh-en: the brevity penalty (bp), id7     and tb     scores (within brackets). tb (the lower the score the better the
translation) measures (ter-id7)/2 and is agnostic to longer translations which will not be penalized by id7. id4 results are
on a large vocabulary (300k) and with unk replacement. the best scores for all id4 systems in each test set are in boldface and
the better smt scores are italisized. lvid4 stands for id4 model trained with large vocab manipulation.

distinct vocabularies.11 thus the de-en id4 sys-
tem does not have any unk tokens.

6.3 smt baselines
our traditional smt baseline is a hybrid syntax-
based tree-to-string model (zhao and al-onaizan,
2008). for zh-en task, we    rst parse the chinese
side with berkeley parser, then align the bilingual
sentences with giza++ and    nally extract hiero/
tree-to-string rules for translation.

the language models for are trained on the en-
glish side of the bitexts along with a large mono-
lingual corpora (around 10b words from gigaword
(ldc2011t07) and google news). we tune our
smt system with pro (hopkins and may, 2011)
to minimize (ter- id7)/2 (tb) on the dev set.

6.4 evaluation
we use two ways to evaluate our models. first
we present translation results for automatic metrics
id7 and tb. we use the dev set to choose the best
model, and then report test set scores for this model.
then, we also assess the alignment quality of
the competing id4 approaches for zh-en. given
the source and target sentence pairs in hand-aligned
alignment datasets, we forced decode with the id4
models to produce the reference translations while
recording the alignments from the attention layer.
we then compute the precision and recall by com-
paring the machine-generated and gold alignments.

11byte-pair encoding learned on the combined german and

english corpus did not perform as well.

11tu et al. (2016) is not comparable to others as they use 1.25
million training set, small vocabulary size 30k, tune on nist
2002 test set, and tune directly to maximize id7.

system
smt
id4
+ ensemble
mi et al. (2016)
local attn (luong et al., 2015)
+ ensemble
temporal attention
+ ensemble

id7     tb    
13.85
26.47
15.60
24.91
13.54
26.99
14.94
25.74
26.42
14.59
12.95
28.20
14.26
26.66
28.32
12.72
table 3: de-en: id7 and tb     scores on the wmt-2014
test set. id4 systems are trained with 60k byte-pair encoded
vocabulary. best single systems are boldfaced and while the
best ensemble scores (from 4 models) are italisized

6.4.1 translation evaluation

the main results are summarized in tables 2
and 3, and we begin with zh-en results. as has
been noted by others, the baseline id4 systems
generally perform poorly than their smt counter-
parts (mt08-web testset is an exception).

the coverage embedding model (labelled mi et al.
(2016)) improves both id7 and tb scores across
by up to 1 and 2 points respectively. our tempo-
ral attention model improve this further by a maxi-
mum of 1.4 tb point. the id7 scores show simi-
lar gains of about 1.5 points; interestingly the id7
scores of our proposed model are even better than
the traditional smt (mt06 and mt08-web) and is
comparable for the mt08-news testset.

the last row of the table lists the id7 scores
from tu et al. (2016). while our id7 scores are
not directly comparable because of the differences in
the training set, we do note here that our scores are

temporal
attn:

temporal
attn:

the protests that began on thursday at 06:30 in front of the mcdonald   s in 40th street and madison avenue
demanded that the casks and cooks of fast-food restaurants receive a minimum wage of usd 15 per hour ,
which is more than doubling the current minimum wage .
      , 5                             ,                                   z            ,        l          
         

table 4: output from our temporal attention model for examples in table 1 showing better translations

system
smt
lvid4
+ temporal attention

id7     ribes    
32.06
0.6894
0.7103
27.54
28.70
0.7232

table 5: en-jp: trained on 16m sentences bitext. id4 sys-
tems use 100k lv model with the candidate translations from
model-1 table along with top 2k most freq target words. shows
id7 and ribes (isozaki et al., 2010) scores.

3 points better than theirs on the mt06 and mt08
testsets in absolute terms. further, our smt system
offers a very strong baseline for two reasons: i) it
is a hiero-style system, which performs much better
than a corresponding phrase-based model for zh-en,
ii) it employs a robust language model trained not
just with the target side of the bitext but with large
monolingual english data as we noted earlier.

for the de-en setting, we compare our tempo-
ral attention model with the coverage-emdedding as
well as local attention. the coverage embedding
model improves the tb score by 0.65. the local
attention model performs better than the coverage
embedding improving the id7 by 0.7. our pro-
posed historical alignment model improves it further
by about 0.35 tb. finally, we tested ensemble mod-
els for baseline, local-attention and temporal atten-
tion id4 systems, which yield 1.7 to 2 id7 points
improvement over the corresponding single systems.
overall, the local attention and historical alignments
ensemble models outperform smt by     2 id7.
we present the results for en-jp setting (not dis-
cussed due to lack of space) that shows our model
considerably improving over baseline id4.

table 4 lists the output from our proposed model
for examples in table 1, showing better translations
without repetitive phrases. we also calculated the
no. of repetitive phrases for de-en (not tabulated for
need of space). we observed our model to produce
40% fewer repetitions compared to local attention.
average length of repetitions reduced from 7.27 for
local attention to 3.47 for temporal attention.

system
maxent
lvid4
+ temporal attention
mi et al. (2016)

prec.
74.86
47.88
50.33
51.11

rec.
77.10
41.06
43.57
41.42

f1
75.96
44.21
46.71
45.76

table 6: zh-en: alignment f 1 scores for different models on
447 hand-aligned sentences.

6.4.2 alignment evaluation

now we turn to evaluate the alignments gener-
ated by our temporal attention model; we do this in
the zh-en setting using 447 hand-aligned sentence
pairs. we compare against the coverage embed-
ding as well as the baseline lvid4. additionally
we also use a maximum id178 (maxent) aligner
trained on 67k hand-aligned data (table 6).

both coverage embedding and historical align-
ments approaches improve the f 1 score by 1.55 and
2.5 respectively over the baseline id4 model. the
temporal attention model gets highest recall and we
hypothesize that this is because the historical align-
ments are better in capturing many-to-many align-
ments. however all the id4 models are way be-
hind the maximum-id178 aligner.

7 conclusion and future work

we propose an extension to the id4 attention
model, where the model memorizes the alignments
temporally and aggregates them in the memory al-
lowing the attention model to remember its deci-
sions over past timesteps. we showed better perfor-
mance on two language pairs over the id4 base-
line and also against two alternative id4 models
employing coverage embedding and local attention.
it would be interesting to see if our tempo-
ral alignments model could be extended further to
memorize the alignments for the phrase-pairs across
individual sentence pairs. secondly, the local atten-
tion model is complementary to our approach and
hence it is natural to combine the two.

acknowledgment
this work is supported by darpa hr0011-12-c-
0015 (bolt), and the views and    ndings in this
paper are those of the authors and are not endorsed
by the darpa.

the views, opinions, and/or    ndings contained in
this article/presentation are those of the author/pre-
senter and should not be interpreted as representing
the of   cial views or policies, either expressed or im-
plied, of the darpa.

references
[bahdanau et al.2014] d. bahdanau, k. cho, and y. ben-
gio. 2014. id4 by jointly
learning to align and translate. arxiv e-prints,
september.

[cho et al.2014] kyunghyun cho, bart van merrienboer,
dzmitry bahdanau, and yoshua bengio. 2014. on
the properties of id4: encoder-
decoder approaches. corr, abs/1409.1259.

[chorowski et al.2014] jan chorowski, dzmitry bah-
danau, kyunghyun cho, and yoshua bengio. 2014.
end-to-end continuous
id103 using
attention-based recurrent nn:    rst results. corr,
abs/1412.1602.

[cohn et al.2016] trevor cohn, cong duy vu hoang,
ekaterina vymolova, kaisheng yao, chris dyer, and
gholamreza haffari. 2016.
incorporating structural
alignment biases into an attentional neural transla-
tion model. arxiv e-prints, january.

[dyer et al.2013] chris dyer, victor chahuneau, and
noah a. smith. 2013. a simple, fast, and effective
reparameterization of ibm model 2. in proceedings of
the 2013 conference of the north american chapter
of the association for computational linguistics:
human language technologies, pages 644   648, at-
lanta, georgia, june. association for computational
linguistics.

[feng et al.2016] shi feng, shujie liu, mu li, and ming
zhou. 2016. implicit distortion and fertility models
for attention-based encoder-decoder id4 model.
arxiv e-prints, january.

[gage1994] philip gage. 1994. a new algorithm for data

compression. c users j., 12(2):23   38, february.

[graves et al.2014] alex graves, greg wayne, and ivo
danihelka. 2014. id63s. corr,
abs/1410.5401.

[hopkins and may2011] mark hopkins and jonathan
in proceedings of

may. 2011. tuning as ranking.
emnlp.

[isozaki et al.2010] hideki

isozaki, tsutomu hirao,
kevin duh, katsuhito sudoh, and hajime tsukada.
2010. automatic evaluation of translation quality
in proceedings of the
for distant language pairs.
2010 conference on empirical methods in natural
language processing, emnlp    10, pages 944   952,
stroudsburg, pa, usa. association for computational
linguistics.

[jean et al.2015] s  ebastien

jean, kyunghyun cho,
roland memisevic, and yoshua bengio. 2015. on
using very large target vocabulary for neural machine
in proceedings of acl, pages 1   10,
translation.
beijing, china, july.

[kalchbrenner and blunsom2013] nal kalchbrenner and
phil blunsom. 2013. recurrent continuous translation
in proceedings of the 2013 conference on
models.
empirical methods in natural language processing,
seattle, october. association for computational lin-
guistics.

[koehn2004] philipp koehn. 2004. pharaoh: a beam
search decoder for phrase-based statistical machine
translation models. in proceedings of amta, pages
115   124.

[luong et al.2015] thang luong, hieu pham,

and
christopher d. manning. 2015. effective approaches
to attention-based id4.
in
proceedings of the 2015 conference on empirical
methods in natural language processing, pages
1412   1421, lisbon, portugal, september. association
for computational linguistics.

[mi et al.2016] haitao mi, baskaran sankaran, zhiguo
wang, and abe ittycheriah. 2016. a coverage em-
bedding model for id4. corr,
abs/1605.03148.

[sennrich et al.2015] rico sennrich, barry haddow, and
alexandra birch.
2015. neural machine trans-
lation of rare words with subword units. corr,
abs/1508.07909.

[sukhbaatar et al.2015] sainbayar sukhbaatar,

arthur
szlam, jason weston, and rob fergus. 2015. end-to-
end memory networks. in c. cortes, n. d. lawrence,
d. d. lee, m. sugiyama, and r. garnett, editors,
advances in neural information processing systems
28, pages 2440   2448. curran associates, inc.

2014.

[sutskever et al.2014] ilya sutskever, oriol vinyals, and
sequence to sequence
quoc v. le.
in advances
learning with neural networks.
in neural
information processing systems 27:
annual conference on neural information processing
systems 2014, december 8-13 2014, montreal,
quebec, canada, pages 3104   3112.

[tu et al.2016] zhaopeng tu, zhengdong lu, yang liu,
xiaohua liu, and hang li. 2016. modeling cover-
age for id4. in proceedings of

annual conference of association for computational
linguistics. association for computational linguis-
tics, august.

[weston et al.2014] jason weston, sumit chopra, and
antoine bordes. 2014. memory networks. corr,
abs/1410.3916.

[xu et al.2015] kelvin xu,

jimmy ba, ryan kiros,
kyunghyun cho, aaron courville, ruslan salakhudi-
nov, rich zemel, and yoshua bengio. 2015. show,
attend and tell: neural image id134 with
visual attention. in david blei and francis bach, edi-
tors, proceedings of the 32nd international conference

on machine learning (icml-15), pages 2048   2057.
jmlr workshop and conference proceedings.

[zeiler2012] matthew d. zeiler. 2012. adadelta: an

adaptive learning rate method. corr.

[zhao and al-onaizan2008] bing zhao and yaser al-
onaizan.
2008. generalizing local and non-local
word-reordering patterns for syntax-based machine
in proceedings of the conference on
translation.
empirical methods in natural language processing,
emnlp    08, pages 572   581, stroudsburg, pa, usa.
association for computational linguistics.

