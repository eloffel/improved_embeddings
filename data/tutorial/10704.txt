edinburgh id4 systems for wmt 16

rico sennrich and barry haddow and alexandra birch

school of informatics, university of edinburgh

{rico.sennrich,a.birch}@ed.ac.uk, bhaddow@inf.ed.ac.uk

6
1
0
2

 

n
u
j
 

7
2

 
 
]
l
c
.
s
c
[
 
 

2
v
1
9
8
2
0

.

6
0
6
1
:
v
i
x
r
a

abstract

systems
each

neural
language

translation
pairs,

we participated in the wmt 2016
shared news translation task by build-
ing
for
four
trained
english   czech,
in both directions:
english   german,
english   romanian
and english   russian. our systems are
based on an attentional encoder-decoder,
using bpe subid40 for
open-vocabulary translation with a    xed
vocabulary. we experimented with us-
ing automatic back-translations of
the
monolingual news corpus as additional
training data, pervasive dropout,
and
target-bidirectional models. all reported
methods give substantial
improvements,
and we see improvements of 4.3   11.2
id7 over our baseline systems.
in the
human evaluation, our systems were the
(tied) best constrained system for 7 out
of 8 translation directions in which we
participated.1 2
1 introduction

we participated in the wmt 2016 shared news
translation task by building neural
translation
systems for four language pairs: english   czech,
english   romanian
english   german,
and english   russian.
our
are
encoder-decoder
based
(bahdanau et al., 2015),
using bpe subword
segmentation for open-vocabulary translation
with a    xed vocabulary (sennrich et al., 2016b).
we experimented with using automatic back-
translations of the monolingual news corpus as

attentional

systems

on

an

1we have

implementation that we
the experiments as an open source toolkit:

used for
https://github.com/rsennrich/nematus

released the

2we

have
synthetic

released
training

   gs,
https://github.com/rsennrich/wmt16-scripts

data

scripts,
and

sample

con-
trained models:

additional
pervasive
bidirectional models.

training data (sennrich et al., 2016a),
dropout
target-

(gal, 2015),

and

2 baseline system

our systems are attentional encoder-decoder net-
works (bahdanau et al., 2015). we base our im-
plementation on the dl4mt-tutorial3, which we en-
hanced with new features such as ensemble decod-
ing and pervasive dropout.

we use minibatches of size 80, a maximum sen-
tence length of 50, id27s of size 500,
and hidden layers of size 1024. we clip the gra-
dient norm to 1.0 (pascanu et al., 2013). we train
the models with adadelta (zeiler, 2012), reshuf-
   ing the training corpus between epochs. we
validate the model every 10 000 minibatches via
id7 on a validation set (newstest2013, new-
stest2014, or half of newsdev2016 for en   ro).
we perform early stopping for single models, and
use the 4 last saved models (with models saved ev-
ery 30 000 minibatches) for the ensemble results.
note that ensemble scores are the result of a sin-
gle training run. due to resource limitations, we
did not train ensemble components independently,
which could result in more diverse models and bet-
ter ensembles.

decoding is performed with id125 with
a beam size of 12. for some language pairs, we
used the amuid4 c++ decoder4 as a more ef   -
cient alternative to the theano implementation of
the dl4mt tutorial.

2.1 byte-pair encoding (bpe)

to enable open-vocabulary translation, we seg-
ment words via byte-pair encoding (bpe)5
(sennrich et al., 2016b). bpe, originally devised

3https://github.com/nyu-dl/dl4mt-tutorial
4https://github.com/emjotde/amuid4
5https://github.com/rsennrich/subword-id4

as a compression algorithm (gage, 1994),
adapted to id40 as follows:

is

first, each word in the training vocabulary is
represented as a sequence of characters, plus an
end-of-word symbol. all characters are added to
the symbol vocabulary. then, the most frequent
symbol pair is identi   ed, and all its occurrences
are merged, producing a new symbol that is added
to the vocabulary. the previous step is repeated
until a set number of merge operations have been
learned.

bpe starts from a character-level segmentation,
but as we increase the number of merge opera-
tions, it becomes more and more different from a
pure character-level model in that frequent charac-
ter sequences, and even full words, are encoded as
a single symbol. this allows for a trade-off be-
tween the size of the model vocabulary and the
length of training sequences. the ordered list of
merge operations, learned on the training set, can
be applied to any text to segment words into sub-
word units that are in-vocabulary in respect to the
training set (except for unseen characters).

to increase consistency in the segmentation of
the source and target text, we combine the source
and target side of the training set for learning bpe.
for each language pair, we learn 89 500 merge op-
erations.

3 experimental features

for

training as described

3.1 synthetic training data
wmt provides
task participants with large
amounts of monolingual data, both in-domain
and out-of-domain. we exploit
this mono-
lingual data
in
(sennrich et al., 2016a). speci   cally, we sample
a subset of the available target-side monolingual
corpora, translate it automatically into the source
side of the respective language pair, and then
use this synthetic parallel data for training. for
example,
the back-translation
is performed with a ro   en system,
and
vice-versa.

for en   ro,

sennrich et al. (2016a) motivate the use of
re-
monolingual data with id20,
ducing over   tting, and better modelling of    u-
ency. we sample monolingual data from the news
crawl corpora6, which is in-domain with respect

6due to recency effects, we expect last year   s corpus to be
most relevant, and sampled from news crawl 2015 for en-
ro, en-ru and en-cs; for en-de, we re-used data from

type
parallel
synthetic (       en)
synthetic (en       )

de
4.2
4.2
3.6

cs ro ru
52.0
2.1
2.0
10.0
8.2
2.0

0.6
2.0
2.3

table 1: amount of parallel and synthetic training
data (number of sentences, in millions) for en-
* language pairs. for synthetic data, we separate
the data according to whether the original mono-
lingual language is english or not.

to the test set.

the

amount of monolingual data back-
translated for each translation direction ranges
from 2 million to 10 million sentences. statistics
about the amount of parallel and synthetic training
data are shown in table 1. with dl4mt, we
observed a translation speed of about 200 000
sentences per day (on a single titan x gpu).

3.2 pervasive dropout
for english   romanian, we observed poor per-
formance because of over   tting. to mitigate this,
we apply dropout to all layers in the network, in-
cluding recurrent ones.

previous work dropped out different units at
each time step. when applied to recurrent con-
nections, this has the downside that it impedes
the information    ow over long distances, and
pham et al. (2014) propose to only apply dropout
to non-recurrent connections.

instead, we follow the approach suggested by
gal (2015), and use the same dropout mask at each
time step. our implementation differs from the
recommendations by gal (2015) in one respect:
we also drop words at random, but we do so on
a token level, not on a type level. in other words,
if a word occurs multiple times in a sentence, we
may drop out any number of its occurrences, and
not just none or all.

in our english   romanian experiments, we
drop out full words (both on the source and tar-
get side) with a id203 of 0.1. for all other
layers, the dropout id203 is set to 0.2.

3.3 target-bidirectional translation
we found that during decoding,
the model
would occasionally assign a high id203 to
words based on the target context alone,
ig-
noring the source sentence. we speculate that

(sennrich et al., 2016a), which was randomly sampled from
news crawl 2007   2014.

system

baseline
+synthetic
+ensemble
+r2l reranking

en   de
dev
test
26.8
22.4
31.6
25.8
27.5
33.1
34.2
28.1

de   en
dev
test
28.5
26.4
36.2
29.9
31.5
37.5
38.6
32.1

table 2: english   german translation results
(id7) on dev (newstest2015) and test (new-
stest2016). submitted system in bold.

this is an instance of the label bias problem
(lafferty et al., 2001).

to mitigate this problem, we experiment with
training separate models that produce the target
text from right-to-left (r2l), and re-scoring the n-
best lists that are produced by the main (left-to-
right) models with these r2l models. since the
right-to-left model will see a complementary tar-
get context at each time step, we expect that the
averaged probabilities will be more robust. in par-
allel to our experiments, this idea was published
by liu et al. (2016).

we increase the size of the n-best-list to 50 for

the reranking experiments.

a possible criticism of the l-r/r-l reranking ap-
proach is that the gains actually come from adding
diversity to the ensemble, since we are now us-
ing two independent runs. however experiments
in (liu et al., 2016) show that a l-r/r-l reranking
systems is stronger than an ensemble created from
two independent l-r runs.

4 results

4.1 english   german
table 2 shows results for english   german. we
observe improvements of 3.4   5.7 id7 from
training with a mix of parallel and synthetic data,
compared to the baseline that is only trained on
parallel data. using an ensemble of the last 4
checkpoints gives further improvements (1.3   1.7
id7). our submitted system includes rerank-
ing of the 50-best output of the left-to-right model
with a right-to-left model     again an ensemble
of the last 4 checkpoints     with uniform weights.
this yields an improvements of 0.6   1.1 id7.

4.2 english   czech
for english   czech, we trained our baseline
model on the complete wmt16 parallel training
set (including czeng 1.6pre (bojar et al., 2016)),

until we observed convergence on our heldout
set (newstest2014). this took approximately 1m
minibatches, or 3 weeks. then we continued
training the model on a new parallel corpus,
comprising 8.2m sentences back-translated from
the czech monolingual news2015, 5 copies of
news-commentary v11, and 9m sentences sam-
pled from czeng 1.6pre. the model used for back-
translation was a neural mt model from earlier
experiments, trained on wmt15 data. the train-
ing on this synthetic mix continued for a further
400,000 minibatches.

the right-left model was trained using a simi-
lar process, but with the target side of the paral-
lel corpus reversed prior to training. the resulting
model had a slightly lower id7 score on the dev
data than the standard left-right model. we can see
in table 3 that back-translation improves perfor-
mance by 2.2   2.8 id7, and that the    nal system
(+r2l reranking) improves by 0.7   1.0 id7 on the
ensemble of 4, and 4.3   4.9 on the baseline.

for czech   english the training process was
similar to the above, except that we created the
synthetic training data (back-translated from sam-
ples of news2015 monolingual english) in batches
of 2.5m, and so were able to observe the effect
of increasing the amount of synthetic data. af-
ter training a baseline model on all the wmt16
parallel set, we continued training with a paral-
lel corpus consisting of 2 copies of the 2.5m sen-
tences of back-translated data, 5 copies of news-
commentary v11, and a matching quantity of data
sampled from czeng 1.6pre. after training this to
convergence, we restarted training from the base-
line model using 5m sentences of back-translated
data, 5 copies of news-commentary v11, and a
matching quantity of data sampled from czeng
1.6pre. we repeated this with 7.5m sentences
from news2015 monolingual, and then with 10m
sentences of news2015. the back-translations
were, as for english   czech, created with an ear-
lier id4 model trained on wmt15 data. our    -
nal czech   english was an ensemble of 8 systems
    the last 4 save-points of the 10m synthetic data
run, and the last 4 save-points of the 7.5m run. we
show this as ensemble8 in table 3, and the +syn-
thetic results are on the last (i.e. 10m) synthetic
data run.

we also show in table 4 how increasing the
amount of back-translated data affects the results.
we see that most of the gain from back-translation

system

baseline
+synthetic
+ensemble
+ensemble8
+r2l reranking

en   cs
dev
test
20.9
18.5
23.7
20.7
22.1
24.8

   

   

cs   en
dev
test
25.3
23.8
30.1
27.2
28.6
31.0
31.4
29.0

22.8

25.8

   

   

system

baseline
+dropout
+remove diacritics
+synthetic
+ensemble

en   ro
dev
test
19.2
20.2
24.2
23.9

-

29.3
29.0

-

28.1
28.2

ro   en
dev
test
22.7
23.6
27.8
28.7
30.0
29.2
33.3
34.8
35.3
33.9

english   czech translation results
table 3:
(id7) on dev (newstest2015) and test (new-
stest2016). submitted system in bold.

table 5: english   romanian translation results
(id7) on dev (newsdev2016), and test (new-
stest2016). submitted system in bold.

system

baseline
+2.5m synthetic
+5m synthetic
+7.5m synthetic
+10m synthetic

best single
test
dev
25.3
23.8
29.4
26.7
27.2
29.3
29.7
27.2
27.2
30.1

ensemble4
test
dev
26.8
25.5
30.4
27.7
28.2
30.4
30.8
28.4
28.6
31.0

czech   english translation results
table 4:
(id7) on dev (newstest2015) and test (new-
stest2016), after continued training with increas-
ing amounts of back-translated synthetic data. for
each row, training was continued from the baseline
model until convergence.

increasing the
comes with the    rst batch, but
amount of back-translated data does gradually im-
prove performance.

4.3 english   romanian
the results of our english   romanian experi-
ments are shown in table 5. this language pair
has the smallest amount of parallel training data,
and we found dropout to be very effective, yield-
ing improvements of 4   5 id7.7

we found that the use of diacritics was inconsis-
tent in the romanian training (and development)
data, so for romanian   english we removed dia-
critics from the romanian source side, obtaining
improvements of 1.3   1.4 id7.

synthetic training data gives improvements of
4.1   5.1 id7. for english   romanian, we found
that the best single system outperformed the en-
semble of the last 4 checkpoints on dev, and we
thus submitted the best single system as primary
system.

7we also tested dropout for en   de with 8 million sen-
tence pairs of training data, but found no improvement after
10 days of training. we speculate that dropout could still
be helpful for datasets of this size with longer training times
and/or larger networks.

system

baseline
+synthetic
+ensemble

en   ru
test
dev
20.3
21.3
24.3
25.8
27.0
26.0

ru   en
test
dev
22.5
22.7
26.9
27.1
28.3
28.0

table 6: english   russian translation results
(id7) on dev (newstest2015) and test (new-
stest2016). submitted system in bold.

4.4 english   russian

for english   russian, we cannot effectively learn
bpe on the joint vocabulary because alphabets
differ. we thus follow the approach described in
(sennrich et al., 2016b),    rst mapping the russian
text into latin characters via iso-9 id68,
then learning the bpe operations on the concate-
nation of the english and latinized russian train-
ing data, then mapping the bpe operations back
into cyrillic alphabet. we apply the latin bpe
operations to the english data (training data and
input), and both the cyrillic and latin bpe opera-
tions to the russian data.

translation results are shown in table 6. as
for the other language pairs, we observe strong
improvements from synthetic training data (4   4.4
id7). ensembles yield another 1.1   1.7 id7.

5 shared task results

table 7 shows the ranking of our submitted sys-
tems at the wmt16 shared news translation task.
our submissions are ranked (tied)    rst for 5 out of
8 translation directions in which we participated:
en   cs, en   de, and en   ro. they are also
the (tied) best constrained system for en   ru
and ro   en, or 7 out of 8 translation directions
in total.

our models
himl-syscomb

also

are
(peter et al., 2016),

used

in qt21-
ranked

direction id7 rank
en   cs
en   de
en   ro
en   ru
cs   en
de   en
ro   en
ru   en

1 of 9
1 of 11
2 of 10
1 of 8
1 of 4
1 of 6
2 of 5
3 of 6

human rank
1
of 20
of 15
1
of 12
1   2
2   5
of 12
of 12
1
of 10
1
of 7
2
5
of 10

shared

(id7)

automatic

at wmt16

submitted systems

and human
table 7:
(uedin-
ranking of our
translation
id4)
task.
automatic rankings are taken from
http://matrix.statmt.org , only con-
human rankings
sidering primary systems.
include anonymous online systems,
and for
en   cs, systems from the tuning task.

news

for en   ro,

1   2
(junczys-dowmunt et al., 2016),
for en   ru, and 1   2 for ru   en.

and

in amu-uedin
2   3

ranked

6 conclusion

we describe edinburgh   s neural machine transla-
tion systems for the wmt16 shared news trans-
lation task. for all translation directions, we ob-
serve large improvements in translation quality
from using synthetic parallel training data, ob-
tained by back-translating in-domain monolingual
target-side data. pervasive dropout on all layers
was used for english   romanian, and gave sub-
stantial improvements. for english   german and
english   czech, we trained a right-to-left model
with reversed target side, and we found rerank-
ing the system output with these reversed models
helpful.

acknowledgments

this project has received funding from the euro-
pean union   s horizon 2020 research and innova-
tion programme under grant agreements 645452
(qt21), 644333 (tramooc) and 644402 (himl).

references

[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2015. neural machine
translation by jointly learning to align and trans-
late. in proceedings of the international conference
on learning representations (iclr).

[bojar et al.2016] ond  rej bojar, ond  rej du  ek, tom
kocmi, jind  rich libovick  , michal nov  k, martin
popel, roman sudarikov, and du  an vari  . 2016.
czeng 1.6: enlarged czech-english parallel cor-
pus with processing tools dockered.
in text,
speech and dialogue: 19th international confer-
ence, tsd 2016, brno, czech republic, september
12-16, 2016, proceedings. springer verlag, septem-
ber 12-16. in press.

[gage1994] philip gage. 1994. a new algorithm for
data compression. c users j., 12(2):23   38, febru-
ary.

[gal2015] yarin gal. 2015. a theoretically grounded
application of dropout in recurrent neural net-
works. arxiv e-prints.

[junczys-dowmunt et al.2016] marcin

junczys-
dowmunt, tomasz dwojak, and rico sennrich.
2016.
the amu-uedin submission to the
wmt16 news translation task: attention-based
id4 models as feature functions in phrase-based
smt.
in proceedings of the first conference on
machine translation (wmt16), berlin, germany.

[lafferty et al.2001] john d. lafferty, andrew mccal-
lum, and fernando c. n. pereira. 2001. conditional
random fields: probabilistic models for segment-
ing and labeling sequence data.
in proceedings
of the eighteenth international conference on ma-
chine learning, pages 282   289, san francisco, ca,
usa. morgan kaufmann publishers inc.

[liu et al.2016] lemao liu, masao utiyama, andrew
finch, and eiichiro sumita. 2016. agreement on
target-bidirectional id4 .
in naacl hlt 16, san diego, ca.

[pascanu et al.2013] razvan pascanu, tomas mikolov,
and yoshua bengio. 2013. on the dif   culty of train-
ing recurrent neural networks. in proceedings of the
30th international conference on machine learn-
ing, icml 2013, pages 1310   1318, , atlanta, ga,
usa.

[peter et al.2016] jan-thorsten peter, tamer alkhouli,
hermann ney, matthias huck, fabienne braune,
alexander fraser, ale   tamchyna, ond  rej bojar,
barry haddow, rico sennrich, fr  d  ric blain, lu-
cia specia, jan niehues, alex waibel, alexandre
allauzen, lauriane aufrant, franck burlot, elena
knyazeva, thomas lavergne, fran  ois yvon, and
marcis pinnis. 2016. the qt21/himl combined
machine translation system. in proceedings of the
first conference on machine translation (wmt16),
berlin, germany.

[pham et al.2014] vu pham, th  odore bluche, christo-
pher kermorvant, and j  r  me louradour.
2014.
dropout improves recurrent neural networks for
handwriting recognition.
in 14th international
conference on frontiers in handwriting recogni-
tion, icfhr 2014, crete, greece, september 1-4,
2014, pages 285   290.

[sennrich et al.2016a] rico sennrich, barry haddow,
and alexandra birch.
improving neu-
ral machine translation models with monolingual
data. in proceedings of the 54th annual meeting of
the association for computational linguistics (acl
2016), berlin, germany.

2016a.

[sennrich et al.2016b] rico sennrich, barry haddow,
and alexandra birch.
2016b. neural machine
translation of rare words with subword units. in
proceedings of the 54th annual meeting of the asso-
ciation for computational linguistics (acl 2016),
berlin, germany.

[zeiler2012] matthew d. zeiler. 2012. adadelta:
corr,

an adaptive learning rate method.
abs/1212.5701.

