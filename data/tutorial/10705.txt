linguistic input features improve id4

rico sennrich and barry haddow

school of informatics, university of edinburgh

rico.sennrich@ed.ac.uk, bhaddow@inf.ed.ac.uk

6
1
0
2

 

n
u
j
 

7
2

 
 
]
l
c
.
s
c
[
 
 

2
v
2
9
8
2
0

.

6
0
6
1
:
v
i
x
r
a

abstract

id4 has recently
achieved impressive results, while using
little in the way of external linguistic in-
formation.
in this paper we show that
the strong learning capability of neural
mt models does not make linguistic fea-
tures redundant; they can be easily incor-
porated to provide further improvements
in performance. we generalize the em-
bedding layer of the encoder in the at-
tentional encoder   decoder architecture to
support the inclusion of arbitrary features,
in addition to the baseline word feature.
we add morphological features, part-of-
speech tags, and syntactic dependency la-
bels as input features to english   german
and english   romanian neural machine
translation systems.
in experiments on
wmt16 training and test sets, we    nd that
linguistic input features improve model
quality according to three metrics: per-
plexity, id7 and chrf3. an open-
source implementation of our neural mt
system is available1, as are sample    les
and con   gurations2.

1 introduction

results

id4 has recently achieved
(bahdanau et al., 2015;
impressive
jean et al., 2015), while
learning from raw,
text and using little
sentence-aligned parallel
linguistic information.3
in the way of external
however, we hypothesize that various levels of
linguistic annotation can be valuable for neural
machine translation. lemmatisation can reduce
data sparseness, and allow in   ectional variants of

the same word to explicitly share a representation
in the model. other types of annotation, such as
parts-of-speech (pos) or syntactic dependency
labels, can help in disambiguation. in this paper
we investigate whether linguistic information is
bene   cial to neural translation models, or whether
their strong learning capability makes explicit
linguistic features redundant.

let us motivate the use of linguistic features us-
ing examples of actual translation errors by neu-
ral mt systems.
in translation out of english,
one problem is that the same surface word form
may be shared between several word types, due to
homonymy or word formation processes such as
conversion. for instance, close can be a verb, ad-
jective, or noun, and these different meanings of-
ten have distinct translations into other languages.
consider the following english   german exam-
ple:

1. we thought a win like this might be close.

2. wir dachten, dass ein solcher sieg nah sein

k  nnte.

3. *wir dachten, ein sieg wie dieser k  nnte

schlie  en.

for the english source sentence in example 1
(our translation in example 2), a neural mt sys-
tem (our baseline system from section 4) mis-
translates close as a verb, and produces the ger-
man verb schlie  en (example 3), even though
close is an adjective in this sentence, which has
the german translation nah.
intuitively, part-
of-speech annotation of the english input could
disambiguate between verb, noun, and adjective
meanings of close.

as a second example, consider the following

1https://github.com/rsennrich/nematus
2https://github.com/rsennrich/wmt16-scripts
3linguistic tools are most commonly used in preprocess-

ing, e.g. for turkish segmentation (g  l  ehre et al., 2015).

german   english example:

4. gef  hrlich ist die route aber dennoch .

dangerous is the route but still .

5. however the route is dangerous .

6. *dangerous is the route , however .

german main clauses have a verb-second (v2)
word order, whereas english word order is gener-
ally svo. the german sentence (example 4; en-
glish reference in example 5) topicalizes the pred-
icate gef  hrlich    dangerous   , putting the subject
die route    the route    after the verb. our baseline
system (example 6) retains the original word or-
der, which is highly unusual in english, especially
for prose in the news domain. a syntactic annota-
tion of the source sentence could support the atten-
tional encoder-decoder in learning which words in
the german source to attend (and translate)    rst.

we will investigate the usefulness of linguistic
features for the language pair german   english,
considering the following linguistic features:

    lemmas

    subword tags (see section 3.2)

    morphological features

    pos tags

    dependency labels

the inclusion of lemmas is motivated by the
hope for a better generalization over in   ectional
variants of the same word form. the other lin-
guistic features are motivated by disambiguation,
as discussed in our introductory examples.

2 id4

we follow the id4 archi-
tecture by bahdanau et al. (2015), which we will
brie   y summarize here.

the id4 system is imple-
mented as an attentional encoder-decoder network
with recurrent neural networks.

the encoder

is a bidirectional neural net-
work with id149 (cho et al., 2014)
that reads an input sequence x = (x1, ..., xm)
and calculates a forward sequence of hidden
      
h m), and a backward sequence
states (
      
      
h j are
(
h 1, ...,
concatenated to obtain the annotation vector hj.

      
h 1, ...,
      
h m). the hidden states

      
h j and

the decoder is a recurrent neural network that
predicts a target sequence y = (y1, ..., yn). each
word yi is predicted based on a recurrent hidden
state si, the previously predicted word yi   1, and

a context vector ci. ci is computed as a weighted
sum of the annotations hj. the weight of each
annotation hj is computed through an alignment
model   ij, which models the id203 that yi is
aligned to xj. the alignment model is a single-
layer feedforward neural network that is learned
jointly with the rest of the network through back-
propagation.

a detailed description can be found in
(bahdanau et al., 2015), although our implemen-
tation is based on a slightly modi   ed form of
this architecture, released for the dl4mt tutorial4.
training is performed on a parallel corpus with
stochastic id119.
for translation, a
id125 with small beam size is employed.

2.1 adding input features
our main innovation over the standard encoder-
decoder architecture is that we represent
the
features
encoder
(alexandrescu and kirchhoff, 2006).

input as a combination of

we here show the equation for the forward
states of the encoder (for the simple id56 case;
consider (bahdanau et al., 2015) for gru):

      
h j = tanh(

      
w exj +

      
u

      
h j   1)

(1)

      
w     rn  m,

where e     rm  kx is a id27 ma-
      
u     rn  n are weight matrices,
trix,
with m and n being the id27 size and
number of hidden units, respectively, and kx be-
ing the vocabulary size of the source language.

we generalize this to an arbitrary number of fea-

tures |f |:

      
h j = tanh(

      
w (

|f |
n

k=1

ekxjk) +

      
u

      
h j   1)

(2)

where k is the vector concatenation, ek    
rmk  kk are the feature embedding matrices, with
p|f |
k=1 mk = m, and kk is the vocabulary size of
the kth feature. in other words, we look up sepa-
rate embedding vectors for each feature, which are
then concatenated. the length of the concatenated
vector matches the total embedding size, and all
other parts of the model remain unchanged.

3 linguistic input features

our generalized model of the previous section
supports an arbitrary number of input features.

4https://github.com/nyu-dl/dl4mt-tutorial

in this paper, we will focus on a number of
well-known linguistic features. our main em-
pirical question is if providing linguistic fea-
tures to the encoder improves the translation qual-
ity of id4 systems, or if
the information emerges from training encoder-
decoder models on raw text, making its inclu-
sion via explicit features redundant. all
lin-
guistic features are predicted automatically; we
use stanford corenlp (toutanova et al., 2003;
minnen et al., 2001; chen and manning, 2014) to
annotate the english input for english   german,
and parzu (sennrich et al., 2013) to annotate the
german input for german   english. we here dis-
cuss the individual features in more detail.

3.1 lemma

using lemmas as input features guarantees sharing
of information between word forms that share the
same base form. in principle, neural models can
learn that in   ectional variants are semantically re-
lated, and represent them as similar points in the
continuous vector space (mikolov et al., 2013).
however, while this has been demonstrated for
high-frequency words, we expect that a lemma-
tized representation increases data ef   ciency; low-
frequency variants may even be unknown to word-
level models. with character- or subword-level
models, it is unclear to what extent they can learn
the similarity between low-frequency word forms
that share a lemma, especially if the word forms
are super   cially dissimilar. consider the follow-
ing two german word forms, which share the
lemma liegen    lie   :

    liegt    lies    (3.p.sg. present)

    l  ge    lay    (3.p.sg. subjunctive ii)

the lemmatisers we use are based on    nite-state
methods, which ensures a large coverage, even
for infrequent word forms. we use the zmorge
analyzer
(schmid et al., 2004;
sennrich and kunz, 2014), and the lemmatiser
in the stanford corenlp toolkit
for english
(minnen et al., 2001).

for german

3.2 subword tags
in our experiments, we operate on the level of
subwords to achieve open-vocabulary translation
with a    xed symbol vocabulary, using a seg-
mentation based on byte-pair encoding (bpe)

(sennrich et al., 2016c). we note that in bpe seg-
mentation, some symbols are potentially ambigu-
ous, and can either be a separate word, or a sub-
word segment of a larger word. also, text is rep-
resented as a sequence of subword units with no
explicit word boundaries, but word boundaries are
potentially helpful to learn which symbols to at-
tend to, and when to forget information in the re-
current layers. we propose an annotation of sub-
word structure similar to popular iob format for
chunking and id39, marking
if a symbol in the text forms the beginning (b), in-
side (i), or end (e) of a word. a separate tag (o)
is used if a symbol corresponds to the full word.

3.3 morphological features
for german   english,
the parser annotates the
german input with morphological features. dif-
ferent word types have different sets of features    
for instance, nouns have case, number and gender,
while verbs have person, number, tense and aspect
    and features may be underspeci   ed. we treat
the concatenation of all morphological features of
a word, using a special symbol for underspeci   ed
features, as a string, and treat each such string as a
separate feature value.

3.4 pos tags and dependency labels
in our introductory examples, we motivated pos
tags and dependency labels as possible disam-
biguators. each word is associated with one pos
tag, and one dependency label. the latter is the
label of the edge connecting a word to its syntac-
tic head, or    root    if the word has no syntactic
head.

3.5 on using word-level features in a

subword model

we segment rare words into subword units using
bpe. the subword tags encode the segmentation
of words into subword units, and need no fur-
ther modi   cation. all other features are originally
word-level features. to annotate the segmented
source text with features, we copy the word   s fea-
ture value to all its subword units. an example is
shown in figure 1.

4 evaluation

we evaluate our systems on the wmt16 shared
translation task english   german. the parallel
training data consists of about 4.2 million sentence
pairs.

root

nsubj

prep

pobj
det

leonidas

nnp

begged
vbd

in
in

words
lemmas
subword tags
pos
dep

le:

b

nnp
nsubj

oni:

i

nnp
nsubj

das

e

nnp
nsubj

leonidas

leonidas

leonidas

the
dt
beg:
beg
b

arena
nn
ged
beg
e

vbd vbd
root
root

root
root

.
.
in
in
o
in
prep

the
the
o
dt
det

arena
arena

o
nn
pobj

.
.
o
.

root

figure 1: original dependency tree for sentence leonidas begged in the arena ., and our feature repre-
sentation after bpe segmentation.

to

via

enable

encode words

open-vocabulary

transla-
joint bpe5
tion, we
(sennrich et al., 2016c),
learning 89 500 merge
operations on the concatenation of the source
and target side of the parallel training data. we
use minibatches of size 80, a maximum sentence
length of 50, id27s of size 500, and
hidden layers of size 1024. we clip the gradient
norm to 1.0 (pascanu et al., 2013). we train the
models with adadelta (zeiler, 2012), reshuf   ing
the training corpus between epochs. we validate
the model every 10 000 minibatches via id7
and perplexity on a validation set (newstest2013).
for neural mt, perplexity is a useful measure
of how well the model can predict a reference
translation given the source sentence. perplex-
ity is thus a good indicator of whether input fea-
tures provide any bene   t to the models, and we re-
port the best validation set perplexity of each ex-
periment. to evaluate whether the features also
increase translation performance, we report case-
sensitive id7 scores with mteval-13b.perl on
two test sets, newstest2015 and newstest2016. we
also report chrf3 (popovi  c, 2015), a character n-
gram f3 score which was found to correlate well
with human judgments, especially for translations
out of english (stanojevi  c et al., 2015).6 the two
metrics may occasionally disagree, partly because
they are highly sensitive to the length of the out-
put. id7 is precision-based, whereas chrf3
considers both precision and recall, with a bias for
recall. for id7, we also report whether differ-
ences between systems are statistically signi   cant
according to a bootstrap resampling signi   cance
test (riezler and maxwell, 2005).

we train models for about a week, and report

5https://github.com/rsennrich/subword-id4
6we use the re-implementation included with the subword

code

feature
subword tags
pos tags
morph. features
dependency labels
lemmas
words

input vocabulary
en
4
46
-
46

embedding
all single
5
5
10
10
10
10
10
10
167
800000 1500000 85000 115
78500
*
*

de model
4
54
1400
46

4
54
1400
33

85000 85000

table 1: vocabulary size, and size of embedding
layer of linguistic features, in system that includes
all features, and contrastive experiments that add
a single feature over the baseline. the embedding
layer size of the word feature is set to bring the
total size to 500.

results for an ensemble of the 4 last saved models
(with models saved every 12 hours). the ensem-
ble serves to smooth the variance between single
models.

decoding is performed with id125 with a

beam size of 12.

to ensure that performance improvements are
not simply due to an increase in the number of
model parameters, we keep the total size of the
embedding layer    xed to 500. table 1 lists the
embedding size we use for linguistic features    
the embedding layer size of the word-level fea-
ture varies, and is set to bring the total embedding
layer size to 500. if we include the lemma feature,
we roughly split the embedding vector one-to-two
between the lemma feature and the word feature.
the table also shows the network vocabulary size;
for all features except lemmas, we can represent
all feature values in the network vocabulary     in
the case of words, this is due to bpe segmenta-
tion. for lemmas, we choose the same vocabulary
size as for words, replacing rare lemmas with a
special unk symbol.

sennrich et al. (2016b) report large gains from
using monolingual in-domain training data, auto-

matically back-translated into the source language
to produce a synthetic parallel training corpus. we
use the synthetic corpora produced in these exper-
iments7 (3.6   4.2 million sentence pairs), and we
trained systems which include this data to compare
against the state of the art. we note that our exper-
iments with this data entail a syntactic annotation
of automatically translated data, which may be a
source of noise. for the systems with synthetic
data, we double the training time to two weeks.

we

features

sentence

linguistic

translation

also evaluate
lower-resourced

for
the
direction
english   romanian, with 0.6 million sen-
training data, and 2.2
tence pairs of parallel
million
pairs
paral-
lel data. we use the same linguistic fea-
tures as for english   german. we follow
sennrich et al. (2016a) in the con   guration, and
use dropout for the english   romanian systems.
we drop out full words (both on the source and
target side) with a id203 of 0.1. for all other
layers, the dropout id203 is set to 0.2.

synthetic

of

4.1 results

2

results

our main
and

for
table
shows
german   english,
english   german.
the baseline system is a neural mt system with
only one input feature, the (sub)words themselves.
for both translation directions, linguistic features
improve the best perplexity on the development
data (47.3     46.2, and 54.9     52.9, respectively).
for german   english, the linguistic features lead
to an increase of 1.5 id7 (31.4   32.9) and
0.5 chrf3 (58.0     58.5), on the newstest2016
test set.
for english   german, we observe
improvements of 0.6 id7 (27.8     28.4) and 1.2
chrf3 (56.0     57.2).

to evaluate the effectiveness of different lin-
guistic features in isolation, we performed con-
trastive experiments in which only a single feature
was added to the baseline. results are shown in
table 3. unsurprisingly, the combination of all
features (table 2) gives the highest improvement,
averaged over metrics and test sets, but most fea-
tures are bene   cial on their own. subword tags
give small improvements for english   german,
but not for german   english. all other features
outperform the baseline in terms of perplexity, and
yield signi   cant improvements in id7 on at least

7the

corpora

are

available

at

one test set. the gain from different features is not
fully cumulative; we note that the information en-
coded in different features overlaps. for instance,
both the dependency labels and the morphologi-
cal features encode the distinction between ger-
man subjects and accusative objects, the former
through different labels (subj and obja), the lat-
ter through grammatical case (nominative and ac-
cusative).

training data.

we also evaluated adding linguistic features
to a stronger baseline, which includes synthetic
parallel
in addition, we com-
pare our neural systems against phrase-based (pb-
smt) and syntax-based (sbsmt) systems by
(williams et al., 2016), all of which make use
of linguistic annotation on the source and/or
target side.
results are shown in table 4.
for german   english, we observe similar im-
provements in the best development perplexity
(45.2     44.1), test set id7 (37.5   38.5) and
chrf3 (62.2     62.8). our test set id7
is on par to the best submitted system to this
year   s wmt 16 shared translation task, which
is similar to our baseline mt system, but which
also uses a right-to-left decoder for reranking
(sennrich et al., 2016a). we expect that linguis-
tic input features and bidirectional decoding are
orthogonal, and that we could obtain further im-
provements by combining the two.

for english   german, improvements in devel-
opment set perplexity carry over (49.7     48.4),
but we see only small, non-signi   cant differences
in id7 and chrf3. while we cannot clearly ac-
count for the discrepancy between perplexity and
translation metrics, factors that potentially lower
the usefulness of linguistic features in this setting
are the stronger baseline, trained on more data,
and the low robustness of linguistic tools in the
annotation of the noisy, synthetic data sets. both
our baseline neural mt systems and the systems
with linguistic features substantially outperform
phrase-based and syntax-based systems for both
translation directions.

in the previous tables, we have reported the best
perplexity. to address the question about the ran-
domness in perplexity, and whether the best per-
plexity just happened to be lower for the systems
with linguistic features, we show perplexity on
our development set as a function of training time
for different systems (figure 2). we can see that
perplexity is consistently lower for the systems

http://statmt.org/rsennrich/wmt16_backtranslations/

system

baseline
all features

ppl    
dev
47.3
46.2

german   english
id7    

chrf3    

test15
27.9
28.7*

test16
31.4
32.9*

test15
54.0
54.8

test16
58.0
58.5

ppl    
dev
54.9
52.9

english   german
id7    

chrf3    

test15
23.0
23.8*

test16
27.8
28.4*

test15
52.6
53.9

test16
56.0
57.2

table 2: german   english translation results: best perplexity on dev (newstest2013), and id7 and
chrf3 on test15 (newstest2015) and test16 (newstest2016). id7 scores that are signi   cantly different
(p < 0.05) from respective baseline are marked with (*).

system

baseline
lemmas
subword tags
morph. features
pos tags
dependency labels

ppl    
dev
47.3
47.1
47.3
47.1
46.9
46.9

german   english
id7    

chrf3    

test15
27.9
28.4
27.7
28.2
28.1
28.1

test16
31.4
32.3*
31.5
32.4*
32.4*
31.8*

test15
54.0
54.6
54.0
54.3
54.1
54.2

test16
58.0
58.7
58.1
58.4
57.8
58.3

ppl    
dev
54.9
53.4
54.7

-

53.2
54.0

english   german
id7    

chrf3    

test15
23.0
23.8*
23.6*

-

24.0*
23.4*

test16
27.8
28.5*
28.1
-

28.9*
28.0

test15
52.6
53.7
53.2

-

53.3
53.1

test16
56.0
56.7
56.4

-

56.8
56.5

table 3: contrastive experiments with individual linguistic features: best perplexity on dev (new-
stest2013), and id7 and chrf3 on test15 (newstest2015) and test16 (newstest2016). id7 scores
that are signi   cantly different (p < 0.05) from respective baseline are marked with (*).

system

pbsmt (williams et al., 2016)
sbsmt (williams et al., 2016)
baseline
all features

ppl    
dev
-
-

45.2
44.1

german   english
id7    

chrf3    

test15
29.9
29.5
31.5
32.1*

test16
35.1
34.4
37.5
38.5*

test15
56.2
56.0
57.0
57.5

test16
60.9
61.0
62.2
62.8

ppl    
dev
-
-

49.7
48.4

english   german
id7    

chrf3    

test15
23.7
24.5
27.5
27.1

test16
28.4
30.6
33.1
33.2

test15
52.6
55.3
56.3
56.5

test16
56.6
59.9
60.5
60.6

table 4: german   english translation results with additional, synthetic training data: best perplexity on
dev (newstest2013), and id7 and chrf3 on test15 (newstest2015) and test16 (newstest2016). id7
scores that are signi   cantly different (p < 0.05) from respective baseline are marked with (*).

y
t
i
x
e
l
p
r
e
p

120

100

80

60

40

0

en-de baseline (synth. data)
en-de all features (synth. data)
de-en baseline (synth. data)
de-en all features (synth. data)

20

10
50
training time (minibatches   10000)

40

30

sentence
gef  hrlich ist die route aber dennoch.

system
source
reference however the route is dangerous.
dangerous is the route, however.
baseline
all features however, the route is dangerous.
[we thought] a win like this might be close.
source
[...] dass ein solcher gewinn nah sein k  nnte.
reference
[...] ein sieg wie dieser k  nnte schlie  en.
baseline
all features [...] ein sieg wie dieser k  nnte nah sein.

table 6: translation examples illustrating the ef-
fect of adding linguistic input features.

60

5 related work

2:

english   german

figure
and
german   english (red) development set per-
plexity as a function of training time (number of
minibatches) with and without linguistic features.

(black)

system
(peter et al., 2016)
baseline
all features
baseline (+synth. data)
all features (+synth. data)

ppl    

-

74.9
72.7
50.9
50.1

id7    
28.9
23.8
24.8*
28.2
29.2*

chrf3    

57.1
52.5
53.5
56.1
56.6

table 5: english   romanian translation results:
best perplexity on newsdev2016, and id7 and
chrf3 on newstest2016. id7 scores that are
signi   cantly different (p < 0.05) from respective
baseline are marked with (*).

trained with linguistic features.

table 5 shows results for a lower-resourced
language pair, english   romanian. with lin-
guistic features, we observe improvements of 1.0
id7 over the baseline, both for the systems
trained on parallel data only (23.8   24.8), and
the systems which use synthetic training data
(28.2   29.2). according to id7, the best sub-
mission to wmt16 was a system combination by
peter et al. (2016). our best system is competitive
with this submission.

table 6 shows translation examples of our base-
line, and the system augmented with linguis-
tic features. we see that the augmented neural
mt systems, in contrast to the respective base-
lines, successfully resolve the reordering for the
german   english example, and the disambigua-
tion of close for the english   german example.

have

neural

features

language

been
linguistic
modelling
used
in
(alexandrescu and kirchhoff, 2006),
and
are
also used in other tasks for which neural models
have recently been employed, such as syntactic
parsing (chen and manning, 2014). this paper
addresses the question whether linguistic features
on the source side are bene   cial
for neural
machine translation. on the target side, linguistic
features are harder to obtain for a generation task
such as machine translation, since this would
require incremental parsing of the hypotheses at
test time, and this is possible future work.

a

is

still

others,

among

our model

incorporates
information from a dependency annotation,
but
sequence-to-sequence model.
eriguchi et al. (2016) propose a tree-to-sequence
model whose encoder computes vector represen-
tations for each phrase in the source tree. their
focus is on exploiting the (unlabelled) structure
of a syntactic annotation, whereas we are focused
on the disambiguation power of the functional
dependency labels.

factored translation models are often used in
phrase-based smt (koehn and hoang, 2007) as a
means to incorporate extra linguistic information.
however, neural mt can provide a much more
   exible mechanism for adding such information.
because phrase-based models cannot easily gen-
eralize to new feature combinations, the individ-
ual models either treat each feature combination
as an atomic unit, resulting in data sparsity, or as-
sume independence between features, for instance
by having separate language models for words and
pos tags. in contrast, we exploit the strong gen-
eralization ability of neural networks, and expect
that even new feature combinations, e.g. a word
that appears in a novel syntactic function, are han-
dled gracefully.

and

one could consider

perform multi-source

the lemmatized rep-
the input as a second source
resentation of
translation
text,
the main technical
(zoph and knight, 2016).
the encoder
difference is that in our approach,
and attention layers are shared between features,
which we deem appropriate for the types of
features that we tested.

6 conclusion

in this paper we investigate whether linguistic in-
put features are bene   cial to neural machine trans-
lation, and our empirical evidence suggests that
this is the case.

we describe a generalization of the encoder
in the popular attentional encoder-decoder archi-
tecture for id4 that al-
lows for the inclusion of an arbitrary number
of input features. we empirically test the in-
clusion of various linguistic features,
including
lemmas, part-of-speech tags, syntactic depen-
dency labels, and morphological features,
into
english   german, and english   romanian neu-
ral mt systems. our experiments show that
the linguistic features yield improvements over
our baseline, resulting in improvements on new-
stest2016 of 1.5 id7 for german   english, 0.6
id7 for english   german, and 1.0 id7 for
english   romanian.

in the future, we expect several developments
that will shed more light on the usefulness of lin-
guistic (or other) input features, and whether they
will establish themselves as a core component of
id4. on the one hand, the
machine learning capability of neural architectures
is likely to increase, decreasing the bene   t pro-
vided by the features we tested. on the other hand,
there is potential to explore the inclusion of novel
features for neural mt, which might prove to be
even more helpful than the ones we investigated,
and the features we investigated may prove espe-
cially helpful for some translation settings, such as
very low-resourced settings and/or translation set-
tings with a highly in   ected source language.

acknowledgments

this project has received funding from the euro-
pean union   s horizon 2020 research and innova-
tion programme under grant agreements 645452
(qt21), and 644402 (himl).

references
[alexandrescu and kirchhoff2006] andrei
2006.

alexan-
factored
drescu and katrin kirchhoff.
neural language models.
in proceedings of
the human language technology conference of
the naacl, companion volume: short papers,
pages 1   4, new york city, usa. association for
computational linguistics.

[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2015. neural machine
translation by jointly learning to align and trans-
late. in proceedings of the international conference
on learning representations (iclr).

[chen and manning2014] danqi chen and christopher
manning. 2014. a fast and accurate dependency
parser using neural networks.
in proceedings of
the 2014 conference on empirical methods in natu-
ral language processing (emnlp), pages 740   750,
doha, qatar. association for computational lin-
guistics.

[cho et al.2014] kyunghyun cho, bart van merrien-
boer, caglar gulcehre, dzmitry bahdanau, fethi
bougares, holger schwenk, and yoshua bengio.
2014. learning phrase representations using id56
encoder   decoder for statistical machine transla-
tion.
in proceedings of the 2014 conference on
empirical methods in natural language processing
(emnlp), pages 1724   1734, doha, qatar. associa-
tion for computational linguistics.

[eriguchi et al.2016] akiko

kazuma
hashimoto, and yoshimasa tsuruoka.
2016.
tree-to-sequence attentional neural machine
translation. arxiv e-prints.

eriguchi,

[g  l  ehre et al.2015]   aglar g  l  ehre, orhan firat,
kelvin xu, kyunghyun cho, lo  c barrault, huei-
chi lin, fethi bougares, holger schwenk, and
yoshua bengio.
2015. on using monolingual
corpora in id4. corr,
abs/1503.03535.

[jean et al.2015] s  bastien

jean,

orhan

firat,
kyunghyun cho, roland memisevic, and yoshua
bengio. 2015. montreal neural machine transla-
tion systems for wmt   15 . in proceedings of the
tenth workshop on id151,
pages 134   140, lisbon, portugal. association for
computational linguistics.

[koehn and hoang2007] philipp koehn

and hieu
in
hoang. 2007. factored translation models.
proceedings of
the 2007 joint conference on
empirical methods in natural language processing
and computational natural language learn-
ing (emnlp-conll), pages 868   876, prague,
czech republic. association for computational
linguistics.

[mikolov et al.2013] tomas mikolov, wen-tau yih, and
geoffrey zweig. 2013. linguistic regularities in

[sennrich et al.2016a] rico sennrich, barry haddow,
and alexandra birch. 2016a. edinburgh neural
machine translation systems for wmt 16. in pro-
ceedings of the first conference on machine trans-
lation (wmt16), berlin, germany.

[sennrich et al.2016b] rico sennrich, barry haddow,
and alexandra birch.
improving neu-
ral machine translation models with monolingual
data. in proceedings of the 54th annual meeting of
the association for computational linguistics (acl
2016), berlin, germany.

2016b.

[sennrich et al.2016c] rico sennrich, barry haddow,
and alexandra birch.
2016c. neural machine
translation of rare words with subword units. in
proceedings of the 54th annual meeting of the asso-
ciation for computational linguistics (acl 2016),
berlin, germany.

[stanojevi  c et al.2015] milo   stanojevi  c, amir kam-
ran, philipp koehn, and ond  rej bojar. 2015. re-
sults of the wmt15 metrics shared task. in pro-
ceedings of the tenth workshop on statistical ma-
chine translation, pages 256   273, lisbon, portugal.
association for computational linguistics.

[toutanova et al.2003] kristina toutanova, dan klein,
christopher d. manning, and yoram singer. 2003.
feature-rich part-of-speech tagging with a cyclic
dependency network. in proceedings of the 2003
human language technology conference of the
north american chapter of
the association for
computational linguistics.

[williams et al.2016] philip williams, rico sennrich,
maria nadejde, matthias huck, barry haddow, and
ond  rej bojar. 2016. edinburgh   s statistical ma-
chine translation systems for wmt16. in proceed-
ings of the first conference on machine translation
(wmt16).

[zeiler2012] matthew d. zeiler. 2012. adadelta:
corr,

an adaptive learning rate method.
abs/1212.5701.

[zoph and knight2016] barret zoph and kevin knight.
2016. multi-source neural translation. in naacl
hlt 2016.

continuous space word representations. in hlt-
naacl, pages 746   751. the association for com-
putational linguistics.

[minnen et al.2001] guido minnen, john a. carroll,
and darren pearce. 2001. applied morphological
processing of english. natural language engineer-
ing, 7(3):207   223.

[pascanu et al.2013] razvan pascanu, tomas mikolov,
and yoshua bengio. 2013. on the dif   culty of train-
ing recurrent neural networks. in proceedings of the
30th international conference on machine learn-
ing, icml 2013, pages 1310   1318, atlanta, usa.

[peter et al.2016] jan-thorsten peter, tamer alkhouli,
hermann ney, matthias huck, fabienne braune,
alexander fraser, ale   tamchyna, ond  rej bojar,
barry haddow, rico sennrich, fr  d  ric blain, lu-
cia specia, jan niehues, alex waibel, alexandre
allauzen, lauriane aufrant, franck burlot, elena
knyazeva, thomas lavergne, fran  ois yvon, and
marcis pinnis. 2016. the qt21/himl combined
machine translation system. in proceedings of the
first conference on machine translation (wmt16),
berlin, germany.

[popovi  c2015] maja popovi  c. 2015. chrf: character n-
gram f-score for automatic mt evaluation. in pro-
ceedings of the tenth workshop on statistical ma-
chine translation, pages 392   395, lisbon, portugal.
association for computational linguistics.

[riezler and maxwell2005] stefan riezler and john t.
maxwell. 2005. on some pitfalls in automatic
evaluation and signi   cance testing for mt. in pro-
ceedings of the acl workshop on intrinsic and ex-
trinsic evaluation measures for machine transla-
tion and/or summarization, pages 57   64, ann ar-
bor, michigan. association for computational lin-
guistics.

[schmid et al.2004] helmut schmid, arne fitschen,
and ulrich heid.
2004. a german computa-
tional morphology covering derivation, composi-
tion, and in   ection. in proceedings of the ivth in-
ternational conference on language resources and
evaluation (lrec 2004), pages 1263   1266.

[sennrich and kunz2014] rico sennrich and beat
kunz. 2014. zmorge: a german morphological
lexicon extracted from wiktionary. in proceedings
of the 9th international conference on language
resources and evaluation (lrec 2014), reykjavik,
iceland.

[sennrich et al.2013] rico sennrich, martin volk, and
gerold schneider.
2013. exploiting synergies
between open resources for german dependency
parsing, pos-tagging, and morphological analy-
sis. in proceedings of the international conference
recent advances in natural language processing
2013, pages 601   609, hissar, bulgaria.

