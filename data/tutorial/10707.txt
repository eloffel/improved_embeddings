6
1
0
2

 

n
u
j
 

4
1

 
 
]
l
c
.
s
c
[
 
 

3
v
9
6
0
6
0

.

5
0
6
1
:
v
i
x
r
a

a hierarchical latent variable encoder-decoder

model for generating dialogues

iulian v. serban*, alessandro sordoni    , ryan lowe(cid:5), laurent charlin(cid:5), joelle pineau(cid:5),

aaron courville* and yoshua bengio*

*department of computer science and operations research, universit   de montr  al, montreal, canada

{iulian.vlad.serban,aaron.courville,yoshua.bengio}@umontreal.ca

   maluuba inc, montreal, canada

{alessandro.sordoni}@maluuba.com

(cid:5)school of computer science, mcgill university, montreal, canada

{ryan.lowe,lcharlin,jpineau}@cs.mcgill.ca

abstract

sequential data often possesses a hierarchical structure with complex dependencies
between subsequences, such as found between the utterances in a dialogue. in an
effort to model this kind of generative process, we propose a neural network-based
generative architecture, with latent stochastic variables that span a variable number
of time steps. we apply the proposed model to the task of dialogue response
generation and compare it with recent neural network architectures. we evaluate
the model performance through automatic id74 and by carrying out a
human evaluation. the experiments demonstrate that our model improves upon
recently proposed models and that the latent variables facilitate the generation of
long outputs and maintain the context.

1

introduction

deep recurrent neural networks (id56s) have recently demonstrated impressive results on a number
of dif   cult machine learning problems involving the generation of sequential structured outputs [9],
including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech
recognition [9].
while these advances are impressive, the underlying id56s tend to have a fairly simple structure, in
the sense that the only variability or stochasticity in the model occurs when an output is sampled.
this is often an inappropriate place to inject variability [2, 6, 1]. this is especially true for sequential
data such as speech and natural language that possess a hierarchical generation process with complex
intra-sequence dependencies. for instance, natural language dialogue involves at least two levels
of structure; within a single utterance the structure is dominated by local statistics of the language,
while across utterances there is a distinct source of uncertainty (or variance) characterized by aspects
such as conversation topic, speaker goals and speaker style.
in this paper we introduce a novel hierarchical stochastic latent variable neural network architecture
to explicitly model generative processes that possess multiple levels of variability. we evaluate the
proposed model on the task of dialogue response generation and compare it with recent neural network
architectures. we evaluate the model qualitatively through manual inspection, and quantitatively
using a human evaluation on amazon mechanical turk and using automatic id74. the

this work was carried out while a.s. was at universit   de montr  al. y.b. is a cifar senior fellow.

m(cid:89)

results demonstrate that the model improves upon recently proposed models. in particular, the results
highlight that the latent variables help to both facilitate the generation of long utterances with more
information content, and to maintain the dialogue context.

2 technical background

2.1 recurrent neural network language model

a recurrent neural network (id56), with parameters   , models a variable-length sequence of tokens
(w1, . . . , wm ) by decomposing the id203 distribution over outputs:

p  (w1, . . . , wm ) =

p  (wm | w1, . . . , wm   1)p (w1).

(1)

m=2

the model processes each observation recursively. at each time step, the model observes an element
and updates its internal hidden state, hm = f (hm   1, wm), where f is a parametrized non-linear
function, such as the hyperbolic tangent, the lstm gating unit [12] or the gru gating unit [5].1
the hidden state acts as a suf   cient statistic, which summarizes the past sequence and parametrizes
the output distribution of the model: p  (wm+1 | w1, . . . , wm) = p  (wm+1 | hm). we assume
the outputs lie within a discrete vocabulary v . under this assumption, the id56 language model
(id56lm) [18], the simplest possible generative id56 for discrete sequences, parametrizes the output
distribution using the softmax function applied to an af   ne transformation of the hidden state hm.
the model parameters are learned by maximizing the training log-likelihood using id119.

2.2 hierarchical recurrent encoder-decoder

the hierarchical recurrent encoder-decoder model (hred) [26, 24] is an extension of the id56lm.
it extends the encoder-decoder architecture [5] to the natural dialogue setting. the hred assumes
that each output sequence can be modelled in a two-level hierarchy: sequences of sub-sequences,
and sub-sequences of tokens. for example, a dialogue may be modelled as a sequence of utterances
(sub-sequences), with each utterance modelled as a sequence of words. similarly, a natural-language
document may be modelled as a sequence of sentences (sub-sequences), with each sentence modelled
as a sequence of words. the hred model consists of three id56 modules: an encoder id56,
a context id56 and a decoder id56. each sub-sequence of tokens is deterministically encoded
into a real-valued vector by the encoder id56. this is given as input to the context id56, which
updates its internal hidden state to re   ect all information up to that point in time. the context id56
deterministically outputs a real-valued vector, which the decoder id56 conditions on to generate the
next sub-sequence of tokens. for additional details see [26, 24].

2.3 a de   cient generation process

in the recent literature, it has been observed that the id56lm and hred, and similar models based
on id56 architectures, have critical problems generating meaningful dialogue utterances [24, 15]. we
believe that the root cause of these problems arise from the parametrization of the output distribution
in the id56lm and hred, which imposes a strong constraint on the generation process: the only
source of variation is modelled through the conditional output distribution. this is detrimental from
two perspectives: from a probabilistic perspective, with stochastic variations injected only at the
low level, the model is encouraged to capture local structure in the sequence, rather than global
or long-term structure. this is because random variations injected at the lower level are strongly
constrained to be in line with the immediate previous observations, but only weakly constrained to be
in line with older observations or with future observations. one can think of random variations as
injected via i.i.d. noise variables, added to deterministic components, for example. if this noise is
injected at a higher level of representation, spanning longer parts of the sequence, its effects could
correspond to longer-term dependencies. second, from a computational learning perspective, the
state hm of the id56lm (or decoder id56 of hred) has to summarize all the past information up to
time step m in order to (a) generate a probable next token (short term goal) and simultaneously (b) to
occupy a position in embedding space which sustains a realistic output trajectory, in order to generate

1we concatenate the lstm cell and cell input hidden states into a single state hm for notational simplicity.

2

probable future tokens (long term goal). due to the vanishing gradient effect, shorter-term goals will
have more in   uence:    nding a compromise between these two disparate forces will likely lead the
training procedure to model parameters that focus too much on predicting only the next output token.
in particular for high-id178 sequences, the models are very likely to favour short-term predictions
as opposed to long-term predictions, because it is easier to only learn hm for predicting the next
token compared to sustaining a long-term trajectory, which at every time step is perturbed by a highly
noisy source (the observed token).

3 latent variable hierarchical recurrent encoder-decoder (vhred)

motived by the previous discussion, we now introduce the latent variable hierarchical recurrent
encoder-decoder (vhred) model. this model augments the hred model with a latent variable
at the decoder, which is trained by maximizing a variational lower-bound on the log-likelihood.
this allows it to model hierarchically-structured sequences in a two-step generation process      rst
sampling the latent variable, and then generating the output sequence   while maintaining long-term
context.
let w1, . . . , wn be a sequence consisting of n sub-sequences, where wn = (wn,1, . . . , wn,mn )
is the n   th sub-sequence and wn,m     v is the m   th discrete token in that sequence. the vhred
model uses a stochastic latent variable zn     rdz for each sub-sequence n = 1, . . . , n conditioned
on all previous observed tokens. given zn, the model next generates the n   th sub-sequence tokens
wn = (wn,1, . . . , wn,mn ):

p  (zn | w1, . . . , wn   1) = n (  prior(w1, . . . , wn   1),   prior(w1, . . . , wn   1)),

p  (wn | zn, w1, . . . , wn   1) =

p  (wn,m | zn, w1, . . . , wn   1, wn,1, . . . , wn,m   1),

(2)

(3)

mn(cid:89)

m=1

where n (  ,   ) is the multivariate normal distribution with mean        rdz and covariance matrix
       rdz  dz, which is constrained to be a diagonal matrix.
the vhred model (   gure 1) contains the same three components as the hred model. the encoder
id56 deterministically encodes a single sub-sequence into a    xed-size real-valued vector. the context
id56 deterministically takes as input the output of the encoder id56, and encodes all previous
sub-sequences into a    xed-size real-valued vector. this vector is fed into a two-layer feed-forward
neural network with hyperbolic tangent gating function. a id127 is applied to the
output of the feed-forward network, which de   nes the multivariate normal mean   prior. similarly, for
the diagonal covariance matrix   prior a different id127 is applied to the net   s output
followed by softplus function, to ensure positiveness [6].
the model   s latent variables are inferred by maximizing the variational lower-bound, which factorizes
into independent terms for each sub-sequence:

log p  (w1, . . . , wn )     n(cid:88)

   kl [q  (zn | w1, . . . , wn)||p  (zn | w1, . . . , wn   1)]

n=1

+ eq  (zn|w1,...,wn) [log p  (wn | zn, w1, . . . , wn   1)] ,

(4)
where kl[q||p ] is the kullback-leibler (kl) divergence between distributions q and p . the
distribution q  (z | w1, . . . , wm ) is the approximate posterior distribution (also known as the
encoder model or recognition model), which aims to approximate the intractable true posterior
distribution:
q  (zn | w1, . . . , wn ) = q  (zn | w1, . . . , wn) = n (  posterior(w1, . . . , wn),   posterior(w1, . . . , wn))

    p  (zn | w1, . . . , wn ),

(5)
where   posterior de   nes the approximate posterior mean and   posterior de   nes the approximate posterior
covariance matrix (assumed diagonal) as a function of the previous sub-sequences w1, . . . , wn   1 and
the current sub-sequence wn. the posterior mean   posterior and covariance   posterior are determined in
the same way as the prior, via a id127 with the output of the feed-forward network,
and with a softplus function applied for the covariance.

3

figure 1: computational graph for vhred model. rounded boxes represent (deterministic) real-
valued vectors. variables z represent latent stochastic variables.

at test time, conditioned on the previous observed sub-sequences (w1, . . . , wn   1), a sample zn
is drawn from the prior n (  prior(w1, . . . , wn   1),   prior(w1, . . . , wn   1)) for each sub-sequence.
this sample is concatenated with the output of the context id56 and given as input to the de-
coder id56 as in the hred model, which then generates the sub-sequence token-by-token.
at training time, for n = 1, . . . , n, a sample zn is drawn from the approximate posterior
n (  posterior(w1, . . . , wn),   posterior(w1, . . . , wn)) and used to estimate the gradient of the varia-
tional lower-bound given by eq. (4). the approximate posterior is parametrized by its own one-layer
feed-forward neural network, which takes as input the output of the context id56 at the current time
step, as well as the output of the encoder id56 for the next sub-sequence.
the vhred model greatly helps to reduce the problems with the generation process used by the
id56lm and hred model outlined above. the variation of the output sequence is now modelled
in two ways: at the sequence-level with the conditional prior distribution over z, and at the sub-
sequence-level (token-level) with the conditional distribution over tokens w1, . . . , wm . the variable z
helps model long-term output trajectories, by representing high-level information about the sequence,
which in turn allows the variable hm to primarily focus on summarizing the information up to token
m. intuitively, the randomness injected by the variable z corresponds to higher-level decisions, like
topic or sentiment of the sentence.

4 experimental evaluation

we consider the problem of conditional natural language response generation for dialogue. this is an
interesting problem with applications in areas such as customer service, technical support, language
learning and entertainment [29]. it is also a task domain that requires learning to generate sequences
with complex structures while taking into account long-term context [17, 27].
we consider two tasks. for each task, the model is given a dialogue context, consisting of one or
more utterances, and the goal of the model is to generate an appropriate next response to the dialogue.
we    rst perform experiments on a twitter dialogue corpus [22]. the task is to generate utterances
to append to existing twitter conversations. the dataset is extracted using a procedure similar to
ritter et al. [22], and is split into training, validation and test sets, containing respectively 749, 060,
93, 633 and 10, 000 dialogues. each dialogue contains 6.27 utterances and 94.16 tokens on average.2
the dialogues are fairly long compared to recent large-scale language modelling corpora, such as
the 1 billion word language model benchmark [4], which focus on modelling single sentences.

2due to twitter   s terms of service we are not allowed to redistribute twitter content. therefore, only the

tweet ids can be made public. these are available at: www.iulianserban.com/files/tweetids.zip.

4

we also experiment on the ubuntu dialogue corpus [17], which contains about 500, 000 dialogues
extracted from the #ubuntu internet relayed chat channel. users enter the chat channel with a
ubuntu-related technical problem, and other users try to help them. for further details see appendix
6.1.3 we chose these corpora because they are large, and have different purposes   ubuntu dialogues
are typically goal driven, where as twitter dialogues typically contain social interaction ("chit-chat").

4.1 training and evaluation procedures

we optimize all models using adam [13]. we choose our hyperparameters and early stop with
patience using the variational lower-bound [9]. at test time, we use id125 with 5 beams
for outputting responses with the id56 decoders [10]. for the vhred models, we sample the
latent variable zn, and condition on it when executing id125 with the id56 decoder. for
ubuntu we use id27 dimensionality of size 300, and for twitter we use id27
dimensionality of size 400. all models were trained with a learning rate of 0.0001 or 0.0002 and with
mini-batches containing 40 or 80 training examples. we use a variant of truncated back-propagation
and we apply gradient clipping. further details are given in appendix 6.2.
baselines on both twitter and ubuntu we compare to an lstm model of 2000 hidden units. on
ubuntu, the hred model has 500, 1000 and 500 hidden units for the encoder, context and decoder
id56s respectively. the encoder id56 is a standard gru id56. on twitter, the hred model
encoder id56 is a bidirectional gru id56 encoder, where the forward and backward id56s each
have 1000 hidden units, and context id56 and decoder id56 have each 1000 hidden units. for
reference, we also include a non-neural network baseline, speci   cally the tf-idf retrieval-based
model proposed in [17].
vhred the encoder and context id56s for the vhred model are parametrized in the same way as
the corresponding hred models. the only difference in the parametrization of the decoder id56 is
that the context id56 output vector is now concatenated with the generated stochastic latent variable.
furthermore, we initialize the feed-forward networks of the prior and posterior distributions with
values drawn from a zero-mean normal distribution with variance 0.01 and with biases equal to zero.
we also multiply the diagonal covariance matrices of the prior and posterior distributions with 0.1 to
make training more stable, because a high variance makes the gradients w.r.t. the reconstruction cost
unreliable, which is fatal at the beginning of the training process.
the vhred   s encoder and context id56s are initialized to the parameters of the corresponding
converged hred models. we also use two heuristics proposed by bowman et al. [3]: we drop
words in the decoder with a    xed drop rate of 25% and multiply the kl terms in eq. (4) by a scalar,
which starts at zero and linearly increases to 1 over the    rst 60, 000 and 75, 000 training batches
on twitter and ubuntu respectively. applying these heuristics helped substantially to stabilize the
training process and make the model use the stochastic latent variables. we experimented with the
batch id172 training procedure for the feed-forward neural networks. but found that this
made training very unstable without any substantial gains in performance w.r.t. the variational bound.
evaluation accurate evaluation of dialogue system responses is a dif   cult problem [8, 20]. inspired
by metrics for machine translation and information retrieval, researchers have begun adopting word-
overlap metrics, however liu et al. [16] show that such metrics have little correlation with human
evaluations of response quality. we therefore carry out a human evaluation to compare responses from
the different models. we also compute several statistics and automatic metrics on model responses to
characterize differences between the model-generated responses.
we carry out the human study for the twitter dialogue corpus on amazon mechanical turk (amt).
we do not conduct amt experiments on ubuntu as evaluating these responses usually requires
technical expertise, which is not prevalent among amt users. we set up the evaluation study as a
series of pairwise comparison experiments.4 we show human evaluators a dialogue context along
with two potential responses, one generated from each model (conditioned on dialogue context). we
ask participants to choose the response most appropriate to the dialogue context. if the evaluators
are indifferent to either of the two responses, or if they cannot understand the dialogue context, they
can choose neither response. for each pair of models we conduct two experiments: one where the

3the pre-processed ubuntu dialogue corpus used is available at www.iulianserban.com/files/

ubuntudialoguecorpus.zip.

4source code for the amt experiments will be released upon publication.

5

table 1: wins, losses and ties (in %) of the vhred model against the baselines based on the human
study on twitter (mean preferences    90% con   dence intervals)

short contexts

long contexts

ties

wins

wins

losses

42.5    2.6
31.9    2.6
17.9    2.5

32.3    2.4
42.0    2.8
51.6    3.3

opponent
vhred vs lstm
vhred vs hred
vhred vs tf-idf
table 2: twitter examples for the neural network models. the     token indicates a change of turn.
context
jamaica blood!! <url>     lol. yes [name]!. [name]!    
what   s poppin in toronto homie

losses
36.8    2.2
29.4    2.6
11.7    2.2

41.9    2.2
41.5    2.8
47.9    3.4

25.2    2.3
26.2    2.5
30.4    3.0

21.3    1.9
29.1    2.6
40.3    3.4

ties

response
vhred: nothing much. just chillin. what   s good with you?
lstm: something!!!!!!!
hred: something!!!!!!
vhred: the sun is shining!!!
lstm: i know!!!!!!!!!
hred: something!!!!!!!

whhhhhhyyyy is it suddenly cold in tallahassee today?? this
is    orida! how can i dream of the beach when my    ngers are
icey cold??     sittin on deck in sun looking at lake travis in
austin! enjoying day b4 birthday
reaches for [name]!     - approaches him warily - hey there
buddy. - swings him into the air - how you doin?     tilts his
head and your wariness and reaches for your face [name].

i think i want an ipad2. but i already have an ibook. want vs.
need.     if you have an ibook. then you deserve an ipad!

ah echt? dan anrtwoord ik volgende keer via twitter als je me
smst.     haha doe maar p. goed bezig hoor myrthe d    
goed bezig? die snap ik even niet p

i love katie waissel more than like, anything in life.     i
wanna c the show again! i need <number> meet her again! n
i want a b-day tweet nxt week doubt that will happen x

my math teacher over here talkn bout nickels nd i   m tryna
make big money i need to be in calculus     lol

vhred: - looks up at you and smiles - you   re so cute. - kisses your cheek -
lstm: - smiles - i   m    ne thank you. how are you?
hred: - grins, looking at you as i look at you -
vhred: that   s what i   m thinking. i   m going to buy a new one.
lstm: i want one!!!!
hred: i don   t know what to do with it. i don   t know what to do.
vhred: haha, ik heb er geen zin in de something, maar ik heb er geen
something meer something
lstm: haha something something something
hred: something something something something something something
something something
vhred: me too. i   m going to have to wait for her to come back. i   m so
excited.
lstm: i   m sure she will. i   m sure she   ll be    ne.
hred: i   m sure she   ll be    ne, i   m sure she   ll be    ne.
vhred: i need to go to the gym and get my hair done lol
lstm: lol
hred: how u been

example contexts contain at least 80 unique tokens (long context), and one where they contain at
least 20 (not necessarily unique) tokens (short context). this helps compare how well each model
can integrate the dialogue context into its response, since it has previously been hypothesized that for
long contexts hierarchical id56s models fare better [24, 26]. screenshots and further details of the
experiments are in appendix 6.4.

4.2 results of human evaluation

the results (table 1) show that vhred is clearly preferred in the majority of the experiments. in
particular, vhred is strongly preferred over the hred and tf-idf baseline models for both short
and long context settings. vhred is also preferred over the lstm baseline model for long contexts;
however, the lstm is preferred over vhred for short contexts. we believe this is because the
lstm baseline tends to output much more generic responses (see table 4); since it doesn   t model the
hierarchical input structure, the lstm model has a shorter memory span, and thus must output a
response based primarily on the end of the last utterance. such    safe    responses are reasonable for a
wider range of contexts, meaning that human evaluators are more likely to rate them as appropriate.
however, we argue that a model that only outputs generic responses is undesirable for dialogue,
as this leads to uninteresting and less engaging conversations. conversely, the vhred model is
explicitly designed for long contexts, and to output a diverse set of responses due to the sampling
of the latent variable. thus, the vhred model generates longer sentences with more semantic
content than the lstm model (see tables 3-4). this can be    riskier    as longer utterances are more
likely to contain small mistakes, which can lead to lower human preference for a single utterance.
however, we believe that response diversity is crucial to maintaining interesting conversations     in
the dialogue literature, generic responses are used primarily as    back-off    strategies in case the agent
has no interesting response that is relevant to the context [25].

6

table 3: evaluation on 1-turn and 3-turns dialogue generation using the proposed embedding metrics.

twitter

ubuntu

model

average greedy extrema

average greedy extrema

lstm
hred
vhred

lstm
hred
vhred

0.512
0.501
0.533

0.657
0.646
0.689

0.389
0.378
0.396

0.561
0.552
0.583

0.366
0.355
0.38

0.374
0.364
0.391

1-turn

3-turns

0.23
0.577
0.542

0.638
0.742
0.777

0.169
0.417
0.384

0.456
0.524
0.536

0.157
0.391
0.363

0.378
0.432
0.448

the above hypotheses are con   rmed upon qualitative assessment of the generated responses (table
2). vhred generates longer and more meaningful responses compared to the lstm model, which
generates mostly generic responses. additionally, we observed that the vhred model has learned
to better model smilies, slang (see    rst example in table 2) and can even continue conversations
in different languages (see    fth example).5 such aspects are not measured by the human study.
further, vhred appears to be better at generating stories or imaginative actions compared to the
generative baseline models (see third example). the last example in table 2 is a case where the
vhred generated response is more interesting, yet may be less preferred by humans as it is slightly
incompatible with the context, compared to the generic lstm response. in the next section, we back
these examples quantitatively, showing that the vhred model learns to generate longer responses
with more information content that share semantic similarity to the context and ground-truth response.

4.3 results of metric-based evaluation

to show the vhred responses are more on-topic and share semantic similarity to the ground-truth
response, we consider three textual similarity metrics based on id27s. the embedding
average (average) metric projects the model response and ground truth response into two separate
real-valued vectors by taking the mean over the id27s in each response, and then computes
the cosine similarity between them [19]. this metric is widely used for measuring textual similarity.
the embedding extrema (extrema) metric similarly embeds the responses by taking the extremum
(maximum of the absolute value) of each dimension, and afterwards computes the cosine similarity
between them.the embedding greedy (greedy) metric is more    ne-grained; it uses cosine similarity
between id27s to    nd the closest word in the human-generated response for each word in
the model response. given the (non-exclusive) alignment between words in the two responses, the
mean over the cosine similarities is computed for each pair of questions [23]. since this metric takes
into account the alignment between words, it should be more accurate for long responses. while
these metrics do not strongly correlate with human judgements of generated responses, we interpret
them as measuring topic similarity: if the model generated response has similar semantic content to
the ground truth human response, then the metrics will yield a high score. to ease reproducibility, we
use the publicly available id97 id27s trained on the google news corpus.6
we compute these metrics in two settings: one where the models generate a single response (1-turn),
and one where they generate the next three consecutive utterances (3-turns) (table 3). overall,
vhred seems to better capture the ground truth response topic than either the lstm or hred
models. the fact that vhred does better in particular in the setting where the model generates three
consecutive utterances strongly suggests that hidden states in both the decoder and context id56s of
the vhred models are better able to follow trajectories which remain on-topic w.r.t the dialogue
context. this supports our computational hypothesis that the stochastic latent variable helps modulate
the training procedure to achieve a better trade-off between short-term and long-term generation. we
also observed the same trend when computing the similarity metrics between the model generated
responses and the corresponding context, which further reinforces this hypothesis.

5there is a notable amount of spanish and dutch conversations in the corpus.
6https://code.google.com/archive/p/id97/

7

|u|, word id178 hw =    (cid:80)

table 4: response information content on 1-turn generation as measured by average utterance length
w   u p(w) log p(w) and utterance id178 hu with respect to the

maximum-likelihood unigram distribution of the training corpus p.

model
lstm
hred
vhred
human

twitter

hw

6.75
6.73
6.88

hu

75.61
78.35
84.56

|u|
11.21
11.64
12.29

ubuntu

hw

6.50
7.53
7.70

hu

27.77
83.16
71.00

|u|
4.27
11.05
9.22

20.57

8.10

166.57

18.30

8.90

162.88

to show that the vhred responses contain more information content than other model responses,
we compute the average response length and average id178 (in bits) w.r.t. the maximum likelihood
unigram model over the generated responses (table 4). the unigram id178 is computed on the
preprocessed tokenized datasets. vhred produces responses with higher id178 per word on
both ubuntu and twitter compared to the hred and lstm models. vhred also produces
longer responses overall on twitter, which translates into responses containing on average 6 bits of
information more than the hred model. since the actual dialogue responses contain even more
information per word than any of the generative models, it reasonable to assume that a higher id178
is desirable. thus, vhred compares favourably to recently proposed models in the literature,
which often output extremely low-id178 (generic) responses such as ok and i don   t know [24, 15].
finally, the fact that vhred produces responses with higher id178 suggests that its responses
are on average more diverse than the responses produced by the hred and lstm models. this
implies that the trajectories of the hidden states of the vhred model traverse a larger area of the
space compared to the hidden states of the hred and lstm baselines, which further supports our
hypothesis that the stochastic latent variable helps the vhred model achieve a better trade-off
between short-term and long-term generation.

5 related work

the use of a stochastic latent variable learned by maximizing a variational lower bound is inspired
by the variational autoencoder (vae) [14, 21]. such models have been used predominantly for
generating images in the continuous domain [11]. however, there has also been recent work applying
these architectures for generating sequences, such as the variational recurrent neural networks
(vid56) [6], which was applied for speech and handwriting synthesis, and stochastic recurrent
networks (storn) [1], which was applied for music generation and motion capture modeling. both
the vid56 and storn incorporate stochastic latent variables into id56 architectures, but unlike the
vhred they sample a separate latent variable at each time step of the decoder. this does not exploit
the hierarchical structure in the data, and thus does not model higher-level variability.
similar to our work is the variational recurrent autoencoder [7] and the variational autoencoder
language model [3], which apply encoder-decoder architectures to generative music modeling and
id38 respectively. the vhred model is different from these in the following ways.
the vhred latent variable is conditioned on all previous sub-sequences (sentences). this enables
the model to generate multiple sub-sequences (sentences), but it also makes the latent variables
co-dependent through the observed tokens. the vhred model builds on the hierarchical architecture
of the hred model, which makes the model applicable to generation conditioned on long contexts.
it has a direct deterministic connection between the context and decoder id56, which allows the
model to transfer deterministic pieces of information between its components.7 crucially, vhred
also demonstrates improved results beyond the autoencoder framework, where the objective is not
input reconstruction but the conditional generation of the next utterance in a dialogue.

7our initial experiments con   rmed that the deterministic connection between the context id56 to the decoder

id56 was indeed bene   cial in terms of lowering the variational bound.

8

6 discussion

we have introduced a novel latent variable neural network architecture, called vhred. the model
uses a hierarchical generation process in order to exploit the structure in sequences and is trained
using a variational lower bound on the log-likelihood. we have applied the proposed model on
the dif   cult task of dialogue response generation, and have demonstrated that it is an improvement
over previous models in several ways, including quality of responses as measured in a human study.
the empirical results highlight the advantages of the hierarchical generation process for modelling
high-id178 sequences. finally, it is worth noting that the proposed model is very general. it can in
principle be applied to any sequential generation task that exhibits a hierarchical structure, such as
document-level machine translation, web query prediction, multi-sentence document summarization,
multi-sentence image id134, and others.

references

[1] bayer, j. and osendorfer, c. (2014). learning stochastic recurrent networks. in nips workshop on advances

in variational id136.

[2] boulanger-lewandowski, n., bengio, y., and vincent, p. (2012). modeling temporal dependencies in

high-dimensional sequences: application to polyphonic music generation and transcription. in icml.

[3] bowman, s. r., vilnis, l., vinyals, o., dai, a. m., jozefowicz, r., and bengio, s. (2015). generating

sentences from a continuous space. arxiv:1511.06349.

[4] chelba, c., mikolov, t., schuster, m., ge, q., brants, t., koehn, p., and robinson, t. (2014). one billion

word benchmark for measuring progress in statistical id38. in interspeech.

[5] cho, k., van merrienboer, b., gulcehre, c., bahdanau, d., bougares, f., schwenk, h., and bengio, y.
(2014). learning phrase representations using id56 encoder   decoder for id151. in
emnlp, pages 1724   1734.

[6] chung, j., kastner, k., dinh, l., goel, k., courville, a., and bengio, y. (2015). a recurrent latent variable

model for sequential data. in nips, pages 2962   2970.

[7] fabius, o. and van amersfoort, j. r. (2014). variational recurrent auto-encoders. arxiv:1412.6581.
[8] galley, m., brockett, c., sordoni, a., ji, y., auli, m., quirk, c., mitchell, m., gao, j., and dolan, b. (2015).

deltaid7: a discriminative metric for generation tasks with intrinsically diverse targets. in acl.

[9] goodfellow, i., courville, a., and bengio, y. (2015). deep learning. mit press.
[10] graves, a. (2012). sequence transduction with recurrent neural networks. in icml rlw.
[11] gregor, k., danihelka, i., graves, a., and wierstra, d. (2015). draw: a recurrent neural network for

image generation. in iclr.

[12] hochreiter, s. and schmidhuber, j. (1997). long short-term memory. neural computation, 9(8).
[13] kingma, d. and ba, j. (2015). adam: a method for stochastic optimization. in iclr.
[14] kingma, d. p. and welling, m. (2014). auto-encoding id58. in iclr.
[15] li, j., galley, m., brockett, c., gao, j., and dolan, b. (2016). a diversity-promoting objective function for

neural conversation models. in naacl.

[16] liu, c.-w., lowe, r., serban, i. v., noseworthy, m., charlin, l., and pineau, j. (2016). how not to
evaluate your dialogue system: an empirical study of unsupervised id74 for dialogue response
generation. arxiv:1603.08023.

[17] lowe, r., pow, n., serban, i., and pineau, j. (2015). the ubuntu dialogue corpus: a large dataset for

research in unstructured multi-turn dialogue systems. in sigdial.

[18] mikolov, t., kara     t, m., burget, l., cernock`y, j., and khudanpur, s. (2010). recurrent neural network

based language model. in interspeech, pages 1045   1048.

[19] mitchell, j. and lapata, m. (2008). vector-based models of semantic composition. in acl, pages 236   244.
[20] pietquin, o. and hastie, h. (2013). a survey on metrics for the evaluation of user simulations. the

knowledge engineering review, 28(01), 59   73.

[21] rezende, d. j., mohamed, s., and wierstra, d. (2014). stochastic id26 and approximate

id136 in deep generative models. in icml.

[22] ritter, a., cherry, c., and dolan, w. b. (2011). data-driven response generation in social media. in

emnlp, pages 583   593.

[23] rus, v. and lintean, m. (2012). a comparison of greedy and optimal assessment of natural language
student input using word-to-word similarity metrics. in building educational applications workshop, acl.

9

[24] serban, i. v., sordoni, a., bengio, y., courville, a. c., and pineau, j. (2016). building end-to-end dialogue

systems using generative hierarchical neural network models. in aaai, pages 3776   3784.

[25] shaikh, s., strzalkowski, t., taylor, s., and webb, n. (2010). vca: an experiment with a multiparty

virtual chat agent. in acl workshop on companionable dialogue systems, pages 43   48.

[26] sordoni, a., bengio, y., vahabi, h., lioma, c., simonsen, j. g., and nie, j.-y. (2015a). a hierarchical

recurrent encoder-decoder for generative context-aware query suggestion. in cikm.

[27] sordoni, a., galley, m., auli, m., brockett, c., ji, y., mitchell, m., nie, j.-y., gao, j., and dolan,
b. (2015b). a neural network approach to context-sensitive generation of conversational responses. in
naacl-hlt.

[28] sutskever, i., vinyals, o., and le, q. v. (2014). sequence to sequence learning with neural networks. in

nips, pages 3104   3112.

[29] young, s., gasic, m., thomson, b., and williams, j. d. (2013). pomdp-based statistical spoken dialog

systems: a review. ieee, 101(5), 1160   1179.

10

appendix

6.1 dataset details

our twitter dialogue corpus was extracted in 2011. we perform a minimal preprocessing on the dataset
to remove irregular punctuation marks and tokenize it using the moses tokenizer: https://github.com/
moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl.
we use the ubuntu dialogue corpus v2.0 extracted in jamuary 2016 from: http://cs.mcgill.ca/~jpineau/
datasets/ubuntu-corpus-1.0/. the preprocessed version of the dataset will be made available to the public.

6.2 model details

the model implementations will be released to the public upon acceptance of the paper.

training and generation

we validate each model on the entire validation set every 5000 training batches.
as mentioned in the main text, at test time we use id125 with 5 beams for outputting responses with the
id56 decoders [10]. we de   ne the beam cost as the log-likelihood of the tokens in the beam divided by the
number of tokens it contains. this is a well-known modi   cation, which is often applied in machine translation
models. in principle, we could sample from the id56 decoders of all the models, but is well known that such
sampling produces poor results in comparison to the id125 procedure. it also introduces additional
variance into the evaluation procedure, which will make the human study very expensive or even impossible
within a limited budget.

baseline models
on ubuntu, the gating function between the context id56 and decoder id56 is a one-layer feed-forward neural
network with hyperbolic tangent activation function.
on twitter, the hred decoder id56 computes a 1000 dimensional real-valued vector for each hidden time step,
which is multiplied with the output context id56. the result is feed through a one-layer feed-forward neural
network with hyperbolic tangent activation function, which the decoder id56 then takes as input. furthermore,
the encoder id56 initial state for each utterance is initialized to the last hidden state of the encoder id56 from
the previous utterance. we found that this worked slightly better for the vhred model, but a more careful
choice of hyperparameters is likely to make this additional step unnecessary for both the hred and vhred
models.

latent variable parametrization

we here describe the formal de   nition of the latent variable prior and approximate posterior distributions. let
w1, . . . , wt be discrete tokens in vocabulary v , which correspond to one sequence (e.g. one dialogue). let
ht,con     rdh,con be the hidden state of the hred context encoder at time t. then the prior mean and covariance
matrix are given as:

  ht,con = tanh(hl2,priortanh(hl1,priorht,con + bl1,prior) + bl2,prior),
  t,prior = h  ,prior  ht,con + b  ,prior,
  t,prior = diag(log(1 + exp(h  ,prior  ht,con + b  ,prior))),

(6)
(7)
(8)
where the parameters are hl1,prior     rdz  dh,con, h  ,prior, h  ,prior, hl2,prior     rdz  dz and
bl1,prior, bl2,prior, b  ,prior, b  ,prior     rdz , and where diag(x) is a function mapping a vector x to a ma-
trix with diagonal elements x and all off-diagonal elements equal to zero. at generation time, the latent variable
is sampled at the end of each utterance: zt     n (  t.prior,   t,prior).
the equations for the approximate posterior are similar. let ht,p     rdh,con+dh,enc be the concatenation of
ht,con and the hidden state of the encoder id56 at the end of the next sub-sequence, which we assume has
dimensionality dh,enc. the approximate posterior mean and covariance matrix are given as:

  ht,p = tanh(hl2,posteriortanh(hl1,posteriorht,p + bl1,posterior)) + bl2,posterior)),

  t,posterior = h  ,posterior  ht,p + b  ,posterior,
  t,posterior = diag(log(1 + exp(h  ,posterior  ht,p + b  ,posterior))),

(9)
(10)
(11)
where hl1,posterior     rdz  (dh,con+dh,enc), h  ,posterior, h  ,posterior, hl2,posterior     rdz  dz and
bl1,posterior, bl2,posterior, b  ,posterior, b  ,posterior     rdz are its parameters. the derivative of the variational
bound is computed by sampling one latent variable at the end of each utterance: zt     n (  t.posterior,   t,posterior).

11

6.3 model examples

model responses for ubuntu are shown in table 5. all model responses are available for download
at www.iulianserban.com/files/ubuntudialoguecorpus.zip and http://www.iulianserban.com/
files/twitterdialoguecorpus.zip.

table 5: ubuntu model examples. the     token indicates a change of turn.

response
vhred: problem is that i am not
able to boot into the live cd, i can
access it from the live cd, but i dont
know how to use it
hred: how would i do that?
lstm: how do i do that?
vhred:
pastebin
/etc/apt/sources. list    le
hred: try " sudo apt-get -f install
"
lstm: something
vhred: fdisk -l gives me a list of
partitions
hred: how do i do that?
lstm: on the live cd?

your

something,

something,

vhred: there is a 64 bit version
of ubuntu
hred: i have no experience with
64bit, so i don   t know what you
mean.
lstm:
something, something, something
vhred: i know, i was just trying
to help you.
hred: how do i do that?
lstm: that is what i was looking
for, thanks
vhred: can you ping the router?
hred: that    s what i thought, but i
don   t know how to    x it
lstm:?
vhred: i have no idea, sorry
hred: yes, you can install grub to
the mbr of your hard drive
lstm: grub is installed on the mbr
of the hard drive

context
i set up my hd such that i have to type a passphrase to access it at boot. how can i remove that passwrd,
and just boot up normal. i did this at install, it works    ne, just tired of having reboots where i need to
be at terminal to type passwd in. help?     backup your data, and re-install without encryption " might
" be the easiest method

hi, everyone. i download the googleearth. deb, and want to install it with : sudo dpkg -i something.
deb, it tells me : google-earth-stable depends on lsb-core (>= 3.2 ) ; however : package lsb-core is
not installed. so i just install the lsb-core, but it comes error : <url> what should i do?     the error
suggests running " sudo apt-get -f install "     i tried that, it comes the same error

i   ve a 2.0 tb hard drive. it no longer boots from the hard drive, but i can see it from gpart and it has
a " boot "    ag. how do i recover it?     the boot    ag doesn   t mean anything in linux why do you
not have a backup? you can mount the partition in livecd and access the data easily ( assuming the
partition is healthy )     i have one hard drive for the moment ( well, i don   t count the 250gb drive )
besides, i don   t have two sata cables i will make a backup as soon as i get another nice sized hard
drive and a sata cable and how do i    nd out if the partition is healthy?     boot to ubuntu livecd and
try and mount the partition
bottom line is usually if you got 64bit why not use it? ( unless there is something that doesn   t work on
64bit, which is amitedly not much )     well, now clip2net is not working... just seems ubuntu runs
more seaid113ssly on 32-bit and after reading this article, im wondering if 64 is a bit hyped? <url>

its comming from my computer 15 people using same router and they have no issues, also it happened
recently, before it was ok     yes but your router may have optional settings for dns which if not set to
the same dns will override your network interface... it    s an option to consider

it looks as if the network    s dhcp server is dead : " < warn > ( eth0) : dhcpv4 request timed out "
    but it isn   t. windows works

i   ve been having trouble with a new 3tb harddrive and a fresh ubuntu install. i install ( as gpt ) and
the install looks ok. but then get grub " error : out of disk " after reboot. i have an october 2011 pc
with updated bios and i _do_ see the 3tb. google searches haven   t been to helpful so far. has anyone
seen grub trouble when installing on 3tb harddrives?     have you tried installing grub again and
sudo update-grub?     i guess i could try that. so i would have to
updating it? sudo grub-install
install grub from an older ubuntu install?

12

6.4 human study on amazon mechanical turk

setup

we choose to use id104 platforms such as amt rather than carrying out in-lab experiments, even
though in-lab experiments usually exhibit less noise and result in higher agreement between human annotators.
we do this because amt experiments involve a larger and more heterogeneous pool of annotators, which implies
less cultural and geographic biases, and because such experiments are easier to replicate, which we believe is
important for benchmarking future research on these tasks.
allowing the amt human evaluators to not assign preference for either response is important, since there are
several reasons why humans may not understand the dialogue context, which include topics they are not familiar
with, slang language and non-english language. we refer to such evaluations as    indeterminable   .
the evaluation setup resembles the classical turing test where human judges have to distinguish between
human-human conversations and human-computer conversations. however, unlike the original turing test, we
only ask human evaluators to consider the next utterance in a given conversation and we do not inform them that
any responses were generated by a computer. apart from minimum context and response lengths we impose no
restrictions on the generated responses.

selection process

at the beginning of each experiment, we brie   y instruct the human evaluator on the task and show them a simple
example of a dialogue context and two potential responses. to avoid presentation bias, we shuf   e the order of
the examples and the order of the potential responses for each example. during each experiment, we also show
four trivial    attention check    examples that any human evaluator who has understood the task should be able to
answer correctly. we discard responses from human evaluators who fail more than one of these checks.
we select the examples shown to human evaluators at random from the test set. we    lter out all non-english
conversations and conversations containing offensive content. this is done by automatically    ltering out all
conversations with non-ascii characters and conversations with profanities, curse words and otherwise offensive
content. this    ltering is not perfect, so we manually skim through many conversations and    lter out conversations
with non-english languages and offensive content. on average, we remove about 1/80 conversations manually.
to ensure that the evaluation process is focused on evaluating conditional dialogue response generation (as
opposed to unconditional single sentence generation), we constrain the experiment by    ltering out examples
with fewer than 3 turns in the context. we also    lter out examples where either of the two presented responses
contain less than 5 tokens. we remove the special token placeholders and apply regex expressions to detokenize
the text.

execution
we run the experiments in batches. for each pairs of models, we carry out 3     5 human intelligence tests (hits)
on amt. each hit contains 70     90 examples (dialogue context and two model responses) and is evaluated by
3     4 unique humans. in total we collect 5363 preferences in 69 hits.
the following are screenshots from one actual amazon mechanical turk (amt) experiment. these screenshots
show the introduction (debrie   ng) of the experiment, an example dialogue and one dialogue context with two
candidate responses, which human evaluators were asked to choose between. the experiment was carried
out using psiturk, which can be downloaded from www.psiturk.org. the source code will be released upon
publication.

13

figure 2: screenshot of the introduction (debrie   ng) of the experiment.

figure 3: screenshot of the introductory dialogue example.

14

figure 4: screenshot of one dialogue context with two candidate responses, which human evaluators
were asked to choose between.

15

