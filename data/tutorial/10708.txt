6
1
0
2

 

n
u
j
 

4
1

 
 
]
l
c
.
s
c
[
 
 

2
v
6
7
7
0
0

.

6
0
6
1
:
v
i
x
r
a

multiresolution recurrent neural networks:

an application to dialogue response generation

iulian vlad serban      
university of montreal
2920 chemin de la tour,
montr  al, qc, canada

tim klinger(cid:5)
ibm research

t. j. watson research center,

yorktown heights,

ny, usa

t. j. watson research center,

t. j. watson research center,

t. j. watson research center,

gerald tesauro(cid:5)
ibm research

yorktown heights,

ny, usa

kartik talamadupula(cid:5)

ibm research

yorktown heights,

ny, usa

bowen zhou(cid:5)
ibm research

yorktown heights,

ny, usa

yoshua bengio      

university of montreal
2920 chemin de la tour,
montr  al, qc, canada

aaron courville   

university of montreal
2920 chemin de la tour,
montr  al, qc, canada

abstract

we introduce the multiresolution recurrent neural network, which extends the
sequence-to-sequence framework to model id86 as two
parallel discrete stochastic processes: a sequence of high-level coarse tokens, and
a sequence of natural language tokens. there are many ways to estimate or learn
the high-level coarse tokens, but we argue that a simple extraction procedure is
suf   cient to capture a wealth of high-level discourse semantics. such procedure
allows training the multiresolution recurrent neural network by maximizing the
exact joint log-likelihood over both sequences. in contrast to the standard log-
likelihood objective w.r.t. natural language tokens (word perplexity), optimizing
the joint log-likelihood biases the model towards modeling high-level abstractions.
we apply the proposed model to the task of dialogue response generation in
two challenging domains:
the ubuntu technical support domain, and twitter
conversations. on ubuntu, the model outperforms competing approaches by a
substantial margin, achieving state-of-the-art results according to both automatic
id74 and a human evaluation study. on twitter, the model appears to
generate more relevant and on-topic responses according to automatic evaluation
metrics. finally, our experiments demonstrate that the proposed model is more
adept at overcoming the sparsity of natural language and is better able to capture
long-term structure.

* this work was carried out while the    rst author was at ibm research.
    email: {iulian.vlad.serban,yoshua.bengio,aaron.courville}@umontreal.ca
(cid:5) email: {tklinger,gtesauro,krtalamad,zhou}@us.ibm.com
    cifar senior fellow

1

introduction

recurrent neural networks (id56s) have been gaining popularity in the machine learning community
due to their impressive performance on tasks such as machine translation [32, 5] and speech recogni-
tion [10]. these results have spurred a cascade of novel neural network architectures [15], including
attention [1, 6], memory [35, 9, 15] and pointer-based mechanisms [19].
the majority of the previous work has focused on developing new neural network architectures within
the deterministic sequence-to-sequence framework. in other words, it has focused on changing the
parametrization of the deterministic function mapping input sequences to output sequences, trained by
maximizing the log-likelihood of the observed output sequence. instead, we pursue a complimentary
research direction aimed at generalizing the sequence-to-sequence framework to multiple input and
output sequences, where each sequence exhibits its own stochastic process. we propose a new class
of id56 models, called multiresolution recurrent neural networks (mrid56s), which model multiple
parallel sequences by factorizing the joint id203 over the sequences. in particular, we impose a
hierarchical structure on the sequences, such that information from high-level (abstract) sequences
   ows to low-level sequences (e.g. natural language sequences). this architecture exhibits a new
objective function for training: the joint log-likelihood over all observed parallel sequences (as
opposed to the log-likelihood over a single sequence), which biases the model towards modeling
high-level abstractions. at test time, the model generates    rst the high-level sequence and afterwards
the natural language sequence. this hierarchical generation process enables it to model complex
output sequences with long-term dependencies.
researchers have recently observed critical problems applying end-to-end neural network architec-
tures for dialogue response generation [28, 16]. the neural networks have been unable to generate
meaningful responses taking dialogue context into account, which indicates that the models have
failed to learn useful high-level abstractions of the dialogue. motivated by these shortcomings, we
apply the proposed model to the task of dialogue response generation in two challenging domains:
the goal-oriented ubuntu technical support domain and non-goal-oriented twitter conversations. in
both domains, the model outperforms competing approaches. in particular, for ubuntu, the model
outperforms competing approaches by a substantial margin according to both a human evaluation
study and automatic id74 achieving a new state-of-the-art result.

2 model architecture

2.1 recurrent neural network language model

we start by introducing the well-established recurrent neural network language model (id56lm) [20,
3]. id56lm variants have been applied to diverse sequential tasks, including dialogue modeling [28],
id133 [7], handwriting generation [8] and music composition [4]. let w1, . . . , wn be a
sequence of discrete variables, called tokens (e.g. words), such that wn     v for vocabulary v . the
id56lm is a probabilistic generative model, with parameters   , which decomposes the id203
over tokens:

p  (w1, . . . , wn ) =

p  (wn|w1, . . . , wn   1).

where the parametrized approximation of the output distribution uses a softmax id56:

p  (wn+1 = v|w1, . . . , wn) =

(cid:80)

exp(g(hn, v))
v(cid:48)   v exp(g(hn, v(cid:48)))
v hn,

,

hn = f (hn   1, wn), g(hn, v) = ot

(3)
where f is the hidden state update function, which we will assume is either the lstm gating unit
[11] or gru gating unit [5] throughout the rest of the paper. for the lstm gating unit, we consider
the hidden state hm to be the lstm cell and cell input hidden states concatenated. the matrix
i     rdh  |v | is the input id27 matrix, where column j contains the embedding for word
index j and dh     n is the id27 dimensionality. similarly, the matrix o     rdh  |v | is the
output id27 matrix. according to the model, the id203 of observing a token w at
position n + 1 increases if the context vector hn has a high dot-product with the id27

(1)

(2)

n(cid:89)

n=1

2

corresponding to token w. most commonly the model parameters are learned by maximizing the
log-likelihood (equivalent to minimizing the cross-id178) on the training set using id119.

2.2 hierarchical recurrent encoder-decoder

our work here builds upon that of sordoni et al. [31], who proposed the hierarchical recurrent
encoder-decoder model (hred). their model exploits the hierarchical structure in web queries in
order to model a user search session as two hierarchical sequences: a sequence of queries and a
sequence of words in each query. serban et al. [28] continue in the same direction by proposing
to exploit the temporal structure inherent in natural language dialogue. their model decomposes
a dialogue into a hierarchical sequence: a sequence of utterances, each of which is a sequence of
words. more speci   cally, the model consists of three id56 modules: an encoder id56, a context
id56 and a decoder id56. a sequence of tokens (e.g. words in an utterance) are encoded into a
real-valued vector by the encoder id56. this in turn is given as input to the context id56, which
updates its internal hidden state to re   ect all the information up to that point in time. it then produces
a real-valued output vector, which the decoder id56 conditions on to generate the next sequence
of tokens (next utterance). due to space limitations, we refer the reader to [31, 28] for additional
information on the model architecture. the hred model for modeling structured discrete sequences
is appealing for three reasons. first, it naturally captures the hierarchical structure we want to model
in the data. second, the context id56 acts like a memory module which can remember things at
longer time scales. third, the structure makes the objective function more stable w.r.t. the model
parameters, and helps propagate the training signal for    rst-order optimization methods [31].

2.3 multiresolution id56 (mrid56)

we consider the problem of generatively modeling multiple parallel sequences. each sequence is
hierarchical with the top level corresponding to utterances and the bottom level to tokens. formally, let
w1, . . . , wn be the    rst sequence of length n where wn = (wn,1, . . . , wn,kn ) is the n   th constituent
sequence consisting of kn discrete tokens from vocabulary v w. similarly, let z1, . . . , zn be the
second sequence, also of length n, where zn = (zn,1, . . . , zn,ln ) is the n   th constituent sequence
consisting of ln discrete tokens from vocabulary v z. in our experiments, each sequence wn will
consist of the words in a dialogue utterance, and each sequence zn will contain the coarse tokens
w.r.t. the same utterance (e.g. the nouns in the utterance).
our aim is to build a probabilistic generative model over all tokens in the constituent sequences
w1, . . . , wn and z1, . . . , zn . let    be the parameters of the generative model. we assume that wn is
independent of zn(cid:48) conditioned on z1, . . . , zn for n(cid:48) > n, and factor the id203 over sequences:

p  (w1, . . . , wn , z1, . . . , zn ) =

p  (zn|z1, . . . , zn   1)

p  (wn|w1, . . . , wn   1, z1, . . . , zn)

n=1

n=1

p  (zn|z1, . . . , zn   1)p  (wn|w1, . . . , wn   1, z1, . . . , zn),

(4)

n(cid:89)

n=1

=

where we de   ne the conditional probabilities over the tokens in each constituent sequence:

n(cid:89)

n(cid:89)

p  (zn|z1, . . . , zn   1) =

p  (wn|w1, . . . , wn   1, z1, . . . , zn) =

p  (zn,m|zn,1, . . . , zn,m   1, z1, . . . , zn   1)

p  (wn,m|wn,1, . . . , wn,m   1, w1, . . . , wn   1, z1, . . . , zn)

m=1

we refer to the distribution over z1, . . . , zn as the coarse sub-model, and to the distribution
over w1, . . . , wn as the natural language sub-model. for the coarse sub-model, we parametrize
the conditional distribution p  (zn|z1, . . . , zn   1) as the hred model described in subsection
2.2, applied to the sequences z1, . . . , zn . for the natural language sub-model, we parametrize
p  (wn|w1, . . . , wn   1, z1, . . . , zn) as the hred model applied to the sequences w1, . . . , wn , but
with one difference. the coarse prediction encoder gru-gated id56 encodes all the previously
generated tokens z1, . . . , zn into a real-valued vector, which is concatenated with the context id56

3

ln(cid:89)
kn(cid:89)

m=1

figure 1: computational graph for the multiresolution recurrent neural network (mrid56). the lower
part models the stochastic process over coarse tokens, and the upper part models the stochastic process
over natural language tokens. the rounded boxes represent (deterministic) real-valued vectors, and
the variables z and w represent the coarse tokens and natural language tokens respectively.

and given as input to the natural language decoder id56. the coarse prediction encoder id56 is
important because it encodes the high-level information, which is transmitted to the natural language
sub-model. unlike the encoder for the coarse-level sub-model, this encoding will be used to generate
natural language and therefore the id56 uses different id27 parameters. at generation
time, the coarse sub-model generates a coarse sequence (e.g. a sequence of nouns), which corresponds
to a high-level decision about what the natural language sequence should contain (e.g. nouns to
include in the natural language sequence). conditioned on the coarse sequence, through the coarse
prediction encoder id56, the natural language sub-model then generates a natural language sequence
(e.g. dialogue utterance). the model is illustrated in figure 1.
we will assume that both z1, . . . , zn and w1, . . . , wn are observed and optimize the parameters
w.r.t. the joint log-likelihood over both sequences. at test time, to generate a response for sequence n
we exploit the probabilistic factorization to approximate the maximum a posteriori (map) estimate:

wn,zn

arg max
    arg max

wn

p  (wn, zn|w1, . . . , wn   1, z1, . . . , zn   1)

p  (wn|w1, . . . , wn   1, z1, . . . , zn   1, zn) arg max

zn

p  (zn|z1, . . . , zn   1),

(5)

where we further approximate the map for each constituent sequence using id125.

4

3 tasks

we consider the task of natural language response generation for dialogue. dialogue systems have
been developed for applications ranging from technical support to language learning and entertainment
[36, 30]. dialogue systems can be categorized into two different types: goal-driven dialogue systems
and non-goal-driven dialogue systems [27]. to demonstrate the versatility of the mrid56, we apply it
to both goal-driven and non-goal-driven dialogue tasks. we focus on the task of conditional response
generation. given a dialogue context consisting of one or more utterances, the model must generate
the next response in the dialogue.

ubuntu dialogue corpus the goal-driven dialogue task we consider is technical support for the
ubuntu operating system, where we use the ubuntu dialogue corpus [18]. the corpus consists
of about 0.5 million natural language dialogues extracted from the #ubuntu internet relayed chat
(irc) channel. users entering the chat channel usually have a speci   c technical problem. the users
   rst describe their problem and afterwards other users try to help them resolve it. the technical
problems range from software-related issues (e.g. installing or upgrading existing software) and
hardware-related issues (e.g.    xing broken drivers or partitioning hard drives) to informational needs
(e.g.    nding software with speci   c functionality). additional details are given in appendix 8.

twitter dialogue corpus the next task we consider is the non-goal-driven task of generating
responses to twitter conversations. we use a twitter dialogue corpus extracted in the    rst half of
2011 using a procedure similar to ritter et al. [24]. unlike the ubuntu domain, twitter conversations
are often more noisy and do not necessarily center around a single topic. we perform a minimal
preprocessing on the dataset to remove irregular punctuation marks and afterwards tokenize it. the
dataset is split into training, validation and test sets containing respectively 749, 060, 93, 633 and
10, 000 dialogues.1

4 coarse sequence representations

we experiment with two procedures for extracting the coarse sequence representations:

noun representation this procedure aims to exploit the basic high-level structure of natural lan-
guage discourse.it is based on the hypothesis that dialogues are topic-driven and that these
topics may be characterized by nouns. in addition to a tokenizer, used by both the hred
and id56lm model, it requires a part-of-speech (pos) tagger to identify the nouns in the
dialogue. the procedure uses a set of 84 and 795 prede   ned stop words for ubuntu and
twitter respectively. it maps a natural language utterance to its coarse representation by
extracting all the nouns using the pos tagger and then removing all stop words and repeated
words (keeping only the    rst occurrence of a word). dialogue utterances without nouns are
assigned the "no_nouns" token. the procedure also extracts the tense of each utterance and
adds it to the beginning of the coarse representation.

activity-entity representation this procedure is speci   c to the ubuntu technical support task,
for which it aims to exploit domain knowledge related to technical problem solving. it is
motivated by the observation that most dialogues are centered around activities and entities.
for example, it is very common for users to state a speci   c problem they want to resolve,
e.g. how do i install program x? or my driver x doesn   t work, how do i    x it? in response
to such questions, other users often respond with speci   c instructions, e.g. go to website
x to download software y or try to execute command x. in such cases, it is clear that
the principal information resides in the technical entities and in the verbs (e.g. install,    x,
download), and therefore that it will be advantageous to explicitly model this structure.
motivated by this observation, the procedure uses a set of 192 activities (verbs), created by
manual inspection, and a set of 3115 technical entities and 230 frequent terminal commands,
extracted automatically from available package managers and from the web. the procedure
uses the pos tagger to extract the verbs from the each natural language utterance. it maps

1due to twitter   s terms of service we are not allowed to redistribute twitter content. therefore, only the tweet
ids can be made public. these are available at: www.iulianserban.com/files/twitterdialoguecorpus.
zip.

5

the natural language to its coarse representation by keeping only verbs from the activity set,
as well as entities from the technical entity set (irrespective of their pos tags). if no activity
is found in an utterance, the representation is assigned the "none_activity" token. the
procedure also appends a binary variable to the end of the coarse representation indicating if
a terminal command was detected in the utterance. finally, the procedure extracts the tense
of each utterance and adds it to the beginning of the coarse representation.

both extraction procedures are applied at the utterance level, therefore there exists a one-to-one
alignment between coarse sequences and natural language sequences (utterances). there also exists a
one-to-many alignment between the coarse sequence tokens and the corresponding natural language
tokens, with the exception of a few special tokens. further details are given in appendix 9. 2

5 experiments

the models are implemented in theano [33]. we optimize all models based on the training set joint
log-likelihood over coarse sequences and natural language sequences using the    rst-order stochastic
gradient optimization method adam [13]. we train all models using early stopping with patience
on the joint-log-likelihood [2]. we choose our hyperparameters based on the joint log-likelihood
of the validation set. we de   ne the 20k most frequent words as the vocabulary and the word
embedding dimensionality to size 300 for all models, with the exception of the id56lm and hred
on twitter, where we use embedding dimensionality of size 400. we apply gradient clipping to stop
the parameters from exploding [23]. at test time, we use a id125 of size 5 for generating the
model responses. further details are given in appendix 10

5.1 baseline models

we compare our models to several baselines used previously in the literature. the    rst is the standard
id56lm with lstm gating function [20] (lstm), which at test time is similar to the id195 lstm
model [32]. the second baseline is the hred model with lstm gating function for the decoder
id56 and gru gating function for the encoder id56 and context id56, proposed for dialogue
response generation by serban et al.[28] [31]. source code for both baseline models will be made
publicly available upon acceptance for publication. for both ubuntu and twitter, we specify the
id56lm model to have 2000 hidden units with the lstm gating function. for ubuntu, we specify
the hred model to have 500, 1000 and 500 hidden units respectively for the encoder id56, context
id56 and decoder id56. for twitter, we specify the hred model to have 2000, 1000 and 1000
hidden units respectively for the encoder id56, context id56 and decoder id56. the third baseline
is the latent variable latent variable hierarchical recurrent encoder-decoder (vhred) proposed by
serban et al. [29]. we use the exact same vhred models as serban et al. [29].
for ubuntu, we introduce a fourth baseline, called hred + activity-entity features, which has
access to the past activity-entity pairs. this model is similar to to the natural language sub-model of
the mrid56 model, with the difference that the natural language decoder id56 is conditioned on
a real-valued vector, produced by a gru id56 encoding only the past coarse-level activity-entity
sub-sequences. this baseline helps differentiate between a model which observes the coarse-level
sequences only as as additional features and a model which explicitly models the stochastic process of
the coarse-level sequences. we specify the model to have 500, 1000, 2000 hidden units respectively
for the encoder id56, context id56 and decoder id56. we specify the gru id56 encoding the past
coarse-level activity-entity sub-sequences to have 500 hidden units.

5.2 multiresolution id56

the coarse sub-model is parametrized as the bidirectional-hred model [28] with 1000, 1000 and
2000 hidden units respectively for the coarse-level encoder, context and decoder id56s. the natural
language sub-model is parametrized as a conditional hred model with 500, 1000 and 2000 hidden
units respectively for the natural language encoder, context and decoder id56s. the coarse prediction
encoder id56 gru id56 is parametrized with 500 hidden units.

2the pre-processed ubuntu dialogue corpus used, as well as the noun representations and activity-entity

representations, are available at www.iulianserban.com/files/ubuntudialoguecorpus.zip.

6

table 1: ubuntu evaluation using precision (p), recall (r), f1 and accuracy metrics w.r.t. activity,
entity, tense and command (cmd) on ground truth utterances, and human    uency and relevancy scores
given on a scale 0-4 (    indicates scores signi   cantly different from baseline models at 90% con   dence)

activity

entity

p

r

f1

p

r

f1

tense
acc.

cmd
acc.

human eval.

fluency relevancy

1.7

1.03

1.18

1.18

0.81

0.87

14.57

94.79

-

-

5.93

4.05

4.34

2.81

2.16

2.22

22.2

92.58

2.98

1.01

6.43

4.31

4.63

3.28

2.41

2.53

20.2

92.02

-

-

7.15

5.5

5.46

3.03

2.43

2.44

28.02

86.69

5.81

3.56

4.04

8.68

5.55

6.31

24.03

90.66

16.84

9.72

11.43

4.91

3.36

3.72

29.01

95.04

2.96

3.48   

3.42   

0.75

1.32   

1.04

model

lstm

hred

vhred

hred +
act.-ent.
mrid56
noun
mrid56
act.-ent.

5.3 ubuntu

evaluation methods
it has long been known that accurate evaluation of dialogue system responses
is dif   cult [26]. liu et al. [17] have recently shown that all automatic id74 adapted for
such evaluation, including word overlap-based metrics such as id7 and meteor, have either
very low or no correlation with human judgment of the system performance. we therefore carry out
an in-lab human study to evaluate the ubuntu models. we recruit 5 human evaluators, and show them
each 30     40 dialogue contexts with the ground truth response and 4 candidate responses (hred,
hred + activity-entity features and mrid56s). for each context example, we ask them to compare
the candidate responses to the ground truth response and dialogue context, and rate them for    uency
and relevancy on a scale 0     4. our setup is very similar to the evaluation setup used by koehn and
monz [14], and comparable to liu et al [17]. further details are given in appendix 11.
we further propose a new set of metrics for evaluating model responses on ubuntu, which compare the
activities and entities in the model generated response with those of the ground truth response. that
is, the ground truth and model responses are mapped to their respective activity-entity representations,
using the automatic procedure discussed in section 4, and then the overlap between their activities
and entities are measured according to precision, recall and f1-score. based on a careful manual
inspection of the extracted activities and entities, we believe that these metrics are particularly
suited for the goal-oriented ubuntu dialogue corpus. the activities and entities re   ect the principal
instructions given in the responses, which are key to resolving the technical problems. therefore,
a model able to generate responses with actions and entities similar to the ground truth human
responses     which often do lead to solving the users problem     is more likely to yield a successful
dialogue system. the reader is encouraged to verify the details and completeness of the activity-entity
representations in appendix 9. scripts to generate the noun and activity-entity representations, and to
evaluate the dialogue responses w.r.t. activity-entity pairs are available online.3

results the results on ubuntu are given in table 1. the mrid56s clearly perform substantially
better than the baseline models both w.r.t. human evaluation and automatic id74. the
mrid56 with noun representations achieves 2x     3x higher scores w.r.t. entities compared to other
models, and the human evaluators also rate its    uency and relevancy substantially higher than other
models. the mrid56 with activity representations achieves 2x     3x higher scores w.r.t. activities
compared to other models and nearly 2x higher scores w.r.t. entities compared to all baselines. human
evaluators also rate its    uency substantially higher than the baseline models. however,its relevancy
is rated only slightly higher compared to the hred model, which we believe is caused by human

3https://github.com/julianser/ubuntu-multiresolution-tools/tree/master/

actentrepresentation.

7

evaluators being more likely to noticing software entities than actions in the dialogue responses (even
though actions are critical to solving the actual technical problem). overall, the results demonstrate
that the mrid56s have learned to model high-level goal-oriented sequential structure on ubuntu.

table 2: ubuntu model examples. the arrows indicate change of turn.

context
hey guys what do you in general use for irc something ubuntu xchat or xchat-
gnome ?     without -gnome. that is just cut down     you mean drop xchat-
gnome and go with xchat ?

when setting up rules with iptables command only writes changes this    le "
etciptables. rules "? i ask this so i can backup before messing anything    
sudo iptables-save something . dat to backup your rules restore with sudo
iptables-restore < something . dat

hello i tried to install ubuntu studio but i get w : failed to fetch <url >404
not found when apt-get update     the ppa you added is not online

hi community.. is there any difference between " /.bashrc " " /.bashrc. local
"     yes, they   re different    les :-) the default /.bashrc sources /.bashrc. local
    sorry i could undersatnd... if i write on terminal " gedit /.bashrc " it opens
a    le but how i open " /.bashrc. local "     " gedit /.bashrc. local "...     but
it just open blank    le nothing inside it

response
mrid56 act. -ent.: im using xchat right now
mrid56 noun: what is xchat-gnome ?
vhred: correct
hred: yes
mrid56 act. -ent.: i don   t see any reason why
you need iptables to do that
mrid56 noun: are you using ubuntu ?
vhred: thx
hred: thanks
mrid56 act. -ent.: try a different mirror
mrid56 noun: something
vhred: but i dont know how to add it
hred: how do i do that ?
mrid56 act. -ent.: open up gedit /.bashrc, there
is a    le called " something "
mrid56 noun: the    le is empty..
vhred: it   s in /.bashrc or /.bashrc
hred: what are you trying to do ?

model responses are shown in table 2.
in general, the mrid56 responses are more coherent
and topic-oriented compared to the other model responses, which usually produce very generic
responses [28]. in particular, the mrid56 with activity-entity representation appears to give more
goal-oriented instructions compared to the mrid56 with noun representation (see examples 2-4 in
table 2). additional examples are shown in appendix 12.

5.4 twitter

evaluation methods for twitter, similar to the ubuntu metrics, we use the precision, recall and
f1 metrics between the model responses and ground truth responses w.r.t. the noun representation.
the reason we propose to use these metrics is similar to the reason given for the ubuntu metrics
related to entities: a good model response is one which includes the same nouns as the ground truth
response. we also compute the tense accuracy, as we did for ubuntu. furthermore, we use the
three embedding-based textual similarity metrics proposed by liu et al. [17]: embedding average
(average), embedding extrema (extrema) and embedding greedy (greedy). all three metrics are
based on computing the textual similarity between the ground truth response and the model response
using id27s. all three metrics measure topic similarity: if a model-generated response
is on the same topic as the ground truth response (e.g. contain paraphrases of the same words),
the metrics will yield a high score. this is a highly desirable property for dialogue systems on an
open platform such as twitter, however it is also substantially different from measuring the overall
dialogue system performance, or the appropriateness of a single response, which would require
human evaluation.

results the results on twitter are given in table 3. the responses of the mrid56 with noun
representation are better than all other models on precision, recall and f1 w.r.t nouns. mrid56 is also
better than all other models w.r.t. tense accuracy, and it is on par with vhred on the embedding-
based metrics. in accordance with our previous results, this indicates that the model has learned
to generate more on-topic responses and, thus, that explicitly modeling the stochastic process over
nouns helps learn the high-level structure. this is con   rmed by qualitative inspection of the generated
responses, which are clearly more topic-oriented. see table 10 in appendix.

6 related work

closely related to our work is the model proposed by ji et al.[12], which jointly models natural
language text and high-level discourse phenomena. however, it only models a discrete class per
sentence at the high level, which must be manually annotated by humans. on the other hand, mrid56
models a sequence of automatically extracted high-level tokens. recurrent neural network models

8

table 3: twitter evaluation using precision (p), recall (r), f1 and accuracy metrics w.r.t. noun
representation, tense accuracy and embedding-based id74 on ground truth utterances.

noun

r

p

f1

tense
acc.

embedding metrics

average greedy extrema

0.71

0.71

0.65

27.06

51.24

38.9

36.58

0.31

0.31

0.29

26.47

50.1

37.83

35.55

0.5

0.51

0.46

26.66

53.26

39.64

37.98

4.82

5.22

4.63

34.48

49.77

40.44

37.45

model

lstm

hred

vhred

mrid56
noun

with stochastic latent variables, such as the variational recurrent neural networks by chung et
al. [7], are also closely related to our work. these models face the more dif   cult task of learning
the high-level representations, while simultaneously learning to model the generative process over
high-level sequences and low-level sequences, which is a more dif   cult optimization problem. in
addition to this, such models assume the high-level latent variables to be continuous, usually gaussian,
distributions.
recent dialogue-speci   c neural network architectures, such as the model proposed by wen et al. [34],
are also relevant to our work. different from the mrid56, they require domain-speci   c hand-crafted
high-level (dialogue state) representations with human-labelled examples, and they usually consist of
several sub-components each trained with a different objective function.

7 discussion

we have proposed the multiresolution recurrent neural network (mrid56) for generatively modeling
sequential data at multiple levels of abstraction. it is trained by optimizing the joint log-likelihood
over the sequences at each level. we apply mrid56 to dialog response generation on two different
tasks, ubuntu technical support and twitter conversations, and evaluate it in a human evaluation study
and via automatic id74. on ubuntu, mrid56 demonstrates dramatic improvements
compared to competing models. on twitter, mrid56 appears to generate more relevant and on-topic
responses. even though abstract information is implicitly present in natural language dialogues,
by explicitly representing information at different levels of abstraction and jointly optimizing the
generation process across abstraction levels, mrid56 is able to generate more    uent, relevant and
goal-oriented responses. the results suggest that the    ne-grained abstraction (low-level) provides
the architecture with increased    uency for predicting natural utterances, while the coarse-grained
(high-level) abstraction gives it the semantic structure necessary to generate more coherent and
relevant utterances. the results also imply that it is not simply a matter of adding additional features
for prediction     mrid56 outperforms a competitive baseline augmented with the coarse-grained
abstraction sequences as features     rather, it is the combination of representation and generation at
multiple levels that yields the improvements. finally, we observe that the architecture provides a
general framework for modeling discrete sequences, as long as a coarse abstraction is available. we
therefore conjecture that the architecture may successfully be applied to broader natural language
generation tasks, such as generating prose and persuasive argumentation, and other tasks involving
discrete sequences, such as music composition. we leave this to future work.

acknowledgments

the authors thank ryan lowe, michael noseworthy, caglar gulcehre, sungjin ahn, harm de vries,
song feng and on yi ching for participating and helping with the human study. the authors thank
orhan firat and caglar gulcehre for constructive feedback, and thank ryan lowe, nissan pow and
joelle pineau for making the ubuntu dialogue corpus available to the public.

9

references
[1] bahdanau, d., cho, k., and bengio, y. (2015). id4 by jointly learning to align and

translate. in iclr.

[2] bengio, y. (2012). practical recommendations for gradient-based training of deep architectures. in neural

networks: tricks of the trade, pages 437   478. springer.

[3] bengio, y., ducharme, r., vincent, p., and janvin, c. (2003). a neural probabilistic language model. the

journal of machine learning research, 3, 1137   1155.

[4] boulanger-lewandowski, n., bengio, y., and vincent, p. (2012). modeling temporal dependencies in

high-dimensional sequences: application to polyphonic music generation and transcription. in icml.

[5] cho, k. et al. (2014). learning phrase representations using id56 encoder   decoder for statistical machine

translation. in proc. of emnlp, pages 1724   1734.

[6] chorowski, j. k., bahdanau, d., serdyuk, d., cho, k., and bengio, y. (2015). attention-based models for

id103. in nips, pages 577   585.

[7] chung, j., kastner, k., dinh, l., goel, k., courville, a., and bengio, y. (2015). a recurrent latent variable

model for sequential data. in nips, pages 2962   2970.

[8] graves, a. (2013). generating sequences with recurrent neural networks. arxiv:1308.0850.
[9] graves, a., wayne, g., and danihelka, i. (2014). id63s. arxiv:1410.5401.
[10] hinton, g. et al. (2012). deep neural networks for acoustic modeling in id103: the shared

views of four research groups. signal processing magazine, ieee, 29(6), 82   97.

[11] hochreiter, s. and schmidhuber, j. (1997). long short-term memory. neural computation, 9(8).
[12] ji, y., haffari, g., and eisenstein, j. (2016). a latent variable recurrent neural network for discourse

relation language models. in naacl-hlt.

[13] kingma, d. and ba, j. (2015). adam: a method for stochastic optimization. in proc. of iclr.
[14] koehn, p. and monz, c. (2006). manual and automatic evaluation of machine translation between european

languages. in workshop on id151, acl, pages 102   121.

[15] kumar, a., irsoy, o., su, j., bradbury, j., english, r., pierce, b., ondruska, p., gulrajani, i., and socher, r.

(2016). ask me anything: dynamic memory networks for natural language processing. icml.

[16] li, j., galley, m., brockett, c., gao, j., and dolan, b. (2016). a diversity-promoting objective function for

neural conversation models. in naacl.

[17] liu, c.-w., lowe, r., serban, i. v., noseworthy, m., charlin, l., and pineau, j. (2016). how not to
evaluate your dialogue system: an empirical study of unsupervised id74 for dialogue response
generation. arxiv:1603.08023.

[18] lowe, r., pow, n., serban, i., and pineau, j. (2015). the ubuntu dialogue corpus: a large dataset for

research in unstructured multi-turn dialogue systems. in proc. of sigdial-2015.

[19] luong, m. t., sutskever, i., le, q. v., vinyals, o., and zaremba, w. (2015). addressing the rare word

problem in id4. in acl.

[20] mikolov, t. et al. (2010). recurrent neural network based language model. in 11th proceedings of

interspeech, pages 1045   1048.

[21] mikolov, t. et al. (2013). distributed representations of words and phrases and their compositionality. in

advances in neural information processing systems, pages 3111   3119.

[22] owoputi, o. et al. (2013). improved part-of-speech tagging for online conversational text with word

clusters. in proc. of acl.

[23] pascanu, r., mikolov, t., and bengio, y. (2012). on the dif   culty of training recurrent neural networks.

proceedings of the 30th international conference on machine learning, 28.

[24] ritter, a., cherry, c., and dolan, w. b. (2011a). data-driven response generation in social media. in

emnlp.

[25] ritter, a., clark, s., mausam, and etzioni, o. (2011b). id39 in tweets: an experimental

study. in proc. of emnlp, pages 1524   1534.

[26] schatzmann, j., georgila, k., and young, s. (2005). quantitative evaluation of user simulation techniques

for spoken dialogue systems. in 6th sigdial workshop on discourse and dialogue.

[27] serban, i. v., lowe, r., charlin, l., and pineau, j. (2015). a survey of available corpora for building

data-driven dialogue systems. corr, abs/1512.05742.

[28] serban, i. v., sordoni, a., bengio, y., courville, a. c., and pineau, j. (2016a). building end-to-end

dialogue systems using generative hierarchical neural network models. in aaai, pages 3776   3784.

[29] serban, i. v., sordoni, a., lowe, r., charlin, l., pineau, j., courville, a., and bengio, y. (2016b). a
hierarchical latent variable encoder-decoder model for generating dialogues. arxiv preprint arxiv:1605.06069.

10

[30] shawar, b. a. and atwell, e. (2007). chatbots: are they really useful? in ldv forum, volume 22.
[31] sordoni, a., bengio, y., vahabi, h., lioma, c., simonsen, j. g., and nie, j.-y. (2015). a hierarchical

recurrent encoder-decoder for generative context-aware query suggestion. in proc. of cikm-2015.

[32] sutskever, i., vinyals, o., and le, q. v. (2014). sequence to sequence learning with neural networks. in

nips, pages 3104   3112.

[33] theano development team (2016). theano: a python framework for fast computation of mathematical

expressions. arxiv e-prints, abs/1605.02688.

[34] wen, t.-h., gasic, m., mrksic, n., rojas-barahona, l. m., su, p.-h., ultes, s., vandyke, d., and young, s.

(2016). a network-based end-to-end trainable task-oriented dialogue system. arxiv:1604.04562.

[35] weston, j., chopra, s., and bordes, a. (2015). memory networks. iclr.
[36] young, s., gasic, m., thomson, b., and williams, j. d. (2013). pomdp-based statistical spoken dialog

systems: a review. proceedings of the ieee, 101(5), 1160   1179.

11

appendix

8 task details

ubuntu we use the ubuntu dialogue corpus v2.0 extracted jamuary, 2016: http://cs.mcgill.ca/
~jpineau/datasets/ubuntu-corpus-1.0/.

twitter we preprocess the dataset using the moses tokenizer extracted june, 2015: https://github.com/
moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl.4

9 coarse sequence representations

nouns

the noun-based procedure for extracting coarse tokens aims to exploit high-level structure of natural language
discourse. more speci   cally, it builds on the hypothesis that dialogues in general are topic-driven and that these
topics may be characterized by the nouns inside the dialogues. at any point in time, the dialogue is centered
around one or several topics. as the dialogue progresses, the underlying topic evolves as well. in addition to the
tokenizer required by the previous extraction procedure, this procedure also requires a part-of-speech (pos)
tagger to identify the nouns in the dialogue suitable for the language domain.
for extracting the noun-based coarse tokens, we de   ne a set of 795 stop words for twitter and 84 stop words for
ubuntu containing mainly english pronouns, punctuation marks and prepositions (excluding special placeholder
tokens). we then extract the coarse tokens by applying the following procedure to each dialogue:

1. we apply the pos tagger version 0.3.2 developed by owoputi and colleagues [22] to extract pos.5 for
twitter, we use the parser trained on the twitter corpus developed by ritter et al. [25]. for ubuntu, we
use the parser trained on the nps chat corpus developed by forsyth and martellwhich was extracted
from irc chat channels similar to the ubuntu dialogue corpus.67

2. given the pos tags, we remove all words which are not tagged as nouns and all words containing

non-alphabet characters.8. we keep all urls and paths.

3. we remove all stop words and all repeated tokens, while maintaining the order of the tokens.

4. we add the "no_nouns" token to all utterances, which do not contain any nouns. this ensures that
no coarse sequences are empty. it also forces the coarse sub-model to explicitly generate at least one
token, even when there are no actual nouns to generate.

5. for each utterance, we use the pos tags to detect three types of time tenses: past, present and future
tenses. we append a token indicating which of the 23 tenses are present at the beginning of each
utterance.9 if no tenses are detected, we append the token "no_tenses".

as before, there exists a one-to-many alignment between the extracted coarse sequence tokens and the natural
language tokens, since this procedure also maintains the ordering of all special placeholder tokens, with the
exception of the "no_nouns" token.
we cut-off the vocabulary at 10000 coarse tokens for both the twitter and ubuntu datasets excluding the special
placeholder tokens. on average a twitter dialogue in the training set contains 25 coarse tokens, while a ubuntu
dialogue in the training set contains 36 coarse tokens.

4due to twitter   s terms and conditions we are unfortunately not allowed to publish the preprocessed dataset.
5www.cs.cmu.edu/~ark/tweetnlp/
6as input to the pos tagger, we replace all unknown tokens with the word "something" and remove all
special placeholder tokens (since the pos tagger was trained on a corpus without these words). we further
reduce any consecutive sequence of spaces to a single space. for ubuntu, we also replace all commands and
entities with the word "something". for twitter, we also replace all numbers with the word "some", all urls with
the word "somewhere" and all heart emoticons with the word "love".

7forsyth, e. n. and martell, c. h. (2007). lexical and discourse analysis of online chat dialog. in semantic

computing, 2007. icsc 2007. international conference on, pages 19   26. ieee.

8we de   ne nouns as all words with tags containing the pre   x "nn" according to the ptb-style tagset.
9note that an utterance may contain several sentences. it therefore often happens that an utterance contains

several time tenses.

12

table 4: unigram and bigram models bits per word on noun representations.

model
unigram
bigram

ubuntu twitter
12.38
10.16
7.26
7.76

model statistics for the unigram and bigram language models are presented in table 4 for the noun representations
on the ubuntu and twitter training sets.10 the table shows a substantial difference in bits per words between the
unigram and bigram models, which suggests that the nouns are signi   cantly correlated with each other.

activity-entity pairs

the activity-entity-based procedure for extracting coarse tokens attempts to exploit domain speci   c knowledge
for the ubuntu dialogue corpus, in particular in relation to providing technical assistance with problem solving.
our manual inspection of the corpus shows that many dialogues are centered around activities. for example, it
is very common for users to state a speci   c problem they want to resolve, e.g. how do i install program x? or
my driver x doesn   t work, how do i    x it?. in response to such queries, other users often respond with speci   c
instructions, e.g. go to website x to download software y or try to execute command x. in addition to the
technical entities, the principle message conveyed by each utterance resides in the verbs, e.g. install, work,    x,
go, to, download, execute. therefore, it seems clear that a dialogue system must have a strong understanding of
both the activities and technical entities if it is to effectively assist users with technical problem solving. it seems
likely that this would require a dialogue system able to relate technical entities to each other, e.g. to understand
that    refox depends on the gcc library, and conform to the temporal structure of activities, e.g. understanding
that the install activity is often followed by download activity.
we therefore construct two word lists: one for activities and one for technical entities. we construct the activity
list based on manual inspection yielding a list of 192 verbs. for each activity, we further develop a list of
synonyms and conjugations of the tenses of all words. we also use id97 id27s [21], trained on
the ubuntu dialogue corpous training set, to identify commonly misspelled variants of each activity. the result
is a dictionary, which maps a verb to its corresponding activity (if such exists). for constructing the technical
entity list, we scrape publicly available resources, including ubuntu and linux-related websites as well as the
debian package manager apt. similar to the activities, we also use the id97 id27s to identify
misspelled and paraphrased entities. this results in another dictionary, which maps one or two words to the
corresponding technical entity. in total there are 3115 technical entities. in addition to this we also compile a list
of 230 frequent commands. examples of the extracted activities, entities and commands can be found in the
appendix.
afterwards, we extract the coarse tokens by applying the following procedure to each dialogue:

1. we apply the technical entity dictionary to extract all technical entities.
2. we apply the pos tagger version 0.3.2 developed by owoputi and colleagues, trained on the nps
chat corpus developed by forsyth and martell as before. as input to the pos tagger, we map all
technical entities to the token "something". this transformation should improve the id52
accuracy, since the corpus the parser was trained on does not contain technical words.

3. given the pos tags, we extract all verbs which correspond to activities.11. if there are no verbs in
an entire utterance and the pos tagger identi   ed the    rst word as a noun, we will assume that the
   rst word is in fact a verb. we do this, because the parser does not work well for tagging technical
instructions in imperative form, e.g. upgrade    refox. if no activities are detected, we append the token
"none_activity" to the coarse sequence. we also keep all urls and paths.

4. we remove all repeated activities and technical entities, while maintaining the order of the tokens.
5. if a command is found inside an utterance, we append the "cmd" token at the end of the utterance.
otherwise, we append the "no_cmd" token to the end of the utterance. this enables the coarse
sub-model to predict whether or not an utterance contains executable commands.

6. as for the noun-based coarse representation, we also append the time tense to the beginning of the

sequence.

as before, there exists a one-to-many alignment between the extracted coarse sequence tokens and the natural
language tokens, with the exception of the "none_activity" and "no_cmd" tokens.

10the models were trained using maximum log-likelihood on the noun representations excluding all special

tokens.

11we de   ne verbs as all words with tags containing the pre   x "vb" according to the ptb-style tagset.

13

table 5: twitter coarse sequence examples

natural language tweets
<   rst_speaker> at pinkberry
with my pink princess enjoying
a precious moment <url>

<second_speaker>-
they
are adorable, alma still speaks
about emma bif sis . hugs
<   rst_speaker> <at> when you
are spray painting, where are
you doing it ? outside ? in your
apartment ? where ?

<second_speaker> <at> mostly
spray painting outside but some
little stuff in the bathroom .

noun representation
present_tenses
princess moment

pinkberry

present_tenses
bif sis hugs

alma

emma

present_tenses spray painting
apartment

present_tenses
bathroom

spray

stuff

table 6: ubuntu coarse sequence examples

natural language dialogues
if you can get a hold of the logs, there    s stuff
from **unknown** about his inability to install
amd64

i   ll check fabbione    s log,
thanks sounds
like he had the same problem i did ew, why ? ...

upgrade lsb-base and acpid

i   m up to date

what error do you get ?

i don   t    nd error
from ?
manually in a root sterm ...

:/ where do i search
acpid works, but i must launch it

activity-entity coarse dialogues
future_tenses
amd64_entity no_cmd

get_activity

install_activity

no_tenses
past_present_tenses
none_activity
no_tenses none_activity no_cmd ...

check_activity

no_cmd
no_cmd

no_tenses
acpid_entity no_cmd

upgrade_activity

lsb_entity

no_tenses none_activity no_cmd

present_tenses get_activity no_cmd

present_tenses
present_future_tenses
acpid_entity root_entity no_cmd ...

discover_activity

no_cmd
work_activity

since the number of unique tokens are smaller than 10000, we do not need to cut-off the vocabulary. on average
a ubuntu dialogue in the training set contains 43 coarse tokens.
our manual inspection of the extracted coarse sequences, show that the technical entities are identi   ed with
very high accuracy and that the activities capture the main intended action in the majority of utterances. due to
the high quality of the extracted activities and entities, we are con   dent that they may be used for evaluation
purposes as well.
scripts
responses w.r.t. activity-entity pairs are available online at:
ubuntu-multiresolution-tools/tree/master/actentrepresentation.

and to evaluate the dialogue
https://github.com/julianser/

to generate the noun and activity-entity representations,

14

stop words for noun-based coarse tokens

ubuntu stop words for noun-based coarse representation:

all another any anybody anyone anything both each each other either everybody everyone everything few he her hers herself him himself
his i it its itself many me mine more most much myself neither no one nobody none nothing one one another other others ours ourselves
several she some somebody someone something that their theirs them themselves these they this those us we what whatever which
whichever who whoever whom whomever whose you your yours yourself yourselves . , ?     -     !

15

twitter stop words for noun-based coarse representation: 12

all another any anybody anyone anything both each each other either everybody everyone everything few he her hers herself him himself
his i it its itself many me mine more most much myself neither no one nobody none nothing one one another other others ours ourselves
several she some somebody someone something that their theirs them themselves these they this those us we what whatever which
whichever who whoever whom whomever whose you your yours yourself yourselves .
, ?     -     !able about above abst accordance
according accordingly across act actually added adj adopted affected affecting affects after afterwards again against ah all almost alone
along already also although always am among amongst an and announce another any anybody anyhow anymore anyone anything anyway
anyways anywhere apparently approximately are aren arent arise around as aside ask asking at auth available away awfully b back bc
be became because become becomes becoming been before beforehand begin beginning beginnings begins behind being believe below
beside besides between beyond biol bit both brief brie   y but by c ca came can cannot can   t cant cause causes certain certainly co com
come comes contain containing contains cos could couldnt d date day did didn didn   t different do does doesn doesn   t doing don done
don   t dont down downwards due during e each ed edu effect eg eight eighty either else elsewhere end ending enough especially et et-al
etc even ever every everybody everyone everything everywhere ex except f far few ff    fth    rst    ve    x followed following follows for
former formerly forth found four from further furthermore g game gave get gets getting give given gives giving go goes going gone
gonna good got gotten great h had happens hardly has hasn hasn   t have haven haven   t having he hed hence her here hereafter hereby
herein heres hereupon hers herself hes hey hi hid him himself his hither home how howbeit however hundred i id ie if i   ll im immediate
immediately importance important in inc indeed index information instead into invention inward is isn isn   t it itd it   ll its itself i   ve j just
k keep keeps kept keys kg km know known knows l ll largely last lately later latter latterly least less lest let lets like liked likely line little
ll    ll lol look looking looks lot ltd m made mate mainly make makes many may maybe me mean means meantime meanwhile merely mg
might million miss ml more moreover most mostly mr mrs much mug must my myself n na name namely nay nd near nearly necessarily
necessary need needs neither never nevertheless new next nine ninety no nobody non none nonetheless noone nor normally nos not
noted nothing now nowhere o obtain obtained obviously of off often oh ok okay old omitted omg on once one ones only onto or ord
other others otherwise ought our ours ourselves out outside over overall owing own p page pages part particular particularly past people
per perhaps placed please plus poorly possible possibly potentially pp predominantly present previously primarily probably promptly
proud provides put q que quickly quite qv r ran rather rd re readily really recent recently ref refs regarding regardless regards related
relatively research respectively resulted resulting results right rt run s said same saw say saying says sec section see seeing seem seemed
seeming seems seen self selves sent seven several shall she shed she   ll shes should shouldn shouldn   t show showed shown showns
shows signi   cant signi   cantly similar similarly since six slightly so some somebody somehow someone somethan something sometime
sometimes somewhat somewhere soon sorry speci   cally speci   ed specify specifying state states still stop strongly sub substantially
successfully such suf   ciently suggest sup sure t take taken taking tbh tell tends th than thank thanks thanx that that   ll thats that   ve the
their theirs them themselves then thence there thereafter thereby thered therefore therein there   ll thereof therere theres thereto thereupon
there   ve these they theyd they   ll theyre they   ve thing things think this those thou though thoughh thousand throug through throughout
thru thus til time tip to together too took toward towards tried tries truly try trying ts tweet twice two u un under unfortunately unless
unlike unlikely until unto up upon ups ur us use used useful usefully usefulness uses using usually v value various ve    ve very via viz vol
vols vs w wanna want wants was wasn wasn   t way we wed welcome well we   ll went were weren weren   t we   ve what whatever what   ll
whats when whence whenever where whereafter whereas whereby wherein wheres whereupon wherever whether which while whim
whither who whod whoever whole who   ll whom whomever whos whose why widely will willing wish with within without won won   t
words world would wouldn wouldn   t www x y yeah yes yet you youd you   ll your youre yours yourself yourselves you   ve z zero

activities and entities for ubuntu dialogue corpus

ubuntu activities:

accept, activate, add, ask, appoint, attach, backup, boot, check, choose, clean, click, comment, compare, compile, compress, change,
af   rm, connect, continue, administrate, copies, break, create, cut, debug, decipher, decompress, de   ne, describe, debind, deattach,
deactivate, download, adapt, eject, email, conceal, consider, execute, close, expand, expect, export, discover, correct, fold, freeze, get,
deliver, go, grab, hash, import, include, install, interrupt, load, block, log, log-in, log-out, demote, build, clock, bind, more, mount, move,
navigate, open, arrange, partition, paste, patch, plan, plug, post, practice, produce, pull, purge, push, put, queries, quote, look, reattach,
reboot, receive, reject, release, remake, delete, name, replace, request, reset, resize, restart, retry, return, revert, reroute, scroll, send, set,
display, shutdown, size, sleep, sort, split, come-up, store, signup, get-ahold-of, say, test, transfer, try, uncomment, de-expand, uninstall,
unmount, unplug, unset, sign-out, update, upgrade, upload, use, delay, enter, support, prevent, loose, point, contain, access, share, buy,
sell, help, work, mute, restrict, play, call, thank, burn, advice, force, repeat, stream, respond, browse, scan, restore, design, refresh,
bundle, implement, programming, compute, touch, overheat, cause, affect, swap, format, rescue, zoomed, detect, dump, simulate,
checkout, unblock, document, troubleshoot, convert, allocate, minimize, maximize, redirect, maintain, print, spam, throw, sync, contact,
destroy

12part of these were extracted from https://github.com/defacto133/twitter-wordcloud-bot/

blob/master/assets/stopwords-en.txt.

16

ubuntu entities (excerpt):

ubuntu_7.04, dmraid, vnc4server, tasksel, aegis, mirage, system-con   g-audit, uif2iso, aumix, unrar, dell, hibernate, ucoded,    nger, zone-
minder, uid18, macaddress, ia32-libs, synergy, aircrack-ng, pulseaudio, gnome, k , bittorrent, systemsettings, cups,    nger, xchm, pan,
uwidget, vnc-java, linux-source, ucommand.com, epiphany, avanade, onboard, uextended, substance, pmount, lilypond, proftpd, unii,
jockey-common, aha, units, xrdp, mp3check, cruft, uemulator, ulivecd, amsn, ubuntu_5.10, acpidump, uadd-on, gpac, ifenslave, pidgin,
soundconverter, kdelibs-bin, esmtp, vim, travel, smartdimmer, uactionscript, scrotwm, fbdesk, tulip, beep, nikto, wine, linux-image,
azureus, vim, make   le, uuid, whiptail, alex, junior-arcade, libssl-dev, update-inetd, uextended, uaiglx, sudo, dump, lockout, overlay-
scrollbar, xubuntu, mdk, mdm, mdf2iso, linux-libc-dev, sms, lm-sensors, dsl, lxde, dsh, smc, sdf, install-info, xsensors, gutenprint,
sensors, ubuntu_13.04, atd, ata, fatrat, fglrx, equinix, atp, atx, libjpeg-dbg, umingw, update-inetd,    refox, devede, cd-r, tango, mixxx,
uemulator, compiz, libpulse-dev, synaptic, ecryptfs, crawl, ugtk+, tree, perl, tree, ubuntu-docs, libsane, gnomeradio, u   lemaker, dyndns,
libfreetype6, daemon, xsensors, vncviewer, vga, indicator-applet, nvidia-173, rsync, members, qemu, mount, rsync, macbook, gsfonts,
synaptic,    nger, john, cam, lpr, lpr, xsensors, lpr, lpr, screen, inotify, signatures, units, ushareware, ufraw, bonnie, nec, fstab, nano,
bless, bibletime, irssi, ujump, foremost, nzbget, ssid, onboard, synaptic, branding, hostname, radio, hotwire, xebia, netid18, xchat, irq,
lazarus, pilot, ucopyleft, java-common, vm, ifplugd, ncmpcpp, irc, uclass, gnome, sram, binfmt-support, vuze, java-common, sauer-
braten, adapter, login

ubuntu commands:

alias, apt-get, aptitude, aspell, awk, basename, bc, bg, break, builtin, bzip2, cal, case, cat, cd, cfdisk, chgrp, chmod, chown, chroot,
chkcon   g, cksum, cmp, comm, command, continue, cp, cron, crontab, csplit, curl, cut, date, dc, dd, ddrescue, declare, df, diff, diff3,
dig, dir, dircolors, dirname, dirs, dmesg, du, echo, egrep, eject, enable, env, eval, exec, exit, expect, expand, export, expr, false, fdformat,
fdisk, fg, fgrep,    le,    nd, fmt, fold, for, fsck, ftp, function, fuser, gawk, getopts, grep, groupadd, groupdel, groupmod, groups, gzip,
hash, head, history, hostname, htop, iconv, id, if, ifcon   g, ifdown, ifup, import, install, ip, jobs, join, kill, killall, less, let, link, ln, local,
locate, logname, logout, look, lpc, lpr, lprm, ls, lsof, man, mkdir, mk   fo, mknod, more, most, mount, mtools, mtr, mv, mmv, nc, nl,
nohup, notify-send, nslookup, open, op, passwd, paste, ping, pkill, popd, pr, printf, ps, pushd, pv, pwd, quota, quotacheck, quotactl, ram,
rar, rcp, read, readonly, rename, return, rev, rm, rmdir, rsync, screen, scp, sdiff, sed, select, seq, set, shift, shopt, shutdown, sleep, slocate,
sort, source, split, ssh, stat, strace, su, sudo, sum, suspend, sync, tail, tar, tee, test, time, timeout, times, touch, top, tput, traceroute, tr,
true, tsort, tty, type, ulimit, umask, unalias, uname, unexpand, uniq, units, unrar, unset, unshar, until, useradd, userdel, usermod, users,
uuencode, uudecode, vi, vmstat, wait, watch, wc, whereis, which, while, who, whoami, write, xargs, xdg-open, xz, yes, zip, admin,
purge

17

10 model details

training

all models were trained with a learning rate of 0.0002 or 0.0001, batches of size either 40 or size 80 and
gradients are clipped at 1.0. we truncate the id26 to batches with 80 tokens we validate on the
entire validation set every 5000 training batches. we choose almost identical hyperparameters for the ubuntu
and twitter models, since the models appear to perform similarly w.r.t. different hyperparameters and since the
statistics of the two datasets are comparable. we use the 20k most frequent words on twitter and ubuntu as the
natural language vocabulary for all the models, and assign all words outside the vocabulary to a special unknown
token symbol. for mrid56, we use a coarse token vocabulary consisting of the 10k most frequent tokens in the
coarse token sequences.

generation

we compute the cost of each id125 (candidate response) as the log-likelihood of the tokens in the beam
divided by the number of tokens it contains. the lsmt model performs better when the id125 is not
allowed to generate the unknown token symbol, however even then it still performs worse than the hred model
across all metrics except for the command accuracy.

baselines

based on preliminary experiments, we found that a slightly different parametrization of the hred baseline
model worked better on twitter. the encoder id56 has a bidirectional gru id56 encoder, with 1000 hidden
units for the forward and backward id56s each, and a context id56 and a decoder id56 with 1000 hidden units
each. furthermore, the decoder id56 computes a 1000 dimensional real-valued vector for each hidden time step,
which is multiplied with the output context id56. the output is feed through a one-layer feed-forward neural
network with hyperbolic tangent activation function, which the decoder id56 then conditions on.

11 human evaluation

all human evaluators either study or work in an english speaking environment, and have indicated that they
have some experience using a linux operating system. to ensure a high quality of the ground truth responses,
human evaluators were only asked to evaluate responses, where the ground truth contained at least one technical
entity. before starting evaluators, were shown one short annotated example with a brief explanation of how to
give annotations. in particular, the evaluators were instructed to use the following reference in figure 2.

figure 2: fluency and relevancy reference table presented to human evaluators.

the 5 evaluators gave 1069 ratings in total. table 7 shows the scores by category.

table 7: ubuntu human    uency and relevancy scores by rating category

``````````

rating level

model
hred
hred + act.-ent.
mrid56 noun
mrid56 act.-ent

fluency

relevancy

0

3
3
1
0

1

11
17
2
2

2

21
19
8
6

3

50
37
52
52

4

49
57
71
74

0

68
69
51
27

1

22
39
45
53

2

19
18
24
39

3

19
6
10
14

4

4
2
4
1

18

12 model examples

model response examples are given in this section. all
the model responses can be downloaded
at www.iulianserban.com/files/twitterdialoguecorpus.zip and www.iulianserban.com/files/
ubuntudialoguecorpus.zip.

table 8: ubuntu model examples. the     token indicates a change of turn.

context
seriously... 50% cpu from xorg running just two apps(terminal
and system monitor ) i   ve been seeing this ever since i upgraded to
12.10 anyone else seeing this     i would do such kind of tests with
    top     or     something enjoy the difference...     that is top saying
50% cpu from xorg     ok.
in that case i would try a window
manager without composite for a try. 50% is too much.

my linux seems to be stuck at rev 18... other systems in the house
are as high as rev 34... any suggestions ?     what    s rev ?     the
linux info says kernel linux says 3.5. something generic

i need some serious help with fullscreen video playback. i al-
ready asked in #lubuntu, but i might have a better chance here
: most video    les either let the player hang or audio/video goes
badly out of sync right away.
i know that it does work with the
same    les and it even did so like two hours ago, but after some
browsing and man page reading, the problem comes up again.
i
have no idea where to look anymore nor what i did to     reproduce
    the     even with vlc
i   m in the process of building a new desktop and i was wondering
if there was any way to, using kvm, start or restart a windows ses-
sion when my girlfriend logs in. i have plenty of memory and my
i5 3570 supports all the necessary virtualisation technologies.    
you can install virtualbox, and i   m sure there    s a command line
method to run vb with a speci   ed installed vm operating system,
and you can make that command a startup command ending with
" & "
hello all, how can i setup two default gateways graphically in net-
work manager ?     you can   t have 2 default gateway on one de-
vice...?     well.. i can     hows that gonna work ?

they are mounted via fstab     sure they are mounted ?     i   ve
been using them all day :[ p :p *

response
mrid56 act. -ent.: are you using xgl ?
mrid56 noun: what window manager are you using ?
hred + act.-ent.: i don   t know how to help you with that,
sorry.
hred: i have no idea how to do that, i just want to know
how to do it
lstm: something ?
mrid56 act. -ent.: how do i compile the kernel ?
mrid56 noun: the linux kernel is 2.6. something
hred + act.-ent.: how do i    nd out what version of ubuntu
i have ?
hred: that    s not what you want to do
lstm: ? what do you mean ?
mrid56 act. -ent.: works    ne in vlc
mrid56 noun: do you use vlc ?
hred + act.-ent.: what video card do you have ?
hred: yeah, i know, but i don   t know how to do that, but i
don   t know how to do that
lstm: ? what do you mean ?

mrid56 act. -ent.: have you installed virtualbox ?
mrid56 noun: are you trying to install ubuntu ?
hred + act.-ent.: i don   t know how to do that, sorry. i
don   t know how to do that.
hred: thanks, i   ll give it a shot.
lstm: thanks, i   ll try that.

mrid56 act. -ent.: what are you looking for ?
mrid56 noun: do you have a default gateway ?
hred + act.-ent.: what are you trying to do ?
hred: i want to set up a static ip, but i dont know how to set
it up
lstm: ?
mrid56 act. -ent.: i see
mrid56 noun: something :/ something ls -ld something
hred + act.-ent.: i can mount it, but when i try to mount
it, it says " mount : special device /dev/sda1 does not exist
hred: what do you mean by mount ?
lstm: ? what are you talking about ?

19

table 9: ubuntu model examples. the     token indicates a change of turn.

context
can you have gnome and kde installed and use one or the other ?
cough erin brainwaves

ugh so my mount points need to be in /media rather than /mnt to
make them appear in the nautilus side-pane :     you could symlink
/mnt to /media afaik

how can i install ubuntu raring ?     ask in #ubuntu+1     they are
afk all of them     so ? doesn   t mean its ontoppic here

how shall i reboot bios 100 time through script     i   m really
curious why anyone would ever want to do that...     for bios
stress testing

what is the current version of wine in ubuntu ? i can see wine 1.5.
however, when i try to install it via synaptic it wants to remove
vlc ?     did you install vlc as a windows version ?     windows
version on ubuntu ?

hey guys, i   m trying to install libreof   ce from the repositories,
but it    s throwing a wobbly. can anyone help ? looks like de-
pendency problems, but when i attempt to resolve them it throws
a wobbly at me. <url >    try     apt-get install -f     yeah, it looks
like you don   t have java install you need to try to run     apt-get
install -f   , just that string try     apt-get purge libreof   ce     it looks
like you had it installed previously     update : just realized i re-
moved it earlier because it wasn   t working. let me re-install. when
i run     apt-get purge libreof   ce     it just complains of unmet depen-
dencies/     but you were able to reinstall    ne ?     no, it throws
the errors seen in the pastebin at me     jwrigley, check to see
if /var/lib/libreof   ce/share/prereg exists     doesn   t appear to exist
when i cd to it in terminal     use apt-get via the command line
tab completion helps a lot run     apt-get purge libreof   ce         oh,
yeah i see now. sorry. now it is telling me to    x dependencies with
apt-get install -f     awesome, so are you all set now ?     well
libreof   ce is still not working, so no.
can anyone help get wireless working ?     whats the trouble ?    
it doesnt show any wireless networks

who can answer : " how can i get mac address of the server i just
connected via pppoe ?"     try tcpdump ? tcpdump -e

response
mrid56 act.-ent.: what are you looking for ?
mrid56 noun: gconf-editor
hred + act.-ent.: what do you want me to do ?
hred: yeah, i just did a fresh install of ubuntu, and it worked
   ne.
lstm: ?
mrid56 act. -ent.: thank you :)
mrid56 noun: you need to mount it in /mnt or /media
hred + act.-ent.: if you don   t know what you are trying to
do, then you should be able to mount it manually, but i don   t
know how
hred: i don   t know how to do that, i just want to be able to
mount them in /media
lstm: what ?
mrid56 act. -ent.:can you just tell me how ?
mrid56 noun: join #ubuntu-offtopic
hred + act.-ent.: what are you trying to do ?
hred: i know, but i dont know how to install it
lstm: ?
mrid56 act. -ent.:do you know how to check bios ?
mrid56 noun: how do i disable the bios ?

hred + act.-ent.:
i don   t want to reboot, i just want
to be able to boot from a usb stick, but i don   t want to have
to do it every time i want to do it
hred: that    s not what you want to do, but you might want
to ask in ##hardware
lstm: ?
mrid56 act. -ent.:what version of ff do you have installed
?
mrid56 noun: what version of ubuntu are you using ?
hred + act.-ent.: how do i install the latest version of wine
? i have the latest version, but i dont know how to install it
hred: yes
lstm: yes
mrid56 act. -ent.: i see.
mrid56 noun: how do i get rid of libreof   ce ?
hred + act.-ent.:
lock
/var/lib/dpkg/lock - open ( 11 resource temporarily unavail-
able ) e : unable to locate package something "
hred: i don   t know what that is, but i don   t know how to
   x it
lstm: is installed

it says " e : could not get

mrid56 act. -ent.: it    s not working.
mrid56 noun: how do i get to the network manager ?
hred + act.-ent.: what is the name of your wireless card ?
hred: are you using ndiswrapper ?
lstm: what do you mean ?
mrid56 act. -ent.: what are you looking for ?
mrid56 noun: what is your mac address ?
hred + act.-ent.: what are you trying to do ?
hred: thanks, i   ll try that
lstm: command not found

20

table 10: twitter model examples. the     token indicates a change of turn.

context

response

kids are so well informed! <url>

russell crowe. amazing actor and good person.
perfect. i am sure.

thank you <name> for being so welcoming.
who knew downtown little rock was so beauti-
ful? loved it!

i   ve got a core i5 macbook air right here if any-
one has any questions. or you can ask in the
comments thread here <url>     how well does
it run photoshop illustrator?     pretty much
just as well as the current macbook pros do.
maybe bigger than jb. haha jk.     i   m fan of
her. she   s so awesome.     hahahahahahahaha
p just amazed.     thinking of dudz?

<number> countdown reminds me of hold on
wilsonphillips

here is a sadbuttrue fact - bad news travels fast
while good news seems to just linger. people
don    t like spreading good news

have you dmed justin bieber yet for me? <smi-
ley>     yeah.

hanging out in the library for the past cou-
ple hours makes me feel like i   ll do great on
this test!     wow, what a nerd lol jk haha p
what!? you changed your bio     do you like
my bio now? i feel bad for changing it but i
like change. p
christian bale must be prepping for a gordon   s
   sh sticks commercial. oscar

some railscamp nz action. <url>     how goes
it?

second netball game for the week. let   s do it!
isupportwomeninsport

mrid56: i love the kids!!!
vhred: wow, you   re so cute!
hred: i   m so happy for you!!!
lstm: that   s the spirit!!!
mrid56: johnny depp is the best!!!
vhred: something!!!
hred: i   m glad you enjoyed it!
lstm: he is a great actor.
mrid56: i love that song!!!
vhred: happy birthday to you!
hred: i love you!!!
lstm: i   m glad you liked it!
mrid56: do you have a macbook pro???!!!
vhred: ok, so what do you want to do?
hred: i   m sure you   ll    nd it soon. i   m sure it   ll be worth it.
lstm: thanks for the heads up. i   ll check it out.

mrid56: yes. i love justin bieber!!!
vhred: i love her.
hred: no, i don    t think so.
lstm: i think she   s just a little bit of a something.
mrid56: it   s a spring break!!! - - - - - - - - - - - - - - - - -
vhred: i   m so excited for you!!!
hred: something!!!
lstm: something!!!
mrid56: i agree with the news!!!
vhred: thank you for the support!
hred: what   s the news?
lstm: i agree, but i don    t think it   s a bad thing to do. i don    t think it   s a bad
thing to do.
mrid56: do you have bieber fever? <smiley>
vhred: dm me your email and i   ll send it to you
hred: thank you
lstm: thank you <smiley>
mrid56: yeah, it   s in the library!!!
vhred: what do you mean?
hred: haha i   m glad i   m not the only one!
lstm: yeah i like it but i don    t know what to do with it lol

mrid56: i   m not a christian bale. i don    t know what he   s talking about. but i don
   t know what he   s talking about. lol. something!!!
vhred: did you see the new harry potter movie?
hred: i don    t think it   s a good movie, but it   s a good movie.
lstm: i   m going to have to watch that.
mrid56: it   s great to see you in nz!!!
vhred: not too bad, just a little tired. how are you?
hred: it   s been a while since i   ve been here.
hred: it   s going to be a long day.
mrid56: what   s netball???!!!
vhred: i   m going to have to go to the gym tomorrow.
hred: i   m going to be in the <number>   s.
lstm: what   s the score?

21

