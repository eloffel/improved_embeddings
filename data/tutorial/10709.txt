neural responding machine for short-text conversation

lifeng shang zhengdong lu hang li

noah   s ark lab

huawei technologies co. ltd.

sha tin, hong kong

{shang.lifeng,lu.zhengdong,hangli.hl}@huawei.com

5
1
0
2

 
r
p
a
7
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
4
6
3
2
0

.

3
0
5
1
:
v
i
x
r
a

abstract

we propose neural responding machine (nrm), a neural network-based response generator for
short-text conversation. nrm takes the general encoder-decoder framework: it formalizes the
generation of response as a decoding process based on the latent representation of the input text,
while both encoding and decoding are realized with recurrent neural networks (id56). the nrm
is trained with a large amount of one-round conversation data collected from a microblogging
service. empirical study shows that nrm can generate grammatically correct and content-wise
appropriate responses to over 75% of the input text, outperforming state-of-the-arts in the same
setting, including retrieval-based and smt-based models.

1 introduction

language conversation is one of the most challenging arti   cial

intelligence problems,
natural
which involves language understanding,
reasoning, and the utilization of common sense knowl-
edge. previous works in this direction mainly focus on either rule-based or learning-based meth-
ods (williams and young, 2007; schatzmann et al., 2006; misu et al., 2012; litman et al., 2000). these
types of methods often rely on manual effort in designing rules or automatic training of model with a par-
ticular learning algorithm and a small amount of data, which makes it dif   cult to develop an extensible
open domain conversation system.

recently due to the explosive growth of microblogging services such as twitter1 and weibo2, the
amount of conversation data available on the web has tremendously increased. this makes a data-driven
approach to attack the conversation problem (ji et al., 2014; ritter et al., 2011) possible. instead of mul-
tiple rounds of conversation, the task at hand, referred to as short-text conversation (stc), only consid-
ers one round of conversation, in which each round is formed by two short texts, with the former being
an input (referred to as post) from a user and the latter a response given by the computer. the research
on stc may shed light on understanding the complicated mechanism of natural language conversation.
previous methods for stc fall into two categories, 1) the retrieval-based method (ji et al., 2014),
and 2) the id151 (smt) based method (ritter et al., 2011). the basic idea of
retrieval-based method is to pick a suitable response by ranking the candidate responses with a linear or
non-linear combination of various matching features (e.g. number of shared words). the main drawbacks
of the retrieval-based method are the following

    the responses are pre-existed and hard to be customized for the particular text or requirement from

the task, e.g., style or attitude.

    the use of matching features alone is usually not suf   cient for distinguishing positive responses from
negative ones, even after time consuming feature engineering. (e.g., a penalty due to mismatched
named entities is dif   cult to be incorporated into the model)

the smt-based method, on the other hand, is generative. basically it treats the response generation as
a translation problem, in which the model is trained on a parallel corpus of post-response pairs. despite

1https://twitter.com/.
2http://www.weibo.com/.

its generative nature, the method is intrinsically unsuitable for response generation, because the responses
are not semantically equivalent to the posts as in translation. actually one post can receive responses
with completely different content, as manifested through the example in the following    gure:

post having my    sh sandwich right now
usera for god   s sake, it is 11 in the morning
userb enhhhh... sounds yummy
userc which restaurant exactly?

1.1 overview
in this paper, we take a probabilistic model to address the response generation problem, and propose
employing a neural encoder-decoder for this task, named neural responding machine (nrm). the
neural encoder-decoder model, as illustrated in figure 1,    rst summarizes the post as a vector repre-
sentation, then feeds this representation to decoder to generate responses. we further generalize this
scheme to allow the post representation dynamically change during the generation process, following the
idea in (bahdanau et al., 2014) originally proposed for neural-network-based machine translation with
automatic alignment.

for god's sake, it is 11 in the morning 

enhhhh... sounds yummy 

which restaurant exactly? 

decoder 

vector 

encoder 

having my fish sandwich right now 

figure 1: the diagram of encoder-decoder framework for automatic response generation.

nrm essentially estimates the likelihood of a response given a post. clearly the estimated id203
should be complex enough to represent all the suitable responses. similar framework has been used
for machine translation with a remarkable success (kalchbrenner and blunsom, 2013; auli et al., 2013;
sutskever et al., 2014; bahdanau et al., 2014). note that in machine translation, the task is to estimate
the id203 of a target language sentence conditioned on the source language sentence with the same
meaning, which is much easier than the task of stc which we are considering here. in this paper, we
demonstrate that nrm, when equipped with a reasonable amount of data, can yield a satisfying estimator
of responses (hence response generator) for stc, despite the dif   culty of the task.

our main contributions are two-folds: 1) we propose to use an encoder-decoder-based neural net-
work to generate a response in stc; 2) we have empirically veri   ed that the proposed method, when
trained with a reasonable amount of data, can yield performance better than traditional retrieval-based
and translation-based methods.

1.2 roadmap
in the remainder of this paper, we start with introducing the dataset for stc in section 2. then we
elaborate on the model of nrm in section 3, followed by the details on training in section 4. after that,
we report the experimental results in section 5. in section 6 we conclude the paper.

2 the dataset for stc

our models are trained on a corpus of roughly 4.4 million pairs of conversations from weibo 3.

3the dataset and its english translation (by machine translation system) will be released soon.

2.1 conversations on sina weibo
weibo is a popular twitter-like microblogging service in china, on which a user can post short messages
(referred to as post in the reminder of this paper) visible to the public or a group of users following
her/him. other users make comment on a published post, which will be referred to as response. just like
twitter, weibo also has the length limit of 140 chinese characters on both posts and responses, making
the post-response pair an ideal surrogate for short-text conversation.

training

test data

labeled dataset

(retrieval-based)

fine tuning
(smt-based)

#posts
#responses
#pairs
#test posts
#posts
#responses
#labeled pairs
#posts
#responses
#pairs

219,905
4,308,211
4,435,959
110
225
6,017
6,017
2,925
3,000
3,000

table 1: some statistics of the dataset. labeled dataset and fine tuning are used by retrieval-based
method for learning to rank and smt-based method for    ne tuning, respectively.

2.2 dataset description
to construct this million scale dataset, we    rst crawl hundreds of millions of post-response pairs, and
then clean the raw data in a similar way as suggested in (wang et al., 2013), including 1) removing trivial
responses like    wow   , 2)    ltering out potential advertisements, and 3) removing the responses after    rst
30 ones for topic consistency. table 1 shows some statistics of the dataset used in this work. it can
be seen that each post have 20 different responses on average. in addition to the semantic gap between
post and its responses, this is another key difference to a general parallel data set used for traditional
translation.

3 neural responding machines for stc

the basic idea of nrm is to build a hidden representation of a post, and then generate the response
based on it, as shown in figure 2. in the particular illustration, the encoder converts the input sequence
x = (x1,          , xt ) into a set of high-dimensional hidden representations h = (h1,          , ht ), which, along
with the attention signal at time t (denoted as   t), are fed to the context-generator to build the context
input to decoder at time t (denoted as ct). then ct is linearly transformed by a matrix l (as part of the
decoder) into a stimulus of generating id56 to produce the t-th word of response (denoted as yt).

in neural translation system, l converts the representation in source language to that of target lan-
guage. in nrm, l plays a more dif   cult role: it needs to transform the representation of post (or some
part of it) to the rich representation of many plausible responses. it is a bit surprising that this can be
achieved to a reasonable level with a linear transformation in the    space of representation   , as validated
in section 5.3, where we show that one post can actually invoke many different responses from nrm.

the role of attention signal is to determine which part of the hidden representation h should be em-
phasized during the generation process. it should be noted that   t could be    xed over time or changes
dynamically during the generation of response sequence y. in the dynamic settings,   t can be function
of historically generated subsequence (y1,          , yt   1), input sequence x or their latent representations,
more details will be shown later in section 3.2.

we use recurrent neural network (id56) for both encoder and decoder, for its natural ability to
summarize and generate word sequence of arbitrary lengths (mikolov et al., 2010; sutskever et al., 2014;
cho et al., 2014).

decoder 

attention signal 

context generator 

encoder 

figure 2: the general framework and data   ow of the encoder-decoder-based nrm.

(cid:856)(cid:857) 

(cid:856)(cid:857) 

(cid:856)(cid:857) 

figure 3: the graphical model of id56 decoder. the dashed lines denote the variables related to the
function g(  ), and the solid lines denote the variables related to the function f (  ).

3.1 the computation in decoder
figure 3 gives the graphical model of the decoder, which is essentially a standard id56 language model
except conditioned on the context input c. the generation id203 of the t-th word is calculated by

p(yt|yt   1,          , y1, x) = g(yt   1, st, ct),

(1)

where yt is a one-hot word representation, g(  ) is a softmax activation function, and st is the hidden state
of decoder at time t calculated by

st = f (yt   1, st   1, ct),

(2)

and f (  ) is a non-linear activation function and the transformation l is often assigned as pa-
rameters of f (  ). here f (  ) can be a logistic function,
the sophisticated long short-term mem-
ory (lstm) unit (hochreiter and schmidhuber, 1997), or the recently proposed gated recurrent
unit (gru) (chung et al., 2014; cho et al., 2014). compared to    ungated    logistic function, lstm and
gru are specially designed for its long term memory: it can store information over extended time steps
without too much decay. we use gru in this work, since it performs comparably to lstm on squence
modeling (chung et al., 2014), but has less parameters and easier to train.

3.2 the computation in encoder
we consider three types of encoding schemes, namely 1) the global scheme, 2) the local scheme, and the
hybrid scheme which combines 1) and 2).

global scheme: figure 4 shows the graphical model of the id56-encoder and related context generator
for a global encoding scheme. the hidden state at time t is calculated by ht = f (xt, ht   1) (i.e. still

gru unit), and with a trivial context generation operation, we essentially use the    nal hidden state ht
as the global representation of the sentence. the same strategy has been taken in (cho et al., 2014) and
(sutskever et al., 2014) for building the intermediate representation for machine translation. this scheme
however has its drawbacks: a vectorial summarization of the entire post is often hard to obtain and may
lose important details for response generation, especially when the dimension of the hidden state is not
big enough4. in the reminder of this paper, a nrm with this global encoding scheme is referred to as
nrm-glo.

context generator 

(cid:856)(cid:857) 

figure 4: the graphical model of the encoder in nrm-glo, where the last hidden state is used as the
context vector ct = ht .

local scheme: recently, bahdanau et al. (2014) and graves (2013) introduced an attention mecha-
nism that allows the decoder to dynamically select and linearly combine different parts of the input
sequence ct = pt
j=1   tj hj, where weighting factors   tj determine which part should be selected to
generate the new word yt, which in turn is a function of hidden states   tj = q(hj , st   1), as pictorially
shown in figure 5. basically, the attention mechanism   tj models the alignment between the inputs
around position j and the output at position t, so it can be viewed as a local matching model. this local
scheme is devised in (bahdanau et al., 2014) for automatic alignment between the source sentence and
the partial target sentence in machine translation. this scheme enjoys the advantage of adaptively focus-
ing on some important words of the input text according to the generated words of response. a nrm
with this local encoding scheme is referred to as nrm-loc.

attention signal 

 
 

 

context generator 

 

 

 

 

(cid:856)(cid:857) 

figure 5: the graphical model of the encoder in nrm-loc, where the weighted sum of hidden sates is
used as the context vector ct = pt

j=1   tj hj.

4sutskever et al. (2014) has to use 4

,

000 dimension for satisfying performance on machine translation, while

(cho et al., 2014) with a smaller dimension perform poorly on translating an entire sentence.

3.3 extensions: local and global model
in the task of stc, nrm-glo has the summarization of the entire post, while nrm-loc can adaptively
select the important words in post for various suitable responses. since post-response pairs in stc are
not strictly parallel and a word in different context can have different meanings, we conjecture that the
global representation in nrm-glo may provide useful context for extracting the local context, therefore
complementary to the scheme in nrm-loc. it is therefore a natural extension to combine the two models
by concatenating their encoded hidden states to form an extended hidden representation for each time
stamp, as illustrated in figure 6. we can see the summarization hg
t is incorporated into ct and   tj to
provide a global context for local matching. with this hybrid method, we hope both the local and global
information can be introduced into the generation of response. the model with this context generation
mechanism is denoted as nrm-hyb.

it should be noticed that the context generator in nrm-hyb will evoke different encoding mechanisms
in the global encoder and the local encoder, although they will be combined later in forming a uni   ed
representation. more speci   cally, the last hidden state of nrm-glo plays a role different from that of
the last state of nrm-loc, since it has the responsibility to encode the entire input sentence. this role
of nrm-glo, however, tends to be not adequately emphasized in training the hybrid encoder when the
parameters of the two encoding id56s are learned jointly from scratch. for this we use the following
trick: we    rst initialize nrm-hyb with the parameters of nrm-loc and nrm-glo trained separately,
then    ne tune the parameters in encoder along with training the parameters of decoder.

(cid:856)(cid:857) 

global  encoder 

(cid:856)(cid:857) 

local  encoder 

c
o
n
t
e
x
t
 
g
e
n
e
r
a
t
o
r
 

attention signal 

figure 6: the graphical model for the encoder in nrm-hyb, while context generator function is ct =
pt

t ] denotes the concatenation of vectors hl

t ], here [hl

j=1   tj[hl

j and hg

j; hg

j; hg

t

to learn the parameters of the model, we maximize the likelihood of observing the original response
conditioned on the post in the training set. for a new post, nrms generate their responses by using a
left-to-right id125 with beam size = 10.

4 experiments

we evaluate three different settings of nrm described in section 3, namely nrm-glo, nrm-loc, and
nrm-hyb, and compare them to retrieval-based and smt-based methods.

implementation details

4.1
we use stanford chinese word segmenter 5 to split the posts and responses into sequences of words.
although both posts and responses are written in the same language, the distributions on words for the
two are different: the number of unique words in post text is 125,237, and that of response text is 679,958.
we therefore construct two separate vocabularies for posts and responses by using 40,000 most frequent
words on each side, covering 97.8% usage of words for post and 96.2% for response respectively. all the

5http://nlp.stanford.edu/software/segmenter.shtml

remaining words are replaced by a special token    unk   . the dimensions of the hidden states of encoder
and decoder are both 1,000, and the dimensions of the word-embedding for post and response are both
620. model parameters are initialized by randomly sampling from a uniform distribution between -0.1
and 0.1. all our models were trained on a nvidia tesla k40 gpu using stochastic id119
algorithm with mini-batch. the training stage of each model took about two weeks.

4.2 competitor models
retrieval-based: with retrieval-based models, for any given post p   , the response r    is retrieved from
a big post-response pairs (p, r) repository. such models rely on three key components: a big repository,
sets of feature functions   i(p   , (p, r)), and a machine learning model to combine these features. in this
work, the whole 4.4 million weibo pairs are used as the repository, 14 features, ranging from simple
cosine similarity to some deep matching models (ji et al., 2014) are used to determine the suitability of
a post to a given post p    through the following linear model

score(p   , (p, r)) = x

  i  i(p   , (p, r)).

i

(3)

following the ranking strategy in (ji et al., 2014), we pick 225 posts and about 30 retrieved responses for
each of them given by a baseline retriever6 from the 4.4m repository, and manually label them to obtain
labeled 6,017 post-response pairs. we use ranking id166 model (joachims, 2006) for the parameters   i
based on the labeled dataset. in comparison to nrm, only the top one response is considered in the
evaluation process.

smt-based:
in smt-based models, the post-response pairs are directly used as parallel data for train-
ing a translation model. we use the most widely used open-source phrase-based translation model-
moses (koehn et al., 2007). another parallel data consisting of 3000 post-response pairs is used to tune
the system. in (ritter et al., 2011), the authors used a modi   ed smt model to obtain the    response   
of twitter    stimulus   . the main modi   cation is in replacing the standard giza++ word alignment
model (och and ney, 2003) with a new phrase-pair selection method, in which all the possible phrase-
pairs in the training data are considered and their associated probabilities are estimated by the fisher   s
exact test, which yields performance slightly better than default setting8. compared to retrieval-based
methods, the generated responses by smt-based methods often have    uency or even grammatical prob-
lems. in this work, we choose the moses with default settings as our smt model.

5 results and analysis

automatic evaluation of response generation is still an open problem. the widely accepted evaluation
methods in translation (e.g. blue score (papineni et al., 2002)) do not apply, since the range of the
suitable responses is so large that it is practically impossible to give reference with adequate coverage. it
is also not reasonable to evaluate with perplexity, a generally used measurement in statistical language
modeling, because the naturalness of response and the relatedness to post can not be well evaluated.
we therefore resort to human judgement, similar to that taken in (ritter et al., 2011) but with important
difference.

5.1 evaluation methods
we adopt human annotation to compare the performance of different models. five labelers with at least
three-year experience of sina weibo are invited to do human evaluation. responses obtained from the
   ve evaluated models are pooled and randomly permuted for each labeler. the labelers are instructed
to imagine that they were the authors of the original posts and judge whether a response (generated or
retrieved) is appropriate and natural to a input post. three levels are assigned to a response with scores
from 0 to 2:

6we use the default similarity function of lucene 7
8reported results showed that the new model outperformed the baseline smt model 57.7% of the time.

post 

(cid:5957)(cid:3933)(cid:2143)(cid:12215)(cid:2416)(cid:18434)(cid:13879)(cid:7269)(cid:1953)(cid:1120)(cid:1264)...(cid:3936)(cid:3950)(cid:5462)(cid:1212) 
there are always 8 players at the italian restricted area. unbelievable! 

related criterion 

labels

response1

(cid:6215)(cid:7269)(cid:5957)(cid:3933)(cid:2143)(cid:19541)(cid:11450)(cid:10809)(cid:17965)(cid:952)(cid:12671)(cid:5563)(cid:8714)(cid:17297)(cid:5430)(cid:4097)(cid:574) 
i am a big fan of the italy team, waiting for the football game to start 

logic consistency  unsuitable

response2 

(cid:5957)(cid:3933)(cid:2143)(cid:11450)(cid:20245)(cid:10399)(cid:3936)(cid:13764)(cid:2729)(cid:1212) 
italian food is absolutely delicious. 

response3

(cid:3936)(cid:3950)(cid:5462)(cid:1212)(cid:2653)! 
unbelievable! 

response4 

(cid:2814)(cid:2814)(cid:2814)(cid:1283)(cid:10092)(cid:7269)0(cid:966)0 (cid:574)(cid:17934)(cid:8919)(cid:11585)(cid:2150)(cid:17937)(cid:10809)(cid:574) 
ha!ha!ha! it is still 0:0, no goal so far. 

response5

(cid:17935)(cid:8601)(cid:7269)(cid:5957)(cid:3933)(cid:2143)(cid:5445)(cid:19560)(cid:4542)(cid:17385)(cid:10809)(cid:574) 
this is exactly the italian defending style football game. 

semantic relevance  unsuitable

generality 

neutral 

scenario dependence  neutral

(cid:178)(cid:178)

suitable

figure 7: an example post and its    ve candidate responses with human annotation. the content of the
post implies that the football match is already started, while the author of response1 is still waiting for
the match to start. response2 talks about the food of italy. response3 is a widely used response, but it
is suitable to this post. response4 states that the current score is still 0:0, it is a suitable response only in
this speci   c scenario.

models
nrm-glo
nrm-loc
nrm-hyb
rtr.-based
smt-based

mean score suitable (+2) neutral (+1) unsuitable (0) agreement

0.969
1.065
1.156
0.984
0.313

34.0%
36.0%
39.3%
29.8%
5.6 %

28.9%
34.5%
37.1%
38.7%
20.0%

37.1%
29.5%
23.6%
31.5%
74.4%

0.397
0.247
0.309
0.346
0.448

table 2: the results of evaluated methods. mean score is the average value of annotated scores over all
annotations. (rtr.-based means the retrieval-based method)

id 

post 

nrm-glo 

nrm-loc 

nrm-hyb 

rtr.-based 

(cid:8709)(cid:5290)(cid:1913)(cid:7196)(cid:18227)(cid:7579)(cid:20750)(cid:10013)(cid:574)

(cid:3920)(cid:1351)(cid:5797)(cid:952)(cid:3796)(cid:5488)(cid:1923)(cid:11233)(cid:2257)(cid:941)

high fever attacks me 
(cid:72)(cid:89)(cid:72)(cid:85)(cid:92)(cid:3)(cid:49)(cid:72)(cid:90)(cid:3)(cid:60)(cid:72)(cid:68)(cid:85)(cid:182)(cid:86)(cid:3)(cid:39)(cid:68)(cid:92)

rest more to boost 
your immune system. 

(cid:8709)(cid:5290)(cid:1913)(cid:7196)(cid:18227)(cid:2545)(cid:2417)(cid:19608)(cid:11585)
(cid:11259)
go to see a doctor 
(cid:72)(cid:89)(cid:72)(cid:85)(cid:92)(cid:3)(cid:49)(cid:72)(cid:90)(cid:3)(cid:60)(cid:72)(cid:68)(cid:85)(cid:182)(cid:86)(cid:3)(cid:39)(cid:68)(cid:92)

(cid:11093)(cid:7195)(cid:5665)(cid:1158)(cid:941)(cid:8814)(cid:17938)(cid:7013)(cid:6455)
(cid:1430)(cid:941)
happy birthday! will 
stand by you forever!  

(cid:11093)(cid:7195)(cid:5665)(cid:1158)(cid:941)(cid:8814)(cid:17938)(cid:5290)(cid:17841)
(cid:9528)(cid:1252)(cid:941)
happy birthday! wish 
you always stay young 
and pretty!  

(cid:12179)(cid:7199)(cid:7195)(cid:5357)(cid:3907)(cid:952)(cid:17633)(cid:1417)(cid:1691)
(cid:5357)(cid:941)
get well soon and stay 
healthy! 

(cid:13879)(cid:1212)(cid:13879)(cid:1212)(cid:952)(cid:4791)(cid:7486)(cid:1091)(cid:20396)
(cid:1264)(cid:2944)(cid:941)
getting old now. time 
has no mercy. 

(cid:20750)(cid:10013)(cid:171)

(cid:43)(cid:76)(cid:74)(cid:75)(cid:3)(cid:73)(cid:72)(cid:89)(cid:72)(cid:85)(cid:171)(cid:3)

(cid:6215)(cid:2592)(cid:7269)(cid:13879)(cid:1212)(cid:1078)(cid:4791)

i just gain only one 
more year. 

(cid:5627)(cid:20145)(cid:7579)(cid:7013)(cid:6455)(cid:941)(cid:2262)(cid:8943)(cid:941)

(cid:3472)(cid:6455)(cid:1413)(cid:952)(cid:3472)(cid:6455)(cid:1089)(cid:2545)(cid:574)

(cid:8709)(cid:3935)(cid:18227)(cid:7269)(cid:13142)(cid:20245)(cid:1137)(cid:1151)(cid:13883)

(cid:3472)(cid:6455)(cid:1413)(cid:941)

i come to offer my 
support. keep it up!  

hold on, keep it up. 

i have been being a 
vegetarian everyday  

hold on! 

(cid:17935)(cid:7269)(cid:2848)(cid:1909)(cid:2944)(cid:971)(cid:11605)(cid:13764)(cid:941)

(cid:19561)(cid:1919)(cid:7236)(cid:4304)(cid:952)(cid:5625)(cid:5883)(cid:14408)(cid:11131)(cid:574)

(cid:17935)(cid:12291)(cid:3935)(cid:8778)(cid:11605)(cid:1091)(cid:7269)(cid:11532)(cid:11450)(cid:574)

where is it? it is so 
beautiful!

such a nice sunny day! 
i am in a great mood. 

it is indeed a very nice 
weather.  

(cid:7101)(cid:4775)(cid:2944) (cid:2096)(cid:2545)(cid:17477)(cid:19848)(cid:4657)
(cid:6324)(cid:9899)(cid:5973) (cid:2814)(cid:2814)
wenshan(cid:15)(cid:3)(cid:79)(cid:72)(cid:87)(cid:182)(cid:86)(cid:3)(cid:74)(cid:82)(cid:3)(cid:82)(cid:88)(cid:87)(cid:3)
to get some 
inspiration. ha! ha! 

1 

2 

3 

4 

(cid:2558)(cid:13879)(cid:1212)(cid:1078)(cid:4791)(cid:952)(cid:5973)(cid:16984)(cid:6215)(cid:1314)
(cid:13562)(cid:952)(cid:6215)(cid:5566)(cid:20750)(cid:1962)
i gain one more year. 
grateful to my group, so 
happy  

(cid:2617)(cid:13142)(cid:12642)(cid:1078)(cid:3935)(cid:952)(cid:3472)(cid:6455)(cid:1413)(cid:952)
(cid:4938)(cid:7531)(cid:6399)(cid:574)
first day of being a 
vegetarian.  hold on, 
cuiduola

(cid:17935)(cid:12291)(cid:3935)(cid:8778)(cid:11605)(cid:5322)(cid:16923)(cid:5212)(cid:1120)(cid:5179)
(cid:19259)(cid:2096)(cid:2545)(cid:17477)(cid:19848)(cid:17285)(cid:14567)
we should go out with 
some cute guys to enjoy 
a great outing in such a 
nice weather. 

figure 8: some responses generated by different models (originally in chinese with their literal english
translation), where the words in boldfaces are entity names.

    suitable (+2): the response is evidently an appropriate and natural response to the post;
    neutral (+1): the response can be a suitable response in a speci   c scenario;
    unsuitable (0): it is hard or impossible to    nd a scenario where response is suitable.
to make the annotation task operable, the suitability of generated responses is judged from the follow-

ing    ve criteria:
(a) grammar and fluency: responses should be natural language and free of any    uency or grammat-

ical errors;

(b) logic consistency: responses should be logically consistent with the test post;
(c) semantic relevance: responses should be semantically relevant to the test post;
(d) scenario dependence: responses can depend on a speci   c scenario but should not contradict the

   rst three criteria;

(e) generality: responses can be general but should not contradict the    rst three criteria;
if any of the    rst three criteria (a), (b), and (c) is contradicted, the generated response should be labeled
as    unsuitable   . the responses that are general or suitable to post in a speci   c scenario should be
labeled as    neutral   . figure 7 shows an example of the labeling results of a post and its responses. the
   rst two responses are labeled as    unsuitable    because of the logic consistency and semantic relevance
errors. response4 depends on the scenario (i.e., the current score is 0:0), and is therefore annotated as
   neutral   .

model a model b

average
rankings

nrm-loc
(1.463, 1.537)
nrm-glo
nrm-hyb nrm-glo
(1.434, 1.566)
nrm-hyb nrm-loc
(1.465, 1.535)
(1.512, 1.488)
rtr.-based nrm-glo
(1.533, 1.467)
rtr.-based nrm-loc
rtr.-based nrm-hyb
(1.552, 1.448)
nrm-hyb
smt
(1.785, 1.215)
rtr.-based (1.738, 1.262)
smt

p value

2.01%
0.01%
3.09%
48.1%
6.20%
0.32%
0.00 %
0.00 %

table 3: p-values and average rankings of friedman test for pairwise model comparison. (rtr.-based
means the retrieval-based method)

5.2 results
our test set consists of 110 posts that do not appear in the training set, with length between 6 to 22
chinese words and 12.5 words on average. the experimental results based on human annotation are
summarized in table 2, consisting of the ratio of three categories and the agreement among the    ve
labelers for each model. the agreement is evaluated by fleiss    kappa (fleiss, 1971), as a statistical
measure of inter-rater consistency. except the smt-based model, the value of agreement is in a range
from 0.2 to 0.4 for all the other models, which should be interpreted as    fair agreement   . the smt-based
model has a relatively higher kappa value 0.448, which is larger than 0.4 and considered as    moderate
agreement   , since the responses generated by the smt often have the    uency and grammatical errors,
making it easy to reach an agreement on such unsuitable cases.

from table 2, we can see the smt method performs signi   cantly worse than the retrieval-based and
nrm models and 74.4% of the generated responses were labeled as unsuitable mainly due to    uency
and relevance errors. this observation con   rms with our intuition that the stc dataset, with one post
potentially corresponding to many responses, can not be simply taken as parallel corpus in a smt model.
surprisingly, more than 60% of responses generated by all the three nrm are labeled as    suitable   
or    neutral   , which means that most generated responses are    uent and semantically relevant to post.
among all the nrm variants

    nrm-loc outperforms nrm-glo, suggesting that a dynamically generated context might be more ef-
fective than a    static       xed-length vector for the entire post, which is consistent with the observation
made in (bahdanau et al., 2014) for machine translation;

    nrm-hyp outperforms nrm-loc and nrm-glo, suggesting that a global representation of post is

complementary to dynamically generated local context.

the retrieval-based model has the similar mean score as nrm-glo, and its ratio on neutral cases
outperforms all the other methods. this is because 1) the responses retrieved by retrieval-based method
are actually wrote by human, so they do not suffer from grammatical and    uency problems, and 2)
the combination of various feature functions potentially makes sure the picked responses semantically
relevant to test posts. however the picked responses are not customized for new test posts, so the ratio
of suitable cases is lower than the three neural generation models.

to test statistical signi   cance, we use the friedman test (howell, 2010), which is a non-parametric
test on the differences of several related samples, based on ranking. table 3 shows the average rankings
over all annotations and the corresponding p-values for comparisons between different pairs of methods.
the comparison between retrieval-based and nrm-glo is not signi   cant and their difference in ranking
is tiny. this indicates that the retrieval-based method is comparable to the nrm-glo method. the nrm-
hyb outperforms all the other methods, and the difference is statistically signi   cant (p < 0.05). the
difference between nrm-loc and retrieval-based method is marginal (p = 0.062). smt is signi   cantly
worse than retrieval-based and nrm-hyb methods.

5.3 case study
figure 8 shows some example responses generated by our nrms (only the one with biggest likelihood
is given) and the comparable retrieval-based model. it is intriguing to notice that three nrm variants
give suitable but quite distinct responses, with different perspectives and choices of words. this, as we
conjecture, is caused by both the architecture variations among models as well as the variations from
random effects like the initialization of parameters. another interesting observation is on the forth exam-
ple, where the retrieval-based method returns a response with the a mismatched entity name    wenshan   ,
which is actually a quite common problem for retrieval-based model, where the inconsistency details
(e.g., dates, named entities), which often render the response unsuitable, cannot be adequately consid-
ered in the matching function employed in retrieving the responses. in contrast, we observe that nrms
tend to make general response and barely generate those details.

post 

(cid:2617)(cid:13142)(cid:12642)(cid:1078)(cid:3935)(cid:952)(cid:3472)(cid:6455)(cid:1413)(cid:952)(cid:4938)(cid:7531)(cid:6399)(cid:574) 
first day of being a vegetarian.  hold on, cuiduola

r1

r2

r3

r4

r5

(cid:3472)(cid:6455)(cid:1413)(cid:952)(cid:3472)(cid:6455)(cid:13142)(cid:20245)(cid:941) 
hold on, keep up your vegetarian diet. 

(cid:5929)(cid:16311)(cid:1691)(cid:5357)(cid:2637)(cid:971)(cid:5929)(cid:16311)(cid:19381)(cid:4661)(cid:2637)(cid:971)(cid:18137)(cid:4711)(cid:1961)(cid:8990)(cid:6215)(cid:11450)(cid:5604)(cid:2448)(cid:2653)(cid:574)
would you like to be healthy? would you like to live 
long? if so, follow me on weibo. 

(cid:17935)(cid:4711)(cid:7269)(cid:1366)(cid:16938)(cid:1123)(cid:11450)(cid:2617)(cid:13142)(cid:2637)(cid:971) 
this is the so-called vegetarian diet ? 

(cid:2617)(cid:20391)(cid:1212)(cid:6275)(cid:7487)(cid:2257)(cid:8778)(cid:2053)(cid:14043) 
eat up so that you will have enough energy to go for 
a diet! 

(cid:17633)(cid:1417)(cid:7269)(cid:19871)(cid:2739)(cid:11450)(cid:7522)(cid:19175)(cid:952)(cid:1091)(cid:16311)(cid:11638)(cid:5723)(cid:941) 
health is the essence for winning your future battles.  
no rush!  

figure 9: multiple responses generated by the nrm-hyb.

we also use the nrm-hyb as an example to investigate the ability of nrm to generate multiple
responses. figure 9 lists 5 responses to the same post, which are gotten with id125 with beam size

= 500, among which we keep only the best one (biggest likelihood) for each    rst word. it can be seen
that the responses are    uent, relevant to the post, and still vastly different from each other, validating our
initial conjecture that nrm, when fueled with large and rich training corpus, could work as a generator
that can cover a lot of modes in its density estimation.

6 conclusions and future work

in this paper, we explored using encoder-decoder-based neural network system, with coined name neural
responding machine, to generate responses to a post. empirical studies con   rm that the newly proposed
nrms, especially the hybrid encoding scheme, can outperform state-of-the-art retrieval-based and smt-
based methods. our preliminary study also shows that nrm can generate multiple responses with great
variety to a given post. in future work, we would consider adding the intention (or sentiment) of users as
an external signal of decoder to generate responses with speci   c goals.

references
[auli et al.2013] michael auli, michel galley, chris quirk, and geoffrey zweig. 2013. joint language and trans-

lation modeling with recurrent neural networks. in emnlp, pages 1044   1054.

[bahdanau et al.2014] dzmitry bahdanau, kyunghyun cho, and yoshua bengio. 2014. id4

by jointly learning to align and translate. arxiv preprint arxiv:1409.0473.

[cho et al.2014] kyunghyun cho, bart van merrienboer, caglar gulcehre, fethi bougares, holger schwenk, and
yoshua bengio. 2014. learning phrase representations using id56 encoder-decoder for statistical machine trans-
lation. arxiv preprint arxiv:1406.1078.

[chung et al.2014] junyoung chung, caglar gulcehre, kyunghyun cho, and yoshua bengio. 2014. empirical

evaluation of gated recurrent neural networks on sequence modeling. arxiv preprint arxiv:1412.3555.

[fleiss1971] joseph l fleiss. 1971. measuring nominal scale agreement among many raters. psychological

bulletin, 76(5):378.

[graves2013] alex graves.

arxiv:1308.0850.

2013.

generating sequences with recurrent neural networks.

preprint

[hochreiter and schmidhuber1997] sepp hochreiter and j  urgen schmidhuber. 1997. long short-term memory.

neural computation, 9(8):1735   1780.

[howell2010] david c. howell. 2010. fundamental statistics for the behavioral sciences. psy 200 (300) quan-

titative methods in psychology series. wadsworth cengage learning.

[ji et al.2014] zongcheng ji, zhengdong lu, and hang li. 2014. an information retrieval approach to short text

conversation. arxiv preprint arxiv:1408.6988.

[joachims2006] thorsten joachims. 2006. training linear id166s in linear time. in sigkdd, pages 217   226. acm.

[kalchbrenner and blunsom2013] nal kalchbrenner and phil blunsom. 2013. recurrent continuous translation

models. in emnlp, pages 1700   1709.

[koehn et al.2007] philipp koehn, hieu hoang, alexandra birch, chris callison-burch, marcello federico, nicola
bertoldi, brooke cowan, wade shen, christine moran, richard zens, et al. 2007. moses: open source toolkit
for id151. in proceedings of the 45th annual meeting of the acl on interactive poster
and demonstration sessions, pages 177   180. acl.

[litman et al.2000] diane litman, satinder singh, michael kearns, and marilyn walker. 2000. njfun: a reinforce-
ment learning spoken dialogue system. in proceedings of the 2000 anlp/naacl workshop on conversational
systems, pages 17   20. acl.

[mikolov et al.2010] tomas mikolov, martin kara     at, lukas burget, jan cernock`y, and sanjeev khudanpur. 2010.

recurrent neural network based language model. in interspeech 2010, pages 1045   1048.

[misu et al.2012] teruhisa misu, kallirroi georgila, anton leuski, and david traum. 2012. reinforcement learn-

ing of question-answering dialogue policies for virtual museum guides. in sigdial, pages 84   93. acl.

[och and ney2003] franz josef och and hermann ney. 2003. a systematic comparison of various statistical

alignment models. computational linguistics, 29(1):19   51.

[papineni et al.2002] kishore papineni, salim roukos, todd ward, and wei-jing zhu. 2002. id7: a method for
in proceedings of the 40th annual meeting on association for

automatic evaluation of machine translation.
computational linguistics, pages 311   318. association for computational linguistics.

[ritter et al.2011] alan ritter, colin cherry, and william b dolan. 2011. data-driven response generation in

social media. in emnlp, pages 583   593. association for computational linguistics.

[schatzmann et al.2006] jost schatzmann, karl weilhammer, matt stuttle, and steve young. 2006. a survey
of statistical user simulation techniques for reinforcement-learning of dialogue management strategies. the
knowledge engineering review, 21(02):97   126.

[sutskever et al.2014] ilya sutskever, oriol vinyals, and quoc vv le. 2014. sequence to sequence learning with

neural networks. in nips, pages 3104   3112.

[wang et al.2013] hao wang, zhengdong lu, hang li, and enhong chen. 2013. a dataset for research on short-

text conversations. in emnlp, pages 935   945.

[williams and young2007] jason d williams and steve young. 2007. partially observable markov decision pro-

cesses for spoken id71. computer speech & language, 21(2):393   422.

