sense embedding learning for word sense induction

linfeng song1, zhiguo wang2, haitao mi2 and daniel gildea1

1department of computer science, university of rochester, rochester, ny 14627

2ibm t.j. watson research center, yorktown heights, ny 10598

6
1
0
2

 

n
u
j
 

2
2

 
 
]
l
c
.
s
c
[
 
 

2
v
9
0
4
5
0

.

6
0
6
1
:
v
i
x
r
a

abstract

conventional word sense induction (wsi)
methods usually represent each instance
with discrete linguistic features or co-
occurrence features, and train a model
for each polysemous word individually.
in this work, we propose to learn sense
embeddings for the wsi task.
in the
training stage, our method induces sev-
eral sense centroids (embedding) for each
polysemous word.
in the testing stage,
our method represents each instance as a
contextual vector, and induces its sense
by    nding the nearest sense centroid in
the embedding space. the advantages
of our method are (1) distributed sense
vectors are taken as the knowledge rep-
resentations which are trained discrimi-
natively, and usually have better perfor-
mance than traditional count-based distri-
butional models, and (2) a general model
for the whole vocabulary is jointly trained
to induce sense centroids under the mutli-
task learning framework. evaluated on
semeval-2010 wsi dataset, our method
outperforms all participants and most of
the recent state-of-the-art methods. we
further verify the two advantages by com-
paring with carefully designed baselines.

1 introduction

word sense induction (wsi) is the task of auto-
matically    nding sense clusters for polysemous
words.
in contrast, id51
(wsd) assumes there exists an already-known
sense inventory, and the sense of a word type is
disambiguated according to the sense inventory.
therefore, id91 methods are generally ap-
plied in wsi tasks, while classi   cation methods

are utilized in wsd tasks. wsi has been success-
fully applied to many nlp tasks such as machine
translation (xiong and zhang, 2014), information
retrieval (navigli and crisafulli, 2010) and novel
sense detection (lau et al., 2012).

however,

(bordag, 2006;

existing methods usually repre-
sent each instance with discrete hand-crafted
features
chen et al., 2009;
van de cruys and apidianaki, 2011;
purandare and pedersen, 2004),
are
designed manually and require linguistic knowl-
edge. most previous methods require learning a
speci   c model for each polysemous word, which
limits their usability for down-stream applications
and loses the chance to jointly learn senses for
multiple words.

which

is

great

there

advance

in
such

semantics,

embedding

a
distributed

re-
as
cent
(mikolov et al., 2013;
word
pennington et al., 2014) and sense embedding
(reisinger and mooney, 2010; huang et al., 2012;
jauhar et al., 2015;
rothe and sch  utze, 2015;
chen et al., 2014; tian et al., 2014). comparing
with id27, sense embedding methods
learn distributed representations for senses of a
polysemous word, which is similar to the sense
centroid of wsi tasks.

in this work, we point out that the wsi task
and the sense embedding task are highly inter-
related, and propose to jointly learn sense cen-
troids (embeddings) of all polysemous words for
the wsi task. concretely, our method induces
several sense centroids (embedding) for each pol-
ysemous word in training stage. in testing stage,
our method represents each instance as a contex-
tual vector, and induces its sense by    nding the
nearest sense centroid in the embedding space.
comparing with existing methods, our method has
two advantages: (1) distributed sense embeddings
are taken as the id99s which

are trained discriminatively, and usually have bet-
ter performance than traditional count-based dis-
tributional models (baroni et al., 2014), and (2) a
general model for the whole vocabulary is jointly
trained to induce sense centroids under the mutli-
task learning framework (caruana, 1997). evalu-
ated on semeval-2010 wsi dataset, our method
outperforms all participants and most of the recent
state-of-the-art methods.

2 methodology

that

2.1 word sense induction
wsi is generally considered as an unsupervised
id91 task under the distributional hypoth-
esis (harris, 1954)
the word meaning is
re   ected by the set of contexts in which it appears.
existing wsi methods can be roughly divided
into feature-based or bayesian.
feature-based
methods    rst represent each instance as a context
then utilize a id91 algorithm on
vector,
the context vectors to induce all
the senses.
(brody and lapata, 2009;
bayesian methods
yao and van durme, 2011;
lau et al., 2012;
goyal and hovy, 2014; wang et al., 2015), on the
other hand, discover senses based on topic mod-
els. they adopt either the lda (blei et al., 2003)
or hdp (teh et al., 2006) model by viewing
each target word as a corpus and the contexts
as pseudo-documents, where a context includes
all words within a window centred by the target
word.
they    rst extract
pseudo-documents for the target word, then train
topic model,    nally pick the most probable topic
for each test pseudo-document as the sense.

for sense induction,

co-occurrences between documents and senses or
between senses and words. overall existing meth-
ods separately train a speci   c vsm for each word.
no methods have shown distributional vectors can
keep knowledge for multiple words while showing
competitive performance.

2.2 sense embedding for wsi
as mentioned in section 1, sense embedding
methods learn a distributed representation for each
sense of a polysemous word. there are two key
factors for sense embedding learning:
(1) how
to decide the number of senses for each polyse-
mous word and (2) how to learn an embedding
representation for each sense. to decide the num-
ber of senses in factor (1), one group of meth-
ods (huang et al., 2012; neelakantan et al., 2014)
set a    xed number k of senses for each word,
and each instance is assigned to the most proba-
ble sense according to equation 1, where   (wt, k)
is the vector for the k-th sense centroid of word w,
and vc is the representation vector of the instance.

st = arg max

sim(  (wt, k), vc)

(1)

k=1,..,k
group

of

another

methods
(li and jurafsky, 2015) employs non-parametric
algorithms to dynamically decide the number
of senses for each word, and each instance
is assigned to a sense following a id203
distribution in equation 2, where st is the set
of already generated senses for wt, and    is a
constant id203 for generating a new sense
for wt.

all of the existing wsi methods have two im-
portant factors: 1) how to group similar instances
(id91 algorithm) and 2) how to represent
context (id99).
for clus-
tering algorithms, feature-based methods use k-
means or graph-based id91 algorithms to as-
sign each instance to its nearest sense, whereas
bayesian methods sample the sense from the prob-
ability distribution among all the senses for each
instance, which can be seen as soft id91 al-
gorithms. as for id99, ex-
isting wsi methods use the vector space model
(vsm) to represent each context. in feature-based
models, each instance is represented as a vector of
values, where a value can be the count of a fea-
ture or the co-occurrence between two words. in
bayesian methods, the vectors are represented as

st    (p(k|  (wt, k), vc)     k     st

   for new sense

(2)

from the above discussions, we can obviously
notice that wsi task and sense embedding task
are inter-related. the two factors in sense em-
bedding learning can be aligned to the two fac-
tors of wsi task. concretely, deciding the num-
ber of senses is the same problem as the id91
problem in wsi task, and sense embedding is a
potential id99 for wsi task.
therefore, sense embedding methods are naturally
applicable to wsi.

in this work, we apply the sense embedding
learning methods for wsi tasks. algorithm 1
lists the    ow of our method. the algorithm iter-
ates several times over a corpus (line 2-3). for

for wt in c do

for iter in [1..i] do

algorithm 1 sense embedding learning for wsi
1: procedure training(corpus c)
2:
3:
4:
5:
6:
7:
8:
9: end procedure

vc     context vec(wt)
st     sense label(wt, vc)
update(wt, st)

end for

end for

each token wt, it calculates the context vector vc
(line 4) for an instance, and then gets the most
possible sense label st for wt (line 5). finally,
both the sense embeddings for st and global word
embeddings for all context words of wt are up-
dated (line 6). we introduce our strategy for con-
text vec in the next section. for sense label func-
tion, a sense label is obtained by either equation
1 or equation 2. for the update function, vec-
tors are updated by the skip-gram method (same
as neelakantan et al. (2014)) which tries to predict
context words with the current sense. in this al-
gorithm, the senses of all polysemous words are
learned jointly on the whole corpus, instead of
training a single model for each individual word
as in the traditional wsi methods. this is actu-
ally an instance of id72, where wsi
models for each target word are trained together,
and all of these models share the same global word
embeddings.

comparing to the traditional methods for wsi
the advantages of our method include:
tasks,
the polysemous words
1) wsi models for all
are trained jointly under the id72
framework; 2) distributed sense embeddings are
taken as the id99s which are
trained discriminatively, and usually have better
performance than traditional count-based distribu-
tional models (baroni et al., 2014). to verify the
two statements, we carefully designed compara-
tive experiments described in the next section.

3 experiment

3.1 experimental setup and baselines

on

our methods

evaluate
of
the

the
semeval-2010 wsi

we
test
set
task
(manandhar et al., 2010).
it contains 8,915
instances for 100 target words (50 nouns and 50
verbs) which mostly come from news domain.

we choose the april 2010 snapshot of wikipedia
(shaoul and westbury, 2010) as our training set,
as it is freely available and domain general.
it
contains around 2 million documents and 990
million tokens. we train and test our models and
the baselines according to the above data setting,
and compare with reported performance on the
same test set from previous papers.

our

for

assigns

(mssg)
and

embedding method, we
sense
se-wsi-   x which adopts
build two systems:
model
multi-sense
skip-gram
(neelakantan et al., 2014)
3
senses for each word type, and se-wsi-crp
(li and jurafsky, 2015) which dynamically de-
cides the number of senses using a chinese
restaurant process. for se-wsi-   x, we learn sense
embeddings for the top 6k frequent words in the
training set. for se-wsi-crp, we    rst learn word
embeddings with id971, then use them as
pre-trained vectors to learn sense embeddings.
all training is under default parameter settings,
and all word and sense embeddings are    xed at
300 dimensions. for fair comparison, we create
se-wsi-   x-cmp by training the mssg model on
the training data of the semeval-2010 wsi task
with the same setting of se-wsi-   x.

we also design baselines to verify the two ad-
vantages of our sense embedding methods. one
(crp-ppmi) uses the same crp algorithm as se-
wsi-crp, but with positive pmi vectors as pre-
trained vectors. the other (we-kmeans) uses the
vectors learned by se-wsi-   x, but separately clus-
ters all the context vectors into 3 groups for each
target word with kmeans. we compute a con-
text vector by averaging the vectors of all selected
words in the context2.

the

our

best

systems:

methods
(1)

compare
following

3.2 comparing on semeval-2010
with
we
the
uoy
which
(korkontzelos and manandhar, 2010)
semeval-
is
the
2010 wsi
(2)
nmflib
(van de cruys and apidianaki, 2011)
which
adopts non-negative id105 to factor
a matrix and then conducts word sense id91
on the test set; (3) nb (choe and charniak, 2013)
which adopts naive bayes with the generative
story that a context is generated by picking a sense

system in
competition;

1https://code.google.com/p/id97/
2a word is selected only if its length is greater than 3, not

the target word, or not in a self-constructed stoplist.

system

v-measure(%)

paired f-score(%)

80-20 sr(%)

uoy (2010)
nmflib (2011)
nb (2013)
spectral (2014)
se-wsi-   x-cmp
se-wsi-   x
se-wsi-crp
crp-ppmi
we-kmeans

all
15.7
11.8
18.0
4.5
16.3
9.8
5.7
2.9
4.6

noun verb all
49.8
20.6
45.3
13.5
23.7
52.9
61.5
4.6
54.3
20.8
55.1
13.5
7.4
55.3
57.7
3.5
5.0
51.2

8.5
9.4
9.9
4.2
9.7
4.3
3.2
2.0
4.1

noun verb all
62.4
38.2
62.6
42.2
52.5
65.4
54.5
-
66.3
54.2
62.9
50.7
49.4
61.2
59.2
53.3
46.5
58.6

66.6
49.8
53.5
71.6
54.3
61.6
63.8
64.0
57.6

fs
noun verb all
59.4
57.3
62.6
-
63.6
58.5
56.3
53.6
53.3

66.8
70.2
69.5
-
70.2
69.2
67.9
67.4
66.4

-
-
-
60.7
66.4
63.0
61.3
59.2
58.6

#ci

11.5
4.80
3.42
1.87
2.61
2.50
2.09
1.76
2.54

table 1: result on semeval-2010 wsi task. 80-20 sr is the supervised recall of 80-20 split supervised
evaluation. fs is the f-score of 80-20 split supervised evaluation. #ci is the average number of clusters
(senses)

and then all context words given the sense; and (4)
spectral (goyal and hovy, 2014) which applies
spectral id91 on a set of distributional
context vectors.

experimental results are shown in table 1. let
us see the results on supervised recall (80-20 sr)
   rst, as it is the main indicator for the task. over-
all, se-wsi-   x-cmp, which jointly learns sense
embedding for 6k words, outperforms every com-
paring systems which learns for each single word.
this shows that sense embedding is suitable and
promising for the task of word sense induction.
trained on out-of-domain data, se-wsi-   x outper-
forms most of the systems, including the best sys-
tem in the shared task (uoy), and se-wsi-crp
works better than spectral and all the baselines.
this also shows the effectiveness of the sense em-
bedding methods. besides, se-wsi-crp is 1.7
points lower than se-wsi-   x. we think the rea-
son is that se-wsi-crp induces fewer senses than
se-wsi-   x (see the last column of table 1). since
both systems induce fewer senses than the golden
standard which is 3.85,
inducing fewer senses
harms the performance. finally, simple as it is,
nb shows a very good performance. however nb
can not bene   t from large-scale data as its number
of parameters is small, and it uses em algorithm
which is generally slow. sense embedding meth-
ods have other advantages that they train a general
model while nb learns speci   c model for each tar-
get word.

as for the unsupervised evaluations, se-wsi-
   x achieves a good v-measure score (vm) with
a few induced senses. pedersen (2010) points out
that bad models can increase vm by increasing

the number of clusters, but doing this will harm
performance on both paired f-score (pf) and sr.
even though uoy, nmflib and nb show better
vm, they (especially uoy) induced more senses
than se-wsi-   x. se-wsi-   x has higher pf than
all others, and higher sr than uoy and nmflib.
trained on the of   cial training data of semeval-
2010 wsi task, se-wsi-   x-cmp achieves the top
performance on both vm and pf, while it induces
a reasonable number of averaged senses. compar-
atively se-wsi-crp has lower vm and induces
fewer senses than se-wsi-   x. one possible reason
is that the    rich gets richer    nature of crp makes
it conservative for making new senses. but its pf
and sr show that it is still a highly competitive
system.

to verify the advantages of our method, we    rst
compare se-wsi-crp with crp-ppmi as their
only difference is the vectors for representing con-
texts. we can see that se-wsi-crp performs sig-
ni   cantly better than crp-ppmi on both sr and
vm. crp-ppmi has higher pf mainly because it
induces fewer number of senses. the above re-
sults prove that using sense embeddings have bet-
ter performance than using count-based distribu-
tional models. besides, se-wsi-   x is signi   cantly
better than we-kmeans on every metric. as we-
kmeans and se-wsi-   x learn sense centroids in
the same vectors space, while the latter performs
joint learning. therefore, the joint learning is bet-
ter than learning separately.

4 related work

k  ageb  ack et al. (2015) proposed two methods to
utilize distributed representations for the wsi

task.
the    rst method learned centroid vec-
tors by id91 all pre-computed context vec-
tors of each target word. the other method sim-
ply adopted mssg (neelakantan et al., 2014) and
changed context vector calculation from the aver-
age of all context word vectors to weighted aver-
age. our work has further contributions. first,
we clearly point out the two advantages of sense
embedding methods: 1) joint learning under the
mutli-task learning framework, 2) better knowl-
edge representation by discriminative training, and
verify them by experiments. in addition, we adopt
various sense embedding methods to show that
sense embedding methods are generally promis-
ing for wsi, not just one method is better than
other methods. finally, we compare our methods
with recent state-of-the-art wsi methods on both
supervised and unsupervised metrics.

5 conclusion

in this paper, we show that sense embedding is
a promising approach for wsi by adopting two
different sense embedding based systems on the
semeval-2010 wsi task. both systems show
highly competitive performance while they learn a
general model for thousands of words (not just the
tested polysemous words). we believe that the two
advantages of our method are: 1) joint learning
under the mutli-task learning framework, 2) better
id99 by discriminative train-
ing, and verify them by experiments.

acknowledgments

funded by nsf iis-1446996. we would like to
thank yue zhang for his insightful comments on
the    rst version of the paper, and the anonymous
reviewers for the insightful comments.

references

[baroni et al.2014] marco baroni, georgiana dinu, and
germ  an kruszewski.
2014. don   t count, pre-
dict! a systematic comparison of context-counting
vs. context-predicting semantic vectors. in proceed-
ings of the 52nd annual meeting of the association
for computational linguistics (volume 1: long pa-
pers), pages 238   247, baltimore, maryland, june.
association for computational linguistics.

[blei et al.2003] d. blei, a. ng, and m. jordan. 2003.
journal of machine

id44.
learning research, 3:993   1022, jan.

[bordag2006] stefan bordag. 2006. word sense induc-
tion: triplet-based id91 and automatic evalua-
tion. in eacl. citeseer.

[brody and lapata2009] samuel brody and mirella la-
pata. 2009. bayesian word sense induction.
in
proceedings of the 12th conference of the european
chapter of the acl (eacl 2009), pages 103   111,
athens, greece, march. association for computa-
tional linguistics.

[caruana1997] rich caruana. 1997. multitask learn-

ing. machine learning, 28(1):41   75.

[chen et al.2009] ping chen, wei ding, chris bowes,
2009. a fully unsupervised
and david brown.
id51 method using depen-
dency knowledge. in proceedings of human lan-
guage technologies: the 2009 annual conference
of the north american chapter of the association
for computational linguistics, pages 28   36, boul-
der, colorado, june. association for computational
linguistics.

[chen et al.2014] xinxiong chen, zhiyuan liu, and
maosong sun. 2014. a uni   ed model for word
sense representation and disambiguation.
in pro-
ceedings of the 2014 conference on empirical meth-
ods in natural language processing (emnlp),
pages 1025   1035, doha, qatar, october. associa-
tion for computational linguistics.

[choe and charniak2013] do kook choe and eugene
charniak. 2013. naive bayes word sense induc-
tion. in proceedings of the 2013 conference on em-
pirical methods in natural language processing,
pages 1433   1437, seattle, washington, usa, oc-
tober. association for computational linguistics.

[goyal and hovy2014] kartik goyal and eduard h
hovy. 2014. unsupervised word sense induction
using distributional statistics.
in coling, pages
1302   1310.

[harris1954] zellig s harris.

1954. distributional

structure. word, 10(2-3):146   162.

[huang et al.2012] eric huang,

richard
socher,
christopher manning, and andrew ng.
2012.
improving word representations via global context
in proceedings
and multiple word prototypes.
of
the association
for computational linguistics (volume 1: long
papers), pages 873   882, jeju island, korea, july.
association for computational linguistics.

the 50th annual meeting of

[jauhar et al.2015] sujay kumar jauhar, chris dyer,
and eduard hovy. 2015. ontologically grounded
multi-sense representation learning for semantic
vector space models.
in proceedings of the 2015
conference of the north american chapter of the
association for computational linguistics: hu-
man language technologies, pages 683   693, den-
ver, colorado, may   june. association for compu-
tational linguistics.

[korkontzelos and manandhar2010] ioannis

ko-
rkontzelos and suresh manandhar.
2010. uoy:
graphs of unambiguous vertices for word sense
induction and disambiguation.
in proceedings
of
the 5th international workshop on semantic
evaluation, pages 355   358, uppsala, sweden, july.
association for computational linguistics.

[k  ageb  ack et al.2015] mikael k  ageb  ack, fredrik jo-
hansson, richard johansson, and devdatt dubhashi.
2015. neural context embeddings for automatic dis-
covery of word senses.
in proceedings of the 1st
workshop on vector space modeling for natural
language processing, pages 25   32, denver, col-
orado, june. association for computational linguis-
tics.

[lau et al.2012] jey han lau, paul cook, diana mc-
carthy, david newman, and timothy baldwin.
2012. word sense induction for novel sense detec-
tion. in proceedings of the 13th conference of the
european chapter of the association for computa-
tional linguistics, pages 591   601, avignon, france,
april. association for computational linguistics.

[li and jurafsky2015] jiwei li and dan jurafsky.
2015. do multi-sense embeddings improve natu-
ral language understanding? in proceedings of the
2015 conference on empirical methods in natu-
ral language processing, pages 1722   1732, lisbon,
portugal, september. association for computational
linguistics.

[manandhar et al.2010] suresh manandhar, ioannis p
klapaftis, dmitriy dligach, and sameer s pradhan.
2010. semeval-2010 task 14: word sense induc-
tion & disambiguation. in proceedings of the 5th in-
ternational workshop on semantic evaluation, pages
63   68. association for computational linguistics.

[mikolov et al.2013] tomas mikolov, ilya sutskever,
kai chen, greg s corrado, and jeff dean. 2013.
distributed representations of words and phrases
and their compositionality. in advances in neural
information processing systems, pages 3111   3119.

[navigli and crisafulli2010] roberto navigli

and
inducing word senses
giuseppe crisafulli. 2010.
in pro-
to improve web search result id91.
ceedings of
the 2010 conference on empirical
methods in natural language processing, pages
116   126, cambridge, ma, october. association for
computational linguistics.

[neelakantan et al.2014] arvind neelakantan, jeevan
shankar, alexandre passos, and andrew mccal-
lum.
2014. ef   cient non-parametric estimation
of multiple embeddings per word in vector space.
in proceedings of the 2014 conference on em-
pirical methods in natural language processing
(emnlp), pages 1059   1069, doha, qatar, october.
association for computational linguistics.

semeval-2. in proceedings of the 5th international
workshop on semantic evaluation, pages 363   366,
uppsala, sweden, july. association for computa-
tional linguistics.

[pennington et al.2014] jeffrey pennington, richard
socher, and christopher manning. 2014. glove:
global vectors for word representation. in proceed-
ings of the 2014 conference on empirical methods
in natural language processing (emnlp), pages
1532   1543, doha, qatar, october. association for
computational linguistics.

[purandare and pedersen2004] amruta purandare and
2004. word sense discrimina-
ted pedersen.
tion by id91 contexts in vector and similarity
spaces.
in hwee tou ng and ellen riloff, edi-
tors, hlt-naacl 2004 workshop: eighth confer-
ence on computational natural language learn-
ing (conll-2004), pages 41   48, boston, mas-
sachusetts, usa, may 6 - may 7. association for
computational linguistics.

[reisinger and mooney2010] joseph reisinger

and
2010. multi-prototype
raymond j. mooney.
vector-space models of word meaning.
in hu-
man language technologies: the 2010 annual
conference of
the north american chapter of
the association for computational linguistics,
pages 109   117, los angeles, california, june.
association for computational linguistics.

[rothe and sch  utze2015] sascha rothe and hinrich
sch  utze. 2015. autoextend: extending word em-
beddings to embeddings for synsets and lexemes.
in proceedings of the 53rd annual meeting of the
association for computational linguistics and the
7th international joint conference on natural lan-
guage processing (volume 1: long papers), pages
1793   1803, beijing, china, july. association for
computational linguistics.

[shaoul and westbury2010] cyrus shaoul and chris
westbury. 2010. the westbury lab wikipedia cor-
pus.

[teh et al.2006] y. w. teh, m. i. jordan, m. j. beal,
and d. m. blei. 2006. hierarchical dirichlet pro-
cesses. journal of the american statistical associa-
tion, 101(476):1566   1581.

[tian et al.2014] fei tian, hanjun dai, jiang bian, bin
gao, rui zhang, enhong chen, and tie-yan liu.
2014. a probabilistic model for learning multi-
prototype id27s.
in proceedings of
coling 2014, the 25th international conference
on computational linguistics: technical papers,
pages 151   160, dublin, ireland, august. dublin
city university and association for computational
linguistics.

[pedersen2010] ted pedersen.

2010. duluth-wsi:
senseclusters applied to the sense induction task of

[van de cruys and apidianaki2011] tim van de cruys
and marianna apidianaki. 2011. latent semantic

word sense induction and disambiguation. in pro-
ceedings of the 49th annual meeting of the associ-
ation for computational linguistics: human lan-
guage technologies, pages 1476   1485, portland,
oregon, usa, june. association for computational
linguistics.

[wang et al.2015] jing wang, mohit bansal, kevin
gimpel, brian ziebart, and clement yu. 2015. a
sense-topic model for word sense induction with un-
supervised data enrichment. transactions of the as-
sociation for computational linguistics, 3:59   71.

[xiong and zhang2014] deyi xiong and min zhang.
2014. a sense-based translation model for statistical
machine translation. in proceedings of the 52nd an-
nual meeting of the association for computational
linguistics (volume 1: long papers), pages 1459   
1469, baltimore, maryland, june. association for
computational linguistics.

[yao and van durme2011] xuchen yao and benjamin
van durme. 2011. nonparametric bayesian word
sense induction.
in proceedings of textgraphs-6:
graph-based methods for natural language pro-
cessing, pages 10   14. association for computa-
tional linguistics.

