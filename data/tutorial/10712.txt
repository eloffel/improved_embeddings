syntactically guided id4

felix stahlberg    and eva hasler    and aurelien waite    and bill byrne      

   department of engineering, university of cambridge, uk

   sdl research, cambridge, uk

6
1
0
2

 

y
a
m
9
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
9
6
5
4
0

.

5
0
6
1
:
v
i
x
r
a

abstract

we investigate the use of hierarchical
phrase-based smt lattices in end-to-end
id4 (id4). weight
pushing transforms the hiero scores for
complete translation hypotheses, with the
full translation grammar score and full n-
gram language model score, into posteri-
ors compatible with id4 predictive prob-
abilities. with a slightly modi   ed id4
beam-search decoder we    nd gains over
both hiero and id4 decoding alone, with
practical advantages in extending id4 to
very large input and output vocabularies.

introduction

1
we report on investigations motivated by the idea
that the structured search spaces de   ned by syn-
tactic machine translation approaches such as hi-
ero (chiang, 2007) can be used to guide neural
machine translation (id4) (kalchbrenner and
blunsom, 2013; sutskever et al., 2014; cho et al.,
2014; bahdanau et al., 2015). id4 and hiero
have complementary strengths and weaknesses
and differ markedly in how they de   ne probabil-
ity distributions over translations and what search
procedures they use.

the id4 encoder-decoder formalism provides
a id203 distribution over translations y = yt
1
of a source sentence x as (bahdanau et al., 2015)

p (yt

1 |x) =

p (yt|yt   1

1

, x) =

g(yt   1, st, ct)

(1)
where st = f (st   1, yt   1, ct) is a decoder state
variable and ct is a context vector depending on
the source sentence and the attention mechanism.
this posterior distribution is potentially very
powerful, however it does not easily lend itself

t(cid:89)

t=1

t(cid:89)

t=1

to sophisticated search procedures. decoding is
done by    id125 to    nd a translation that ap-
proximately maximizes the conditional probabil-
ity    (bahdanau et al., 2015). search looks only
one word ahead and no deeper than the beam.
hiero de   nes a synchronous context-free gram-
mar (sid18) with rules: x     (cid:104)  ,   (cid:105), where   
and    are strings of terminals and non-terminals in
the source and target languages. a target language
sentence y can be a translation of a source lan-
guage sentence x if there is a derivation d in the
grammar which yields both y and x: y = y(d),
x = x(d). this de   nes a regular language y
over strings in the target language via a projection
of the sentence to be translated: y = {y(d) :
x(d) = x} (iglesias et al., 2011; allauzen et al.,
2014). scores are de   ned over derivations via a
log-linear model with features {  i} and weights
  . the decoder searches for the translation y(d)
in y with the highest derivation score s(d) (chi-
ang, 2007, eq. 24) :

          (2)
(cid:125)

  y = y

          argmax
pg(d)    (cid:81)

d:x(d)=x

(cid:124)

(cid:123)(cid:122)

s(d)

(cid:81)

(x   (cid:104)  ,  (cid:105))   d

hiero decoders attempt

where plm is an id165 language model and
i   i(x     (cid:104)  ,   (cid:105))  i.
to avoid search er-
rors when combining the translation and lan-
guage model for the translation hypotheses (chi-
ang, 2007; iglesias et al., 2009). these procedures
search over a vast space of translations, much
larger than is considered by the id4 id125.
however the hiero context-free grammars that
make ef   cient search possible are weak models of
translation. the basic hiero formalism can be ex-
tended through    soft syntactic constraints    (venu-
gopal et al., 2009; marton and resnik, 2008) or by

pg(d)plm (y(d))  lm

adding very high dimensional features (chiang et
al., 2009), however the translation score assigned
by the grammar is still only the product of prob-
abilities of individual rules. from the modelling
perspective, this is an overly strong conditional in-
dependence assumption. id4 clearly has the po-
tential advantage in incorporating long-term con-
text into translation scores.

id4 and hiero differ in how they    consume   
source words. hiero applies the translation rules to
the source sentence via the cyk algorithm, with
each derivation yielding a complete and unam-
biguous translation of the source words. the id4
beam decoder does not have an explicit mecha-
nism for tracking source coverage, and there is ev-
idence that may lead to both    over-translation    and
   under-translation    (tu et al., 2016).

id4 and hiero also differ in their internal rep-
resentations. the id4 continuous representa-
tion captures morphological, syntactic and seman-
tic similarity (collobert and weston, 2008) across
words and phrases. however, extending these rep-
resentations to the large vocabularies needed for
open-domain mt is an open area of research (jean
et al., 2015a; luong et al., 2015; sennrich et al.,
2015; chitnis and denero, 2015). by contrast,
hiero (and other symbolic systems) can easily use
translation grammars and language models with
very large vocabularies (hea   eld et al., 2013; lin
and dyer, 2010). moreover, words and phrases
can be easily added to a fully-trained symbolic
mt system. this is an important consideration
for commercial mt, as customers often wish to
customise and personalise smt systems for their
own application domain. adding new words and
phrases to an id4 system is not as straightfor-
ward, and it is not clear that the advantages of the
continuous representation can be extended to the
new additions to the vocabularies.

id4 has the advantage of including long-range
context in modelling individual translation hy-
potheses. hiero considers a much bigger search
space, and can incorporate id165 language mod-
els, but a much weaker translation model. in this
paper we try to exploit the strengths of each ap-
proach. we propose to guide id4 decoding using
hiero. we show that restricting the search space of
the id4 decoder to a subset of y spanned by hi-
ero effectively counteracts id4 modelling errors.
this can be implemented by generating translation
lattices with hiero, which are then rescored by the

id4 decoder. our approach addresses the lim-
ited vocabulary issue in id4 as we replace id4
oovs with lattice words from the much larger hi-
ero vocabulary. we also    nd good gains from neu-
ral and kneser-ney id165 language models.

2 syntactically guided id4 (sgid4)
2.1 hiero predictive posteriors
the hiero decoder generates translation hypothe-
ses as weighted    nite state acceptors (wfsas), or
lattices, with weights in the tropical semiring. for
a translation hypothesis y(d) arising from the hi-
ero derivation d, the path weight in the wfsa
is     log s(d), after eq. 2. while this representa-
tion is correct with respect to the hiero translation
grammar and language model scores, having hi-
ero scores at the path level is not convenient for
working with the id4 system. what we need are
predictive probabilities in the form of eq. 1.

the hiero wfsas are determinised and min-
imised with epsilon removal under the tropical
semiring, and weights are pushed towards the ini-
tial state under the log semiring (mohri and riley,
2001). the resulting transducer is stochastic in the
log semiring, i.e. the log sum of the arc log prob-
abilities leaving a state is 0 (= log 1).
in addi-
tion, because the wfsa is deterministic, there is
a unique path leading to every state, which corre-
sponds to a unique hiero translation pre   x. sup-
pose a path to a state accepts the translation pre   x
yt   1
. an outgoing arc from that state with symbol
1
y has a weight that corresponds to the (negative
log of the) id155
phiero(yt = y|yt   1

, x).

(3)

1

this id155 is such that for a hi-
ero translation yt
1 = y(d) accepted by the wfsa

phiero(yt

1 ) =

phiero(yt|yt   1

1

, x)     s(d).

t=1

(4)
the hiero wfsas have been transformed so that
their arc weights have the negative log of the con-
ditional probabilities de   ned in eq. 3. all the
id203 mass of this distribution is concen-
trated on the hiero translation hypotheses. the
complete translation and language model scores
computed over the entire hiero translations are
pushed as far forward in the wfsas as possible.
this is commonly done for left-to-right decoding
in id103 (mohri et al., 2002).

t(cid:89)

2.2 id4   hiero decoding
as above, suppose a path to a state in the wfsa
accepts a hiero translation pre   x yt   1
, and let yt
be a symbol on an outgoing arc from that state. we
de   ne the joint id4+hiero score as

1

log p (yt|yt   1

, x) =

1

(cid:26)log pn m t (yt|yt   1

  hiero log phiero(yt|yt   1
, x) +
1
yt       n m t
, x)
log pn m t (unk|yt   1
, x) yt (cid:54)      n m t

1

1

  n m t

(5)

note that the id4-hiero decoder only con-
siders hypotheses in the hiero lattice. as dis-
cussed earlier, the hiero vocabulary can be much
larger than the id4 output vocabulary   n m t . if
a hiero translation contains a word not in the id4
vocabulary, the id4 model provides a score and
updates its decoder state as for an unknown word.
our decoding algorithm is a natural extension of
id125 decoding for id4. due to the form
of eq. 5 we can build up hypotheses from left-to-
right on the target side. thus, we can represent
1, hs) by a transla-
a partial hypothesis h = (yt
1 and an accumulated score hs. at
tion pre   x yt
each iteration we extend the current hypotheses by
one target token, until the best scoring hypothesis
reaches a    nal state of the hiero lattice. we re-
fer to this step as node expansion, and in sec. 3.1
we report the number of node expansions per sen-
tence, as an indication of computational cost.

we can think of the decoding algorithm as
breath-   rst search through the translation lattices
with a limited number of active hypotheses (a
beam). rescoring is done on-the-   y: as the de-
coder traverses an edge in the wfsa, we update
its weight by eq. 5. the output-synchronous char-

# sentences
# word tokens
# unique words
oov (hiero)
oov (id4)

train set
en
4.2m

de

dev set
en

de

6k

test set
en

de

2.7k

59k
106m 102m 138k 138k
647k 1.5m 13k
13k
20k
0.0% 0.0% 0.8% 1.6% 1.0% 2.0%
1.6% 5.5% 2.5% 7.5% 3.1% 8.8%
fr

62k
9k

en

en

fr

fr

en
12.1m

# sentences
# word tokens
# unique words 1.6m 1.7m 14k
oov (hiero)
oov (id4)

81k
11k
0.0% 0.0% 0.6% 0.6% 0.4% 0.4%
3.5% 3.8% 4.5% 5.3% 5.0% 5.3%

305m 348m 138k 155k
17k

71k
10k

6k

3k

table 1: parallel texts and vocabulary coverage on
news-test2014.

acteristic of id125 enables us to compute
the id4 posteriors only once for each history
based on previous calculations.

alternatively, we can think of the algorithm as
id4 decoding with revised posterior probabil-
ities:
instead of selecting the most likely sym-
bol yt according the id4 model, we adjust the
id4 posterior with the hiero posterior scores and
delete id4 entries that are not allowed by the lat-
tice. this may result in id4 choosing a different
symbol, which is then fed back to the neural net-
work for the next decoding step.

3 experimental evaluation

we evaluate sgid4 on the wmt news-test2014
test sets (the    ltered version) for english-german
(en-de) and english-french (en-fr). we also re-
port results on wmt news-test2015 en-de.

the en-de training set includes europarl v7,
common crawl, and news commentary v10. sen-
tence pairs with sentences longer than 80 words
or length ratios exceeding 2.4:1 were deleted, as
were common crawl sentences from other lan-
guages (shuyo, 2010). the en-fr id4 system
was trained on preprocessed data (schwenk, 2014)
used by previous work (sutskever et al., 2014;
bahdanau et al., 2015; jean et al., 2015a), but
with truecasing like our hiero baseline. follow-
ing (jean et al., 2015a), we use news-test2012 and
news-test2013 as a development set. the id4 vo-
cabulary size is 50k for en-de and 30k for en-fr,
taken as the most frequent words in training (jean
et al., 2015a). tab. 1 provides statistics and shows
the severity of the oov problem for id4.

the basic id4 system is built using the
blocks framework (van merri  enboer et al., 2015)
based on the theano library (bastien et al., 2012)
with standard hyper-parameters (bahdanau et al.,
2015): the encoder and decoder networks consist
of 1000 id149 (cho et al., 2014).
the decoder uses a single maxout (goodfellow et
al., 2013) output layer with the feed-forward at-
tention model (bahdanau et al., 2015).

the en-de hiero system uses rules which en-
courage verb movement (de gispert et al., 2010).
the rules for en-fr were extracted from the full
data set available at the wmt   15 website using a
shallow-1 grammar (de gispert et al., 2010). 5-
gram kneser-ney language models (kn-lm) for
the hiero systems were trained on wmt   15 par-
allel and monolingual data (hea   eld et al., 2013).

sgid4

(jean et al., 2015a, tab. 2)
setup
basic id4
id4-lv
+ unk replace
   
+ reshuf   e
+ ensemble

   
19.40
21.59

id7 setup
16.46
16.95 hiero
18.89 id4-hiero

basic id4

+ tuning
+ reshuf   e

id7
16.31
19.44
20.69
21.43
21.87

sgid4

(jean et al., 2015a, tab. 2)
setup
basic id4
id4-lv
+ unk replace
   
+ reshuf   e
+ ensemble

   
34.60
37.19

id7 setup
29.97
33.36 hiero
34.11 id4-hiero

basic id4

+ tuning
+ reshuf   e

id7
30.42
32.86
35.37
36.29
36.61

(a) english-german

(b) english-french

table 2: id7 scores on news-test2014 calculated with multi-id7.perl. id4-lv refers to the
id56search-lv model from (jean et al., 2015a) for large output vocabularies.

vocab.

id4 grammar kn-lm nplm
scores
scores

scores

scores

search
space
lattice
lattice

(cid:88)
(cid:88)

(cid:88)
(cid:88)

1
2
3 unrestricted
4
5
6
7
8
9
10
11
12
13
14
15
16 neural mt     umontreal-mila (jean et al., 2015b)

100-best
100-best
100-best
1000-best
1000-best
1000-best
lattice
lattice
lattice
lattice
lattice
lattice

hiero
hiero
id4
hiero
hiero
hiero
hiero
hiero
hiero
id4
hiero
hiero
hiero
hiero
hiero

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

# of node exp-
ansions per sen.
   
   
254.8
2,233.6
(dfs: 832.1)

21,686.2
(dfs: 6,221.8)
243.3
243.3
243.3
240.5
243.9
244.3

(cid:88)

(cid:88)

(cid:88)

(cid:88)

id7
(single)

id7

(ensemble)

21.1 (hiero)
21.7 (hiero)

19.5
22.8
22.9
22.9
23.3
23.4
23.5
20.3
23.0
23.0
23.4
23.4
24.0
22.8

21.8
23.3
23.4
23.3
23.8
23.9
24.0
21.4
24.2
24.2
24.5
24.4
24.4
25.2

table 3: id7 english-german news-test2015 scores calculated with mteval-v13a.pl.

our sgid4 system1 is built with the pyfst inter-
face 2 to openfst (allauzen et al., 2007).

3.1 sgid4 performance
tab. 2 compares our combined id4+hiero de-
coding with id4 results in the literature. we use
a beam size of 12. in en-de and in en-fr, we    nd
that our basic id4 system performs similarly
(within 0.5 id7) to previously published results
(16.31 vs. 16.46 and 30.42 vs. 29.97).

in id4-hiero, decoding is as described in
sec. 2.2, but with   hiero = 0. the decoder
searches through the hiero lattice, ignoring the
hiero scores, but using hiero word hypotheses in
place of any unks that might have been produced
by id4. the results show that id4-hiero is
much more effective in    xing id4 oovs than
the    unk replace    technique (luong et al., 2015);
this holds in both en-de and en-fr.

for the id4-hiero+tuning systems, lattice
mert (macherey et al., 2008) is used to optimise
  hiero and   n m t on the tuning sets. this yields
further gains in both en-fr and en-de, suggesting

1http://ucam-smt.github.io/sgid4/html/
2https://pyfst.github.io/

that in addition to    xing unks, the hiero predic-
tive posteriors can be used to improve the id4
translation model scores.

tab. 3 reports results of our en-de system with
reshuf   ing and tuning on news-test2015. id7
scores are directly comparable to wmt   15 re-
sults 3. by comparing row 3 to row 10, we see that
constraining id4 to the search space de   ned by
the hiero lattices yields an improvement of +0.8
id7 for single id4. if we allow hiero to    x
id4 unks, we see a further +2.7 id7 gain
(row 11). the majority of gains come from    x-
ing unks, but there is still improvement from the
constrained search space for single id4.

we next investigate the contribution of the hi-
ero system scores. we see that, once lattices
are generated, the kn-lm contributes more to
rescoring than the hiero grammar scores (rows 12-
14). further gains can be achieved by adding a
feed-forward neural language model with nplm
(vaswani et al., 2013) (row 15). we observe that
n-best list rescoring with id4 (neubig et al.,
2015) also outperforms both the hiero and id4

3http://matrix.statmt.org/matrix/systems list/1774

figure 1: performance with nplm over beam size
on english-german news-test2015. a beam of 12
corresponds to row 15 in tab. 3.

determini- minimi- weight
pushing

sation

sation

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

sentences
per second
2.51
1.57
1.47

table 4: time for lattice preprocessing operations
on english-german news-test2015.

baselines, although lattice rescoring gives the best
results (row 9 vs. row 15). lattice rescoring with
sgid4 also uses far fewer node expansions per
sentence. we report n-best rescoring speeds for
rescoring each hypothesis separately, and a depth-
   rst (dfs) scheme that ef   ciently traverses the n-
best lists. both these techniques are very slow
compared to lattice rescoring. fig. 1 shows that
we can reduce the beam size from 12 to 5 with
only a minor drop in id7. this is nearly 100
times faster than dfs over the 1000-best list.
cost of lattice preprocessing as described in
sec. 2.1, we applied determinisation, minimisa-
tion, and weight pushing to the hiero lattices in
order to work with probabilities. tab. 4 shows that
those operations are generally fast4.
lattice size for previous experiments we set
the hiero pruning parameters such that lattices had
8,510 nodes on average. fig. 2 plots the id7
score over the lattice size. we    nd that sgid4
works well on lattices of moderate or large size,
but pruning lattices too heavily has a negative ef-
fect as they are then too similar to hiero    rst best
hypotheses. we note that lattice rescoring involves
nearly as many node expansions as unconstrained
id4 decoding. this con   rms that the lattices at
8,510 nodes are already large enough for sgid4.

4testing environment: ubuntu 14.04, linux 3.13.0, single

intel r(cid:13) xeon r(cid:13) x5650 cpu at 2.67 ghz

figure 2: sgid4 performance over lattice size
on english-german news-test2015. 8,510 nodes
per lattice corresponds to row 14 in tab. 3.

local softmax
in sgid4 decoding we have
the option of normalising the id4 translation
probabilities over the words on outgoing words
from each state rather than over the full 50,000
words translation vocabulary. there are    4.5 arcs
per state in our en-de   14 lattices, and so avoiding
the full softmax could cause signi   cant computa-
tional savings. we    nd this leads to only a modest
0.5 id7 degradation: 21.45 id7 in en-de   14,
compared to 21.87 id7 using id4 probabili-
ties computed over the full vocabulary.

modelling errors vs. search errors
in our en-
de   14 experiments with   hiero = 0 we    nd
that constraining the id4 decoder to the hiero
lattices yields translation hypotheses with much
lower id4 probabilities than unconstrained ba-
sic id4 decoding: under the id4 model, id4
hypotheses are 8,300 times more likely (median)
than id4-hiero hypotheses. we conclude (ten-
tatively) that basic id4 is not suffering only
from search errors, but rather that id4-hiero
discards some hypotheses ranked highly by the
id4 model but lower in the evaluation metric.

4 conclusion

we have demonstrated a viable approach to syn-
tactically guided id4 for-
mulated to exploit the rich, structured search space
generated by hiero and the long-context transla-
tion scores of id4. sgid4 does not suffer from
the severe limitation in vocabulary size of basic
id4 and avoids any dif   culty of extending dis-
tributed word representations to new vocabulary
items not seen in training data.

acknowledgements

this work was supported in part by the u.k. en-
gineering and physical sciences research council
(epsrc grant ep/l027623/1).

references
cyril allauzen, michael riley, johan schalkwyk, wo-
jciech skut, and mehryar mohri. 2007. openfst: a
general and ef   cient weighted    nite-state transducer
in implementation and application of au-
library.
tomata, pages 11   23. springer.

cyril allauzen, bill byrne, de adri`a gispert, gonzalo
iglesias, and michael riley. 2014. pushdown au-
tomata in id151. volume 40,
issue 3 - september 2014, pages 687   723.

dzmitry bahdanau, kyunghyun cho, and yoshua ben-
gio. 2015. id4 by jointly
learning to align and translate. in iclr.

fr  ed  eric bastien, pascal lamblin, razvan pascanu,
james bergstra, ian goodfellow, arnaud bergeron,
nicolas bouchard, david warde-farley, and yoshua
bengio. 2012. theano: new features and speed im-
in deep learning and unsupervised
provements.
id171 nips 2012 workshop.

david chiang, kevin knight, and wei wang. 2009.
11,001 new features for statistical machine transla-
tion. in acl, pages 218   226.

david chiang. 2007. hierarchical phrase-based trans-
lation. computational linguistics, 33(2):201   228.

rohan chitnis and john denero. 2015. variable-
length word encodings for neural translation models.
in emnlp, pages 2088   2093.

kyunghyun cho, bart van merrienboer, caglar gul-
cehre, fethi bougares, holger schwenk, and yoshua
bengio.
2014. learning phrase representations
using id56 encoder-decoder for statistical machine
translation. in emnlp.

ronan collobert and jason weston. 2008. a uni   ed
architecture for natural language processing: deep
in pro-
neural networks with multitask learning.
ceedings of the 25th international conference on
machine learning, pages 160   167. acm.

adri`a de gispert, gonzalo iglesias, graeme black-
wood, eduardo r banga, and william byrne. 2010.
hierarchical phrase-based translation with weighted
   nite-state transducers and shallow-n grammars.
computational linguistics, 36(3):505   533.

ian goodfellow, david warde-farley, mehdi mirza,
aaron courville, and yoshua bengio. 2013. max-
out networks. in icml, pages 1319   1327.

kenneth hea   eld,

ivan pouzyrevsky, jonathan h.
clark, and philipp koehn. 2013. scalable modi   ed
in acl,
kneser-ney language model estimation.
pages 690   696.

gonzalo iglesias, adri`a de gispert, eduardo r banga,
and william byrne. 2009. hierarchical phrase-
based translation with weighted    nite state transduc-
ers. in naacl-hlt, pages 433   441.

gonzalo iglesias, cyril allauzen, william byrne,
adri`a de gispert, and michael riley. 2011. hier-
archical phrase-based translation representations. in
emnlp, pages 1373   1383.

s  ebastien jean, kyunghyun cho, roland memisevic,
and yoshua bengio. 2015a. on using very large
target vocabulary for id4. in
acl, pages 1   10.

s  ebastien jean, orhan firat, kyunghyun cho, roland
memisevic, and yoshua bengio. 2015b. montreal
id4 systems for wmt15. in
proceedings of the tenth workshop on statistical
machine translation, pages 134   140.

nal kalchbrenner and phil blunsom. 2013. recurrent
in emnlp, page

continuous translation models.
413.

jimmy lin and chris dyer. 2010. data-intensive text

processing with mapreduce. morgan &claypool.

minh-thang luong, ilya sutskever, quoc v le, oriol
vinyals, and wojciech zaremba. 2015. addressing
the rare word problem in id4.
in acl.

wolfgang macherey, franz josef och, ignacio thayer,
and jakob uszkoreit. 2008. lattice-based minimum
error rate training for id151.
in emnlp, pages 725   734.

yuval marton and philip resnik. 2008. soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. in acl, pages 1003   1011.

mehryar mohri and michael riley. 2001. a weight
large vocabulary speech

pushing algorithm for
recognition. in interspeech, pages 1603   1606.

mehryar mohri, fernando pereira, and michael ri-
2002. weighted    nite-state transducers in
ley.
id103. computer speech and lan-
guage, 16(1).

graham neubig, makoto morishita, and satoshi naka-
mura. 2015. neural reranking improves subjective
quality of machine translation: naist at wat2015.
in workshop on asian translation, pages 35   41.

holger schwenk.

2014.

universit du maine.

http://www-lium.univ-lemans.fr/
  schwenk/nid4-shared-task/.
accessed 1-march-2016].

[online;

rico sennrich, barry haddow, and alexandra birch.
2015. id4 of rare words with
subword units. arxiv preprint arxiv:1508.07909.

2010.

nakatani shuyo.

language detection li-
brary for java. http://code.google.com/
p/language-detection/.
[online; accessed
1-march-2016].

ilya sutskever, oriol vinyals, and quoc v le. 2014.
sequence to sequence learning with neural net-
works. in advances in neural information process-
ing systems, pages 3104   3112.

zhaopeng tu, zhengdong lu, yang liu, xiaohua liu,
and hang li. 2016. coverage-based neural machine
translation. arxiv preprint arxiv:1601.04811.

bart van merri  enboer, dzmitry bahdanau, vincent du-
moulin, dmitriy serdyuk, david warde-farley, jan
chorowski, and yoshua bengio. 2015. blocks and
fuel: frameworks for deep learning. arxiv preprint
arxiv:1506.00619.

ashish vaswani, yinggong zhao, victoria fossum, and
david chiang. 2013. decoding with large-scale
neural language models improves translation.
in
emnlp, pages 1387   1392.

ashish venugopal, andreas zollmann, noah a. smith,
and stephan vogel. 2009. preference grammars:
softening syntactic constraints to improve statistical
in naacl-hlt, pages 236   
machine translation.
244.

