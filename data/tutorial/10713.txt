greedy, joint syntactic-id29 with id200s

   school of computer science, carnegie mellon university, pittsburgh, pa 15213, usa

swabha swayamdipta    miguel ballesteros    chris dyer    noah a. smith   
   natural language processing group, universitat pompeu fabra, barcelona, spain

   google deepmind, london, uk

   computer science & engineering, university of washington, seattle, wa 98195, usa

8
1
0
2

 
l
u
j
 

5

 
 
]
l
c
.
s
c
[
 
 

2
v
4
5
9
8
0

.

6
0
6
1
:
v
i
x
r
a

swabha@cs.cmu.edu, miguel.ballesteros@upf.edu,

cdyer@cs.cmu.edu, nasmith@cs.washington.edu

abstract

we present a transition-based parser that
jointly produces syntactic and semantic
dependencies.
it learns a representation
of the entire algorithm state, using stack
long short-term memories. our greedy in-
ference algorithm has linear time, includ-
ing feature extraction. on the conll
2008   9 english shared tasks, we obtain
the best published parsing performance
among models that jointly learn syntax
and semantics.

introduction

1
we introduce a new joint syntactic and semantic
dependency parser. our parser draws from the
algorithmic insights of the incremental structure
building approach of henderson et al. (2008), with
two key differences. first, it learns representations
for the parser   s entire algorithmic state, not just the
top items on the stack or the most recent parser
states; in fact, it uses no expert-crafted features at
all. second, it uses entirely greedy id136 rather
than id125. we    nd that it outperforms all
previous joint parsing models, including hender-
son et al. (2008) and variants (gesmundo et al.,
2009; titov et al., 2009; henderson et al., 2013) on
the conll 2008 and 2009 (english) shared tasks.
our parser   s multilingual results are comparable to
the top systems at conll 2009.

joint models like ours have frequently been pro-
posed as a way to avoid cascading errors in nlp
pipelines; varying degrees of success have been at-
tained for a range of joint syntactic-semantic anal-
ysis tasks (sutton and mccallum, 2005; hender-
son et al., 2008; toutanova et al., 2008; johansson,
2009; llu    s et al., 2013, inter alia).

one reason pipelines often dominate is that they
make available the complete syntactic parse tree,

and arbitrarily-scoped syntactic features   such as
the    path    between predicate and argument, pro-
posed by gildea and jurafsky (2002)   for seman-
tic analysis. such features are a mainstay of high-
performance id14 (srl) sys-
tems (roth and woodsend, 2014; lei et al., 2015;
fitzgerald et al., 2015; foland and martin, 2015),
but they are expensive to extract (johansson, 2009;
he et al., 2013).

this study shows how recent advances in repre-
sentation learning can bypass those expensive fea-
tures, discovering cheap alternatives available dur-
ing a greedy parsing procedure. the speci   c ad-
vance we employ is the id200 (dyer et al.,
2015), a neural network that continuously summa-
rizes the contents of the stack data structures in
which a transition-based parser   s state is conven-
tionally encoded. id200s were shown to ob-
viate many features used in syntactic dependency
parsing; here we    nd them to do the same for joint
syntactic-semantic id33.

we believe this is an especially important    nd-
ing for greedy models that cast parsing as a se-
quence of decisions made based on algorithmic
state, where linguistic theory and researcher intu-
itions offer less guidance in feature design.

our system   s performance does not match that
of the top expert-crafted feature-based systems
(zhao et al., 2009; bj  orkelund et al., 2010; roth
and woodsend, 2014; lei et al., 2015), systems
which perform optimal decoding (t  ackstr  om et
al., 2015), or of systems that exploit additional,
differently-annotated datasets (fitzgerald et al.,
2015). many advances in those systems are or-
thogonal to our model, and we expect future work
to achieve further gains by integrating them.

because our system is very fast    with an
end-to-end runtime of 177.6  18 seconds to parse
the conll 2009 english test data on a single
core   we believe it will be useful in practical set-

root

sbj

vc

oprd

im

tmp

all are expected to reopen soon

expect.01

reopen.01

a1

c-a1

a1

am-tmp

figure 1: example of a joint parse. syntactic de-
pendencies are shown by arcs above the sentence
and semantic dependencies below; predicates are
marked in boldface. c- denotes continuation of
argument a1. correspondences between depen-
dencies might be close (between expected and to)
or not (between reopen and all).

tings. our open-source implementation has been
released.1

2

joint syntactic and semantic
id33

we largely follow the transition-based, synchro-
nized algorithm of henderson et al. (2013) to pre-
dict joint parse structures. the input to the algo-
rithm is a sentence annotated with part-of-speech
tags. the output consists of a labeled syntactic de-
pendency tree and a directed srl graph, in which
a subset of words in the sentence are selected as
predicates, disambiguated to a sense, and linked
by labeled, directed edges to their semantic argu-
ments. figure 1 shows an example.

2.1 transition-based procedure
the two parses are constructed in a bottom-up
fashion,
incrementally processing words in the
sentence from left to right. the state of the pars-
ing algorithm at timestep t is represented by three
stack data structures: a syntactic stack st, a se-
mantic stack mt   each containing partially built
structures   and a buffer of input words bt. our
algorithm also places partial syntactic and seman-
tic parse structures onto the front of the buffer,
so it is also implemented as a stack. each arc in
the output corresponds to a transition (or    action   )
chosen based on the current state; every transition
modi   es the state by updating st, mt, and bt to
st+1, mt+1, and bt+1, respectively. while each
state may license several valid actions, each action

1https://github.com/clab/

joint-lstm-parser

has a deterministic effect on the state of the algo-
rithm.

initially, s0 and m0 are empty, and b0 contains
the input sentence with the    rst word at the front
of b and a special root symbol at the end.2 execu-
tion ends on iteration t such that bt is empty and
st and mt contain only a single structure headed
by root.

of b and pushes it on s.

2.2 transitions for joint parsing
there are separate sets of syntactic and semantic
transitions; the former manipulate s and b, the
latter m and b. all are formally de   ned in ta-
ble 1. the syntactic transitions are from the    arc-
eager    algorithm of nivre (2008). they include:
    s-shift, which copies3 an item from the front
    s-reduce pops an item from s.
    s-right((cid:96)) creates a syntactic dependency.
let u be the element at the top of s and v be
the element at the front of b. the new depen-
dency has u as head, v as dependent, and label
(cid:96). u is popped off s, and the resulting structure,
rooted at u, is pushed on s. finally, v is copied
to the top of s.
    s-left((cid:96)) creates a syntactic dependency with
label (cid:96) in the reverse direction as s-right. the
top of s, u, is popped. the front of b, v, is
replaced by the new structure, rooted at v.

the semantic transitions are similar, operating

and pushes it on m.

on the semantic stack.
    m-shift removes an item from the front of b
    m-reduce pops an item from m.
    m-right(r) creates a semantic dependency.
let u be the element at the top of m and v,
the front of b. the new dependency has u as
head, v as dependent, and label r. u is popped
off m, and the resulting structure, rooted at u,
is pushed on m.
    m-left(r) creates a semantic dependency
with label r in the reverse direction as m-
right. the buffer front, v, is replaced by the
new v-rooted structure. m remains unchanged.

2this works better for the arc-eager algorithm (balles-
teros and nivre, 2013), in contrast to henderson et al. (2013),
who initialized with root at the buffer front.

3note that in the original arc-eager algorithm (nivre,
2008), shift and right-arc actions move the item on the
buffer front to the stack, whereas we only copy it (to allow
the semantic operations to have access to it).

   ed and the algorithm returns to syntactic transi-
tions. this implies that, for each word, its left-
side syntactic dependencies are resolved before its
left-side semantic dependencies. an example run
of the algorithm is shown in figure 3.

2.3 constraints on transitions
to ensure that the parser never enters an invalid
state, the sequence of transitions is constrained,
following henderson et al. (2013). actions that
copy or move items from the buffer (s-shift,
s-right and m-shift) are forbidden when the
buffer is empty. actions that pop from a stack
(s-reduce and m-reduce) are forbidden when
that stack is empty. we disallow actions corre-
sponding to the same dependency, or the same
predicate to be repeated in the sequence. repet-
itive m-swap transitions are disallowed to avoid
in   nite swapping. finally, as noted above, we re-
strict the parser to syntactic actions until it needs
to shift an item from b to s, after which it can
only execute semantic actions until it executes an
m-shift.

asymptotic runtime complexity of this greedy
algorithm is linear in the length of the input, fol-
lowing the analysis by nivre (2009).5

3 statistical model
the transitions in   2 describe the execution paths
our algorithm can take; like past work, we apply
a statistical classi   er to decide which transition to
take at each timestep, given the current state. the
novelty of our model is that it learns a    nite-length
vector representation of the entire joint parser   s
state (s, m, and b) in order to make this decision.

3.1 stack long short-term memory (lstm)
lstms are recurrent neural networks equipped
with specialized memory components in addition
to a hidden state (hochreiter and schmidhuber,
1997; graves, 2013) to model sequences. stack
lstms (dyer et al., 2015) are lstms that al-
low for stack operations: query, push, and pop.
a    stack pointer    is maintained which determines
which cell in the lstm provides the memory and
hidden units when computing the new memory
cell contents. query provides a summary of the
stack in a single    xed-length vector. push adds

semantic transitions, hence we only copy it.

5the analysis in (nivre, 2009) does not consider swap
actions. however, since we constrain the number of such ac-
tions, the linear time complexity of the algorithm stays intact.

figure 2: example of an srl graph with an arc
from predicate problem.01 to itself,    lling the a2
role. our self(a2) transition allows recovering
this semantic dependency.

because srl graphs allow a node to be a se-
mantic argument of two parents   like all in the
example in figure 1   m-left and m-right
do not remove the dependent from the semantic
stack and buffer respectively, unlike their syntactic
equivalents, s-left and s-right. we use two
other semantic transitions from henderson et al.
(2013) which have no syntactic analogues:
    m-swap swaps the top two items on m, to al-
    m-pred(p) marks the item at the front of b
as a semantic predicate with the sense p, and
replaces it with the disambiguated predicate.

low for crossing semantic arcs.

the conll 2009 corpus introduces semantic
self-dependencies where many nominal predicates
(from nombank) are marked as their own argu-
ments; these account for 6.68% of all semantic
arcs in the english corpus. an example involving
an eventive noun is shown in figure 2. we intro-
duce a new semantic transition, not in henderson
et al. (2013), to handle such cases:
    m-self(r) adds a dependency, with label r be-
tween the item at the front of b and itself. the
result replaces the item at the front of b.

note that the syntactic and semantic transitions
both operate on the same buffer, though they in-
dependently specify the syntax and semantics, re-
spectively. in order to ensure that both syntactic
and semantic parses are produced, the syntactic
and semantic transitions are interleaved. only syn-
tactic transitions are considered until a transition is
chosen that copies an item from the buffer front to
the syntactic stack (either s-shift or s-right).
the algorithm then switches to semantic transi-
tions until a buffer-modifying transition is taken
(m-shift).4 at this point, the buffer is modi-

4had we moved the item at the buffer front during the
syntactic transitions, it would have been unavailable for the

andhasothercongresshave.03problem.01a2problemsa1am-disa1a0st
s

(u, u), s

(u, u), s

(u, u), s

s
s
s
s
s
s
s

mt
m
m

m

m
m

(u, u), m
(u, u), m
(u, u), m

bt

(v, v), b

b

(v, v), b

(v, v), b
(v, v), b

b

(v, v), b
(v, v), b

(u, u), (v, v), m

b

m
m

(v, v), b
(v, v), b

action
s-shift
s-reduce
s-right((cid:96))
s-left((cid:96))
m-shift
m-reduce
m-right(r)
m-left(r)
m-swap
m-pred(p)
m-self(r)

st+1

(v, v), s

s

mt+1
m
m

(v, v), (gs(u, v, l), u), s

m

bt+1

(v, v), b

b

(v, v), b

s
s
s
s
s
s
s
s

m (gs(v, u, l), v), b

(v, v), m
m
(gm(u, v, r), u), m

b
b

m     u r    v
(u, u), m (gm(v, u, r), v), b m     u r    v

(v, v), b

(v, v), (u, u), m
m
m (gm(v, v, r), v), b m     v r    v

(gd(v, p), v), b

b

   
   

dependency

   
   

s     u (cid:96)    v
s     u (cid:96)    v

   
   

table 1: parser transitions along with the modi   cations to the stacks and the buffer resulting from each.
syntactic transitions are shown above, semantic below. italic symbols denote symbolic representations
of words and relations, and bold symbols indicate (learned) embeddings (  3.5) of words and relations;
each element in a stack or buffer includes both symbolic and vector representations, either atomic or
recursive. s represents the set of syntactic transitions, and m the set of semantic transitions.

an element to the top of the stack, resulting in a
new summary. pop, which does not correspond to
a conventional lstm operation, moves the stack
pointer to the preceding timestep, resulting in a
stack summary as it was before the popped item
was observed. implementation details (dyer et al.,
2015; goldberg, 2015) and code have been made
publicly available.6

using id200s, we construct a represen-
tation of the algorithm state by decomposing it
into smaller pieces that are combined by recursive
function evaluations (similar to the way a list is
built by a concatenate operation that operates on a
list and an element). this enables information that
would be distant from the    top    of the stack to be
carried forward, potentially helping the learner.

four

3.2 id200s for joint parsing
our algorithm employs
id200s,
one each for the s, m, and b data struc-
tures.like dyer et al. (2015), we use a fourth stack
lstm, a, for the history of actions   a is never
popped from, only pushed to. figure 4 illustrates
the architecture. the algorithm   s state at timestep
t is encoded by the four vectors summarizing the
four id200s, and this is the input to the clas-
si   er that chooses among the allowable transitions
at that timestep.
let st, mt, bt, and at denote the summaries
of st, mt, bt, and at, respectively. let at =
allowed(st, mt, bt, at) denote the allowed tran-
sitions given the current stacks and buffer. the
parser state at time t is given by a recti   ed linear
unit (nair and hinton, 2010) in vector yt:
yt = elementwisemax{0, d + w[st; mt; bt; at]}

6https://github.com/clab/lstm-parser

where w and d are the parameters of the classi-
   er. the transition selected at timestep t is

arg max
     at
    arg max
     at

q   +         yt

(1)

score(   ; st, mt, bt, at)

where      and q   are parameters for each transi-
tion type   . note that only allowed transitions are
considered in the decision rule (see   2.3).

3.3 composition functions
to use id200s, we require vector representa-
tions of the elements that are stored in the stacks.
speci   cally, we require vector representations of
atoms (words, possibly with part-of-speech tags)
and parse fragments. word vectors can be pre-
trained or learned directly; we consider a concate-
nation of both in our experiments; part-of-speech
vectors are learned and concatenated to the same.
to obtain vector representations of parse frag-
ments, we use neural networks which recursively
compute representations of the complex structured
output (dyer et al., 2015). the tree structures here
are always ternary trees, with each internal node   s
three children including a head, a dependent, and
a label. the vectors for leaves are word vectors
and vectors corresponding to syntactic and seman-
tic relation types.

the vector for an internal node is a squashed
(tanh) af   ne transformation of its children   s vec-
tors. for syntactic and semantic attachments, re-
spectively, the composition function is:

gs(v, u, l) = tanh(zs[v; u; l] + es)
gm(v, u, r) = tanh(zm[v; u; r] + em)

(2)
(3)

d
e
t
c
e
p
x
e

c
v

   
   
e
r
a

   

1
0

.
t
c
e
p
x
e
   
1
   
a

   

   

l
l
a

o
t

-

1
   
a
   
c
1
0

o
t

d
r
p
o

   
   
d
e
t
c
e
p
x
e

   

.
t
c
e
p
x
e

   

i

   
m
   
o
t

   

n
o
o
s

n
o
o
s

p
m
   
t
   
-
m
a

t

   
p
m
   
n
e
p
o
e
r

t

o
o
r

   
   

t
o
o
r

   

   

   

   

   

   

e
r
a

   

   

   

.

1
0
n
e
p
o
e
r

   

1
0

.

n
e
p
o
e
r

   
1
   
a

   

   

l
l
a

   

n
e
p
o
e
r

y
c
n
e
d
n
e
p
e
d

b

e
r
a
   
   

j
b
s

   

   

   

l
l
a

   

]
t
o
o
r

]
t
o
o
r

,

n
o
o
s

,

n
o
o
s

,

n
e
p
o
e
r

,

n
e
p
o
e
r

,

o
t

,

o
t

,

d
e
t
c
e
p
x
e

,

d
e
t
c
e
p
x
e

,
e
r
a

,
e
r
a

,
l
l
a
[

,
l
l
a
[

]
t
o
o
r

]
t
o
o
r

]
t
o
o
r

,

n
o
o
s

,

n
e
p
o
e
r

,

o
t

,

d
e
t
c
e
p
x
e

,
e
r
a
[

,

n
o
o
s

,

n
o
o
s

,

n
e
p
o
e
r

,

n
e
p
o
e
r

,

o
t

,

o
t

,

d
e
t
c
e
p
x
e

,

d
e
t
c
e
p
x
e

,
e
r
a
[

,
e
r
a
[

]
t
o
o
r

]
t
o
o
r

]
t
o
o
r

]
t
o
o
r

]
t
o
o
r

,

n
o
o
s

,

n
e
p
o
e
r

,

n
o
o
s

,

n
o
o
s

,

n
o
o
s

,

n
e
p
o
e
r

,

n
e
p
o
e
r

,

n
e
p
o
e
r

,

o
t

,

o
t

,

o
t

,

o
t

,

d
e
t
c
e
p
x
e
[

,

d
e
t
c
e
p
x
e
[

,

d
e
t
c
e
p
x
e
[

,

d
e
t
c
e
p
x
e
[

,

n
o
o
s

,

n
e
p
o
e
r

,

o
t

,

d
e
t
c
e
p
x
e
[

]
e
r
a

]
e
r
a

]
e
r
a

]
t
o
o
r

,

n
o
o
s

,

n
e
p
o
e
r

,

o
t
[

]
d
e
t
c
e
p
x
e

m

]
[

]
[

]
l
l
a
[

]
l
l
a
[

]
l
l
a
[

,
l
l
a
[

,
l
l
a
[

,
l
l
a
[

]
l
l
a
[

]
l
l
a
[

,
l
l
a
[

]
d
e
t
c
e
p
x
e

]
d
e
t
c
e
p
x
e

]
d
e
t
c
e
p
x
e

]
d
e
t
c
e
p
x
e

]
d
e
t
c
e
p
x
e

]
l
l
a
[

]
l
l
a
[

]
e
r
a
[

]
e
r
a
[

,
e
r
a
[

,
e
r
a
[

,
e
r
a
[

,
e
r
a
[

,
e
r
a
[

]
[

s

]
[

)
1
0
.
t
c
e
p
x
e
(
d
e
r
p
-
m

)
c
v
(
t
h
g

i

r

-
s

e
c
u
d
e
r
m

-

)
1
a
(
t
f
e
l
-
m

t
f
i

h
s
-
m

n
o
i
t
i
s
n
a
r
t

)
j
b
s
(
t
f
e
l
-
s

t
f
i

h
s
-
s

t
f
i

h
s
-
m

t
f
i

h
s
-
s

t
f
i

h
s
-
m

]
t
o
o
r

,

n
o
o
s

,

n
e
p
o
e
r

,

o
t
[

]
d
e
t
c
e
p
x
e

,
l
l
a
[

]
o
t

,
d
e
t
c
e
p
x
e

,
e
r
a
[

)
d
r
p
o
(
t
h
g

i

r

-
s
*
*
*

]
t
o
o
r

]
t
o
o
r

,

n
o
o
s

,

n
o
o
s

,

n
e
p
o
e
r

,

n
e
p
o
e
r

,

o
t
[

,

o
t
[

]
t
o
o
r

,

n
o
o
s

,

n
e
p
o
e
r
[

]
t
o
o
r

]
t
o
o
r

]
t
o
o
r

]
t
o
o
r

]
t
o
o
r

,

n
o
o
s

,

n
o
o
s

,

n
o
o
s

,

n
e
p
o
e
r
[

,

n
e
p
o
e
r
[

,

n
e
p
o
e
r
[

,

n
o
o
s

,

n
o
o
s

,

n
e
p
o
e
r
[

,

n
e
p
o
e
r
[

]
t
o
o
r

,

n
o
o
s
[

]
t
o
o
r

,

n
o
o
s
[

]
t
o
o
r

]
t
o
o
r

,

n
o
o
s
[

,

n
o
o
s
[

]
t
o
o
r
[

]
t
o
o
r
[

]
t
o
o
r
[

]
t
o
o
r
[

]
t
o
o
r
[

]
t
o
o
r
[

]
t
o
o
r
[

]
t
o
o
r
[

]
[

]
d
e
t
c
e
p
x
e

]
o
t

]
o
t

]
o
t

,
l
l
a
[

]
l
l
a
[

,
l
l
a
[

,
l
l
a
[

,
l
l
a
[

]
l
l
a
[

]
l
l
a
[

]
[

]
n
e
p
o
e
r
[

]
n
e
p
o
e
r
[

]
n
e
p
o
e
r
[

]
n
o
o
s
[

]
n
o
o
s
[

]
n
o
o
s
[

]
n
o
o
s
[

]
n
o
o
s
[

]
n
o
o
s
[

]
n
o
o
s
[

]
t
o
o
r
[

]
[

]
[

]
n
e
p
o
e
r

]
n
e
p
o
e
r

]
n
e
p
o
e
r

]
n
e
p
o
e
r

]
n
e
p
o
e
r

]
n
e
p
o
e
r

,
o
t

,
o
t

,
o
t

,
o
t

,
o
t

,
o
t

,
d
e
t
c
e
p
x
e

,
d
e
t
c
e
p
x
e

,
d
e
t
c
e
p
x
e

,
d
e
t
c
e
p
x
e

,
d
e
t
c
e
p
x
e

,
d
e
t
c
e
p
x
e

]
o
t

]
o
t

]
o
t

,
d
e
t
c
e
p
x
e

,
d
e
t
c
e
p
x
e

,
d
e
t
c
e
p
x
e

,
e
r
a
[

,
e
r
a
[

,
e
r
a
[

,
e
r
a
[

,
e
r
a
[

,
e
r
a
[

,
e
r
a
[

,
e
r
a
[

,
e
r
a
[

)
1
0
.
n
e
p
o
e
r
(
d
e
r
p
-
m

)

m

i
(
t
h
g

i

r

-
s

)
1
a
-
c

(
t
h
g

i

-

r
m

e
c
u
d
e
r
m

-

t
f
i

h
s
-
m

e
c
u
d
e
r
m

-

)
1
a
(
t
f
e
l
-
m

e
c
u
d
e
r
m

-

t
f
i

h
s
-
m

]
n
o
o
s

,
n
e
p
o
e
r

,
o
t

,
d
e
t
c
e
p
x
e

,
e
r
a
[

)
p
m

t
(
t
h
g

i

r

-
s

]
n
o
o
s

]
n
o
o
s

]
n
o
o
s

,
n
e
p
o
e
r

,
n
e
p
o
e
r

,
n
e
p
o
e
r

]
n
e
p
o
e
r

,
o
t

,
o
t

,
o
t

,
o
t

]
o
t

,
d
e
t
c
e
p
x
e

,
d
e
t
c
e
p
x
e

,
d
e
t
c
e
p
x
e

,
d
e
t
c
e
p
x
e

,
d
e
t
c
e
p
x
e

]
d
e
t
c
e
p
x
e

,
e
r
a
[

,
e
r
a
[

,
e
r
a
[

,
e
r
a
[

,
e
r
a
[

,
e
r
a
[

]
e
r
a
[

]
[

]
t
o
o
r
[

]
t
o
o
r
[

]
t
o
o
r
[

)
p
m
t
-
m
a
(
t
h
g

i

-

r
m

e
c
u
d
e
r
m

-

e
c
u
d
e
r

e
c
u
d
e
r

e
c
u
d
e
r

e
c
u
d
e
r

-
s

-
s

-
s

-
s

t
f
i

h
s
-
m

)
t
o
o
r
(
t
f
e
l
-
s

e
c
u
d
e
r
m

-

t
f
i

h
s
-
m

t
f
i

h
s
-
s

figure 3: joint parser transition sequence for the sentence in figure 1,    all are expected to reopen soon.   
syntactic labels are in lower-case and semantic role labels are capitalized. *** marks the operation
predicted in figure 4.

where v and u are vectors corresponding to atomic
words or composed parse fragments; l and r are
learned vector representations for syntactic and se-
mantic labels respectively. syntactic and semantic
parameters are separated (zs, es and zm, em, re-
spectively).

finally, for predicates, we use another recur-
sive function to compose the word representa-

tion, v with a learned representation for the dis-
mabiguated sense of the predicate, p:

gd(v, p) = tanh(zd[v; p] + ed)

(4)

where zd and ed are parameters of the model.
note that, because syntactic and semantic transi-
tions are interleaved, the fragmented structures are
a blend of syntactic and semantic compositions.

figure 4: id200 for joint parsing. the state
illustrated corresponds to the ***-marked row in
the example transition sequence in fig. 3.

figure 5 shows an example.

3.4 training
training the classi   er requires transforming each
training instance (a joint parse) into a transition se-
quence, a deterministic operation under our tran-
sition set. given a collection of algorithm states
at time t and correct classi   cation decisions   t, we
minimize the sum of log-loss terms, given (for one
timestep) by:

    log

(cid:80)

exp(q  t +     t    yt)
  (cid:48)   at

exp(q  (cid:48) +     (cid:48)    yt)

(5)

with respect to the classi   er and lstm parame-
ters. note that the loss is differentiable with re-
spect to the parameters; gradients are calculated
using id26. we apply stochastic gra-
dient descent with dropout for all neural network
parameters.

3.5 pretrained embeddings
following dyer et al. (2015),    structured skip-
gram    embeddings (ling et al., 2015) were used,
trained on the english (afp section), german,
spanish and chinese gigaword corpora, with a
window of size 5; training was stopped after 5
epochs. for out-of-vocabulary words, a randomly
initialized vector of the same dimension was used.

3.6 predicate sense disambiguation
predicate sense disambiguation is handled within
the model (m-pred transitions), but since senses

figure 5: example of a joint parse tree fragment
with vector representations shown at each node.
the vectors are obtained by recursive composition
of representations of head, dependent, and label
vectors. syntactic dependencies and labels are in
green, semantic in blue.

are lexeme-speci   c, we need a way to handle un-
seen predicates at test time. when a predicate is
encountered at test time that was not observed in
training, our system constructs a predicate from
the predicted lemma of the word at that position
and defaults to the    01    sense, which is correct
for 91.22% of predicates by type in the english
conll 2009 training data.

4 experimental setup

our model is evaluated on the conll shared
tasks on joint syntactic and semantic dependency
parsing in 2008 (surdeanu et al., 2008) and
2009 (haji  c et al., 2009). the standard training,
development and test splits of all datasets were
used. per the shared task guidelines, automati-
cally predicted pos tags and lemmas provided in
the datasets were used for all experiments. as
a preprocessing step, pseudo-projectivization of
the syntactic trees (nivre et al., 2007) was used,
which allowed an accurate conversion of even the
non-projective syntactic trees into syntactic transi-
tions. however, the oracle conversion of semantic
parses into transitions is not perfect despite using
the m-swap action, due to the presence of multi-
ple crossing arcs.7

the standard id74 include the
syntactic labeled attachment score (las), the se-

7for 1.5% of english sentences in the conll 2009 en-
glish dataset, the transition sequence incorrectly encodes the
gold-standard joint parse; details in henderson et al. (2013).

rootsoonreopentoallareexpectedexpect.01allallsbja1mbss-right (oprd)...m-pred(expect.01)m-reducem-left(a1)aarevcm-shiftburnthefeelnmoddobja1thebuid56modtheburnfeeldobjnmodtheburnfeeldobjnmoda1feel.01feel.01feelfeel.01mantic f1 score on both in-domain (wsj) and out-
of-domain (brown corpus) data, and their macro
average (macro f1) to score joint systems. be-
cause the task was de   ned somewhat differently
in each year, each dataset is considered in turn.

4.1 conll 2008
the conll 2008 dataset contains annotations
from the id32 (marcus et al., 1993),
propbank (palmer et al., 2005) and nom-
bank (meyers et al., 2004). the shared task evalu-
ated systems on predicate identi   cation in addition
to predicate sense disambiguation and srl.

to identify predicates, we trained a zero-
markov order bidirectional lstm two-class clas-
si   er. as input to the classi   er, we use learned rep-
resentations of word lemmas and pos tags. this
model achieves an f1 score of 91.43% on marking
words as predicates (or not).

hyperparameters the input representation for
a word consists of pretrained embeddings (size
100 for english, 80 for chinese, 64 for ger-
man and spanish), concatenated with additional
learned word and pos tag embeddings (size 32
and 12, respectively). learned embeddings for
syntactic and semantic arc labels are of size 20
and predicates 100. two-layer lstms with hid-
den state dimension 100 were used for each of the
four stacks. the parser state yt and the composi-
tion function g are of dimension 100. a dropout
rate of 0.2 (zaremba et al., 2014) was used on all
layers at training time, tuned on the development
data from the set of values {0.1, 0.2, 0.3, 1.0}. the
learned representations for actions are of size 100,
similarly tuned from {10, 20, 30, 40, 100}. other
hyperparameters have been set intuitively; careful
tuning is expected to yield improvements (weiss
et al., 2015).

an initial learning rate of 0.1 for stochastic gra-
dient descent was used and updated in every train-
ing epoch with a decay rate of 0.1 (dyer et al.,
2015). training is stopped when the development
performance does not improve for approximately
6   7 hours of elapsed time. experiments were run
on a single thread on a cpu, with memory require-
ments of up to 512 mb.

4.2 conll 2009
relative to the conll 2008 task (above), the
main change in 2009 is that predicates are pre-
identi   ed, and systems are only evaluated on pred-

icate sense disambiguation (not
identi   cation).
hence, the bidirectional lstm classi   er is not
used here. the preprocessing for projectivity, and
the hyperparameter selection is the same as in
  4.1.

in addition to the joint approach described in
the preceding sections, we experiment here with
several variants:
semantics-only:
the set of syntactic transitions
s, the syntactic stack s, and the syntactic compo-
sition function gs are discarded. as a result, the set
of constraints on transitions is a subset of the full
set of constraints in   2.3. effectively, this model
does not use any syntactic features, similar to col-
lobert et al. (2011) and zhou and xu (2015). it
provides a controlled test of the bene   t of explicit
syntax in a semantic parser.
all semantic transitions in m, the
syntax-only:
semantic stack m, and the semantic composition
function gm are discarded. s-shift and s-right
now move the item from the front of the buffer to
the syntactic stack, instead of copying. the set of
constraints on the transitions is again a subset of
the full set of constraints. this model is an arc-
eager variant of dyer et al. (2015), and serves to
check whether id29 degrades syntac-
tic performance.
hybrid:
the semantics parameters are trained
using automatically predicted syntax from the
syntax-only model. at test time, only seman-
tic parses are predicted. this setup bears simi-
larity to other approaches which pipeline syntax
and semantics, extracting features from the syn-
tactic parse to help srl. however, unlike other
approaches, this model does not offer the entire
syntactic tree for feature extraction, since only the
partial syntactic structures present on the syntactic
stack (and potentially the buffer) are visible at a
given timestep. this model helps show the effect
of joint prediction.

5 results and discussion
conll 2008 (table 2) our joint model signif-
icantly outperforms the joint model of hender-
son et al. (2008), from which our set of tran-
sitions is derived, showing the bene   t of learn-
ing a representation for the entire algorithmic
state. several other joint learning models have
been proposed (llu    s and m`arquez, 2008; jo-
hansson, 2009; titov et al., 2009) for the same

model

joint models:
llu    s and m`arquez (2008)
henderson et al. (2008)
johansson (2009)
titov et al. (2009)
conll 2008 best:
#3: zhao and kit (2008)
#2: che et al. (2008)
#2: ciaramita et al. (2008)
#1: j&n (2008)
joint (this work)

las

sem. macro
f1

f1

85.8
87.6
86.6
87.5

87.7
86.7
87.4
89.3
89.1

70.3
73.1
77.1
76.1

76.7
78.5
78.0
81.6
80.5

78.1
80.5
81.8
81.8

82.2
82.7
82.7
85.5
84.9

table 2: joint parsers: comparison on the conll
2008 test (wsj+brown) set.

task; our joint model surpasses the performance
of all these models. the best reported systems on
the conll 2008 task are due to johansson and
nugues (2008), che et al. (2008), ciaramita et
al. (2008) and zhao and kit (2008), all of which
pipeline syntax and semantics; our system   s se-
mantic and overall performance is comparable to
these. we fall behind only johansson and nugues
(2008), whose success was attributed to carefully
designed global srl features integrated into a
pipeline of classi   ers, making them asymptoti-
cally slower.

conll 2009 english (table 3) all of our
models (syntax-only, semantics-only, hybrid and
joint) improve over gesmundo et al.
(2009)
and henderson et al. (2013), demonstrating the
bene   t of our entire-parser-state representation
learner compared to the more locally scoped
model.

given that syntax has consistently proven useful
in srl, we expected our semantics-only model
to underperform hybrid and joint, and it did. in
the training domain, syntax and semantics bene-
   t each other (joint outperforms hybrid). out-
of-domain (the brown test set), the hybrid pulls
ahead, a sign that joint over   ts to wsj. as a
syntactic parser, our syntax-only model performs
slightly better than dyer et al. (2015), who achieve
89.56 las on this task.
joint parsing is very
slightly better still.

the overall performance of joint is on par with
the other winning participants at the conll 2009
shared task (zhao et al., 2009; che et al., 2009;
gesmundo et al., 2009), falling behind only zhao
et al. (2009), who carefully designed language-

speci   c features and used a series of pipelines for
the joint task, resulting in an accurate but compu-
tationally expensive system.

state-of-the-art srl systems (shown in the last
block of table 3) which use advances orthog-
onal to the contributions in this paper, perform
better than our models. many of these systems
use expert-crafted features derived from full syn-
tactic parses in a pipeline of classi   ers followed
by a global reranker (bj  orkelund et al., 2009;
bj  orkelund et al., 2010; roth and woodsend,
2014); we have not used these features or rerank-
ing. lei et al. (2015) use syntactic parses to obtain
interaction features between predicates and their
arguments and then compress feature representa-
tions using a low-rank tensor. t  ackstr  om et al.
(2015) present an exact id136 algorithm for
srl based on id145 and their lo-
cal and structured models make use of many syn-
tactic features from a pipeline; our search pro-
cedure is greedy. their algorithm is adopted
by fitzgerald et al. (2015) for id136 in a model
that jointly learns representations from a combina-
tion of propbank and framenet annotations; we
have not experimented with extra annotations.

our system achieves an end-to-end runtime of
177.6  18 seconds to parse the conll 2009 en-
glish test set on a single core. this is almost 2.5
times faster than the pipeline model of lei et al.
(2015) (439.9  42 seconds) on the same machine.8

conll 2009 multilingual (table 4) we tested
the joint model on the non-english conll 2009
datasets, and the results demonstrate that it adapts
easily   it is on par with the top three systems in
most cases. we note that our chinese parser relies
on pretrained id27s for its superior
performance; without them (not shown), it was on
par with the others. japanese is a small-data case
(4,393 training examples), illustrating our model   s
dependence on reasonably large training datasets.
we have not extended our model to incorporate
morphological features, which are used by the sys-
tems to which we compare. future work might in-
corporate morphological features where available;
this could potentially improve performance, espe-
cially in highly in   ective languages like czech.
an alternative might be to infer word-internal rep-
resentations using character-based word embed-

8see

srlparser; unlike other state-of-the-art systems,
one is publicly available.

https://github.com/taolei87/
this

model

conll   09 best:
#3 g+    09
#2 c+    09
#1 z+    09a
this work:
syntax-only
sem.-only
hybrid
joint
pipelines:
r&w    14
l+    15
t+    15
f+    15

las sem. f1
(wsj)

sem. f1
(brown)

macro
f1

88.79
88.48
89.19

89.83

89.83
89.94

86.03
87.00
87.69

87.20
87.45

83.24
85.51
86.15

84.39
84.58
84.97

86.34
86.58
87.30
87.80

70.65
73.82
74.58

73.87
75.64
74.48

75.90
75.57
75.50
75.50

table 3: comparison on the conll 2009 english
test set. the    rst block presents results of other
models evaluated for both syntax and semantics on
the conll 2009 task. the second block presents
our models. the third block presents the best pub-
lished models, each using its own syntactic pre-
processing.

dings, which was found bene   cial for syntactic
parsing (ballesteros et al., 2015).

language
catalan
chinese
czech
english
german
japanese
spanish
average

#1 c+   09
81.84
76.38
83.27
87.00
82.44
85.65
81.90
82.64

#2 z+    09a
83.01
76.23
80.87
87.69
81.22
85.28
83.31
82.52

#3 g+    09
82.66
76.15
83.21
86.03
79.59
84.91
82.43
82.14

joint
82.40
79.27
79.53
87.45
81.05
80.91
83.11
81.96

table 4: comparison of macro f1 scores on the
multilingual conll 2009 test set.

6 related work
other approaches to joint modeling, not consid-
ered in our experiments, are notable. llu    s et al.
(2013) propose a graph-based joint model using
id209 for agreement between syn-
tax and semantics, but do not achieve competi-
tive performance on the conll 2009 task. lewis
et al. (2015) proposed an ef   cient joint model for
id35 syntax and srl, which performs better than
a pipelined model. however, their training neces-
sitates id35 annotation, ours does not. moreover,
their evaluation metric rewards semantic depen-
dencies regardless of where they attach within the
argument span given by a propbank constituent,

making direct comparison to our evaluation infea-
sible. krishnamurthy and mitchell (2014) pro-
pose a joint id35 parsing and id36
model which improves over pipelines, but their
task is different from ours. li et al. (2010) also
perform joint syntactic and semantic dependency
parsing for chinese, but do not report results on
the conll 2009 dataset.

there has also been an increased interest in
models which use neural networks for srl. col-
lobert et al. (2011) proposed models which per-
form many nlp tasks without hand-crafted fea-
tures. though they did not achieve the best results
on the constituent-based srl task (carreras and
m`arquez, 2005), their approach inspired zhou and
xu (2015), who achieved state-of-the-art results
using deep bidirectional lstms. our approach
for dependency-based srl is not directly compa-
rable.

7 conclusion
we presented an incremental, greedy parser for
joint syntactic and semantic id33.
our model surpasses the performance of previous
joint models on the conll 2008 and 2009 en-
glish tasks, without using expert-crafted, expen-
sive features of the full syntactic parse.

acknowledgments
the authors thank sam thomson, lingpeng kong,
mark yatskar, eunsol choi, george mulcaire, and
luheng he, as well as the anonymous review-
ers, for many useful comments. this research
was supported in part by darpa grant fa8750-
12-2-0342 funded under the deft program and
by the u.s. army research of   ce under grant
number w911nf-10-1-0533. any opinion,    nd-
ings, and conclusions or recommendations ex-
pressed in this material are those of the author(s)
and do not necessarily re   ect the view of the
u.s. army research of   ce or the u.s. govern-
ment. miguel ballesteros was supported by the
european commission under the contract num-
bers fp7-ict-610411 (project multisensor)
and h2020-ria-645012 (project kristina).

references
[ballesteros and nivre2013] miguel ballesteros

and
joakim nivre. 2013. going to the roots of de-
computational linguistics,
pendency parsing.
39(1):5   13.

[ballesteros et al.2015] miguel ballesteros, chris dyer,
and noah a. smith. 2015.
improved transition-
based parsing by modeling characters instead of
words with lstms. in proc. of emnlp.

[bj  orkelund et al.2009] anders bj  orkelund,

love
hafdell, and pierre nugues. 2009. multilingual
id14. in proc. of conll.

[bj  orkelund et al.2010] anders bj  orkelund,

bernd
bohnet, love hafdell, and pierre nugues. 2010.
a high-performance syntactic and semantic depen-
dency parser. in proc. of coling.

[carreras and m`arquez2005] xavier carreras and llu    s
introduction to the conll-2005
in proc. of

m`arquez. 2005.
shared task: id14.
conll.

[che et al.2008] wanxiang che, zhenghua li, yuxuan
hu, yongqiang li, bing qin, ting liu, and sheng
li. 2008. a cascaded syntactic and semantic de-
pendency parsing system. in proc. of conll.

[che et al.2009] wanxiang

zhenghua
li,
yongqiang li, yuhang guo, bing qin,
and
ting liu. 2009. multilingual dependency-based
syntactic and id29. in proc. of conll.

che,

[ciaramita et al.2008] massimiliano

ciaramita,
giuseppe attardi, felice dell   orletta, and mi-
hai surdeanu.
desrl: a linear-time
id14 system. in proc. of conll.

2008.

[collobert et al.2011] ronan collobert, jason weston,
l  eon bottou, michael karlen, koray kavukcuoglu,
and pavel kuksa.
2011. natural language pro-
cessing (almost) from scratch. journal of machine
learning research, 12:2493   2537.

[dyer et al.2015] chris dyer, miguel ballesteros,
wang ling, austin matthews, and noah a. smith.
2015. transition-based id33 with
stack long short-term memory. in proc. of acl.

[fitzgerald et al.2015] nicholas fitzgerald, oscar
t  ackstr  om, kuzman ganchev, and dipanjan das.
2015. semantic role labelling with neural network
factors. in proc. of emnlp.

[foland and martin2015] william r. foland and james
2015. dependencybased semantic role
in

martin.
labeling using convolutional neural networks.
proc. of *sem.

[gesmundo et al.2009] andrea gesmundo, james hen-
derson, paola merlo, and ivan titov. 2009. a latent
variable model of synchronous syntactic-semantic
parsing for multiple languages. in proc. of conll.

[gildea and jurafsky2002] daniel gildea and daniel
jurafsky. 2002. automatic labeling of semantic
roles. computational linguistics, 28(3):245   288.

[graves2013] alex graves.

generat-
ing sequences with recurrent neural networks.
arxiv:1308.0850.

2013.

[haji  c et al.2009] jan haji  c, massimiliano cia-
ramita, richard johansson, daisuke kawahara,
maria ant`onia mart    , llu    s m`arquez, adam mey-
ers, joakim nivre, sebastian pad  o, jan   st  ep  anek,
pavel stra  n  ak, mihai surdeanu, nianwen xue, and
yi zhang. 2009. the conll-2009 shared task:
syntactic and semantic dependencies in multiple
languages. in proc. of conll.

[he et al.2013] he he, hal daum  e iii, and jason eis-
ner. 2013. dynamic feature selection for depen-
dency parsing. in proc. of emnlp.

[henderson et al.2008] james henderson, paola merlo,
gabriele musillo, and ivan titov. 2008. a latent
variable model of synchronous parsing for syntactic
and semantic dependencies. in proc. of conll.

[henderson et al.2013] james henderson, paola merlo,
ivan titov, and gabriele musillo. 2013. multi-
lingual joint parsing of syntactic and semantic de-
pendencies with a latent variable model. computa-
tional linguistics, 39(4):949   998.

[hochreiter and schmidhuber1997] sepp hochreiter
and j  urgen schmidhuber. 1997. long short-term
memory. neural computation, 9(8):1735   1780.

[johansson and nugues2008] richard johansson and
pierre nugues. 2008. dependency-based syntactic-
semantic analysis with propbank and nombank. in
proc. of conll.

[johansson2009] richard johansson. 2009. statistical

bistratal id33. in proc. of emnlp.

[krishnamurthy and mitchell2014] jayant

krishna-
murthy and tom m. mitchell. 2014. joint syntactic
and id29 with combinatory categorial
grammar. in proc. of acl.

[lei et al.2015] tao lei, yuan zhang, llu    s m`arquez
i villodre, alessandro moschitti, and regina barzi-
lay. 2015. high-order low-rank tensors for semantic
role labeling. in proc. of naacl.

[lewis et al.2015] mike lewis, luheng he, and luke
zettlemoyer. 2015. joint a* id35 parsing and se-
mantic role labelling. in proc. of emnlp.

[li et al.2010] junhui li, guodong zhou,

hwee tou ng.
mantic parsing of chinese. in proc. of acl.

2010.

and
joint syntactic and se-

[ling et al.2015] wang ling, chris dyer, alan black,
and isabel trancoso. 2015. two/too simple adapta-
tions of id97 for syntax problems. in proc. of
naacl.

[goldberg2015] yoav goldberg. 2015. a primer on
neural network models for natural language process-
ing. arxiv:1510.00726.

[llu    s and m`arquez2008] xavier llu    s

and llu    s
m`arquez. 2008. a joint model for parsing syntactic
and semantic dependencies. in proc. of conll.

[llu    s et al.2013] xavier llu    s, xavier carreras, and
llu    s m`arquez. 2013. joint arc-factored parsing of
syntactic and semantic dependencies. transactions
of the acl, 1:219   230.

[toutanova et al.2008] kristina

toutanova,
aria
haghighi, and christopher d. manning.
2008.
a global joint model for id14.
computational linguistics, 34(2):161   191.

[weiss et al.2015] david weiss, chris alberti, michael
collins, and slav petrov. 2015. structured train-
ing for neural network transition-based parsing. in
proc. of acl.

[zaremba et al.2014] wojciech

sutskever, and oriol vinyals.
neural network id173. arxiv:1409.2329.

ilya
zaremba,
2014. recurrent

[zhao and kit2008] hai zhao and chunyu kit. 2008.
parsing syntactic and semantic dependencies with
two single-stage maximum id178 models. in proc.
of conll.

[zhao et al.2009] hai zhao, wenliang chen, jun   ichi
kazama, kiyotaka uchimoto, and kentaro tori-
sawa. 2009. multilingual dependency learning: ex-
ploiting rich features for tagging syntactic and se-
mantic dependencies. in proc. of conll.

[zhou and xu2015] jie zhou and wei xu. 2015. end-
to-end learning of id14 using re-
current neural networks. in proc. of acl.

[marcus et al.1993] mitchell p. marcus, mary ann
marcinkiewicz, and beatrice santorini.
1993.
building a large annotated corpus of english:
the id32. computational linguistics,
19(2):313   330.

[meyers et al.2004] adam meyers, ruth reeves,
catherine macleod, rachel szekely, veronika
zielinska, brian young, and ralph grishman. 2004.
the nombank project: an interim report. in proc.
of naacl.

[nair and hinton2010] vinod nair and geoffrey e.
hinton. 2010. recti   ed linear units improve re-
stricted id82s. in proc. of icml.

[nivre et al.2007] joakim nivre, johan hall, jens nils-
son, atanas chanev, g  ulsen eryigit, sandra k  ubler,
svetoslav marinov, and erwin marsi. 2007. malt-
parser: a language-independent system for data-
driven id33. natural language en-
gineering, 13:95   135.

[nivre2008] joakim nivre. 2008. algorithms for de-
terministic incremental id33. com-
putational linguistics, 34(4):513   553.

[nivre2009] joakim nivre. 2009. non-projective de-
pendency parsing in expected linear time. in proc.
of acl.

[palmer et al.2005] martha palmer, daniel gildea, and
paul kingsbury. 2005. the proposition bank: an
annotated corpus of semantic roles. computational
linguistics, 31(1):71   106.

[roth and woodsend2014] michael roth and kristian
woodsend. 2014. composition of word represen-
tations improves semantic role labelling. in proc. of
emnlp.

[surdeanu et al.2008] mihai surdeanu, richard johans-
son, adam meyers, llu    s m`arquez, and joakim
nivre. 2008. the conll-2008 shared task on joint
parsing of syntactic and semantic dependencies. in
proc. of conll.

[sutton and mccallum2005] charles sutton and an-
drew mccallum. 2005. joint parsing and semantic
role labeling. in proc. of conll.

[t  ackstr  om et al.2015] oscar t  ackstr  om, kuzman
ganchev, and dipanjan das.
ef   cient
id136 and structured learning for semantic role
labeling. transactions of the acl, 3:29   41.

2015.

[titov et al.2009] ivan titov, james henderson, paola
merlo, and gabriele musillo. 2009. online graph
planarisation for synchronous parsing of semantic
and syntactic dependencies. in proc. of ijcai.

