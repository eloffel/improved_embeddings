id4 with external phrase memory

yaohua tang

the university of hong kong

tangyh@hku.hk

fandong meng

institute of computing technology

chinese academy of sciences

mengfandong@ict.ac.cn

6
1
0
2

 

n
u
j
 

6

 
 
]
l
c
.
s
c
[
 
 

1
v
2
9
7
1
0

.

6
0
6
1
:
v
i
x
r
a

zhengdong lu and hang li

noah   s ark lab, huawei technologies

lu.zhengdong@huawei.com

hangli.hl@huawei.com

abstract

in this paper, we propose phrasenet, a neu-
ral machine translator with a phrase mem-
ory which stores phrase pairs in symbolic
form, mined from corpus or speci   ed by hu-
man experts. for any given source sentence,
phrasenet scans the phrase memory to deter-
mine the candidate phrase pairs and integrates
tagging information in the representation of
source sentence accordingly. the decoder uti-
lizes a mixture of word-generating compo-
nent and phrase-generating component, with
a speci   cally designed strategy to generate a
sequence of multiple words all at once. the
phrasenet not only approaches one step to-
wards incorporating external knowledge into
id4, but also makes
an effort to extend the word-by-word genera-
tion mechanism of recurrent neural network.
our empirical study on chinese-to-english
translation shows that, with carefully-chosen
phrase table in memory, phrasenet yields 3.45
id7 improvement over the generic neural
machine translator.

1

introduction

id4 (id4), although only
proposed recently, has shown great potential, and ar-
guably surpassed id151 on
tasks like english-german translation (sennrich et
al., 2015). in addition to its superior ability in mod-
eling the semantics of source sentence and language
   uency of the target sentence, id4 as a framework
also has remarkable    exibility in accommodating
other form of knowledge.

philip l.h. yu
plhyu@hku.hk

the university of hong kong

in this paper, we explore the possibility of equip-
ping regular neural machine translator with an exter-
nal memory storing rules that specify phrase-level
correspondence between the source and target lan-
guages. those rules are in symbolic form, which can
be either extracted from a parallel corpus or given
by experts. we tailor the encoder, decoder and the
attention model of the neural translator to help lo-
cate phrases in the source and generate their trans-
lations in the target. the proposed model is called
phrasenet. phrasenet is not only one step towards
incorporating external knowledge in to neural ma-
chine translation, but also an effort to extend the
word-by-word generation mechanism of recurrent
neural network.

1.1 model overview
the overall diagram of phrasenet is displayed in
figure 1. basically, for a given source sentence,
phrasenet    rst encodes the sentence to a representa-
tion with an id56 encoder, scans the phrase mem-
ory to select candidate phrase pairs and then tags
the source representation accordingly (section 3.2).
phrasenet then generates both words and phrases
with an id56 decoder.
it dynamically determines
at each time step with its probabilistic model con-
sisting of a mixture of word-generation mode and
phrase-generation mode (section 3). to maintain
the state consistency of id56 when running in differ-
ent modes, the decoder of phrasenet will go through
   idle run    (section 3.6) after generating a multiple-
word phrase.
our contribution is of three-folds:

1. we propose an end-to-end learning algorithm
for id4 with an external

about the source sentence, focusing on the parts sur-
rounding the corresponding word.

at decoding time t,

the attention model uses
st   1 (the id56 states) and eyt   1 (the embedding
of previous target word yt   1) to    query    the en-
coded h, marks each element of h a score etj =
f (st   1, hj, eyt   1). the non-linear function f can
take on many forms, but we concatenate the three
inputs and feed it to a neural network with one hid-
den layer and tanh as activation function. the scores
are then normalized to {  tj}, serving as the weights
of {hj{ to the target word, which then gives the con-
j=1   tjhj. the context vec-
tor ct is then used to update the hidden state of the
decoder:

text vector hj, ct =(cid:80)tx

st = fu(st   1, ct, eyt   1),

(1)
where the function fu is gru (cho et al., 2014;
chung et al., 2014). to predict a target word, the de-
coder combines st, ct and eyt   1, feeds to a one-layer
mlp with tanh as activation function, followed by a
softmax function,

(cid:110)

p(yt = yi|y<t, x;   )    

i wo tanh(cid:0)uost   1 + coct + voeyt   1

exp

vt

(cid:1)(cid:111)

,

(2)

where wo, uo, co and vo are weight matrices. vi
is an one-hot indicator vector for yi.
3 models
in this section, we will give more details of
phrasenet, more speci   cally on the preprocessing,
encoder and decoders. with two different vari-
ants of the mixture models used in decoder, we
naturally have two variants of phrasenet, namely
phrasenetgate and phrasenetsof tmax.
3.1 preprocessing
the phrase table p is a list of rules. each rule
(cid:48)
contains a source phrase p
k and its translation, a
target phrase pk. figure 2 gives an example of
the phrase table. for simplicity, we    rst limit our-
selves to a subset of rules with the strongest source-
target correspondence, which is in contrast to that in
phrase-based id151 (smt).
in smt, a phrase could have multiple translations
with a id203 distribution over them. here we
restrict that for each source phrase in p, it has only
one translation with id203 almost equal to 1.
this will limit the size of p but can guarantee that

figure 1: the overall diagram of phrasenet.

phrase memory, which to our knowledge it is
the    rst effort in this direction;

2. we propose a way to handle the generation of

multiple words with an id56 decoder;

3. our empirical studies on chinese-english trans-
lation tasks show the ef   cacy of our models:
phrasenet achieves on average 3.45 id7 im-
provement over its generic counterpart.

roadmap the remainder of this paper is orga-
nized in the following way. in section 2, we will
give a brief introduction to attention-based neural
machine translation as the background. in section 3,
we will introduce phrasenet, including its two vari-
ants. in section 4, we will report our experiments
on applying phrasenet to chinese-english transla-
tion tasks. then in section 5 and 6, we will give a
brief review of related work and conclude the paper.

2 background

our work is built upon the attention-based neural
machine translation model that learns to align and
translate jointly (bahdanau et al., 2015), which will
be referred to as id56search.

id56search uses a bidirectional id56 (schuster
and paliwal, 1997) to encode the source sentence.
it consists of two independent id56s. the for-
      
      
ward id56 reads the source sentence from left to
h tx). the backward id56
right
      
h = (
reads the source sentence from right to left
      
      
h =
h tx). the representation of the source
h 1, . . . ,
(
      
      
sentence h is then de   ned as the concatenation of
h and
h . each element in h contains information

      
h 1, . . . ,

figure 2: the phrase table pxy. for the kth pair (p
stands for the source phrase and pk for the target phrase.

(cid:48)
(cid:48)
k, pk), p
k

the feasible rules are    reliable    enough and greatly
simpli   es the model design and training.
we will introduce the collection of such a phrase
table p later. at the moment, let us assume that we
have the table p which will be utilized to preprocess
the sentence pair before encoding. in order to tag
a source sentence x = (x1, x2, . . . , xtx), we need
to locate its contained phrases. hence we will    nd
out the rules in p whose source phrase appears in
x, and denote these rules as px. to calculate the
likelihood during training, we also need to    nd out
the rules in px whose target phrase appears in the
target sentence y, denote as pxy (pxy     px).
for simplicity, we remove from px the short rules
that intersect with the others, which means if the
source phrases of two rules are overlapped, we re-
move the rule whose source phrase has fewer words.
we choose at most np (a hype-parameter) phrases
for each sentence with the maximum coverage.

the non-overlapping rules in px and pxy split the
words of source and target sentences into groups as
showing in figure 3. the words in source sentence x
are split into two groups, phrases px and words not-
in-phrases wx, while the words in target sentence
y are split into two groups, phrases pxy and words
not-in-phrases wy.
3.2 encoder
we use the regular encoder from id56search, but
(cid:48)
add tags to hi (meng et al., 2015), h
i = [hi, tagi].
the tags are used to help the model locate and dis-
criminate different phrases.

each tagi is an indicator vector with length np.
for example in figure 3, suppose np = 5 and
(cid:48)
we    nd three source phrases p
2 =
(cid:48)
3 = (x10, x11) in sentence x, we
(x6, x7, x8), p
concatenate a tag vector (1, 0, 0, 0, 0) to each of

(cid:48)
1 = (x2, x3), p

figure 3: example of a sentence pair being split
into
(cid:48)
(cid:48)
(cid:48)
groups. px = {(p
3, p3)}, pxy =
2, p2), (p
1, p1), (p
(cid:48)
{(p
2, p2)}. wx = {x0, x1, x4, x5, x9}, wy =
{y0, y1, y2, y3, y4, y7, y8, y9, y14}.

(cid:48)
1, p1), (p

h2, h3, concatenate a vector (0, 1, 0, 0, 0) to each of
h6, h7, h8, concatenate a vector (0, 0, 1, 0, 0) to h10
and h11. for all other not-in-phrase words wx, we
also add a trivial tag vector (0, 0, 0, 0, 0) to their hi.
figure 4 shows an example of such concatenating.

figure 4: the diagram for encoder.

3.3 decoder
unlike the decoder of id56search that only has word
mode, our decoders also have phrase mode. for a
two-word target phrase pt = (yt, yt+1), it can either
be generated by word mode (one by one) or phrase
mode (as a whole). in our models, we add upon the
id56search another component which has two func-
tions, (1) makes decision between phrase mode and
word mode, (2) chooses the right target phrase if the
decision is phrase mode.

with the help of attention model, the new compo-
nent tries to capture the signals from the encoded
representations of a source sentence and translate
part of the source sentence (source phrase) directly
to the target output as a whole at proper decoding
moments. the tags added in section 3.2 will play
an important role in the process. if we view h as
a short-term memory as it changes from sentence
to sentence, then the phrase table could be called a
phrase memory which is a long-term memory. the
decoder queries the short-term memory to choose

the segment of source while it consults the phrase
memory to choose the right phrase.

we

have

two model

namely
phrasenetgate and phrasenetsof tmax, each cor-
responding to a different
implementation of the
mixture model in decoder.

variants,

3.4 phrasenetgate

figure 5: the diagram for phrasenetgate.

the decoder for phrasenetgate is illustrated in
figure 5. in phrasenetgate, the decoder will    rst use
a gate fz to determine the mode at time t by issuing a
binary indicator variable (zt     {0, 1}), where 0 rep-
resents word mode and 1 represents phrase mode.
then for each mode, it will calculate the word prob-
abilities and phrase probabilities respectively.
in
word mode, a classi   er fw outputs a id203
distribution over the words in target vocabulary v;
while in phrase-mode a phrase classi   er fp deter-
mines the id203 distribution over the phrases
in px. the    nal id203 of output is given by
the probabilities of modes as well as the id203
of individual output generated in each mode. it is a
mixture model since a phrase like china daily
can be generated in both modes, and the    nal prob-
ability of it is therefore the sum of the probabilities
of it being generated from each. for a target phrase
that is not in word vocabulary, i.e., it contains unk,
the id203 of that can only be from the phrase
mode.

one snapshot of the decoding is the following.
let us suppose that at time t = 1, the decoder has
generated the word y1 in the word mode, and the cur-
rent state is s1 , moving to time t = 2. with the state
s1 , the attention model (same as in id56search)    rst
generates the context c2 as a weighted sum of the h

(from encoder). with s1, c2 and ey1, the decoder
could move to update the next state s2 and gener-
ating the next word (or phrase). more speci   cally,
in phrasenetgate, the state s2 is updated in the same
way as in id56search (equation (1)). the gener-
ation of the next word/phrase can be described as
follows, let   s denote st = (st, ct, eyt   1):

1. step-1: generate the decision variable z2 with

p(z2 = 1|s2;   ) = fz(s2)
p(z2 = 0|s2;   ) = 1     fz(s2)

2. step-2a: if z2 = 0 (word mode), generate a
word based on s2 with the regular word vocab-
ulary v, same as id56search (equation 2);

3. step-2b: if z2 = 1 (phrase mode), generate

target phrase pj     px with id203:
pp(y2 = pj|s2, 1;   )    

j wp tanh (ups2 + cpc2 + vpey1)(cid:9) ,

exp(cid:8)ut

where wp, up, cp and vp are weight matri-
ces. uj is an one-hot indicator vector for pj.

4. step-3: calculate the    nal probabilities and

sample the next word (or phrase):
p(y2 = wi) = p(z2 = 0|s2;   )p(wi|s2, 0;   )
p(y2 = pj) = p(z2 = 1|s2;   )p(pj|s2, 1;   )

(cid:20) p(y2 = w)

p(y2 = p)

(cid:21)

,

p(y2) =

where the size of p(y2) is np plus the number
of words in vocabulary v. the next word or
phrase will be sampled according to p(y2). if
next generation is a phrase, the decoder will go
through an    idle run    process (section 3.6) to
generate words in p2, after that the decoder re-
places the tags tagi of those source words of
(cid:48)
(cid:48)
2 has already been
2 to all-zero vectors as p
p
decoded.

et

al.,

similar

2016a),

to (gulcehre

in
phrasenetgate, fz is a three-layered neural net-
work using noisy-tanh activation for the    rst two
layers (gulcehre et al., 2016b) and we add residual
connection (he et al., 2015) from the    rst layer
to the second hidden layer, the output layer uses
sigmoid as activation function.

3.5 phrasenetsof tmax
with phrasenetgate the decision of phrase mode is
made before seeing the actual content of the tar-
get phrase, which fails to make use of the language
model and semantic relevance on the target side. to
address this drawback we devise phrasenetsof tmax,
which takes the candidate phrases and candidate
words in the same softmax, as illustrated in fig-
ure 6. to do this, all the phrases need to embed-
ded as vectors, where the embedding model is also
learned in the id4 training. it is also worth to men-
tion that phrasenetsof tmax can potentially handle
the case where one source phrase may correspond to
multiple candidate target phrases, since the decoder
can distinguish them based on their content. this
modeling advantage, however, will not be explored
in this paper.

figure 6: the diagram for phrasenetsof tmax.

(cid:48)
k, pk), we have several choices to
given a rule (p
calculate its embedding.
in this paper, we choose
to use a separate backward id56 to encode pk and
choose the last state as the embedding for it. that
way, the embedding will keep more information of
the    rst word of pk, therefore facilitate a potential
language model in scoring (yi   1, si, pk).

suppose that at time t = 1, the decoder has gen-
erated the word y1 in the word mode, and the cur-
rent state is s1 , moving to time t = 2. with the
state s1 , the decoder makes an attentive read to h
to obtain c2. the state s2 is updated in the same
way as in equation (1). the generation of the next
word/phrase can be described as follows:

1. step-1: calculate the word score for each

word in v,

  wi = vt

i ww tanh(uws2 + cwc2 + vwey1),

where ww, uw, cw and vw are weight ma-
trices.

2. step-2: calculate the phrase score for each

phrase in px
  pj = wq tanh(uqs2+cqc2+vqey1+rqepj ),

(cid:48)
where epj is the embeddings of rule (p
j, pj).
wq, uq, cq, vq and rq are weight matrices.
3. step-3: calculate the probabilities of all words

and phrases through softmax

p(y2|s2, c2, ey1) = softmax([  w,   p])

in the softmax, the phrases will compete di-
rectly with words, which is different from
the phrasenetgate where phrase probabilities
and word probabilities are calculated inde-
pendently. while phrasenetgate has dif   cul-
ties in calculating the scores for each phrase,
phrasenetsof tmax has the    exibility to adapt to
the new setting with embeddings. if the choice
is a phrase, the decoder will go through    idle
run    process and the tags of those source words
of the chosen phrase will be set to all-zero.

is vastly different

idle run for multi-word phrases

3.6
our decoder
from that of
id56search, but it is still generally built on the ba-
sic word-by-word decoding mechanism. to further
accommodate the phrase mode in which multiple
words are generated all at once, we introduce the
   idle run   . basically, if at time t a multiple-word
phrase is chosen, the decoding id56 will run ex-
actly the same way as in word mode with regard to
state update and attention, only that the generation
of words in the rest of phrase is pre-determined at t.
this process is called idle run, which can be illus-
trated through the following example. in figure 3, if
at time t = 5, the decoder decides to go with phrase
mode and generate p1, the decoder will not really
output p1 = (y5, y6) at once. to keep the updating
of st (equation (1)), the decoder will    rst output y5
and use ey5 and other required elements to update s5

to s6, uses s6 to generates y6. with ey6 and other re-
quired elements, the decoder will update the state to
s7 and at the time t = 7, the decoder starts to make
its next decision, phrase mode or word mode for the
coming words. during the output of p1, the decoder
does not need to make decisions or sample words as
it is already in one phrase mode.

3.7 the probabilistic model for phrases
given a target phrase p = {yt, yt+1, yt+2}, in prin-
ciple, its words could be chosen either from vocabu-
lary v or entirely retrieved from phrase table p. so
in general each word is potentially generated from
a mixture id203 model. in the case that there
are out-of-vocabulary words (unks) in the phrases,
which are faily common in practice, the mixture
model degenerates to phrase mode only.

for an uni   ed notation, we introduce an indica-
tor variable iunk into the mixture id203 model,
which is summarized as follows,

p(yt, yt+1,yt+2|y<t, x;   )

=iunk    t+2(cid:89)

p(zi = 0, yi|si;   )

(3)

i=t

+p(zt = 1, pt = p|st;   ).

where iunk = 1 means there is no unks in the
phrase, and 0 otherwise.
for phrasenetgate, p(zt = 0, yt|st;   ) fac-
torizes into p(zt = 0|st;   )p(yt|st, 0;   ), and
p(zt = 1, pt = p|st;   ) factorizes into p(zt =
1|st;   )pp(pt = p|st, 1;   ). for phrasenetsof tmax,
there is no explicit variable zt, the indicator of mode
is implicitly absorbed into the choice of words and
phrases. the id203 p(zt = 0, yt|st;   ) can
therefore be re-written as p(yt|st;   ) and p(zt =
1, pt = p|st;   ) as p(pt = p|st;   ).
for normal words that are not part of phrases wy,
they can only be generated by word mode, which is
the same as id56search.

given a pair of source and target sentence x =
(x1, x2, . . . , xtx) and y = (y1, y2, . . . , yty ), the
id203 of this pair of sentences is:
p(y|x;   ) =

p(yi|y<i, x;   )

p(pj|y<j, x;   ),

(cid:89)

(cid:89)

yi   wy

pj   pxy

here p(pj|y<j, x;   ) refers to the mixture probabil-
ity (equation (3)) of output the words in pj. for

a given batch of the source and target sequences
{x}n and {yn}, the objective is to minimize the
negative log-likelihood:

n(cid:88)

k=1

p(y(k)|x(k);   )

l =     1
n

4 experiments

we report our empirical
study on applying
phrasenetgate and phrasenetsof tmax to chinese-
to-english translation, and comparing it against
id56search and smt models.
4.1 phrase table p
as mentioned before, when we design our model,
our de   nition for    phrase    is different from that
used in phrase-based id151.
for each source phrase, our models only support
an unique translation (target phrase). therefore, we
only choose those phrase pairs that the source phrase
almost always translates to the target phrase. we
also hope the contexts for the source phrases are rel-
atively    xed so that the models can learn the pat-
terns of translation easier. with these considera-
tions, we focus our attention on    ve categories of
phrases: dates, names, numbers, locations and or-
ganizations. apart from these    ve categories, we
also collect some other phrases that ful   l our re-
quirements. figure 7 shows several examples of our
phrase table.

figure 7: examples of phrase for each category.

the phrase pairs are collected from several sources.
the    rst source consists of extracted phrase pairs
from a bi-lingual corpus using the method described
in (meng et al., 2014), (ren et al., 2009) and
(frantzi et al., 2000). the second source is the
ldc dictionary. the third source is from proper
nouns dictionaries, which contain many commonly

used chinese-to-english translation pairs for proper
nouns. we have also generated some chinese-to-
english phrase translations, especially dates, num-
bers and chinese names, by prede   ned rules. there
are two formats of chinese names, mandarin names
and cantonese names, both of their english coun-
terparts could be generated according to their pro-
nunciation rules. numbers can also be generated by
prede   ned rules, like    1345     1,345   . using these
rules, we transform several formats of chinese num-
bers to english numbers.

4.2 setup
our training data contains 1.25m sentence pairs ob-
tained from ldc corpora1, with 27.9m chinese
words and 34.5m english words respectively. we
use nist 2002 (nist02) dataset as our develop-
ment set, and the nist 2003 (nist03), nist 2004
(nist04), nist 2005 (nist05), 2006 (nist06)
and 2008 (nist08) datasets as our test sets. the
case-insensitive 4-gram nist id7 score (pap-
ineni et al.2002) is used as our evaluation metric.

in training the neural networks, we limit the
source and target vocabularies to the most frequent
16k words (one of the words is reserved for the un-
known words (unk)) in chinese and english, cov-
ering approximately 95.8% and 98.3% of the two
corpora respectively. we train each model with the
sentences of length up to 50 words in training data.
the id27 dimension is 620 and the size
of a hidden layer is 1000. we set np as 10. in both
the id56search and our models, we adopt the cover-
age models introduced in (tu et al., 2016) to mitigate
the problem of over-translation.

we compare our models with state-of-the-art

smt and id56search:

1. moses (koehn et al.2007): an open source
phrase-based translation system with default
con   guration and a 4-gram language model
trained on the target portion of training data;

2. id56search (bahdanau et al.2015): an atten-

tional id4 model with default setting.2

1the

corpora

ldc2003e14,
ldc2004t08 and ldc2005t06.

hansards

include ldc2002e18, ldc2003e07,
ldc2004t07,

portion

of

2we use the code from (https://github.com/

4.3 translation performance
table 1 shows the translation performances mea-
sured in id7 score. clearly both the proposed
phrasenetgate and phrasenetsof tmax signi   cantly
improves the translation quality in all cases. more
speci   cally, on average, phrasenetgate yields about
3.45 id7 score improvement over our baseline,
phrasenetsof tmax yields about 2.13 id7 score im-
provement over our baseline. also, id56search with
expanded vocabulary (30k words) is 1.65 id7 be-
hind phrasenetgate. surprisingly, phrasenetsof tmax
comes behind phrasenetgate, despite its potential
ability to take the content of the target phrase into
the decision. we conjecture that this might be due to
the dif   culty in directly comparing the scores from
two different types of scoring functions in the same
pool of softmax.

it is also reasonable to doubt that our models are
just generate the target phrases without consider-
ing the positions, as this will also (almost surely)
increase the 1-gram and 2-gram id7 scores and
hence increase the    nal id7 scores. to fur-
ther verfy this, table 2 compares our models with
id56search measured in 4-gram id7 score, which
capture overlapping of generated targets and refer-
ence on longer segments.

our models, especially phrasenetgate, still per-
incidating that the

form better than id56search,
phrases are put into the right places.

4.4 samples of translation
we also give two examples from test set compar-
ing our phrasenetgate with id56search, and more
examples can be found in supplementary materi-
als. as demonstrated through those examples, when
there are phrases found in the source sentences,
phrasenetgate has a better chance to generate the
corresponding target phrases correctly at proper lo-
cations. this could happen when the source phrases
consist words all in the vocabulary, but more fre-
quently when there are unk words there, showing
that phrasenetgate is also a strong model to solve
the unk problem. another interesting observation,
e.g., the second example in figure 8 is that for some
common phrases phrasenet sometimes ignores the

kyunghyuncho/dl4mt-material) with minor modi   ca-
tions.

models
nist03 nist04 nist05 nist06 nist08 ave.
moses
30.06
31.61
id56search (16k)
29.86
32.19
id56search (30k)
31.66
33.96
33.31
36.01
phrasenetgate (16k)
phrasenetsof tmax (16k)
34.07
31.99
table 1: evaluation of translation quality, where we use boldface digits to denote the best performance.

nist02
33.41
34.96
36.04
37.68
36.60

33.48
33.85
35.82
37.69
35.93

30.75
30.79
33.05
34.61
33.37

31.07
30.32
31.88
32.70
31.96

23.37
22.13
23.61
25.52
24.62

models
id56search (16k)
phrasenetgate (16k)
phrasenetsof tmax (16k)

nist02
16.89
18.97
17.72

nist03 nist04 nist05 nist06 nist08 ave.
15.64
14.97
16.47
17.95
16.23
15.42

17.77
19.11
17.94

16.02
17.21
16.42

15.26
16.14
15.53

10.16
11.92
10.98

table 2: evaluation of translation quality in 4-gram id7 score.

figure 8: example of phrasenetgate on test sets compared with id56search. id40 is applied on the input, where
underlined are unk words. the phrases highlighted by boxes (with or without colors) are those phrases in our phrase table. the
highlighted phrases without colors are phrases generated by word mode or not generated.

suggestion of phrase mode, but still generate the en-
tire phrase correctly from its word mode. this shows
phrasenet maintains a healthy and    exible balance
between word and phrase mode.
5 related work
probably the work that is closest to phrasenet is the
recently proposed neural generative qa (genqa)
(yin et al., 2015), where a set of triples are stored
in a qa memory, and a neural network queries this
memory for words to use in generating the answer.
more speci   cally, phrasenetgate has the same gating
strategy as in genqa. still, phrasenet is different
from that in several important ways: 1) phrasenet
can handle multiple phrases in one sentence, and 2)

the

phrasenet can generate multi-word expression.
softmax with multiple modes

in
phrasenetsof tmax is very similar to the recently
proposed copynet (gu et al., ). but the generative
mode in copynet still follows a strict word-by-word
fashion and therefore a soft-decision between modes
has to be made for each mode.
in a similar way,
phrasenet is related to (gulcehre et al., 2016a) and
(cheng and lapata, 2016).
6 conclusions and future work
we propose a neural machine translator which can
leverage an external phrase memory, and empirically
show its ef   cacy on chinese-english translation.

international joint conference on natural language
processing (volume 1: long papers), pages 20   30,
beijing, china, july. association for computational
linguistics.

[ren et al.2009] zhixiang ren, yajuan l  u, jie cao, qun
liu, and yun huang. 2009. improving statistical ma-
chine translation using domain bilingual multiword ex-
pressions. in proceedings of the workshop on multi-
word expressions: identi   cation, interpretation, dis-
ambiguation and applications, pages 47   54. associa-
tion for computational linguistics.

[schuster and paliwal1997] mike schuster and kuldip k
paliwal. 1997. bidirectional recurrent neural net-
signal processing, ieee transactions on,
works.
45(11):2673   2681.

[sennrich et al.2015] rico sennrich, barry haddow, and
improving neural machine
arxiv

alexandra birch. 2015.
translation models with monolingual data.
preprint arxiv:1511.06709.

[tu et al.2016] zhaopeng tu, zhengdong lu, yang liu,
xiaohua liu, and hang li. 2016. modeling coverage
for id4.

[yin et al.2015] jun yin, xin jiang, zhengdong lu,
lifeng shang, hang li, and xiaoming li.
2015.
neural generative id53. arxiv preprint
arxiv:1512.01337.

references
[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio.
2015. neural machine
translation by jointly learning to align and translate.
in iclr.

[cheng and lapata2016] jianpeng cheng and mirella la-
pata. 2016. neural summarization by extracting sen-
tences and words. arxiv preprint arxiv:1603.07252.

[cho et al.2014] kyunghyun cho, bart van merrienboer,
caglar gulcehre, dzmitry bahdanau, fethi bougares,
holger schwenk, and yoshua bengio. 2014. learn-
ing phrase representations using id56 encoder   decoder
for id151. in proceedings of
the 2014 conference on empirical methods in natural
language processing (emnlp), pages 1724   1734,
doha, qatar, october. association for computational
linguistics.

[chung et al.2014] junyoung chung, caglar gulcehre,
kyunghyun cho, and yoshua bengio. 2014. empir-
ical evaluation of gated recurrent neural networks on
sequence modeling. arxiv preprint arxiv:1412.3555.
[frantzi et al.2000] katerina frantzi, sophia ananiadou,
and hideki mima. 2000. automatic recognition of
in-
multi-word terms:. the c-value/nc-value method.
ternational journal on digital libraries, 3(2):115   
130.

[gu et al.] jiatao gu, zhengdong lu, hang li, and vic-
tor o.k. li.
incorporating copying mechanism in
sequence-to-sequence learning. in acl2016. associ-
ation for computational linguistics.

[gulcehre et al.2016a] caglar gulcehre, sungjin ahn,
ramesh nallapati, bowen zhou, and yoshua bengio.
2016a. pointing the unknown words. arxiv preprint
arxiv:1603.08148.

[gulcehre et al.2016b] caglar

gulcehre,
marcin
moczulski, misha denil,
and yoshua bengio.
2016b. noisy id180. arxiv preprint
arxiv:1603.00391.

[he et al.2015] kaiming he, xiangyu zhang, shaoqing
ren, and jian sun. 2015. deep residual learning for
image recognition. arxiv preprint arxiv:1512.03385.
[meng et al.2014] fandong meng, deyi xiong, wenbin
jiang, and qun liu. 2014. modeling term transla-
tion for document-informed machine translation.
in
in proceedings of the 2014 conference on empirical
methods in natural language processing, pages 546   
556, doha, qatar.

[meng et al.2015] fandong meng,

zhengdong lu,
mingxuan wang, hang li, wenbin jiang, and qun
liu.
2015. encoding source language with con-
volutional neural network for machine translation.
in proceedings of the 53rd annual meeting of the
association for computational linguistics and the 7th

