on the convergent properties of id27 methods

steven skiena
yingtao tian
{yittian,vvkulkarni,bperozzi,skiena}@cs.stonybrook.edu

stony brook university

vivek kulkarni

bryan perozzi

6
1
0
2

 

y
a
m
2
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
6
5
9
3
0

.

5
0
6
1
:
v
i
x
r
a

abstract

things over different

do id27s converge to learn
similar
initializa-
tions? how repeatable are experiments
with id27s? are all word em-
bedding techniques equally reliable? in
this paper we propose evaluating meth-
ods for learning word representations by
their consistency across initializations. we
propose a measure to quantify the simi-
larity of the learned word representations
under this setting (where they are subject
to different random initializations). our
preliminary results illustrate that our met-
ric not only measures a intrinsic prop-
erty of id27 methods but also
correlates well with other evaluation met-
rics on downstream tasks. we believe our
methods are is useful in characterizing ro-
bustness     an important property to con-
sider when developing new word embed-
ding methods. 1

introduction

1
id27s learned using neural methods
have been shown to be tremendously effective on
several nlp tasks (mikolov et al., 2010; chen
and manning, 2014; socher et al., 2013; plank
et al., 2016). but the question    what properties
are exhibited by id27s that are learned
by different techniques?    remains largely unex-
plored. mikolov et al. (2013b) show that embed-
dings learned by skip-gram model reveal linear
substructures. arora et al. (2015) attempt to ex-
plain this linear substructure by proposing a gen-
erative model based on id93 over dis-
course vectors and (arora et al., 2016) show that
1copyright   2016 this is the authors draft of the work.
it is posted here for your personal use. not for redistribution.

the different senses of a word can be recovered by
a sparse coding in such embedding spaces.

in this work, we carry forward this line of re-
search to understand the different properties of
id27 methods. first we motivate our
research direction by noting the following:
(a)
the optimization to learn id27s is usu-
ally non-convex and hence depends on the ran-
dom initialization and (b) id27s (like
skip-gram or glove) learned by neural models un-
der different random initializations mostly con-
verge to yield very similar performances on nlp
tasks. this observation begs the following ques-
tion: do id27s obtained under two dif-
ferent random initializations actually learn equiv-
alent features? if not, is their similar performance
on downstream tasks merely a coincidence? to
illustrate with an example, consider the perfor-
mance of glove embeddings obtained on two in-
dependent runs (differing only in the random ini-
tialization) on the analogy task (mikolov et al.,
2013a) in table 1. although both embeddings
show a high accuracy on this task (see table 2),
a closer look at the questions where they disagree
reveals differences in the relationships they learn.
an improved understanding of such properties of
id27 methods will enable the devel-
opment of improved models and learning algo-
rithms (which will will in-turn boost performance
on downstream nlp tasks).

speci   cally our contributions are as follows:
    mapping-based similarity measure be-
tween dimensions: we propose a similarity
measure between dimensions of two sets of
embeddings, based on correlations using one-
to-one or many-to-one linear transformation
between these dimensions.

    intrinsic way to evaluate embeddings:
based on this similarity measure, we propose

category
syntactics (superlative)
syntactics (opposite)
semantics (capital&state)
semantics (family)

question

bad : worst     slow : ?

acceptable : unacceptable     ethical : ?

london : england     madrid : ?
brothers : sisters     grandson : ?

glove-1
slowest
moral
spain
niece

glove-2

pace

unethical
ronaldo

granddaughter

table 1: a set of examples of questions in word analogy task (mikolov et al., 2013a) where two independent runs of glove
embeddings differ in the answers. two questions are picked from the syntactic section and two from the semantic section of
the task. the correct answer is highlighted in bold.

emb.

sem.
glove-1 79.75
glove-2 79.67
67.72
68.37

sg-1
sg-2

syn.
69.28
69.34
64.34
63.62

tot.
74.02
74.01
65.83
65.72

  

0.89

0.79

table 2: performance of two independent runs of glove and
skip-gram on word analogy task (mikolov et al., 2013a).   
represents the agreement on the answers between the two
runs as computed by krippendorff   s alpha.

a new, intrinsic way to quantify the similar-
ity between two such embedding spaces. our
preliminary results show that our measure
correlates with a measure of model agree-
ment on the well known analogy task.

2 background and setup
here we describe the datasets and word embed-
dings we use in our experiments. we analyze
two id27s glove (pennington et al.,
2014) and skip-gram (sg) (mikolov et al., 2013a)
where we set the number of dimensions d to be
300. our corpora consists of a combination of
english wikipedia dump with 1.6 billions tokens
and the english section of the news crawl2 with
4.3 billion tokens totaling to 6 billion tokens. we
use similar settings as pennington et al. (2014).
we set the vocabulary to the 400, 000 most fre-
quent words, use a context window of size 10, and
make 50 iterations through the whole dataset. we
use news crawl for two reasons: it is a public
available corpus of the same genre as gigawords
5 (news in formal english), and news crawl is
also available in czech, german, finish and rus-
sia which allows our analysis to extend to multi-
lingual settings.

we train both glove and skip-gram embed-
dings twice using different random initializa-
tions and order of corpus, and refer to them

2available at http://www.statmt.org/wmt16/
translation-task.html. we use articles from 2007
to 2015.

figure 2: correlations between corresponding features ob-
tained using one-to-one matching, sorted in descending order.
both glove and skip-gram demonstrate moderate correlation.
glove is marginally better than skip-gram. note that random
embeddings demonstrate correlations of 0.

as glove-1, glove-2, sg-1, sg-2 respec-
tively. the accuracies of these embeddings on
the word analogy task is shown in table 2. note
that these accuracies are comparable to scores
obtained on the same task in pennington et al.
(2014). we denote these 4 embeddings as e(m)
(a matrix of dimensions |v |    d) where m    
{glove-1, glove-2, sg-1, sg-2}.

i

let e(m)

denote feature dimension i of e(m)
(column i of e(m)). we now seek to quantify the
similarity between embeddings e(m) and e(n),
the methods for which we discuss in forthcoming
sections.

3 similarities of id27s

how similar are the embeddings learned from in-
dependent runs over the same corpus? more for-
mally, given a pair of d dimensional embeddings
e(m) and e(n) differing only in their initial ran-
dom initialization:

    can we de   ne a measure of similarity be-

tween the dimensions (e(m)

, e(n)

)?

i

j

    given such a measure, can we align dimen-
sions in e(m) to dimensions in e(n) to quan-

(a) histogram of feature-wise correlations between
two runs of glove.

(b) histogram of feature-wise correlations between
two runs of skip-gram.

figure 1: distributions of correlations     histogram of correlations of all raw values in   (i, j) in blue, correlations of 1-
to-1 matched   (i, j) in green, and sample canonical correlations from cca in red, discussed in section 3.1, 3.2 and 3.3,
respectively. results between two runs of glove embeddings are shown in sub   gure 1a, and results between two runs of
skip-gram embeddings are shown in sub   gure 1b. also shown are kernel density estimates and medians of corresponding
correlations.

d(cid:88)

d=1

tify the similarity between e(m) and e(n)?

the aggregated correlation is given by

3.1 measure of similarity between feature

dimensions

  1to1 =

1
d

  (e(n)

d , e(m)
ad

).

(1)

i

j

, e(n)

given e(m) and e(n) we de   ne the similarity be-
tween them as   (i, j) =   (e(m)
) where
  (x, y) is de   ned as the pearson correlation co-
ef   cient between the column vectors x and y. we
note that this similarity measure is well-suited to
capture linear relationships between feature di-
mensions. we leave exploring metrics that capture
non-linear relationships (like mutual information)
between feature dimensions to future work.

in figure 1 we show in blue, the histogram of
values in correlation matrices   (i, j) on e(m) and
e(n) obtained on the glove and skip-gram embed-
dings.

3.2 one-to-one alignment

since the optimization problems involved in
glove and skip-gram are inherently non-convex,
the embeddings learned could potentially corre-
spond to different local optima. this implies the
features learned on different runs could be equiv-
alent under some manifold transformation. there-
fore as a    rst step, we ask: is there a one-to-one
alignment between features in two embeddings?
(similar to li et al. (2016))

therefore, given e(m) and e(n), we seek
to    nd an one-to-one matching represented as
a = (a1, a2, . . . ad) meaning that for each d    
{1, 2, . . . , d}, e(n)
ad , where

is matched with e(m)

d

by building a complete bipartite graph, this can be
converted into an instance of maximum weighted
bipartite matching (west, 1996), which can be
solved effectively using hopcroft-karp algorithm
(hopcroft and karp, 1973) in polynomial time.

this perfect bipartite matching allows us to per-
mute features (columns) of e(m) to correspond-
ingly match features in e(n). in figure 1, we show
in green the histogram of matched correlations for
both glove (figure 1a) and skip-gram (figure 1b).
finally, we show the correlations between
matched dimensions in figure 2. observe that
using the one-to-one alignment between the di-
mensions of the embeddings, the glove embed-
dings display a stronger correlation (21.6%) than
skip-gram (19.5%). note that random embeddings
show correlations of 0. we believe this indicates
that glove embeddings are more stable and consis-
tent than skip-gram under random initialization.

3.3 many-to-one mapping
in previous section we sought a one-to-one align-
ment between the features in two embeddings,
one drawback of which is its restrictiveness since
it assumes both models learn exactly the same
set of features equivalent under permutations. in
this section, we hypothesize that a feature in e(m)
could correspond to multiple features in e(n).
hence we relax the restrictions to a many-to-one

matching, by assuming a single dimension in e(n)
can be obtained to some degree from a linear
transformation of multiple dimensions in e(m) .
we capture this by measuring linear relationship
between e(n) and e(m) transformed using cca
(canonical correlation analysis).
given two matrices x     r|t|  d1 and y    
r|t|  d2, cca    nds two projections p     r|t|  d
and q     r|t|  d (d     min(d1, d2)) that maxi-
mizes the correlation   (p xi, qyj) between cor-
responding columns

arg max

p,q

  (p xi, qyi).

(2)

using the projection matrices p and q, one can
now project the inputs x and y to yield u = p x
and v = qy such that   (p xi, qyi) is maxi-
mized. therefore we propose to use cca on two
embeddings e(n) and e(m) of dimension d and
de   ne the similarity between e(n) and e(m) as

d(cid:88)

i=1

d(cid:88)

i=1

  cca =

1
d

  (p xi, qyi).

(3)

in figure 1, we show in red, the histogram of
sample canonical correlations obtained from cca
for glove and skip-gram embedding methods.
these values stand out from one-to-one match-
ing of features, and show different distributions
for glove and skip-gram. also in figure 3 we
show the sample canonical correlations obtained
in descending order. both skip-gram and glove
demonstrate strong correlations on several dimen-
sions as compared to random embeddings.
it is
also clear that glove is better than skip-gram as
measured by   cca. this is consistent with previ-
ous observation in figure 2.

finally, we show that our proposed metric,   cca,
correlates well with the agreement on the analogy
task, as shown in table 3. we emphasize that our
metric is completely intrinsic, does not require any
external data for evaluation, and yet captures use-
ful convergent properties of embedding models.

emb.

  
glove-1 and glove-2 0.89
0.79

sg-1 and sg-2

  cca
0.74
0.66

table 3:   cca correlates well with the agreement scores (rep-
resented by    computed by krippendorffs alpha) on the anal-
ogy task. note that higher   cca implies higher   .

figure 3: sample canonical correlations obtained from cca.
note that cca by design gives dimensions in descending or-
der of sample canonical correlations. glove demonstrates
near perfect correlations in most correlated 200 dimensions.
aggregatively glove is better than skip-gram measured by
  cca. we also show the performance of random embeddings
in red.

4 conclusion
in this paper we proposed an intrinsic metric for
evaluating id27s     their consistency
across random initializations. our preliminary
results showed that while total performance re-
mained consistent across embeddings, there was
a sizeable disagreement between each instance of
a particular embedding. to examine this differ-
ence in more depth, we investigated the similarity
between the dimensions of each embedding space.
furthermore we propose methods to align dimen-
sions of id27s and an intrinsic metric
  cca to measure the similarity of the learned word
embeddings, which our preliminary results show
may correlate with downstream tasks in terms of
model agreement, with the popular word analogy
task in mikolov et al. (2013a) as an example. we
believe the methods and metrics proposed in our
work will enable a deeper investigation into the
convergent properties of embedding models, and
lead to improved optimization algorithms and per-
formance on downstream nlp tasks.

references
sanjeev arora, yuanzhi li, yingyu liang, tengyu ma,
and andrej risteski. 2015. id93 on con-
text spaces: towards an explanation of the myster-
ies of semantic id27s. arxiv preprint
arxiv:1502.03520.

sanjeev arora, yuanzhi li, yingyu liang, tengyu ma,
and andrej risteski. 2016. id202ic struc-
ture of word senses, with applications to polysemy.
arxiv preprint arxiv:1601.03764.

danqi chen and christopher d manning. 2014. a fast
and accurate dependency parser using neural net-
works. in empirical methods in natural language
processing (emnlp).

john e hopcroft and richard m karp. 1973. an
n  5/2 algorithm for maximum matchings in bipartite
graphs. siam journal on computing, 2(4):225   231.

yixuan li, jason yosinski, jeff clune, hod lipson,
and john hopcroft. 2016. convergent learning: do
different neural networks learn the same represen-
in international conference on learning
tations?
representation (iclr    16).

tomas mikolov, martin kara     at, lukas burget, jan
cernock`y, and sanjeev khudanpur. 2010. recur-
rent neural network based language model. in in-
terspeech, volume 2, page 3.

tomas mikolov, quoc v le, and ilya sutskever.
2013a. exploiting similarities among languages for
nternational conference on
machine translation.
learning representations (iclr).

tomas mikolov, wen-tau yih, and geoffrey zweig.
2013b. linguistic regularities in continuous space
word representations. in hlt-naacl, pages 746   
751.

jeffrey pennington, richard socher, and christo-
pher d. manning. 2014. glove: global vectors for
word representation. in empirical methods in nat-
ural language processing (emnlp), pages 1532   
1543.

b. plank, a. s  gaard, and y. goldberg. 2016. mul-
tilingual part-of-speech tagging with bidirectional
long short-term memory models and auxiliary
loss. arxiv e-prints, april.

richard socher, alex perelygin, jean y wu, jason
chuang, christopher d manning, andrew y ng,
and christopher potts. 2013. recursive deep mod-
els for semantic compositionality over a sentiment
in proceedings of the conference on
treebank.
empirical methods in natural language processing
(emnlp), volume 1631, page 1642. citeseer.

douglas b west. 1996. introduction to id207.

