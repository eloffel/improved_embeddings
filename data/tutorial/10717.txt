correlation-based intrinsic evaluation of word vector representations

yulia tsvetkov    manaal faruqui    chris dyer      
   carnegie mellon university    google deepmind
{ytsvetko,mfaruqui,cdyer}@cs.cmu.edu

6
1
0
2

 

n
u
j
 

1
2

 
 
]
l
c
.
s
c
[
 
 

1
v
0
1
7
6
0

.

6
0
6
1
:
v
i
x
r
a

abstract

we introduce qvec-cca   an intrinsic
evaluation metric for word vector repre-
sentations based on correlations of learned
vectors with features extracted from lin-
guistic resources. we show that qvec-
cca scores are an effective proxy for a
range of extrinsic semantic and syntactic
tasks. we also show that the proposed
evaluation obtains higher and more consis-
tent correlations with downstream tasks,
compared to existing approaches to in-
trinsic evaluation of word vectors that are
based on word similarity.

1 introduction

being linguistically opaque, vector-space repre-
sentations of words   id27s   have
standalone items.
limited practical value as
they are effective, however,
in representing
meaning   through individual dimensions and
combinations of thereof   when used as features
in downstream applications (turian et al., 2010;
lazaridou et al., 2013;
socher et al., 2013;
bansal et al., 2014; guo et al., 2014, inter alia).
thus, unless it is coupled with an extrinsic task,
intrinsic evaluation of word vectors has little value
in itself. the main purpose of an intrinsic eval-
uation is to serve as a proxy for the downstream
task the embeddings are tailored for. this paper
advocates a novel approach to constructing such a
proxy.

what are the desired properties of an intrinsic
evaluation measure of id27s? first,
retraining models that use id27s as
features is often expensive. a computationally ef-
   cient intrinsic evaluation that correlates with ex-
trinsic scores is useful for faster prototyping. sec-
ond, an intrinsic evaluation that enables interpre-

tation and analysis of properties encoded by vector
dimensions is an auxiliary mechanism for analyz-
ing how these properties affect the target down-
stream task. it thus facilitates re   nement of word
vector models and, consequently, improvement of
the target task. finally, an intrinsic evaluation that
approximates a range of related downstream tasks
(e.g., semantic text-classi   cation tasks) allows to
assess generality (or speci   city) of a word vec-
tor model, without actually implementing all the
tasks.

tsvetkov et al. (2015) proposed an evaluation
measure   qvec   that was shown to correlate
well with downstream semantic tasks. addition-
ally, it helps shed new light on how vector spaces
encode meaning thus facilitating the interpretation
of word vectors. the crux of the method is to cor-
relate distributional word vectors with linguistic
word vectors constructed from rich linguistic re-
sources, annotated by domain experts. qvec can
easily be adjusted to speci   c downstream tasks
(e.g., part-of-speech tagging) by selecting task-
speci   c linguistic resources (e.g., part-of-speech
annotations). however, qvec suffers from two
weaknesses. first, it is not invariant to linear
transformations of the embeddings    basis, whereas
the bases in id27s are generally arbi-
trary (szegedy et al., 2014). second, it produces
an unnormalized score:
the more dimensions in
the embedding matrix the higher the score. this
precludes comparison of models of different di-
mensionality. in this paper, we introduce qvec-
cca, which simultaneously addresses both prob-
lems, while preserving major strengths of qvec.1

2 qvec and qvec-cca

we introduce qvec-cca   an intrinsic evaluation
measure of the quality of id27s. our

1https://github.com/ytsvetko/qvec

method is a modi   cation of qvec   an evalua-
tion based on alignment of embeddings to a ma-
trix of features extracted from a linguistic resource
(tsvetkov et al., 2015). we review qvec, and
then describe qvec-cca.

qvec. the main idea behind qvec is to quan-
tify the linguistic content of id27s
by maximizing the correlation with a manually-
annotated linguistic resource. let the number of
common words in the vocabulary of the word em-
beddings and the linguistic resource be n. to
quantify the semantic content of embeddings, a
semantic/syntactic linguistic matrix s     rp   n
is constructed from a semantic/syntactic database,
with a column vector for each word. each word
vector is a distribution of the word over p linguis-
tic properties, based on annotations of the word
in the database. let x     rd  n be embed-
ding matrix with every row as a dimension vec-
tor x     r1  n . d denotes the dimensionality of
id27s. then, s and x are aligned to
maximize the cumulative correlation between the
aligned dimensions of the two matrices. specif-
ically, let a     {0, 1}d  p be a matrix of align-
ments such that aij = 1 iff xi is aligned to sj,
otherwise aij = 0.
if r(xi, sj) is the pearson   s
correlation between vectors xi and sj, then qvec
is de   ned as:

qvec = max

a:pj aij    1

x

s

x

x

i=1

j=1

r(xi, sj)    aij

the constraint pj aij     1, warrants that one dis-
tributional dimension is aligned to at most one lin-
guistic dimension.

qvec-cca. to measure correlation between
the embedding matrix x and the linguistic ma-
trix s, instead of cumulative dimension-wise cor-
relation we employ canonical correlation analysis
(hardoon et al., 2004, cca). cca    nds two sets
of basis vectors, one for x    and the other for s   ,
such that the correlations between the projections
of the matrices onto these basis vectors are maxi-
mized. formally, cca    nds a pair of basis vectors
v and w such that

3 linguistic dimension word vectors

both qvec and qvec-cca rely on a matrix of
linguistic properties constructed from a manu-
ally crafted linguistic resource. linguistic re-
sources are invaluable as they capture generaliza-
tions made by domain experts. however, resource
construction is expensive, therefore it is not al-
ways possible to    nd an existing resource that
captures exactly the set of optimal lexical prop-
erties for a downstream task. resources that cap-
ture more coarse-grained, general properties can
be used instead, for example, id138 for seman-
tic evaluation (fellbaum, 1998), or id32
(marcus et al., 1993, ptb) for syntactic evalua-
tion. since these properties are not an exact match
to the task, the intrinsic evaluation tests for a nec-
essary (but possibly not suf   cient) set of general-
izations.

semantic vectors. to evaluate the semantic
content of word vectors, tsvetkov et al. (2015)
exploit supersense annotations in a id138-
annotated corpus   semcor (miller et al., 1993).
the resulting supersense-dimension matrix has
4,199 rows (supersense-annotated nouns and verbs
that occur in semcor at least 5 times2), and 41
columns: 26 for nouns and 15 for verbs. example
vectors are shown in table 1.

word
   sh
duck
chicken

nn.animal

nn.food

0.68
0.31
0.33

0.16
0.00
0.67

        

        

        

        

vb.motion

0.00
0.69
0.00

table 1: linguistic dimension word vector matrix
with semantic vectors, constructed using semcor.

syntactic vectors. similar to semantic vectors,
we construct syntactic vectors for all words with
5 or more occurrences in the training part of the
ptb. vector dimensions are probabilities of the
part-of-speech (pos) annotations in the corpus.
this results in 10,865 word vectors with 45 in-
terpretable columns, each column corresponds to
a pos tag from the ptb; a snapshot is shown in
table 2.

qvec-cca = cca(x   , s   )

4 experiments

= max
v,w

r(x   v, s   w)

thus, qvec-cca ensures invariance to the matri-
ces    bases    rotation, and since it is a single corre-
lation, it produces a score in [   1, 1].

experimental setup. we replicate the experi-
mental setup of tsvetkov et al. (2015):

2we exclude sparser word types to avoid skewed proba-

bility estimates of senses of polysemous words.

word
spring
fall
light

ptb.nn

ptb.vb

0.94
0.49
0.52

0.02
0.43
0.02

        

        

        

        

ptb.jj
0.00
0.00
0.41

table 2: linguistic dimension word vector matrix
with syntactic vectors, constructed using ptb.

train

21 word

cwindow,

(mikolov et al., 2013);

   rst
vector mod-
    we
variants of cbow and skip-gram
els:
their
models
structured
modi   cations
skip-gram,
and cbow with attention
(ling et al., 2015b; ling et al., 2015a); glove
latent
vectors
semantic analysis
vectors
(church and hanks, 1990);
retro   tted
glove and lsa vectors (faruqui et al., 2015).

(pennington et al., 2014);

based
and

(lsa)

    we then evaluate these word vector mod-
els using existing intrinsic evaluation meth-
ods: qvec and the proposed qvec-cca, and
also word similarity tasks using the word-
sim353 dataset (finkelstein et al., 2001, ws-
353), men dataset (bruni et al., 2012), and
siid113x-999 dataset
(hill et al., 2014, sim-
lex).3

    in addition, the same vectors are evaluated using
extrinsic text classi   cation tasks. our semantic
benchmarks are four binary categorization tasks
from the 20 newsgroups (20ng); sentiment
analysis task (socher et al., 2013, senti); and
the metaphor detection (tsvetkov et al., 2014,
metaphor).

    finally, we compute the pearson   s correlation
coef   cient r to quantify the linear relationship
between the intrinsic and extrinsic scorings.
the higher the correlation, the better suited the
intrinsic evaluation to be used as a proxy to the
extrinsic task.

we extend the setup of tsvetkov et al. (2015)
with two syntactic benchmarks, and evaluate
qvec-cca with the syntactic matrix. the    rst
task is id52; we use the lstm-crf
model (lample et al., 2016), and the second is de-
pendency parsing (parse), using the stack-lstm
model of dyer et al. (2015).

results. to test the ef   ciency of qvec-cca in
capturing the semantic content of word vectors,
we evaluate how well the scores correspond to the

3we employ an implementation of a suite of word similar-
ity tasks at wordvectors.org (faruqui and dyer, 2014).

scores of word vector models on semantic bench-
marks. qvec and qvec-cca employ the seman-
tic supersense-dimension vectors described in   3.
in table 3, we show correlations between intrin-
sic scores (word similarity/qvec/qvec-cca) and
extrinsic scores across semantic benchmarks for
300-dimensional vectors. qvec-cca obtains high
positive correlation with all the semantic tasks,
and outperforms qvec on two tasks.

ws-353
men
siid113x
qvec
qvec-cca

20ng metaphor senti
0.46
0.55
0.76
0.55
0.51
0.56
0.88
0.74
0.77
0.93

0.25
0.49
0.44
0.75
0.73

table 3: pearson   s correlations between word
similarity/qvec/qvec-cca scores and the down-
stream text classi   cation tasks.

in table 4, we evaluate qvec and qvec-cca
on syntactic benchmarks. we    rst use linguistic
vectors with dimensions corresponding to part-of-
speech tags (denoted as ptb). then, we use lin-
guistic vectors which are a concatenation of the
semantic and syntactic matrices described in   3
for words that occur in both matrices; this setup
is denoted as ptb+sst.

ws-353
men
siid113x
qvec
qvec-cca
qvec
qvec-cca

pos parse
0.68
-0.38
-0.32
0.51
-0.21
0.20
0.39
0.23
0.23
0.50
0.37
0.28
0.23
0.63

ptb

ptb+sst

table 4: pearson   s correlations between word
similarity/qvec/qvec-cca scores and the down-
stream syntactic tasks.

although some word similarity tasks obtain
high correlations with syntactic applications, these
results are inconsistent, and vary from a high
negative to a high positive correlation. con-
versely, qvec and qvec-cca consistently ob-
tain moderate-to-high positive correlations with
the downstream tasks.

comparing performance of qvec-cca in ptb
and ptb+sst setups sheds light on the importance
of linguistic signals captured by the linguistic ma-
trices. appending supersense-annotated columns

to the linguistic matrix which already contains
pos-annotated columns does not affect correla-
tions of qvec-cca with the id52 task,
since the additional linguistic information is not
relevant for approximating how well dimensions
of id27s encode pos-related prop-
erties.
in the case of id33   the
task which encodes not only syntactic, but also
semantic information (e.g., captured by subject-
verb-object relations)   supersenses introduce rel-
evant linguistic signals that are not present in pos-
annotated columns. thus, appending supersense-
annotated columns to the linguistic matrix im-
proves correlation of qvec-cca with the depen-
dency parsing task.

5 conclusion

we introduced qvec-cca   an approach to in-
trinsic evaluation of id27s. we also
showed that both qvec and qvec-cca are not
limited to semantic evaluation, but are general
approaches, that can evaluate word vector con-
tent with respect
to desired linguistic proper-
ties. semantic and syntactic linguistic features
that we use to construct linguistic dimension ma-
trices are rather coarse, thus the proposed eval-
uation can approximate a range of downstream
tasks, but may not be suf   cient to evaluate    ner-
grained features. in the future work we propose
to exploit existing semantic, syntactic, morpho-
logical, and typological resources (e.g., univer-
sal dependencies treebank (agi  c et al., 2015) and
wals (dryer and haspelmath, 2013)), and also
multilingual resources (e.g., danish supersenses
(mart  nez alonso et al., 2015)) to construct better
linguistic matrices, suited for evaluating vectors
used in additional nlp tasks.

acknowledgments

this work was supported by the national science
foundation through award iis-1526745. we thank
benjamin wilson for helpful comments.

krister lind  n, nikola ljube  i  c, teresa lynn,
christopher manning, h  ctor alonso mart  nez,
ryan mcdonald, anna missil  , simonetta monte-
magni, joakim nivre, hanna nurmi, petya osen-
ova, slav petrov, jussi piitulainen, barbara plank,
prokopis prokopidis, sampo pyysalo, wolfgang
seeker, mojgan seraji, natalia silveira, maria simi,
kiril simov, aaron smith, reut tsarfaty, veronika
vincze, and daniel zeman. 2015. universal de-
pendencies 1.1. lindat/clarin digital library at
institute of formal and applied linguistics, charles
university in prague.

[bansal et al.2014] mohit bansal, kevin gimpel, and
karen livescu. 2014. tailoring continuous word
representations for id33. in proc. of
acl.

[bruni et al.2012] elia bruni, gemma boleda, marco
baroni, and nam-khanh tran. 2012. distributional
semantics in technicolor. in proc. of acl.

[church and hanks1990] kenneth ward church and
patrick hanks. 1990. word association norms, mu-
tual information, and id69. computational
linguistics, 16(1):22   29.

[dryer and haspelmath2013] matthew s. dryer and
martin haspelmath, editors. 2013. wals online.
max planck institute for evolutionary anthropol-
ogy. http://wals.info/.

[dyer et al.2015] chris dyer, miguel ballesteros,
wang ling, austin matthews, and noah a. smith.
2015. transition-based id33 with
stack long short-term memory. in proc. of acl.

[faruqui and dyer2014] manaal faruqui and chris
dyer. 2014. community evaluation and exchange
of word vectors at wordvectors.org. in proc. of acl
(demonstrations).

[faruqui et al.2015] manaal faruqui, jesse dodge, su-
jay kumar jauhar, chris dyer, noah a. smith, and
eduard hovy. 2015. retro   tting word vectors to
semantic lexicons. in proc. of naacl.

[fellbaum1998] christiane fellbaum, editor.

1998.
id138: an electronic lexical database. mit
press.

[finkelstein et al.2001] lev

finkelstein,

evgeniy
gabrilovich, yossi matias, ehud rivlin, zach
solan, gadi wolfman, and eytan ruppin. 2001.
placing search in context: the concept revisited. in
proc. of www.

references
[agi  c et al.2015]   eljko agi  c, maria jesus aranzabe,
aitziber atutxa, cristina bosco, jinho choi, marie-
catherine de marneffe, timothy dozat, rich  rd
farkas, jennifer foster, filip ginter, iakes goe-
naga, koldo gojenola, yoav goldberg, jan ha-
ji  c, anders tr  rup johannsen, jenna kanerva, juha
kuokkala, veronika laippala, alessandro lenci,

[guo et al.2014] jiang guo, wanxiang che, haifeng
wang, and ting liu.
2014. revisiting embed-
ding features for simple semi-supervised learning.
in proc. of emnlp.

[hardoon et al.2004] david r hardoon, sandor szed-
2004. canoni-
mak, and john shawe-taylor.
cal correlation analysis: an overview with appli-
cation to learning methods. neural computation,
16(12):2639   2664.

[hill et al.2014] felix hill, roi reichart, and anna ko-
rhonen. 2014. siid113x-999: evaluating semantic
models with (genuine) similarity estimation. corr,
abs/1408.3456.

[tsvetkov et al.2014] yulia tsvetkov, leonid boytsov,
anatole gershman, eric nyberg, and chris dyer.
2014. metaphor detection with cross-lingual model
transfer. in proc. of acl, pages 248   258.

[tsvetkov et al.2015] yulia tsvetkov, manaal faruqui,
wang ling, guillaume lample, and chris dyer.
2015. evaluation of word vector representations
by subspace alignment. in proc. of emnlp, pages
2049   2054.

[turian et al.2010] joseph turian, lev ratinov, and
yoshua bengio. 2010. word representations: a sim-
ple and general method for semi-supervised learn-
ing. in proc. of acl.

[lample et al.2016] guillaume lample, miguel balles-
teros, sandeep subramanian, kazuya kawakami,
and chris dyer. 2016. neural architectures for
id39. in proc. of naacl.

[lazaridou et al.2013] angeliki lazaridou, eva maria
vecchi, and marco baroni. 2013. fish transporters
and miracle homes: how compositional distribu-
tional semantics can help np parsing.
in proc. of
emnlp.

[ling et al.2015a] wang ling, lin chu-cheng, yulia
tsvetkov, silvio amir, ramon fermandez, chris
dyer, alan w black, and isabel trancoso. 2015a.
not all contexts are created equal: better word rep-
resentations with variable attention.
in proc. of
emnlp.

[ling et al.2015b] wang ling, chris dyer, alan black,
and isabel trancoso. 2015b. two/too simple adap-
tations of id97 for syntax problems. in proc.
of naacl.

[marcus et al.1993] mitchell p marcus, mary ann
1993.
marcinkiewicz, and beatrice santorini.
building a large annotated corpus of english:
the id32. computational linguistics,
19(2):313   330.

[mart  nez alonso et al.2015] h  ctor mart  nez alonso,
anders johannsen, sussi olsen, sanni nimb, nico-
lai hartvig s  rensen, anna braasch, anders s  -
gaard, and bolette sandford pedersen. 2015. super-
sense tagging for danish. in proc. of nodalida,
page 21.

[mikolov et al.2013] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013. ef   cient esti-
mation of word representations in vector space. in
proc. of iclr.

[miller et al.1993] george a. miller, claudia leacock,
randee tengi, and ross t. bunker. 1993. a seman-
tic concordance. in proc. of hlt, pages 303   308.

[pennington et al.2014] jeffrey pennington, richard
socher, and christopher d. manning. 2014. glove:
global vectors for word representation. in proc. of
emnlp.

[socher et al.2013] richard socher, alex perelygin,
jean wu, jason chuang, christopher d. manning,
andrew y. ng, and christopher potts. 2013. recur-
sive deep models for semantic compositionality over
a sentiment treebank. in proc. of emnlp.

[szegedy et al.2014] christian

szegedy, wojciech
ilya sutskever, joan bruna, dumitru
zaremba,
erhan, ian goodfellow, and rob fergus.
2014.
intriguing properties of neural networks. in proc. of
iclr.

