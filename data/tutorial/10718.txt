automatic selection of context con   gurations for improved

class-speci   c word representations

ivan vuli  c1, roy schwartz2,3, ari rappoport4

roi reichart5, anna korhonen1

1 language technology lab, dtal, university of cambridge

2 cs & engineering, university of washington and 3allen institute for ai

4 institute of computer science, the hebrew university of jerusalem
5 faculty of industrial engineering and management, technion, iit

7
1
0
2

 

n
u
j
 

2
1

 
 
]
l
c
.
s
c
[
 
 

3
v
8
2
5
5
0

.

8
0
6
1
:
v
i
x
r
a

{iv250,alk23}@cam.ac.uk

roysch@cs.washington.edu

arir@cs.huji.ac.il roiri@ie.technion.ac.il

abstract

this paper is concerned with identifying
contexts useful for training word represen-
tation models for different word classes
such as adjectives (a), verbs (v), and
nouns (n). we introduce a simple yet ef-
fective framework for an automatic selec-
tion of class-speci   c context con   gurations.
we construct a context con   guration space
based on universal dependency relations
between words, and ef   ciently search this
space with an adapted id125 algo-
rithm. in word similarity tasks for each
word class, we show that our framework is
both effective and ef   cient. particularly, it
improves the spearman   s    correlation with
human scores on siid113x-999 over the best
previously proposed class-speci   c contexts
by 6 (a), 6 (v) and 5 (n)    points. with our
selected context con   gurations, we train on
only 14% (a), 26.2% (v), and 33.6% (n)
of all dependency-based contexts, resulting
in a reduced training time. our results gen-
eralise: we show that the con   gurations our
algorithm learns for one english training
setup outperform previously proposed con-
text types in another training setup for en-
glish. moreover, basing the con   guration
space on universal dependencies, it is possi-
ble to transfer the learned con   gurations to
german and italian. we also demonstrate
improved per-class results over other con-
text types in these two languages.

introduction

1
dense real-valued word representations (embed-
dings) have become ubiquitous in nlp, serving
as invaluable features in a broad range of tasks
(turian et al., 2010; collobert et al., 2011; chen

and manning, 2014). the omnipresent id97
skip-gram model with negative sampling (sgns)
(mikolov et al., 2013) is still considered a ro-
bust and effective choice for a word representation
model, due to its simplicity, fast training, as well as
its solid performance across semantic tasks (baroni
et al., 2014; levy et al., 2015). the original sgns
implementation learns word representations from
local bag-of-words contexts (bow). however, the
underlying model is equally applicable with other
context types (levy and goldberg, 2014a).

recent work suggests that    not all contexts are
created equal   . for example, reaching beyond stan-
dard bow contexts towards contexts based on de-
pendency parses (bansal et al., 2014; melamud
et al., 2016) or symmetric patterns (schwartz et al.,
2015, 2016) yields signi   cant improvements in
learning representations for particular word classes
such as adjectives (a) and verbs (v). moreover,
schwartz et al. (2016) demonstrated that a subset
of dependency-based contexts which covers only
coordination structures is particularly effective for
sgns training, both in terms of the quality of the
induced representations and in the reduced training
time of the model. interestingly, they also demon-
strated that despite the success with adjectives and
verbs, bow contexts are still the optimal choice
when learning representations for nouns (n).

in this work, we propose a simple yet effec-
tive framework for selecting context con   gurations,
which yields improved representations for verbs,
adjectives, and nouns. we start with a de   nition of
our context con   guration space (sect. 3.1). our ba-
sic de   nition of a context refers to a single typed (or
labeled) dependency link between words (e.g., the
amod link or the dobj link). our con   guration
space then naturally consists of all possible sub-
sets of the set of labeled dependency links between
words. we employ the universal dependencies (ud)
scheme to make our framework applicable across

languages. we then describe (sect. 3.2) our adapted
id125 algorithm that aims to select an opti-
mal context con   guration for a given word class.
we show that sgns requires different context
con   gurations to produce improved results for each
word class. for instance, our algorithm detects that
the combination of amod and conj contexts is
effective for adjective representation. moreover,
some contexts that boost representation learning for
one word class (e.g., amod contexts for adjectives)
may be uninformative when learning representa-
tions for another class (e.g., amod for verbs). by
removing such dispensable contexts, we are able
both to speed up the sgns training and to improve
representation quality.

we    rst experiment with the task of predicting
similarity scores for the a/v/n portions of the
benchmarking siid113x-999 evaluation set, running
our algorithm in a standard sgns experimental
setup (levy et al., 2015). when training sgns with
our learned context con   gurations it outperforms
sgns trained with the best previously proposed
context type for each word class: the improvements
in spearman   s    rank correlations are 6 (a), 6 (v),
and 5 (n) points. we also show that by building
context con   gurations we obtain improvements on
the entire siid113x-999 (4    points over the best
baseline). interestingly, this context con   guration
is not the optimal con   guration for any word class.
we then demonstrate that our approach is ro-
bust by showing that transferring the optimal con-
   gurations learned in the above setup to three
other setups yields improved performance. first,
the above context con   gurations, learned with the
sgns training on the english wikipedia corpus,
have an even stronger impact on siid113x999 per-
formance when sgns is trained on a larger corpus.
second, the transferred con   gurations also result
in competitive performance on the task of solv-
ing class-speci   c toefl questions. finally, we
transfer the learned context con   gurations across
languages: these con   gurations improve the sgns
performance when trained with german or italian
corpora and evaluated on class-speci   c subsets of
the multilingual siid113x-999 (leviant and reichart,
2015), without any language-speci   c tuning.

2 related work

word representation models typically train on
(word, context) pairs. traditionally, most models
use bag-of-words (bow) contexts, which represent

a word using its neighbouring words, irrespective
of the syntactic or semantic relations between them
(collobert et al., 2011; mikolov et al., 2013; mnih
and kavukcuoglu, 2013; pennington et al., 2014, in-
ter alia). several alternative context types have been
proposed, motivated by the limitations of bow
contexts, most notably their focus on topical rather
than functional similarity (e.g., coffee:cup vs. cof-
fee:tea). these include dependency contexts (pad  
and lapata, 2007; levy and goldberg, 2014a), pat-
tern contexts (baroni et al., 2010; schwartz et al.,
2015) and substitute vectors (yatbaz et al., 2012;
melamud et al., 2015).

several recent studies examined the effect of con-
text types on word representation learning. mela-
mud et al. (2016) compared three context types on
a set of intrinsic and extrinsic evaluation setups:
bow, dependency links, and substitute vectors.
they show that the optimal type largely depends on
the task at hand, with dependency-based contexts
displaying strong performance on semantic similar-
ity tasks. vuli  c and korhonen (2016) extended the
comparison to more languages, reaching similar
conclusions. schwartz et al. (2016), showed that
symmetric patterns are useful as contexts for v and
a similarity, while bow still works best for nouns.
they also indicated that coordination structures,
a particular dependency link, are more useful for
verbs and adjectives than the entire set of dependen-
cies. in this work, we generalise their approach: our
algorithm systematically and ef   ciently searches
the space of dependency-based context con   gura-
tions, yielding class-speci   c representations with
substantial gains for all three word classes.

previous attempts on specialising word represen-
tations for a particular relation (e.g., similarity vs
relatedness, antonyms) operate in one of two frame-
works: (1) modifying the prior or the regularisation
of the original training procedure (yu and dredze,
2014; wieting et al., 2015; liu et al., 2015; kiela
et al., 2015; ling et al., 2015b); (2) post-processing
procedures which use lexical knowledge to re   ne
previously trained word vectors (faruqui et al.,
2015; wieting et al., 2015; mrk  i  c et al., 2017).
our work suggests that the induced representations
can be specialised by directly training the word rep-
resentation model with carefully selected contexts.

3 context selection: methodology

the goal of our work is to develop a methodology
for the identi   cation of optimal context con   gura-

3.1 context con   guration space
we focus on the con   guration space based on
dependency-based contexts (deps) (pad   and la-
pata, 2007; utt and pad  , 2014). we choose this
space due to multiple reasons. first, dependency
structures are known to be very useful in captur-
ing functional relations between words, even if
these relations are long distance. second, they have
been proven useful in learning id27s
(levy and goldberg, 2014a; melamud et al., 2016).
finally, owing to the recent development of the
universal dependencies (ud) annotation scheme
(mcdonald et al., 2013; nivre et al., 2016)1 it is
possible to reason over dependency structures in a
multilingual manner (e.g., fig. 1). consequently,
a search algorithm in such deps-based con   gura-
tion space can be developed for multiple languages
based on the same design principles. indeed, in this
work we show that the optimal con   gurations for
english translate to improved representations in
two additional languages, german and italian.

and so, given a (ud-)parsed training corpus,
for each target word w with modi   ers m1, . . . , mk
and a head h, the word w is paired with context el-
   1
ements m1_r1, . . . , mk_rk, h_r
h , where r is the
type of the dependency relation between the head
and the modi   er (e.g., amod), and r   1 denotes
an inverse relation. to simplify the presentation,
we adopt the assumption that all training data for
the word representation model are in the form of
such (word, context) pairs (levy and goldberg,
2014a,c), where word is the current target word,
and context is its observed context (e.g., bow,
positional, dependency-based). a naive version of
deps extracts contexts from the parsed corpus
without any post-processing. given the example
from fig. 1, the deps contexts of discovers are:
scientist_nsubj, stars_dobj, telescope_nmod.

deps not only emphasises functional similar-
ity, but also provides a natural implicit grouping
of related contexts. for instance, all pairs with
the shared relation r and r   1 are taken as an r-
based context bag, e.g., the pairs {(scientist, aus-
tralian_amod), (australian, scientist_amod   1)}
from fig. 1 are inserted into the amod con-
text bag, while {(discovers, stars_dobj), (stars,
discovers_dobj   1)} are labelled with dobj.

assume that we have obtained m distinct depen-
dency relations r1, . . . , rm after parsing and post-
processing the corpus. the j-th individual context

1http://universaldependencies.org/ (v1.4 used)

figure 1: extracting dependency-based contexts.
top: an example english sentence from (levy and
goldberg, 2014a), now ud-parsed. middle: the
same sentence in italian, ud-parsed. note the sim-
ilarity between the two parses which suggests that
our context selection framework may be extended
to other languages. bottom: prepositional arc col-
lapsing. the uninformative short-range case arc
is removed, while a    pseudo-arc    specifying the
exact link (prep:with) between discovers and
telescope is added.

tions for word representation model training. we
hope to get improved word representations and,
at the same time, cut down the training time of
the word representation model. fundamentally, we
are not trying to design a new word representation
model, but rather to    nd valuable con   gurations
for existing algorithms.

the motivation to search for such training con-
text con   gurations lies in the intuition that the dis-
tributional hypothesis (harris, 1954) should not
necessarily be made with respect to bow contexts.
instead, it may be restated as a series of statements
according to particular word relations. for example,
the hypothesis can be restated as:    two adjectives
are similar if they modify similar nouns   , which
is captured by the amod typed dependency rela-
tion. this could also be reversed to re   ect noun
similarity by saying that    two nouns are similar
if they are modi   ed by similar adjectives   . in an-
other example,    two verbs are similar if they are
used as predicates of similar nominal subjects    (the
nsubj and nsubjpass dependency relations).
first, we have to de   ne an expressive context
con   guration space that contains potential train-
ing con   gurations and is effectively decomposed
so that useful con   gurations may be sought algo-
rithmically. we can then continue by designing a
search algorithm over the con   guration space.

australianscientistdiscoversstarswithtelescopeamodnsubjdobjcasenmodscienziatoaustralianoscoprestellecontelescopioamodnsubjdobjcasenmodaustralianscientistdiscoversstarswithtelescopeamodnsubjdobjcasenmodprep:withalgorithm 1: best con   guration search
input
:set of m individual context bags:
s = {r(cid:48)

2, . . . , r(cid:48)

1, r(cid:48)

m}

1 build: pool of those k     m candidate individual

context bags {r1, . . . , rk} for which
e(ri) >= threshold, i     {1, . . . , m}, where e(  ) is
a    tness function.

2 build: k-set con   guration rp ool = {r1, . . . , rk} ;
3 initialize: (1) set of candidate con   gurations

r = {rp ool} ; (2) current level l = k ; (3) best
con   guration ro =     ;

4 search:
5 repeat
6
7

rn         ;
ro     arg max
r   r   {ro}
foreach r     r do

foreach ri     r do

e(r) ;

build new (l     1)-set context
con   guration r  ri = r     {ri} ;
if e(r  ri )     e(r) then
rn     rn     {r  ri} ;

l     l     1 ;
r     rn ;

15 until l == 0 or r ==    ;

output :best con   guration ro

8
9
10

11
12

13
14

3.2 class-speci   c con   guration search
alg. 1 provides a high-level overview of the al-
gorithm. an example of its    ow is given in fig. 2.
starting from s, the set of all possible m individual
context bags, the algorithm automatically detects
the subset sk     s, |sk| = k, of candidate indi-
vidual bags that are used as the initial pool (line 1
of alg. 1). the selection is based on some    tness
(goal) function e. in our setup, e(r) is spear-
man   s    correlation with human judgment scores
obtained on the development set after training the
word representation model with the con   guration
r. the selection step relies on a simple threshold:
we use a threshold of        0.2 without any    ne-
tuning in all experiments with all word classes.

we    nd this step to facilitate ef   ciency at a minor
cost for accuracy. for example, since amod denotes
an adjectival modi   er of a noun, an ef   cient search
procedure may safely remove this bag from the
pool of candidate bags for verbs.

the search algorithm then starts from the full
k-set rp ool con   guration (line 3) and tests k
(k     1)-set con   gurations where exactly one in-
dividual bag ri is removed to generate each such
con   guration (line 10). it then retains only the set
of con   gurations that score higher than the origin
k-set con   guration (lines 11-12, see fig. 2). us-
ing this principle, it continues searching only over
lower-level (l     1)-set con   gurations that further

figure 2: an illustration of alg. 1. the search space
is presented as a dag with direct links between
origin con   gurations (e.g., ri + rj + rk) and all
its children con   gurations obtained by removing
exactly one individual bag from the origin (e.g., ri+
rj, rj + rk). after automatically constructing the
initial pool (line 1), the entry point of the algorithm
is the rp ool con   guration (line 2). thicker blue
circles denote visited con   gurations, while the gray
circle denotes the best con   guration found.

bag, j = 1, . . . , m, labelled rj, is a bag (or a mul-
tiset) of (word, context) pairs where context has
   1
one of the following forms: v_rj or v_r
, where v
j
is some vocabulary word. a context con   guration
is then simply a set of individual context bags, e.g.,
r = {ri, rj, rk}, also labelled as r: ri + rj + rk.
we call a con   guration consisting of k individual
context bags a k-set con   guration (e.g., in this
example, r is a 3-set con   guration).2

although a brute-force exhaustive search over
all possible con   gurations is possible in theory and
for small pools (e.g., for adjectives, see tab. 2), it
becomes challenging or practically infeasible for
large pools and large training data. for instance,
based on the pool from tab. 2, the search for the
optimal con   guration would involve trying out
210   1 = 1023 con   gurations for nouns (i.e., train-
ing 1023 different word representation models).
therefore, to reduce the number of visited con-
   gurations, we present a simple heuristic search
algorithm inspired by id125 (pearl, 1984).

2a note on the nomenclature and notation: each context
con   guration may be seen as a set of context bags, as it does
not allow for repetition of its constituent context bags. for
simplicity and clarity of presentation, we use dependency
relation types (e.g., ri = amod, rj = acl) as labels for context
bags. the reader has to be aware that a con   guration r =
{ri, rj, rk} is not by any means a set of relation types/names,
but is in fact a multiset of all (word, context) pairs belonging
to the corresponding context bags labelled with ri, rj, rk.

ri+rj+rk+rlri+rj+rkri+rj+rlri+rk+rlrj+rk+rlri+rjri+rkrj+rkri+rlrj+rlrk+rlrirjrkrle(rpool  ri)>e(rpool)e(rpool)e(rpool  rl)<e(rpool)e(rpool  ri  rj)<e(rpool  ri)improve performance over their l-set origin con   g-
uration. it stops if it reaches the lowest level or if
it cannot improve the goal function any more (line
15). the best scoring con   guration is returned (n.b.,
not guaranteed to be the global optimum).

in our experiments with this heuristic, the search
for the optimal con   guration for verbs is performed
only over 13 1-set con   gurations plus 26 other con-
   gurations (39 out of 133 possible con   gurations).3
for nouns, the advantage of the heuristic is even
more dramatic: only 104 out of 1026 possible con-
   gurations were considered during the search.4

4 experimental setup

implementation details

4.1
word representation model we experiment
with sgns (mikolov et al., 2013), the standard
and very robust choice in vector space modeling
(levy et al., 2015). in all experiments we use
id97f, a reimplementation of id97
able to learn from arbitrary (word, context)
pairs.5 for details concerning the implementation,
we refer the reader to (goldberg and levy, 2014;
levy and goldberg, 2014a).

the sgns preprocessing scheme was replicated
from (levy and goldberg, 2014a; levy et al., 2015).
after lowercasing, all words and contexts that ap-
peared less than 100 times were    ltered. when
considering all dependency types, the vocabulary
spans approximately 185k word types.6 further,
all representations were trained with d = 300 (very
similar trends are observed with d = 100, 500).

the same setup was used in prior work
(schwartz et al., 2016; vuli  c and korhonen, 2016).
keeping the representation model    xed across ex-
periments and varying only the context type allows
us to attribute any differences in results to a sole
factor: the context type. we plan to experiment with
other representation models in future work.

3the total is 133 as we have to include 6 additional 1-set
con   gurations that have to be tested (line 1 of alg. 1) but are
not included in the initial pool for verbs (line 2).

4we also experimented with a less conservative variant
which does not stop when lower-level con   gurations do not
improve e; it instead follows the path of the best-scoring
lower-level con   guration even if its score is lower than that of
its origin. as we do not observe any signi   cant improvement
with this variant, we opt for the faster and simpler one.

5https://bitbucket.org/yoavgo/id97f
6sgns for all models was trained using stochastic gradient
descent and standard settings: 15 negative samples, global
learning rate: 0.025, subsampling rate: 1e     4, 15 epochs.

universal dependencies
as labels the
adopted ud scheme leans on the universal
stanford dependencies (de marneffe et al., 2014)
complemented with the universal pos tagset
(petrov et al., 2012). it
is straightforward to
   translate    previous annotation schemes to ud
(de marneffe et al., 2014). providing a consistently
annotated inventory of categories for similar
syntactic constructions across languages,
the
ud scheme facilitates representation learning in
languages other than english, as shown in (vuli  c
and korhonen, 2016; vuli  c, 2017).

individual context bags standard post-parsing
steps are performed in order to obtain an initial
list of individual context bags for our algorithm:
(1) prepositional arcs are collapsed ((levy and
goldberg, 2014a; vuli  c and korhonen, 2016), see
fig. 1). following this procedure, all pairs where
the relation r has the form prep:x (where x is
a preposition) are subsumed to a context bag la-
belled prep; (2) similar labels are merged into a
single label (e.g., direct (dobj) and indirect ob-
jects (iobj) are merged into obj); (3) pairs with
infrequent and uninformative labels are removed
(e.g., punct, goeswith, cc).

coordination-based contexts are extracted as in
prior work (schwartz et al., 2016), distinguishing
between left and right contexts extracted from the
conj relation; the label for this bag is conjlr.
we also utilise the variant that does not make the
distinction, labeled conjll. if both are used, the
label is simply conj=conjlr+conjll.7

consequently, the individual context bags we
use in all experiments are: subj, obj, comp,
nummod, appos, nmod, acl, amod, prep,
adv, compound, conjlr, conjll.

4.2 training and evaluation
we run the algorithm for context con   guration se-
lection only once, with the sgns training setup
described below. our main evaluation setup is pre-
sented below, but the learned con   gurations are
tested in additional setups, detailed in sect. 5.

training data our training corpus is the cleaned
and tokenised english polyglot wikipedia data
(al-rfou et al., 2013),8 consisting of approxi-

7given the coordination structure boys and girls,
conjlr training pairs are (boys, girls_conj),
(girls,
boys_conj   1), while conjll pairs are (boys, girls_conj),
(girls, boys_conj).

8https://sites.google.com/site/rmyeid/projects/polyglot

mately 75m sentences and 1.7b word tokens. the
wikipedia data were pos-tagged with universal
pos (upos) tags (petrov et al., 2012) using the
state-of-the art turbotagger (martins et al., 2013).9
the parser was trained using default settings (id166
mira with 20 iterations, no further parameter tun-
ing) on the train+dev portion of the ud treebank
annotated with upos tags. the data were then
parsed with ud using the graph-based mate parser
v3.61 (bohnet, 2010)10 with standard settings on
train+dev of the ud treebank.

evaluation we experiment with the verb pair
(222 pairs), adjective pair (111 pairs), and noun
pair (666 pairs) portions of siid113x-999. we re-
port spearman   s    correlation between the ranks
derived from the scores of the evaluated models
and the human scores. our evaluation setup is bor-
rowed from levy et al. (2015): we perform 2-fold
cross-validation, where the context con   gurations
are optimised on a development set, separate from
the unseen test data. unless stated otherwise, the
reported scores are always the averages of the 2
runs, computed in the standard fashion by apply-
ing the cosine similarity to the vectors of words
participating in a pair.

4.3 baselines
baseline context types we compare the con-
text con   gurations found by alg. 1 against baseline
contexts from prior work:
- bow: standard bag-of-words contexts.
- posit: positional contexts (sch  tze, 1993; levy
and goldberg, 2014b; ling et al., 2015a), which
enrich bow with information on the sequential
position of each context word. given the example
from fig. 1, posit with the window size 2 extracts
the following contexts for discovers: australian_-2,
scientist_-1, stars_+2, with_+1.
- deps-all: all dependency links without any con-
text selection, extracted from dependency-parsed
data with prepositional arc collapsing.
- coord: coordination-based contexts are used
as fast lightweight contexts for improved repre-
sentations of adjectives and verbs (schwartz et al.,
2016). this is in fact the conjlr context bag, a
subset of deps-all.
- sp: contexts based on symmetric patterns (sps,
(davidov and rappoport, 2006; schwartz et al.,
2015)). for example, if the word x and the word

context group
conjlr (a+n+v)
obj (n+v)
prep (n+v)
amod (a+n)
compound (n)
adv (v)
nummod (-)

adj
0.415
-0.028
0.188
0.479
-0.124
0.197
-0.142

verb
0.281
0.309
0.344
0.058
-0.019
0.342
-0.065

noun
0.401
0.390
0.387
0.398
0.416
0.104
0.029

table 1: 2-fold cross-validation results for an illus-
trative selection of individual context bags. results
are presented for the noun, verb and adjective sub-
sets of siid113x-999. values in parentheses denote
the class-speci   c initial pools to which each context
is selected based on its    score (line 1 of alg. 1).

adjectives verbs
amod,
conjlr,
conjll

prep,
acl, obj,
comp, adv,
conjlr,
conjll

nouns
amod, prep,
compound, subj,
obj, appos, acl,
nmod, conjlr,
conjll

table 2: automatically constructed initial pools of
candidate bags for each word class (sect. 3.2).

y appear in the lexico-syntactic symmetric pattern
   x or y    in the sgns training corpus, then y is an
sp context instance for x, and vice versa.

the development set was used to tune the win-
dow size for bow and posit (to 2) and the pa-
rameters of the sp extraction algorithm.11

baseline greedy search algorithm we also
compare our search algorithm to its greedy vari-
ant: at each iteration of lines 8-12 in alg. 1, rn
now keeps only the best con   guration of size l     1
that perform better than the initial con   guration of
size l, instead of all such con   gurations.

5 results and discussion
5.1 main evaluation setup
not all context bags are created equal first,
we test the performance of individual context bags
across siid113x-999 adjective, verb, and noun sub-
sets. besides providing insight on the intuition be-
hind context selection, these    ndings are important
for the automatic selection of class-speci   c pools
(line 1 of alg. 1). the results are shown in tab. 1.
the experiment supports our intuition (see
sect. 3.2): some context bags are de   nitely not
useful for some classes and may be safely removed

9http://www.cs.cmu.edu/~ark/turboparser/
10https://code.google.com/archive/p/mate-tools/

11the sp extraction algorithm is available online:

homes.cs.washington.edu/   roysch/software/dr06/dr06.html

baselines
bow (win=2)
posit (win=2)
coord (conjlr)
sp
deps-all
con   gurations: verbs

pool-all
prep+acl+obj+adv+conj
prep+acl+obj+comp+conj
prep+obj+comp+adv+conj
prep+acl+adv+conj (best)
prep+acl+obj+adv
prep+acl+adv
prep+acl+conj
acl+obj+adv+conj
acl+obj+adv

(verbs)
0.336
0.345
0.283
0.349
0.344

0.379
0.393
0.344
0.391   
0.409
0.392
0.407
0.390
0.345
0.385

baselines
bow (win=2)
posit (win=2)
coord (conjlr)
sp
deps-all
con   gurations: nouns

pool-all
amod+subj+obj+appos+compound+nmod+conj
amod+subj+obj+appos+compound+conj
amod+subj+obj+appos+compound+conjlr
amod+subj+obj+compound+conj (best)
amod+subj+obj+appos+conj
subj+obj+compound+conj
amod+subj+compound+conj
amod+subj+obj+compound
amod+obj+compound+conj

(nouns)
0.435
0.437
0.392
0.372
0.441

0.469
0.478
0.487
0.476   
0.491
0.470
0.479
0.481
0.478
0.481

table 3: results on the siid113x-999 test data over (a) verbs and (b) nouns subsets. only a selection
of context con   gurations optimised for verb and noun similarity are shown. pool-all denotes a
con   guration where all individual context bags from the verbs/nouns-oriented pools (see table 2) are
used. best denotes the best performing con   guration found by alg. 1. other con   gurations visited by
alg. 1 that score higher than the best scoring baseline context type for each word class are in gray. scores
obtained using a greedy search algorithm instead of alg. 1 are in italic, marked with a cross (   ).

baselines
bow (win=2)
posit (win=2)
coord (conjlr)
sp
deps-all
con   gurations: adjectives
pool-all: amod+conj (best)
amod+conjlr
amod+conjll
conj

(adjectives)
0.489
0.460
0.407
0.395
0.360

0.546   
0.527
0.531
0.470

table 4: results on the siid113x-999 adjectives sub-
set with adjective-speci   c con   gurations.

when performing the class-speci   c sgns training.
for instance, the amod bag is indeed important for
adjective and noun similarity, and at the same time
it does not encode any useful information regarding
verb similarity. compound is, as expected, use-
ful only for nouns. tab. 1 also suggests that some
context bags (e.g., nummod) do not encode any in-
formative contextual evidence regarding similarity,
therefore they can be discarded. the initial results
with individual context bags help to reduce the pool
of candidate bags (line 1 in alg. 1), see tab. 2.

searching for improved con   gurations next,
we test if we can improve class-speci   c represen-
tations by selecting class-speci   c con   gurations.
results are summarised in tables 3 and 4. indeed,
class-speci   c con   gurations yield better represen-
tations, as is evident from the scores: the improve-

ments with the best class-speci   c con   gurations
found by alg. 1 are approximately 6    points for ad-
jectives, 6 points for verbs, and 5 points for nouns
over the best baseline for each class.

the improvements are visible even with con   g-
urations that simply pool all candidate individual
bags (pool-all), without running alg. 1 beyond
line 1. however, further careful context selection,
i.e., traversing the con   guration space using alg. 1
leads to additional improvements for v and n
(gains of 3 and 2.2    points). very similar improved
scores are achieved with a variety of con   gurations
(see tab. 3), especially in the neighbourhood of the
best con   guration found by alg. 1. this indicates
that the method is quite robust: even sub-optimal12
solutions result in improved class-speci   c repre-
sentations. furthermore, our algorithm is able to
   nd better con   gurations for verbs and nouns com-
pared to its greedy variant. finally, our algorithm
generalises well: the best scoring con   guration on
the dev set is always the best one on the test set.

training: fast and/or accurate? carefully se-
lected con   gurations are also likely to reduce
sgns training times. indeed, the con   guration-
based model trains on only 14% (a), 26.2% (v),
and 33.6% (n) of all dependency-based contexts.
the training times and statistics for each con-
text type are displayed in tab. 5. all models

12the term optimal here and later in the text refers to the

best con   guration returned by our algorithm.

context type
bow (win=2)
posit (win=2)
coord (conjlr)
sp
deps-all
best-adj
best-verbs
best-nouns

training time
179mins 27s
190mins 12s
4mins 11s
1mins 29s
103mins 35s
14mins 5s
29mins 48s
41mins 14s

# pairs
5.974g
5.974g
129.69m
46.37m
3.165g
447.4m
828.55m
1.063g

table 5: training time (wall-clock time reported) in
minutes for sgns (d = 300) with different context
types. best-* denotes the best scoring con   gura-
tion for each class found by alg. 1. #pairs shows
a total number of pairs used in sgns training for
each context type.

were trained using parallel training on 10 intel(r)
xeon(r) e5-2667 2.90ghz processors. the results
indicate that class-speci   c con   gurations are not
as lightweight and fast as sp or coord contexts
(schwartz et al., 2016). however, they also suggest
that such con   gurations provide a good balance
between accuracy and speed: they reach peak per-
formances for each class, outscoring all baseline
context types (including sp and coord), while
training is still much faster than with    heavyweight   
context types such as bow, posit or deps-all.
now that we veri   ed the decrease in training
time our algorithm provides for the    nal training,
it makes sense to ask whether the con   gurations it
   nds are valuable in other setups. this will make
the fast training of practical importance.

5.2 generalisation: con   guration transfer
another training setup we    rst test whether
the context con   gurations learned in sect. 5.1 are
useful when sgns is trained in another english
setup (schwartz et al., 2016), with more training
data and other annotation and parser choices, while
evaluation is still performed on siid113x-999.

in this setup the training corpus is the 8b words
corpus generated by the id97 script.13 a
preprocessing step now merges common word
pairs and triplets to expression tokens (e.g.,
bilbo_baggins). the corpus is parsed with labelled
stanford dependencies (de marneffe and manning,
2008) using the stanford pos tagger (toutanova
et al., 2003) and the stack version of the malt
parser (goldberg and nivre, 2012). sgns prepro-
cessing and parameters are also replicated; we now

13code.google.com/p/id97/source/browse/trunk/

context type
bow (win=2)
posit (win=2)
coord (conjlr)
sp
deps-all
best-adj
best-verbs
best-nouns
best-all

adj
0.604
0.585
0.629
0.649
0.574
0.671
0.392
0.581
0.616

verbs
0.307
0.400
0.413
0.458
0.389
0.348
0.455
0.327
0.402

nouns all
0.501
0.471
0.428
0.414
0.492
0.504
0.478
0.535
0.519

0.464
0.469
0.430
0.444
0.464
0.449
0.448
0.489
0.506

table 6: results on the a/v/n siid113x-999 sub-
sets, and on the entire set (all) in the setup from
schwartz et al. (2016). d = 500. best-* are again
the best class-speci   c con   gs returned by alg. 1.

train 500-dim embeddings as in prior work.14

results are presented in tab. 6. the imported
class-speci   c con   gurations, computed using a
much smaller corpus (sect. 5.1), again outperform
competitive baseline context types for adjectives
and nouns. the best-verbs con   guration is
outscored by sp, but the margin is negligible. we
also evaluate another con   guration found using
alg. 1 in sect. 5.1, which targets the overall im-
proved performance without any    ner-grained di-
vision to classes (best-all). this con   guration
(amod+subj+obj+compound+prep+adv+conj) out-
performs all baseline models on the entire bench-
mark. interestingly, the non-speci   c best-all
con   guration falls short of a/v/n-speci   c con   gu-
rations for each class. this unambiguously implies
that the    trade-off    con   guration targeting all three
classes at the same time differs from specialised
class-speci   c con   gurations.
experiments on other languages we next test
whether the optimal context con   gurations com-
puted in sect. 5.1 with english training data are
also useful for other languages. for this, we train
sgns models on the italian (it) and german (de)
polyglot wikipedia corpora with those con   gura-
tions, and evaluate on the it and de multilingual
siid113x-999 (leviant and reichart, 2015).15

our results demonstrate similar patterns as for
english, and indicate that our framework can be
easily applied to other languages. for instance, the
best-adj con   guration (the same con   guration
as in tab. 4 and tab. 7) yields an improvement of 8

14the    translation    from labelled stanford dependencies
into ud is performed using the mapping from de marneffe
et al. (2014), e.g., nn is mapped into compound, and rcmod,
partmod, infmod are all mapped into one bag: acl.

15http://leviants.com/ira.leviant/multilingualvsmdata.html

context type
bow (win=2)
posit (win=2)
coord (conjlr)
sp
deps-all
best-adj
best-verbs
best-nouns

adj-q
31/41
32/41
26/41
26/41
31/41
32/41
24/41
30/41

verb-q
14/19
13/19
11/19
11/19
14/19
12/19
15/19
14/19

noun-q
16/19
15/19
8/19
12/19
16/19
15/19
16/19
17/19

table 7: results on the a/v/n toefl question
subsets. the reported scores are in the following
form: correct_answers/overall_questions. adj-q
refers to the subset of toefl questions targeting
adjectives; similar for verb-q and noun-q. best-*
refer to the best class-speci   c con   gurations from
tab. 3 and tab. 4.

   points and 4    points over the strongest adjectives
baseline in it and de, respectively. we get similar
improvements for nouns (it: 3    points, de: 2   
points), and verbs (it: 2, de: 4).

toefl evaluation we also verify that the se-
lection of class-speci   c con   gurations (sect. 5.1) is
useful beyond the core siid113x evaluation. for this
aim, we evaluate on the a, v, and n toefl ques-
tions (landauer and dumais, 1997). the results are
summarised in tab. 7. despite the limited size of
the toefl dataset, we observe positive trends in
the reported results (e.g., v-speci   c con   gurations
yield a small gain on verb questions), showcasing
the potential of class-speci   c training in this task.

6 conclusion and future work

we have presented a novel framework for select-
ing class-speci   c context con   gurations which
yield improved representations for prominent word
classes: adjectives, verbs, and nouns. its design
and dependence on the universal dependencies
annotation scheme makes it applicable in differ-
ent languages. we have proposed an algorithm that
is able to    nd a suitable class-speci   c con   gura-
tion while making the search over the large space
of possible context con   gurations computation-
ally feasible. each word class requires a different
class-speci   c con   guration to produce improved
results on the class-speci   c subset of siid113x-999
in english, italian, and german. we also show that
the selection of context con   gurations is robust as
once learned con   guration may be effectively trans-
ferred to other data setups, tasks, and languages
without additional retraining or    ne-tuning.

in future work, we plan to test the framework
with    ner-grained contexts, investigating beyond
pos-based word classes and dependency links. ex-
ploring more sophisticated algorithms that can ef-
   ciently search richer con   guration spaces is also
an intriguing direction. another research avenue
is application of the context selection idea to other
representation models beyond sgns tested in this
work, and experimenting with assigning weights to
context subsets. finally, we plan to test the porta-
bility of our approach to more languages.

acknowledgments

this work is supported by the erc consolidator
grant lexical: lexical acquisition across lan-
guages (no 648909). roy schwartz was supported
by the intel collaborative research institute for
computational intelligence (icri-ci). the authors
are grateful to the anonymous reviewers for their
helpful and constructive suggestions.

references
rami al-rfou, bryan perozzi, and steven skiena.
2013. polyglot: distributed word representations
in conll. pages 183   192.
for multilingual nlp.
http://www.aclweb.org/anthology/w13-3520.

mohit bansal, kevin gimpel, and karen livescu.
2014. tailoring continuous word representations
in acl. pages 809   815.
for id33.
http://www.aclweb.org/anthology/p14-2131.

marco baroni, georgiana dinu,

and germ  n
kruszewski. 2014.
don   t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. in acl. pages
238   247.
http://www.aclweb.org/anthology/p14-
1023.

marco baroni, brian murphy, eduard barbu, and
strudel: a corpus-
properties
cognitive science pages 222   254.

massimo poesio. 2010.
based
semantic model
and types.
https://doi.org/10.1111/j.1551-6709.2009.01068.x.

based

on

2010.

bernd bohnet.

dependency

fast
tradiction.
http://www.aclweb.org/anthology/c10-1011.

top
is
in coling.

parsing

accuracy
not
a
pages

and
con-
89   97.

danqi chen and christopher d. manning. 2014.
a fast and accurate dependency parser using
in emnlp. pages 740   750.
neural networks.
http://www.aclweb.org/anthology/d14-1082.

ronan collobert,

michael karlen, koray kavukcuoglu,

jason weston, l  on bottou,
and

language pro-
pavel p. kuksa. 2011.
journal of
cessing (almost)
machine
12:2493   2537.
http://dl.acm.org/citation.cfm?id=1953048.2078186.

natural
from scratch.
research

learning

omer levy and yoav goldberg. 2014b.

lin-
guistic regularities in sparse and explicit word
in conll. pages 171   180.
representations.
http://www.aclweb.org/anthology/w14-1618.

dmitry davidov and ari rappoport. 2006.

ef-
   cient unsupervised discovery of word cat-
egories
high
in acl. pages 297   304.
frequency words.
http://www.aclweb.org/anthology/p06-1038.

symmetric

patterns

using

and

marie-catherine de marneffe, timothy dozat, natalia
silveira, katri haverinen, filip ginter, joakim
nivre, and christopher d. manning. 2014. univer-
sal stanford dependencies: a cross-linguistic typol-
ogy. in lrec. pages 4585   4592. http://www.lrec-
conf.org/proceedings/lrec2014/summaries/1062.html.

marie-catherine de marneffe and christopher d. man-
ning. 2008. the stanford typed dependencies repre-
sentation. in proceedings of the workshop on cross-
framework and cross-domain parser evaluation.
pages 1   8. http://www.aclweb.org/anthology/w08-
1301.

manaal faruqui, jesse dodge, sujay kumar jauhar,
chris dyer, eduard hovy, and noah a. smith.
retro   tting word vectors to semantic
2015.
in naacl-hlt. pages 1606   1615.
lexicons.
http://www.aclweb.org/anthology/n15-1184.

yoav goldberg and omer levy. 2014. id97 ex-
plained: deriving mikolov et al.   s negative-sampling
corr abs/1402.3722.
word-embedding method.
http://arxiv.org/abs/1402.3722.

yoav goldberg and joakim nivre. 2012.

oracle

dynamic
parsing.
http://www.aclweb.org/anthology/c12-1059.

in coling.

arc-eager

pages

for

a
dependency
959   976.

zellig

s. harris.

1954.

structure.
https://doi.org/10.1080/00437956.1954.11659520.

word

distributional
10(23):146   162.

douwe kiela, felix hill, and stephen clark. 2015.
specializing id27s for similarity or
in emnlp. pages 2044   2048.
relatedness.
http://aclweb.org/anthology/d15-1242.

thomas k. landauer and susan t. dumais. 1997.
solutions to plato   s problem: the latent seman-
tic analysis theory of acquisition, induction, and
psychological re-
representation of knowledge.
view 104(2):211   240. https://doi.org/10.1037/0033-
295x.104.2.211.

omer levy and yoav goldberg. 2014c.

ral id27 as
torization.
pages
http://papers.nips.cc/paper/5477.pdf.

in nips.

neu-
implicit matrix fac-
2177   2185.

omer levy, yoav goldberg, and ido dagan. 2015. im-
proving distributional similarity with lessons learned
from id27s. transactions of the acl
3:211   225.

wang ling, chris dyer, alan w. black, and
simple
prob-
in naacl-hlt. pages 1299   1304.

isabel trancoso.
adaptations
lems.
http://www.aclweb.org/anthology/n15-1142.

two/too
syntax

of id97

2015a.

for

wang ling, yulia tsvetkov, silvio amir, ramon fer-
mandez, chris dyer, alan w black, isabel tran-
coso, and chu-cheng lin. 2015b. not all contexts
are created equal: better word representations with
in emnlp. pages 1367   1372.
variable attention.
http://aclweb.org/anthology/d15-1161.

quan liu, hui

jiang, si wei, zhen-hua ling,
learning semantic word
on
knowledge
1501   1511.

and yu hu. 2015.
embeddings
based
constraints.
http://www.aclweb.org/anthology/p15-1145.

ordinal
pages

in acl.

andr   f. t. martins, miguel b. almeida, and noah a.
smith. 2013. turning on the turbo: fast third-order
in acl. pages 617   
non-projective turbo parsers.
622. http://www.aclweb.org/anthology/p13-2109.

ryan t. mcdonald,

joakim nivre, yvonne
quirmbach-brundage, yoav goldberg, dipan-
jan das, kuzman ganchev, keith b. hall, slav
petrov, hao zhang, oscar t  ckstr  m, claudia
bedini, n  ria bertomeu castell  , and jungmee
lee. 2013.
universal dependency annotation
in acl. pages 92   97.
for multilingual parsing.
http://www.aclweb.org/anthology/p13-2017.

oren melamud, ido dagan, and jacob goldberger.
2015. modeling word meaning in context with sub-
in naacl-hlt. pages 472   482.
stitute vectors.
http://www.aclweb.org/anthology/n15-1050.

oren melamud, david mcclosky,

patwardhan, and mohit bansal. 2016.
role of context
learning id27s.
http://www.aclweb.org/anthology/n16-1118.

siddharth
the
types and dimensionality in
in naacl-hlt.

ira leviant and roi reichart. 2015.

separated by
an un-common language: towards judgment lan-
corr
guage informed vector space modeling.
abs/1508.00106. http://arxiv.org/abs/1508.00106.

tomas mikolov, ilya sutskever, kai chen, gregory s.
corrado, and jeffrey dean. 2013. distributed repre-
sentations of words and phrases and their composi-
tionality. in nips. pages 3111   3119.

omer levy and yoav goldberg. 2014a. dependency-
in acl. pages 302   308.

based id27s.
http://www.aclweb.org/anthology/p14-2050.

andriy mnih and koray kavukcuoglu. 2013. learning
id27s ef   ciently with noise-contrastive
estimation. in nips. pages 2265   2273.

ivan vuli  c and anna korhonen. 2016.

is    universal
syntax    universally useful for learning distributed
in acl. pages 518   524.
word representations?
http://anthology.aclweb.org/p16-2084.

john wieting, mohit bansal, kevin gimpel,
from paraphrase
paraphrase model
transactions of the acl 3:345   358.

and karen livescu. 2015.
database
compositional
and back.
http://aclweb.org/anthology/q15-1025.

to

mehmet ali yatbaz, enis sert, and deniz yuret. 2012.
learning syntactic categories using paradigmatic
representations of word context. in emnlp. pages
940   951.
http://www.aclweb.org/anthology/d12-
1086.

mo yu and mark dredze. 2014. improving lexical em-
beddings with semantic knowledge. in acl. pages
545   550.
http://www.aclweb.org/anthology/p14-
2089.

nikola mrk  i  c, ivan vuli  c, diarmuid    s  aghdha, ira
leviant, roi reichart, milica ga  i  c, anna korho-
nen, and steve young. 2017. semantic specialisa-
tion of distributional word vector spaces using mono-
lingual and cross-lingual constraints. transactions
of the acl https://arxiv.org/abs/1706.00374.

joakim nivre et al. 2016. universal dependencies 1.4.
lindat/clarin digital library at institute of for-
mal and applied linguistics, charles university in
prague.

sebastian pad   and mirella lapata. 2007. dependency-
construction
space mod-
computational linguistics 33(2):161   199.

based
els.
https://doi.org/10.1162/coli.2007.33.2.161.

semantic

of

judea pearl. 1984. heuristics: intelligent search strate-

gies for computer problem solving .

jeffrey pennington, richard socher, and christopher
manning. 2014. glove: global vectors for word
in emnlp. pages 1532   1543.
representation.
http://www.aclweb.org/anthology/d14-1162.

slav petrov, dipanjan das, and ryan t. mcdon-
a universal part-of-speech tagset.
ald. 2012.
in lrec. pages 2089   2096.
http://www.lrec-
conf.org/proceedings/lrec2012/summaries/274.html.

hinrich sch  tze. 1993.

tion from scratch.
http://www.aclweb.org/anthology/p93-1034.

part-of-speech induc-
in acl. pages 251   258.

roy schwartz, roi reichart, and ari rappoport. 2015.
symmetric pattern based id27s for im-
proved word similarity prediction. in conll. pages
258   267.
http://www.aclweb.org/anthology/k15-
1026.

roy schwartz, roi reichart, and ari rappoport.
2016.
symmetric patterns and coordinations:
fast and enhanced representations of verbs and
in naacl-hlt. pages 499   505.
adjectives.
http://www.aclweb.org/anthology/n16-1060.

kristina toutanova, dan klein, christopher d. man-
ning, and yoram singer. 2003.
feature-rich
part-of-speech tagging with a cyclic dependency
in naacl-hlt. pages 173   180.
network.
http://aclweb.org/anthology/n/n03/.

joseph

p.

turian,

lev-arie

and
yoshua bengio.
representa-
tions: a simple and general method for semi-
in acl. pages 384   394.
supervised learning.
http://www.aclweb.org/anthology/p10-1040.

ratinov,

2010.

word

jason utt and sebastian pad  . 2014. crosslingual
and multilingual construction of syntax-based vector
space models. transactions of the acl 2:245   258.

ivan vuli  c. 2017. cross-lingual syntactically informed
in eacl. pages
http://www.aclweb.org/anthology/e17-

distributed word representations.
408   414.
2065.

