word representation models for morphologically rich languages in

id4

ekaterina vylomova,1 trevor cohn,1 and xuanli he1 and gholamreza haffari2

1department of computing and information systems, university of melbourne

2faculty of information technology, monash university

6
1
0
2

 

n
u
j
 

4
1

 
 
]
e
n
.
s
c
[
 
 

1
v
7
1
2
4
0

.

6
0
6
1
:
v
i
x
r
a

evylomova@gmail.com tcohn@unimelb.edu.au

xuanlih@student.unimelb.edu.au gholamreza.haffari@monash.edu

abstract

dealing with the co mplex word forms in mor-
phologically rich languages is an open prob-
lem in language processing, and is particularly
important in translation.
in contrast to most
modern neural systems of translation, which
discard the identity for rare words, in this pa-
per we propose several architectures for learn-
ing word representations from character and
morpheme level word decompositions. we in-
corporate these representations in a novel ma-
chine translation model which jointly learns
word alignments and translations via a hard
attention mechanism. evaluating on trans-
lating from several morphologically rich lan-
guages into english, we show consistent im-
provements over strong baseline methods, of
between 1 and 1.5 id7 points.

introduction

1
models of end-to-end machine translation based on
neural networks have been shown to produce excel-
lent translations, rivalling or surpassing traditional
id151 systems (kalchbren-
ner and blunsom, 2013; sutskever et al., 2014; bah-
danau et al., 2015). a central challenge in neural
mt is handling rare and uncommon words. conven-
tional neural mt models use a    xed modest-size vo-
cabulary, such that the identity of rare words are lost,
which makes their translation exceedingly dif   cult.
accordingly sentences containing rare words tend
to be translated much more poorly than those con-
taining only common words (sutskever et al., 2014;
bahdanau et al., 2015). the rare word problem is
particularly exacerbated when translating from mor-
phology rich languages, where the several morpho-
logical variants of words result in a huge vocabulary

with a heavy tail distribution. for example in rus-
sian, there are at least 70 words for dog, encoding
case, gender, age, number, sentiment and other se-
mantic connotations. many of these words share a
common lemma, and contain regular morphological
af   xation; consequently much of the information re-
quired for translation is present, but not in an acces-
sible form for models of neural mt.

in this paper, we propose a solution to this prob-
lem by constructing word representations compo-
sitionally from smaller sub-word units, which oc-
cur more frequently than the words themselves. we
show that these representations are effective in han-
dling rare words, and increase the generalisation ca-
pabilities of neural mt beyond the vocabulary ob-
served in the training set. we propose several neu-
ral architectures for compositional word representa-
tions, and systematically compare these methods in-
tegrated into a novel neural mt model.

more speci   cally, we make use of character se-
quences or morpheme sequences in building word
representations. these sub-word units are combined
using recurrent neural networks (id56s), convolu-
tional neural networks (id98s), or simple bag-of-
units. this work was inspired by research into com-
positional word approaches proposed for language
modelling (e.g., botha and blunsom (2014), kim et
al. (2016)), with a few notable exceptions (ling et
al., 2015b; sennrich et al., 2015; costa-juss   and
fonollosa, 2016), these approaches have not been
applied to the more challenging problem of transla-
tion. we integrate these word representations into a
novel neural mt model to build robust word repre-
sentations for the source language.

our novel neural mt model, is based on the oper-
ation sequence model (osm; durrani et al. (2011),

feng and cohn (2013)), which considers transla-
tion as a sequential decision process. the decisions
involved in generating each target word is decom-
posed into separate translation and alignment fac-
tors, where each factor is modelled separately and
conditioned on a rich history of recent translation
decisions. our osm can be considered as a form of
attentional encoder-decoder bahdanau et al. (2015)
with hard attention in which each decision is con-
textualised by at most one source word, contrasting
with the soft attention in bahdanau et al. (2015).

integrating the word models into our neural osm,
we provide     for the    rst time     a comprehensive
and systematic evaluation of the resulting word rep-
resentations when translating into english from sev-
eral morphologically rich languages, russian, esto-
nian, and romanian. our evaluation includes both
intrinsic and extrinsic metrics, where we compare
these approaches based on their translation perfor-
mance as well as their ability to recover synonyms
for the rare words. we show that morpheme and
character representation of words leads much bet-
ter heldout perplexity although the improvement on
the translation id7 scores is more modest. intrin-
sic analysis shows that the recurrent encoder tends
to capture more morphosyntactic information about
words, whereas convolutional network better en-
codes the lemma. both these factors provide dif-
ferent strengths as part of a translation model, which
might use lemmas to generalise over words sharing
translations, and morphosyntax to guide reordering
and contextualise subsequent translation decisisions.
these factors are also likely to be important in other
language processing applications.

2 related work

most neural models for nlp rely on words as their
basic units, and consequently face the problem of
how to handle tokens in the test set that are out-of-
vocabulary (oov), i.e., did not appear in the train-
ing set (or are considered too rare in the training
set to be worth including in the vocabulary.) often
these words are either assigned a special unk token,
which allows for application to any data, however
it comes at the expense of modelling accuracy es-
pecially in structured problems like language mod-
elling and translation, where the identity of the word
is paramount in making the next decision.

one solution to oov problem is modelling sub-

word units, using a model of a word from its com-
posite morphemes. luong et al. (2013) proposed a
recursive combination of morphs using af   ne trans-
formation, however this is unable to differentiate
between the compositional and non-compositional
cases. botha and blunsom (2014) aim to address
this problem by forming word representations from
adding a sum of each word   s morpheme embeddings
to its id27.

morpheme based methods rely on good morpho-
logical analysers, however these are only available
for a limited set of languages. unsupervised analy-
sers (creutz and lagus, 2007) are prone to segmen-
tation errors, particularly on fusional or polysyn-
thetic languages.
in these settings, character-level
word representations may be more appropriate. sev-
eral authors have proposed convolutional neural net-
works over character sequences, as part of mod-
els of id52 (santos and zadrozny,
2014), language models (kim et al., 2015) and ma-
chine translation (costa-juss   and fonollosa, 2016).
these models are able to capture not just ortho-
graphic similarity, but also some semantics. another
strand of research has looked at recurrent architec-
tures, using long-short term memory units (ling et
al., 2015a; ballesteros et al., 2015) which can cap-
ture long orthographic patterns in the character se-
quence, as well as non-compositionality.

all of the aforementioned models were shown to
consistently outperform standard word-embedding
approaches. but there is no systematic investiga-
tion of the various modelling architectures or com-
parison of characters versus morpheme as atomic
units of word composition. in our work we consider
both morpheme and character levels and study 1)
wether character-based approaches can outperform
morpheme-based, and, importantly, 2) what linguis-
tic lexical aspects are best encoded in each type of
architecture, and their ef   cacy as part of a machine
translation model when translating from morpholog-
ically rich languages.

3 operation sequence model

the    rst contribution of this paper is a neural net-
work variant of the operational sequence model
(osm) (durrani et al., 2011; feng and cohn, 2013).
in osm, the translation is modelled as a sequen-
tial decision process. the words of the target sen-
tence are generated one at a time in a left-to right

figure 1: illustration of the neural operation sequence
model for an example sentence-pair.

order, similar to the decoding strategy in traditional
phrase-based smt. the decisions involved in gener-
ating each target word is decomposed into a number
of separate factors, where each factor is modelled
separately and conditioned on a rich history of re-
cent translation decisions.

in previous work (durrani et al., 2011; feng and
cohn, 2013), the sequence of operations is modelled
as markov chain with a bounded history, where each
translation decision is conditioned on a    nite history
of past decisions. using deep neural architectures,
we model the sequence of translation decisions as
a non-markovian chain, i.e. with unbounded his-
tory. therefore, our approach is able to capture
long-range dependencies which are commonplace in
translation and missed by previous approaches.

more speci   cally, the operations are (i) genera-
tion of a target word, (ii) jumps over the source sen-
tence to capture re-ordering (to allow different sen-
tence reordering in the target vs. source language),
(iii) aligning to null to capture gappy phrases, and
(iv)    nishing the translation process. the probabil-
ity of a sequence of operations to generate a tar-
get translation t for a given source sentence s is
p(t, a|s) =

|t|+1(cid:89)

|t|(cid:89)

p(  j|t<j,   <j, s)

p(tj|t<j,   j, s)

(1)

j=1

j=1

where   j is a jump action moving over the source
sentence (to align a target word to a source word
or null) or    nishing the translation process   |t|+1 =
finish. it is worth noting that the sequence of oper-
ations for generating a target translation (in a left-to-

figure 2: model architecture for the several approaches
to learning word representations, showing from left: bag-
of-morphs, bilstm over morphs, and the character con-
volution. note that the bilstm is also applied at the
character level. the input word, t  ppi-de-ga, is estonian
for speckled, bearing plural (de) and comitative (ga) suf-
   xes.

(cid:17)

(cid:16)

hj   1, r(t)
tj   1

right order) has a 1-to-1 correspondence to an align-
ment a, so the use of p (t, a|s) in the left-hand-side.
our model generates the target sentence and the
sequence of operations with a recurrent neural net-
work (figure 1). at each stage, the id56 state is a
function of the previous state, the previously gener-
ated target word, and an aligned source word, hj =
using a single layer per-
mlp
ceptron (mlp) which applies an af   ne transforma-
tion to the concatentated input vectors followed by a
tanh activation function, where r(t)     rvt   et and
r(s)     rvs  es are id27 matrices with
vs the size of the source vocabulary, vt the size of
the target vocabulary, and et and es the word em-
bedding sizes for the target and source languages,
respectively.

, r(s)
sij

the model then generates the target word ti and

index of the source word to be translated next,1

tj     softmax(cid:0) a   ne(hj)(cid:1)

ij+1     softmax

  (s, i   j, t   j)b(f)
+ r(s)w(sh)hj + r(s)w(st)r(t)
tj

(cid:17)

(cid:16)

where a   ne performs an af   ne transformation of its
input,2 and the parameters include w(sh)     res  h,
w(ts)     ret   h, b(f)     rf , and f is the dimen-
sionality of the feature vector   (.) representing the
1the indices 0 and |s| + 1 represent the null and finish

operations.

2an af   ne transform multiplies the input vector by a matrix
and adding a bias vector, equivalent to a full connected hidden
layer with linear activation.

t  ppidega  lookupaveragingt  ppidegalstm intwo directionst  ppidega+highway networkmax-poolid165 convolutionsinduced alignment structure (explained in the next
paragraph). the matrix encoding of the source sen-
tence r(s)     r(|s|+2)  es is de   ned as

r(s) =

rnull, r(s)

s1 , . . . , r(s)

s|s|, rfinish

(2)

(cid:104)

(cid:105)

.

where it includes the embeddings of the source sen-
tence words and the null and finish actions.
the feature matrix   (|s|, i   j, t   j)     r(|s|+2)  f
captures the important aspects between a candidate
position for the next alignment and the current align-
ment position;
this is reminiscent of the features
captured in the id48 alignment model. the fea-
ture vector in each row is composed of two parts
:3 (i) the    rst part is a one-hot vector activating the
proper feature depending whether ij+1     ij is equal
to {0, 1,    2,       1} or if the action is null or
finish, and (ii) the second part consists of two fea-
tures ij+1     ij and ij+1   ij
|s|

note that the neural osm can be considered as
a hard attentional model, as opposed to the soft at-
tentional neural translation model (bahdanau et al.,
2015).
in their soft attentional model, a dynamic
summary of the source sentence is used as context
to each translation decision, which is formulated as
a weighted average of the encoding of all source po-
sitions.
in the hard attentional model this context
comes from the encoding of a single    xed source
position. this has the bene   t of allowing external
information to be included into the model, here the
predicted alignments from high quality word align-
ment tools, which have complementary strengths
compared to neural network translation models.
4 word representation models
now we turn to the problem of learning word rep-
resentations. as outlined above, when translating
morphologically rich languages, treating word types
as unique discrete atoms is highly naive and will
compromise translation quality. for better accuracy,
we would need to characterise words by their sub-
word units, in order to capture the lemma and mor-
phological af   xes, thereby allowing better generali-
sation between similar word forms.

in order to test this hypothesis, we consider both
morpheme and character level encoding methods
3more generally,   (.) can capture any aspects of the past
alignment decisions, hence it can be used to impose structural
biases to constrain the alignment space in neural osm, e.g.
symmetry, fertility, and position bias.

which we compare to the baseline id27
approach. for each type of sub-word encoder we
learn two word representations: one estimated from
the sub-units and the id27.4 then we
run max pooling over both embeddings to obtain the
word representation, rw = mw (cid:11) ew, where mw
is the embedding of word w and ew is the sub-word
encoding. the max pooling operation (cid:11) captures
non-compostionality in the semantic meaning of a
word relative to its sub-parts. we assume that the
model would favour unit-based embeddings for rare
words and word-based for more common ones.
let u be the vocabulary of sub-word units, i.e.,
morphemes or characters, eu be the dimensional-
ity of unit embeddings, and m     reu  |u| be the
matrix of unit embeddings. suppose that a word w
from the source dictionary is made up of a sequence
of units uw := [u1, . . . , u|w|], where |w| stands for
the number of constituent units in the word. we
combine the representation of sub-word units using
a lstm recurrent neural networks (id56), convolu-
tional neural network (id98), or simple bag-of-units
(described below). the resulting word representa-
tions are then fed to our neural osm in eqn (2) as
the source id27s.

4.1 bag of sub-word units
this method is inspired by (botha and blunsom,
2014) in which the embeddings of sub-word units
mu,

are simply added together, ew = (cid:80)

u   uw
where mu is the embedding of sub-word unit u.

j = lstm(h   
j+1, muj ) where h   

4.2 bidirectional lstm encoder
the encoding of the word is formulated using a pair
of lstms (denoted bi-lstm) one operating left-
to-right over the input sequence and another oper-
ating right-to-left, h   
j   1, muj ) and
h   
j = lstm(h   
j and h   
j are
the lstm hidden states.5 the source word is then
represented as a pair of hidden states, from left- and
right-most states of lstms. these are fed into mul-
tilayer perception (mlp) with a single hidden layer
and a tanh activation function to form the word rep-
resentation, ew = mlp

h   
|uw|, h   

(cid:16)

(cid:17)

.

1

4we only include id27s for common words; rare

words share a unk embedding.

5the memory cells are computed as part of the recurrence,

suppressed here for clarity.

4.3 convolutional encoder
the last word encoder we consider is a convolu-
tional neural network,
inspired by a similar ap-
proach in language modelling (kim et al., 2016).
let uw     reu  |u|w denote the unit-level repre-
sentation of w, where the jth column corresponds
to the unit embedding of uj. the idea of unit-
level id98 is to apply a kernel ql     reu  kl with
the width kl to uw to obtain a feature map fl    
r|u|w   kl+1. more formally, for the jth element of
the feature map the convolutional representation is
fl(j) = tanh((cid:104)uw,j, ql(cid:105)+b), where uw,j     reu  kl
is a slice from uw which spans the representations
of the jth unit and its preceding kl     1 units, and

i,j aijbij = tr(cid:0)abt(cid:1) denotes the

(cid:104)a, b(cid:105) = (cid:80)

frobenius inner product. for example, suppose that
the input has size [4    9], and a kernel has size
[4    3] with a sliding step being 1. then, we ob-
tain a [1    7] feature map. this process implements
a character id165, where n is equal to the width
of the    lter. the word representation is then de-
rived by max pooling the feature maps of the ker-
nels:    l
in order to
capture interactions between the character id165s
obtained by the    lters, a highway network (srivas-
tava et al., 2015) is applied after the max pooling
layer, ew = t (cid:12) mlp(rw) + (1     t) (cid:12) rw, where
t = mlp  (rw) is a sigmoid gating function which
modulates between a tanh mlp transformation of
the input (left component) and preserving the input
as is (right component).

rw(l) = maxj fl(j).

:

5 experiments

the setup. we compare the different word rep-
resentation models based on three morphologically
rich languages using both exterinsic and intrinsic
evaluations. for exterinsic evaluation, we investi-
gate their effects in translating to english from esto-
nian, romanian, and russian using our neural osm.
for intrinsic evaluation, we investigate how accu-
rately the models recover semantically/syntactically
related words to a set of given words.

datasets. we use parallel bilingual data from eu-
roparl for estonian-english and romanian-english
(koehn, 2005), and web-crawled parallel data for
russian-english (antonova and misyurev, 2011).
for preprocessing, we tokenize, lower-case, and    l-
ter out sentences longer than 30 words. further-

language

et-en
a
w
4.93
6.71
4.62
5.75
5.44
4.52
4.48
5.91
4.63
5.61

ro-en
a
w
3.22
3.35
2.95
3.37
2.95
3.20
3.30
3.13
3.14
3.30

15.81
14.64
13.02
15.96
15.89

bilstmword
bilstmchar
id98char
avemorph
bilstmmorph
table 3: word (w) and alignment (a) perplexities results
for the development data.

ru-en
a
w
5.95
5.22
5.10
5.12
5.19

more, we apply a frequency threshold of 5, and re-
place all low-frequency words with a special unk
token. we split the corpora into three partitions:
training (100k), development(10k), and test(10k);
table 1 provides the datasets statistics.

morfessor training. we use morfessor cat-
map (creutz and lagus, 2007) to perform mor-
phological analysis needed for morph-based neu-
ral models. morfessor does not rely on any
linguistic knowledge,
instead it relays on mini-
mum description length principle to construct a
set of stems, af   xes and paradigms that explains
the data. each word form is then represented as
(pre   x)   (stem)+(suf   x)   .

we ran morfessor on the entire initial datasets, i.e
before    ltering out long sentences. the word per-
plexity is the only morfessor parameter that has to
be adjusted. the parameter depends on the vocab-
ulary size:
larger vocabulary requires higher per-
plexity number; setting the perplexity threshold to
a small value results in over-splitting. we experi-
mented with various thresholds and tuned these to
yield the most reasonable morpheme inventories.6

table 1 presents the percentage of unknown
words in the test for each source language . for re-
construction we considered the words from the na-
tive alphabet only. the recovering rate depends on
the model. for characters all the words could be eas-
ily rebuilt. in case of morpheme-based approach the
quality mainly depends on the morfessor output and
the level of id40.
in terms of mor-
phemes, estonian presents the highest reconstruc-
tion rate, therefore we expect it to bene   t the most
from the morpheme-based models. romanian, on
the other hand, presents the lowest unknown words
rate being the most morphologically simple out of
the three languages. morfessor quality for russian

6 the selected thresholds were 600, 60 and 240 for russian,

romanian and estonian, respectively.

set
ru-en
ro-en
et-en

train

tokens

types

development

tokens

types

tokens

test
types

oov rate

1,639k-1,809k 145k-65k 150k-168k 35k-18k 150k-167k 35k-18k
13k-8k
1,782k-1,806k
1,411k-1,857k
21k-8k

181k-183k
141k-188k

182k-183k
142k-189k

38k-24k
90k-25k

13k-9k
21k-9k

45%
30%
45%

table 1: corpus statistics for parallel data between russian/romanian/estonian and english. the oov rate are the
fraction of word types in the source language that are in the test set but are below the frequency cut-off or unseen in
training.

language

phrase-based baseline
bilstmchar
id98char
bilstmmorph
bilstmword

ru-en

id7 meteor
15.02
15.81
15.94
15.61
15.70

44.07
44.97
45.09
44.96
44.98

et-en

id7 meteor
24.40
26.14
25.97
26.14
26.03

57.23
58.47
58.45
58.48
58.33

ro-en

id7 meteor
39.68
41.10
41.09
41.15
40.97

71.25
72.13
72.06
72.20
72.15

table 2: id7 and meteor scores for re-ranking the test sets.

was the worst one, so we expect that russian should
mainly bene   t from character-based models.

5.1 extrinsic evaluation: mt
training. we annotate the training sentence-pairs
with their sequence of operations to training the neu-
ral osm model. we    rst run a word aligner7 to align
each target word to a source word. we then read
off the sequence of operations by scanning the tar-
get words in a left-to-right order. as a result, the
training objective consists of maximising the joint
id203 of target words and their alignments eqn
1, which is performed by stochastic id119
(sgd). the training stops when the likelihood ob-
jective on the development set starts decreasing.

for the re-ranker, we use the standard features
generated by moses8 as the underlying phrase-based
mt system plus two additional features coming
from the neural mt model. the neural features are
based on the generated alignment and the transla-
tion probabilities, which correspond to the    rst and
second terms in eqn 1, respectively. we train the re-
ranker using mert (och, 2003) with 100 restarts.
translation metrics. we use id7 (papineni et
al., 2002) and meteor9 (denkowski and lavie,
2014) to measure the translation quality against the
reference. id7 is purely based on the exact match
of id165s in the generated and reference transla-
tion, whereas meteor takes into account matches
based on stem, synonym, and paraphrases as well.
this is particularly suitable for our morphology rep-

7we made use of fast_align in our experiments https:

//github.com/clab/fast_align.

8https://github.com/moses-smt.
9http://www.cs.cmu.edu/~alavie/meteor/.

resentation learning methods since they may result
in using the translation of paraphrases. we train the
paraphrase table of meteor using the entire initial
bilingual corpora based on pivoting (bannard and
callison-burch, 2005).

results. table 3 shows the translation and align-
ment perplexities of the development sets when the
models are trained. as seen, the id98char model
leads to lower word and alignment perplexities in
almost all cases. this is interesting, and shows
the power of this model in    tting to morpholog-
ically complex languages using only their charac-
ters. table 2 presents id7 and meteor score
results, where the re-ranker is optimised by the me-
teor and id7 when reporting the corresponding
score. as seen, re-ranking based on neural models   
scores outperforms the phrase-based baseline. fur-
thermore, the translation quality of the bilstmmorph
model outperforms others for romanian and esto-
nian, whereas the id98char model outperforms oth-
ers for russian which is consistent with our expec-
tations. we assume that replacing morfessor with
real morphology analyser for each language should
improve the performance of morpheme-based mod-
els, but leave it for future research. however, the
translation quality of the neural models are not sig-
ni   cantly different, which may be due to the convo-
luted contributions of high and low frequency words
into id7 and meteor. therefore, we investi-
gate our representation learning models intrinsically
in the next section.

intrinsic evaluation

5.2
we now take a closer look at the embeddings learned
by the models, based on how well they capture the

semantic and morphological information in the near-
est neighbour words. learning representations for
low frequency words is harder than that for high-
frequency words, since they cannot capitalise as re-
liably on their contexts. therefore, we split the test
lexicon into 6 subsets according to their frequency
in the training set: [0-4], [5-9], [10-14], [15-19],
[20-50], and 50+. since we set out word frequency
threshold to 5 for the training set, all words appear-
ing in the frequency band [0,4] are in fact oovs for
the test set. for each word of the test set, we take its
top-20 nearest neighbours from the whole training
lexicon (without threshold) using cosine metric.

e, p(e(cid:48)|e) = (cid:80)

semantic evaluation. we investigate how well
the nearest neighbours are interchangable with a
query word in the translation process. so we for-
malise the notion of semantics of the source words
based on their translations in the target language.
we use pivoting to de   ne the id203 of a can-
didate word e(cid:48) to be the synonym of the query word
f p(f|e)p(e(cid:48)|f ), where f is a tar-
get language word, and the translation probabilities
inside the summation are estimated using a word-
based translation model trained on the entire bilin-
gual corpora (i.e. before splitting into train/dev/test
sets). we then take the top-5 most probable words
as the gold synonyms for each query word of the test
set.10

we measure the quality of predicted near-
est neighbours using the multi-label accuracy,11
w   s 1[g(w)   n(w)(cid:54)=   ], where g(w) and n (w)
1|s|
are the sets of gold standard synonyms and near-
est neighbors for w respectively; the function 1[c]
is one if the condition c is true, and zero other-
wise. in other words, it is the fraction of words in
s whose nearest neighbours and gold standard syn-
onyms have non-empty overlap.
table 4 presents the semantic evaluation results.
as seen, on words with frequency     50, the id98char
model performs best across all of the three lan-
guages. its superiority is particularly interesting for
the oov words (i.e. the frequency band [0,4]) where
the model has cooked up the representations com-

(cid:80)

10we remove query words whose frequency is less than a
threshold in the initial bilingual corpora, since pivoting may not
result in high quality synonyms for such words.

11we evaluated using mean reciprocal rank (mrr) measure
as well, and obtained results consistent with the multi-label ac-
curacy (omitted due to space constraints).

model freq.

0-4

10-14

5-9
russian

bilstmword
bilstmchar
id98char
avemorph
bilstmmorph

bilstmword
bilstmchar
id98char
avemorph
bilstmmorph

-

0.16
0.43
0.03
0.01

-

0.02
0.59
0.05
0.01

bilstmword
bilstmchar
id98char
avemorph
bilstmmorph

-

0.13
0.48
0.07
0.013

0.49
0.36
0.48
0.34
0.77
0.71
0.33
0.21
0.24
0.38
romanian
0.63
0.47
0.55
0.41
0.82
0.81
0.52
0.40
0.38
0.53
estonian
0.62
0.48
0.39
0.48
0.75
0.70
0.40
0.29
0.36
0.45

15-19

20-50

50+

0.61
0.59
0.77
0.40
0.49

0.71
0.62
0.84
0.61
0.61

0.76
0.71
0.81
0.55
0.65

0.81
0.74
0.88
0.71
0.72

0.91
0.85
0.81
0.78
0.85

0.91
0.83
0.84
0.84
0.82

0.70
0.55
0.76
0.47
0.52

0.79
0.63
0.78
0.56
0.60

0.90
0.78
0.78
0.76
0.76

table 4: semantic evaluation of nearest neighbours us-
ing multi-label accuracy on words in different frequency
bands.

pletely based on the characters. for high frequency
words (> 50), the bilstmword outperforms the other
models.
morphological evaluation. we now turn to eval-
uating the morphological component. for this eval-
uation, we focus on russian since it has a notori-
ously hard morphology. we run another morpholog-
ical analyser, mystem (segalovich, 2003), to gener-
ate linguistically tagged morphological analyses for
a word, e.g. pos tags, case, person, plurality, etc.
we represent each morphological analysis with a bit
vector showing the presence of these grammatical
features. each word is then assigned a set of bit
vectors corresponding to the set of its morphological
analyses. as the morphology similarity between two
words, we take the minimum of hamming similar-
ity12 between the corresponding two sets of bit vec-
tors. table 5(a) shows the average morphology sim-
ilarity between the words and their nearest neigh-
bours across the frequency bands. likewise, we rep-
resent the words based on their lemma features; ta-
ble 5(b) shows the average lemma similarity. we can
see that both character-based models capture mor-
phology far better than morpheme-based ones, es-
pecially in the cases of oov words. but it is also
clear that id98 tends to outperform bi-lstm in case
where we compare lemmas, and bi-lstm seems to
be better at capturing af   xes.

12the hamming similarity is the number of bits having the

same value in two given bit vectors.

model \ freq.

0-4

bilstmword
bilstmchar
id98char
avemorph
bilstmmorph

-

0.91
0.79
0.74
0.72

model \ freq.

0-4

bilstmword
bilstmchar
id98char
avemorph
bilstmmorph

-

0.05
0.20
0.02
0.00

5-9

10-14

russian
0.77
0.85
0.79
0.76
0.77

0.74
0.84
0.80
0.73
0.75

(a)

5-9

10-14

russian
0.05
0.08
0.41
0.03
0.04

0.03
0.05
0.37
0.02
0.02

15-19

20-50

50+

0.78
0.85
0.79
0.77
0.79

0.81
0.85
0.79
0.79
0.80

0.84
0.86
0.79
0.79
0.83

15-19

20-50

50+

0.06
0.10
0.42
0.04
0.06

0.09
0.13
0.44
0.06
0.09

0.15
0.18
0.41
0.12
0.15

(b)

table 5: morphology analysis for nearest neighbours
based on (a) grammar tag features, and (b) lemma fea-
tures.

now we take a closer look at the character-based
models. we manually created a set of non-existing
russian words of three types. words in the    rst set
consist of known root and af   xes, but their combina-
tion is atypical, although one might guess the mean-
ing. the second type corresponds to the words with
non-existing(nonsense) root, but meaningful af   xes,
so one might guess its part of speech and some other
properties, e.g. gender, plurality, case. finally, a
third type comprises of the words with all known
root and morphemes, but the combination is abso-
lutely not possible in the language and the meaning
is hard to guess.

table 6 shows that id98 is strongly biased to-
wards longest substring matching from the begin-
ning of the word, and it yields better recall in re-
trieving words sharing same lemma. bi-lstm, on
the other hand, is mainly focused on matching the
patterns from both ends regardless the middle of the
word. and it results in higher recall of the words
sharing same grammar features.

6 conclusion

this paper proposes a novel translation model in-
corporating a hard attentional mechanism.
in this
context, we have compared four different models
of morpheme- and character-level word representa-
tions for the source language. these models lead
to more robust encodings of words in morpholog-
ical rich languages, and overall better translations
than simple id27s. our detailed anal-
yses have shown that word-embeddings are superior
for frequent words, whereas convolutional method

nemec-kos  t

german-ness (s,f,nom,sg)

id98char
nemec+k+oe
german (a,nom,sg,plen,n)
nemec+k+om
german (a,abl,sg,plen,n)
nemec+k+ogo
german (a,gen,sg,plen,n)
nemec+k+oi
german (a,gen,sg,plen,f)
nemec
german (s,m,anim,nom,sg)

bilstmchar
ne+blagodar+n+os  t
ingratitude (s,f,nom,sg)
ne+forma  l +n+os  t
informality (s,f,nom,sg)
ne+mysl+im+os  t
unthinkableness (s,f,nom,sg)
ne+kompeten+t+n+os  t
incompetence (s,f,nom,sg)
ne+gramot+n+os  t
illiteracy (s,f,nom,sg)

butjav-ka

butjav-vy (s,f,sg,nom,nons)

id98char
bu  l var+a
boulvard (s,m,gen,sg)
bu  l var+e
boulvard (s,m,abl,sg)
bu  l var
boulvard (s,m,nom,sg)
bulav+ki
pins (s,f,nom,pl)
bu  l var+ov
boulvard (s,m,gen,pl)

bilstmchar
dubin+k+a
truncheon (s,f,nom,sg)
d  orak+a
djoraka (s,f,nom,sg,nons)
  irot+a
latitude (s,f,nom,sg)
mozambik
mozambique (s,geo,m,nom,sg)
mazeka
mazeka (s,f,nom,sg,nons)

pere-ton-ul-sja

re-sunk himself (v,pf,intr,praet,sg,indic,m)

id98char
pere+me  +a+l+i+  s
mixed (v,pf,intr,praet,pl,indic)
pere+rug+a+l+o+  s
squabbled (v,pf,intr,praet,sg,indic,n)
pri+kos+n+u+l+sja
touched (v,intr,praet,sg,indic,m,pf)
pri+kas+n+u+l+sja
touched (v,intr,praet,sg,indic,m,pf,inc)
pere+rod+i+l+sja
reborn (v,intr,praet,sg,indic,m,pf)

bilstmchar
pere+mest+i+l+sja
moved (v,intr,praet,sg,indic,m,pf)
za+pis+a+l+sja
enrolled (v,intr,praet,sg,indic,m,pf)
za+blud+i+l+sja
lost (v,pf,intr,praet,sg,indic,m)
za+pusk+a+l+sja
launched (v,ipf,intr,praet,sg,indic,m)
pere+men+i+l+a+s
changed (v,pf,intr,praet,sg,indic,f)

table 6: analysis of the    ve most similar words to nonsense rus-
sian words, under the id98char and bilstmchar word encodings based
on cosine similarity. the diacritic    indicates softness. pos tags:
s-noun, a-adjective, v-verb; gender: m-masculine,
f -feminine, n-
neuter; number: sg-singular, pl-plural; case: nom-nominative, gen-
genitive, dat-dative, acc-accusative, ins-instrumental, abl-prepositional,
loc-locative; tense: praes-present, inpraes-continuous, praet-past, pf -
perfect, ipf -imperfect; indic-indicative; transitivity: trans-transitive,
intr-intransitive; adjective form: br-brevity, plen-full form, poss-
possessive; comparative: supr-superlative, comp-comparative; noun
person: 1p-   rst, 2p-second, 3p-third;other: geo-geolocation, nons-
nonsense, inc-incorrect spelling, famn-family name, praed-predicative

is best for handling rare words. comparison of the
convolutional and recurrent methods over character
sequences has shown that the convolutional method
better captures the lemma, which is of critical impor-
tance for translating out-of-vocabulary words, and
would also be key in many other semantic applica-
tions.

references
[antonova and misyurev2011] alexandra antonova and
alexey misyurev. 2011. building a web-based paral-

lel corpus and    ltering out machine-translated text. in
proceedings of the 4th workshop on building and us-
ing comparable corpora: comparable corpora and
the web, pages 136   144. association for computa-
tional linguistics.

[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio.
2015. neural machine
translation by jointly learning to align and translate. in
proceedings of the international conference on learn-
ing representations (iclr), san diego, ca.

[ballesteros et al.2015] miguel ballesteros, chris dyer,
and noah a smith. 2015. improved transition-based
parsing by modeling characters instead of words with
lstms. arxiv preprint arxiv:1508.00657.

[bannard and callison-burch2005] colin bannard and
chris callison-burch. 2005. id141 with bilin-
gual parallel corpora. in proceedings of the 43rd an-
nual meeting on association for computational lin-
guistics, pages 597   604. association for computa-
tional linguistics.

[botha and blunsom2014] jan a botha and phil blun-
som. 2014. compositional morphology for word rep-
resentations and language modelling. arxiv preprint
arxiv:1405.4273.

[costa-juss   and fonollosa2016] marta costa-juss   and
jose fonollosa. 2016. character-based neural ma-
chine translation. arxiv preprint arxiv:1603.00810.

[creutz and lagus2007] mathias creutz and krista la-
gus. 2007. unsupervised models for morpheme seg-
mentation and morphology learning. acm transac-
tions on speech and language processing (tslp),
4(1):3.

[denkowski and lavie2014] michael denkowski

and
2014. meteor universal: language
alon lavie.
speci   c translation evaluation for any target language.
in in proceedings of the ninth workshop on statistical
machine translation. citeseer.

[durrani et al.2011] nadir durrani, helmut schmid, and
alexander m. fraser. 2011. a joint sequence trans-
lation model with integrated reordering. in the 49th
annual meeting of the association for computational
linguistics (acl), pages 1045   1054.

[feng and cohn2013] yang feng and trevor cohn. 2013.
a markov model of machine translation using non-
parametric bayesian id136. in proceedings of the
51st annual meeting of the association for computa-
tional linguistics (acl), pages 333   342.

[kalchbrenner and blunsom2013] nal kalchbrenner and
phil blunsom. 2013. recurrent continuous translation
models. in emnlp, pages 1700   1709.

[kim et al.2015] yoon kim, yacine jernite, david son-
2015. character-
arxiv preprint

tag, and alexander m rush.
aware neural
arxiv:1508.06615.

language models.

[kim et al.2016] yoon kim, yacine jernite, david son-
tag, and alexander rush. 2016. character-aware neu-
ral language models. in proceedings of the thirtieth
aaai conference on arti   cial intelligence (aaai-16).
[koehn2005] philipp koehn. 2005. europarl: a parallel
corpus for id151. in mt sum-
mit, volume 5, pages 79   86.

[ling et al.2015a] wang ling, tiago lu  s, lu  s marujo,
ram  n fernandez astudillo, silvio amir, chris dyer,
alan w black, and isabel trancoso. 2015a. finding
function in form: compositional character models for
open vocabulary word representation. arxiv preprint
arxiv:1508.02096.

[ling et al.2015b] wang ling,
and alan black.

dyer,
based id4.
arxiv:1511:04586.

isabel trancoso, chris
character-
arxiv preprint

2015b.

[luong et al.2013] thang luong, richard socher, and
christopher d manning. 2013. better word represen-
tations with id56s for morphology.
in conll, pages 104   113. citeseer.

[och2003] franz josef och. 2003. minimum error rate
training in id151. in proceed-
ings of the 41st annual meeting on association for
computational linguistics-volume 1, pages 160   167.
association for computational linguistics.

[papineni et al.2002] kishore papineni, salim roukos,
todd ward, and wei-jing zhu. 2002. id7: a method
for automatic evaluation of machine translation.
in
proceedings of the 40th annual meeting on association
for computational linguistics, pages 311   318. associ-
ation for computational linguistics.

[santos and zadrozny2014] cicero d. santos and bianca
zadrozny. 2014. learning character-level represen-
tations for part-of-speech tagging. in proceedings of
the 31st international conference on machine learn-
ing (icml-14), pages 1818   1826.

[segalovich2003] ilya segalovich. 2003. a fast mor-
phological algorithm with unknown word guessing in-
duced by a dictionary for a web search engine.
in
mlmta, pages 273   280. citeseer.

[sennrich et al.2015] rico sennrich, barry haddow, and
alexandra birch. 2015. id4
arxiv preprint
of rare words with subword units.
arxiv:1508.07909.

[srivastava et al.2015] rupesh k srivastava, klaus greff,
and j  rgen schmidhuber. 2015. training very deep
networks. in advances in neural information process-
ing systems, pages 2368   2376.

[sutskever et al.2014] ilya sutskever, oriol vinyals, and
quoc vv le. 2014. sequence to sequence learning
with neural networks. in neural information process-
ing systems (nips), pages 3104   3112, montr  al.

