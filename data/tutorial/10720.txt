separating answers from queries for neural reading comprehension

dirk weissenborn

language technology lab, dfki

alt-moabit 91c
berlin, germany

6
1
0
2

 

p
e
s
7
2

 

 
 
]
l
c
.
s
c
[
 
 

3
v
6
1
3
3
0

.

7
0
6
1
:
v
i
x
r
a

dirk.weissenborn@dfki.de

abstract

we present a novel neural architecture for an-
swering queries, designed to optimally lever-
age explicit support in the form of query-
answer memories. our model is able to re-
   ne and update a given query while sepa-
rately accumulating evidence for predicting
the answer. its architecture re   ects this sep-
aration with dedicated embedding matrices
and loosely connected information pathways
(modules) for updating the query and accu-
mulating evidence. this separation of respon-
sibilities effectively decouples the search for
query related support and the prediction of
the answer. on recent benchmark datasets for
reading comprehension, our model achieves
state-of-the-art results. a qualitative analysis
reveals that the model effectively accumulates
weighted evidence from the query and over
multiple support retrieval cycles which results
in a robust answer prediction.

1

introduction

recent advances in many nlp tasks were achieved
by utilizing neural architectures that employ some
form of external memory. making use of explicit
memories enables these models to bridge long-range
dependencies and solve more complex reasoning
tasks that might involve multiple observations. neu-
ral architectures equipped with explicit memories
are able to achieve impressive results on a variety of
nlp tasks. memory networks (weston et al., 2015;
sukhbaatar et al., 2015), for example, are able to an-
swer questions which require a higher level of read-

ing comprehension and possibly reason over multi-
ple observations.

the use of some form of external memory ap-
pears essential when tackling complex queries that
require comprehension of a given context (support).
the memory module stores explicit, contextual in-
formation of the support which either contains the
correct answer or clues that can lead to it. for
instance, attention-based architectures (hermann et
al., 2015) encode supporting contexts typically with
(bi-directional) recurrent neural networks (id56)
into h-dimensional latent representations (hidden
states), which jointly serve as a form of memory.
end-to-end memory networks (sukhbaatar et al.,
2015) are similar although they split the support into
individual parts that are separately encoded to form
memories. these systems utilize the same learned
query representation both for selecting memories
(matching) and predicting the actual answer (pre-
diction) which can affect the overall performance.
recent work successfully addressed this issue by di-
rectly using the retrieved hidden states (cheng et al.,
2016) or the attention weights (kadlec et al., 2016)
as pointers to the answer (vinyals et al., 2015) for
prediction.

in this work we propose a novel end-to-end neural
architecture for answering queries. it explicitly sep-
arates queries from answers which is re   ected in the
representation of supporting knowledge as query-
answer pairs and in the general architecture. in par-
ticular, we employ dedicated embedding matrices
and loosely connected information pathways (mod-
ules) for updating the query and answer representa-
tion. this separation of responsibilities increases the

capabilities of the model to search through the sup-
port while selectively accumulating evidence for the
answer in parallel. the representation of the sup-
port re   ects the task of answering queries directly
which facilitates its utilization by the model. we
evaluate our approach on two reading comprehen-
sion tasks that involve answering cloze-style (tay-
lor, 1953) queries, namely the id98/dailymail qa
task (hermann et al., 2015) and the named entity
(ne) subtask of the children   s book test (cbt)
(hill et al., 2016). these datasets provide only one
document as support per query but this is not a re-
striction because our model can also handle multi-
ple documents. our contributions are the follow-
ing: i) we introduce a new representation of support-
ing memories in form of query-answer pairs (  3.1)
based on which ii) we develop a neural architec-
ture for answering queries that leverages this rep-
resentation (  3), iii) we evaluate our system on two
reading comprehension benchmark datasets against
other competitive systems achieving state-of-the-art
results (  4.2), and iv) we give insights into the sys-
tems ability to utilize multiple support retrieval cy-
cles for improving its reading comprehension per-
formance (  4.3 and   4.4).

2 related work

utilizing explicit memory in end-to-end differen-
tiable neural architectures has enabled models to
solve complex tasks that require learning simple al-
gorithms, or processing and reasoning over large
amounts of contextual information. traditional ar-
chitectures, such as id56s like the lstm (hochre-
iter and schmidhuber, 1997) or gru (chung et al.,
2014), are not suited for these kind of tasks due
to their limited memory capacity and dif   culties to
learn long-range dependencies in large contexts.

graves et al. (2014) introduced neural turing
machines (ntm). ntms augment traditional id56s
with external memory that can be written to and
read from. the memory is composed of a prede-
   ned number of writable slots. they are addressable
via content or position shifts which allows solving
simple algorithmic tasks. the capacity is also lim-
ited, but the external memory slots can carry infor-
mation over long ranges more easily than traditional
id56s. ntms inspired subsequent work on using

different kinds of external memory, like queues and
stacks for solving transduction tasks (grefenstette et
al., 2015) or neural theorem provers to perform    rst-
order (rockt  aschel and riedel, 2016) id136.

attention-based architectures store information,
typically in form of hidden id56 states, dynamically
for each time-step while processing a given con-
text. these states are retrieved through an attention
mechanism that softly selects a state that matches
a given query state. this can be viewed as keep-
ing the encoded context in memory. such architec-
tures achieved impressive results on tasks that in-
volve larger contexts such as reading comprehen-
sion (hermann et al., 2015; kadlec et al., 2016;
chen et al., 2016), machine translation (bahdanau
et al., 2015; luong et al., 2015) or recognizing tex-
tual entailment (rockt  aschel et al., 2016; cheng et
al., 2016).

based on ideas of the attention mechanism, end-
to-end memory networks (sukhbaatar et al., 2015)
select explicit memories for query answering. mem-
ories are encoded into two representations:
i) the
input representation for query matching and ii)
the output representation for subsequent utilization.
this distinction is important, because the represen-
tation that is used to match the original query has a
different responsibility than the representation that is
used to answer or update the query. thus, attention-
based approaches for answering queries using sup-
porting documents can be considered a special case
of memory networks where hidden states form both
the input- and output representation of the individ-
ual memories which are jointly encoded. variants
of memory networks have achieved very good re-
sults in various nlp tasks, such as language mod-
eling (sukhbaatar et al., 2015), reading compre-
hension (hill et al., 2016) and id53
(sukhbaatar et al., 2015; kumar et al., 2015; miller
et al., 2016).

one important contribution of memory net-
works is the idea of re   ning or updating the query
(sukhbaatar et al., 2015) or memories (kumar et al.,
2015) for multiple memory retrieval cycles before
answering the query. this idea lead to signi   cant
improvements for architectures that employ the at-
tention mechanism iteratively for reading compre-
hension tasks (sordoni et al., 2016).

recently, other forms of explicit memory have

been suggested for neural architectures. for in-
stance, associative memory can be used to effec-
tively compress multiple memories into redundant
copies of a single memory array.
it has shown
very promising results, e.g., for language model-
ing (danihelka et al., 2016) or recognizing textual
entailment (weissenborn, 2016), and might there-
fore be suited to compress large amounts of external
memories when used in conjunction with our model.

3 query-answer neural network

our query-answer neural network utilizes support-
ing knowledge in the form of explicit query-answer
pairs (z, y) to predict the answer a to a given query
q. answers from support (y) are weighted via
matching scores between their corresponding sup-
port query z and the actual query q. once a weighted
query-answer pair (  z,   y) has been retrieved, it is
used to update the current query and the predicted
answer representation for a subsequent support re-
trieval cycle (hop). this process can be repeated for
a speci   ed number of hops (t ). finally, we use the
predicted answer representation after t hops as in-
put for answer classi   cation given a set of possible
answer candidates. note that this approach does not
require supporting answers to correspond to the an-
swer candidates.

3.1 supporting knowledge

our model stores supporting knowledge as pairs of
queries (z) and answers (y). given a set of support-
ing documents, (z, y)-pairs are formed by i) detect-
ing task-speci   c spans-of-interest (sois), ii) form-
ing (z, y)-pairs for each soi. in this work we con-
sider cloze-style (z, y)-pairs. thus, given a (z, y)-
pair z corresponds to an entire document with a gap
at a particular soi (cloze-text) and y to the    ller of
this gap. consider the following example:

query:
q: schweinsteiger plays for the

national team of
support document 1:
d: schweinsteiger scored against

ukraine

support document 2:
d: germany played against ukraine

from this example we extract the following sup-
porting query-answer pairs, if we identi   ed all coun-
tries (underlined) as sois:
support qa 1:
z: schweinsteiger scored against
y: ukraine
support qa 2:
z:
y: germany
support qa 3:
z: germany played against
y: ukraine

played against ukraine

note that spans-of-interest can cover almost any-
thing, e.g., entire sentences or only single words.
de   ning sois and their respective answers can be
adapted to the needs of the task at hand.

3.2 encoding queries
given a (supporting) document d = (x1, ..., xn )
of symbols and spans-of-interest (sois) p =
{(ls, le) | ls, le     [1, n ]}, at    rst all symbols xl are
embedded by an embedding matrix ei. next, the
entire document d is encoded by a bi-directional
l     rh of the
id56 resulting in representations hf
l     rh of the backward-id56
forward-id56 and hb
for each document position l     [1, n ]. afterwards,
we form the following query representation for each
(ls, le)     p :

(cid:34)

(cid:35)

zl =wq

hf
hb

ls   1
le+1

wq     rh  2h

(1)

the trainable parameter-matrix wq is initialized
with [i n; i n] and additional random noise, where i n
is the identity matrix. thus, initially zl corresponds
roughly to the sum of the forward state hf
ls   1 of the
left context and the backward state hb
le+1 of the right
context. in order to ensure that the query represen-
tation only considers the outer context of the respec-
tive soi it is required that ls     le.1 encoding sup-
porting queries in this way has the advantage, that
the entire context is encoded in contrast to restricting

1it is also possible to de   ne ls > le, s.t. the span-of-interest
becomes part of the query which might be important for some
tasks. however, we are not considering these types of tasks in
this work.

figure 1: illustration of our architecture which demonstrates an support retrieval cycle (hop) along with its
corresponding update of the query and answer utilizing supporting queries and answers ((z, y)-pairs). the
query representation is initialized (t = 0) by encoding the query string q. the initial answer representation
is computed based on the initial query representation.

the context to a    xed-size context window or sen-
tence. furthermore, word-order and positional in-
formation is captured naturally by employing id56s.
encoding the actual query q is identical to the encod-
ing supporting queries z.

3.3 encoding answers
in this work, we consider answers to be individual
symbols. however, this approach can be extended
to sequences of symbols as well.
answer candidates answer candidates c     aq
for a query q are embedded (c) by a second embed-
ding matrix eo.

sois within the support.2 there are two encodings
of y with different applications: i) its correspond-
ing output embedding yo from eo, and ii) its corre-
sponding input embedding yi from ei. yo is used
to update the current answer of the model and yi is
used to update the query.

the intuition behind using yi for updating the
query representation is that we want to use the an-
swer as if it was a word of the original query and
would thus be embedded by ei. yo corresponds to
the embeddings of eo that are only used for answer
prediction.

it is possible to    x eo to the identity matrix which
results in the accumulation of support weights as

supporting answers for cloze-style queries sup-
porting answers (y) correspond to the symbols at

2note, supporting answers do not necessarily have to corre-

spond to an actual answer candidate.

answers  yianswers  yoqueries  zqueryanswerquery qtt + 1supportz1z2...y1y2...queriesanswerssupport-weightsscalar gateelement-wise gatescore for each answer candidate. this is similar to
using the attention weights for answer scoring as de-
scribed in kadlec et al. (2016).

3.4 supporting answer retrieval
a supporting answer   y is selected softly from all
(m) supporting (z, y)-pairs by softmax-weights
based on similarity scores between all zk and the
query q. these support weights can be viewed
as attention weights over the respective supporting
(z, y)-pairs.

(cid:80)m
exp(q    zk)
k(cid:48)=1 exp(q    zk(cid:48))
m(cid:88)

      yi

      

  k

k
yo
k
zk

k=1

  k =

       =

         yi

  yo
  z

(2)

(3)

3.5 query & answer update
the query representation q and predicted answer
representation a are consecutively updated by us-
ing supporting (z, y)-pairs realizing multi-hop sup-
port retrieval. for instance, in the example of   3.1
the model might    nd support qa 1 to    t the orig-
inal query best and retrieve the (wrong) answer
(ukraine).
it is reasonable to update the original
query with support qa 1 which includes the answer
ukraine. the subsequent, updated query eventually
leads to the correct answer germany of support qa
2. figure 1 illustrates this process.

we utilize the weighted support-queries   zt and
their corresponding answer input-representations yi
t
(eq. 3) to update the current query qt by an element-
wise weighted addition (eq. 4), where q0 = q (the
encoded query).

      u q
(cid:18)

            
      qt
(cid:20)qt
(cid:21)
(cid:19)
+ bq
g
  zt
t ) (cid:12)   qt
t (cid:12) qt + (1     gq

  yi
t
  zt

u q
g

c

  qt = tanh

gq
t = sigmoid

qt+1 = gq

(4)

the answer is initialized by a gated linear trans-
formation of the initial query q0 (eq. 5). the query-
answer-gate ga
q decides whether the query itself can

be utilized to infer the answer for a speci   c task or
not. the answer representation at hop t represented
by at is updated to at+1 by adding the gated, re-
trieved answer   yo
t (eq. 6). the scalar answer accu-
mulation gate ga
t (eq. 7) depends on: i) the similarity
between the current query qt and the weighted sup-
port queries   zt, ii) the similarity of the original query
encoded as answer a0 and the weighted support an-
swer representation   yo
t retrieved from support and
iii)   t which measures the highest answer candidate
id203 if   yo
t was the    nal answer representation
(eq. 8 which refers to   3.6).

a0 = sigmoid(ga
t   yo
t

at+1 = at + ga

q ) u a
q q

      ua

g

       qt (cid:12)   zt

a0 (cid:12)   yo

t

ga
t = sigmoid

  t = max
c   aq

pq(c|  yo
t )

  t

       + ba

(5)
(6)

       (7)

(8)

g, ua

the trainable parameters of this module have
c     rh  3h; u q
g    
q , ua, ba     r.

the following dimensions: u q
rh  2h; ua     rh  h; b  
g     rh; ga
3.6 answer scoring & training
after a maximum number of hops t , scores sq,c of
all answer candidates c     aq are calculated using
the inner product between their respective embed-
dings c and the    nal answer representation at :

   c     aq : sq(a, c) = a    c

(cid:80)

pq(c|a) =

esq(a,c)

esq(a,c(cid:48))

c(cid:48)   aq

(9)

finally,

the model

is trained by minimizing
the cross-id178 loss using the softmax-weights
(eq. 9) of candidate scores as the predicted proba-
bilities.

4 experiments
4.1 setup
dataset we evaluate our architecture on two re-
cently proposed benchmarks for reading compre-
hension. both benchmarks require a system to an-
swer a cloze-style query solely based on a single

supporting document. hermann et al. (2015) cre-
ated two datasets (id98, dailymail) from news ar-
ticles. for each article, queries were created from
their respective summaries by removing a named en-
tity from the summary sentence that has to be pre-
dicted. all articles in the dataset are pre-processed
by id39, co-reference resolution
and entity anonymization. similar in mind, hill et
al. (2016) created the children   s book test (cbt).
for this dataset, passages of children   s books of 21
sentences were extracted. within the last sentence
of the passage a word is removed that has to be pre-
dicted. the dataset is split into subtasks depending
on the part-of-speech tag of the word that has to be
predicted. we evaluate our model on the named en-
tity (ne) subtask because it is the most challenging
subtask for traditional language models.
input presentation & encoding the input to the
model consists of the context document and the
query. the actual query is the cloze-text for the posi-
tion of the removed named entity, which is replaced
by a placeholder symbol. the entire input (docu-
ment + query) is encoded by a bi-directional gru
from which the query and answer representations are
computed as described in   3.2 and   3.3. support-
ing (z, y)-pairs are extracted at occurrences of an
answer candidate (all entities for id98/dailymail,
given for cbt) in the context document in form of
cloze-text (query) and its corresponding    ller (an-
swer).
training for all experiments, we use a hidden di-
mension (and embedding-size) of h = 256. we
train models with and without pre-trained word vec-
tors. the input embedding matrix ei is partially ini-
tialized with 100-dimensional glove-embeddings
(pennington et al., 2014) and randomly for the rest
(156 dimensions) when using pre-trained word vec-
tors.
in general, embeddings are initialized with
a gaussian of 0-mean and 0.1-stddev, matrices as
described in glorot and bengio (2010) and biases
with 0, except for the encoder gru update-gate
bias which is initialized with 1. dropout is ap-
plied with a rate of 0.2 to the embedded input words
for id173. we train our system using mini-
batch sgd with adam (kingma and ba, 2015) for
optimization using an initial learning-rate of 0.001
that is halved whenever the accuracy on the de-

velopment set drops between checkpoints and the
the    rst entire epoch has passed.
if the accuracy
drops between entire epochs training is stopped. the
mini-batch sizes/respective checkpoint iterations are
128/500 for the dailymail and id98 datasets3, and
32/1000 for the cbt ne dataset. we trained both
single models and ensembles of 5 models. note,
that similar to chen et al. (2016), we do not con-
sider all words but only entities as answer candi-
dates for the id98/dailymail dataset. our models
are trained with 4 and 8 consecutive support retrieval
cycles (hops) from the support, which performed
better than using only 1 or 2. all models were im-
plemented with tensorflow (abadi et al., 2015).

4.2 empirical results
the results of our model (qann) on the two bench-
marks are presented in table 1a. they show that our
trained single models and ensembles achieve state-
of-the-art results on the cbt ne and the daily-
mail dataset and perform similar to recent work on
the id98 dataset. an important observation is that
our model outperforms the memory networks by
a large margin. even with self-supervision, which
explicitly introduces a training objective for select-
ing the correct memory at each hop, memory net-
works are clearly outperformed by our system. we
attribute this to the query-answer representation of
our supporting memory and the related architectural
changes that separate end-to-end memory networks
from qanns.

for our models we observe that using 4 instead of
8 support retrieval cycles (hops) makes a difference
only if glove is not used for initialization. using
glove to (partially) initialize embeddings gives a
boost in performance on all datasets.

we would like to point out that the only sys-
tems with comparable results to ours use either
the attention-weights over context-words (iterative-
and gated attentive reader, epireader, inspired by
kadlec et al. (2016)) or the retrieved hidden state
(stanford attentive reader) to predict the    nal an-
swer. those works follow a similar idea as this work
which separates the answer that is used for predic-
tion from the query.

3batch-size is reduced to 64 for ensembles due to memory

constraints

model
attentive reader1
stanford attentive reader (glove)2

-
-

63.0
72.4

cbt ne id98 dailymail

epireader3 (answer re-ranking)

69.7

74.0

multi-hop systems

impatient reader1
memnns (window)4
memnns (window + self-sup.)4
ga reader5
iterative attentive reader6

qann (4 hops)
qann (8 hops)

qann (4 hops, glove)
qann (8 hops, glove)

ensembles

epireader3 (answer re-ranking)
memnns (window + self-sup.)4
ga reader5
iterative attentive reader6

qann (4 hops, glove)

(a)

-

49.3
66.6
69.0
68.6

69.4
70.3

70.6
70.6

71.8
66.6
71.9
72.0

72.9

63.8
60.6
66.8
73.8
73.3

72.6
73.4

73.7
73.6

-

68.4
77.4
76.1

76.8

69.0
75.8

-

68.0

-
-

75.7

-

76.6
76.4

76.9
77.2

-
-

78.1

-

79.2

hops cbt ne id98

1
2
3

4

5
6
7

67.0
72.0
73.3

73.7

73.3
73.8
73.2

60.4
67.0
70.0

70.6

71.2
71.0
70.7

(b)

table 1: (a) accuracies of different models on 3 benchmark test datasets for reading comprehension. her-
mann et al. (2015) 1, chen et al. (2016) 2, trischler et al. (2016) 3, hill et al. (2016) 4, dhingra et al. (2016) 5,
sordoni et al. (2016) 6. note, that the works 3,5,6 are very recent. (b) accuracies of qanns on the cbt ne
and id98 testsets when employing a model trained with 4 support retrieval cycles, but applied on a varying
number of retrieval cycles (hops).

the epireader (trischler et al., 2016) achieves
slightly better results on the id98 dataset, because
it employs a second neural network (reasoner) that
re-ranks the output of an attention based model to
improve prediction. this idea is orthogonal to our
work and can be used to improve qanns similarly.

the iterative attentive reader (sordoni et al.,
2016) alternates between attention on speci   c parts
of the query and attention on the context document.
the authors found that attending over the query is
very useful, however, the attention is usually set on
the placeholder symbol which is similar in our ap-
proach. they achieve similar results on the id98
dataset but are outperformed by our models on the
cbt ne dataset. we attribute this improvement
to our answer update mechanism (  3.5) which ac-
cumulates dedicated answer embeddings from sup-
port over multiple hops and from the original query

through gates. thus, the    nal answer prediction de-
pends not only on the attention (support) weights,
but also on the query itself and the answer embed-
dings. this advantage cannot be exploited as much
in the id98 and dailymail datasets because entities
and thus all answers are anonymized. as illustra-
tion, figure 2c provides an example of a correctly
predicted answer that does not align with the com-
puted support weights (see   4.4 for more details).
4.3
we trained our models with different numbers of
support retrieval cycles (hops). we found that us-
ing at least 4 hops leads to a signi   cantly better per-
formance than using only 1 or 2 hops. this indi-
cates that multiple consecutive support retrievals and
respective query updates are important for a robust
performance on the reading comprehension tasks.

impact of multiple answer retrievals

in addition, we evaluate the differences in perfor-

(a) id98: correct answer retrieved after    rst hop but wrong prediction after subsequent hops.

(b) id98: wrong answer retrieved after    rst hop but corrected in subsequent hops.

(c) cbt ne: retrieved answers are wrong yet system predicts answer correctly.

figure 2: examples of support (attention) weights for each hop for models trained with    xed 4 hops. the
legends show the activity of the respective answer gates for each hop in brackets. the predicted answer is
underlined and the correct answer is displayed in bold-face.

mance when varying the number of hops when us-
ing our model trained on 4 hops. this evaluation
gives insights into accuracy gains and the stability
of answer prediction when increasing the number of
hops. the results presented in table 1b demonstrate
that the model gains most until 3 hops, after which
results are quite stable. the most pronounced differ-
ence occurs between using only 1 and 2 hops. the
relative stability of performance between 3 to 7 hops
indicates that the system learns to utilize the gat-
ing mechanisms which decide to keep or update the
current query and accumulate answers successfully.
even though our model was trained with 4 hops, the
best results for the cbt ne and id98 testsets were
achieved when utilizing the model with additional
hops (5 for the cbt ne and 6 for id98). surpris-
ingly, for cbt we found a rather large improvement
of about 0.6 percentage points in accuracy.

4.4 analysis
in a qualitative analysis of our system on sampled
documents from the id98 and cbt ne dataset, we
found that the correct answer is retrieved already af-
ter the    rst hop and kept until prediction in most
cases (64% on 1000 sampled id98 examples). how-
ever, there are interesting exceptions to this rule that
are displayed in figure 2.
it shows example doc-
uments with respective attention weights for sup-
porting spans-of-interest (positions of answer can-
didates) in each hop. a general observation is that
the support weights are very pronounced for the    rst
hop and spread over an increasing number of posi-
tions with each additional hop. we    nd that highly
weighted positions can vary signi   cantly between
hops which shows that the query is updated. as we
have shown empirically in   4.3 this can have a posi-
tive effect (8.4%), see for example figure 2b. how-
ever, sometimes this can also result in an incorrect
prediction, although the answer was correctly found
in the    rst hop (2.7%) as demonstrated in figure 2a.
a very interesting example from the cbt ne val-
idation set is displayed in figure 2c. it shows that the
system puts high support weights on different posi-
tions in the document, but never on the correct an-
swer. nevertheless, to our surprise the model pre-
dicts the answer correctly anyway. one explana-
tion might be that the model has learned that gen-
eral words like    people    or    sea    are not good an-

swers for the cbt ne dataset (e.g., through answer
embeddings with small norm). another explanation
is that the query itself (which is also used to form
the    nal answer representation) puts a strong bias
on the    nal answer. to test the latter hypothesis we
set the query-gate of eq. 5 to 0 which effectively re-
moves the query representation from the predicted
answer representation. we found that the prediction
changed to    people    which can be explained by the
support weights. this    nding con   rms our premise
that the query is able to put a bias on the    nal an-
swer, and that the use of the query itself maybe be
bene   cial for answer prediction.

5 conclusion

we have presented a new type of neural network ar-
chitecture for answering queries.
it is end-to-end
trainable and learns to utilize knowledge in the form
of supporting query-answer pairs to infer the an-
swer to a given query.
it explicitly separates the
query representation used for selecting support from
the answer representation used for prediction. re-
sults on recently proposed benchmark datasets for
the task of reading comprehension show that our
model outperforms or approaches various state-of-
the-art baselines. this shows that the idea of ex-
plicitly separating query and answer is important for
tasks that involve answering queries.

future work involves the extension of this archi-
tecture to be able to properly handle other kinds of
queries, e.g., list-queries or queries expecting gener-
ated answers. furthermore, we believe that our ar-
chitecture is suited for the successful application to a
variety of tasks in the area of information extraction.

acknowledgments

we thank thomas demeester, thomas werkmeis-
ter, sebastian krause, tim rockt  aschel and sebas-
tian riedel for their comments on an early draft
of this work. this research was supported by the
german federal ministry of education and re-
search (bmbf) through the projects all sides
(01iw14002), bbdc (01is14013e), and software
campus (01is12050, sub-project genie).

references
mart    n abadi, ashish agarwal, paul barham, eugene
brevdo, zhifeng chen, craig citro, greg s. corrado,
andy davis, jeffrey dean, matthieu devin, sanjay
ghemawat, ian goodfellow, andrew harp, geoffrey
irving, michael isard, yangqing jia, rafal jozefow-
icz, lukasz kaiser, manjunath kudlur, josh leven-
berg, dan man  e, rajat monga, sherry moore, derek
murray, chris olah, mike schuster, jonathon shlens,
benoit steiner, ilya sutskever, kunal talwar, paul
tucker, vincent vanhoucke, vijay vasudevan, fer-
nanda vi  egas, oriol vinyals, pete warden, martin
wattenberg, martin wicke, yuan yu, and xiaoqiang
zheng.
2015. tensorflow: large-scale machine
learning on heterogeneous systems. software avail-
able from tensor   ow.org.

dzmitry bahdanau, kyunghyun cho, and yoshua ben-
2015. id4 by jointly

gio.
learning to align and translate. in iclr.

danqi chen, jason bolton, and christopher d. manning.
2016. a thorough examination of the id98/daily mail
reading comprehension task. in acl.

jianpeng cheng, li dong, and mirella lapata. 2016.
long short-term memory-networks for machine read-
ing. arxiv preprint arxiv:1601.06733.

junyoung chung, caglar gulcehre, kyunghyun cho,
and yoshua bengio. 2014. empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arxiv preprint arxiv:1412.3555.

ivo danihelka, greg wayne, benigno uria, nal kalch-
brenner, and alex graves. 2016. associative long
short-term memory. arxiv preprint arxiv:1602.03032.
bhuwan dhingra, hanxiao liu, william w cohen,
2016. gated-attention
arxiv preprint

and ruslan salakhutdinov.
readers for text comprehension.
arxiv:1606.01549.

xavier glorot and yoshua bengio. 2010. understand-
ing the dif   culty of training deep feedforward neural
networks. in aistats, volume 9, pages 249   256.

alex graves, greg wayne,

turing machines.

and ivo danihelka.
arxiv preprint

2014.
neural
arxiv:1410.5401.

edward grefenstette, karl moritz hermann, mustafa su-
leyman, and phil blunsom. 2015. learning to trans-
duce with unbounded memory. in nips, pages 1819   
1827.

karl moritz hermann, tomas kocisky, edward grefen-
stette, lasse espeholt, will kay, mustafa suleyman,
and phil blunsom. 2015. teaching machines to read
and comprehend. in nips, pages 1693   1701.

felix hill, antoine bordes, sumit chopra, and jason we-
ston. 2016. the goldilocks principle: reading chil-
dren   s books with explicit memory representations. in
iclr, volume abs/1511.02301.

sepp hochreiter and j  urgen schmidhuber. 1997. long
short-term memory. neural computation, 9(8):1735   
1780.

rudolf kadlec, martin schmid, ondrej bajgar, and jan
kleindienst. 2016. text understanding with the atten-
tion sum reader network. acl.

diederik kingma and jimmy ba.

2015. adam: a

method for stochastic optimization. in iclr.

ankit kumar, ozan irsoy, jonathan su, james bradbury,
robert english, brian pierce, peter ondruska, ishaan
gulrajani, and richard socher. 2015. ask me any-
thing: dynamic memory networks for natural lan-
guage processing. corr, abs/1506.07285.

minh-thang luong, hieu pham, and christopher d
manning. 2015. effective approaches to attention-
arxiv preprint
based id4.
arxiv:1508.04025.

alexander miller, adam fisch, jesse dodge, amir-
hossein karimi, antoine bordes, and jason weston.
2016. key-value memory networks for directly read-
ing documents. arxiv preprint arxiv:1606.03126.

jeffrey pennington, richard socher, and christopher d
manning. 2014. glove: global vectors for word rep-
resentation. in emnlp, volume 14, pages 1532   1543.
tim rockt  aschel and sebastian riedel. 2016. learn-
ing knowledge base id136 with neural theorem
provers. in akbc (naacl).

tim rockt  aschel, edward grefenstette, karl moritz her-
mann, tom  a  s ko  cisk`y, and phil blunsom. 2016. rea-
soning about entailment with neural attention. iclr.
alessandro sordoni, phillip bachman, and yoshua ben-
iterative alternating neural attention for

gio. 2016.
machine reading. arxiv preprint arxiv:1606.02245.

sainbayar sukhbaatar, jason weston, rob fergus, et al.
2015. end-to-end memory networks. in nips, pages
2431   2439.

wilson l taylor. 1953. cloze procedure: a new tool for
measuring readability. journalism and mass commu-
nication quarterly, 30(4):415.

adam trischler, zheng ye, xingdi yuan, and kaheer
suleman.
2016. natural language comprehension
with the epireader. arxiv preprint arxiv:1606.02270.
oriol vinyals, meire fortunato, and navdeep jaitly.

2015. id193. in nips, pages 2692   2700.

dirk weissenborn. 2016. neural associative memory for

dual-sequence modeling. in repl4nlp (acl).

jason weston, sumit chopra, and antoine bordes. 2015.

memory networks. in iclr.

