an attentional neural conversation model with improved speci   city

kaisheng yao

microsoft research

redmond, usa

baolin peng

chinese university

of hong kong

geoffrey zweig
microsoft research

redmond, usa

kam-fai wong
chinese university

of hong kong

kaisheny@microsoft.com

blpeng@se.cuhk.edu.hk

gzweig@microsoft.com

kfwong@se.cuhk.edu.hk

6
1
0
2

 

n
u
j
 

3

 
 
]
l
c
.
s
c
[
 
 

1
v
2
9
2
1
0

.

6
0
6
1
:
v
i
x
r
a

abstract

in this paper we propose a neural conversation
model for conducting dialogues. we demon-
strate the use of this model to generate help
desk responses, where users are asking ques-
tions about pc applications. our model is
distinguished by two characteristics. first, it
models intention across turns with a recurrent
network, and incorporates an attention model
that is conditioned on the representation of in-
tention. secondly, it avoids generating non-
speci   c responses by incorporating an idf
term in the objective function. the model is
evaluated both as a pure generation model in
which a help-desk response is generated from
scratch, and as a retrieval model with perfor-
mance measured using recall rates of the cor-
rect response. experimental results indicate
that the model outperforms previously pro-
posed neural conversation architectures, and
that using speci   city in the objective function
signi   cantly improves performances for both
generation and retrieval.

1

introduction

in recent years, neural network based conversa-
tion models (serban et al., 2015b; sordoni et al.,
2015b; vinyals and le, 2015; shang et al., 2015)
have emerged as a promising complement to tradi-
tional partially observable markov decision process
(pomdp) models (young et al., 2013). the neural
net based techniques require little in the way of ex-
plicit linguistic knowledge, e.g., creating a seman-
tic parser, and therefore have promise of scalabil-
ity,    exibility, and language independence. broadly

speaking, there are two approaches to building a
neural conversation model. the    rst is to train what
is essentially a conversation-conditioned language
model, which is used in generative mode to produce
a likely response to a given conversation context.
the second approach is retrieval-based, and aims at
selecting a good response from a list of candidates
taken from the training corpus.

neural conversation models are usually trained
similarly to id4 mod-
els (sutskever et al., 2014; cho et al., 2014), which
treat response generation as a surface-to-surface
transformation. while simple, it is possible that
these models would bene   t from explicit model-
ing of conversational dynamics, speci   cally the at-
tention and intention processes hypothesized in dis-
course theory (grosz and sidner, 1986). a recent
improvement along these lines is the hierarchical re-
current encoder-decoder in (serban et al., 2015b;
sordoni et al., 2015b) that incorporates two levels
of recurrent networks, one for generating words and
one for modeling dependence between conversation
turns. in this paper, we extend this approach further,
and propose an explicit intention/attention model.

we further tackle a second key problem of neu-
ral conversation models, their tendency to gener-
ate generic, non-speci   c responses (vinyals and le,
2015; li et al., 2016). to address the problem of
speci   city, a maximum mutual information (mmi)
method for generation was proposed in (li et al.,
2016), which models both sides of the conversation,
each conditioned on the other. while we    nd this
method effective, training the additional model dou-
bles the computational cost.

the contributions of this paper are as follows.
first, we introduce a novel attention with intention
neural conversation model that integrates an atten-
tion mechanism into a hierarchical model. without
the attention mechanism, the model resembles the
models in (serban et al., 2015b) but performs bet-
ter. visualization of the intention layer in the model
shows that it indeed is relevant to intent. second,
we address the speci   city problem by incorporat-
ing inverse document frequency (idf) (salton and
buckley, 1988; ramos, 2003) into the training pro-
cess. the proposed training algorithm uses rein-
forcement learning with the idf value of the gen-
erated sentences as a reward signal. to the best of
our knowledge, this is the    rst method incorporat-
ing speci   city into the training objective function.
empirically, we    nd that it performs better than the
dual-model method of (li et al., 2016). lastly, we
demonstrate that the proposed model also performs
well for retrieval-based conversation modeling. us-
ing a recently proposed evaluation metric (lowe
et al., 2015; lowe et al., 2016), we observed that
this model was able to incorporate term-frequency
inverse document frequency (tf-idf) (salton and
buckley, 1988) and signi   cantly outperformed a tf-
idf retrieval baseline and the model without using
tf-idf.

2 the model

the proposed model
is in the encoder-decoder
framework (sutskever et al., 2014) but incorporates
a hierarchical structure to model dependence be-
tween turns of conversation process. the encoder
network processes the user input and represents it
as a vector. this vector is the input to a recurrent
network that models context or intention to gener-
ate response in the decoder network. the decoder
generates a response sequence word-by-word. for
each word, the decoder uses an attention model on
the words in the user input. following (grosz and
sidner, 1986), we refer the conversation context as
intention. because an attention model is used in the
decoder, we denote this model as attention with in-
tention (awi) neural conversation model. a detailed
illustration for a particular turn is in figure 1. we
elaborate each component of this model in the fol-
lowing sections.

figure 1:
illustration of the awi model in one
turn to generate a response sequence [x,y,z] for in-
put sequence [a,b,c,d] and the previous response
[x   ,y   ,z   ].

2.1 encoder network
given a user input sequence with length m at turn
k, the encoder network converts it into a sequence of
m : m = 1,       , m ], with vector
vectors x(k) = [x(k)
m     rde denoting a id27 represen-
x(k)
tation of the word at position m. the model uses
a feed-forward network to process this sequence. it
has two inputs. the    rst is a simple word unigram
feature, extracted as the average of the word embed-
dings in x(k). the second input is a representation
of the previous response. this representation is also
a word unigram feature, but is applied on the past re-
sponse. the output, u(k)     rdx, from the top layer
of the feed-forward network is a vector representa-
tion of the user input. in addition to this vector rep-
resentation, the encoder network outputs the vector
sequence x(k).

intention network

2.2
the middle layer represents a conversation context,
which we denote it as the intention hypothesized
in (grosz and sidner, 1986). at turn k, it is a vector
z(k)     rdz. to model turn-level dynamics, the ac-
tivity in z(k) is dependent on the activity in the previ-
ous turn z(k-1) and the user input at the current turn.
therefore, it is natural to represent z(k) as follows

(cid:16)

wiz(k-1) + uiu(k)(cid:17)

z(k) = tanh

,

(1)

where tanh() is a tanh operation. wi     rdz  dz
and ui     rdz  dx are matrices to transform inputs
to a space with dimension dz. usually, we apply
multiple layers of the above non-linear processing,
and the higher layer only applies wi to the output
from the layer below. notice that wis are untied
across layers. the output from the top one is the
representation of the intention network.

2.3 decoder network
n     rv , a vector
decoder network has output y(k)
with dimension of vocabulary size v . each element
of the vector represents a id203 of generating
a particular word at position n. this id203
is conditioned on y(k)
n-1, the word generated before,
the above described intention vector z(k) and the en-
coder output x(k). to compute this id203, it
n     rv . the
uses softmax on a response vector r(k)
id203 is de   ned as follows

p(y(k)

n |y(k)

n-1, z(k), x(k)) = softmax(r(k)

n ). (2)

the decoder uses the following modules to generate
response vector r(k)
n .

id56 for position n, the hidden state v(k)
n
id56 is recursively computed as

v(k)
n = tanh

vrv(k)

n-1 + wry(k)

n-1 + urr(k)
n-1

(cid:17)

of the

,

(3)

(cid:16)

n-1, y(k)

n-1 and r(k)

where v(k)
n-1 each represent the previous
hidden state, the word generated and the response
vector for word position n     1. vr     rdr  dr,
wr     rdr  dv and ur     rdr  dv are matrices to
transform their right side inputs to a space with di-
mension dr. during training, y(k)
n-1 is a one-hot vector
with its non-zero element corresponding to the index
of the word at n     1. during test, it is still a one-hot
vector but the index of the non-zero element is from
id125 or greedy search at position n     1. we
apply multiple layers of the above process on v(k)
n ,
with the higher layer uses only the lower layer out-
put in the left side of (3). parameters such as vr are
untied across layers. the top level response in (3) is
the id56 output. to incorporate conversation con-
text, the initial state v(k)
at n = 0 is set to z(k) from
n
the intention network.

attention layer we use the content-based atten-
tion mechanism (bahdanau et al., 2015; luong et al.,
2015). it is a single layer neural network that aligns
the target side hidden state v(k)
n-1 at the previous po-
sition n     1 with the source side representation u(k)
m
at word position m. the alignment weight wynm is
computed as follows

eynm = bt tanh(vav(k)
wynm =

,

n-1 + uax(k)
m ),

(cid:80)m
(cid:88)

exp eynm
j=1 exp(eynj)
wynmx(k)
m ,

a(k)
n

=

(4)
(5)

(6)

m

where va     rda  dv and ua     rda  dx are matrices
to transform their inputs to a space with dimension
da. b     rda is a vector. the softmax operation in
(5) is normalized on the user input sequence. a(k)
in
n
(6) is the output of the attention layer.

response generation we use the following feed-
n    
forward network to generate a response vector r(k)
rdv , using the decoder id56 output v(k)
and the
n
attention layer output a(k)

n ; i.e.,

r(k)
n

= tanh

vgv(k)

n + uga(k)
n

,

(7)

where vg     rdv   dr and ug     rdv   da are ma-
trices to transform their right side inputs to a space
with dimension dv . similarly as those networks de-
scribed above, we use untied multiple layers to gen-
erate r(k)
n , and the input to the higher layer only use
the output from the layer below. the top layer output
is fed into softmax operation in eq. (2) to compute
the id203 of generating words.

input similarity feature we construct a linear di-
rect connection between user inputs and the output
layer. to achieve this, a large matrix p with dimen-
sion v    v is used to project each input word to a
high dimension vector with dimension size v . the
projected words are then averaged to have a vector
m(k) with dimension v . this vector is added to the
response vector r(k)
n , so that eq. (2) is computed as
softmax(r(k)
n + m(k)). since m(k) is applied to all
word positions at turn k, it provides a global bias to
the output distribution.

(cid:16)

(cid:17)

n(cid:89)

3 training and decoding algorithms
this section presents training and decoding algo-
rithms for response generation and retrieval. sec-
tion 3.1 is the standard cross-id178 training. it is
used for training both generation and retrieval mod-
els. section 3.2 introduces training and decoding al-
gorithms to enhance speci   city for generation. al-
gorithms for training and decoding for retrieval are
described in sec. 3.3.

3.1 maximum-likelihood training
the standard training method maximizes the proba-
bility of predicting the correct word y(k)
n given user
input x(k), context z(k), and the past prediction y(k)
n-1;
i.e., the objective is maximizing the following log-
likelihood w.r.t. the model parameter   ,

l(y(k)) = log

p(y(k)

n |y(k)

n-1, z(k), x(k);   ).

(8)

n=1

a problem with this training is that the learned
models are not optimized for the    nal metric (och,
2003; ranzato et al., 2016). another problem is that
decoding to maximize sentence likelihood typically
results in non-speci   c high-frequency words (li et
al., 2016).

improving speci   city for generation

3.2
we propose using inverse document
frequency
(idf) (salton and buckley, 1988) to measure speci-
   city of a response.
idf is used in decoding
in sec. 3.2.2. we describe a novel algorithm in
sec. 3.2.3 that incorporates idf in training objec-
tive.
3.2.1 speci   city

idf for a word w is de   ned as
n

idf (w) = log

|c     c : w     c| ,

(9)

where n is the number of sentences in a corpus c
and c denotes a sentence in that corpus. the denom-
inator represents the number of sentences in which
the word w appears. a property of idf is that words
occur very frequently have small idf values.

we further de   ne a sentence-level idf as the av-

erage of idf values of words in a sentence; i.e.,

idf (c) =

1
|c|

idf (w),

(10)

(cid:88)

w   c

where the denominator |c| is the number of occur-
rence of words in sentence c. a corpus-level idf
value is similarly computed on a corpus with an av-
erage operation as idf (c) = 1|c|
c   c,w   c idf (w)
and the denominator in the equation is the number
of occurrence of words in the corpus.
3.2.2 reranking with idf

(cid:80)

one way to improve speci   city is using idf to
rerank hypotheses from id125 decoding. the
length-normalized log-likelihood scores of these hy-
potheses are interpolated with sentence-level idf
scores. tuning the interpolation weight is on a de-
velopment set using minimum error rate training
(mert) (och, 2003). the interpolation weight that
achieves the highest id7 (papineni et al., 2002)
score on the development set is used for testing.
3.2.3

incorporating idf in training objective

alternatively, we cast our problem of optimizing
a model directly for speci   city in the reinforcement
learning framework (sutton and barto, 1988). the
decoder is an agent with its policy from eq.
(2).
its action is to generate a word using the policy, and
therefore it has v actions to take at each time. at the
end of generating a whole sequence of words for re-
sponse, the agent receives a reward, calculated as the
sentence level idf score of the generated response.
training therefore should    nd a policy that maxi-
mizes the expected reward.

this problem can be solved using rein-
force (williams, 1992), in which the gradient to
update model parameter is calculated as follows

         (cid:16)
    log(cid:81)

r(w(k))     b(x(k))
n |y(k)
n-1, z(k), x(k);   ))
     

n p(y(k)

(11)

,

(cid:17)

where r(w(k)) = idf (w(k)) is the idf score of the
generated response w(k) at turn k. b(x(k)) is called
reinforcement baseline. it in practice can be set em-
pirically as an arbitrary number to improve conver-
gence (zaremba and sutskever, 2015). one conve-
nient way of estimating the baseline is the mean of
the idf values on the training set, which is used in
this paper.

notice that the idf score is computed on the de-
coded responses w(k), but the log-likelihood is com-

puted on the correct response. therefore, the al-
gorithm improves likelihood of the correct response
and also encourages generating responses with high
idf scores.

3.3 training and decoding for retrieval
the conversation model can be used for retrieval
of the correct responses from candidates. we
brie   y describe tf-idf in sec. 3.3.1. section 3.3.2
presents the algorithm to train awi model for re-
trieval. section 3.3.3 combines the model with tf-
idf. notice that tf-idf uses idf to penalize non-
speci   c words, combining the awi model with tf-
idf should have improved speci   city, which could
lead to improved performances for retrieval.

3.3.1 tf-idf

term-frequency inverse document frequency (tf-
idf) (salton and buckley, 1988) is an estab-
lished baseline for conversation model used for re-
trieval (lowe et al., 2015). the term-frequency (tf)
is a count of the number of times a word appears in
a given context, and idf puts penalty on how often
the word appears in the corpus. the tf-idf is a
vector for a context computed as follows for a word
w in a context c,

tf idf (w, c, c) = o(w, c)    log

n

|c     c : w     c| ,

where o(w, c) is the number of times the word w
occurs in the context c. for retrieval, a vector for a
conversation history is    rstly created, with element
computed as above for each word in the conversation
history. then, a tf-idf vector is computed for each
response. similarity of these vectors are measured
using cosine similarity. the responses with the top
k similarities are selected as the top k outputs.

3.3.2 training models with ranking criterion

in order to train awi model for retrieval, the
model needs two types of responses. the positive
response is the correct one, and the negative re-
sponses are those randomly sampled from the train-
ing set. for a response w(k), its length-normalized
log-likelihood is computed as follows,

llk(w(k)) =

l(w(k))
|w(k)|

,

(12)

where l(w(k)) is computed using eq. (8) with y(k)
substituted by w(k). |w(k)| is the number of words
in w(k).

the objective is to have high recall rates such that
the correct responses are ranked higher than neg-
ative responses. to this end, the algorithm maxi-
mizes the difference between the length-normalized
log-likelihood of the correct responses to the length-
normalized log-likelihood of negative responses;
i.e.,

r = max{llk(y(k))     llk(w(k))},

(13)

naturally,

where y(k) is the correct response and w(k) is the
negative response.
3.3.3 ranking with awi together with tf-idf
the length-normalized log-likelihood
score from the model trained in sec. 3.3.2 can be
interpolated with the similarity score from tf-idf
in sec. 3.3.1. the optimal interpolation weight is
selected on a development set if it achieves the best
recall rates.

4 experiments
4.1 data
we use a real-world commercial data set to evaluate
the models. the data contains human-human dia-
logues from a helpdesk chat service for services of
of   ce and windows. in this service, costumers seek
helps on computer and software related issues from
human agents. training set consists of 141,204 dia-
logues with 2 million turns. the average number of
turns in a dialogue is 12, with the largest number of
140 turns and the minimum number of 1 turn. more
than 90% of dialogues have 25 or fewer turns. the
number of tokens is 25,410,683 in the source side
and is 37,796,000 in the target side. the vocabulary
size is 8098 including words from both source and
target sides. development and test sets each have
10,000 turns. the test set has 125,451 tokens in its
source side and 187,118 in its target side.

4.2 training details
unless otherwise stated, all of the recurrent net-
works have two layers. the encoder network uses
id27 initialized from 300-dimension
glove vector (pennington et al., 2014) trained

from 840 billion words. therefore, the embedding
dimension de is 300. the hidden layer dimension
for encoder, dx, is 1000. decoder dimension, dr, is
1000. the intention network has a 300 dimension
vector; i.e., dz = 300. the alignment dimension
da is 100. all parameters, except biases, are initial-
ized using uniform distribution in [   1.0, 1.0] but are
scaled to be inversely proportional to the number of
parameters. all bias vectors are initialized to zero.

the maximum number of training epochs is 10.
we use rmsprop with momentum (tieleman and
hinton, 2012) to update models. we use perplex-
ity to monitor the progress of training; learning rate
is halved if the perplexity on the development set
is increased. the gradient is re-scaled whenever its
norm exceeds 5.0. to speed-up training, dialogues
with the same number of turns are processed in one
batch. the batch size is 20.

hyper-parameters such as initial learning rate and
dimension sizes are optimized on the development
set. these parameters are then used on the test set.
decoding uses id125 with a beam width of 1.
decoding stops when an end of sentence is gener-
ated.

4.3 id74
as our model is used both for generation and re-
trieval, we use some established measures in the
literature for comparison. the    rst measure is
id7 (papineni et al., 2002), which uses the n-
gram (dunning, 1994) to compute similarities be-
tween references and responses,
together with a
penalty for sentence brevity. we use id7 with
4-gram. while id7 may unfairly penalize para-
phrases with different wording, it has found corre-
lated well with human judgement on responses gen-
eration tasks (galley et al., 2015). the second mea-
sure is perplexity (brown et al., 1992), which mea-
sures the likelihood of generating a word given ob-
servations. we use it in section 4.4.1 to compare
the proposed model with respect to other neural net-
work models that also report perplexity. however,
since our training algorithms in sec. 3.2 is designed
to improve speci   city, which is not directly corre-
lated with the standard likelihood, we only report
perplexity in section 4.4.1. the third metric is cor-
pus level idf score for speci   city, computed in (10).
since our model is also used for retrieval, we

models
id165
id195 (vinyals and le,
2015)
hred (serban et al.,
2015a)
awi

id7 perplexity

0.0
1.82

6.14

9.29

280.5
12.64

13.82

11.52

table 1: results on the test set.

adopt a response selection measure proposed in
(lowe et al., 2015), in which the performance of a
conversation model is measured by the recall rate of
those correct responses in the top ranks. this met-
ric is called recall@k (r@k). the model is asked
to select the k most likely responses, and it is cor-
rect if the true response is among these k responses.
the number of candidates for retrieval is 10, follow-
ing (lowe et al., 2015). this measure is observed
to correlate well with human judgment for retrieval
based conversation model (lowe et al., 2016; liu et
al., 2016).

4.4 performance as a generation model
4.4.1 comparison with other methods

we compared the awi model with the sequence-
to-sequence (id195) (vinyals and le, 2015)
and the hierarchical
recurrent encoder-decoder
(hred) (serban et al., 2015a) models. all of the
models had a two layers of encoder and decoder.
the hidden dimensions for the encoder and decoder
were set to 200 in all of the models. the hidden di-
mension for the intention network was set to 50. all
of the models had their optimal setup on the develop-
ment set. both id195 and hred used long short-
term memory networks (hochreiter and schmidhu-
ber, 1997). the number of parameters was approxi-
mately 4.48    106 for id195 and 4.50    106 for
hred. awi didn   t have the input similarity feature
and it had 5.71  106 parameters. greedy search was
used in this experiment.

table 1 shows that awi model outperforms other
neural network models both in id7 and perplex-
ity. for comparison, id7 and perplexity scores
from an unconditional id165 model are also re-
ported, which are much worse than those from neu-
ral network based models. the id7 score for n-
gram was obtained by sampling from the id165

models
awi

awi + sampling

awi + mmi (li et al., 2016)

awi + idf

ir-awi

id7 idf
2.35
11.42
2.76
7.02
11.47
2.37
2.83
11.43
11.70
2.40

table 2: performance for generation.

and comparing the sampled response to a typical re-
sponse. its response has id7 score of 0.08 if using
id7 with 1-gram. because it cannot be matched
in 4 gram with the typical response, its id7 with
4 gram is 0. on one experiment, not shown in the
table due to space limitation, with smaller training
set, we observed these models performed worse yet
similarly. this suggests that the bene   t of incorpo-
rating hierarchical structure in both hred and awi
is more apparent with larger training data.

4.4.2 results with speci   city improved models
we report id7 and idf scores in table 2. the
baseline is awi trained with standard cross-id178
in sec. 3.1. for comparison, we used a sampling
method (mikolov et al., 2011) to generate responses,
denoted as    awi + sampling   . using sampling
would lead to an appropriate number of infrequent
words and therefore an idf score that is similar to
that of the reference responses. indeed this is ob-
served in    awi + sampling    in the table. it has an
idf score of 2.76, close to the idf score of 2.74
from the training set. however, sampling produces
worse id7 scores, though it has higher idf score
than awi.

we also report result using mmi method for de-
coding (li et al., 2016), denoted as    awi + mmi   .
this uses a backward directional model trained for
generating source from target, and its decoding uses
reranking algorithm in sec. 3.2.2. the optimal inter-
polation weight for the backward directional model
was 0.05. both id7 and idf scores are improved.
on the other hand,    awi + mmi    requires an ad-
ditional model as complicated as the baseline awi
model.

alternatively, awi results are reranked with
the sentence-level idf scores using algorithm in
sec. 3.2.2. the optimal interpolation weight to idf
was 0.035. this result, denoted as    awi + idf   ,

figure 2: id167 visualization of intention vectors.

has improved id7 and idf scores, in compari-
son to the baseline awi. compared against    awi +
mmi   , it has similar id7 but higher idf scores.
this suggests that idf score is more directly related
to speci   city than using mmi.

the result from using speci   city as reward to train
a model in sec. 3.2.3 is denoted as idf-rewarded
awi model or    ir-awi   . the reinforcement base-
line b(x(k)) was empirically set to 1.0. we also ex-
perimented with a larger value to 1.5 for the base-
line and didn   t observe much performance differ-
ences.    ir-awi    consistently outperforms    awi   
and    awi + mmi   .

4.4.3 analysis

figure 2 uses id167 (van der maaten and hinton,
2008) to visualize the intention vectors.
it shows
clear clusters even though training intention vectors
doesn   t use explicit labels. in order to relate these
clusters with explicit meaning, we look at the re-
sponses generated from these intention vectors, and
tag similar responses with the same color. some re-
sponses types are clearly clustered such as    greet-
ing    and    close this chat   . others types are though
more distributed and cannot    nd a clear tag for these
responses. we therefore leave them untagged.

we also show two examples of responses in ta-
bles 3 and 4 from awi and ir-awi. responses from
awi+mmi and awi+idf are the same as from
awi, so we only list responses from awi and ir-
awi. these responses are reasonable. however, the
ir-awi responses in these tables are more speci   c
than the generic responses from awi.

60402002040608060402002040close this chatgreetinghow can i help ?awi
okay,
that information.

thank you for

ir-awi
alright, kindly click
the link below and up-
date me once you are
on the page that is ask-
ing for a six digit code
http://(cid:104)webpage(cid:105).

table 3: examples of responses. conversation his-
tory is as follows. user said,    i don   t have another
computer that supports miracast. the adapter appears
as a    device    but does not get connected. do you
want remote access to it?    agent replied,    would
that be alright for you ?    user then said,    yes.   
for reference, the human agent responds,    alright,
kindly click the link below and update me once you
are on the page http://(cid:104)webpage(cid:105), i would like to set
your expection.   

awi
may i know how did
you upgrade to win-
dows 10?

ir-awi
may i have the product
key for windows 8.1?

table 4: examples of responses. conversation his-
tory is as follows. user said,    windows activate error
code : (cid:104)errorcode(cid:105),    agent replied,    i am sorry for
the inconvenience. but nothing to worry about, i will
surely help you with this. may i know the previous
os?    user then said,    8.1.    for reference, the human
agent responds,    okay, may i know, do you have a
product key for windows 8.1?   

4.5 performance for retrieval
we report recall rates, r@1 and r@5, in table 5.
clearly, awi model trained with ranking criterion
in sec. 3.3.2 outperforms tf-idf. importantly, awi
model was able to combine tf-idf using method
described in sec. 3.3.3, obtaining signi   cant perfor-
mance improvement.

5 related work

our work is related both to goal and non-goal ori-
ented dialogue systems as the proposed model can
be used as a language generation component in a
goal-oriented dialogue (young et al., 2013) or sim-
ply to produce chit-chat style dialogue without a

models
tf-idf
awi

r@1 r@5
73.95
28.54
77.01
33.57
awi + tf-idf 40.70
85.39

table 5: retrieval results for the models using 1 in
10 recall rates (%).

speci   c goal (ritter et al., 2010; banchs and li,
2012; ameixa et al., 2014). whereas traditionally
a language generation component (henderson et al.,
2014; gasic et al., 2013; wen et al., 2015) rely on
explicit state (williams, 2009) in pomdp frame-
work for goal-oriented dialogue system (young et
al., 2013), the proposed model may relax such re-
quirement. however, grounding the generated con-
versation with actions and knowledge is not studied
in this paper. it will be a future work.

the proposed model is related to the recent works
in (shang et al., 2015; vinyals and le, 2015; sor-
doni et al., 2015a), which use an encoder-decoder
framework to model conversation. the closest work
is in (sordoni et al., 2015a). this model differs from
that work in using an attention model, an additional
input similarity feature, and its decoder architecture.
importantly, this model is used not only for genera-
tion as in those previous work, but also for retrieval.
prior work to potentially increase speci   city or
diversity aims at producing multiple outputs (car-
bonell and goldstein, 1998; gimpel et al., 2013) and
our work is the same as in (li et al., 2016) to produce
a single nontrivial output. instead of using an objec-
tive function in (li et al., 2016) that has an indirect
relation to speci   city, our model uses a speci   city
measure directly for training and decoding.

6 conclusions

we have presented a novel attentional neural conver-
sation model with enhanced speci   city using idf. it
has been evaluated for both response generation and
retrieval. we have observed signi   cant performance
improvements in comparison to alternative methods.

references
[ameixa et al.2014] david ameixa, luisa coheur, pedro
fialho, and paulo quaresma. 2014. luck, i am your

father: dealing with out-of-domain requests by using
movies subtitles. in intelligent virtual agents.

[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio.
2015. neural machine
translation by jointly learning to align and translate. in
proceedings of the international conference on learn-
ing representations (iclr), san diego, ca.

[banchs and li2012] rafael e. banchs and haizhou li.
2012. iris: a chat-oriented dialogue system based on
the vector space model. in acl.

[brown et al.1992] peter brown, vincent pietra, robert
mercer, stephen pietra, and jennifer lai. 1992. an
estimate of an upper bound for the id178 of english.
computational linguistics, 18(1):31   40.

[carbonell and goldstein1998] jaime carbonell and jade
goldstein. 1998. the use of mmr, diversity-based
reranking for reordering documents and producing
summaries. research and development in information
retrieval, pages 335   336.

[cho et al.2014] kyunghyun cho, bart van merrienboer,
caglar gulcehre, dzmitry bahdanau, fethi bougares,
holger schwenk, and yoshua bengio. 2014. learning
phrase representations using id56 encoderdecoder for
id151. in emnlp.

[dunning1994] ted dunning. 1994. statistical identi   -
cation of language. technical report technical report
mccs 94-273, new mexico state university.

[galley et al.2015] michel galley, chris quirk, alessan-
dro sordoni, yangfeng ji, michael auli, chris quirk,
margaret mitchell, jianfeng gao, and bill dolan.
2015. deltaid7: a discriminative metric for gener-
ation tasks with intrinsically diverse targets. in acl,
pages 445   450.

[gasic et al.2013] m. gasic, c. breslin, m. henderson,
d. kim, m. szummer, b. thomson, p. tsiakoulis,
and s. young. 2013. online policy optimisation of
bayesian spoken dialogue systems via human interac-
tion. in icassp.

[gimpel et al.2013] kevin gimpel, dhruv batra, chris
dyer, and gregory shakhnarovich. 2013. a system-
atic exploration of diversity in machine translation. in
emnlp.

[grosz and sidner1986] barbara j. grosz and candace l.
sidner. 1986. attention, intentions, and the structure
of discourse. computational linguistics, 12:175   204.
blaise
thomson, and steve young.
2014. word-based
dialog state tracking with recurrent neural networks.
in sigdial.

[henderson et al.2014] matthew henderson,

[hochreiter and schmidhuber1997] sepp hochreiter and
jurgen schmidhuber. 1997. long short-term memory.
neural computation, 9(8).

[li et al.2016] jiwei li, michel galley, chris brockett,
jianfeng gao, and bill dolan. 2016. a diversity-
promopting objective function for neural conversation
model. in naacl.

[liu et al.2016] chia-wei liu, ryan lowe, iulian v. ser-
ban, michael noseworthy, laurent charlin, and joelle
pineau. 2016. how not to evaluate your dialogue
system: an empirical study of unsupervised evalu-
ation metics for dialogue response generation.
in
arxiv:1603.08023 [cs.cl].

[lowe et al.2015] ryan lowe, nissan pow, iulian serban,
and joelle pineau. 2015. the ubuntu dialogue corpus:
a large dataset for research in unstructured multi-turn
dialogue systems. in sigdial.

[lowe et al.2016] ryan lowe, iulian v. serban, mike
noseworthy, laurent charlin, and joelle pineau.
2016. on the evluation of dialogue systems with next
utterance classi   cation. in submitted to sigdial.

[luong et al.2015] minh-thang luong, hieu pham, and
christopher d. manning. 2015. effective approaches
to attention-based id4.
in
emnlp.

[mikolov et al.2011] tomas mikolov, stefan kombrink,
lukas burget, jan honza cernock, and sanjeev khu-
danpur. 2011. extensions of recurrent neural network
language model. in icassp, pages 5528   5531.

[och2003] franz josef och. 2003. minimum error rate

training for id151. in acl.

[papineni et al.2002] kishore papineni, salim roukos,
2002. id7: a
todd ward, and wei-jing zhu.
method for automatic evaluation of machine transla-
tion. in acl.

[pennington et al.2014] jeffrey

richard
socher, and christopher d. manning. 2014. glove:
in emnlp,
global vectors for word representation.
pages 1532   1543.

pennington,

[ramos2003] juan ramos. 2003. using tf-idf to deter-

mine word relevance in document queires. in icml.

[ranzato et al.2016] marcaurelio

sumit
chopra, michael auli,
and wojciech zaremba.
2016. sequence level training with recurrent neural
networks. in iclr.

ranzato,

[ritter et al.2010] alan ritter, colin cherry, and bill
dolan. 2010. unsupervised modeling of twitter con-
versation. in naacl.

[salton and buckley1988] gerard salton and christopher
buckley. 1988. term-weighting approaches in auto-
matic text retrieval. information processing and man-
agement, 24(5):513   523.

[serban et al.2015a] iulian v. serban, alessandro sor-
doni, yoshua bengio, aaron courville, and joelle
pineau. 2015a. building end-to-end dialogue systems
using generative hierachical neural network models.
in aaai.

serban,

[serban et al.2015b] iulian v.

sordoni, yoshua bengio, aaron courville,
joelle pineau.
work generative models for movie dialogues.
arxiv:1507.04808.

alessandro
and
hierarchical neural net-
in

2015b.

[shang et al.2015] lifeng shang, zhengdong lu, and
hang li. 2015. neural responding machine for short-
text conversation. in acl.

[sordoni et al.2015a] alessandro sordoni, michel galley,
michael auli, chris brockett, yangfeng ji, margaret
mitchell, jian-yun nie, jianfeng gao, and bill dolan.
2015a. a neural network approach to context-sensitive
generation of conversation responses. in naacl.

[sordoni et al.2015b] allessandro sordoni, yoshua ben-
gio, hossein vahabi, christina lioma, jacob g. si-
monsen, and jian-yun nie. 2015b. a hierarchical re-
current encoder-decoder for generative context-aware
query suggestion. in cikm.

[sutskever et al.2014] ilya sutskever, oriol vinyals, and
quoc v. le. 2014. sequence to sequence learning
with neural networks. in neural information process-
ing systems (nips), pages 3104   3112, montr  eal.

[sutton and barto1988] richard s. sutton and andrew g.
barto. 1988. id23: an introduc-
tion. mit press.

[tieleman and hinton2012] t. tieleman and g. hinton.
2012. lecture 6.5-rmsprop: divide the gradient by
a running average of its recent magnitude. in cours-
era: neural networks for machine learning.
van

der
maaten and geoffrey hinton. 2008. visualizing data
usin id167. jmlr.

[van der maaten and hinton2008] laurens

[vinyals and le2015] oriol vinyals and quoc v. le.
in icml deep

2015. a nerual converstion model.
learning workshop.

[wen et al.2015] t.-h. wen, m. gasic, n. mrksic, p.-h.
su, d. vandyke, and s. young. 2015. semantically
conditioned lstm-based id86
for spoken dialogue systems. in emnlp.

[williams1992] ronald williams. 1992. simple statis-
tical gradient-   ow algorithms for connectionist rein-
forcement learning. machine learning, 8:229   256.

[williams2009] jason d. williams. 2009. spoken dia-
logue systems: challenges, and opportunities for re-
search. in asru.

[young et al.2013] steve young, milica gasic, blaise
thomson, and jason d. williams. 2013. pomdp-
based statistical spoken id71: a review.
proceedings of the ieee, 101:1160   1179.

[zaremba and sutskever2015] wojciech zaremba

and
ilya sutskever. 2015. id23 neural
turing machines. in arxiv:1505.00521 [cs.lg].

