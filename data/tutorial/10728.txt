5
1
0
2

 

b
e
f
9
1

 

 
 
]
e
n
.
s
c
[
 
 

5
v
9
2
3
2

.

9
0
4
1
:
v
i
x
r
a

under review as a conference paper at iclr 2015

recurrent neural network id173

wojciech zaremba   
new york university
woj.zaremba@gmail.com

ilya sutskever, oriol vinyals
google brain
{ilyasu,vinyals}@google.com

abstract

we present a simple id173 technique for recurrent neural networks
(id56s) with long short-term memory (lstm) units. dropout, the most suc-
cessful technique for regularizing neural networks, does not work well with id56s
and lstms. in this paper, we show how to correctly apply dropout to lstms,
and show that it substantially reduces over   tting on a variety of tasks. these tasks
include id38, id103, image id134, and
machine translation.

   

1

introduction

the recurrent neural network (id56) is neural sequence model that achieves state of the art per-
formance on important tasks that include id38 mikolov (2012), id103
graves et al. (2013), and machine translation kalchbrenner & blunsom (2013).
it is known that
successful applications of neural networks require good id173. unfortunately, dropout
srivastava (2013), the most powerful id173 method for feedforward neural networks, does
not work well with id56s. as a result, practical applications of id56s often use models that are
too small because large id56s tend to over   t. existing id173 methods give relatively small
improvements for id56s graves (2013). in this work, we show that dropout, when correctly used,
greatly reduces over   tting in lstms, and evaluate it on three different problems.

the code for this work can be found in https://github.com/wojzaremba/lstm.

2 related work

dropout srivastava (2013) is a recently introduced id173 method that has been very suc-
cessful with feed-forward neural networks. while much work has extended dropout in various ways
wang & manning (2013); wan et al. (2013), there has been relatively little research in applying it
to id56s. the only paper on this topic is by bayer et al. (2013), who focuses on    marginalized
dropout    wang & manning (2013), a noiseless deterministic approximation to standard dropout.
bayer et al. (2013) claim that conventional dropout does not work well with id56s because the re-
currence ampli   es noise, which in turn hurts learning. in this work, we show that this problem can
be    xed by applying dropout to a certain subset of the id56s    connections. as a result, id56s can
now also bene   t from dropout.

independently of our work, pham et al. (2013) developed the very same id56 id173 method
and applied it to handwriting recognition. we rediscovered this method and demonstrated strong
empirical results over a wide range of problems. other work that applied dropout to lstms is
pachitariu & sahani (2013).

   work done while the author was in google brain.

1

under review as a conference paper at iclr 2015

there have been a number of architectural variants of the id56 that perform better on problems with
long term dependencies hochreiter & schmidhuber (1997); graves et al. (2009); cho et al. (2014);
jaeger et al. (2007); koutn    k et al. (2014); sundermeyer et al. (2012). in this work, we show how
to correctly apply dropout to lstms, the most commonly-used id56 variant; this way of applying
dropout is likely to work well with other id56 architectures as well.

in this paper, we consider the following tasks: id38, id103, and ma-
chine translation. id38 is the    rst task where id56s have achieved substantial suc-
cess mikolov et al. (2010; 2011); pascanu et al. (2013). id56s have also been successfully used
for id103 robinson et al. (1996); graves et al. (2013) and have recently been applied
to machine translation, where they are used for id38, re-ranking, or phrase model-
ing devlin et al. (2014); kalchbrenner & blunsom (2013); cho et al. (2014); chow et al. (1987);
mikolov et al. (2013).

3 regularizing id56s with lstm cells

in this section we describe the deep lstm (section 3.1). next, we show how to regularize them
(section 3.2), and explain why our id173 scheme works.

we let subscripts denote timesteps and superscripts denote layers. all our states are n-dimensional.
t     rn be a hidden state in layer l in timestep t. moreover, let tn,m : rn     rm be an af   ne
let hl
t be an input
transform (w x + b for some w and b). let     be element-wise multiplication and let h0
t to predict yt, since l is the number of layers
word vector at timestep k. we use the activations hl
in our deep lstm.

3.1 long-short term memory units

the id56 dynamics can be described using deterministic transitions from previous to current hidden
states. the deterministic state transition is a function

for classical id56s, this function is given by

id56 : hl   1

t

, hl

t   1     hl

t

t = f (tn,nhl   1
hl

t + tn,nhl

t   1), where f     {sigm, tanh}

the lstm has complicated dynamics that allow it to easily    memorize    information for an extended
number of timesteps. the    long term    memory is stored in a vector of memory cells cl
t     rn. al-
though many lstm architectures that differ in their connectivity structure and id180,
all lstm architectures have explicit memory cells for storing information for long periods of time.
the lstm can decide to overwrite the memory cell, retrieve it, or keep it for the next time step. the
lstm architecture used in our experiments is given by the following equations graves et al. (2013):

lstm : hl   1

t

, hl

t   1     hl

t, cl

t

i
f
o
g

   
      

   
      

sigm
sigm
sigm
tanh

=    
      

t   1, cl
   
      

t   1 + i     g

t = f     cl
cl
t = o     tanh(cl
hl
t)

t2n,4n(cid:18)hl   1
t   1(cid:19)

t
hl

in these equations, sigm and tanh are applied element-wise. figure 1 illustrates the lstm equa-
tions.

3.2 id173 with dropout

the main contribution of this paper is a recipe for applying dropout to lstms in a way that success-
fully reduces over   tting. the main idea is to apply the dropout operator only to the non-recurrent

2

under review as a conference paper at iclr 2015

hl

t

t   1

hl   1
         
output
gate

      
      o
      
   
        

   

hl
t

hl

t   1

t

hl   1
         

input
gate

      
      i
      
   
   
      g
      
   
         
      

input

modulation

gate

hl   1

t

         
         

hl

t   1

cell

      ct
      
       
     
      f
   
      
         
hl   1

         

t

t   1

hl

forget gate

figure 1: a graphical representation of lstm memory cells used in this paper (there are minor
differences in comparison to graves (2013)).

yt   2
   

yt   1
   

yt
   

yt+1
   

yt+2
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

xt   2

xt   1

xt

xt+1

xt+2

figure 2: regularized multilayer id56. the dashed arrows indicate connections where dropout is
applied, and the solid lines indicate connections where dropout is not applied.

connections (figure 2). the following equation describes it more precisely, where d is the dropout
operator that sets a random subset of its argument to zero:

t2n,4n(cid:18)d(hl   1
t   1 (cid:19)

hl

)

t

i
f
o
g

   
      

   
      

=    
      

sigm
sigm
sigm
tanh

   
      

t   1 + i     g

t = f     cl
cl
t = o     tanh(cl
hl
t)

our method works as follows. the dropout operator corrupts the information carried by the units,
forcing them to perform their intermediate computations more robustly. at the same time, we do not
want to erase all the information from the units. it is especially important that the units remember
events that occurred many timesteps in the past. figure 3 shows how information could    ow from
an event that occurred at timestep t     2 to the prediction in timestep t + 2 in our implementation of
dropout. we can see that the information is corrupted by the dropout operator exactly l + 1 times,

3

under review as a conference paper at iclr 2015

yt   2
   

yt   1
   

yt
   

yt+1
   

yt+2
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

xt   2

xt   1

xt

xt+1

xt+2

figure 3: the thick line shows a typical path of information    ow in the lstm. the information is
affected by dropout l + 1 times, where l is depth of network.

the meaning of life is that only if an end would be of the whole supplier. widespread rules are re-
garded as the companies of refuses to deliver.
in balance of the nation    s information and loan
growth associated with the carrier thrifts are in the process of slowing the seed and commercial paper.

the meaning of life is nearly in the    rst several months before the government was addressing such a move as
president and chief executive of the nation past from a national commitment to curb grounds. meanwhile the
government invests overcapacity that criticism and in the outer reversal of small-town america.

figure 4: some interesting samples drawn from a large regularized model conditioned on    the
meaning of life is   . we have removed    unk   ,    n   ,    $    from the set of permissible words.

and this number is independent of the number of timesteps traversed by the information. standard
dropout perturbs the recurrent connections, which makes it dif   cult for the lstm to learn to store
information for long periods of time. by not using dropout on the recurrent connections, the lstm
can bene   t from dropout id173 without sacri   cing its valuable memorization ability.

4 experiments

we present results in three domains: id38 (section 4.1), id103 (section
4.2), machine translation (section 4.3), and image id134 (section 4.4).

4.1 id38

we conducted word-level prediction experiments on the penn tree bank (ptb) dataset marcus et al.
(1993), which consists of 929k training words, 73k validation words, and 82k test words. it has 10k
words in its vocabulary. we downloaded it from tomas mikolov   s webpage   . we trained regularized
lstms of two sizes; these are denoted the medium lstm and large lstm. both lstms have
two layers and are unrolled for 35 steps. we initialize the hidden states to zero. we then use the
   nal hidden states of the current minibatch as the initial hidden state of the subsequent minibatch
(successive minibatches sequentially traverse the training set). the size of each minibatch is 20.

   http://www.fit.vutbr.cz/  imikolov/id56lm/simple-examples.tgz

4

under review as a conference paper at iclr 2015

model

validation set

test set

a single model

pascanu et al. (2013)
cheng et al.
non-regularized lstm
medium regularized lstm
large regularized lstm

120.7
86.2
82.2
model averaging

107.5
100.0
114.5
82.7
78.4

mikolov (2012)
cheng et al.
2 non-regularized lstms
5 non-regularized lstms
10 non-regularized lstms
2 medium regularized lstms
5 medium regularized lstms
10 medium regularized lstms
2 large regularized lstms
10 large regularized lstms
38 large regularized lstms
model averaging with dynamic id56s and id165 models
mikolov & zweig (2012)

100.4
87.9
83.5
80.6
76.7
75.2
76.9
72.8
71.9

83.5
80.6
96.1
84.1
80.0
77.0
73.3
72.0
73.6
69.5
68.7

72.9

table 1: word-level perplexity on the penn tree bank dataset.

the medium lstm has 650 units per layer and its parameters are initialized uniformly in
[   0.05, 0.05]. as described earlier, we apply 50% dropout on the non-recurrent connections. we
train the lstm for 39 epochs with a learning rate of 1, and after 6 epochs we decrease it by a factor
of 1.2 after each epoch. we clip the norm of the gradients (normalized by minibatch size) at 5.
training this network takes about half a day on an nvidia k20 gpu.

the large lstm has 1500 units per layer and its parameters are initialized uniformly in
[   0.04, 0.04]. we apply 65% dropout on the non-recurrent connections. we train the model for
55 epochs with a learning rate of 1; after 14 epochs we start to reduce the learning rate by a factor
of 1.15 after each epoch. we clip the norm of the gradients (normalized by minibatch size) at 10
mikolov et al. (2010). training this network takes an entire day on an nvidia k20 gpu.

for comparison, we trained a non-regularized network. we optimized its parameters to get the best
validation performance. the lack of id173 effectively constrains size of the network, forc-
ing us to use small network because larger networks over   t. our best performing non-regularized
lstm has two hidden layers with 200 units per layer, and its weights are initialized uniformly in
[   0.1, 0.1]. we train it for 4 epochs with a learning rate of 1 and then we decrease the learning rate
by a factor of 2 after each epoch, for a total of 13 training epochs. the size of each minibatch is 20,
and we unroll the network for 20 steps. training this network takes 2-3 hours on an nvidia k20
gpu.

table 1 compares previous results with our lstms, and figure 4 shows samples drawn from a single
large regularized lstm.

4.2 id103

deep neural networks have been used for acoustic modeling for over half a century (see
bourlard & morgan (1993) for a good review). acoustic modeling is a key component in map-
ping acoustic signals to sequences of words, as it models p(st|x) where st is the phonetic state at
time t and x is the acoustic observation. recent work has shown that lstms can achieve excellent
performance on acoustic modeling sak et al. (2014), yet relatively small lstms (in terms of the
number of their parameters) can easily over   t the training set. a useful metric for measuring the
performance of acoustic models is frame accuracy, which is measured at each st for all timesteps
t. generally, this metric correlates with the actual metric of interest, the word error rate (wer).

5

under review as a conference paper at iclr 2015

model
non-regularized lstm 71.6
regularized lstm
69.4

68.9
70.5

training set validation set

table 2: frame-level accuracy on the icelandic speech dataset. the training set has 93k utterances.

test perplexity

model
non-regularized lstm 5.8
regularized lstm
5.0
lium system

test id7 score
25.9
29.03
33.30

table 3: results on the english to french translation task.

since computing the wer involves using a language model and tuning the decoding parameters for
every change in the acoustic model, we decided to focus on frame accuracy in these experiments.
table 2 shows that dropout improves the frame accuracy of the lstm. not surprisingly, the training
frame accuracy drops due to the noise added during training, but as is often the case with dropout,
this yields models that generalize better to unseen data. note that the test set is easier than the train-
ing set, as its accuracy is higher. we report the performance of an lstm on an internal google
icelandic speech dataset, which is relatively small (93k utterances), so over   tting is a great concern.

4.3 machine translation

we formulate a machine translation problem as a language modelling task, where an lstm is trained
to assign high id203 to a correct translation of a source sentence. thus, the lstm is trained on
concatenations of source sentences and their translations sutskever et al. (2014) (see also cho et al.
(2014)). we compute a translation by approximating the most probable sequence of words using a
simple id125 with a beam of size 12. we ran an lstm on the wmt   14 english to french
dataset, on the    selected    subset from schwenk (2014) which has 340m french words and 304m
english words. our lstm has 4 hidden layers, and both its layers and id27s have
1000 units. its english vocabulary has 160,000 words and its french vocabulary has 80,000 words.
the optimal dropout id203 was 0.2. table 3 shows the performance of an lstm trained
with and without dropout. while our lstm does not beat the phrase-based lium smt system
schwenk et al. (2011), our results show that dropout improves the translation performance of the
lstm.

4.4

image id134

we applied the dropout variant to the image id134 model of vinyals et al. (2014). the
image id134 is similar to the sequence-to-sequence model of sutskever et al. (2014),
but where the input image is mapped onto a vector with a highly-accurate pre-trained convolutional
neural network (szegedy et al., 2014), which is converted into a caption with a single-layer lstm
(see vinyals et al. (2014) for the details on the architecture). we test our dropout scheme on lstm
as the convolutional neural network is not trained on the image caption dataset because it is not large
(mscoco (lin et al., 2014)).

our results are summarized in the following table 4. in brief, dropout helps relative to not using
dropout, but using an ensemble eliminates the gains attained by dropout. thus, in this setting, the
main effect of dropout is to produce a single model that is as good as an ensemble, which is a
reasonable improvement given the simplicity of the technique.

5 conclusion

we presented a simple way of applying dropout to lstms that results in large performance in-
creases on several problems in different domains. our work makes dropout useful for id56s, and
our results suggest that our implementation of dropout could improve performance on a wide variety
of applications.

6

under review as a conference paper at iclr 2015

model
non-regularized model
regularized model
10 non-regularized models

test perplexity
8.47
7.99
7.5

test id7 score
23.5
24.3
24.4

table 4: results on the image id134 task.

6 acknowledgments

we wish to acknowledge tomas mikolov for useful comments on the    rst version of the paper.

references
bayer, justin, osendorfer, christian, chen, nutan, urban, sebastian, and van der smagt, patrick. on fast

dropout and its applicability to recurrent networks. arxiv preprint arxiv:1311.0701, 2013.

bourlard, h. and morgan, n. connectionist id103: a hybrid approach. kluwer academic

publishers, 1993.

cheng, wei-chen, kok, stanley, pham, hoai vu, chieu, hai leong, and chai, kian ming a. language

modeling with sum-product networks.

cho, kyunghyun, van merrienboer, bart, gulcehre, caglar, bougares, fethi, schwenk, holger, and bengio,
yoshua. learning phrase representations using id56 encoder-decoder for id151. arxiv
preprint arxiv:1406.1078, 2014.

chow, y, dunham, m, kimball, o, krasner, m, kubala, g, makhoul, j, price, p, roucos, s, and schwartz, r.
byblos: the bbn continuous id103 system. in acoustics, speech, and signal processing, ieee
international conference on icassp   87., volume 12, pp. 89   92. ieee, 1987.

devlin, j., zbib, r., huang, z., lamar, t., schwartz, r., and makhoul, j. fast and robust neural network joint

models for id151. in acl, 2014.

graves, alex. generating sequences with recurrent neural networks. arxiv preprint arxiv:1308.0850, 2013.

graves, alex, liwicki, marcus, fern  andez, santiago, bertolami, roman, bunke, horst, and schmidhuber,
j  urgen. a novel connectionist system for unconstrained handwriting recognition. pattern analysis and
machine intelligence, ieee transactions on, 31(5):855   868, 2009.

graves, alex, mohamed, abdel-rahman, and hinton, geoffrey. id103 with deep recurrent neural
networks. in acoustics, speech and signal processing (icassp), 2013 ieee international conference on,
pp. 6645   6649. ieee, 2013.

hochreiter, sepp and schmidhuber, j  urgen. long short-term memory. neural computation, 9(8):1735   1780,

1997.

jaeger, herbert, luko  sevi  cius, mantas, popovici, dan, and siewert, udo. optimization and applications of

echo state networks with leaky-integrator neurons. neural networks, 20(3):335   352, 2007.

kalchbrenner, n. and blunsom, p. recurrent continuous translation models. in emnlp, 2013.

koutn    k, jan, greff, klaus, gomez, faustino, and schmidhuber, j  urgen. a clockwork id56. arxiv preprint

arxiv:1402.3511, 2014.

lin, tsung-yi, maire, michael, belongie, serge, hays, james, perona, pietro, ramanan, deva, doll  ar, piotr,
and zitnick, c lawrence. microsoft coco: common objects in context. arxiv preprint arxiv:1405.0312,
2014.

marcus, mitchell p, marcinkiewicz, mary ann, and santorini, beatrice. building a large annotated corpus of

english: the id32. computational linguistics, 19(2):313   330, 1993.

mikolov, tom  a  s. statistical language models based on neural networks. phd thesis, ph. d. thesis, brno

university of technology, 2012.

mikolov, tomas and zweig, geoffrey. context dependent recurrent neural network language model. in slt,

pp. 234   239, 2012.

7

under review as a conference paper at iclr 2015

mikolov, tomas, kara     at, martin, burget, lukas, cernock`y, jan, and khudanpur, sanjeev. recurrent neural

network based language model. in interspeech, pp. 1045   1048, 2010.

mikolov, tomas, deoras, anoop, povey, daniel, burget, lukas, and cernocky, jan. strategies for training large
scale neural network language models. in automatic id103 and understanding (asru), 2011
ieee workshop on, pp. 196   201. ieee, 2011.

mikolov, tomas, le, quoc v, and sutskever, ilya. exploiting similarities among languages for machine trans-

lation. arxiv preprint arxiv:1309.4168, 2013.

pachitariu, marius and sahani, maneesh. id173 and nonlinearities for neural language models: when

are they needed? arxiv preprint arxiv:1301.5650, 2013.

pascanu, razvan, gulcehre, caglar, cho, kyunghyun, and bengio, yoshua. how to construct deep recurrent

neural networks. arxiv preprint arxiv:1312.6026, 2013.

pham, vu, kermorvant, christopher, and louradour, j  er  ome. dropout improves recurrent neural networks for

handwriting recognition. arxiv preprint arxiv:1312.4569, 2013.

robinson, tony, hochberg, mike, and renals, steve. the use of recurrent neural networks in continuous speech

recognition. in automatic speech and speaker recognition, pp. 233   258. springer, 1996.

sak, h., vinyals, o., heigold, g., senior, a., mcdermott, e., monga, r., and mao, m. sequence discriminative

distributed training of long short-term memory recurrent neural networks. in interspeech, 2014.

schwenk, holger. university le mans, 2014.

http://www-lium.univ-lemans.fr/  schwenk/cslm_joint/paper.

schwenk, holger, lambert, patrik, barrault, lo    c, servan, christophe, a   i, haithem, abdul-rauf, sadaf, and
shah, kashif. lium   s smt machine translation systems for wmt 2011. in proceedings of the sixth workshop
on id151, pp. 464   469. association for computational linguistics, 2011.

srivastava, nitish. improving neural networks with dropout. phd thesis, university of toronto, 2013.

sundermeyer, martin, schl  uter, ralf, and ney, hermann. lstm neural networks for id38. in

interspeech, 2012.

sutskever, ilya, vinyals, oriol, and le, quoc vv. sequence to sequence learning with neural networks. in

advances in neural information processing systems, pp. 3104   3112, 2014.

szegedy, christian, liu, wei, jia, yangqing, sermanet, pierre, reed, scott, anguelov, dragomir, erhan, du-
mitru, vanhoucke, vincent, and rabinovich, andrew. going deeper with convolutions. arxiv preprint
arxiv:1409.4842, 2014.

vinyals, oriol, toshev, alexander, bengio, samy, and erhan, dumitru. show and tell: a neural image caption

generator. arxiv preprint arxiv:1411.4555, 2014.

wan, li, zeiler, matthew, zhang, sixin, cun, yann l, and fergus, rob. id173 of neural networks
using dropconnect. in proceedings of the 30th international conference on machine learning (icml-13),
pp. 1058   1066, 2013.

wang, sida and manning, christopher. fast dropout training. in proceedings of the 30th international confer-

ence on machine learning (icml-13), pp. 118   126, 2013.

8

