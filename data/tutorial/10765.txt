551

proceedings of the 2016 conference on empirical methods in natural language processing, pages 551   561,

austin, texas, november 1-5, 2016. c(cid:13)2016 association for computational linguistics

longshort-termmemory-networksformachinereadingjianpengcheng,lidongandmirellalapataschoolofinformatics,universityofedinburgh10crichtonstreet,edinburgheh89ab{jianpeng.cheng,li.dong}@ed.ac.uk,mlap@inf.ed.ac.ukabstractinthispaperweaddressthequestionofhowtorendersequence-levelnetworksbetterathandlingstructuredinput.weproposeama-chinereadingsimulatorwhichprocessestextincrementallyfroid113fttorightandperformsshallowreasoningwithmemoryandatten-tion.thereaderextendsthelongshort-termmemoryarchitecturewithamemorynetworkinplaceofasinglememorycell.thisen-ablesadaptivememoryusageduringrecur-rencewithneuralattention,offeringawaytoweaklyinducerelationsamongtokens.thesystemisinitiallydesignedtoprocessasinglesequencebutwealsodemonstratehowtointe-grateitwithanencoder-decoderarchitecture.experimentsonlanguagemodeling,sentimentanalysis,andnaturallanguageid136showthatourmodelmatchesoroutperformsthestateoftheart.1introductionhowcanasequence-levelnetworkinducerelationswhicharepresumedlatentduringtextprocessing?howcanarecurrentnetworkattentivelymemorizelongersequencesinawaythathumansdo?inthispaperwedesignamachinereaderthatautomaticallylearnstounderstandtext.thetermmachineread-ingisrelatedtoawiderangeoftasksfromanswer-ingreadingcomprehensionquestions(clarketal.,2013),tofactandrelationextraction(etzionietal.,2011;faderetal.,2011),ontologylearning(poonanddomingos,2010),andtextualentailment(da-ganetal.,2005).ratherthanfocusingonaspeci   ctask,wedevelopageneral-purposereadingsimula-tor,drawinginspirationfromhumanlanguagepro-cessingandthefactlanguagecomprehensionisin-crementalwithreaderscontinuouslyextractingthemeaningofutterancesonaword-by-wordbasis.inordertounderstandtexts,ourmachinereadershouldprovidefacilitiesforextractingandrepre-sentingmeaningfromnaturallanguagetext,storingmeaningsinternally,andworkingwithstoredmean-ingstoderivefurtherconsequences.ideally,suchasystemshouldberobust,open-domain,andde-gradegracefullyinthepresenceofsemanticrep-resentationswhichmaybeincomplete,inaccurate,orincomprehensible.itwouldalsobedesirabletosimulatethebehaviorofenglishspeakerswhopro-cesstextsequentially,froid113fttoright,   xatingnearlyeverywordwhiletheyread(rayner,1998)andcreatingpartialrepresentationsforsentencepre-   xes(konieczny,2000;tanenhausetal.,1995).languagemodelingtoolssuchasrecurrentneuralnetworks(id56)bodewellwithhumanreadingbe-havior(frankandbod,2011).id56streateachsen-tenceasasequenceofwordsandrecursivelycom-poseeachwordwithitspreviousmemory,untilthemeaningofthewholesentencehasbeenderived.inpractice,however,sequence-levelnetworksaremetwithatleastthreechallenges.the   rstoneconcernsmodeltrainingproblemsassociatedwithvanishingandexplodinggradients(hochreiter,1991;bengioetal.,1994),whichcanbepartiallyamelioratedwithgatedactivationfunctions,suchasthelongshort-termmemory(lstm)(hochreiterandschmidhu-ber,1997),andgradientclipping(pascanuetal.,2013).thesecondissuerelatestomemorycom-pressionproblems.astheinputsequencegetscom-pressedandblendedintoasingledensevector,suf-552

thefbiischasingacriminalontherun.thethefbiischasingacriminalontherun.thethefbifbiischasingacriminalontherun.thethefbifbiisischasingacriminalontherun.thethefbifbiisischasingchasingacriminalontherun.thethefbifbiisischasingchasingaacriminalontherun.thethefbifbiisischasingchasingaacriminalcriminalontherun.thethefbifbiisischasingchasingaacriminalcriminalonontherun.thethefbifbiisischasingchasingaacriminalcriminalononthetherun.thethefbifbiisischasingchasingaacriminalcriminalononthetherunrun.figure1:illustrationofourmodelwhilereadingthesentencethefbiischasingacriminalontherun.colorredrepresentsthecurrentwordbeing   xated,bluerepresentsmemories.shadingindicatesthede-greeofmemoryactivation.   cientlylargememorycapacityisrequiredtostorepastinformation.asaresult,thenetworkgeneral-izespoorlytolongsequenceswhilewastingmemoryonshorterones.finally,itshouldbeacknowledgedthatsequence-levelnetworkslackamechanismforhandlingthestructureoftheinput.thisimposesaninductivebiaswhichisatoddswiththefactthatlanguagehasinherentstructure.inthispaper,wedevelopatextprocessingsystemwhichaddressestheselimitationswhilemaintainingtheincremental,generativepropertyofarecurrentlanguagemodel.recentattemptstorenderneuralnetworksmorestructureawarehaveseentheincorporationofexter-nalmemoriesinthecontextofrecurrentneuralnet-works(westonetal.,2015;sukhbaataretal.,2015;grefenstetteetal.,2015).theideaistousemultiplememoryslotsoutsidetherecurrencetopiece-wisestorerepresentationsoftheinput;readandwriteoperationsforeachslotcanbemodeledasanat-tentionmechanismwitharecurrentcontroller.wealsoleveragememoryandattentiontoempowerarecurrentnetworkwithstrongermemorizationcapa-bilityandmoreimportantlytheabilitytodiscoverrelationsamongtokens.thisisrealizedbyinsert-ingamemorynetworkmoduleintheupdateofare-currentnetworktogetherwithattentionformemoryaddressing.theattentionactsasaweakinductivemodulediscoveringrelationsbetweeninputtokens,andistrainedwithoutdirectsupervision.asapointofdeparturefrompreviouswork,thememorynet-workweemployisinternaltotherecurrence,thusstrengtheningtheinteractionofthetwoandlead-ingtoarepresentationlearnerwhichisabletorea-sonovershallowstructures.theresultingmodel,whichwetermlongshort-termmemory-network(lstmn),isareadingsimulatorthatcanbeusedforsequenceprocessingtasks.figure1illustratesthereadingbehaviorofthelstmn.themodelprocessestextincrementallywhilelearningwhichpasttokensinthememoryandtowhatextenttheyrelatetothecurrenttokenbeingprocessed.asaresult,themodelinducesundirectedrelationsamongtokensasanintermediatestepoflearningrepresentations.wevalidatetheperfor-manceofthelstmninlanguagemodeling,sen-timentanalysis,andnaturallanguageid136.inallcases,wetrainlstmnmodelsend-to-endwithtask-speci   csupervisionsignals,achievingperfor-mancecomparableorbettertostate-of-the-artmod-elsandsuperiortovanillalstms.2relatedworkourmachinereaderisarecurrentneuralnetworkex-hibitingtwoimportantproperties:itisincremental,simulatinghumanbehavior,andperformsshallowstructurereasoningoverinputstreams.recurrentneuralnetwork(id56s)havebeensuc-cessfullyappliedtovarioussequencemodelingandsequence-to-sequencetransductiontasks.thelatterhaveassumedseveralguisesintheliteraturesuchasmachinetranslation(bahdanauetal.,2014),sen-tencecompression(rushetal.,2015),andreadingcomprehension(hermannetal.,2015).akeycon-tributingfactortotheirsuccesshasbeentheabil-itytohandlewell-knownproblemswithexplodingorvanishinggradients(bengioetal.,1994),leadingtomodelswithgatedactivationfunctions(hochre-iterandschmidhuber,1997;choetal.,2014),andmoreadvancedarchitecturesthatenhancethein-formation   owwithinthenetwork(koutn    ketal.,2014;chungetal.,2015;yaoetal.,2015).aremainingpracticalbottleneckforid56sismemorycompression(bahdanauetal.,2014):sincetheinputsarerecursivelycombinedintoasinglememoryrepresentationwhichistypicallytoosmallintermsofparameters,itbecomesdif   culttoaccu-ratelymemorizesequences(zarembaandsutskever,2014).intheencoder-decoderarchitecture,thisproblemcanbesidesteppedwithanattentionmech-anismwhichlearnssoftalignmentsbetweenthede-codingstatesandtheencodedmemories(bahdanau553

etal.,2014).inourmodel,memoryandattentionareaddedwithinasequenceencoderallowingthenetworktouncoverlexicalrelationsbetweentokens.theideaofintroducingastructuralbiastoneu-ralmodelsisbynomeansnew.forexample,itisre   ectedintheworkofsocheretal.(2013a)whoapplyrecursiveneuralnetworksforlearningnaturallanguagerepresentations.inthecontextofrecur-rentneuralnetworks,effortstobuildmodular,struc-turedneuralmodelsdatebacktodasetal.(1992)whoconnectarecurrentneuralnetworkwithanex-ternalmemorystackforlearningcontextfreegram-mars.recently,westonetal.(2015)proposemem-orynetworkstoexplicitlysegregatememorystor-agefromthecomputationofneuralnetworksingen-eral.theirmodelistrainedend-to-endwithamem-oryaddressingmechanismcloselyrelatedtosoftat-tention(sukhbaataretal.,2015)andhasbeenap-pliedtomachinetranslation(mengetal.,2015).grefenstetteetal.(2015)de   neasetofdifferen-tiabledatastructures(stacks,queues,anddequeues)asmemoriescontrolledbyarecurrentneuralnet-work.tranetal.(2016)combinethelstmwithanexternalmemoryblockcomponentwhichinteractswithitshiddenstate.kumaretal.(2016)employastructuredneuralnetworkwithepisodicmemorymodulesfornaturallanguageandalsovisualques-tionanswering(xiongetal.,2016).similartotheabovework,weleveragememoryandattentioninarecurrentneuralnetworkforinduc-ingrelationsbetweentokensasamoduleinalargernetworkresponsibleforrepresentationlearning.asapropertyofsoftattention,allintermediaterela-tionsweaimtocapturearesoftanddifferentiable.thisisincontrasttoshift-reducetypeneuralmod-els(dyeretal.,2015;bowmanetal.,2016)wheretheintermediatedecisionsarehardandinductionismoredif   cult.finally,notethatourmodelcapturesundirectedlexicalrelationsandisthusdistinctfromworkondependencygrammarinduction(kleinandmanning,2004)wherethelearnedhead-modi   erre-lationsaredirected.3themachinereaderinthissectionwepresentourmachinereaderwhichisdesignedtoprocessstructuredinputwhileretain-ingtheincrementalityofarecurrentneuralnetwork.thecoreofourmodelisalongshort-termmem-ory(lstm)unitwithanextendedmemorytapethatexplicitlysimulatesthehumanmemoryspan.themodelperformsimplicitrelationanalysisbetweentokenswithanattention-basedmemoryaddressingmechanismateverytimestep.inthefollowing,we   rstreviewthestandardlongshort-termmemoryandthendescribeourmodel.3.1longshort-termmemoryalongshort-termmemory(lstm)recurrentneu-ralnetworkprocessesavariable-lengthsequencex=(x1,x2,      ,xn)byincrementallyaddingnewcontentintoasinglememoryslot,withgatescon-trollingtheextenttowhichnewcontentshouldbememorized,oldcontentshouldbeerased,andcur-rentcontentshouldbeexposed.attimestept,thememoryctandthehiddenstatehtareupdatedwiththefollowingequations:2664itftot  ct3775=2664ssstanh3775w  [ht 1,xt](1)ct=ft ct 1+it   ct(2)ht=ot tanh(ct)(3)wherei,f,andoaregateactivations.comparedtothestandardid56,thelstmusesadditivemem-oryupdatesanditseparatesthememorycfromthehiddenstateh,whichinteractswiththeenvironmentwhenmakingpredictions.3.2longshort-termmemory-networkthe   rstquestionthatariseswithlstmsistheex-tenttowhichtheyareabletomemorizesequencesunderrecursivecompression.lstmscanproducealistofstaterepresentationsduringcomposition,however,thenextstateisalwayscomputedfromthecurrentstate.thatistosay,giventhecurrentstateht,thenextstateht+1isconditionallyindependentofstatesh1      ht 1andtokensx1      xt.whiletherecur-sivestateupdateisperformedinamarkovmanner,itisassumedthatlstmsmaintainunboundedmem-ory(i.e.,thecurrentstatealonesummarizeswellthetokensithasseensofar).thisassumptionmayfailinpractice,forexamplewhenthesequenceislong554

figure2:longshort-termmemory-network.colorindicatesdegreeofmemoryactivation.orwhenthememorysizeisnotlargeenough.an-otherundesiredpropertyoflstmsconcernsmodel-ingstructuredinput.anlstmaggregatesinforma-tiononatoken-by-tokenbasisinsequentialorder,butthereisnoexplicitmechanismforreasoningoverstructureandmodelingrelationsbetweentokens.ourmodelaimstoaddressbothlimitations.oursolutionistomodifythestandardlstmstructurebyreplacingthememorycellwithamemorynet-work(westonetal.,2015).theresultinglongshort-termmemory-network(lstmn)storesthecontextualrepresentationofeachinputtokenwithauniquememoryslotandthesizeofthememorygrowswithtimeuntilanupperboundofthememoryspanisreached.thisdesignenablesthelstmtoreasonaboutrelationsbetweentokenswithaneuralattentionlayerandthenperformnon-markovstateupdates.althoughitisfeasibletoapplybothwriteandreadoperationstothememorieswithattention,weconcentrateonthelatter.weconceptualizethereadoperationasattentivelylinkingthecurrentto-kentopreviousmemoriesandselectingusefulcon-tentwhenprocessingit.althoughnotthefocusofthiswork,thesigni   canceofthewriteoperationcanbeanalogouslyjusti   edasawayofincremen-tallyupdatingpreviousmemories,e.g.,tocorrectwronginterpretationswhenprocessinggardenpathsentences(ferreiraandhenderson,1991).thearchitectureofthelstmnisshowninfig-ure2andtheformalde   nitionisprovidedasfol-lows.themodelmaintainstwosetsofvectorsstoredinahiddenstatetapeusedtointeractwiththeenvironment(e.g.,computingattention),andamem-orytapeusedtorepresentwhatisactuallystoredinmemory.1therefore,eachtokenisassociatedwithahiddenvectorandamemoryvector.letxtde-notethecurrentinput;ct 1=(c1,      ,ct 1)denotesthecurrentmemorytape,andht 1=(h1,      ,ht 1)theprevioushiddentape.attimestept,themodelcomputestherelationbetweenxtandx1      xt 1throughh1      ht 1withanattentionlayer:ati=vttanh(whhi+wxxt+w  h  ht 1)(4)sti=softmax(ati)(5)thisyieldsaid203distributionoverthehiddenstatevectorsofprevioustokens.wecanthencom-puteanadaptivesummaryvectorfortheprevioushiddentapeandmemorytapedenotedby  ctand  ht,respectively:     ht  ct =t 1  i=1sti     hici (6)andusethemforcomputingthevaluesofctandhtintherecurrentupdateas:2664itftot  ct3775=2664ssstanh3775w  [  ht,xt](7)ct=ft   ct+it   ct(8)ht=ot tanh(ct)(9)wherev,wh,wxandw  harethenewweighttermsofthenetwork.akeyideabehindthelstmnistouseattentionforinducingrelationsbetweentokens.theserela-tionsaresoftanddifferentiable,andcomponentsofalargerrepresentationlearningnetwork.althoughitisappealingtoprovidedirectsupervisionfortheattentionlayer,e.g.,withevidencecollectedfromadependencytreebank,wetreatitasasubmod-ulebeingoptimizedwithinthelargernetworkinadownstreamtask.itisalsopossibletohaveamorestructuredrelationalreasoningmodulebystackingmultiplememoryandhiddenlayersinanalternat-ingfashion,resemblingastackedlstm(graves,1forcomparison,lstmsmaintainahiddenvectorandamemoryvector;memorynetworks(westonetal.,2015)haveasetofkeyvectorsandasetofvaluevectors.555

2013)oramulti-hopmemorynetwork(sukhbaataretal.,2015).thiscanbeachievedbyfeedingtheoutputhktofthelowerlayerkasinputtotheupperlayer(k+1).theattentionatthe(k+1)thlayeriscomputedas:ati,k+1=vttanh(whhk+1i+wlhkt+w  h  hk+1t 1)(10)skip-connections(graves,2013)canbeappliedtofeedxttoupperlayersaswell.4modelingtwosequenceswithlstmnnaturallanguageprocessingtaskssuchasmachinetranslationandtextualentailmentareconcernedwithmodelingtwosequencesratherthanasingleone.astandardtoolformodelingtwosequenceswithrecurrentnetworksistheencoder-decoderar-chitecturewherethesecondsequence(alsoknownasthetarget)isbeingprocessedconditionedonthe   rstone(alsoknownasthesource).inthissectionweexplainhowtocombinethelstmnwhichap-pliesattentionforintra-relationreasoning,withtheencoder-decodernetworkwhoseattentionmodulelearnstheinter-alignmentbetweentwosequences.figures3aand3billustratetwotypesofcombina-tion.wedescribethemodelsmoreformallybelow.shallowattentionfusionshallowfusionsimplytreatsthelstmnasaseparatemodulethatcanbereadilyusedinanencoder-decoderarchitecture,inlieuofastandardid56orlstm.asshowninfigure3a,bothencoderanddecoderaremodeledaslstmnswithintra-attention.meanwhile,inter-attentionistriggeredwhenthedecoderreadsatar-gettoken,similartotheinter-attentionintroducedinbahdanauetal.(2014).deepattentionfusiondeepfusioncombinesinter-andintra-attention(initiatedbythedecoder)whencomputingstateupdates.weusedifferentno-tationtorepresentthetwosetsofattention.follow-ingsection3.2,candhdenotethetargetmemorytapeandhiddentape,whichstorerepresentationsofthetargetsymbolsthathavebeenprocessedsofar.thecomputationofintra-attentionfollowsequa-tions(4)   (9).additionally,weusea=[a1,      ,am]andy=[g1,      ,gm]torepresentthesourcemem-orytapeandhiddentape,withmbeingthelengthofthesourcesequenceconditionedupon.wecomputeinter-attentionbetweentheinputattimesteptandtokensintheentiresourcesequenceasfollows:btj=uttanh(wggj+wxxt+w  g  gt 1)(11)ptj=softmax(btj)(12)afterthatwecomputetheadaptiverepresentationofthesourcememorytape  atandhiddentape  gtas:     gt  at =m  j=1ptj     gjaj (13)wecanthentransfertheadaptivesourcerepresen-tation  attothetargetmemorywithanothergatingoperationrt,analogoustothegatesinequation(7).rt=s(wr  [  gt,xt])(14)thenewtargetmemoryincludesinter-alignmentrt   at,intra-relationft   ct,andthenewinputin-formationit   ct:ct=rt   at+ft   ct+it   ct(15)ht=ot tanh(ct)(16)asshownintheequationsaboveandfigure3b,themajorchangeofdeepfusionliesintherecurrentstorageoftheinter-alignmentvectorinthetargetmemorynetwork,asawaytohelpthetargetnet-workreviewsourceinformation.5experimentsinthissectionwepresentourexperimentsforeval-uatingtheperformanceofthelstmnmachinereader.westartwithlanguagemodelingasitisanaturaltestbedforourmodel.wethenas-sessthemodel   sabilitytoextractmeaningrepre-sentationsforgenericsentenceclassi   cationtaskssuchassentimentanalysis.finally,weexaminewhetherthelstmncanrecognizethesemanticrelationshipbetweentwosentencesbyapplyingittoanaturallanguageid136task.ourcodeisavailableathttps://github.com/cheng6076/snli-attention.556

(a)decoderwithshallowattentionfusion.(b)decoderwithdeepattentionfusion.figure3:lstmnsforsequence-to-sequencemodeling.theencoderusesintra-attention,whilethedecoderincorporatesbothintra-andinter-attention.thetwo   gurespresenttwowaystocombinetheintra-andinter-attentioninthedecoder.modelslayersperplexitykn5   141id561129lstm1115lstmn1108slstm3115glstm3107dlstm3109lstmn3102table1:languagemodelperplexityonthepenntreebank.thesizeofmemoryis300forallmodels.5.1languagemodelingourlanguagemodelingexperimentswerecon-ductedontheenglishpenntreebankdataset.fol-lowingcommonpractice(mikolovetal.,2010),wetrainedonsections0   20(1mwords),usedsec-tions21   22forvalidation(80kwords),andsec-tions23   24(90kwordsfortesting).thedatasetcontainsapproximately1milliontokensandavo-cabularysizeof10k.theaveragesentencelengthis21.weuseperplexityasourevaluationmetric:ppl=exp(nll/t),wherenlldenotesthenega-tiveloglikelihoodoftheentiretestsetandtthecorrespondingnumberoftokens.weusedstochas-ticgradientdescentforoptimizationwithanini-tiallearningrateof0.65,whichdecaysbyafactorof0.85perepochifnosigni   cantimprovementhasbeenobservedonthevalidationset.werenormal-izethegradientifitsnormisgreaterthan5.themini-batchsizewassetto40.thedimensionsofthewordembeddingsweresetto150forallmodels.inthissuiteofexperimentswecomparedthelstmnagainstavarietyofbaselines.the   rstoneisakneser-ney5-gramlanguagemodel(kn5)whichgenerallyservesasanon-neuralbaselineforthelanguagemodelingtask.wealsopresentper-plexityresultsforthestandardid56andlstmmodels.wealsoimplementedmoresophisti-catedlstmarchitectures,suchasastackedlstm(slstm),agated-feedbacklstm(glstm;chungetal.(2015))andadepth-gatedlstm(dlstm;yaoetal.(2015)).thegated-feedbacklstmhasfeedbackgatesconnectingthehiddenstatesacrossmultipletimestepsasanadaptivecontrolofthein-formation   ow.thedepth-gatedlstmusesadepthgatetoconnectmemorycellsofverticallyadjacentlayers.ingeneral,bothglstmanddlstmareabletocapturelong-termdependenciestosomede-gree,buttheydonotexplicitlykeeppastmemories.wesetthenumberoflayersto3inthisexperiment,mainlytoagreewiththelanguagemodelingexper-imentsofchungetal.(2015).alsonotethatthattherearenosingle-layervariantsforglstmanddlstm;theyhavetobeimplementedasmulti-layersystems.thehiddenunitsizeofthelstmnandallcomparisonmodels(exceptkn5)wassetto300.theresultsofthelanguagemodelingtaskareshownintable1.perplexityresultsforkn5andid56aretakenfrommikolovetal.(2015).ascanbeseen,thesingle-layerlstmnoutperformsthese557

hesitsdownatthepianoandplaysourviewisthatwemayseeapro   tdeclineproducts<unk>havetobe   rsttobewinnerseveryoneintheworldiswatchingusverycloselyfigure4:examplesofintra-attention(languagemodeling).boldlinesindicatehigherattentionscores.arrowsdenotewhichwordisbeingfocusedwhenattentioniscomputed,butnotthedirectionoftherelation.twobaselinesandthelstmbyasigni   cantmar-gin.amongstalldeeparchitectures,thethree-layerlstmnalsoperformsbest.wecanstudythemem-oryactivationmechanismofthemachinereaderbyvisualizingtheattentionscores.figure4showsfoursentencessampledfromthepenntreebankval-idationset.althoughweexplicitlyencouragethereadertoattendtoanymemoryslot,muchattentionfocusesonrecentmemories.thisagreeswiththelinguisticintuitionthatlong-termdependenciesarerelativelyrare.asillustratedinfigure4themodelcapturessomevalidlexicalrelations(e.g.,thede-pendencybetweensitsandat,sitsandplays,every-oneandis,isandwatching).notethatarcshereareundirectedandaredifferentfromthedirectedarcsdenotinghead-modi   errelationsindependencygraphs.5.2sentimentanalysisoursecondtaskconcernsthepredictionofsenti-mentlabelsofsentences.weusedthestanfordsen-timenttreebank(socheretal.,2013a),whichcon-tains   ne-grainedsentimentlabels(verypositive,positive,neutral,negative,verynegative)for11,855sentences.followingpreviousworkonthisdataset,modelsfine-grainedbinaryrae(socheretal.,2011)43.282.4rntn(socheretal.,2013b)45.785.4did56(irsoyandcardie,2014)49.886.6did98(blunsometal.,2014)48.586.8id98-mc(kim,2014)48.088.1t-id98(leietal.,2015)51.288.6pv(leandmikolov,2014)48.787.8ct-lstm(taietal.,2015)51.088.0lstm(taietal.,2015)46.484.92-layerlstm(taietal.,2015)46.086.3lstmn47.686.32-layerlstmn47.987.0table2:modelaccuracy(%)onthesentimenttree-bank(testset).thememorysizeoflstmnmodelsissetto168tobecompatiblewithpreviouslypub-lishedlstmvariants(taietal.,2015).weused8,544sentencesfortraining,1,101forval-idation,and2,210fortesting.theaveragesentencelengthis19.1.inaddition,wealsoperformedabi-naryclassi   cationtask(positive,negative)afterre-movingtheneutrallabel.thisresultedin6,920sen-tencesfortraining,872forvalidationand1,821fortesting.table2reportsresultsonboth   ne-grainedandbinaryclassi   cationtasks.weexperimentedwith1-and2-layerlstmns.forthelattermodel,wepredictthesentimentla-belofthesentencebasedontheaveragedhiddenvectorpassedtoa2-layerneuralnetworkclassi   erwithreluastheactivationfunction.themem-orysizeforbothlstmnmodelswassetto168tobecompatiblewithpreviouslstmmodels(taietal.,2015)appliedtothesametask.weusedpre-trained300-dglove840bvectors(penningtonetal.,2014)toinitializethewordembeddings.thegradientforwordswithgloveembeddings,wasscaledby0.35inthe   rstepochafterwhichallwordembeddingswereupdatednormally.weusedadam(kingmaandba,2015)forop-timizationwiththetwomomentumparameterssetto0.9and0.999respectively.theinitiallearningratewassetto2e-3.theid173constantwas1e-4andthemini-batchsizewas5.adropoutrateof0.5wasappliedtotheneuralnetworkclassi   er.wecomparedourmodelwithawiderangeoftop-performingsystems.mostofthesemodels(includ-ingours)arelstmvariants(thirdblockintable2),recursiveneuralnetworks(   rstblock),orconvolu-558

tionalneuralnetworks(id98s;secondblock).re-cursivemodelsassumetheinputsentencesarerep-resentedasparsetreesandcantakeadvantageofannotationsatthephraselevel.lstm-typemodelsandid98saretrainedonsequentialinput,withtheexceptionofct-lstm(taietal.,2015)whichop-eratesovertree-structurednetworktopologiessuchasconstituenttrees.forcomparison,wealsoreporttheperformanceoftheparagraphvectormodel(pv;leandmikolov(2014);seetable2,secondblock)whichneitheroperatesontreesnorsequencesbutlearnsdistributeddocumentrepresentationsparam-eterizeddirectly.theresultsintable2showthatboth1-and2-layerlstmnsoutperformthelstmbaselineswhileachievingnumberscomparabletostateoftheart.thenumberoflayersforourmodelswassettobecomparabletopreviouslypublishedresults.onthe   ne-grainedandbinaryclassi   cationtasksour2-layerlstmnperformsclosetothebestsystemt-id98(leietal.,2015).figure5showsexamplesofintra-attentionforsentimentwords.interestingly,thenetworklearnstoassociatesentimentimportantwordssuchasthoughandfantasticornotandgood.5.3naturallanguageid136theabilitytoreasonaboutthesemanticrelation-shipbetweentwosentencesisanintegralpartoftextunderstanding.wethereforeevaluateourmodelonrecognizingtextualentailment,i.e.,whethertwopremise-hypothesispairsareentailing,contradic-tory,orneutral.forthistaskweusedthestan-fordnaturallanguageid136(snli)dataset(bowmanetal.,2015),whichcontainspremise-hypothesispairsandtargetlabelsindicatingtheirrelation.afterremovingsentenceswithunknownlabels,weendupwith549,367pairsfortraining,9,842fordevelopmentand9,824fortesting.thevocabularysizeis36,809andtheaveragesentencelengthis22.weperformedlower-casingandtok-enizationfortheentiredataset.recentapproachesusetwosequentiallstmstoencodethepremiseandthehypothesisrespectively,andapplyneuralattentiontoreasonabouttheirlogi-calrelationship(rockt  ascheletal.,2016;wangandjiang,2016).furthermore,rockt  ascheletal.(2016)showthatanon-standardencoder-decoderarchitec-turewhichprocessesthehypothesisconditionedonit   stoughtowatchbutit   safantasticmoviealthoughididn   thatethisone,it   snotverygoodeitherfigure5:examplesofintra-attention(sentimentanalysis).boldlines(red)indicateattentionbe-tweensentimentimportantwords.thepremiseresultssigni   cantlyboostsperformance.weuseasimilarapproachtotacklethistaskwithlstmns.speci   cally,weusetwolstmnstoreadthepremiseandhypothesis,andthenmatchthembycomparingtheirhiddenstatetapes.weperformaveragepoolingforthehiddenstatetapeofeachlstmn,andconcatenatethetwoaveragestoformtheinputtoa2-layerneuralnetworkclassi   erwithreluastheactivationfunction.weusedpre-trained300-dglove840bvectors(penningtonetal.,2014)toinitializethewordem-beddings.out-of-vocabulary(oov)wordswereinitializedrandomlywithgaussiansamples(  =0,s=1).weonlyupdatedoovvectorsinthe   rstepoch,afterwhichallwordembeddingswereup-datednormally.thedropoutratewasselectedfrom[0.1,0.2,0.3,0.4].weusedadam(kingmaandba,2015)foroptimizationwiththetwomomentumpa-rameterssetto0.9and0.999respectively,andtheinitiallearningratesetto1e-3.themini-batchsizewassetto16or32.forafaircomparisonagainstpreviouswork,wereportresultswithdifferenthid-den/memorydimensions(i.e.,100,300,and450).wecomparedvariantsofourmodelagainstdif-ferenttypesoflstms(seethesecondblockinta-ble3).speci   cally,theseincludeamodelwhichencodesthepremiseandhypothesisindependentlywithtwolstms(bowmanetal.,2015),asharedlstm(rockt  ascheletal.,2016),aword-by-wordattentionmodel(rockt  ascheletal.,2016),andamatchinglstm(mlstm;wangandjiang(2016)).thismodelsequentiallyprocessesthehypothesis,andateachpositiontriestomatchthecurrentwordwithanattention-weightedrepresentationofthepremise(ratherthanbasingitspredictionsonwholesentenceembeddings).wealsocomparedourmod-559

modelsh|q|mtestbowconcatenation      59.8lstm(bowmanetal.,2015)100221k77.6lstm-att(rockt  ascheletal.,2016)100252k83.5mlstm(wangandjiang,2016)3001.9m86.1lstmn100260k81.5lstmnshallowfusion100280k84.3lstmndeepfusion100330k84.5lstmnshallowfusion3001.4m85.2lstmndeepfusion3001.7m85.7lstmnshallowfusion4502.8m86.0lstmndeepfusion4503.4m86.3table3:parametercounts|q|m,sizeofhiddenunith,andmodelaccuracy(%)onthenaturallan-guageid136task.elswithabag-of-wordsbaselinewhichaveragesthepre-trainedembeddingsforthewordsineachsen-tenceandconcatenatesthemtocreatefeaturesforalogisticregressionclassi   er(   rstblockintable3).lstmnsachievebetterperformancecomparedtolstms(withandwithoutattention;2ndblockintable3).wealsoobservethatfusionisgen-erallybene   cial,andthatdeepfusionslightlyim-provesovershallowfusion.oneexplanationisthatwithdeepfusiontheinter-attentionvectorsarere-currentlymemorizedbythedecoderwithagatingoperation,whichalsoimprovestheinformation   owofthenetwork.withstandardtraining,ourdeepfu-sionyieldsthestate-of-the-artperformanceinthistask.althoughencouraging,thisresultshouldbein-terpretedwithcautionsinceourmodelhassubstan-tiallymoreparameterscomparedtorelatedsystems.wecouldcomparedifferentmodelsusingthesamenumberoftotalparameters.however,thiswouldin-evitablyintroduceotherbiases,e.g.,thenumberofhyper-parameterswouldbecomedifferent.6conclusionsinthispaperweproposedamachinereadingsimula-tortoaddressthelimitationsofrecurrentneuralnet-workswhenprocessinginherentlystructuredinput.ourmodelisbasedonalongshort-termmem-oryarchitectureembeddedwithamemorynetwork,explicitlystoringcontextualrepresentationsofin-puttokenswithoutrecursivelycompressingthem.moreimportantly,anintra-attentionmechanismisemployedformemoryaddressing,asawaytoin-duceundirectedrelationsamongtokens.theat-tentionlayerisnotoptimizedwithadirectsuper-visionsignalbutwiththeentirenetworkindown-streamtasks.experimentalresultsacrossthreetasksshowthatourmodelyieldsperformancecomparableorsuperiortostateoftheart.althoughourexperimentsfocusedonlstms,theideaofbuildingmorestructureawareneuralmodelsisgeneralandcanbeappliedtoothertypesofnet-works.whendirectsupervisionisprovided,simi-lararchitecturescanbeadaptedtotaskssuchasde-pendencyparsingandrelationextraction.inthefu-ture,wehopetodevelopmorelinguisticallyplausi-bleneuralarchitecturesabletoreasonovernestedstructuresandneuralmodelsthatlearntodiscovercompositionalitywithweakorindirectsupervision.acknowledgmentswethankmembersoftheilccattheschoolofinformaticsandtheanonymousreviewersforhelp-fulcomments.thesupportoftheeuropeanre-searchcouncilunderawardnumber681760   trans-latingmultiplemodalitiesintotext   isgratefullyacknowledged.referencesjacobandreas,marcusrohrbach,trevordarrell,anddanklein.2016.learningtocomposeneuralnet-worksforquestionanswering.inproceedingsofthe2016naacl:hlt,pages1545   1554,sandiego,california.dzmitrybahdanau,kyunghyuncho,andyoshuaben-gio.2014.neuralmachinetranslationbyjointlylearningtoalignandtranslate.inproceedingsofthe2014iclr,banff,alberta.yoshuabengio,patricesimard,andpaolofrasconi.1994.learninglong-termdependencieswithgradientdescentisdif   cult.neuralnetworks,ieeetransac-tionson,5(2):157   166.philblunsom,edwardgrefenstette,andnalkalchbren-ner.2014.aconvolutionalneuralnetworkformod-ellingsentences.inproceedingsofthe52ndacl,pages655   665,baltimore,maryland.samuelrbowman,gaborangeli,christopherpotts,andchristopherdmanning.2015.alargeannotatedcor-pusforlearningnaturallanguageid136.inpro-ceedingsofthe2015emnlp,pages22   32,lisbon,portugal.560

samuelrbowman,jongauthier,abhinavrastogi,raghavgupta,christopherdmanning,andchristo-pherpotts.2016.afastuni   edmodelforparsingandsentenceunderstanding.inproceedingsofthe54thacl,pages1466   1477,berlin,germany.kyunghyuncho,bartvanmerri  enboer,caglargul-cehre,dzmitrybahdanau,fethibougares,holgerschwenk,andyoshuabengio.2014.learningphraserepresentationsusingid56encoder-decoderforstatis-ticalmachinetranslation.inproceedingsofthe2014emnlp,pages1724   1734,doha,qatar.junyoungchung,caglargulcehre,kyunghyuncho,andyoshuabengio.2015.gatedfeedbackrecurrentneu-ralnetworks.inproceedingsofthe32ndicml,pages2067   2075,lille,france.peterclark,philharrison,andniranjanbalasubrama-nian.2013.astudyoftheknowledgebaserequire-mentsforpassinganelementarysciencetest.inpro-ceedingsofthe3rdworkshoponautomatedkbcon-struction,sanfrancisco,california.idodagan,orenglickman,andbernardomagnini.2005.thepascalrecognisingtextualentailmentchallenge.inproceedingsofthepascalchallengesworkshoponrecognisingtextualentailment.sreerupadas,c.leegiles,andguozhengsun.1992.learningcontext-freegrammars:capabilitiesandlim-itationsofarecurrentneuralnetworkwithanexter-nalstackmemory.inproceedingsofthe14thannualconferenceofthecognitivesciencesociety,pages791   795.morgankaufmannpublishers.chrisdyer,miguelballesteros,wangling,austinmatthews,andnoahasmith.2015.transition-baseddependencyparsingwithstacklongshort-termmem-ory.inproceedingsofthe53rdacl,pages334   343,beijing,china.orenetzioni,anthonyfader,janarachristensen,stephensoderland,andmausam.2011.openin-formationextraction:thesecondgeneration.inpro-ceedingsofthe22ndijcai,pages3   10,barcelona,spain.anthonyfader,stephensoderland,andorenetzioni.2011.identifyingrelationsforopeninformationex-traction.inproceedingsofthe2011emnlp,pages1535   1545,edinburgh,scotland,uk.fernandaferreiraandjohnm.henderson.1991.recov-eryfrommisanalysesofgarden-pathsentences.jour-nalofmemoryandlanguage,30:725   745.stefanl.frankandrensbod.2011.insensitivityofthehumansentence-processingsystemtohierarchicalstructure.pyschologicalscience,22(6):829   834.alexgraves.2013.generatingsequenceswithrecurrentneuralnetworks.arxivpreprintarxiv:1308.0850.edwardgrefenstette,karlmoritzhermann,mustafasu-leyman,andphilblunsom.2015.learningtotrans-ducewithunboundedmemory.inadvancesinneuralinformationprocessingsystems,pages1819   1827.karlmoritzhermann,tomaskocisky,edwardgrefen-stette,lasseespeholt,willkay,mustafasuleyman,andphilblunsom.2015.teachingmachinestoreadandcomprehend.inadvancesinneuralinformationprocessingsystems,pages1684   1692.sepphochreiterandj  urgenschmidhuber.1997.longshort-termmemory.neuralcomputation,9(8):1735   1780.sepphochreiter.1991.untersuchungenzudynamis-chenneuronalennetzen.diploma,technischeuniver-sit  atm  unchen.ozanirsoyandclairecardie.2014.deeprecursiveneuralnetworksforcompositionalityinlanguage.inadvancesinneuralinformationprocessingsystems,pages2096   2104.yoonkim.2014.convolutionalneuralnetworksforsentenceclassi   cation.inproceedingsofthe2014emnlp,pages1746   1751,doha,qatar.diederikkingmaandjimmyba.2015.adam:amethodforstochasticoptimization.inproceedingsofthe2015iclr,sandiego,california.dankleinandchristophermanning.2004.corpus-basedinductionofsyntacticstructure:modelsofde-pendencyandconstituency.inproceedingsofthe42ndacl,pages478   485,barcelona,spain.larskonieczny.2000.localityandparsingcomplexity.journalofpsycholinguistics,29(6):627   645.jankoutn    k,klausgreff,faustinogomez,andj  urgenschmidhuber.2014.aclockworkid56.inpro-ceedingsofthe31sticml,pages1863   1871,beijing,china.ankitkumar,ozanirsoy,jonathansu,jamesbradbury,robertenglish,brianpierce,peterondruska,ishaangulrajani,andrichardsocher.2016.askmeany-thing:dynamicmemorynetworksfornaturallan-guageprocessing.inproceedingsofthe33rdicml,newyork,ny.quocvleandtomasmikolov.2014.distributedrepresentationsofsentencesanddocuments.inpro-ceedingsofthe31sticml,pages1188   1196,beijing,china.taolei,reginabarzilay,andtommijaakkola.2015.moldingid98sfortext:non-linear,non-consecutiveconvolutions.inproceedingsofthe2015emnlp,pages1565   1575,lisbon,portugal.fandongmeng,zhengdonglu,zhaopengtu,hangli,andqunliu.2015.adeepmemory-basedarchitec-tureforsequence-to-sequencelearning.inproceed-ingsoficlr-workshop2016,sanjuan,puertorico.561

tomasmikolov,martinkara     at,lukasburget,jancer-nock`y,andsanjeevkhudanpur.2010.recurrentneu-ralnetworkbasedlanguagemodel.inproceedingsof11thinterspeech,pages1045   1048,makuhari,japan.tomasmikolov,armandjoulin,sumitchopra,michaelmathieu,andmarc   aurelioranzato.2015.learninglongermemoryinrecurrentneuralnetworks.inpro-ceedingsoficlrworkshop,sandiego,california.razvanpascanu,tomasmikolov,andyoshuabengio.2013.onthedif   cultyoftrainingrecurrentneuralnetworks.inproceedingsofthe30thicml,pages1310   1318,atlanta,georgia.jeffreypennington,richardsocher,andchristopherd.manning.2014.glove:globalvectorsforwordrepresentation.inproceedingsofthe2014emnlp,pages1532   1543,doha,qatar.hoifungpoonandpedrodomingos.2010.unsuper-visedontologyinductionfromtext.inproceedingsofthe48thannualmeetingoftheassociationforcom-putationallinguistics,pages296   305,uppsala.keithrayner.1998.eyemovementsinreadingandin-formationprocessing:20yearsofresearch.psycho-logicalbulletin,124(3):372   422.timrockt  aschel,edwardgrefenstette,karlmoritzher-mann,tom  a  sko  cisk`y,andphilblunsom.2016.rea-soningaboutentailmentwithneuralattention.inpro-ceedingsofthe2016iclr,sanjuan,puertorico.alexandermrush,sumitchopra,andjasonweston.2015.aneuralattentionmodelforabstractivesen-tencesummarization.inproceedingsofthe2015emnlp,pages379   389,lisbon,portugal.richardsocher,erichhuang,jeffreypennin,christo-pherdmanning,andandrewyng.2011.dynamicpoolingandunfoldingrecursiveautoencodersforpara-phrasedetection.inadvancesinneuralinformationprocessingsystems,pages801   809.richardsocher,alexperelygin,jeanwu,jasonchuang,christopherd.manning,andrewng,andchristopherpotts.2013a.recursivedeepmodelsforsemanticcompositionalityoverasentimenttreebank.inpro-ceedingsofthe2013emnlp,pages1631   1642,seat-tle,washington.richardsocher,alexperelygin,jeanywu,jasonchuang,christopherdmanning,andrewyng,andchristopherpotts.2013b.recursivedeepmodelsforsemanticcompositionalityoverasentimenttree-bank.inproceedingsofthe2013emnlp,pages1631   1642,seattle,washingtton.sainbayarsukhbaatar,jasonweston,robfergus,etal.2015.end-to-endmemorynetworks.inadvancesinneuralinformationprocessingsystems,pages2431   2439.kaishengtai,richardsocher,andchristopherdman-ning.2015.improvedsemanticrepresentationsfromtree-structuredlongshort-termmemorynetworks.inproceedingsofthe53rdacl,pages1556   1566,bei-jing,china.michaelk.tanenhaus,michaelj.spivey-knowlton,kathleenm.eberhard,andjuluec.sedivy.1995.in-tegrationofvisualandlinguisticinformationinspokenlanguagecomprehension.science,268:1632   1634.ketran,ariannabisazza,andchristofmonz.2016.re-currentmemorynetworkforlanguagemodeling.inproceedingsofthe15thnaacl,sandiego,ca.shuohangwangandjingjiang.2016.learningnatu-rallanguageid136withlstm.inproceedingsofthe2016naacl:hlt,pages1442   1451,sandiego,california.jasonweston,sumitchopra,andantoinebordes.2015.memorynetworks.inproceedingsofthe2015iclr,sandiego,usa.caimingxiong,stephenmerity,andrichardsocher.2016.dynamicmemorynetworksforvisualandtex-tualquestionanswering.inproceedingsofthe33rdicml,newyork,ny.kaishengyao,trevorcohn,katerinavylomova,kevinduh,andchrisdyer.2015.depth-gatedrecurrentneuralnetworks.arxivpreprintarxiv:1508.03790.wojciechzarembaandilyasutskever.2014.learningtoexecute.arxivpreprintarxiv:1410.4615.